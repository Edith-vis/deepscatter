id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
23a70d774ac5c2871c92dfd9adecae74628eadb9	bi-manual robotic paper manipulation based on real-time marker tracking and physical modelling	robot hand;computer graphics;real time;sensory feedback;three dimensional;robot manipulator;deformable objects;dexterous manipulators;visualization;robot vision computer graphics dexterous manipulators object tracking;visual tracking method bi manual robotic paper manipulation real time marker tracking soft body physics model articulated robot hands anthropomorphic 20 dof shadow dexterous hands occlusion strong deformation mathematical representation;robot vision;three dimensional displays;object tracking;physical modelling;mathematical model;robustness;physical model;mathematical model robot kinematics three dimensional displays robustness cameras visualization;contact geometry;visual tracking;cameras;robot kinematics	The ability to manipulate deformable objects, such as textiles or paper, is a major prerequisite to bringing the capabilities of articulated robot hands closer to the level of manual intelligence exhibited by humans. We concentrate on the manipulation of paper, which affords us a rich interaction domain and that has not yet been solved for anthropomorphic robot hands. A key ability needed for this is the robust tracking and modelling of paper under conditions of occlusion and strong deformation. We present a marker based framework that realizes these properties robustly and in real-time. We compare a purely mathematical representation of the paper manifold with a soft-body-physics model and demonstrate the use of our visual tracking method to facilitate the coordination of two anthropomorphic 20 DOF Shadow Dexterous Hands while they grasp a flat-lying piece of paper, using a combination of visually guided bulging and pinching.	articulated robot;correspondence problem;humans;kinect;mathematical model;physics engine;real-time clock;real-time computing;real-time locating system;real-time transcription;robustification;unfolding (dsp implementation);video tracking	Christof Elbrechter;Robert Haschke;Helge J. Ritter	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094742	three-dimensional space;computer vision;simulation;visualization;eye tracking;physical model;computer science;engineering;artificial intelligence;video tracking;mathematical model;computer graphics;robot kinematics;robustness;contact geometry;computer graphics (images)	Robotics	62.471024613862376	-33.06724304533791	198438
af4f560b30003c906b1faca954328d0108570c18	a finite memory model for haptic teleoperation	robot sensing systems;finite memory model;object recognition;propiocepcion;submarine vehicle;capteur tactile;spatial data interpretation;spatial data;memoire finie;tactile sensor;sensor tactil;manipulateur;proprioception;haptic interfaces remotely operated vehicles teleoperators humans robot sensing systems object recognition system testing feedback orbital robotics space vehicles;teleoperators;robotics;remotely operated vehicles;pattern recognition telecontrol marine systems;orbital robotics;remote operation;artefacto submarino;underwater systems finite memory model haptic teleoperation feature identification spatial data interpretation haptic cognitive model;feedback;manipulador;feature identification;haptic cognitive model;teleaccion;cognition;pattern recognition;telecontrol;robotica;cognicion;system testing;engin sous marin;humans;robotique;reconnaissance forme;haptic teleoperation;reconocimiento patron;haptic interfaces;cognitive model;article;teleaction;manipulator;space vehicles;underwater systems;marine systems;memory model	This paper is concerned with modeling the haptic exploration and recognition of objects using a teleoperator mechanism, mainly underwater. Three factors are identified which determine the success with which objects may be recognized: a) correct feature identification, b) interpretation of spatial data between features, and c) the ability to retain in memory the sequence of features identified. A haptic cognitive model is proposed which incorporates these factors, with emphasis on part c). The performance of the cognitive model is assessed with respect to previously obtained data, and further experiments are conducted to determine the number of features an operator can correctly recall when probing a remote object. >	haptic technology;memory model (programming)	Morris Driels;Philip Beierl	1994	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.286390	remotely operated underwater vehicle;memory model;cognitive model;computer vision;simulation;cognition;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;manipulator;feedback;spatial analysis;proprioception;robotics;system testing;tactile sensor	Robotics	63.23053493187843	-32.27476569269009	198499
6e15e23cd49cb020329be84bd20c3b1e61bfdb26	robot programming for manipulators through volume sweeping and augmented reality		Today's industrial robots require that human operators teach motions in advance. However, conventional methods for robot programming need deep knowledge and skills about robots or great effort for inputting information of working environment into computers. Therefore, a robot programming method in which everyone can easily teach robots “good” motions is demanded. For this purpose, our group proposed a robot programming method that uses manual volume sweeping by operators and automatic motion planning together to generate motion plans with short cycle times. Because a swept volume is the space through which the robot has passed without collision, it is movable space of the robot that can be used in motion planning. In this paper, we proposed using augmented reality in this programming method. We constructed a system in which operators can perceive obtained swept volumes and generated paths intuitively through augmented reality. Teaching experiments showed that non-skilled operators can make a robot move in shorter time than teaching/playback by direct teaching.	augmented reality;computer;experiment;industrial robot;motion planning;prototype	Yasumitsu Sarai;Yusuke Maeda	2017	2017 13th IEEE Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2017.8256120	simulation;collision;operator (computer programming);robot;motion planning;augmented reality;computer science	Robotics	67.96903380423008	-32.17503087431133	199178
