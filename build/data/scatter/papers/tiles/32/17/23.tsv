id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
2d681281b30de6c8cad27c77932515403c251487	towards affordable fault-tolerant nanosatellite computing with commodity hardware		Modern embedded and mobile-market processor technology is a cornerstone of miniaturized satellite design. This type of lighter, cheaper, and rapidly developed spacecraft has enabled a variety of new commercial and scientific missions. However micro-and nanosatellites (<100kg) currently are not considered suitable for critical, high-priority, and complex multi-phased missions, due to their low reliability. The hardware fault tolerance (FT) concepts used aboard larger spacecraft can usually not be used, due to tight energy and mass constraints, as well as disproportional costs. Thus, we developed a hardware-software hybrid FT-approach, which enables FT through software-side coarse-grain lockstep, FPGA reconfiguration, and thread-level mixed criticality. This allows our FPGA-based proof-of-concept implementation to deliver strong fault coverage even for missions with a long duration, but also to adapt to varying performance requirements during the mission. In this paper, we present the implementation results on a tiled multiprocessor system-on-a-chip (MPSoC) design we developed as an ideal platform for our approach. We provide details on the validation of our approach through fault injection, which show that our lockstep implementation is effective and efficient for providing FDIR within our system, and show in direct comparison that our results are consistent with related work. These results show that our architecture is effective, overhead efficient, and remains within the tight energy, complexity, and cost limitations of even very small spacecraft such as CubeSats. To our knowledge, this is the first fault mitigation approach offering strong fault tolerance, which can uphold computational correctness viable for miniaturized spacecraft and is not dependent on proprietary processor cores.		Christian M. Fuchs;Nadia Murillo;Aske Plaat;Erik van der Kouwe	2018	2018 IEEE 27th Asian Test Symposium (ATS)	10.1109/ATS.2018.00034	fault coverage;mpsoc;computer hardware;real-time computing;mixed criticality;fault tolerance;computer science;control reconfiguration;lockstep;architecture;fault injection	EDA	4.584526868244955	57.63905233586574	163264
93a6f897ff3b5380bc5a5bcaa0660f4e4fc463d3	a low-power mixed-signal baseband system design for wireless sensor networks	cmos integrated circuits;ultra low power;platform based design;design process;1 v mixed signal baseband system design wireless sensor networks rf interface system level phase platform based design early late gate synchronization scheme field programmable analog array cmos process 200 muw 0 13 micron;wireless sensor network;field programmable analogue arrays;integrated circuit design;low power;field programmable analog array;low power electronics wireless sensor networks cmos integrated circuits integrated circuit design field programmable gate arrays field programmable analogue arrays;system design;low power electronics;power consumption;field programmable gate arrays;baseband wireless sensor networks design methodology silicon field programmable analog arrays radio frequency process design prototypes field programmable gate arrays energy consumption;wireless sensor networks;design methodology	We present the design methodology and a silicon implementation of a baseband system for use in wireless sensor network applications. Starting from the RF interface, our design process began with a system level phase inspired by the platform-based design (PBD) methodology extended to the analog domain. The functional design was based on an early-late gate synchronization scheme. The PBD approach was used to explore two alternative solutions: a predominantly digital one and a predominantly analog one. To validate the functional aspect of the design, a prototype implementation based on an FPGA and a field programmable analog array (FPAA) was derived using the PBD approach. Finally, we mapped the system level description to silicon aiming at an ultra low power implementation exploiting weak inversion in a 0.13/spl mu/CMOS process leading to an overall power consumption of 200/spl mu/W with a 1V supply.	baseband;cmos;field-programmable analog array;field-programmable gate array;functional design;low-power broadcasting;mixed-signal integrated circuit;platform-based design;programming by demonstration;prototype;radio frequency;systems design	Yanmei Li;Fernando De Bernardinis;Brian P. Otis;Jan M. Rabaey;Alberto L. Sangiovanni-Vincentelli	2005	Proceedings of the IEEE 2005 Custom Integrated Circuits Conference, 2005.	10.1109/CICC.2005.1568606	embedded system;electronic engineering;wireless sensor network;computer science;engineering;electrical engineering	EDA	4.880886789988897	54.830271002760675	163778
7b76849c13c7e8f1ec84d8168d938bb7d4c0e093	hermes-glp: a gals network on chip router with power control techniques	processing element;switching activity;communication system traffic control;network on a chip power control system on a chip mobile communication power dissipation asynchronous communication timing communication system traffic control traffic control displays;noc communication architectures;logic design;dynamic frequency scaling network on chip globally asynchronous locally synchronous;network on chip;clocks;design practice;traffic control;network on chip asynchronous circuits integrated circuit design logic design low power electronics network routing;system on a chip;synchronous system;network routing;chip;integrated circuit design;low power;complex system;synchronization;system design;asynchronous communication;power dissipation;displays;low power soc design hermes glp gals network on chip router power control techniques noc communication architectures gals system design globally asynchronous locally synchronous system;low power electronics;mobile communication;writing;asynchronous circuits;ip networks;power control techniques;low power soc design;power reduction;globally asynchronous locally synchronous system;gals system design;hermes glp gals network on chip router;globally asynchronous locally synchronous;network on a chip;dynamic frequency scaling;power control;timing	The evolution of deep submicron technologies allows the development of increasingly complex Systems on a Chip (SoC). However, this evolution is rendering less viable some well-established design practices. Examples are the use of multi-point communication architectures (e. g. busses) and designing fully synchronous systems. In addition, power dissipation is becoming one of the main design concerns due e. g. to the increasing use of mobile products. An alternative to overcome such problems is adopting Networks on Chip (NoCs) communication architectures supporting globally asynchronous locally synchronous (GALS) system design. This work proposes a GALS router with associated power control techniques, which enables low power SoC design. This is in contrast with previous works which centered attention in power reduction of SoC processing elements instead. The paper describes the asynchronous communication interface and the employed power control mechanism. The results obtained from simulation at the RTL level with timing show that, even when submitted to large rates of traffic injection, the proposed NoC displays a significant reduction in switching activity and consequently in power dissipation.	cpu power dissipation;complex systems;gateway (telecommunications);globally asynchronous locally synchronous;network on a chip;rendering (computer graphics);router (computing);simulation;system on a chip;systems design;very-large-scale integration	Julian J. H. Pontes;Matheus T. Moreira;Rafael Soares;Ney Laert Vilar Calazans	2008	2008 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2008.90	embedded system;electronic engineering;real-time computing;engineering	Arch	3.3139988688993487	59.306830909739695	165014
a375851defd8bc6e683f81678527eb7e87e52b6b	a fault-tolerant attitude determination system based on cots devices	computers;analytical models;software;flash memory;microprocessors;reliability engineering;detectors;cmos integrated circuits;microcontrollers;random access memory;pins;reliability;sensor systems;cmos technology;magnetic fields;circuit faults;fault tolerant;converters;sensors;clocks;routing;satellite fault tolerant cots attitude determination;earth;oscillators;magnetic sensors;magnetometers;testing;event detection;transient analysis;cots devices;failure analysis;computer architecture;recovery capability attitude determination cots devices scientific satellite fault tolerance transient faults failure cases;computer aided software engineering;fault tolerant system;computational modeling;redundancy;magnetic field measurement;monitoring;fault tolerant systems;magnetic separation;satellite;scientific satellite;transient faults;feature extraction;satellites;fault detection;fault tolerance;fault tolerant systems position measurement microcontrollers satellites fault tolerance costs fault detection circuit faults computer vision space missions;position measurement;aerospace electronics;fault tolerance artificial satellites attitude measurement failure analysis;magnetic circuits;transient fault;artificial satellites;space missions;attitude determination;recovery capability;support vector machine classification;space technology;computer science;cots;peer to peer computing;tunneling magnetoresistance;attitude measurement;switches;coordinate measuring machines;radiation hardening;ieee 1394 standard;program processors;integrated circuits;algorithm design and analysis;failure cases;fault diagnosis	In this paper we present a low cost fault-tolerant attitude determination system to a scientific satellite using COTS devices. We related our experience in developing the attitude determination system, where we combine proven fault tolerance techniques to protect the whole system composed only by COTS from the effects produced by transient faults. We detailed the failure cases and the detection, reconfiguration and recovery schemes that assure the fault-tolerant condition. A testbed system was used to inject faults, evaluate the recovery capability of the fault-tolerant system and validate the solution proposed.	error detection and correction;fault tolerance;testbed	Ricardo de Oliveira Duarte;Luiz de Siqueira Martins-Filho;Guilherme F. T. Knop;Ricardo S. Prado	2008	2008 14th IEEE International On-Line Testing Symposium	10.1109/IOLTS.2008.20	reliability engineering;embedded system;fault tolerance;electronic engineering;real-time computing;computer science;engineering;satellite	Robotics	9.53219838382609	59.98332712993271	165339
06a19457a1d81a6489e92db15f4add9686f7accf	multi-phase watermark for ip core protection		Embedding a strong watermark is sufficient to prove IP core ownership during conflict resolution process. However, watermark embedded at lower levels of abstraction may incur design overhead and complexity. Further, watermark embedded at lower level of design does not help to protect a reusable IP core generated during architectural synthesis (at higher abstraction level). This paper presents a low overhead multi-phase watermark implanted during architectural synthesis that is more robust and tamper tolerant than existing single phase watermarks at higher abstraction level. The average reduction percentage of proposed multi-phase watermark approach compared to other related approach in terms of area, latency and cost is greater than 6%, 5% and 6% respectively.	abstraction layer;complexity;embedded system;overhead (computing);principle of abstraction;semiconductor intellectual property core	Anirban Sengupta;Dipanjan Roy	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326058	computer vision;real-time computing;artificial intelligence;latency (engineering);robustness (computer science);digital watermarking;watermark;computer science;abstraction layer;abstraction	EDA	6.43258504384319	56.99404899124187	165441
786d2c81ac5f795806b16a1cf292387261ae3f1a	paceline: improving single-thread performance in nanoscale cmps through core overclocking	multiprocessing systems microprocessor chips;power density;chip multiprocessor;design practice;transient faults paceline core overclocking processor frequencies single thread performance leader checker microarchitecture safely clocked checker core chip multiprocessor power density hardware design complexity;processor frequencies;chip;safely clocked checker core;frequency prefetching yarn hardware application software timing clocks safety manufacturing processes microarchitecture;hardware design complexity;core overclocking;single thread performance;transient faults;transient fault;hardware design;multiprocessing systems;geometric mean;paceline;soft error;microprocessor chips;leader checker microarchitecture	Under current worst-case design practices, manufacturers specify conservative values for processor frequencies in order to guarantee correctness. To recover some of the lost performance and improve single-thread performance, this paper presents the Paceline leader-checker microarchitecture. In Paceline, a leader core runs the thread at higher-than-rated frequency, while passing execution hints and prefetches to a safely-clocked checker core in the same chip multiprocessor. The checker redundantly executes the thread faster than without the leader, while checking the results to guarantee correctness. Leader and checker cores periodically swap functionality. The result is that the thread improves performance substantially without significantly increasing the power density or the hardware design complexity of the chip. By overclocking the leader by 30%, we estimate that Paceline improves SPECint and SPECfp performance by a geometric mean of 21% and 9%, respectively. Moreover, Paceline also provides tolerance to transient faults such as soft errors.	best, worst and average case;clock rate;correctness (computer science);microarchitecture;multi-core processor;multiprocessing;netburst (microarchitecture);overclocking;paging;specfp;specint;soft error;thread (computing)	Brian Greskamp;Josep Torrellas	2007	16th International Conference on Parallel Architecture and Compilation Techniques (PACT 2007)	10.1109/PACT.2007.52	chip;computer architecture;parallel computing;real-time computing;geometric mean;soft error;computer science;operating system;power density	Arch	7.493309353397095	59.56663257714405	166424
c3d45f989639ce81c7892d8d88a8eefbce322b06	design and optimization of communication fabrics: an industrial perspective	energy efficient;design quality;broadcast;networks on chip;time to market;design space exploration;high performance;design methodology;transmission line	Design of SoCs as well as general purpose client and server systems rely increasingly on integrating available IP cores such as processing cores, accelerators and memory blocks to improve energy efficiency, reduce time to market and/or cost. As a result, the communication fabric, i.e. the glue logic between the modules and IP blocks, becomes a critical component with significant implications on system-level power, performance and area. At the same time, if not designed carefully, fabric design can adversely impact the design time and functional correctness due to its complex distributed nature. Therefore, tools and methodologies that are aware of physical constraints (area, wire congestion), design quality (power, performance) and correctness (freedom of deadlocks and livelocks) are needed for efficient design space exploration and fabric generation and design. This tutorial will overview design and optimization of communication fabrics by using examples from both the high performance CMP and SoC domains. Design methodology and outstanding challenges will be illustrated using the xPLORE framework.	correctness (computer science);deadlock;design space exploration;mathematical optimization;network congestion;server (computing);system on a chip;xplore	Ümit Y. Ogras;Michael Kishinevsky	2012		10.1145/2347655.2347662	embedded system;real-time computing;simulation;probabilistic design;design methods;telecommunications;engineering;electrical engineering;operating system;transmission line;efficient energy use;computer network	EDA	3.473395791194616	59.06399327995781	166635
5a371fecc979f202a5135806cb76d41a6396e8b6	design of fault tolerant network interfaces for nocs	fault tolerance networks on chip system on chips network interface;triple modular redundant;random access memory;fault tolerant;network on chip;routing;nickel;integrated circuit design;technology scaling;fault tolerant system;fault tolerant systems;network on chip fault tolerance integrated circuit design;system on chip;registers;fault tolerance;networks on chip;triple modular redundancy implementation fault tolerant network interfaces design networks on chip complex ip based system on chips deep submicron domain functional fault model ni components;single event upset;fault model;network interface;systems on chip;table lookup;soft error;system on chips;nickel fault tolerance fault tolerant systems table lookup random access memory routing registers	Networks-on-Chip (NoCs) appeared as a strategy to deal with the communication requirements of complex IP-based System-on-Chips. As the complexity of designs increases and the technology scales down into the deep-submicron domain, the probability of malfunctions and failures in the NoC components increases. This paper focuses on the study and evaluation of techniques for increasing reliability and resilience of Network Interfaces (NIs). NIs act as interfaces between IP cores and the communication infrastructure, a faulty behavior in them could affect therefore the overall system. In this work, we propose a functional fault model for the NI components, and we present a two-level fault tolerant solution that can be employed for mitigating the effects of both single-event upset soft errors and hard errors on the NI. Experiments show that with a limited overhead we can obtain a significant reliability of the NI, while saving up to 83% in area with respect to a standard Triple Modular Redundancy implementation, as well as a significant energy reduction.	binary decoder;critical path method;deadlock;experiment;fault model;fault tolerance;hamming code;network interface controller;network on a chip;online and offline;overhead (computing);requirement;router (computing);sensor;single event upset;system on a chip;triple modular redundancy;very-large-scale integration	Leandro Fiorin;Laura Micconi;Mariagiovanna Sami	2011	2011 14th Euromicro Conference on Digital System Design	10.1109/DSD.2011.54	embedded system;fault tolerance;parallel computing;real-time computing;computer science;network on a chip;computer network	EDA	6.3984760813815145	59.82613606156721	167104
174171c5632f095601e4f477d3df96612a5c695d	domino effect protection on dataflow error detection and recovery	multicore processing image edge detection parallel processing vectors benchmark testing runtime fault tolerance;parallel programming error detection fault tolerance microprocessor chips;runtime;error free executions domino effect protection dataflow error detection and recovery dfer parallel programming;vectors;image edge detection;multicore processing;fault tolerance;parallel processing;benchmark testing	Dataflow Error Detection and Recovery (DFER) was shown to be a good approach to address errors in the scope of parallel programming. Previous work showed that this technique presents good performance by imposing reduced overhead in error-free executions. However, in the presence of errors excessive rollbacks may occur, characterizing the Domino Effect. In this paper we propose a scheme that addresses this issue by protecting execution from the Domino Effect. Our experimental results show that without adding any significant overheads to the original DFER version we are able to reduce in up to 40% the total execution time in situations where errors are detected. Furthermore, since there are no significant overheads, the execution time in error-free situations remains the same as in the baseline.	baseline (configuration management);dataflow;error detection and correction;executable space protection;fault tolerance;multi-core processor;overhead (computing);parallel computing;programmer;run time (program lifecycle phase);runtime system;software bug;very-large-scale integration	Tiago A. O. Alves;Leandro A. J. Marzulo;Sandip Kundu;Felipe Maia Galvão França	2014	2014 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)	10.1109/DFT.2014.6962087	multi-core processor;embedded system;parallel processing;benchmark;fault tolerance;parallel computing;real-time computing;computer science;operating system;distributed computing	Arch	6.1612178021463055	59.31994770769833	167427
3e6b095328afa9904004cef3008c87744f576c72	fast start-up for spartan-6 fpgas using dynamic partial reconfiguration	high priority bitstream fast start up spartan 6 fpga dynamic partial reconfiguration tool flow configuration method monolithic full device configuration timing critical modules;software;field programmable gate array;control systems;monolithic integrated circuits;dynamic partial reconfiguration;partial reconfiguration;clocks;timing critical modules;monolithic integrated circuits field programmable gate arrays;control system;monolithic full device configuration;configuration method;high priority bitstream;spartan 6 fpga;fast start up;tool flow;field programmable gate arrays vehicle dynamics clocks timing control systems software;field programmable gate arrays;vehicle dynamics;timing	This paper introduces the first available tool flow for Dynamic Partial Reconfiguration on the Spartan-6 family. In addition, the paper proposes a new configuration method called Fast Start-up targeting modern FPGA architectures, where the FPGA is configured in two-steps, instead of using a single (monolithic) full device configuration. In this novel approach, only the timing-critical modules are loaded at power-up using the first high-priority bitstream, while the non-timing critical modules are loaded afterwards. This two-step or prioritized FPGA start-up is used in order to meet the extremely tight startup timing specifications found in many modern applications, like PCI-express or automotive applications. Finally, the developed tool flow and methods for Fast Start-up have been used and tested to implement a CAN-based automotive ECU on a Spartan-6 evaluation board (i.e., SP605). By using this novel approach, it was possible to decrease the initial bitstream size and hence, achieve a configuration time speed-up of up to 4.5×, when compared to a standard configuration solution.	bitstream;design tool;engine control unit;field-programmable gate array;microprocessor development board;pci express	Joachim Meyer;Juanjo Noguera;Michael Hübner;Lars Braun;Oliver Sander;R. M. Gil;Rodney Stewart;Jürgen Becker	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763244	embedded system;electronic engineering;real-time computing;computer science;control system;operating system;field-programmable gate array	EDA	7.193835822677887	54.052992004258954	168118
ff7ba1ef68a2da09787e07cda6d2631ebc68f6a3	high level specification of embedded listeners for monitoring of network-on-chips	noc based multiprocessor system on chip;debugging;simplified language for listeners;noc based systems;noc paradigm;design process;performance monitoring;logic design;network on chip;clocks;monitoring network on a chip hardware buildings debugging process design specification languages filters bandwidth multiprocessing systems;network on chips monitoring;routing strategy embedded listeners network on chips monitoring noc paradigm on chip communication infrastructure debugging performance monitoring noc based systems design process high level specification language sillis simplified language for listeners generic monitoring hardware noc based multiprocessor system on chip;system on a chip;specification language;chip;generic monitoring hardware;on chip communication infrastructure;routing strategy;embedded systems;high level synthesis;integrated circuit design;sillis;monitoring;high level specification language;specification languages;multiprocessor system on chip;program debugging;switches;embedded listeners;specification languages embedded systems high level synthesis integrated circuit design logic design microprocessor chips network on chip program debugging;benchmark testing;microprocessor chips;hardware	Nowadays, the Network-on-Chip (NoC) paradigm has become more and more popular for building an on-chip communication infrastructure. Like in every traditional network, debugging and performance monitoring are also very important issues in NoC-based systems. Unfortunately, the design process of monitoring hardware is a time consuming activity. The work presented in this paper is based on a high level specification language, called SiLLis (Simplified Language for Listeners), for the convenient development of generic monitoring hardware. SiLLis allows the designer to define complex filter rules on a high abstraction level. In this way, the design time as well as the bandwidth requirements for monitoring data can be drastically reduced. To present the benefits of SiLLis, we define a performance monitor that is integrated into a NoC-based multiprocessor System-on-Chip and can be used both to analyze the performance of the system and to optimize the routing strategy at run-time. By using SiLLis, the performance monitor can be realized with a area overhead of only 0.58 % per NoC node.	abstraction layer;debugging;embedded system;high-level programming language;mpsoc;multiprocessing;network on a chip;overhead (computing);programming paradigm;requirement;routing;specification language;system on a chip	Christoph Puttmann;Mario Porrmann;Paolo Roberto Grassi;Marco D. Santambrogio;Ulrich Rückert	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537894	chip;system on a chip;embedded system;benchmark;computer architecture;logic synthesis;real-time computing;design process;specification language;network switch;computer science;operating system;network on a chip;high-level synthesis;debugging;integrated circuit design	Arch	5.516543695975056	54.26096529921903	168520
08e40562da4f207b17bec0d2469f0a8ae475e021	short-circuits on fpgas caused by partial runtime reconfiguration	reconfiguration;multiplexing field programmable gate arrays switches encoding tiles writing clocks;clocks;short circuits;fpga;multiplexing;fpga short circuits partial runtime reconfiguration reconfiguration;writing;tiles;short circuit;bitstream manipulation;field programmable gate arrays;switches;encoding;off the shelf;partial runtime reconfiguration;field programmable gate arrays short circuit fpga partial runtime reconfiguration bitstream manipulation configuration data bitstream scanner;configuration data;bitstream scanner	In this paper, we show how short-circuits on FPGAs can be caused by partial runtime reconfiguration. Short-circuit can even occur on FPGAs that do not offer any tristate resources just by using off the shelf vendor tools without any bitstream manipulation. The duration of the here presented short-circuits ranges from short spikes up to persistent short-circuits that remain active during runtime. Short-circuits will result in increased current consumption and can thus harm the system and must therefore be prevented. An algorithm is derived that detects whether configuration data will cause short-circuits. We implemented this algorithm in a bitstream scanner that can also be used in systems at runtime.	algorithm;bitstream;field-programmable gate array;persistent data structure;run time (program lifecycle phase);toolchain	Christian Beckhoff;Dirk Koch;Jim Tørresen	2010	2010 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2010.117	embedded system;parallel computing;real-time computing;computer science;short circuit;field-programmable gate array	EDA	8.012935396613202	59.60163930789229	168617
801b3979a7b828c4fd54fcbedb3eccab77bb5283	massively parallel analog computing: ariadne’s thread was made of memristors	memristors switches computational modeling resistance instruction sets spice;maze solving memristor memristive grid programmable circuits computing shortest path	This paper explores memristive grids where emergent computation arises through collective device interactions. Computing efficiency of the grids is studied in several scenarios and new composite memristive structures are utilized in shortest path and maze-solving computations. The dependence of the computing medium behavior on the symmetry of both the underlying geometry and the employed devices, is validated through SPICE-level circuit simulations, which highlight important computing inefficiencies. Particular circuit-models of memristive connections enable precise mapping of the target application on the computing medium. Extraordinary functionalities emerge when novel memristive computing components, comprising different electrical characteristics from their structural elements, are introduced in the grid. Applying assisted-computation, by incorporating the concept of Ariadne’s thread, leaded to better computing results, which could find application in routing and path computing problems.	analog computer;computation;emergence;interaction;memristor;routing;spice;shortest path problem;simulation	Ioannis Vourkas;Dimitrios Stathis;Georgios Ch. Sirakoulis	2018	IEEE Transactions on Emerging Topics in Computing	10.1109/TETC.2015.2420353	massively parallel;computer science;grid;memristor;theoretical computer science;computation;analog computer;shortest path problem;thread (computing);instruction set	HPC	7.956012495931517	55.74825076091993	170166
e489ff0739bba0d4ae821d254a6a342376ee0308	modelling correlated transient failures in fault-tolerant systems	time redundancy correlated transient failures modelling fault tolerant systems hardware redundancy real time control applications electromagnetic noise radiation correlated transient failures redundant systems;fault tolerant systems redundancy electromagnetic radiation hardware working environment noise electromagnetic transients process control aircraft actuators protection;electromagnetic transients;integrable model;real time control;working environment noise;actuators;time redundancy;protection;fault tolerant system;fault tolerant computing;redundancy;correlated transient failures modelling;fault tolerant systems;correlated transient failures;modelling fault tolerant computing;redundant systems;process control;electromagnetic noise;real time control applications;radiation;electromagnetic radiation;aircraft;hardware;hardware redundancy	Massive hardware redundancy has long been proposed as a means to achieving high reliability in critical real-time control applications. However, such an approach is only effective against independently occurring failures. Environmental disturbances, such as electromagnetic noise and radiation, often give rise to correlated transient failures in redundant systems. Mere processor redundancy is ineffective against such failures, and time redundancy must be used instead. An integrated model that takes into account both hardware and time redundancy is presented. >	fault tolerance	C. Mani Krishna;Adit D. Singh	1989		10.1109/FTCS.1989.105595	dual modular redundancy;control engineering;electronic engineering;real-time computing;engineering	EDA	9.210213408487189	59.15584843029655	170309
067336421c906bea408776865e960610f7485e44	increasing reliability of programmable mixed-signal systems by applying design diversity redundancy	software;design diversity tmr;double fault tolerance;reliability;circuit faults;fault tolerant;design engineering;mixed signal;prototypes;double fault tolerance reliability programmable mixed signal system design diversity redundancy mixed signal circuit;mixed signal circuit;error analysis;computer architecture;design diversity tmr fault tolerance mixed signal;programmable mixed signal system;redundancy;circuit reliability;fault tolerant systems;digital filters;fault tolerance;redundancy circuit reliability design engineering fault tolerance;low pass filters;tunneling magnetoresistance;fault injection;design diversity redundancy;redundancy circuit faults low pass filters digital filters hardware fault tolerance computer architecture error analysis fault tolerant systems prototypes;hardware	This work explores the concept of design diversity redundancy applied to mixed-signal (MS) circuits. Results from fault injection experiments show a very good ability of the system to tolerate double faults.	experiment;fault injection;mixed-signal integrated circuit	Gabriel de M. Borges;Luiz Fernando Gonçalves;Tiago R. Balen;Marcelo Lubaszewski	2010	2010 15th IEEE European Test Symposium	10.1109/ETSYM.2010.5512730	triple modular redundancy;reliability engineering;embedded system;fault tolerance;electronic engineering;real-time computing;computer science;engineering;redundancy	Embedded	9.79235811045996	59.61903839111916	170755
f248a213067c5e1c01b9562a643b729a7f423c3c	static probabilistic worst case execution time estimation for architectures with faulty instruction caches	hard real-time;wcet;static probabilistic analysis;faulty caches	Semiconductor technology evolution suggests that permanent failure rates will increase dramatically with scaling, in particular for SRAM cells. While well known approaches such as error correcting codes exist to recover from failures and provide fault-free chips, they will not be affordable anymore in the future due to their non-scalable cost. Consequently, other approaches like fine grain disabling and reconfiguration of hardware elements (e.g. individual functional units or cache blocks) will become economically necessary. This fine-grain disabling will lead to degraded performance compared to a fault-free execution.   To the best of our knowledge, all static worst-case execution time (WCET) estimation methods assume fault-free architectures. Their result is not safe anymore when using fine grain disabling of hardware components, which degrades performance. In this paper we provide the first method that statically calculates a probabilistic WCET bound in the presence of permanent faults in instruction caches. The proposed method, from a given program, cache configuration and probability of cell failure, derives a probabilistic WCET bound. The proposed method, because it relies on static analysis, is guaranteed to identify the longest program path, its probabilistic nature only stemming from the presence of faults. The method is computationally tractable because it does not require an exhaustive enumeration of the possible locations of faulty cache blocks. Experimental results show that it provides WCET estimates very close to, but never below, the method that derives probabilistic WCETs by enumerating all possible locations of faulty cache blocks. The proposed method not only allows to quantify the impact of permanent faults on WCET estimates, but also can be used in architectural exploration frameworks to select the most appropriate fault management mechanisms.	best, worst and average case;run time (program lifecycle phase);worst-case execution time	Damien Hardy;Isabelle Puaut	2013		10.1145/2516821.2516842	parallel computing;real-time computing;computer science;distributed computing	EDA	6.337271118644986	58.88612523335061	171180
381a61e1bf7ea7fd4ec004712c5f2084335baaa2	self-repairing digital system with unified recovery process inspired by endocrine cellular communication	cellular radio;light emitting diodes;stem cell bio inspired engineering dynamic routing endocrine cellular communication redundancy self repair;network routing;fault tolerant computing;redundancy;redundancy cellular radio fault tolerant computing field programmable gate arrays light emitting diodes network routing;light emitting diodes self repairing digital system unified recovery process endocrine cellular communication fault tolerant systems complex rerouting process cell replacement functional circuit normal operating hardware fault recovery hardware overhead fault coverage structural layer gene control layer interconnections system module genome faulty module system functions encoded data spare stem module dynamic routing system neighboring spare module field programmable gate array digital clock;field programmable gate arrays;computer architecture microprocessors circuit faults genomics bioinformatics biochemistry hardware	Self-repairing digital systems have recently emerged as the most promising alternative for fault-tolerant systems. However, such systems are still impractical in many cases, particularly due to the complex rerouting process that follows cell replacement. They lose efficiency when the circuit size increases, due to the extra hardware in addition to the functional circuit and the unutilization of normal operating hardware for fault recovery. In this paper, we propose a system inspired by endocrine cellular communication, which simplifies the rerouting process in two ways: 1) by lowering the hardware overhead along with the increasing size of the circuit and 2) by reducing the hardware unutilized for fault recovery while maintaining good fault-coverage. The proposed system is composed of a structural layer and a gene-control layer. The structural layer consists of novel modules and their interconnections. In each module of our system, the encoded data, called the genome, contains information about the function and the connection. Therefore, a faulty module can be replaced and the whole system's functions and connections are maintained by simply assigning the same encoded data to a spare (stem) module. In existing systems, a huge amount of hardware, such as a dynamic routing system, is required for such an operation. The gene-control layer determines the neighboring spare module in the structural layer to replace the faulty module without collision. We verified the proposed mechanism by implementing the system with a field-programmable gate array with the application of a digital clock whose status can be monitored with light-emitting-diodes. In comparison with existing methods, the proposed architecture and mechanism are efficient enough for application with real fault-tolerant systems dealing with harsh and remote environments, such as outer space or deep sea.	algorithm;circuit restoration;compiler;digital forensics framework (dff);digital electronics;diode;dual modular redundancy;flops;fault coverage;fault tolerance;field-programmability;field-programmable gate array;flip-flop (electronics);hardware description language;international components for unicode;language code;mobile phone;overhead (computing);router (computing);routing;scalability;sequential logic;vhdl;vhsic;write combining	Isaak Yang;Sung Hoon Jung;Kwang-Hyun Cho	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2203618	embedded system;routing;electronic engineering;real-time computing;telecommunications;computer science;engineering;electrical engineering;operating system;redundancy;field-programmable gate array;computer network;light-emitting diode	EDA	7.702398734300249	56.91548865694425	171432
1f19708f6b2764a1236d699fd3d1fd4d57af3805	balancing soft error coverage with lifetime reliability in redundantly multithreaded processors	silicon;voltage control;multi threading;reliability;microarchitecture;yarn;microprocessor industry;lifetime reliability;dynamic voltage scaling;soft error protection;power aware computing;hybrid approach;redundancy;redundancy microprocessor chips multi threading power aware computing;dynamic voltage scaling soft error coverage lifetime reliability redundantly multithreaded processors silicon reliability microprocessor industry redundant multithreading soft error protection;redundant multithreading;voltage control silicon protection computer errors microprocessors dynamic voltage scaling logic arrays yarn computer science computer industry;redundantly multithreaded processors;soft error coverage;program processors;soft error;benchmark testing;microprocessor chips;silicon reliability	Silicon reliability is a key challenge facing the microprocessor industry. Processors need to be designed such that they are resilient against both soft errors and lifetime reliability phenomena. However, techniques developed to address one class of reliability problems may impact other aspects of silicon reliability. In this paper, we show that Redundant Multi-Threading (RMT), which provides soft error protection, exacerbates lifetime reliability. We then explore two different architectural approaches to tackle this problem, namely, Dynamic Voltage Scaling (DVS) and partial RMT. We show that each approach has certain strengths and weaknesses with respect to performance, soft error coverage, and lifetime reliability. We then propose and evaluate a hybrid approach that combines DVS and partial RMT. We show that this approach provides better improvement in lifetime reliability than DVS or partial RMT alone, buys back a significant amount of performance that is lost due to DVS, and provides nearly complete soft error coverage.	central processing unit;dynamic voltage scaling;ibm notes;microarchitecture;microprocessor;sensor;soft error;thread (computing);virtual economy	Taniya Siddiqua;Sudhanva Gurumurthi	2009	2009 IEEE International Symposium on Modeling, Analysis & Simulation of Computer and Telecommunication Systems	10.1109/MASCOT.2009.5363142	embedded system;benchmark;parallel computing;real-time computing;multithreading;soft error;microarchitecture;computer science;operating system;reliability;redundancy;silicon	Arch	6.7390879686996445	59.98096077850768	171996
1f4c5b3b511d1c4efb71d1281a0c02ebdaa73073	the design of a latency constrained, power optimized noc for a 4g soc	discrete port configurations;functional timing requirements;optimization program;optimisation;design process;telecommunication links;time measurement;network on chip;benchmark;link bandwidths;simulated annealing;system on a chip;chip;tuning;simulated annealing based mapping process;telecommunication network routing;total router area;telecommunication network routing network on chip optimisation radio access networks simulated annealing telecommunication links;cost optimization;system on chip;energy consumption;power optimization;point to point timing requirements;pair wise delays;bandwidth;packet transmission;time to market;noc interconnect;end to end latency constraints;power consumption;functional unit;benchmark noc interconnect network on chip system on chip power constrained 4g wireless modem module mapping schemes power consumption simulated annealing based mapping process energy consumption packet transmission functional timing requirements end to end latency constraints pair wise delays optimization program power optimized mapping point to point timing requirements discrete port configurations link bandwidths total router area;high performance;power constrained 4g wireless modem;delay constraint optimization design optimization network on a chip productivity throughput time to market process design system on a chip cost function;module mapping schemes;radio access networks;timing;power optimized mapping	Network on-Chip (NoC) is being adopted by chip architects as a means to improve design productivity. As the number of modules connected to a bus increase, its physical implementation becomes very complex, and achieving the desired throughput and latency requires time consuming custom modifications. Conversely, NoCs are designed separately from the functional units of the system to handle all foreseen inter-module communication needs. Their inherent scalable architecture facilitates the integration of the system and shortens the time-to-market of complex products. In this work, we discuss and evaluate the design process of a NoC for a state-of-the-art system on-chip (SoC). More specifically, we describe our experience in designing a cost optimized NoC interconnect for a high-performance, power constrained 4G wireless modem. We focus on the power and performance aspects of various module mapping schemes, looking for a tradeoff that is characterized by a minimal power consumption that still meets the timing requirements of all targeted applications. Using a simulated annealing based mapping process, we place the system's modules on a grid, minimizing the dynamic energy consumed by the transmission of packets over the NoC.	mobile broadband modem;network on a chip;requirement;scalability;simulated annealing;system on a chip;throughput	Rudy Beraha;Isask'har Walter;Israel Cidon;Avinoam Kolodny	2009	2009 3rd ACM/IEEE International Symposium on Networks-on-Chip	10.1109/NOCS.2009.5071449	system on a chip;embedded system;real-time computing;computer science;operating system;network on a chip;computer network	EDA	3.155959860148295	59.95371071873069	172146
2717ceb62b7eee8dd22a1dd37c220ed7cb8498ff	exponent monitoring for low-cost concurrent error detection in fpu control logic	low cost concurrent error detection;logic circuits floating point arithmetic;microprocessors;residue code based method exponent monitoring low cost concurrent error detection fpu control logic nonintrusive concurrent error detection floating point unit datapath corruption ieee 754 floating point representation opensparc t1 processor;floating point unit;concurrent error detection;logic circuits;residue code based method;transient analysis;fpu control logic;assembly;datapath corruption;nonintrusive concurrent error detection;monitoring;exponent monitoring;registers;pipelines;opensparc t1 processor;floating point;error detection;floating point arithmetic;ieee 754 floating point representation;monitoring transient analysis pipelines hardware registers microprocessors assembly;hardware	We present a non-intrusive concurrent error detection (CED) method for protecting the control logic of a contemporary floating point unit (FPU). The proposed method is based on the observation that control logic errors lead to extensive datapath corruption and affect, with high probability, the exponent part of the IEEE 754 floating point representation. Thus, exponent monitoring can be utilized to detect errors in the control logic of the FPU. Predicting the exponent involves relatively simple operations, therefore our method incurs significantly lower overhead than the classical approach of duplicating the control logic of the FPU. Indeed, experimental results on the openSPARC T1 processor show that, as compared to control logic duplication, which incurs an area overhead of 17.9% of the FPU size, our method incurs an area overhead of only 5.8% yet still achieves detection of over 95% of transient errors in the FPU control logic. Moreover, the proposed method offers the ancillary benefit of also detecting 98.1% of datapath errors that affect the exponent, which cannot be detected via duplication of control logic. Finally, when combined with a classical residue code-based method for the fraction, our method leads to a complete CED solution for the entire FPU which provides a coverage of 94.4% of all errors at an area cost of 16.32% of the FPU size.	datapath;error detection and correction;floating-point unit;network packet;opensparc;overhead (computing);sensor;with high probability	Michail Maniatakos;Yiorgos Makris;Prabhakar Kudva;Bruce M. Fleischer	2011	29th VLSI Test Symposium	10.1109/VTS.2011.5783727	embedded system;electronic engineering;parallel computing;real-time computing;computer science;floating point;operating system	EDA	8.158876922211077	59.83371848628605	172314
ec07ba52a7a06d51d7abeb814087b5fc82cd44a2	open source precision timed soft processor for cyber physical system applications	cache storage;processor architecture;multiple thread;multi threading;openfire soft processor;public domain software cache storage field programmable gate arrays instruction sets microprocessor chips multiprocessing systems multi threading precision engineering;branch prediction;multilevel cache hierarchy;fpga implementation open source precision cyber physical system application processor architecture timing predictability branch prediction out of order execution multilevel cache hierarchy precision timed processor open source pret processor cps application openfire soft processor xilinx microblaze multiple thread;open source pret processor;precision engineering;cyber physical systems;public domain software;computer architecture;fpga implementation;precision timed architecture;out of order execution;registers;data dependence;pipelines;cyber physical systems soft processor precision timed architecture;soft processor;cps application;multiprocessing systems;field programmable gate arrays;cyber physical system application;precision timed processor;open source precision;xilinx microblaze;instruction sets pipelines computer architecture registers timing hardware;microprocessor chips;instruction sets;hardware;open source;timing predictability;timing	Modern processor architectures sacrifice timing predictability to improve average performance. Branch prediction, out-of-order execution, and multi-level cache hierarchies complicate accurate execution time estimates. The timing demands of Cyber Physical Systems (CPS) have led some to propose new processor architectures, including Precision Timed (PRET) processors, which simplify analysis of execution time by removing the sources of indeterminacy. This paper presents an open source PRET processor for use in CPS applications based on the Open Fire soft processor clone of the Xilinx Micro Blaze. By interleaving instructions from multiple threads onto the pipeline all data dependencies are removed. Because of this, and with all instructions completing in identical time, accurate execution time calculations are possible. In an FPGA implementation the PRET modifications significantly improve overall throughput with only a modest increase in area.	best, worst and average case;branch predictor;central processing unit;cycle basis;data dependency;field-programmable gate array;forward error correction;microkernel;nondeterministic algorithm;open-source software;openfire;out-of-order execution;overhead (computing);reconfigurable computing;register file;relocation (computing);run time (program lifecycle phase);thread (computing);throughput	Stephen Craven;Daniel Long;Jason Smith	2010	2010 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2010.72	embedded system;computer architecture;parallel computing;real-time computing;multithreading;microarchitecture;computer science;out-of-order execution;operating system;instruction set;pipeline transport;processor register;cyber-physical system;public domain software;branch predictor;field-programmable gate array	Arch	6.94289612675966	58.577565307992174	172326
c54807495b87adc1cf13e50624b38063db536594	standard cell-based low power embedded controller design	standard cells;power gating;clock gating;low power;dvfs;microcontroller	Microcontrollers represent unavoidable parts of state-of-the-art system-on-chips (SoCs) and they are widely embedded as IP blocks. This paper describes design steps and the application of available low-power techniques, to the design of a microcontroller IP core with 8051 instruction set, based on a prescribed standard cell libraries. Choice of the technology node and the cell library supplier is a design challenge that was considered and conclusions reached. The necessary steps of microcontroller design flow are presented which enable power reduction at several abstraction levels. An optimal microcontroller was designed to be embedded in various SoCs. The goal was to get energy-efficient microcontroller operation in applications which don't require intensive data processing. The impact of technology scaling on microcontroller energy efficiency is considered by comparison of the results obtained from implementations in three standard cell technologies. Moreover, power dissipation models are created which allow for microcontroller's power estimation in low throughput sensors networks applications.	embedded controller;embedded system;standard cell	Borisav Jovanovic;Milunka Damnjanovic;Predrag M. Petkovic;Vanco B. Litovski	2015	Journal of Circuits, Systems, and Computers	10.1142/S0218126615500772	microcontroller;embedded system;electronic engineering;real-time computing;computer science;operating system;computer-on-module;clock gating	EDA	3.177154667114836	55.06596274346622	172344
1cdcaf1d72525051d456d911492399813c11ac97	runtime techniques to mitigate soft errors in network-on-chip (noc) architectures		As aggressive scaling continues to push multiprocessor system-on-chips (MPSoCs) to new limits, complex hardware structures combined with stringent area and power constraints will continue to diminish reliability. Waning reliability in integrated circuits will increase the susceptibility of transient and permanent faults. There is an urgent demand for adaptive error correction coding (ECC) schemes in network-on-chips to provide fault tolerance and improve overall resiliency of MPSoC architectures. The goal of adaptive ECC schemes should be to maximize power savings when faults are infrequent and increase application speedup by boosting fault coverage when faults are frequent. In this paper, we propose runtime adaptive scrubbing (RAS), a novel multilayered error correction and detection scheme with three modes of operation enabled by an area-efficient configurable encoder for encoding packets on the switch-to-switch (s2s) layer, thus preventing faults from accumulating up the network stack and onto the end-to-end layer. As fault rates fluctuate we propose a dynamic methodology for improving fault localization and intelligently adapt fault coverage on demand to sustain graceful network degradation. RAS successfully improves network resiliency, fault localization, and fault coverage as compared to traditional static s2s schemes. Simulation results demonstrate that static RAS improves network speedup by 10% for Splash-2/PARSEC benchmarks on a  $8 \times 8$  mesh network while reducing area overhead by 14% and incurring on an average 6.6% power penalty by boosting fault tolerance when fault rates increase. Further, our dynamic RAS scheme maintains 97.88% of network performance for real applications while incurring 20% power penalty.	benchmark (computing);block cipher mode of operation;electronic data processing;elegant degradation;encoder;end-to-end principle;error detection and correction;fault coverage;fault tolerance;forward error correction;image scaling;integrated circuit;line code;mpsoc;memory scrubbing;mesh networking;multiprocessing;network on a chip;network performance;overhead (computing);protocol stack;simulation;soft error;speedup;system on a chip	Travis Boraten;Avinash Karanth Kodi	2018	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2664066	mpsoc;software fault tolerance;real-time computing;stuck-at fault;electronic engineering;fault coverage;parallel computing;computer science;fault tolerance;general protection fault;network performance;network on a chip	EDA	6.326661144639537	60.16894653809435	173145
883870ee0900dd84ab3585acbd69917d6cd1e012	pipeline optimizations of architecting stt-ram as registers in rad-hard environment		Electromagnetic radiation effects can cause several types of errors on traditional SRAM-based registers such as single event upset (SEU) and single event functional interrupt (SEFI). Especially in aerospace where radiation is quite intense, the stability and correctness of systems are greatly affected. By exploiting the beneficial features of high radiation resistance and non-volatility, spin-transfer torque RAM (STT-RAM), a kind of emerging nonvolatile memory (NVM), is promising to be used as registers to avoid errors caused by radiation. However, substituting SRAM with STT-RAM in registers will affect system performance because STT-RAM suffers from long write latency. The early write termination (EWT) method has been accepted as an effective technique to mitigate write problems by terminating redundant writes. Based on the above background, this paper proposes to build registers by STT-RAM for embedded systems in rad-hard environment. Targeting the microarchitecture level of pipeline, the impact of architecting STT-RAM-based registers is discussed considering data hazard due to data dependencies. Furthermore, integrated with the EWT technique, a Read Merging method is proposed to eliminate redundant normal reads or sensing reads which are conducted along with a write. As a result of carrying out these actions, the energy and performance can be improved greatly. The results report 68% (and 75%) and 32% (and 39%) improvements on performance (and energy) by the proposed Read Merging method compared to the cases where STT-RAM is naively used as registers and intelligently used by integrating EWT, respectively.	correctness (computer science);data dependency;embedded system;hazard (computer architecture);microarchitecture;newman's lemma;non-volatile memory;nonvolatile bios memory;radiation hardening;single event upset;static random-access memory;volatility	Zhiyao Gong;Keni Qiu;Weiwen Chen;Yuanhui Ni;Yuanchao Xu;Jianlei Yang	2017	2017 IEEE Trustcom/BigDataSE/ICESS	10.1109/Trustcom/BigDataSE/ICESS.2017.321	latency (engineering);static random-access memory;interrupt;parallel computing;computer security;computer science;single event upset;real-time computing;hazard (computer architecture);non-volatile memory;microarchitecture;radiation	EDA	6.970687125661102	60.25116602725426	173227
bbf4ab84ec5ca28611291e2851fdd825e8e69e0c	a method based on bochs for accelerating the x86 timing emulator	bochs;timing simulator;x86 compatible processor	System structure software simulation technology is a method on realizing the simulation of the computer system hardware in the system structure levels of functionality and performance characteristics by applying the software. The system structure software simulation technology has become an indispensable link for the design, evaluation, and verification of a modern computer system. The speed is very important factor to constrain the use of simulator. Two levels of technical measures from the development and running of simulator are taken to improve the simulator speed. A method based on Bochs is presented and improve to verify the timing simulator. This approach can maintain the accuracy of timing simulation based on the timing simulator to further accelerate the speed of the verification. © 2011 Springer-Verlag Berlin Heidelberg.		J.-M. Huang;R. R. Xiao;Hongyan Guo;Kaixin Han	2011		10.1007/978-3-642-23756-0_52	computer architecture	EDA	3.1554178566746938	56.35522110066963	173427
24b761ab6cf597f132b66d3b87a29979a01edaf0	an improved technique for reducing false alarms due to soft errors	false alarm identification;microprocessors;roll back recovery system;concurrent error detection;error correction redundancy cmos technology microprocessors system performance registers cmos logic circuits art computer aided instruction usa councils;check pointing interval;system performance;checkpointing;error analysis;fault tolerance;microprocessor chips checkpointing error detection fault tolerance;soft errors;error detection;exclusion techniques soft errors error detection false alarm identification roll back recovery system check pointing interval;soft error;microprocessor chips;exclusion techniques	A significant fraction of soft errors in modern microprocessors has been reported to never lead to a system failure. Any concurrent error detection scheme that raises alarm every time a soft error is detected is not well heeded because most of these alarms are false and responding to them will affect system performance negatively. This paper improves state of the art in detecting and preventing false alarms. Existing techniques are enhanced by a methodology to handle soft errors on address bits. Furthermore, we demonstrate benefit of false alarm identification in implementing a roll-back recovery system by first calculating the optimum check pointing interval for a roll-back recovery system and then showing that the optimal number of check-points decreases by orders of magnitude when exclusion techniques are used even if the implementation of exclusion technique is not perfect	error detection and correction;microprocessor;sensor;soft error	Sandip Kundu;Ilia Polian	2006	12th IEEE International On-Line Testing Symposium (IOLTS'06)	10.1109/IOLTS.2006.10	embedded system;fault tolerance;electronic engineering;real-time computing;error detection and correction;soft error;computer hardware;computer science;computer performance	Embedded	7.383584796197809	60.16616572885082	173642
e7f1ae5d83e63af00d727f89dfc318ebd2834090	variability mitigation in nanometer cmos integrated systems: a survey of techniques from circuits to software	resilient systems cmos integrated circuits performance evaluation error analysis parallel architectures;cmos integrated circuits;performance evaluation;variability approximate computing resilient systems timing errors;error analysis;parallel architectures;resilient systems	Variation in performance and power across manufactured parts and their operating conditions is an accepted reality in modern microelectronic manufacturing processes with geometries in nanometer scales. This article surveys challenges and opportunities in identifying variations, their effects and methods to combat these variations for improved microelectronic devices. We focus on computing devices and their design at various levels to combat variability. First, we provide a review of key concepts with particular emphasis on timing errors caused by various variability sources. We consider methods to predict and prevent, detect and correct, and finally conditions under which such errors can be accepted; we also consider their implications on cost, performance and quality. We provide a comparative evaluation of methods for deployment across various layers of the system from circuits, architecture, to application software. These can be combined in various ways to achieve specific goals related to observability and controllability of the variability effects, providing means to achieve cross-layer or hybrid resilience. We then provide examples of real world resilient single-core and parallel architectures. We find that parallel architectures and parallelism in general provide the best means to combat and exploit variability to design resilient and efficient systems. Using programmable accelerator architectures such as clustered processing elements and GP-GPUs, we show how system designers can coordinate propagation of timing error information and its effects along with new techniques for memoization (i.e., spatial or temporal reuse of computation). This discussion naturally leads to use of these techniques into emerging area of “approximate computing,” and how these can be used in building resilient and efficient computing systems. We conclude with an outlook for the emerging field.	approximate computing;approximation algorithm;best, worst and average case;cmos;central processing unit;channel (communications);characteristic impedance;clock rate;computation;conceptualization (information science);cyber-physical system;data point;distributed computing;emoticon;explicit parallelism;general-purpose computing on graphics processing units;graphics processing unit;heart rate variability;image scaling;interaction;linear programming relaxation;memoization;microsoft outlook for mac;numerical analysis;occam's razor;parallel computing;pervasive informatics;requirement;rollback (data management);sampling (signal processing);scalability;semiconductor device fabrication;single-core;software deployment;software propagation;spatial variability;systems design;web search engine	Abbas Rahimi;Luca Benini;Rajesh K. Gupta	2016	Proceedings of the IEEE	10.1109/JPROC.2016.2518864	electronic engineering;real-time computing;computer science;theoretical computer science;cmos	EDA	4.841289968117089	58.34954428777436	173683
e39676949049e88d357c70c23c7f025a721b1a17	escaping the academic sandbox: realizing vpr circuits on xilinx devices	bitstream;cad;hardware description languages;xdl;field programmable gate arrays video recording design automation hardware design languages routing table lookup digital signal processing;network routing;public domain software;vpr;temperature vpr xdl bitstream power;field programmable gate arrays;temperature;public domain software cad field programmable gate arrays hardware description languages network routing;size 40 nm academic sandbox vpr circuits open source method fpga cad researchers xilinx devices verilog to routing suite place and route cad tool xilinx bitstreams xilinx design language xdl map netlist par tool virtex 6 device cad runtime closed source equivalent physical measurements on chip power consumption die temperature vtr to bitstream heterogeneous packing;power	This paper presents a new, open-source method for FPGA CAD researchers to realize their techniques on real Xilinx devices. Specifically, we extend the Verilog-To-Routing (VTR) suite, which includes the VPR place-and-route CAD tool on which many FPGA innovations have been based, to generate working Xilinx bitstreams via the Xilinx Design Language (XDL). Currently, we can faithfully translate VPR's heterogeneous packing and placement results into an exact Xilinx `map' netlist, which is then routed by its `par' tool. We showcase the utility of this new method with two compelling applications targeting a 40nm Virtex-6 device: a fair comparison of the area, delay, and CAD runtime of academia's state-of-the-art VTR How with a commercial, closed-source equivalent, along with a CAD experiment evaluated using physical measurements of on-chip power consumption and die temperature, over time. This extended How - VTR-to-Bitstream - is released to the community with the hope that it can enhance existing research projects as well as unlock new ones.	ansi escape code;algorithm;american and british english spelling differences;bitstream;computer-aided design;discrepancy function;download;field-programmable gate array;mathematical optimization;netlist;open-source software;place and route;power optimization (eda);routing;sim lock;set packing;static timing analysis;verilog;xilinx ise	Eddie Hung;Fatemeh Eslami;Steven J. E. Wilton	2013	2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2013.40	embedded system;routing;parallel computing;real-time computing;temperature;computer science;operating system;power;cad;hardware description language;programming language;bitstream;public domain software;field-programmable gate array	EDA	6.539884794128581	54.66614783820749	173831
270be28ce4f4c203bb1a7e704cc064772e7efc38	extension of systemc framework towards power analysis	power analysis;integrated circuit;power estimation;hardware description languages;integrated circuit design c language hardware description languages;integrated circuit design;c language;character generation;power dissipation;power estimations systemc c framework power analysis tool complex integrated circuits design high abstraction level system description pktool environment	In the last years power dissipation has assumed a more and more relevant role in the design of complex integrated circuits, leading to an increasing need for effective power analysis tools. At present, a major demand is given by the capability to perform power estimations and optimizations in the first design phases, onto high abstraction level system description. As a consequence, the modern tools are required to be compatible with design languages able to address high abstraction levels, such as SystemC/C++. This paper describes the framework of a recently developed SystemC/C++ power analysis tool, the PKtool environment. The discussion underlines the followed implementation approach as fundamental guideline for explaining the features of this tool and the conceptual differences with other commercial tools.	abstraction layer;c++;integrated circuit;systemc	Massimo Conti;Giovanni B. Vece;Sara Colazilli	2009	2009 Forum on Specification & Design Languages (FDL)		embedded system;computer architecture;real-time computing;power analysis;computer science;dissipation;theoretical computer science;integrated circuit;hardware description language;high-level synthesis;programming language;power optimization;statistics;integrated circuit design	EDA	4.1345905375771554	54.59939188031609	174000
578385e47a3576ba650fa1c85278240bd95b1aff	processor design with asymmetric reliability	reliability engineering;error correction codes;asymmetric fault tolerant design techniques processor design asymmetric reliability versatile asymmetric error detection correction framework instruction level vulnerability analysis noisy network asymmetric error correction coding schemes;decoding;reliability exploration;reliability exploration asymmetric reliability high level processor design;reliability engineering decoding error correction codes vliw runtime encoding;integrated circuit reliability error detection codes fault tolerance;runtime;asymmetric reliability;vliw;encoding;high level processor design	Continuous shrinking of device size has introduced reliability as a new design challenge for embedded processors. Error mitigation techniques trade off reliability for other design metrics such as performance and power consumption. State-of-the-art fault-tolerant designs involve cross-layer error management, which lead to an over-protected system. To address the overhead issue, asymmetric reliability utilizes unequal protection levels for different system components based on various criticality requirements. In this paper, We propose a versatile asymmetric error detection/correction framework based on instruction-level vulnerability analysis. Inspired from information-theoretic view of processor as a noisy network, asymmetric error correction coding schemes are designed and exploited to efficiently trade off reliability for other performance constraints. Multiple novel asymmetric fault-tolerant design techniques are proposed, which are evaluated through a range of experiments.	central processing unit;embedded system;error detection and correction;experiment;fault tolerance;forward error correction;information theory;overhead (computing);processor design;requirement;self-organized criticality;software design	Zheng Wang;Goutam Paul;Anupam Chattopadhyay	2014	2014 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2014.63	electronic engineering;parallel computing;real-time computing;computer science	Embedded	7.210339329296525	60.20000324806601	174284
2266f0132bb110ab1f25c365122c57ebdd987392	a novel low overhead fault tolerant kogge-stone adder using adaptive clocking	parallel computing;fault tolerance clocks throughput degradation adders circuit faults concurrent computing manufacturing processes design for manufacture bridge circuits;degradation;low overhead fault tolerant kogge stone adder;concurrent computing;circuit faults;adaptive clocking;fault tolerant;shift operator;fault tolerant technique;clocks;flip flop circuits;design for manufacture;flip flops;kogge stone adder;testing;arithmetic logic unit;defects;alu design;arithmetic logic unit low overhead fault tolerant kogge stone adder adaptive clocking fault tolerant technique redundancy alu design parallel computing output register fault model fault free adder scan flip flops superscalar processor faulty adder throughput degradation reduction superscalar pipeline defective adder;speed;manufacturing processes;faulty adder;fault tolerant adder;fault free adder;redundancy;scheduling stuck at faults fault tolerant adder adaptive clocking kogge stone adder;scan flip flops;superscalar pipeline;adders;scheduling;stuck at faults;output register;pipelines;fault tolerance;yield loss;superscalar processor;flip flops adders fault tolerance fault trees;sulfate minerals;defective adder;process engineering;fault model;high speed;high frequency;flip flop;bridge circuits;industrial engineering;throughput degradation reduction;throughput;fault trees	As the feature size of transistors gets smaller, fabricating them becomes challenging. Manufacturing process follows various corrective design-for-manufacturing (DFM) steps to avoid shorts/opens/bridges. However, it is not possible to completely eliminate the possibility of such defects. If spare units are not present to replace the defective parts, then such failures cause yield loss. In this paper, we present a fault tolerant technique to leverage the redundancy present in high speed regular circuits such as Kogge-Stone adder (KSA). Due to its regularity and speed, KSA is widely used in ALU design. In KSA, the carries are computed fast by computing them in parallel. Our technique is based on the fact that even and odd carries are mutually exclusive. Therefore, defect in even bit can only corrupt the even Sum outputs whereas the odd Sums are computed correctly (and vice versa). To efficiently utilize the above property of KSA in presence of defects, we perform addition in two- clock cycles. In cycle-1, one of the correct set of bits (even or odd) are computed and stored at output registers. In cycle-2, the operands are shifted by one bit and the remaining sets of bits (odd or even) are computed and stored. This allows us to tolerate the defect at the cost of throughput degradation while maintaining high frequency and yield. The proposed technique can tolerate any number of faults as long as they are confined to either even or odd bits (but not in both). Further, this technique is applicable for any type of fault model (stuck-at, bridging, complete opens/shorts). We performed simulations on 64-bit KSA using 180nm devices. The results indicate that the proposed technique incur less that 1% area overhead. Note that there is very little throughput degradation (<0.3%) for the fault-free adders. The proposed technique utilizes the existing scan flip-flops for storage and shifting operation to minimize the area/performance overhead. Finally, the proposed technique is used in a superscalar processor, whereby the faulty adder is assigned lower priority than fault-free adders to reduce the overall throughput degradation. Experiments performed using Simplescalar for a superscalar pipeline (with four integer adders) show throughput degradation of 0.5% in the presence of a single defective adder.	64-bit computing;adder (electronics);arithmetic logic unit;bridging (networking);carry-lookahead adder;clock rate;clock signal;computation;crazy stone (software);design for manufacturability;elegant degradation;flops;fault model;fault tolerance;flip-flop (electronics);image scaling;mutual exclusion;operand;overhead (computing);pipeline (computing);semiconductor research corporation;simulation;software bug;sparse matrix;superscalar processor;throughput;transistor	Swaroop Ghosh;Patrick Ndai;Kaushik Roy	2008	2008 Design, Automation and Test in Europe	10.1145/1403375.1403462	embedded system;fault tolerance;electronic engineering;parallel computing;real-time computing;concurrent computing;computer science;engineering	EDA	8.024484398248884	58.71453258074255	174668
6870835e35c3e4b524dff085996ea992c5c3ca45	evaluation techniques for on-line testing of robust systems based on critical tasks distribution	single bit upset sensitivity;automotive electronics;collaborative hardening;random access memory;circuit faults emulation sensitivity hardware robustness random access memory real time systems;circuit faults;network links;on line testing;emulation;error mitigation techniques;error accumulation;sensitivity;error propagation;error mitigation;evaluation techniques;aerospace electronics;critical task distribution;space vehicle electronics automotive electronics;robustness;hardware emulation evaluation techniques online testing robust digital system design critical task distribution functional complexity circuit sensitiveness aerospace electronics automotive electronics collaborative hardening error mitigation techniques network links error propagation error mitigation error accumulation single bit upset sensitivity sbu sensitivity;sbu sensitivity;circuit sensitiveness;online testing;space vehicle electronics;functional complexity;hardware emulation;robust digital system design;hardware;real time systems	The process of designing robust digital systems is getting heavier and longer due to the increase of functional complexity and circuit sensitiveness [1][2]. Furthermore, aerospace and automotive electronics are including more digital systems with critical tasks distribution among several single and cheaper modules. Although collaborative hardening is providing very interesting results in terms of cost and reliability, the design process becomes more difficult. Redundant tasks, hardware and software, must be evaluated together with the global and local error mitigation techniques. Also, network links affect error propagation and mitigation. Finally, the error accumulation must be considered in these systems working in harsh conditions. In this paper we present a general method for evaluating the Single Bit Upsets (SBU) sensitivity of complex digital systems with critical tasks distribution and collaborative hardening. This method performs a detailed analysis of signal integrity along system operation thanks to hardware emulation in the early steps of the design cycle.	digital electronics;emulator;error detection and correction;online and offline;propagation of uncertainty;signal integrity;single event upset;software propagation;tree accumulation	Anna Vaskova;Celia López-Ongil;Mario García-Valderas;Marta Portela-García;Luis Entrena	2011	2011 IEEE 17th International On-Line Testing Symposium	10.1109/IOLTS.2011.5994539	reliability engineering;embedded system;emulation;electronic engineering;real-time computing;sensitivity;computer science;engineering;electrical engineering;propagation of uncertainty;operating system;programming language;statistics;robustness;hardware emulation	Embedded	8.488204537594624	59.413454004845235	174708
747ab004ffeafb6ee3f3c42dafe2837f4742f111	towards achieving reliable and high-performance nanocomputing via dynamic redundancy allocation	self assembly;parallel rendering;bottom up;operant conditioning;hardware reliability;performance;redundancy allocation;redundant design;high performance computer;nanoscale architecture;high performance;high speed	Nanoelectronic devices are considered to be the computational fabrics for the emerging nanocomputing systems due to their ultra-high speed and integration density. However, the imperfect bottom-up self-assembly fabrication leads to excessive defects that have become a barrier for achieving reliable computing. In addition, transient errors continue to be a problem. The massive parallelism rendered by nanoscale integration opens up new opportunities but also poses challenges on how to manage such massive resources for reliable and high-performance computing. In this paper, we propose a nanoarchitecture solution to address these emerging challenges. By using dynamic redundancy allocation, the massive parallelism is exploited to jointly achieve fault (defect/error) tolerance and high performance. Simulation results demonstrate the effectiveness of the proposed technique under a range of fault rates and operating conditions.	nanocomputer;parallel computing;self-assembly;simulation;software bug;supercomputer	Shuo Wang;Lei Wang;Faquir C. Jain	2009	JETC	10.1145/1482613.1482615	embedded system;parallel computing;real-time computing;simulation;performance;computer science;operating system;parallel rendering;operant conditioning;top-down and bottom-up design;nanotechnology;self-assembly	HPC	5.812954022501294	59.940061887103205	175127
3cca2995cdbdc91b63bc7ecb9080dd7089b89ee3	formal verification of architectural power intent	power intent verification assertion formal verification low power verification;low power verification;data mining;assertion;system on a chip;power tructor formal verification architectural power intent high level properties architectural power management strategy power management control logic low level per domain control signal high level artifacts low level control sequence upf specifications on chip power management logic formal extraction timing information;transient analysis;integrated circuit design;formal verification;registers;low power electronics;transient analysis timing power control bridge circuits system on a chip registers data mining;power intent verification;electronic engineering computing;low power electronics electronic engineering computing formal verification integrated circuit design;bridge circuits;power control;timing	This paper presents a verification framework that attempts to bridge the disconnect between high-level properties capturing the architectural power management strategy and the implementation of the power management control logic using low-level per-domain control signals. The novelty of the proposed framework is in demonstrating that the architectural power intent properties developed using high-level artifacts can be automatically translated into properties over low-level control sequences gleaned from UPF specifications of power domains, and that the resulting properties can be used to formally verify the global on-chip power management logic. The proposed translation uses a considerable amount of domain knowledge and is also not purely syntactic, because it requires formal extraction of timing information for the low-level control sequences. We present a tool, called POWER-TRUCTOR which enables the proposed framework, and several test cases of significant complexity to demonstrate the feasibility of the proposed framework.	formal verification;high- and low-level;power domains;power management;test case	Aritra Hazra;Sahil Goyal;Pallab Dasgupta;Ajit Pal	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2011.2180548	system on a chip;embedded system;electronic engineering;real-time computing;assertion;formal verification;power control;computer science;operating system;processor register;programming language;low-power electronics;integrated circuit design	EDA	4.952415378036288	54.72177435579427	175559
b9b4d1413967363402819554f86c7577fa5d3b3c	sustainable (re-) configurable solutions for the high volume soc market	high volume soc design;soc market;mask programmable hardware technology;reconfigurable architectures runtime computer architecture system on a chip time to market costs computer applications application specific integrated circuits hardware paper technology;logic design;reconfigurable computing;reconfigurable architectures;embedded systems;reconfigurable architecture;system on chip embedded systems logic design reconfigurable architectures;system on chip;high volume application specific standard product;embedded run time reconfigurable architecture;mask programmable hardware technology high volume soc design embedded run time reconfigurable architecture high volume application specific standard product soc market;time to market;new products	The application of embedded run-time configurable architectures to System-on-chip design has long been considered a possible major enabling factor, especially in the direction of lowering time-to-market of new products as well as mitigating NRE costs related to verification, bug-fixes and product upgrades. In fact, while achieving significant success in specific application fields, reconfigurable computing has so far mostly failed to reach the high-volume application specific standard products (ASSP) that both in terms of volumes and revenues represent the largest share of today's SoC market. This is essentially due to the area overhead induced by these solutions with respect to standard ASIC design styles, which is unaffordable for the low margins that characterize this specific product class. In this paper, the exploitation of mask-programmable hardware technologies for deploying high volume ASSP is evaluated as a possible mitigation factor to the above discussed issues. The paper provides an introduction to mask-programmable technologies as well as an overview and a classification of most significant available trends and solutions in the field. In particular, the application of mask-level programmability in the context of the most significant trends in reconfigurable architectures is thoroughly discussed. In the authors' opinion it is both useful and necessary to capitalize on and exploit the valuable legacy created by 10 years of exploration of reconfigurable architectures in the context of the new possibilities offered by the emergence of mask-programmable options as a significant factor in SoC design.	anti-spam smtp proxy;application-specific integrated circuit;embedded system;emergence;field-programmable gate array;full scale;overhead (computing);reconfigurable computing;software deployment;system on a chip	Fabio Campi;Luca Ciccarelli;Claudio Mucci	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536541	system on a chip;embedded system;computer architecture;parallel computing;logic synthesis;real-time computing;reconfigurable computing;computer science;operating system	EDA	9.637226170500496	56.130818244505456	176452
3b21094f97989a99fdc7ecabfc50e7036eeddcb7	memory optimization in single chip network switch fabrics	distributed memory;switching networks;greedy packets;limiting factor;queueing theory;high bandwidth network switches;queue memory effectiveness maximization;space exploration;packet switching;design space;chip;integrated circuit design;network switch;circuit simulation;large scale;greedy packets single chip network switch fabrics memory optimization high bandwidth network switches overall design requirements design space itrs on chip memory architectural technique queue memory effectiveness maximization simulation results dropped packet probability distributed packets;telecommunication network routing;memory optimization;permission;memory architecture;switches intelligent networks fabrics bandwidth space exploration space technology permission large scale systems chip scale packaging packet switching;fabrics;bandwidth;soc;architectural technique;space technology;intelligent networks;itrs;single chip network switch fabrics;circuit optimisation;switches;overall design requirements;queueing theory switching networks packet switching memory architecture circuit optimisation integrated circuit design circuit simulation telecommunication network routing;simulation results;dropped packet probability;on chip memory;distributed packets;chip scale packaging;large scale systems	Moving high bandwidth (10Gb/s+) network switches from the large scale, rack mount design space to the single chip design space requires a re-evaluation of the overall design requirements. In this paper, we explore the design space for these single chip devices by evaluating the ITRS. We find that unlike ten years ago when interconnect was scarce, the limiting factor in today's designs is on-chip memory. We then discuss an architectural technique for maximizing the effectiveness of queue memory in a single chip switch. Next, we show simulation results that indicate that a more than two order of magnitude improvement in dropped packet probability can be achieved by re-distributing memory and allowing sharing between the switch's ports. Finally, we evaluate the cost of the optimized architecture in terms of other on-chip resources.	19-inch rack;mathematical optimization;network packet;network switch;program optimization;requirement;simulation	David Whelihan;Herman Schmit	2002		10.1145/513918.514052	embedded system;electronic engineering;real-time computing;telecommunications;network switch;computer science;operating system;computer network	Arch	3.0968735903179088	60.10293270963296	176865
efb10173739db7aa9011b21f22550366bbcc7881	redundancy based interconnect duplication to mitigate soft errors in sram-based fpgas		Soft error induced reliability problem has already become a major concern for modern SRAM-based FPGAs (Field Programmable Gate Arrays) even at the ground level. In this paper, we propose a duplication-with-recovery (DWR) technique to recover the configuration bit faults on interconnects, which contribute to the majority of soft errors in FPGAs. Based on a study on the detailed routing structure in real FPGAs, DWR leverages redundant resources for interconnect duplication and enables fault recovery with lightweight circuit-level support. Compared with traditional fault tolerant techniques, DWR retains the fault recovering capability but eliminates expensive copies. The experimental results show that a large portion of the interconnects can be protected, which in consequence significantly reduces the vulnerable configuration bits. In addition, DWR does not alter the placement and routing from standard design flow, and therefore does not affect the design closure but greatly improves the design reliability in a cost-effective way.	design closure;direct web remoting;electrical connection;fault tolerance;field-programmable gate array;height above ground level;place and route;routing;soft error;static random-access memory	Naifeng Jing;Jiacheng Zhou;Jian-Fei Jiang;Xin Chen;Weifeng He;Zhigang Mao	2015	2015 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		three-dimensional integrated circuit;routing;electronic engineering;parallel computing;real-time computing;network switch;computer science;engineering;through-silicon via;field-programmable gate array	EDA	6.857485646405501	59.5811679848914	178197
5d6eeb225def5039966bf738631794da78f6cf94	islands of synchronicity, a design methodology for soc design	design for testability;synchronisation;system-on-chip;0.13 micron;soc design;design for testability;hierarchical physical design;islands of synchronicity;micron technology designs;skeleton of reuse;synchronous islands;system-on-chip	To meet the challenges of faster time to market and growing design complexity, a methodology and supporting infrastructure for advanced System-on-Chip design have been developed and applied to 0.13 micron technology designs.The Islands of Synchronicity methodology uses locally synchronous islands to produce a timing-closure friendly design style that is widely applicable across different architectures. This approach enables a modular, hierarchical physical design strategy which significantly eases top-level timing closure problems. The resultant design flow is supported by the Skeleton of Reuse, a collection of IP generators and tools that automate many of the steps in SoC implementation.	physical design (electronics);resultant;synchronicity;system on a chip;timing closure	A. P. Niranjan;Paul C. Wiscombe	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		system on a chip;physical design;embedded system;synchronization;electronic engineering;real-time computing;design methods;computer science;engineering;design flow;design for testing	EDA	5.785482542763225	53.905907272794536	178822
c7aea08d848c6ab898a9a9801d2cfc39eb62ae8a	masking of x-values by use of a hierarchically configurable register	masking of x values;hierarchically configurable mask register;industrial design	In this paper we consider masking of unknowns (X-values) for VLSI circuits. We present a new hierarchical method of X-masking which is a major improvement of the method proposed in [4], called WIDE1. By the method proposed, the number of observable scan cells is optimized and data volume for X-masking can be significantly reduced in comparison to WIDE1. This is demonstrated for three industrial designs. In cases where all X-values have to be masked the novel approach is especially efficient.		Thomas Rabenalt;Michael Gössel;Andreas Leininger	2011	J. Electronic Testing	10.1007/s10836-010-5179-2	parallel computing;real-time computing;industrial design;computer hardware;computer science	Logic	8.200013274853534	57.2424764179795	178961
d59f932ef4c300f4afd26ac5852131f5765f82a2	system-level design solutions: enabling the iot explosion	iot system level design methodology internet of things;internet of things hardware software codesign;internet of things hardware security ip networks system level design privacy manuals	Analysts estimate that there will be 50 billion internet-connected devices by 2020, from 25 billion in 2015. This predicted explosion of IoT devices affects various evolving and growing markets as well as entirely new applications. Despite the variety of target applications, all such devices demand low energy/power consumption, high reliability, connectivity, interoperability, security and privacy. Furthermore, while meeting all these demands, time-to-market is a critical metric determining whether an IoT product will capture market share and thus the greatest revenue opportunity. Thus, a design methodology that meets design goals while ensuring fast time-to-market is a critical need. In this paper, we discuss the vision and building blocks of such a design methodolgoy, which would hopefully become an enabling technology for future IoT development and growth.	design flow (eda);electronic system-level design and verification;high- and low-level;high-level synthesis;interoperability;level design;system integration;systems design	Liwei Yang;Yao Chen;Wei Zuo;Tan Nguyen;Swathi T. Gurumani;Kyle Rupnow;Deming Chen	2015	2015 IEEE 11th International Conference on ASIC (ASICON)	10.1109/ASICON.2015.7517023	engineering;internet privacy;world wide web;computer security	EDA	9.562460331711797	56.212212984201926	179799
663b05278db8eaf19cac5ba102051a0f879df5a5	multi-clock latency-insensitive architecture and wrapper synthesis	globally asynchronous locally synchronous systems gals;clock buffer tree delay;design automation;formal specification;system on a chip;specification language;synchronous system;chip;clock distribution;low latency;system on chip;indexation;synchronizers;latency insensitive systems;globally asynchronous locally synchronous;system architecture;component wrapper language cwl;high performance	This paper presents an architecture and a wrapper synthesis approach for the design of multi-clock systems-on-chips. We build upon the initial work on multi-clock latency-insensitive systems by Singh and Theobald [1], and provide a detailed system architecture with the following capabilities and benefits: (i) modules are stalled only when needed, thereby avoiding unnecessary stalling, (ii) adequate metastability resolution is provided, (iii) handshake interfaces between modules are highperformance and low-latency, i.e., capable of transfering data packets on every clock cycle, (iv) IP cores with large clock distribution delays are correctly handled, and (v) an automated approach is provided for wrapper synthesis from formal specifications. For wrapper synthesis, we chose the Component Wrapper Language (CWL) from Hitachi/Fujitsu [2] as the specification language. Our synthesis approach has been implemented in a prototype tool. Synthesis results for a small set of examples are provided.	clock signal;emoticon;network packet;prototype;specification language;system on a chip;systems architecture	Ankur Agiwal;Montek Singh	2006	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2005.05.033	system on a chip;embedded system;parallel computing;real-time computing;electronic design automation;computer science;programming language;systems architecture	EDA	5.90657691474121	53.94424223069193	180158
805030df9d1878725e66abda9590c9bdf8f1c24d	system-on-chip design: impact on education and research	architectural design;system approach;software platform;energy efficient;design research;engineering schools deep submicron technology hardware software platforms system design research methodologies tools libraries soc architects;systems engineering;technological forecasting electronic engineering education research initiatives systems engineering;chip;system on a chip process design silicon analog computers computer interfaces radio frequency communications technology packaging energy consumption fuels;information and communication technology;complex system;system on chip;system design;electronic engineering education;information society;research initiatives;power consumption;hardware description language;design methodology;technological forecasting	Deep-submicron technology is rapidly leading to exceedingly complex, billion-transistor chips. Within a decade, these chips will deliver enormous computing power as well as RF and analog interfaces for information and communication technology. At the same time, portability and the need for inexpensive packaging will limit power consumption to a few watts or less. These systems-on-chip (SOCs), designed at the processor-memory level, will fuel the future information society. Such designs depend, however, on SOC architects who can bridge the gap between software-centric system specifications and their implementation in novel, energy-efficient silicon architectures. Designing such hardware-software platforms will require a global-system approach from concept to implementation, which may well require a rethinking of present engineering schools. Chips will no longer be stand-alone components but complete silicon boards encapsulating complex system knowledge. These boards will be specified far above the hardware-description-language level and implemented in new, heterogeneous architectures designed at the processor-memory level. Today we know very little about the nature of these future architectures, let alone the design methodology. We will instead need the design research center concept. The research center's goal will be to perform cross-disciplinary system design research to create new methodologies, tools, libraries, and courses-distributed via the Internet-to produce enough SOC architects worldwide.	system on a chip	Hugo De Man	1999	IEEE Design & Test of Computers	10.1109/54.785820	chip;system on a chip;embedded system;information and communications technology;technology forecasting;complex systems;electronic engineering;design research;design methods;telecommunications;computer science;systems engineering;engineering;electrical engineering;operating system;software engineering;efficient energy use;hardware description language;computer engineering;systems design	EDA	9.246277498871075	55.69486099152853	180777
861fb7850b487e6829a2bab23f54fd2a6862c8e6	special purpose vs. general purpose hardware for da	recent advance;general purpose approach;general purpose computer;special hardware;special need;cad work;general purpose hardware;unique problem;special purpose;process design;computational modeling;computer aided design;hardware;design automation;computer graphics	This workshop will explore the need for special hardware as an aid to the user of Computer Aided Design. Traditionally, all CAD work has been done by general purpose computers which are not tailored to all the special needs of CAD. However, recently advances have been made in understanding how special hardware can help solve some of the unique problems associated with graphics, simulation and other types of analysis.  The tradeoffs that occur when examining special vs. general purpose hardware are generally performance vs. flexibility, and performance vs. cost. Also, the expandibility of the general purpose approach has been a strong motivating factor. However, recent advances in high-speed, distributed networks may make it possible to have the best of both approaches.	computer;computer-aided design;graphics;simulation	T. H. Bruggere	1982	19th Design Automation Conference	10.1145/800263.809226	process design;simulation;electronic design automation;computer science;computer aided design;computer graphics;computational model;computer engineering	EDA	5.239206280935393	53.768802903203515	181050
c43b78c709f4102f9a7b1b73e6f215b90badadcc	early timing estimation for system-level design using fpgas (abstract only)	resource utilization;cycle time;design process;building block;physical design;fpga;performance metric;early timing analysis;levels of abstraction;system design;system level design;timing analysis;delay estimation	FPGA devices provide flexible, fast, and low-cost prototyping and production solutions for system design. However, as the design complexity continues to rise, the design and synthesis iterations become a labor intensive and time consuming ordeal. Consequently, it becomes imperative to raise the level of abstraction for FPGA designs, while providing insight into performance metrics early in the design process. In particular, an important design time problem is to determine the maximum clock frequency that a circuit can achieve on a specific FPGA target before full synthesis and implementation. This early quantification can greatly help evaluate key design characteristics without reverting to tedious runs of the full implementation flow. In this work, we focus on the predictability of timing delay of circuits composed of high-level blocks on an FPGA. We are well aware of difficulties in tackling uncertainties in early timing estimation, e.g., an inherent gap between a high-level representation and gates/wires; extremely difficult delay estimation due to the randomness in physical design tools, etc. We show that the estimation uncertainties can be mitigated through a carefully characterized timing database of primitive building blocks and refined timing analysis models. We primarily focus on applications composed of data-intensive word-level arithmetic computations from the DSP domain and specified using static dataflow models. Our experiments indicate that for these applications, timing estimates can be obtained reliably within a good error margin on average and in the worst case. As future work, we plan to fine tune the timing database by modeling resource utilization effects and inter-primitive/actor routing delay via variants of Rent's rule and related efforts. We are also interested in exploring dynamic sub-cycle timing characterization.		Hugo A. Andrade;Arkadeb Ghosal;Rhishikesh Limaye;Sadia Malik;Newton Petersen;Kaushik Ravindran;Trung N. Tran;Guoqiang Wang;Guang Yang	2012		10.1145/2145694.2145761	physical design;embedded system;in situ resource utilization;parallel computing;real-time computing;simulation;design process;cycle time variation;computer science;operating system;electronic system-level design and verification;static timing analysis;field-programmable gate array;systems design	EDA	3.199184721279545	54.82323548642954	181938
fcad42b1020be3214e83ef71e0f5702b0c57e855	a low-cost platform for the prototyping and characterization of digital circuit ips	systems on chip socs;test processors;test and characterization;asics;circuit intellectual properties	A novel low-cost platform for prototyping and characterizing the performance of digital circuit intellectual properties (IPs) has been developed. Compromised of several HW/SW components, it allows developers of circuit IPs to verify the functionality of any number of IPs on the same prototype chip and characterize their speeds without the need for any expensive test equipment special/custom IP-wrappers, or high-speed test board design. A complete prototype of the proposed platform has been realized and successfully used to test a prototype IC fabricated in a 150nm technology with frequencies up to 2.1GHz. Design conditions/constraints for portability to any fabrication process have been developed and verified using measurements from the fabricated IC. Low-cost platform for prototyping and speed characterization of digital IPs.All serial and fixed interface, hence same test board can be used to test and characterize any IP.No limit on number of IP prototypes in the test chip nor on the number of I/Os per IP.Portable, on-chip, sw-controlled DCO with very small area allows high speed characterization with low interface speed, resulting in low system cost.		Muhammad E. S. Elrabaa;Amran Al-Aghbari;Mohammed Alasli;Aiman H. El-Maleh;Abdelhafid Bouhraoua;Mohammad Alshayeb	2016	Integration	10.1016/j.vlsi.2016.01.005	embedded system;electronic engineering;computer hardware;engineering;operating system	EDA	8.019957992382455	53.65687766148762	181972
9eed31eb9d2f17f05807d9a35f828134f54bb662	relocation of reconfigurable modules on xilinx fpga	table lookup field programmable gate arrays routing wires performance evaluation software digital signal processing;multiplying circuits;logic design;reconfigurable architectures;multiplier xilinx fpga reconfigurable module relocation design flow dynamic partial reconfiguration dpr partial configuration bitstream single partial bitstream fpga device adder locations;adders;functional block relocation fpga partial reconfiguration;reconfigurable architectures adders field programmable gate arrays logic design multiplying circuits;field programmable gate arrays	This paper presents a design flow that allows relocation of reconfigurable modules on Xilinx FPGAs using dynamic partial reconfiguration (DPR). Relocation of these modules is performed without requirements of re-implementing the design. The article describes the relocation procedure based on modifications of major address of the partial configuration bitstream. This approach allows using single partial bitstream for multiple areas in FPGA device. It reduces a number of partial bitstreams stored in memory, saves the implementation time and it can increase dependability of the system. The proposed flow is demonstrated on a simple example with multiplier and adder locations mutually exchanged.	adder (electronics);bitstream;dependability;design flow (eda);field-programmable gate array;overhead (computing);personal identification number;relocation (computing);requirement;routing;spartan;vii	Tomas Drahonovsky;Martin Rozkovec;Ondřej Novák	2013	2013 IEEE 16th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2013.6549812	embedded system;electronic engineering;parallel computing;logic synthesis;real-time computing;reconfigurable computing;computer science;adder;field-programmable gate array	Arch	8.236996544895552	56.94505222270491	182145
6d63f5a543649c7ab9c249927eca580e1f1f3952	reliable radix-4 complex division for fault-sensitive applications	xilinx virtex 6 radix 4 complex division fault sensitive applications signal processing astronomy nonlinear rf measurements complex number division architectures sweeney robertson tocher division error detection mechanisms simplified detecting code hardware redundancy look up table fault simulations field programmable gate array fpga xilinx spartan 6;circuit faults;srt division concurrent error detection ced fpga recomputing with shifted operands reso;read only memory hardware computer architecture circuit faults redundancy registers;computer architecture;redundancy;registers;table lookup error detection codes field programmable gate arrays redundancy;read only memory;hardware	Complex division is commonly used in various applications in signal processing and control theory including astronomy and nonlinear RF measurements. Nevertheless, unless reliability and assurance are embedded into the architectures of such structures, the sub-optimal (and thus erroneous) results could undermine the objectives of such applications. As such, in this paper, we present schemes to provide complex number division architectures based on Sweeney, Robertson, and Tocher-division with error detection mechanisms. Different error detection architectures are proposed in this paper which can be tailored based on the eventual objectives of the designs in terms of area and time requirements, among which we pinpoint carefully the schemes based on recomputing with shifted operands to be able to detect faults based on recomputations for different operands in addition to the unified parity (simplified detecting code) and hardware redundancy approach. The design also implements a minimized look up table approach which favors in error detection based designs and provides high fault coverage with relatively-low overhead. Additionally, to benchmark the effectiveness of the proposed schemes, extensive error detection assessments are performed for the proposed designs through fault simulations and field-programmable gate array (FPGA) implementations; the design is implemented on Xilinx Spartan-6 and Xilinx Virtex-6 FPGA families.	benchmark (computing);control theory;division algorithm;embedded system;error detection and correction;extensibility;fault coverage;fault detection and isolation;field-programmability;field-programmable gate array;lookup table;nonlinear system;operand;overhead (computing);pipeline (computing);radio frequency;redundancy (engineering);requirement;sensor;signal processing;simulation;type signature	Mehran Mozaffari Kermani;Niranjan Manoharan;Reza Azarderakhsh	2015	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2015.2394389	embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system;processor register;redundancy;read-only memory;algorithm	EDA	8.95013592829313	59.80776299646429	182201
894879db716a843559bb5a6c568ac450b8586df5	ibm zenterprise redundant array of independent memory subsystem	error correction codes enterprise resource planning synchronization multiprocessing systems dram chips	array of independent memory subsystem P. J. Meaney L. A. Lastras-Montaño V. K. Papazova E. Stephens J. S. Johnson L. C. Alves J. A. O’Connor W. J. Clarke The IBM zEnterpriseA system introduced a new and innovative redundant array of independent memory (RAIM) subsystem design as a standard feature on all zEnterprise servers. It protects the server from single-channel errors such as sudden control, bus, buffer, and massive dynamic RAM (DRAM) failures, thus achieving the highest System zA memory availability. This system also introduced innovations such as DRAM and channel marking, as well as a novel dynamic cyclic redundancy code channel marking. This paper describes this RAIM subsystem and other reliability, availability, and serviceability features, including automatic channel error recovery; data and clock interface lane calibration, recovery, and repair; intermittent lane sparing; and specialty engines for maintenance, periodic calibration, power, and power-on controls.	control bus;cyclic redundancy check;dimm;dynamic random-access memory;edmund m. clarke;failure cause;firmware;ibm system z;input/output;item unique identification;redundant array of independent memory;segmentation fault;server (computing)	Patrick J. Meaney;Luis Alfonso Lastras-Montaño;Vesselina K. Papazova;Eldee Stephens;J. S. Johnson;Luiz C. Alves;James A. O'Connor;William J. Clarke	2012	IBM Journal of Research and Development	10.1147/JRD.2011.2177106	embedded system;parallel computing;real-time computing;telecommunications;computer science;operating system;redundant array of independent memory	DB	6.897779291040503	56.454902001694066	182243
a4e590c50ecc720d579659b6d5385bd78e0c7fe2	novel dram mitigation technique	microprocessors;error detection codes;random access memory;error correcting code;error correction codes;fault tolerant;error detection and correcting radiation mitigation technique dram cell memory nonsymmetrical structure fault tolerant architectures seu mbu;dram cell memory;mbu;data mining;error detection and correcting;mbu dram radiation mitigation error correcting code seu;laser beam effects;computer architecture;multichip modules dram chips error correction codes error detection codes fault tolerance laser beam effects;multichip modules;error correction code;seu;nonsymmetrical structure;periodic structures;fault tolerance;semiconductor lasers;partial discharges;radiation mitigation technique;mitigation;dram;dram chips;radiation;random access memory error correction codes aerospace electronics computer errors voltage capacitors error correction electrons space technology redundancy;fault tolerant architectures	This paper describes a novel patented radiation mitigation technique for DRAM cell memories. Because of their non-symmetrical structure, only one state is sensitive to radiations. This particular property is used to detect and correct upsets. Several fault-tolerant architectures are proposed, which are able to detect and correct all SEU/ MBU in a word, whatever its length. Overhead in term of additional memory cells is lower than other classical mitigation techniques.	cell (microprocessor);dynamic random-access memory;fault tolerance;memory cell (binary)	Antonin Bougerol;Florent Miller;Nadine Buard	2009	2009 15th IEEE International On-Line Testing Symposium	10.1109/IOLTS.2009.5195991	reliability engineering;fault tolerance;electronic engineering;parallel computing;real-time computing;error detection and correction;computer science;statistics	Arch	8.860589219588498	60.40132324752271	183151
746502b106b3513681d829a5c5237102cb16776e	essential fault-tolerance metrics for noc infrastructures	network on chip fault tolerance network synthesis;fault tolerance metrics design;network synthesis;fault tolerant;network on chip;top down;system performance specifications;fault tolerance network on a chip fault tolerant systems fabrics quality of service redundancy system on a chip power system interconnection system performance power dissipation;system performance;noc infrastructures;fault tolerance;network on chip communication architectures;system performance specifications fault tolerance metrics design noc infrastructures network on chip communication architectures	Fault-tolerant design of network-on-chip communication architectures requires the addressing of issues pertaining to different elements described at different levels of design abstraction - these may be specific to architecture, interconnection, communication and application issues. Assessing the effectiveness of a particular fault-tolerant implementation can be a challenging task for designers, constrained with tight system performance specifications and other requirements In this paper, we provide a top-down view of fault-tolerance methods for NoC infrastructures, and present a range of metrics used for estimating their quality. We illustrate the use of these metrics by simulating a few simple but realistic fault-tolerant scenarios.	fault tolerance;interconnection;network on a chip;requirement;simulation;top-down and bottom-up design;video game graphics	Cristian Grecu;Lorena Anghel;Partha Pratim Pande;André Ivanov;Resve A. Saleh	2007	13th IEEE International On-Line Testing Symposium (IOLTS 2007)	10.1109/IOLTS.2007.31	reliability engineering;embedded system;fault tolerance;parallel computing;real-time computing;computer science;engineering;computer performance;network on a chip;software fault tolerance	Embedded	3.0567847598422446	59.31799918654073	184349
5d1048c2a33b66f6fdaac466ba6417ee5ecee930	persistent cad for in-the-field power optimization	power optimization	A major focus within the Integrated Chip (IC) industry is reducing power consumption of devices. In this paper, we explore the idea of persistent CAD algorithms that constantly improve the power consumption of consumer devices that use FPGAs. The idea is that the field-programmability of an FPGA allows updates to be deployed in the field, and as CAD algorithms find optimizations for a design, these optimizations can be deployed into the field. To explore this idea, we have created a persistent placement algorithm for FPGAs using a genetic algorithm. We describe the design of this genetic algorithm, and then use it in an experiment to show the impact on power consumption. Our results for one of the larger MCNC benchmarks, clma, shows that over a 60 minute period better placement solutions are found, but the rate at which these solutions are found decreases quickly.	benchmark (computing);computer-aided design;field-programmability;field-programmable gate array;genetic algorithm;integrated circuit;maxima and minima;microprocessor;persistent data structure;power optimization (eda);software release life cycle	Peter Jamieson	2010			cad;control engineering;power optimization;computer science	EDA	7.399596220686284	55.03848498298978	184685
2b33286e29e75877dfaca16d3f8474f656b31c5d	architecture and design of an open ate to incubate the development of third-party instruments	modules automatic test equipment system on chip integrated circuit testing conformance testing software architecture;device under test;instruments system testing computer architecture system on a chip software tools control systems automatic testing automatic test equipment semiconductor device testing hardware;plug and play;open architecture automatic test equipment ate;automatic test equipment;system on a chip;power supply;open architecture;software architecture;conformance testing;openstar;system on chip;integrated circuit testing;next generation;functional unit;system on a chip soc tester open architecture tester open architecture automatic test equipment ate openstar;modules;open architecture tester;open architecture tester open architecture automatic test equipment open ate third party instruments system on a chip ic soc ic openstar specification optimal test configuration modular unit device under test dut hardware module digital pincard analog card device power supply dps waveform generator software module test executive tool system monitoring licensing tools unit level controller microsoft office utility;system on a chip soc tester	To test next-generation system-on-a-chip (SoC) ICs, an open architecture automatic test equipment (ATE) has been conceived. Open architecture provides a framework to integrate software and instruments of different vendors into the ATE. The specifications of this framework, known as OPENSTAR specifications, have been developed by the Semiconductor Test Consortium (STC). The deployment of third-party instruments and modules in this framework is plug-and-play to achieve the optimal test configuration for a given SoC. In this test system, each modular unit can be replaced with another modular unit from a different vendor, and the tester can be reconfigured to map the test resources according to the requirements of device-under-test (DUT). The only restriction in using the third party modules is that each modular unit must adhere to the standard interfaces of the integrating framework and should conform to the OPENSTAR specifications. Hardware modules can be any functional unit such as a digital pincard, an analog card, device power supply (DPS), instruments such as waveform generator, etc. Similarly, software modules can be a tool or utility such as a test executive tool, system monitoring or licensing tools, unit-level controllers, database, microsoft office utilities, application specific software for controlling equipment, etc. The basic structure of this test system, module structure, calibration/diagnostics and synchronization as well as system reconfigurability is described in this paper.	built-in test equipment;consortium;database;device under test;execution unit;microsoft lumia;open architecture;pictbridge;plug and play;power supply;reconfigurability;requirement;semiconductor;software deployment;system monitor;system on a chip	Rochit Rajsuman;Masuda Noriyuki;Kazuhiro Yamashita	2005	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2005.856714	system on a chip;embedded system;automatic test equipment;computer hardware;computer science;engineering;computer engineering	SE	9.168276361667935	54.03952332616167	185551
024e9154e01026a4023ab3d3936d512c18e62c1b	performance prediction of throughput-centric pipelined global interconnects with voltage scaling	pipelined global interconnects;throughput per energy area tpea;chip;technology scaling;parallel computer architecture;performance prediction;numerical experiment;voltage scaling;flip flop;design methodology	Due to the ever increasing demand for computing capacity, throughput-centric design for on-chip global interconnects has played an important role in the emerging parallel computing architectures. In this paper, we explore the performance of flip-flop-based pipelined global interconnects with more design freedoms under the voltage and technology scaling for different applications. Based on the derived accurate voltage-scaled models of pipelined interconnects, we propose a general evaluation flow using numerical experiments to study the impact of pipelining depth, voltage scaling, and different processes on the performance of pipelined interconnects under four different design objectives. Our experimental results show that, with the dedicated throughput-centric optimization, at 45 nm node, up to 25x overall throughput-per-energy-area (TPEA) improvement can be obtained with only 4x increase on the interconnect latency compared with the conventional minimum-latency design, making this new design methodology more promising in the future nodes.	dynamic voltage scaling;electrical connection;experiment;flops;flip-flop (electronics);image scaling;mathematical optimization;numerical analysis;parallel computing;performance prediction;pipeline (computing);throughput	Yulei Zhang;James F. Buckwalter;Chung-Kuan Cheng	2010		10.1145/1811100.1811118	chip;electronic engineering;parallel computing;real-time computing;design methods;telecommunications;computer science	EDA	2.981713456955274	58.8814238033291	185943
a1da5fe62fc09a091c3035cadf477f6e581aeb3c	performance simulator based on hardware resources constraints for ion trap quantum computer	performance evaluation;logic design;quantum gates;steane code performance simulator hardware resource constraints ion trap quantum computer elementary qubit operations scalable implementation concrete scheduling procedure quantum cad tools execution time error performances quantum circuit physical gates musiqc architecture three qubit bernstein vazirani algorithm;quantum gates circuit cad logic design performance evaluation;resource reduction quantum performance simulator hardware constraints resource trade off;circuit cad;logic gates hardware quantum computing ions fault tolerance fault tolerant systems computers	Efforts to build quantum computers using ion-traps have demonstrated all elementary qubit operations necessary for scalable implementation. Modular architectures have been proposed to construct modest size quantum computers with up to 104 - 106 qubits using technologies that are available today (MUSIQC architecture). Concrete scheduling procedure to execute a given quantum algorithm on such a hardware is a significant task, but existing quantum CAD tools generally do not account for the underlying connectivity of the qubits or the limitation on the hardware resources available for the scheduling. We present a scheduler and performance simulator that fully accounts for these resource constraints, capable of estimating the execution time and error performances of executing a quantum circuit on the hardware. We outline the construction of tool components, and describe the process of mapping the qubits to ions and scheduling the physical gates in the MUSIQC architecture. Using this tool, we quantify the trade-off between hardware resource constraints and performance of the computer and show that at an expense of x fold increase in latency, a minimum of 1.6x resource reduction is possible for executing a three-qubit Bernstein-Vazirani algorithm encoded using Steane code.	ancilla bit;coherent;computer hardware;computer science;computer-aided design;decoherence-free subspaces;error detection and correction;integrated circuit;john reif;performance prediction;phil bernstein;quantum algorithm;quantum circuit;quantum computing;quantum decoherence;qubit;run time (program lifecycle phase);scalability;scheduling (computing);simulation;steane code;time complexity	Muhammad Ahsan;Byung-Soo Choi;Jungsang Kim	2013	2013 IEEE 31st International Conference on Computer Design (ICCD)	10.1109/ICCD.2013.6657073	embedded system;electronic engineering;quantum information;logic synthesis;real-time computing;computer science;theoretical computer science;operating system;computer engineering;quantum gate	EDA	7.5667639712842245	56.674678431452335	186886
bc01c1e27657ced585347f3955c589a1e45a1c27	the trimedia processor: the price-performance challenge for media processing	silicon;digital signal processing;clocks;home appliances;null;multimedia systems;multimedia systems clocks home appliances process design energy consumption frequency internet streaming media silicon digital signal processing;process design;internet;media processing;streaming media;energy consumption;frequency	Multimedia technology is making an inroad into the home via a variety of applications, such as digital set-top boxes, Internet appliances, and videophones. Looking at just these few examples, it seems not only conceivable that the average household will have several multimedia appliances, it seems likely. The already large and still growing number of standards in this field calls for a programmable solution. However, the price-performance of mainstream programmable processors is off by an order of magnitude for consumer markets. In this paper, we discuss the trade offs to get to an optimal price performance point for media processing, based on experience with the TriMedia processor design.	central processing unit;embedded controller;embedded system;internet appliance;media processor;personal computer;processor design;set-top box;trimedia (mediaprocessor)	Frans Sijstermans	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237696	process design;embedded system;real-time computing;the internet;media processor;computer hardware;telecommunications;computer science;operating system;digital signal processing;frequency;silicon	EDA	9.635711703720062	55.871624144582356	187312
f87e52c67cfbc89058f0260dd5b00e425a8eaa65	efficient system-level functional verification methodology for multimedia applications	correctness verification;silicon;teleconferencing;design automation;system level functional verification methodology;formal specification;functional verification;design exploration;videoconferencing decoder system level functional verification methodology multimedia applications design exploration minimum area requirements validation techniques system level methodology formal verification loop oriented transformations correctness verification;decoding;multimedia applications;application software;videoconferencing decoder;space exploration;multimedia application;multimedia systems;process design;minimum area requirements;multimedia systems algorithm design and analysis application software silicon process design space exploration design automation teleconferencing decoding multidimensional systems;formal verification;low power;validation techniques;control flow;formal specification formal verification teleconferencing decoding multimedia systems;loop oriented transformations;algorithm design and analysis;multidimensional systems;system level methodology	Multimedia application design exploration should begin at the system level, to meet low-power and minimum-area requirements. Existing validation techniques mainly concentrate on lower abstraction levels. This system-level methodology combines formal verification of loop-oriented transformations with correctness verification of arithmetic constructs and related control flows. A videoconferencing-decoder example illustrates the methodology's efficiency.	correctness (computer science);formal verification;low-power broadcasting;requirement;video decoder	Miroslav Cupák;Francky Catthoor;Hugo De Man	2003	IEEE Design & Test of Computers	10.1109/MDT.2003.1188263	process design;algorithm design;application software;real-time computing;verification;teleconference;multidimensional systems;electronic design automation;formal verification;software verification;computer science;theoretical computer science;space exploration;formal specification;high-level verification;silicon;programming language;control flow;intelligent verification;functional verification;computer engineering	EDA	4.698694864735508	53.44502617090422	189423
be7d57632ff1320056e959b9f86bb33fc1e3c96c	a reconfigurable network-on-chip architecture for heterogeneous cmps in the dark-silicon era	reconfiguration;multicore processing topology silicon system on chip network topology ports computers registers;network on chip computer architecture multiprocessing systems;dark silicon;energy consumption reconfigurable network on chip architecture heterogeneous cmp dark silicon era core specialization dark silicon challenge cheaper silicon area application specific cores single billion transistor multicore chip dark portion powered cores bypass switches distant active nodes onchip communication;reconfiguration dark silicon noc;noc	Core specialization is a promising solution to the dark silicon challenge. This approach trades off the cheaper silicon area with energy-efficiency by integrating a selection of many diverse application-specific cores into a single billion-transistor multicore chip. Each application then activates the subset of cores that best matches its processing requirements. These cores act as a customized application-specific CMP for the application. Such an arrangement of cores requires some special on-chip inter-core communication treatment to efficiently connect active cores. In this paper, we propose a reconfigurable network-on-chip that leverages the routers of the dark portion of the chip to customize the topology for the powered cores at any time. To this end, routers of the dark parts of the chip are used as bypass switches that can directly connect distant active nodes in the network. Our experimental results show considerable reduction in energy consumption and latency of on-chip communication.	dark silicon;inter-process communication;multi-core processor;network on a chip;network switch;partial template specialization;requirement;router (computing);transistor	Mehdi Modarressi;Hamid Sarbazi-Azad	2014	2014 IEEE 25th International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2014.6868637	embedded system;parallel computing;real-time computing;computer science;control reconfiguration;network operations center	EDA	2.9045651502522007	59.943064184955304	189672
652ca7de55954fbffb8725136c1bdb7cc42c1834	breaking the energy barrier in fault-tolerant caches for multicore systems	fault tolerance;cache;error correction;energy efficiency;multicore;computer architecture;vlsi	Balancing cache energy efficiency and reliability is a major challenge for future multicore system design. Supply voltage reduction is an effective tool to minimize cache energy consumption, usually at the expense of increased number of errors. To achieve substantial energy reduction without degrading reliability, we propose an adaptive fault-tolerant cache architecture, which provides appropriate error control for each cache line based on the number of faulty cells detected at reduced supply voltages. Our experiments show that the proposed approach can improve energy efficiency by more than 25% and energy-execution time product by over 10%, while improving reliability up to 4X using Mean-Error-To-Failure (METF) metric, compared to the next-best solution at the cost of 0.08% storage overhead.	cpu cache;error detection and correction;experiment;fault tolerance;multi-core processor;overhead (computing);run time (program lifecycle phase);symmetric multiprocessing;systems design	Paul Ampadu;Meilin Zhang;Vladimir Stojanovic	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		multi-core processor;fault tolerance;computer architecture;parallel computing;real-time computing;cache;computer science;operating system;efficient energy use;very-large-scale integration;smart cache;cache algorithms;cache pollution	EDA	6.466130261789276	60.162808854418124	190795
5ea19186df1d02b016fd82dcb925c3109f885bbd	dynamic simulation of direct torque control of induction motors with fpga based accelerators	dynamic simulation direct torque control fpga;personal computer direct torque control induction motors fpga based accelerators dynamic simulation framework dtc emulation framework exploratory system optimizations virtex s fpga innovative design techniques hardware time multiplexing built in dedicated floating point ip cores computational cores dtc dynamic simulation speed matlab;field programmable gate arrays computational modeling torque mathematical model matlab induction motors stators;torque control field programmable gate arrays induction motors machine control multiplexing;multiplexing;machine control;induction motors;field programmable gate arrays;torque control	We present an efficient FPGA based implementation of a dynamic simulation framework for Direct Torque Control (DTC) of induction motors. The merit of the proposed DTC emulation framework lies in that it is completely implemented within an FPGA, and therefore can be easily utilized for exploratory system optimizations. The high performance and low area foot-print of the proposed framework implemented on a Virtex-S FPGA is the result of several innovative design techniques based on hardware time-multiplexing and the utilization of built-in dedicated floating point IP cores. Our experimental results demonstrate that when FPGAs are utilized as computational cores, the DTC dynamic simulation speed can be improved by almost an order of magnitude compared to a software implementation run in Matlab on a personal computer.	built-in self-test;dynamic simulation;emulator;field-programmable gate array;matlab;mathematical optimization;multidisciplinary design optimization;multiplexing;personal computer;reconfigurability;virtex (fpga)	Hamed Sajjadi Kia;Mohammad A. Zare;Rajesh G. Kavasseri;Cristinel Ababei	2013	2013 International Conference on Reconfigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2013.6732281	embedded system;real-time computing;computer science;induction motor;multiplexing;field-programmable gate array	EDA	3.1099337071296485	56.60185249938822	191322
08262c99e0f3ba9ec250a250ff19d46e0849d5f1	ersa: error resilient system architecture for probabilistic applications	bayesian network;reliability;probability;probabilistic logic resilience hardware instruction sets reliability engineering computer architecture;energy efficient;sram chips integrated circuit reliability probability;probabilistic applications;robust systems error resilience hardware errors probabilistic applications reliability;control flow;error rate;error resilience;hardware errors;integrated circuit reliability;system architecture;static memory errors error resilient system architecture algorithm ersa redundancy techniques robust system architecture configurable reliability high order bit errors control flow errors asymmetric reliability many core architectures intelligent software optimizations error injection multicore ersa hardware prototype k means clustering ldpc decoding bayesian network inference sram;k means clustering;flip flop;sram chips;robust systems	There is a growing concern about the increasing vulnerability of future computing systems to errors in the underlying hardware. Traditional redundancy techniques are expensive for designing energy-efficient systems that are resilient to high error rates. We present Error Resilient System Architecture (ERSA), a low-cost robust system architecture for emerging killer probabilistic applications such as Recognition, Mining and Synthesis (RMS) applications. While resilience of such applications to errors in low-order bits of data is well-known, execution of such applications on error-prone hardware significantly degrades output quality (due to high-order bit errors and crashes). ERSA achieves high error resilience to high-order bit errors and control errors (in addition to low-order bit errors) using a judicious combination of 3 key ideas: (1) asymmetric reliability in many-core architectures, (2) error-resilient algorithms at the core of probabilistic applications, and (3) intelligent software optimizations. Error injection experiments on a multi-core ERSA hardware prototype demonstrate that, even at very high error rates of 20,000 errors/second/core or 2x10-4 error/cycle/core (with errors injected in architecturally-visible registers), ERSA maintains 90% or better accuracy of output results, together with minimal impact on execution time, for probabilistic applications such as K-Means clustering, LDPC decoding and Bayesian networks. Moreover, we demonstrate the effectiveness of ERSA in tolerating high rates of static memory errors that are characteristic of emerging challenges such as Vccmin problems and erratic bit errors. Using the concept of configurable reliability, ERSA platforms may also be adapted for general-purpose applications that are less resilient to errors (but at higher costs).	algorithm;bayesian network;cluster analysis;cognitive dimensions of notations;experiment;general-purpose markup language;k-means clustering;low-density parity-check code;manycore processor;most significant bit;multi-core processor;prototype;run time (program lifecycle phase);systems architecture	Larkhoon Leem;Hyungmin Cho;Jason Bau;Quinn A. Jacobson;Subhasish Mitra	2010	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2011.2179038	embedded system;electronic engineering;parallel computing;real-time computing;word error rate;computer science;theoretical computer science;operating system;probability;bayesian network;reliability;efficient energy use;control flow;statistics;systems architecture;k-means clustering	Arch	7.122931856812857	60.3237396541897	191405
7758c69b96ceb2a0c3e602b24041d462d5a528ea	revivenet: a self-adaptive architecture for improving lifetime reliability via localized timing adaptation	reliability;self adaptive;mean time to failure self adaptive architecture lifetime reliability model aggressive technology scaling aging sensor revivenet architecture hardware implemented aging aware architecture synergistic agent localized timing adaptation mechanism aging induced delay weibull distribution;sensors;clocks;timing adaptation;lifetime reliability;nbti;circuit stability;aging;weibull distribution;weibull distribution ageing delays reliability sensors;aging sensor;ageing;aging sensors delay clocks circuit stability;nbti lifetime reliability self adaptive aging sensor timing adaptation;adaptive architecture;delays;local time	The aggressive technology scaling poses serious challenges to lifetime reliability. A parament challenge comes from a variety of aging mechanisms that can cause gradual performance degradation of circuits. Prior work shows that such progressive degradation can be reliably detected by dedicated aging sensors, which provides a good foundation for proposing a new scheme to improve lifetime reliability. In this paper, we propose ReviveNet, a hardware-implemented aging-aware and self-adaptive architecture. Aging awareness is realized by deploying dedicated aging sensors, and self-adaptation is achieved by employing a group of synergistic agents. Each agent implements a localized timing adaptation mechanism to tolerate aging-induced delay on critical paths. On the evaluation, a reliability model based on widely used weibull distribution is presented. Experimental results show that, without compromising with any nominal architectural performance, ReviveNet can improve the Mean-Time-To-Failure by up to 48.7 percent, at the expense of 9.5 percent area overhead and small power increase.	adaptive architecture;elegant degradation;image scaling;mean time between failures;overhead (computing);progressive scan;sensor;synergy	Guihai Yan;Yinhe Han;Xiaowei Li	2011	IEEE Transactions on Computers	10.1109/TC.2011.33	ageing;real-time computing;statistics	EDA	5.784495496121116	60.15313813908614	191844
1d28ae5746f08405a3b7b3efe1a48b0d0b2f369e	a reconfigurable self-healing embryonic cell architecture	functional unit;fault tolerant	The long-term goal of our work described in this paper is the development of a biologically-inspired cellular fault-tolerant hardware system. The basic structure of the system is a multi-cellular embryonic array that is capable of achieving self-diagnostics, self-repair and fault recovery. The ‘nucleolus’ of the cell is a general purpose function unit comprised of a 2-to-1 multiplexer and a Dtype flip-flop. Input and configuration data to the function unit are provided by the DNA segment memory and I/O router. A diagnostic logic monitors the error free operation of the cell. When an error is detected the diagnostic logic requests the reconfiguration unit to kill the cell, transferring its function to a fault-free neighbouring cell. Once permission is granted, the faulty cell is eliminated and becomes transparent. The functionality of each cell will shift in the array until a spare cell is found. Finally the whole embryonic array recovers. Fault-free operation is then continued.	cell (microprocessor);flops;fault tolerance;flip-flop (electronics);input/output;multiplexer;router (computing)	Xiangrong Zhang;Gabriel Dragffy;Anthony G. Pipe;Nigel Gunton;Quan Min Zhu	2003			parallel computing;fault tolerance;architecture;computer science	ML	7.64185788182166	56.8856121740091	192247
3d7392c07125915ae2eb3a7ecbac2b968d8ce44e	using tag-match comparators for detecting soft errors	and fault tolerance;processor architecture;dynamic scheduling logic tag match comparator soft error detection microprocessor design out of order execution;fault tolerant;logic design;high energy;testing;chip;out of order execution;comparators circuits;dynamic scheduling logic;control structure;tag match comparator;data dependence;scheduling;soft error detection;identification technology;logic testing;transient fault;microprocessor design;processor architectures;soft error;control structure reliability;computer errors microprocessors fault detection broadcasting circuit faults logic pipelines registers electrical fault detection out of order;processor architectures control structure reliability testing and fault tolerance;dynamic scheduling;microprocessor chips;scheduling comparators circuits identification technology logic design logic testing microprocessor chips	Soft errors caused by high energy particle strikes are becoming an increasingly important problem in microprocessor design. With increasing transistor density and die sizes, soft errors are expected to be a larger problem in the near future. Recovering from these unexpected faults may be possible by reexecuting some part of the program only if the error can be detected. Therefore it is important to come up with new techniques to detect soft errors and increase the number of errors that are detected. Modern microprocessors employ out-of-order execution and dynamic scheduling logic. Comparator circuits, which are used to keep track of data dependencies, are usually idle. In this paper, we propose various schemes to exploit on-chip comparators to detect transient faults. Our results show that around 50% of the errors on the wakeup logic can be detected with minimal hardware overhead by using the proposed techniques.	comparator;data dependency;integrated circuit;microprocessor;operand;out-of-order execution;overhead (computing);processor design;register renaming;scheduling (computing);sensor;soft error;transistor	Gulay Yalcin;Oguz Ergin	2007	IEEE Computer Architecture Letters	10.1109/L-CA.2007.14	chip;embedded system;fault tolerance;computer architecture;parallel computing;logic synthesis;real-time computing;soft error;dynamic priority scheduling;microarchitecture;computer science;out-of-order execution;operating system;software testing;control flow;scheduling	Arch	7.640989607768916	59.67584463951788	192546
99b4656d86c1290fc9caeabaa2078c8f7fb22afa	a unified design flow to automatically generate on-chip monitors during high-level synthesis of hardware accelerators	trusted architectures high level synthesis hls safety security;runtime;monitoring;system on chip;synchronization;safety hls trusted architectures security;monitoring hardware runtime system on chip algorithm design and analysis synchronization;algorithm design and analysis;hardware	"""Security and safety are more and more important in embedded system design. A key issue, hence lies in the ability of systems to respond safely when errors occur at runtime, to prevent unacceptable behaviors that can lead to failures or sensitive data leakage. In this paper, we propose a design approach that automatically generates on-chip monitors (OCMs) during high-level synthesis (HLS) of hardware accelerators (HWaccs). OCM checks at runtime the input/output timing behavior, the control flow execution and algorithmic properties (via American National Standards Institute C assertions) of the monitored HWacc. OCM is implemented separately from the HWacc and an original technique is introduced for their synchronization. Two synthesis options are proposed to tradeoff between performance and area. Experiment results show that error detection on the control flow is <inline-formula> <tex-math notation=""""LaTeX"""">$16\times $ </tex-math></inline-formula> better compared to the existing approaches while the cost of assertions is reduced by 17.48% on average. The impact on execution time (i.e., latency of the HWacc) is decreased by <inline-formula> <tex-math notation=""""LaTeX"""">$2.76\times $ </tex-math></inline-formula> at no area penalty and up to <inline-formula> <tex-math notation=""""LaTeX"""">$4.5\times $ </tex-math></inline-formula> with less than 10% extra-area. The clock period overhead is at worst less than 5% and the overhead on the synthesis time of the HWacc to generate OCMs is 7.44% on average."""	algorithm;clock rate;control flow;design flow (eda);embedded system;error detection and correction;hardware acceleration;high- and low-level;high-level synthesis;input/output;overhead (computing);run time (program lifecycle phase);spectral leakage;systems design	Mohamed Ben Hammouda;Philippe Coussy;Loïc Lagadec	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2016.2587278	system on a chip;embedded system;algorithm design;synchronization;electronic engineering;parallel computing;real-time computing;computer science;operating system;algorithm;computer network	EDA	5.068573950088288	59.15533288759881	192828
a5bf1aef4c35f3fd003b07ef45e957f5c66e3f2f	challenges in managing timing and wiring contracts during hierarchical floorplanning and design closure	physical synthesis;hierarchical design	The growing complexity and size of designs have driven chip implementation teams to adopt hierarchical design methodologies that divide-and-conquer the design closure task. Wherein, the large chip is partitioned into physical blocks with boundary constraints for physical synthesis which are then integrated at the top-level to achieve overall design closure. Each block could be further partitioned until the block sizes are manageable from a tool turnaround time perspective. A key aspect to efficient hierarchical design involves generation of realistic block-level timing budgets and physical constraints (contracts) to enable parallel implementation of each block. In a typical hierarchical design flow, these block-level contracts are created based on early estimations of timing and wiring from the floorplanning phase but evolve as the design progresses, often requiring several iterations of design integration to converge. High-performance designs require efficient contract management while allowing seamless design closure optimizations across levels of hierarchy. This talk will focus on some of the challenges in managing contracts with emphasis on the implications for physical synthesis tools used in hierarchical design closure.	contract management;converge;design closure;floorplan (microelectronics);iteration;seamless3d;wiring	Shyam Ramji	2013		10.1145/2451916.2451966	real-time computing;simulation;design flow;mathematics;physics	EDA	7.758744595283139	54.78324711425149	193513
2a725b9a68461fb0c4a4f11df3352d44723814d4	resource-driven optimizations for transient-fault detecting superscalar microarchitectures	error aleatorio;resistance electrique;microprocessor;resistencia electrica componente;optimisation;early retirement;phenomene transitoire;microarchitecture;floating point unit;optimizacion;redundancia;penurie;reponse transitoire;vulnerability;effet dimensionnel;erreur aleatoire;computer architecture;vulnerabilite;performance improvement;transient response;respuesta transitoria;vulnerabilidad;detection defaut;redundancy;architecture ordinateur;penuria;size effect;fenomeno transitorio;transient fault;microarquitectura;random error;optimization;floating point;microprocesseur;coma flotante;arquitectura ordenador;transients;efecto dimensional;resistor;shortage;microprocesador;deteccion imperfeccion;soft error;redondance;defect detection;virgule flottante	Increasing microprocessor vulnerability to soft errors induced by neutron and alpha particle strikes prevents aggressive scaling and integration of transistors in future technologies if left unaddressed. Previously proposed instruction-level redundant execution, as a means of detecting errors, suffers from a severe performance loss due to the resource shortage caused by the large number of redundant instructions injected into the superscalar core. In this paper, we propose to apply three architectural enhancements, namely 1) floating-point unit sharing (FUS), 2) prioritizing primary instructions (PRI), and 3) early retiring of redundant instructions (ERT), that enable transient-fault detecting redundant execution in superscalar microarchitectures with a much smaller performance penalty, while maintaining the original full coverage of soft errors. In addition, our enhancements are compatible with many other proposed techniques, allowing for further performance improvement.	floating-point unit;incident response team;microarchitecture;microprocessor;sensor;superscalar processor;transistor	Jie S. Hu;Greg M. Link;Johnsy K. John;Shuai Wang;Sotirios G. Ziavras	2005		10.1007/11572961_17	resistor;floating-point unit;embedded system;real-time computing;soft error;microarchitecture;vulnerability;economic shortage;computer science;floating point;operating system;redundancy;random error;transient response	Arch	6.306652577609064	60.33590239077671	194430
016b20f4f3e807e4df2d18f567daa6e85b7e610d	modeling and analysis of micro-ring based silicon photonic interconnect for embedded systems	micro rings;doped microring resonators microring based silicon photonic interconnect embedded systems silicon photonic device fabrication technologies inter core interconnect inter die interconnect embedded many core processors power efficiency reliability system metrics silicon photonics device characteristics design space exploration electromagnetic simulation design spaces parametrized transfer matrices;micro rings silicon photonics optical interconnects;ring resonator;power efficiency;optical interconnections embedded systems integrated optics;integrated optics;design space;embedded system;embedded systems;silicon photonics mathematical model calibration optical switches optical waveguides passband;optical interconnects;silicon photonics;optical interconnect;information design;design space exploration;networked systems;analytical model;optical interconnections;modeling and analysis	Recent advances in silicon photonic device and fabrication technologies make silicon photonic interconnect a promising communication fabric to address the inter-core and inter-die interconnect challenges for future embedded many-core processors. Informed design decisions in silicon photonic interconnection require optimization of performance, power efficiency, and reliability for different application scenarios. Optimizing these network and system metrics require understanding of silicon photonics device characteristics. However, existing design space exploration methodologies rely on time-consuming electromagnetic simulations or measurement of fabricated devices. In this paper, we introduce analytical models of devices, explore their design spaces, and apply them to different applications. The analytical models consist of parametrized transfer-matrices, with parameters categorized as fabrication-induced parameters and design parameters. Fabrication-induced parameters can be calibrated against measurements of fabricated devices to achieve high accuracy, whereas design parameters help in extrapolating the device characteristics. We develop and calibrate analytical models of widely used passive and doped micro-ring resonators. Three case studies of silicon photonic interconnects are discussed to represent different embedded applications and quantify the design trade-offs including performance requirements, power efficiency, and reliability constraints from the network system level.	calibration (statistics);categorization;central processing unit;design space exploration;doping (semiconductor);electrical connection;embedded system;extrapolation;interconnection;manycore processor;mathematical optimization;optimizing compiler;performance per watt;requirement;semiconductor device fabrication;simulation;transfer matrix	Moustafa Mohamed;Zhijun Li;Xi Chen;Alan Rolf Mickelson;Li Shang	2011	2011 Proceedings of the Ninth IEEE/ACM/IFIP International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)	10.1145/2039370.2039406	materials science;electronic engineering;electrical engineering;nanotechnology	EDA	3.4781321044347027	58.26371110201207	197120
54a76655caa1c8bda3b48c51c1339f015573ade5	software-controlled dynamically swappable hardware design in partially reconfigurable systems	signal image and speech processing;circuits and systems;control structures and microprogramming;partial reconfiguration;electronic circuits and devices;hardware design	We propose two basic wrapper designs and an enhanced wrapper design for arbitrary digital hardware circuit designs such that they can be enhanced with the capability for dynamic swapping controlled by software. A hardware design with either of the proposed wrappers can thus be swapped out of the partially reconfigurable logic at runtime in some intermediate state of computation and then swapped in when required to continue from that state. The context data is saved to a buffer in the wrapper at interruptible states, and then the wrapper takes care of saving the hardware context to communication memory through a peripheral bus, and later restoring the hardware context after the design is swapped in. The overheads of the hardware standardization and the wrapper in terms of additional reconfigurable logic resources and the time for context switching are small and generally acceptable. With the capability for dynamic swapping, high priority hardware tasks can interrupt low-priority tasks in real-time embedded systems so that the utilization of hardware space per unit time is increased.		Chun-Hsian Huang;Pao-Ann Hsiung	2008	EURASIP J. Emb. Sys.	10.1155/2008/231940	hardware compatibility list;embedded system;computer architecture;real-time computing;computer science;operating system;hardware register	EDA	6.751396443362478	57.33429525485972	198530
31016f0f159e98204e7e5b0383ccfbe080594ed5	the effect of sparse switch patterns on the area efficiency of multi-bit routing resources in field-programmable gate arrays	processing element;storage allocation;digital signal processors;field programmable gate array;pins;memory management;routing tracks;routing;storage allocation field programmable gate arrays network routing;multibit addressable memory cells;multibit processing elements;network routing;configuration memory sharing routing resources;logic gates;memory architecture;fpga routing resources sparse switch patterns multibit routing resources field programmable gate arrays multibit processing elements digital signal processors multibit addressable memory cells configuration memory sharing routing resources routing tracks;routing field programmable gate arrays pins switches memory architecture memory management logic gates;digital signal processor;multibit routing resources;sparse switch patterns;field programmable gate arrays;switches;empirical evaluation;fpga routing resources	The increased use of multi-bit processing elements such as digital signal processors, multipliers, multi-bit addressable memory cells, and CPU cores has presented new opportunities for Field-Programmable Gate Array (FPGA) architects to utilize the regularity of multi-bit signals to increase the area efficiency of FPGAs. In particular, configuration memory sharing has been traditionally used to exploit multi-bit regularity for area. We observe that the process of creating configuration memory sharing routing resources often leads to the use of much sparser switch patterns for connecting multi-bit elements to their routing tracks. In this work, we empirically evaluate the effect of these sparse switch patterns on the area efficiency of FPGAs. It is shown that the sparse switch patterns alone contribute significantly to the area reduction observed in configuration memory sharing FPGAs. In particular, our experiments show that, without configuration memory sharing, sparse switch patterns can reduce the implementation area of multi-bit routing resources by 10.4% while configuration memory sharing contributes to an additional 1.2% in area savings. The observation holds over a wide range of connection block flexibility values and demonstrates that efficient switch pattern designs can be effectively used to increase the area efficiency of FPGA routing resources.	central processing unit;digital signal processor;experiment;field-programmability;field-programmable gate array;routing;sparse matrix	Ping Chen;Andy Ye	2008	2008 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2008.4629975	embedded system;digital signal processor;routing;parallel computing;real-time computing;computer science;field-programmable gate array	EDA	2.936608130484593	59.07355377768692	198548
5d9b4536f5c919e7acad61ecac30b8eb3e4d442f	implementation of self-healing asynchronous circuits at the example of a video-processing algorithm	asynchronous circuits hardware circuit faults field programmable gate arrays space technology space missions electrical fault detection fault detection runtime fault tolerance;detectors;self healing fsl design;video signal processing asynchronous circuits;circuit faults;video signal processing;hardware pre processing;video processing;asynchronous circuit;system recovery;registers;video processing unit;self healing fsl design self healing asynchronous circuit video processing algorithm hardware pre processing video processing unit autonomous reconfiguration standard fsl circuit;pipelines;asynchronous circuits;video processing algorithm;field programmable gate arrays;standard fsl circuit;self healing asynchronous circuit;hardware;autonomous reconfiguration	This paper presents a self-healing, asynchronous implementation of a small part of the hardware pre-processing to be used in the video processing unit of GAIA, a scientific mission of the European Space Agency (ESA). With the applied concept the circuit is able to detect permanent faults during runtime and to recover from them by an autonomous reconfiguration. The work describes how to transform a standard FSL circuit into a self-healing FSL design and presents first results of hardware experiments.	algorithm;autonomous robot;esa;experiment;fmrib software library (fsl);graphics processing unit;preprocessor;video processing	Thomas Panhofer;Werner Friesenbichler;Andreas Steininger	2010	2010 International Conference on Dependable Systems and Networks Workshops (DSN-W)	10.1109/DSNW.2010.5542609	embedded system;detector;real-time computing;asynchronous circuit;computer science;operating system;distributed computing;pipeline transport;video processing;processor register;field-programmable gate array	EDA	5.902328899803225	54.94223984382838	198711
4093f59c54cec0493177c4d3c172280ea1213855	introducing redundant computations in rtl data paths for reducing bist resources	behavioral synthesis;redundant operations;data flow graphs;degree of freedom;data flow graph;built in self test;resource sharing;control dependence;behavioral synthesis built in self test	The need for considering BIST requirements during the scheduling and assignment stages of behavioral synthesis has been demonstrated in previous research and techniques for reducing BIST resources of a data path during these stages of synthesis have been developed. However, the degree of freedom that can be exploited during scheduling and assignment to minimize these resources is often limited by the data and control dependencies of a behavior. In this paper, we propose transformation of a behavior before scheduling and assignment, namely introducing redundant computations such that the resulting data path is testable using few BIST resources. The transformation makes use of spare capacity of modules to add redundancy that enables test paths to be shared among the modules. A technique for identifying potential BIST resource sharing problems in a behavior and resolving them by redundant computations is presented. Introduiction of redundant computations is performed without compromising the latency and functional resource requirement of the behavior.	built-in self-test;computation;dependence analysis;high-level synthesis;requirement;scheduling (computing);software testability	Ishwar Parulkar;Sandeep K. Gupta;Melvin A. Breuer	2001	ACM Trans. Design Autom. Electr. Syst.	10.1145/383251.383253	shared resource;embedded system;parallel computing;real-time computing;computer science;data-flow analysis;distributed computing;degrees of freedom;programming language	EDA	6.158285104258995	56.893115406310244	199200
b3d36de2bb6d6e7af7cb15bb05193b0d43655113	a context saving fault tolerant approach for a shared memory many-core architecture	system on chip embedded systems fault tolerance multiprocessing systems system recovery;software;context computer architecture fault tolerance fault tolerant systems software synchronization hardware;full error recovery shared memory many core architecture context saving fault tolerant approach runtime fault tolerance transient faults permanent faults process variability aging effects upset susceptibility reschedule tasks p2012 embedded multicore architecture industrial image processing;computer architecture;fault tolerant systems;rollback noc based mpsoc fault recovery context saving checkpointing;synchronization;fault tolerance;context;hardware	Mechanisms for runtime fault-tolerance in many-core architectures are mandatory to cope with transient and permanent faults. This issue is even more relevant with aggressive technology nodes due to process variability, aging effects, and susceptibility to upsets, among other factors. This work proposes to save periodically the context and to re-schedule tasks to the last reliable known state and avoid the faulty processor. This technique is implemented on an embedded multicore architecture named P2012. The proposed fault-tolerant approach induces a limited overhead of 9.37% in an industrial image processing application while guaranteeing a full-error recovery if any error is detected.	embedded system;fault tolerance;heart rate variability;image processing;intel core (microarchitecture);mpsoc;manycore processor;multi-core processor;overhead (computing);run time (program lifecycle phase);shared memory	Eduardo Wächter;Nicolas Ventroux;Fernando Gehm Moraes	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7168947	embedded system;synchronization;fault tolerance;parallel computing;real-time computing;telecommunications;computer science;engineering;general protection fault;software fault tolerance	Embedded	5.982573779669761	59.30640596936754	199439
d63bd92740a9bac9f6797ff901efd9e1cd2c854a	system level approaches for mitigation of long duration transient faults in future technologies	cycle time;checker circuits long duration transient faults mitigation transient pulses verification schemes matrix multiplication;fault tolerant;transient pulses;transient analysis fault tolerant computing;long duration transient faults;transient analysis;verification schemes;fault tolerant computing;checker circuits;transient fault;mitigation;matrix multiplication;soft error;circuit faults clocks fault tolerant systems single event upset pulse circuits frequency application software silicon switches costs	The evolution of the technology in search of smaller and faster devices brings along the need for a new paradigm in the design of circuits tolerant to soft errors. The current assumption of transient pulses shorter than the cycle time of the circuit will no longer be true, thereby precluding the use of most of the mitigation techniques proposed so far. With transient faults duration spanning more than one clock cycle of operation, new fault tolerance solutions, working at the system level, with low area and performance overheads, must be devised. In this paper we propose the first steps in the direction of using low cost verification schemes at the algorithmic level, applied to general purpose matrix multiplication applications. Experimental results obtained with two different implementations of checker circuits using the proposed technique are presented and discussed.	clock signal;fault tolerance;file spanning;matrix multiplication;programming paradigm;soft error	Carlos Arthur Lang Lisbôa;Marcelo Ienczczak Erigson;Luigi Carro	2007	12th IEEE European Test Symposium (ETS'07)	10.1109/ETS.2007.39	fault tolerance;electronic engineering;parallel computing;real-time computing;soft error;matrix multiplication;cycle time variation;computer science;engineering	EDA	8.512473420021074	58.77270637299543	199641
