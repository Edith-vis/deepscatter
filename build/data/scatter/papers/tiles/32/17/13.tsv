id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
25dfd8fc46249c82e4573c4ee39f4f34c326e1d5	correlating twitter with the stock market through non-gaussian svar	biological system modeling;stock markets;time series analysis;data visualization;mathematical model;predictive models;twitter	In this paper, we aim at studying the correlation between Twitter and the stock market. Specifically, we first apply non-Gaussian SVAR (structural vector autoregression) to identify possible relationships among the Twitter and stock market factors. Compared with conventional models such as Granger causality method which assume that the error items are Gaussian and only consider time-lag effect, non-Gaussian SVAR is under the assumption that the error items are non-Gaussian, better fitting the data in the stock market, and takes both instantaneous and time-lagged effects into account. We also visualize some distinctive relationships in parallel coordinates which is a well-developed multivariate visualization technique but seldom used in financial studies to the best of knowledge. Then, with the purpose of examining whether the Twitter-stock market relationship returned by non-Gaussian SVAR can help predict the stock market indicators, we build a series of regression models to predict DJI (Dow Jones Industrial Average Index) return in a sliding time window. Our experiments demonstrate that all the Twitter factors correlate with DJI return, and only the negative sentiment in tweets (posts on Twitter) is associated with DJI return volatility. Moreover, the lagged Twitter factors are more effective than the lagged stock market indicators in terms of predicting DJI return in the period of our data set.	autoregressive model;causality;experiment;jones calculus;parallel coordinates;time series;vector autoregression;volatility	Shaohua Tan;Xinhai Liu;Shuai Zhao;Yunhai Tong	2016	2016 Eighth International Conference on Advanced Computational Intelligence (ICACI)	10.1109/ICACI.2016.7449835	financial economics;marketing;data mining;business	ML	4.8441666005572	-14.52979021563042	159475
cad8d566a56a503e876cc23f3409e1164c7aa32d	gain-loss separability and coalescing in risky decision making	modelizacion;analyse risque;taxe;decision risk;tax;game theory;methode empirique;analisis datos;behavioral analysis;risk analysis;prospect theory;teoria decision bajo riesgo;taux erreur;analisis forma;metodo empirico;gestion risque;risk management;empirical method;tasa;teoria juego;theorie jeu;prise decision;separability;satisfiability;utility preference;modelisation;analisis riesgo;data analysis;separabilidad;prediction theory;analyse comportementale;estimacion parametro;preferencia;decision choice;error rate;eleccion modal;choix modal;analyse donnee;separabilite;analisis conductual;preference;modele donnee;pattern analysis;theorie decision hazardeuse;gestion riesgo;parameter estimation;estimation parametre;modal choice;theorie prediction;toma decision;cumulative prospect theory;indice error;modeling;analyse forme;data models	This experiment tested two behavioral properties of risky decision making, gain-loss separability and coalescing. Cumulative prospect theory (CPT) implies both properties, but the transfer of attention exchange model (TAX) violates both. Original prospect theory satisfies gain-loss separability but may or may not satisfy coalescing, depending on whether editing rules are assumed. A configural form of CPT proposed by Wu and Markle violates gain-loss separability but satisfies coalescing. New tests were designed and conducted to test these theories against specific predictions of a TAX model. This model used parameters estimated from previous data together with simple new assumptions to extend TAX to gambles with negative and mixed consequences. Contrary to all three forms of prospect theory, systematic violations of both coalescing and of gain-loss separability were observed. Violations of GLS were confirmed by analyses of individual data patterns by means of an error model in which each choice can have a different rate of error. Without estimating any parameters from the new data, the TAX model predicted the majority choices in the new data fairly well, correctly predicting when modal choices would violate gain-loss separability, when they would satisfy it, and when indifference would be observed. Risky Decision Making 2	cpt (file format);generalized least squares;linear separability;modal logic;theory	Michael H. Birnbaum;Jeffrey P. Bahra	2007	Management Science	10.1287/mnsc.1060.0592	prospect theory;data modeling;game theory;econometrics;systems modeling;risk analysis;economics;risk management;word error rate;artificial intelligence;finance;mathematical economics;estimation theory;data analysis;empirical research;management;statistics;satisfiability	ML	3.1219659646710083	-9.989496717842313	159741
7e4d8e9883fc8b17a32fb83224229115ed8f11ae	real-time traffic data smoothing from gps sparse measures using fuzzy switching linear models		Traffic is one of the urban phenomena that have been attracting substantial interest in different scientific and industrial communities since many decades. Indeed, traffic congestions can have severe negative effects on people's safety, daily activities and quality of life, resulting into economical, environmental and health burden for both governments and organizations. Traffic monitoring has become a hot multi-disciplinary research topic that aims to minimize traffic's negative effects by developing intelligent techniques for accurate traffic states’ estimation, control and prediction. In this paper, we propose a novel algorithm for traffic state estimation from GPS data and using fuzzy switching linear models. The use of fuzzy switches allows the representation of intermediate traffic states, which provides more accurate traffic estimation compared to the traditional hard switching models, and consequently enables making better proactive and in-time decisions. The proposed algorithm has been tested on open traffic datasets collected in England, 2014. The results of the experiments are promising, with a maximum absolute relative error equal to 9.04%. © 2017 The Authors. Published by Elsevier B.V. Peer-review under responsibility of the Conference Program Chairs.	algorithm;approximation error;experiment;global positioning system;linear model;network switch;real-time transcription;smartphone;smoothing;sparse;traffic exchange;unsupervised learning;usability	Zied Bouyahia;Hedi Haddad;Nafaâ Jabeur;Stéphane Derrode	2017		10.1016/j.procs.2017.06.136	traffic generation model;machine learning;data mining;fuzzy logic;artificial intelligence;linear model;global positioning system;smoothing;computer science;approximation error;insync adaptive traffic control system	HCI	8.854847645462337	-14.297559864707218	161818
cb3f6cdd642f90ddd846c43ec6cbff51affc984e	application of cascaded correlation neural network for financial performance prediction and analysis of bsnl		In this paper Cascaded Correlation Neural network is used to integrate fundamental and technical analysis for financial performance prediction in Public Sector Enterprise Bharat Sanchar Nigam Limited (BSNL). The analysis is made based on the financial statement variables and macroeconomic variables. Experiments have eight years’ financial data and macroeconomic data of Bharat Sanchar Nigam Limited had taken for analysis. The analytical and technical analysis in comparison with trend analysis results show that financial statement variables and macroeconomic variables together generate significant prediction of the future performance of the enterprise and accordingly effective decision can be made to improve the effectiveness of the PSU-BSNL.	artificial neural network;performance prediction;power supply unit (computer)	N. Albert Singh;T. Naryanan	2014		10.1007/978-3-319-20294-5_44	machine learning	ML	3.523865337531317	-16.392070915124695	162800
7a21bbfc279b0192f1dd89915a2db21485832a54	using significant classification rules to analyze korean customers' power consumption behavior: incremental tree induction using cascading-and-sharing method	power engineering computing data mining decision making decision trees electricity supply industry load electric pattern classification power consumption;korean customers power consumption behavior analysis;training;temperature sensors;classification algorithms classification tree analysis training data mining load forecasting temperature sensors;cascading and sharing method;incremental mining;data mining;load forecasting;power consumption behavior;power engineering computing;load electric;classification rules;incremental tree induction algorithm significant classification rules korean customers power consumption behavior analysis cascading and sharing method power load analysis electrical industry data mining technique decision making;classification algorithms;power consumption behavior incremental mining decision tree induction significant classification rules;pattern classification;significant classification rules;classification tree analysis;power consumption;electricity supply industry;electricity industry;decision tree induction;decision trees;electrical industry;power load analysis;data mining technique;incremental tree induction algorithm	Power load analysis is an important issue in electrical industry. Data mining techniques are widely studied methodology for power load analysis and it helps decision making on electrical industry. In this paper, we propose an incremental tree induction algorithm using Cascading-and-Sharing method, and use mined significant classification rules to analyze customers’ power consumption behavior in General, Education and Regular groups.	algorithm;data mining;decision tree;mined	Minghao Piao;Meijing Li;Keun Ho Ryu	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.503	statistical classification;computer science;electric power industry;machine learning;data mining	Robotics	8.63079471624554	-16.57673458632269	163277
b0e6c01526b9653019bb7f9dcc5b1d22a7b90243	hierarchical modeling of seed variety yields and decision making for future planting plans		Eradicating hunger and malnutrition is a key development goal of the 21st century. We address the problem of optimally identifying seed varieties to reliably increase crop yield within a risk-sensitive decision making framework. Specifically, we introduce a novel hierarchical machine learning mechanism for predicting crop yield (the yield of different seed varieties of the same crop). We integrate this prediction mechanism with a weather forecasting model, and propose three different approaches for decision making under uncertainty to select seed varieties for planting so as to balance yield maximization and risk. We apply our model to the problem of soybean variety selection given in the 2016 Syngenta Crop Challenge. Our prediction model achieves a median absolute error of 3.74 bushels per acre and thus provides good estimates for input into the decision models. Our decision models identify the selection of soybean varieties that appropriately balance yield and risk as a function of the farmer’s risk aversion level. More generally, our models support farmers in decision making about which seed varieties to plant.		Huaiyang Zhong;Xiaocheng Li;David B. Lobell;Stefano Ermon;Margaret L. Brandeau	2017	CoRR		twenty-first century;sowing;management science;decision model;random forest;crop yield;maximization;risk aversion;engineering	ML	6.297975075348595	-10.717630917530094	163383
36b49107cd263eb30fdc9bae68491865847ccf7f	validation of a per-lane traffic state estimation scheme for highways with connected vehicles		This study presents a thorough microscopic simulation investigation of a recently developed model-based approach for per-lane density estimation, as well as on-ramp and off-ramp flow estimation, for highways in the presence of connected vehicles. The estimation methodology is mainly based on the assumption that a certain percentage of vehicles is equipped with Vehicle Automation and Communication Systems (VACS), which provide the necessary measurements used by the estimator, namely vehicle speed and position measurements. In addition, a minimum number of conventional flow detectors is needed. In the investigation, a calibrated and validated, with real data, microscopic multi-lane model is employed, which concerns a stretch of motorway A20 from Rotterdam to Gouda in the Netherlands. It is demonstrated that the proposed methodology provides satisfactory estimation performance even for low penetration rates of connected vehicles.	algorithm;automation;connected car;diagram;experiment;kalman filter;network congestion;onset (audio);penetration test;ramp simulation software for modelling reliability, availability and maintainability;real-time transcription;requirement;sensor	Sofia Papadopoulou;Claudio Roncoli;Nikolaos Bekiaris-Liberis;Ioannis Papamichail;Markos Papageorgiou	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317704	simulation;automation;estimator;communications system;density estimation;engineering;control engineering	Robotics	9.561661493203859	-12.192542536292065	164216
99237d9b711cdc0aadabe8313d66176fd24308ef	selection of an investment portfolio by means of a mathematical model of optimization applied to mexican stock-market in period of debacle	mathematical optimization model;economic debacle;capital assets pricing model;return maximization;stock market;stock markets investment linear programming markov processes mathematical analysis;mexican stock market investment portfolio mathematical optimization model;mexican stock market;joints;data mining;mathematical analysis;investment;stock markets;stock exchange;investments portfolios mathematical model;risk minimization investment portfolio mathematical model optimization mexican stock market linear programming markowitz model capital assets pricing model behavior analysis economic debacle return maximization;investment portfolio;capital asset pricing model;decision support systems;linear programming;mathematical model;markowitz model;linear program;optimization;markov processes;risk minimization;behavior analysis;optimization model	In this paper appears a behavior analysis of the Mexican stock-market between a previous period to the economic debacle of the 2008 and during the period of the first month of the debacle. We propose to use a mathematical model of linear programming for the selection of an investment portfolio in Mexican stock-market. The mathematical model is based on the modification of Markowitz Model and Capital Assets Pricing Model. The model proposed contains two functions objective: 1) to maximize the return and 2) to minimize the risk. The Application of the proposed model to select an investment portfolio was realized with the public data of Mexican stock-market taking a sample from the stock-exchange, than they are part of the calculation of the Index of Prices and Quotations. The results show that our model is able to determine an investment portfolio with very different scenes and in period of debacle. Also it does determine that when comparing two investment portfolios are better to select the portfolio with the greater return instead of the portfolio with the smaller risk.	information privacy;linear programming;mathematical model;optimizing compiler	José Crispín Zavala Díaz;Dalia V. Garcia-Villagomez;Jorge A. Ruiz-Vanoye;Ocotlán Díaz-Parra	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.117	post-modern portfolio theory;capital asset pricing model;stock exchange;holding period return;investment strategy;replicating portfolio;investment;linear programming;modern portfolio theory;mathematical model;portfolio optimization;rate of return on a portfolio;investment performance;black–litterman model	Vision	3.8499999443620267	-10.520412381458483	164401
239e2054f1464c9696600111b6921e49ce23fc19	nonparametric estimation of a hedonic price index for adsl connections in the european market using the akaike information criterion	akaike information criterion;hedonic price;consumer choice;nonparametric estimation;adsl connections;sliced inverse regression;local polynomial regression;hedonic method;hedonic price index;hedonic model;asymmetric digital subscriber line	It is evidence that the demand for Asymmetric Digital Subscriber Line (ADSL) connections increases day by day in all European countries as much as worldwide and it is well known that Internet’s penetration is considerably amazing. At the same time, due to the fierce competition among ADSL connections providers, several packages are offered in attractive tariffs. As a product consists of various characteristics that consumers value, the question that arise could be summarized to the following: How should consumers’ choices and preferences for ADSL connections affect tariffs and what are the more significant and powerful characteristics that shape tariffs of ADSL connections? This paper provides a hedonic price analysis of ADSL connections for the European market. A problem which is posed is the selection of the best model. So, in order to estimate prices a sliced inverse regression (SIR) is performed, without knowing the shape of the function. Then by applying Local Polynomial Regression (LPR) a possible shape of the hedonic function is given. Among several candidate hedonic models and by applying Akaike Information Criterion (AIC), the best one is derived. In order to evaluate the proposed methodology, tariffs’ data have been collected from 15 European countries over the period from 2003 to C. Deligiorgi ( ) · C. Michalakelis · A. Vavoulas · D. Varoutas Department of Informatics and Telecommunications, University of Athens, Athens 15784, Greece e-mail: ntina@di.uoa.gr C. Michalakelis e-mail: michalak@di.uoa.gr A. Vavoulas e-mail: vavoulas@di.uoa.gr D. Varoutas e-mail: arkas@di.uoa.gr 2005. Apart from tariffs, information on characteristics such as supported data rate (DR), maximum consumed data volume (V) and maximum allowed minutes on line (T) have been collected and analyzed.	akaike information criterion;asymmetric digital subscriber line;best practice;data rate units;email;hedonic regression;higher-order function;informatics;polynomial;sliced inverse regression	C. Deligiorgi;Christos Michalakelis;Alexander Vavoulas;Dimitris Varoutas	2007	Telecommunication Systems	10.1007/s11235-008-9066-4	econometrics;hedonic index;akaike information criterion;computer science;sliced inverse regression;asymmetric digital subscriber line;statistics	ECom	5.320074941641579	-16.078049849400102	164997
2da7ec10d4d87982c49891bce2eb1abb073ae9a2	a study on the use of machine learning methods for incidence prediction in high-speed train tracks		In this paper a study of the application of methods based on Computational Intelligence (CI) procedures to a forecasting problem in railway maintenance is presented. Railway maintenance is an important and long-standing problem that is critical for safe, comfortable and economic transportation. With the advent of high-speed lines, the problem has even more importance nowadays. We have developed a study, applying forecasting procedures from Statistics and CI, to examine the feasibility of predicting one-month-ahead faults on two high-speed lines in Spain. The data are faults recorded by a measurement train which traverses the lines monthly. The results indicate that CI methods are competitive in this forecasting task against the Statistical regression methods, with e-support vector regression outperforming the other employed methods. So, application of CI methods is feasible in this forecasting task and it is useful in the planning process of track maintenance.	incidence matrix;machine learning	Christoph Bergmeir;Gregorio Ismael Sainz Palmero;Carlos Martínez Bertrand;José Manuel Benítez	2013		10.1007/978-3-642-38577-3_70	simulation;artificial intelligence;machine learning	ML	7.625995049126128	-16.866617056691357	166600
6f4b01a00ef7b4d598eba790fe0941fcb194bbdd	ideas by statistical mechanics (ism)	statistical mechanics;statistical mechanical model;risk management;model evolution;copula financial riskmanagement code;correlated multivariate system;financial market;simulated annealing;neocortical interactions;financial risk management;exogenous interaction;exogenous external connectivities	Ideas by Statistical Mechanics (ISM) is a generic program to model evolution and propagation of ideas/patterns throughout populations subjected to endogenous and exogenous interactions. The program is based on the author’s work in Statistical Mechanics of Neocortical Interactions (SMNI), and uses the author’s Adaptive Simulated Annealing (ASA) code for optimizations of training sets, as well as for importance-sampling to apply the author’s copula financial risk-management codes, Trading in Risk Dimensions (TRD), for assessments of risk and uncertainty. This product can be used for decision support for projects ranging from diplomatic, information, military, and economic (DIME) factors of propagation/evolution of ideas, to commercial sales, trading indicators across sectors of financial markets, advertising and political campaigns, etc. It seems appropriate to base an approach for propagation of ideas on the only system so far demonstrated to develop and nurture ideas, i.e., the neocortical brain. A statistical mechanical model of neocortical interactions, developed by the author and tested successfully in describing short-term memory and EEG indicators, is the proposed model. ISM develops subsets of macrocolumnar activity of multivariate stochastic descriptions of defined populations, with macrocolumns defined by their local parameters within specific regions and with parameterized endogenous inter-regional and exogenous external connectivities. Parameters of subsets of macrocolumns will be fit using ASA to patterns representing ideas. Parameters of external and inter-regional interactions will be determined that promote or inhibit the spread of these ideas. Tools of financial risk management, developed by the author to process correlated multivariate systems with differing non-Gaussian distributions using modern copula analysis, importancesampled using ASA, will enable bona fide correlations and uncertainties of success and failure to be calculated. Marginal distributions will be evolved to determine their expected duration and stability using algorithms developed by the author, i.e., PATHTREE and PATHINT codes.	adaptive simulated annealing;algorithm;decision support system;electroencephalography;implicit shape model;importance sampling;interaction;long short-term memory;marginal model;population;qr code;risk management;sampling (signal processing);software propagation	Lester Ingber	2007	Transactions of the SDPS		mathematical optimization;simulation;simulated annealing;risk management;statistical mechanics;computer science;artificial intelligence;machine learning;mathematics;operations research;algorithm;quantum mechanics;statistics	ML	3.7621030898208314	-12.77390261101476	167250
bb234f78490df5fccbc28972777a4cc8cf5fe856	predictive models with endogenous variables for quality control in customized scenarios affected by multiple setups	structural model;pls regression;kalman filter;endogenous variables;arima	The crescent demand for customized products has challenged industries with reduced lot sizes. As a result, frequent product model changing and short series of observable variables decreased the performance of many traditional tools used in process control. This paper proposes the use of endogenous variables in predictive models aimed at overcoming the multiple setup and short production runs problems found in customized manufacturing systems. The endogenous variables describe the type/model of manufactured products, while the response variable predicts a product quality characteristic. Three robust predictive models, ARIMA, structural model with stochastic parameters fitted by Kalman filter, and Partial Least Squares (PLS) regression, are tested in univariate time series relying on endogenous variables. The PLS modeling yielded better predictions in real manufacturing data, while the structural model led to more robust results in simulated data. 2013 Elsevier Ltd. All rights reserved.	autoregressive integrated moving average;kalman filter;observable;partial least squares regression;predictive modelling;stochastic modelling (insurance);time series	André L. Korzenowski;Michel J. Anzanello;Marcelo S. Portugal;Carla Schwengber ten Caten	2013	Computers & Industrial Engineering	10.1016/j.cie.2013.04.011	kalman filter;econometrics;autoregressive integrated moving average;computer science;engineering;operations management;machine learning;mathematics;statistics	AI	5.702613060281054	-15.010836621235047	167858
abe5f8126dd58cb7dc5a09d9143c7b113e80f26a	sparse-coding-based household clustering for demand response services	demand response;household clustering;companies;hidden markov models;load management;mathematical model;smart meter;encoding;power demand;sparse coding;smart meters	For the development of a smart grid, smart meter is a key device to measure the electric power usage of network-connected houses. Smart meters are currently being installed into households, and play the important role for providing a demand response service. A demand response is a necessary service in order to adjust supply-demand balancing because the balance is kept by the cost in the electricity market. Therefore, the customers of the electricity supply companies are expected to be optimally assigned as a group. In this paper, we separate a set of households into clusters as optimally assigned customers. When conducting the clustering, the utilization of unsupervised learning using data from a smart meter is required. In this study, we propose a method of household clustering for a demand response event by sparse coding, which is a type of neural network. The proposed method generates a power consumption model of each household, finds simple relationship distances between households, and conducts hierarchical clustering based on these distances. In addition, to extract the characteristics of fluctuating load usage, we conduct data normalization that cuts off at a fixed load usage of each household. To confirm the effect of the proposed clustering method, the usage tendency of household air conditioning (A/C) units was evaluated.	artificial neural network;cluster analysis;common criteria;hierarchical clustering;neural coding;smart meter;sparse matrix;unsupervised learning	Shintaro Ikeda;Hiroaki Nishi	2016	2016 IEEE 25th International Symposium on Industrial Electronics (ISIE)	10.1109/ISIE.2016.7744982	simulation;marketing;operations management;business	Embedded	8.591707169187618	-16.841017472039255	168295
2f74bf4cf87536310f8d6bcd4f9249eb2a96d565	to bid or not to bid in streamlined ec2 spot markets		"""Previously, Amazon EC2 Spot prices were always driven by short-term trends in supply and demand, requiring consumers to have an in-depth understanding of Spot markets and the bidding process in order to make """"intelligent"""" time-vs-money-vs-value trade-offs. However, with the newly announced streamlined access model for Spot instances, Amazon states that the Spot prices will adjust more gradually based on long-term trends instead of reacting to short-term fluctuations in demand and supply. Therefore, consumers are no longer required to understand Spot markets and bidding, and yet can save up to 90% off the On-Demand prices. In this paper, we study the pricing patterns before and after the introduction of the modified model using standard statistical approaches including econometric inequality indices (the Gini coefficient and the Theil index), logistic regression, a hybrid forecasting technique based on Naïve, and Principal Component Analysis. Our findings confirm the announcements made by Amazon including less frequent Spot price changes, disappearance of sudden spikes, and smooth (but not necessarily gradual) adjustments in the Spot prices. Rather surprisingly, with the introduction of the new model, the median Spot prices have risen in the majority of the Spot markets. In addition, even in the changed access model Spot price forecasting can still yield valuable insights into the evolution and structure of a given Spot market, although there may no longer be a need for sophisticated bidding strategies."""	amazon elastic compute cloud (ec2);coefficient;darknet market;f-spot;logistic regression;principal component analysis;social inequality;theil index	Mohan Baruwal Chhetri;Markus Lumpe;Quoc Bao Vo;Ryszard Kowalczyk	2018	2018 IEEE International Conference on Services Computing (SCC)	10.1109/SCC.2018.00024	financial economics;logistic regression;theil index;gini coefficient;spot contract;spot market;bidding;supply and demand;economics	Robotics	4.627992870414827	-14.898425966860485	168416
d899e13cf090560509f815404b9dc062ab8f76cc	applications of data mining techniques to electric load profiling			data mining;electrical load	Barnaby Pitt	2000				ML	7.548889717771231	-11.498337114957575	168741
1f33ee2c12dafa34e7191f7569af3c521b735197	signal disaggregation via sparse coding with featured discriminative dictionary	low sample rate shape and activation features discriminative dictionary disaggregation;water conservation data mining encoding signal processing smart meters;disaggregation;low sample rate;data mining;signal processing;discriminative dictionary;shape and activation features;activation features signal disaggregation sparse coding featured discriminative dictionary scfdd freshwater shortage water conservation device level consumption smart meter whole home water consumption component appliances;encoding;smart meters;water conservation;dictionaries encoding shape performance evaluation water conservation market research testing	As the issue of freshwater shortage is increasing daily, it's critical to take effective measures for water conservation. Based on previous studies, device level consumption could lead to significant conservation of freshwater. However, current smart meter deployments only produce low sample rate aggregated data. In this paper, we examine the task of separating whole-home water consumption into its component appliances. A key challenge is to address the unique features of low sample rate data. To this end, we propose Sparse Coding with Featured Discriminative Dictionary (SCFDD) by incorporating inherent shape and activation features to capture the discriminative characteristics of devices. In addition, extensive experiments were performed to validate the effectiveness of SCFDD.	benchmark (computing);data mining;dictionary;discriminative model;experiment;freshwater ecosystem;machine learning;neural coding;sampling (signal processing);smart meter;sparse;sparse matrix	Bingsheng Wang;Feng Chen;Haili Dong;Arnold P. Boedihardjo;Chang-Tien Lu	2012	2012 IEEE 12th International Conference on Data Mining	10.1109/ICDM.2012.146	water conservation;speech recognition;computer science;signal processing;pattern recognition;data mining;encoding	DB	8.68268669929245	-16.02032208964554	169072
e8c4f7eb954d28c519a79c0c1c80749a95afce58	a latent-class model for estimating product-choice probabilities from clickstream data		This paper analyzes customer product-choice behavior based on the recency and frequency of each customer’s page views on e-commerce sites. Recently, we devised an optimization model for estimating product-choice probabilities that satisfy monotonicity, convexity, and concavity constraints with respect to recency and frequency. This shape-restricted model delivered high predictive performance even when there were few training samples. However, typical e-commerce sites deal in many different varieties of products, so the predictive performance of the model can be further improved by integration of such product heterogeneity. For this purpose, we develop a novel latent-class shape-restricted model for estimating product-choice probabilities for each latent class of products. We also give a tailored expectation-maximization algorithm for parameter estimation. Computational results demonstrate that higher predictive performance is achieved with our latent-class model than with the previous shape-restricted model and common latent-class logistic regression.	clickstream;cluster analysis;collaborative filtering;computation;concave function;convex function;customer relationship management;e-commerce;estimation theory;expectation–maximization algorithm;experiment;latent class model;logistic regression;mathematical optimization;microelectronics and computer technology corporation;page view;recommender system	Naoki Nishimura;Noriyoshi Sukegawa;Yuichi Takano;Jiro Iwanaga	2018	Inf. Sci.	10.1016/j.ins.2017.11.014	econometrics;computer science;artificial intelligence;machine learning;data mining;statistics	AI	4.611856961567404	-12.061895544644289	169307
8d20f7eb1cc91597eb9a7f369566d24152404788	relationship between job opportunities and economic environments measured from data in internet job searching sites	job opportunities;regression analysis;fluctuation scaling;governmental grid square statistics data	This study investigates the number of job opportunities collected from a Japanese job searching site (“fromA navi”) and an international job searching site (“Indeed”). We confirm that a relationship between the number of job opportunities and socioeconomic quantities (the population, the numbers of firms and workers) in each 1-km grid square in Japan. The number of workers is the best explanatory variable to explain the number of job opportunities in Japan. The regression coefficients can be used as an indicator to grasp Japanese macroeconomic conditions. From a global point of view, we analyse the number of job opportunities in about 16,000 cities all over the world. We confirm the daily number of job opportunities in each city varies in time and show some associations with macroeconomic indicators. We compute a relationship between means of the daily number of job opportunities and their standard deviations and confirm that it follows a scaling relationship with power law exponent α = 1. A possible model based on Poisson processes with intensity of which varies in time on the basis of a common noise is proposed to explain the phenomenon empirically observed. c © 2015 The Authors. Published by Elsevier B.V. Peer-review under responsibility of KES International.	coefficient;image scaling	Aki-Hiro Sato;Chihiro Shimizu;Takayuki Mizuno;Takaaki Ohnishi;Tsutomu Watanabe	2015		10.1016/j.procs.2015.08.191	simulation;computer science;machine learning;data mining;management science;regression analysis;statistics	HPC	3.8067574194744305	-13.095481822111752	170532
611362abf3f007fddf0410a2f1c39b33d840c0e1	u s west implements a cogent analytical model for optimal vehicle replacement	optimal vehicle replacement;model-based replacement process;replacement score;diverse fleet;annual benefit;replacement subject;cogent analytical model;estimated replacement cost;estimated maintenance cost;opportunity cost;fleet capital expenditure;annual maintenance cost	We developed and implemented a model-based replacement process for a diverse fleet of vehicles at U S WEST, a major telecommunications company (since June 2000, Qwest Communications International). The model considers relevant age-dependent factors, including annual maintenance cost, opportunity cost of downtime, depreciation, and salvage value. It assigns a replacement score to each candidate vehicle based on age, type, estimated replacement cost, and estimated maintenance cost in the next year of operation. The model then rank-orders the vehicles by score and identifies them for replacement subject to a budget constraint on fleet capital expenditure. Through implementation of the model-based process, the company expects an annual benefit of more than $13 million.		Dennis C. Dietz;Paul A. Katz	2001	Interfaces	10.1287/inte.31.5.65.9659	simulation;computer science;engineering;operations management;policy-based design;operations research	DB	5.436753108412675	-10.372120622144898	171257
6d8559c9f632b4979f8ccde057e94a047c0b3e18	regression cloud models and their applications in energy consumption of data center		As cloud data center consumes more and more energy, both researchers and engineers aim to minimize energy consumption while keeping its services available. A good energy model can reflect the relationships between running tasks and the energy consumed by hardware and can be further used to schedule tasks for saving energy. In this paper, we analyzed linear and nonlinear regression energy model based on performance counters and system utilization and proposed a support vector regression energy model. For performance counters, we gave a general linear regression framework and compared three linear regression models. For system utilization, we compared our support vector regression model with linear regression and three nonlinear regression models. The experiments show that linear regression model is good enough to model performance counters, nonlinear regression is better than linear regression model for modeling system utilization, and support vector regression model is better than polynomial and exponential regression models.		Yanshuang Zhou;Na Li;Hong Li;Yongqiang Zhang	2015	J. Electrical and Computer Engineering	10.1155/2015/143071	econometrics;proper linear model;simulation;multivariate adaptive regression splines;engineering;linear regression;linear model;path coefficient;nonparametric regression;regression analysis;statistics	Metrics	9.381806766296942	-16.700259224910106	172684
88472dafc59e0a2b5558079e4860e99e41257198	optimal life-cycle portfolio choice for chinese residents with housing	analytical models;optimisation;chinese resident;investments;optiaml;finance;life cycle;financial management;risk analysis;property market;real estate market;transaction cost;stock investment;risk management;biological system modeling;crowding out;conference management;portfolios;investment;technology management;owner occupied housing;optimal life cycle portfolio choice;transaction cost optimal life cycle portfolio choice chinese resident stochastic labor income risky owner occupied house housing rental market chinese real estate market stock investment downpayment ratio;chinese real estate market;stochastic processes;life cycle costing;downpayment ratio;engineering management;loans and mortgages;stochastic labor income;stochastic processes investment life cycle costing optimisation property market;owning and renting;risky owner occupied house;predictive models;housing rental market;economics;portfolio choice;numerical models;home ownership;dynamic;housing;housing optiaml dynamic portfolio choice owning and renting;portfolios risk management risk analysis investments finance predictive models conference management financial management technology management engineering management	We develop a dynamic realistically model of a typical Chinese resident with stochastic labor income, risky owner-occupied house, housing rental market and costly adjustment in housing of Chinese real estate market, and try to analyze the optimal life-cycle portfolio choice for Chinese residents with housing factors. Our analysis indicates that home ownership crowds out stock investment of Chinese residents. Downpayment ratio and transaction cost of housing crowd out the stockholdings of young homeowners, and have significant impact on residents’ life-cycle portfolio choice.	modern portfolio theory;stochastic process	Lingling Huang;Zhixin Liu	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.65	actuarial science;finance;business	AI	3.65852140802964	-10.440233818915965	173518
6cc5ab3ff9ba2ed360676ae2abda89bfe15273e1	approaches to forecasting demands for library network services	text;methods;predictor variables;information services;reference services;interlibrary loans;network services;prediction;library networks	The problem of forecasting monthly demands for library network services is considered, especially in terms of using forecasts as inputs to policy analysis models and in terms of the use of forecasts as an aid to budgeting and staffing decisions. Forecasting methods considered include BoxJenkins time-series methodology, adaptive filtering, and linear regression. Using demand data from the Illinois Library and Information Network for 1971-1978, it is shown that fading-memory regression is the most appropriate method, in terms of both accuracy and ease of use.		Jong H. Kang;Willian Bill Rouse	1980	JASIS	10.1002/asi.4630310405	prediction;computer science;data science;data mining;database;operations research;world wide web;information system;statistics	HPC	5.41402584808978	-16.033013056094653	174369
d36b9b086fe509acdfaa02049484f880e5f2d842	wavelet evolutionary network for complex-constrained portfolio rebalancing	k means cluster analysis;bounding;wavelet shrinkage;hopfield neural network;performance metric;stock exchange;indexation;portfolio rebalancing;tokyo stock exchange;wavelet based filter;proportional transaction costs;proportional transaction cost and class constraints;cardinality;data envelope analysis;k means clustering;optimal portfolio;wavelet network;heuristic algorithm;covariance matrix	Portfolio rebalancing problem deals with resetting the proportion of different assets in a portfolio with respect to changing market conditions. The constraints included in the portfolio rebalancing problem are basic, cardinality, bounding, class and proportional transaction cost. In this study, a new heuristic algorithm named wavelet evolutionary network WEN is proposed for the solution of complex-constrained portfolio rebalancing problem. Initially, the empirical covariance matrix, one of the key inputs to the problem, is estimated using the wavelet shrinkage denoising technique to obtain better optimal portfolios. Secondly, the complex cardinality constraint is eliminated using  k -means cluster analysis. Finally, WEN strategy with logical procedures is employed to find the initial proportion of investment in portfolio of assets and also rebalance them after certain period. Experimental studies of WEN are undertaken on Bombay Stock Exchange, India BSE200 index, period: July 2001–July 2006 and Tokyo Stock Exchange, Japan Nikkei225 index, period: March 2002–March 2007 data sets. The result obtained using WEN is compared with the only existing counterpart named Hopfield evolutionary network HEN strategy and also verifies that WEN performs better than HEN. In addition, different performance metrics and data envelopment analysis are carried out to prove the robustness and efficiency of WEN over HEN strategy.	algorithm;bit error rate;book;cardinality (data modeling);cluster analysis;computation;computational finance;computational intelligence;computer science;decision theory;diversification (finance);heuristic;k-means clustering;machine learning;mathematical optimization;nl (complexity);noise reduction;pattern recognition;programmable sound generator;soft computing;wavelet	N. C. Suganya;G. A. Vijayalakshmi Pai	2012	Int. J. Systems Science	10.1080/00207721.2011.601351	heuristic;cardinality;covariance matrix;mathematical optimization;stock exchange;bounding overwatch;portfolio optimization;data envelopment analysis;mathematics;mathematical economics;statistics;k-means clustering	AI	7.013693171304833	-16.505380080323295	174657
ed8c2097c586f981b3d93fa10cb1e05bf4f750e3	regime switching: italian financial markets over a century	stock market;banking sector;regime switching;second and third industrial revolutions;markov switching model;new economy;financial market;volatility regimes;markov switching models;stock market volatility;financial crise;industrial revolution;international financial markets	The frequency of crashes and the magnitude of crises in international financial markets are growing more severe over time. Recent financial crises are not singular events portrayed in recent accounts, rather, they erupt in circumstances that are very similar to the economic and financial environments of the earlier eras. This paper analyzes the Italian stock market in two very peculiar periods (1901–1911 and 1993– 2004): the “Second” and the “Third industrial revolution”. We use Markov Switching Models to test whether the Italian stock market volatility has increased in the long run and whether it can be represented by different regimes. We find that volatility regimes exist; that Banking sector has a central role and “New economy” sectors perform quite well while traditional sectors do not, in both periods.	crash (computing);crisis (dynamical systems);disk sector;markov chain;new economy;volatility	Margherita Velucchi	2009	Statistical Methods and Applications	10.1007/s10260-007-0075-3	industrial revolution;volatility smile;market depth;financial system;financial market	Metrics	3.9026622120865717	-13.106144666501473	175302
e853d94d925fab36bbfc443a08c320edeb29eec5	apl is important at usaa (abstract)	manova;experimental design;latin square;sum of squares;anova;mean	United Services Automobile Association (USAA), Automobile Actuary Pricing Unit. This presentais a leading provider of insurance and financial tion provides an overview of the APL rate-making services. Automobile insurance represents the tools, how they interact, and why they are crucial highest percentage of USAA’S multi-billion dollar in establishing accurate, equitable, and competitive operations. Rate-making applications, written in rates. APL, constitute the core set of tools used by the APL Quote Quad 217 Aikens	apl	Rhonda K. Aikens	1995		10.1145/206913.207013	multivariate analysis of variance;econometrics;combinatorics;analysis of variance;latin square;mathematics;explained sum of squares;design of experiments;statistics;mean	ML	4.701895349694488	-10.692021671517294	176142
bc2bacf7a89a9e25d89498550eefc2f55870df7a	beyond the power law - a new approach to analyze city size distributions	europa;loi puissance;metodo analisis;analisis datos;ley poder;fonction repartition;ville;city size distribution;ciudad;taille;rumania;funcion distribucion;data analysis;modelo;distribution function;methode analyse;power laws;analysis method;pareto distribution;talla;town;romania;analyse donnee;modele;power law;europe;size;roumanie;models	This work proposes a new approach to analyze the city size distribution (CSD). We present a general equation for the rank size logarithmic plot, with a new positive exponent a. When a = 1, the Pareto distribution is yielded; when a 5 1, the log of the curves exhibits a concave distribution. We studied the CSDs of 41 cases in 35 countries (in several countries we examined cities and metropolitan areas or agglomerations) in order to apply our new equation. We determined accurately the exponent a for 31 cases. In 18 cases we received a = 1, in one case a < 1, and in 12 cases a > 1. However, for the other cases, either the distributions were not homogeneous, or the data exhibited significant fluctuations which precluded a good determination of the exponent a. Based on this analysis, we developed a series of models (based on the models of town growth of Gabaix and of Blank and Solomon) in order to describe the different CSDs. The results of these models include power laws as well as cases that are represented by concave distributions on a logarithmic plot of the rank size. 2007 Elsevier Ltd. All rights reserved.	cambridge structural database;chaos theory;computer simulation;concave function;pareto efficiency	Lucien Benguigui;Efrat Blumenfeld-Lieberthal	2007	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2006.11.002	power law;geography;calculus;mathematics;cartography;statistics	ML	2.9973511805928528	-12.996569661148584	176280
edeeec273adfa9a8536104d6c33a73eaaedb6343	estimation of daily bicycle traffic volumes using sparse data		Calculating the annual average daily bicycle (AADB) volume at a particular cycling facility requires the availability of year-round daily volume data. Automatic counters (e.g., loop detectors) used to collect such continuous data are subject to periodic malfunctions, leading to sporadic data gaps. This problem could affect the calculated values of the AADBs and impact the estimates of the daily and monthly adjustment factors at these count stations. The impacts become even more significant if the data gaps take place frequently and/or for long periods. This research tackles the problem of missing cycling traffic volumes at count locations that potentially experience frequent sensor malfunctions during the year. The method is also applicable to any other similar research problem (e.g. missing volumes at vehicle count locations). A data-driven, yet novel, model is proposed to estimate missing volumes at some locations, using data from other nearby count locations as well as the historical volumes of the same location. The model is motivated by the spatial–temporal relationship of cycling volumes of similar nearby facilities. The proposed model is dynamic as it assumes no prior knowledge about which locations may experience sensor malfunction (i.e., missing volumes). The model is referred to as the “autoencoder neural network” and it belongs to the family of Artificial Neural Networks (ANNs). This model expresses the relationship between a vector of input variables and itself. Hence, if a daily bicycle count is available on one day at a specific location, it will be used as a model input; whereas, a missing daily volume will be treated as an output variable that needs to be determined. The model was tested using a large dataset of about 13,000 daily bicycle volumes from the City of Vancouver, Canada. The data were collected between 2009 and 2011 at 22 different count locations. The model showed a strong estimation power with an average error of about 10%. Sensitivity analyses were carried out to investigate the impact of different model parameters on the estimation accuracy. The optimum set of model parameters was consequently defined.	sparse matrix	Mohamed El Esawey;Ahmed Ibrahem Mosa;Khaled Nasr	2015	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2015.09.002	simulation;geography;operations management;data mining;statistics	HPC	8.380182236214543	-16.058126013465525	176951
43d8399d88c830e51335a3dc9053a98e44e70072	dtg big data analysis for fuel consumption estimation			big data	Wonhee Cho;Eunmi Choi	2017	JIPS	10.3745/JIPS.04.0031	real-time computing;computer science;big data;fuel efficiency	ML	7.540958665454698	-11.524478979843156	179206
968381dfa7fa54e135d8c8b95e32743638a68c76	estimating path travel-time reliability	estimation theory;coefficient of variation;roadway segments path travel time reliability advanced traveler information system normal distribution trip travel time variance lognormal distribution visual inspection trip travel time coefficient;travel time;normal distribution;road traffic;lognormal distribution;advanced traveler information system;conditional expectation;path travel time reliability;traffic information systems estimation theory normal distribution road traffic;traffic information systems;mathematical models;visual inspection;goodness of fit test;trip travel time variance;roadway segments;trip travel time coefficient;vehicles distributed computing intelligent transportation systems time measurement stability fluctuations testing fluid flow measurement gaussian distribution surveillance	The estimation of path or trip travel-time reliability is critical to any advanced traveler information system. The state-of-practice procedures for estimating path travel-time reliability assumes that travel times follow a normal distribution and requires a measure of trip travel-time variance. The study analyzes AVI data from San Antonio and demonstrates through goodness-of-fit tests that the assumption of normality is, from a theoretical standpoint, inconsistent with field travel-time observations and that a lognormal distribution is more representative of roadway travel times. However, visual inspection of the data demonstrates that the normality assumption may be sufficient from a practical standpoint given its computational simplicity. The paper then proposes five methods for the estimation of path travel-time variance from its component segment travel-time variances. The analysis demonstrates that computing the trip travel-time coefficient of variation as the conditional expectation over all realizations of roadway segments provides estimates within 13% of field observations for both uncongested and congested conditions	coefficient;computation;information system;time deviation;visual inspection	Hesham A. Rakha;Ihab El-Shawarby;Mazen Arafeh;François Dion	2006	2006 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2006.1706748	simulation;engineering;operations management;statistics	Robotics	9.577843958767446	-11.62482414280109	179661
2fa1cc63e28e36b6346050899cea52f5b18c3963	project crashing using a fuzzy multi-objective model considering time, cost, quality and risk under fast tracking technique: a case study				Mohammad Reza Feylizadeh;Amin Mahmoudi;Morteza Bagherpour;Deng-Feng Li	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-18171	machine learning;fuzzy logic;mathematics;artificial intelligence	SE	7.285489354506632	-12.898825101183393	181102
a127a0c9435ccb8a8de6d20357a3cec049c1cd69	product design model for impact toughness estimation in steel plate manufacturing	loss measurement;graphical simulation tool;low alloy steel plates;steel plate manufacturing;testing;production engineering computing;steel product design temperature measurement artificial neural networks production data models mathematical model;mechanical factors;artificial neural networks;graphical simulation tool product design model impact toughness estimation steel plate manufacturing low alloy steel plates charpy v measurements lib transformation;plates structures;steel manufacture;lib transformation;charpy v measurements;mathematical model;steel;production;production planning;predictive models;temperature measurement;product design;temperature;product design model;simulation tool;impact toughness estimation;virtual manufacturing;data models;steel manufacture plates structures product design production engineering computing	The purpose of this study was to develop a product design model for impact toughness estimation of low-alloy steel plates. Based on these estimates, the rejection probability of steel plates can be approximated. The target variable was formulated from three Charpy-V measurements with a LIB transformation, because the mean of the measurements would have lost valuable information.The method is suitable for all steel grades in production and it is not restricted to a few test temperatures. There were differences between the performances of different product groups, but overall performance was promising. Next the developed model will be implemented into a graphical simulation tool that is in daily use in the product planning department and already contains some other mechanical property models. The model will guide designers in predicting the related risk of rejection and in producing desired properties in the product at lower cost.	approximation algorithm;graph toughness;graphical user interface;performance;rejection sampling;simulation	Satu Tamminen;Ilmari Juutilainen;Juha Röning	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633919	data modeling;temperature;temperature measurement;computer science;machine learning;mathematical model;predictive modelling;software testing;product design;artificial neural network	Robotics	6.51037296583318	-11.931315942205641	182177
45f0ea49f3f64302736435b521eb33f8961032c8	a dcc analysis of two stock market returns volatility with an oil price factor: an evidence study of singapore and thailand's stock markets		This paper uses the Singapore and the Thailand’s stock prices of material from January 4, 2000 to July 20, 2007, discussing the model construction and their associations of between Singapore and Thailand’s stock markets, and also uses Student's t distribution to analyze the proposed model. The empirical results show that the mutual affects of the Singapore and the Thailand’s stock markets may construct in bivariate IGARCH (1, 1) model with a DCC. The empirical result also shows that between Singapore and Thailand’s stock market returns exists the positive relationsnamely two stock market return’s volatility are synchronized influence, the average estimation value of the DCC coefficient of two stock market returns equals to 0.3876. Also, Singapore and Thailand's stock markets do not have the asymmetrical effect in the research data period. These evidences may suggest stock market investors or international fund managersbefore investing in Singapore must consider the Thailand stock price return’s volatility risk and its connection. Therefore, in the stock market, investors and managers may not neglect the influence of the foreign country’s stock market return volatility behavior; otherwise, his decision will not achieve the anticipated effect.	bivariate data;coefficient;jaccard index;return statement;volatility	Wann-Jyi Horng;Jih-Ming Chyan	2009	JCIT		cost price;stock exchange;stock market bubble;market capitalization;stock dilution;order;market depth;stock market index;non-qualified stock option;primary market;restricted stock;stock;market maker	ECom	3.352312559587248	-13.693311658252185	182304
f69d5bf1d45120354ce657a4bfda740aa2f96c71	probabilistic programming for nitrate pollution control: comparing different probabilistic constraint approximations	probabilistic constraints;pollution control;normal distribution;best approximation;natural variation;nitrate pollution;probability distribution;probabilistic programming;profitability;stochastic programming;chance constraints;chance constraint	Agricultural nitrate emissions within a river catchment are, due to rainfall and other sources of natural variation, uncertain. A regulator aiming to reduce nitrate emissions into surface and groundwater faces a trade-off between reliability in achieving emission standards and the cost of compliance to agriculture. This paper explores this trade-off by comparing different assumptions about the probability distribution of nitrate emissions and thus the probabilistic constraint included in the catchment model. Three categories of probabilistic constraints are considered: (1) nonparametric, (2) normal and (3) lognormal. The results indicate that the restrictiveness of the non-parametric assumption could lead to a significant reduction in profit relative to the normal and lognormal. The lognormal assumption, although it is theoretically correct, cannot be generalised to the case of correlated emissions. However, ignoring the dependence between different sources of nitrate emissions introduces more bias than mis-specifying their distribution. Therefore a probabilistic constraint based on a correlated normal distribution of emissions gives the best approximation for nitrate emissions in this study. 2002 Elsevier Science B.V. All rights reserved.	approximation;exponent bias;mathematical optimization;probabilistic turing machine;stochastic optimization;stochastic process;word lists by frequency	Athanasios Kampas;Ben White	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00254-0	normal distribution;stochastic programming;probability distribution;econometrics;mathematical optimization;mathematics;statistics;profitability index	AI	3.0273590323448683	-10.064011506771983	183596
b71e2e9c2a8ad80627ec7972aa6dbcbfd071a703	a robust model for optimal time-of-day speed control at highway work zones	modelizacion;highway work zones;communication system traffic control;time varying;velocity control;measures of effectiveness;perforation;road traffic;work zone operation posted speed limit robustness approach time of day speed control variable speed limit;intelligent transportation systems;robustness approach;real time;gestion trafic;traffic control;temps minimal;robust control;traffic flow;control velocidad;work zone operation;traffic management;indexing terms;traffic safety;flow models;optimal time of day speed limit control;time varying system;time of day;optimal control;modelisation;modele ecoulement;traffic control transportation;simulation experiment;velocity control optimal control road safety road traffic traffic control;systeme parametre variable;robustesse;temps reel;seguridad trafico;real time speed limit optimal time of day speed limit control highway work zones time varying traffic condition traffic safety;minimum time;gestion trafico;tiempo real;robustness;variable speed limit;time varying traffic condition;sistema parametro variable;securite trafic;road safety;communication system control;road transportation;real time speed limit;tiempo minimo;modeling;speed control;commande vitesse;robust control velocity control road transportation traffic control optimal control communication system traffic control throughput communication system control road safety intelligent transportation systems;posted speed limit;historical data;throughput;robustez;time of day speed control;volume data	This paper proposes a new speed control strategy, named time-of-day speed limit (TOD SL) control, for highway work-zone operations. The main purposes of the TOD SL control are to overcome the difficulty in setting the optimal real-time speed limit due to the lack of detectors and to maximize the use of available data such as the historical volume data on the target work zone. Its core logic is to divide the entire day of operations into a number of control periods and to accommodate the time-varying traffic conditions within each control period. The measure of effectiveness (MOE) selected in the TOD SL model takes into account both the operational efficiency and traffic safety. To encompass all possible traffic conditions during each control period, the control model employs traffic flow relations calibrated from historical data to estimate the speed and density data with available volume under possible traffic scenarios. The performance of the proposed TOD SL control has been evaluated with the simulation experiments and compared with the other speed control strategies based on the selected measures of effectiveness	control theory;experiment;moe;real-time transcription;sl (complexity);sensor;simulation	Kyeong-Pyo Kang;Gang-Len Chang	2006	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2006.869605	robust control;intelligent transportation system;throughput;active traffic management;simulation;systems modeling;index term;optimal control;computer science;engineering;traffic flow;control theory;electronic speed control;transport engineering;robustness	Robotics	9.405260259589557	-12.319652331392895	183620
1be06ef660bfc00d7a5fe5542eaaf231e05df260	large-scale spatial data processing on gpus and gpu-accelerated clusters	spatial data;gpgpu;large scale;data parallel design	The massive data parallel computing power provided by inexpensive commodity Graphics Processing Units(GPUs) makes large-scale spatial data processing on GPUs and GPU-accelerated clusters attractive from both a research and practical perspective. In this article, we report our works on data parallel designs of spatial indexing, spatial joins and several other spatial operations, including polygon rasterization, polygon decomposition and point interpolation. The data parallel designs are further scaled out to distributed computing nodes by integrating single-node GPU implementations with High-Performance Computing (HPC) toolset and the new generation in-memory Big Data systems such as Cloudera Impala. In addition to introducing GPGPU computing background and outlining data parallel designs for spatial operations, references to individual works are provided as a summary chart for interested readers to follow more details on designs, implementations and performance evaluations.	big data;data parallelism;data system;distributed computing;general-purpose computing on graphics processing units;graphics processing unit;in-memory database;interpolation;parallel computing;rasterisation	Le Gruenwald	2014	SIGSPATIAL Special	10.1145/2766196.2766201	parallel computing;computer science;theoretical computer science;operating system;database;spatial analysis;general-purpose computing on graphics processing units;statistics;computer graphics (images)	HPC	5.9611685803241405	-12.014088581676779	183811
0480a05f725450d6e87261db461bcc46042c5ced	prediction of the bridge monitoring data based on support vector machine	support vector machines bridges monitoring time series analysis predictive models strain kernel;bridge monitoring;short term prediction bridge monitoring data prediction support vector machine mass health monitoring data bridge structure time series phase space svm online security strain tilt monitoring data pian yan zi bridge arma;arma bridge monitoring prediction support vector machine;arma;support vector machine;time series autoregressive moving average processes bridges structures condition monitoring security structural engineering computing;prediction	According to the mass health monitoring data accumulated of bridge structure for a long time, this paper proposes a method for reconstitute the time series in phase space. Since the phase points are regressed by support vector machine (SVM), the relevant time series of past behavior patterns are established. Then, it could infer the future development trend and form the basis of the online security early warning of bridge structure. The strain and tilt monitoring data of Pian Yan-zi bridge in Chongqing are analyzed and compared with the prediction data of the auto regression moving average (ARMA). The results show that: (1) as to the bridge monitoring data, the prediction accuracy of SVM is better than that of ARMA; (2) With the increase of the number of the prediction steps, the prediction accuracy of ARMA drops dramatically. And ARMA is only applicable for short-term prediction while SVM is able to predict a longer period of time. (3) SVM prediction requires a smaller size of samples for modeling but with higher prediction efficiency.	binary prefix;in-phase and quadrature components;long short-term memory;support vector machine;time series	Hao Tang;Guangwu Tang;Libo Meng	2015	2015 11th International Conference on Natural Computation (ICNC)	10.1109/ICNC.2015.7378090	speech recognition;engineering;machine learning;data mining	ML	9.516284325696263	-15.662256969417799	188535
236f2eeb78606556cebc5ff3d7bfe5870e3d4657	risk management and business credit scoring	estimation theory;banking;financial management;risk management;business credit scoring;statistical analysis;inferential statistics;model;inferential statistical methods risk management company asset base banks credit risk insolvency probability estimation corporate default business credit scoring model decision tree analysis croatian financial system information value;decision trees;inferential statistics risk management business credit scoring model decision trees;statistical analysis banking decision trees estimation theory financial management organisational aspects risk management;organisational aspects	Risk management is focused on preventing the losses and protecting the company's asset base. Banks are most exposed to credit risk, so their goal is to minimize the losses that occur as a result of default or insolvency. Business credit scoring models are designed to estimate the probability of corporate default. In this paper we presented the business credit scoring model based on a decision tree analysis. The emphasis in the research is on the selection of the variables that are relevant for the business credit scoring model applicable to the Croatian financial system. In order to reduce the number of variables included in the analysis, we suggest the use of the Information Value and inferential statistical methods.	decision tree;inferential theory of learning;risk management	Ljiljanka Kvesic;Gordana Dukic	2012	Proceedings of the ITI 2012 34th International Conference on Information Technology Interfaces	10.2498/iti.2012.0478	credit reference;statistical inference;actuarial science;risk management;decision tree;credit history;estimation theory;credit valuation adjustment;statistics;financial risk management;credit enhancement	DB	4.617636665741717	-16.44197326971484	188728
0ed74953cea9c33bd0bf758695f87f7b3abfb873	my mobile music: an adaptive personalization system for digital audio players	service provider;michel wedel chung tuck siong;information technology;customization;one to one marketing;personalization;mobile music;service marketing;collaborative filtering;dissertation;simulation study;marketing music statistics information science my mobile music an adaptive personalization system for digital audio players university of maryland college park roland t rust;digital audio players;services marketing	Title of Document: MY MOBILE MUSIC: AN ADAPTIVE PERSONALIZATION SYSTEM FOR DIGITAL AUDIO PLAYERS Tuck Siong Chung, Ph.D., 2007 Directed By: Professor Roland T. Rust, Department of Marketing Professor Michel Wedel, Department of Marketing This paper develops a music recommendation system that automates the downloading of songs into a mobile digital audio device. The system tailors the compositions of the songs to the preferences of individuals based on past behaviors. We describe and predict individual listening behaviors using a lognormal hazard function. Our recommendation system is the first to accomplish this and there is as of this moment no existing alternative. Our proposed approach provides an improvement over alternative methods that could be used for product recommendations. Our system has a number of distinct features. First, we use a Sequential Monte Carlo algorithm that enables the system to deal with massive historical datasets containing listening behavior of individuals. Second, we apply a variable selection procedure that helps to reduce the dimensionality of the problem, because in many applications the collection of songs needs to be described by a very large number of explanatory variables. Third, our system recommends a batch of products rather than a single product, taking into account the predicted utility and the uncertainty in the parameter estimates, and applying experimental design methods. MY MOBILE MUSIC: AN ADAPTIVE PERSONALIZATION SYSTEM FOR DIGITAL AUDIO PLAYERS	design of experiments;download;failure rate;feature selection;michel hénon;monte carlo algorithm;personalization;recommender system;rust	Tuck Siong Chung	2007	Marketing Science	10.1287/mksc.1080.0371	economics;marketing;personalization;advertising;information technology	ML	4.712844761734475	-12.166214533097842	188947
726d26114e46f3b07a8927bd61e70bf5562f9234	assessing the potential impacts of urban expansion on regional carbon storage by linking the lusd-urban and invest models	impact;期刊论文;regional carbon storage;lusd urban model;invest model;urban expansion	The timely and effective assessment of the impacts of urban expansion on regional carbon storage is an important issue in the fields of urban ecology and sustainability science. This study used a new model to assess the impacts of urban expansion on regional carbon storage by linking the LUSD-urban and InVEST models. First, the LUSD-urban model was used to simulate urban expansion. Then, the InVEST model was adopted to assess the impacts on regional carbon storage. The linked model combines the strengths of these two models. Not only can it simulate and project the process of urban expansion but it can also assess the impacts of urban expansion on regional carbon storage. A case study in Beijing showed that the relative error between the simulated carbon storage loss and the actual loss was less than 12%. We argue that the linked model can be applied to assess the ecological effects of future urban expansion. We developed a model to assess the impacts of urban expansion on carbon storage.The linked model combines the strengths of the LUSD-urban and the InVEST model.The potential impacts of future urban expansion on carbon storage can be estimated.A case study in Beijing confirms that the model is relatively efficient and accurate.		Chunyang He;Da Zhang;Qingxu Huang;Yuanyuan Zhao	2016	Environmental Modelling and Software	10.1016/j.envsoft.2015.09.015	environmental engineering;environmental resource management;impact	NLP	6.908582750743158	-11.198261636395358	190000
23ab5bc34b98352a82728c1cae9e702a2c4fc0ae	uspd doubling or declining in next decade estimated by wasd neuronet using data as of october 2013		Recently, the total public debt outstanding (TPDO) of the United States has increased rapidly, and to more than ($17) trillion on October 18, 2013. It is important and necessary to conduct the TPDO projection for better policies making and more effective measurements taken. In this paper, we present the ten-year projection for the public debt of the United States (termed also the US public debt, USPD) via a 3-layer feed-forward neuronet. Specifically, using the calendar year data on the USPD from the Department of the Treasury, the neuronet is trained, and then is applied to projection. Via a series of numerical tests, we find that there are several possibilities of the change of the USPD in the future, which are classified into two categories in terms of projection trend: the continuous-increase trend and the increase-peak-decline trend. In the most possible situation, the neuronet indicates that the TPDO of the United States is projected to increase, and it will double in 2019 and double again in 2024.	intranet;period-doubling bifurcation	Yunong Zhang;Zhengli Xiao;Dongsheng Guo;Mingzhi Mao;Hongzhou Tan	2015		10.1007/978-981-10-0356-1_75	finance;economics;debt;treasury	AI	3.680451741347224	-15.161020044769883	192017
be3fce67f8ed82d21e05e739bbdd46861c92c936	a neural network trained to select aircraft maneuvers during air combat: a comparison of network and	air combat maneuvering;expert systems;rule based systems;neural nets;expert fighter pilots;manoeuvre;aerospace computing;neural nets aerospace computing expert systems military computing;airspace conditions;transfer task air combat maneuvering airspace conditions expert fighter pilots maneuver selections manoeuvre neural network rule based systems;maneuver selections;transfer task;military computing;neural network	Research to develop a neural network model that selects aircraft maneuvers in the domain of air-combat maneuvering is described. A methodology for converting rule-based systems into a neural network was established. A comparison between the neural network and a rule-based expert system was undertaken. Differences between the architectures were explored, and hypotheses as to causes of differential performance were made. Both models were compared with expert fighter pilots on a transfer task. The neural network agreed with maneuver selections made by expert fighter pilots 2.5 times more often than the rule-based system. These findings were explained in terms of the ability of neural nets to generalize maneuver selections to novel airspace conditions. Implications of these results were also discussed		D. C. McMahon	1990		10.1109/IJCNN.1990.137554	simulation;computer science;artificial intelligence;machine learning;operations research;artificial neural network	ML	7.227103168536205	-13.348068473265633	193029
6e5526e8349ef81a84bf89f843bd9ad08cfb2364	energy consumption evaluation based on a personalized driver–vehicle model	vehicles energy consumption hidden markov models acceleration mathematical model data models adaptation models;probability weighted arx driving behavior reproduction energy consumption evaluation hybrid systems;acceleration;hidden markov models;energy consumption;mathematical model;vehicles;adaptation models;data models	A new approach to evaluate personalized energy consumption is presented in this paper. The method consists of identifying driver–vehicle dynamics using the probability weighted autoregressive model, which is one of the multi-mode ARX models, and then of reproducing the driver–vehicle behavior in a vehicle-following task. The energy consumption of the vehicle is estimated from the velocity profile calculated by using the driver–vehicle model. In this paper, driving simulator and real-world driving data were recorded to identify the driver–vehicle model in various situations. As a result, real-world energy consumption could be reproduced in a variety of situations with an average error of 1.9% and a standard deviation within 1.5%. Several promising applications of the energy consumption evaluation are introduced in this paper, such as an online energy consumption prediction, a powertrain choice-assistance system for car buyers, and a solution to estimate the macroscopic energy consumption of aggregated vehicles in a traffic flow.	algorithmic efficiency;arx;autoregressive model;behavior model;decimation (signal processing);driving simulator;dynamical system;hybrid system;personalization;simulation;time delay and integration;velocity (software development)	Thomas Wilhelem;Hiroyuki Okuda;Blaine Levedahl;Tatsuya Suzuki	2017	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2608381	acceleration;control engineering;data modeling;econometrics;simulation;computer science;engineering;mathematical model;hidden markov model;statistics	Robotics	9.03473378960687	-14.532858271143695	193538
98558a02279e9155331709a9c774bff7c938334f	impact of grading of ipos in short run price performance in india: a regression model approach	stock returns ipo grading initial public offering short run price performance india capital markets sebi credit rating agencies market efficiency multiple linear dummy variable regression analysis market fluctuation;fluctuations;companies;investment;stock markets;stock markets investment pricing regression analysis;sensitivity;indexes;regression analysis initial public offer short run price performance ipo grading;companies fluctuations indexes sensitivity investment stock markets	Capital markets all over the world are subject to information asymmetry where the potential investors have inferior knowledge about the company. As a step to make markets efficient SEBI introduced a new mechanism of grading of IPOs in 2006. Grades assigned by different credit rating agencies acts as signal of quality of the company. The objective of this study is to analyze the impact of grading of IPOs in short run price performance. Price performance is one indicator of market efficiency. Using sample of 121 IPOs listed on NSE from 2006 to 2013, IPO returns for 6 months post offer day is calculated. Control variables Beta and 6 months market return are also introduced. Statistical tool multiple linear dummy variable regression analysis is used to understand the dependence of returns from IPO to the grades assigned taking market fluctuation and sensitivity of stock returns to market fluctuations as control variables.	control variable (programming);dummy variable (statistics);network search engine;quantum fluctuation	S. Neeraja;P. Balasubramanian	2015	2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2015.7275719	sensitivity;investment;computer science;market depth;control theory	AI	3.150092744589068	-12.236752507883242	193974
0eb45123aef31f9b196f47e3dc7d62e21eac0310	assessing the sensitivity of water networks to noisy mass loads using monte carlo simulation	network design;reliability;water reuse;operant conditioning;process integration;water network;water consumption;water saving;mass transfer water using processes;network configuration;tp chemical technology;water use;network reliability;mass load fluctuation;product quality;monte carlo simulation;mass transfer	Assessing the sensitivity of water networks to noisy mass loads using Monte Carlo simulation Raymond R. Tan a,∗, Dominic C.Y. Foo b,1, Zainuddin A. Manan c a Chemical Engineering Department, De La Salle University-Manila, 2401 Taft Avenue, 1004 Manila, Philippines b Chemical Engineering Pilot Plant, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia c Department of Chemical Engineering, Universiti Teknologi Malaysia, 81310 Skudai, Johor, Malaysia	foobar;linear algebra;monte carlo method;os-tan;simulation	Raymond R. Tan;Dominic Chwan Yee Foo;Zainuddin A. Manan	2007	Computers & Chemical Engineering	10.1016/j.compchemeng.2006.11.005	water use;network planning and design;simulation;environmental engineering;computer science;engineering;operant conditioning;reliability;mathematics;reliability;process integration;statistics;monte carlo method;mass transfer	AI	6.675728740694894	-10.044082339386511	194021
8184d866cd7c6012e9b6d72dbab126a5e299bbfa	traffic jam modeling and simulation	jamming uninterruptible power systems traffic control asymptotic stability stability analysis lead;simulation;traffic modelling simulation;traffic;traffic density traffic jam modeling traffic jam simulation stop and go wave modeling microscopic car following model asymmetric traffic theory first order macroscopic models second order macroscopic models traffic disturbance model phase transition model traffic speed	In this paper various theories on traffic jams and stop-and-go wave modeling were reviewed and discussed. These include traditional microscopic car-following model, asymmetric traffic theory, first-order and second-order macroscopic models, traffic disturbance model, phase transition model as well as macroscopic simulation. The mechanism of stop-and-go waves, causes, generation, propagation, and absorption was discussed. A macroscopic simulation model was developed which suits for both free flow and stop-and-go traffic conditions. Ten hours of macroscopic simulation was performed on a section of freeway in California. The model predicted traffic speed and density were compared with field measured data. It was concluded that by applying proper boundary conditions for each roadway segment and incorporating capacity drop in the model, the macroscopic simulation model can reasonably predict the stop-and-go traffic states.	first-order predicate;freeway;jam;open road tolling;simulation;software propagation	Derek Yin;Tony Z. Qiu	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338916	traffic generation model;simulation;intelligent driver model;microscopic traffic flow model;telecommunications;engineering;traffic congestion reconstruction with kerner's three-phase theory;traffic flow;three-phase traffic theory;newell's car-following model;transport engineering;traffic wave;network traffic simulation	Robotics	9.716346196422027	-10.601897675750775	194351
1a79a33d1731bfd5f597b55a2e5c090418dfdf43	the traffic phases of road networks	networks;traffic flow	We study the relation between the average traffic flow and the vehicle density on road networks that we call 2D-traffic fundamental diagram. We show that this diagram presents mainly four phases. We analyze different cases. First, the case of a junction managed with a priority rule is presented, four traffic phases are identified and described, and a good analytic approximation of the fundamental diagram is obtained by computing a generalized eigenvalue of the dynamics of the system. Then, the model is extended to the case of two junctions, and finally to a regular city. The system still presents mainly four phases. The role of a critical circuit of non-priority roads appears clearly in the two junctions case. In Section 4, we use traffic light controls to improve the traffic diagram. We present the improvements obtained by open-loop, local feedback, and global feedback strategies. A comparison based on the response times to reach the stationary regime is also given. Finally, we show the importance of the design of the junction. It appears that if the junction is enough large, the traffic is almost not slowed down by the junction.	approximation;capacity optimization;diagram;jam;mathematical optimization;simulation;stationary process	Nadir Farhi;Maurice Goursat;Jean-Pierre Quadrat	2009	CoRR		environmental health;engineering;suicide prevention;human factors and ergonomics;injury prevention;traffic flow;three-phase traffic theory;transport engineering;forensic engineering;computer security;mechanical engineering	Metrics	9.923516820435228	-10.32590357280551	194477
4ff3083554e4a28f38dc3cae8bf12ff600e1cf73	mathematical models of the adjustment of the internal structure for the chinese first industry and responding measure	regression analysis agriculture grey systems linear programming;mathematical model agriculture power generation economics educational institutions economic indicators agricultural products linear programming industrial economics extraterrestrial measurements linear regression;linear regression;qualitative analysis;internal structure;agriculture mathematical model internal structure chinese first industry responding measure constraint conditions grey system linear regression qualitative analysis linear programming model;mathematical model the first industry structure adjustment;linear programming;mathematical model;grey systems;linear program;agriculture;regression analysis;structural adjustment	The constraint conditions of some linear programming are presented with grey system, linear regression and qualitative analysis. Therefore, the linear programming model to adjust the internal structure of the first industry in China is developed. And its responding adjusting plans on the internal structure of the first industry in the period of ldquoeleven-fiverdquo is advised. Lastly, the keys and the strategies of adjusting the internal structure of agriculture are analyzed.	mathematical model	Yaoguo Dang;Zhengxin Wang;Chuanmin Mi	2008		10.1109/ICSMC.2008.4811523	econometrics;agriculture;proper linear model;computer science;linear programming;linear regression;linear model;mathematical model;mathematical economics;regression analysis;statistics	Vision	4.155151806163655	-10.909202958829649	194728
53b656618e08abbff36bab7feffd8db6f6fecf78	investigation of florida housing prices using predictive time series model		In the ever-changing1 real estate market, there are certain factors that have a huge impact on the fluctuating of house prices, and there are relationships between those factors. The only way to understand the behavior of the changes would be to explore historical data of those factors that impact the housing prices most. Many research papers have studied to predict the trends of house prices, in various countries, using various statistical models, and machine learning methods. Florida housing prices not only depend on various economics indexes but also impact by other natural disasters factors such as hurricane etc. which will be considered in future research. This project will involve the study of the housing market in Florida; we will be considering various factors such as the number of sales, employment rate, interest rate, GDP, inflation rate, hurricane strikes etc. Dataset from 1985 to 2016 will use in our analysis and case study and the time series is adopted. The prediction of this model is promising.	machine learning;statistical model;time series	Temilola Aderibigbe;Hongmei Chi	2018		10.1145/3219104.3229253	finance;statistical model;anomaly detection;time series;real estate;interest rate;economics;inflation	AI	4.752460128862544	-15.16364288702503	195369
55149bf6a858a1de07663620e30a5bb5919e4f0c	a gaussian process regression method for urban road travel time prediction		Urban road travel time prediction has become a crucial component in Intelligent Transportation Systems (ITS). In order to improve the overall operational efficiency of the urban expressway network, this paper proposes a Gaussian Process Regression (GPR) based prediction method to estimate the short period travel time on urban road. Compared with the approaches based on neural network and support vector machine regression model, GPR is easier to be implemented. Moreover, the proposed method is able to adapt to the urban road environment and has the advantages of hyper-parameter self-adaptive acquisition. Based on people's regular travel habits, this method considers the number of vehicles passing through the road section, the average vehicle speed, and the travel time jointly to predict the travel time of the next time period. We then predict the traveling time on the workdays and the weekends with the proposed method respectively. Results show that the prediction made by the proposed method is consistent with the actual result with 2.8% mean absolute percentage error and 97.2% prediction accuracy.	approximation error;artificial neural network;autoregressive integrated moving average;gaussian process;kriging;real-time transcription;support vector machine	Yage Wu;Zhu Xiao	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393394	regression analysis;support vector machine;artificial neural network;machine learning;operational efficiency;artificial intelligence;mean absolute percentage error;computer science;intelligent transportation system;gaussian process;kriging	Robotics	8.71228284515258	-14.817252446546203	195461
aeabc67a9052a0f9ff0f521dd1217d04bd797721	chinese regional energy efficiency change and its determinants analysis: malmquist index and tobit model	total factor energy efficiency;tobit model;malmquist index;determinants	China becomes the largest energy consumer in 2010 but its energy productivity is well below the world average. To meet China’s fast growing energy using, energy efficiency should be especially emphasized under China’s energy policy. This paper focuses on the regional level of energy efficiency change in China. And we analyze total factor energy efficiency for 30 Chinese provinces over the period 1998–2009 using Malmquist index method and Tobit analysis. The Malmquist estimation results suggest there is a dropping change trend of energy productivity growth. Chinese energy efficiency still faces with huge regional disparity, but the energy technical efficiency reflects convergence in the nationwide and west region. As a result of Tobit regression, we find that industrial structure, energy consumption structure and institutional factor have different influences on energy efficiency.	binocular disparity;international ergonomics association;regional lockout;tobit model	Wendong Lv;Xiaoxin Hong;Kuangnan Fang	2015	Annals OR	10.1007/s10479-012-1094-5	econometrics;determinant;economics;operations management;tobit model;mathematics;economy	AI	2.883289575336306	-14.390598268566132	196697
3723121af1e3f3257004511fa29de17afa750fcf	real time regression analysis in internet of stock market cycles		Abstract This paper is bases on the data of fund holding stocks held by equity funds from 2013 to 2017 in China’s stock market, empirically analyzing whether the investment behaviors of mutual funds follow value investment that improve fund performance in both bull and bear markets. The research indicates that there exists a significant difference in the correlation between industry concentration and fund performance in the bear and bull markets. Specifically, industry concentration and fund performance is negatively correlated in the bull market and is positively correlated in the bear market. The empirical results indicate that stock market in China demonstrates the phenomenon that “the bear market follows value investment while the bull market ignores value investment” to a certain extent. It is illustrated that fund investment behaviors in the bear market fundamentally meet the standards of value investment and illustrate that mutual funds have a certain ability to extract valuable information. However, fund managers in China tend to concentrate their holdings in a few industries in the bull market with the aim to gain high returns. The excessively concentrated investment strategies not only fail to bring them excess returns, but also reduce their performance. Therefore, fund managers should maintain conservative and prudent investment strategies when constructing portfolios and should not over-concentrate on specific industries.		Li Xiaolin	2018	Cognitive Systems Research	10.1016/j.cogsys.2018.07.012	finance;regression analysis;machine learning;psychology;stock market;equity (finance);stock (geology);china;artificial intelligence;investment strategy;stock market cycles;phenomenon	ECom	3.1646594914231505	-13.621605767462578	197318
9f95fc8c3c7647c2f603dbe3fd802250ab0ae7b5	using neural networks for forecasting of commodity time series trends		Time series of commodity prices are investigated on two scales - across commodities for a portfolio of items available from the database@ of the International Monetary Fund on monthly averages scale, as well as high quality trade event tick data for crude oil futures contract from the market in Japan. The degree of causality is analyzed for both types of data using feed-forward neural network architecture. It is found that within the portfolio of commodities the predictability highly varies from stochastic behavior consistent with the efficient market hy- pothesis up to the predictability rates of ninety percent. For the crude oil in Japan, we analyze one month (January 2000) series of a mid-year delivery contract with 25,210 events, using several schemes for causality extraction. Both the event-driven sequence grid and second-wide implied time grid are used as the input data for the neural network. Using half of the data for network training, and the rest for validation, it is found in general that the degree of trend extraction for the single next event is in the sixty percent range, which can increase up to the ninety percent range when the symbolization technique is introduced to denoise the un- derlying data of normalized log returns. Auxiliary analysis is performed that incorporates the extra input information of trading volumes. The time distribution of trading event arrivals is found to exhibit interesting features consistent with several modes of trading strategies.	artificial neural network;time series	Akira Sato;Lukas Pichl;Taisei Kaizoji	2013		10.1007/978-3-642-37134-9_8	data mining;statistics	ML	6.346486667296578	-16.565390451973926	198401
a2eca3de3c99754fae10348f9761bb81ef14f1c6	modeling regime switching in day-ahead market prices using markov model	forecasting;mathematical model;predictive models;markov processes;correlation;switches;data models	The accurate price forecasting of electricity market is crucial for profit maximizing producers and consumers in liberalized power markets. In all market places (day-ahead, intra-day and real-time) accurate price prediction is needed to generate optimal bids and maximize the profit. This paper first presents three methods for forecasting day-ahead market prices, namely Generalized Autoregressive Conditional Heterosedastic (GARCH), Holt-Winter (HW) and Mean Reversion and Jump Diffusion (MRJD). These methods are based on three broad methodologies of time series analysis, exponential-smoothing and stochastic processes. The dynamics of hourly prices in day-ahead market are varying from day to day. Each forecasting tool is suitable to capture one type of price dynamics. To capture this phenomenon, we combine GARCH, HW and MRJD methods using proposed Markov switch. The proposed Markov model is tested using Nordic day-ahead prices.	ai winter;autoregressive model;capture one;markov chain;markov model;performance;real-time clock;reversion (software development);smoothing;stochastic process;time complexity;time series	Yelena Vardanyan;Mohammad Reza Hesamzadeh	2016	2016 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)	10.1109/ISGTEurope.2016.7856316	financial economics;econometrics;economics;microeconomics	Metrics	5.337411480555417	-14.660251240756406	198513
a2cab5c76f200d3e4f5064727b459660cec0b92d	analysis of lead times of metallic components in the aerospace industry through a supported vector machine model	computer aided analysis;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;learning;supervised learning;industrie;modelo matematico;aerospace industry;classification;lead time;aprendizaje;supported vector machines;apprentissage;industry;survival analysis;mathematical model;analisis asistido;support vector machine;applied mathematics;clasificacion;supply chain management	The aim of the present paper is the analysis of the factors that have influence over the lead time of batches of metallic components of aerospace engines. The approach used in this article employs support vector machines (SVMs). They are a set of related supervised learning methods used for classification and regression. In this research a model that estimates whether a batch is going to be finished on the forecasted time or not was developed using some sample batches. The validity of this model was checked using a different sample of similar components. This model allows predicting the manufacturing time before the start of the manufacturing. Therefore a buffer time can be taken into account in order to avoid delays with respect to the customer's delivery. Further, some other researches have been performed over the data in order to determine which factors have more influence in manufacturing delays. Finally, conclusions of this study are exposed.		Francisco Javier de Cos Juez;P. J. García Nieto;J. Martínez Torres;Javier Taboada Castro	2010	Mathematical and Computer Modelling	10.1016/j.mcm.2010.03.017	support vector machine;supply chain management;simulation;biological classification;mathematical model;mathematics;survival analysis;aerospace;supervised learning;operations research;algorithm;statistics	Theory	6.510442591411626	-15.788945746038067	198768
2c503dae0a2a9de2e789f5605fa82af96ab68780	risk minimization with self-organizing maps for mutual fund investment	kohonen self organizing map;financial crisis;mutual fund investment;cybernetics;investments;mathematics;instruments;probability;self organizing kohonen map;risk management;portfolios;risk sensitive portfolio optimization;companies;investment;portfolio optimization;genetic algorithm risk minimization self organizing kohonen map mutual fund investment profitability risk sensitive portfolio optimization probability;mutual funds;self organising feature maps;self organizing feature maps;genetic algorithm;robustness;genetic algorithms;self organized map;profitability;risk minimization;security;self organising feature maps genetic algorithms investment probability profitability risk management;high efficiency;mutual fund;historical data;optimization methods;reactive power;risk management self organizing feature maps mutual funds investments portfolios mathematics cybernetics robustness instruments optimization methods	The problem of optimal mutual fund investment taking into account possible risks is considered. In this paper we consider lost profit in the growing market and a loss in a falling market as a possible risk. Our studies show that the efficiency of mutual funds can be estimated by nine main parameters obtained by historical data. Evaluation and ranking criteria sets for mutual funds are defined by the help of Kohonen Self-Organizing Maps. We propose to use a simplified ranking consisting of five categories. The methodology of constructing optimal strategies for risk-sensitive portfolio optimization is proposed. The performance of constructed portfolio is superior to the most mutual funds and other portfolios. The proposed methodology underwent a test for last four years and showed high efficiency and robustness both in growing and falling (during current world financial crisis) markets.	category theory;mathematical optimization;organizing (structure);self-organization;self-organizing map	Andrei A. Lukyanitsa;Sergei V. Nosov;Alexei G. Shishkin	2009	2009 IEEE Congress on Evolutionary Computation	10.1109/CEC.2009.4983235	genetic algorithm;investment strategy;actuarial science;cybernetics;risk management;investment;computer science;artificial intelligence;mutual fund separation theorem	Vision	3.855022061429018	-10.357174062788706	199183
21070031c4d6cd34fb054926e57e9cbcb127c65d	simulation study on improvement of air quality by introducing electric vehicles	traffic simulation;pm2 5;electric vehicle;air quality	Recently, Chinese megacities have suffered serious air pollution. Previous studies have pointed out that transportation systems have become one of the major sources of air pollution and on-road pollutant concentrations are significantly higher than off-road. Electric vehicle (EV) introduction is proposed as a method to alleviate the current situation. In order to better understand the benefit of the use of EVs in Beijing, a simulation platform has been developed to evaluate the improvement of air quality with the use of EVs quantitatively within the selected area. Four scenarios with different EV penetration rates are proposed and the results revealed 5%, 10%, 15% EV penetration rates which will bring about improvement of 0.86%, 9.01% and 12.23% for PM2.5, 0.92%, 9.01% and 13.32% for nitrogen oxides (NOx), 0.95%, 8.86% and 13.73% for CO, respectively. The results revealed a promising improvement of air quality with the introduction of EVs.	simulation	Yongxiang Du;Jianping Wu;Kezhen Hu;Yue Guo	2015	IJMSSC	10.1142/S1793962315500427	air quality index;simulation;environmental engineering;engineering;transport engineering	HCI	8.220862799898883	-10.107362597836689	199206
49150d02cfd7071292e532e39ce7db2632fc4080	volatility surface calibration in illiquid market environment		In this paper, we show the fragility of widely-used Stochastic Volatility Inspired (SVI) methodology. Especially, we highlight the sensitivity of SVI to the fitting penalty function. We compare different weight functions and propose to use a novel methodology, the implied vega weights. Moreover, we unveil the relationship between vega weights and the minimization task of observed and fitted price differences. Besides, we show that implied vega weights can stabilize SVI surfaces in illiquid market conditions. INTRODUCTION Vanilla options are traded with finite number of strikes and maturities. Thus, we can observe only some points of the implied volatility surface. It is known that vanilla prices are arbitrage free hence exotic option traders would like to calibrate their prices to vanillas (Dupire 1994). The main difficulty is that calibration methods need the implied volatility surface. To overcome this problem we have to construct an arbitrage free surface from the observed points (Schönbucher 1998, Gatheral 2013). In this paper we provide a robust arbitrage free surface fitting methodology. Chapters are structured as follows: Section 2. is a brief overview of SVI. In Section 3. we compare the different weight functions and present our implied vega weight L methodology. In Section 4. we summarize the findings.	traders;volatility;weight function	Laszlo Nagy;Mihaly Ormos	2017		10.7148/2017-0148	market environment;calibration;volatility smile;economics;financial economics;volatility swap	AI	3.0732931904901157	-11.037717791037597	199284
12663c75fbb3668c4850f7235a2bc1ea94c883ba	methods based on fuzzy sets to solve problems of safe ship control	ship control;fuzzy set;dynamic program;fuzzy set theory;branch and bound method;genetic algorithm;collision avoidance;process model;subjective assessment	  In this article author describes three methods based on fuzzy set theory to determinate safe ship trajectories in the collision  situation in fuzzy environment: branch and bound method, dynamic programming method and method based on genetic algorithms.  Optimal safe ship trajectory in collision situation is presented as multistage decision-making in a fuzzy environment. The  Collision Avoidance Regulations, the maneuverability parameters of the ship and the navigator’s subjective assessment in making  a decision are taken under consideration in the process model.    		Mostefa Mohamed-Seghir	2008		10.1007/978-90-481-3662-9_64	mathematical optimization;defuzzification;fuzzy transportation;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy set;fuzzy set operations;fuzzy control system	Robotics	7.3181567645435734	-12.931341686217431	199340
9dd364e09eb2087384b31b6d75b8080d62fa2666	a temporal approach to the parisian risk model			financial risk modeling	Bin Li;Gordon E. Willmot;Jeff T. Y. Wong	2018	J. Applied Probability	10.1017/jpr.2018.18	combinatorics;mathematics	Vision	4.2714801628455765	-13.500724849587199	199469
