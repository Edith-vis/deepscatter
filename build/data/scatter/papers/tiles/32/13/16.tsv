id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
79ef22161dd475b7e24c70ce1ed9e331014941d9	on the semantics of trust and caching in the semantic web	information sources;stable models;knowledge representation and reasoning;well founded semantics;distributed environment;web ontology language;semantic web;knowledge base	The Semantic Web is a distributed environment for knowledge representation and reasoning. The distributed nature brings with it failing data sources and inconsistencies between autonomous knowledge bases. To reduce problems resulting from unavailable sources and to improve performance, caching can be used. Caches, however, raise new problems of imprecise or outdated information. We propose to distinguish between certain and cached information when reasoning on the semantic web, by extending the well known $\mathcal{FOUR}$ bilattice of truth and knowledge orders to $\mathcal{FOUR-C}$, taking into account cached information. We discuss how users can be offered additional information about the  reliability  of inferred information, based on the availability of the corresponding information sources. We then extend the framework towards $\mathcal{FOUR-T}$, allowing for multiple  levels of trust  on data sources. In this extended setting, knowledge about trust in information sources can be used to compute, how well an inferred statement can be trusted and to resolve inconsistencies arising from connecting multiple data sources. We redefine the stable model and well founded semantics on the basis of $\mathcal{FOUR-T}$, and reformalize the Web Ontology Language OWL2 based on logical bilattices, to augment OWL knowledge bases with trust based reasoning.	cache (computing);semantic web	Simon Schenk	2008		10.1007/978-3-540-88564-1_34	knowledge representation and reasoning;knowledge base;computer science;knowledge management;artificial intelligence;semantic web;data mining;database;web ontology language;world wide web;distributed computing environment	AI	-23.930943339641242	7.1266225682880435	165386
f4ec93653db570983b10e8fab028b6c1fba2d40f	temporal granularity and indeterminacy in reasoning about actions and change: an approach based on the event calculus	temporal granularity;temporal indeterminacy;event calculus;reasoning about action;temporal information;real world application;situation calculus;temporal reasoning	In many real-world applications, temporal information is often imprecise about the temporal location of events (indeterminacy) and comes at different granularities. Formalisms for reasoning about events and change, such as the Event Calculus (EC) and the Situation Calculus, do not usually provide mechanisms for handling such information, and very little research has been devoted to the goal of extending them with these capabilities. In this paper, we propose TGIC (Temporal Granularity and Indeterminacy event Calculus), an approach based on the EC ontology to represent events with imprecise location and to deal with them on different timelines.	algorithm;communication endpoint;divergence (computer science);event calculus;indeterminacy in concurrent computation;international olympiad in informatics;maximal set;newman's lemma;nil;polynomial;precondition;pseudocode;rewriting;situation calculus;table (database);time complexity;timeline	Luca Chittaro;Carlo Combi	2002	Annals of Mathematics and Artificial Intelligence	10.1023/A:1015851820698	computer science;artificial intelligence;data mining;mathematics;event calculus;situation calculus;algorithm	AI	-19.28023559736931	8.143584796403461	165749
3019a2eac93b4a746fe2bc532d8671a51f5e8efa	encoding classifications into lightweight ontologies	graph theory;distributed system;lenguaje natural;langage propositionnel;ontologie;teoria grafo;systeme reparti;web pages;red www;web semantique;langage naturel;reseau web;forma normal;lenguaje proposicional;classification;propositional language;theorie graphe;qa076 computer software;sistema repartido;internet;web semantica;natural language;semantic web;normal form;world wide web;ontologia;forme normale;ontology;clasificacion	Classifications have been used for centuries with the goal of cataloguing and searching large sets of objects. In the early days it was mainly books; lately it has also become Web pages, pictures and any kind of digital resources. Classifications describe their contents using natural language labels, an approach which has proved very effective in manual classification. However natural language labels show their limitations when one tries to automate the process, as they make it very hard to reason about classifications and their contents. In this paper we introduce the novel notion of Formal Classification, as a graph structure where labels are written in a propositional concept language. Formal Classifications turn out to be some form of lightweight ontologies. This, in turn, allows us to reason about them, to associate to each node a normal form formula which univocally describes its contents, and to reduce document classification and query answering to reasoning about	a-normal form;book;document classification;image;lightweight ontology;natural language;ontology (information science);web page	Fausto Giunchiglia;Maurizio Marchese;Ilya Zaihrayeu	2007	J. Data Semantics	10.1007/978-3-540-70664-9_3	natural language processing;the internet;epistemology;biological classification;computer science;artificial intelligence;graph theory;semantic web;ontology;web page;data mining;database;linguistics;natural language;programming language;world wide web;algorithm	AI	-19.726815267511576	10.58733767011685	165922
d63a963af509447ad404fc813d100fc342da6c13	modal aspects of object types and part-whole relations and the de re/de dicto distinction	cognitive psychology;conceptual model;formal ontology	In a series of publications, we have proposed a foundational system of ontological categories which has been used to evaluate and improve the quality of conceptual modeling languages and models. In this article, we continue this work by employing theories from Formal Ontology, Cognitive Psychology and Philosophical Logic to systematically investigate some important modal aspects of the ontological categories represented in structural conceptual models. In particular, we focus on Object Types and Part-Whole Relations, formally characterizing some modal properties that motivate the proposal of a number of distinctions within these categories. In addition, we show how two types of modality known in philosophical logic (de re/de dicto modality) can be used to address some subtle issues that appear in conceptual diagrams when different sorts of object types and part-whole relations are combined.	diagram;formal ontology;modal logic;modality (human–computer interaction);modeling language;object type (object-oriented programming);theory	Giancarlo Guizzardi	2007		10.1007/978-3-540-72988-4_2	knowledge management;artificial intelligence;conceptual model;algorithm	SE	-22.354153333055677	9.095391952913166	166468
2c70d7aa3dbcfbaf1fd713b402defe072036283e	computing multi-relational sufficient statistics for large databases	multi relational databases;virtual join;sufficient statistics;relational algebra	Databases contain information about which relationships do and do not hold among entities. To make this information accessible for statistical analysis requires computing sufficient statistics that combine information from different database tables. Such statistics may involve any number of positive and negative relationships. With a naive enumeration approach, computing sufficient statistics for negative relationships is feasible only for small databases. We solve this problem with a new dynamic programming algorithm that performs a virtual join, where the requisite counts are computed without materializing join tables. Contingency table algebra is a new extension of relational algebra, that facilitates the efficient implementation of this Möobius virtual join operation. The Möbius Join scales to large datasets (over 1M tuples) with complex schemas. Empirical evaluation with seven benchmark datasets showed that information about the presence and absence of links can be exploited in feature selection, association rule mining, and Bayesian network learning.	algorithm;association rule learning;bayesian network;benchmark (computing);contingency table;dynamic programming;entity;feature selection;relational algebra;relational database management system;table (database)	Zhensong Qian;Oliver Schulte;Yan Lindsay Sun	2014		10.1145/2661829.2662010	hash join;recursive join;sufficient statistic;relational algebra;computer science;theoretical computer science;machine learning;data mining;database;mathematics;sort-merge join	DB	-26.271058862023462	6.348787190547941	169213
0913895fdffe644d03bfba947ce136d5867f640b	getting unique solution in data exchange	unique solution;source instance;schema mapping;valid target instance;data exchange;target data;one-way mapping;database schema;target instance;source data;abduction problem	A schema mapping is a high-level specification in which the relationship between two database schemas is described. In data exchange, schema mappings are one-way mappings that describe which data can be brought from source data to target data. Therefore, given a source instance and a mapping, there might be more than one valid target instance. This fact causes many problems in query answering over target data for non-conjunctive queries. To make query answering feasible for all queries, we focus on a methodology for extending the original schema mapping to guarantee the uniqueness of target instance corresponding to a source instance. To this end, we introduce a theoretical framework where the problem is transformed to an abduction problem, namely, definability abduction. We apply the framework to relational data exchange setting and solve the problem by pointing out minimal solutions according to a specific semantic minimality criterion.	abductive reasoning;conjunctive query;database schema;high- and low-level;one-way function;source data	Nhung Ngo	2013	PVLDB	10.14778/2536274.2536332	data exchange;theoretical computer science;data mining;database;mathematics	DB	-25.676123928793768	9.51901490365488	169273
ad5e61d9cac39f4670fc36b2a6f0cee4ef1dfa86	consequence-based reasoning for description logics with disjunctions and number restrictions		Classification of description logic (DL) ontologies is a key computational problem in modern data management applications, so considerable effort has been devoted to the development and optimisation of practical reasoning calculi. Consequence-based calculi combine ideas from hypertableau and resolution in a way that has proved very effective in practice. However, existing consequence-based calculi can handle either Horn DLs (which do not support disjunction) or DLs without number restrictions. In this paper, we overcome this important limitation and present the first consequence-based calculus for deciding concept subsumption in the DL ALCHIQ. Our calculus runs in exponential time assuming unary coding of numbers, and on ELH ontologies it runs in polynomial time. The extension to disjunctions and number restrictions is technically involved: we capture the relevant consequences using first-order clauses, and our inference rules adapt paramodulation techniques from first-order theorem proving. By using a well-known preprocessing step, the calculus can also decide concept subsumptions in SRIQ—a rich DL that covers all features of OWL 2 DL apart from nominals and datatypes. We have implemented our calculus in a new reasoner called Sequoia. We present the architecture of our reasoner and discuss several novel and important implementation techniques such as clause indexing and redundancy elimination. Finally, we present the results of an extensive performance evaluation, which revealed Sequoia to be competitive with existing reasoners. Thus, the calculus and the techniques we present in this paper provide an important addition to the repertoire of practical implementation techniques for description logic reasoning.	automated theorem proving;calculi;computational problem;description logic;excretory function;first-order predicate;horn clause;ibm sequoia;indexes;inference;mathematical optimization;ontology (information science);performance evaluation;polynomial;preprocessor;reasoning - publishing subsection;resolution (logic);rule (guideline);semantic reasoner;subsumption architecture;time complexity;unary coding;unary operation;web ontology language	Andrew Bate;Boris Motik;Bernardo Cuenca Grau;David Tena Cucala;Ian Horrocks	2018	J. Artif. Intell. Res.	10.1613/jair.1.11257		AI	-23.962575985338002	8.882780625684248	171046
2f182dcaf37226e596b57e75e2febf740616975f	a constraint optimization method for large-scale distributed view selection	constraint optimization problem;modeling and management;distributed database design;query processing and optimization;materialized views	View materialization is a commonly used technique in many data-intensive systems to improve the query performance. Increasing need for large-scale data processing has led to investigating the view selection problem in distributed complex scenarios where a set of cooperating computer nodes may share data and issue numerous queries. In our work, the view selection and data placement problem is studied given a limited amount of resources e.g. storage space capacity per computer node and maximum view maintenance cost. We also consider the IO and CPU costs for each computer node as well as the network bandwidth. To address this problem, we have proposed a constraint programming approach which is based on constraint reasoning to tackle problems that aim to satisfy a set of constraints. Then, we have designed a set of efficient heuristics that result in a drastic reduction in the solution space so that the problem becomes solvable for complex scenarios consisting of realistically large numbers of sites, queries and views. Our experimental study shows that our approach performs consistently better compared to a practical approach designed for large-scale distributed environments which uses a genetic algorithm to compute which view has to be materialized at what computer node.	constrained optimization	Imene Mami;Zohra Bellahsene;Remi Coletta	2016	Trans. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-662-49534-6_3	computer science;theoretical computer science;data mining;database	Graphics	-26.080685910707434	4.379097283141148	171272
2e539e9943708c1d7a6de6c8461f26c04a186136	bridging the gap between owl and relational databases	owl;relational database;satisfiability;semantic web;relational databases;knowledge base	Schema statements in OWL are interpreted quite differently from analogous statements in relational databases. If these statements are meant to be interpreted as integrity constraints (ICs), OWL's interpretation may seem confusing and/or inappropriate. Therefore, we propose an extension of OWL with ICs that captures the intuition behind ICs in relational databases. We discuss the algorithms for checking IC satisfaction for different types of knowledge bases, and show that, if the constraints are satisfied, we can disregard them while answering a broad range of positive queries.	abox;algorithm;bridging (networking);constraint satisfaction;data integrity;database schema;kaon;model checking;ontology (information science);peano axioms;relational database;semantic reasoner;tbox;web ontology language	Boris Motik;Ian Horrocks;Ulrike Sattler	2007		10.1145/1242572.1242681	knowledge base;codd's theorem;relational calculus;relational database;computer science;data mining;database;conjunctive query;world wide web	Web+IR	-25.048696922346377	9.696310359942288	171604
51c72d6c8aa64862cb00494223966086d5206078	data expiration and aggregate queries		We investigate the space requirements for summaries needed for maintaining exact answers to aggregate queries over histories of relational databases. We show that, in general, a super-logarithmic lower bound (in the length of the history) on space needed to maintain a summary of the history in order to be able to maintain answers to counting queries. We also develop a natural restriction on the use of aggregation that allows for a logarithmic upper bound and in turn maintaining the summary of the history using counters.	aggregate data;aggregate function;algorithm;count–min sketch;first-order predicate;graham scan;günter böckle;hall effect;holism;jan bergstra;jones calculus;jun wang (scientist);knowledge representation and reasoning;partial evaluation;relational database;requirement;springer (tank);turing completeness;yang	David Toman	2009			automotive engineering;internal combustion engine;computer science;expiration	DB	-25.48243255429231	8.454426656568431	171778
e11aa7c6c479f4d9b9f23adc2fafc852814dabd3	declarative reasoning approaches for agent coordination		Reasoning about Action and Change (RAC) and Answer Set Programming (ASP) are two well-known fields in AI for logic-based reasoning. Each paradigm bears unique features and a possible integration can lead to more effective ways to address hard AI problems. In this paper, we report on implementations that embed RAC formalisms and concepts in ASP and present the experimental results obtained, building on a graph-based problem setting that introduces casual and temporal requirements.	ai-complete;answer set programming;artificial intelligence;declarative programming;oracle rac;programming paradigm;requirement;stable model semantics	Filippos Gouidis;Theodore Patkos;Giorgos Flouris;Dimitris Plexousakis	2014		10.1007/978-3-319-07064-3_42	implementation;computer science;artificial intelligence;machine learning;event calculus;answer set programming;casual;graph	AI	-19.82089226626574	8.983343082232862	171785
745e78f72a6eed04e7cc2da05af3301746e7d957	advanced processing for ontological queries	computer science and information systems	Ontology-based data access is a powerful form of extending database technology, where a classical extensional database (EDB) is enhanced by an ontology that generates new intensional knowledge which may contribute to answer a query. The ontological integrity constraints for generating this intensional knowledge can be specified in description logics such as DL-Lite. It was recently shown that these formalisms allow for very efficient query-answering. They are, however, too weak to express simple and useful integrity constraints that involve joins. In this paper we introduce a more expressive formalism that takes joins into account, while still enjoying the same low query-answering complexity. In our framework, ontological constraints are expressed by sets of rules that are so-called tuple-generating dependencies (TGDs). We propose the language of sticky sets of TGDs, which are sets of TGDs with a restriction on multiple occurrences of variables (including joins) in the rule bodies. We establish complexity results for answering conjunctive queries under sticky sets of TGDs, showing, in particular, that ontological conjunctive queries can be compiled into first-order and thus SQL queries over the given EDB instance. We also show how sticky sets of TGDs can be combined with functional dependencies. In summary, we obtain a highly expressive and effective ontological modeling language that unifies and generalizes both classical database constraints and important features of the most widespread tractable description logics.	adobe flash lite;bus (computing);cobham's thesis;compiler;conjunctive query;data access;data integrity;description logic;first-order predicate;functional dependency;intensional logic;modeling language;relational database;rough set;sql;semantics (computer science);sticky bit	Andrea Calì;Georg Gottlob;Andreas Pieris	2010	PVLDB	10.14778/1920841.1920912	computer science;theoretical computer science;data mining;database	DB	-24.41004618119617	8.917523485869738	171911
287cfa9eec04d01b956270a64e6b37f645b8602f	assumptions in relational database theory	query language;relational database;functional dependency	"""Many results in relational database theory on the structure of dependencies, query languages, and databases in general have now been established. However, neither (a) the reliance of these results on various assumptions, nor (b) the desirability or reasonableness of these assumptions themselves have been closely examined. These assumptions are nontrivial: examples include the universal relation assumption and the lossless join assumption.The purpose of the present paper is to clarify many of the existing assumptions, and point out weaknesses. This is desirable both to harden the statements of previous results, and to evaluate recent suggestions that certain assumptions (such as the acyclic JD assumption) may be useful for modeling """"real world"""" databases. Specifically, studies are made of assumptions made for (1) universal relations, (2) functional dependency inference, and (3) decomposition theory. We show that:• Some assumptions (such as uniqueness of relationships among attributes) can be more powerful than they appear;• common treatment of FDs is sometimes inappropriate, and for example FD inferences such as {A → B, B → C} |= A → C can be incorrect;• the 'decomposition' approach to design may be hard to justify in real terms; and• Acyclic JDs may have drawbacks in eliminating ambiguity in queries and in modeling real enterprises.It is hoped that this exposition will help clarify some confusing issues in this field, and will lead to a better understanding of which assumptions are reasonable and useful in modeling the """"real world""""."""	database theory;directed acyclic graph;emoticon;functional dependency;hardening (computing);jd - java decompiler;lossless compression;query language;relational database;universal relation assumption	Paolo Atzeni;Douglas Stott Parker	1982		10.1145/588111.588113	relational database;computer science;theoretical computer science;data mining;database;mathematics;functional dependency;algorithm;query language	DB	-25.2057691593703	10.123103538094544	172104
0b706b9b6b27f64b3e78e1e051844ef5e42b55c0	the datalogdl combination of deduction rules and description logics	sld resolution;datalog;semantic web;description logic;knowledge representation;hybrid rules;tableaux algorithms	Uniting ontologies and rules has become a central topic in the Semantic Web. Bridging the discrepancy between these two knowledge representations, this paper introduces DatalogDL as a family of hybrid languages, where Datalog rules are parameterized by various DL (description logic) languages ranging from ALC to SHIQ. Making DatalogDL a decidable system with complexity of EXPTIME, we propose independent properties in the DL body as the restriction to hybrid rules, and weaken the safeness condition to balance the trade-off between expressivity and reasoning power. Building on existing well-developed techniques, we present a principled approach to enrich (RuleML) rules with information from (OWL) ontologies, and develop a prototype system combining a rule engine (OO jDREW) with a DL reasoner (RACER).	bridging (networking);business rules engine;datalog;description logic;discrepancy function;exptime;natural deduction;ontology (information science);prototype;ruleml;semantic web;semantic reasoner	Jing Mei;Zuoquan Lin;Harold Boley;Jie Li;Virendrakumar C. Bhavsar	2007	Computational Intelligence	10.1111/j.1467-8640.2007.00311.x	knowledge representation and reasoning;description logic;semantic web rule language;computer science;artificial intelligence;sld resolution;theoretical computer science;semantic web;database;datalog;algorithm	AI	-21.325410102626844	9.164251509720543	173426
6c308a597c319f71da21e594a181791804da01c1	drago: distributed reasoning architecture for the semantic web	distributed system;logica formal;ontologie;sistema experto;systeme reparti;formal specification;web semantique;logica descripcion;semantics;semantica;semantique;specification formelle;especificacion formal;semantic mapping;sistema repartido;decision procedure;design and implementation;web semantica;formal logic;semantic web;ontologia;distributed description logics;systeme expert;description logic;semantic relations;logique formelle;ontology;logique description;knowledge base;expert system	The paper addresses the problem of reasoning with multiple ontologies interrelated with semantic mappings. This problem is becoming more and more relevant due to the necessity of building a scalable ontological reasoning tools for the Semantic Web. In contrast to the so called global approach, in which reasoning with multiple semantically related ontologies is performed in a global knowledge base that encodes both ontologies and semantic mappings, we propose a distributed reasoning approach in which reasoning is the result of combination via semantic mappings of local reasonings chunks performed in single ontologies. The paper presents a tableau-based distributed reasoning procedure which is sound and complete w.r.t. Distributed Description Logics, the formal framework used to represent multiple semantically connected ontologies. The paper also describes the design and implementation principles of a distributed reasoning system, called DRAGO (Distributed Reasoning Architecture for a Galaxy of Ontology), that implements such distributed decision procedure.	decision problem;description logic;directed acyclic graph;distributed algorithm;knowledge base;long division;method of analytic tableaux;ontology (information science);reasoning system;scalability;semantic web	Luciano Serafini;Andrei Tamilin	2005		10.1007/11431053_25	knowledge representation and reasoning;opportunistic reasoning;knowledge base;description logic;computer science;artificial intelligence;adaptive reasoning;theoretical computer science;non-monotonic logic;semantic web;ontology;data mining;formal specification;database;semantics;reasoning system;automated reasoning;logic;expert system	AI	-21.10003032476221	9.455731713509161	174399
362c4a6dee61a1a79b3346d04e33bc6d9c0f45a2	model correspondence as a basis for schema domination	computer and systems sciences;information science;schema integration;systemvetenskap;er modelling;domination;data och systemvetenskap;conceptual modelling;information capacity	Conceptual schemata each representing some component of a system in the making, can be integrated in a variety of ways. Herein, we explore some fundamental notions of this. In particular, we examine some ways in which integration using correspondence assertions affects the interrelationship of two component schemata. Our analysis of the logic leads us to reject the commonly asserted requirement of constraining correspondence assertions to single predicates from a source schema. Much previous work has focussed on dominance with regard to preservation of information capacity as a primary integration criterion. However, even though it is desirable that the information capacity of a combined schema dominate one or both of its constituent schemata, we here discuss some aspects of why domination based on information capacity alone is insufficient for the integration to be semantically satisfactory, and we provide a framework for detecting mappings that prevent schema domination. 2010 Elsevier B.V. All rights reserved.	channel capacity;dominating set;sensor	Guy Davies;Love Ekenberg	2010	Knowl.-Based Syst.	10.1016/j.knosys.2010.01.010	information science;computer science;three schema approach;conceptual schema;artificial intelligence;machine learning;algorithm	AI	-20.25810198855312	4.262163879474698	175160
892c9730e8ad5ae310b08faa53bbf98b509582ad	testing bag-containment of conjunctive queries	conjunctive queries;semantic relations	Under the bag-theoretic semantics relations are bags of tuples, that is, a tuple may have any number of duplicates. Under this semantics, a conjunctive query $Q$ is bag-contained in a conjunctive query $Q^{\prime }$ , denoted $ Q\leq _bQ^{\prime }$ , if for all databases ${\cal D}$ , $Q({\cal D})$ , the result of applying $Q$ to ${\cal D}$ , is a subbag of $ Q^{\prime }({\cal D)}$ . It is not known whether testing $Q\leq _bQ^{\prime }$ is decidable. In this paper we prove that $Q\leq _bQ^{\prime }$ can be tested on a finite set of canonical databases built from the body of $Q$ . Using that result we give a procedure that decides the bag-containment problem of conjunctive queries in a large number of cases.	acm transactions on database systems;computability;computer science;conjunctive query;database;ibm notes;logic programming;relational algebra;relational database;sethi–ullman algorithm;springer (tank);symposium on principles of database systems;symposium on theory of computing;theory;wa-tor	Nieves R. Brisaboa;Héctor J. Hernández	1997	Acta Informatica	10.1007/s002360050097	discrete mathematics;computer science;database;mathematics;conjunctive query;algorithm	DB	-24.826860746973562	11.137734315697527	175257
6bea471864b6912f40ea48b72db2f6c5f5c21acb	preferences, links, and probabilities for ranking objects in ontologies	bayesian network;description logic	In previous work, we have introduced variable-strength conditional preferences for ranking objects in ontologies. In this paper, we continu e his line of research. We propose a new ranking of objects, which integrates this userdefined preference ranking of objects with Google’s importance rankin g (called PageRank) based on the link structure between the objects. We also propose to use probabilistic description logics based on Bayesian networks and the descr iption logic DL-Lite to compute the ranking of incompletely specified objects.	adobe flash lite;bayesian network;description logic;ontology (information science);pagerank	Thomas Lukasiewicz;Jörg Schellhase	2006			machine learning;data mining;mathematics;information retrieval	AI	-22.82437019282785	6.773563143345721	175594
23cb5fd2a8f9a87ec51b2a2d0848f51af9512dc6	randomised optimisation of discrimination networks considering node-sharing		Because of their ability to efficiently store, access, and process data, Database Management Systems (DBMSs) and Rule-based Systems (RBSs) are used in many information systems as information processing units. A basic function of a RBS and a function of many DBMSs is to match conditions on the available data. To improve performance intermediate results are stored in Discrimination Networks (DNs). The resulting memory consumption and runtime cost depend on the structure of the DN. A lot of research has been done in the area of optimising DNs. In this paper, we focus on re-using network parts considering multiple rule conditions and exploiting the characteristics of equivalences. We present an approach incorporating the potential of both concepts and balance their application in a randomised fashion. To evaluate the algorithms developed, they were implemented and yielded promising results. Shortcomings of this approach are discussed and their removal constitutes our current work.	algorithm;database;file spanning;free variables and bound variables;heuristic;information processing;information system;mathematical optimization;maurice herlihy;maximal set;microsoft outlook for mac;minimum spanning tree;mutual exclusion;parallel computing;result set;rule-based system;transactional memory;turing completeness	Fabian Ohler;Karl-Heinz Krempels;Christoph Terwelp	2016		10.5220/0005905002570267	computer science;data mining	DB	-24.820446826078193	6.5899967327951	176022
117e867b50e31cebd969390192a02ff6e07ab59e	fuzzy ontology modeling by utilizing fuzzy set and fuzzy description logic		In this paper, we propose a fuzzy ontology model by using fuzzy set and description fuzzy logic in order to support fuzzy inference model and fuzzy ontology integration. As in previous studies, elements of a fuzzy ontology are fuzzed by using a class member function based on the fuzzy set theory. The significant contribution of this work is to supplement the fuzzification of concepts with respect to the corresponding ontology. Besides, we clarify the set Z by using fuzzy description logic representing for constrains among ontology elements which have never been mentioned before. In this paper, we also execute the proposed fuzzy ontology implemented in OWL2 language. We employ Protege to present the fuzzy ontology design in weather domain for demonstration.	description logic;fuzzy logic;fuzzy set	Xuan Hung Quach;Thi Lan Giao Hoang	2018		10.1007/978-3-319-76081-0_2	machine learning;web ontology language;fuzzy logic;data mining;fuzzy set;artificial intelligence;protégé;ontology;description logic;computer science;inference	AI	-22.722193462171607	6.312001215482358	176702
131b48186fae182037d3d19302c93a9af78c0b89	dlvhex: a prover for semantic-web reasoning under the answer-set semantics	logic programming application specific processors semantic web ontologies resource description framework atomic layer deposition logic design engines data mining libraries;rdf datasets;meta reasoning;programming language semantics;nonmonotonic logic;answer set programming;dlvhex;inference mechanisms;higher order atoms;higher order;logic programming;answer set semantics;semantic web;description logic knowledge bases dlvhex semantic web reasoning answer set semantics hex programs nonmonotonic logic programs higher order atoms meta reasoning rdf datasets;description logic;semantic web reasoning;nonmonotonic logic programs;description logic knowledge bases;hex programs;semantic web inference mechanisms logic programming programming language semantics;knowledge base	We present the system dlvhex, a solver for HEX-programs, which are nonmonotonic logic programs admitting both higher-order atoms as well as external atoms. Higher-order features are widely acknowledged as being useful for various tasks, including meta-reasoning. Furthermore, the possibility to exchange knowledge with external sources in a fully declarative paradigm such as answer-set programming (ASP) becomes increasingly important, in particular in view of applications in the semantic-Web area. Through external atoms, HEX-programs can deal with external knowledge and reasoners of various nature, such as RDF datasets or description-logics knowledge bases	answer set programming;declarative programming;description logic;hex;non-monotonic logic;programming paradigm;semantic web;solver;stable model semantics	Thomas Eiter;Giovambattista Ianni;Roman Schindlauer;Hans Tompits	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.64	knowledge base;description logic;higher-order logic;computer science;artificial intelligence;theoretical computer science;non-monotonic logic;answer set programming;semantic web;database;programming language;logic programming	AI	-21.071744366705868	9.632388307516909	176952
a97bf9ef2f78d3b89ce253edbed0f2ab91c3c527	trust models for rdf data: semantics and complexity	trust;rdf reasoning;islands of tractability;entailment	Due to the openness and decentralization of the Web, mechanisms to represent and reason about the reliability of RDF data become essential. This paper embarks on a formal analysis of RDF data enriched with trust information by focusing on the characterization of its model-theoretic semantics and on the study of relevant reasoning problems. The impact of trust values on the computational complexity of well-known concepts related to the entailment of RDF graphs is studied. In particular, islands of tractability are identified for classes of acyclic and nearly-acyclic graphs. Moreover, an implementation of the framework and an experimental evaluation on real data are discussed.	computational complexity theory;directed acyclic graph;openness;world wide web	Valeria Fionda;Gianluigi Greco	2015			logical consequence;computer science;theoretical computer science;data mining;database;trustworthy computing;rdf schema	AI	-22.341335318592833	8.244852483069986	177135
1d54b943441d91ef28b4a040cfa4b3f8364dd8cc	on the relationship between description logic-based and f-logic-based ontologies	frame logic;ontology meta modeling;semantic web language layering;equality safe formulas;f logic programming;description logic;ontology translation	Many popular ontology languages are based on (subsets of) first-order predicate logic, with classes represented by unary predicates and properties by binary predicates. Specifically, the Semantic Web ontology language OWL DL is based on the Description Logic SHOIN . F-Logic is an ontology language which is also based on first-order logic, but classes and properties are modeled as terms rather than predicates. Our goal is to enable interoperation between predicate-based and FLogic-based ontology languages. To this end, we define an intuitive translation from predicate-based ontologies to F-Logic ontologies and show that this translation preserves entailment for large classes of ontology languages, including most of OWL DL. Specifically, we define the class of equality-safe (E-safe) formulas, show that the Description Logic SHIQ is E-safe, and show that the translation preserves validity of E-safe formulas. We use these results to close the open problem of layering F-Logic programming on top of Description Logic Programs and we show that our results apply to HILOG, a syntactically higher-order, but semantically first-order language. Finally, we show that our results also apply to a meta-modeling extension for Description Logics (v-semantics).	description logic;f-logic;first-order logic;first-order predicate;hilog;interoperation;logic programming;metamodeling;ontology (information science);predicate (mathematical logic);semantic web;unary operation;web ontology language	Jos de Bruijn;Stijn Heymans	2008	Fundam. Inform.		predicate logic;dynamic logic;natural language processing;upper ontology;f-logic;description logic;higher-order logic;ontology components;ontology inference layer;computer science;ontology;artificial intelligence;predicate functor logic;predicate variable;ontology language;programming language;web ontology language;process ontology;algorithm	AI	-22.147563662007045	9.973095298634979	177136
2a6b8483895821d6063ec5a922d4bb4bf7fcbdf0	lower and upper bounds for sparql queries over owl ontologies	query bounds;owl ontologies;sparql queries;query extension;query answering	The recent standardization of the SPARQL 1.1 Entailment Regimes [2], which extend the SPARQL Query Language [3] with the capability of querying also for implicit knowledge makes the need for an efficient evaluation of complex queries over OWL ontologies urgent. We present an approach for optimizing the evaluation of SPARQL queries over OWL ontologies using SPARQL’s OWL Direct Semantics entailment regime. Such queries consist of axiom templates, i.e., Description Logic (DL) axioms with variables in place of concept, role and individual names. Answers to such queries are mappings of query (concept, role or individual) variables to corresponding (concept, role or individual) names that instantiate the axiom templates to axioms entailed by the queried knowledge base (KB). Since computing query answers over an expressive KB is computationally very costly, approximation techniques have been proposed that use a weakened version of the KB to compute a lower bound (yields sound but potentially incomplete results) and a strengthened version to compute an upper bound (yields complete but potentially unsound results) for the results [9, 7, 8]. Another well-known technique is to compute the bounds from a complete and clash-free tableau generated by a DL reasoner [4, 5]. Deterministically derived facts are used as lower bound, while also non-deterministically derived ones are considered for the upper bound. Answers in the “gap”, i.e., potential answers in the upper but not the lower bound, usually have to be checked individually by performing a consistency check with a fully fledged OWL 2 DL reasoner. While we also use bounds, we allow for much more expressive queries than related approaches. To optimize the evaluation of possible query answers in the gap, we present a query extension approach that uses the TBox of the queried KB to extend the query with additional parts. We show that the resulting query is equivalent to the original one and we use the additional parts that are simple to evaluate for restricting the bounds of subqueries of the initial query. In an empirical evaluation we show that the proposed query extension approach can lead to a significant decrease in the query execution time of up to four orders of magnitude. More details about our method as well as more evaluation results can be found in the extended version of our paper [1].	approximation;description logic;deterministic algorithm;knowledge base;long division;ontology (information science);query language;run time (program lifecycle phase);sparql;sql;semantic reasoner;tbox;web ontology language	Birte Glimm;Yevgeny Kazakov;Ilianna Kollia;Giorgos B. Stamou	2015			sargable;query optimization;query expansion;web query classification;boolean conjunctive query;computer science;sparql;query by example;data mining;database;rdf query language;web search query;information retrieval;query language;spatial query	Web+IR	-24.227101621289098	8.406835750378725	177439
6ab6a92a1ed1e77d56a3ad389fe777d9be75a543	from generalization of syntactic parse trees to conceptual graphs	syntactic parsing;semantic representation;conceptual graph;similarity measure;logical form	We define sentence generalization and generalization diagrams as a special sort of conceptual graphs which can be constructed automatically from syntactic parse trees and support semantic classification task. Similarity measure between syntactic parse trees is developed as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the syntactic generalization level and semantic generalization level (anti-unification of logic forms). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at	conceptual graph;diagram;logic form;parse tree;parsing;similarity measure;unification (computer science)	Boris A. Galitsky;Gabor Dobrocsi;Josep Lluís de la Rosa i Esteva;Sergei O. Kuznetsov	2010		10.1007/978-3-642-14197-3_19	natural language processing;conceptual graph;logical form;syntactic predicate;machine learning;pattern recognition;mathematics	NLP	-20.584036461073644	5.686440891813842	177806
205e8a3dd1c73da36b63e3038989dee7a69312e9	universal marker and functional relation: semantics and operations	semantica formal;intelligence artificielle;formal semantics;raisonnement;functional dependency;semantique formelle;conceptual schema;conceptual graph;razonamiento;artificial intelligence;inteligencia artificial;logic programs;reasoning;knowledge base	The universal marker (i.e., universal quantifier) and the functional relation are two useful notations that make Conceptual Graph (CG') representations more concise in expressing universally quantified facts and functional dependencies, which are commonly used in knowledge bases, logic programs and data conceptual schemas. We introduce an expansion rule that formally defines the semantics of CGs containing universal markers and/or functional relations. On the basis of this formal semantics, we define two reasoning operations that are performed directly on CGs with these two notations to make them more useful. One operation is the universal CG projection defining the subsumption relation on the extended CGs. The other operation is the universal concept join performing universal instantiations and inheritances simultaneously in one graph operation. Both the operations are proved to be sound with respect to their described interpretations.		Tru H. Cao;Peter N. Creasy	1997		10.1007/BFb0027887	conceptual graph;knowledge base;computer science;conceptual schema;artificial intelligence;theoretical computer science;machine learning;formal semantics;database;mathematics;functional dependency;programming language;reason;algorithm	Logic	-19.712361042587755	7.165001899515754	177917
e298f472c6b98ab6c1f69b19f323fa76360236b5	enabling attack behavior prediction in ubiquitous environments	intrusion detection system attack behavior prediction ubiquitous environment pervasive computing conceptual semantic description information resource management probabilistic theory uncertain knowledge representation breadth and depth bayesian classifier inference probabilistic algorithm ontology;belief networks;bayesian classifier;probability;security model;breadth and depth bayesian classifier;intrusion detection inference algorithms pervasive computing information management resource management information resources knowledge representation information security bayesian methods ontologies;attack behavior prediction;pervasive computing;probabilistic algorithm;inference mechanisms;semantic networks;semantic networks ubiquitous computing security of data belief networks inference mechanisms probability pattern classification ontologies artificial intelligence;ontologies artificial intelligence;probabilistic model;information integration;information resource management;semantic description;pattern classification;ubiquitous computing;probabilistic theory;knowledge representation;uncertain knowledge representation;conceptual semantic description;ontology;security of data;inference probabilistic algorithm;intrusion detection system;ubiquitous environment	The pervasive computing paradigm has raised issues such as conceptual semantic descriptions and ambient management of information resources. The probabilistic theory on the other hand provides uncertain knowledge representation schemes that are semantically inefficient. However, security models related to attacks exploits both semantic and probabilistic modeling. Issues such as attack prediction and classification of attacker's intentions are of high importance in IDS environments. In this paper we propose a novel Breadth and Depth Bayesian classifier and an inference probabilistic algorithm. The inference algorithm is applied over well defined conceptual information integrated in a hybrid IDS by means of ontologies.	knowledge representation and reasoning;naive bayes classifier;ontology (information science);programming paradigm;randomized algorithm;ubiquitous computing	Theodoros Anagnostopoulos;Christos Anagnostopoulos;Stathes Hadjiefthymiades	2005	ICPS '05. Proceedings. International Conference on Pervasive Services, 2005.	10.1109/PERSER.2005.1506559	computer security model;intrusion detection system;statistical model;naive bayes classifier;computer science;information integration;machine learning;pattern recognition;probability;data mining;semantic network;randomized algorithm;ubiquitous computing	AI	-23.12689975913389	5.117740512505133	178505
61ae724698c5dc004c5282fa35ff4ebfadc158a8	a paradigm for learning queries on big data	concept learning;user interactions;big data;query languages;learning;query inference	Specifying a database query using a formal query language is typically a challenging task for non-expert users. In the context of big data, this problem becomes even harder as it requires the users to deal with database instances of big sizes and hence difficult to visualize. Such instances usually lack a schema to help the users specify their queries, or have an incomplete schema as they come from disparate data sources. In this paper, we propose a novel paradigm for interactive learning of queries on big data, without assuming any knowledge of the database schema. The paradigm can be applied to different database models and a class of queries adequate to the database model. In particular, in this paper we present two instantiations that validated the proposed paradigm for learning relational join queries and for learning path queries on graph databases. Finally, we discuss the challenges of employing the paradigm for further data models and for learning cross-model schema mappings.	big data;data model;database model;database schema;graph database;programming paradigm;query language;relational algebra	Angela Bonifati;Radu Ciucanu;Aurélien Lemay;Slawomir Staworko	2014		10.1145/2658840.2658842	geography;media studies;engineering physics;operations research	DB	-21.825195485959004	7.34853308149267	179105
1ae7961269ac837d62356fe42c39f34a4702116b	implication of functional dependencies for recursive queries	base relacional dato;methode recursive;relational data model;interrogation base donnee;metodo recursivo;interrogacion base datos;recursive method;relational database;satisfiability;functional dependency;deductive database;datalog;base dato deductiva;dependance fonctionnelle;relational model;base donnee relationnelle;base donnee deductive;dependencia funcional;database query;problem solving;functional dependence;deductive databases	After two decades of research in Deductive Databases, SQL99 [12] brings them again to the foreground given that SQL99 includes queries with linear recursion. Therefore some of the problems solved for the relational model demand our attention again. In this paper, we tackle the implication of functional dependencies (also known as the FD-FD implication problem) in the deductive model framework. The problem is as follows. Given P, F, and f, where P is a Datalog program, F is a set of functional dependencies defined on the predicates of P, and f is a fd defined over the predicates of P, is it true that for all databases d defined exclusively on the extensional predicates of P, d satisfies F implies that P(d) -the output database- satisfies f. Unlike the implication problem of functional dependencies in the relational data model, this problem is undecidable for general Datalog programs. In this paper, we provide two methods to check if a given set of fds will be satisfied by the output database (without computing such database) for a class of Datalog programs.	functional dependency;hierarchical and recursive queries in sql;recursion (computer science)	José R. Paramá;Nieves R. Brisaboa;Miguel R. Penabad;Ángeles S. Places	2003		10.1007/978-3-540-39866-0_49	relational model;dependency theory;computer science;theoretical computer science;database;mathematics;programming language;algorithm	DB	-25.7133057604001	10.715238598261822	179425
5d3c7fd1ec808c872f8ddba03ff2c3fe67ad942d	mknf knowledge bases in multi-context systems		In this paper we investigate the relationship between Multi-Context Systems and Hybrid MKNF Knowledge Bases. Multi-Context Systems provide an effective and modular way to integrate knowledge from different heterogeneous sources (contexts) through so-called bridge rules. Hybrid MKNF Knowledge Bases, based on the logic of minimal knowledge and negation as failure (MKNF), allow for a seamless combination of description logic ontology languages with non-monotonic logic programming rules. In this paper, we not only show that Hybrid MKNF Knowledge Bases can be used as particular contexts in Multi-Context Systems, but we also provide transformations from the former into the latter, without the need for an explicit Hybrid MKNF context, hence providing a way for agents to reason with Hybrid MKNF Knowledge Bases within Multi-Context Systems without the need for specialized Hybrid MKNF reasoners.	computational complexity theory;decision problem;description logic;diode bridge;first-order predicate;ivy bridge (microarchitecture);knowledge representation and reasoning;logic programming;multi categories security;negation as failure;non-monotonic logic;seamless3d;semantic reasoner;semantics (computer science);well-founded semantics;whole earth 'lectronic link	Martin Homola;Matthias Knorr;João Leite;Martin Slota	2012		10.1007/978-3-642-32897-8_11	computer science;knowledge management;artificial intelligence;algorithm	AI	-20.671438326424322	9.670264172281241	180231
59b2d7daf06d248dded950961376f9378fbaf21c	an infrastructure for probabilistic reasoning with web ontologies	004 informatik	We present an infrastructure for probabilistic reasoning with ontologies that is based on our Markov logic engine ROCKIT. Markov logic is a template language that combines first-order logic with log-linear graphical models. We show how to translate OWL-EL as well as RDF schema to Markov logic and how to use ROCKIT for applying MAP inference on the given set of formulas. The resulting system is an infrastructure for log linear logics that can be used for probabilistic reasoning with both extended OWL-EL and RDF schema. We describe our system and illustrate its benefits by presenting two application scenarios. These scenarios are ontology matching, and knowledge base verification, with a special focus on temporal reasoning. Our results indicate that our system, which is based on a well-founded probabilistic semantics, is capable of solving relevant problems as good as or better than state of the art systems that have specifically been designed for the respective problem.	first-order logic;first-order predicate;graphical model;knowledge base;log-linear model;map;marginal model;markov chain;markov logic network;ontology (information science);ontology alignment;probabilistic semantics;rdf schema;semantic web;template processor;user interface;verification and validation;web ontology language;world wide web	Jakob Huber;Mathias Niepert;Jan Nößner;Joerg Schoenfisch;Christian Meilicke;Heiner Stuckenschmidt	2017	Semantic Web	10.3233/SW-160219	probabilistic ctl;philosophy;sociology;probabilistic logic	AI	-22.09893190136605	7.0890680204034195	180333
d48e463d522c1768ea3ded0ce8069c30414a9c9f	inclusion dependencies reloaded	implication;sql;axiomatization;semantics;null;computational complexity;inclusion dependency	Inclusion dependencies form one of the most fundamental classes of integrity constraints. Their importance in classical data management is reinforced by modern applications such as data cleaning and profiling, entity resolution and schema matching. Surprisingly, the implication problem of inclusion dependencies has not been investigated in the context of SQL, the de-facto industry standard. Codd's relational model of data represents the idealized special case of SQL in which all attributes are declared NOT NULL. Driven by the SQL standard recommendation, we investigate inclusion dependencies and NOT NULL constraints under simple and partial semantics. Partial semantics is not natively supported by any SQL implementation but we show how classical results on the implication problem carry over into this context. Interestingly, simple semantics is natively supported by every SQL implementation, but we show that the implication problem is not finitely axiomatizable in this context. Resolving this conundrum we establish an optimal solution by identifying the desirable class of not-null inclusion dependencies (NNINDs) that subsumes simple and partial semantics as special cases, and whose associated implication problem has the same computational properties as inclusion dependencies in the relational model. That is, NNIND implication is 2-ary axiomatizable and PSPACE-complete to decide. Our proof techniques bring also forward a chase procedure for deciding NNIND implication, the NP-hard subclass of typed acyclic NNINDs, and the tractable subclasses of NNINDs whose arity is bounded.	cobham's thesis;data integrity;directed acyclic graph;pspace-complete;plasma cleaning;profiling (computer programming);referential integrity;relational model;sql;technical standard	Henning Köhler;Sebastian Link	2015		10.1145/2806416.2806539	sql;dependency theory;computer science;theoretical computer science;data mining;database;semantics;computational complexity theory;null;algorithm	DB	-24.387357591555563	9.570948655151284	180422
0e4370dbc008651b01fb96a0a1f761a78a685aa7	functional description of geoprocessing services as conjunctive datalog queries	computadora;tratamiento datos;semantic web service;computers;conjunctive queries;decouverte;frame structure;ordinateur;bâti;web processing service;data processing;traitement donnee;web service;web service modeling framework;logic programming;spatial data infrastructure;discoveries;semantic description;geoprocessing services;infrastructures;service discovery;logic programs;descubierta;semantic anotation;geoprocessing service;infrastructure	Discovery of suitable web services is a crucial task in Spatial Data Infrastructures (SDI). In this work, we develop a novel approach to the discovery of geoprocessing services (WPS). Discovery requests and Web Processing Services are annotated as conjunctive queries in a logic programming (LP) language and the discovery process is based on Logic Programming query containment checking between these descriptions. Besides the types of input and output, we explicitly formalise the relation between them and hence are able to capture the functionality of a WPS more precisely. The use of Logic Programming query containment allows for effective reasoning during discovery. Furthermore, the relative simplicity of the semantic descriptions is advantageous for their creation by non-logics experts. The developed approach is applicable in the Web Service Modeling Framework (WSMF), a state-of-the-art semantic web service framework.	algorithm;best, worst and average case;canonical account;conjunctive query;datalog;electronic signature;first-order logic;geoprocessing;input/output;jack lutz;knowledge base;level of detail;lightweight ontology;logic programming;numerical analysis;polynomial;postcondition;programming paradigm;requirement;sql;semantic web service;shared variables;time complexity;type signature;undecidable problem;user interface;wps office;web framework;world wide web;worst-case complexity	Daniel Fitzner;Jörg Hoffmann;Eva Klien	2011	GeoInformatica	10.1007/s10707-009-0093-4	web service;spatial data infrastructure;web processing service;data processing;computer science;data mining;database;service discovery;conjunctive query;programming language;logic programming;world wide web;remote sensing	DB	-24.959822702093614	8.210985627527181	181141
3f740827d4800d3be257604aa23e71bb5548ea12	combinatorial problem solving over relational databases: view synthesis through constraint-based local search	second order;local search algorithm;temporal constraint networks;controllability;combinatorial problems;relational database;proof of concept;view synthesis;temporal conceptual workflow design;computational complexity;declarative languages;datavetenskap datalogi;computer science;information system;temporal workflow analysis;database management system;local search;cost model	Solving combinatorial problems is increasingly crucial in business applications, in order to cope with hard problems of practical relevance. In these settings, data typically reside on centralised information systems, in form of possibly large relational databases, serving multiple concurrent transactions run by different applications. We argue that the use of current solvers in these scenarios may not be a viable option, and study the applicability of extending information systems (in particular database management systems) to offer combinatorial problem solving facilities. In particular we present a declarative language based on sql for modelling combinatorial problems as second-order views of the data and study the applicability of constraint-based local search for computing such views, presenting novel techniques for local search algorithms explicitly designed to work directly on relational databases, also addressing the different cost model of querying data in the new framework. We also describe and experiment with a proof-of-concept implementation.	analysis of algorithms;centralisation;declarative programming;information system;local search (optimization);problem solving;relational database;relevance;sql;search algorithm;view synthesis	Toni Mancini;Pierre Flener;Justin Pearson	2012		10.1145/2245276.2245295	computer science;local search;theoretical computer science;machine learning;data mining;database	DB	-26.10571504243762	5.67350952068833	182677
c248f6a1e4fd82b70ed256c1332e83206c390628	unifying causality, diagnosis, repairs and view-updates in databases		In this work we establish and point out connections between the notion of query-answer causality in databases and database repairs, model-based diagnosis in its consistencybased and abductive versions, and database updates through views. The mutual relationships among these areas of data management and knowledge representation shed light on each of them and help to share notions and results they have in common. In one way or another, these are all approaches to uncertainty management, which becomes even more relevant in the context of big data that have to be made sense of.	abductive reasoning;big data;causality;database;knowledge representation and reasoning;uncertainty quantification	Leopoldo E. Bertossi;Babak Salimi	2014	CoRR		discrete mathematics;theoretical computer science;algorithm	DB	-21.103233698860254	6.746510033903692	183042
817b85aa2c360526410d7e2d09598f4520640ae3	on leveraging statistical and relational information for the representation and recognition of complex human activities	004 informatik	Machine activity recognition aims to automatically predict human activities from a series of sensor signals. It is a key aspect to several emerging applications, especially in the pervasive computing field. However, this problem faces several challenges due to the complex, relational and ambiguous nature of human activities. These challenges still defy the majority of traditional pattern recognition approaches, whether they are knowledge-based or data-driven. Concretely, the current approaches to activity recognition in sensor environments fall short to represent, reason or learn under uncertainty, complex relational structure, rich temporal context and abundant common-sense knowledge. Motivated by these shortcomings, our work focuses on the combination of both data-driven and knowledgebased paradigms in order to address this problem. In particular, we propose two logic-based statistical relational activity recognition frameworks which we describe in two different parts. The first part presents a Markov logic-based framework addressing the recognition of complex human activities under realistic settings. Markov logic [RD06] is a highly flexible statistical relational formalism combining the power of firstorder logic with Markov networks by attaching real-valued weights to formulas in first-order logic. Thus, it unites both symbolic and probabilistic reasoning and allows to model the complex relational structure as well as the inherent uncertainty underlying human activities and sensor data. We focus on addressing the challenge of recognizing interleaved and concurrent activities while preserving the intuitiveness and flexibility of the modelling task. Using three different models we evaluate and prove the viability of using Markov logic networks for that problem statement. We also demonstrate the crucial impact of domain knowledge on the recognition outcome. Implementing an exhaustive model including heterogeneous information sources comes, however, at considerable knowledge engineering efforts. Hence, employing a standard, widely used formalism can alleviate that by enhancing the portability, the re-usability and the extension of the model. In the second part of this document, we apply a hybrid approach that goes one step further than Markov logic network towards a formal, yet intuitive conceptualization of the domain of discourse. Concretely, we propose an activity recognition framework based on log-linear description logic [NNS11], a probabilistic variant of description logics. Log-linear description logic leverages the principles of Markov logic while allowing for a formal conceptualization of the domain of discourse, backed up with powerful reasoning and consistency check tools. Based on principles from the activity theory [KN12], we focus on addressing the challenge of representing and recognizing human activities at three levels of granularity: operations, actions and activities. Complying with real-life scenarios, we assess and discuss the viability of the proposed framework. In particular, we show the positive impact of augmenting the proposed multi-level activity ontology with weights compared to using its conventional weight-free variant.	activity recognition;backup;conceptualization (information science);description logic;domain of discourse;first-order logic;first-order predicate;knowledge engineering;log-linear model;markov chain;markov logic network;markov random field;pattern recognition;real life;semantics (computer science);software portability;ubiquitous computing;usability	Rim Helaoui	2016			description logic;statistical relational learning;computer science;artificial intelligence;machine learning;data mining	AI	-21.732407908962223	6.846502774496205	185293
3622d258dfeedaf129379cd7f4dcf1444bb683d6	htn planning for web service composition using shop2	owl s;htn planning;hierarchical task network;semantics;web service;web service composition;shop2;internet;online systems;calculus;web services;algorithms;planning;models;situation calculus;ai planning;information theory;machine translation	Automated composition of Web Services can be achieved by using AI planning techniques. Hierarchical Task Network (HTN) planning is especially well-suited for this ask. In this paper, we describe how HTN planning system SHOP2 can be used with OWL -S eb Service descriptions. We provide a sound and complete algorithm to translate OWL -S service descriptions to a SHOP2 domain. We prove the correctness of the algorithm b y showing the correspondence to the situation calculus semantics of OWL-S. We implemented a system that plans over sets of OWL-S descriptions using SHOP2 and then execute s the resulting plans over the Web. The system is also capable of executing information-pr oviding Web Services during the planning process. We discuss the challenges and dif ficulties of using planning in the information-rich and human-oriented context of Web Service s.	algorithm;concurrency (computer science);contingency plan;correctness (computer science);hierarchical task network;mental state;modulo operation;owl-s;precondition;run time (program lifecycle phase);situation calculus;web service;world wide web	Evren Sirin;Bijan Parsia;Dan Wu;James A. Hendler;Dana S. Nau	2004	J. Web Sem.	10.1016/j.websem.2004.06.005	web service;web modeling;information theory;web standards;computer science;knowledge management;artificial intelligence;ws-policy;database;semantics;machine translation;world wide web;owl-s;hierarchical task network	AI	-20.310363626995485	9.641505049981882	185298
5e8bd80c99d8d3cbac8d4733cdb576cc4f9e2e3a	a universal model for non-procedural database languages	query language;expressive power;equivalence relation;first order	We propose a language which can express every computable query. The language is syntactically based on the first-order predicate calculus, but semantically is interpreted as post-conditions, unlike the customary calculus query languages. The language has a capability to restrict itself to reasonable queries, accepting criteria of reasonability as a parameter.	computable function;database;first-order logic;first-order predicate;query language	Naphtali Rishe	1996	Fundam. Inform.	10.3233/FI-1996-2614	natural language processing;query optimization;data control language;computer science;first-order logic;mathematics;rdf query language;equivalence relation;programming language;expressive power;algorithm;query language;object query language	DB	-24.136471452771328	10.386302900552696	186495
4db1cae57450ec8c6989625eedc0a0c0cdf6a6f9	query answering in probabilistic datalog+/- ontologies under group preferences	probabilistic logic ontologies databases uncertainty merging semantics complexity theory;social networking online computational complexity datalog ontologies artificial intelligence query processing semantic web;latent factor model;query processing;ego networks;personalized recommendation;ontologies artificial intelligence;datalog;computational complexity;polynomial time query answering probabilistic datalog ontologies group preferences social semantic web link structure web pages search results user preferences ontological knowledge datalog ontology language k rank queries daq disjunctions of atomic queries top k queries data complexity;social networking online;semantic web;microblogging;homophily	In the recent years, the Web has been changing more and more towards the so-called Social Semantic Web. Rather than being based on the link structure between Web pages, the ranking of search results in the Social Semantic Web needs to be based on something new - we believe that it can be based on user preferences and underlying ontological knowledge. Modeling uncertainty is also playing an increasingly important role in these domains, since uncertainty can arise due to many uncontrollable factors. In this paper, we propose an extension of the Data log+/- ontology language with a model for representing preferences of groups of users and a model for representing the (probabilistic) uncertainty in the domain. Assuming that more probable answers are more preferable, this raises the question of how to rank query results, since the preferences of single users may be in conflict both with the probability-based preferences as well as with each other. To this end, we propose preference merging and aggregation operators, respectively, and study their semantic and computational properties. Based on these operators, we provide algorithms for answering k-rank queries for DAQs (disjunctions of atomic queries), which generalize top-k queries based on the iterative computation of classical skyline answers, and show that, under certain reasonable conditions, they run in polynomial time in the data complexity.	algorithm;computation;datalog;iterative method;multi-user;ontology (information science);social semantic web;time complexity;user (computing);world wide web	Thomas Lukasiewicz;Maria Vanina Martinez;Gerardo I. Simari;Oana Tifrea-Marciuska	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.26	web query classification;semantic search;computer science;social semantic web;data mining;database;datalog;web search query;information retrieval	AI	-23.492330099793854	6.814609671692369	187133
37e6573b0314d29977701c45a41d873b73c1d52d	on the equivalence of distributed systems with queries and communication	distributed system;rewrite rule;distributed data management;equivalence;computational complexity;xml document;optimization;active xml	Distributed data management systems consist of peers that store, exchange and process data in order to collaboratively achieve a common goal, such as evaluate some query. We study the equivalence of such systems. We model a distributed system by a collection of Active XML documents, i.e., trees augmented with function calls for performing tasks such as sending, receiving and querying data. As our model is quite general, the equivalence problem turns out to be undecidable. However, we exhibit several restrictions of the model, for which equivalence can be effectively decided. We also study the computational complexity of the equivalence problem, and present an axiomatization of equivalence, in the form of a set of equivalence-preserving rewrite rules allowing us to optimize a system by rewriting it into an equivalent, but possibly more efficient system.	axiomatic system;computational complexity theory;distributed computing;rewrite (programming);rewriting;turing completeness;undecidable problem;xml schema	Serge Abiteboul;Balder ten Cate;Yannis Katsis	2011		10.1145/1938551.1938570	equivalence;xml;computer science;equivalence partitioning;theoretical computer science;data mining;database;programming language;computational complexity theory;algorithm	DB	-22.18995752840938	10.990146531420228	188283
0f02864aa53589f89099e6025638ce2869fcc816	scaling up inductive logic programming by learning from interpretations	decision tree learning;decision tree;knowledge discovery in databases;top down;discovery;inductive logic programming;large data sets;relational database;data mining;scaling up;expressive power;first order;first order decision trees;knowledge discovery in database;learning from interpretations	When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently. Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting). As a case study, we present two alternative implementations of the ILP system TILDE (Top-down Induction of Logical DEcision trees): TILDEclassic, which loads all data in main memory, and TILDELDS, which loads the examples one by one. We experimentally compare the implementations, showing TILDELDS can handle large data sets (in the order of 100,000 examples or 100 MB) and indeed scales up linearly in the number of examples.	algorithm;association rule learning;cluster analysis;computer data storage;decision tree;experiment;extrapolation;inductive logic programming;inductive reasoning;megabyte;scalability;tilde	Hendrik Blockeel;Luc De Raedt;Nico Jacobs;Bart Demoen	1999	Data Mining and Knowledge Discovery	10.1023/A:1009867806624	multi-task learning;inductive bias;statistical relational learning;decision tree learning;relational database;computer science;machine learning;decision tree;top-down and bottom-up design;first-order logic;data mining;inductive programming;logic programming;expressive power;algorithm	ML	-25.034489940791865	7.069024934275503	189245
b53123a9638d2d0d1f0fa76f612d637a53613a63	conflicts handling in cooperative intrusion detection: a description logic approach	topology;conflicts;description logics;network analysers;semantics;intrusion detection;network analyzer;partial lexicographic inference conflicts handling cooperative intrusion detection intrusion detection system ids network analyzer vulnerability analyzer;vulnerability analyzer;conflicts description logics intrusion detection;cooperative systems;cooperative intrusion detection;cognition;lenses;xml;ids;conflicts handling;description logic;intrusion detection xml cognition topology correlation lenses semantics;correlation;security of data cooperative systems network analysers;partial lexicographic inference;security of data;intrusion detection system	In cooperative intrusion detection, several intrusion detection systems (IDS), network analyzers, vulnerability analyzers and other analyzers are deployed in order to get an overview of the system under consideration. In this case, the definition of a shared vocabulary describing the different information is prominent. Since these pieces of information are structured, we first propose to use description logics which ensure the reasoning decidability. Besides, the analyzers used in cooperative intrusion detection are not totally reliable. The second contribution of this paper is to handle these inconsistencies induced by the use of several analyzers using the so-called partial lexicographic inference.	description logic;intrusion detection system;lexicographical order;vocabulary	Safa Yahi;Salem Benferhat;Tayeb Kenaza	2010	2010 22nd IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2010.128	intrusion detection system;computer science;artificial intelligence;theoretical computer science;data mining;semantics;computer security	Robotics	-21.8919201593449	6.084382914911694	189619
d0e01f883850791908a7b09cd768e354b8f4ea89	a unified formalism for fuzzy data types representation	unified formalism;web ontology language fuzzy data type representation xml schema owl;xml schema;owl;xml data handling fuzzy set theory knowledge representation languages ontologies artificial intelligence;fuzzy data;fuzzy description logic;data type;fuzzy set theory;ontologies artificial intelligence;customized fuzzy data type;knowledge representation languages;web ontology language;xml owl fuzzy logic semantic web fuzzy systems fuzzy sets ontologies best practices educational institutions information science;best practices;customized fuzzy data type fuzzy description logic fuzzy data type representation unified formalism;xml;semantic web;ontologies;data handling;description logic;fuzzy data type representation;reflection	This paper mainly discusses the fuzzy extension to the data type representation mechanism of the existing XML schema and OWL. In accordance with the needs of practical application, we prompt a kind of fuzzy extension form to XML schema data typing and point out the limitations of OWL data typing. Furthermore, we mainly investigate the description logic playing as the underlining theory counterpart of the corresponding Web ontology language, and make the following contributions: (i) define the fuzzy data type group and the general formalism to represent fuzzy data type information; (ii) define the fuzzy data type query questions based on the fuzzy data type group and discuss the decidability of these query questions.	data (computing);description logic;fuzzy logic;ontology (information science);semantics (computer science);web ontology language;xml schema	Hailong Wang;Zongmin Ma;Li Yan;Jingwei Cheng	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.487	fuzzy logic;fuzzy classification;computer science;artificial intelligence;fuzzy number;theoretical computer science;neuro-fuzzy;data mining;database;fuzzy associative matrix;web ontology language;fuzzy set operations	DB	-22.668147231553036	6.059757571284015	190409
e2cd8dd273140bcce6f549ed6484da2aa4039f73	semigroup techniques in recursive query optimization	query optimization	Because first-order relational database query languages lack expressive power [AhU179], the use of recursion as a query primitive has received increasing attention in the database community [ChHa85], [GMN84], [Ullm85]. Unfortunately, the addition of recursion to a query language carries a heavy penalty when we come to query optimization, because it has been discovered [Vard88], [GMSV87] that many natural questions one would like answered about general recursive programs, even in the relatively simple Datalog context, are either combinatorially difficult [SaYaSO] or even undecidable [Vard88]. Recent authors have therefore turned to the identification of restricted classes of recursive programs for which particular optimizations can be performed efficiently [Ioan89], [Sar89a], [RSUV89] and much useful effort has been made to isolate what it is that makes a given program difficult to optimize. Here, we shall take as our starting point two ideas from recent query optimization research, program boundedness and rule commutativity, and we shall hope to explain how classical ideas from mathematical semigroup theory can be used not only to codify some previous results in the query optimization literature, but also how it can give us new results about query optimization and point directions for future research.	datalog;first-order predicate;mathematical optimization;query language;query optimization;recursion (computer science);relational database;undecidable problem;μ-recursive function	Thane E. Plambeck	1990		10.1145/298514.298553	sargable;query optimization;query expansion;boolean conjunctive query;computer science;database	DB	-24.649956727108677	10.61357789766187	190829
7d2c16c6be1c75ef92ae83a1b0cc6927c8dc26f8	dependencies to optimize ontology based data access.		Query answering in Ontology Based Data Access (OBDA) exploits the knowledge of an ontology’s TBox to deal with incompleteness of the ABox (or data source). Current query-answering techniques with DL-Lite require exponential size query reformulations, or expensive data pre-processing. Also, these techniques present severe redundancy issues when dealing with ABoxes that are already (partially) complete. It has been shown that addressing redundancy is not only required for tractable implementations of decision procedures, but may also allow for sizable improvements in execution times. Considering the previous observations, in this paper we extend the results aiming at improving query answering performance in OBDA systems that were developed in [9] for DL-LiteF , to the case where also role inclusions are present in the TBox. Specifically, we first show that we can characterize completeness of an ABox by means of dependencies, and that we can use these to optimize DL-LiteA TBoxes. Second, we show that in OBDA systems we can create ABox repositories that appear to be complete w.r.t. a significant portion of any DL-LiteA TBox. The combination of these results allows us to design OBDA systems based on DL-LiteA in which redundancy is minimal, the exponential aspect of query answering is notably reduced and that can be implemented efficiently using existing RDBMSs.	abox;adobe flash lite;cobham's thesis;data access;data pre-processing;experiment;ontology (information science);preprocessor;rdf schema;relational algebra;relational database management system;rewriting;sparql;tbox;time complexity;xfig	Mariano Rodriguez-Muro;Diego Calvanese	2011			database;ontology-based data integration;process ontology	AI	-24.610566987759444	8.321643088521146	191221
9491964f7c2f42c9eeaff641af22ddaf34684b84	reasoning and explanation in el and in expressive description logics	fundamental reasoning service;light-weight dl el;completion method;tableau method;expressive description logic;present method;expressive dls;dl system;computed subsumption relationship;reasoning method;powerful reasoning service;description logic;web ontology language;formal semantics	Description Logics (DLs) are the formalism underlying the standard web ontology language OWL 2. DLs have formal semantics which are the basis for powerful reasoning services. In this paper, we introduce the basic notions of DLs and the techniques that realize subsumption—the fundamental reasoning service of DL systems. We discuss two reasoning methods for this service: the tableau method for expressive DLs such as ALC and the completion method for the light-weight DL EL. We also present methods for generating explanations for computed subsumption relationships in these two DLs.	description logic;method of analytic tableaux;ontology (information science);semantics (computer science);subsumption architecture;web ontology language	Anni-Yasmin Turhan	2010		10.1007/978-3-642-15543-7_1	epistemology	AI	-21.12302522299658	9.188910906113112	192804
4464f1aa3de757001d00443439679b09d7536140	on the incorporation of interval-valued fuzzy sets into the bousi-prolog system: declarative semantics, implementation and applications		In this paper we analyse the benefits of incorporating interval-valued fuzzy sets into the Bousi-Prolog system. A syntax, declarative semantics and implementation for this extension is presented and formalised. We show, by using potential applications, that fuzzy logic programming frameworks enhanced with them can correctly work together with lexical resources and ontologies in order to improve their capabilities for knowledge representation and reasoning.	automatic equipment identification;declarative programming;fuzzy logic;fuzzy set;knowledge representation and reasoning;logic programming;modeling language;olami–feder–christensen model;ontology (information science);programming language;prolog;somos sequence;soundness (interactive proof);unicast	Clemente Rubio-Manzano;Martin Pereira-Fariña	2017	CoRR		knowledge representation and reasoning;fuzzy logic;syntax;natural language processing;semantics;fuzzy set;ontology (information science);computer science;prolog;artificial intelligence	AI	-19.860080902366	7.260549200631218	193504
28c96b275b5f1004ebb29c4154913681099ed2ee	the role of abduction in database view updating	abduction;non monotonic reasoning;view updates;deductive databases	The problem of view updating in databases consists in modifying the extension of view relations (i.e., relations defined in terms of “base” ones) transforming only the content of the extensional database, i.e., the extensional representation of base relations. This task is non-deductive in nature and its relationships with non-monotonic reasoning, and specifically with abduction, have been recently pointed out. In the paper we investigate the role of abduction in view updating, singling out similarities and differences between view updating and abduction. View updating is regarded as a two-step process: first, view definitions (and constraints) are used to reduce a view update into updates on base relations; then, the content of the extensional database is taken into account to determine the actual transformations to be performed. The first step is abductive in nature; we apply to such a step a definition of abduction based on deduction, which characterizes by means of a unique logical formula the conditions on base predicates which accomplish an update request. We then show how, in the second step, the set of transactions to be performed can be obtained from the formula generated in the first step. We provide a formal result showing the correctness of the approach.	abductive reasoning;correctness (computer science);database transaction;natural deduction;non-monotonic logic;view (sql);well-formed formula	Luca Console;Maria Luisa Sapino;Daniele Theseider Dupré	1995	Journal of Intelligent Information Systems	10.1007/BF00961655	abductive reasoning;computer science;artificial intelligence;data mining;database;algorithm	DB	-25.224164995886614	8.856951753831364	194368
98011c922a85defa64afcc422b229249d95448e6	pattern alternatives for referring to multiple indirectly specified objects		We introduce the general problem of capturing references to multiple objects that are indirectly specified via their type and/or relationship to an explicit object. We describe the setting in an abstract way, provide three alternative patterns for representing it in OWL (based on an existential restriction, a placeholder individual, and a shortcut property) and discuss their comparative advantages and disadvantages. The patterns are also aligned with the closest one among the popular logical pattern families, classes as property value, and other related research.	existential quantification;keyboard shortcut;web ontology language	Vojtech Svátek;Jan Kluka;Miroslav Vacura;Martin Homola	2017			computer science	ML	-23.168006898024	9.975919636381516	194468
a381d979eb8dbfe2d88f05542f0329d907c4e6c7	constraint-based morpho-phonology		"""In this paper, we develop a new generative paradigm with which to capture phonological generalizations. Our framework differs from standard generative frameworks inasmuch as we eschew all derivational analyses. Thus, we dispense with procedural transformations of underlying and intermediate representations into surface forms by means of the cyclic application of relatively unconstrained context-sensitive rewriting rules, lnstead, we adopt a strictly monostratal approach, wherein a single level of articulatory representation is subject to linguistic constraints expressed declaratively using well-understood logical tools. In order for our enterprise to succeed, we will require a rich representational system. To this end, we follow the lead of autosegmental and metrical phonology, taking our representations to be organized around natural groupings of articulators. A further similarity to autosegmental analyses and some traditional generative analyses is that we allow underspecification in our lexical rcpresentationsl But in contrast to these other theories, we adopt a single, concrete, surface-based representational system, rather than abstract underlying and intermediate representations of uncertain status. In particular, our approach is strictly monotonic, disallowing stages of analysis in which ill-formed representations are constructed and repaired. Instead, the linguistic constraints we impose, both universal and parochial, combined with possibly underspecified lexical representat.ions, conspire to fully determine surface representations. The result is a fully declarative system, albeit one which can be provided with a procedural interpretation in which lexicai (syntactic and semantic) representatk)ns are incrementally refined into surface representations, or vice-versa, by the application of constraints, citlmr sc~q,cntially or in parallel. Wc haw"""" chosen to employ feature structures for our phonological rel)rcsentations, a natural candidate for cu,straiut-based linguistic theories. Feature structures provide two mechanisms for constructing linguistic representations. The first is a multiple inheritance hierarchy of types, which allows the multi-dimensional classification of structures. The second mechanism is that of fi~atures, whose values are themselves modeled by feat . r e structures."""	context-sensitive grammar;multiple inheritance;programming paradigm;rewriting;theory	Michael Mastroianni;Bob Carpenter	1994			natural language processing;generative grammar;metrical phonology;underspecification;syntax;phonology;hierarchy;generalization;rewriting;mathematics;artificial intelligence	NLP	-20.757130351232856	4.797449046835193	194514
4d44f91d468bfe78b71b280897565458047b521a	minimizing human effort in reconciling match networks	schema matching;reconciliation	Schema and ontology matching is a process of establishing correspondences between schema attributes and ontology concepts, for the purpose of data integration. Various commercial and academic tools have been developed to support this task. These tools provide impressive results on some datasets. However, as the matching is inherently uncertain, the developed heuristic techniques give rise to results that are not completely correct. In practice, post-matching human expert effort is needed to obtain a correct set of correspondences. We study this post-matching phase with the goal of reducing the costly human effort. We formally model this human-assisted phase and introduce a process of matching reconciliation that incrementally leads to identifying the correct correspondences. We achieve the goal of reducing the involved human effort by exploiting a network of schemas that are matched against each other. We express the fundamental matching constraints present in the network in a declarative formalism, Answer Set Programming that in turn enables to reason about necessary user input. We demonstrate empirically that our reasoning and heuristic techniques can indeed substantially reduce the necessary human involvement.	answer set programming;baseline (configuration management);exploit (computer security);foreign key;formal language;functional dependency;heuristic;impedance matching;interaction;ontology alignment;semantics (computer science);stable model semantics	Nguyen Quoc Viet Hung;Tri Kurniawan Wijaya;Zoltán Miklós;Karl Aberer;Eliezer Levy;Victor Shafran;Avigdor Gal;Matthias Weidlich	2013		10.1007/978-3-642-41924-9_19	computer science;knowledge management;machine learning;data mining;database	AI	-23.29445088437584	7.7069694782733515	194590
3c7b6096048db391a36fc8aaee564a30eb57565c	on lossless transformation of database schemes not necessarily satisfying universal instance assumption	lossless tranformation;arbitrary relational expression;relational scheme;relational mapping;relational expression;algorithm work;lossless transformation;database scheme;open world assumption;inclusion dependency;universal instance assumption;multirelational database scheme;databases scheme	"""Given a multirelational database scheme and a relational mapping f transforming it, an important question is whether the resulting scheme is equivalent to the original one. This question was addressed in the literature with respect to those relational schemes that satisfy the so called universal relation assumption; however, no study was ever concerned with multirelational (data base) schemes that do not necessarily satisfy this assumption.We present two general definitions of lossless transformation of the database scheme based on the so-called closed world and open world assumptions. While both definitions seem to be practically justified, the one based on the open world assumption is more """"tractable"""" We are able to test losslessness defined in such a way for a wide class of relational expressions and dependencies. An algorithm for testing losslessness of a mappings (which are arbitrary relational expressions built up from projections, cartesian products and restrictions) is presented in the paper. Moreover, given a lossless transformation, our algorithm enables us to explicitly construct an """"inverted"""" mapping that restores the corresponding state of the original database. The application of the algorithm to schemes specified by differrent types of dependencies is described. In particular the application of the algorithm for schemes specified by inclusion dependencies is presented. In this case the algorithm works for families of inclusion dependencies having finite chase property. This class of inclusion dependencies is characterized in the paper."""	algorithm;cobham's thesis;database;lossless compression;object-relational mapping;open world;open-world assumption;referential integrity;universal relation assumption	Tomasz Imielinski;Nicolas Spyratos	1984		10.1145/588011.588049	discrete mathematics;dependency theory;theoretical computer science;database;mathematics;algorithm	DB	-25.84574267250371	10.354900206088338	194939
1ecf1229d6a01d0fc488ce3e63d1e22a85e9180c	object histories which avoid certain subsequences	donnee historique;data base query;base donnee;coaccion;interrogation base donnee;contrainte;database;interrogacion base datos;functional dependency;constraint;dependance fonctionnelle;triple;base datos;dependencia funcional;representabilite;historical data;representability	In an earlier paper, one of the authors introduced a record-based model for describing historical data for objects (here called “object histories”). The major construct in the model is a computation-tuple sequence scheme (abbreviated CSS) which specifies the set of all “valid object histories for the same type of object. In follow-up articles, the effects of interval queries and projections on object histories were examined. Now, one of the components in a CSS is a finite set of constraints on object histories. In the present investigation the notion of bad-subsequence constraint is delined and CSS in which each constraint is of this kind are studied. (A bad-subsequence constraint e is specilied by a given set g of object histories. An object history ti satislies e if ri has no subsequence which is a sequence in 4.) Among the results are the following: (i) Necessary and sufhcient conditions for a set of object histories described by a given CSS to be described by another CSS having only bad-subsequence constraints, i.e., when a given CSS is bad-subsequence representable; (ii) a characterization for when a bad-subsequence-representable CSS is also locally representable (in the sense of one of the earlier papers); and (iii) connections of bad-subsequence representability with functional-dependency representability. It? 1987 Academvz Press. Inc.	cascading style sheets;computation;functional dependency;interval arithmetic	Seymour Ginsburg;Marc Gyssens	1987	Inf. Comput.	10.1016/0890-5401(87)90019-8	computer science;artificial intelligence;mathematics;functional dependency;constraint;algorithm	DB	-26.32466564192798	11.164155621472466	195339
5ddb1314858b93134c40a7eba018704041605603	semantic formalization of interactive reasoning functionality	fonctionalite raisonnement interactif;semantics;base connaissance;logique propositionnelle;systeme base connaissances;semantica;semantique;article letter to editor;propositional logic;base conocimiento;logica proposicional;knowledge based systems;knowledge base	In this paper a semantical framework is developed that provides a logical description of the functionality of an interactive reasoning process. The concept of functionality description defines the functionality of a reasoning process abstracting from specific inference relations or knowledge bases. Moreover, a domain description is formalised. A number of properties of a functionality description are identified, and related to properties of the domain. It is established under which conditions a functionality, can be implemented by an inference relation and a knowledge base.	knowledge base	Jan Treur	2002	Int. J. Intell. Syst.	10.1002/int.10043	natural language processing;knowledge base;computer science;artificial intelligence;semantics;propositional calculus;algorithm	AI	-19.866168584332186	7.521549886654749	195574
cfa41dec2a8b6456223ea8f74d8f777a5c219bb4	using a distributional semantic vector space with a knowledge base for reasoning in uncertain conditions		The inherent inflexibility and incompleteness of commonsense knowledge bases (KB) has limited their usefulness. We describe a system called Displacer for performing KB queries extended with the analogical capabilities of the word2vec distributional semantic vector space (DSVS). This allows the system to answer queries with information which was not contained in the original KB in any form. By performing analogous queries on semantically related terms and mapping their answers back into the context of the original query using displacement vectors, we are able to give approximate answers to many questions which, if posed to the KB alone, would return no results. We also show how the hand-curated knowledge in a KB can be used to increase the accuracy of a DSVS in solving analogy problems. In these ways, a KB and a DSVS can make up for each other’s weaknesses.	approximation algorithm;commonsense knowledge (artificial intelligence);displacement mapping;knowledge base;word2vec	Douglas Summers-Stay;Clare R. Voss;Taylor Cassidy	2016	CoRR		knowledge base;analogy;computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm	AI	-23.50950972137092	7.241048059681543	196482
14cbe19c6239be6d0365eda332471547a9837f70	omreasoner: combination of multi-matchers for ontology matching: results for oaei 2014		Ontology matching produces correspondences between entities of two ontologies. The OMReasoner is unique in that it creates an extensible framework for combination of multiple individual matchers, and reasons about ontology matching by using external dictionary WordNet and description logic reasoner. It handles ontology matching in both literal and semantic level, and it makes use of the semantic part of OWL-DL as well as structure. This paper describes the result of OMReasoner in the OAEI 2014 competition in three tracks: benchmark, conference, and MultiFarm. 1 Presentation of the system Ontology matching finds correspondences between semantically related entities of the ontologies. It plays a significant role in many application domains. Many approaches to ontology matching have been proposed: the implementation of match may use multiple match algorithms or matchers, and the following largelyorthogonal classification criteria are considered [1-3]: schema-level and instance-level, element-level and structure-level, syntactic and semantic, language-based and constraint-based. Many approaches focus on syntactic aspects instead of semantic ones. OMReasoner achieves the matching by means of some external dictionary and reasoning techniques. Still, this approach includes strategy of combination of (mainly syntactical) multi-matchers (e.g., EditDistance matcher) before match reasoning. 1.1 State, purpose, general statement The matching process can be defined as a function f. A’=f(O1, O2, A, p, r) Where O1 and O2 are a pair of ontologies as input to match, A is the input alignment between these ontologies and A’ is new alignment returned, p is a set of parameters (e.g., weight w and threshold τ) and r is a set of oracles and resources. Alignments express correspondences between two entities. A correspondence must express two corresponding entities and the relation that is supposed to hold between them. Given two ontologies, a correspondence is a 5-tuple:<id,e1,e2,R,n>, where . id is a unique identifier of the given correspondence; . e1 and e2 are the entities of the first and the second ontology respectively; . R is a relation (e.g., equivalence(=), more general(>), less general(<), disjointness  )) holding between e1 and e2. In OAEI campaign, equivalence is mainly considered; . n is a confidence measure (typically in the [0 1] range) for the correspondence between e1 and e2. C1’,C2’, R1’,R2’ p ( w,τ) r dictionary	algorithm;application domain;benchmark (computing);description logic;dictionary;entity;identifier;literal (mathematical logic);ontology (information science);ontology alignment;oracle machine;semantic reasoner;turing completeness;unique key;wordnet	Guohua Shen;Yinling Liu;Fei Wang;Jia Si;Zi Wang;Zhiqiu Huang;Dazhou Kang	2014			upper ontology;ontology alignment;ontology inference layer;computer science;ontology;theoretical computer science;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-22.051045842701644	9.808577937533409	196684
0070b18eef086390e6a11d67c838a51d02fe1739	reasoning on data models in schema translation	model generation;data model	"""We refer to the problem of translating schemas from a model to another, in a model independent framework. Specifically, we consider an approach where translations are specified as Datalog programs. In this context we show how it is possible to reason on models and schemas involved as input and output for a translation. The various notions are formalized: (i) concise descriptions of models in terms of sets of constructs, with associated propositional formulas; (ii) a notion of signature for translation rules (with the property that signatures can be automatically computed out of rules); (iii) the """"application"""" of signatures to models. The main result is that the target model of a translation can be completely characterized given the description of the source model and the signatures of the rules. This result is being exploited in the framework of a tool that implements model generic translations."""	data model;xml schema	Paolo Atzeni;Giorgio Gianforme;Paolo Cappellari	2008		10.1007/978-3-540-77684-0_13	natural language processing;data model;computer science;data mining;database;mathematics;algorithm	DB	-23.849345437406413	10.167341137501602	197504
9507c633b0179c9fe3c4ec0b1df065aa5cca747f	discovering new knowledge from graph data using inductive logic programming	systeme intelligent;procesamiento informacion;adquisicion del conocimiento;semantica formal;systeme apprentissage;sistema inteligente;inductive logic programming;logical programming;acquisition connaissances;formal semantics;semantique formelle;polynomial time algorithm;learning systems;first order;programmation logique;knowledge acquisition;information processing;consistency checking;intelligent system;graph algorithm;inductive inference;logic programs;knowledge representation;traitement information;programacion logica;structured data;knowledge discovery	We present a method for discovering new knowledge from structural data which are represented by graphs in the framework of inductive logic programming. A graph, or network, is widely used for representing relations between various data and expressing a small and easily understandable hypothesis. Formal Graph System (FGS) is a kind of logic programming system which directly deals with graphs just like first order terms. By employing refutably inductive inference algorithms and graph algorithmic techniques, we are developing a knowledge discovery system KD-FGS, which acquires knowledge directly from graph data by using FGS as a knowledge representation language.#R##N##R##N#In this paper we develop a logical foundation of our knowledge discovery system. A term tree is a pattern which consists of variables and treelike structures. We give a polynomial-time algorithm for finding a unifier of a term tree and a tree in order to make consistency checks efficiently. Moreover we give experimental results on some graph theoretical notions with the system. The experiments show that the system is useful for finding new knowledge.	inductive logic programming	Tetsuhiro Miyahara;Takayoshi Shoudai;Tomoyuki Uchida;Tetsuji Kuboyama;Kenichi Takahashi;Hiroaki Ueda	1999		10.1007/3-540-48751-4_21	spqr tree;knowledge representation and reasoning;null model;graph product;null graph;information processing;data model;computer science;clique-width;artificial intelligence;theoretical computer science;inductive reasoning;machine learning;formal semantics;first-order logic;abstract semantic graph;database;mathematics;graph;moral graph;critical graph;programming language;intersection graph;algorithm;tree decomposition;graph rewriting	AI	-22.42328586610497	4.195967715171114	197518
415ec5a8cdf4bf9cbc115ad031b75bf7251c3333	learning schema mappings	schema mappings;learning model;efficient algorithm;data exchange;schema mapping;global as view;learning theory;computational learning theory;probably approximately correct	A schema mapping is a high-level specification of the relationship between a source schema and a target schema. Recently, a line of research has emerged that aims at deriving schema mappings automatically or semi-automatically with the help of data examples, i.e., pairs consisting of a source instance and a target instance that depict, in some precise sense, the intended behavior of the schema mapping. Several different uses of data examples for deriving, refining, or illustrating a schema mapping have already been proposed and studied.  In this paper, we use the lens of computational learning theory to systematically investigate the problem of obtaining algorithmically a schema mapping from data examples. Our aim is to leverage the rich body of work on learning theory in order to develop a framework for exploring the power and the limitations of the various algorithmic methods for obtaining schema mappings from data examples. We focus on GAV schema mappings, that is, schema mappings specified by GAV (Global-As-View) constraints. GAV constraints are the most basic and the most widely supported language for specifying schema mappings.  We present an efficient algorithm for learning GAV schema mappings using Angluin's model of exact learning with membership and equivalence queries. This is optimal, since we show that neither membership queries nor equivalence queries suffice, unless the source schema consists of unary relations only. We also obtain results concerning the learnability of schema mappings in the context of Valiant's well known PAC (Probably-Approximately-Correct) learning model. Finally, as a byproduct of our work, we show that there is no efficient algorithm for approximating the shortest GAV schema mapping fitting a given set of examples, unless the source schema consists of unary relations only.	algorithm;computational learning theory;database schema;high- and low-level;learnability;probably approximately correct learning;semiconductor industry;turing completeness;unary operation;user error;xml schema	Balder ten Cate;Víctor Dalmau;Phokion G. Kolaitis	2012		10.1145/2274576.2274596	data exchange;information schema;schema;semi-structured model;computer science;conceptual schema;theoretical computer science;learning theory;star schema;data mining;xml schema;database;mathematics;document schema definition languages;computational learning theory;superkey;database schema;probably approximately correct learning;algorithm	DB	-25.45329885156628	9.605982715105798	197698
9a7ef2ef2d4c81e637ceeb06476b3d22a2dedee9	polynomial algorithms for computing a single preferred assertional-based repair		This paper investigates different approaches for handling inconsistent DL-Lite knowledge bases in the case where the assertional base is prioritized and inconsistent with the terminological base. The inconsistency problem often happens when the assertions are provided by multiple conflicting sources having different reliability levels. We propose different inference strategies based on the selection of one consistent assertional base, called a preferred repair. For each strategy, a polynomial algorithm for computing the associated single preferred repair is proposed. Selecting a unique repair is important since it allows an efficient handling of queries. We provide experimental studies showing (from a computational point of view) the benefits of selecting one repair when reasoning under inconsistency in lightweight knowledge bases.	adobe flash lite;algorithm;computation;consistency model;deductive language;knowledge base;lightweight ontology;ontology (information science);polynomial	Abdelmoutia Telli;Salem Benferhat;Mustapha Bourahla;Zied Bouraoui;Karim Tabia	2016	KI - Künstliche Intelligenz	10.1007/s13218-016-0466-4	theoretical computer science;data mining;algorithm	AI	-24.05952692466062	7.899333492832623	198200
07a1f39731969d9611ce7db25982758777ff8c9b	formal semantics-preserving translation from fuzzy er model to fuzzy owl dl ontology	fuzzy owl dl ontology;formal semantics;ontology learning;description logic;reasoning;fuzzy er model;fuzzy database	Ontology is an important part of the W3C standards for the Semantic Web, and how to quickly and cheaply construct Web ontologies has become a key technology to enable the Semantic Web. However, information imprecision and uncertainty exist in many real-world applications, thus constructing fuzzy ontology by extracting domain knowledge from fuzzy database models (e.g., fuzzy ER model) can profitably support fuzzy ontology development. In this paper, we propose an approach for constructing fuzzy ontology from fuzzy ER model, in which the fuzzy ontology consists of fuzzy ontology structure and instances. Firstly, we give the formal definition and the semantics of fuzzy ER models. Then, we introduce the fuzzy extension of ontology language OWL DL, i.e., fuzzy OWL DL. Based on the fuzzy OWL DL, a kind of fuzzy ontology called fuzzy OWL DL ontology is presented. Furthermore, we consider the fuzzy ER schema and the corresponding database instances, and translate them into the fuzzy ontology structure and the fuzzy ontology instances, respectively. Finally, since a fuzzy OWL DL ontology is equivalent to a fuzzy Description Logic f-SHOIN(D) knowledge base, how the reasoning problems of fuzzy ER models (e.g., satisfiability, subsumption, and redundancy) may be reduced to reasoning on f-SHOIN(D) knowledge bases is investigated, which will further contribute to constructing fuzzy OWL DL ontology that exactly meet application's needs. Of course, the correctness of the translation and reasoning problems are proved completely.	entity–relationship model	Zongmin Ma;Fu Zhang;Li Yan;Yanhui Lv	2010	Web Intelligence and Agent Systems	10.3233/WIA-2010-0199	fuzzy logic;upper ontology;description logic;ontology inference layer;adaptive neuro fuzzy inference system;fuzzy classification;computer science;knowledge management;ontology;artificial intelligence;neuro-fuzzy;formal semantics;data mining;database;fuzzy associative matrix;ontology-based data integration;owl-s;process ontology;reason;suggested upper merged ontology	AI	-22.71131272277463	6.594827322434951	198598
4fe5e41b0bf045ef5fbe960c373585bee2309081	active learning of gav schema mappings		"""Schema mappings are syntactic specifications of the relationship between two database schemas, typically called the source schema and the target schema. They have been used extensively in formalizing and analyzing data inter-operability tasks, especially data exchange and data integration. There is a growing body of research on deriving schema mappings from data examples, that is, pairs of source and target instances that depict the behavior of the unknown schema mapping. One of the approaches used in this endeavor casts the derivation of a schema mapping from data examples as a learning problem. Earlier work has shown that GAV mappings (global-as-view schema mappings) are learnable in Angluin's model of exact learning with membership queries and equivalence queries. Here, we validate the practical applicability of this theoretical result by designing and implementing an active learning algorithm, called GAV-Learn that derives a syntactic specification of a GAV mapping from a given set of data examples and from a """"black-box"""" implementation. We analyze the properties of GAV-Learn and, among other results, we show that it produces a GAV mapping that has minimal size and is a good approximation of the unknown GAV mapping. Furthermore, we carry out a detailed experimental evaluation that demonstrates the effectiveness of GAV-Learn along different metrics. In particular, we compare GAV-Learn with two earlier approaches for deriving GAV mappings from data examples, and establish that it performs significantly better than the two baselines."""	active learning (machine learning);algorithm;approximation;black box;database schema;interoperability;operability;schema evolution;turing completeness;type conversion	Balder ten Cate;Phokion G. Kolaitis;Kun Qian;Wang Chiew Tan	2018		10.1145/3196959.3196974	theoretical computer science;data integration;equivalence (measure theory);active learning;syntax;conformance testing;computer science;data exchange;schema (psychology);database schema	DB	-25.470311793903864	9.590654380468557	199005
097cfd2aefef610e76a9c7cd91e5226dbc9066e5	qualitative spatial reasoning using topological and directional information in owl	owl;direction relations;formal specification;sowl reasoner qualitative spatial reasoning topological information directional information web ontology language reasoning approach choros reasoning engine ontology pellet spatial cone shaped directional logic formalism csd logic formalism region connection calculus rdf semantic relation resource description framework;pellet;semantics;inference mechanisms;resource description framework;ontologies artificial intelligence;spatial ontology;formal verification;specification languages;calculus;cognition owl ontologies calculus resource description framework semantics optimization;cognition;ontologies;optimization;reasoning pellet spatial ontology topologic relations direction relations;reasoning;topologic relations;specification languages formal specification formal verification inference mechanisms ontologies artificial intelligence	We investigate on potential improvements to reasoning approaches designed for spatial information in OWL. First, we introduce CHOROS, a qualitative spatial reasoning engine for ontologies in OWL. Building upon Pellet Spatial, CHOROS supports consistency checking and query answering for spatial information using Region-Connection Calculus (RCC), but also using the Cone-Shaped Directional (CSD) logic formalism. It works with all RCC and CSD relations in combination with standard RDF/OWL semantic relations in an OWL ontology and can answer SPARQL queries with spatial and non-spatial relations. We also present SOWL, a spatial reasoner for both relation calculi implemented in SWRL and runs under Pellet. We discuss and evaluate possible optimizations of CHOROS and compare its performance with that of SOWL. The experimental results demonstrate that CHOROS runs significantly faster than its respective SWRL implementation in most cases.	best, worst and average case;cambridge structural database;cubic function;java;local consistency;ontology (information science);out of memory;region connection calculus;sparql;scalability;semantic web rule language;semantic reasoner;semantics (computer science);spatial–temporal reasoning;web ontology language	George Christodoulou;Euripides G. M. Petrakis;Sotiris Batsakis	2012	2012 IEEE 24th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2012.86	cognition;formal verification;computer science;ontology;artificial intelligence;theoretical computer science;pellet;rdf;data mining;formal specification;database;semantics;reason	AI	-21.732562724947606	9.865777945278614	199357
