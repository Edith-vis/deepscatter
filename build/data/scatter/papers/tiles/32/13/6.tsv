id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
8531613645df93b5feddb1c658e229972a71dd04	large-scale text to image retrieval using a bayesian k-neighborhood model	bayesian approach;large scale;image retrieval	In this paper we introduce a new approach aimed at solving the problem of image retrieval from text queries. We propose to estimate the word relevance of an image using a neighborhood-based estimator. This estimation is obtained by counting the number of word-relevant images among the K-neighborhood of the image. To this end a Bayesian approach is adopted to define such a neighborhood. The local estimations of all the words that form a query are naively combined in order to score the images according to that query. The experiments show that the results are better and faster than the state-of-the-art techniques. A special consideration is done for the computational behaviour and scalability of the proposed approach.	computation;experiment;image retrieval;neighborhood operation;relevance;scalability	Roberto Paredes	2010		10.1007/978-3-642-14980-1_47	visual word;computer science;pattern recognition;data mining;information retrieval	Vision	-22.94035135001692	-62.47116681557231	148805
6d4f88c165a0b64b5020035525fb1dfbe31fa39e	simple and accurate feature selection for hierarchical categorisation	digital documents;hierarchical structure;categorisation;error reduction;top down;web hierarchies;machine learning;feature selection;hierarchical categorisation	Categorisation of digital documents is useful for organisation and retrieval. While document categories can be a set of unstructured category labels, some document categories are hierarchically structured. This paper investigates automatic hierarchical categorisation and, specifically, the role of features in the development of more effective categorisers. We show that a good hierarchical machine learning-based categoriser can be developed using small numbers of features from pre-categorised training documents. Overall, we show that by using a few terms, categorisation accuracy can be improved substantially: unstructured leaf level categorisation can be improved by up to 8.6%, while top-down hierarchical categorisation accuracy can be improved by up to 12%. In addition, unlike other feature selection models --- which typically require different feature selection parameters for categories at different hierarchical levels --- our technique works equally well for all categories in a hierarchical structure. We conclude that, in general, more accurate hierarchical categorisation is possible by using our simple feature selection technique.	categorization;document;feature selection;machine learning;top-down and bottom-up design	Wahyu Wibowo;Hugh E. Williams	2002		10.1145/585058.585079	computer science;machine learning;pattern recognition;top-down and bottom-up design;data mining;feature selection	NLP	-20.69209392815503	-64.21881001706336	149899
56c015783f7e3ce71193a738261effe911df0141	boosting and combination of classifiers for natural language call routing systems	similarity metric;combination of classifiers;vector space model;information retrieval;synergistic effect;constrained minimization;boosting;linear interpolation;natural language;classification error;discriminative training;classification accuracy;call routing;relevance feedback	In this paper, we present different techniques to improve natural language call routing. We first describe methods to improve a single classifier: boosting, discriminative training (DT) and automatic relevance feedback (ARF). An interesting feature of some of these algorithms is the ability to re-weight the training data in order to focus the classifier on documents judged difficult to classify. We explore ways of deriving and combining uncorrelated classifiers in order to improve accuracy; we discuss specifically the linear interpolation and the constrained minimization techniques. All these approaches are probabilistic and are inspired from the information retrieval domain. They are evaluated using two similarity metrics, a common cosine measure from the vector space model, and a beta measure which had given good results in the similar task of e-mail steering.#R##N##R##N#Compared to the baseline classifiers, we show an interesting improvement in the classification accuracy on call routing for a banking task: up to 20% reported for the ARF method, up to 30% for the boosting technique, and more than 45% for the DT approach. Another relative improvement of 11% is also obtained when we combine the classifiers with the constrained minimization approach using a confusion measure and DT. More importantly, synergistic effects of DT on the boosting algorithm were demonstrated: more iterations were possible because DT reduced the classification error rate of individual classifiers trained on re-weighted data by an average of 72%.	boosting (machine learning);natural language;routing	Imed Zitouni;Hong-Kwang Jeff Kuo;Chin-Hui Lee	2003	Speech Communication	10.1016/S0167-6393(03)00103-1	random subspace method;speech recognition;computer science;machine learning;pattern recognition;data mining;natural language;linear interpolation;vector space model;boosting	ML	-19.545634689244288	-63.01041591756012	150059
d98155df1abb40ef05f953c0f52f273e0ea627d2	disambiguation of named entities in cultural heritage texts using linked data sets	named entity disambiguation centrality linked data data fusion digital humanities	This paper proposes a graph-based algorithm baptized REDEN for the disambiguation of authors’ names in French literary criticism texts and scientific essays from the 19th century. It leverages knowledge from different Linked Data sources in order to select candidates for each author mention, then performs fusion of DBpedia and BnF individuals into a single graph, and finally decides the best referent using the notion of graph centrality. Some experiments are conducted in order to identify the best size of disambiguation context and to assess the influence on centrality of specific relations represented as edges. This work will help scholars to trace the impact of authors’ ideas across different works and time periods.	entity;linked data;named-entity recognition;word-sense disambiguation	Carmen Brando;Francesca Frontini;Jean-Gabriel Ganascia	2015		10.1007/978-3-319-23201-0_51	natural language processing;computer science;data mining;database;information retrieval	NLP	-24.373527678066566	-59.49152805393044	150764
841744a44512440daff6cd0b7a6f886ab3b4c22a	wsf2: a novel framework for filtering web spam		Over the last years, research on web spam filtering has gained interest from both academia and industry. In this context, although there are a good number of successful antispam techniques available (i.e., content-based, link-based, and hiding), an adequate combination of different algorithms supported by an advanced web spam filtering platform would offer more promising results. To this end, we propose theWSF2 framework, a new platform particularly suitable for filtering spam content on web pages. Currently, our framework allows the easy combination of different filtering techniques including, but not limited to, regular expressions and well-known classifiers (i.e., Naı̈ve Bayes, Support Vector Machines, and C5.0). Applying our WSF2 framework over the publicly available WEBSPAM-UK2007 corpus, we have been able to demonstrate that a simple combination of different techniques is able to improve the accuracy of single classifiers on web spam detection. As a result, we conclude that the proposed filtering platform is a powerful tool for boosting applied research in this area.	adaboost;anti-spam techniques;benchmark (computing);c4.5 algorithm;dnsbl;email filtering;extensibility;heuristic (computer science);library (computing);mathematical optimization;naive bayes classifier;parsing;plug-in (computing);regular expression;relevance;sensor;software engineering;spamassassin;spamdexing;spamming;support vector machine;uniform resource identifier;web content;web page;web search engine	Jorge Fdez-Glez;David Ruano-Ordás;Rosalía Laza;José Ramon Méndez;Reyes Pavón;Florentino Fernández Riverola	2016	Scientific Programming	10.1155/2016/6091385	computer science;data mining;world wide web;information retrieval	Web+IR	-21.003183516655916	-61.00583044298825	150925
f3e50289fd215d7159791545f641c2076752a74e	emfet: e-mail features extraction tool		EMFET is an open source and flexible tool that can be used to extract a large number of features from any email corpus with emails saved in EML format. The extracted features can be categorized into three main groups: header features, payload (body) features, and attachment features. The purpose of the tool is to help practitioners and researchers to build datasets that can be used for training machine learning models for spam detection. So far, 140 features can be extracted using EMFET. EMFET is extensible and easy to use. The source code of EMFET is publicly available at GitHub (https://github.com/WadeaHijjawi/EmailFeaturesExtraction)	anti-spam techniques;attachments;categorization;email;emotion markup language;machine learning;open-source software	Wadi' Hijawi;Hossam Faris;Ja'far Alqatawna;Ibrahim Aljarah;Ala' M. Al-Zoubi;Maria Habib	2017	CoRR		database;source code;header;extensibility;payload;computer science	NLP	-20.32603759113804	-60.51242517512156	151295
159123c053fe914d825a343a589fbb22a2a8c1d8	hierarchical document classification using automatically generated hierarchy	hierarchical structure;linear discriminant projection;automatic generation;text classification;document categorization;exponential growth;hierarchy generation;document classification;text categorization	Automated text categorization has witnessed a booming interest with the exponential growth of information and the ever-increasing needs for organizations. The underlying hierarchical structure identifies the relationships of dependence between different categories and provides valuable sources of information for categorization. Although considerable research has been conducted in the field of hierarchical document categorization, little has been done on automatic generation of topic hierarchies. In this paper, we propose the method of using linear discriminant projection to generate more meaningful intermediate levels of hierarchies in large flat sets of classes. The linear discriminant projection approach first transforms all documents onto a low-dimensional space and then clusters the categories into hier- archies accordingly. The paper also investigates the effect of using generated hierarchical structure for text classification. Our experiments show that generated hierarchies improve classification performance in most cases.	categorization;document classification;experiment;linear discriminant analysis;statistical classification;time complexity	Tao Li;Shenghuo Zhu;Mitsunori Ogihara	2006	Journal of Intelligent Information Systems	10.1007/s10844-006-0019-7	exponential growth;machine learning;pattern recognition;data mining	Web+IR	-20.223899264419046	-63.66749360619678	151693
73292c3fe3f1f6aa90a6b0b2c813def961ee28a3	constructing knowledge nets from real-world natural language texts		Mining knowledge from textual data has traditionally been applied on web-based publicly available resources, such as Wikipedia, online news, scientific publications and social medial such as Facebook and Twitter. In recent years, the increased maturity and accessibility of natural language processing and text mining algorithms have now raised serious interests beyond academia to uncover the chunk of corporate knowledge buried in hard-to-process natural language descriptions and reports. Such textual data are often in larger volumes and contain more valuable than numerical data. To make unstructured textual data written in natural languages ready for use in downstream data mining tasks, a typical workflow consists of anonymisation, text normalisation, domain entity and relation recognition. Despite intensive research efforts in the past decade, this seemingly simple workflow remains challenging when facing real-world text. The dominant issues are but not limited to: • Lack of labelled training data: despite the fact that supervised learning based algorithms are more effective than unsupervised ones, it is often difficult if not impossible to obtain large amount of labelled documents. • Not-so-reproducible results: the results of current algorithms are highly dependent on the chosen combination of the pre-processing steps and their ordering. The performance are sensitive to the statistical and linguistic features chosen. • Lack of domain-independent representations: there is the lack of assurance that algorithms performing well for one text will also work for another. In this invited talk, we will take constructing knowledge nets as the objective, looking into the methods and techniques ranging from traditional feature-based distributional representation of words and phrases to the more recent deep learning enabled distributed vector representations. A demo c ©2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. WWW 2017 Companion, April 3–7, 2017, Perth, Australia. ACM 978-1-4503-4914-7/17/04. http://dx.doi.org/10.1145/3041021.3055360 . Figure 1: A Knowlege Net Constructed from Textual Records of a prototype system (as shown in Figure 1) will showcase how the knowledge net construction workflow could be realised using textual records in real world databases.	accessibility;algorithm;capability maturity model;data mining;database;deep learning;downstream (software development);level of measurement;medial graph;natural language processing;numerical analysis;preprocessor;prototype;scientific literature;supervised learning;text corpus;text mining;text normalization;www;web application;wikipedia;world wide web	Wei Liu	2017		10.1145/3041021.3055360	ontology learning;supervised learning;world wide web;data mining;computer science;deep learning;natural language;natural language processing;ranging;workflow;training set;text mining;artificial intelligence	ML	-22.075845782556843	-62.234851023824284	152230
e041db48c1ffe1d7bcb703f53b49cbdee612cf65	a sentiment analysis tool for determining the promotional success of fashion images on instagram		Sentiment Analysis (SA) or Opinion Mining is the process of analysing natural language texts to detect an emotion or a pattern of emotions towards a certain product to make a decision about that product. SA is a topic of text mining, Natural Language Processing (NLP) and web mining disciplines. Research in SA is currently at its peak given the amount of data generated from social media networks. The concept is that consumers are expressing exactly what they need, want and expect from a product but on the other hand the companies don’t have the tools to analyse and understand these feelings to satisfy these consumers accordingly. One of the applications that generate a high rate of reactions and sentiments in social networks is Instagram. This study focuses on analysing the reactions generated by the top 50 fashion houses on Instagram given their top 20 images with the highest number of likes. The approach taken in this study is to qualify the visual aesthetics of fashion images and to establish why some succeed on social media more than others. The basic question asked in this paper is whether there are certain visual aesthetics that appeal more to the user and are therefore more successful on social media than others as determined by a measure we introduce, ‘Social Value’. To do so, a sentiment analysis tool is developed to measure the proposed social value of each image. An input of comments from each image will be processed. Each comment will go through a preprocessing phase; each word will be placed through a lexicon to identify if it is positive or negative. The output of the lexicon is a score value assigned to each comment to identify its degree of positivity, negativity, or it has no effect on the social value. Adding to these results, the number of likes and shares would also be taken into consideration quantifying the image’s value. A cumulative result is then produced to determine the social value of an image.	baseline (configuration management);comment (computer programming);instagram;lexicon;machine learning;natural language processing;negativity (quantum mechanics);preprocessor;sentiment analysis;social media;social network;subject-matter expert;text mining;value (ethics);web mining	Mohamed AbdelFattah;Dahab Galal;Nada Hassan;Doaa Elzanfaly;Greg Tallent	2017	iJIM		computer science;data mining;world wide web;sentiment analysis;social value orientations	AI	-22.318791758969542	-59.52888897139611	152669
f99ff05212dc01d66c5d84b43189436c8d3c4674	combining user-based and global lexicon features for sentiment analysis in twitter	rule based fusing user based lexicon features global lexicon features sentiment analysis twitter public data stream source social media text;rule based fusing sentiment analysis feature construction user based features global features;user interfaces knowledge based systems sentiment analysis social networking online;sentiment analysis twitter bars feature extraction speech mutual information	Generally speaking, sentiment lexicons employed in the majority of current sentiment analysis systems are trained globally from public data stream source or other large independent corpus. However, sentiments are rather subjective and personal states of mind that the individuality and diversity of characteristics, particular writing habit and idiolect could play a crucial role in the judgment of sentiment expressed by a specific user. In this paper, we present a novel feature construction method to combine user-based and global lexicon features in sentiment analysis for short social media text. After the creation of user-based sentiment lexicons from user-timeline corpus, a rule-based fusing approach is adopted subsequently to generate user-based lexicon features in combination with general lexicon features. Experiments show that user-based features may capture potential user preferences hence adjusting the bias caused by representing an individual's sentiment with an averaged lexicon score, and our proposed method yield better results in comparison with some of the state-of-the-art sentiment analysis systems in twitter.	experiment;feature vector;information privacy;lexicon;logic programming;sentiment analysis;social media;timeline;user (computing)	Zhou Jin;Yujiu Yang;Xianyu Bao;Biqing Huang	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727792	natural language processing;computer science;pattern recognition;data mining;sentiment analysis	NLP	-21.203398193205512	-59.270685698390714	153079
69f37aba893bf558aa56bf98e51d8d40de82a6a0	alignment-based preprocessing of personal ontologies on semantic social network	social network;ontology alignment	"""In semantic social network, the relations between users are inferred by measuring the similarity between the corresponding personal ontologies. However, """"over-enriched"""" personal ontologies have caused some difficulties in being discriminated from other personal ontologies. For efficiently annotating resources in their own repositories, people simply append ontology fragments retrieved from standard ontologies and from other neighbors' personal ontologies along to social links. In this paper, we propose a preprocessing method to extract preferential concepts for comparing with social semantics. In order to prune out irrelevant concepts from personal ontologies, alignment-based concept classification process is designed by checking these two main criteria;  i ) redundancy (e.g., if there already exist semantically identical concepts), and  ii ) tendency (e.g., if there exist semantically declined concepts). Finally, we want to show an application scenario to demonstrate our contributions."""	ontology (information science);preprocessor;social network	Jason J. Jung;Hong-Gee Kim;GeunSik Jo	2007		10.1007/978-3-540-74827-4_33	ontology components;computer science;data mining;database;information retrieval	Web+IR	-24.570770517079854	-62.5000572268446	154140
5673c1d5650933c64b8011ef0647578cc04b22fc	using multiple resources in graph-based semi-supervised sentiment classification	adopted algorithm multiple resource graph based semisupervised sentiment classification heterogeneous mass semantic dictionaries unlabeled corpora heuristic rules graph based semisupervised algorithm similarity matrices;graph based method sentiment analysis polarity classification;sentiment analysis;polarity classification;graph based method;pattern classification graph theory	For sentiment classification, there exist a heterogeneous mass of resources such as semantic dictionaries, unlabeled corpora, and heuristic rules. In this paper, based on a graph-based semi-supervised algorithm, we focus on exploiting multiple resources to construct similarity matrices which are fused by simple but effective schemes. We reported encouraging results of the experiments in sentiment classification, which indicate that the adopted algorithm can utilize multiple resources to improve performance.	algorithm;dictionary;existential quantification;experiment;heuristic;semi-supervised learning;semiconductor industry;text corpus	Ge Xu;Houfeng Wang	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.18	computer science;machine learning;pattern recognition;data mining;sentiment analysis	AI	-19.772605696902957	-65.80307760613877	155003
b31214520ee8efb511b79ff7b6eaa60d1a675fc9	the challenge of understanding the flow of sentiments in social media documents	opinion mining;sentiment analysis;polarity;social media	This talk is focused on a key task in the area of Opinion Mining and Sentiment Analysis: polarity classification of social media documents (e.g. blog posts). Estimating polarity is much more demanding than estimating topicality. As a matter of fact, the effectiveness of polarity classification is still modest and does not compare with the effectiveness of standard retrieval tasks. Polarity estimation is severely affected by parts of the text that are off-topic or that simply do not express any opinion. In fact, the key sentiments in a document often appear in specific locations of the text. Furthermore, there are usually conflicting opinions in a given document and this mixed set of opinions harms the performance of automatic methods designed to estimate the overall orientation of the text.  In this talk, I will argue that understanding the flow of sentiments in a text is a major challenge for effectively predicting the document's orientation towards a given topic. I will briefly outline some possible avenues to address this challenging issue and review some recent papers that take steps in this direction.	blog;off topic;sentiment analysis;social media	David E. Losada	2011		10.1145/2065023.2065025	political science;data mining;multimedia;world wide web	Web+IR	-22.498166925646384	-59.47760537513487	155145
15ca74e2fe07c64a2aaa9338289ca8b2bfa1de4e	machine learning for machine data from a cati network		This is a machine learning application paper involving big data. We present high-accuracy prediction methods of rare events in semi-structured machine log files, which are produced at high velocity and high volume by NORC’s computer-assisted telephone interviewing (CATI) network for conducting surveys. We judiciously apply natural language processing (NLP) techniques and data-mining strategies to train effective learning and prediction models for classifying uncommon error messages in the log—without access to source code, updated documentation or dictionaries. In particular, our simple but effective approach of features preallocation for learning from imbalanced data coupled with naive Bayes classifiers can be conceivably generalized to supervised or semisupervised learning and prediction methods for other critical events such as cyberattack detection.	apache hadoop;big data;computation;computer-assisted telephone interviewing;cross-validation (statistics);data logger;data mining;data pre-processing;dictionary;documentation;error message;extreme value theory;han unification;ibm naval ordnance research calculator;inbound marketing;josh wolf (journalist);kelly criterion;machine learning;montgomery modular multiplication;naive bayes classifier;natural language processing;parsing;preprocessor;prototype;rare events;real-time clock;receiver operating characteristic;requirement;spark;semi-supervised learning;semiconductor industry;server (computing);server log;social media analytics;velocity (software development)	Sou-Cheng T. Choi	2015	CoRR		semi-supervised learning;computer science;online machine learning;machine learning;pattern recognition;data mining;active learning;statistics;generalization error	ML	-19.232714736364326	-65.58337560210063	155967
2872aa13ba8a9eec4980b35eb2b16c5234e92931	classification methods		Table of	cluster analysis;data mining;html attribute;kerrison predictor;numerical analysis;supervised learning;test set;unsupervised learning	Aijun An	2009				ML	-23.47272936732398	-61.887635625182625	156293
04d749d184452bf7cf1b3490fadd794ab9edc0bb	automated classification of author's sentiments in citation using machine learning techniques: a preliminary study	bi gram word statistics automated classification author s sentiments machine learning techniques journal articles books web links body text citation tag citation sentences support vector machine based text categorization technique svm comment on articles medline citation field complimentary opinions contradictory opinions radial basis kernel function input feature vectors n grams word statistics biomedical journal titles input feature vector uni gram word statistics;support vector machines dictionaries accuracy text categorization kernel citation analysis training;citation analysis;medline;author s sentiments;comment on;support vector machines biology computing citation analysis feature extraction learning artificial intelligence radial basis function networks;medline citation analysis author s sentiments comment on support vector machine n grams word statistics;n grams word statistics;support vector machine	Scientific papers generally include citations to external sources such as journal articles, books, or Web links to refer to works that are related in an important way to the research. The reason for the citation appears within the sentences surrounding the citation tag in the body text, and represents the relationship between the citation and cited works as supportive, contrastive, corrective, etc. This could be an important clue for researchers seeking relevant previous work or approaches for a certain research purpose. We propose to develop an automated method to identify the citing author's sentiments toward the cited external sources expressed in citation sentences using machine-learning techniques and linguistic cues. As a preliminary study, this paper presents a support vector machine (SVM)-based text categorization technique to classify the author's sentiments specifically toward Comment-on (CON) articles. CON, a MEDLINE citation field, indicates previously published articles commented on by authors of a given article expressing possibly complimentary or contradictory opinions. An SVM with a radial basis kernel function (RBF) is implemented, and Input feature vectors for the SVM are created based on n-grams word statistics representing the distribution of words in CON sentences. Experiments conducted on a set of CON sentences collected from 414 different online biomedical journal titles show that the SVM with a RBF yields the best result for an input feature vector combining uni-gram and bi-gram word statistics.	book;categorization;document classification;feature vector;grams;medline;machine learning;n-gram;radial (radio);radial basis function;support vector machine;web	In Cheol Kim;George R. Thoma	2015	2015 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)	10.1109/CIBCB.2015.7300319	support vector machine;speech recognition;computer science;machine learning;data mining;citation analysis;information retrieval	NLP	-23.21860882937647	-65.67407291353426	156383
d7617b26fa544fb229f4aa57f91b9b067ddaebb8	comparison of balancing techniques for multimedia ir over imbalanced datasets	pattern classification information retrieval multimedia systems;inference relevance;information retrieval system;text classification balancing techniques multimedia information retrieval supervised classification problem inference relevance data classification;information retrieval;user study;training;vocabulary;supervised classification;supervised classification problem;data mining;multimedia systems;multimedia information retrieval;text classification;qa75 electronic computers computer science;log file analysis;streaming media;feature extraction;multimedia communication;pattern classification;music information retrieval image retrieval feedback information retrieval video sharing classification algorithms content based retrieval bridges algorithm design and analysis large scale systems;field data;balancing techniques;user interaction;data classification;z665 library science information science;test collection;evaluation framework	A promising method to improve the performance of information retrieval systems is to approach retrieval tasks as a supervised classification problem. Previous user interactions, e.g. gathered from a thorough log file analysis, can be used to train classifiers which aim to inference relevance of retrieved documents based on user interactions. A problem in this approach is, however, the large imbalance ratio between relevant and non-relevant documents in the collection. In standard test collection as used in academic evaluation frameworks such as TREC, non-relevant documents outnumber relevant documents by far. In this work, we address this imbalance problem in the multimedia domain. We focus on the logs of two multimedia user studies which are highly imbalanced. We compare a na¨ıve solution of randomly deleting documents belonging to the majority class with various balancing algorithms coming from different fields: data classification and text classification. Our experiments indicate that all algorithms improve the classification performance of just deleting at random from the dominant class.	1:1 pixel mapping;algorithm;document classification;experiment;information retrieval;interaction;machine learning;multinomial logistic regression;olami–feder–christensen model;oversampling;physical information;randomness;relevance;sampling (signal processing);serial digital video out;supervised learning;text retrieval conference;text-based (computing);usability testing;vocabulary	Pablo Bermejo;Frank Hopfgartner;José A. Gámez;Jose Miguel Puerta;Joemon M. Jose	2009	2009 24th International Symposium on Computer and Information Sciences	10.1109/ISCIS.2009.5291904	feature extraction;computer science;machine learning;pattern recognition;data mining;database;information retrieval	Web+IR	-20.74364257661249	-62.58614192300068	156680
fe9011431e4feede5ed11f7acfbb0c38469e5ecd	improving automatic chinese text categorization by error correction	error threshold;hypergraph partition;error correction;k nearest neighbor;hypergraph partitioning;classification accuracy;rocchio;text categorization	In this paper we use the miss-classified news in training data as a feedback to improve the classification accuracy. We isolate the miss-classified news from the news of original classes to form new subclasses, and modify Rocchio linear classifier by using the subclasses to form new prototype vectors such that there are more than one centroid to represent each class. We have two methods, error threshold and entropy threshold, to evaluate whether the isolation of miss-classified news is worthy or not. Experimental result shows that our approaches improves Rocchio's micro-level accuracy and achieves similar performance as kNN, but with less classification time. On the other hand, with the entropy of miss- classified news, we can figure out the ambiguity between classes and sketch the diagram of the relationship between classes as a suggestion to reorganize the structure of classes for news in the future.	categorization;diagram;document classification;entropy (information theory);error detection and correction;feedback;linear classifier;prototype	Jyh-Jong Tsay;Jing-Doo Wang	2000		10.1145/355214.355215	computer science;machine learning;pattern recognition;data mining	NLP	-24.262838181884483	-66.03159928674833	157199
0594c62529549fae2c91909539c4df8682fce442	multinomial naive bayes for text categorization revisited	busqueda informacion;bayes estimation;analisis contenido;optimisation;linguistique;optimum;analisis estadistico;optimizacion;information retrieval;naive bayes;intelligence artificielle;conference contribution;naive bayes classifier;estimacion bayes;content analysis;linguistica;statistical analysis;recherche information;optimo;machine exemple support;analyse statistique;artificial intelligence;optimization;inteligencia artificial;computer science;support vector machine;maquina ejemplo soporte;analyse contenu;vector support machine;text categorization;multinomial naive bayes classifier;estimation bayes;linguistics	This paper presents empirical results for several versions of the multinomial naive Bayes classifier on four text categorization problems, and a way of improving it using locally weighted learning. More specifically, it compares standard multinomial naive Bayes to the recently proposed transformed weight-normalized complement naive Bayes classifier (TWCNB) [1], and shows that some of the modifications included in TWCNB may not be necessary to achieve optimum performance on some datasets. However, it does show that TFIDF conversion and document length normalization are important. It also shows that support vector machines can, in fact, sometimes very significantly outperform both methods. Finally, it shows how the performance of multinomial naive Bayes can be improved using locally weighted learning. However, the overall conclusion of our paper is that support vector machines are still the method of choice if the aim is to maximize accuracy.	algorithm;categorization;document classification;multinomial logistic regression;naive bayes classifier;support vector machine;test data;tf–idf	Ashraf M. Kibriya;Eibe Frank;Bernhard Pfahringer;Geoff Holmes	2004		10.1007/978-3-540-30549-1_43	naive bayes classifier;content analysis;computer science;machine learning;pattern recognition;data mining	ML	-21.66358823795031	-64.28862857168635	157843
0b689506700876fb9eafd3e0cf284ea1c35d5470	incorporating temporal information for document classification	recurrent linear genetic programming;feature selection techniques;temporal sequence patterns;genetic programming encoding text categorization support vector machines support vector machine classification kernel computer science pattern analysis performance analysis text analysis;linear genetic programming;text analysis genetic algorithms pattern classification self organising feature maps;text analysis;document classification system;temporal information;self organising feature maps;self organizing feature maps;self organized feature map;pattern classification;genetic algorithms;feature selection;temporal sequence patterns temporal information document classification system recurrent linear genetic programming encoded word sequences self organizing feature maps feature selection techniques;document classification;article;encoded word sequences	In this paper, we propose a novel document classification system where the Recurrent Linear Genetic Programming is employed to classify documents that are represented in encoded word sequences by Self Organizing feature Maps. The results using different feature selection techniques on Reuters 21578 data set show that the proposed system can analyze the temporal sequence patterns of a document and achieve competitive performance on classification.	document classification;feature selection;linear genetic programming	Xiao Luo;A. Nur Zincir-Heywood	2007	2007 IEEE 23rd International Conference on Data Engineering Workshop	10.1109/ICDEW.2007.4401067	genetic algorithm;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection	DB	-20.68390120387755	-65.06062186881525	158107
20e2934054ef99b448f3099fc9c5488d30ce1468	a generalized framework for revealing analogous themes across related topics	related topic;empirical result;unsupervised algorithmic framework;interesting commonality;generalized framework;alternative method;different topic;previous initial work;different religion;analogous theme;thematic correspondence;distributional data	This work addresses the task of identifying thematic correspondences across subcorpora focused on different topics. We introduce an unsupervised algorithmic framework based on distributional data clustering, which generalizes previous initial works on this task. The empirical results reveal interesting commonalities of different religions. We evaluate the results through measuring the overlap of our clusters with clusters compiled manually by experts. The tested variants of our framework are shown to outperform alternative methods applicable to the task.	cluster analysis;compiler;unsupervised learning	Zvika Marx;Ido Dagan;Eli Shamir	2005			computer science;data science;theoretical computer science;machine learning;data mining	NLP	-25.256675713960416	-59.53314262709359	158593
8acc53d8323ed5d2275de31071337db21e91bf65	information extraction and summarization from medical documents	information extraction		information extraction	Constantine D. Spyropoulos;Vangelis Karkaletsis	2005	Artificial intelligence in medicine	10.1016/j.artmed.2004.08.002	multi-document summarization;computer science;data science;data mining;information retrieval	AI	-25.0827062709859	-61.11249737245346	158864
361f6799198c4a258c5dbb4c9863a521f290ff4c	visual classifier training for text document retrieval	visual classifier training;user evaluation;human computer interaction;performance evaluation;query processing;filter criteria;iterative feedback loops;information retrieval;active learning;interactive visualization;user evaluation visual analytics human computer interaction information retrieval active learning classification;text analysis data visualisation interactive systems iterative methods learning artificial intelligence pattern classification query processing;classification methods;text analysis;user controlled classification methods;classification;search queries;text document retrieval;learning systems;iterative methods;training data;text search visual classifier training text document retrieval search queries filter criteria machine learning classification methods interactive classifier training interactive visualization labeled documents iterative feedback loops user controlled classification methods;data visualisation;human computer interaction information retrieval performance evaluation visual analytics training data learning systems classification;machine learning;interactive classifier training;pattern classification;labeled documents;learning artificial intelligence;visual analytics;interactive systems;text search	Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.	active learning (machine learning);algorithm;baseline (configuration management);boolean expression;complement system proteins;document retrieval;feedback;generalization (psychology);hope (emotion);imagery;information needs;interactive visualization;interactivity;iterative method;keyword;large;machine learning;precision and recall;question (inquiry);social network;statistical classification;support vector machine;text corpus;tracer;usability testing;visual analytics;weakness;web search query	Florian Heimerl;Steffen Koch;Harald Bosch;Thomas Ertl	2012	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.277	computer vision;training set;full text search;visual analytics;interactive visualization;biological classification;computer science;machine learning;data mining;iterative method;active learning;information retrieval;data visualization	Visualization	-20.548151741127693	-61.96611875152842	158918
a177ca9536c7f412039c8b3971b49047e84732ca	summarizing online discussions by filtering posts	discussion board;title term frequency;information retrieval;description term frequency;text summarization social media information retrieval;online discussion summarizer;text summarization;term frequency;post filtering;data mining;online discussion;argon;discussion boards;accuracy;facebook;term frequency inverse post frequency;humans;author reputation online discussion summarizer post filtering discussion boards unsupervised information retrieval techniques term frequency inverse post frequency title term frequency description term frequency;author reputation;social media;filtering frequency information retrieval data mining data preprocessing user generated content filters;radio access networks;unsupervised information retrieval techniques	In this paper, we attempt to summarize online discussions by filtering posts. Selecting the highly related posts from the discussion boards leads to a summarized version of the discussion. Online Discussion Summarizer (ODS) is based on unsupervised information retrieval techniques. Four features are used in the summarization function; which are the term frequency inverse post frequency, title term frequency, description term frequency and author reputation. This paper shows that combining the four features in the same function results in higher accuracy than using each alone. ODS was able to summarize online discussions with an accuracy of 72%, precession of 83% and recall of 62%.	content-control software;information retrieval;operational data store;tf–idf;unsupervised learning	Mohamed Altantawy;Ahmed G. Rafea;Sherif Aly	2009	2009 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2009.5211592	social media;computer science;automatic summarization;data mining;database;accuracy and precision;argon;tf–idf;world wide web;information retrieval	SE	-22.23438024810157	-61.03579940058922	159339
9b8d932788bbf546069d416dd9953924a4c2e9f6	ghent university-iminds at mediaeval 2013 diverse images: relevance-based hierarchical clustering	hierarchical clustering;reranking;social images;technology and engineering	In this paper, we attempt to tackle the MediaEval 2013 Retrieving Diverse Social Images challenge, which is a filter and refinement problem on a Flickr-based ranked set of social images. We developed three different approaches, using visual data, textual data and a combination thereof, respectively. Hierarchical clustering on highly relevant images, combined with a greedy approach to complement the ranking, forms the basis of our approach.	cluster analysis;flickr;greedy algorithm;hierarchical clustering;refinement (computing);relevance;text corpus	Baptist Vandersmissen;Abhineshwar Tomar;Fréderic Godin;Wesley De Neve;Rik Van de Walle	2013			geography;data science;machine learning;data mining	AI	-25.444397147198373	-60.50434703987126	160607
0c4ae96a28d4ea7ac834752db9795c2276ae9183	contextual analysis processing methods able to interpret sentiments evaluation representations	textual emoticon contextual analysis processing sentiments evaluation representation context understanding sentence coherence semantic representation word meaning semantic analysis cooccurrence expression evaluation polarity anaphora endophora idiomatic expression;manuals;object recognition;sentiments evaluation representation;semantic representation;word meaning;cooccurrence expression;evaluation extraction;evaluation extraction sentiment analysis semantic coherence;text analysis;contextual analysis processing;data mining;sentence coherence;anaphora;grammars;endophora;internet;feature extraction;dictionaries;sentiment analysis;context understanding;internet user generated content skin digital cameras heart data mining natural languages dairy products information analysis microwave integrated circuits;evaluation polarity;computational linguistics;text analysis computational linguistics grammars;textual emoticon;semantic coherence;idiomatic expression;context;semantic analysis	"""A contextual analysis processing technique, consisting in determining the context understanding together with coherences in sentences, of concepts and phenomena related to each others, must be able to simultaneously interpret accurately a sequence of multiple semantic representations. A word frequently carries different meanings according to the context. For example, having a “big heart” is usually understood as a quality, whereas the expression “big words” has rather a pejorative connotation. The “Evaluation polarity” of a word has a probability of changing considerably according to the context. By applying a semantic analysis of co-occurrence expressions, based on the use of phrases having an absolute evaluation polarity, we developed a system achieving analysis capable of: detecting """"the role relations between words"""", the relationship of meaning in a sentence, identifying transitions in the topic, anaphora, endophora, and analyzing even idiomatic expressions and textual emoticons. Our system evaluated correctly “positive” or “negative” nuance for 75.0 % of those."""	anaphora (linguistics);emoticon;sensor	Miho Itoh	2009	2009 IEEE International Conference on Semantic Computing	10.1109/ICSC.2009.98	natural language processing;the internet;feature extraction;computer science;computational linguistics;cognitive neuroscience of visual object recognition;linguistics;rule-based machine translation;sentiment analysis	SE	-23.891090880676945	-64.94014474114103	161485
ba10220f0a63ed5ae916ca9fd6d7ae681cddff99	construct weak ranking functions for learning linear ranking function	normalization;final ranking function;learning to rank;weak ranking function	Many Learning to Rank models, which apply machine learning techniques to fuse weak ranking functions and enhance ranking performances, have been proposed for web search. However, most of the existing approaches only apply the  Min --- Max  normalization method to construct the weak ranking functions without considering the differences among the ranking features. Ranking features, such as the content-based feature  BM  25 and link-based feature  PageRank  , are different from each other in many aspects. And it is unappropriate to apply an uniform method to construct weak ranking functions from ranking features. In this paper, comparing the three frequently used normalization methods:  Min --- Max  ,  Log  ,  Arctan  normalization, we analyze the differences among three normalization methods when constructing the weak ranking functions, and propose two normalization selection methods to decide which normalization should be used for a specific ranking feature. The experimental results show that the final ranking functions based on normalization selection methods significantly outperform the original one.	ranking (information retrieval)	Guichun Hua;Min Zhang;Yiqun Liu;Shaoping Ma;Hang Yin	2011		10.1007/978-3-642-25631-8_5	ranking;ranking;computer science;machine learning;normalization;pattern recognition;data mining;ranking svm;learning to rank	ML	-20.796726395212982	-63.069012803511896	161577
56bcd97b8ed938fa925d2c5795a603ca1d9ffed2	improving the accuracy of question classification with machine learning	machine learning semisupervised learning taxonomy supervised learning information science classification algorithms learning systems costs search engines information retrieval;classification algorithm;supervised learning;information retrieval;question answering systems;automatic group;question taxonomy;classification;semi supervised learning;question classification;learning methods;machine learning;question answering system;learning artificial intelligence classification information retrieval;learning artificial intelligence;hierarchical classifiers;question taxonomy question classification machine learning question answering systems hierarchical classifiers semi supervised learning	Question classification is an important phase in question answering systems. In this paper, we propose to apply i) hierarchical classifiers, ii) hierarchical classifiers in combination with semi-supervised learning and iii) hierarchy expansion for question classification for improving the precision. When the number of classes is large, the performance of classification algorithms may be affected. In order to improve the performance by reducing the number of classes for each classifier, we propose to use hierarchical classifiers according to the question taxonomy, in which each internal node is attached a classifier. We try to use semi-supervised learning to consume unlabeled questions with expectation to improve the performance of classifiers in the hierarchy. We explored different applications of learning methods in for each classifier of the hierarchy: a) supervised learning for all classifiers at all levels; b) semi-supervised learning for the first-level classifier and supervised learning for other classifiers; c) semi-supervised learning for all classifiers. The experiments show that the first method (a) has better results than those of flat classification; the second method (b) produces better results than those of the first method while the effort to increase the performance of fine classifiers in the last method (c) is not so successful. As another effort, we propose to automatically group question classes by clustering in order to expand a node which has a large number of classes in the question taxonomy. The experiment also shows that the overall precision is improved.	algorithm;cluster analysis;emoticon;experiment;machine learning;question answering;semi-supervised learning;semiconductor industry;statistical classification;supervised learning;tree (data structure)	Nguyen Thanh Tri;Minh Le Nguyen;Akira Shimazu	2007	2007 IEEE International Conference on Research, Innovation and Vision for the Future	10.1109/RIVF.2007.369162	random subspace method;cascading classifiers;computer science;machine learning;linear classifier;pattern recognition;data mining	ML	-20.482171783758776	-64.4301885793567	162416
ca6db4bee52f303c7ef7a18c37413e0a621d1fc9	a novel data mining approach for detecting spam emails using robust chi-square features	chi square evaluation;dimensionality reduction;spam filtering;feature selection;text categorization	In spam filtering techniques, the classification of emails are performed on the basis of a collection words that are extracted from the training set. The accuracy and performance of the classifier highly depends on features and length of feature space. Feature selection methods are used in such scenario for evaluating the best features for classification. In an attempt to develop strong spam filtering model we rank the features using Chi--Square feature ranking method and also investigate the effectiveness of feature length on classification accuracy. The results are promising and also the feature ranking method proposed is effective than other methods referred in the literature.	anti-spam techniques;chi;data mining;email filtering;feature selection;feature vector;spamming;test set	Mugdha Sharma;Jasmeen Kaur	2015		10.1145/2791405.2791507	feature hashing;feature;computer science;bag-of-words model;machine learning;pattern recognition;data mining;feature	Web+IR	-20.612552062704513	-63.34804603647499	162717
4ffa5ed422cceb732889713901be441630255959	taxonomy-based regression model for cross-domain sentiment classification	domain adaptation;opinion mining;sentiment classification	Most cross-domain sentiment classification techniques consider a domain as a whole set of instances for training. However, many online shopping websites organize their data in terms of taxonomy. This paper takes Amazon shopping website as an example, and proposes a tree-structured domain representation scheme in which each node in the tree is encoded as a bit sequence to preserve its relationship with all the other nodes in the tree. To select an appropriate source node for training in the domain taxonomy, we propose a Taxonomy-Based Regression Model (TBRM) which predicts the accuracy loss from multiple source nodes to a target node using the tree-structured domain representation combined with domain similarity and domain complexity. The source node with the smallest accuracy loss is used to train a classifier which makes a prediction on the target node. The results show that our TBRM achieves better performance than the regression models without considering the taxonomy information.	evolutionary taxonomy;online shopping;statistical classification;taxonomy (general)	Cong-Kai Lin;Yang-Yin Lee;Chi-Hsin Yu;Hsin-Hsi Chen	2013		10.1145/2505515.2507843	computer science;artificial intelligence;machine learning;pattern recognition;data mining;database;world wide web;information retrieval;sentiment analysis	ML	-19.279142402744256	-64.59500166358963	162802
fdd6008e2b97d7a83795968603ca5d6fc618adab	the application of naive bayes classifier in name disambiguation		Name repetition exists in the academic resource management system, which brings difficulties to academic evaluation, information retrieval, citation analysis and so on. According as different authors use function words in different habits, the Naive Bayes classifier was used to study in this paper. Based on the assumption of feature independence, this paper selects 26 common function words with high frequency as statistical frequency standard, use Naive Bayes classifier to classify texts. Experiments show that the method has a high accuracy rate.	naive bayes classifier;word-sense disambiguation	Na Li;Jin Han	2017		10.1007/978-3-319-68542-7_52	computer science;naive bayes classifier;citation analysis;machine learning;frequency;artificial intelligence;pattern recognition	ML	-23.040394796975107	-63.14838069002954	162971
5de0837fccfe92903f8b777b6e94f4855e44f485	microblog hot event detection based on restart random walk and modularity		Using traditional method to extract semantic relations between words hardly applied to micro-blog, which make finding hot event not sensitive. We propose a new method based on restart random walk and Modularity. The semantic relation between items is calculated by conducting the restart random walk iteratively on graph, and then the semantic correlation matrix is constructed. Next, the idea of Modularity is introduced to design algorithm for word clustering, which make a series of micro-blog hot events obtain. The experimental results show that our method has a higher accuracy compared with the kindred method, and hot events could be detected effectively.		Xiaohong Li;Anirban Bhaduri;Anna Vráblová;Huifang Ma;Na Qin	2018		10.1007/978-3-030-00828-4_27	random walk;microblogging;covariance matrix;cluster analysis;modularity;social media;artificial intelligence;mathematics;graph;pattern recognition	NLP	-22.565176513963262	-64.77310114342814	163044
f93df05d8a814faa5d3004775ee2b6969e053d01	text recognition with k-means clustering		A thesaurus is a reference work that lists words grouped together according to similarity of meaning (containing synonyms and sometimes antonyms), in contrast to a dictionary, which contains definitions and pronunciations. This paper proposes an innovative approach to improve the classification performance of Persian texts considering a very large thesaurus. The paper proposes a flexible method to recognize and categorize the Persian texts employing a thesaurus as a helpful knowledge. In the corpus, when utilizing the thesaurus the method obtains a more representative set of wordfrequencies comparing to those obtained when the method disables the thesaurus. Two types of word relationships are considered in our used thesaurus. This is the first attempt to use a Persian thesaurus in the field of Persian information retrieval. The k-nearest neighbor classifier, decision tree classifier and k-means clustering algorithm are employed as classifier over the frequency based features. Experimental results indicate enabling thesaurus causes the method significantly outperforms in text classification and clustering.	categorization;cluster analysis;computer cluster;decision tree;dictionary;document classification;information retrieval;k-means clustering;k-nearest neighbors algorithm;nearest neighbour algorithm;optical character recognition;reference work;statistical classification;thesaurus	Mohammad Iman Jamnejad;Ali Heidarzadegan;Mohsen Meshki	2014	Research in Computing Science		natural language processing;computer science;data mining;information retrieval	AI	-24.402456591392344	-65.92437038376244	163678
b277c7c5f19ce807cad95f855bdb7bcf415391c9	chinese person name disambiguation based on two-stage clustering	social relation;important feature selection;two stage clustering;core evidence;person name disambiguation		word-sense disambiguation	Jie Zhou;Bicheng Li;Yongwang Tang	2016	JACIII	10.20965/jaciii.2016.p0755	natural language processing;social relation;machine learning;pattern recognition	NLP	-22.65211928443035	-65.3434995422702	163955
58c065e93a5137823e2bf375e717279cf528efc3	a framework for product description classification in e-commerce		We propose the Hierarchical Product Classification (HPC) framework for the purpose of classifying products using a hierarchical product taxonomy. The framework uses a classification system with multiple classification nodes, each residing on a different level of the taxonomy. The innovative part of the framework stems from the definition of classification recipes that can be used to construct high-quality classifier nodes, using the product descriptions in the most optimal way. These classifier recipes are specifically tailored for the e-commerce domain. The use of these classifier recipes enables flexible classifiers that adjust to the taxonomy depth-specific characteristics of product taxonomies. Furthermore, in order to gain insight into which components are required to perform high quality product classification, we evaluate several feature selection methods and classification techniques in the context of our framework. Based on 3000 product descriptions obtained from Amazon.com, HPC achieves an overall accuracy of 76.80% for product classification. Using 110 categories from CircuitCity.com and Amazon.com, we obtain a precision of 93.61% for mapping the categories to the taxonomy of shopping.com.	algorithm;algorithmic efficiency;computation;cosine similarity;display resolution;e-commerce payment system;feature selection;naive bayes classifier;nearest-neighbor interpolation;semantic similarity;statistical classification;support vector machine;test set;wordnet	Damir Vandic;Flavius Frasincar;Uzay Kaymak	2018	J. Web Eng.		e-commerce;product description;data mining;feature selection;computer science;product classification;internet security;hierarchical clustering;classifier (linguistics)	Web+IR	-19.313453467507067	-64.7608569649406	165748
435aa94bd78c1672c6bce2d1c58517c09a357d9a	sequential clustering and contextual importance measures for incremental update summarization		Unexpected events such as accidents, natural disasters and terrorist attacks represent an information situation where it is crucial to give users access to important and non-redundant information as early as possible. Previous work uses either a fast but inaccurate pipeline approach or a precise but slow clustering approach. Instead, we propose to use sequential clustering for grouping information so that we are able to publish sentences at each time step. By doing so, we combine the best of both clustering and pipeline approaches and create a fast and precise real-time system. Experiments on the TREC Temporal Summarization 2015 shared task dataset show that our system achieves better results compared to the state-of-the-art.	automatic summarization;cluster analysis;experiment;incremental backup;pipeline (computing);precision and recall;query expansion;real-time clock;real-time computing;real-time locating system;zero-knowledge proof	Markus Zopf;Eneldo Loza Mencía;Johannes Fürnkranz	2016			automatic summarization;computer science;cluster analysis;artificial intelligence;pattern recognition	NLP	-26.111342587634816	-65.40770431087357	166142
05b779ff2a961d7fe1251f766f10fc9c3914ca1f	emotion classification of online news articles from the reader's perspective	document handling;support vector machines;search engines;helium;information retrieval;bismuth;training;emotion recognition;classification;web search engine;accuracy;internet;feature extraction;search engines document handling emotion recognition information retrieval internet pattern classification;classification algorithms;sentiment analysis;pattern classification;intelligent agent web search search engines content based retrieval information services web sites internet computer science application software feedback;reader emotion;emotion ranking online news articles document classification reader emotion categories reader emotion classification web search engine document retrieval;sentiment analysis reader emotion classification	Past studies on emotion classification focus on the writer’s emotional state. This research addresses the reader aspect instead. The classification of documents into reader-emotion categories has several applications. One of them is to integrate reader-emotion classification into a web search engine to allow users to retrieve documents that contain relevant contents and at the same time instill proper emotions. In this paper, we automatically classify documents into reader-emotion categories, and examine classification performance under different feature settings. Experiments show that certain feature combinations achieve good accuracy. We also compare the best classifier’s classification results with the emotional distributions of documents to determine how closely the classifier models the underlying reader behavior. Finally, we investigate the feasibility of emotion ranking.	algorithm;artificial intelligence;best practice;bigram;blog award;computation;computational linguistics;empirical methods in natural language processing;entity–relationship model;experiment;haplogroup cz (mtdna);holographic principle;lu decomposition;lexicon;local interconnect network;machine learning;nl (complexity);photogrammetry;sentiment analysis;statistical classification;support vector machine;text corpus;thematic map;web search engine;window function;yang	Kevin Hsin-Yih Lin;Changhua Yang;Hsin-Hsi Chen	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.197	support vector machine;web query classification;the internet;web search engine;feature extraction;biological classification;computer science;machine learning;bismuth;data mining;accuracy and precision;helium;world wide web;information retrieval;sentiment analysis	Web+IR	-22.791634590869144	-60.81005340913661	166619
116cfce6827bba120226aece85b70142de6ed060	subject extraction method of urban complaint data		By analyzing a large number of complaints information, we put forward a method based on rules to extract subject words of urban complaint data. We first segment the title of complaint data using stop words and extract candidate subject words, and then filter them using position features, overlapping word features and dependency features to get subject words. The experimental results show that the method proposed is effective to extract subject words of urban complaint data.		Zhian Dong;Xueqiang Lv	2017	2017 IEEE International Conference on Big Knowledge (ICBK)	10.1109/ICBK.2017.11	complaint;data mining;artificial intelligence;stop words;pattern recognition;computer science	Robotics	-24.52323890891296	-65.81106846493732	167292
3d5daa0743cfc77effc8a72acdda8bde3801304c	a comparative study of named entity recognition for arabic using ensemble learning approaches	machine learning algorithms;measurement;bagging;prediction algorithms;learning systems;decision trees;algorithm design and analysis	The ensemble learning has been successfully applied to many Natural Language Processing (NLP) tasks. For the Arabic Named Entity Recognition (NER) task, most studies in the literature have only focused on traditional classification methods and until now no one to the best of our knowledge has studied the ensemble learning for the Arabic NER task. In this paper, we apply six ensemble learning approaches to the Arabic NER task and we present a comparative study between these six ensemble learning approaches and six traditional classification approaches on two Arabic NER datasets (ANERcorp and AQMAR). The empirical results show that the ensemble learning methods significantly outperform the traditional classification methods. The Random Forests method achieves the best F1-measure results of 86.57% and 82.51% on each dataset, respectively.	adaboost;ensemble learning;f1 score;feature selection;k-nearest neighbors algorithm;multilayer perceptron;named entity;named-entity recognition;natural language processing;random forest	Ismail El Bazi;Nabil Laachfoubi	2015	2015 IEEE/ACS 12th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2015.7507143	natural language processing;algorithm design;bootstrap aggregating;prediction;computer science;machine learning;decision tree;pattern recognition;ensemble learning;measurement;statistics	NLP	-20.324421019194748	-65.90812314571387	167731
b4d4a9aedfa6ac2b718ead025cbc606d0f6fd8b8	hierarchical analysis of text similarity data.				Alexander Mehler	2002	KI		semantic similarity	NLP	-24.47019175596531	-61.322428898503254	168011
7b9ed7e995eb79d0d680dc1ddec68bbfa25d350a	applications and comparisons of fuzzy similarity measures	pragmatics;compounds;text analysis;shape recognition;shape measurement;fuzzy set theory;fuzzy sets;shape;text analysis fuzzy set theory natural language processing shape recognition;feature extraction;pixel;comparative study;shape pixel compounds fuzzy sets shape measurement pragmatics feature extraction;fuzzy features fuzzy similarity measures shape recognition arabic sentence recognition;fuzzy similarity measures;fuzzy features;arabic sentence recognition;similarity measure;natural language processing	We present a comparative study between fuzzy similarity measures applied to shape recognition and Arabic sentences recognition described with fuzzy features. The objective is to demonstrate that the choice of a fuzzy similarity is important and can influence results in any researsh topic.	emoticon;fuzzy concept;similarity measure	Leila Baccour;Adel M. Alimi	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584276	natural language processing;fuzzy classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;fuzzy set;pragmatics	Robotics	-24.132563529445775	-64.27710054558268	168055
efea6636bdf9f103d71d320004c98e02885ee821	an empirical study of massively parallel bayesian networks learning for sentiment extraction from unstructured text	structure learning;opinion mining;bayesian network;empirical study;learning algorithm;parallel algorithm;large scale;sentiment analysis;mapreduce;cloud computing;bayesian networks	Extracting sentiments from unstructured text has emerged as an important problem in many disciplines, for example, to mine online opinions from the Internet. Many algorithms have been applied to solve this problem. Most of them fail to handle the large scale web data. In this paper, we present a parallel algorithm for BN (Bayesian Networks) structure leaning from large-scale dateset by using a MapReduce cluster. Then, we apply this parallel BN learning algorithm to capture the dependencies among words, and, at the same time, finds a vocabulary that is efficient for the purpose of extracting sentiments. The benefits of using MapReduce for BN structure learning are discussed. The performance of using BN to extract sentiments is demonstrated by applying it to real web blog data. Experimental results on the web data set show that our algorithm is able to select a parsimonious feature set with substantially fewer predictor variables than in the full data set and leads to better predictions about sentiment orientations than several usually used methods.		Wei Chen;Lang Zong;Weijing Huang;Gaoyan Ou;Yue Wang;Dongqing Yang	2011		10.1007/978-3-642-20291-9_47	computer science;artificial intelligence;data science;machine learning;bayesian network;data mining;database;world wide web;sentiment analysis	NLP	-22.651371350671347	-61.94382106999895	168216
53268bc9b5fad7918b20605e410f30cffff83244	a lda-based method for automatic tagging of youtube videos	video signal processing;video retrieval;multimedia systems;weighted word list lda based method automatic tagging youtube video automatic speech recognition system spoken content extraction keyword extraction tagging system recognition error video transcription latent dirichlet allocation weighted term;keyword extraction;social networking online;keyword extraction audio categorization structuring multimedia collection speech recognition;speech recognition;videos tagging speech recognition speech robustness acoustics semantics;index terms audio categorization;video signal processing multimedia systems social networking online speech recognition video retrieval;structuring multimedia collection	This article presents a method for automatic tagging of Youtube videos. The proposed method combines an automatic speech recognition (ASR) system, that extracts the spoken contents, and a keyword extraction component that aims at finding a small set of tags representing a video. In order to improve the robustness of the tagging system to the recognition errors, a video transcription is represented in a topic space obtained by a Latent Dirichlet Allocation (LDA), in which each dimension is automatically characterized by a list of weighted terms. Tags are extracted by combining the weighted word list of the best LDA classes. We evaluate this method by employing the user-provided tags of Youtube videos as reference and we investigate the impact of the topic model granularity. The obtained results demonstrate the interest of such model to improve the robustness of the tagging system.	keyword extraction;latent dirichlet allocation;speech recognition;topic model;transcription (software)	Mohamed Morchid;Georges Linarès	2013	2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)	10.1109/WIAMIS.2013.6616126	speech recognition;computer science;pattern recognition;information retrieval	NLP	-24.87329072117388	-62.69448488305394	168608
365ae188892c9f3423df5f110900153cc6990ad4	adaptive name matching in information integration	internet adaptive name matching information integration duplicate database records string similarity measures textual similarity measures heterogeneous information sources web pages machine learning;duplicate detection;information sources;web pages;text mining;digital library;text analysis;database integration;data mining;costs databases optical character recognition software data mining optical recording character recognition web pages couplings object detection uncertainty;information integration;internet;machine learning;natural language;distributed databases;similarity measures;learning artificial intelligence;coreference resolution;string matching;learning artificial intelligence text analysis internet distributed databases string matching;record linkage	When information is combined from heterogeneous information sources, data records that refer to equivalent entities need to be identified. However, records that describe the same object may differ syntactically: for example, the same person can be referred to as “William Jefferson Clinton” and “bill clinton”. Figures 1 and 2 below present more complex examples of duplicate records that are not identical.	communications security;em (typography);entity;machine learning	Mikhail Bilenko;Raymond J. Mooney;William W. Cohen;Pradeep Ravikumar;Stephen E. Fienberg	2003	IEEE Intelligent Systems	10.1109/MIS.2003.1234765	text mining;record linkage;the internet;computer science;information integration;data integration;web page;data mining;database;natural language;information retrieval;string searching algorithm	ML	-24.089528265173946	-63.16957160503967	168777
370f6e6d9785859fa1ed853231fd8009e732bda3	approaches to samples selection for machine learning based classification of textual data	sample selection;information retrieval;euclidean distance;text classification;machine learning;computational complexity;natural language;text similarity;biased sampling;textual patterns;data consistency;bag of words;natural language processing	The paper focuses on the process of selecting representative sample documents written in a natural language that can be used as the basis for automatic selection or classification of textual documents. A method of selecting the examples from a larger set of candidate examples, called automatic biased sample selection, is compared to random and manual selection. The methods are evaluated by experiments carried out with real world data consisting of customer reviews, with different document representations and similarity measures. The presented approach, that provided satisfactory results, faces problems related to processing user created content and huge computational complexity and can be used as an alternative to manual selection and evaluation of textual samples.	computational complexity theory;experiment;machine learning;natural language;text corpus;user-generated content	Frantisek Darena;Jan Zizka	2013	Computing and Informatics		sampling bias;computer science;bag-of-words model;machine learning;pattern recognition;data mining;euclidean distance;database;natural language;data consistency;computational complexity theory;information retrieval	ML	-21.33207988262815	-63.57891106662714	169219
c7984e2d1ad12e6f90fc477b3b1d033eddb3a5bb	semantics-empowered social computing	social network services;semantic services;social network services user generated content neural networks ontologies natural language processing humans information retrieval speech myspace;social networking online meta data natural language processing semantic web;social computing;web 3 0 user generated content semantic social web;semantic web 2 0;semantics empowered social computing;semantic web 2 0 semantic services internet computing user generated content social semantic web domain knowledge;bepress selected works;statistical natural language processing;semantic social web;domain knowledge;media;artificial neural networks;internet computing;social networking online;semantic web;social semantic web;meta data;ontologies;user generated content;web 3 0;blogs;natural language processing;knowledge based systems;metadata creation semantics empowered social computing social semantic web domain knowledge statistical natural language processing;metadata creation	In this article, we discuss some of the challenges in marking-up or annotating UGC, a first step toward the realization of the social semantic Web. Using examples from real-world UGC, we show how domain knowledge can effectively complement statistical natural language processing techniques for metadata creation.	item unique identification;natural language processing;social semantic web;social computing;stochastic grammar;user-generated content	Amit P. Sheth;Meena Nagarajan	2009	IEEE Internet Computing	10.1109/MIC.2009.21	natural language processing;media;computer science;ontology;artificial intelligence;semantic web;social semantic web;user-generated content;metadata;world wide web;information retrieval;domain knowledge;social computing	Web+IR	-23.79059217197588	-61.167958296275906	169399
d66c022ec478e8d85e5335254ba4032534ebef44	applications of text clustering based on semantic body for chinese spam filtering	semantic similarity;semantic body;spam filter;text clustering;lexical chain	The effect of spam filtering method based on statistics is not good enough in filtering the new-type spam with synonymous substitution and camouflage, because the method based on statistics ignores the semantic relation between words in the text, and only judges from the word itself. So, a method of spam filtering based on the semantic body is proposed in this paper. The method adopts lexical chain based on HowNet and TFIDF method based on statistics to extract e-mail features, and handle spam with text clustering method. The result of the experiment shows that the new method proposed in this pager provides a good effect in filtering new-type spam.	anti-spam techniques;cluster analysis;email filtering;ontology components;principle of good enough;spamming;tf–idf	Qiu-yu Zhang;Hui-juan Yang	2012	JCP	10.4304/jcp.7.11.2612-2616	semantic similarity;document clustering;computer science;bag-of-words model;lexical chain;pattern recognition;world wide web;information retrieval	Web+IR	-25.33540642454855	-65.1310877584167	170497
b3105f5936655f98dbfe09860d73132eeee2bb34	open relation extraction from chinese microblog text	semantics;information services;data mining;syntactics;feature extraction;ores;information entropy	In recent years, the rapid development of microblog provides entity relation extraction (ERE) with a new carrier. However, the characteristics of microblog also bring some challenges for ERE research. Considering the characteristics of microblog, the paper proposes an unsupervised open relation extraction (ORE) method, namely MICRO-ORE. Firstly, MICRO-ORE uses the left-right information entropy method to automatically extract the key phrases from microblog texts, and links them to the external knowledge sources to normalize microblog texts and add the semantic information. Secondly, according to the Chinese syntactic features, MICROORE formulates the extraction rules to extract the relation tuples. We evaluate the proposed method with SINA microblog texts and show that it can extract more information than the traditional relation extraction methods, and meet the accuracy demand. To our best Knowledge, MICRO-ORE is the first Chinese ORE method for microblog texts.	entity;entropy (information theory);epitaxy;natural language processing;real-time transcription;relationship extraction;requirement;scalability;social network	Jing Xu;Liang Gan;Zhou Yan;Quanyuan Wu;Yan Jia	2016	2016 IEEE First International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2016.34	computer science;data mining;world wide web;information retrieval	NLP	-25.05489005469548	-64.8020419355223	170975
9856222323362179f1c5260e3589181d9c7807e6	faceted navigation on text	human reading process semantic search faceted navigation web search;information retrieval;text analysis;set theory;semantics humans robustness navigation feature extraction drugs information processing;human reading process faceted navigation multiple facet extraction keyword sets reader interest representation subsets text features cognition process;cognition;human reading process;web search;semantic search;text analysis cognition information retrieval set theory;faceted navigation	This paper proposes a method which is not for summarization but for extracting multiple facets from a text according to the keyword sets representing readers' interests, so that readers can obtain the interested facets and carry out faceted navigation on text. A facet is a meaningful combination of the subsets of the text. Previous text process technologies are mostly based on text features such as word frequency, sentence location, syntax analysis and discourse analysis. These approaches neglect the cognition process of human reading. The proposed method considers human reading process. Experiments show that the facet extraction is effective and robust.	cognition;experiment;faceted classification;link relation;mind;network model;parsing;semantic network;upsampling;word lists by frequency	Bei Xu	2012	2012 Eighth International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2012.33	natural language processing;cognition;semantic search;computer science;data mining;information retrieval;set theory	NLP	-25.874432517416402	-62.8781373992925	171059
0eb967c9e65c2e626661866b3b4805b97a54091e	exploiting partial decision trees for feature subset selection in e-mail categorization	decision tree;information filtering;document representation;feature space;indexing methods;supervised machine learning;text classification;col;machine learning;feature subset selection;feature selection;classification accuracy;text categorization	In this paper we propose PARTfs which adopts a supervised machine learning algorithm, namely partial decision trees, as a method for feature subset selection. In particular, it is shown that an aggressive reduction of the feature space can be achieved with PARTfs while still allowing for comparable classification results with conventional feature selection metrics. The approach is empirically verified by employing two different document representations and four different text classification algorithms that are applied to a document collection consisting of personal e-mail messages. The results show that a reduction of the feature space in the magnitude of ten is achievable without loss of classification accuracy.	algorithm;archive;categorization;decision tree;document classification;email;feature selection;feature vector;machine learning;supervised learning	Helmut Berger;Dieter Merkl;Michael Dittenbach	2006		10.1145/1141277.1141536	feature learning;feature vector;feature extraction;computer science;machine learning;decision tree;linear classifier;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm;feature;dimensionality reduction	ML	-20.368839860903226	-63.34991888375548	171453
3f475d275de6c9dd9a59f3d664cf42c0da61336f	a modified tripartite model for document representation in internet sociology		Seven years ago Peter Mika (Yahoo! Research) proposed a tripartite model of actors, concepts and instances for document representation in the study of social networks. We propose a modified model, where instead of document authors we consider textual mentions of persons and institutions as actors. This representation proves to be more appropriate for the solution of a range of Internet Sociology tasks. In the paper we describe experiments with the modified model and provide some background on the tools that can be used to build it. The model is tested on the experimental corpora of Russian news (educational domain). The research reflects the pilot study findings.	sociology of the internet	Mikhail Alexandrov;Vera Danilova;Xavier Blanco	2015		10.1007/978-3-319-22852-5_27	computer science;artificial intelligence;world wide web	Web+IR	-23.09350920197824	-61.125784787941235	171462
0f58272ec2556923a4085163b5579ea7d5c348bd	text categorization using feature projections	training data;tcfp algorithm;feature projection technique;individual feature projection;training document;text categorization;new approach;training process;final classification;naive bayes;text summarization;term frequency	This paper proposes a new approach for text categorization, based on a feature projection technique. In our approach, training data are represented as the projections of training documents on each feature. The voting for a classification is processed on the basis of individual feature projections. The final classification of test documents is determined by a majority voting from the individual classifications of each feature. Our empirical results show that the proposed approach, Text Categorization using Feature Projections (TCFP), outperforms k-NN, Rocchio, and Naïve Bayes. Most of all, TCFP is about one hundred times faster than k-NN. Since TCFP algorithm is very simple, its implementation and training process can be done very easily. For these reasons, TCFP can be a useful classifier in the areas, which need a fast and high-performance text categorization task.	categorization;document classification;k-nearest neighbors algorithm;naive bayes classifier	Youngjoong Ko;Jungyun Seo	2002			naive bayes classifier;computer science;automatic summarization;machine learning;pattern recognition;data mining;tf–idf	ML	-20.790359762810137	-64.62470026321516	171520
55cdd06cab2694095ec62aca72a5eedde1ad1b2b	tokenization and proper noun recognition for information retrieval	noun;information retrieval;tokenization proper noun recognition natural language processing techniques advanced tokenizer complex linguistic phenomena;text analysis;natural languages;data mining;info eu repo semantics article;information retrieval natural language processing information analysis performance analysis filters text recognition employment indexing proposals natural languages;natural languages text analysis data mining;natural language processing	In this paper we consider a set of natural language processing techniques that can be used to analyze large amounts of texts, focusing on the advanced tokenizer which accounts for a number of complex linguistic phenomena, as well as for pre-tagging tasks such as proper noun recognition. We also show the results of several experiments performed in order to study the impact of the strategy chosen for the recognition of proper nouns.	experiment;information retrieval;lexical analysis;natural language processing;tokenization (data security)	Francisco-Mario Barcala;Jesús Vilares;Miguel A. Alonso;Jorge Graña Gil;Manuel Vilares Ferro	2002		10.1109/DEXA.2002.1045906	natural language processing;noun;text mining;question answering;computer science;natural language;information retrieval	NLP	-24.516834467488753	-64.77306045375911	172641
c3dc2fb75636f9ea4f47360ecdc26676aa4b353c	lvq for text categorization using a multilingual linguistic resource	multilingual information retrieval;vector space model;exponentiated gradient;information retrieval;vector space model vsm;competitive learning;polyglot bible;learning vector quantization lvq;text categorization tc;high performance;natural language processing;text categorization;multilingual information retrieval mir;learning vector quantization	Neural learning has been used with e6ectiveness in natural language processing tasks. Particularly, the Widrow–Ho6 and the Kivinen–Warmuth exponentiated gradient (based on neural learning rules) algorithms have been used in text categorization, improving the results obtained by the well-known Rocchio’s algorithm. The high performance of competitive learning algorithms, recently applied to solve information retrieval problems, leads us to use them in the speci=c text categorization tasks. This paper presents a multilingual categorization system based on neural learning, using the polyglot Bible as training collection, both in Spanish and English. The method we suggest is based on using the LVQ algorithm to build a classi=er that learns the training multilingual collection. We have performed experiments with the four algorithm which show that the ideas we describe are promising and are worth further investigation. c © 2002 Elsevier B.V. All rights reserved.	artificial neural network;categorization;competitive learning;document classification;experiment;fo (complexity);genetic algorithm;information retrieval;learning vector quantization;least mean squares filter;machine learning;natural language processing;supervised learning;vanishing gradient problem;whole earth 'lectronic link;word-sense disambiguation	María Teresa Martín-Valdivia;Manuel García Vega;Luis Alfonso Ureña López	2003	Neurocomputing	10.1016/S0925-2312(02)00633-1	natural language processing;speech recognition;learning vector quantization;computer science;machine learning;pattern recognition;competitive learning;vector space model	NLP	-20.94333595437339	-65.49178319972805	172767
74d0617d03f7240c6b4f70f311071c9209dfd0de	unsupervised topic detection model and its application in text categorization	term frequency measure;topic detection;text classification;topic identification;text categorization	In most of the research, topic detection is defined as the task of finding out different themes from the collection of documents. Our topic detection approach is about finding a topic for every document in the corpus. Any word or group of words which tells what the document is about is defined as the topic of the document. In this paper, we propose a novel topic detection approach using an unsupervised model. It is a simple yet effective approach for topic detection and finding keywords from the corpus. The keywords are extracted by identifying the relationship between the words in a set of unstructured data automatically, without any set of training data. The keyword extraction is based on an hypothesis for word decomposition which says that the words in bigram or trigram word vectors would have words that can be potential distribution of words from the unigram word vector. After keyword extraction, topics are determined for each document using our proposed algorithm of topic detection. The proposed algorithm finds the most suitable topic for each document. The topics detected in the entire corpus and the keywords related with each topic are stored and analyzed. We use the standard term frequency (TF) measure for finding the keywords. The effectiveness and accuracy of keywords is judged by using these keywords as features for classification and comparing the results against the standard bag-of- words approach. The topics detected by our algorithm are found to be relevant to the document. The experimental results using keywords show that the dimensionality of the corpus is drastically reduced while maintaining and in most of the cases, improving F-measure of categorization. Thus, it shows that our approach of feature selection for text categorization not only improves the classification accuracy but also reduces considerably the time required for classification.	algorithm;bag-of-words model;bigram;categorization;document classification;f1 score;feature selection;keyword extraction;n-gram;text corpus;tf–idf;trigram;unsupervised learning;word embedding	Yashodhara V. Haribhakta;Arti Malgaonkar;Parag Kulkarni	2012		10.1145/2381716.2381775	natural language processing;document clustering;computer science;pattern recognition;information retrieval	NLP	-24.0678132640569	-65.8369682043891	172863
c41469d4096ac27aaf3a50fdddff5c9301dc97fa	automatic chinese term extraction based on cognition theory	cognition theory;cognition data mining statistics frequency estimation natural languages explosions automation frequency measurement statistical analysis mutual information;knowledge based method;automatic chinese term extraction;low frequency;frequency estimation;natural languages;frequency measurement;data mining;linguistic knowledge;research paper;statistical analysis;mc scp measure automatic chinese term extraction cognition theory linguistic knowledge statistics information c value measures knowledge based method;c value measures;cognition;natural language processing cognition knowledge based systems;statistics;statistics information;term extraction;mutual information;explosions;basic research;mc scp measure;natural language processing;knowledge based systems;knowledge base;automation	"""Term extraction is a basic research topic to establish knowledge bases. This paper puts forward a new automatic Chinese term extraction based on cognition theory. Supervised by both linguistic knowledge and statistics information of research papers, we improve the traditional fair SCP and C-Value measures originally developed for multi-words, and then present a new comprehensive metric called MC-SCP measure, which also fit for uni-words, to examine """"unithood"""" and """"termhood"""" of candidate terms. The experimental results show that our method is expected even for terms with low frequency."""	cognition;knowledge base;secure copy;terminology extraction	Wei Li;Cong Wang;Dong-na Shi	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525204	natural language processing;knowledge base;cognition;computer science;artificial intelligence;data science;automation;machine learning;data mining;low frequency;natural language;mutual information;statistics	Robotics	-24.32878556953043	-64.33906837429048	173214
7805b91c414250bda3ba0e557b16d431693d16a1	hasker: an efficient algorithm for string kernels. application to polarity classification in various languages		Abstract String kernels have successfully been used for various NLP tasks, ranging from text categorization by topic to native language identification. In this paper, we present a simple and efficient algorithm for computing various spectrum string kernels. When comparing two strings, we store the p -grams in the first string into a hash table, and then we apply a hash table lookup for the p -grams that occur in the second string. In terms of time, we show that our algorithm can outperform a state-of-the-art tool for computing string similarity. In terms of accuracy, we show that our approach can reach state-of-the-art performance for polarity classification in various languages. Our efficient implementation is provided online for free at http://string-kernels.herokuapp.com .	algorithm	Marius Popescu;Cristian Grozea;Radu Tudor Ionescu	2017		10.1016/j.procs.2017.08.207	artificial intelligence;theoretical computer science;hash table;machine learning;string kernel;string (computer science);string searching algorithm;commentz-walter algorithm;rabin–karp algorithm;computer science;boyer–moore string search algorithm;algorithm;string metric	NLP	-19.37562563873003	-63.64665731341769	173533
05ee27450fe8a0abb12276438f19973bf1331fe4	concept extraction based on association linked network	text keyword;communities semantics equations data mining knowledge engineering dictionaries mathematical model;semantic dictionary;aln of keyword;semantic representation;text analysis dictionaries entropy internet;graph characteristics;semantics;text analysis;data mining;concept extraction;internet;association linked network;dictionaries;keyword extraction;semantic level;mathematical model;webpage;mutual information;kaln community;wordnet;entropy;communities;kaln community concept extraction association linked network text keyword semantic level semantic representation semantic dictionary wordnet keyword extraction webpage aln of keyword graph characteristics entropy mutual information;knowledge engineering	Text keywords at different semantic levels have different semantic representation abilities. Although words have been organized by semantic dictionaries (e.g. WordNet) with exact semantics, the dictionaries can not be constructed automatically by machine and there are still many words which are not included in the dictionaries. This paper proposes a novel method to automatically extract keywords of higher semantic level which named concept. According to the Association Linked Network (ALN) of webpages, the ALN of keywords (kALN) is constructed first which holds the keywords of a domain and the relations among these keywords. By analyzing graph characteristics of kALN, keywords are grouped into communities. Then drawing on Entropy and Mutual Information, concepts are extracted from each kALN community. Experimental results show that the proposed method of concept extraction is acceptable in accuracy and complexity.	dictionary;mutual information;wordnet	Xiao Wei;Xiangfeng Luo	2010	2010 Sixth International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2010.11	natural language processing;entropy;wordnet;the internet;computer science;artificial intelligence;knowledge engineering;web page;mathematical model;data mining;semantics;mutual information;information retrieval;statistics	AI	-25.688060025326838	-63.97647709503933	173783
ed5cd9f69c74d6f6dd8a298d51e622d1f57a9bb4	research on product feature extraction for chinese reviews	manuals;itemsets;data mining;feature extraction;algorithm design and analysis;tagging	Opinion mining technique aims at automatically obtaining valuable information by retrieving and processing product reviews on the web, product feature extraction is the key step for opinion mining. Currently, the product feature extraction has been studied a lot in the field of English. However, the research of Chinese feature extraction is insufficient. There exist many issues such as small size of datasets, low precision rate, etc. In this paper, we put forward a feature extraction algorithm for Chinese reviews based on Apriori algorithm on the basis of Chinese language characteristics. Experiments with a large number of real network reviews data are carried out and good results are obtained. The experiment results show that this algorithm is effective and has a certain practical value.	apriori algorithm;feature extraction;text corpus	Guiying Wei;Wenming Cai;Lan Qin	2016	2016 International Conference on Logistics, Informatics and Service Sciences (LISS)	10.1109/LISS.2016.7854478	feature extraction;computer science;data science;data mining;information retrieval	DB	-24.07369251205024	-65.18439986476635	173940
1efa9630512c2ad44150de45cf797606e606a7ae	generating a context-aware sentiment lexicon for aspect-based product review mining	user generated product reviews context aware sentiment lexicon aspect based product review mining;sentiment lexicons sentiment analysis web content mining;text analysis data mining social sciences computing;context aware;web content mining;context aware sentiment lexicon;sentiment lexicons;text analysis;data mining;user generated product reviews;social sciences computing;sentiment analysis;context dependent;aspect based product review mining	A great share of current sentiment analysis techniques is based on special purpose lexicons providing information about the semantic orientation (e.g. positive, negative, neutral) of its entries. Due to the high labor costs of manually assembling such resources, recent work has focused on automatically inducing the polarity of given terms. We follow this line of work while focusing on the domain of user-generated product reviews, a main field of application for sentiment analysis. In this domain, a major observation is that the semantic orientation of terms is often context-dependent which poses an additional challenge to the automatic construction of such lexicons (e.g. positive: “longbattery life” vs. negative: “long shutter lag time”). We propose a novel unsupervised method to induce a context-aware sentiment lexicon by utilizing the semi-structuredness of user-generated product reviews.	baseline (configuration management);context-sensitive language;experiment;inductive reasoning;lexicon;movie projector;semiconductor industry;sensor;sentiment analysis;user-generated content	Jürgen Broß;Heiko Ehrig	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.56	natural language processing;text mining;computer science;context-dependent memory;data mining;information retrieval;sentiment analysis	NLP	-21.992088723934465	-66.04606114545983	173942
398f6844a99cf4e2c847c1887bfb8e9012deccb3	using pagerank in feature selection	object representation;learning algorithm;curse of dimensionality;data mining;feature space;machine learning;random walk;high dimensional data;feature selection;classification accuracy;information gain	Feature selection is an important task in data mining becaus e it allows to reduce the data dimensionality and eliminates the no isy variables. Traditionally, feature selection has been applied in supervis d scenarios rather than in unsupervised ones. Nowadays, the amount of unsupervised data available on the web is huge, thus motivating an increasing interest in fe atur selection for unsupervised data. In this paper we present some results in t he domain of document categorization. We use the well-known PageRank algor ithm to perform a random-walk through the feature space of the documents. Thi allows to rank and subsequently choose those features that better represe nt the data set. When compared with previous work based on information gain, our m ethod allows classifiers to obtain good accuracy especially when few features a retained.	categorization;data mining;document classification;feature selection;feature vector;field electron emission;information gain in decision trees;kullback–leibler divergence;pagerank;selection algorithm;unsupervised learning;whole earth 'lectronic link	Dino Ienco;Rosa Meo;Marco Botta	2008			feature learning;curse of dimensionality;feature vector;feature extraction;computer science;machine learning;linear classifier;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm;feature;dimensionality reduction	ML	-19.863456413882787	-63.53142324555632	174736
daf8fb7b7ee9a0dd785f9ca3c70b6218a5e0a20f	solution architectures for retaining data quality problems in automatically generated test data			data quality;test data	Martin Oberhofer;Philip Woodall;Alexander Borek	2013			data mining;test data;computer science;data quality	SE	-23.497695763210302	-62.45342415153002	174900
e5f67454ea277a7e84ca66b94e8e26daadc8b045	distributed optimization of classifier committee hyperparameters		In this paper, we propose an optimization workflow to predict classifiers accuracy based on the exploration of the space composed of different data features and the configurations of the classification algorithms. The overall process is described considering the text classification problem. We take three main features that affect text classification and therefore the accuracy of classifiers. The first feature considers the words that comprise the inputtext; here we use the N-gram concept with different N values. The second feature considers the adoption of textual pre-processing steps such as the stop-word filtering and stemming techniques. The third feature considers the classification algorithms hyperparameters. In this paper, we take the well-known classifiers K-Nearest Neighbors (KNN) and Naive Bayes (NB) where K (from KNN) and a-priori probabilities (from NB) are hyperparameters that influence accuracy. As a result, we explore the feature space (correlation among textual and classifier aspects) and we present an approximation model that is able to predict classifiers	approximation;coefficient;document classification;form;feature vector;k-nearest neighbors algorithm;mathematical optimization;memory-level parallelism;n-gram;naive bayes classifier;preprocessor;simulation;statistical classification;stemming;whole earth 'lectronic link	Sanzhar Aubakirov;Paulo Trigo;Darhan Ahmed-Zaki	2018		10.5220/0006884101710179	hyperparameter;classifier (linguistics);artificial intelligence;computer science;pattern recognition	Web+IR	-19.48852388061697	-65.31205634745334	175170
9fb1d79d147184171b4acadbf0cbfadd6b9a0f29	recommending who to follow in the software engineering twitter space		With the advent of social media, developers are increasingly using it in their software development activities. Twitter is one of the popular social mediums used by developers. A recent study by Singer et al. found that software developers use Twitter to “keep up with the fast-paced development landscape.” Unfortunately, due to the general-purpose nature of Twitter, it’s challenging for developers to use Twitter for their development activities. Our survey with 36 developers who use Twitter in their development activities highlights that developers are interested in following specialized software gurus who share relevant technical tweets.  To help developers perform this task, in this work we propose a recommendation system to identify specialized software gurus. Our approach first extracts different kinds of features that characterize a Twitter user and then employs a two-stage classification approach to generate a discriminative model, which can differentiate specialized software gurus in a particular domain from other Twitter users that generate domain-related tweets (aka domain-related Twitter users). We have investigated the effectiveness of our approach in finding specialized software gurus for four different domains (JavaScript, Android, Python, and Linux) on a dataset of 86,824 Twitter users who generate 5,517,878 tweets over 1 month. Our approach can differentiate specialized software experts from other domain-related Twitter users with an F-Measure of up to 0.820. Compared with existing Twitter domain expert recommendation approaches, our proposed approach can outperform their F-Measure by at least 7.63%.	android;discriminative model;general-purpose markup language;javascript;linux;python;recommender system;silo (dataset);social media;software developer;software development;software engineering;subject-matter expert	Abhishek Sharma;Yuan Tian;Agus Sulistya;Dinusha Wijedasa;David Lo	2018	ACM Trans. Softw. Eng. Methodol.	10.1145/3266426	world wide web;computer science;subject-matter expert;theoretical computer science;recommender system;android (operating system);javascript;software development;social media;python (programming language);software	SE	-19.97234566543246	-59.823080627495216	175343
dcd09fd0b563eb33c108e874fad0f21f3ed9a855	relative rank statistics for dialog analysis	relative rank statistic;word frequency rank statistic;approach benefit;relative rank differential statistic;tf-idf cosine distance approach;event detection;non-parametric approach;dialog segment;dialog analysis;document tag;semantic evolution;term frequency;word frequency	We introduce the relative rank differential statistic which is a non-parametric approach to document and dialog analysis based on word frequency rank-statistics. We also present a simple method to establish semantic saliency in dialog, documents, and dialog segments using these word frequency rank statistics. Applications of our technique include the dynamic tracking of topic and semantic evolution in a dialog, topic detection, automatic generation of document tags, and new story or event detection in conversational speech and text. Our approach benefits from the robustness, simplicity and efficiency of non-parametric and rank based approaches and consistently outperformed term-frequency and TF-IDF cosine distance approaches in several experiments conducted.	cosine similarity;discriminant;distortion;experiment;information filtering system;knuth–morris–pratt algorithm;latent semantic analysis;neural coding;online and offline;online chat;rrdtool;tf–idf;web query classification;word lists by frequency;dialog	Juan M. Huerta	2008			natural language processing;speech recognition;computer science;pattern recognition;word lists by frequency;linguistics;tf–idf	NLP	-24.87788424473042	-62.709804715934574	175500
d81490e3b9fe92b093bc2ec955b7e00c41e96312	enhanced filter feature selection methods for arabic text categorization		The﻿filtering﻿of﻿a﻿large﻿amount﻿of﻿data﻿is﻿an﻿important﻿process﻿in﻿data﻿mining﻿tasks,﻿particularly﻿ for﻿the﻿categorization﻿of﻿unstructured﻿high﻿dimensional﻿data.﻿Therefore,﻿a﻿feature﻿selection﻿process﻿ is﻿desired﻿to﻿reduce﻿the﻿space﻿of﻿high﻿dimensional﻿data﻿into﻿small﻿relevant﻿subset﻿dimensions﻿that﻿ represent﻿the﻿best﻿features﻿for﻿text﻿categorization.﻿In﻿this﻿article,﻿three﻿enhanced﻿filter﻿feature﻿selection﻿ methods,﻿Category﻿Relevant﻿Feature﻿Measure,﻿Modified﻿Category﻿Discriminated﻿Measure,﻿and﻿Odd﻿ Ratio2,﻿are﻿proposed.﻿These﻿methods﻿combine﻿the﻿relevant﻿information﻿about﻿features﻿in﻿both﻿the﻿ inter-﻿and﻿intra-category.﻿The﻿effectiveness﻿of﻿the﻿proposed﻿methods﻿with﻿Naïve﻿Bayes﻿and﻿associative﻿ classification﻿is﻿evaluated﻿by﻿traditional﻿measures﻿of﻿text﻿categorization,﻿namely,﻿macro-averaging﻿of﻿ precision,﻿recall,﻿and﻿F-measure.﻿Experiments﻿are﻿conducted﻿on﻿three﻿Arabic﻿text﻿datasets﻿used﻿for﻿ text﻿categorization.﻿The﻿experimental﻿results﻿showed﻿that﻿the﻿proposed﻿methods﻿are﻿able﻿to﻿achieve﻿ better﻿and﻿comparable﻿results﻿when﻿compared﻿to﻿12﻿well﻿known﻿traditional﻿methods. KEywoRdS Arabic Text Categorization, Associative Classification, Feature Selection, Naïve Bayes	categorization;document classification;feature selection;naive bayes classifier	Abdullah Saeed Ghareb;Azuraliza Abu Bakar;Qasem A. Al-Radaideh;Abdul Razak Hamdan	2018	IJIRR	10.4018/IJIRR.2018040101	naive bayes classifier;arabic;clustering high-dimensional data;filter (signal processing);associative property;categorization;feature selection;artificial intelligence;recall;pattern recognition;computer science	ML	-21.002996935076943	-64.85456415523635	176907
b6b602640e6940ec4957f86738b344fc5f7be71e	what makes a good biography?: multidimensional quality analysis based on wikipedia article feedback data	wikipedia;trustworthiness;quality assessment;machine learning;collaborative knowledge resources;neutrality;document classification;natural language processing	With more than 22 million articles, the largest collaborative knowledge resource never sleeps, experiencing several article edits every second. Over one fifth of these articles describes individual people, the majority of which are still alive. Such articles are, by their nature, prone to corruption and vandalism. Manual quality assurance by experts can barely cope with this massive amount of data. Can it be effectively replaced by feedback from the crowd? Can we provide meaningful support for quality assurance with automated text processing techniques? Which properties of the articles should then play a key role in the machine learning algorithms and why? In this paper, we study the user-perceived quality of Wikipedia articles based on a novel Wikipedia user feedback dataset. In contrast to previous work on quality assessment which mostly relied on judgements of active Wikipedia authors, we analyze ratings of ordinary Wikipedia users along four quality dimensions (Complete, Well written, Trustworthy and Objective). We first present an empirical analysis of the novel dataset with over 36 million Wikipedia article ratings. We then select a subset of biographical articles and perform classification experiments to predict their quality ratings along each of the dimensions, exploring multiple linguistic, surface and network properties of the rated articles. Additionally, we study the classification performance and differences for the biographies of living and dead people as well as those for men and women. We demonstrate the effectiveness of our approach by the F-scores of 0.94, 0.89, 0.73, and 0.73 for the dimensions Complete, Well written, Trustworthy, and Objective. Based on the results, we believe that the quality assessment of big textual data can be effectively supported by current text classification and language processing tools.	algorithm;document classification;experiment;machine learning;objective-c;text corpus;trustworthy computing;wikipedia	Lucie Flekova;Oliver Ferschke;Iryna Gurevych	2014		10.1145/2566486.2567972	trustworthiness;computer science;data science;machine learning;data mining;brand;database;world wide web	Web+IR	-24.043622555430705	-59.53980415649231	177143
11952770b1b2a934f371c4de9b734683dc482733	across-document neighborhood expansion: umass at tac kbp 2012 entity linking		Last year’s competition demonstrated that the NER context contains important information that should not be ignored in entity linking. State-of-the-art approaches use a joint model of candidate assignments, after Wikipedia candidates have been selected. Current candidate approaches may lead to very large candidate sets. UMass has two objectives for our TAC submission. First, we use cross-document context information to perform entity neighborhood expansion and estimate the importance of entity context using corpus-wide information. Second, we use probabilistic information retrieval that incorporates the neighborhood information to generate a ranked candidate set in a single step. The result is a small candidate set that even for less than 50 candidates contains the true answer in 95% of the cases, allowing for computationally intensive inference in the next phase. It turns out that our best performing run simply predicts the top candidate of the unsupervised candidate ranking, outperforming more than half of the contestants.	entity linking;information retrieval;unsupervised learning;wikipedia;windows 95	Laura Dietz;Jeff Dalton	2012			anchor text;data mining;computer science;probabilistic logic;entity linking;inference;ranking	NLP	-26.23493741289595	-65.92786327904811	177342
042609a543792107b78aafe45f834cb9ee2111cf	update summarization based on novel topic distribution	sentence extraction;text summarization;multi document summarization;prior knowledge;generalized singular value decomposition;summary evaluation;iterative residual rescaling;semantic space;latent semantic analysis	This paper deals with our recent research in text summarization. The field has moved from multi-document summarization to update summarization. When producing an update summary of a set of topic-related documents the summarizer assumes prior knowledge of the reader determined by a set of older documents of the same topic. The update summarizer thus must solve a novelty vs. redundancy problem. We describe the development of our summarizer which is based on Iterative Residual Rescaling (IRR) that creates the latent semantic space of a set of documents under consideration. IRR generalizes Singular Value Decomposition (SVD) and enables to control the influence of major and minor topics in the latent space. Our sentence-extractive summarization method computes the redundancy, novelty and significance of each topic. These values are finally used in the sentence selection process. The sentence selection component prevents inner summary redundancy. The results of our participation in TAC evaluation seem to be promising.	automatic summarization;multi-document summarization;singular value decomposition;the sentence;update (sql)	Josef Steinberger;Karel Jezek	2009		10.1145/1600193.1600239	multi-document summarization;latent semantic analysis;computer science;automatic summarization;pattern recognition;data mining;database;information retrieval	Web+IR	-25.515846665589596	-63.409033271909564	178668
950a3e0fc84096c309623d24078ca98d82940d46	a novel method of automobiles' chinese nickname recognition	search engine;nickname recognition;data mining;maximum matching;named entity recognition;machine learning;automobile;named entity	Nowadays, we have noticed that the free writing style becomes more and more popular. People tend to use nicknames to replace the original names. However, the traditional named entity recognition does not perform well on the nickname recognition problem. Thus, we chose the automobile domain and accomplished a whole process of Chinese automobiles' nickname recognition. This paper discusses a new method to tackle the problem of automobile's nickname recognition in Chinese text. First we have given the nicknames a typical definition. Then we have used methods of machine learning to acquire the probabilities of transition and emission based on our training set. Finally the nicknames are identified through maximum matching on the optimal state sequence. The result revealed that our method can achieve competitive performance in nickname recognition. We got precision 95.2%; recall 91.5% and F-measure 0.9331 on our passages test set. The method will contribute to build a database of nicknames, and could be used in data mining and search engines on automobile domain, etc.		Cheng Wang;Wenyuan Yu;Wenxin Li;Zhuoqun Xu	2009		10.1007/978-3-642-00831-3_7	speech recognition;computer science;machine learning;pattern recognition;data mining;search engine;matching	Vision	-24.071354893568238	-66.05321497420225	178696
c5c25905dc0816a9fedcbdddde9bb2f64feb6dff	class-driven correlation learning for chinese document categorization using discriminative features	correlation learning;discriminative features;learning methods;document categorization;decision rule	This paper proposes a class-driven correlation learning method for Chinese document categorization to assign one suitable category in the first level to a document. Discriminative features are selected from candidate terms with high occurrence probability in each category. Class-driven correlation learning is then performed to produce a set of projections and further construct a code matrix to record the correlations between different classes of documents. A new document is classified by implementing the decision rule through the results from class-driven correlation learning. The competitive results from the experiments performed on TanCorp corpus indicate the superiority of the proposed method.	categorization;corpus linguistics;document classification;experiment	Xian Wu;Lingli Zhou;Xiang Li;Jianhuang Lai	2011		10.1145/2043674.2043715	computer science;machine learning;pattern recognition;information retrieval;categorization	Web+IR	-21.133986060438787	-64.00112740386133	178704
c40a218857bf00c1972a7da482b18175dcdb0805	stretching bayesian learning in the relevance feedback of image retrieval	distance measure;posterior probability;small samples;bayesian learning;experimental evaluation;learning strategies;relevance feedback;image retrieval	This paper is about the work on user relevance feedback in image retrieval. We take this problem as a standard two-class pattern classification problem aiming at refining the retrieval precision by learning through the user relevance feedback data. However, we have investigated the problem by noting two important unique characteristics of the problem: small sample collection and asymmetric sample distributions between positive and negative samples. We have developed a novel approach to stretching Bayesian learning to solve for this problem by explicitly exploiting the two unique characteristics, which is the methodology of BAyesian Learning in Asymmetric and Small sample collections, thus called BALAS. Different learning strategies are used for positive and negative sample collections in BALAS, respectively, based on the two unique characteristics. By defining the relevancy confidence as the relevant posterior probability, we have developed an integrated ranking scheme in BALAS which complementarily combines the subjective relevancy confidence and the objective feature-based distance measure to capture the overall retrieval semantics. The experimental evaluations have confirmed the rationale of the proposed ranking scheme, and have also demonstrated that BALAS is superior to an existing relevance feedback method in the current literature in capturing the overall retrieval semantics.	design rationale;image retrieval;relevance feedback;similarity measure;statistical classification	Ruofei Zhang;Zhongfei Zhang	2004		10.1007/978-3-540-24672-5_28	computer vision;image retrieval;computer science;machine learning;pattern recognition;data mining;mathematics;posterior probability;bayesian inference;statistics	Vision	-20.025322840545908	-62.2934258633411	178706
93b6329091e215b9ef007a85c07635f09e7b8adb	knowledge graph refinement: a survey of approaches and evaluation methods	004 informatik	In the recent years, different web knowledge graphs, both free and commercial, have been created, with DBpedia, YAGO, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and NLP methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of knowledge graphs, various refinement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph refinement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.	correctness (computer science);dbpedia;error detection and correction;freebase;holism;knowledge graph;knowledge base;literal (computer programming);machine learning;natural language processing;performance evaluation;refinement (computing);scalability;semiconductor industry;usability;wikipedia;yago	Heiko Paulheim	2017	Semantic Web	10.3233/SW-160218	philosophy;sociology	NLP	-24.193406582659815	-59.52398111084708	179360
5d52691fa4d9ce94626c8b524afd26e6c24f52ab	linear operators in information retrieval		In this paper, we propose a pseudo-relevance feedback approach based on linear operators: vector space basis change and cross product. The aim of pseudo-relevance feedback methods based on vector space basis change IBM (Ideal Basis Method) is to optimally separate relevant and irrelevant documents. Whereas the aim of pseudo-relevance feedback method based on cross product AI (Absorption of irrelevance) is to effectively exploit irrelevant documents. We show how to combine IBM methods with AI methods. The combination methods IBM+AI are evaluated experimentally on two TREC collections (TREC-7 ad hoc and TREC-8 ad hoc). The experiments show that these methods improve previous works. 1998 ACM Subject Classification H.3.3 Information Search and Retrieval	basis (linear algebra);experiment;hoc (programming language);information retrieval;relevance feedback;text retrieval conference	Hawete Hattab;Rabeb Mbarek	2017		10.4230/OASIcs.SLATE.2017.23	basis (linear algebra);machine learning;speech recognition;operator (computer programming);computer science;ibm;exploit;cross product;artificial intelligence	AI	-25.71395713848257	-61.55322606251326	179829
2eab9b0bcd4968b8ff65485d19d05e460c5d8e94	hyperlink ensembles: a case study in hypertext classification	web pages	In this paper , we introducehyperlink ensembles,a novel type of ensemble classifierfor classifyinghypertext documents.Insteadof usingthetext on a page for deriving featuresthat canbe usedfor training a classifier , we suggestto use portionsof texts from all pagesthatpoint to thetargetpage.A hyperlinkensemble is formed by obtainingone predictionfor eachhyperlink that points to a page. Theseindividual predictionsfor eachhyperlink aresubsequentlycombinedto a final predictionfor theclassof thetargetpage.We explorefour differentwaysof combiningtheindividual predictionsandfour differenttechniquesfor identifying relevant text portions. The utility of our approachis demonstratedon a set of Web-pagesthatrelateto ComputerScienceDepartments.	hyperlink;hypertext;neural ensemble	Johannes Fürnkranz	2002	Information Fusion	10.1016/S1566-2535(02)00090-8	computer science;machine learning;web page;data mining;world wide web;information retrieval	Web+IR	-24.1589372991418	-61.69238146960217	180060
59f2acbbb2d7a087bda16cc3eef550adfb0b0e5a	experiments on routing, filtering and chinese text retrieval in trec-5	principal component analysis	We describes our experiments in the routing, ltering and Chinese text retrieval. We based our routing and ltering experiments on our discriminant project algorithm. The algorithm sequentially constructs a series of orthogonal axis from the training documents using the Gram-Schmidt procedure. It then rotates the resulting subspace using principal component analysis so that the axis are ordered by their importance. For Chinese text retrieval, we experimented both with an automatic method and a manual method. For the automatic method, we use all phrases in the description eld and compute the aggregate scores using the simple tf:id f formula. We then manually construct boolean phrase queries which are thought to improve the results.	aggregate data;algorithm;apache axis;discriminant;document retrieval;experiment;principal component analysis;routing;schmidt decomposition	Chong-Wah Ngo;Kok F. Lai	1996			filter (signal processing);data mining;information retrieval;orthogonal coordinates;computer science;principal component analysis;eigenvalues and eigenvectors;phrase;semantics;pattern recognition;artificial intelligence;discriminant	Web+IR	-22.45029576036322	-64.35399293679981	181346
8b834a3d81909bc35a2a94ccccee6417d71fdb6f	graphrep: boosting text mining, nlp and information retrieval with graphs		Graphs have been widely used as modeling tools in Natural Language Processing (NLP), Text Mining (TM) and Information Retrieval (IR). Traditionally, the unigram bag-of-words representation is applied; that way, a document is represented as a multiset of its terms, disregarding dependencies between the terms. Although several variants and extensions of this modeling approach have been proposed, the main weakness comes from the underlying term independence assumption; the order of the terms within a document is completely disregarded and any relationship between terms is not taken into account in the final task. To deal with this problem, the research community has explored various representations, and to this direction, graphs constitute a well-developed model for text representation. The goal of this tutorial is to offer a comprehensive presentation of recent methods that rely on graph-based text representations to deal with various tasks in Text Mining, NLP and IR.	bag-of-words model;information retrieval;n-gram;natural language processing;text mining	Michalis Vazirgiannis;Fragkiskos D. Malliaros;Giannis Nikolentzos	2018		10.1145/3269206.3274273	information retrieval;boosting (machine learning);deep learning;multiset;natural language processing;statistical assumption;computer science;artificial intelligence;graph;text mining	Web+IR	-21.560662376920444	-66.10005229849708	181957
9305a8258701d3139d59e983d66fb70ae472824a	question matching based on fuzzy set	libraries;fuzzy sets libraries algorithm design and analysis fuzzy systems telecommunication computing telecommunication standards measurement standards natural language processing natural languages search engines;fuzzy set;information retrieval;probability density function;question answering system question matching fuzzy set sentence similarity computing;data mining;fuzzy set theory;question answering system;pattern matching fuzzy set theory information retrieval natural language processing;pattern matching;dictionaries;question matching;sentence similarity;natural language processing;algorithm design and analysis;sentence similarity computing;question answering;question matching question answering system sentence similarity	Sentence similarity computing plays an important role in the question answering (QA) system. Because there are many question expressions for one meaning, we present a new approach to match question based on fuzzy set. In this paper, we establish a library of standard questions. Each standard question is relative with a series of Keywords. The main focus of this paper lies with matching of standard questions and questions asked by users. An experimental system based on the proposed method has been built, and the results of our experiments shows the proposed method is effective for question matching.	algorithm;experiment;experimental system;fuzzy set;question answering;semantic similarity	Tianjiao Gu;Fuji Ren	2008	2008 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2008.4906799	natural language processing;computer science;data mining;information retrieval	DB	-24.720731076516564	-64.1108673076004	182408
dcc86f46667f831ac46e54e1d3a3067563900d43	relation extraction from wikipedia leveraging intrinsic patterns	wikipedia;semantics;intrinsic patterns;distant supervision;relation extraction;training data;internet;athena framework relation extraction wikipedia intrinsic patterns textual knowledge structured knowledge extraction;electronic publishing;wikipedia relation extraction distant supervision intrinsic patterns knowledge base;encyclopedias;electronic publishing internet encyclopedias knowledge based systems training data semantics;knowledge based systems;web sites knowledge acquisition;knowledge base	Enormous efforts of human volunteers have made Wikipedia become a treasure of textual knowledge. Relation extraction that aims at extracting structured knowledge in the unstructured texts in Wikipedia is an appealing but quite challenging problem because it's hard for machines to understand plain texts. Existing methods are not effective enough because they understand relation types in textual level without exploiting knowledge behind plain texts. In this paper, we propose a novel framework called Athena leveraging Intrinsic Patterns which are patterns that can understand relation types in semantic level to solve this problem. Extensive experiments show that Athena significantly outperforms existing methods.	blog;entity linking;experiment;information extraction;intranet;relationship extraction;unique identifier;web page;wikipedia	Yulong Gu;Weidong Liu;Jiaxing Song	2015	2015 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT)	10.1109/WI-IAT.2015.175	training set;knowledge base;the internet;computer science;knowledge management;artificial intelligence;data science;machine learning;data mining;brand;semantics;knowledge extraction;electronic publishing;world wide web;encyclopedia	AI	-24.061574989934332	-63.067854780613054	182415
383bb9e676dd000cdc0c90f0c1bee67ceb3de31b	spamcloud: a mapreduce based anti-spam architecture	filtering;architectures text processing algorithms performance distributed computing;unsolicited electronic mail;support vector machines;antispam architecture;internet scale algorithms;text processing;training;performance;information filtering;distributed computing;computer architecture;software architecture;internet;spam filtering;spam filtering spamcloud mapreduce antispam architecture internet scale algorithms hadoop map reduce framework;research evaluation;classification algorithms;support vector machines computer architecture filtering classification algorithms training unsolicited electronic mail;algorithms;unsolicited e mail information filtering internet software architecture;architectures;mapreduce;unsolicited e mail;hadoop map reduce framework;spamcloud	From a computing perspective and context, Spam can be described as an Internet scale problem. A potential approach for tackling Spam is consequently via the application of Internet scale algorithms and techniques. A number of approaches exist which are pitched to tackle this type of challenge in such fashion, including MapReduce. This research evaluates the degree of feasibility and applicability of Hadoop's Map Reduce Framework when applied to spam filtering.	algorithm;anti-spam techniques;apache hadoop;email filtering;mapreduce;spamming	Godwin Caruana;Maozhen Li;Hao Qi	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569282	filter;statistical classification;support vector machine;software architecture;the internet;performance;computer science;machine learning;data mining;database;world wide web	DB	-20.84176679652385	-60.972081507263766	182447
fb9424035216ac866ada3f04bad1dfa7661f1dfd	using cross-document random walks for topic-focused multi-document	within document relationships cross document random walks topic focused multidocument summarization graph ranking based methods cross document relationships;graph theory;search engines;information retrieval;text analysis graph theory information retrieval search engines;text analysis;multi document summarization;topic focused multidocument summarization;cross document relationships;random walk;computer science data mining;graph ranking based methods;cross document random walks;within document relationships	Graph-ranking based methods have been developed for generic multi-document summarization in recent years and they make uniform use of the relationships between sentences to extract salient sentences. This paper proposes to integrate the relevance of the sentences to the specified topic into the graph-ranking based method for topic-focused multi-document summarization. The cross-document relationships and the within-document relationships between sentences are differentiated and we apply the graph-ranking based method using each individual kind of sentence relationships and explore their relative importance for topic-focused multi-document summarization. Experimental results on DUC2003 and DUC2005 demonstrate the great importance of the cross-document relationships between sentences for topic-focused multi-document summarization. Even the approach based only on the cross-document sentence relationships can perform better than or at least as well as the approaches based on both kinds of sentence relationships	automatic summarization;multi-document summarization;relevance	Xiaojun Wan;Jianwu Yang;Jianguo Xiao	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence (WI 2006 Main Conference Proceedings)(WI'06)	10.1109/WI.2006.182	natural language processing;multi-document summarization;computer science;graph theory;automatic summarization;data mining;random walk;information retrieval	AI	-26.367967453318656	-63.50040852496554	183436
6f49c2dcdc019372c8a9616eb6dafd83c9f8fe24	improving the performance of text categorization using automatic summarization	silicon;text categorization automatic summarization feature selection;automatic summarization;training;text analysis;text summarization;data mining;feature vector;distance measurement;computational complexity;text categorization space technology computational modeling computer simulation computer science testing noise reduction equations;classification algorithms;feature selection;text analysis computational complexity;text categorization;knn algorithm text categorization automatic summarization feature vector space computing complexity text summarization feature selection;noise	In order to reduce the dimensionality of feature vector space and reduce the computing complexity of categorization, each document of the train set is summarized automatically and two approaches to text categorization based on these summaries are proposed: in the first approach, the text summarization is directly used for feature selection and categorization instead of the original text; in the second approach, each summary is used to select and weight features for each document, and free texts are classified using KNN algorithm. Experimental results show that the two proposed methods using automatic summarization can not only reduce the time of classifier training, but also improve the performance of text categorization.	automatic summarization;categorization;document classification;feature selection;feature vector;k-nearest neighbors algorithm	Xiao-yu Jiang;Xiaozhong Fan;Zhi-Fei Wang;Keliang Jia	2009	2009 International Conference on Computer Modeling and Simulation	10.1109/ICCMS.2009.29	text graph;text mining;feature vector;multi-document summarization;boosting methods for object categorization;computer science;noise;automatic summarization;machine learning;pattern recognition;silicon;computational complexity theory;information retrieval	Vision	-21.639502301685628	-63.865413124079296	183765
cdf3080979edaf64d9b805b55368d1cc593c1c67	test of complementarity on sentence extraction methods	computacion informatica;filologias;informacion documentacion;linguistica;ciencias basicas y experimentales;grupo a;ciencias sociales;grupo b	In this work three approaches to sentence extraction methods are analyzed. We try to find if the used methods show some complementary features. In order to accomplish this goal, the methods of sentence extraction were applied and combined, analyzing the results in the theoretical framework that we propose. We test three approaches: graph-based, keyword-based and representation-based. The methods were tested using the text collection DUC 2002, obtaining the best performance for a very simple method based on representation index. Even though no complementary methods were found, the results allow to identify some relevant features of the methods.	complementarity theory;sentence extraction;upsampling	Alberto Bañuelos-Moro;Héctor Jiménez-Salazar;José de Jesús Lavalle-Martínez	2008	Procesamiento del Lenguaje Natural		algorithm	NLP	-25.85825069366929	-63.572464844623276	184282
a9e157ef30a971154ad66b45735fd486d7f12845	improvement of factor model with text information based on factor model construction process				Ryosuke Saga;Tetsuya Fujita;Kodai Kitami;Kazunori Matsumoto	2013		10.3233/978-1-61499-262-2-222	data mining;computer science	Web+IR	-23.672430384455737	-61.829167404796564	185267
1833076fbc36eba5d9ff4df016f6848cb1ace28b	creating descriptive visual words for tag ranking of compressed social image	visualization image coding feature extraction vocabulary frequency measurement flowcharts discrete cosine transforms;compressed social image visual words tag ranking descriptive;visualwordrank ranking algorithm descriptive visual words compressed social image tag ranking visual word description method tag recommendation tag annotation unsupervised clustering methods;social networking online data compression image coding	Visual words description method has been widely applied in the fields of social image's tag ranking, tag recommendation and annotation. At present, visual words are usually obtained by unsupervised clustering methods which lead to generate many unnecessary and non-descriptive words. Therefore, how to make visual words be descriptive has become a very meaningful task for tag ranking of social image. However, for compressed social image on the network, visual words are created after fully decompressing a compressed image into pixel domain. In this paper, creating descriptive visual words in compressed domain is proposed for tag ranking of compressed social image. Firstly, the traditional visual words are created by using the partly decoded data; then the descriptive visual words are selected from traditional visual words by the VisualWordRank ranking algorithm; finally the descriptive visual words are applied to rank the tag of social image. Experimental results show the descriptive visual words can improve the accuracy of tag ranking, which further prove our method has more descriptive ability. Besides that, our method also reduces the processing time for compressed social image greatly.	algorithm;cluster analysis;code;pixel;tag cloud	Xin Liu;Jing Zhang;Li Zhuo;Ying Yang	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351536	computer science;machine learning;pattern recognition;information retrieval	Vision	-23.548659371906464	-65.45597358580575	185539
05f7cec9639021f78bc7793a732ed3aaf717e9c7	modeling personality traits of filipino twitter users		Recent studies in the field of text-based personality recognition experiment with different languages, feature extraction techniques, and machine learning algorithms to create better and more accurate models; however, little focus is placed on exploring the language use of a group of individuals defined by nationality. Individuals of the same nationality share certain practices and communicate certain ideas that can become embedded into their natural language. Many nationals are also not limited to speaking just one language, such as how Filipinos speak Filipino and English, the two national languages of the Philippines. The addition of several regional/indigenous languages, along with the commonness of codeswitching, allow for a Filipino to have a rich vocabulary. This presents an opportunity to create a text-based personality model based on how Filipinos speak, regardless of the language they use. To do so, data was collected from 250 Filipino Twitter users. Different combinations of data processing techniques were experimented upon to create personality models for each of the Big Five. The results for both regression and classification show that Conscientiousness is consistently the easiest trait to model, followed by Extraversion. Classification models for Agreeableness and Neuroticism had subpar performances, but performed better than those of Openness. An analysis on personality trait score representation showed that classifying extreme outliers generally produce better results for all traits except for Neuroticism and Openness.	algorithm;embedded system;experiment;feature extraction;machine learning;natural language;openness;performance;personally identifiable information;recommender system;rich internet application;text-based (computing);tf–idf;topic model;vocabulary	Edward P. Tighe;Charibeth K. Cheng	2018			social psychology;big five personality traits;psychology	ML	-21.0796405891784	-59.70266621021467	185779
02a8b5c2679fe441e25c9048674a438f8c5a7f39	topic and trend detection in text collections using latent dirichlet allocation	generic model;latent dirichlet allocation;trend detection	Algorithms that enable the process of automatically mining distinct topics in document collections have become increasingly important due to their applications in many fields and the extensive growth of the number of documents in various domains. In this paper, we propose a generative model based on latent Dirichlet allocation that integrates the temporal ordering of the documents into the generative process in an iterative fashion. The document collection is divided into time segments where the discovered topics in each segment is propagated to influence the topic discovery in the subsequent time segments. Our experimental results on a collection of academic papers from CiteSeer repository show that segmented topic model can effectively detect distinct topics and their evolution over time.	algorithm;archive;citeseerx;generative model;iteration;latent dirichlet allocation;topic model	Levent Bolelli;Seyda Ertekin;C. Lee Giles	2009		10.1007/978-3-642-00958-7_84	latent dirichlet allocation;dynamic topic model;pachinko allocation;computer science;data science;machine learning;pattern recognition;data mining;information retrieval;hierarchical dirichlet process	ML	-25.339132801657566	-59.42678885994503	187646
8f774aa56626cde7e502270959cae8ccd1aa9576	comparing similarity of concepts identified by temporal patterns of terms in biomedical research documents	drugs;pattern clustering;document handling;tree data structures document handling medical computing pattern clustering;tree data structures;medical computing;temporal text mining;temporal clustering;tree structure concept similarity biomedical research document relationship analysis temporal trend structured vocabulary temporal pattern extraction method automatic term extraction value clustering medical taxonomy;temporal clustering temporal text mining knowledge base;knowledge base	In this paper, we present an analysis of a relationship between temporal trends of automatically extracted terms in medical research document and their similarities on a structured vocabulary. In order to obtain the temporal trends, we used our temporal pattern extraction method that combines an automatic term extraction, an importance index of the terms, and clustering for the values in each period. By using a set of medical research documents that were published every year, we extracted temporal patterns of the automatically extracted terms. Then, we calculated their similarities on the medical taxonomy by defining a distance on the tree structure. For analyzing the relationship between the terms included in the patterns and the similarity of the terms on the taxonomy, the differences of the averaged similarities of the terms in each pattern are compared between the two trends of the temporal patterns.		Shusaku Tsumoto;Hidenao Abe	2012		10.1109/ICCI-CC.2012.6311131	computer science;pattern recognition;data mining;information retrieval	ML	-25.831384062949407	-63.91824740751022	188263
5d8b9f8a8f110143961d31de18d0816af8eb6033	a rough set and svm based approach to chinese textual affect sensing	databases;chinese textual affect sensing;support vector machines natural languages databases algorithm design and analysis intelligent systems design engineering emotion recognition text recognition engines tagging;automatic text classification chinese textual affect sensing rough set svm natural language chinese text emotion detection chinese text document text categorization human computer interaction;human computer interaction;information systems;support vector machines;sensors;rough set theory;training;text analysis classification emotion recognition human computer interaction natural languages rough set theory support vector machines;text analysis;emotion recognition;natural languages;classification;chinese text document;textual affect sensing affective computing;feature extraction;automatic text classification;natural language;svm;chinese text emotion detection;rough set;textual affect sensing;text categorization;affective computing	Text is an important modality for human-computer interaction, so studying the relationship between natural language and affective information as well as assessing the underpinned affective qualities of natural language has been the focus of research community. Several approaches have been performed to sense affect from English text, but the study on Chinese text emotion detection is still at the beginning. In this paper, we devote ourselves to sensing affective information from Chinese documents with the aim to group those into a set of emotions. A rough set and SVM based approach is adopted to categorize text into four emotional classes, including happy, sad, anger and surprise. Meanwhile, a Chinese textual emotion database is established to assist the processing.	categorization;emotion recognition;human–computer interaction;modality (human–computer interaction);natural language;rough set	Xia Mao;Zhijun Li;Haiyan Bao	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.54	natural language processing;support vector machine;speech recognition;rough set;computer science;machine learning;pattern recognition;affective computing;natural language	Robotics	-22.8910679876258	-65.02198007175959	188954
108d69c378cc1faeba62616d5cbc197f7c5d1cf9	multidimensional text classification for drug information	analysis of variance;machine learning;indexing terms;natural languages;k nearest neighbor;text analysis;hierarchical model;search engines;learning artificial intelligence;search engine	This paper proposes a multidimensional model for classifying drug information text documents. The concept of multidimensional category model is introduced for representing classes. In contrast with traditional flat and hierarchical category models, the multidimensional category model classifies each document using multiple predefined sets of categories, where each set corresponds to a dimension. Since a multidimensional model can be converted to flat and hierarchical models, three classification approaches are possible, i.e., classifying directly based on the multidimensional model and classifying with the equivalent flat or hierarchical models. The efficiency of these three approaches is investigated using drug information collection with two different dimensions: 1) drug topics and 2) primary therapeutic classes. In the experiments, k-nearest neighbor, na/spl inodot//spl uml/ve Bayes, and two centroid-based methods are selected as classifiers. The comparisons among three approaches of classification are done using two-way analysis of variance, followed by the Scheffe/spl acute/'s test for post hoc comparison. The experimental results show that multidimensional-based classification performs better than the others, especially in the presence of a relatively small training set. As one application, a category-based search engine using the multidimensional category concept was developed to help users retrieve drug information.	bayesian network;categories;class;classes - encounter;dimensions;document classification;experiment;hoc (programming language);k-nearest neighbors algorithm;large;naive bayes classifier;numerical aperture;online analytical processing;organizing (structure);sgpl1 gene;sample variance;single linkage cluster analysis;sodium;spironolactone;statistical classification;test set;web search engine;benefit;search a word	Verayuth Lertnattee;Thanaruk Theeramunkong	2004	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2004.832	text mining;computer science;artificial intelligence;machine learning;pattern recognition;data mining;world wide web;information retrieval;search engine	Web+IR	-21.98964419542997	-63.22102688830759	188961
39b59b03b5900651b5a92ac9a7d13bce42c6d62f	document type classification in online digital libraries	document type classification;supervised learning;scholarly digital libraries;structural features	Online digital libraries make it easier for researchers to search for scientific information. They have been proven as powerful resources in many data mining, machine learning and information retrieval applications that require high-quality data. The quality of the data highly depends on the accuracy of classifiers that identify the types of documents that are crawled from the Web, e.g., as research papers, slides, books, etc., for appropriate indexing. These classifiers in turn depend on the choice of the feature representation. We propose novel features that result in high-accuracy classifiers for document type classification. Experimental results on several datasets show that our classifiers outperform models that are employed in current systems.	bag-of-words model;book;data mining;digital library;information retrieval;library (computing);logic programming;machine learning;statistical classification;world wide web	Cornelia Caragea;Jian Wu;Sujatha Das Gollapalli;C. Lee Giles	2016			computer science;machine learning;pattern recognition;data mining;supervised learning;information retrieval	AI	-22.008260882558922	-61.543243056634985	189811
a86ca16740e87254364f9c7714062db573debf21	dynamic clustering of streaming short documents	cluster based retrieval;clustering;topic models;streaming text	Clustering technology has found numerous applications in mining textual data. It was shown to enhance the performance of retrieval systems in various different ways, such as identifying different query aspects in search result diversification, improving smoothing in the context of language modeling, matching queries with documents in a latent topic space in ad-hoc retrieval, summarizing documents etc. The vast majority of clustering methods have been developed under the assumption of a static corpus of long (and hence textually rich) documents. Little attention has been given to streaming corpora of short text, which is the predominant type of data in Web 2.0 applications, such as social media, forums, and blogs. In this paper, we consider the problem of dynamically clustering a streaming corpus of short documents. The short length of documents makes the inference of the latent topic distribution challenging, while the temporal dynamics of streams allow topic distributions to change over time. To tackle these two challenges we propose a new dynamic clustering topic model - DCT - that enables tracking the time-varying distributions of topics over documents and words over topics. DCT models temporal dynamics by a short-term or long-term dependency model over sequential data, and overcomes the difficulty of handling short text by assigning a single topic to each short document and using the distributions inferred at a certain point in time as priors for the next inference, allowing the aggregation of information. At the same time, taking a Bayesian approach allows evidence obtained from new streaming documents to change the topic distribution. Our experimental results demonstrate that the proposed clustering algorithm outperforms state-of-the-art dynamic and non-dynamic clustering topic models in terms of perplexity and when integrated in a cluster-based query likelihood model it also outperforms state-of-the-art models in terms of retrieval quality.	algorithm;blog;cluster analysis;discrete cosine transform;diversification (finance);hoc (programming language);language model;perplexity;smoothing;social media;streaming media;text corpus;topic model;web 2.0	Shangsong Liang;Emine Yılmaz;Evangelos Kanoulas	2016		10.1145/2939672.2939748	data stream clustering;dynamic topic model;document clustering;computer science;data science;machine learning;data mining;topic model;cluster analysis;information retrieval;clustering high-dimensional data	ML	-25.28533988496293	-59.29369168117189	190625
33eea39243f76612610b60a67e38fb23d42b9b8c	research of affective recognize based on n-gram	computers;feature extraction emotion recognition;emotion recognition;emotion extraction;psychology;n gram technology;n gram arithmetic affective recognize emotion recognition n gram technology affective units emotion extraction;affective recognize;artificial neural networks;computational modeling;artificial neural networks emotion recognition knowledge based systems entropy computational modeling psychology computers;feature extraction;n gram arithmetic;affective units;entropy;knowledge based systems	Generally, people have the opinion that the emotion is a macroscopically concept. In this paper, we put the emotion into affective units and introduce the affective units recognize which are built on the N-gram technology. Affective units recognize are just the technology about how to recognize the emotions information of human and how to compute them in granular state. The use of N-gram arithmetic can work out the problem in the process of recognize. Here, we discuss the question in two aspects. First, how to extract the emotion into small ones take can make the measurement to be uniform? Second, what prior is it by using N-gram arithmetic? In the last of the paper, there will be an experiment to discuss the feasibility of this method.	n-gram	Xue Weimin;Lin Benjing;Yu Bing	2008	2008 IEEE International Conference on Granular Computing	10.1109/GRC.2008.4664657	entropy;feature extraction;computer science;artificial intelligence;knowledge-based systems;machine learning;computational model;artificial neural network	Robotics	-22.819410181598368	-64.97689158826245	190694
ca8c2b9ea7f98b95260ee36583fa4fbed5942ade	local word bag model for text categorization	word processing information retrieval learning artificial intelligence text analysis;hierarchical clustering;documents representation;information retrieval;text processing;text analysis;discriminative machine learning;discriminative machine learning local word bag model text categorization text processing documents representation cosine distance information retrieval vg pyramid match kernel;vg pyramid match kernel;text categorization kernel support vector machines extraterrestrial measurements data mining laboratories machine intelligence asia data engineering text processing;machine learning;linear time;local word bag model;cosine distance;learning artificial intelligence;bag of words;similarity measure;text categorization;word processing	Many text processing applications adopted the bag of words (BOW) model representation of documents, in which each document is represented as a vector of weighted terms or n-grams, and then the cosine distance between two vectors is used as the similarity measurement. Although the great success in information retrieval and text categorization, the conventional BOW model ignores the detailed local text information, i.e. the co-occurrence pattern of words at sentence or paragraph level. In this paper, we propose a novel approach to represent a document as a set of local tf-idf vectors, or what we called local word bags (LWB). By encapsulating local information distributed around a document into multiple LWBs, we can measure the similarity of two documents via the partial match of their corresponding local bags. To perform the matching efficiently, we introduce the local word bag kernel (LWB kernel), a variant of VG-Pyramid match kernel. The new kernel enables the discriminative machine learning methods like SVM to compute the partial matching between two sets of LWBs in linear time after an one time hierarchical clustering procedure over all local bags at the initialization stage. Experiments on real world datasets demonstrate the effectiveness of our new approach.	bag-of-words model;categorization;cluster analysis;cosine similarity;document classification;experiment;grams;hierarchical clustering;information retrieval;kernel (operating system);lvm;machine learning;microsoft word for mac;n-gram;tf–idf;time complexity;unordered associative containers (c++)	Wen Pu;Ning Liu;Shuicheng Yan;Jun Yan;Kunqing Xie;Zheng Chen	2007	Seventh IEEE International Conference on Data Mining (ICDM 2007)	10.1109/ICDM.2007.69	time complexity;text mining;speech recognition;computer science;bag-of-words model;machine learning;pattern recognition;hierarchical clustering	ML	-24.59251290043118	-65.20190581473638	190797
7f4e4e87e0103e2280feec7db79aeac410df37cf	convolution kernels with feature selection for natural language processing tasks	conventional method;original kernel calculation process;natural language processing;real nlp task;convolution kernel;statistical feature selection;natural language processing task;over-fitting problem;new approach;nlp task;feature selection	Convolution kernels, such as sequence and tree kernels, are advantageous for both the concept and accuracy of many natural language processing (NLP) tasks. Experiments have, however, shown that the over-fitting problem often arises when these kernels are used in NLP tasks. This paper discusses this issue of convolution kernels, and then proposes a new approach based on statistical feature selection that avoids this issue. To enable the proposed method to be executed efficiently, it is embedded into an original kernel calculation process by using sub-structure mining algorithms. Experiments are undertaken on real NLP tasks to confirm the problem with a conventional method and to compare its performance with that of the proposed method.	algorithm;computer performance;convolution;embedded system;experiment;feature selection;kernel (operating system);natural language processing;structure mining	Jun Suzuki;Hideki Isozaki;Eisaku Maeda	2004			natural language processing;computer science;theoretical computer science;machine learning;pattern recognition	AI	-19.604147853562104	-64.39995434391312	191984
895a07ecbd1c4474b5f33ff781970abb1d294bf5	detecting topics popular in the recent past from a closed caption tv corpus as a categorized chronicle data		In this paper, we propose a method for extracting topics we were interested in over the course of the past 28 months from a closed-caption TV corpus. Each TV program is assigned one of the following genres: drama, informational or tabloid-style program, music, movie, culture, news, variety, welfare, or sport. We focus on informational/tabloid-style programs, dramas and news in this paper. Using our method, we extracted bigrams that formed part of the signature phrase of a heroine and the name of a hero in a popular drama, as well as recent world, domestic, showbiz, and so on news. Experimental evaluations show that our simple method is as useful as the LDA model for topic detection, and our closed-caption TV corpus has the potential value to act as a rich, categorized chronicle for our culture and social life.	bigram;categorization;closed-circuit television;sensor	Hajime Mochizuki;Kohji Shibano	2015		10.5220/0005612103420349	speech recognition;computer science;multimedia;information retrieval	NLP	-21.734557577443628	-59.566864208816845	192098
aca6bbec565356c7964c121b6b1d39ba0229c0e1	latent dirichlet learning for document summarization	dirichlet distribution;sentence based latent dirichlet allocation;sentence extraction;document summarization;text analysis learning artificial intelligence;automatic summarization;hierarchical sentence representation;latent dirichlet learning;resource management;vector space;bayesian methods;sentence selection automatic text document summarization latent dirichlet learning hierarchical word representation hierarchical sentence representation sentence based latent dirichlet allocation;text analysis;speech;hierarchical word representation;data mining;latent dirichlet allocation;sentence selection;hierarchical representation;computational modeling;fine structure;mixture model;optimization;learning artificial intelligence;document summarization latent dirichlet allocation language model sentence extraction;data mining linear discriminant analysis computer science internet web pages compaction functional analysis sampling methods convergence speech;automatic text document summarization;language model;data models	Automatic summarization is developed to extract the representative contents or sentences from a large corpus of documents. This paper presents a new hierarchical representation of words, sentences and documents in a corpus, and infers the Dirichlet distributions for latent topics and latent themes in word level and sentence level, respectively. The sentence-based latent Dirichlet allocation (SLDA) is accordingly established for document summarization. Different from the vector space summarization, SLDA is built to fit the fine structure of text documents, and is specifically designed for sentence selection. SLDA acts as a sentence mixture model with a mixture of Dirichlet themes, which are used to generate the latent topics in observed words. The theme model is inherent to distinguish sentences in a summarization system. In the experiments, the proposed SLDA outperforms other methods for document summarization in terms of precision, recall and F-measure.	automatic summarization;experiment;f1 score;latent dirichlet allocation;mixture model;text corpus;the sentence	Ying-Lang Chang;Jen-Tzung Chien	2009	2009 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2009.4959927	latent dirichlet allocation;natural language processing;dirichlet distribution;data modeling;speech recognition;vector space;bayesian probability;computer science;speech;resource management;fine structure;automatic summarization;machine learning;pattern recognition;mixture model;computational model;statistics;language model	NLP	-22.715793825623077	-63.90347309152479	192195
bc029fd3304f31b4fc3b136e1c62b03db914b3c1	tcsst: transfer classification of short & sparse text using external data	wikipedia;short sparse text mining;classification;external data;transfer learning;conference proceeding	"""Short & sparse text is becoming more prevalent on the web, such as search snippets, micro-blogs and product reviews. Accurately classifying short & sparse text has emerged as an important while challenging task. Existing work has considered utilizing external data (e.g. Wikipedia) to alleviate data sparseness, by appending topics detected from external data as new features. However, training a classifier on features concatenated from different spaces is not easy considering the features have different physical meanings and different significance to the classification task. Moreover, it exacerbates the """"curse of dimensionality"""" problem. In this study, we propose a transfer classification method, TCSST, to exploit the external data to tackle the data sparsity issue. The transfer classifier will be learned in the original feature space. Considering that the labels of the external data may not be readily available or sufficiently enough, TCSST further exploits the unlabeled external data to aid the transfer classification. We develop novel strategies to allow TCSST to iteratively select high quality unlabeled external data to help with the classification. We evaluate the performance of TCSST on both benchmark as well as real-world data sets. Our experimental results demonstrate that the proposed method is effective in classifying very short & sparse text, consistently outperforming existing and baseline methods."""	baseline (configuration management);benchmark (computing);blog;concatenation;curse of dimensionality;display resolution;feature vector;iteration;iterative method;neural coding;sampling (signal processing);sparse matrix;statistical classification;wikipedia	Guodong Long;Ling Chen;Xingquan Zhu;Chengqi Zhang	2012		10.1145/2396761.2396859	natural language processing;biological classification;transfer of learning;computer science;artificial intelligence;machine learning;pattern recognition;data mining;brand;database;world wide web;information retrieval	AI	-22.113282929948383	-62.31351986994828	194270
ec2485fb7adb7cdbba1b28e737cd6fa416a97159	wordnet-based and n-grams-based document clustering: a comparative study	document clustering;data representation wordnet document clustering unsupervised classifications kohonen self organizing maps n grams representation cosine distance euclidean distance squared euclidean distance manhattan distance reuters 21578 corpus f measure entropy lexical knowledge;pattern clustering;document handling;n grams;reuters 21578 document clustering self organizing maps of kohonen n grams wordnet similarity;self organising feature maps document handling pattern classification pattern clustering;government;reuters 21578 corpus;euclidean distance;data representation;unsupervised classifications;manhattan distance;distance measurement;self organising feature maps;kohonen self organizing maps;lexical knowledge;laboratories computer science self organizing feature maps euclidean distance broadband communication information technology application software biomedical measurements entropy internet;classification algorithms;similarity;comparative study;pattern classification;unsupervised classification;support vector machine classification;f measure;n grams representation;ontologies;self organizing maps of kohonen;self organized map;wordnet;cosine distance;entropy;reuters 21578;neurons;squared euclidean distance	A great number of methods of unsupervised classifications also called clustering were applied to the textual documents. In this paper, we initially propose the method of the self-organizing maps of Kohonen for the clustering of the textual documents based on the n-grams representation. The same method based on the synsets of WordNet as terms for the representation of the textual documents will be studied thereafter. The effects of these methods are examined in several experiments using 4 measurements of similarity: the Cosine distance, the Euclidean distance, the squared Euclidean distance and the Manhattan distance. The reuters-21578 corpus is used for evaluation. The evaluation was done, by using the F-measure and the entropy. The results obtained show that in spite of the good results obtained by the method of the n-grams, the fact of adding lexical knowledge in the representation makes it possible to build a better classification.	cluster analysis;computer cluster;entropy (information theory);euclidean distance;experiment;f1 score;grams;n-gram;organizing (structure);self-organization;self-organizing map;statistical classification;synonym ring;taxicab geometry;unsupervised learning;word-sense disambiguation;wordnet	Abdelmalek Amine;Zakaria Elberrichi;Michel Simonet;Mimoun Malki	2008	2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications	10.1109/BROADCOM.2008.7	machine learning;pattern recognition;data mining;mathematics	NLP	-23.13943375282521	-64.50322290880347	194952
27c21569b794fbcf3e27e55fe739488f4701f92e	geometrical feature based ranking using grey relational analysis (gra) for writer identification	handwriting recognition;t technology general;niobium;text analysis;grey relational analysis;writer identification;accuracy;accuracy writer identification features ranking feature combination grey relational analysis;hidden markov models;handwriting recognition feature extraction;global feature extraction methods geometrical feature based ranking grey relational analysis gra writer identification feature extraction process feature ranking method higher order united moment invariant humi;feature extraction;features ranking;feature extraction accuracy handwriting recognition niobium hidden markov models text analysis;feature combination	The author's unique characteristic is determined by the variation of generated features from feature extraction process. Different sets of features produced are based on different feature extraction methods (local or global). Thus, the process has led to the production of high dimensional datasets that contributing to many irrelevant or redundant features. The main problem however is to find a way to identify the most significant features. The features ranking method using Grey Relational Analysis (GRA) is proposed to find the significance of each feature and give ranking to the features. This study presents the Higher-Order United Moment Invariant (HUMI) as the global feature extraction methods. The combinations of features with the higher ranking are discretized and used as the subsets of features to identify the writer. The result demonstrates that the average classification accuracy of five classifiers by using just the combination of two most significant features have yielded a better performance than using all features.	discretization;feature extraction;grey relational analysis;image moment;randomness;relevance	Intan Ermahani A. Jalil;Azah Kamilah Muda;Siti Mariyam Hj. Shamsuddin;Anca L. Ralescu	2013	2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2013.7054118	niobium;feature extraction;computer science;grey relational analysis;machine learning;pattern recognition;data mining;accuracy and precision;handwriting recognition;feature	Vision	-20.50998519031372	-63.354032920094944	195981
7d41f5870389d6914aeea42bce6d427640125e25	giving temporal order to news corpus	tdt;timestamp estimation tdt stream data incremental clustering;temporal data;incremental clustering;stream data;text analysis;computational linguistics text analysis;event detection informatics data mining usa councils streaming media internet machine learning supervised learning training data;stream data temporal order incremental clustering tdt2 corpus timestamp estimation;computational linguistics;timestamp estimation	We propose a new mechanism to give temporal order to a news article in a form of times-tamps. Here we learn temporal data in advance to extract ordering by means of incremental clustering and then we estimate most likely order to news text. In this work, we examine TDT2 corpus and we show how well our approach works by some experiments.	algorithm;categorization;cluster analysis;computation;computational linguistics;computer science;digital library;document classification;experiment;heuristic;information retrieval;ishikawa diagram;lam/mpi;linguistic data consortium;medical transcription	Hiroshi Uejima;Takao Miura;Isamu Shioya	2004	16th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2004.65	data stream clustering;terminal deoxynucleotidyl transferase;computer science;artificial intelligence;data science;computational linguistics;machine learning;data mining;temporal database;information retrieval	Robotics	-24.21262086920376	-63.8268605711846	196065
6139e5a5778955dc142cb2ec87f6938d0e5af795	evaluating the descriptive power of instagram hashtags	instagram;machine learning;image tagging;hashtags;article;image retrieval	Image tagging is an essential step for developing Automatic Image Annotation (AIA) methods that are based on the learning by example paradigm. However, manual image annotation, even for creating training sets for machine learning algorithms, requires hard effort and contains human judgment errors and subjectivity. Thus, alternative ways for automatically creating training examples, i.e., pairs of images and tags, are pursued. In this work, we investigate whether tags accompanying photos in the Instagram can be considered as image annotation metadata. If such a claim is proved then Instagram could be used as a very rich, easy to collect automatically, source of training data for the development of AIA techniques. Our hypothesis is that Instagram hashtags, and especially those provided by the photo owner/creator, express more accurately the content of a photo compared to the tags assigned to a photo during explicit image annotation processes like crowdsourcing. In this context, we explore the descriptive power of hashtags by examining whether other users would use the same, with the owner, hashtags to annotate an image. For this purpose 1000 Instagram images were collected and one to four hashtags, considered as the most descriptive ones for the image in question, were chosen among the hashtags used by the photo owner. An online database was constructed to generate online questionnaires containing 20 images each, which were distributed to experiment participants so they can choose the best suitable hashtag for every image according to their interpretation. Results show that an average of 66% of the participants hashtag choices coincide with those suggested by the photo owners; thus, an initial evidence towards our hypothesis confirmation can be claimed. c ⃝ 2016 Qassim University. Production and Hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Peer review under responsibility of Qassim University. ∗ Corresponding author. E-mail addresses: s.giannoulakis@cut.ac.cy (S. Giannoulakis), nic http://dx.doi.org/10.1016/j.jides.2016.10.001 2352-6645/ c ⃝ 2016 Qassim University. Production and Hosting by E license (http://creativecommons.org/licenses/by-nc-nd/4.0/). olas.tsapatsoulis@cut.ac.cy (N. Tsapatsoulis). lsevier B.V. This is an open access article under the CC BY-NC-ND J O U R N A L O F I N N O VA T I O N I N D I G I T A L E C O S Y S T E M S 3 ( 2 0 1 6 ) 1 1 4 – 1 2 9 115	algorithm;automatic image annotation;crowdsourcing;hashtag;instagram;machine learning;norsk data;programming paradigm;tag (metadata)	Stamatios Giannoulakis;Nicolas Tsapatsoulis	2016	J. Innovation in Digital Ecosystems	10.1016/j.jides.2016.10.001	computer science;data mining;world wide web;information retrieval	NLP	-22.185376240966832	-60.0277327043635	196167
ed2b20c97f93869953f2c4d045554d9b3f8adaef	navigating the data lake with datamaran: automatically extracting structure from log datasets		"""Organizations routinely accumulate semi-structured log datasets generated as the output of code; these datasets remain unused and uninterpreted, and occupy wasted space---this phenomenon has been colloquially referred to as """"data lake'' problem. One approach to leverage these semi-structured datasets is to convert them into a structured relational format, following which they can be analyzed in conjunction with other datasets. We present DATAMARAN, an tool that extracts structure from semi-structured log datasets with no human supervision. DATAMARAN automatically identifies field and record endpoints, separates the structured parts from the unstructured noise or formatting, and can tease apart multiple structures from within a dataset, in order to efficiently extract structured relational datasets from semi-structured log datasets, at scale with high accuracy. Compared to other unsupervised log dataset extraction tools developed in prior work, DATAMARAN does not require the record boundaries to be known beforehand, making it much more applicable to the noisy log files that are ubiquitous in data lakes. DATAMARAN can successfully extract structured information from all datasets used in prior work, and can achieve 95% extraction accuracy on automatically collected log datasets from GitHub---a substantial 66% increase of accuracy compared to unsupervised schemes from prior work. Our user study further demonstrates that the extraction results of DATAMARAN are closer to the desired structure than competing algorithms."""	algorithm;data logger;semiconductor industry;unsupervised learning;usability testing	Yihan Gao;Silu Huang;Aditya G. Parameswaran	2018		10.1145/3183713.3183746	data mining;fold (higher-order function);computer science;information retrieval;disk formatting;phenomenon	DB	-21.961433377879178	-62.0560728948576	196424
570dbccc3085274cff3e0a1323257df74f6c945c	automatic distinction of fernando pessoas' heteronyms	text mining;haslab haslab uminho;machine learning;svm;authorship classification	Text Mining has opened a vast array of possibilities concerning automatic information retrieval from large amounts of text documents. A variety of themes and types of documents can be easily analyzed. More complex features such as those used in Forensic Linguistics can gather deeper understanding from the documents, making possible performing difficult tasks such as author identification. In this work we explore the capabilities of simpler Text Mining approaches to author identification of unstructured documents, in particular the ability to distinguish poetic works from two of Fernando Pessoas’ heteronyms: Álvaro de Campos and Ricardo Reis. Several processing options were tested and accuracies of 97% were reached, which encourage further developments.	information retrieval;text mining;theme (computing)	João B Teixeira;Marco Couto	2015		10.1007/978-3-319-23485-4_78	support vector machine;text mining;computer science;machine learning;pattern recognition;data mining	ML	-23.649452450341045	-64.77161928526817	196597
1ff6a08dd741215785d57d7de6f5b604a81ec3a3	a method of subtopic classification of search engine suggests by integrating a topic model and word embeddings			microsoft word for mac;topic model;web search engine	Tian Nie;Yi Ding;Chen Zhao;Youchao Lin;Takehito Utsuro	2018	IJSI	10.4018/IJSI.2018070105	information retrieval;topic model;search engine;computer science	NLP	-25.191779969618576	-61.83361769734207	196618
886f7675abbaed563c18966fdc0891ad0124beea	text assisted insight ranking using context-aware memory network		Extracting valuable facts or informative summaries from multi-dimensional tables, i.e. insight mining, is an important task in data analysis and business intelligence. However, ranking the importance of insights remains a challenging and unexplored task. The main challenge is that explicitly scoring an insight or giving it a rank requires a thorough understanding of the tables and costs a lot of manual efforts, which leads to the lack of available training data for the insight ranking problem. In this paper, we propose an insight ranking model that consists of two parts: A neural ranking model explores the data characteristics, such as the header semantics and the data statistical features, and a memory network model introduces table structure and context information into the ranking process. We also build a dataset with text assistance. Experimental results show that our approach largely improves the ranking precision as reported in multi evaluation metrics.	approximation algorithm;evaluation function;information;network model;similarity measure	Qi Zeng;Liangchen Luo;W. Huang;Yang Tang	2018	CoRR		artificial intelligence;machine learning;header;semantics;network model;business intelligence;training set;computer science;ranking	Web+IR	-19.668133684178702	-60.4054762689413	196888
75dee1680f09b85eac866710b579c82a11078f2f	multi-topic based query-oriented summarization	question answering;probabilistic model	Query-oriented summarization aims at extracting an informative summary from a document collection for a given query. It is very useful to help users grasp the main information related to a query. Existing work can be mainly classified into two categories: supervised method and unsupervised method. The former requires training examples, which makes the method limited to predefined domains. While the latter usually utilizes clustering algorithms to find ‘centered’ sentences as the summary. However, the method does not consider the query information, thus the summarization is general about the document collection itself. Moreover, most of existing work assumes that documents related to the query only talks about one topic. Unfortunately, statistics show that a large portion of summarization tasks talk about multiple topics. In this paper, we try to break limitations of the existing methods and study a new setup of the problem of multi-topic based query-oriented summarization. We propose using a probabilistic approach to solve this problem. More specifically, we propose two strategies to incorporate the query information into a probabilistic model. Experimental results on two different genres of data show that our proposed approach can effectively extract a multi-topic summary from a document collection and the summarization performance is better than baseline methods. The approach is quite general and can be applied to many other mining tasks, for example product opinion analysis and question answering.	academic search;algorithm;archive;automatic summarization;baseline (configuration management);cluster analysis;hypertext transfer protocol;information;multi-document summarization;query expansion;question answering;statistical model;topic model;unsupervised learning;upsampling;word lists by frequency	Jie Tang;Limin Yao;Dewei Chen	2009		10.1137/1.9781611972795.98	computer science;automatic summarization;probabilistic analysis of algorithms;machine learning;artificial intelligence;pattern recognition;divergence-from-randomness model;cluster analysis;data mining;probabilistic logic;probabilistic ctl;probabilistic relevance model;question answering	Web+IR	-26.360861479359023	-63.26017303612301	197233
3dbea298b0c14df482e1af0371067ecde9c25f96	classifying xml documents by using genre features	relevance model;probability;information retrieval community;information retrieval;logistic model;regression model;data fusion;rank probability model;information retrieval search engines logistics testing databases expert systems mathematical model bismuth mathematics web search;linear model;statistical regression technique regression relevance model data fusion information retrieval community rank probability model;regression relevance model;regression analysis;statistical regression technique;sensor fusion;relevance feedback;sensor fusion probability regression analysis relevance feedback	The categorization of documents is traditionally topic-based. This paper presents a complementary analysis of research and experiments on genre to show that encouraging results can be obtained by using genre structure (form) features. We conducted an experiment to assess the effectiveness of using eXtensible Mark-Up Language (XML) tag information, and part-of-speech (P-O-S) features, for the classification of genres, testing the hypothesis that if a focus on genre can lead to high precision on normal textual documents, then good results can be achieved using XML tag information in addition to P-O-S information. An experiment was carried out on a subsection of the initiative for the evaluation of XML (INEX) 1.4 collection. The features were extracted and documents were classified using machine learning algorithms, which yielded encouraging results for logistic regression and neural networks. We propose that utilizing these features and training a classifier may benefit retrieval for most World Wide Web (WWW) technologies such as XML and eXtensible Hypertext Markup Language) XHTML.	algorithm;artificial neural network;categorization;experiment;logistic regression;machine learning;markup language;www;world wide web;xhtml;xml	Malcolm Clark;Stuart N. K. Watt	2007	18th International Workshop on Database and Expert Systems Applications (DEXA 2007)	10.1109/DEXA.2007.120	computer science;machine learning;pattern recognition;data mining;sensor fusion;regression analysis;statistics	Web+IR	-21.761400458243045	-63.16574560673332	197846
341a42ab5ca5f6b8100a2f42dc026579a0d1f1b0	análisis de sentimientos y minería de opiniones: el corpus emotiblog	computacion informatica;filologias;info eu repo semantics article;informacion documentacion;aprendizaje automatico;linguistica;machine learning;emotiblog;ciencias basicas y experimentales;sentiment analysis;analisis de sentimientos;grupo a;ciencias sociales;grupo b	EmotiBlog is a collection of blog posts created and annotated for detecting subjective expressions in the new textual genres born with the Web 2.0. Previous work has demonstrated the relevance of the Machine learning systems as tool for detecting opinionated information. In this paper we explore additional features for a deep analysis of these techniques. Moreover, we compare EmotiBlog with the JRC collection. The obtained results demonstrate the usefulness of EmotiBlog and support us to continue in this research path.	blog;machine learning;relevance;sensor;web 2.0;world wide web	Javi Fernández;Ester Boldrini;José Manuel Gómez Soriano;Patricio Martínez-Barco	2011	Procesamiento del Lenguaje Natural		computer science;sentiment analysis	NLP	-25.741371326455855	-63.120107438874655	197996
0574e2876f0a1c7470d5764f483e828a8774605c	experiments in clustering homogeneous xml documents to validate an existing typology	organisational structure;unsupervised classification;xml document;feature selection	This paper presents some experiments in clustering homogeneous XML documents to validate an existing classification or more generally an organisational structure. Our approach integrates techniques for extracting knowledge from documents with unsupervised classification (clustering) of documents. We focus on the feature selection used for representing documents and its impact on the emerging classification. We mix the selection of structured features with fine textual selection based on syntactic characteristics. We illustrate and evaluate this approach with a collection of Inria activity reports for the year 2003. The objective is to cluster projects into larger groups (Themes), based on the keywords or different chapters of these activity reports. We then compare the results of clustering using different feature selections, with the official theme structure used by Inria.	biological anthropology;cluster analysis;database normalization;experiment;feature selection;norm (social);preprocessor;statistical classification;tf–idf;unsupervised learning;xml	Thierry Despeyroux;Yves Lechevallier;Brigitte Trousse;Anne-Marie Vercoustre	2005	CoRR		organizational structure;xml;computer science;data science;machine learning;data mining;feature selection;world wide web;information retrieval	Web+IR	-25.520288091672963	-60.03272153302837	198023
77a86695df168a010c01cec1f3ffa1e4b0e17ac8	combining content with user preferences for non-fiction multimedia recommendation: a study on ted lectures	content based filtering;ted lectures;collaborative filtering;content based multimedia indexing;multimedia recommendation;recommender systems	This paper introduces a new dataset and compares several methods for the recommendation of non-fiction audio visual material, namely lectures from the TED website. The TED dataset contains 1,149 talks and 69,023 profiles of users, who have made more than 100,000 ratings and 200,000 comments. The corresponding metadata, which we make available, can be used for training and testing generic or personalized recommender systems. We define content-based, collaborative, and combined recommendation methods for TED lectures and use cross-validation to select the best parameters of keyword-based (TFIDF) and semantic vector space-based methods (LSI, LDA, RP, and ESA). We compare these methods on a personalized recommendation task in two settings, a cold-start and a non-cold-start one. In the cold-start setting, semantic vector spaces perform better than keywords. In the non-cold-start setting, where collaborative information can be exploited, content-based methods are outperformed by collaborative filtering ones, but the proposed combined method shows acceptable performances, and can be used in both settings. For the generic recommendation task, LSI and RP again outperform TF-IDF.	algorithm;automatic summarization;benchmark (computing);bookmark (world wide web);cold start;collaborative filtering;comment (computer programming);cross-validation (statistics);esa;experiment;explicit semantic analysis;feature selection;feedback;ground truth;information;local-density approximation;modality (human–computer interaction);performance;personalization;rp (complexity);recommender system;risk assessment;speech recognition;ted;test set;tf–idf;user (computing);user-generated content	Nikolaos Pappas;Andrei Popescu-Belis	2013	Multimedia Tools and Applications	10.1007/s11042-013-1840-y	computer science;collaborative filtering;machine learning;multimedia;world wide web;information retrieval;recommender system	Web+IR	-25.84834293380213	-60.80141074182527	198421
861db09f8e89c30da2521cc3d2e58dcacf91166e	developing a dataset for technology structure mining	directed graphs;lexical representation;sentence annotation;information extraction;text mining;technology structure mining;scientific corpus mapping;semantics;text analysis;speech;natural languages;acl anthology corpus;data mining;human annotator;conference paper;labeled digraph;text mining technology structure mining scientific corpus mapping labeled digraph technology structure graph acl anthology corpus sentence annotation human language technology lexical representation termino conceptual layer semantic relations linguistic criteria human annotator;semantics ontologies data mining humans natural languages xml speech;nlp technology structure mining text mining;xml;technology structure graph;termino conceptual layer semantic relations;ontologies;humans;human language technology;text analysis data mining directed graphs natural language processing;natural language processing;nlp;linguistic criteria	This paper describes steps that have been taken to construct a development dataset for the task of Technology Structure Mining. We have defined the proposed task as the process of mapping a scientific corpus into a labeled digraph named a Technology Structure Graph as described in the paper. The generated graph expresses the domain semantics in terms of interdependencies between pairs of technologies that are named (introduced) in the target scientific corpus. The dataset comprises a set of sentences extracted from the ACL Anthology Corpus. Each sentence is annotated with at least two technologies in the domain of Human Language Technology and the interdependence between them. The annotations - technology mark-up and their interdependencies - are expressed at two layers: lexical and termino-conceptual. Lexical representation of technologies comprises varying lexicalizations of a technology. However, at the termino-conceptual layer all these lexical variations refer to the same concept. We have adopted the same approach for representing Semantic Relations, at the lexical layer a semantic relation is a predicate i.e. defined based on the sentence surface structure, however at the termino-conceptual layer semantic relations are classified into conceptual relations either taxonomic or non-taxonomic. Morover, the contexts that interdependencies are extracted from are classified into five groups based on the linguistic criteria and syntactic structure that are identified by the human annotators. The dataset initially comprises of 482 sentences. We hope this effort results in a benchmark that can be used for the technology structure mining task as defined in the paper.	benchmark (computing);directed graph;interdependence;language technology;modality (human–computer interaction);ontology components;parsing;part-of-speech tagging;structure mining;text corpus	Behrang Q. Zadeh;Paul Buitelaar;Fergal Monaghan	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.73	natural language processing;text mining;xml;directed graph;computer science;speech;ontology;data mining;database;linguistics;natural language;information retrieval	NLP	-25.065662954768314	-65.7363783863731	198469
05fb2a9cac2152b75ee6e3865b8db1bd68db972d	combination of multiple bipartite ranking for multipartite web content quality evaluation	multipartite ranking;decoding design;bipartite ranking;web content quality evaluation;encoding design	Web content quality evaluation is crucial to various web content processing applications. Bagging has a powerful classification capacity by combining multiple classifiers. In this study, similar to Bagging, multiple pairwise bipartite ranking learners are combined to solve the multipartite ranking problems for web content quality evaluation. Both encoding and decoding mechanisms are used to combine bipartite rankers to form a multipartite ranker and, ∗Corresponding author. Tel: +86-010-58812272 Email addresses: xbjin9801@gmail.com (Xiao-Bo Jin), gengguanggang@cnnic.cn (Guang-Gang Geng), minghe.sun@utsa.edu (Minghe Sun), zdxzzit@hotmail.com (Dexian Zhang) Preprint submitted to Neurocomputing August 21, 2014 hence, the multipartite ranker is called MultiRank.ED. Both binary encoding and ternary encoding extend each rank value to an L − 1 dimensional vector for a ranking problem with L different rank values. Predefined weighting and adaptive weighting decoding mechanisms are used to combine the ranking results of bipartite rankers to obtain the final ranking results. In addition, some theoretical analyses of the encoding and the decoding strategies in the MultiRank.ED algorithm are provided. Computational experiments using the DC2010 datasets show that the combination of binary encoding and predefined weighting decoding yields the best performance in all four combinations. Furthermore, this combination performs better than the best winning method of the DC2010 competition.	algorithm;binary file;bootstrap aggregating;c4.5 algorithm;call of duty: black ops;computation;email;experiment;nist hash function competition;neurocomputing;uniform resource identifier;web content;web page;xiaoice	Xiao-Bo Jin;Guanggang Geng;Minghe Sun;Dexian Zhang	2015	Neurocomputing	10.1016/j.neucom.2014.08.067	machine learning;pattern recognition;data mining;mathematics;ranking svm	Web+IR	-20.562684012560016	-63.46266071379528	198684
5c6232065b89520cf7fe0c1314a18d0b774cf687	applying lda in contextual image retrieval - redcad participation at imageclef flickr photo retrieval 2012		This paper describes our participation in photo Flickr retrieval task at the ImageCLEF 2012 Campaign. Our aim is to evaluate the performance of topic models, such as Latent Dirichlet Allocation (LDA), in image retrieval based on the textual information surrounding the images. To do this, we propose to extract topics from Flickr user tags using the LDA topic model. Then, we use the Jensen-Shannon Divergence measure to compute the similarity between queries and user tags representing images.	flickr;image retrieval;jensen's inequality;latent dirichlet allocation;shannon (unit);topic model	Hatem Awadi;Mouna Torjmen Khemakhem;Maher Ben Jemaa	2012				AI	-25.19104517488163	-60.774975483619805	198915
