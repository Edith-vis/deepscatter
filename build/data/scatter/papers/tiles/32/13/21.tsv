id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
eaa9ba77ff406fdced2a1efc77cb563c9c7423da	modelling the influence of unreliable software in distributed computer systems	distributed system;control systems;system dependability model;software system;unreliable software;distributed processing;distributed computing;software systems;physics computing;software reliability distributed processing fault tolerant computing;software performance;distributed computer systems;fault tolerant computing;error propagation;fault tolerant systems;distributed computing system;error correction;model unreliable software distributed computer systems system dependability model software system;transient fault;model;couplings;software reliability;distributed computing software systems computer errors hardware couplings physics computing error correction software performance fault tolerant systems control systems;computer errors;hardware	The author develops a system dependability model accounting for the influence of software faults. When one software system resides on a set of distributed computers or processing units, it is necessary to comprehend the logical linkage between system units. The model is based on the concept that a logical fault in a system will cause an inconsistent (erroneous) internal state in the processing unit where the fault is activated, and that this error can cause a failure of the processing unit, be corrected without causing a failure, or propagate to other cooperating processing units. The model also accounts for the logical hardware faults and the transient faults, which have a similar pattern of manifestations. Analysis of the stationary dependability characteristics of distributed systems is discussed. An approximate procedure which allows evaluation of fairly large systems is outlined. A brief example is presented which demonstrates that even a low error propagation may have large performance consequences. >	distributed computing	Bjarne E. Helvik	1988		10.1109/FTCS.1988.5311	real-time computing;computer science;distributed computing;computer engineering	Metrics	-23.74975511964455	41.887636361932074	187602
8abe23627b356c7dc212255d7a01e07d89c00852	time bounds for shared objects in partially synchronous systems	shared objects;time complexity;synchronous system;thesis;partially synchronous system;bounds on time complexity;linearizability	Shared objects are a key component in today’s ever growing distributed systems. Applications ranging from electronic commerce to social media on hand-held devices require shared data. Linearizability (or atomicity) is a popular and easy-to-use consistency condition for such shared objects which gives the illusion of sequential execution of operations. We focus on shared objects of arbitrary data types (e.g., stacks, queues, sets, read-modify-write objects) in addition to basic read/write registers. These applications share data among a large number, n, of geographically dispersed processes, which can communicate only over a message passing system. We assume no process failures and a reliable point-to-point message passing system. We also assume the system is partially synchronous in that it provides bounded but uncertain message delays and approximately synchronized clocks. More specifically, the time for the delivery of any message between any two processes falls in a certain range of [d− u, d], where d is the message delay upper bound and u is the message delay uncertainty. We assume that the time for local computation is negligible compared to the message delays. The maximum clock skew between any two processes is denoted . It has been shown that the optimal value of in this model is (1− 1 n )u [3]. To guarantee the linearizability of an object of arbitrary data type, a centralized mechanism can perform each operation with time at most 2d in the worst case, since the message from the invoking process to the control center takes at most d and the response message also takes at most d. Alternatively, one can use a total order broadcast primitive, but this is not faster than the centralized scheme when taking into account the time overhead to implement the totally ordered broadcast on top of a point-to-point message system [1]. Increasing pressure to speed up applications raises ∗Supported in part by NSF grant 0964696	atomicity (database systems);best, worst and average case;centralized computing;clock skew;computation;distributed computing;e-commerce;fibre channel point-to-point;ibm notes;linearizability;message passing;mobile device;optimization problem;overhead (computing);process (computing);read-modify-write;social media	Jiaqi Wang;Jennifer L. Welch;Hyunyoung Lee	2011		10.1145/1993806.1993877	time complexity;real-time computing;linearizability;computer science;theoretical computer science;mathematics;distributed computing;algorithm	Arch	-20.809412181521232	45.59470244360064	187744
8e3e0ce896ee27ca3df4790436a785b16966b895	enabling snap-stabilizatio	propagationof information with feedback;protocols;formal specification;fault tolerant;termination detection;arbitrary networks snap stabilizing protocol leader election protocol reset protocol snapshot protocol termination detection protocol feedback protocol;formal specification protocols fault tolerant computing multiprocessing systems stability;protocols feedback distributed computing broadcasting nominations and elections computer science process design fault tolerant systems fuzzy systems algorithm design and analysis;stability;self stabilization;fault tolerant computing;snapshot;fault tolerance;leader election;multiprocessing systems;snap stabilization;reset protocols;propagation of information with feedback	A snap-stabilizing protocol guarantees that the system always behaves according to its specification provided some processor initiated the protocol. We present how to snapstabilize some important protocols, like Leader Election, Reset, Snapshot, and Termination Detection. We use a Snap-stabilizing Propagation of Information with Feedback protocol for arbitrary networks as the key module in the above transformation process. Finally, we design a universal transformer to provide a snap-stabilizing version of any protocol (which can be self-stabilized with the transformer of [15]).	leader election;modulo operation;path integral formulation;self-stabilization;semiconductor industry;shared memory;snapshot (computer storage);snapshot isolation;transformer	Alain Cournier;Ajoy Kumar Datta;Franck Petit;Vincent Villain	2003		10.1109/ICDCS.2003.1203447	fault tolerance;universal composability;real-time computing;two-phase commit protocol;computer science;distributed computing;computer network	Theory	-23.926326016064127	45.140483284525594	188351
71430e8ce56c28bd46f26cbede9b884cb61f37cb	brief announcement: on the possibility of consensus in asynchronous systems with finite average response times	consensus;eventually perfect failure detectors;asynchronous systems;asynchronous system;impossibility;failure detector	Despite of those weak assumptions, the FA model permits the implementation of an eventually perfect failure detector. Hence, the consensus problem can be solved deterministically in this asynchronous system model. The intuition why an eventually perfect failure detector can be implemented in the FA model is as follows. Each process can implement a clock with a bounded but unknown speed. To do this, a process iteratively increments an integer variable and uses the value of the variable as its clock value. Because incrementing an integer takes an unknown time ≥ G > 0, the maximum speed of such a clock is bounded. The relative speed of two clocks is however unbounded because there is no bound on how slowly a clock can proceed. This clock is sufficient to provide a “subjective” notion of slow and fast messages. The acknowledgment of a slow message m arrives after the timeout for m expired and the acknowledgment of a fast message n arrives before the timeout for n expires. The timeout for messages is dynamically adapted according to the classification of earlier messages. However, unlike most other timeout-based failure detectors, a process increases the time-out when it receives a fast message but might decrease the timeout when it receives a slow message. One can show that this strategy can be used to ensure that the failure detector will eventually be perfect. To guarantee the the failure detector is eventually perfect, timeouts might have to grow quite large over time. One can decrease the expected time for termination of protocols without sacrificing the guaranteed termination by fusing our failure detector with an adaptive timeout-based failure detector, i.e., one that adjusts its timeouts according to the current system behavior but which does not generally guarantee termination in the FA model.	acknowledgment index;asynchronous system;average-case complexity;consensus (computer science);failure detector;sensor;timeout (computing)	Christof Fetzer;Ulrich Schmid	2004		10.1145/1011767.1011869	asynchronous system;real-time computing;consensus;computer science;control theory;mathematics;distributed computing;chandra–toueg consensus algorithm;failure detector	OS	-21.723084670968223	44.3869869808096	188828
3d61597e8a1a2b92ee691ef324d5da36411efe3b	an overview of pyramid machines for image processing	software;distributed system;complexite;processing;systeme reparti;general and miscellaneous mathematics computing and information science;image processing;logiciel;multiprocessor;software complexity;sistema informatico;complejidad;procesamiento imagen;pyramide;conception;computer system;complexity;traitement image;computer architecture;sistema repartido;pyramid;diseno;programming 990200 mathematics computers;logicial;design;systeme informatique;document types;piramide;multiprocesador;reviews;materiel informatique;material informatica;parallel processing;hardware;multiprocesseur	This paper is intended to review the existing hierarchical multiprocessor machines dedicated to Image Processing (IP). S ome general design concepts are given and compared in terms of hardware and sofware complexity. -. Invited lecture given at the 2nd International Conference on Advances in Pattern Recognition and Digital Techniques, Indian Statistical Institute Calcutta, New Delhi, India, January 6-9, 1986	image processing;multiprocessing;pattern recognition	Vito Di Gesù	1989	Inf. Sci.	10.1016/0020-0255(89)90041-8	parallel processing;image processing;computer science;artificial intelligence;theoretical computer science;pyramid;algorithm	Robotics	-19.142023677492055	41.68529795730975	188889
21fabfac6ba033f37b1bcafd8de1006450d61377	racerx: effective, static detection of race conditions and deadlocks	developpement logiciel;sistema operativo;fiabilidad;reliability;interprocedural analysis;race condition;race;acces concurrent;deadlock detection;program verification;verificacion programa;operating system;desarrollo logicial;fiabilite;software development;deadlock;interbloqueo;systeme exploitation;program checking;interblocage;verification programme;race detection	This paper describes RacerX, a static tool that uses flow-sensitive, interprocedural analysis to detect both race conditions and deadlocks. It is explicitly designed to find errors in large, complex multithreaded systems. It aggressively infers checking information such as which locks protect which operations, which code contexts are multithreaded, and which shared accesses are dangerous. It tracks a set of code features which it uses to sort errors both from most to least severe. It uses novel techniques to counter the impact of analysis mistakes. The tool is fast, requiring between 2-14 minutes to analyze a 1.8 million line system. We have applied it to Linux, FreeBSD, and a large commercial code base, finding serious errors in all of them. RacerX is a static tool that uses flow-sensitive, interprocedural analysis to detect both race conditions and deadlocks. It uses novel strategies to infer checking information such as which locks protect which operations, which code contexts are multithreaded, and which shared accesses are dangerous. We applied it to FreeBSD, Linux and a large commercial code base and found serious errors in all of them.	commercial code (communications);deadlock;freebsd;ibm notes;interprocedural optimization;linux;lock (computer science);multithreading (computer architecture);race condition;rewriting;stefan savage;ted;thread (computing)	Dawson R. Engler;Ken Ashcraft	2003		10.1145/945445.945468	parallel computing;real-time computing;computer science;software development;deadlock;operating system;reliability;distributed computing;race condition;programming language;computer security;deadlock prevention algorithms;race	OS	-21.098833424702423	39.454453393015584	189778
330a5c6d9ce662d5242f237a70c0239a621b17d5	runassert: a non-intrusive run-time assertion for parallel programs debugging	debugging;uncertainty principle;debugging programming model;instruments;uncertainty;reconfigurable architectures;reconfigurable logic;parallel sequences;parallel programming;program verification;runtime;probes;parallel programs debugging;scaling up;multicore environments;programming model;computer architecture;runtime debugging multicore processing probes hardware reconfigurable logic parallel programming uncertainty instruments costs;parallel architectures;monitoring;system on chip;multicore processing;integrated circuit modeling;language extension;soc;nonintrusive run time assertion;nonintrusive hardware configuration logic;program debugging;system on chip parallel architectures parallel programming program debugging program verification reconfigurable architectures;magnetic cores;parallel programs;architecture;programming;many core;nonuniform debugging architecture;parallel sequences nonintrusive run time assertion parallel programs debugging multicore environments soc nonuniform debugging architecture nonintrusive hardware configuration logic;race detection;hardware	Multicore environments are rapidly emerging and are widely used in SoC, but accompanying parallelism programming and debugging impact the ordinary sequential world. Unfortunately, according to Heisenberg's uncertainty principle, the instrument trying to probe the target will cause probe effects. Therefore, current intrusive debugging methodologies for sequential programs cannot be used directly in parallel programs in a multicore environment. This work developed a non-intrusive run-time assertion (RunAssert) for parallel program development based on a novel non-uniform debugging architecture. Our approaches are as follows: (a) a current language extension for parallel program debugging (b) corresponding non-intrusive hardware configuration logic and checking methodologies and (c) several reality cases using the extensions mentioned above. In general, the target program can be executed at its original speed without altering the parallel sequences, thereby eliminating the possibility of probe effect. The net hardware cost is relatively low, the reconfigurable logic for RunAssert is 0.6%-2.5% in a NUDA cluster with 8 cores, such that RunAssert can readily scale up for increasingly complex multicore systems.	assertion (software development);debugging;emoticon;multi-core processor;parallel computing;probe effect;reconfigurable computing;system on a chip;uncertainty principle	Chi-Neng Wen;Shu-Hsuan Chou;Tien-Fu Chen;Tay-Jyi Lin	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)	10.1109/DATE.2010.5457193	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;algorithmic program debugging;programming language	EDA	-19.51031648683926	39.47659124707686	190149
64f4fd50f600fe18fa137d705c70e5aaaed24b95	non-blocking atomic commitment with an unreliable failure detector	nonblocking atomic commitment;unreliable failure detector;protocols;data manager processes;atomic commit protocols;abstract consensus problem;distributed processing;data management;abstract consensus problem nonblocking atomic commitment unreliable failure detector transactional system protocol data manager processes;protocols detectors concurrent computing contracts delay h infinity control broadcasting project management upper bound;asynchronous system;transaction processing protocols distributed processing;transactional system;transaction processing;consensus problem;protocol	In a transactional system, an atomic commitment protocol ensures that for any transaction , all Data Manager processes agree on the same outcome (commit or abort). A non-blocking atomic commitment protocol enables an outcome to be decided at every correct process despite the failure of others. In this paper we apply, for the rst time, the fundamental result of Chandra and Toueg on solving the abstract consensus problem, to non-blocking atomic commitment. More precisely, we present a non-blocking atomic commitment protocol in an asynchronous system augmented with an unreliable failure detector that can make an innnity of false failure suspicions. If no process is suspected to have failed, then our protocol is similar to a three phase commit protocol. In the case where processes are suspected, our protocol does not require any additionnal termination protocol: failure scenarios are handled within our regular protocol and are thus much simpler to manage.	asynchronous system;atomic commit;blocking (computing);consensus (computer science);failure detector;non-blocking algorithm;two-phase commit protocol	Rachid Guerraoui;Mikel Larrea;André Schiper	1995		10.1109/RELDIS.1995.518722	three-phase commit protocol;asynchronous system;communications protocol;protocol;real-time computing;two-phase commit protocol;consensus;transaction processing;atomic commit;data management;computer science;database;distributed computing;computer security;computer network	Theory	-22.07996977983699	44.46589606217362	190191
5015d48e7b97a3ba631347be6509442ec2f97b04	verification of peterson's algorithm for leader election in a unidirectional asynchronous ring using nusmv	distributed system;formal model;degree of freedom;formal verification;model checking;leader election;distributed algorithm	The finite intrinsic nature of the most distributed algorithms gives us this ability to use model checking tools for verification of this type of algorithms. In this paper, I attempt to use NuSMV as a model checking tool for verifying necessary properties of Peterson’s algorithm for leader election problem in a unidirectional asynchronous ring topology. Peterson’s algorithm for an asynchronous ring supposes that each node in the ring has a unique ID and also a queue for dealing with storage problem. By considering that the queue can have any combination of values, a constructed model for a ring with only four nodes will have more than a billion states. Although it seems that model checking is not a feasible approach for this problem, I attempt to use several effective limiting assumptions for hiring formal model checking approach without losing the correct functionality of the Peterson’s algorithm. These enforced limiting assumptions target the degree of freedom in the model checking process and significantly decrease the CPU time, memory usage and the total number of page faults. By deploying these limitations, the number of nodes can be increased from four to eight in the model checking process with NuSMV.	central processing unit;distributed algorithm;formal language;formal verification;interpretation (logic);item unique identification;leader election;model checking;nusmv;peterson's algorithm;ring network;verification and validation;working set size	Amin Ansari	2008	CoRR		model checking;distributed algorithm;formal verification;computer science;peterson's algorithm;theoretical computer science;leader election;distributed computing;degrees of freedom;abstraction model checking;algorithm	SE	-22.37985925657539	43.7525291230248	190614
685b5cc633dc72e033ff391d4d348471204e2b31	local-spin group mutual exclusion algorithms	modelizacion;distributed system;approximation asymptotique;controle acces;virtual memory;systeme reparti;group mutual exclusion;shared memory;systeme multiprocesseur memoire repartie;asymptotic optimality;multiprocessor;memoria compartida;memoria virtualmente compartida;distributed computing;cache memory;program verification;antememoria;exclusion mutual;mutual exclusion;memoire virtuellement partagee;modelisation;antememoire;verificacion programa;sistema repartido;sistema multiprocesador memoria distribuida;memoire virtuelle;cache coherence;calculo repartido;access control;distributed memory multiprocessor system;exclusion mutuelle;asymptotic approximation;multiprocesador;verification programme;critical section;distributed shared memory;modeling;calcul reparti;memoria virtual;memoire partagee;aproximacion asintotica;multiprocesseur	Group mutual exclusion (GME) is a natural generalisation of the classical mutual exclusion problem. In GME, when a process leaves the non-critical section it requests a session; processes are allowed to be in the critical section simultaneously if they have requested the same session. We present GME algorithms (where the number of sessions is not known a priori) that use O(N) remote memory references in distributed shared memory (DSM) multiprocessors, where N is the number of processes, and prove that this is asymptotically optimal even if there are only two sessions that processes can request. We also present an algorithm for two-session GME that requires O(logN) remote memory references in cache-coherent (CC) multiprocessors. This establishes a complexity separation between the CC and DSM models: there is a problem (two-session GME) that is provably more efficiently solvable in the former than in the latter.	algorithm;mutual exclusion	Robert Danek;Vassos Hadzilacos	2004		10.1007/978-3-540-30186-8_6	distributed shared memory;shared memory;cache coherence;parallel computing;multiprocessing;systems modeling;cpu cache;mutual exclusion;computer science;virtual memory;access control;operating system;distributed computing;critical section;algorithm	ML	-24.9031461662163	41.978511453533464	191145
602d0cb0665ea11cd3d84bcf7a68be20d529df77	policy-based access control for weakly consistent replication	replication;security logic;access control policy;design and implementation;replicated data;access control;eventual consistency;reading and writing	Combining access control with weakly consistent replication presents a challenge if the resulting system is to support eventual consistency. If authorization policy can be temporarily inconsistent, any given operation may be permitted at one node and yet denied at another. This is especially troublesome when the operation in question involves a change in policy. Without a careful design, permanently divergent state can result.  We describe and evaluate the design and implementation of an access control system for weakly consistent replication where peers are not uniformly trusted. Our system allows for the specification of fine-grained access control policy over a collection of replicated items. Policies are expressed using a logical assertion framework and access control decisions are logical proofs. Policy can grow to encompass new nodes through fine-grain delegation of authority. Eventual consistency of the replicated data is preserved despite the fact that access control policy can be temporarily inconsistent.	access control;assertion (software development);authorization;control system;eventual consistency;formal proof	Ted Wobber;Thomas L. Rodeheffer;Douglas B. Terry	2010		10.1145/1755913.1755943	replication;computer science;access control;database;distributed computing;eventual consistency;computer security	Security	-24.705630343621024	46.17083172945782	192226
9357be32a1602073e95ad341ccc9d142bf187f39	formal fault tolerance analysis of algorithms for redundant systems in early design stages		Redundant techniques, that use voting principles, are often used to increase the reliability of systems by ensuring fault tolerance. In order to increase the efficiency of these redundancy strategies we propose to exploit the inherent fault masking properties of software-algorithms at application-level. An important step in early development stages is to choose from a class of algorithms that achieve the same goal in different ways, one or more that should be executed redundantly. In order to evaluate the resilience of the algorithm variants, there is a great need for a quantitative reasoning about the algorithms fault tolerance in early design stages.	algorithm;analysis of algorithms;fault tolerance	Andrea Höller;Nermin Kajtazovic;Christopher Preschern;Christian Kreiner	2014		10.1007/978-3-319-12241-0_6	reliability engineering;computer architecture;real-time computing;software fault tolerance	EDA	-23.114559565587015	40.96990778615944	192439
412cd61103c2805b3758b5d3baed68af0768b301	adapting to intermittent faults in multicore systems	dynamic change;lenguaje programacion;duracion;sistema operativo;fiabilidad;reliability;programming language;availability;disponibilidad;intermittent faults;performance;processeur multicoeur;procesador multinucleo;duration;tension electrique;intermitencia;operating system;overcommitted system;fiabilite;voltage;langage programmation;intermittency;cost effectiveness;design;systeme exploitation;multicore processors;multicore processor;resource availability;intermittence;voltaje;disponibilite;duree	Future multicore processors will be more susceptible to a variety of hardware failures. In particular, intermittent faults, caused in part by manufacturing, thermal, and voltage variations, can cause bursts of frequent faults that last from several cycles to several seconds or more. Due to practical limitations of circuit techniques, cost-effective reliability will likely require the ability to temporarily suspend execution on a core during periods of intermittent faults.  We investigate three of the most obvious techniques for adapting to the dynamically changing resource availability caused by intermittent faults, and demonstrate their different system-level implications. We show that system software reconfiguration has very high overhead, that temporarily pausing execution on a faulty core can lead to cascading livelock, and that using spare cores has high fault-free cost. To remedy these and other drawbacks of the three baseline techniques, we propose using a thin hardware/firmware layer to manage an overcommitted system -- one where the OS is configured to use more virtual processors than the number of currently available physical cores. We show that this proposed technique can gracefully degrade performance during intermittent faults of various duration with low overhead, without involving system software, and without requiring spare cores.	baseline (configuration management);central processing unit;deadlock;firmware;multi-core processor;operating system;overhead (computing)	Philip M. Wells;Koushik Chakraborty;Gurindar S. Sohi	2008		10.1145/1346281.1346314	multi-core processor;embedded system;parallel computing;real-time computing;voltage;computer science;operating system	Arch	-19.532866043499524	41.49439921828217	192712
c9e3537575272e515db565ae496ec20e876d364f	transparent fault tolerance for grid applications	tolerancia falta;virtual machine;haute performance;fault tolerant;grid applications;java virtual machine;distributed computing;langage java;multiple machine;analyse temporelle;machine virtuelle;panne;analisis temporal;checkpointing;time analysis;solucion particular;grid;maquina multiple;fault tolerant system;appel procedure a distance;rejilla;remote method invocation;rmi;fault tolerance;sistema tolerando faltas;punto reanudacion;pana;alto rendimiento;breakdown;grille;calculo repartido;timing analysis;lenguaje java;systeme tolerant les pannes;point reprise;machine multiple;maquina virtual;high performance;remote procedure calls;calcul reparti;solution particuliere;tolerance faute;particular solution;java language	A major challenge facing grid applications is the appropriate handling of failures. In this paper we address the problem of making parallel Java applications based on Remote Method Invocation (RMI) fault tolerant in a way transparent to the programmer. We use globally consistent checkpointing to avoid having to restart long-running computations from scratch after a system crash. The application’s execution state can be captured at any time also when some of the application’s threads are blocked waiting for the result of a (nested) remote method call. We modify only the program’s bytecode which makes our solution independent from a particular Java Virtual Machine (JVM) implementation. The bytecode transformation algorithm performs a compile time analysis to reduce the number of modifications in the application’s code which has a direct impact on the application’s performance. The fault tolerance extensions encompass also the RMI components such as the RMI registry. Since essential data as checkpoints are replicated, our system is resilient to simultaneous failures of multiple machines. Experimental results show negligible performance overhead of our fault-tolerance extensions.	algorithm;application checkpointing;compile time;compiler;computation;fault tolerance;java remote method invocation;java virtual machine;method (computer programming);overhead (computing);programmer;remote procedure call	Pawel Garbacki;Bartosz Biskupski;Henri E. Bal	2005		10.1007/11508380_68	fault tolerance;parallel computing;real-time computing;computer science;operating system;database;distributed computing;programming language;computer security;algorithm	HPC	-19.931498028453017	41.4090333125107	192789
b359088e58a604a3907726a4505a01a53ffbf210	time and resilient master clocks in cyber-physical systems		Since many years, it has been acknowledged that the role of time is fundamental to the design of distributed algorithms [21]. This is exacerbated in cyber-physical distributed systems, and consequently in Systems-of-Systems, where it is sometimes impossible to say which one of two observed environmental events occurred first.	cyber-physical system	Andrea Ceccarelli;Francesco Brancati;Bernhard Frömel;Oliver Höftberger	2016		10.1007/978-3-319-47590-5_6	master clock	Logic	-24.50545066879962	42.61383925935936	192980
cb536b701c8a8b0271f6fbdb9a89bad1cf7be1bb	evaluating and optimizing stabilizing dining philosophers	distributed algorithms;algorithm design and analysis semantics fault tolerance fault tolerant systems safety throughput alternators;simulation;semantics;software fault tolerance;alternators;self stabilization;fault tolerant systems;fault tolerance;safety;system switching self stabilizing dining philosophers algorithms fault tolerance critical section access system load;dining philosophers;fault tolerance self stabilization dining philosophers simulation distributed algorithms;algorithm design and analysis;throughput	We study theoretical and practical aspects of five of the most well-known self-stabilizing dining philosophers algorithms. We theoretically prove that three of them are incorrect. For practical evaluation, we simulate all algorithms and evaluate their fault-tolerance, latency and throughput of critical section access. We present a new combined algorithm that achieves the best throughput of the two remaining correct algorithms by determining the system load and switching between these basic algorithms. We prove the combined algorithm correct, simulate it and study its performance characteristics.	algorithm;critical section;dining philosophers problem;fault tolerance;load (computing);optimizing compiler;self-stabilization;simulation;throughput	Jordan Adamek;Mikhail Nesterenko;Sébastien Tixeuil	2015	2015 11th European Dependable Computing Conference (EDCC)	10.1109/EDCC.2015.11	self-stabilization;algorithm design;throughput;fault tolerance;parallel computing;real-time computing;computer science;distributed computing;semantics;dining philosophers problem;software fault tolerance	HPC	-21.640094345326254	45.764699890665625	193243
c87e1e66937fa9d20705a1ea59d78e86bec23daa	self-stabilizing mutual exclusion in the presence of faulty nodes	distributed algorithms;fault tolerance terminology automata sufficient conditions system recovery;fault tolerant;self adjusting systems;software fault tolerance;online failures;distributed algorithms software fault tolerance self adjusting systems;sufficient conditions;processor rings;mutual exclusion;automata;self stabilization;system recovery;token rings;self stabilizing mutual exclusion;fault tolerance;correctness proof self stabilizing mutual exclusion self stabilizing mutual exclusion faulty nodes ratchetft distributed fault tolerant mutual exclusion algorithm processor rings online failures faulty processors;faulty processors;terminology;distributed systems;ratchetft distributed fault tolerant mutual exclusion algorithm;correctness proof;faulty nodes	The paper presents the RatchetFT distributed fault tolerant mutual exclusion algorithm for processor rings. RatchetFT is self-stabilizing, in that if mutual exclusion is lost due to any sequence of online failures and repairs of processors, mutual exclusion will eventually be regained. This research demonstrates that self-stabilization can be achieved in the presence of faulty processors, provided that these faulty processors always appear to behave incorrectly. Self-stabilization is achievable even if faulty processor behavior is not restricted to transient failures or other simple failure models. The key results of the paper include the specification of RatchetFT and a detailed sketch of its correctness proof. >	mutual exclusion;self-stabilization	Richard W. Buskens;Ronald P. Bianchini	1995		10.1109/FTCS.1995.466988	parallel computing;real-time computing;computer science;distributed computing	Robotics	-22.660204176850016	44.12268517185274	194414
58abd82dde6c4f64b4800d13faa05acfca927435	safety of recovery protocol preserving mw session guarantee in mobile systems	recouvrement arriere;entrada salida;session guarantee;informatique mobile;protocole transmission;securite;recubrimiento atras;semantics;semantica;semantique;safety properties;checkpointing;input output;protocolo transmision;monotonic writes;safety;punto reanudacion;rollback recovery;point reprise;mobile systems;mobile computing;seguridad;entree sortie;transmission protocol	In this paper checkpointing and rollback-recovery protocol rVsMW for mobile systems is presented. The protocol preserves Monotonic Writes session guarantee required by clients, despite failures of servers. The costs of rollback-recovery are minimized, by exploiting semantics of operations and properties of MW guarantee. The proof of safety property of rVsMW is included.	microwave	Jerzy Brzezinski;Anna Kobusinska	2006		10.1007/11751632_125	input/output;embedded system;real-time computing;computer science;operating system;database;semantics;mobile computing;computer security	Mobile	-21.037006930827154	42.78687974804294	194741
01f0804e7f4355fb94be14bcccd719f3fd34a68b	demonstration of fault tolerance for corba applications	eternal systems incorporated fault tolerance corba infrastructure air defense application interception management guidance program replica consistency darpa university of california santa barbara;fault tolerant;application software;interception management;eternal systems incorporated;darpa;distributed computing;university of california;fault tolerance fault tolerant systems application software hardware costs middleware programming profession computer networks distributed computing protection;computer networks;guidance program;replicated databases fault tolerant computing distributed object management military computing;protection;fault tolerant computing;fault tolerant systems;air defense application;programming profession;fault tolerance;distributed object management;replica consistency;middleware;corba infrastructure;university of california santa barbara;program development;replicated databases;military computing;hardware	This demonstration of a Fault Tolerant CORBA infrastructure exhibits a simple air defense application in which the interception management and guidance program is replicated over three computers to protect it against faults. The Fault Tolerant CORBA infrastructure maintains continuous service of the interception management and guidance program, through faults and recovery, and maintains strong replica consistency, despite asynchrony and faults. With the Fault Tolerant CORBA infrastructure, fault tolerance is achieved with minimal modification of the application program, no special programming skills and shorter program development timescales. The Fault Tolerant CORBA infrastructure being demonstrated was developed originally under DARPA sponsorship at the University of California, Santa Barbara, and is now available commercially from Eternal Systems, Inc.	asynchronous i/o;common object request broker architecture;computer;fault tolerance	Louise E. Moser;P. M. Melliar-Smith	2003		10.1109/DISCEX.2003.1194929	embedded system;real-time computing;engineering;distributed computing;software fault tolerance	Networks	-25.744953974873702	44.02519927491326	195872
b407cbb9b33b08ba9cf0e9bc897d928e47a25544	virtual checkpoints: architecture and performance	tolerancia falta;eficacia sistema;virtual memory;failure tolerance;recuperacion;architecture systeme;virtual memory translation hardware;performance evaluation;performance evaluation fault tolerant computing;implementation;sistema informatico;performance systeme;computer system;recovery;trace;system performance;ejecucion;virtual checkpoints;point controle;fault tolerant computing;fault tolerance application software counting circuits hardware performance analysis computational modeling analytical models computer architecture very large scale integration fault detection;fault tolerance;performance analysis;memoire virtuelle;trace driven simulation virtual checkpoints failure tolerance performance analysis rollback recovery virtual memory translation hardware address space;rollback recovery;arquitectura sistema;systeme informatique;recuperation;traza;system architecture;high performance;trace driven simulation;tolerance faute;address space;memoria virtual;rollback	AbstrucfCheckpoint and rollback recovery is a technique that allows a system to tolerate a failure by periodically saving the entire state and if an error is detected, rolling back to the prior checkpoint. This paper presents a technique that embeds the support for checkpoint and rollback recovery directly into the virtual memory translation hardware. The scheme is general enough to be implemented on various scopes of data such as a portion of an address space, a single address space or multiple address spaces. The technique can provide a very high performance scheme for implementing checkpoint and rollback recovery. We have analyzed the performance of the scheme using a trace driven simulation. The overhead is a function of the interval between checkpoints and becomes very small for intervals greater than lo6 references. However, the scheme is shown to be feasible for intervals as small as loo0 references under certain conditions.	address space;overhead (computing);simulation;transaction processing system	Nicholas S. Bowen;Dhiraj K. Pradhan	1992	IEEE Trans. Computers	10.1109/12.142677	embedded system;fault tolerance;parallel computing;real-time computing;recovery;rollback;computer science;virtual memory;operating system;trace;computer performance;address space;implementation;systems architecture	Arch	-20.447215851415578	44.33519350143428	196123
37f3e45d4ca19d0242bda2f470cc905181cd3a4a	runtime anomaly detection in embedded systems by binary tracing and hidden markov models	error rate runtime anomaly detection embedded system binary tracing hidden markov model embedded computing system deployed software embedded computing device human intervention memory sequence memory reference sequence pin tracing tool randomly selected block discrete cosine transform spec 2006 cpu benchmark suite;detection algorithms;memory address;anomaly detection;training;system recovery discrete cosine transforms embedded systems error statistics hidden markov models program debugging storage management;hmm;runtime;hidden markov models computational modeling detection algorithms discrete cosine transforms benchmark testing runtime training;computational modeling;hidden markov models;discrete cosine transforms;memory address anomaly detection hmm run time analysis spectral analysis;run time analysis;spectral analysis;benchmark testing	Embedded computing systems are very vulnerable to anomalies that can occur during execution of deployed software. Anomalies can be due for example to faults, bugs or deadlocks during executions. These anomalies can have very dangerous consequences on the systems controlled by embedded computing devices. Embedded systems are designed to perform autonomously, i.e. Without any human intervention, and thus the possibility to debug an application to manage the anomaly is very difficult if not impossible. Anomaly detection algorithms are the primary means of being aware of anomalous conditions. In this paper we describe a novel approach to detect an anomaly during execution of one or more applications. The algorithm exploits the differences between the behavior of memory sequences generated during executions. Memory reference sequences are monitored in real time using the PIN tracing tool. The memory reference sequence is divided into randomly selected blocks and spectrally described with the Discrete Cosine Transform. Experimental analysis is based on the SPEC 2006 CPU benchmark suite, and show very low error rates for the anomalies tested.	algorithm;anomaly detection;benchmark (computing);central processing unit;deadlock;discrete cosine transform;embedded system;hidden markov model;markov chain;randomness;software bug	Alfredo Cuzzocrea;Enzo Mumolo;Riccardo Cecolin	2015	2015 IEEE 39th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2015.89	memory address;benchmark;parallel computing;real-time computing;computer science;theoretical computer science;operating system;database;computational model;hidden markov model	Embedded	-21.34318372644789	40.19786708150869	197913
5a0491e69444ba6991c12272f8b1c526fe83d938	starplane - a national dynamic photonic network controlled by grid applications	grid applications;sciences;universiteitsbibliotheek;computer applications;computer networks;computer network;network control;computer application;vu;wavelengths;design methodology	In the Netherlands Grid applications running on the distributed ASCI compute cluster DAS-3 have access to dedicated lightpaths on the SURFnet6 network. These lightpaths are implemented using special dynamically reconfigurable photonic switches. StarPlane is the research project that develops the middleware for these applications to directly control that photonic network. Topology changes at runtime, fast reconfiguration times and exclusive use of photonic equipment in the core are the most original aspects of this project.	control plane;fits;jason;maxine d. brown;middleware;network switch;network topology;real-time clock;real-time computing;reconfigurability;run time (program lifecycle phase)	Paola Grosso;Li Xu;Jan-Philip Velders;Cees T. A. M. de Laat	2007	Internet Research	10.1108/10662240710830235	simulation;design methods;computer science;wavelength;computer applications;operations research;world wide web;computer security;computer network	HPC	-25.585482536005685	40.4664672081263	198205
3ac27cba1fabac9f21f77117448e327f6e8607e9	on the nonexistence of resilient consensus protocols	tolerancia falta;crash failure;protocole transmission;fault tolerant;asynchrone;message passing system;sistema informatico;transmission message;computer system;message transmission;protocolo transmision;fault tolerance;systeme informatique;tolerance faute;asincrono;asynchronous;transmision mensaje;transmission protocol	In [3], Fischer, Lynch and Paterson proved that in asynchronous message passing systems there cannot exist a consensus protocol that tolerates even a single undetectable crash failure. Their proof of this fundamental result relies on operational details such as sending and receiving messages, etc. Chandy and Misra [1] have taken an axiomatic non-operational approach to the consensus problem. Their idea is to define asynchronous systems and consensus protocols by a set of axioms, making no mention of operational details. The result in [1] is weaker than that in [3], since it is not assumed in [1] that all messages sent are eventually delivered. In this note we use the Chandy and Misra approach to prove a result that is similar to the one in [3]. We do so without the need of introducing any operational notions. We believe that our proof is simpler than that in [3]. Most of the ideas that appear in [3] and [1] are used here, as well as new key ideas. The result here is slightly more general than that in [3], since we use a weaker resiliency requirement and assume that any enabled event eventually is either applied or becomes not enabled, in a model where processes may be nondeterministic. The rest of the paper is organized as follows. In Section 2, we present three axioms capturing the nature of asynchronous message passing systems. In Section 3, we present five axioms defining resilient consensus protocols. In Section 4, we prove that there does not exist an asynchronous resilient consensus protocol by showing that the set of eight axioms is inconsistent.	asynchronous system;consensus (computer science);misra c;message passing;michael j. fischer;nondeterministic algorithm	Gadi Taubenfeld	1991	Inf. Process. Lett.	10.1016/0020-0190(91)90221-3	fault tolerance;real-time computing;computer science;distributed computing;computer security	Theory	-21.96190848457803	43.9455985447101	198606
