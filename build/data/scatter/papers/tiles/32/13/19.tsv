id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
a8ee6e4d07aeff60292605c6404785b8a170d02a	lem: reusable engineering of real-world semantics	proof assistants;qa 76 software;conference paper;computer programming;specification languages;lem;real world semantics	Recent years have seen remarkable successes in rigorous engineering: using mathematically rigorous semantic models (not just idealised calculi) of real-world processors, programming languages, protocols, and security mechanisms, for testing, proof, analysis, and design. Building these models is challenging, requiring experimentation, dialogue with vendors or standards bodies, and validation; their scale adds engineering issues akin to those of programming to the task of writing clear and usable mathematics. But language and tool support for specification is lacking. Proof assistants can be used but bring their own difficulties, and a model produced in one, perhaps requiring many person-years effort and maintained over an extended period, cannot be used by those familiar with another.  We introduce Lem, a language for engineering reusable large-scale semantic models. The Lem design takes inspiration both from functional programming languages and from proof assistants, and Lem definitions are translatable into OCaml for testing, Coq, HOL4, and Isabelle/HOL for proof, and LaTeX and HTML for presentation. This requires a delicate balance of expressiveness, careful library design, and implementation of transformations - akin to compilation, but subject to the constraint of producing usable and human-readable code for each target. Lem's effectiveness is demonstrated by its use in practice.	automated theorem proving;central processing unit;code generation (compiler);compiler;coq (software);functional programming;general-purpose markup language;hol (proof assistant);html;haskell;human-readable medium;isabelle;latex;matita;ocaml;over-the-top content;programming language;proof assistant;racket;reduction strategy (code optimization);semantic data model;type class;type system;usability	Dominic P. Mulligan;Scott Owens;Kathryn E. Gray;Tom Ridge;Peter Sewell	2014		10.1145/2628136.2628143	computer science;computer programming;programming language;algorithm	PL	-21.212282034735672	27.026042114044575	159178
3c5c58b22d77935bb84411b870eee1c07dd91694	optimized compilation of around advice for aspect oriented programs	optimizing compiler;java programming;aspect oriented programming	The technology that supports Aspect-Oriented Programming (AOP) tools is inherently intrusive, since it changes the behavior of base application code. Advice weaving performed by AspectJ compilers must introduce crosscutting behavior defined in advice into Java programs without causing great performance overhead. This paper shows the techniques applied by the ajc and abc AspectJ compilers for around advice weaving, and identifies problems in code they produce. The problems analyzed are advice and shadow implementation repetition and context variable repetition. Performance gain provided by solving these problems is discussed, showing that bytecode size, running time and memory consumption can be reduced by these optimizations. It is assumed that the reader is familiar with AOP and AspectJ constructs.	advice (programming);aspect-oriented programming;aspectj;call stack;coexist (image);compiler;entry point;experiment;java;loader (computing);local variable;mathematical optimization;overhead (computing);pointcut;programmer;recursion;regular expression;software development;time complexity	Eduardo S. Cordeiro;Roberto da Silva Bigonha;Mariza Andrade da Silva Bigonha;Fabio Tirelo	2007	J. UCS	10.3217/jucs-013-06-0753	parallel computing;real-time computing;aspect-oriented programming;computer science;operating system;optimizing compiler;programming language;algorithm	PL	-25.47573349670202	29.79760260332692	159401
3a653c54583fb1532e635b6de0f8c9a227c8318c	capturing hiproofs in hol light	hierarchical tree;tactic proof script;hol light;proof recording;complementary method;additional structure;ordinary proof tree;hol light kernel;hierarchical proof tree;hierachical proof tree;capturing hiproofs	Hierarchical proof trees (hiproofs for short) add structure to ordinary proof trees, by allowing portions of trees to be hierarchically nested. The additional structure can be used to abstract away from details, or to label particular portions to explain their purpose. In this paper we present two complementary methods for capturing hiproofs in HOL Light, along with a tool to produce web-based visualisations. The first method uses tactic recording, by modifying tactics to record their arguments and construct a hierarchical tree; this allows a tactic proof script to be modified. The second method uses proof recording, which extends the HOL Light kernel to record hierachical proof trees alongside theorems. This method is less invasive, but requires care to manage the size of the recorded objects. We have implemented both methods, resulting in two systems: Tactician and HipCam.	boxing;code refactoring;expectation propagation;hol (proof assistant);hol light;hands-on computing;high- and low-level;laboratory for foundations of computer science;level of detail;mathematical induction;object type (object-oriented programming);proof assistant;semiconductor industry;text simplification;user space;web application	Steven Obua;Mark Adams;David Aspinall	2013		10.1007/978-3-642-39320-4_12	discrete mathematics;theoretical computer science;mathematics;algorithm	PL	-24.24606912729082	26.535010929081878	160287
a0f11fca9446a364985cea2c449fcf2f9255ad41	a verified compiler for a structured assembly language	source code;logic;formal verification;assembly;hardware	W e describe the verification of a compiler for a subset of the Vista language: a structured assembly language for the Viper microprocessor. This proof has been mechanically checked using the HOL system. W e conszder how the compiler correctness theorem could be used t o deduce safety and liveness properties of compiled code f r o m theorems stating that these properties hold of the source code. W e also show how secwe compilation can be achieved using automated theorem proving techniques.	assembly language;automated theorem proving;compiler correctness;correctness (computer science);hol (proof assistant);liveness;viper microprocessor	Paul Curzon	1991	1991., International Workshop on the HOL Theorem Proving System and Its Applications		computer architecture;compiler;parallel computing;formal verification;compiler correctness;computer science;assembly;programming language;logic;source code	Logic	-20.474291885660666	27.369092439885538	161020
74b9e42edc13144b8d37217962cdcbeabe479a1a	aspectual code generators for easy generation of fixml to oo mappings		This paper provides a solution to the TTC 2014 FIXML study case. The case requires the implementation of a straightforward mapping from XML messages in the FIXML format to a set of source files implementing the schema of such a message and, optionally, an instantiation with the data from the message. There is a requirement for producing code in a range of programming languages. The biggest challenge for transformation design in this study case is that the same tag may occur in multiple places in the FIXML message, but with a different set of attributes. The generator must merge all of these occurrences into a single representation in the generated code. We demonstrate how the use of symmetric, language-aware code generators relieves the transformation developer almost entirely from considering this requirement. As a result, the transformation specifications we have written are extremely straightforward and simple. We present generation to Java and C#.	financial information exchange;java;programming language;universal instantiation;xml	Steffen Zschaler;Sobhan Yassipour Tehrani	2014			theoretical computer science;algorithm;computer science	PL	-23.841726570078595	26.407725780861906	161085
83b077d4f6d94a6b47cee7a661e55bb3cbdf809f	parameterized verification of transactional memories	phase locking;verification;automatic verification;modelizacion;distributed memory;sistema infinito;controle acces;quantization;atomicidad;gestion memoire;tratamiento transaccion;reliability;hardware verification;cuantificacion;data race;automatic proving;shared memory;storage access;verificacion hardware;protocole transmission;verrouillage phase;theorie resonance adaptative;software model checking;protocol verification;verification materiel;memoria compartida;acces concurrent;software verification;storage management;satisfiabilite;processus leger logiciel;simultaneidad informatica;enganche de fase;resumen;demostracion automatica;program verification;quantification;satisfiability;teoria de la resonancia adaptiva;modelisation;demonstration automatique;proceso ligero logicial;protocolo transmision;gestion memoria;verificacion programa;serialisabilidad;acceso simultaneo;concurrency;atomicity;atomicite;software transactional memory;resume;acces memoire;serializability;thread software;acceso memoria;serialisabilite;invariante;access control;state explosion;transaction processing;transactional memory;memoire repartie;verification programme;abstract;modeling;simultaneite informatique;systeme infini;invariant;traitement transaction;memoire partagee;adaptive resonance theory;parameterized verification;satisfactibilidad;infinite system;transmission protocol	We describe an automatic verification method to check whether transactional memories ensure strict serializability a key property assumed of the transactional interface. Our main contribution is a technique for effectively verifying parameterized systems. The technique merges ideas from parameterized hardware and protocol verification--verification by invisible invariants and symmetry reduction--with ideas from software verification--template-based invariant generation and satisfiability checking for quantified formulæ (modulo theories). The combination enables us to precisely model and analyze unbounded systems while taming state explosion.  Our technique enables automated proofs that two-phase locking (TPL), dynamic software transactional memory (DSTM), and transactional locking II (TL2) systems ensure strict serializability. The verification is challenging since the systems are unbounded in several dimensions: the number and length of concurrently executing transactions, and the size of the shared memory they access, have no finite limit. In contrast, state-of-the-art software model checking tools such as BLAST and TVLA are unable to validate either system, due to inherent expressiveness limitations or state explosion.	blast;graphic art software;lock (computer science);model checking;modulo operation;serializability;shared memory;software transactional memory;software verification;two-phase locking;verification and validation	Michael Emmi;Rupak Majumdar;Roman Manevich	2010		10.1145/1806596.1806613	shared memory;transactional memory;parallel computing;real-time computing;verification;systems modeling;distributed memory;concurrency;quantization;commitment ordering;transaction processing;software verification;computer science;access control;adaptive resonance theory;invariant;software transactional memory;reliability;database;race condition;programming language;serializability;atomicity;satisfiability	PL	-22.647201273607866	31.053932459740164	161176
b302fd5defe19bef88feae29febaa68ced105451	experience with the setl optimizer	programming language;level set;data representation;program analysis;data structure	The structure of an existing optimizer for the very high-level, set theoretically oriented programming language SETL is described, and its capabilities are illustrated. The use of novel techniques (supported by state-of-the-art interprocedural program analysis methods) enables the optimizer to accomplish various sophisticated optimizations, the most significant of which are the automatic selection of data representations and the systematic elimination of superfluous copying operations. These techniques allow quite sophisticated data-structure choices to be made automatically.	data structure;high- and low-level;mathematical optimization;program analysis;programming language;setl	Stefan M. Freudenberger;Jacob T. Schwartz;Micha Sharir	1983	ACM Trans. Program. Lang. Syst.	10.1145/357195.357197	program analysis;data structure;computer science;level set;theoretical computer science;external data representation;programming language;algorithm	PL	-22.402309079722233	26.968994369710398	161794
733b8db12a863c0e6ab7e8a27d503aa9ab6ee333	building formal method tools in the isabelle/isar framework	isabelle plug-ins;formal proof;concrete formal methods tool;isar architecture;generic system framework;formal method tool;extensible state component;explicit infrastructure;isar underlying recent version;isar framework;extensible syntax;lcf approach	We present the generic system framework of Isabelle/Isar underlying recent versions of Isabelle. Among other things, Isar provides an infrastructure for Isabelle plug-ins, comprising extensible state components and extensible syntax that can be bound to tactical ML programs. Thus the Isabelle/Isar architecture may be understood as an extension and refinement of the traditional “LCF approach”, with explicit infrastructure for building derivative systems. To demonstrate the technical potential of the framework, we apply it to a concrete formal methods tool: the HOL-Z 3.0 environment, which is geared towards the analysis of Z specifications and formal proof of forward-refinements.	documentation;energy (psychological);experiment;formal methods;formal proof;hol (proof assistant);hol light;human-readable medium;intuitionistic type theory;isabelle;key;logical framework;programming language implementation;refinement (computing)	Markus Wenzel;Burkhart Wolff	2007		10.1007/978-3-540-74591-4_26	computer science;theoretical computer science;programming language;algorithm	Logic	-21.260249307768653	27.557918683579278	161831
cc2ecc94256b543043ad9661c5a6b03fd4abe279	e-acsl, a runtime verification tool for safety and security of c programs (tool paper)		This tool paper presents E-ACSL, a runtime verification tool for C programs capable of checking a broad range of safety and security properties expressed using a formal specification language. E-ACSL consumes a C program annotated with formal specifications and generates a new C program that behaves similarly to the original if the formal properties are satisfied, or aborts its execution whenever a property does not hold. This paper presents an overview of E-ACSL and its specification language.	ansi/iso c specification language;formal specification;runtime verification	Julien Signoles;Nikolai Kosmatov;Kostyantyn Vorobyov	2017			theoretical computer science;software engineering;runtime verification;computer science	PL	-19.279095318272812	27.854442775782463	162066
f7f7eff2e36372359af8cf24141a8dde65ae790c	exploration of language specifications by compilation to first-order logic	formal specification;first order theorem proving;type systems;declarative languages;domain specific languages	Exploration of language specifications helps to discover errors and inconsistencies early during the development of a programming language. We propose exploration of language specifications via application of existing automated first-order theorem provers (ATPs). To this end, we translate language specifications and exploration tasks to first-order logic, which many ATPs accept as input. However, there are several different strategies for compiling a language specification to first-order logic, and even small variations in the translation may have a large impact on the time it takes ATPs to find proofs.  In this paper, we present a systematic empirical study on how to best compile language specifications to first-order logic such that existing ATPs can solve typical exploration tasks efficiently. We have developed a compiler product line that implements 36 different compilation strategies and used it to feed language specifications to 4 existing first-order theorem provers. As a benchmark, we developed a language specification for typed SQL with 50 exploration goals. Our study empirically confirms that the choice of a compilation strategy in general greatly influences prover performance and shows which strategies are advantageous for prover performance.	apl;automated theorem proving;benchmark (computing);compiler;first-order logic;first-order predicate;programming language specification;sql	Sylvia Grewe;Sebastian Erdweg;Michael Raulf;Mira Mezini	2016		10.1145/2967973.2968606	object language;specification language;computer science;domain-specific language;theoretical computer science;formal specification;programming language;programming language specification;algorithm	PL	-21.32140402805161	25.73365860381671	162497
ca0afdcbb7d39a5a9a51debf3c3167b101cf345b	a functional approach to generic programming using adaptive traversals	functional programming;data structures;traversals;generic programming	Writing functions over complex user-defined datatypes can be tedious and error prone. Generic (or polytypic) programming and higher order functions like foldr have resolved some of these issues, but can be too general to be practically useful for larger collections of data types. In this paper we present a traversal-based approach to generic programming using function sets. Our traversal is an adaptive, higher-order function that employs an asymmetric type-based multiple dispatch to fold over arbitrarily complex structures. We introduce our approach in the context of our Scheme library implementation, present a typed model of our system, and provide a proof of type soundness, showing that our flexible, adaptive approach is both useful and safe.	ap computer science a;cognitive dimensions of notations;conditional (computer programming);data structure;definition;depth-first search;dynamic dispatch;fold (higher-order function);formal verification;functional approach;generic programming;higher-order function;multiple dispatch;mutual recursion;operator overloading;programmer;scheme;structural induction;traverse;tree traversal;type safety;type system	Bryan Chadwick;Karl J. Lieberherr	2010	Higher-Order and Symbolic Computation	10.1007/s10990-011-9064-1	reactive programming;theoretical computer science;mathematics;programming paradigm;programming language;algorithm	PL	-21.103917092413162	28.688888720453512	162795
f2be61e92d56a2948f2beba2c5c9f1048d16f47f	towards verification of cyber-physical systems with utp and isabelle/hol		In this paper, we outline our vision for building verification tools for Cyber-Physical Systems based on Hoare and He’s Unifying Theories of Programming (UTP) and interactive proof technology in Isabelle/HOL. We describe our mechanisation and explain some of the design decisions that we have taken to get a convenient and smooth implementation. In particular, we describe our use of lenses to encode state. We illustrate our work with an example UTP theory and describe the implementation of three foundational theories: designs, reactive processes, and the hybrid relational calculus. We conclude by reflecting on how tools are linked by unifying theories. This paper is dedicated to Bill Roscoe on the occasion of his 60th birthday.	algebraic equation;ana (programming language);augusto sampaio;cyber-physical system;dexter (malware);dexter kozen;duration calculus;encode;european association for theoretical computer science;fm broadcasting;finnish meteorological institute;formal aspects of computing;formal methods;formal verification;hol (proof assistant);hoare logic;hybrid system;hypertext transfer protocol;intel mcs-48;interaction-free measurement;interactive proof system;interrupt;isabelle;jim woodcock;jones calculus;journal of logic and computation;kernel (operating system);l4 microkernel family;lecture notes in computer science;logical volume management;multi-model database;numerical analysis;operating system;pei-yuan wei;postcondition;precondition;proof assistant;quantum key distribution;real-time locating system;real-time transcription;relational calculus;ron sun;semantics (computer science);springer (tank);symposium on operating systems principles;theory;unifying theories of programming	Simon Foster;Jim Woodcock	2017		10.1007/978-3-319-51046-0_3	computer science;theoretical computer science;algorithm	Logic	-21.281890343947715	27.606422151957425	163000
7456806e8550d375d523a6292fa77c27b3e0befd	retracing some paths in process algebra	process algebra	The very existence of the conference bears witness to the fact that “concurrency theory” has developed into a subject unto itself, with substantially different emphases and techniques to those prominent elsewhere in the semantics of computation. Whatever the past merits of this separate development, it seems timely to look for some convergence and unification. In addressing these issues, I have found it instructive to trace some of the received ideas in concurrency back to their origins in the early 1970’s. In particular, I want to focus on a seminal paper by Robin Milner [Mil75] , which led in a fairly direct line to his enormously influential work on [Mil80, Mil89]. I will take (to the extreme) the liberty of of applying hindsight, and show how some different paths could have been taken, which, it can be argued, lead to a more unified approach to the semantics of computation, and moreover one which may be better suited to modelling today’s concurrent, object-oriented languages, and the type systems and logics required to support such languages.	computation;concurrency (computer science);process calculus;type system;unification (computer science)	Samson Abramsky	1996		10.1007/3-540-61604-7_44	combinatorics;process calculus;discrete mathematics;computer science;mathematics;programming language;algorithm;conditional event algebra;algebra	PL	-21.022211669377498	30.021791919515184	163397
33879d5b230fd4e2320e68f84d105712e3a296af	a unified memory model for pointers	modelizacion;teoria demonstracion;sistema operativo;virtual memory;australian research council;haute performance;theorie preuve;formal specification;proof theory;intelligence artificielle;specification formelle;theorem proving;modelisation;especificacion formal;demonstration theoreme;theorem prover;general solution;marcador;pilot project;pointer;operating system;memoire virtuelle;alto rendimiento;pointeur;artificial intelligence;systeme exploitation;inteligencia artificial;demostracion teorema;modeling;high performance;formal specification and verification;memoria virtual;memory model	One of the challenges in verifying systems level code is the low-level, untyped view of the machine state that operating systems have. We describe a way to faithfully formalise this view while at the same time providing an easy-to-use, abstract and typed view of memory where possible. We have used this formal memory model to verify parts of the virtual memory subsystem of the L4 high-performance microkernel. All formalisations and proofs have been carried out in the theorem prover Isabelle and the verified code has been integrated into the current implementation of L4.	automated theorem proving;high- and low-level;isabelle;memory model (programming);microkernel;operating system;state (computer science);verification and validation	Harvey Tuch;Gerwin Klein	2005		10.1007/11591191_33	computer science;artificial intelligence;operating system;database;mathematics;overlay;automated theorem proving;programming language;algorithm	PL	-23.781240545630336	29.959719582576966	163788
2a264730abc9c7e1315d7a282de2ba0289c87826	the data-parallel categorical abstract machine	data parallel;abstract machine	Data-parallel ML is proposed for compilation to a distributed version (DPCAM) of Cousineau, Curien and Mauny's Categorical Abstract Machine. The DPCAM is a static network of CAMs which dynamically restrict the MIMD execution mode: nodes execute the same program and communicate only while executing the same function body. Programs violating this restriction or exhibiting unbounded spatial recursion abort or deadlock. The execution model thus de nes a class of SPMD programs. With respect to syntax and types, DPML is Mini-ML enriched with localization and remote evaluation functions. Its values are arrays of size matching the DPCAM network and containing ML values. Pointwise functions, data-parallel primitives and systolic algorithms are easily programmed. To improve the language's portability there is no automatic virtualization mechanism. Hence DPML is an intermediate target for more elaborate data-parallel languages, bridging the gap between direct source-language processor allocation and fully automatic allocation.	algorithm;bridging (networking);categorical abstract machine;compiler;data parallelism;deadlock;mimd;natural language processing;recursion;remote evaluation;spmd;software portability	Gaétan Hains;Christian Foisy	1993		10.1007/3-540-56891-3_5	pointer machine;machine learning;pattern recognition;programming language	PL	-22.50270058010721	29.789022059475915	165334
f98916d981d5fbd39f8b9713e93c77831778f8aa	turing plus: a comparison with c and pascal	code generation;type checking;exception handling;separate compilation;dependent types;high level language	It is desirable to do systems programming in a high-level language. C is attractive since it yields efficient machine code without placing many restrictions on the programmer. This lack of restrictions and language-supported checking, however, can make C code unreadable and unmaintainable. Pascal is attractive since it provides strong type checking in a relatively elegant package but Pascal compilers don't produce efficient code. Turing Plus offers the advantages of both these languages, including efficient machine code and strong type checking. It also has a number of other features that make it a very attractive alternative. For example, Turing Plus offers controlled access to machine-dependencies, type checked separate compilation, linkage to external routines, language-specified concurrency and exception handling. Portable compilers with replaceable code generators (allowing easy access to cross-compilation) are available and a Turing Plus to C translator is under construction to allow fast portability to all systems supporting C compilers.	accessibility;concurrency (computer science);cross compiler;exception handling;high- and low-level;high-level programming language;linkage (software);machine code;pascal;programmer;software portability;strong and weak typing;system programming;turing;type system	Stephen G. Perelgut;James R. Cordy	1988	SIGPLAN Notices	10.1145/44304.44318	exception handling;dependent type;computer science;theoretical computer science;programming language;high-level programming language;algorithm;code generation	PL	-24.38971908646022	27.717612167081818	166165
7c73b0c0e8a822401077f373d8d1ac5a8eb38507	precise and maximal race detection from incomplete traces	data race;maximal causality;precise;incomplete trace	We present RDIT, a novel dynamic technique to detect data races in multithreaded programs with incomplete trace information, i.e., in the presence of missing events. RDIT is both precise and maximal: it does not report any false alarms and it detects a maximal set of true traces from the observed incomplete trace. RDIT is underpinned by a sound BarrierPair model that abstracts away the missing events by capturing the invocation data of their enclosing methods. By making the least conservative abstraction that a missing method introduces synchronization only when it has a memory address in scope that overlaps with other events or other missing methods, and by formulating maximal thread causality as logical constraints, RDIT guarantees to precisely detect races with maximal capability. RDIT has been applied in seven real-world large concurrent systems and has detected dozens of true races with zero false alarms. Comparatively, existing algorithms such as Happens-Before, Causal- Precedes, and Maximal-Causality which are known to be precise all report many false alarms when missing synchronizations.	algorithm;causality;concurrency (computer science);ibm notes;maximal independent set;maximal set;memory address;thread (computing);tracing (software)	Jeff Huang;Arun K. Rajagopalan	2016		10.1145/2983990.2984024	real-time computing;computer science;data mining;race condition;programming language;algorithm	PL	-20.0056935483664	31.395083106264295	166186
321679c5fd1f624cfc332953408fed924484cc09	timed abstract non-interference	intervalo tiempo;modelizacion;language based security;sistema temporizado;program counter;timing channels;metodo formal;timed system;methode formelle;semantics;program verification;semantica;time interval;semantique;formal method;modelisation;non interference;verificacion programa;data privacy;systeme temporise;abstract interpretation;verification programme;security;modeling;confidentialite donnee;intervalle temps	In this paper, we introduce a timed notion of abstract non-interference. This is obtained by considering semantics which observe time elapsed in computations. Timing channels can be modeled in this way either by letting the attacker to observe time as a public variable or reckon the time elapsed by observing the computational traces’ length, corresponding to observe the program counter. In the first case abstract non-interference provides a model for abstracting the information about time, namely we can for example consider models of attackers that can observe only intervals of time, or other more abstract properties. In the second case abstract non-interference provides a model for attackers able to observe properties of trace length, e.g., the public memory during the whole computation. We investigate when adding the observation of time does not increase the attacker’s power in disclosing confidential information about data. This models the absence of timing channels in language-based security.	acm sigact;abstract interpretation;automata theory;computer programming;computer security;confidentiality;data security;data-flow analysis;declassification;eacsl;esop;higher-order and symbolic computation;ibm i;interference (communication);language-based security;lecture notes in computer science;non-interference (security);parallel computing;program analysis;program counter;springer (tank);symposium on principles of programming languages;timing channel;turing completeness;type system;typed lambda calculus	Roberto Giacobazzi;Isabella Mastroeni	2005		10.1007/11603009_22	program counter;real-time computing;formal methods;systems modeling;computer science;operating system;database;distributed computing;semantics;computer security;algorithm	PL	-22.247615264071914	30.48079614675152	167362
fb29c8a76ad628cf9fd483aaa8a11327fe743195	specification and implementation problems for c	machine abstraite;langage c;abstract state machine;maquina abstracta;semantics;lenguaje especializado;langage java;interpretacion abstracta;specification programme;semantica;semantique;specification language;abstract machine;c language;lenguaje java;lenguaje especificacion;langage specialise;interpretation abstraite;abstract interpretation;program specification;special purpose language;langage specification;especificacion programa;lenguaje c;java language	During the attempts to build an Abstract State Machine (ASM) model for the semantics of C# programs, we tried to directly and faithfully reflect the intuitions and design decisions which are expressed in the C# Language Specification. This work and the comparison between the corresponding ASM models for C# and Java brought to light a few gaps and mistakes in the C# reference manual, inconsistencies with different implementations of C#. Some of these critical cases (especially the gaps) will be fully and correctly specified here by ASM rules of the model for C#.	abstract state machines;java;programming language specification	Nicu G. Fruja	2004		10.1007/978-3-540-24773-9_10	parallel computing;specification language;computer science;semantics;abstract machine;programming language;algorithm;abstract state machines	PL	-23.70567819493019	28.76128397810968	168475
fe7473d902177cc974b2f856f35f70cf1e2f43c7	bmclua: a translator for model checking lua programs		Lua is a programming language designed as scripting language, which is fast, lightweight, and suitable for embedded applications. Due to its features, Lua is widely used in the development of games and interactive applications for digital TV. However, during the development phase of such applications, some errors may be introduced, such as deadlock, arithmetic overflow, and division by zero. This paper describes a novel verification approach for software written in Lua, using as backend the Efficient SMTBased Context-Bounded Model Checker (ESBMC). Such an approach, called bounded model checking - Lua (BMCLua), consists in translating Lua programs into ANSI-C source code, which is then verified with ESBMC. Experimental results show that the proposed verification methodology is effective and efficient, when verifying safety properties in Lua programs. The performed experiments have shown that BMCLua produces an ANSI-C code that is more efficient for verification, when compared with other existing approaches. To the best of our knowledge, this work is the first that applies bounded model checking to the verification of Lua programs.	ansi c;apl;deadlock;division by zero;embedded system;experiment;lua;model checking;programming language;scripting language;verification and validation	Felipe R. Monteiro;Francisco A. P. Januário;Lucas C. Cordeiro;Eddie Batista de Lima Filho	2017	ACM SIGSOFT Software Engineering Notes	10.1145/3127360.3127367	model checking;programming language;software engineering;deadlock;division by zero;arithmetic overflow;software;source code;computer science;scripting language;bounded function	SE	-21.263362439144004	28.432441000702536	169306
03afb63d21bc5914d4b10cbb640108d13f1f479b	automatic generation and management of interprocedural program analyses	automatic generation;target language;fortran;program analysis;abstract interpretation	We have designed and implemented an interprocedural program analyzer generator, called system Z. Our goal is to automate the generation and management of semantics-based interprocedural program analysis for a wide range of target languages. System Z is based on the abstract interpretation framework. The input to system Z is a high-level specification of an abstract interpreter. The output is a C code for the specified interprocedural program analyzer. The system provides a high-level command set (called projection expressions) in which the user can tune the analysis in accuracy and cost. The user writes projection expressions for selected domains; system Z takes care of the remaining things so that the generated analyzer conducts an analysis over the projected domains, which will vary in cost and accuracy according to the projections. We demonstrate the system's capabilities by experiments with a set of generated analyzers which can analyze C, FORTRAN, and SCHEME programs.	abstract interpretation;care-of address;experiment;fortran;high- and low-level;ibm system z;program analysis;scheme	Kwangkeun Yi;Williams Ludwell Harrison	1993		10.1145/158511.158642	program analysis;real-time computing;computer science;theoretical computer science;programming language	PL	-20.9262965618086	26.076944067215198	169482
aa36554aacbb64f60c89f890937104e5430a7eb2	how concurrent logic programming could benefit from using linda-like operations	data storage;interprocess communication	 . The aim of this paper is to analyze the concept of extending theparallel logic programming paradigm with Linda-like operations, thus facilitatingdistributed data storage, access and management. Data are accessedin a uniform fashion, regardless if they are stored locally or remotely. Neitherperpetual processes to maintain the distributed data nor establishing sometimesvery complex of interprocess communication channels to broadcastmessages or collect results is needed, in contrast... 	concurrent logic programming;linda (coordination language)	Grzegorz Czajkowski;Krzysztof Zielinski	1993			concurrent constraint logic programming;futures and promises;computer architecture;parallel computing;declarative programming;concurrency;computer science;functional logic programming;programming paradigm;hardware description language;inductive programming;programming language;prolog;concurrent object-oriented programming	HCI	-26.132169276349984	25.830421966576708	169601
da4127c2b3e4fe7ac15107f3a641e083daf0ceab	a resource semantics and abstract machine for safe: a functional language with regions and explicit deallocation	abstract machines;memory management;code generation;functional languages;certifying compilers	In this paper we summarise Safe, a first-order functional language for programming small devices and embedded systems with strict memory requirements, which has been introduced elsewhere. It has some unusual memory management features such as heap regions and explicit cell deallocation. It is targeted at a Proof Carrying Code environment, and consistently with this aim the Safe compiler provides machine checkable certificates about important safety properties such as the absence of dangling pointers and bounded memory consumption. The kernel of the paper is devoted to developing part of the Safe compileru0027s back-end, by deriving an appropriate abstract machine from the language semantics, by providing the code generation functions, and by formally proving that the translation is sound, both in the semantic and in the memory consumption senses.	abstract machine;functional programming;memory management	Manuel Montenegro;Ricardo Peña-Marí;Clara Segura	2014	Inf. Comput.	10.1016/j.ic.2014.01.003	memory safety;computer science;theoretical computer science;abstract machine;programming language;functional programming;algorithm;code generation;memory management	Logic	-21.290092379956626	29.036516018189246	169717
5250bbf89e5ef12e441fca061c22dc3c1c74699a	static interpretation of higher-order modules in futhark: functional gpu programming in the large		Session: Compilation and ConcurrencyWe present a higher-order module system for the purely functional data-parallel array language Futhark. The module language has the property that it is completely eliminated at compile time, yet it serves as a powerful tool for organizing libraries and complete programs. The presentation includes a static and a dynamic semantics for the language in terms of, respectively, a static type system and a provably terminating elaboration of terms into terms of an underlying target language. The development is formalized in Coq using a novel encoding of semantic objects based on products, sets, and finite maps. The module language features a unified treatment of module type abstraction and core language polymorphism and is rich enough for expressing practical forms of module composition.	compile time;compiler;coq (software);graphics processing unit;library (computing);machine that always halts;map;modular programming;organizing (structure);parallel array;programming in the large and programming in the small;programming language;rewriting;type system	Martin Elsman;Troels Henriksen;Danil Annenkov;Cosmin E. Oancea	2018	PACMPL	10.1145/3236792	polymorphism (computer science);compiler;programming language;theoretical computer science;computer science;semantics;functional programming;normalization property;elaboration;compile time;abstraction	PL	-24.289873311012524	27.009192529720515	169746
cbe5af2ec7fe2896085d8e652fdb94feba08021e	newspeak: an unexceptional language	unexceptional language;newspeak;high level languages;compile time	NewSpeak is a language designed for use in safety-critical programs. It tries to limit the freedom of the programmer to the kind of ideas in programming that are reasonably easy to formalise, without making these restrictions unduly onerous. Its principal characteristic is that it has no exceptional values or states. Incorrect constructions which would lead to exceptional behaviour, such as range violations or numerical overflow, are all dealt with at compile time.	newspeak	Ian F. Currie	1986	Software Engineering Journal		compile time;computer science;theoretical computer science;programming language;high-level programming language;newspeak;algorithm	SE	-21.674067051591145	29.89691281121553	169763
69b7456f3d47fed3745239b5f67996a0b9a1a5c9	comprehensive formal verification of an os microkernel	isabelle hol;microkernel;sel4;operating systems	We present an in-depth coverage of the comprehensive machine-checked formal verification of seL4, a general-purpose operating system microkernel.  We discuss the kernel design we used to make its verification tractable. We then describe the functional correctness proof of the kernel's C implementation and we cover further steps that transform this result into a comprehensive formal verification of the kernel: a formally verified IPC fastpath, a proof that the binary code of the kernel correctly implements the C semantics, a proof of correct access-control enforcement, a proof of information-flow noninterference, a sound worst-case execution time analysis of the binary, and an automatic initialiser for user-level systems that connects kernel-level access-control enforcement with reasoning about system behaviour. We summarise these results and show how they integrate to form a coherent overall analysis, backed by machine-checked, end-to-end theorems.  The seL4 microkernel is currently not just the only general-purpose operating system kernel that is fully formally verified to this degree. It is also the only example of formal proof of this scale that is kept current as the requirements, design and implementation of the system evolve over almost a decade. We report on our experience in maintaining this evolving formally verified code base.	access control;best, worst and average case;binary code;cobham's thesis;coherence (physics);correctness (computer science);end-to-end principle;fastpath;formal proof;formal verification;general-purpose modeling;kernel (operating system);l4 microkernel family;operating system;requirement;run time (program lifecycle phase);user space;verification and validation;worst-case execution time	Gerwin Klein;June Andronick;Kevin Elphinstone;Toby C. Murray;Thomas Sewell;Rafal Kolanski;Gernot Heiser	2014	ACM Trans. Comput. Syst.	10.1145/2560537	real-time computing;computer science;theoretical computer science;operating system;programming language	OS	-20.042725053508594	28.907583943720677	171182
2313547a61946eb1c2c1fadc580e626b0c5123f3	specification predicates with explicit dependency information	publikationer;konferensbidrag;artiklar;rapporter	Specifications of programs use auxiliary symbols to encapsulate concepts for a variety of reasons: readability, reusability, structuring and, in particular, for writing recursive definitions. The definition of these symbols often depends implicitly on the value of other locations such as fields that are not stated explicitly as arguments. These hidden dependencies make the verification process substantially more difficult. In this paper we develop a framework that makes dependency on locations explicit. This allows to define general simplification rules that avoid unfolding of predicate definitions in many cases. A number of non-trivial case studies show the usefulness of the concept.	axiomatic system;cognitive dimensions of notations;correctness (computer science);definition;formal verification;key;level of detail;precondition;predicate transformer semantics;recursion;unfolding (dsp implementation);utility	Richard Bubel;Reiner Hähnle;Peter H. Schmitt	2008			computer science;database;algorithm	PL	-22.361022865203868	26.69927916510522	171339
205441300e3d5d538d5fbec40795c13c57f4ad71	type checking circus specifications	formal methods;formal languages;formal method;concurrency;type checking;design and implementation;reactive system;refinement tool;formal language;type system	Circus is a formal language that combines Z, CSP and additional constructors of Morgan’s refinement calculus. It is aimed at the development by refinement of state-rich reactive systems. In this work, we define the Circus type system and describe the design and implementation of a type checker. We developed the type checker based directly on the typing rules that formalise the type system of Circus. We believe that this contributed to the robust construction of the type checker. We also discuss the validation strategy of the type checker, including integrations with other Circus tools.	community z tools;error message;formal language;process calculus;refinement (computing);refinement calculus;strong and weak typing;type rule;type signature;type system	Manuela Xavier;Ana Cavalcanti;Augusto Sampaio	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2007.08.027	formal language;formal methods;computer science;theoretical computer science;programming language;algorithm	PL	-20.82026920430248	26.584630310046617	172183
33e8779418e9ee4bde577581e89d5c12c372a3a0	objects for lexical analysis	analyse lexicale;persistent aspects;automata estado finito;maquina estado finito;flex;scanneur;object oriented programming;scanner;aspect weaver;codificacion;object oriented;aspect oriented programming;finite state automata;coding;barredor;oriente objet;source code;finite automaton;object oriented databases;automate fini;lex;separation of concerns;machine etat fini;programmation orientee objet;lolo;orientado objeto;analisis lexical;lexical analysis;finite state machine;codage;object oriented paradigm	This paper presents a new idea for lexical analysis: lolo (language-oriented lexer objects) is strictly based on the object orientation paradigm. We introduce the idea behind the system, describe the implementation, and compare it to the conventional approach using lex[1] or flex[2].lolo[3] extracts symbols from a sequence of input characters belonging to the ASCII or Unicode sets. lolo scanners can be extended without access to the source code: symbol recognizers can be derived by inheritance and an executing scanner can be reconfigured for different contexts. Recognizer actions are represented by objects which may be replaced at any time. Recognizers need not be based on finite state automata; therefore, lolo can recognize symbols that systems like lex cannot recognize directly.	automata theory;compiler-compiler;finite-state machine;java;lex (software);lexical analysis;parsing;programming paradigm;regular expression;unicode;yacc	Bernd Kühl;Axel-Tobias Schreiner	2002	SIGPLAN Notices	10.1145/568600.568610	lexical analysis;computer science;artificial intelligence;finite-state machine;programming language;object-oriented programming;algorithm	PL	-25.832747672151758	27.050165349907775	172321
2688db96a0e77bb43a6cf5ba201b6212f95f13b0	experience in using a typed functional language for the development of a security application	cs se;cs cr	Developing an application with strong security requirements presupposes to identify the possible threats and to address all of them. Adverting or diverting the execution of an application is the goal of any attack which means that its targets span almost all the stages of development of the application and every aspect of its execution. On a first step in using a Formal Integrated Development Environment, we developed an industrial security application which was independently successfully assessed. This development was part of a study initiated and funded by the French Network and Information Security Agency (ANSSI1). The overall study was carried out by participants from industry (SafeRiver2, Normation3 and Oppida4) and academia (CEDRIC5 and Inria6). The subject of this study was to determine what features of functional programming languages can help to prevent attacks and what are the ones which bring vulnerabilities. The results can be found on the ANSSI website.7 The development part of the study was led by SafeRiver and was informally and separately evaluated by an ANSSI team and an independent assessor. In this paper, we present this experience of development. It uses a typed functional language. We describe how the formal grounding of the semantics of this language and its compiler have allowed for a trustworthy development and have facilitated the fulfillment of the security specification. We did not address a formal description of security requirements nor mechanically-checked proofs of their fulfillments. But, some of the requirements have been reified onto type properties and thus automatically checked by the typing phase of the compilation process.	code;compiler;coq (software);description logic;documentation;functional programming;high-level programming language;information security;integrated development environment;library (computing);linear algebra;ocaml;reification (computer science);relevance;requirement;type system;validator;vulnerability (computing);xml namespace	Damien Doligez;Christèle Faure;Thérèse Hardin;Manuel Maarek	2014		10.4204/EPTCS.149.6	computer science;database;programming language;algorithm	SE	-21.471651180915895	27.464698234762995	173290
0d3a53fff887fa7464499ce97c24b542d7c25fa0	building object-oriented data structures in ada 95	reducibility among np complete problems;graph algorithms;shortest path problem	This paper demonstrates how the inheritance and storage control facilities of Ada 95, along with the generic package mechanism of Ada 83, can be used to create object-oriented data structures. By way of illustration, a class hierarchy of generic linear collections (stacks, queues, lists, and ordered lists) is presented.	ada;class hierarchy;data structure;stack (abstract data type)	David C. Hunter	1997		10.1145/2817460.2817506	discrete mathematics;mathematics;programming language;algorithm	PL	-25.583434641357847	25.50291829741228	173562
993d57f5eb2f677017c6999a5f6969514304babe	designing structured cobol programs	cobol;constructional rules;control structures;structured programming;postprocessor;program structures;abstract types;documentation	TYPES IN COBOL COBOL provides the following data types: 1 . Primitive datatypes (a) numbers in different code representations (b) character strings (a) records (b) tables. 2. Composed data types All instructions of COBOL refer to these data types. Beyond that COBOL provides no imtruments for handling abstract data types. Since abstract types have proved to be a good concept for modular programming it is desirable to simulate this technique as far as possible. The following method is simple and easy to implement: Each abstract data object is represented by the form 01 (abstract-data-object) PIC x(( length-of-the-object)) and has to be detailed by a record or a table within the LINKAGE SECTION of a program containing operators for this type. A weakness is that the length of the object is dependent on factors like machine type and compiler. All abstract operators referring to a certain abstract data type are formulated by COBOL subprograms and combined to one COBOL program representing a module. DESIGNING STRUCTURED COBOL PROGRAMS 467 Outside of a module only the length of an abstract data object and its operators are known. Operators for data objects are usually: (i) fetching and storing of values of the object (ii) input/output to external devices (iii) selection of components of the objects (iv) application dependent processing of the object. Instructions already provided by COBOL for this concept are: (i) the MOVE instruction which permits fetching and storing (ii) READ/WRITE instructions for standard input/output (iii) the OF clause and subscripts or indices for the selection of components. EXAMPLE The rules may be demonstrated by a simple program for processing data objects of the type DATE. After reading a date and a time distance in days a new date with the given distance should be computed. The top down flowchart shown in Figure 1 is implemented by two COBOL ( i n days) I < read o l d date and d istance i n days Add o b j e c t s of t h e t y p e DCITE and TIME DISTANCE EOF new month := o l d month	a new kind of science;abstract data type;c file input/output;cobol;compiler;emoticon;end-of-file;flowchart;modular programming;simulation;standard streams;subroutine;top-down and bottom-up design	Otto K. Ferstl;Elmar J. Sinz	1982	Softw., Pract. Exper.	10.1002/spe.4380120507	documentation;computer science;software engineering;cobol;programming language;structured programming;second-generation programming language	PL	-26.053058486044318	26.299038809271202	173608
86a7c9b4afacbceaab1f45cd463973c8c2545525	modeling basic lotos by fsms for conformance testing	labeled transition system;input output;conformance testing;fault coverage;finite state machine	A challenging issue is the derivation of a nite test suite from a given LOTOS speci cation modeled by a labeled transition system (LTS) such that complete fault coverage is guaranteed for a certain class of implementations with respect to a particular conformance relation. It is shown in this paper that this problem can be solved by translating an LTS into an input/output nite state machine (FSM) for trace or failure semantics, respectively, and subsequently applying existing FSM-based methods for test derivation with complete fault coverage. It is also demonstrated that the obtained tests can be further optimized taking into account the speci cs of the FSMs constructed from the LTSs.	conformance testing;failure semantics;fault coverage;finite-state machine;input/output;language of temporal ordering specification;test suite;transition system	Q. M. Tan;Alexandre Petrenko;Gregor von Bochmann	1995		10.1007/978-0-387-34892-6_9	reliability engineering;real-time computing;computer science;distributed computing	SE	-20.15974854287427	26.506467507596636	173738
3ebac497ab72b365e215f9fc02ec46b116261e41	object-sensitive type analysis of php	object sensitivity;abstract garbage collection;php;cost and precision;static analysis;monotone frameworks	In this paper we develop an object-sensitive type analysis for PHP, based on an extension of the notion of monotone frameworks to deal with the dynamic aspects of PHP, and following the framework of Smaragdakis et al. for object-sensitive analysis.  We consider a number of instantiations of the framework to see how the choices affect the running cost of the analysis, and the precision of the outcome. In this setting we have not been able to reproduce the major gains reported by Smaragdakis et al., but do find that abstract garbage collection substantially increases the scalability of our analyses.	garbage collection (computer science);php;scalability;monotone	Henk Erik Van der Hoek;Jurriaan Hage	2015		10.1145/2678015.2682535	real-time computing;simulation;computer science;database;static analysis	PL	-20.610316116412836	29.65696690389073	173804
6116b5eb8ad7e9673665efae733e5ddc03c699b0	specialization scenarios: a pragmatic approach to declaring program specialization	declarative programming;lenguaje programacion;sistema operativo;symbolic computation;semantics based program manipulation;representation graphique;programming language;declaration of specialization opportunity;langage evolue;program transformation;specification programme;transformation programme;software engineering;manipulation programme basee semantique;evaluation partielle;declaration of specialization opportunities;program specialization;calculo simbolico;transformacion programa;operating system;predictable partial evaluation;partial evaluation;genie logiciel;langage programmation;grafo curva;systeme exploitation;lenguaje evolucionado;evaluacion parcial;program specification;high level language;declaration opportunite specialisation;ingenieria informatica;calcul symbolique;especificacion programa;graphics	Partial evaluation is a program transformation that automatically specializes a program with respect to invariants. Despite successful application in areas such as graphics, operating systems, and software engineering, partial evaluators have yet to achieve widespread use. One reason is the difficulty of adequately describing specialization opportunities. Indeed, underspecialization or overspecialization often occurs, without any feedback as to the source of the problem. We have developed a high-level, module-based language allowing the program developer to guide the choice of both the code to specialize and the invariants to exploit during the specialization process. To ease the use of partial evaluation, the syntax of this language is similar to the declaration syntax of the target language of the partial evaluator. To provide feedback, declarations are checked during the analyses performed by partial evaluation. The language has been successfully used by a variety of users, including students having no previous experience with	compiler;declaration (computer programming);feedback;graphics;high- and low-level;interpreter (computing);invariant (computer science);operating system;partial evaluation;partial template specialization;program transformation;software engineering	Anne-Françoise Le Meur;Julia L. Lawall;Charles Consel	2004	Higher-Order and Symbolic Computation	10.1023/B:LISP.0000029448.02877.78	symbolic computation;declarative programming;computer science;graphics;programming language;partial evaluation;high-level programming language;algorithm	PL	-23.89407074445159	29.0438542968053	173881
a716a74160f63efaaa38b260d37e6c879e66a5e0	chaperone contracts for higher-order sessions		Contracts have proved to be an effective mechanism that helps developers in identifying those modules of a program that violate the contracts of the functions and objects they use. In recent years, sessions have established as a key mechanism for realizing inter-module communications in concurrent programs. Just like values flow into or out of a function or object, messages are sent on, and received from, a session endpoint. Unlike conventional functions and objects, however, the kind, direction, and properties of messages exchanged in a session may vary over time, as the session progresses. This feature of sessions calls for contracts that evolve along with the session they describe.   In this work, we extend to sessions the notion of chaperone contract (roughly, a contract that applies to a mutable object) and investigate the ramifications of contract monitoring in a higher-order language that features sessions. We give a characterization of correct module, one that honors the contracts of the sessions it uses, and prove a blame theorem. Guided by the calculus, we describe a lightweight implementation of monitored sessions as an OCaml module with which programmers can benefit from static session type checking and dynamic contract monitoring using an off-the-shelf version of OCaml.		Hernán C. Melgratti;Luca Padovani	2017	PACMPL	10.1145/3110279	real-time computing;computer science	PL	-22.698771017385166	31.93761305162212	174431
34093e873e9d35bd6bed6201711c27bedd9adcab	static analysis of programs with imprecise probabilistic inputs		Having a precise yet sound abstraction of the inputs of numerical programs is important to analyze their behavior. For many programs, these inputs are probabilistic, but the actual distribution used is only partially known. We present a static analysis framework for reasoning about programs with inputs given as imprecise probabilities: we define a collecting semantics based on the notion of previsions and an abstract semantics based on an extension of Dempster-Shafer structures. We prove the correctness of our approach and show on some realistic examples the kind of invariants we are able to infer.	correctness (computer science);invariant (computer science);numerical analysis;static program analysis	Assalé Adjé;Olivier Bouissou;Jean Goubault-Larrecq;Eric Goubault;Sylvie Putot	2013		10.1007/978-3-642-54108-7_2	theoretical computer science;algorithm	PL	-19.552362313614573	28.54081325957529	174455
46aac855784e894242488bcfc1f0779503042f8b	lava: hardware design in haskell	langage fonctionnel;equipement informatique;lenguaje programacion;concepcion asistida;computer aided design;concepcion circuito;formal specification;programming language;implementation;circuit design;lenguaje funcional;functional programming;specification formelle;fast fourier transform;ejecucion;especificacion formal;formal verification;system design;polymorphism;conception assistee;langage programmation;hardware design;equipo informatico;verification formelle;polymorphisme;conception circuit;programmation fonctionnelle;polimorfismo;higher order functions;type classes;hardware description language;functional language;programacion funcional;computer equipment;functional programming language	Lava is a tool to assist circuit designers in specifying, designing, verifying and implementing hardware. It is a collection of Haskell modules. The system design exploits functional programming language features, such as monads and type classes, to provide multiple interpretations of circuit descriptions. These interpretations implement standard circuit analyses such as simulation, formal verification and the generation of code for the production of real circuits.Lava also uses polymorphism and higher order functions to provide more abstract and general descriptions than are possible in traditional hardware description languages. Two Fast Fourier Transform circuit examples illustrate this.	fast fourier transform;formal verification;functional programming;hardware description language;haskell;higher-order function;monad (functional programming);programming language;simulation;systems design;type class;verification and validation	Per Bjesse;Koen Claessen;Mary Sheeran;Satnam Singh	1998		10.1145/289423.289440	polymorphism;fast fourier transform;formal verification;computer science;theoretical computer science;circuit design;formal specification;hardware description language;programming language;functional programming;implementation;higher-order function;algorithm;systems design	PL	-23.812691234416192	30.15453987346085	174926
350795523676e071a64d8d60acd30252db2c7eec	verifying rust programs with smack		Rust is an emerging systems programming language with guaranteed memory safety and modern language features that has been extensively adopted to build safety-critical software. However, there is currently a lack of automated software verifiers for Rust. In this work, we present our experience extending the SMACK verifier to enable its usage on Rust programs. We evaluate SMACK on a set of Rust programs to demonstrate a wide spectrum of language features it supports.	memory safety;rust;smack;system programming language	Marek S. Baranowski;Shaobo He;Zvonimir Rakamaric	2018		10.1007/978-3-030-01090-4_32	computer engineering;theoretical computer science;modern language;software;system programming;rust;memory safety;computer science	SE	-21.663890085598705	32.130230820508615	174984
4664aad36a3b0b7b1dbc86ddd699e793a7681678	generating more practical compilers by partial evaluation	partial evaluation;practical compilers	Partial evaluation has traditionally been done for untyped strict functional languages such as Scheme. We look at the difficulties in finding a feasible type for our partial evaluation function that avoids using too many layers of encoding and present a solution using generating extensions. We then look at constructing the generating extension from a suitably annotated original and the ease with which this can be implemented this using the Template Haskell infrastructure.	compiler;evaluation function;functional programming;partial evaluation;scheme;template haskell	Rogardt Heldal	1991			computer science;theoretical computer science;programming language;algorithm	PL	-23.0728126210983	26.18193179050815	175329
439e51d002a41d16919f37933614ca15958cba6b	a strategy for compiling classes, inheritance, and dynamic binding	herencia;compilacion;visibilite;visibilidad;fiabilidad;reliability;compilateur;heritage;forma normal;semantics;qa 76 software;langage java;compiler;semantica;semantique;computer programming;refinement method;dynamic binding;visibility;interpreteur;fiabilite;normal form;compilation;lenguaje java;forme normale;interpreter;methode raffinement;inheritance;metodo afinamiento;interprete;compilador;java language	This paper presents a refinement strategy for the compilation of a subset of Java that includes classes, inheritance, dynamic binding, visibility control, and recursion. We tackle the problem of compiler correctness by reducing the task of compilation to that of program refinement. More specifically, refinement laws are used as compilation rules to reduce the source program to a normal form that models an interpreter running the target code. The compilation process is formalized within a single and uniform semantic framework, where translations or comparisons between semantics are avoided. Each compilation rule can be proved correct with respect to the algebraic laws of the language.	late binding	Adolfo Duran;Ana Cavalcanti;Augusto Sampaio	2003		10.1007/978-3-540-45236-2_18	single compilation unit;compiler;dynamic compilation;interpreter;native image generator;visibility;computer science;theoretical computer science;just-in-time compilation;reliability;computer programming;semantics;compilation error;programming language;algorithm	PL	-23.39637207394137	28.944259432388478	175881
31ce6cdf5b6657d7c34d7cb6e7a5986e9810c563	a method to generate verification condition generator	grammar;software;generators;user needs;c language verification condition generator vcgens compilers verification tools vcgen2 vcgengen;verification condition;verification condition generator;calculus generators syntactics grammar software production prototypes;action function program verification hoare logic verification condition;prototypes;program verification;functional programming;hoare logic;program compilers c language formal verification;c language;formal verification;syntactics;calculus;production;action function;program compilers;domain specificity	We propose a method to generate certain verification condition generators (VCGens, for short) automatically to be used in certifying compilers or other verification tools in this paper, to alleviate the burden of developing various kinds of VCGens in the domain-specific program verification tools. We introduce a new methodology for describing the rules in the verification condition calculation. We have implemented a prototype of VCGEN2(VCGenGen) using C++. This tool provides a series of interfaces named action functions to the users. Users can describe the calculation rules by combining these action functions. And our tool also embeds a parser generator, so users need to feed in the grammar of the languages along with the calculation rules. If there is no error, VCGEN2 outputs the corresponding VCGen with respect to the user-defined languages and rules. We have used our prototype to generate a number of VCGens successfully as demonstration.	c++;compiler;compiler-compiler;formal verification;programmer;prototype;separation logic;verification condition generator	Zhaopeng Li;Yonghui Zhang;Yiyun Chen	2011	2011 Fifth International Conference on Theoretical Aspects of Software Engineering	10.1109/TASE.2011.25	formal verification;computer science;theoretical computer science;grammar;prototype;hoare logic;programming language;functional programming;intelligent verification;algorithm;functional verification	SE	-20.73711423665865	25.927379541416347	176597
46bd41c8bf40d3f2ba949494a6a363eeffc22eab	homogeneous family sharing	herencia;anotacion;ombre;theorie type;shadow classes;compilateur;views;heritage;semantics;annotation;compiler;semantica;semantique;teoria de tipos;family inheritance;sombra;shadow;object oriented;type theory;oriente objet;technical report;inheritance;orientado objeto;languages;compilador;type system	Recent work has introduced class sharing as a mechanism for adapting a family of related classes with new functionality. This paper introduces homogeneous family sharing, implemented in the J&h language, in which the sharing mechanism is lifted from class-level sharing to true family-level sharing. Compared to the original (heterogeneous) class sharing mechanism, homogeneous family sharing provides useful new functionality and substantially reduces the annotation burden on programmers by eliminating the need for masked types and sharing declarations. This is achieved through a new mechanism, shadow classes, which permit homogeneous sharing of all related classes in shared families. The new sharing mechanism has a straightforward semantics, which is formalized in the J&h calculus. The soundness of the J&h type system is proved. The J&h language is implemented as an extension to the J& language. To demonstrate the effectiveness of family sharing, the Polyglot compiler framework is ported to J&h.	compiler;programmer;sequent calculus;type system	Xin Qi;Andrew C. Myers	2010		10.1145/1869459.1869502	shadow;compiler;type system;computer science;technical report;theoretical computer science;semantics;programming language;object-oriented programming;view;type theory;algorithm	PL	-24.69546701587197	27.33867585740714	178733
1dc22468e2f8c8dfaece05c85f06f5a61f4083c1	prototyping proof carrying code	verification condition generator;programming language;proof carrying code	We introduce a generic framework for proof carrying code, developed and mechanically verified in Isabelle/HOL. The framework defines and proves sound a verification condition generator with minimal assumptions on the underlying programming language, safety policy, and safety logic. We demonstrate its usability for prototyping proof carrying code systems by instantiating it to a simple assembly language with procedures and a safety policy for arithmetic overflow.	assembly language;hol (proof assistant);isabelle;programming language;proof-carrying code;prototype;usability;verification condition generator	Martin Wildmoser;Tobias Nipkow;Gerwin Klein;Sebastian Nanz	2004		10.1007/1-4020-8141-3_27	computer science;theoretical computer science;programming language;algorithm;code generation	PL	-21.11572933568536	27.43475903681739	179856
5eba152bd5e898cfb048e6875fe48ac4008a9d33	the seahorn verification framework	syntax;program verification computers;software verification;semantics;compilers;computer programs;proving;model checking;safety;software development tools;algorithms;checkout;abstract interpretation;experimentation;models;programming languages	In this paper, we present SeaHorn, a software verification framework. The key distinguishing feature of SeaHorn is its modular design that separates the concerns of the syntax of the programming language, its operational semantics, and the verification semantics. SeaHorn encompasses several novelties: it (a) encodes verification conditions using an efficient yet precise inter-procedural technique, (b) provides flexibility in the verification semantics to allow different levels of precision, (c) leverages the state-of-the-art in software model checking and abstract interpretation for verification, and (d) uses Horn-clauses as an intermediate language to represent verification conditions which simplifies interfacing with multiple verification tools based on Horn-clauses. SeaHorn provides users with a powerful verification tool and researchers with an extensible and customizable framework for experimenting with new software verification techniques. The effectiveness and scalability of SeaHorn are demonstrated by an extensive experimental evaluation using benchmarks from SV-COMP 2015 and real avionics code.	abstract interpretation;autopilot;avionics;emoticon;experiment;horn clause;model checking;modular design;operational semantics;programming language;scalability;software verification;systemverilog	Arie Gurfinkel;Temesghen Kahsai;Anvesh Komuravelli;Jorge A. Navas	2015		10.1007/978-3-319-21690-4_20	model checking;compiler;verification;syntax;software verification;computer science;theoretical computer science;database;semantics;high-level verification;runtime verification;programming language;intelligent verification;algorithm;functional verification	Logic	-19.707350037598534	27.64806494642127	179987
bcad9b08d6a02d005d1cf0515ec72f8dba231d02	static slicing of explicitly synchronized languages	concurrent programming;articulo;csp;program slicing	Static analysis of concurrent languages is a complex task due to the non-deterministic execution of processes. If the concurrent language being studied allows process synchronization, then the analyses are even more complex (and thus expensive), e.g., due to the phenomenon of deadlock. In this work we introduce a static analysis technique based on program slicing for concurrent and explicitly synchronized languages in general, and CSP in particular. Concretely, given a particular point in a specification, our technique allows us to know what parts of the specification must necessarily be executed before this point, and what parts of the specification could be executed before it. Our technique is based on a new data structure that extends the Synchronized Control Flow Graph (SCFG). We show that this new data structure improves the SCFG by taking into account the context in which processes are called and, thus, it makes the slicing process more precise. The technique has been implemented and tested with real specifications, producing good results. After formally defining our technique, we describe our tool, its architecture, its main applications and the results obtained from several experiments conducted in order to measure the performance of the tool.	control flow graph;data structure;deadlock;experiment;parallel computing;program slicing;static program analysis;synchronization (computer science)	Michael Leuschel;Marisa Llorens;Javier Oliver;Josep Silva;Salvador Tamarit	2012	Inf. Comput.	10.1016/j.ic.2012.02.005	program slicing;real-time computing;concurrent computing;computer science;communicating sequential processes;programming language;algorithm	PL	-19.18246276435059	29.07326110014564	180134
a4112ef56b211c9da43c4cff3228787be2ae90e3	copying and comparing: problems and solutions	lenguaje programacion;complex objects;software tool;language class;object oriented language;programming language;program transformation;object oriented programming;transformation programme;cloning;automatic generation;program optimization;transformacion programa;classe langage;smalltalk;langage programmation;optimisation programme;programmation orientee objet;object model;optimizacion programa;clase lenguaje;clonage	In object oriented programming, it is sometimes necessary to copy objects and to compare them for equality or inequality. We discuss some of the issues involved in copying and comparing objects and we address the problem of generating appropriate copying and comparing operations automatically, a service that is not provided by most object oriented languages and environments. Automatic generation appears to be not only desirable, because hand-coding these methods is mechanical and yet error-prone, but also feasible, because the form of the code is simple and largely predictable. Some languages and some object models presented in the literature do support generic copying and comparing, typically defining separate “shallow” and “deep” versions of both operations. A close examination of these definitions reveals inadequacies. If the objects involved are simple, copying and comparing them is straightforward. However, there are at least three areas in which insufficient attention has been given to copying and comparing complex objects: (1) values are not distinguished from objects; (2) aggregation is not distinguished from association; and (3) the correct handling of linked structures other than trees is neglected. Solving the third problem requires a mechanism built into the language, such as exists in Eiffel. Building such a mechanism without modifying the language requires a language with sufficient reflexive facilities, such as Smalltalk. Even then, the task is difficult and the result is likely to be insecure. We show that fully automatic generation of copying and comparing operations is not feasible because compilers and other software tools have access only to the structure of the objects and not to their semantics. Nevertheless, it is possible to provide default methods that do most of the work correctly and can be fine-tuned with a small additional amount of hand-coding. We include an example that illustrates the application of our proposals to C++. It is based on additional declarations handled by a preprocessor.	automatic differentiation;c++;cognitive dimensions of notations;compiler;eiffel;hand coding;library (computing);linked data structure;preprocessor;programmer;smalltalk;social inequality;stan (fan);superuser	Peter Grogono;Markku Sakkinen	2000		10.1007/3-540-45102-1_11	computer science;artificial intelligence;programming language;object-oriented programming;algorithm	PL	-23.274076545106514	26.662505702387737	180601
27f1053e23237c9d6c8034f3bed14d014cd254b3	subclassing errors, oop, and practically checkable rules to prevent them	object oriented programming;functional programming;software development	This paper considers an example of Object-Oriented Program ming (OOP) leading to subtle errors that break separation of interface and implementations. A comprehensive principle that guards against such errors is undecidable. The paper in troduces a set ofmechanically verifiablerules that prevent these insidious problems. Although the r ul s seem restrictive, they are powerful and expressive, as we show on several familiar exam ples. The rules contradict both the spirit and the letter of t he OOP. The present examples as well as available theoretical a nd experimental results pose a question if OOP is conducive t o software development at all.	assembly language;compile time;compiler;gaussian blur;goto;illegal opcode;lint (software);microcode;postcondition;scheme;software bug;software development;undecidable problem	Oleg Kiselyov	2003	CoRR		computer science;theoretical computer science;software development;programming language;object-oriented programming;functional programming;algorithm	PL	-22.03952913291939	29.31236316373354	180751
87b88ea0bfeffe2d210e156be68973c183f944a9	inference of session types from control flow	session types;control flow;imperative programming;program analysis;type inference	This is a study of a technique for deriving the session type of a program written in a statically typed imperative language from its control flow. We impose on our unlabelled session type syntax a well-formedness constraint based upon normalisation and explore the effects thereof. We present our inference algorithm declaratively and in a form suitable for implementation, and illustrate it with examples. We then present an implementation of the algorithm using a program analysis and transformation toolkit.	algorithm;bi-directional text;c++;client–server model;compile time;compiler;computation;control flow;correctness (computer science);functional programming;haskell;imperative programming;inference engine;left 4 dead 2;program analysis;semiconductor industry;server (computing);substitution failure is not an error;template (c++);template processor;turing completeness;type conversion;type inference;type safety;type system	Peter Collingbourne;Paul H. J. Kelly	2010	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2010.06.003	program analysis;imperative programming;computer science;type inference;database;programming language;control flow;algorithm	PL	-22.843340807423616	26.38890557248059	180856
6dfb0f04ec932012b919b4620f2f1495d111fc7f	a coq formalisation of sql’s execution engines		In this article, we use the Coq proof assistant to specify and verify the low level layer of SQL’s execution engines. To reach our goals, we first design a high-level Coq specification for data-centric operators intended to capture their essence. We, then, provide two Coq implementations of our specification. The first one, the physical algebra, consists in the low level operators found in systems such as Postgresql or Oracle. The second, SQL algebra, is an extended relational algebra that provides a semantics for SQL. Last, we formally relate physical algebra and SQL algebra. By proving that the physical algebra implements SQL algebra, we give high level assurances that physical algebraic and SQL algebra expressions enjoy the same semantics. All this yields the first, to our best knowledge, formalisation and verification of the low level layer of an RDBMS as well as SQL’s compilation’s physical optimisation: fundamental steps towards mechanising SQL’s compilation chain.	compiler;coq (software);formal system;high- and low-level;high-level programming language;linear algebra;mathematical optimization;oracle database;postgresql;proof assistant;relational algebra;sql	Véronique Benzaken;Evelyne Contejean;Ch. Keller;E. Martins	2018		10.1007/978-3-319-94821-8_6	relational database management system;implementation;programming language;algorithm;sql;operator (computer programming);relational algebra;oracle;expression (mathematics);proof assistant;computer science	Security	-20.518793845301822	26.383060981436326	181257
d7b031d633826d3f78203ed81589b642a9bbd661	static serializability analysis for causal consistency		Many distributed databases provide only weak consistency guarantees to reduce synchronization overhead and remain available under network partitions. However, this leads to behaviors not possible under stronger guarantees. Such behaviors can easily defy programmer intuition and lead to errors that are notoriously hard to detect.   In this paper, we propose a static analysis for detecting non-serializable behaviors of applications running on top of causally-consistent databases. Our technique is based on a novel, local serializability criterion and combines a generalization of graph-based techniques from the database literature with another, complementary analysis technique that encodes our serializability criterion into first-order logic formulas to be checked by an SMT solver. This analysis is more expensive yet more precise and produces concrete counter-examples.   We implemented our methods and evaluated them on a number of applications from two different domains: cloud-backed mobile applications and clients of a distributed database. Our experiments demonstrate that our analysis is able to detect harmful serializability violations while producing only a small number of false alarms.	apache cassandra;causal consistency;causal filter;cycle detection;data store;distributed computing;distributed database;experiment;first-order logic;first-order predicate;high- and low-level;java;local consistency;mobile app;overhead (computing);programmer;satisfiability modulo theories;sensor;serializability;serialization;solver;static program analysis;weak consistency	Lucas Brutschy;Dimitar Dimitrov;Peter Müller;Martin T. Vechev	2018		10.1145/3192366.3192415	computer science;real-time computing;causal consistency;weak consistency;satisfiability modulo theories;distributed database;small number;programmer;static analysis;serializability	PL	-20.850345611614273	31.49844379271243	182028
094520538c6d03dbcdd4c0e843c2385b7e0a0735	class analysis of object-oriented programs through abstract interpretation	machine abstraite;interpretazione astratta;lenguaje programacion;object oriented language;programming language;flot donnee;maquina abstracta;flujo datos;object oriented programming;semantica;abstract machine;optimal operation;dataflow analysis;object oriented;langage programmation;funcionamiento optimo;oriente objet;analisi;data flow;abstract interpretation;programmation orientee objet;orientado objeto;fonctionnement optimal	We use abstract interpretation to deene a uniform formalism for presenting and comparing class analyses for object-oriented languages. We consider three domains for class analysis derived from three techniques present in the literature, viz., rapid type analysis, a simple dataaow analysis and the constraint technique of Palsberg and Schwartzbach. We obtain three static analyses which are provably correct and whose abstract operations are provably optimal. Moreover, we prove that our formalisation of the analysis of Palsberg and Schwartzbach is more precise than that of the dataaow analysis.	abstract interpretation;correctness (computer science);semantics (computer science);static program analysis;viz: the computer game	Thomas P. Jensen;Fausto Spoto	2001		10.1007/3-540-45315-6_17	computer science;theoretical computer science;database;distributed computing;abstract machine;programming language;object-oriented programming;algorithm	PL	-23.094611784626554	29.283200978128615	182075
500ba49329f035c7d8671aa359054ed58c6a1631	programming language approaches to bidirectional transformation	model-driven engineering;extra information;programming environment;bidirectional transformation;structured editor;involved transformation;data domain;diverse discipline;programming language approach;certain consistency condition;application scenario	Bidirectional transformations are pairs of transformations going back and forth between two data domains, possibly taking extra information into account to disambiguate in one or both directions, while being governed by certain consistency conditions. Application scenarios are view-update propagation in databases, model-driven engineering, and programming environments like structured editors, among others. Consequently, diverse disciplines contribute to the interest and development in this area. We focus on approaches that, rather than being centered on the data, take the involved transformations seriously as programs to analyze and manipulate.	bidirectional transformation;dos;database;model-driven engineering;programming language;software propagation;structured programming	Janis Voigtländer	2012		10.1145/2427048.2427049	computer science;theoretical computer science;algorithm	PL	-24.32615133781522	29.07075728952243	182148
05070b2471a44c934e8961d03a2406b9053bccbd	automatic scalable atomicity via semantic locking	abstract data types;semantics;concurrency;automatic locking;transactions	In this article, we consider concurrent programs in which the shared state consists of instances of linearizable abstract data types (ADTs). We present an automated approach to concurrency control that addresses a common need: the need to atomically execute a code fragment, which may contain multiple ADT operations on multiple ADT instances. We present a synthesis algorithm that automatically enforces atomicity of given code fragments (in a client program) by inserting pessimistic synchronization that guarantees atomicity and deadlock-freedom (without using any rollback mechanism). Our algorithm takes a commutativity specification as an extra input. This specification indicates for every pair of ADT operations the conditions under which the operations commute. Our algorithm enables greater parallelism by permitting commuting operations to execute concurrently. We have implemented the synthesis algorithm in a Java compiler and applied it to several Java programs. Our results show that our approach produces efficient and scalable synchronization.	abstract data type;algorithm;atomicity (database systems);client (computing);concurrency (computer science);concurrency control;deadlock;java compiler;linearizability;parallel computing;scalability;synchronization (computer science)	Guy Golan-Gueta;G. Ramalingam;Shmuel Sagiv;Eran Yahav	2015	TOPC	10.1145/3040223	parallel computing;real-time computing;concurrency;computer science;operating system;database;distributed computing;semantics;programming language;abstract data type;algorithm	PL	-20.49382744837918	31.010915263174393	182348
88b9ddc3d198e84ed568f30eddd7457749112f08	operational semantics of a kernel of the language electre	transition state;program behavior;lenguaje programacion;distributed system;systeme reparti;programming language;context free language;real time;sistema informatico;operational semantics;computer system;lenguaje cf;sistema repartido;modulo programa;informatique theorique;estado transitorio;temps reel;electre;langage programmation;tiempo real;comportement programme;systeme informatique;module programme;etat transition;langage cf;program module;computer theory;informatica teorica	Perraud, J., 0. Roux and M. Huou, Operational semantics of a kernel of the language ELECTRE, Theoretical Computer Science 97 (1992) 83-103. The real-time language ELECTRE describes behaviours of a real-time application using tasks called modules. Tasks are activated and pre-empted by events that come either from tasks themselves or from the controlled real-time application. To describe a current state of the application one needs both a program ELECTRE and the history of past event occurrences. We give operational semantics for a kernel of the language using a transition system whose transitions are calculated by attribute evaluation on a context-free grammar. It proves that any event occurrence turns any state into a new one, in a deterministic way.	attribute grammar;compiler;context-free grammar;context-free language;correctness (computer science);env;interpreter (computing);kernel (operating system);operational semantics;parallel computing;real-time clock;real-time computing;real-time locating system;real-time transcription;semantics (computer science);theoretical computer science;transition system	Jean Perraud;Olivier F. Roux;Marc Huou	1992	Theor. Comput. Sci.	10.1016/0304-3975(92)90388-V	electre;computer science;artificial intelligence;context-free language;transition state;programming language;operational semantics;algorithm	PL	-23.910514332884134	32.20061654472713	182784
057459287ae6e54f98afd8df936aec4cc0441f91	data abstraction mechanisms in sina/st	object oriented language	This paper describes a new data abstraction mechanism in an object-oriented model of computing. The data abstraction mechanism described here has been devised in the context of the design of Sina/st language. In Sina/st no language constructs have been adopted for specifying inheritance or delegation, but rather, we introduce simpler mechanisms that can support a wide range of code sharing strategies without selecting one among them as a language feature. Sina/st also provides a stronger data encapsulation than most of the existing object-oriented languages. This language has been implemented on the SUN 3 workstation using Smalltalk.	abstraction (software engineering);compiler;computer multitasking;data structure;evert willem beth;model of computation;object manager;programming language;simulation;smalltalk;user interface;workstation	Mehmet Aksit;Anand R. Tripathi	1988		10.1145/62083.62107	natural language processing;computer science;database;low-level programming language;programming language;object-oriented programming	PL	-26.329979143693212	27.02717437152309	182864
c6378976735b44fec16f91ef7a69c02b6a419825	implementing a semi-causal domain-specific language for context detection over binary sensors		In spite of the fact that many sensors in use today are binary (i.e. produce only values of 0 and 1), and that useful context-aware applications are built exclusively on top of them, there is currently no development approach specifically targeted to binary sensors. Dealing with notions of state and state combinators, central to binary sensors, is tedious and error-prone in current approaches. For instance, developing such applications in a general programming language requires writing code to process events, maintain state and perform state transitions on events, manage timers and/or event histories.   In another paper, we introduced a domain specific language (DSL) called Allen, specifically targeted to binary sensors. Allen natively expresses states and state combinations, and detects contexts on line, on incoming streams of binary events. Expressing state combinations in Allen is natural and intuitive due to a key ingredient: semi-causal operators. That paper focused on the concept of the language and its main operators, but did not address its implementation challenges. Indeed, online evaluation of expressions containing semi-causal operators is difficult, because semi-causal sub-expressions may block waiting for future events, thus generating unknown values, besides 0 and 1. These unknown values may or may not propagate to the containing expressions, depending on the current value of the other arguments.   This paper presents a compiler and runtime for the Allen language, and shows how they implement its state combining operators, based on reducing complex expressions to a core subset of operators, which are implemented natively. We define several assisted living applications both in Allen and in a general scripting language. We show that the former are much more concise in Allen, achieve more effective code reuse, and ease the checking of some domain properties.	causal filter;code reuse;cognitive dimensions of notations;combinatory logic;compiler;digital subscriber line;domain-specific language;programming language;scripting language;semiconductor industry;sensor;state (computer science);timer	Nic Volanschi;Bernard P. Serpette;Charles Consel	2018		10.1145/3278122.3278134	compiler;operator (computer programming);code reuse;programming language;combinatory logic;scripting language;domain-specific language;expression (mathematics);computer science;context awareness	PL	-26.33068412123845	29.26250021094716	183121
58a3bbd2865d3a44f7fec322f6490d437564b2ab	an operational semantics of the java card firewall	lenguaje programacion;semantica operacional;java card;programming language;operational semantics;langage java;semantique operationnelle;langage programmation;lenguaje java;bytecode;java language	This paper presents an operational semantics for a subset of Java Card bytecode, focussing on aspects of the Java Card firewall, method invocation, field access, variable access, shareable objects and contexts. The goal is to provide a precise description of the Java Card firewall using standard tools from operational semantics. Such a description is necessary for formally arguing the correctness of tools for validating the security of Java Card applications.	application programming interface;bartpe;belief revision;berg connector;byte;code;continuation;cornelia boldyreff;correctness (computer science);denotational semantics;experience;firewall (computing);formal specification;gosling emacs;hol (proof assistant);hoare logic;isabelle;java card;java modeling language;java bytecode;java virtual machine;lecture notes in computer science;linear algebra;loose source routing;mathematical optimization;operational semantics;programming language specification;runtime system;smart card application protocol data unit;software framework;subroutine;type safety	Marc Éluard;Thomas P. Jensen;Ewen Denney	2001		10.1007/3-540-45418-7_9	java data objects;java card;java api for xml-based rpc;real-time computing;jsr 94;java concurrency;jar;computer science;operating system;java modeling language;interface;strictfp;real time java;programming language;java;operational semantics;generics in java;scala;java applet;java annotation;non-blocking i/o	PL	-25.328273968553344	29.445235022082436	183955
81664fffebcecfd425801cf2116079803a6c8d43	implementing application-specific object-oriented theories in hol	modelizacion;high order logic;semantica operacional;semantic memory;operational semantics;logica orden superior;modelisation;semantique operationnelle;object oriented;oriente objet;logique ordre superieur;memoire semantique;modeling;orientado objeto;abstract types;memoria semantica;memory model	This paper presents a theory of Object-Oriented concepts embedded shallowly in HOL for the verification of OO analysis models. The theory is application-specific in the sense that it is automatically constructed depending on the type information of the application. This allows objects to have attributes of arbitrary types, making it possible to verify models using not only basic types but also highly abstracted types specific to the target domain. The theory is constructed by definitional extension based on the operational semantics of a heap memory model, which guarantees the soundness of the theory. This paper mainly focuses on the implementation details of the theory.	aliasing;communication diagram;definition;ecoop;embedded system;fosd mixin layers;formal methods;formal verification;hol (proof assistant);hoare logic;interaction-free measurement;interactive theorem proving (conference);java;kevin lano;lecture notes in computer science;memory management;memory model (programming);object constraint language;operational semantics;shriram krishnamurthi;software development;software engineering;theory;unified modeling language;verification condition generator	Kenro Yatake;Toshiaki Aoki;Takuya Katayama	2005		10.1007/11560647_33	memory model;systems modeling;semantic memory;computer science;artificial intelligence;theoretical computer science;programming language;object-oriented programming;operational semantics;algorithm	PL	-23.250809743613058	29.50234152514918	185059
6c7b19fd3339e63aafd63d64091406145a9327ef	implementation of a finite state machine with active libraries in c++	library design;articulo;error detection;implementation of a finite state machine with active libraries;finite state machine	Generative programming is an approach to generating customized programming components or systems. C++ template metaprogramming is a generative programming style. With metaprogramming we can reduce the runtime cost of our programs, more extensible libraries can be made, and the creation of active libraries is also supported. Active libraries are not passive collections of routines or objects, as are traditional libraries, but take an active role in generating code. Active libraries provide higher abstractions and can optimize those abstractions themselves. Our goal is to demonstrate the connection between Finite State Machines and active libraries. One of the fields where Finite State Machines are applicable is the implementation of complex protocol definitions. Since often only the results of test cases of a protocol are obtainable, the developer himself has to define and implement his own state machine description. With the help of active libraries we are able to use a compile-time Finite State Machine implementation, and do some monitoring at compile-time, such as consistency checking of the state machine’s state table, or error checking. Another aim of the compile-time state machine implementation is the enhanced effectiveness. In this paper we introduce the Finite State Machine’s formal definition, and then discuss the possible implementation techniques. We analyze the functionality, and place special emphasis on compile-time solutions. We describe the algorithms that carry out error checking and transformations on the Finite State Machine’s state table. Our implementation of the state minimization algorithm based on the Boost::MPL library is also described.	algorithm;boost;c++;compile time;compiler;error message;finite-state machine;library (computing);moore reduction procedure;requirement;state diagram;state transition table;template metaprogramming;μ operator	Zoltan Juhasz;Ádám Sipos;Zoltán Porkoláb	2007		10.1007/978-3-540-88643-3_14	computer science;theoretical computer science;engineering drawing;computer engineering	PL	-24.124660465073227	27.550492797500688	185072
eca6f52ecbc272752fd3852d4ac470f2bdb39236	an executable semantics of object-oriented models for simulation and theorem proving	theorem proving	This paper presents an executable semantics of OO models. We made it possible to conduct both simulation and theorem proving on the semantics by implementing its underlying heap memory structure within the expressive intersection of the functional language ML and the theorem prover HOL. This paper also presents a verification system ObjectLogic which supports simulation and theorem proving of OO models based on the executable semantics. As an application example, we show a verification of a UML model of a practical firewall system.	automated theorem proving;executable;firewall (computing);functional programming;hol (proof assistant);memory management;server (computing);simulation;test suite;turing completeness;unified modeling language;verification condition generator	Kenro Yatake;Takuya Katayama	2008			discrete mathematics;computer science;theoretical computer science;programming language	Logic	-20.645095245875456	27.445376089449564	186426
f3b8ffea97dce018429b2bf4f3995b42e22058f8	result certification of static program analysers with automated theorem provers		The automation of the deductive approach to program verication crucially depends on the ability to e ciently infer and discharge program invariants. In an ideal world, user-provided invariants would be strengthened by incorporating the result of static analysers as untrusted annotations and discharged by automated theorem provers. However, the results of object-oriented analyses are heavily quanti ed and cannot be discharged, within reasonable time limits, by state-of-the-art automated theorem provers. In the present work, we investigate an original approach for verifying automatically and e ciently the result of certain classes of object-oriented static analyses using o -the-shelf automated theorem provers. We propose to generate veri cation conditions that are generic enough to capture, not a single, but a family of analyses which encompasses Java bytecode veri cation and Fähndrich and Leino typesystem for checking null pointers. For those analyses, we show how to generate tractable veri cation conditions that are still quanti ed but fall in a decidable logic fragment that is reducible to the E ectively Propositional logic. Our experiments con rm that such veri cation conditions are e ciently discharged by o -the-shelf automated theorem provers.	abstract interpretation;automated theorem proving;bernays–schönfinkel class;byte;class hierarchy;cobham's thesis;coq (software);discharger;epr paradox;experiment;formal verification;java bytecode;operational semantics;pointer analysis;propositional calculus;scalability;static program analysis;verification and validation	Frédéric Besson;Pierre-Emmanuel Cornilleau;Thomas P. Jensen	2013		10.1007/978-3-642-54108-7_16	theoretical computer science;programming language;algorithm	SE	-20.07605232422986	26.90613998171187	186459
8ae3745f16265091183e5015c677b3186524c5a3	learning refinement types	refinement types;learning;higher order verification;testing	We propose the integration of a random test generation system (capable of discovering program bugs) and a refinement type system (capable of expressing and verifying program invariants), for higher-order functional programs, using a novel lightweight learning algorithm as an effective intermediary between the two. Our approach is based on the well-understood intuition that useful, but difficult to infer, program properties can often be observed from concrete program states generated by tests; these properties act as likely invariants, which if used to refine simple types, can have their validity checked by a refinement type checker. We describe an implementation of our technique for a variety of benchmarks written in ML, and demonstrate its effectiveness in inferring and proving useful invariants for programs that express complex higher-order control and dataflow.	algorithm;benchmark (computing);dataflow;refinement (computing);software bug;type system;verification and validation;whole earth 'lectronic link	He Zhu;Aditya V. Nori;Suresh Jagannathan	2015		10.1145/2784731.2784766	simulation;computer science;theoretical computer science;software testing;programming language;algorithm	PL	-19.507181144662116	27.611060003139308	186642
703ac8cebae1013f8687d2d180fabbc6165576f0	a concurrency abstraction model for avoiding inheritance anomaly in object-oriented programs	concurrent object oriented programming;object oriented programming	In a concurrent object-oriented programming language one would like to be able to inherit behavior and realize synchronization control without compromising the flexibility of either the inheritance mechanism or the synchronization mechanism. A problem called the inheritance anomaly arises when synchronization constraints are implemented within the methods of a class and an attempt is made to specialize methods through inheritance. The anomaly occurs when a subclass violates the synchronization constraints assumed by the superclass. A subclass should have the flexibility to add methods, add instance variables, and redefine inherited methods. Ideally, all the methods of a superclass should be reusable. However, if the synchronization constraints are defined by the superclass in a manner prohibiting incremental modification through inheritance, they cannot be reused, and must be reimplemented to reflect the new constraints; hence, inheritance is rendered useless. We have proposed a novel model of concurrency abstraction, where (a) the specification of the synchronization code is kept separate from the method bodies, and (b) the sequential and concurrent parts in the method bodies of a superclass are inherited by its subclasses in an orthogonal manner.	anomaly detection	Sandeep Kumar;Dharma P. Agrawal	2001		10.1007/3-540-45403-9_4	real-time computing;computer science;distributed computing;composition over inheritance;algorithm	PL	-25.925170302551834	29.787326004604314	186765
f819dbf82f45b553d43028c503bbfb3d0ea68dd4	using units of measurement in formal specifications	verification;engineering;modelizacion;arithmetic operation;formal specification;osciloscopio;physique;programming language;logiciel;operation arithmetique;z specification language;dimensional analysis;formal languages;conception;computer systems;specification language;ingenierie;specification formelle;modelisation;physics;tipificacion;type checking;computer programming languages;typing;unit of measurement;typage;design;systeme informatique;units of measurement;lenguaje especificacion;computer software;verificacion;oscilloscope;application;modeling;langage specification;operation aritmetica;applications;langage programmation ordinateur;langage formel	In the physical sciences and engineering, units of measurement provide a valuable aid to both the exposition and comprehension of physical systems. In addition, they provide an error checking facility comparable to static type checking commonly found with programming languages. It is argued that units of measurement can provide similar benefits in the specification and design of software and computer systems. To demonstrate this, we present an extension of the Z specification notation with support for the incorporation of units in specifications and demonstrate the feasibility of static dimensional analysis of the resulting language.	formal specification;programming language;type system	Ian J. Hayes;Brendan P. Mahony	1995	Formal Aspects of Computing	10.1007/BF01211077	computer science;units of measurement;formal specification;programming language;information technology;algorithm	SE	-23.712408884979183	29.89614415773731	187581
1cabbbe4a2d7fbe22f479f9b5528e857a1fefe23	development of rtos for plc using formal methods	systeme temps reel;controleur logique programmable;coreano;nuclear safety;verificacion modelo;usine fabrication;nuclear reactor;controlador logica programable;reseau electrique;factory;electrical network;linea montaje;critical level;diagramme etat;logiciel a securite critique;reacteur nucleaire;red electrica;metodo formal;methode formelle;verification modele;real time operating system;program verification;analisis automatico;seguridad nuclear;analisis programa;sistema reactivo;korean;formal method;surete nucleaire;verificacion programa;automatic analysis;diagrama estado;model checking;programmable logical controller;coreen;safety critical software;reactive system;assembly line;analyse automatique;systeme reactif;state diagram;fabrica;nuclear power plant;real time system;centrale nucleaire;sistema tiempo real;program analysis;analyse programme;reactor nuclear;verification programme;central nuclear;programmable logic controller;chaine montage	Programmable logic controller(PLC) is a computer system for instrumentation and control (IC Statecharts for specification and model checking for verification, and we give the results of applying formal methods to RTOS.		Jin Hyun Kim;Su-Young Lee;Young Ah Ahn;Jae-Hwan Sim;Jin Seok Yang;Na-Young Lee;Jin-Young Choi	2004		10.1007/978-3-540-30476-0_40	program analysis;model checking;embedded system;electrical network;state diagram;real-time operating system;reactive system;computer science;factory;programmable logic controller;nuclear reactor;programming language;algorithm;korean	Logic	-23.926683922853634	31.508381152741475	188070
5822de61be9de604e7b88f1ea2eda33a921bd084	verification of fm9801: an out-of-order microprocessor model with speculative execution, exceptions, and program-modifying capability	pipelined microprocessor;satisfiability;out of order;theorem prover;out of order execution;formal verification;speculative execution	We have verified the FM9801, a microprocessor design whose features include speculative execution, out-of-order issue and completion of instructions using Tomasulo's algorithm, and precise exceptions and interrupts. As a correctness criterion, we used a commutative diagram that compares the result of the pipelined execution from a flushed state to another flushed state with that of the sequential execution. Like many pipelined microprocessors, the FM9801 may not operate correctly if the executed program modifies itself. We discuss the condition under which the processor is guaranteed to operate correctly. In order to show that the correctness criterion is satisfied, we introduce an intermediate abstraction that records the history of executed instructions. Using this abstraction, we define a number of invariant properties that must hold during the operation of the FM9801. We verify these invariant properties, and then derive the proof of the commutative diagram from them. The proof has been mechanically checked by the ACL2 theorem prover.	exception handling;microprocessor;speculative execution	Jun Sawada;Warren A. Hunt	2002	Formal Methods in System Design	10.1023/A:1014122630277	parallel computing;real-time computing;computer science;out-of-order execution;programming language;algorithm	Logic	-20.135453673915652	31.05215656818242	188449
cdda4cbc0c9277dafe23dcea3b7a6fa4567b2302	verifying low-level implementations of high-level datatypes	network coding;open source	For efficiency and portability, network packet processing code is typically written in low-level languages and makes use of bit-level operations to compactly represent data. Although packet data is highly structured, low-level implementation details make it difficult to verify that the behavior of the code is consistent with high-level data invariants. We introduce a new approach to the verification problem, using a high-level definition of packet types as part of a specification rather than an implementation. The types are not used to check the code directly; rather, the types introduce functions and predicates that can be used to assert the consistency of code with programmer-defined data assertions. We describe an encoding of these types and functions using the theories of inductive datatypes, bit vectors, and arrays in the Cvc3 SMT solver. We present a case study in which the method is applied to open-source networking code and verified within the Cascade verification platform.	bit array;bit-level parallelism;data structure;declarative programming;embedded system;formal verification;high- and low-level;inductive type;linked data;network packet;open-source software;pointer (computer programming);programmer;rewriting;software portability;solver;source lines of code;theory	Christopher L. Conway;Clark W. Barrett	2010		10.1007/978-3-642-14295-6_28	code word;systematic code;parallel computing;linear network coding;computer science;theoretical computer science;programming language;algorithm	Logic	-22.247440988131814	27.76002670700516	189169
9b2f29ac875900b632facf1e2391908a38cb2069	type systems for the masses: deriving soundness proofs and efficient checkers	first order theorem proving;type systems;type checking;type soundness	The correct definition and implementation of non-trivial type systems is difficult and requires expert knowledge, which is not available to developers of domain-specific languages (DSLs) in practice. We propose Veritas, a workbench that simplifies the development of sound type systems. Veritas provides a single, high-level specification language for type systems, from which it automatically tries to derive soundness proofs and efficient and correct type-checking algorithms. For verification, Veritas combines off-the-shelf automated first-order theorem provers with automated proof strategies specific to type systems. For deriving efficient type checkers, Veritas provides a collection of optimization strategies whose applicability to a given type system is checked through verification on a case-by-case basis. We have developed a prototypical implementation of Veritas and used it to verify type soundness of the simply-typed lambda calculus and of parts of typed SQL. Our experience suggests that many of the individual verification steps can be automated and, in particular, that a high degree of automation is possible for type systems of DSLs.	algorithm;automated proof checking;automated theorem proving;digital subscriber line;domain-specific language;first-order predicate;high- and low-level;mathematical optimization;prototype;sql;simply typed lambda calculus;specification language;type inference;type safety;type system;vampire theorem prover;workbench	Sylvia Grewe;Sebastian Erdweg;Pascal Wittmann;Mira Mezini	2015		10.1145/2814228.2814239	discrete mathematics;theoretical computer science;mathematics;algorithm	PL	-20.044869147101657	25.73789145196624	189586
1dc5218e7ef47c1f1426eae8790215921e489194	time regions and effects for resource usage analysis	types;semantica operacional;memory protocols;operational semantics;logical programming;program verification;analisis programa;verificacion programa;tipificacion;semantique operationnelle;systeme type;typing;programmation logique;effects;protocole memoire;typage;program analysis;analyse programme;resource usage;verification programme;programacion logica	Various resources such as files and memory are associated with certain protocols about how they should be accessed. For example, a memory cell that has been allocated should be eventually deallocated, and after the deallocation, the cell should no longer be accessed. Igarashi and Kobayashi recently proposed a general type-based method to check whether a program follows such resource access policies, but their analysis was not precise enough for certain programs. In this paper, we refine their type-based analysis by introducing a new notion of time regions. The resulting analysis combines the merits of two major previous approaches to type-based analysis of resource usage -- linear-type-based and effect-based approaches.	computer file;memory cell (binary);memory management;usage analysis	Naoki Kobayashi	2003		10.1145/604174.604182	program analysis;computer science;artificial intelligence;programming language;operational semantics;algorithm	PL	-20.966406933057684	30.89109995404108	189593
168346335afddc1ac838a165661f9cff7a754fe6	automated specification inference in a combined domain via user-defined predicates		Discovering program specifications automatically for heap-manipulating programs is a challenging task due to the complexity of aliasing and mutability of data structures. This task is further complicated by an expressive domain that combines shape, numerical and bag information. In this paper, we propose a compositional analysis framework that would derive the summary for each method in the expressive abstract domain, independently from its callers. We propose a novel abstraction method with a bi-abduction technique in the combined domain to discover pre-/post-conditions that could not be automatically inferred before. The analysis does not only infer memory safety properties, but also finds relationships between pure and shape domains towards full functional correctness of programs. A prototype of the framework has been implemented and initial experiments have shown that our approach can discover interesting properties for non-trivial programs.		Shengchao Qin;Guanhua He;Wei-Ngan Chin;Florin Craciun;Mengda He;Zhong Ming	2017	Sci. Comput. Program.	10.1016/j.scico.2017.05.007	theoretical computer science;programming language;predicate (grammar);memory safety;computer science;correctness;inference;aliasing;abstraction;data structure	Logic	-19.61297006787477	27.511580572549384	189740
41ec32552d2b3a3f2442a9e640f69e004c48e727	enforcer - efficient failure injection	no determinismo;essai programme;preuve programme;program proof;test cubierta;availability;disponibilidad;unit testing;metodo formal;methode formelle;simultaneidad informatica;program verification;injection faute;formal method;inyeccion falta;verificacion programa;concurrency;non determinism;program testing;non determinisme;test coverage;prueba programa;concurrent programs;coordinacion;verification programme;fault injection;simultaneite informatique;disponibilite;couverture test;coordination	Non-determinism of the thread schedule is a well-known prob lem in concurrent programming. However, other sources of non-d eterminism exist which cannot be controlled by an application, such as networ k availability. Testing a program with its communication resources being unavai l ble is difficult, as it requires a change on the host system, which has to be coordi nate with the test suite. Essentially, each interaction of the application wi th the environment can result in a failure. Only some of these failures can be tested . Our work identifies such potential failures and develops a strategy for testing all relevant outcomes of such actions. Our tool, Enforcer, combines the structure of unit tests, coverage information, and fault injection. By taking advantage of a u nit test infrastructure, performance can be improved by orders of magnitude compared to previous approaches. Our tool has been tested on several real-world pro grams, where it found faults without requiring extra test code.	concurrent computing;exception handling;fastest;fault injection;grams;java;library (computing);nat (unit);run time (program lifecycle phase);scheduling (computing);test case;test suite;unit testing	Cyrille Artho;Armin Biere;Shinichi Honiden	2006		10.1007/11813040_28	availability;real-time computing;simulation;formal methods;concurrency;computer science;unit testing;code coverage;programming language;algorithm	SE	-22.675247190199993	32.131686993000535	189787
4aeb3cab624e040f07b0e3152320757fd44e1538	a constraint-based formalism for consistency in replicated systems	optimisation sous contrainte;modelizacion;constrained optimization;distributed system;atomicidad;replication;systeme reparti;shared memory;conmutatividad;optimistic replication;memoria compartida;par a par;vivacidad;distributed computing;semantics;rep;simultaneidad informatica;semantica;semantique;replicacion;weak consistency;vivacite;optimizacion con restriccion;modelisation;consistency model;concurrency;sistema repartido;syn;atomicity;poste a poste;atomicite;causalite;scheduling;replicated data;liveness;calculo repartido;commutativity;optim;peer to peer;common property;modeling;simultaneite informatique;calcul reparti;ordonnancement;reglamento;memoire partagee;commutativite;causality;causalidad	We present a formalism for modeling replication in a distributed system with concurrent users sharing information. It is based on actions, which represent operations requested by independent users, and constraints, representing scheduling relations between actions. The formalism encompasses semantics of shared data, such as commutativity or conflict between actions, and user intents such as causal dependence or atomicity. It enables us to reason about the consistency properties of a replication protocol or of classes of protocols. It supports weak consistency (optimistic protocols) as well as the stronger pessimistic protocols. Our approach clarifies the requirements and assumptions common to all replication systems. We are able to prove a number of common properties. For instance consistency properties that appear different operationally are proved equivalent under suitable liveness assumptions. The formalism enables us to design a new, generalised peer-to-peer consistency protocol.	algorithm;atomicity (database systems);causality;distributed computing;eventual consistency;formal system;liveness;peer-to-peer;requirement;scheduling (computing);semantics (computer science);weak consistency	Marc Shapiro;Karthikeyan Bhargavan;Nishith Krishna	2004		10.1007/11516798_24	shared memory;weak consistency;constrained optimization;replication;systems modeling;causality;concurrency;computer science;consistency model;release consistency;database;mathematics;distributed computing;semantics;causal consistency;eventual consistency;programming language;commutative property;scheduling;sequential consistency;atomicity;algorithm;local consistency;liveness;strong consistency	OS	-25.123261441371813	32.28092871293928	190561
ab7a5c5994f3d78fb8edac2ae608336b1ed92764	slicing concurrent java programs using indus and kaveri	lenguaje programacion;outil logiciel;essai programme;front end;interfase usuario;software tool;preuve programme;partition method;program proof;tranchage;object oriented language;indus;langage c;programming language;tool support;java programming;maintenance;user interface;program dependences;concurrent java;program comprehension;program transformation;simultaneidad informatica;langage java;concurrent program;program verification;transformation programme;software engineering;analisis programa;software architecture;verificacion programa;slicing;c language;transformacion programa;concurrency;development environment;methode partition;programa puesta a punto;program testing;programa aplicacion;application program;object oriented;programme application;programa competidor;prueba programa;chapeado;mantenimiento;architecture analysis;langage programmation;industrial application;oriente objet;lenguaje java;interface utilisateur;metodo particion;program analysis;analyse programme;program slicing;verification programme;herramienta software;kaveri;simultaneite informatique;orientado objeto;programme debogage;architecture logiciel;debugging program;lenguaje c;programme concurrent;java language	Program slicing is a program analysis and transformation technique that has been successfully used in a wide range of applications including program comprehension, debugging, maintenance, testing, and verification. However, there are only few fully featured implementations of program slicing that are available for industrial applications or academic research. In particular, very little tool support exists for slicing programs written in modern object-oriented languages such as Java, C#, or C++. In this paper, we present Indus—a robust framework for analyzing and slicing concurrent Java programs, and Kaveri—a feature-rich Eclipse-based GUI front end for Indus slicing. For Indus, we describe the underlying tool architecture, analysis components, and program dependence capabilities required for slicing. In addition, we present a collection of advanced features useful for effective slicing of Java programs including calling-context sensitive slicing, scoped slicing, control slicing, and chopping. For Kaveri, we discuss the design goals and basic capabilities of the graphical facilities integrated into a Java development environment to present the slicing information. This paper is an extended version of a tool demonstration paper presented at the International Conference on Fundamental Aspects of Software Engineering (FASE 2005). Thus, the paper highlights tool capabilities and engineering issues and refers the reader to other papers for technical details.	c++;debugging;eclipse;graphical user interface;java;program analysis;program comprehension;program slicing;software engineering;software feature	Venkatesh Prasad Ranganath;John Hatcliff	2007	International Journal on Software Tools for Technology Transfer	10.1007/s10009-007-0043-0	program slicing;real-time computing;computer science;operating system;programming language;object-oriented programming	SE	-24.76385567794303	30.184895414702535	190706
1936d49c95cf6454c159262da89851cdf5f9588d	from event-b specifications to programs for distributed algorithms	local computations;distributed algorithms;formal specification;distributed algorithms java computational modeling context synchronization syntactics program processors;event b;formal methods;theorem proving;program testing;visidia;distributed systems formal methods event b distributed algorithms local computations visidia;distributed systems;local computations visidia;event b specifications program experimentation program testing program visualization classical distributed computing systems java code b2visidia formal proofs distributed algorithms;program visualisation;theorem proving distributed algorithms formal specification java program testing program visualisation;java	Formal proofs of distributed algorithms are long, hard and tedious. We propose a general approach, based on the formal method Event-B, to automatically generate correct programs of distributed algorithms. Our approach is implemented with a translation tool, called B2Visidia, that generates Java code from an Event-B specification related to distributed algorithms. The resulting code can be run on classical distributed computing systems. To execute the induced programs, we use a tool called Visidia that can be used for experimenting, testing and visualizing programs of distributed algorithms.	b-method;compiler;distributed algorithm;distributed computing;experiment;formal methods;formal specification;java applet	Mohamed Tounsi;Mohamed Mosbah;Dominique Méry	2013	2013 Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises	10.1109/WETICE.2013.44	distributed algorithm;formal methods;computer science;theoretical computer science;formal specification;distributed computing;automated theorem proving;distributed design patterns;programming language;java;code mobility	SE	-26.308534964671146	31.462442047092974	191220
a500318df652be8be82eb7f2a58dd6d9fb5c7684	algorithmic debugging for lazy functional languages	elektroteknik och elektronik;electrical engineering electronic engineering information engineering;functional language	Lazy functional languages are declarative and allow the programmer to write programs where operational issues such as the evaluation order are left implicit. It is desirable to maintain a declarative view also during debugging so as to avoid burdening the programmer with operational details, for example concerning the actual evaluation order which tends to be difficult to follow. Conventional debugging techniques focus on the operational behaviour of a program and thus do not constitute a suitable foundation for a general-purpose debugger for lazy functional languages. Yet, the only readily available, general-purpose debugging tools for this class of languages are simple, operational tracers. This thesis presents a technique for debugging lazy functional programs declaratively and an efficient implementation of a declarative debugger for a large subset of Haskell. As far as we know, this is the first implementation of such a debugger which is sufficiently efficient to be useful in practice. Our approach is to construct a declarative trace which hides the operational details, and then use this as the input to a declarative (in our case algorithmic) debugger. The main contributions of this thesis are: A basis for declarative debugging of lazy functional programs is developed in the form of a trace which hides operational details. We call this kind of trace the Evaluation Dependence Tree (EDT). We show how to construct EDTs efficiently in the context of implementations of lazy functional languages based on graph reduction. Our implementation shows that the time penalty for tracing is modest, and that the space cost can be kept below a user definable limit by storing one portion of the EDT at a time. Techniques for reducing the size of the EDT are developed based on declaring modules to be trusted and designating certain functions as starting-points for tracing. We show how to support source-level debugging within our framework. A large subset of Haskell is handled, including list comprehensions. Language implementations are discussed from a debugging perspective, in particular what kind of support a debugger needs from the compiler and the run-time system. We present a working reference implementation consisting of a compiler for a large subset of Haskell and an algorithmic debugger. The compiler generates fairly good code, also when a program is compiled for debugging, and the resource consumption during debugging is modest. The system thus demonstrates the feasibility of our approach	debugging;lazy evaluation	Henrik Nilsson;Peter Fritzson	1994	J. Funct. Program.	10.1017/S095679680000109X	parallel computing;real-time computing;computer science;algorithmic program debugging;programming language;functional programming	PL	-23.637726670078862	26.476530354592924	191694
2465199eec1467d2beb2af594bdb3c17325fef57	verification of parameterized bus arbitration protocol	concepcion sistema;sistema informatico;computer system;formal verification;model checking;system design;distributed parameter system;systeme parametre reparti;sistema parametro repartido;verification formelle;systeme informatique;systeme parallele;parallel system;conception systeme;automobile industry;sistema paralelo	Model Checking is well established as a veriication technique for nite-state systems. Several important types of systems, such as protocols parameterized by the number of processes, are however inherently innnite-state, hence Model Checking cannot be applied directly to determine correctness of the system. We present here a case study on the veriication of such a parameterized protocol, the SAE-J1850 data transfer procotol. This is an standard in the automobile industry, where it is used to transmit data between various sensors and micro-controllers in an automobile. The protocol communicates data over a single-wire bus, and provides on-they arbitration between competing transmissions. Our veriication eeort is interesting from many aspects : it proves correctness for arbitrary instances, is largely automated, and uses abstraction in an essential way. The abstractions used are exact, in the sense that a property is true of the parameterized protocol ii it is true of the nite-state abstraction.	correctness (computer science);model checking;sae j1939;selective area epitaxy;sensor	E. Allen Emerson;Kedar S. Namjoshi	1998		10.1007/BFb0028766	model checking;embedded system;real-time computing;formal verification;computer science;distributed parameter system;programming language;algorithm;systems design	Logic	-24.19383383550978	31.726364601641784	191917
3bf3854b28698cfbb99ca4cb88a48f1635c5e001	flexjava: language support for safe and modular approximate programming	modular approximate programming;language design	Energy efficiency is a primary constraint in modern systems. Approximate computing is a promising approach that trades quality of result for gains in efficiency and performance. State- of-the-art approximate programming models require extensive manual annotations on program data and operations to guarantee safe execution of approximate programs. The need for extensive manual annotations hinders the practical use of approximation techniques. This paper describes FlexJava, a small set of language extensions, that significantly reduces the annotation effort, paving the way for practical approximate programming. These extensions enable programmers to annotate approximation-tolerant method outputs. The FlexJava compiler, which is equipped with an approximation safety analysis, automatically infers the operations and data that affect these outputs and selectively marks them approximable while giving safety guarantees. The automation and the language–compiler codesign relieve programmers from manually and explicitly an- notating data declarations or operations as safe to approximate. FlexJava is designed to support safety, modularity, generality, and scalability in software development. We have implemented FlexJava annotations as a Java library and we demonstrate its practicality using a wide range of Java applications and by con- ducting a user study. Compared to EnerJ, a recent approximate programming system, FlexJava provides the same energy savings with significant reduction (from 2× to 17×) in the number of annotations. In our user study, programmers spend 6× to 12× less time annotating programs using FlexJava than when using EnerJ.	admissible numbering;approximate computing;approximation algorithm;compiler;imperative programming;java;program analysis;programmer;scalability;software development;usability testing	Jongse Park;Hadi Esmaeilzadeh;Xin Zhang;Mayur Naik;William Harris	2015		10.1145/2786805.2786807	real-time computing;computer science;theoretical computer science;software engineering;programming language	SE	-19.42078791260097	27.16317035065458	191918
9fc7e56551c078b2b539c06a34c24024ab06ce55	representing layered monads	types;continuations;bytecode;type system;polymorphic recursion;subroutines;java	There has already been considerable research on constructing modular, monad-based specifications of computational effects (state, exceptions, nondeterminism, etc.) in programming languages. We present a simple framework in this tradition, based on a Church-style effect-typing system for an ML-like language. The semantics of this language is formally defined by a series of monadic translations, each one expanding away a layer of effects. Such a layered specification is easy to reason about, but its direct implementation (whether by parameterized interpretation or by actual translation) is often prohibitively inefficient.By exploiting deeper semantic properties of monads, however, it is also possible to derive a vastly more efficient implementation: we show that each layer of effects can be uniformly simulated by continuation-passing, and further that multiple such layers can themselves be simulated by a standard semantics for call/cc and mutable state. Thus, even multi-effect programs can be executed in Scheme or SML/NJ at full native speed, generalizing an earlier single-effect result. As an example, we show how a simple resumption-based semantics of concurrency allows us to directly simulate a shared-state program across all possible dynamic interleavings of execution threads.	call-with-current-continuation;concurrency (computer science);continuation-passing style;exception handling;immutable object;monad (functional programming);nondeterministic algorithm;programming language;scheme;simulation;standard ml of new jersey;state (computer science)	Andrzej Filinski	1999		10.1145/292540.292557	type system;computer science;theoretical computer science;subroutine;continuation;programming language;java;algorithm	PL	-21.77592129751299	28.495204044170723	192772
9137b46b85b90bcaa487bd66f0de8148746142a8	ownership, encapsulation and the disjointness of type and effect	encapsulation;lenguaje programacion;controle acces;systeme commande;sistema control;droit a la propriete;sistema experto;programming language;aliasing;ownership;encapsulacion;object oriented programming;expression evaluation;control system;langage programmation;ownership types;access control;systeme expert;type and effects systems;evaluation expression;evaluacion expresion;programmation orientee objet;type system;repliegue espectro;expert system;repliement spectre	Ownership types provide a statically enforceable notion of object-level encapsulation. We extend ownership types with computational effects to support reasoning about object-oriented programs. The ensuing system provides both access control and effects reporting. Based on this type system, we codify two formal systems for reasoning about aliasing and the disjointness of computational effects. The first can be used to prove that evaluation of two expressions will never lead to aliases, while the latter can be used to show the non-interference of two expressions.	access control;aliasing;computation;encapsulation (networking);formal system;interference (communication);non-interference (security);type system	Dave Clarke;Sophia Drossopoulou	2002		10.1145/582419.582447	aliasing;type system;encapsulation;computer science;control system;artificial intelligence;access control;programming language;object-oriented programming;computer security;expert system;algorithm	PL	-23.54043678929548	29.800554089957437	193133
0f30462958b56c285f37876e62f1b4543c2c3c58	an operational semantics for javascript	security properties;operational semantics	We define a small-step operational semantics for the ECMAScript standard language corresponding to JavaScript, as a basis for analyzing security properties of web applications and mashups. The semantics is based on the language standard and a number of experiments with different implementations and browsers. Some basic properties of the semantics are proved, including a soundness theorem and a characterization of the reachable portion of the heap.	authentication;bookmarklet;caja project;ecmascript;embedded system;experiment;javascript;mashup (web application hybrid);mathematical optimization;online advertising;operational semantics;rewriting;web application	Sergio Maffeis;John C. Mitchell;Ankur Taly	2008		10.1007/978-3-540-89330-1_22	action semantics;failure semantics;computer science;theoretical computer science;formal semantics;database;programming language;well-founded semantics;operational semantics;denotational semantics	PL	-20.86629890010116	28.017756295130585	193168
df67fb5efad8dad7bb3998f4869bd9b206eb08a0	data decision diagrams for petri net analysis	distributed system;system verification;queueing network;systeme reparti;decision tree;red petri;decision diagram;arbol decision;program verification;red cola espera;linear functionals;verificacion programa;sistema repartido;reseau file attente;estructura datos;structure donnee;petri nets;verification programme;petri net;data structure;arbre decision;reseau petri	This paper presents a new data structure, the Data Decision Diagrams, equipped with a mechanism allowing the definition of application-specific operators. This mechanism is based on combination of inductive linear functions offering a large expressiveness while alleviating for the user the burden of hard coding traversals in a shared data structure. We demonstrate the pertinence of our system through the implementation of a verification tool for various classes of Petri nets including self modifying and queuing nets. Topics. Petri Nets, Decision Diagram, System verification.	concurrent data structure;hard coding;influence diagram;linear function;petri net;relevance;self-modifying code	Jean-Michel Couvreur;Emmanuelle Encrenaz-Tiphène;Emmanuel Paviot-Adet;Denis Poitrenaud;Pierre-André Wacrenier	2002		10.1007/3-540-48068-4_8	real-time computing;data structure;computer science;database;distributed computing;programming language;petri net;algorithm	SE	-24.4920837911875	30.96874646127044	193320
a87b69a8aff0ca86a218391aac20048b976a9fb5	verified compilation of linearizable data structures: mechanizing rely guarantee for semantic refinement		Compiling concurrent and managed languages involves implementing sophisticated interactions between client code and the runtime system. An emblematic runtime service, whose implementation is particularly error-prone, is concurrent garbage collection. In a recent work [31], we implement an on-the-fly concurrent garbage collector, and formally prove its functional correctness in the Coq proof assistant. The garbage collector is implemented in a compiler intermediate representation featuring abstract concurrent data structures.  The present paper extends this work by considering the concrete implementation of some of these abstract concurrent data structures. We formalize, in the Coq proof assistant, a theorem establishing the semantic correctness of a compiling pass which translates abstract, atomic data structures into their concrete, fine-grained concurrent implementations.  At the crux of the proof lies a generic result establishing once and for all a simulation relation, starting from a carefully crafted rely-guarantee specification. Inspired by the work of Vafeiadis [28], implementations are annotated with linearization points. Semantically, this instrumentation reflects the behavior of abstract data structures.	abstract data type;cognitive dimensions of notations;compiler;coq (software);correctness (computer science);data structure;garbage collection (computer science);interaction;intermediate representation;linearizability;proof assistant;refinement (computing);runtime system;simulation	Yannick Zakowski;David Cachera;Delphine Demange;David Pichardie	2018		10.1145/3167132.3167333	linearizability;compiler;runtime system;concurrent data structure;theoretical computer science;correctness;garbage collection;data structure;computer science;proof assistant	PL	-21.200806018213687	28.382142762091224	193540
a036da8e6b46e5fc6368f69e8d2fd1561deb2259	responders: language support for interactive applications	sistema interactivo;herencia;lenguaje programacion;patron conception;interfase usuario;commande logique;compilateur;programming language;user interface;heritage;reutilizacion;xml language;gestion evenement;patron concepcion;langage java;compiler;control logico;reuse;sistema reactivo;systeme conversationnel;analyse syntaxique;metamodel;interactive application;metamodele;interactive system;analisis sintaxico;object oriented;metamodelo;syntactic analysis;event management;design pattern;programmation interactive;control flow;reactive system;langage programmation;systeme reactif;logic control;oriente objet;lenguaje java;interface utilisateur;gestion aconticimiento;inheritance;drag and drop;orientado objeto;langage xml;lenguaje xml;interactive programming;compilador;reutilisation;java language	A variety of application domains are interactive in nature: a primary task involves responding to external actions. In this paper, we introduce explicit programming language support for interactive programming, via the concept of a responder. Responders include a novel control construct that allows the interactive logic of an application to be naturally and modularly expressed. In contrast, the standard approaches to interactive programming, based on the event-driven style or the state design pattern, fragment this logic across multiple handlers or classes, with the control flow among fragments expressed only indirectly. We describe ResponderJ, an extension to Java supporting responders. A responder is simply a class with additional abilities, and these abilities interact naturally with the existing features of classes, including inheritance. We have implemented ResponderJ as an extension to the Polyglot compiler for Java. We illustrate ResponderJ’s utility in practice through two case studies: the implementation of a GUI supporting drag-and-drop functionality, and a re-implementation of the control logic of JDOM, a Java library for parsing and manipulating XML files.	application domain;compiler;control flow;drag and drop;event-driven architecture;graphical user interface;interactive programming;jdom;java;language construct;parsing;programming language;software design pattern;state management;xml	Brian Chin;Todd D. Millstein	2006		10.1007/11785477_17	metamodeling;compiler;xml;reactive system;computer science;parsing;reuse;database;design pattern;programming language;object-oriented programming;user interface;control flow;algorithm;logic control	PL	-26.071846764165006	27.475611239087392	193773
bdf95308517fbe9555f1fd8a5846155ea716fcf9	linear haskell: practical linearity in a higher-order polymorphic language		Linear type systems have a long and storied history, but not a clear path forward to integrate with existing languages such as OCaml or Haskell. In this paper, we study a linear type system designed with two crucial properties in mind: backwards-compatibility and code reuse across linear and non-linear users of a library. Only then can the benefits of linear types permeate conventional functional programming. Rather than bifurcate types into linear and non-linear counterparts, we instead attach linearity to function arrows. Linear functions can receive inputs from linearly-bound values, but can also operate over unrestricted, regular values.   To demonstrate the efficacy of our linear type system — both how easy it can be integrated in an existing language implementation and how streamlined it makes it to write programs with linear types — we implemented our type system in ghc, the leading Haskell compiler, and demonstrate two kinds of applications of linear types: mutable data with pure interfaces; and enforcing protocols in I/O-performing functions.		J Bernardy;Mathieu Boespflug;Ryan Newton;Simon L. Peyton Jones;Arnaud Spiwack	2017	PACMPL	10.1145/3158093	theoretical computer science;compiler;programming language;linear function;code reuse;polymorphism (computer science);haskell;functional programming;computer science;linearity;linear logic	PL	-22.549037374314686	25.797613143936086	193791
8aa7b967d753ff7ab2f65c8488b8089b0d0df49f	modular specification of hybrid systems in charon	lenguaje programacion;parallel composition;systeme commande;sistema control;time scale;architecture systeme;sistema hibrido;programming language;systeme discret;hierarchized structure;programmation modulaire;structure hierarchisee;programacion modular;continuous system;specification language;systeme continu;control system;sistema continuo;hybrid system;langage programmation;modular programming;arquitectura sistema;lenguaje especificacion;sistema discreto;system architecture;langage specification;estructura jerarquizada;discrete system;systeme hybride	We propose a language, called Charon, for modular specification of interacting hybrid systems. For hierarchical description of the system architecture, Charon supports building complex agents via the operations of instantiation, hiding, and parallel composition. For hierarchical description of the behavior of atomic components, Charon supports building complex modes via the operations of instantiation, scoping, and encapsulation. Features such as weak preemption, history retention, and externally defined Java functions, facilitate the description of complex discrete behavior. Continuous behavior can be specified using differential as well as algebraic constraints, and invariants restricting the flow spaces, all of which can be declared at various levels of the hierarchy. The modular structure of the language is not merely syntactic, but can be exploited during analysis. We illustrate this aspect by presenting a scheme for modular simulation in which each mode can be compiled solely based on the locally declared information to execute its discrete and continuous updates, and furthermore, submodes can integrate at a finer time scale than the enclosing modes.	compiler;encapsulation (networking);hybrid system;interaction;invariant (computer science);java;linear algebra;preemption (computing);scope (computer science);simulation;systems architecture;universal instantiation	Rajeev Alur;Radu Grosu;Yerang Hur;Vijay Kumar;Insup Lee	2000		10.1007/3-540-46430-1_5	simulation;specification language;computer science;control system;artificial intelligence;discrete system;modular programming;programming language;algorithm;hybrid system	AI	-25.333563703467778	31.29876044120138	194610
b9a60f92a20b272cf05954510f96975003d0f26d	c → haskell, or yet another interfacing tool		  This paper discusses a new method for typed functional languages to access libraries written in a lower-level language. More  specifically, it introduces an interfacing tool that eases Haskell access to C libraries. The tool obtains information about  the C data type definitions and function signatures by analysing the C header files of the library. It uses this information  to compute the missing details in the template of a Haskell module that implements a Haskell binding to the C library. Hooks  embedded in the binding file signal where, which, and how C objects are accessed from Haskell. The Haskell code in the binding  file determines Haskell type signatures and marshaling details. The approach is lightweight and does not require an extra  interface description language.    		Manuel M. T. Chakravarty	1999		10.1007/10722298_8	computer architecture;parallel computing;programming language	Logic	-24.65277563976833	27.37878057624802	196264
3d915989a51bd90f2b33841e8220fe56d74b088e	the scallina grammar - towards a scala extraction for coq		In response to the challenges associated with a Coq-based extraction of readable and traceable Scala code, the Scallina project defines a grammar delimiting a common subset of Gallina and Scala along with an optimized translation strategy for programs conforming to the aforementioned grammar. The Scallina translator shows how these contributions can be transferred into a working prototype. A typical application features a user implementing a functional program in Gallina, the core language of Coq, proving this program’s correctness with regards to its specification and making use of Scallina to synthesize readable Scala components.	coq (software);scala	Youssef El Bakouny;Dani Mezher	2018		10.1007/978-3-030-03044-5_7	programming language;functional programming;scala;code generation;correctness;core language;computer science;grammar	NLP	-21.67935901313445	26.843201408588715	196754
f7f12f2fb97ad5324688bd911d59a6e6625420d0	symbolic abstract contract synthesis in a rewriting framework		We propose an automated technique for inferring software contracts from programs that are written in a non-trivial fragment of C, called KernelC, that supports pointer-based structures and heap manipulation. Starting from the semantic definition of KernelC in the K framework, we enrich the symbolic execution facilities recently provided by K with novel capabilities for assertion synthesis that are based on abstract subsumption. Roughly speaking, we define an abstract symbolic technique that explains the execution of a (modifier) C function by using other (observer) routines in the same program. We implemented our technique in the automated tool KindSpec 2.0, which generates logical axioms that express preand post-condition assertions by defining the precise input/output behavior of the C routines.	ansi/iso c specification language;abstract interpretation;algebraic specification;algorithm;application programming interface;assertion (software development);automata theory;correctness (computer science);diwan-khane;formal specification;graph (discrete mathematics);input/output;java;linear algebra;model checking;modifier key;parameter (computer programming);pointer (computer programming);postcondition;programmer;prototype;random testing;rewriting;spec#;subsumption architecture;symbolic execution;test case;unit testing	María Alpuente;Daniel Pardo;Alicia Villanueva	2016		10.1007/978-3-319-63139-4_11	discrete mathematics;programming language;confluence;algorithm	SE	-19.386503459089308	26.208233669996535	197185
364b5e3a1e3003eaac90a481138fcee953bd4c64	static analysis of cloud elasticity		We propose a static analysis technique that computes upper bounds of virtual machine usages in a concurrent language with explicit acquire and release operations of virtual machines. In our language it is possible to delegate other (ad-hoc or third party) concurrent code to release virtual machines (by passing them as arguments of invocations). Our technique is modular and consists of (i) a type system associating programs with behavioural types that records relevant information for resource usage (creations, releases, and concurrent operations), (ii) a translation function that takes behavioural types and return cost equations, and (iii) an automatic off-the-shelf solver for the cost equations. A soundness proof of the type system establishes the correctness of our technique with respect to the cost equations. We have experimentally evaluated our technique using a cost analysis solver and we report some results. The experiments show that our analysis allows us to derive bounds for programs that are better than other techniques, such as those based on amortized analysis.	cloud computing;elasticity (data store);static program analysis	Abel Garcia;Cosimo Laneve;Michael Lienhardt	2017	Sci. Comput. Program.	10.1016/j.scico.2017.03.008	theoretical computer science;programming language;correctness;static analysis;real-time computing;computer science;virtual machine;concurrent computing;delegate;cloud computing;solver;amortized analysis	Logic	-20.585538987368068	29.688776297773433	198433
56f8993bdbbcfa612b8690ede8bea8a8cdaa7134	memory usage verification for oo programs	developpement logiciel;gestion memoire;theorie type;object oriented language;memory management;analyse statique;meetings and proceedings;securite;book chapter;storage management;expresion aritmetica;program verification;effet dimensionnel;analisis estatica;garbage collection;gestion memoria;verificacion programa;symbolic presburger arithmetic expressions;presburger arithmetic;object oriented;data structures;desarrollo logicial;size effect;indexation;type theory;arithmetic expression;estructura datos;software development;safety;expression arithmetique;oriente objet;structure donnee;aritmetico presburger;dependent types;efecto dimensional;static analysis;heap memory;verification programme;arithmetique presburger;seguridad;orientado objeto;data structure;type system	We present a new type system for an object-oriented (OO) lang uage that characterizes the sizes of data structures and the amou nt of heap memory required to successfully execute methods that operate on th ese data structures. Key components of this type system include type assertions t hat use symbolic Presburger arithmetic expressions to capture data structu re sizes, the effect of methods on the data structures that they manipulate, and the amount of memory that methods allocate and deallocate. For each method, we co ns rvatively capture the amount of memory required to execute the method as a funct ion of the sizes of the method’s inputs. The safety guarantee is that the meth od will never attempt to use more memory than its type expressions specify. We have impl mented a type checker to verify memory usages of OO programs. Our expe rience is that the type system can precisely and effectively capture memor y b unds for a wide range of programs.	data structure;memory management;presburger arithmetic;type system;yottabyte	Wei-Ngan Chin;Huu Hai Nguyen;Shengchao Qin;Martin C. Rinard	2005		10.1007/11547662_7	shared memory;distributed memory;data structure;computer science;theoretical computer science;operating system;database;overlay;flat memory model;programming language;object-oriented programming;algorithm;memory management	PL	-22.59626434574984	30.73608144133842	198712
76f34599f6e34c4c572daab627e4a00c0b41b32e	challenges in compiling coq	testing;concurrency;static analysis;dynamic analysis	"""The Coq proof assistant is increasingly used for constructing verified software, including everything from verified microkernels to verified databases. Programmers typically write code in Gallina (the core functional language of Coq) and construct proofs about those Gallina programs. Then, through a process of """"extraction"""", the Gallina code is translated to either OCaml, Haskell, or Scheme and compiled by a conventional compiler to produce machine code. Unfortunately, this translation often results in inefficient code, and it fails to take advantage of the dependent types and proofs. Furthermore, it's a bit embarrassing that the process is not formally verified.  Working with Andrew Appel's group at Princeton, we are trying to formalize as much of the process of extraction and compilation as we can, all within Coq. I will talk about both the opportunities this presents, as well as some of the key challenges, including the inability to preserve types through compilation, and the difficulty that axioms present."""	andrew appel;compiler;coq (software);database;dependent type;formal verification;functional programming;haskell;machine code;ocaml;programmer;proof assistant;scheme	J. Gregory Morrisett	2016		10.1145/2967973.2970379	concurrency;computer science;theoretical computer science;dynamic program analysis;software testing;programming language;static analysis;algorithm	PL	-20.486684560640082	27.263864924656893	199058
b8c772a31577d1f9979efd68cc6c3aa9df9b1cf7	a new approach to debugging optimized code	visual feedback	Debugging optimized code is a desirable capability not provided by most current debuggers. Users are forced to debug the unoptimized code when a bug occurs in the optimized version. Current research offers partial solutions for a small class of optimizations, but not a unified approach that handles a wide range of optimizations, such as the sophisticated optimizations performed by supercomputer compilers. The trend with current research is to make the effects of optimization transparent, i.e., provide the same behavior as that of the unoptimized program. We contend that this approach is neither totally feasible nor entirely desirable. Instead, we propose a new approach based on the premise that one should be able to debug the optimized code. This implies mapping the current state of execution back to the original source, tracking the location of variables, and mapping compiler-synthesized variables back to user-defined induction variables. To aid the user in understanding program behavior, various visual means are provided, e.g., different forms of highlighting and annotating of the source/assembly code. While this unavoidably requires the user to have a basic understanding of the optimizations performed, it permits the user to see what is actually happening, infer the optimizations performed, and detect bugs. An example illustrates the effectiveness of visual feedback. To support conventional debugger functionality for optimized code, the compiler must generate additional information. Current compiler-debugger interfaces (CDIs) were neither designed to handle this new information nor are they extensible in a straight forward manner. Therefore, a new CDI was designed that supports providing visual feedback and the debugging of optimized code. This paper specifies the details of a new CDI and relates each feature back to the debugger functionality it supports.	assembly language;compiler;debugger;debugging;don woods (programmer);liveness;lloyd's algorithm;mathematical optimization;powell's method;requirement;software bug;source tracking;stabs;supercomputer	Gary Brooks;Gilbert J. Hansen;Steve Simmons	1992		10.1145/143095.143108	real-time computing;computer science;theoretical computer science;operating system;programming language	PL	-24.449915930531628	29.09940654745462	199859
