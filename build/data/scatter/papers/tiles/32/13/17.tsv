id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7b2557a052f2bf9804486428d041a615fba22483	philosophy of the minizinc challenge	journal article;search;comparing solvers;modelling languages;artificial intelligence and image processing	MiniZinc arose as a response to the extended discussion at CP2006 of the need for a standard modelling language for CP. This is a challenging problem, and we believe MiniZinc makes a good attempt to handle the most obvious obstacle: there are hundreds of potential global constraints, most handled by few or no systems. A standard input language for solvers gives us the capability to compare different solvers. Hence, every year since 2008 we have run the MiniZinc Challenge comparing different solvers that support MiniZinc. In this report we discuss the philosophy behind the challenge, why we do it, how we do it, and why we do it that way.	constraint programming;modeling language;standard streams	Peter J. Stuckey;Ralph Becket;Julien Fischer	2010	Constraints	10.1007/s10601-010-9093-0	mathematical optimization;computer science;artificial intelligence;mathematics;programming language;algorithm	AI	-20.032099572471886	13.867109001606508	165269
4e1bce49cd17295f5a85d623832efb9278b195e2	towards computing revised models for fo theories	model generation;revision;computer network;domain knowledge;knowledge representation	In many real-life computational search problems, one is not only interested in finding a solution, but also in maintaining it under varying circumstances. E.g., in the area of network configuration, an initial configuration of a computer network needs to be obtained, as well as a new configuration when one of the machines in the network breaks down. Currently, most such revision problems are solved manually, or with highly specialized software. A recent declarative approach to solve (hard) computational search problems involving a lot of domain knowledge, is by finite model generation. Here, the domain knowledge is specified as a logic theory T , and models of T correspond to solutions of the problem. In this paper, we extend this approach to also solve revision problems. In particular, our method allows to use the same theory to describe the search problem and the revision problem, and applies techniques from current model generators to find revised solutions.	aggregate data;algorithm;computation;experiment;fo (complexity);heuristic (computer science);interactivity;overhead (computing);prototype;reachability;real life;search problem;theory (mathematical logic)	Johan Wittocx;Broes De Cat;Marc Denecker	2009		10.1007/978-3-642-20589-7_6	knowledge representation and reasoning;computer science;artificial intelligence;theoretical computer science;domain knowledge;algorithm	AI	-19.70652409762761	15.467001057532071	165728
36267634438e29b3eb05a20cb0785413d214edd7	incremental evaluation of rules and its relationship to parallelism	real time;computer science;deductive databases	Rule interpreters usually start with an initial database and perform the inference procedure in cycles, ending with a final database. In a real time environment> it is possible to receive updates to the initial database after the inference procedure has started or even after it has ended. We present an algorithm for incremental maintenance of the deductive database in the presence of such updates. Interestingly, the same algorithm is useful for parallel and distributed rule processing in the following sense. When the processors evaluating a program operate asynchronously, then they may have different views of the database. The procedure we present can be views.	algorithm;central processing unit;deductive database	Ouri Wolfson;Hasanat M. Dewan;Salvatore J. Stolfo;Yechiam Yemini	1991		10.1145/115790.115799	computer science;theoretical computer science;programming language;implicit parallelism;algorithm	DB	-25.01126794645877	14.466029212582525	166722
9c284fc02f52b10b3e0e69d9eea8b9bd1ec55693	a logic model for temporal authorization delegation with negation	conjunto independiente;answer sets;logica temporal;licence procedure;resolucion conflicto;securite;independent set;hierarchized structure;temporal logic;autorizacion;semantics;structure hierarchisee;autorisation;semantica;semantique;modelo logico;ensemble independant;resolution conflit;safety;logic model;conflict resolution;seguridad;logique temporelle;estructura jerarquizada;modele logique	In this paper, we present a logic based approach to temporal decentralized authorization administration that supports time constrained authorization delegations, both positive and negative authorizations, and implicit authorizations. A set of domain-independent rules are given to capture the features of temporal delegation correctness, temporal conflict resolution and temporal authorization propagation along the hierarchies of subjects, objects and access rights. The basic idea is to combine these general rules with a set of domain-specific rules defined by users to derive the authorizations holding at any time in the system. In addition, some important semantic properties including the unique answer set property are further investigated.	access control;answer set programming;authorization;correctness (computer science);disjunctive normal form;domain-specific language;expressive power (computer science);logic programming;negation as failure;non-monotonic logic;software propagation;stable model semantics	Chun Ruan;Vijay Varadharajan;Yan Zhang	2003		10.1007/10958513_24	data mining;mathematics;computer security;algorithm	AI	-21.85203439106055	11.777681264731818	167508
46b62f12903fff47286faae64ce30cfd67fd224b	an inductive learning perspective on automated generation of feature models from given product specifications		For explicit representation of commonality and variability of a product line, a feature model is mostly used. An open question is how a feature model can be inductively learned in an automated way from a limited number of given product specifications in terms of features.  We propose to address this problem through machine learning, more precisely inductive generalization from examples. However, no counter-examples are assumed to exist. Basically, a feature model needs to be complete with respect to all the given example specifications. First results indicate the feasibility of this approach, even for generating hierarchies, but many open challenges remain.	feature model;heart rate variability;inductive reasoning;machine learning;randomness;reverse engineering;test set	Hermann Kaindl;Stefan Kramer;Ralph Hoch	2018		10.1145/3233027.3233031	feature model;control engineering;product design specification;computer science;machine learning;hierarchy;artificial intelligence;learning theory	AI	-21.785167190512784	13.5121764485257	168628
a942eebae2457ed1e511a1018dfa5924b8c19b17	hidden conditioned homomorphism for xpath fragment containment	query language;base donnee;informatique mobile;lenguaje xpath;xml language;branching;interrogation base donnee;database;false negative;xpath language;interrogacion base datos;base dato;hombre;lenguaje interrogacion;homomorphism;enfant;inclusion expresion;inclusion expression;nino;langage expression chemin;ramificacion;human;homomorphisme;child;ramification;langage interrogation;homomorfismo;mobile computing;database query;langage xml;lenguaje xml;homme;query containment	As a query language for navigating XML trees and selecting a set of element nodes, XPath is ubiquitous in XML applications. One important issue of XPath queries is checking containment. In particular, we investigate a frequently used fragment of XPath that consists of node tests, the child axis (/), the descendant axis (//), branches ([]) and label wildcards (*). For special classes of pattern trees, the homomorphism algorithm returns false negatives. In order to address this problem, we propose two containment techniques, conditioned homomorphism and hidden conditioned homomorphism, and then present sound algorithms to check containment. The analytical result is given with an experiment.	xpath	Yuguo Liao;Jianhua Feng;Yong Zhang;Lizhu Zhou	2006		10.1007/11733836_32	homomorphism;xml;el niño;branching;computer science;theoretical computer science;data mining;database;ramification;programming language;mobile computing;algorithm;query language	DB	-25.82792786402103	11.74503147211416	169868
1660ecdd982910147c43e0bcff2415ff18884f2e	object-oriented reasoning about action and change	reasoning about action;technology;computer and information science;teknikvetenskap;natural sciences;engineering and technology;teknik och teknologier;object oriented;datavetenskap datalogi;computer science;modeling methodology	As the scope of logics of action and change continues to increase and powerful research tools are developed, it becomes possible to model larger and more complex scenarios. Unfortunately the scenarios become harder to read and diicult to modify and debug with increasing size and complexity. These problems have been overlooked in the action and change community due to the fact that only smaller toy problems are considered. Sound modeling methodology is as essential as the primitives of the modeling language. The object-oriented paradigm is one structur-ing mechanism that alleviates these problems and provides a systematic means of scenario construction. The topic of this paper is to demonstrate how many ideas from the object orientation paradigm can be used when reasoning about action and change, we show this by integrating the technique directly in an existing logic of action and change without any modiication to the underlying logical language or semantics.	modeling language;programming paradigm	Joakim Gustafsson	2001			computer science;knowledge management;artificial intelligence;machine learning;technology	AI	-22.54523241764395	14.26533520714453	170409
ab730c43b94200e082bfaf4992597b8cb47320ef	mathematical software – icms 2016		s of the Invited Talks Invited Plenary Speakers and Talks Invited Plenary Speakers Wolfram Decker University of Kaiserslautern, Germany Jack Dongarra University of Tennessee, USA Vladimir Voevodsky IAS Princeton, USA Stephen Watt University of Waterloo, Canada Abstracts of Invited Plenary Talkss of Invited Plenary Talks 1. Current Challenges in the Development of Open Source Computer Algebra Software Wolfram Decker (University of Kaiserslautern) Computer algebra is facing new challenges as more and more of the abstract concepts of pure mathematics are made constructive, with interdisciplinary methods playing a significant role. On the mathematical side, while we wish to provide cutting-edge techniques for applications in various areas, the implementation of an advanced and more abstract computational machinery often depends on a long chain of more specialized algorithms and efficient data structures at various levels. On the software development side, for cross-border approaches to solving mathematical problems, the efficient interaction of systems specializing in different areas is indispensable. In this talk, I will report on ongoing collaboration between groups of developers of several well-established open source computer algebra system specializing in commutative algebra and algebraic geometry, group and representation theory, convex and polyhedral geometry, and number theory. The ultimate goal of this collaboration is to integrate the systems, together with other packages and libraries, into a next generation computer algebra system surpassing the combined mathematical capabilities of the underlying systems. 2. With Extreme Scale Computing the Rules Have Changed Jack Dongarra (University of Tennessee) In this talk we will look at the current state of high performance computing and look at the next stage of extreme computing. With extreme computing there will be fundamental changes in the character of floating point arithmetic and data movement. In this talk we will look at how extreme scale computing has caused algorithm and software developers to changed their way of thinking on how to implement and program certain applications. 3. UniMath a library of mathematics formalized in the univalent style Vladimir Voevodsky (IAS Princeton) The univalent style of formalization of mathematics in the type theories such as the ones used in Coq, Agda or Lean is based on the discovery in 2009 of a new class of models of such type theories. These “univalent models” led to the new intuition that resulted in the introduction into the type theory of the concept of h-level (homotopy level). This most important concept implies in particular that to obtain good intuitive behavior one should define propositions as types of h-level 1 and sets as types of h-level 2. Instead of syntactic Prop one then defines a type hProp(U) the type of types of h-level 1 in the universe U and the type hSet(U) the type of types of h-level 2 in U. With types of h-level 1 and 2 one can efficiently formalize all of the set-theoretic mathematics. With types of h-level 3 one can efficiently formalize mathematics at the level of categories etc. Univalent style allows to directly formalize constructive mathematics and to formalize classical mathematics by adding the excluded middle axiom for types of h-level 1 and the axiom of choice for types of h-level 2. UniMath is a growing library of constructive mathematics formalized in the univalent style using a small subset of Coq language. 4. Toward an International Mathematical Knowledge Base Stephen Watt (University of Waterloo) The notion of a comprehensive digital mathematics library has been a dream for some decades. More than in many other areas, results in mathematics have lasting value – once proven, always true. It is not uncommon for a research article to have primary references to work decades earlier. Another quality of mathematics is its precision; there is a clarity to mathematical definitions and results. This makes mathematics an ideal subject for mechanized treatment of knowledge. This talk shall outline the challenges and opportunities in transforming the complete mathematical literature into a knowledge base to be used by mathematicians and software systems alike. XVIII Invited Plenary Speakers and Talks	agda;algorithm;cpu cache;computation;computer algebra system;convex function;coq (software);curry–howard correspondence;data structure;ibm websphere extreme scale;international automated systems;internet authentication service;knowledge base;lean integration;library (computing);linear algebra;mathematical software;open-source software;polyhedron;set theory;software developer;software development;software system;supercomputer;type theory	Gert-Martin Greuel;Thorsten Koch;Peter Paule;Andrew Sommese	2016		10.1007/978-3-319-42432-3	computational science;mathematical software;computer science	PL	-20.00723523807576	18.06299752921588	171511
58eb84e7878de6fa9b68a25d23162940b1be8a9f	computerizing mathematical text with mathlang	proof assistant;proof assistants;software tool;logical framework;noun;theorem provers;proof checkers;logical foundations of mathematics;mathematical typesetting;set theory;theorem prover;category theory;mathematical vernacular;undergraduate student;natural language;type theory;mathematical knowledge management;type system	Mathematical texts can be computerised in many ways that capture differing amounts of the mathematical meaning. At one end, there is document imaging, which captures the arrangement of black marks on paper, while at the other end there are proof assistants (e.g., Mizar, Isabelle, Coq, etc.), which capture the full mathematical meaning and have proofs expressed in a formal foundation of mathematics. In between, there are computer typesetting systems (e.g., LTEX and Presentation MathML) and semantically oriented systems (e.g., Content MathML, OpenMath, OMDoc, etc.). The MathLang project was initiated in 2000 by Fairouz Kamareddine and Joe Wells with the aim of developing an approach for computerising mathematical texts which is flexible enough to connect the different approaches to computerisation, which allows various degrees of formalisation, and which is compatible with different logical frameworks (e.g., set theory, category theory, type theory, etc.) and proof systems. The approach is embodied in a computer representation, which we call MathLang, and associated software tools, which are being developed by ongoing work. Four Ph.D. students (Manuel Maarek (2002/2007), Krzysztof Retel (since 2004), Robert Lamar (since 2006)), and Christoph Zengler (since 2008) and over a dozen master’s degree and undergraduate students have worked on MathLang. The project’s progress and design choices are driven by the needs for computerising representative mathematical texts chosen from various branches of mathematics. Currently, MathLang supports entry of mathematical text either in an XML format or using the TEXMACS editor. Methods are provided for adding, checking, and displaying various information aspects. One aspect is a kind of weak type system that assigns categories (term, statement, noun (class), adjective (class modifier), etc.) to parts of the text, deals with binding names to meanings, and checks that a kind of grammatical sense is maintained. Another aspect allows weaving together mathematical meaning and visual presentation and can associate natural language text with its mathematical meaning. Another aspect allows identifying chunks of text, marking their roles (theorem, definition, explanation, example, section, etc.), and indicating relationships between the chunks (A uses B, A contradicts B, A follows from B, etc.). Software tool support can use this aspect to check and explain the overall logical structure of a text. Further aspects are being designed to allow adding additional formality to a text such as proof structure and details of how a human-readable proof is encoded into a fully formalised version (previously we used Mizar and Isabelle but here, for the first time we develop the MathLang formalisation into Coq). [24] surveyed the status of the MathLang project up to November 2007. This paper 1 L. J. of the IGPL, Vol. 0 No. 0, pp. 1–51 0000 c © Oxford University Press 2 Computerising Mathematical Text with MathLang picks on from that survey, fills in a number of formalisation and implementation gaps and creates a formalisation path via MathLang into Coq. We show for the first time how the DRa information can be used to automatically generate proof skeletons for different theorem provers, we formalise and implement the textual order of a text and explain how it can be derived from the original text. Our proposed generic algorithm (for generating the proof skeleton which depends on the original mathematical text and the desired theorem prover), is highly configurable and caters for arbitrary theorem provers. This generic algorithm as well as all the new algorithms and concepts we present here, are implemented in our software tool. Furthermore, we give hints for the development of an algorithm which is able to convert parts of a CGa annotated text automatically into the syntax of a special theorem prover. To test our approach we create the complete path of encoding in and formalising through MathLang, for the first chapter of Landau’s book ”Grundlagen der Analysis”. We started with the plain text document, annotated categories and mathematical roles to chunks of text and automatically generated in MathLang, a Coq and a Mizar proof skeleton for the chapter. We used our hints to convert parts of the CGa annotated text of Landau’s first chapter into Coq. Both the Coq proof skeleton and the converted CGa parts into Coq, simplified the process of the full formalisation of the first chapter of Landau’s book in Coq. This full formalisation in Coq illustrates the use of MathLang in different theorem proving settings. Previously we have only used Mizar and Isabelle. With the work in this paper, we show that MathLang is able to handle different formal foundations. Since November 2007 [24], significant progress has been made on MathLang which is reported here for the first time.	algorithm;automated theorem proving;category theory;coq (software);generic programming;human-readable medium;isabelle;item unique identification;mizar;modifier key;natural language;omdoc;programming tool;proof assistant;set theory;type system;type theory;xml;year 10,000 problem	Fairouz Kamareddine;Joe B. Wells	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.03.063	noun;logical framework;type system;computer science;mathematical knowledge management;mathematics;automated theorem proving;natural language;proof assistant;programming language;type theory;algorithm;category theory;set theory	PL	-26.05289014284401	18.214539251332713	172703
06ed00000cd82fdf8bdb01fda0d55d906fb430e3	interleaved semantic interpretation in environment-based parsing	interleaved semantic interpretation;application environment;input sentence;structural ambiguity;full set;environment model;overall algorithm;polynomial time;rival constituent;logical operator;environment-based parsing;polynomial-time parsing algorithm;semantic interpretation	This paper extends a polynomial-time parsing algorithm that resolves structural ambiguity in input sentences by calculating and comparing the denotations of rival constituents, given some model of the application environment (Schuler, 2001). The algorithm is extended to incorporate a full set of logical operators, including quanti ers and conjunctions, into this calculation without increasing the complexity of the overall algorithm beyond polynomial time, both in terms of the length of the input and the number of entities in the environment model.	algorithm;best, worst and average case;entity;logical connective;parsing;polynomial;semantic interpretation;time complexity;user interface;worst-case complexity	William Schuler	2002	CoRR		natural language processing;time complexity;semantic interpretation;computer science;theoretical computer science;machine learning;programming language;algorithm	NLP	-21.192897729848085	15.360604075534782	174253
ddc6cd3ee83335f69b8b89f0627b5bad85d1b55a	dlab: a declarative language bias formalism	declarative languages;inductive learning;concept learning	We describe the principles and functionalities of Dlab (Declarative LAnguage Bias). Dlab can be used in inductive learning systems to deene syntactically and traverse eeciently nite subspaces of rst order clausal logic, be it a set of propositional formulae, association rules, Horn clauses, or full clauses. A Prolog implementation of Dlab is available by ftp access.	association rule learning;declarative programming;horn clause;prolog;semantics (computer science);traverse	Luc Dehaspe;Luc De Raedt	1996		10.1007/3-540-61286-6_185	natural language processing;concept learning;computer science;machine learning;programming language	DB	-20.317096862554894	12.212740152955277	174359
068bdf2b1829a7da7250d2b824d3d2c27ed640b5	finding suitable programs: semantic search with incomplete and lightweight specifications	lattices semantics encoding concrete syntactics mashups;smt solvers;formal specification;mashups;lattices;programming language;search engines;information retrieval;semantics;search engines formal specification information retrieval programming languages;code reuse;syntactics;levels of abstraction;constraints semantic search program composition code reuse smt solvers;semantic search;programming languages suitable program finding semantic search incomplete specifications lightweight specifications suitable code finding code search literature syntactic approach semantic approach smt solver program repository matching program searching program encodings abstraction level yahoo pipes mashup language;program composition;encoding;constraints;programming languages;concrete	Finding suitable code for reuse is a common task for programmers. Two general approaches dominate the code search literature: syntactic and semantic. While queries for syntactic search are easy to compose, the results are often vague or irrelevant. On the other hand, a semantic search may return relevant results, but current techniques require developers to write specifications by hand, are costly as potentially matching code need to be executed to verify congruence with the specifications, or only return exact matches. In this work, we propose an approach for semantic search in which programmers specify lightweight, incomplete specifications and an SMT solver automatically identifies programs from a repository, encoded as constraints, that match the specifications. The repository of programs is automatically encoded offline so the search for matching programs is efficient. The program encodings cover various levels of abstraction to enable partial matches when no or few exact matches exists. We instantiate this approach on a subset of the Yahoo! Pipes mashup language, and plan to extend our techniques to more traditional programming languages as the research progresses.	mashup (web application hybrid);online and offline;principle of abstraction;programmer;programming language;relevance;semantic search;solver;vagueness;zeller's congruence	Kathryn T. Stolee	2012	2012 34th International Conference on Software Engineering (ICSE)	10.1109/ICSE.2012.6227034	concrete;semantic search;computer science;theoretical computer science;operating system;lattice;formal specification;database;semantics;programming language;encoding;mashup	SE	-25.996859669767023	15.286622088407617	174388
547380afeb6f6dcd538be653584a079c7934e4e6	an agent-based model for the development of intelligent mobile services	design model;design and development;belief desire intention;pervasive computing;agent based model;conceptual model;mobile computer;proof of concept;dynamic environment;quality of information;mobile service;autonomous agent;adaptive behaviour;next generation	Actions. The idea behind abstract actions is similar to a procedure call in imperative programming languages. An abstract action 〈abstractaction〉 is an expression of the form 〈atom〉, i.e. a first order expression in which the predicate starts with a lowercase letter. The execution of an abstract action passes parameters from the plan in which it occurs to another plan associated with it by a PC-rule. Programming constructs implement these actions in the prototype. 142 CHAPTER 5. PROTOTYPE Test Action. The test actions allow the program to check whether the agent has certain beliefs and goals. Expressions of the form 〈test〉 consistof either a belief or a goal query expression, and can be used inside a plan to (i) instantiate variables in subsequent actions (if the test succeeds), or (ii) block the plan’s execution if the test fails. A belief query has the form B(φ), where φ consists of literals composed by conjunction and disjunction operators. A PROLOG query to the belief base implements this functionality, which returns the substitution set for the free variables. A goal query has the form G(φ), where φ consists of atoms composed by conjunction and disjunction operators, and is used to query to individual goals in the intention base. Goal Dynamics Action. The action 〈adoptgoal〉, to adopt a goal, has two different forms: • adopta(φ) to add a goal φ to the beginning of the goal base, i.e. with higher priority in the execution queue, and; • adoptz(φ) to goal φ to the end of the goal base, i.e. with lower priority in the execution queue. In addition, the action 〈dropgoal〉, to drop an existing goal, has three possible forms: • dropGoal(φ) to drop all goals that are a logical subgoal of φ; • dropSubGoal(φ) to drop all goals that have φ as a logical subgoal, and; • dropExactGoal(φ) to drop only the goal φ. The application implements these actions via programming constructs. The action to “adopt a goal” is actually implemented as “adopt a desire”. Similarly, the action to “drop a goal” is implemented as “drop a desire”. Hence, the parameter φ is replaced by the object Desire, described before. This allows external modules to either assert or retract desires at run-time.	agent-based model;atom;command queue;free variables and bound variables;imperative programming;programming language;prolog;prototype;subroutine;test case	Fernando Luiz Koch	2009			simulation;systems engineering;engineering;knowledge management;mobile business development	AI	-24.499094197457048	17.412660360883265	174468
499b57645dd9afbb84c2d8b1e535c8cd2cae55c7	intelligent backtracking in plan-based deduction	graph search;automated deduction;mechanical theorenm proving automated deduction backtracking;probability density function;search strategy;data mining;mechanical theorenm proving;logic programming;calculus;graphical representation;backtracking;joining processes;councils;computer science algorithm design and analysis logic programming councils;computer science;article;algorithm design and analysis;labeling;reactive power	This paper develops a method of mechanical deduction based on a graphical representation of the structure of proofs. Attempts to find a refutation(s) are recorded in the form of plans, corresponding to portions of an AND/OR graph search space and representing a purely deductive structure of derivation. This method can be applied to any initial base (set of nonnecessarily Horn clauses). Unlike the exhaustive (blind) backtracking which treats all the goals deduced in the course of a proof as equally probable sources of failure, his approach detects the exact source of failure. Only a small fragment of the solution space is kept on disk as a collection of pairs, each of which consists of a plan and a graph of constraints. The search strategy and the method of nonredundant processing of individual pairs which leads to a solution (if it exists) is presented. This approach is compared¿on a special case¿with a blind backtracking algorithm for which an exponential improvement is demonstrated. Some important implementation problems are discussed, and toplevel design of a mechanical deduction system implementing our algorithm is presented. It is proven that the algorithm is complete in the following sense: if for a given base a resolution refutation exists, then this refutation is found by the algorithm.	arcs (computing);automated theorem proving;backtracking;best, worst and average case;brute-force search;concurrency (computer science);concurrent algorithm;concurrent computing;conflict (psychology);data structure;denotational semantics;distributed control system;experiment;feasible region;formal system;graph - visual representation;graph traversal;horn clause;mike lesser;natural deduction;overhead (computing);pascal;probability;prolog;redundancy (engineering);requirement;solutions;exponential	Stan Matwin;Tomasz Pietrzykowski	1985	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.1985.4767724	algorithm design;labeling theory;probability density function;computer science;theoretical computer science;machine learning;mathematics;geometry;ac power;logic programming;algorithm;statistics;backtracking	AI	-19.518342649665936	16.771913726777544	175520
0249769ce212ede6f820977bd8528dbf78f315a3	a constraint programming based approach to detect ontology inconsistencies	constraint programming	This paper proposes a constraint programming based approach to handle ontologies consistency, and more precisely user-defined consistencies. In practice, ontologies consistency is not still well handled in the current software environments. This is due to the algorithmic and language limitations of the tools developed to handle the consistency constraints. The idea of this paper is to tackle this problem by exploiting constraint programming which is proved to be efficient and provides various techniques to handle many types of constraints on different domains.	algorithm;bioinformatics;consistency model;constraint programming;cryptographic service provider;ontology (information science);plug-in (computing);protégé	Moussa Benaissa;Yahia Lebbah	2011	Int. Arab J. Inf. Technol.		constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;programming domain;reactive programming;computer science;theoretical computer science;data mining;inductive programming;constraint satisfaction problem;algorithm;local consistency	AI	-22.98894848994888	12.098832159198114	175636
420691ff4bc6928b4c1691a70bded69615f87efc	partial redundant modeling	global constraint;matrix model	In the context of previous work on redundant model ing, permutation problems and matrix modeling, we introduce the noti on of partial redundant modeling and categorical channeling constraints. To do so, we look at problems with categorical structure. Categorical channe li g constraints also introduce the more general notion of channeling constrai nts over global constraints in redundant models. This paper provides only the motivation for partial redundant modeling and channeling categorical constra i ts. Future work will include implementation and testing as well as checkin g for scalability and flexibility of the proposed technique.	null-terminated string;scalability	Tiziana Ligorio;Susan L. Epstein	2005		10.1007/11564751_94	mathematical optimization;computer science	NLP	-24.209028996278136	14.252812660284347	177175
ef5b4779a3fc1579cd7ff39b6d41c14adaf2506d	simple yet efficient improvements of sat based bounded model checking	concepcion asistida;model based reasoning;evaluation performance;computer aided design;diseno circuito;verificacion modelo;raisonnement base sur modele;performance evaluation;evaluacion prestacion;circuit design;metodo formal;methode formelle;verification modele;logique propositionnelle;program verification;constraint satisfaction;intelligence economique;formal method;bounded model checking;satisfaction contrainte;verificacion programa;model checking;propositional logic;conception assistee;competitive intelligence;conception circuit;satisfaccion restriccion;logica proposicional;sat solver;inteligencia economica;verification programme	In this paper, we show how proper benchmarking, which matches day-to-day use of formal methods, allows us to assess direct improvements for SAT use for formal methods. Proper uses of our benchmark allowed us to prove that previous results on tuning SAT solver for Bounded Model Checking (BMC) were overly optimistic and that a simpler algorithm was in fact more efficient.	algorithm;benchmark (computing);boolean satisfiability problem;formal methods;model checking;solver	Emmanuel Zarpas	2004		10.1007/978-3-540-30494-4_13	model checking;simulation;formal methods;competitive intelligence;constraint satisfaction;computer science;artificial intelligence;model-based reasoning;computer aided design;circuit design;propositional calculus;boolean satisfiability problem;programming language;algorithm	Logic	-19.512327387464964	17.692940178848726	177186
5319dcb276da718aff1f2091f2abe8995a2a8614	nomore : a system for computing preferred answer sets	integrated approach;programacion conjunto respuesta;representacion conocimientos;answer sets;answer set programming;circonscription;dependence graph;logical programming;circumscription;programmation logique;programmation par ensemble reponse;representation connaissance;knowledge representation;programacion logica;circonscripcion	The integration of preferences into Answer Set Programming (ASP) constitutes an important practical device for distinguishing certain preferred answer sets from non-preferred ones. Up to now, the preference semantics we are considering in this system description were incorporated into answer set solvers either by meta-interpretation [3] or by pre-compilation front-ends [2]; therefore, such kinds of preferences were never integrated into the core existing ASP solvers. Unlike this, the nomore system pursues an integrative approach to preference handling. Its theoretical background is described in [6]. The system itself is an early branch of the nomore++ ASP solver [8]. The approach relies on rule dependency graphs and computes answer sets by coloring this graph by following a certain strategy. It integrates preferences as an additional type of edges among nodes representing rules. The idea is to start from an uncolored graph and to employ specific operators that turn a partially colored graph gradually into a totally colored one that represents a preferred answer set. Apart from describing the nomore system, we focus in our experimental section on the question how an integrative approach compares to a compilationbased approach. To this end, we use the plp system [9] in connection with nomore as well as smodels [11] as its respective back-end ASP solvers. (Ab)using nomore as a standard ASP solver allows us to obtain comparable results, although its performance is much more inferior than that of smodels as well as the full-fledged nomore++ system [8].	answer set programming;compiler;graph coloring;rule 184;solver;stable model semantics;turned a	Susanne Grell;Kathrin Konczak;Torsten Schaub	2005		10.1007/11546207_34	knowledge representation and reasoning;computer science;artificial intelligence;answer set programming;mathematics;circumscription;algorithm	AI	-19.828478096695964	12.417896106990117	177252
71bb49d40dcaae7baae0b955539d2d83b08bf2b1	verification of mathematical formulae based on a combination of context-free grammar and tree grammar	theoretical computer science;context free;computer science all;document database;context free grammar;biochemistry genetics and molecular biology all;formal grammar;ground truth	This paper proposes the use of a formal grammar for the verification of mathematical formulae for a practical mathematical OCR system. Like a C compiler detecting syntax errors in a source file, we want to have a verification mechanism to find errors in the output of mathematical OCR. Linear monadic context-free tree grammar (LM-CFTG) was employed as a formal framework to define “well-formed” mathematical formulae. For the purpose of practical evaluation, a verification system for mathematical OCR was developed, and the effectiveness of the system was demonstrated by using the ground-truthed mathematical document database INFTY CDB-1.	compiler;context-free grammar;context-free language;document-oriented database;formal grammar;optical character recognition;sensor;syntax error;well-formed formula	Akio Fujiyoshi;Masakazu Suzuki;Seiichi Uchida	2008		10.1007/978-3-540-85110-3_35	grammar systems theory;natural language processing;synchronous context-free grammar;link grammar;ground truth;operator-precedence grammar;regular grammar;computer science;artificial intelligence;affix grammar;theoretical computer science;regular tree grammar;stochastic grammar;extended affix grammar;formal grammar;context-free grammar;programming language;attribute grammar;unrestricted grammar;adaptive grammar;grammar-based code;mildly context-sensitive grammar formalism;algorithm;lexical grammar	PL	-20.80340225811828	17.940797796168297	177275
a8d52b049d3b1dae04f7a7720745c6f97f7b0073	diagnosability study of technological systems	model based diagnosis;faults modeling;diagnosability;fault model	This paper describes an approach to study the diagnosability of technological systems, by characterizing their observable behaviors. Due to the interaction between many components, faults can occur in a technological system and cause hard damages not only to its integrity but also to its environment. Though a diagnosis system is a suitable solution to detect and identify faults, it is first important to ensure the diagnosability of the system: will the diagnosis system always be able to detect and identify any fault, without any ambiguity, when it occurs? In this paper, we present an approach to identify and integrate faults in a model of a technological system. Then we use these models for the diagnosability study of faults by characterizing their observable behaviors.	embedded system;microsoft windows;observable;semantics (computer science);temporal logic	Michel Batteux;Philippe Dague;Nicolas Rapin;Philippe Fiani	2011		10.1007/978-3-642-21822-4_20	real-time computing;fault model	SE	-24.14536016512282	15.108439504904801	177345
b1778699033a39519c9246b64fa5a1cfb23f646e	grammatical verification for mathematical formula recognition based on context-free tree grammar	grammatical analysis;context free;computational mathematics;mathematical ocr;document database;computational theory and mathematics;formal grammar;formula recognition;ground truth;applied mathematics;structural analysis;context free tree grammar;structure analysis	This paper proposes the use of a formal grammar for the verification of mathematical formulae for a practical mathematical OCR system. Like a C compiler detecting syntax errors in a source file, we want to have a verification mechanism to find errors in the output of mathematical OCR. A linear monadic context-free tree grammar (LM-CFTG) is employed as a formal framework to define “well-formed” mathematical formulae. A cubic time parsing algorithm for LM-CFTGs is presented. For the purpose of practical evaluation, a verification system for mathematical OCR is developed, and the effectiveness of the system is demonstrated by using the ground-truthed mathematical document database InftyCDB-1 and a misrecognition database newly constructed for this study.		Akio Fujiyoshi;Masakazu Suzuki;Seiichi Uchida	2010	Mathematics in Computer Science	10.1007/s11786-010-0023-8	grammar systems theory;natural language processing;regular grammar;computer science;affix grammar;regular tree grammar;mathematics;structural analysis;programming language;attribute grammar;grammar-based code;algorithm	Logic	-20.71698087622411	17.95412376149954	178113
498623cf59713dc26f00e513a0b0c15eb385adbe	planning using actions with control parameters		Although PDDL is an expressive modelling language, a significant limitation is imposed on the structure of actions: the parameters of actions are restricted to values from finite (in fact, explicitly enumerated) domains. There is one exception to this, introduced in PDDL2.1, which is that durative actions may have durations that are chosen (possibly subject to explicit constraints in the action models) by the planner. A motivation for this limitation is that it ensures that the set of grounded actions is finite and, ignoring duration, the branching factor of action choices at a state is therefore finite. Although the duration parameter can make this choice infinite, very few planners support this possibility, but restrict themselves to durative actions with fixed durations. In this paper we motivate a proposed extension to PDDL to allow actions with infinite domain parameters, which we call control parameters. We illustrate reasons for using this modelling feature and then describe a planning approach that can handle domains that exploit it, implemented in a new planner, POPCORN (Partial-Order Planning with Constrained Real Numerics). We show that this approach scales to solve interesting problems.		Emre Savas;Maria Fox;Derek Long;Daniele Magazzeni	2016		10.3233/978-1-61499-672-9-1185	computer science;artificial intelligence;machine learning	AI	-20.08852153451018	15.969291227181603	178660
70db8b9bd3a7d03a2db3999e1a4cfc764f2f1438	reasoning about plan revision in agent programs	high level languages;agent programming languages;multi agent systems high level languages;multi agent systems;plan revision;high level programming primitives plan revision belief desire and intention bdi agent programming languages;propositional dynamic logic;propositional dynamic logic agent programming languages plan revision;cognition computer languages programming educational institutions autonomous agents multiagent systems	This talk is on reasoning about agent programs written in Belief, Desire and Intention (BDI) agent programming languages. BDI programming languages (for example, [1], [2], [3]) have high-level programming primitives which correspond to the beliefs, goals and plans of an AI agent. A program contains a set of rules which allow the agent to adopt plans given its current beliefs and goals. Plans are essentially imperative programs. For example, an agent may have a rule which says that if it believes that it is currently located in room 1 and its goal is to be in room 2, then a suitable plan to adopt would be to exit room 1, turn right, move forward for 3 meters, turn right, and enter room 2.	belief revision;high- and low-level;high-level programming language;imperative programming	Natasha Alechina	2012	2012 19th International Symposium on Temporal Representation and Reasoning	10.1109/TIME.2012.23	fourth-generation programming language;declarative programming;computer science;artificial intelligence;third-generation programming language;functional logic programming;programming paradigm;procedural programming;ontology language;inductive programming;fifth-generation programming language;programming language theory;programming language;logic programming;second-generation programming language;comparison of multi-paradigm programming languages;algorithm	AI	-24.165697757144123	17.69731200287556	180426
353d4082fff3af23c80a1a4d007b383d837397b0	learning constraint satisfaction problems: an ilp perspective		We investigate the problem of learning constraint satisfaction problems from an inductive logic programming perspective. Constraint satisfaction problems are the underlying basis for constraint programming and there is a long standing interest in techniques for learning these. Constraint satisfaction problems are often described using a relational logic, so inductive logic programming is a natural candidate for learning such problems. So far, there is however only little work on the intersection between learning constraint satisfaction problems and inductive logic programming. In this note, we point out several similarities and differences between the two classes of techniques and use these to propose several interesting research challenges.	constraint programming;constraint satisfaction problem;inductive logic programming;relational algebra	Luc De Raedt;Anton Dries;Tias Guns;Christian Bessiere	2016		10.1007/978-3-319-50137-6_5	mathematical optimization;constraint learning;machine learning;algorithm	AI	-19.23935536971385	12.76148100912707	180633
60775a7b50aa3de6d579cdc2fc50265e3971f05c	mixed-variable requirements roadmaps and their role in the requirements engineering of adaptive systems	requirement engineering;adaptive system;monitoring and control	The requirements roadmap concept is introduced as a solution to the problem of the requirements engineering of adaptive systems. The concept requires a new general definition of the requirements problem which allows for quantitative (numeric) variables, together with qualitative (binary boolean) propositional variables, and distinguishes monitored from controlled variables for use in control loops. We study the consequences of these changes, and argue that the requirements roadmap concept bridges the gap between current general definitions of the requirements problem and its notion of solution, and the research into the relaxation of requirements, the evaluation of their partial satisfaction, and the monitoring and control of requirements, all topics of particular interest in the engineering of requirements for adaptive systems [3]. From the theoretical perspective, we show clearly and formally the fundamental differences between more traditional conception of requirements engineering (e.g., Zave & Jackson [16]) and the requirements engineering of adaptive systems (from Fickas & Feather [6], over Letier & van Lamsweerde [9], and up to Whittle et al. [14] and the most recent research). From the engineering perspective, we define a proto-framework for early requirements engineering of adaptive systems, which illustrates the features needed in future requirements frameworks for this class of systems. Keywords-requirements roadmap, requirements problem, adaptive systems, relaxation of requirements, monitoring	adaptive system;control flow;integer programming;jackson;linear programming relaxation;mathematical optimization;modeling language;multi-objective optimization;pamela zave;probabilistic turing machine;propositional variable;requirement;requirements analysis;requirements engineering	Ivan Jureta;Alexander Borgida;Neil A. Ernst	2011	CoRR		software requirements specification;requirements management;requirement prioritization;computer science;systems engineering;engineering;adaptive system;requirement;software engineering;needs analysis;system requirements specification;functional specification;management science;requirements engineering;non-functional testing;functional requirement;non-functional requirement;systems design	SE	-25.501393058084343	15.861321660267095	180691
1628b8dba0aab41f352a9a95ace34ebe86217536	learning models of predicate logical theories with neural networks based on topos theory	programming language;learning model;automatic generation;theorem proving;topos theory;first order logic;neural network;approaches to learning	This chapter presents an approach to learn first-order logical theories with neural networks. We discuss representation issues  for this task in terms of a variable-free representation of predicate logic using topos theory and the possibility to use  automatically generated equations (induced by the topos) as input for a neural network. Besides the translation of first-order  logic into a variable-free representation, a programming language fragment for representing variable-free logic, the structure  of the used neural network for learning, and the overall architecture of the system are discussed. Finally, an evaluation  of the approach is presented by applying the framework to theorem proving problems.  	artificial neural network;neural network software	Helmar Gust;Kai-Uwe Kühnberger;Peter Geibel	2007		10.1007/978-3-540-73954-8_10	discrete mathematics;algorithmic learning theory;machine learning;mathematics;algorithm	NLP	-19.526867041273544	11.809535425287288	181803
55e0adc54b9b50faf3c9ba1e7d03e8effaae4630	dynamic logic programming and world state evaluation in computer games	dynamic logic	In this paper we propose a framework for world state evaluation in computer games based on Dynamic Logic Programming (DynLoP). Computer games (especially role-playing and adventure games) offer an exact, coherent and relatively small and simple world description and are usually built on game engines, which provide scripting capabilities. A common task of game scripting involves evaluation of the world state in a game, usually to find out whether a player has already completed a task (game quest). In this paper we describe a framework for building a Dynamic Logic Program based on a description of a game world, quests and events in the game. Stable models of such a program are then used to determine the status of a quest or the whole game. Because of declarative nature of DynLoP, its use allows easier and simpler queries and quest (task) characterization than current imperative scripting languages used in game engines. Furthermore, the world of a computer game provides an excellent environment for evaluation of DynLoP and its various semantics.	actor model;agent-based model;coherence (physics);game engine;imperative programming;logic programming;multi-agent system;pc game;pathfinding;point of view (computer hardware company);scripting language;semantics (computer science);temporal logic	Jozef Siska	2006			computational logic;game programming;state (computer science);functional logic programming;theoretical computer science;logic programming;computer network programming;dynamic logic (digital electronics);prolog;computer science	AI	-21.48298454897614	17.953994775802222	181991
c088fba08ecfb89e5a5bcdea7acdf30eba676ec8	optimizing active databases using the split technique.	split technique;active databases	 . A method to perform nonmonotonic relational rule computationsis presented, called the split technique, The goal is to avoid redundant computationswith rules that can insert and delete sets of tuples specified by the rulebody. The method is independent of the control strategy that governs rulefiring. Updatable relations are partitioned, as the computation progresses,into blocks of tuples such that tuples within a block are indiscernible fromeach other based on the computation so... 		Serge Abiteboul;Allen Van Gelder	1992		10.1007/3-540-56039-4_40	database;computer science	Robotics	-21.85715976342053	17.337091321714606	182353
fa99fc3952b56f2de405243b0bb02d1675ec46d4	theories of programming and formal methods		The purpose of this paper is to present some set-theoreticmodels of computation. This topic and its usefulness are clearly related to those presented in the book by Hoare and He: “Unifying Theories of Programming” [12]. However, we prefer to use here the term “computation” to that of “programming” as our purpose is not so much to unify various ways of programming (using different programming languages) but rather to see how various mechanical computation paradigms (be they sequential, distributed, parallel, and so on) can be given a unified mathematical theory. Our purpose is also to study how these computations can be specified and then developed by means of refinements and proofs.	computation;formal methods;hoare logic;programming language;refinable function;unifying theories of programming	Zhiming Liu;Jim Woodcock;Huibiao Zhu	2013		10.1007/978-3-642-39698-4	management science;formal methods;computer science	PL	-21.81357748733263	15.84263051015368	182770
65507e43efaefaa342084e4d8f37aa4fe979498e	heuristics for a default logic reasoning system	local search techniques;non monotonic reasoning;genetic algorithms;default logic	In Artificial Intelligence, Default Logic is recognized as a powerful framework for knowledge representation when one has to deal with incomplete information. Its expressive power is suitable for non monotonic reasoning, but the counterpart is its very high level of theoretical complexity. Today, some operational systems are able to deal with real world applications. However, finding a default logic extension in a practical way is not yet possible in whole generality. This paper which is an extended version of18 shows how heuristics such as Genetic Algorithms and Local Search techniques can be used and combined to build an automated default reasoning system. We give a general description of the required basic components and we exhibit experimental results.		Pascal Nicolas;Frédéric Saubion;Igor Stéphan	2001	International Journal on Artificial Intelligence Tools	10.1142/S0218213001000635	genetic algorithm;computer science;artificial intelligence;non-monotonic logic;machine learning;reasoning system;automated reasoning;default logic;algorithm	AI	-19.402013505247247	13.699333163410994	183116
665282dc1f2ced4c7d5454b2a16df17a690d517f	a temporal logic for modelling activities of daily living		Behaviour support technology is aimed at assisting people in organizing their Activities of Daily Living (ADLs). Numerous frameworks have been developed for activity recognition and for generating specific types of support actions, such as reminders. The main goal of our research is to develop a generic formal framework for representing and reasoning about ADLs and their temporal relations. This framework should facilitate modelling and reasoning about 1) durative activities, 2) relations between higher-level activities and subactivities, 3) activity instances, and 4) activity duration. In this paper we present a temporal logic as an extension of the logic TPTL for specification of real-time systems. Our logic TPTLbih is defined over Behaviour Identification Hierarchies (BIHs) for representing ADL structure and typical activity duration. To model execution of ADLs, states of the temporal traces in TPTLbih comprise information about the start, stop and current execution of activities. We provide a number of constraints on these traces that we stipulate are desired for the accurate representation of ADL execution, and investigate corresponding validities in the logic. To evaluate the expressivity of the logic, we give a formal definition for the notion of Coherence for (complex) activities, by which we mean that an activity is done without interruption and in a timely fashion. We show that the definition is satisfiable in our framework. In this way the logic forms the basis for a generic monitoring and reasoning framework for ADLs. 2012 ACM Subject Classification Theory of computation → Modal and temporal logics	activity recognition;interrupt;logic form;organizing (structure);real-time clock;real-time computing;temporal logic;theory of computation;tracing (software)	Malte S. Kließ;Catholijn M. Jonker;M. Birna van Riemsdijk	2018		10.4230/LIPIcs.TIME.2018.17	activities of daily living;expressivity;machine learning;hierarchy;activity recognition;temporal logic;artificial intelligence;computer science	AI	-24.846046398542782	15.256715616978349	183232
e1e3dc07e301822ba02913954ab2a4f0624c84ed	coba 2.0: a consistency-based belief change system	faculty of science environment engineering and technology;280399;computer software;sat solver;belief change;knowledge base	We describe COBA 2.0, an implementation of a consistency-ba sed framework for expressing belief change, focusing here on re vision and contraction, with the possible incorporation of integrity constra ints. This general framework was first proposed in [1]; following a review of this work , we present COBA 2.0’s high-level algorithm, work through several exam ples, and describe our experiments. A distinguishing feature of COBA 2.0 is tha t it builds on SATtechnology by using a module comprising a state-of-the-art SAT-solver for consistency checking. As well, it allows for the simultaneous s pecification of revision, multiple contractions, along with integrity const raints, with respect to a given knowledge base.	algorithm;applet;belief revision;boolean satisfiability problem;conjunctive normal form;const (computer programming);data integrity;experiment;high- and low-level;java;javadoc;knowledge base;prototype;run time (program lifecycle phase);sed;solver;vocabulary	James P. Delgrande;Daphne H. Liu;Torsten Schaub;Sven Thiele	2007		10.1007/978-3-540-75256-1_10	knowledge base;computer science;knowledge management;artificial intelligence;machine learning;mathematics;management science;boolean satisfiability problem	AI	-22.347341219750913	12.571632993302963	184266
29b6ffc72f92d445b708926b665f09df526d5d39	natural language concept analysis	concept analysis;language processing;natural language;relational model	Vera Kamphuis Dept. of Language and Speech University of Nijmegen The Netherlands v.kamphuis@let.kun.nl Janos Sarbo Computing Science Institute University of Nijmegen The Netherlands janos@cs.kun.nl Abstract Can we do text analysis without phrase structure? We think the answer could be positive. In this paper we outline the basics of an underlying theory which yields hierarchical structure as the result of more abstract principles relating to the combinatorial properties of linguistic units. We discuss the nature of these properties and illustrate our model with examples.	formal concept analysis;natural language;network address translation;phrase structure rules;ural (computer)	Vera Kamphuis;Janos J. Sarbo	1998		10.3115/1603899.1603934	natural language processing;language identification;computer science;artificial intelligence;linguistics	NLP	-26.015314702378205	11.919367042345835	185111
102a6592889061896a1012970f159bc728f5fc06	on the specification of visual languages: multisets as first-class citizens	rewrite systems;visual language	We propose a new paradigm for the specification of the syntax and semantics of visual languages, where multisets, contrary to so far existing works, are integrated as first-class citizens, in the sense that they can be manipulated without any limitation. We show that the resulting systems, called multiset rewriting systems, are extremely expressive for reasoning about multidimensional objects. Moreover, we provide a thorough analysis of their properties, like confluence and termination, and of their possible extensions.	first-class citizen	Massimo Marchiori	1996		10.1007/BFb0037422	natural language processing;computer science;programming language;algorithm	NLP	-24.042880634174747	13.425197044487412	185214
1a84c12450b2a90260e0971164bbf22ea9e09b24	declarative heuristic search for pattern set mining	heuristic programming constraint handling data mining;greedy search;search space;search problems itemsets programming mathematical model optimization reactive power data mining;declarative pattern mining constraints constraint programming greedy search;heuristic programming;exhaustive search declarative heuristic search pattern set mining constraint programming;search strategy;data mining;pattern mining;heuristic search;declarative;constraint programming;constraint handling;depth first search;constraints;exhaustive search	Recently, constraint programming has been proposed as a declarative framework for constraint-based pattern mining. In constraint programming, a problem is modelled in terms of constraints and search is done by a general solver. Similar to most pattern mining algorithms, these solvers typically employ exhaustive depth-first search, where constraints are used to prune the search space and make the search viable. In this paper we investigate the use of a similar declarative approach to the problem of pattern set mining. In pattern set mining one is searching for a small and useful set of patterns. In contrast to pattern mining, however, exhaustive search is not common in pattern set mining, the search space is often far too large to make such an approach practical. In this paper, we investigate an approach which aims to make general pattern set mining feasible by using a recently developed general solver that supports exhaustive as well as heuristic search. The key idea in this solver is that next to a declarative specification of the constraints also a high-level declarative description of the search is given. By separating the model and the search from the solver, the approach offers the advantage of reusing constraints and search strategies declaratively, while also allowing fast heuristic search.	algorithm;brute-force search;constraint (mathematics);constraint programming;data mining;declarative programming;depth-first search;heuristic;high- and low-level;solver	Tias Guns;Siegfried Nijssen;Albrecht Zimmermann;Luc De Raedt	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.60	beam search;mathematical optimization;constraint programming;greedy algorithm;heuristic;breadth-first search;computer science;machine learning;brute-force search;data mining;incremental heuristic search;best-first search	AI	-23.12274374554059	12.83432704122345	185376
45c4e26271a809653553d2449cc07260abc03e81	efficient reordering of prolog programs	prolog database management systems;database management systems;prolog;trees mathematics;prolog programming language;query languages;programming profession costs runtime logic programming testing time measurement clocks nasa marine vehicles geography;redundancy;logic programming;prolog clauses reordering prolog programs;optimization;computer program integrity;inference;markov chains	Prolog programs are often inefficient. Prolog execution corresponds to a depth-first traversal of an AND/OR graph; traversing subgraphs in another order can be less expensive. In this paper we show how reordering of Prolog clauses and especially goals can prevent unnecessary search. We characterize the restrictions on reordering and show how they may be detected. We design a new system of calling modes for Prolog, geared to reordering, and discuss ways to infer them automatically. We present an improved method for determining a good goal order for Prolog clauses, and use it as the basis for a reordering system, showing how it can be guided by information about modes and restrictions to generate reordered Prolog that behaves correctly.	depth-first search;prolog;tree traversal	Markian M. Gooley;Benjamin W. Wah	1988		10.1109/ICDE.1988.105452	constraint programming;horn clause;computer science;theoretical computer science;database;programming language;prolog;logic programming;algorithm	DB	-19.835662862921097	16.746918547711076	186248
d1df3a0b024bb712c91d89e11e5539c7d5a5a4c0	solving queries by tree projections	semijoin reduction;base relacional dato;proyeccion;acyclicity;monotone join expression;tree;query processing;qual graph;traitement requete;arbol;chase;theorie modeles;relational database;tableau;join;dependance inclusion;polynomial time algorithm;jointure;tree schema;projection;semijoin;database schema;base donnee relationnelle;graph algorithm;arbre;hypergraph;algorithme graphe;computational efficiency;teoria modelos;inclusion dependency;tree projection;model theory	Suppose a database schema <bold>D</bold> is extended to <bold>D¯</bold> by adding new relation schemas, and states for <bold>D</bold> are extended to states for <bold>D¯</bold> by applying joins and projections to existing relations. It is shown that certain desirable properties that <bold>D¯</bold> has with respect to <bold>D</bold>. These properties amount to the ability to compute efficiently the join of all relations in a state for <bold>D</bold> from an extension of this state over <bold>D¯</bold>. The equivalence is proved for unrestricted (i.e., both finite and infinite) databases. If <bold>D¯</bold> is obtained from <bold>D</bold> by adding a set of new relation schemas that form a tree schema, then the equivalence also holds for finite databases. In this case there is also a polynomial time algorithm for testing the existence of a tree projection of <bold>D¯</bold> with respect to <bold>D</bold>.	acm transactions on database systems;algorithm;database schema;graph coloring;maximal set;np-completeness;p (complexity);polynomial;reduced cost;reduction (complexity);relational algebra;relational database;turing completeness	Yehoshua Sagiv;Oded Shmueli	1993	ACM Trans. Database Syst.	10.1145/155271.155277	projection;relational algebra;relational database;computer science;chase;database;tree;database schema;algorithm;model theory	DB	-25.073646589106275	11.350509039152673	187591
6cdc33ba08f987315f675c6fac30ffd95551b2fc	eliminating unwanted loops in prolog		Modifications to Prolog are proposed that make it possible to express transitive and symmetrical relations and biconditionals. This is done by blocking recursion under circumstances that would lead to infinite loops.	blocking (computing);infinite loop;prolog;recursion	Michael A. Covington	1985	SIGPLAN Notices	10.1145/988284.988288	computer science;theoretical computer science;programming language;algorithm	PL	-21.74109949086005	17.384411661911958	187606
13b7f3409d57f4ecd07ab8dfbbec6a27ecf2e884	from zinc to design model	optimisation sous contrainte;modelizacion;design model;constrained optimization;lenguaje programacion;rewrite rule;langage modelisation;programming language;plug and play;programacion basada sobre modelo;langage declaratif;langage evolue;busca local;optimizacion con restriccion;modelisation;mixed integer program;intermediate language;programacion mixta entera;modelling language;lenguaje modelizacion;lenguaje intermediario;reecriture;declarative language;constraint programming;programmation basee sur modele;langage programmation;programmation partiellement en nombres entiers;mixed integer programming;zinc;lenguaje evolucionado;model based programming;rewriting;high level language;modeling;local search;lenguaje declarativo;recherche locale;langage intermediaire;reescritura;qa76 computer software	We describe a preliminary implementation of the high-level modelling language Zinc. This language supports a modelling methodology in which the same Zinc model can be automatically mapped into different design models, thus allowing modellers to easily “plug and play” with different solving techniques and so choose the most appropriate for that problem. Currently, mappings to three very different design models based on constraint programming (CP), mixed integer programming (MIP) and local search are provided. Zinc is the first modelling language that we know of that supports such solver and technique-independent modelling. It does this by using an intermediate language called Flattened Zinc, and rewrite rules for transforming the Flattened Zinc model into one that is tailored to a particular solving technique.	consistency model;constraint programming;eclipse;entity;experiment;formal language;high- and low-level;integer programming;linear programming;local search (optimization);mercury;model transformation;modeling language;plug and play;prototype;rewrite (programming);rewriting;scalability;software propagation;solver;zaf	Reza Rafeh;Maria Garcia de la Banda;Kim Marriott;Mark Wallace	2007		10.1007/978-3-540-69611-7_14	natural language processing;constraint programming;constrained optimization;rewriting;computer science;artificial intelligence;local search;zinc;programming language;intermediate language;algorithm	AI	-20.71847884479386	15.364153385579858	188131
3e9823adca4880707011ec4343d78c1bae28c518	domain model acquisition in the presence of static relations in the lop system		We present a new domain model acquisition algorithm, LOP, that induces static predicates by using a combination of the generalised output from LOCM2 and a set of optimal plans as input to the learning system. We observe that static predicates can be seen as restrictions on the valid groundings of actions. Without the static predicates restricting possible groundings, the domains induced by LOCM2 produce plans that are typically shorter than the true optimal solutions. LOP works by finding a set of minimal static predicates for each operator that preserves the length of the optimal plan.	algorithm;domain model;knowledge engineer;language-oriented programming	Peter Gregory;Stephen Cresswell	2015			discrete mathematics;machine learning;artificial intelligence;operator (computer programming);predicate (grammar);domain model;mathematics	DB	-20.258401790446477	16.063365512917382	188510
5ae13bd772823f13a31b54d01e6c8f2cf8f87874	a basic model of kbs software		This document will describe a simple model of conventional software development, show how KBS software development diiers from the conventional and then deene a collection of terms which are important with respect to the quality of the software under development. We will be deliberately vague about the exact scope of the term software development; it will certainly cover the activity of implementation, but it is also intended to cover some aspects of software speciication and design. The term KBS software is very diicult to pin down, as certain features which are argued to be characterising, such as knowledge bases, rules and symbolic representation, turn out to be diicult to deene in any meaningful sense or are not characterising at all. The deenition which is used in this document views the characterising feature of KBS software as being deened by a non-deterministic operator and its aaect on the behaviour of the program which is being developed. In order to deene a model of software development, it will be necessary to keep the notion of a program simple and precise. The-calculus has been chosen as the class of programs which will motivate the discussion. This has the advantage of being small, can be given a simple semantics and captures many classes of programming language. The rest of this document is divided into the following sections: A simple model of program development is described in terms of the-calculus and the SECD machine. The characteristic feature of KBS software is described and its impact on software development is discussed. A collection of terms which aaect software quality are deened with respect to the model of software development. The presentation will be kept informal but precise, we have in mind a formal underlying presentation of these ideas which can be developed if the basic notions turn out to be useful. A program will be deened to be an expression in the-calculus whose concrete syntax is deened as follows: where E is to be thought of as the set of all programs. Such a syntactic deenition states nothing about the meaning of each program. One way of giving the programs a meaning is by deening how they will be performed using a transition machine. Diierent machines will attribute diierent meanings to the programs by encoding speciic builtin evaluation mechanisms and types of value. This will be the way in which the programs will be given a …	knowledge base;knowledge-based systems;mind;parse tree;programming language;secd machine;software development;software quality;vagueness	Tony Clark	2018	CoRR		backporting;software peer review;computer science;software construction;software design description;systems engineering;software verification and validation;software development;package development process;social software engineering	SE	-25.660456843386793	17.474620550673333	189176
3e5ee99ab7fd1394b677643a2996126efd26b136	trust negotiation with nonmonotonic access policies	recurso internet;distributed system;confiance;systeme reparti;psychologie sociale;negociation;internet resource;securite;circonscription;ressource internet;intelligence artificielle;satisfiability;circumscription;confidence;sistema repartido;confianza;negociacion;safety;bargaining;psicologia social;artificial intelligence;social psychology;inteligencia artificial;seguridad;trust negotiation;circonscripcion	We study the structure of nonmonotonic access policies for internet-based resources. We argue that such policies could be divided into two parts: the locally designed policies and imported policies. Imported policies should always be monotonic while the local policies could be nonmonotonic. We develop a safe proof procedure for nonmonotonic trust negotiation where safety means that access to a resource is granted only if its access policy is satisfied.	action potential;credential;internet;logic programming;mitchell corporation;non-monotonic logic;partial evaluation;privacy;public key certificate;qp state machine frameworks;requirements analysis;role-based access control;scheme;trust management (information system);ws-trust	Phan Minh Dung;Phan Minh Thang	2004		10.1007/978-3-540-30179-0_6	computer science;artificial intelligence;confidence;computer security;negotiation;circumscription;algorithm;satisfiability	AI	-21.896491525169576	11.763427188408327	190470
27b8a27af807e416a0ee9fae5e910b29815d8b3c	efficient induction of executable logic programs from examples	systeme intelligent;learning;sistema inteligente;algoritmo recursivo;inductive logic programming;logical programming;input output;aprendizaje;apprentissage;algorithme recursif;programmation logique;intelligent system;recursive algorithm;logic programs;programacion logica	Some inductive logic programming (ILP) systems use determinate literals to efficiently induce logic programs. A determinate literal is a literal that does not distinguish positive examples from negative examples, but produces information in variables introduced by the literal. The concept of determinate literals, however, is not reflected by the concept of input/output mode of predicate attributes properly, and so a system using determinate literals may induce inconsistent logic programs with predicate mode or inexecutable programs. The paper extends the concept of determinate literals and proposes input and output determinate literals. These literals function as pre-processor and post-processor against other literals. The paper also describes an implementation of the method and experimentations.	executable;logic programming	Nobuhiro Inuzuka;Hirohisa Seki;Hidenori Itoh	1997		10.1007/3-540-63875-X_54	input/output;computer science;artificial intelligence;theoretical computer science;operating system;mathematics;programming language;algorithm;recursion	AI	-20.01254946719665	11.469905777721614	191315
9c929322d51f68bec27f9b87b82fa282d61e240b	a method of analysis of fault trees with time dependencies	concepcion asistida;arbol defecto;fault tree;computer aided design;time dependent;red petri;logiciel a securite critique;analysis and modelling;failure analysis;expressive power;protection;control system;time petri net;dependance du temps;time dependence;arbre defaut;security systems;safety critical software;safety critical system;conception assistee;systeme securite;proteccion;analisis averia;analyse dommage;petri net;dependencia del tiempo;reseau petri;fault tree analysis	Safety is one of the biggest concerns in the design of computer-aided control systems. In order to make the system as safe as possible a number of analysis techniques has been developed. One of them is Fault Tree Analysis. Fault tree (FT) represents causal and generalization relations between events (e.g. between hazard and its causes). However, original FT cannot express either time relations between events or times of: detection of a danger situation and protection. A new method based on systems of inequalities and equalities for analysis of FTs with time dependencies is proposed in the paper. The method can be used for analysis of protections too. FT analysis and modelling of protection using systems of inequalities and equalities will be illustrated by an example.#R##N##R##N#Formal models of FT gates used in the paper have the same expressive power as Timed Petri Net (TPN) models of FT gates from the paper [5]. However, present analysis method has greater decision power than classic TPN analysis method because the present method can be applied for much greater FTs. Additionally, the present approach results in more clear final conclusions.	fault tree analysis	Jan Magott;Pawel Skrobanek	2000		10.1007/3-540-40891-6_16	reliability engineering;fault tree analysis;engineering;control system;artificial intelligence;operations research;algorithm	Theory	-24.269532392969623	15.430515679277427	191339
08a3ffd3429ff7e372b21c8922aa6bbe425ab520	speeding up symbolic reasoning for relational queries		The ability to reason about relational queries plays an important role across many types of database applications, such as test data generation, query equivalence checking, and computer-assisted query authoring. Unfortunately, symbolic reasoning about relational queries can be challenging because relational tables are multisets (bags) of tuples, and the underlying languages, such as SQL, can introduce complex computation among tuples.   We propose a space refinement algorithm that soundly reduces the space of tables such applications need to consider. The refinement procedure, independent of the specific dataset application, uses the abstract semantics of the query language to exploit the provenance of tuples in the query output to prune the search space. We implemented the refinement algorithm and evaluated it on SQL using three reasoning tasks: bounded query equivalence checking, test generation for applications that manipulate relational data, and concolic testing of database applications. Using real world benchmarks, we show that our refinement algorithm significantly speeds up (up to 100×) the SQL solver when reasoning about a large class of challenging SQL queries, such as those with aggregations.	aggregate function;algorithm;assertion (software development);benchmark (computing);column (database);concolic testing;encode;experiment;formal equivalence checking;query language;refinement (computing);relational database;sql;solver;symbolic computation;test data generation;turing completeness	Chenglong Wang;Alvin Cheung;Rastislav Bodík	2018	PACMPL	10.1145/3276527	theoretical computer science;computer science	PL	-23.33787110358331	11.362794154416214	191417
5c7c3313218f3a86b0400121272a70f810c5386a	refining and extending the procedural net		"""This paper presents a new definition for Plans The objects defined are called Plan Nets, and are similar in spirit to Sacerdoti's Procedural Nets (1975) It is argued that Plan Nets are more descriptive tham Procedural Nets, because they can easily describe iterative behaviour The Plan Net definition is motivated by providing an operational semantics for the Procedural Net, and noticing that all Procedural Net state spaces are """"loop free"""" This is seen to restrict the behaviours that can be described by the Procedural Net to those which do not. include iteration It is suggested that Plan Net. state spaces can contain loops, and thus can describe iterative behaviour"""	iteration;operational semantics;petri net	Mark Drummond	1985			computer science;artificial intelligence;procedural modeling;algorithm	AI	-23.557427289960106	18.243072769610247	192210
981ace039dae7471bee6debf1ba099bd4cafb625	automata for branching and layered temporal structures: an investigation into regularities of infinite transition systems		Since 2002, FoLLI, the Association for Logic, Language, and Information (www.folli.org), has awarded an annual prize for an outstanding dissertation in the ﬁelds of logic, language, and information. The prize is named after the well-known Dutch logician Evert Willem Beth, whose interdisciplinary interests are in many ways exemplary for the aims of FoLLI. It is sponsored by the E.W. Beth Foundation. Dissertations submitted for the prize are judged on technical depth and strength, originality, and impact made in at least two of the three ﬁelds of logic, language, and computation. Every year the competition is strong and the interdisciplinary character of the award stimulates lively discussions and debates. Recipients of the award are given the opportunity to prepare a book version of their thesis for publication in the FoLLI Publications on Logic, Language and Information. This volume is based on the PhD thesis of Gabriele Puppis, who was the winner of the E.W. Beth dissertation award for 2007. Puppisu0027s thesis focuses on logic and computation and, more speciﬁcally, on automata-based decidability techniques for time granularity and on a new method for deciding monadic second-order theories of trees. In the ﬁrst part of the thesis Puppis deﬁnes and systematically exploits various classes of sequential automata in order to solve a number of relevant problems in time granularity (e.g., equivalence, conversion, minimization, optimization, etc.). For each application, he investigates expressiveness and complexity and provides algorithms working on automatabased representations of time granularity. The core of the remaining part of Puppisu0027s thesis is a generalization of the Elgot-Rabin automata-based decision method. He deﬁnes a powerful reduction over colored trees and obtains the generalization by overcoming a number of technical diﬃculties, and thus not only solves the original decision problems, but also gives a precise and inspiring comparison of the newly introduced technique with more classical ones, such as, for example, Shelahu0027s composition method. In both parts of the thesis Dr. Puppis shows mastering and deep understanding of the topic, an elegant and concise presentation of the results, and an insightful overall view on the subject. The results presented by Puppis represent a signiﬁcant step towards a better understanding of the changes in granularity levels that humans make so easily in cognition of time, space, and other phenomena, whereas their logical and computational structure poses diﬃcult conceptual and computational challenges.	automaton	Gabriele Puppis	2010		10.1007/978-3-642-11881-4	stereochemistry;computational chemistry	Logic	-22.291999739278996	14.556596880400335	192256
06533eb53c3fa8d679cac8cbadb26e6865a4c7de	inductive theorem proving and computer algebra in the mathweb software bus	sistema experto;calcul formel;intelligence artificielle;calculo formal;theorem proving;demonstration theoreme;induccion;induction;artificial intelligence;inteligencia artificial;systeme expert;demostracion teorema;computer algebra;expert system	Reasoning systems have reached a high degree of maturity in the last decade. However, even the most successful systems are usually not general purpose problem solvers but are typically specialised on problems in a certain domain. The MathWeb Software Bus (MathWeb-SB) is a system for combining reasoning specialists via a common software bus. We describe the integration of the ?Clam system, a reasoning specialist for proofs by induction, into the MathWeb-SB. Due to this integration, ?Clam now offers its theorem proving expertise to other systems in the MathWeb-SB. On the other hand, ?Clam can use the services of any reasoning specialist already integrated. We focus on the latter and describe first experiments on proving theorems by induction using the computational power of the Maple system within ?Clam.	software bus;symbolic computation	Jürgen Zimmer;Louise A. Dennis	2002		10.1007/3-540-45470-5_28	computer science;artificial intelligence;machine learning;database;mathematics;automated theorem proving;programming language;computer security;expert system;algorithm	Logic	-19.588723832282444	17.60918838977842	192901
452f8fb74dbbeca956e414da56f6233bc8afe76c	enhancing sharing for precision	sharing analysis;mode analysis;abstract interpretation.	Regarding the precision of combined domains including Jacobs and Langen’s Sharing there is a core of techniques, such as the standard integration with freeness and linearity information, that are widely used and well accepted. However, a number of other proposals for refined domain combinations have been circulating more or less clandestinely for years. One feature that is common to these proposals is that they do not seem to have undergone experimental evaluation. We question whether significantly more precision is obtainable thanks to these techniques. In particular, we discuss and/or experimentally evaluate: helping Sharing with definitely ground variables computed with Pos; the incorporation of explicit structural information into the domain of analysis; more sophisticated ways of integrating Sharing and Pos; the issue of reordering the bindings in the computation of the abstract mgu; an original proposal concerning the addition of a domain recording the set of variables that are deemed to be ground or free; a more refined way of using linearity to improve the analysis; the issue of whether tracking compoundness allows to compute more precise sharing information; and, finally, the recovery of hidden information in the combination of Sharing with the usual domain for freeness.	computation;experiment;unification (computer science)	Roberto Bagnara;Enea Zaffanella;Patricia M. Hill	1999			data mining;abstract interpretation;computation;linearity;computer science	AI	-23.303218541928256	16.24713479432986	193119
995f8809369545603ba92ebd529782167d0c53c2	requirements-driven automatic configuration of natural language applications	annotation schemes;natural language processing;tools and resources;natural language	The paper proposes a model for dynamical building of architectures intended to process natural language. The representation that stays at the base of the model is a hierarchy of XML annotation schemas in which the parent-child links are defined by subsumption relations. We show how the hierarchy may be augmented with processing power by marking the edges with names of processors, each realising an elementary NL processing step, able to transform the annotation corresponding to the parent node onto that corresponding to the child node. The paper describes a navigation algorithm in the hierarchy, which computes paths linking a start node to a destination node, and which automatically configures architectures of serial and parallel combinations of processors.	algorithm;central processing unit;item unique identification;nl (complexity);natural language;subsumption architecture;tree (data structure);xml namespace	Dan Cristea;Corina Forascu;Ionut Pistol	2006			natural language processing;programming language	NLP	-21.994602401872775	16.32847738767156	193444
498133e89ced87ed13312b8e9f2d5016d482f55f	tweety: a comprehensive collection of java libraries for logical aspects of artificial intelligence and knowledge representation	knowledge representation	This paper presents Tweety, an open source project for scientific experimentation on logical aspects of artificial intelligence and particularly knowledge representation. Tweety provides a general framework for implementing and testing knowledge representation formalisms in a way that is familiar to researchers used to logical formalizations. This framework is very general, widely applicable, and can be used to implement a variety of knowledge representation formalisms from classical logics, over logic programming and computational models for argumentation, to probabilistic modeling approaches. Tweety already contains over 15 different knowledge representation formalisms and allows easy computation of examples, comparison of algorithms and approaches, and benchmark tests. This paper gives an overview on the technical architecture of Tweety and a description of its different libraries. We also provide two case studies that show how Tweety can be used for empirical evaluation of different problems in artificial intelligence.	actionscript;algorithm;artificial intelligence;benchmark (computing);command-line interface;computation;computational model;declarative programming;existential quantification;experiment;gnu;general-purpose modeling;information technology architecture;java;knowledge representation and reasoning;library (computing);logic programming;ontology (information science);open-source software;programming language;recursion;reflow soldering;simulation;text corpus;usability;user interface;web ontology language	Matthias Thimm	2014			knowledge representation and reasoning;computer science;artificial intelligence;theoretical computer science;algorithm	AI	-22.378840331121534	13.033205597053188	193606
337c328afc3a5e57d565cb3e20b8d695b3516760	tractability and structural closures in attribute logic type signatures	potential disagreement;current practice;structural closure;feature-based framework;subtype covering;unique feature introduction;grammar developer;realistic grammar signature;attribute logic type signature;feature logic	This paper considers three assumptions conventionally made about signatures in typed feature logic that are in potential disagreement with current practice among grammar developers and linguists working within feature-based frameworks such as HPSG: meet-semilatticehood, unique feature introduction, and the absence of subtype covering. It also discusses the conditions under which each of these can be tractably restored in realistic grammar signatures where they do not already exist.	antivirus software;compiler;constraint handling rules;disjunctive normal form;electronic signature;head-driven phrase structure grammar;np-completeness;neural coding;time complexity;type signature	Gerald Penn	2001			machine learning;data mining;algorithm	NLP	-21.61190214557327	13.174243241784295	194178
231f70ee960a24d4ffcd04aee4a5bc04a08d5e80	towards probabilistic formal analysis of sats-simultaneously moving aircraft (sats-sma)	formal verification;probabilistic analysis;model checking;sats concept of operations;aircraft safety	The objective of NASA’s Small Aircraft Transportation System (SATS) Concept of Operations (ConOps) is to facilitate high volume operation of advanced small aircraft operating in non-towered, non-radar airports. This system can provide improved and accessible air travel at a lower cost. Given the safety-critical nature of SATS, its analysis accuracy is extremely important. However, the commonly used analysis techniques, like pilot/computer simulation and traditional model checking, do not ascertain an error-free and complete verification of SATS due to the wide range of possibilities involved in SATS or the inability to capture the randomized and unpredictable aspects of the SATS ConOps environment in their models. Another limitation of these studies is that a limited speed range was used in the analysis. To overcome these limitations, we propose to formulate the SATS ConOps as a fully synchronous and probabilistic model, i.e., SATS-SMA, that supports simultaneously moving aircraft. The distinguishing features of our work include the preservation of safety of aircraft while providing a precise timing model, which is closer to reality compared to the previous hybrid analyses. Important insights related to the aircraft take-off and landing operations during the instrument meteorological conditions are also presented.	computer simulation;markov chain;model checking;prism (surveillance program);parametric model;probabilistic analysis of algorithms;randomized algorithm;static timing analysis;statistical model	Muhammad Usama Sardar;Nida Afaq;Osman Hasan;Khaza Anuarul Hoque	2017	Journal of Automated Reasoning	10.1007/s10817-017-9416-6	sma*;probabilistic analysis of algorithms;model checking;mathematics;small aircraft transportation system;probabilistic logic;simulation;instrument meteorological conditions;concept of operations;formal verification	SE	-24.346447918635363	15.922012778431688	196515
45a6e26f988f6daa8d7b5b2b348a67776762dfd8	on the rewriting and efficient computation of bound disjunctive datalog queries	disjunctive datalog;optimization problem;logic programming;magic set technique;theory;logic programs;constraints;deductive databases	In this paper we present a technique for the optimization of bound queries over disjunctive deductive databases with constraints. The proposed approach consists of two distinct phases: i) the rewriting of queries for propagating bindings from the query goal into the program, and ii) the use of specialized algorithms computing rewritten queries. The rewriting of queries is based on the exploitation of a binding propagation technique which reduces the size of the data relevant to answer the query and, consequently, minimizes both the complexity of computing a single model and the whole number of models to be considered. As for general queries the rewriting technique does not ensure soundness, we present two sound and complete algorithms computing rewritten queries under brave and cautious reasoning. The efficiency of our algorithms has been proved by several experiments considering both classical search and optimization problems.	algorithm;computation;datalog;deductive database;disjunctive normal form;experiment;mathematical optimization;query optimization;rewriting;software propagation	Sergio Greco;Ester Zumpano	2003		10.1145/888251.888265	optimization problem;computer science;theoretical computer science;datalog;programming language;logic programming;theory;algorithm	DB	-22.888587427224895	11.499340157846953	199292
