id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
f9bbe34a8692f02d456280ae90c4eb8ce1bd1fd3	kce_dalab@maponsms-fire2018: effective word and character-based features for multilingual author profiling		This paper illustrates the work on identification of gender and age-group in Multilingual Author Profiling on SMS messages (MAPonSMS) shared task conducted in the Forum for Information Retrieval and Evaluation (FIRE 2018). To develop the Multilingual Author profiling system, the organizers released the training corpus which includes multilingual (Roman Urdu and English) SMS messages and its corresponding profiles. In gender identification, a profile may be either male or female. The author's age-group fall into one of the three categories: 15-19, 20-24, 25-xx. We have developed the author profiling system using the word and character-based Term Frequency & Inverse Document Frequency (TFIDF) features and classify with Support Vector Machine classifier. The proposed system achieved the State-of-Art performance in the multilingual author profiling on SMS task. The accuracy obtained for identification of age-group is 65% and for gender, it is 87%. The performance is also evaluated jointly where the accuracy gained is 57%. We also experimented with the system by changing different parameters and report the crossvalidation accuracy.		V Devi SharmilaDevi;S Kannimuthu;Gelli Ravikumar;M Kumar AnandKumar	2018				NLP	-22.53127417413269	-69.2645532624352	130581
0eaabbe182c7582d6f12605ba0cefc10e630c158	automated essay scoring in the presence of biased ratings		Studies in Social Sciences have revealed that when people evaluate someone else, their evaluations often reflect their biases. As a result, rater bias may introduce highly subjective factors that make their evaluations inaccurate. This may affect automated essay scoring models in many ways, as these models are typically designed to model (potentially biased) essay raters. While there is sizeable literature on rater effects in general settings, it remains unknown how rater bias affects automated essay scoring. To this end, we present a new annotated corpus containing essays and their respective scores. Different from existing corpora, our corpus also contains comments provided by the raters in order to ground their scores. We present features to quantify rater bias based on their comments, and we found that rater bias plays an important role in automated essay scoring. We investigated the extent to which rater bias affects models based on hand-crafted features. Finally, we propose to rectify the training set by removing essays associated with potentially biased scores while learning the scoring model.	automated essay scoring;lexicon;rectifier;subject (philosophy);test set;text corpus	Evelin Amorim;Márcia Cançado;Adriano Veloso	2018			natural language processing;machine learning;artificial intelligence;computer science;automated essay scoring	NLP	-21.021981705212834	-71.247976552252	131113
5b27c6e38a28898542787b0cf48a91b4fbe9b367	labeling of query words using conditional random field		This paper describes our approach on Query Word Labeling as an attempt in the shared task on Mixed Script Information Retrieval at Forum for Information Retrieval Evaluation (FIRE) 2015. The query is written in Roman script and the words were in English or transliterated from Indian regional languages. A total of eight Indian languages were present in addition to English. We also identified the Named Entities and special symbols as part of our task. A CRF based machine learning framework was used for labeling the individual words with their corresponding language labels. We used a dictionary based approach for language identification. We also took into account the context of the word while identifying the language. Our system demonstrated an overall accuracy of 75.5% for token level language identification. The strict F-measure scores for the identification of token level language labels for Bengali, English and Hindi are 0.7486, 0.892 and 0.7972 respectively. The overall weighted F-measure of our system was 0.7498. CCS Concepts • Computing methodologies~Natural language processing • Computing methodologies~Information extraction	conditional random field;dictionary;entity;information extraction;information retrieval;language identification;machine learning;natural language processing	Satanu Ghosh;Souvick Ghosh;Dipankar Das	2015			natural language processing;speech recognition;computer science;query language	NLP	-24.0133858251073	-71.59725628580298	131290
5cedfa000de7ea37fdb83627673a8d324aa47cb2	semi-supervised learning with induced word senses for state of the art word sense disambiguation		Word Sense Disambiguation (WSD) aims to determine the meaning of a word in context, and successful approaches are known to benefit many applications in Natural Language Processing. Although, supervised learning has been shown to provide superior WSD performance, current sense-annotated corpora do not contain a sufficient number of instances per word type to train supervised systems for all words. While unsupervised techniques have been proposed to overcome this data sparsity problem, such techniques have not outperformed supervised methods. In this paper, we propose a new approach to building semi-supervised WSD systems that combines a small amount of sense-annotated data with information from Word Sense Induction, a fully-unsupervised technique that automatically learns the different senses of a word based on how it is used. In three experiments, we show how sense induction models may be effectively combined to ultimately produce high-performance semi-supervised WSD systems that exceed the performance of state-of-the-art supervised WSD techniques trained on the same sense-annotated data. We anticipate that our results and released software will also benefit evaluation practices for sense induction systems and those working in low-resource languages by demonstrating how to quickly produce accurate WSD systems with minimal annotation effort.	experiment;natural language processing;semi-supervised learning;semiconductor industry;sparse matrix;supervised learning;text corpus;unsupervised learning;web services for devices;word sense;word-sense disambiguation;word-sense induction	Osman Baskaya;David Jurgens	2016	J. Artif. Intell. Res.	10.1613/jair.4917	natural language processing;semeval;computer science;machine learning;pattern recognition	AI	-22.227116456818326	-71.75444469022572	131878
6a9c0bf49d4af58b5c1194a739e3443105e05a9b	l2f/inesc-id at semeval-2017 tasks 1 and 2: lexical and semantic features in word and textual similarity		This paper describes our approach to the SemEval-2017 “Semantic Textual Similarity” and “Multilingual Word Similarity” tasks. In the former, we test our approach in both English and Spanish, and use a linguistically-rich set of features. These move from lexical to semantic features. In particular, we try to take advantage of the recent Abstract Meaning Representation and SMATCH measure. Although without state of the art results, we introduce semantic structures in textual similarity and analyze their impact. Regarding word similarity, we target the English language and combine WordNet information with Word Embeddings. Without matching the best systems, our approach proved to be simple and effective.	algorithm;lexicon;long short-term memory;machine learning;microsoft word for mac;semeval;weight function;word2vec;wordnet	Pedro Fialho;Hugo Rodrigues;Luísa Coheur;Paulo Quaresma	2017		10.18653/v1/S17-2032	artificial intelligence;natural language processing;computer science;semantic similarity;semeval;lexical density	NLP	-26.380961355999407	-68.36342489592882	131917
8cafc0df38bdf407dfc90a392b273c12879eda7a	investigating the role of argumentation in the rhetorical analysis of scientific publications with neural multi-task learning models		Exponential growth in the number of scientific publications yields the need for effective automatic analysis of rhetorical aspects of scientific writing. Acknowledging the argumentative nature of scientific text, in this work we investigate the link between the argumentative structure of scientific publications and rhetorical aspects such as discourse categories or citation contexts. To this end, we (1) augment a corpus of scientific publications annotated with four layers of rhetoric annotations with argumentation annotations and (2) investigate neural multi-task learning architectures combining argument extraction with a set of rhetorical classification tasks. By coupling rhetorical classifiers with the extraction of argumentative components in a joint multi-task learning setting, we obtain significant performance gains for different rhetorical analysis tasks.	citation index;computer graphics;computer multitasking;digital rhetoric;encoder;high- and low-level;machine learning;multi-task learning;recurrent neural network;scientific literature	Anne Lauscher;Goran Glavas;Simone Paolo Ponzetto;Kai Eckert	2018			argumentative;cognitive science;natural language processing;artificial intelligence;argumentation theory;rhetoric;computer science;citation;scientific writing;multi-task learning;rhetorical question	NLP	-19.730506403247972	-70.92971308469731	132428
dad9c737ed86d4c4c0534549d2a2bec59c4ca9f0	n-gram based approach for automatic prediction of essay rubric marks		Automatic Essay Scoring, applied to the prediction of grades for dimensions of a scoring rubric, can provide automatic detailed feedback on students’ written assignments. We apply a character and word n-gram based technique proposed originally for authorship identification—Common N-Gram (CNG) classifier—to this task. We report promising results for the rubric mark prediction for essays by CNG, and perform analysis of suitability of different types of n-grams for the task.	n-gram	Magdalena Jankowska;Colin Conrad;Jabez Harris;Vlado Keselj	2018		10.1007/978-3-319-89656-4_30	rubric;n-gram;machine learning;artificial intelligence;computer science	NLP	-21.55534158554069	-70.37206535153894	132575
458a20aaad8c76355d26737789beb23e8004edd2	effective information extraction with semantic affinity patterns and relevant regions	information extraction	We present an information extraction system that decouples the tasks of finding relevant regions of text and applying extraction patterns. We create a self-trained relevant sentence classifier to identify relevant regions, and use asemantic affinitymeasure to automatically learn domain-relevant extraction patterns. We then distinguish primary patterns fromsecondarypatterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training.	affinity analysis;algorithm;information extraction;linear classifier;pattern recognition;relevance;statistical classification;the sentence	Siddharth Patwardhan;Ellen Riloff	2007			natural language processing;relationship extraction;computer science;pattern recognition;data mining;information extraction;information retrieval	NLP	-23.969717673503045	-70.0493120112842	133175
0bde026d3cd31c0a3e05cb7fe563e5703a071cef	sentiment analysis based on probabilistic models using inter-sentence information	probabilistic model;conditional random field;sentiment analysis;naive bayes	This paper proposes a new method of the sentiment analysis utilizing inter-sentence structures especially for coping with reversal phenomenon of word polarity such as quotation of other’s opinions on an opposite side. We model these phenomenon using Hidden Conditional Random Fields(HCRFs) with three kinds of features: transition features, polarity features and reversal (of polarity) features. Polarity features and reversal features are doubly added to each word, and each weight of the features are trained by the common structure of positive and negative corpus in, for example, assuming that reversal phenomenon occured for the same reason (features) in both polarity corpus. Our method achieved better accuracy than the Naive Bayes method and as good as SVMs.	naive bayes classifier;sentiment analysis	Kugatsu Sadamitsu;Satoshi Sekine;Mikio Yamamoto	2008			sentiment analysis;naive bayes classifier;divergence-from-randomness model;bayesian programming;machine learning;conditional random field;computer science;probabilistic relevance model;artificial intelligence;pattern recognition;phenomenon;sentence	NLP	-19.166653375562014	-69.50272937686509	133178
74ae9c4de0e2dec55bd7aaa4e7d6cb41f740d041	automatic categorization for improving spanish into spanish sign language machine translation	automatic tagging;syntactic semantic information;statistical language translation;spanish sign language lse;automatic categorization	This paper describes a preprocessing module for improving the performance of a Spanish into Spanish Sign Language (Lengua de Signos Espanola: LSE) translation system when dealing with sparse training data. This preprocessing module replaces Spanish words with associated tags. The list with Spanish words (vocabulary) and associated tags used by this module is computed automatically considering those signs that show the highest probability of being the translation of every Spanish word. This automatic tag extraction has been compared to a manual strategy achieving almost the same improvement. In this analysis, several alternatives for dealing with non-relevant words have been studied. Non-relevant words are Spanish words not assigned to any sign. The preprocessing module has been incorporated into two well-known statistical translation architectures: a phrase-based system and a Statistical Finite State Transducer (SFST). This system has been developed for a specific application domain: the renewal of Identity Documents and Driver's License. In order to evaluate the system a parallel corpus made up of 4080 Spanish sentences and their LSE translation has been used. The evaluation results revealed a significant performance improvement when including this preprocessing module. In the phrase-based system, the proposed module has given rise to an increase in BLEU (Bilingual Evaluation Understudy) from 73.8% to 81.0% and an increase in the human evaluation score from 0.64 to 0.83. In the case of SFST, BLEU increased from 70.6% to 78.4% and the human evaluation score from 0.65 to 0.82.	categorization;machine translation	Verónica López-Ludeña;Rubén San-Segundo-Hernández;Juan Manuel Montero-Martínez;Ricardo de Córdoba;Javier Ferreiros;José Manuel Pardo	2012	Computer Speech & Language	10.1016/j.csl.2011.09.003	natural language processing;speech recognition;computer science;linguistics	NLP	-23.885556570877952	-71.71714271440221	133591
3daa37901fc04e0d139871e82a4d95f8fbf2b9f8	recurrent neural networks for text classification of news articles from the reuters corpus			artificial neural network;document classification;recurrent neural network	Garen Zohrab Arevian	2007				ML	-21.423886282978035	-69.4125327509736	134219
706211217834d7b073956cf93ee8319cf52958ef	combining conditional random fields and first-order logic for modeling hidden content structure in sentiment analysis	text analysis formal logic random processes;probabilitic graphical model;first order logic probabilitic graphical model conditional random fields topic model;conditional random fields;conditional random fields unlabeled documents f1 measure sentence level baseline cscrf hucrf hidden unit crf lda latent dirichlet allocation hidden nodes graphical models movie review corpus amazon corpus multiaspect rating prediction task multiaspect sentence labeling task multiaspect sentiment analysis tasks long distance dependency problem latent word level topic nodes first order logic features word level content structure semisupervised approach first order logic representation hidden content structure modeling;hidden markov models labeling markov random fields accuracy computational modeling graphical models training;first order logic;topic model	The paper develops a connection between the first-order logic representation and the content structure model in sentiment analysis applications. We propose a modified semi-supervised approach to study the word-level content structure with well-designed first-order logic features. The word-level content structure is the Conditional Random Fields (CRF) with latent word-level topic nodes. Introducing first-order logic features into our model can solve the long-distance dependency problem. The new approach is applied to two multi-aspect sentiment analysis tasks: the multi-aspect sentence labeling task and the multi-aspect rating prediction task. We use the data from Amazon corpus and movie-review corpus. We compare our method with other three hidden nodes graphical models, i.e. the Latent Dirichlet Allocation (LDA), the Hidden-Unit CRF (HUCRF), and the Content Structure using CRF (CSCRF, which is considered as our sentence-level baseline). Experimental results demonstrate that our method outperforms the sentence-level baseline by 2.1% of the F1 measure in the multi-aspect sentence labeling task, and by 2.1% of the Accuracy in the rating prediction task. Our method outperforms other two methods at most by 16.6% and 10.3% separately in the multi-aspect sentence labeling task and the rating prediction task. By using 3000 unlabeled documents, our method improves the F1-measure in the multi-aspect sentence labeling task by 8.2%, and improves the Accuracy in the rating prediction task by 3.0%, using 400 unlabeled reviews.	algorithm;baseline (configuration management);conditional random field;f1 score;first-order logic;first-order predicate;graphical model;latent dirichlet allocation;mac os x 10.3 panther;semi-supervised learning;semiconductor industry;sentiment analysis	Lei Wu;Yinghua Zhang;Wensheng Zhang;Jue Wang	2013	2013 Ninth International Conference on Natural Computation (ICNC)	10.1109/ICNC.2013.6818138	natural language processing;computer science;machine learning;pattern recognition	NLP	-19.683505275905283	-68.80337281228984	134309
d4a3ba689240897c403efb8a6df198196066bec3	document classification system based on hmm word map	hidden markov model;text classification;hidden markov models;feature extraction;multi layer perceptron;document classification;word clustering	In this article, a system based on Hidden Markov Models (HMM) for document organization is presented. The purpose of the system is the classification of a document collection in terms of document content. The system possesses a two-level hybrid connectionist architecture that comprises (i) an automatically created word map using a HMM, which functions as a feature extraction module and (ii) a supervised MLP-based classifier, which provides the final classification result. A series of experiments, which have been performed on Modern Greek text-only documents, is presented. These experiments illustrate the effectiveness of the proposed system.		Nikolaos Tsimboukakis;George Tambouratzis	2008		10.1145/1456223.1456229	speech recognition;feature extraction;computer science;machine learning;linear classifier;pattern recognition;multilayer perceptron;hidden markov model	ML	-20.340332606477897	-70.09673203428972	135226
65edf2a33e053280467a5252b3ff85c1bd8ba2aa	author profiling and aggressiveness detection in spanish tweets: mex-a3t 2018		In this paper we evaluate a set of different well know strategies for text classification at MEX-A3T: Authorship and Aggressiveness analysis in Twitter case study in Mexican Spanish 2018. The main idea is to evaluate the performance of the different strategies on Spanish classification tasks that have been successfully for English classification tasks. The strategies that we evaluate are four: 1) a Bag of Terms (BoT), 2) a Second Order Attributes (SOA) representation, 3) a Convolutional Neural Network (CNN) models and 4) a huge ensemble of n-grams at word and character level. The obtained results are in agreement with previous studies that have shown high performance for words and n-grams, and represent a very first attempt to bring these and other ideas, such as distributional representations and neural networks, to the Spanish classification tasks. The strategies presented get a better performance than the average results of other participants, demonstrating significant results for these tasks.	artificial neural network;convolutional neural network;document classification;grams;mex file;n-gram	Mario Ezra Aragón;Adrián Pastor López-Monroy	2018			profiling (computer programming);bioinformatics;computer science	NLP	-21.2997312027293	-69.53343350500212	135382
23d429bd936d0456185c76d546c070581d611a2d	summarizing dialogic arguments from social media		Online argumentative dialog is a rich source of information on popular beliefs and opinions that could be useful to companies as well as governmental or public policy agencies. Compact, easy to read, summaries of these dialogues would thus be highly valuable. A priori, it is not even clear what form such a summary should take. Previous work on summarization has primarily focused on summarizing written texts, where the notion of an abstract of the text is well defined. We collect gold standard training data consisting of five human summaries for each of 161 dialogues on the topics of Gay Marriage, Gun Control and Abortion. We present several different computational models aimed at identifying segments of the dialogues whose content should be used for the summary, using linguistic features and Word2vec features with both SVMs and Bidirectional LSTMs. We show that we can identify the most important arguments by using the dialog context with a best F-measure of 0.74 for gun control, 0.71 for gay marriage, and 0.67 for abortion.	bidirectional reflectance distribution function;charging argument;computational model;information source;social media;topic model;unsupervised learning;word2vec;dialog	Amita Misra;Shereen Oraby;Shubhangi Tandon;S. SharathT.;Pranav Anand;Marilyn A. Walker	2017	CoRR		argumentative;automatic summarization;word2vec;artificial intelligence;natural language processing;public policy;computer science;dialogic;social media;dialog box;a priori and a posteriori	NLP	-21.767338379689345	-67.8656387979452	136117
3c96658cefda66d00ae2378423d5c46b328833f3	korean named entity recognition using hmm and cotraining model	hmm	Named entity recognition is important in sophisticated information service system such as Question Answering and Text Mining since most of the answer type and text mining unit depend on the named entity type. Therefore we focus on named entity recognition model in Korean. Korean named entity recognition is difficult since each word of named entity has not specific features such as the capitalizing feature of English. It has high dependence on the large amounts of hand-labeled data and the named entity dictionary, even though these are tedious and expensive to create. In this paper, we devise HMM based named entity recognizer to consider various context models. Furthermore, we consider weakly supervised learning technique, CoTraining, to combine labeled data and unlabeled data.	data dictionary;finite-state machine;hidden markov model;named entity;question answering;supervised learning;text mining	Euisok Chung;Yi-Gyu Hwang;Myung-Gil Jang	2003		10.1145/1118935.1118956	natural language processing;speech recognition;pattern recognition;entity linking	NLP	-23.893389274643546	-72.5301356111104	136747
2fc462d0507f0b682de72f86ac6384ed7c87f0d2	multi-engine approach for named entity recognition in bengali	conference paper	This paper reports about a multi-engine approach for the development of a NER system in Bengali by combining the classifiers such as Maximum Entropy (ME), Conditional Random Field (CRF) and Support Vector Machine (SVM) with the help of weighted voting approach. The training set consists of approximately 272K wordforms, out of which 150K wordforms have been manually annotated with the four major named entity (NE) tags such as Person, Location, Organization and Miscellaneous tags. An appropriate tag conversion routine has been defined in order to convert the 122K wordforms of the IJCNLP-08 NER shared task, into the desired forms. The classifiers make use of the different contextual information of the words along with the variety of features that are helpful in predicting the various NE classes. Lexical context patterns, which are generated from an unlabeled corpus of 3 million wordforms in a semi-automatic way, have been used as the features of the classifiers in order to improve their performance. In addition, we have developed a number of techniques to post-process the output of each of the classifiers in order to reduce the errors and to improve the performance. Finally, we have applied weighted voting approach to combine the systems. Results show the effectiveness of the proposed approach with the overall average recall, precision, and f-score values of 93.98%, 90.63%, and 92.28%, respectively, which shows an improvement of 14.92% in f-score over the best performing baseline SVM based system and an improvement of 18.36% in f-score over the least performing baseline ME based system. The proposed system also outperforms the other existing Bengali NER system.	baseline (configuration management);conditional random field;experiment;f1 score;maximum entropy spectral estimation;ner model;named entity;semiconductor industry;support vector machine;tag cloud;test set;video post-processing	Asif Ekbal;Sivaji Bandyopadhyay	2008			geography;genealogy;traditional medicine;cartography	NLP	-23.08930719224712	-71.07317443549543	136758
e40734f9cfb22d1d55ac0455e4669a6f7485c25b	extracting fairness policies from legal documents		Machine Learning community is recently exploring the implications of bias and fairness with respect to the AI applications. The definition of fairness for such applications varies based on their domain of application. The policies governing the use of such machine learning system in a given context are defined by the constitutional laws of nations and regulatory policies enforced by the organizations that are involved in the usage. Fairness related laws and policies are often spread across the large documents like constitution, agreements, and organizational regulations. These legal documents have long complex sentences in order to achieve rigorousness and robustness. Automatic extraction of fairness policies, or in general, any specific kind of policies from large legal corpus can be very useful for the study of bias and fairness in the context of AI applications. We attempted to automatically extract fairness policies from publicly available law documents using two approaches based on semantic relatedness. The experiments reveal how classical Wordnetbased similarity and vector-based similarity differ in addressing this task. We have shown that similarity based on word vectors beats the classical approach with a large margin, whereas other vector representations of senses and sentences fail to even match the classical baseline. Further, we have presented thorough error analysis and reasoning to explain the results with appropriate examples from the dataset for deeper insights.	baseline (configuration management);error analysis (mathematics);experiment;fairness measure;machine learning;semantic similarity;word embedding	Rashmi Nagpal;Chetna Wadhwa;Mallika Gupta;Samiulla Shaikh;Sameep Mehta;Vikram Goyal	2018	CoRR		applications of artificial intelligence;data mining;semantic similarity;machine learning;robustness (computer science);mathematics;artificial intelligence;wordnet	ML	-21.034684800782866	-71.15865012262685	136964
7321c8c3881d82244dd8fd55b9e087b848d71136	intelligent tunisian arabic morphological analyzer		Nowadays, Internet users act deeply on Internet content through Web 2.0. They are increasingly directing Web 2.0 and so political, economic, financial and social environments all over the world and particularly in Tunisia that is suffering from environment unsteadiness since the political revolution in 2011. Thus, Web 2.0 monitoring, that requires the use of natural language processing tools, is becoming increasingly necessary. Indeed, Tunisian Web 2.0 monitoring requires the use of Tunisian Arabic processing tools seen that Tunisian Internet users are intensively using Tunisian Arabic for communication. However, Tunisian Arabic is an under-resourced language. Few contributions exist for Tunisian Arabic processing and particularly for Tunisian Arabic morphological analysis. In this case, we suggest an intelligent Tunisian Arabic morphological analyzer that extracts words morphemes and identifies lexical and grammatical labels out of context. For, our analyzer adopts rule-based approach through an expert system and calls aebWordNet and Tunisian Arabic lexical dictionary. In this paper, we present this morphological analyzer that attempts 98.4% as decision and, 58.94% and 84.41% as precision respectively for morphemes decomposition and labeling.	alloy analyzer;automatic summarization;dictionary;error detection and correction;expert system;forward error correction;logic programming;mathematical morphology;natural language processing;sequence labeling;web 2.0	Nadia B. M. Karmani;Hsan Soussou;Adel M. Alimi	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945666	arabic;morphology (linguistics);computer science;natural language processing;the internet;lexical analysis;morpheme;web 2.0;artificial intelligence	Web+IR	-23.288473074951035	-68.10072324758195	136994
6b22f5f7f3d4dd4f177dd1277eaf1e36d6e3c885	a random walk–based model for identifying semantic orientation		Automatically identifying the sentiment polarity of words is a very important task that has been used as the essential building block of many natural language processing systems such as text classification, text filtering, product review analysis, survey response analysis, and on-line discussion mining. We propose a method for identifying the sentiment polarity of words that applies a Markov random walk model to a large word relatedness graph, and produces a polarity estimate for any given word. The model can accurately and quickly assign a polarity sign and magnitude to any word. It can be used both in a semi-supervised setting where a training set of labeled words is used, and in a weakly supervised setting where only a handful of seed words is used to define the two polarity classes. The method is experimentally tested using a gold standard set of positive and negative words from the General Inquirer lexicon. We also show how our method can be used for three-way classification which identifies neutral words in addition to positive and negative words. Our experiments show that the proposed method outperforms the state-of-the-art methods in the semi-supervised setting and is comparable to the best reported values in the weakly supervised setting. In addition, the proposed method is faster and does not need a large corpus. We also present extensions of our methods for identifying the polarity of foreign words and out-of-vocabulary words.	dictionary;document classification;experiment;lexicon;markov chain;natural language processing;online and offline;semi-supervised learning;semiconductor industry;signed number representations;single sign-on;test set;text corpus;vocabulary;wordnet	Ahmed Hassan Awadallah;Amjad Abu-Jbara;Wanchen Lu;Dragomir R. Radev	2014	Computational Linguistics	10.1162/COLI_a_00192	machine learning;pattern recognition	NLP	-23.787278919683303	-70.73196652232974	137422
5c8e18341106dc0e055ca7ae7a1ec626f3acf732	syllabs team at clef mc2 task 1: content analysis.		This paper describes the participation of the Syllabs Team in the content analysis task of the CLEF MC2 Evaluation lab. In the current state of our work, we offer preliminary solutions to first detect the language of the microblogs used within the task, then extract the named entities that will be later used to recognize Wikipedia entities and finally, detect microblogs that deal with festivals.	cluster analysis;language identification;lexicon;logic programming;named entity;preprocessor;rule-based system;wikipedia;word-sense disambiguation	Olivier Hamon;Chloé Monnin;Claude de Loupy	2017			world wide web;information retrieval;content analysis;clef;computer science	NLP	-22.95234053253231	-69.11795554455006	137576
fdcbe48669ac0da81439a69c754c7c1f84c8e40b	development of unsupervised and supervised learning systems for multilingual text categorization	supervised learning		categorization;document classification;supervised learning;unsupervised learning	Chung-Hong Lee;Hsin-Chang Yang	2007			supervised learning;computer science;natural language processing;categorization;unsupervised learning;machine learning;artificial intelligence	NLP	-22.79915975828941	-68.45255012602833	137723
0d67c566dbe25b0de0af999bd0c33ecbe830ea5e	a unified knowledge based approach for sense disambiguationm and semantic role labeling	semantic interpretation;semantic role labeling;background knowledge;knowledge base	In this paper, we present a unified knowledge based approach for sense disambiguation and semantic role labeling. Our ap proach performs both tasks through a single algorithm that matches candidate semantic interpretations to background knowledge to select the best matching candidate. We evaluate our approach on a corpus of sentences collected from various domains and show how our approach performs well on both sense disambiguation and semantic role labeling.	algorithm;semantic role labeling;text corpus;word-sense disambiguation	Peter Z. Yeh;Bruce W. Porter;Ken Barker	2006			natural language processing;semantic role labeling;knowledge base;semantic interpretation;semantic similarity;semantic computing;semeval;semantic search;computer science;artificial intelligence;data mining;information retrieval	NLP	-26.13681701112739	-70.60683146891733	138722
506f13dcb0a80df38b87a39e136cf0587918410c	from unlabelled tweets to twitter-specific opinion words	conference contribution;sentiment analysis;computer science;lexicon generation;twitter	In this article, we propose a word-level classification model for automatically generating a Twitter-specific opinion lexicon from a corpus of unlabelled tweets. The tweets from the corpus are represented by two vectors: a bag-of-words vector and a semantic vector based on word-clusters. We propose a distributional representation for words by treating them as the centroids of the tweet vectors in which they appear. The lexicon generation is conducted by training a word-level classifier using these centroids to form the instance space and a seed lexicon to label the training instances. Experimental results show that the two types of tweet vectors complement each other in a statistically significant manner and that our generated lexicon produces significant improvements for tweet-level polarity classification.	bag-of-words model;lexicon	Felipe Bravo-Marquez;Eibe Frank;Bernhard Pfahringer	2015		10.1145/2766462.2767770	natural language processing;speech recognition;computer science;pattern recognition;information retrieval;sentiment analysis	NLP	-24.266559458865576	-70.73550032639743	139144
1a4b06604a877aef42c07424a13a74248a3ee0d1	imtku textual entailment system for recognizing inference in text at ntcir-11 rite-val		In this paper, we describe the IMTKU (Information Management at TamKang University) textual entailment system for recognizing inference in text at NTCIR-11 RITE-VAL (Recognizing Inference in Text). We proposed a textual entailment system using a Statistics approach that integrate semantic features and machine learning techniques for recognizing inference in text at NTCIR-11 RITE-VAL task. We submitted 3 official runs for BC, MC subtask. In NTCIR-11 RITE-VAL task, IMTKU team achieved 0.2911 in the CT-MC subtask, 0.5275 in the CT-BC subtask; 0.2917 in the CS-MC subtask, 0.5325 in the CS-BC subtask. * myday@mail.tku.edu.tw	information management;machine learning;textual entailment;variable assembly language	Min-Yuh Day;Ya-Jung Wang;Che-Wei Hsu;En-Chun Tu;Shang-Yu Wu;Huai-Wen Hsu;Yu-An Lin;Yu-Hsuan Tai;Cheng-Chia Tsai	2013			natural language processing;computer science;pattern recognition;data mining	NLP	-22.69745104154819	-69.68505250604645	139410
07097ba7111119cec20c64d9a48a83c20341fd4c	pelesent: cross-domain polarity classification using distant supervision		The enormous amount of texts published daily by Internet users has fostered the development of methods to analyze this content in several natural language processing areas, such as sentiment analysis. The main goal of this task is to classify the polarity of a message. Even though many approaches have been proposed for sentiment analysis, some of the most successful ones rely on the availability of large annotated corpus, which is an expensive and time-consuming process. In recent years, distant supervision has been used to obtain larger datasets. So, inspired by these techniques, in this paper we extend such approaches to incorporate popular graphic symbols used in electronic messages, the emojis, in order to create a large sentiment corpus for Portuguese. Trained on almost one million tweets, several models were tested in both same domain and cross-domain corpora. Our methods obtained very competitive results in five annotated corpora from mixed domains (Twitter and product reviews), which proves the domain-independent property of such approach. In addition, our results suggest that the combination of emoticons and emojis is able to properly capture the sentiment of a message.	corpus linguistics;emoji;emoticon;machine learning;natural language processing;sentiment analysis;social media;text corpus	Edilson Anselmo Corrêa Júnior;Vanessa Q. Marinho;Leandro Borges dos Santos;Thales Felipe Costa Bertaglia;Marcos Vinícius Treviso;Henrico Bertini Brum	2017	2017 Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2017.45	sentiment analysis;artificial intelligence;natural language processing;the internet;computer science;portuguese	NLP	-23.0246341716664	-67.69462829393864	139671
9f73b2bc5ce719d8f38a2803556d691b9cad0e21	clustering for unsupervised relation identification	hierarchical clustering;cluster algorithm;information extraction;unsupervised relation identification;evaluation metric;clustering;feature extraction;relation learning;relational learning	"""Unsupervised Relation Identification is the task of automatically discovering interesting relations between entities in a large text corpora. Relations are identified by clustering the frequently co-occurring pairs of entities in such a way that pairs occurring in similar contexts end up belonging to the same clusters. In this paper we compare several clustering setups, some of them novel and others already tried. The setups include feature extraction and selection methods and clustering algorithms. In order to do the comparison, we develop a clustering evaluation metric, specifically adapted for the relation identification task. Our experiments demonstrate significant superiority of the single-linkage hierarchical clustering with the novel threshold selection technique over the other tested clustering algorithms. Also, the experiments indicate that for successful relation identification it is important to use rich complex features of two kinds: features that test both relation slots together (""""relation features""""), and features that test only one slot each (""""entity features""""). We have found that using both kinds of features with the best of the algorithms produces very high-precision results, significantly improving over the previous work."""	algorithm;cluster analysis;entity;experiment;feature extraction;feature selection;general-purpose markup language;general-purpose modeling;hierarchical clustering;high-availability cluster;linkage (software);maximal set;named-entity recognition;text corpus;turing;unsupervised learning	Benjamin Rozenfeld;Ronen Feldman	2007		10.1145/1321440.1321499	correlation clustering;constrained clustering;fuzzy clustering;feature extraction;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;information extraction;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	NLP	-25.181345825811153	-66.71262583638274	139877
63fe1fcc9f89bfc659043c650d2ea98dc5fee6c0	joint sentiment and emotion classification with integer linear programming	会议论文	As two foundational tasks in sentiment analysis, sentiment classification and emotion classification have been considered separately and studied independently in the literature. In this paper, we put forward an Integer Linear Programming (ILP)-driven joint learning approach to leveraging the relationship between these two tasks. Empirical study verifies the appropriateness and effectiveness of our proposed approach to joint sentiment and emotion classification.	integer programming;linear programming;norm (social);sentiment analysis	Rong Wang;Shoushan Li;Guodong Zhou;Hanxiao Shi	2015		10.1007/978-3-319-22324-7_25	speech recognition;computer science;machine learning;pattern recognition	NLP	-19.35126075199702	-66.6538299863524	140194
fc5df05796edb1cc97b867e1b6dd17c8f762c2e4	node: a benchmark of natural language arguments	computational linguistics abstract bipolar argumentation corpora;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	In the latest years, natural models of argumentation and argument mining are becoming more and more important topics in the argumentation community. Given this tendency, there is the need to produce standard datasets on which natural language approaches to argumentation can be evaluated. In this paper, we present NoDE, a benchmark of natural language arguments composed of three datasets, built from different textual sources and annotated highlighting positive and negative connections between arguments.	benchmark (computing);natural language	Elena Cabrio;Serena Villata	2014		10.3233/978-1-61499-436-7-449	natural language processing;computer science;artificial intelligence;linguistics;algorithm	AI	-26.365688155169007	-72.49388668483694	141142
e67d51baa9951ac8227e4e176422badd6c2f0836	multi-layer ensembling techniques for multilingual intent classification		In this paper we determine how multi-layer ensembling improves performance on multilingual intent classification. We develop a novel multi-layer ensembling approach that ensembles both different model initializations and different model architectures. We also introduce a new banking domain dataset and compare results against the standard ATIS dataset and the Chinese SMP2017 dataset to determine ensembling performance in multilingual and multi-domain contexts. We run ensemble experiments across all three datasets, and conclude that ensembling provides significant performance increases, and that multi-layer ensembling is a no-risk way to improve performance on intent classification. We also find that a diverse ensemble of simple models can reach perform comparable to much more sophisticated state-of-the-art models. Our best F1 scores on ATIS, Banking, and SMP are 97.54%, 91.79%, and 93.55% respectively, which compare well with the state-of-the-art on ATIS and best submission to the SMP2017 competition. The total ensembling performance increases we achieve are 0.23%, 1.96%, and 4.04% F1 respectively.	automatic transmitter identification system (television);bi-directional text;ensemble learning;experiment;layer (electronics);natural language processing;network architecture;random neural network;symmetric multiprocessing	Charles Costello;Ruixi Lin;Vishwas Mruthyunjaya;Bettina Bolla;Charles Jankowski	2018	CoRR		machine learning;artificial intelligence;computer science	NLP	-19.707187422429797	-72.62329191438423	141181
127b77e5fb6df84fe2aabdeb72a2591aab6891d9	reddit: a gold mine for personality prediction		Automated personality prediction from social media is gaining increasing attention in natural language processing and social sciences communities. However, due to high labeling costs and privacy issues, the few publicly available datasets are of limited size and low topic diversity. We address this problem by introducing a large-scale dataset derived from Reddit, a source so far overlooked for personality prediction. The dataset is labeled with Myers-Briggs Type Indicators (MBTI) and comes with a rich set of features for more than 9k users. We carry out a preliminary feature analysis, revealing marked differences between the MBTI dimensions and poles. Furthermore, we use the dataset to train and evaluate benchmark personality prediction models, achieving macro F1-scores between 67% and 82% on the individual dimensions and 82% accuracy for exact or one-off accurate type prediction. These results are encouraging and comparable with the reliability of standardized tests.	antivirus gold;baseline (configuration management);benchmark (computing);british national vegetation classification;deep learning;f1 score;interaction;n-gram;natural language processing;precomputation;privacy;profiling (computer programming);social media	Matej Gjurkovic;Jan Snajder	2018			social psychology;personality;psychology	NLP	-20.188559864738984	-68.2653453933701	141315
72309c5e8ece97f98604004c1e0161550bab4c50	named entity recognition from spontaneous open-domain speech	system evaluation;named entity recognition;machine learning;spontaneous speech;named entity	This paper presents an analysis of named entity recognition and classification in spontaneous speech transcripts. We annotated a significant fraction of the Switchboard corpus with six named entity classes and investigated a battery of machine learning models that include lexical, syntactic, and semantic attributes. The best recognition and classification model obtains promising results, approaching within 5% a system evaluated on clean textual data.	machine learning;named entity;spontaneous order;telephone switchboard;text corpus	Mihai Surdeanu;Jordi Turmo;Eli Comelles	2005			computer science;machine learning;entity linking	NLP	-23.30337635181427	-72.02863022955225	141749
0426dc16ca4378d58d2cfe5c3781146e6856f044	tri-layer-cluster generation model for activity prediction	pattern clustering;document handling;testing training predictive models accuracy computational modeling conferences electronic mail;wplda lda model tri layer cluster activity prediction;stanford classifier tri layer cluster generation model activity prediction topic layer activity layer word layer supervised topic model latent dirichlet allocation word pair generation lda model topic specific activity distribution;pattern clustering document handling pattern classification;pattern classification;activity prediction;lda model;tri layer cluster;wplda	We propose a topic model capable of generating tri-layer clusters, each of which is composed of a topic layer, an activity layer and a word layer. The objective is to better predict activities involved in documents by considering general topics of the activities for clustering. The proposed model is a supervised topic model based on the Latent Dirichlet Allocation (LDA). As a follow-up study of word-pair generation LDA (wpLDA) model, the model introduces the topic-specific activity distribution as an external input, with an activity node inserted into the main generation thread. In addition, we refer to D. Ramage et al.'s one-to-one correspondence to directly learn word-activity tags. An experiment was conducted to prove the feasibility of this model. We chose ten top-listed activities from the wish clusters obtained by the previous wpLDA research, and used each as the key words to extract thirty tweets for training and five for testing, respectively, tagging the tweets with the corresponding activities. By applying the proposed model, we obtained the expected tri-layer clusters in the training phase. Then, in the testing phase, we utilized the activity-specific word distribution derived from the training results to learn the activities of the testing documents. The Stanford Classifier was put forward as the control group, and the activity prediction accuracy demonstrates that the proposed model exhibits the superiority in multi-activity prediction.	cluster analysis;experiment;latent dirichlet allocation;one-to-one (data model);topic model;triangular function	Dandan Zhu;Yusuke Fukazawa;Jun Ota	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.51	computer science;machine learning;pattern recognition;data mining	AI	-20.212525455997106	-68.73477580069311	141809
9c69d97e809e6c97f2b6d41c8f85f0f2960f4806	retuyt in tass 2017: sentiment analysis for spanish tweets using svm and cnn		This article presents classifiers based on SVM and Convolutional Neural Networks (CNN) for the TASS 2017 challenge on tweets sentiment analysis. The classifier with the best performance in general uses a combination of SVM and CNN. The use of word embeddings was particularly useful for improving the classifiers performance.	convolutional neural network;sentiment analysis;tass times in tonetown;word embedding	Aiala Rosá;Luis Chiruzzo;Mathías Etcheverry;Santiago Castro	2017	CoRR		machine learning;sentiment analysis;natural language processing;artificial intelligence;convolutional neural network;support vector machine;computer science;classifier (linguistics)	NLP	-20.4069939316933	-70.64988292826477	141828
b995e0cb673f6a99c7f1d9c9365e31f282195cb0	detecting hedges scope based on phrase structures and dependency structures	text analysis bioinformatics feature extraction knowledge based systems learning artificial intelligence natural language processing random processes;biomedical text hedge scope detection phrase structure dependency structure unreliable information uncertain information machine learning rule based method phrase based system conditional random field based model feature vectors scope tags conll 2010 biological corpus speculative language science text;rule based;text analysis;manual rules phrase structures dependency structures conditional random field;strontium;feature vector;machine learning;feature extraction;random processes;conditional random field;phrase structures;dependence structure;manual rules;dependency structures;learning artificial intelligence;natural language processing;knowledge based systems;bioinformatics	To distinguish facts from unreliable or uncertain information, hedges have to be identified. This paper presents an approach to hedges scope detection based on phrase structures and dependency structures. First, phrase structures and dependency structures are used for hedges scope detection respectively. Phrase structures are adapted as important features for hedges scope detection by a machining learning method. Dependency structures are used to detect hedges scope by a rule-based method. Then, the phrase-based system and the dependency-based system are combined by a Conditional Random Field (CRF)-based model, which simply extends the feature vectors with the scope tags generated by the two individual phrase-based and dependency-based systems. Experiments on the CoNLL-2010 biological corpus show that our model achieves F-scores of 55.47% on hedges scope detection based on phrase structures using machine learning and 55.67% based on dependency structures using manual rules, and 58.97% based on dependency structures and phrase structures using our combined method. The analysis results show that phrase structures and dependency structures are both effective for hedges scope detection and their combination can improve the scope detection performance further.	conditional random field;feature vector;logic programming;machine learning;sensor	Huiwei Zhou;Xiaoyan Li;Degen Huang;Yuansheng Yang;Jianjun Ma	2011	2011 7th International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2011.6138235	natural language processing;speech recognition;computer science;pattern recognition	SE	-24.59105740191772	-70.46643492995439	142151
04e36563b770d3c8c76d0a164f8926557f0815c4	learning to extract relations from the web using minimal supervision	web documents;relation extraction;named entity	We present a new approach to relation extraction that requires only a handful of training examples. Given a few pairs of named entities known to exhibit or not exhibit a particular relation, bags of sentences containing the pairs are extracted from the web. We extend an existing relation extraction method to handle this weaker form of supervision, and present experimental results demonstrating that our approach can reliably extract relations from web documents.	named entity;relationship extraction;web page;world wide web	Razvan C. Bunescu;Raymond J. Mooney	2007			natural language processing;relationship extraction;computer science;data mining;world wide web;information retrieval	NLP	-25.965610405052647	-66.33905456230032	142251
4c8041265cafcca2c1bede49be3c261840b652f2	extracting causal relations among complex events in natural science literature		Causal relation extraction is the task of identifying and extracting the causal relations occurring in a text. We present an approach that is especially suitable for extracting relations between complex events – which are assumed to be already identified – as found in natural science literature, supporting literature-based knowledge discovery. The approach is based on supervised learning, exploiting a wide range of linguistic features. Experimental results indicate that even with a limited amount of training data, reasonable accuracy can be obtained by using a pipeline of classifiers, optimising hyper-parameters, down-weighting negative instances and applying feature selection methods.	causal filter	Biswanath Barik;Erwin Marsi;Pinar Öztürk	2017		10.1007/978-3-319-59569-6_13	information extraction;knowledge extraction;data mining;supervised learning;computer science;relationship extraction;feature selection;training set;natural science;text mining	NLP	-19.978798422621896	-66.44615608356969	142253
9215c71c861cc659838f87fa03c8ffda846d55b6	iitpsemeval: sentiment discovery from 140 characters		This paper presents an overview of the system developed and submitted as a part of our participation to the SemEval-2015 Task 10 that deals with Sentiment Analysis in Twitter. We build a Support Vector Machine (SVM) based supervised learning model for Subtask A (term level task) and Subtask B (message level task). We also participate in Subtask E viz., determining degree of polarity, and build a very simple system by employing the available lexical resources. Experiments with the 2015 official datasets show F1 scores of 81.31% and 58.80% for Task A and Task B, respectively. For Subtask E, our model achieves a score of 0.413 on Kendal’s Tau metric.	feature vector;grams;kendall tau distance;lexicon;n-gram;semeval;sentiment analysis;smoothing;sparse matrix;supervised learning;support vector machine;task computing;viz: the computer game;word-sense disambiguation	Ayush Kumar;Vamsi Krishna;Asif Ekbal	2015			natural language processing;speech recognition;computer science;data science;machine learning;data mining	NLP	-22.236784348952057	-69.65155723454673	142625
444c2fdb626a4de1aaca396bb44a97cd5b116246	text categorization based on semantic cluster-hidden markov models	hidden markov models;semantic cluster;text categorization;text serialization	A new text categorization algorithm based on Hidden Markov Model is proposed. At first, semantic clusters are obtained from training data set. The association between semantic clusters is modeled as Hidden Markov Model. Combining with the forward algorithm, the strategy could realize automatic text categorization. From the simulation, the proposed text categorization algorithm is better in categorization precision. Moreover, it works well independent of the number of considered categories compared to the priori art algorithms.	categorization;document classification;hidden markov model;markov chain	Fang Li;Tao Dong	2013		10.1007/978-3-642-38715-9_24	natural language processing;semantic computing;explicit semantic analysis;machine learning;pattern recognition;categorization	NLP	-23.566414765789908	-72.64691169996915	142959
8047c15466af6ffe4ffe68325a78a9b13744e967	identification of opinion holders	判解;opinion mining;lee;lun wei;chia ying;法律詞典;論文;opinion holder identification;大陸法學;法規;ku;月旦法學;法律題庫;裁判時報;conditional random field;月旦知識庫;法學資料庫;support vector machine;hsin hsi;tssci;教學;chen	Opinion holder identification aims to extract entities that express opinions in sentences. In this paper, opinion holder identification is divided into two subtasks: author’s opinion recognition and opinion holder labeling. Support vector machine (SVM) is adopted to recognize author’s opinions, and conditional random field algorithm (CRF) is utilized to label opinion holders. New features are proposed for both methods. Our method achieves an f-score of 0.734 in the NTCIR7 MOAT task on the Traditional Chinese side, which is the best performance among results of machine learning methods proposed by participants, and also it is close to the best performance of this task. In addition, inconsistent annotations of opinion holders are analyzed, along with the best way to utilize the training instances with inconsistent annotations being proposed.	algorithm;co-training;conditional random field;f1 score;heuristic;lexicon;machine learning;named entity;named-entity recognition;parsing;sequence labeling;support vector machine	Lun-Wei Ku;Chia-Ying Lee;Hsin-Hsi Chen	2009	IJCLCLP		speech recognition;engineering;artificial intelligence;data mining	NLP	-22.604862061388367	-69.82337182280465	143258
33316dd85eaaa6f0dbddefa823479435caaaaed6	sentiment lexicon generation for an under-resourced language		Sentiment analysis and opinion mining are actively explored nowadays. One of the most important resources for the sentiment analysis task is sentiment lexicon. This paper presents our study in building domain-specific sentiment lexicon for Indonesian language. Our main contributions are (1) methods to expand sentiment lexicon using sentiment patterns and (2) a technique to classify the polarity of a word using the sentiment score. Our method is able to generate sentiment lexicon automatically by using a small seed of sentiment words, user reviews, and part-ofspeech (POS) tagger. We develop the lexicon for Indonesian language using a set of seed words translated from English sentiment lexicon and expand them using sentiment patterns found in the user reviews. Our results show that the proposed method can generate additional lexicon with sentiment accuracy of 77.7%.	brill tagger;lexicon;part-of-speech tagging;point of sale;sentiment analysis;user review	Clara Vania;Moh. Ibrahim;Mirna Adriani	2014	Int. J. Comput. Linguistics Appl.		sentiment analysis;linguistics;natural language processing;artificial intelligence;lexicon;computer science	NLP	-22.94128924440153	-67.56481561635022	143482
0cb58e47f6ccbfbeb28af2f1a20ac94b0814b307	nuig-unlp at semeval-2016 task 1: soft alignment and deep learning for semantic textual similarity		We present a multi-feature system for computing the semantic similarity between two sentences. We introduce the use of soft alignment for computing text similarity, and also evaluate different methods to produce it. The main features used by our system are based on alignment and Explicit Semantic Analysis. Our system was above the median scores for 4 out of the 5 datasets at SemEval 2016 STS Task 1.	algorithm;deep learning;explicit semantic analysis;semeval;semantic similarity	John P. McCrae;Kartik Asooja;Nitish Aggarwal;Paul Buitelaar	2016			semantic similarity;natural language processing;computer science;information retrieval;semeval;artificial intelligence;deep learning	NLP	-21.99704098540975	-72.32047373479932	143739
c5c816cbf93235769551e0463079a14ee4902625	product feature mining with nominal semantic structure	unsupervised learning;opinion mining;pattern clustering;product feature mining;opinionated pair stream;supervised classification;trees mathematics;data mining;product feature extraction semantic structure dependency parsing;syntactic dependency knowledge;nominal semantic structure;user generated content analysis;matrix decomposition;factorization method;feature extraction;dependency tree;co clustering approach product feature mining nominal semantic structure opinion mining user generated content analysis unsupervised learning syntactic dependency knowledge dependency tree semantic structure parsing opinionated pair stream factorization method;product feature extraction;semantic structure parsing;dependency parsing;user generated content;semantic structure;unsupervised learning data mining matrix decomposition pattern clustering trees mathematics;co clustering approach	Opinion mining is of great significance in the analysis of user generated content. While there is some progress in supervised classification of opinion, the unsupervised learning of product features has drawn less attention. Unlike previous approaches based on basic syntactic pattern, our product feature mining utilizes syntactic dependency knowledge in a novel way by discriminating nominal and non-nominal terms. A nominal semantic structure will be parsed based on a dependency tree together with our model treating non-nominal terms as the semantic neighbors of the associated nominal terms. The semantic structure parsing will produce an opinionated pair stream with couples of nominal terms and its semantic neighbors, based on which fine-grained product features can be obtained by co-clustering approach via factorization method. Evaluation on average cluster entropies, perplexity and manual evaluation demonstrated advantage of our model. Product features highly cohesive in fine-grain are extracted automatically.	biclustering;cluster analysis;nominal terms (computer science);parsing;perplexity;sentiment analysis;supervised learning;unsupervised learning;user-generated content	Tian-Jie Zhan;Chun-hung Li	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.121	natural language processing;unsupervised learning;semantic computing;feature extraction;computer science;machine learning;pattern recognition;data mining;matrix decomposition;user-generated content;dependency grammar	NLP	-20.43061199590777	-67.97586677042908	143953
2fa2253bcddc2a31d3bfddbfa05afbd9218e0893	ju cse nlp $@$ semeval 2017 task 7: employing rules to detect and interpret english puns		The problem of detection and interpretation of English puns falls under the area of word sense disambiguation in natural language processing, which deals with the sense of a word used in a sentence from the readers’ perspective. We have tried to design a system to identify puns from a sentence by developing a cyclic dependency– based system which is implemented based on some rules which are actually statistical inferences taken from a set of random data collected from the Web.	natural language processing;randomness;semeval;word sense;word-sense disambiguation;world wide web	Aniket Pramanick;Dipankar Das	2017		10.18653/v1/S17-2073	computer science;natural language processing;artificial intelligence;semeval;pun;speech recognition	NLP	-22.692828211675593	-68.16849742152897	143991
19eba94485350d5db23f078e03a8997c7ebb6ed2	mi&t lab at semeval-2017 task 4: an integrated training method of word vector for sentiment classification		A CNN method for sentiment classification task in Task 4A of SemEval 2017 is presented. To solve the problem of word2vec training word vector slowly, a method of training word vector by integrating word2vec and Convolutional Neural Network (CNN) is proposed. This training method not only improves the training speed of word2vec, but also makes the word vector more effective for the target task. Furthermore, the word2vec adopts a full connection between the input layer and the projection layer of the Continuous Bag-of-Words (CBOW) for acquiring the semantic information of the original sentence.	bag-of-words model;convolutional neural network;semeval;teaching method;word embedding;word2vec	Jingjing Zhao;Yan Yang;Bing Xu	2017		10.18653/v1/S17-2114		NLP	-19.327007916575457	-71.49841640475346	144102
225912557c87fd594ff20d32d0baaa89e2b18959	the role of algorithm bias vs information source in learning algorithms for morphosyntactic disambiguation	speech tagging;algorithm bias vs information;disambiguation problem;different source;large class;morphosyntactic disambiguation;useful benchmark problem;memory-based learning;different method;natural language processing;information source;maximum entropy model	Morphosyntactic Disambiguation (Part of Speech tagging) is a useful benchmark problem for system comparison because it is typical for a large class of Natural Language Processing (NLP) problems that can be defined as disambiguation in local context. This paper adds to the literature on the systematic and objective evaluation of different methods to automatically learn this type of disambiguation problem. We systematically compare two inductive learning approaches to tagging: MXP O S T (based on maximum entropy modeling) and MBT (based on memory-based learning). We investigate the effect of different sources of information on accuracy when comparing the two approaches under the same conditions. Results indicate that earlier observed differences in accuracy can be at tr ibuted largely to differences in information sources used, rather than to algorithm bias. 1 C o m p a r i n g T a g g e r s Morphosyntactic Disambiguation (Part of Speech tagging) is concerned with assigning morpho-syntactic categories (tags) to words in a sentence, typically by employing a complex interaction of contextual and lexical clues to trigger the correct disambiguation. As a contextual clue, we might for instance assume that it is unlikely that a verb will follow an article. As a lexical (morphological) clue, we might assign a word like better the tag comparative if we notice that its suffix is er. POS tagging is a useful first step in text analysis, but also a prototypical benchmark task for the type of disambiguation problems which is paramount in natural language processing: assigning one of a set of possible labels to a linguistic object given different information sources derived from the linguistic context. Techniques working well in the area of POS tagging may also work well in a large range of other NLP problems such as word sense disambiguation and discourse segmentation, when reliable annotated corpora providing good predictive information sources for these problems become	algorithm;benchmark (computing);boston mxp;dual total correlation;information source;instance-based learning;model-based testing;natural language processing;part-of-speech tagging;tag cloud;text corpus;word sense;word-sense disambiguation	Guy De Pauw;Walter Daelemans	2000			natural language processing;computer science;machine learning;pattern recognition	NLP	-25.22710467962645	-71.97482899646648	144345
27c9085316bf75d41dbc93b58ff9a7879bdc930b	mixtures of biased sentiment analysers	62 07;60 08;mixture model;62h30;sentiment analysis;bias modelling;90 08;em algorithm;crowdsourcing;62 04	Modelling bias is an important consideration when dealing with inexpert annotations. We are concerned with training a classifier to perform sentiment analysis on news media articles, some of which have been manually annotated by volunteers. The classifier is trained on the words in the articles and then applied to non-annotated articles. In previous work we found that a joint estimation of the annotator biases and the classifier parameters performed better than estimation of the biases followed by training of the classifier. An important question follows from this result: can the annotators be usefully clustered into either predetermined or data-driven clusters, based on their biases? If so, such a clustering could be used to select, drop or otherwise categorise the annotators in a crowdsourcing task. This paper presents work on fitting a finite mixture model to the annotators’ bias. We develop a model and an algorithm and demonstrate its properties on simulated data. We then demonstrate the clustering that exists in our motivating dataset, namely the analysis of potentially economically relevant news articles from Irish online news sources.	approximation;bayesian information criterion;biclustering;categorization;cluster analysis;cobham's thesis;crowdsourcing;expectation–maximization algorithm;ground truth;maxima and minima;mixture model;oracle machine;probabilistic database;random-access memory;relevance;sentiment analysis;simulation;spamming;variational principle	Michael Salter-Townshend;Thomas Brendan Murphy	2014	Adv. Data Analysis and Classification	10.1007/s11634-013-0150-6	expectation–maximization algorithm;computer science;data science;machine learning;mixture model;data mining;crowdsourcing;sentiment analysis;statistics	ML	-20.101633864670966	-67.3888403673569	144460
2fd19ec3e697c5e6291aaf6825bb1c097fa1d3bd	multilingual sms-based author profiling: data and methods		In the recent years, many benchmark author profiling corpora have been developed for various genres including Twitter, social media, blogs, hotel reviews and e-mail, etc. However, no such standard evaluation resource has been developed for Short Messaging Service (SMS), a popular medium of communication, which is very useful for author profiling. The primary aim of this study is to develop a large multilingual (English and Roman Urdu) benchmark SMS-based author profiling corpus. The proposed corpus contains 810 author profiles, wherein each profile consists of an aggregation of SMS messages as a single document of an author, along with seven demographic traits associated with each author profile: gender, age, native language, native city, qualification, occupation and personality type (introvert/extrovert). The secondary aims of this study include the following: (1) annotating the proposed corpus for code-switching annotations at the lexical level (approximately 0.69 million tokens are manually annotated for code-switching) and (2) applying the stylometry-based method (groups of sixty-four features) and the content-based method (twelve features) for gender identification in order to demonstrate how our proposed corpus can be used for the development and evaluation of various author profiling methods. The results show that the content-based character 5-gram feature outperformed all the other features by obtaining the accuracy score of 0.975 and F 1 score of 0.947 for gender identification while using the entire corpus. Furthermore, our proposed corpora (SMS–AP–18 and code-switched SMS–AP–18) are freely and publicly available for research purpose.	profiling (computer programming)	Mehwish Fatima;Saba Anwar;Amna Naveed;Waqas Arshad	2018	Natural Language Engineering	10.1017/S1351324918000244	data mining;personality type;profiling (computer programming);short message service;computer science;first language;stylometry;social media	DB	-22.546871808026044	-69.09488762743024	144511
ae24f6306848d3b6cd084ffc1e1dbd11e185ff46	distributional inclusion vector embedding for unsupervised hypernymy detection		Modeling hypernymy, such as poodle is-a dog, is an important generalization aid to many NLP tasks, such as entailment, relation extraction, and question answering. Supervised learning from labeled hypernym sources, such as WordNet, limit the coverage of these models, which can be addressed by learning hypernyms from unlabeled text. Existing unsupervised methods either do not scale to large vocabularies or yield unacceptably poor accuracy. This paper introduces {it distributional inclusion vector embedding (DIVE)}, a simple-to-implement unsupervised method of hypernym discovery via per-word non-negative vector embeddings which preserve the inclusion property of word contexts. In experimental evaluations more comprehensive than any previous literature of which we are aware---evaluating on 11 datasets using multiple existing as well as newly proposed scoring functions---we find that our method provides up to double the precision of previous unsupervised methods, and the highest average performance, using a much more compact word representation, and yielding many new state-of-the-art results. In addition, the meaning of each dimension in DIVE is interpretable, which leads to a novel approach on word sense disambiguation as another promising application of DIVE.	authorization;baseline (configuration management);best, worst and average case;data science;experiment;grams;ibm notes;information retrieval;is-a;knowledge base;mean squared error;n-gram;natural language processing;non-negative matrix factorization;poodle;performance;question answering;relationship extraction;scalability;scoring functions for docking;skip list;supervised learning;unsupervised learning;vocabulary;wordnet	Haw-Shiuan Chang;ZiYun Wang;Luke Vilnis;Andrew McCallum	2018			machine learning;supervised learning;artificial intelligence;relationship extraction;question answering;word-sense disambiguation;logical consequence;wordnet;embedding;computer science;non-negative matrix factorization	NLP	-22.47293719483218	-72.53972254399041	144679
94b8f47e7bbcfb7be3345265c2b9b6092b0d1203	word sense disambiguation using wikipedia		This paper describes explorations in word sense disambiguation using Wikipedia as a source of sense annotations. Through experiments on four different languages, we show that the Wikipedia-based sense annotations are reliable and can be used to construct accurate sense classifiers.	wikipedia;word sense;word-sense disambiguation	Bharath Dandala;Rada Mihalcea;Razvan C. Bunescu	2013		10.1007/978-3-642-35085-6_9	semeval	NLP	-26.271008730315753	-71.27969304840597	144770
670d80ca8a3b9bf50c66e29fdc5a65b5206518c2	morphological analyzer for agglutinative languages using machine learning approaches	text analysis knowledge based systems learning artificial intelligence natural language processing;rule based approach;language use;learning;support vector machines;sequence labeling;rule based;training;text analysis;speech;segmentation;tamil;complex agglutinative natural languages;morphological analyzer;machine learning;natural language;morphology structure;morphological analysis;machine learning natural languages labeling search engines computer networks speech synthesis speech analysis communications technology paper technology morphology;sequence labeling machine learning morhological analyzer morpheme segmentation;tamil morphological analyzer machine learning complex agglutinative natural languages morphology structure rule based approach sequence labeling sequence training kernel method;morpheme;kernel method;learning artificial intelligence;morhological analyzer;natural language processing;sequence training;knowledge based systems;labeling;tagging	This paper is based on morphological analyzer using machine learning approach for complex agglutinative natural languages. Morphological analysis is concerned with retrieving the structure, the syntactic and morphological properties or the meaning of a morphologically complex word. The morphology structure of agglutinative language is unique and capturing its complexity in a machine analyzable and generatable format is a challenging job. Generally rule based approaches are used for building morphological analyzer system. In rule based approaches what works in the forward direction may not work in the backward direction. This new and state of the art machine learning approach based on sequence labeling and training by kernel methods captures the non-linear relationships in the different aspect of morphological features of natural languages in a better and simpler way. The overall accuracy obtained for the morphologically rich agglutinative language (Tamil) was really encouraging.	alloy analyzer;dictionary;kernel method;machine learning;mathematical morphology;natural language;nonlinear system;sequence labeling	V. Dhanalakshmi;M. Anand Kumar;R. U. Rekha;C. Arun Kumar;K. P. Soman;Sankaran Rajendran	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.184	rule-based system;natural language processing;sequence labeling;morpheme;kernel method;text mining;speech recognition;morphological analysis;computer science;speech;artificial intelligence;knowledge-based systems;machine learning;pattern recognition;natural language;segmentation;tamil	AI	-24.27866787575019	-66.6059456614134	145015
58413faf1917145147188c30c7c53c025c5ea7af	deeppatent: patent classification with convolutional neural networks and word embedding	patent classification;text classification;convolutional neural network;machine learning;word embedding;94-02;y	Patent classification is an essential task in patent information management and patent knowledge mining. However, this task is still largely done manually due to the unsatisfactory performance of current algorithms. Recently, deep learning methods such as convolutional neural networks (CNN) have led to great progress in image processing, voice recognition, and speech recognition, which has yet to be applied to patent classification. We proposed DeepPatent, a deep learning algorithm for patent classification based on CNN and word vector embedding. We evaluated the algorithm on the standard patent classification benchmark dataset CLEF-IP and compared it with other algorithms in the CLEF-IP competition. Experiments showed that DeepPatent with automatic feature extraction achieved a classification precision of 83.98%, which outperformed all the existing algorithms that used the same information for training. Its performance is better than the state-of-art patent classifier with a precision of 83.50%, whose performance is, however, based on 4000 characters from the description section and a lot of feature engineering while DeepPatent only used the title and abstract information. DeepPatent is further tested on USPTO-2M, a patent classification benchmark data set that we contributed with 2,000,147 records after data cleaning of 2,679,443 USA raw utility patent documents in 637 categories at the subclass level. Our algorithms achieved a precision of 73.88%.	algorithm;artificial neural network;benchmark (computing);body of uterus;categories;convolutional neural network;data mining;deep learning;feature engineering;feature extraction;graphics processing unit;greater;image processing;information management;largest;legal patent;machine learning;natural science disciplines;neural network simulation;personality character;scientific publication;silo (dataset);software patent;speech recognition;sputter cleaning;text corpus;titan;unbalanced circuit;weatherstar;word embedding;newton per square metre;subclass	Shaobo Li;Jie Hu;Yuxin Cui;Jianjun Hu	2018	Scientometrics	10.1007/s11192-018-2905-5	image processing;patent classification;word embedding;data mining;convolutional neural network;feature extraction;deep learning;machine learning;classifier (linguistics);feature engineering;artificial intelligence;computer science	ML	-20.360441344922446	-70.51589724262202	145826
3d604f88215a11a2ae44be78791aa9109ee98da3	sentipers: a sentiment analysis corpus for persian		Sentiment Analysis (SA) is a major field of study in natural language processing, computational linguistics and information retrieval. Interest in SA has been constantly growing in both academia and industry over the recent years. Moreover, there is an increasing need for generating appropriate resources and datasets in particular for low resource languages including Persian. These datasets play an important role in designing and developing appropriate opinion mining platforms using supervised, semi-supervised or unsupervised methods. In this paper, we outline the entire process of developing a manually annotated sentiment corpus, SentiPers, which covers formal and informal written contemporary Persian. To the best of our knowledge, SentiPers is a unique sentiment corpus with such a rich annotation in three different levels including document-level, sentence-level, and entity/aspect-level for Persian. The corpus contains more than 26,000 sentences of users’ opinions from digital product domain and benefits from special characteristics such as quantifying the positiveness or negativity of an opinion through assigning a number within a specific range to any given sentence. Furthermore, we present statistics on various components of our corpus as well as studying the inter-annotator agreement among the annotators. Finally, some of the challenges that we faced during the annotation process will be discussed as well.	algorithm;computational linguistics;deep learning;document;information retrieval;inter-rater reliability;machine learning;natural language processing;negativity (quantum mechanics);semi-supervised learning;semiconductor industry;sentiment analysis;supervised learning;text corpus	Pedram Hosseini;Ali Ahmadian Ramaki;Hassan Maleki;Mansoureh Anvari;Seyed Abolghasem Mirroshandel	2018	CoRR		artificial intelligence;natural language processing;sentiment analysis;computational linguistics;persian;computer science;negativity effect;sentence;annotation	NLP	-20.925315981120626	-71.38785952730386	145951
bcd0facdafa2bba6e8f2abde989ff5cb18d3ab9e	semi-supervised dependency parsing using generalized tri-training	dependency parsing	Martins et al. (2008) presented what to the best of our knowledge still ranks as the best overall result on the CONLLX Shared Task datasets. The paper shows how triads of stacked dependency parsers described in Martins et al. (2008) can label unlabeled data for each other in a way similar to co-training and produce end parsers that are significantly better than any of the stacked input parsers. We evaluate our system on five datasets from the CONLL-X Shared Task and obtain 10–20% error reductions, incl. the best reported results on four of them. We compare our approach to other semisupervised learning algorithms.	algorithm;co-training;machine learning;parsing;semi-supervised learning;semiconductor industry;triangular function;usb attached scsi	Anders Søgaard;Christian Rishøj	2010			natural language processing;computer science;data mining;database;linguistics;programming language;dependency grammar	NLP	-22.22155954274094	-71.90315433734253	146186
4d6ea3ac792865e97da73d5856b417d31256e9f9	biomedical event annotation with crfs and precision grammars	computer software and services not elsewhere classified;natural language processing;conference proceeding	This work describes a system for the tasks of identifying events in biomedical text and marking those that are speculative or negated. The architecture of the system relies on both Machine Learning (ML) approaches and hand-coded precision grammars. We submitted the output of our approach to the event extraction shared task at BioNLP 2009, where our methods suffered from low recall, although we were one of the few teams to provide answers for task 3.		Andrew MacKinlay;David Martínez;Timothy Baldwin	2009		10.3115/1572340.1572351	natural language processing;computer science;theoretical computer science;data mining	NLP	-24.16205658191249	-70.0661418675009	146198
161f87acaa165f86380877ff2ff625933d919e57	robust lexical features for improved neural network named-entity recognition		Neural network approaches to Named-Entity Recognition reduce the need for carefully handcrafted features. While some features do remain in state-of-the-art systems, lexical features have been mostly discarded, with the exception of gazetteers. In this work, we show that this is unfair: lexical features are actually quite useful. We propose to embed words and entity types into a lowdimensional vector space we train from annotated data produced by distant supervision thanks to Wikipedia. From this, we compute — offline — a feature vector representing each word. When used with a vanilla recurrent neural network model, this representation yields substantial improvements. We establish a new state-of-the-art F1 score of 87.95 on ONTONOTES 5.0, while matching state-of-the-art performance with a F1 score of 91.73 on the over-studied CONLL-2003 dataset.	artificial neural network;entity;f1 score;feature vector;graphics processing unit;named-entity recognition;network model;online and offline;recurrent neural network;titan;vocabulary;whole earth 'lectronic link;wikipedia	Abbas Ghaddar;Philippe Langlais	2018			machine learning;artificial intelligence;artificial neural network;feature vector;vector space;computer science;f1 score;named-entity recognition;pattern recognition;recurrent neural network	NLP	-19.256592728741644	-72.86051292845035	146257
49c11df15b5df18bbf4e0cea66c24f2e48f394a6	word sense disambiguation using wordnet domains	databases;noun;availability;wordnet domains 3 1;text analysis;lexical database;industries;natural languages;hard disks;psychology;word sense disambiguation;artificial neural networks communications technology computer architecture industries computer science databases psychology;text analysis natural language processing;computer architecture;artificial neural networks;local context word sense disambiguation domains wordnet wordnet domains;local context;dictionaries;wordnet domains;domain oriented text analysis;communications technology;domains;wordnet;computer science;natural language processing word sense disambiguation domain oriented text analysis unsupervised approach wordnet domains 3 1 lexical database;semantic relations;unsupervised approach;natural language processing;large scale systems	In this paper we present the methodology for word sense disambiguation based on domain information. Domain is a set of words in which there is a strong semantic relation among the words. The words in the sentence contribute to determine the domain of the sentence. The availability of WordNet domains makes the domain-oriented text analysis possible. The domain of the target word can be fixed based on the domains of the content words in the local context. This approach can be effectively used to disambiguate nouns. We present the unsupervised approach to word sense disambiguation using the WordNet domains. The model determines the domain of the target word and the sense corresponding to this domain is taken as the correct sense. We have used the WordNet domains 3.1 as lexical database.	algorithm;brill tagger;care-of address;code;factotum (software);lexical database;ontology components;part-of-speech tagging;telephone number;unsupervised learning;word sense;word-sense disambiguation;wordnet	Sopan Govind Kolte;Sunil G. Bhirud	2008	2008 First International Conference on Emerging Trends in Engineering and Technology	10.1109/ICETET.2008.231	natural language processing;wordnet;semeval;computer science;linguistics;information retrieval	NLP	-24.549081727376585	-66.67527984171468	146579
ccf7e2e89092874104a756690f2945d05b81054c	construction of emotional lexicon using potts model		Emotion is an instinctive state of mind aroused by some specific objects or situation. Exchange of textual information is an important medium for communication and contains a rich set of emotional expressions. The computational approaches to emotion analysis in textual data require annotated lexicons with polarity tags. In this paper we propose a novel method for constructing emotion lexicon annotated with Ekman‟s six basic emotion classes (anger, disgust, fear, happy, sad and surprise). We adopt the Potts model for the probability modeling of the lexical network. The lexical network has been constructed by connecting each pair of words in which one of the two words appears in the gloss of the other. Starting with a small number of emotional seed words, the emotional categories of other words have been determined. With manual checking of top 200 words from each class an average precision of 85.41% has been	computation;computational linguistics;fuzzy concept;gloss (annotation);information retrieval;lexicon;potts model;sentiment analysis;text corpus	Braja Gopal Patra;Hiroya Takamura;Dipankar Das;Manabu Okumura;Sivaji Bandyopadhyay	2013			natural language processing;speech recognition;machine learning;linguistics	NLP	-26.339167218688463	-70.56393070199495	146908
c8d6511f71d2f5124b7ca3649d5df594d2115b57	an unsupervised entity resolution framework for english and arabic datasets			unsupervised learning	Abdelkrim Ouhab;Mimoun Malki;Djamel Berrabah;Faouzi Boufarès	2017	IJSITA	10.4018/IJSITA.2017100102	arabic;natural language processing;knowledge management;computer science;name resolution;artificial intelligence	NLP	-24.001565497329526	-72.66329647148459	147827
30c3ba4d0f026f8d86498ce939eb409e065854ce	elirf-upv at semeval-2018 tasks 1 and 3: affect and irony detection in tweets		This paper describes the participation of ELiRF-UPV team at tasks 1 and 3 of Semeval2018. We present a deep learning based system that assembles Convolutional Neural Networks and Long Short-Term Memory neural networks. This system has been used with slight modifications for the two tasks addressed both for English and Spanish. Finally, the results obtained in the competition are reported and discussed.	artificial neural network;convolutional neural network;deep learning;long short-term memory;neural network software	José-Ángel González;Lluís-F. Hurtado;Ferran Plà	2018			natural language processing;semeval;artificial intelligence;irony;computer science	ML	-21.082453573112815	-70.19546893662373	147937
32cbd065ac9405530ce0b1832a9a58c7444ba305	part-of-speech tagging for twitter: annotation, features, and experiments	english language;social communication;speech analysis;media;internet	We address the problem of part-of-speech tagging for English data from the popular microblogging service Twitter. We develop a tagset, annotate data, develop features, and report tagging results nearing 90% accuracy. The data and tools have been made available to the research community with the goal of enabling richer text analysis of Twitter and related social media data sets.	brill tagger;domain adaptation;part-of-speech tagging;semi-supervised learning;semiconductor industry;social media;supervised learning	Kevin Gimpel;Nathan Schneider;Brendan T. O'Connor;Dipanjan Das;Daniel Mills;Jacob Eisenstein;Michael Heilman;Dani Yogatama;Jeffrey Flanigan;Noah A. Smith	2011			the internet;media;computer science;english;multimedia;internet privacy;world wide web	NLP	-21.89089739784507	-67.16500199311783	147974
35cbf98266b94d7d31d67e09faf57f8ea6f2204f	hltcoe approaches to knowledge base population at tac 2009		The HLTCOE participated in the entity linking and slot filling tasks at TAC 2009. A machine learning-based approach to entity linking, operating over a wide range of feature types, yielded good performance on the entity linking task. Slot-filling based on sentence selection, application of weak patterns and exploitation of redundancy was ineffective in the slot filling task.	archive;code refactoring;entity linking;failure analysis;heuristic;knowledge base;machine learning	Paul McNamee;Mark Dredze;Adam Gerber;Nikesh Garera;Timothy W. Finin;James Mayfield;Christine D. Piatko;Delip Rao;David Yarowsky;Markus Dreyer	2009			computer science;artificial intelligence;machine learning;data mining	NLP	-26.150421642763725	-67.50431395882467	148069
cbe8b4ef6223395ab3265e2e3195cb4313bf7e50	utu: adapting biomedical event extraction system to disorder attribute detection		In this paper we describe our entry to the SemEval 2015 clinical text analysis task. We participated only in the disorder attribute detection task 2a. Our main goal was to assess how well an information extraction system originally developed for a different task and domain can be utilized in this task. Our system, based on SVM and CRF classifiers, showed promising results, placing 3rd out of 6 participants in this task with performance of 0.857 measured in weighted accuracy, the official evaluation metric.	computation;conditional random field;information extraction;scientific literature;semeval	Kai Hakala	2015		10.18653/v1/S15-2064	speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-23.152961238093667	-69.7761158556068	149351
e67e4b66eaf7842af67f6326427bf7119469bbfb	bundeli folk-song genre classification with knn and svm		While large data dependent techniques have made advances in between-genre classification, the identification of subtypes within a genre has largely been overlooked. In this paper, we approach automatic classification of within-genre Bundeli folk music into its subgenres; Gaari, Rai and Phag. Bundeli, which is a dominant dialect spoken in a large belt of Uttar Pradesh and Madhya Pradesh has a rich resource of folk songs and an attendant folk tradition. First, we successfully demonstrate that a set of common stopwords in Bundeli can be used to perform broad genre classification between standard Bundeli text (newspaper corpus) and lyrics. We then establish the problem of structural and lexical similarity in within-genre classification using n-grams. Finally, we classify the lyrics data into the three genres using popular machinelearning classifiers: Support Vector Machine (SVM) and kNN classifiers achieving 91.3% and 85% and accuracy respectively. We also use a Naı̈ve Bayes classifier which returns an accuracy of 75%. Our results underscore the need to extend popular classification techniques to sparse and small corpora, so as to perform hitherto neglected within genre classification and also exhibit that well known classifiers can also be employed in classifying ‘small’ data.	big data;feature vector;grams;k-nearest neighbors algorithm;machine learning;modal logic;n-gram;naive bayes classifier;sparse matrix;support vector machine;text corpus	Ayushi Pandey;Indranil Dutta	2014			support vector machine;natural language processing;naive bayes classifier;lexical similarity;lyrics;underscore;distributed computing;computer science;artificial intelligence	ML	-20.985959348310576	-67.10235973393367	149454
66e18413c85fdcfa8e9045ea3c34598310697934	predicting unknown time arguments based on cross-event propagation	cross-event propagation;related event;unknown time argument;time argument;unknown event time argument;news article;statistical learning;rule based	Many events in news articles don’t include time arguments. This paper describes two methods, one based on rules and the other based on statistical learning, to predict the unknown time argument for an event by the propagation from its related events. The results are promising – the rule based approach was able to correctly predict 74% of the unknown event time arguments with 70% precision.	machine learning;multinomial logistic regression;principle of maximum entropy;software propagation	Prashant Gupta;Heng Ji	2009			rule-based system;computer science;artificial intelligence;machine learning;data mining	NLP	-22.429024497720917	-67.90264683918976	149842
9ddb4277e87447b872e8a85852a0221e09590124	learning fine-grained knowledge about contingent relations between everyday events		Much of the user-generated content on social media is provided by ordinary people telling stories about their daily lives. We develop and test a novel method for learning fine-grained common-sense knowledge from these stories about contingent (causal and conditional) relationships between everyday events. This type of knowledge is useful for text and story understanding, information extraction, question answering, and text summarization. We test and compare different methods for learning contingency relation, and compare what is learned from topic-sorted story collections vs. general-domain stories. Our experiments show that using topicspecific datasets enables learning finergrained knowledge about events and results in significant improvement over the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the relations between events that we learn from topic-sorted stories are judged as contingent.	amazon mechanical turk;automatic summarization;causality;contingency (philosophy);experiment;information extraction;question answering;social media;the turk;user-generated content	Elahe Rahimtoroghi;Ernesto Hernandez;Marilyn A. Walker	2016			contingency;information extraction;automatic summarization;artificial intelligence;computer science;natural language processing;social media;question answering	NLP	-19.559930037831265	-68.53771757524956	150095
c982352219c07a20a56daa637889ad80849e8a19	automatic classification of e-mail messages by message type		This srticfe describes a system that automatically classifies E-mail messages in the HUMANIST electronic discussion group into one of four clasaes, Question, Response, Announcement, or Administmtfve. A total of 1,372 messages were analyzed. The automatic classification of a message was based on string metching between a message text and pmdefined string sets for each of the message types. The system’s automated abitity to accurately classify a message was compared against manually assigned codes. The Cohen’s Kappa of .55 suggested that there was a statistical agreement between the automatic and manually assigned codes.	code	Andrew D. May	1997	JASIS	10.1002/(SICI)1097-4571(199701)48:1%3C32::AID-ASI5%3E3.0.CO;2-2		Web+IR	-24.86761772504124	-69.40350679247003	150799
74432f1e6d345a4603724a68bc09008ebb451041	three-phase opinion analysis system at ntcir-6		We developed an opinion analysis system at NTCIR-6. Our system can detect opinion sentences and extract opinion holders by executing three phases: (1) opinion sentence classification by SVM that distinguishes an opinionated sentence from others, (2) opinion-holder candidate extraction using named entity recognition, (3) opinion-holder detection by rules that find the correspondence between the sentence and the holder. Characteristics of the system are the following two points: (a) in phase 1, the end of a sentence expression is added to the feature in SVM vector, and (b) phase 3 is separated into the author detection and the others. As a result of the evaluation, in opinion sentence judgment both precision and the recall ratio improved based on point (a). In opinion holder extraction, precision has improved greatly based on point (b).	emoticon;in-phase and quadrature components;named entity;named-entity recognition;precision and recall;support vector machine	Hironori Mizuguchi;Masaaki Tsuchida;Dai Kusui	2007			computer science;pattern recognition;data mining;information retrieval	NLP	-24.30821745529147	-68.9803365555413	150845
35df1fc96dd74ac300c77a0bf74559e6cd2aa4eb	kdsl: a knowledge-driven supervised learning framework for word sense disambiguation		We propose KDSL, a new word sense disambiguation (WSD) framework that utilizes knowledge to automatically generate sense-labeled data for supervised learning. First, from WordNet, we automatically construct a semantic knowledge base called DisDict, which provides refined feature words that highlight the differences among word senses, i.e., synsets. Second, we automatically generate new sense-labeled data by DisDict from unlabeled corpora. Third, these generated data, together with manually labeled data and unlabeled data, are fed to a neural framework conducting supervised and unsupervised learning jointly to model the semantic relations among synsets, feature words and their contexts. The experimental results show that KDSL outperforms several representative state-of-the-art methods on various major benchmarks. Interestingly, it performs relatively well even when manually labeled data is unavailable, thus provides a potential solution for similar tasks in a lack of manual annotations.	experiment;knowledge base;protologism;supervised learning;synonym ring;text corpus;unsupervised learning;web services for devices;word sense;word-sense disambiguation;wordnet	Shi Yin;Yi Zhou;Chenguang Li;Shangfei Wang;Jianmin Ji;Xiaoping Chen;Ruili Wang	2018	CoRR		labeled data;supervised learning;machine learning;unsupervised learning;artificial intelligence;semantic memory;computer science;word-sense disambiguation;wordnet	NLP	-22.36901624722383	-71.89137346809103	151038
ba81b31598b85259b20399e485a1ab156db5511f	an unsupervised approach to recognizing discourse relations	discourse relation;arbitrary span;cue phrase;unsupervised approach;massive amount;computational linguistics;natural language;semantics;classification	We presentan unsupervisedapproachto recognizingdiscourserelationsof CONTRAST, EXPLANATION-EVIDENCE, CONDITION andELABORATION that hold betweenarbitraryspansof texts. We show that discourserelation classifierstrained on examples that are automaticallyextractedfrom massi ve amountsof text can be usedto distinguishbetweensomeof theserelationswith accuraciesashigh as 93%, even whenthe relationsarenot explicitly markedby cuephrases.	unsupervised learning	Daniel Marcu;Abdessamad Echihabi	2002			natural language processing;biological classification;computer science;computational linguistics;machine learning;semantics;linguistics;natural language	NLP	-24.959760540380714	-72.39990172122282	151121
1ec24afeae2dc0f58e0bae4099de2e4bef904938	using syntactic and contextual information for sentiment polarity analysis	opinion mining;contextual information;sentiment polarity analysis;sentiwordnet;part of speech;context dependent	A new method for sentiment polarity analysis is presented. The method first assigns scores to a sentence using SentiWordNet and then uses heuristics to handle context dependent sentiment expressions. Instead of using score of all synsets of a word listed in SentiWordNet we use score of synsets of the same parts of speech only. Our method shows significant improvement on movie-review dataset over the baseline.	baseline (configuration management);heuristic (computer science);synonym ring	Shaishav Agrawal;Tanveer J. Siddiqui	2009		10.1145/1655925.1656037	natural language processing;speech recognition;part of speech;computer science;context-dependent memory;pattern recognition;sentiment analysis	NLP	-23.234801426677777	-72.43851048036252	151305
9c555eeb4d64d5c7b96bb05cebe8170096e1add7	blinov: distributed representations of words for aspect-based sentiment analysis at semeval 2014		The article describes our system submitted to the SemEval-2014 task on Aspect-Based Sentiment Analysis. The methods based on distributed representations of words for the aspect term extraction and aspect term polarity detection tasks are presented. The methods for the aspect category detection and category polarity detection tasks are presented as well. Well-known skip-gram model for constructing the distributed representations is briefly described. The results of our methods are shown in comparison with the baseline and the best result.	artificial neural network;baseline (configuration management);n-gram;semeval;sentiment analysis;skip list;terminology extraction	Pavel Blinov;Eugeny V. Kotelnikov	2014			natural language processing;computer science;machine learning;communication	NLP	-22.999467691948833	-69.87139090389125	151419
8c4b26ee4b838e204c71fe36f467fbbcbdd72652	an algorithm for aspects of semantic interpretation using an enhanced wordnet	wordnet verb class;spatial adjunct;thematic role;enhanced wordnet;wordnet ontology;syntactic relation;semantic interpretation;noun	An algorithm for semantic interpretation is explained. The algorithm is based on predicates defined for WordNet verb classes. The algorithm is driven by the definition of these predicates whose thematic roles are linked to the WordNet ontology for nouns and to the syntactic relations that realize them. The algorithm has been tested in the identification of the meaning of the verb, thematic roles, and temporal and spatial adjuncts.	algorithm;semantic interpretation;wordnet	Fernando Gomez	2001			natural language processing;noun;semantic interpretation;computer science;linguistics;information retrieval	NLP	-26.407090670724838	-71.08883576794615	151959
0675267406be6d4885331a8d4dc4b6d004f0c084	using rhetorical structure in sentiment analysis		A deep, fine-grain analysis of rhetorical structure highlights crucial sentiment-carrying text segments.	memory segmentation;sentiment analysis	Alexander Hogenboom;Flavius Frasincar;Franciska de Jong;Uzay Kaymak	2015	Commun. ACM	10.1145/2699418	natural language processing;speech recognition	NLP	-19.998204824897265	-70.75758211086404	152187
2cc966b21634d06c1f52b08b801ae7fbdbc83958	ecnucs: a surface information based system description of sentiment analysis in twitter in the semeval-2013 (task 2)		This paper briefly reports our submissions to the two subtasks of Semantic Analysis in Twitter task in SemEval 2013 (Task 2), i.e., the Contextual Polarity Disambiguation task (an expression-level task) and the Message Polarity Classification task (a message-level task). We extract features from surface information of tweets, i.e., content features, Microblogging features, emoticons, punctuation and sentiment lexicon, and adopt SVM to build classifier. For subtask A, our system on twitter data ranks 2 on unconstrained rank and on SMS data ranks 1 on unconstrained rank.	emoticon;lexicon;semeval;sentiment analysis;word-sense disambiguation	Tiantian Zhu;Fangxi Zhang;Lan Man	2013			computer science;pattern recognition;data mining;information retrieval	NLP	-22.5614633308071	-69.4236102233391	152214
4852352d478257e36eee46434a29cbc106f2512d	learning from measurements in crowdsourcing models: inferring ground truth from diverse annotation types		Annotated corpora enable supervised machine learning and data analysis. To reduce the cost of manual annotation, tasks are often assigned to internet workers whose judgments are reconciled by crowdsourcing models. We approach the problem of crowdsourcing using a framework for learning from rich prior knowledge, and we identify a family of crowdsourcing models with the novel ability to combine annotations with differing structures: e.g., document labels and word labels. Annotator judgments are given in the form of the predicted expected value of measurement functions computed over annotations and the data, unifying annotation models. Our model, a specific instance of this framework, compares favorably with previous work. Furthermore, it enables active sample selection, jointly selecting annotator, data item, and annotation structure to reduce annotation effort. Annotierte Korpora ermöglichen überwachtes maschinelles Lernen und Datenanalyse. Um die Kosten für manuelle Annotationen zu vermeiden, werden Aufgaben häufig Internetarbeitern zugewiesen, deren Urteile durch Crowdsourcing-Modelle abgeglichen werden. Wir nähern uns dem Problem des Crowdsourcings, indem wir einen Rahmen für das Lernen aus reichem Vorwissen vorschlagen, und wir bestimmen eine Familie von Crowdsourcing-Modellen mit der Fähigkeit, Annotationen mit unterschiedlichen Strukturen zu kombinieren: z.B., Dokumentbezeichnungen und Wortbezeichnungen. Bewertungen werden in Form des vorhergesagten erwarteten Werts von Messfunktionen (measurement functions) gegeben, die über Annotationen und die Daten berechnet werden. Darin werden die vorherige Annotationsmodelle vereinheitlicht. Unser Modell, eine spezifische Instanz dieses Rahmens, schneidet im Vergleich zu früheren Arbeiten positiv ab. Darüber hinaus ermöglicht es die aktive Stichprobenauswahl, indem Kommentator, Datenelement, und Annotationsstruktur gemeinsam ausgewählt werden, um den Annotationskosten zu reduzieren.	crowdsourcing;data item;eine and zwei;experiment;gaussian process;ground truth;interaction;machine learning;primary direction;structured prediction;supervised learning;text corpus;unified model;v-model	Paul Felt;Eric K. Ringger;Jordan L. Boyd-Graber;Kevin D. Seppi	2018			natural language processing;artificial intelligence;ground truth;computer science;crowdsourcing;annotation	NLP	-19.5506922620557	-67.58499618994752	152273
a286f06e27a0607e2af85892980c0c13072fa619	unsupervised fine-grained sentiment analysis system using lexicons and concepts		Sentiment is mainly analyzed at a document, sentence or aspect level. Document or sentence levels could be too coarse since polar opinions can co-occur even within the same sentence. In aspect level sentiment analysis often opinion-bearing terms can convey polar sentiment in different contexts. Consider the following laptop review: “the big plus was a large screen but having a large battery made me change my mind,” where polar opinions co-occur in the same sentence, and the opinion term that describes the opinion targets (“large”) encodes polar sentiments: a positive for screen, and a negative for battery. To parse these differences, our approach is to identify opinions with respect to the specific opinion targets, while taking the context into account. Moreover, considering that there is a problem of obtaining an annotated training set in each context, our approach uses unlabeled data.	lexicon;sentiment analysis	Nir Ofek;Lior Rokach	2014		10.1007/978-3-319-12024-9_3	sentiment analysis;parsing;training set;artificial intelligence;pattern recognition;sentence;computer science;lexicon	NLP	-20.496373704995005	-68.78688598044694	152391
8cb77762a562ae5d0013ea89021d662630ff989d	sense space for word sense disambiguation		Word sense disambiguation is essential for semantic analysis in many natural language-related applications, such as information retrieval, data mining, and machine translation. One of the effective models for word sense disambiguation is the word space model that represents context vectors and sense vectors in a word vector space. In this paper, we extend the word vector space model to reflect a more finegrained meaning in context vectors by incorporating embedded senses. Using a large Korean sense-tagged corpus, we build an embedded sense space with supervised learning and evaluate the effectiveness of the sense embedding for word sense disambiguation.	data mining;embedded system;information retrieval;machine translation;natural language;supervised learning;word embedding;word sense;word-sense disambiguation	Myung Yun Kang;Tae-Hong Min;Jae Sung Lee	2018	2018 IEEE International Conference on Big Data and Smart Computing (BigComp)	10.1109/BigComp.2018.00122	supervised learning;machine translation;semantics;computational linguistics;machine learning;vector space;embedding;vector space model;artificial intelligence;context model;computer science	NLP	-25.1879809106934	-70.03897436413347	152497
216c6d29a6f57c37ef8f26f88b6ec9be5b855a66	from vqa to multimodal cqa: adapting visual qa models for community qa tasks.		In this work, we present novel methods to adapt visual QA models for community QA tasks of practical significance automated question category classification and finding experts for question answering on questions containing both text and image. To the best of our knowledge, this is the first work to tackle the multimodality challenge in CQA, and is an enabling step towards basic question-answering on imagebased CQA. First, we analyze the differences between visual QA and community QA datasets, discussing the limitations of applying VQA models directly to CQA tasks, and then we propose novel augmentations to VQA-based models to best address those limitations. Our model, with the augmentations of an image-text combination method tailored for CQA and use of auxiliary tasks for learning better grounding features, significantly outperforms the text-only and VQA model baselines for both tasks on real-world CQA data from Yahoo! Chiebukuro, a Japanese counterpart of Yahoo! Answers. Introduction and Background Community question answering (CQA) platforms enable users to crowd-source answers to posted queries, search and explore questions, and share knowledge through answers. As the size of the user base increases, so does the information content, making it imperative to carefully design methods for categorizing and organizing information and identifying relevant content for personalized recommendations. Such end-tasks are of significant practical importance to QA platforms, making them a big focus in information retrieval and natural language processing domains. The CQA task of automatic question classification is useful for tagging newly posted questions and suggesting an appropriate question category to the asking user, and also acts as a good first step towards potentially answering the question. This has led to many studies focusing on this kind of classification (Zhang and Lee 2003), (Saha, Saha, and Schneider 2013), (Stanley and Byrne 2013), (Tamaki et al. 2018). Another useful problem to tackle is that of retrieving “experts” for answering posted questions. Here, the aim is to identify and retrieve users from the community who are likely to provide answers to a given question. This provides an efficient way to make the community well-knit, provide ∗Work done during an internship at Yahoo Japan Corporation Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. better content to askers, and recommend only the relevant questions from a gigantic pool of queries to the potential answerers. Previous works include (Zhao et al. 2016), (Riahi et al. 2012), and (Liu, Croft, and Koll 2005), among others. A recurring feature in these tasks has been that the data is comprised only of text. Research datasets from platforms such as Stack Exchange and Quora and from collections like TREC-QA rarely contain questions with a combination of text and images. In this work, we use data from Yahoo! JAPAN’s community QA website Yahoo! Chiebukuro, where questions accompanied by an image form a considerable percentage (∼10%) of the total posted questions (Fig. 1(a)). With Stack Exchange sites supporting images (∼7%, 11%, 12% and 20% image-based questions for cs, datascience, movies, and anime stackexhange sites respectively), not to mention the numerous image-based threads on discussion platforms like Reddit, the advantages of solutions for multimodal CQA are not limited to the Chiebukuro site. Models using only text can give reasonable performances for multimodal CQA tasks (as we’ll see in our results), but there is potential to gain substantial improvements by utilizing the image data. While many CQA samples have images that contain very little information, are irrelevant, or have characteristics that are rarely repeated across samples (as in the bottom-right example in Fig. 1a), it is easy to identify a couple of broad categories where image data will be essential for our end-tasks: i) where the image contains the actual question, and the question loses meaning without the image (Fig. 1a bottom-left example), and ii) where the image is necessary to make sense of the question text (top-mid & topright examples in Fig. 1a). Images can also help reinforce the inferences from textual features (Fig. 1a top-left), or provide disambiguation over multiple topics inferred from text (‘plants’ and ‘shoes’ from Fig. 1a bottom-mid example). Thus, we focus on methods to best exploit the combined image-text information from multimodal CQA questions. Our main tasks are classification and expert retrieval. An additional motive is to enable our developed models to be used to answer external knowledge based factoid questions (similar to (Wang et al. 2015) & (Yeh, Lee, and Darrell 2008)) on CQA. With this in mind, and to not reinvent the wheel, we leverage the success of visual question answering (VQA) models in dealing with text-image question pairs, and build novel augmentations to adapt them for CQA tasks. ar X iv :1 80 8. 09 64 8v 1 [ cs .C L ] 2 9 A ug 2 01 8 Figure 1: (a): Question pairs from YC-CQA (translated); (b): Typical pairs for VQA, taken from (Kafle and Kanan 2017). In its most common form, the VQA task (Antol et al. 2015) involves taking as input an image-question pair (examples in Fig. 1b) and outputting an answer either as a generated sequence, or using classification on a predefined set. One of the main ideas behind its proposal has been to connect the advances in computer vision and NLP, so as to provide an “AI-complete” task. However, given the nature of the questions and images, its direct practical applicability is limited. The questions are short, direct, and query the image, or at the most require common sense or objective encyclopedic knowledge. This is in contrast with the nature of questions found on discussion and community QA platforms, where askers seek human expertise, and the question texts provide context outside the input image, or are supported by the image. It is therefore important to properly identify and resolve the shortcomings of VQA models to enable better understanding of the image-text data from CQA. A natural counterpart to VQA is answering the CQA questions with images. However, this is a far more difficult and subjective task compared to answering in VQA due to varying answer lengths and composition, the requirement of non-trivial external knowledge that must be modified according to the question’s context, and the necessity of a combination of human opinions. We instead want to be able to use the results, inferences, and models from this work to answer a subset of simpler factoid-based CQA image questions in our future work. Given the significant percentage that image-based questions occupy on Chiebukuro, and the current policy on the site making it mandatory for asking users to provide a category from among hundreds of choices, improving automated category classification simplifies the introduction of the feature that suggests appropriate category to the askers. It can even allow them to skip this part by assigning the predicted category automatically. Providing better expert retrieval has the obvious benefit of improving the responsiveness and quality of the QA service as a whole. Finally, the decision to use VQA-inspired end-to-end learning architectures makes our models generalizable and usable for image-based sections on other QA/discussion platforms, along with possessing the potential for extension to question answering. Therefore, in this paper, • We closely analyze the differences between visual question answering and image-based community question answering tasks, and identify the challenges in CQA that may hinder the performance of VQA models. • Following this, we propose modifications to VQAinspired models for a better CQA-task performance. Our key contributions include learning an additional global weight for image representation for the image-text combination step and introducing two auxiliary tasks to learn better grounding features. • We evaluate our model against baselines from text-only & VQA models, and some other frequently used methods for image-text combination, on the Chiebukuro dataset. • Finally, we use an ablation study to quantify the contributions of each of our suggested changes. We are also making our code for the models publicly available1. To the best of our knowledge, our work is the first to tackle the challenges of multimodal CQA, and also to adapt VQA models for tasks on questions posted by humans seeking the expertise of the community (as opposed to straightforward questions that query the image). It is worth noting that (Tamaki et al. 2018) deal with the same dataset, comparing joint embedding methods for a basic classification task, but do not attempt to address any CQA-specific challenges. Understanding VQA-CQA Differences It is crucial to understand the differences between the question-image pairs in VQA and those in CQA in order to identify the unique challenges posed by the new dataset and address them by means of appropriate modifications. Our dataset is derived from questions posted in 2014 on Yahoo Chiebukuro (YC-CQA), which allows questions both with and without an image. In this work, we only deal with questions accompanied by an image. A simple statistical comparison highlighting different aspects of the data is presented in Table 1, contrasting our dataset with the most commonly used datasets for VQA. To better understand the contrast, https://github.com/avikalp7/VQAtoCQA Table 1: Statistics for VQA datasets and Yahoo Chiebukuro (YC-CQA) dataset. Dataset Total # of Questions Question Text’s Vocab Size Answer Text’s Vocab Size Average Question Length Average Answer Length Total # of Question Categories Average Categories per Question DAQUAR 12468 2520 823 11.53 1.15 3 1.00 COCO-QA 117684 12047 430 8.65 1.00 4 1.00 VQA v2 658111 26749 29548 6.20 1.16 65 1.00 YC-CQA 319944 176921 335658 71.54 62.15 38	ai-complete;append;artificial intelligence;baseline (configuration management);categorization;comparison sort;computer vision;emoticon;end-to-end principle;imperative programming;information retrieval;multimodal interaction;natural language processing;organizing (structure);performance;personalization;qa & ux manager;question answering;reinventing the wheel;relevance;requirement;responsiveness;sakai project;self-information;stack exchange;stanley (vehicle);statistical classification;text corpus;text-based user interface;vocabulary;word lists by frequency;word-sense disambiguation;xfig	Avikalp Srivastava;Hsin Wen Liu;Sumio Fujita	2018	CoRR		machine learning;information retrieval;artificial intelligence;question answering;computer science	NLP	-20.46288108682379	-72.79075863864642	153176
8a57d8a9df5f53dce591a9c8db1df7f5c2b2d4a9	centement at semeval-2018 task 1: classification of tweets using multiple thresholds with self-correction and weighted conditional probabilities		In this paper we present our contribution to SemEval-2018, a classifier for classifying multi-label emotions of Arabic and English tweets. We attempted “Affect in Tweets”, specifically Task E-c: Detecting Emotions (multi-label classification). Our method is based on preprocessing the tweets and creating word vectors combined with a self correction step to remove noise. We also make use of emotion specific thresholds. The final submission was selected upon the best performance achieved, selected when using a range of thresholds. Our system was evaluated on the Arabic and English datasets provided for the task by the competition organisers, where it ranked 2nd for the Arabic dataset (out of 14 entries) and 12th for the English dataset (out of 35 entries).	multi-label classification;preprocessor;word embedding	Tariq Ahmad;Allan Ramsay;Hanady Ahmed	2018			machine learning;conditional probability;semeval;computer science;artificial intelligence	NLP	-22.239757562696415	-70.0633995592508	153509
5f9bf03fbb4487a2bd42dbd45835768d6133de6d	grounded semantic parsing for complex knowledge extraction		Recently, there has been increasing interest in learning semantic parsers with indirect supervision, but existing work focuses almost exclusively on question answering. Separately, there have been active pursuits in leveraging databases for distant supervision in information extraction, yet such methods are often limited to binary relations and none can handle nested events. In this paper, we generalize distant supervision to complex knowledge extraction, by proposing the first approach to learn a semantic parser for extracting nested event structures without annotated examples, using only a database of such complex events and unannotated text. The key idea is to model the annotations as latent variables, and incorporate a prior that favors semantic parses containing known events. Experiments on the GENIA event extraction dataset show that our approach can learn from and extract complex biological pathway events. Moreover, when supplied with just five example words per event type, it becomes competitive even among supervised systems, outperforming 19 out of 24 teams that participated in the original shared task.	database;gene regulatory network;information extraction;latent variable;operational semantics;parsing;prototype;pubmed;question answering	Ankur P. Parikh;Hoifung Poon;Kristina Toutanova	2015			natural language processing;speech recognition;computer science;machine learning;data mining	NLP	-21.784286198772296	-71.50820734240072	153593
74adc10d455e6207311e06a4e3f4eac4f97e189f	t-saf: twitter sentiment analysis framework using a hybrid classification scheme		Of the many social media sites available, users prefer microblogging services such as Twitter to learn about product services, social events, and political trends. Twitter is considered an important source of information in sentiment analysis applications. Supervised and unsupervised machine learning-based techniques for Twitter data analysis have been investigated in the last few years, often resulting in an incorrect classification of sentiments. In this paper, we focus on these issues and present a unified framework for classifying tweets using a hybrid classification scheme. The proposed method aims at improving the performance of Twitter-based sentiment analysis systems by incorporating 4 classifiers: (a) a slang classifier, (b) an emoticon classifier, (c) the SentiWordNet classifier, and (d) an improved domain-specific classifier. After applying the preprocessing steps, the input text is passed through the emoticon and slang classifiers. In the next stage, SentiWordNet-based and domain-specific classifiers are applied to classify the text more accurately. Finally, sentiment classification is performed at sentence and document levels. The findings revealed that the proposed method overcomes the limitations of previous methods by considering slang, emoticons, and domain-specific terms.	comparison and contrast of classification schemes in linguistics and metadata;saf-t;sentiment analysis;store and forward	Muhammad Zubair Asghar;Fazal Masood Kundi;Shakeel Ahmad;Aurangzeb Khan;Furqan Khan	2018	Expert Systems	10.1111/exsy.12233	microblogging;sentiment analysis;data mining;slang;computer science;unsupervised learning;emoticon;classifier (linguistics);social media;sentence	NLP	-21.294522444142626	-67.76514692208862	154390
9b1e6362fcfd298cd382c0c24f282d86f174cac8	text classification from positive and unlabeled data using misclassified data correction		This paper addresses the problem of dealing with a collection of labeled training documents, especially annotating negative training documents and presents a method of text classification from positive and unlabeled data. We applied an error detection and correction technique to the results of positive and negative documents classified by the Support Vector Machines (SVM). The results using Reuters documents showed that the method was comparable to the current state-of-the-art biasedSVM method as the F-score obtained by our method was 0.627 and biased-SVM was 0.614.	algorithm;document classification;error detection and correction;f1 score;statistical classification;support vector machine;text corpus	Fumiyo Fukumoto;Yoshimi Suzuki;Suguru Matsuyoshi	2013			computer science;pattern recognition;data mining;information retrieval	ML	-22.715185909996144	-70.61720764498942	154447
68aa19e5f2bc4a2bb379860b003a9152573f21e2	ijcnlp-2017 task 2: dimensional sentiment analysis for chinese phrases		This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 19 teams registered for this shared task for twodimensional sentiment analysis, 13 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing. All data sets with gold standards and scoring script are made publicly available to researchers.	affective computing;sentiment analysis;web standards	Liang-Chih Yu;Lung-Hao Lee;Jin Wang;Kam-Fai Wong	2017			natural language processing;pattern recognition;computer science;sentiment analysis;artificial intelligence	NLP	-22.88439477349133	-66.75672834440812	154868
8680d71d7bee0ab16cb4b2d87b4955baa5a4885a	tag confidence measure for semi-automatically updating named entity recognition	dubious tag;tag posterior probability;tag confidence measure;correct ne tag;base model;active learning;entity recognition;manual correction;entire sentence posterior probability;system cost;individual ne tag	We present two techniques to reduce machine learning cost, i.e., cost of manually annotating unlabeled data, for adapting existing CRF-based named entity recognition (NER) systems to new texts or domains. We introduce the tag posterior probability as the tag confidence measure of an individual NE tag determined by the base model. Dubious tags are automatically detected as recognition errors, and regarded as targets of manual correction. Compared to entire sentence posterior probability, tag posterior probability has the advantage of minimizing system cost by focusing on those parts of the sentence that require manual correction. Using the tag confidence measure, the first technique, known as active learning, asks the editor to assign correct NE tags only to those parts that the base model could not assign tags confidently. Active learning reduces the learning cost by 66%, compared to the conventional method. As the second technique, we propose bootstrapping NER, which semiautomatically corrects dubious tags and updates its model.	active learning (machine learning);blog;conditional random field;machine learning;named entity;named-entity recognition;semiconductor industry;www	Kuniko Saito;Kenji Imamura	2009			computer science;pattern recognition;data mining;information retrieval	NLP	-24.104460565400675	-71.55543987722966	155406
cd3b85ac0d3f0c65a40ce9e5c19396303cfb3c80	empirist: aiphes - robust tokenization and pos-tagging for different genres		We present our system used for the AIPHES team submission in the context of the EmpiriST shared task on “Automatic Linguistic Annotation of ComputerMediated Communication / Social Media”. Our system is based on a rulebased tokenizer and a machine learning sequence labelling POS tagger using a variety of features. We show that the system is robust across the two tested genres: German computer mediated communication (CMC) and general German web data (WEB). We achieve the second rank in three of four scenarios. Also, the presented systems are freely available as open source components.	brill tagger;computer-mediated communication;lexical analysis;logic programming;machine learning;open-source software;part-of-speech tagging;point of sale;social media;tokenization (data security);world wide web	Steffen Remus;Gerold Hintz;Christian Biemann;Christian M. Meyer;Darina Benikova;Judith Eckle-Kohler;Margot Mieskes;Thomas Arnold	2016		10.18653/v1/W16-2613	tokenization (data security);natural language processing;computer science;artificial intelligence	NLP	-22.690178111633312	-69.38410048007255	155413
222cc8e74ff4b4ae54999c08db9827e20d2f47d2	fs-ner: a lightweight filter-stream approach to named entity recognition on twitter data	fs ner;crf;named entity recognition;twitter	Microblog platforms such as Twitter are being increasingly adopted by Web users, yielding an important source of data for web search and mining applications. Tasks such as Named Entity Recognition are at the core of many of these applications, but the effectiveness of existing tools is seriously compromised when applied to Twitter data, since messages are terse, poorly worded and posted in many different languages. Also, Twitter follows a streaming paradigm, imposing that entities must be recognized in real-time. In view of these challenges and the inappropriateness of existing tools, we propose a novel approach for Named Entity Recognition on Twitter data called FS-NER (Filter-Stream Named Entity Recognition). FS-NER is characterized by the use of filters that process unlabeled Twitter messages, being much more practical than existing supervised CRF-based approaches. Such filters can be combined either in sequence or in parallel in a flexible way. Moreover, because these filters are not language dependent, FS-NER can be applied to different languages without requiring a laborious adaptation. Through a systematic evaluation using three Twitter collections and considering seven types of entity, we show that FS-NER performs 3% better than a CRF-based baseline, besides being orders of magnitude faster and much more practical.	baseline (configuration management);conditional random field;data mining;named entity;named-entity recognition;object storage;programming paradigm;real-time transcription;web search engine	Diego Marinho de Oliveira;Alberto H. F. Laender;Adriano Veloso;Altigran Soares da Silva	2013		10.1145/2487788.2488003	computer science;data mining;internet privacy;world wide web	ML	-20.84164821819771	-66.53977135445218	156510
f0c006caea00278d676ff67ae3a5affb04019364	document classification in structured military messages		We present new results for the DSTO project on document classification of military messages. We report more specifically on the improvements to the Part-Of-Speech (POS) tagging, a probabilistic process that assigns a tag to a token, and discuss the training for Date Time Groups POS tags. A new implementation of the rule-based classifier is described. The results obtained on two databases of real military messages are encouraging and the document classification module has now been integrated with a query user interface.	brown corpus;database;document classification;logic programming;part-of-speech tagging;user interface	Oliver Carr;Dominique Estival	2003			data mining;internet privacy;world wide web	Web+IR	-23.26822335600687	-69.44378957106932	156857
623b80ee5e4d834c395ff98fbb47cfe516ca8b33	preco: a large-scale dataset in preschool vocabulary for coreference resolution		We introduce PreCo, a large-scale English dataset for coreference resolution. The dataset is designed to embody the core challenges in coreference, such as entity representation, by alleviating the challenge of low overlap between training and test sets and enabling separated analysis of mention detection and mention clustering. To strengthen the training-test overlap, we collect a large corpus of 38K documents and 12.5M words which are mostly from the vocabulary of Englishspeaking preschoolers. Experiments show that with higher training-test overlap, error analysis on PreCo is more efficient than the one on OntoNotes, a popular existing dataset. Furthermore, we annotate singleton mentions making it possible for the first time to quantify the influence that a mention detector makes on coreference resolution performance. The dataset is freely available at https:// preschool-lab.github.io/PreCo/.	cluster analysis;error analysis (mathematics);experiment;https;human reliability;java annotation;text corpus;vocabulary	Hong Chen;Zhenhua Fan;Hao Lu;Alan L. Yuille;Shu Rong	2018			natural language processing;artificial intelligence;information retrieval;cluster analysis;computer science;singleton;coreference;vocabulary	NLP	-25.002446661939576	-73.16423016416582	157185
1fb8a9078c2ae431634f5f38c67cd40a25eb8d1d	how to improve text summarization and classification by mutual cooperation on an integrated framework	binary independence model;text summarization;text classification;cluster based language model;support vector machine	An effective integrated framework using both of summary and category information.The summarization technique utilizes the category information from classification.The classification technique utilizes the summary information from summarization.This integrated framework achieves significant improvement. Text summarization and classification are core techniques to analyze a huge amount of text data in the big data environment. Moreover, as the need to read texts on smart phones, tablets and television as well as personal computers continues to grow, text summarization and classification techniques become more important and both of them do essential processes for text analysis in many applications.Traditional text summarization and classification techniques have individually been considered as different research fields in this literature. However, we find out that they can help each other as text summarization makes use of category information from text classification and text classification does summary information from text summarization. Therefore, we propose an effective integrated learning framework using both of summary and category information in this paper. In this framework, the feature-weighting method for text summarization utilizes a language model to combine feature distributions in each category and text, and one for text classification does the sentence importance scores estimated from the text summarization.In the experiments, the performances of the integrated framework are better than ones of individual text summarization and classification. In addition, the framework has some advantages of easy implementation and language independence because it is based on only simple statistical approaches and POS tagger.	automatic summarization	Hyoungil Jeong;Youngjoong Ko;Jungyun Seo	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.05.001	text graph;support vector machine;text mining;multi-document summarization;binary independence model;computer science;automatic summarization;machine learning;noisy text analytics;pattern recognition;data mining;information retrieval	ML	-22.053227330617748	-66.57210399467662	157518
351ac37be54a055c88907c1e87354450f34579a9	sriubc-core: multiword soft similarity models for textual similarity		In this year’s Semantic Textual Similarity evaluation, we explore the contribution of models that provide soft similarity scores across spans of multiple words, over the previous year’s system. To this end, we explored the use of neural probabilistic language models and a TF-IDF weighted variant of Explicit Semantic Analysis. The neural language model systems used vector representations of individual words, where these vectors were derived by training them against the context of words encountered, and thus reflect the distributional characteristics of their usage. To generate a similarity score between spans, we experimented with using tiled vectors and Restricted Boltzmann Machines to identify similar encodings. We find that these soft similarity methods generally outperformed our previous year’s systems, albeit they did not perform as well in the overall rankings. A simple analysis of the soft similarity resources over two word phrases is provided, and future areas of improvement are described.	explicit semantic analysis;language model;semantic similarity;tf–idf	Eric Yeh	2013			natural language processing;semantic similarity;speech recognition;machine learning;pattern recognition;similarity heuristic	NLP	-21.517289146931194	-72.93093995588758	157765
c3b9d4a62394cc8fc7488db2b731f036a2d0f051	data-driven broad-coverage grammars for opinionated natural language generation (onlg)		Opinionated natural language generation (ONLG) is a new, challenging, NLG task in which we aim to automatically generate human-like, subjective, responses to opinionated articles online. We present a data-driven architecture for ONLG that generates subjective responses triggered by users’ agendas, based on automatically acquired wide-coverage generative grammars. We compare three types of grammatical representations that we design for ONLG. The grammars interleave different layers of linguistic information, and are induced from a new, enriched dataset we developed. Our evaluation shows that generation with Relational-Realizational (Tsarfaty and Sima’an, 2008) inspired grammar gets better language model scores than lexicalized grammars à la Collins (2003), and that the latter gets better humanevaluation scores. We also show that conditioning the generation on topic models makes generated responses more relevant to the document content.	baseline (configuration management);deep learning;emergence;experiment;generative grammar;interleaved memory;language model;linear algebra;natural language generation;off topic;relevance;round-robin scheduling;scoring functions for docking;social media;topic model;user profile	Tomer Cagan;Stefan L. Frank;Reut Tsarfaty	2017		10.18653/v1/P17-1122	computer science;natural language processing;artificial intelligence;machine learning;natural language generation;rule-based machine translation;data-driven	NLP	-19.324968463751574	-69.58102706185598	158379
52019c64adf89cfe2568b0f1722638374e1d59bc	cross-language learning from bots and users to detect vandalism on wikipedia	wikipedia;wikipedia languages;cross language learning;users;encyclopaedias;bots;text analysis;maintenance engineering;journal article;editors;internet;machine learning approaches;vandalism detection bots;feature extraction;open access encyclopedias;web sites;counter vandalism bots;transfer learning;small pan wikipedia vandalism data sets;electronic publishing;learning artificial intelligence;encyclopedias electronic publishing internet maintenance engineering feature extraction conferences;encyclopedias;text features;web sites encyclopaedias learning artificial intelligence text analysis;vandalism;feature engineering;machine learning approaches cross language learning open access encyclopedias counter vandalism bots text features wikipedia languages small pan wikipedia vandalism data sets vandalism detection bots;conferences	Vandalism, the malicious modification of articles, is a serious problem for open access encyclopedias such as Wikipedia. The use of counter-vandalism bots is changing the way Wikipedia identifies and bans vandals, but their contributions are often not considered nor discussed. In this paper, we propose novel text features capturing the invariants of vandalism across five languages to learn and compare the contributions of bots and users in the task of identifying vandalism. We construct computationally efficient features that highlight the contributions of bots and users, and generalize across languages. We evaluate our proposed features through classification performance on revisions of five Wikipedia languages, totaling over 500 million revisions of over nine million articles. As a comparison, we evaluate these features on the small PAN Wikipedia vandalism data sets, used by previous research, which contain approximately 62,000 revisions. We show differences in the performance of our features on the PAN and the full Wikipedia data set. With the appropriate text features, vandalism bots can be effective across different languages while learning from only one language. Our ultimate aim is to build the next generation of vandalism detection bots based on machine learning approaches that can work effectively across many languages.	computational complexity theory;invariant (computer science);machine learning;statistical classification;super robot monkey team hyperforce go!;surround sound;tokenization (data security);victor pan;wikipedia;working set	Khoi-Nguyen Tran;Peter Christen	2015	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2014.2339844	natural language processing;maintenance engineering;the internet;feature;feature extraction;transfer of learning;computer science;machine learning;data mining;brand;multimedia;electronic publishing;world wide web;encyclopedia	Web+IR	-22.29655709195965	-66.7730132754535	158650
131eba1f2eddc4e6700b65c9f688109bf9e12ba1	author profiling using corpus statistics, lexicons and stylistic features notebook for pan at clef-2013		This paper describes our participation in the 9th PAN evaluation lab in the author profiling task. The proposed approach relies on the extraction of stylistic, lexicon and corpus-based features, which were combined with a logistic classifier. These three sets of features contain pairwise intersections and even some features that belong to all categories. A comprehensive comparison of the contribution of several feature subsets is presented. In particular, a set of features based on Bayesian inference provided the most important contribution. We developed our system in the Spanish training corpus, once developed it was used, with minor changes, for the English documents, too. The proposed system was ranked 6th in the official ranking for Spanish documents among 17 submitted systems. This result shows that our approach is meaningful and competitive for predicting demographics from text.	bayesian network;lexicon;machine learning;stylometry;text corpus	Maria De-Arteaga;Sergio Jiménez;George Dueñas;Sergio Mancera;Julia Baquero	2013			eval;profiling (computer programming);data mining;clef;demographics;pairwise comparison;ranking;computer science;bayesian inference;lexicon	NLP	-22.47147448886639	-69.33977872426938	159108
1b5c71a3bcc2185a9a251bd1f41a420fc73af397	learning relational facts from the web: a tolerance rough set approach	granular methodologies;semi supervised learning;web mining;tolerance rough sets	A key issue when mining web information is the labeling problem.Tolerance rough sets are used to structure categorical instances and relations.We report a semi-supervised algorithm (TPL) that labels the ontological information.The Never Ending Language Learner (Nell) system provides the ontology.The performance of TPL compares well with CBS and CPL and handles concept drift. A key issue when mining web information is the labeling problem: data are abundant on the web but is unlabeled. In this paper, we address this problem by proposing (i) a granular model that structures categorical noun phrase instances as well as semantically related noun phrase pairs from a given corpus representing unstructured web pages with a tolerance form of rough sets, (ii) a semi-supervised Tolerant Pattern Learning (TPL) algorithm that labels categorical instances as well as relations. This work is an extension of the TPL algorithm presented in our earlier paper. Our model treats noun phrases, which are described as sets of their co-occurring contextual patterns. We use the ontological information from the Never Ending Language Learner (Nell) system. We compared the performance of our algorithm with Coupled Bayesian Sets (CBS) and Coupled Pattern Learner (CPL) algorithms for categorical and relational extractions, respectively. Experimental results suggest that TPL can achieve comparable performance with CBS and CPL in terms of precision.	rough set	Cenker Sengoz;Sheela Ramanna	2015	Pattern Recognition Letters	10.1016/j.patrec.2014.12.005	natural language processing;web mining;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Vision	-25.68245761385897	-69.92067423505561	159193
b37cfe91dafe2ee37c5d368db971399221700273	document summarization using positive pointwise mutual information		The degree of success in document summarization processes depends on the performance of the method used in identifying significant sentences in the documents. The collection of unique words characterizes the major signature of the document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive Pointwise Mutual Information, which works well for measuring semantic similarity in the TermSentence-Matrix, is used in our method to assign weights for each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated from this weighted TSM, is then used to extract a summary from the document. Our experiments show that such a method would outperform most of the existing methods in producing summaries from large documents.	algorithm;automatic summarization;experiment;ibm spectrum protect (tivoli storage manager);latent semantic analysis;pointwise mutual information;semantic similarity;stemming;text corpus;the sentence	S. Aji;M. Ramachandra Kaimal	2012	CoRR		multi-document summarization;computer science;automatic summarization;pattern recognition;data mining;information retrieval	NLP	-26.407942853752168	-67.18296135980262	159490
6cd551196900dac331df7fec6e5a0f96c85e50cc	events beyond ace: curated training for events		We explore a human-driven approach to annotation, curated training (CT), in which annotation is framed as teaching the system by using interactive search to identify informative snippets of text to annotate, unlike traditional approaches which either annotate preselected text or use active learning. A trained annotator performed 80 hours of CT for the thirty event types of the NIST TAC KBP Event Argument Extraction evaluation. Combining this annotation with ACE results in a 6% reduction in error and the learning curve of CT plateaus more slowly than for full-document annotation. 3 NLP researchers performed CT for one event type and showed much sharper learning curves with all three exceeding ACE performance in less than ninety minutes, suggesting that CT can provide further benefits when the annotator deeply understands the system.	ace;information;natural language processing	Ryan Gabbard;Jay DeYoung;Marjorie Freedman	2018	CoRR		natural language processing;active learning;artificial intelligence;nist;computer science;learning curve;event type;annotation	NLP	-21.828896981539806	-70.99931750971365	159745
5ad033e6d5d56eb2765ca615d8b3dd095abeca55	an overview of a distributional word representation for an arabic named entity recognition system		This study attempts to describe and discuss the different approaches and methods dedicated to Named Entity Recognition (NER) systems in various languages, in order to justify the choice of a distributional approach for an Arabic NER system using deep learning methods and a Neural Network word representation (Embeddings) as an add-in feature in the unsupervised learning process.	named-entity recognition	Chaimae Azroumahli;Yacine El Younoussi;Ferdaouss Achbal	2017		10.1007/978-3-319-76357-6_13	machine learning;arabic;unsupervised learning;artificial neural network;deep learning;named-entity recognition;computer science;artificial intelligence	NLP	-20.387249977235538	-71.30507121532084	160081
b3258c34d430f6abae48a283121967944b13f2f5	two improvements to detect duplicates in stack overflow	software;data mining;html;computational modeling;feature extraction;mathematical model;conferences	Stack Overflow is one of the most popular question-and-answer sites for programmers. However, there are a great number of duplicate questions that are expected to be detected automatically in a short time. In this paper, we introduce two approaches to improve the detection accuracy: splitting body into different types of data and using word-embedding to treat word ambiguities that are not contained in the general corpuses. The evaluation shows that these approaches improve the accuracy compared with the traditional method.	microsoft word for mac;programmer;stack overflow	Yuji Mizobuchi;Kuniharu Takayama	2017	2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)	10.1109/SANER.2017.7884678	computer science;data mining;database;world wide web	SE	-22.521558140316873	-66.76974676825478	160216
99fd86ca7cf5ece1b9eb3c45e7a689a1ff32e1ad	learning domain-specific polarity lexicons	databases;polarity detection;motion pictures;training;text analysis;motion pictures accuracy feature extraction databases computational linguistics usa councils training;usa councils;data mining;accuracy;machine learning;feature extraction;sentiment analysis;natural language processing sentiment analysis lexicon adaptation polarity detection machine learning;lexicon adaptation;text analysis domain specific polarity lexicon learning sentiment analysis automatic sentiment estimation general purpose polarity lexicon adaptation algorithm movie reviews hotel dataset movie dataset hotel reviews natural language processing;computational linguistics;text analysis data mining learning artificial intelligence natural language processing;learning artificial intelligence;natural language processing;qa075 electronic computers computer science	Sentiment analysis aims to automatically estimate the sentiment in a given text as positive or negative. Polarity lexicons, often used in sentiment analysis, indicate how positive or negative each term in the lexicon is. However, since creating domain-specific polarity lexicons is expensive and time consuming, researchers often use a general purpose or domain independent lexicon. In this work, we address the problem of adapting a general purpose polarity lexicon to a specific domain and propose a simple yet effective adaptation algorithm. We experimented with two sets of reviews from the hotel and movie domains and observed that while our adaptation techniques changed the polarity values for only a small set of words, the overall test accuracy increased significantly: 77% to 83% in the hotel dataset and 61% to 66% in the movie dataset.	algorithm;lexicon;open-source software;sentiment analysis	Gülsen Demiröz;Berrin A. Yanikoglu;Dilek Tapucu;Yücel Saygin	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.120	natural language processing;text mining;speech recognition;feature extraction;computer science;computational linguistics;machine learning;data mining;accuracy and precision;sentiment analysis	NLP	-22.643144245763747	-67.22364158741549	160505
f547707799a01cf330927836067e158fb7c80c1d	emotionally-relevant features for classification and regression of music lyrics	recognition of group emotion affective computing affective computing applications music retrieval and generation natural language processing;emotion recognition semantics affective computing context music adaptation models feature extraction	This research addresses the role of lyrics in the music emotion recognition process. Our approach is based on several state of the art features complemented by novel stylistic, structural and semantic features. To evaluate our approach, we created a ground truth dataset containing 180 song lyrics, according to Russell's emotion model. We conduct four types of experiments: regression and classification by quadrant, arousal and valence categories. Comparing to the state of the art features (ngrams - baseline), adding other features, including novel features, improved the F-measure from 69.9, 82.7 and 85.6 percent to 80.1, 88.3 and 90 percent, respectively for the three classification experiments. To study the relation between features and emotions (quadrants) we performed experiments to identify the best features that allow to describe and discriminate each quadrant. To further validate these experiments, we built a validation set comprising 771 lyrics extracted from the AllMusic platform, having achieved 73.6 percent F-measure in the classification by quadrants. We also conducted experiments to identify interpretable rules that show the relation between features and emotions and the relation among features. Regarding regression, results show that, comparing to similar studies for audio, we achieve a similar performance for arousal and a much better performance for valence.	baseline (configuration management);emotion recognition;experiment;ground truth;n-gram;statistical classification	Ricardo Malheiro;Renato Panda;Paulo Gomes;Rui Pedro Paiva	2018	IEEE Transactions on Affective Computing	10.1109/TAFFC.2016.2598569	psychology;natural language processing;speech recognition;computer science;affective computing;communication	NLP	-19.770073599243236	-69.01306585251464	160734
7c5ed6d2adaaf98dd09d5e4859a05dc853f1bb99	fine granular aspect analysis using latent structural models	online custom review;sentence-level sentiment polarity classification;fine granular aspect analysis;structural learning model;aspect-level sentiment analysis;preliminary experiment;chinese restaurant review;joint sentiment classification;informative sentence;aspect analysis;latent structural model;aspect-oriented summarization	In this paper, we present a structural learning model for joint sentiment classification and aspect analysis of text at various levels of granularity. Our model aims to identify highly informative sentences that are aspect-specific in online custom reviews. The primary advantages of our model are two-fold: first, it performs document-level and sentence-level sentiment polarity classification jointly; second, it is able to find informative sentences that are closely related to some respects in a review, which may be helpful for aspect-level sentiment analysis such as aspect-oriented summarization. The proposed method was evaluated with 9,000 Chinese restaurant reviews. Preliminary experiments demonstrate that our model obtains promising performance.	aspect-oriented software development;experiment;information;sentiment analysis	Lei Fang;Minlie Huang	2012			natural language processing;speech recognition;data science;data mining;sentiment analysis	NLP	-19.46344241517877	-68.4485444966999	160776
3a3f1656557fe000772d2a01c4667056719d1d3e	open information extraction for spanish language based on syntactic constraints		Open Information Extraction (Open IE) serves for the analysis of vast amounts of texts by extraction of assertions, or relations, in the form of tuples 〈argument 1; relation; argument 2〉. Various approaches to Open IE have been designed to perform in a fast, unsupervised manner. All of them require language specific information for their implementation. In this work, we introduce an approach to Open IE based on syntactic constraints over POS tag sequences targeted at Spanish language. We describe the rules specific for Spanish language constructions and their implementation in EXTRHECH, an Open IE system for Spanish. We also discuss language-specific issues of implementation. We compare EXTRHECH’s performance with that of REVERB, a similar Open IE system for English, on a parallel dataset and show that these systems perform at a very similar level. We also compare EXTRHECH’s performance on a dataset of grammatically correct sentences against its performance on a dataset of random texts extracted from the Web, drastically different in their quality from the first dataset. The latter experiment shows robustness of EXTRHECH on texts from the Web.	experiment;grams;information extraction;n-gram;shallow parsing;unsupervised learning;word lists by frequency;world wide web	Alisa Zhila;Alexander F. Gelbukh	2014		10.3115/v1/P14-3011	natural language processing;computer science;machine learning;data mining;database;linguistics	NLP	-25.946876098546056	-72.20855316696593	160955
41d135f193dde0f4d0760d39e56a84bc90891a44	ocr performance prediction using a bag of allographs and support vector regression	libraries;support vector regression svr;bag of allographs;training;ocr performance prediction;historical documents ocr ocr performance prediction template matching bag of allographs support vector regression svr;optical character recognition software;accuracy;vectors;image edge detection;optical character recognition software vectors accuracy libraries buildings training image edge detection;ocr;historical documents;template matching;buildings	In this paper, we describe a novel and simple technique for prediction of OCR results without using any OCR. The technique uses a bag of allographs to characterize textual components. Then a support vector regression (SVR) technique is used to build a predictor based on the bag of allographs. The performance of the system is evaluated on a corpus of historical documents. The proposed technique produces correct prediction of OCR results on training and test documents within the range of standard deviation of 4.18% and 6.54% respectively. The proposed system has been designed as a tool to assist selection of corpora in libraries and specify the typical performance that can be expected on the selection.	historical document;kerrison predictor;library (computing);optical character recognition;performance prediction;support vector machine;text corpus	Tapan Kumar Bhowmik;Thierry Paquet;Nicolas Ragot	2014	2014 11th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2014.72	speech recognition;template matching;computer science;machine learning;pattern recognition;accuracy and precision	SE	-23.23311174016221	-71.35454525627554	161430
4843d86052c6d13e46c96ea4bdd400d0d8086a91	using machine learning algorithms for author profiling in social media		In this paper we present our approach of solving the PAN 2016 Author Profiling Task. It involves classifying users’ gender and age using social media posts. We used SVM classifiers and neural networks on TF-IDF and verbosity features. Results showed that SVM classifiers are better for English datasets and neural networks perform better for Dutch and Spanish datasets.	algorithm;artificial neural network;machine learning;social media;tf–idf	Daniel Dichiu;Irina Rancea	2016			data science;profiling (computer programming);social media;machine learning;artificial intelligence;computer science	NLP	-21.61745619160142	-69.4035789861438	161662
4366265adef6925ff6323eac780824576b8edb3f	remed: automatic relation extraction from medical documents	text mining;data mining;concept relation;dependency tree parser;data correlation	The large amount of unstructured medical documents written in natural language bears a massive quantity of knowledge, whose extraction becomes useful. An automatic relation identification strategy leads to the discovery of relations, (possible unknown) interactions, and associations between medical conditions, investigations and treatments. The current paper introduces a learning based approach for the automatic discovery of relations between medical concepts, entitled REMed. We propose an original list of features, grouped into four categories with the following distribution: lexical - 3, context - 6, grammatical -- 4 and syntactic - 4. We analyzed the influence of each category on the classification performance and determined that the performance of the REMed solution is comparable with similar solutions. We report the overall F-measure as 74.9% that outperforms the best solution reported in the similar systems with 1.2%. This performance was achieved mostly by the features from the lexical and context categories.	f1 score;interaction;lexicon;natural language;relationship extraction	Mihaela Porumb;Ioana Barbantan;Camelia Lemnaru;Rodica Potolea	2015		10.1145/2837185.2837239	computer science;pattern recognition;data mining;information retrieval	NLP	-24.529544614031416	-67.84894832245472	161909
178473cfa4b513d4dd0e6decff858bf45e336dc6	cuny-blender tac-kbp2010 entity linking and slot filling system description		The CUNY-BLENDER team participated in the following tasks in TAC-KBP2010: Regular Entity Linking, Regular Slot Filling and Surprise Slot Filling task (per:disease slot). In the TAC-KBP program, the entity linking task is considered as independent from or a pre-processing step of the slot filling task. Previous efforts on this task mainly focus on utilizing the entity surface information and the sentence/document-level contextual information of the entity. Very little work has attempted using the slot filling results as feedback features to enhance entity linking. In the KBP2010 evaluation, the CUNY-BLENDER entity linking system explored the slot filling attributes that may potentially help disambiguate entity mentions. Evaluation results show that this feedback approach can achieve 9.1% absolute improvement on micro-average accuracy over the baseline using vector space model. For Regular Slot Filling we describe two bottom-up Information Extraction style pipelines and a top-down Question Answering style pipeline. Experiment results have shown that these pipelines are complementary and can be combined in a statistical re-ranking model. In addition, we present several novel approaches to enhance these pipelines, including query expansion, Markov Logic Networks based cross-slot/cross-system reasoning. Finally, as a diagnostic test, we also measured the impact of using external knowledge base and Wikipedia text mining on Slot Filling.	baseline (configuration management);blender (software);entity linking;feedback;information extraction;knowledge base;markov chain;markov logic network;pipeline (computing);preprocessor;query expansion;question answering;text mining;top-down and bottom-up design;wikipedia	Zheng Chen;Suzanne Tamang;Adam Lee;Xiang Li;Wen-Pin Lin;Matthew G. Snover;Javier Artiles;Marissa Passantino;Heng Ji	2010			computer science;knowledge management;data mining	NLP	-24.832362148650724	-69.61654298577129	162267
368123bff3b3f6d3e79a8211a3ce0f185d27493e	aspect-category-based sentiment classification with aspect-opinion relation	google;electronic mail;touch sensitive screens;support vector machines;training;batteries;sentiment analysis	In recent years, researches of aspect-category-based sentiment analysis have been approached in terms of predefined categories. In this paper, we target two sub-tasks of SemEval-2014 Task 4 dedicated to aspect-based sentiment analysis: detecting aspect category and aspect category polarity. Also, a pre-identified set of aspect categories {food, price, service, ambience, miscellaneous} defined by SemEval-2014 have been used in this paper. The majority of the submissions worked on these two sub-tasks with machine learning mainly with n-grams and sentiment lexicon features. The difficulty for these submissions is that some opinion word (e.g., “good”) is general and cannot be referred to any particular category. By contrast, we use aspect-opinion pairs as one of the features in this paper to overcome this difficulty. To detect these pairs, we identify the opinion words in customer reviews, and then detect their related aspect terms by dependency rule. This system has been done on restaurant domain applying to Chinese customer reviews. Our experiment achieved 87.5% of accuracy by using Word2Vec to detect aspect category polarity. Aspect-opinion pair features employed in this system contribute to 88.3% of accuracy. When all features are employed, the accuracy is improved from 84.4% to 89.0%. Experimental results demonstrate the effectiveness of aspect-opinion pair features applied to the aspect-category-based sentiment classification system.	airline control program (acp);algorithm;grams;lexicon;machine learning;multi-label classification;n-gram;semeval;sensor;sentiment analysis;word2vec	Yi-Lin Tsai;Yu-Chun Wang;Chen-Wei Chung;Shih-Chieh Su;Richard Tzong-Han Tsai	2016	2016 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2016.7880153	engineering;data science;data mining;information retrieval;sentiment analysis	NLP	-23.000259478724708	-66.71159003910228	162337
2ed42a85b319f8ae69cd87e16a7520289a9a70e0	argument labeling of explicit discourse relations using lstm neural networks		Argument labeling of explicit discourse relations is a challenging task. The state of the art systems achieve slightly above 55% F-measure but require hand-crafted features. In this paper, we propose a Long Short Term Memory (LSTM) based model for argument labeling. We experimented with multiple configurations of our model. Using the PDTB dataset, our best model achieved an F1 measure of 23.05% without any feature engineering. This is significantly higher than the 20.52% achieved by the state of the art RNN approach, but significantly lower than the feature based state of the art systems. On the other hand, because our approach learns only from the raw dataset, it is more widely applicable to multiple textual genres and languages.	daisy digital talking book;f1 score;feature engineering;long short-term memory;neural networks;random neural network	Sohail Hooda;Leila Kosseim	2017		10.26615/978-954-452-049-6_042	natural language processing;machine learning;artificial intelligence;artificial neural network;long short term memory;feature engineering;computer science	NLP	-19.329182513734146	-71.6956970632796	162381
297419a5a299f413a877967ad510df6cdadbfd70	a probabilistic model for japanese zero pronoun resolution integrating syntactic and semantic features	probabilistic model	This paper proposes a method to resolve Japanese zero pronouns by identifying their antecedents. Our method uses a probabilistic model, which is decomposed into syntactic and semantic properties. A syntactic model is trained based on corpora annotated with anaphoric relations. However, a semantic model is trained based on a large-scale unannotated corpus, so as to counter the data sparseness problem. We also propose the notion of certainty to improve the accuracy of zero pronoun resolution. We show the effectiveness of our proposed method by way of experiments.	anaphora (linguistics);experiment;neural coding;statistical model;text corpus	Kazuhiro Seki;Atsushi Fujii;Tetsuya Ishikawa	2001			semantic data model;semantic property;divergence-from-randomness model;syntax;natural language processing;pronoun;certainty;statistical model;artificial intelligence;computer science	NLP	-21.460423087808206	-73.14274339621242	162711
2c41fb6c7a1297a09a045f8c8f4fbba8800229a3	parma: a predicate argument aligner		We introduce PARMA, a system for crossdocument, semantic predicate and argument alignment. Our system combines a number of linguistic resources familiar to researchers in areas such as recognizing textual entailment and question answering, integrating them into a simple discriminative model. PARMA achieves state of the art results on an existing and a new dataset. We suggest that previous efforts have focussed on data that is biased and too easy, and we provide a more difficult dataset based on translation data with a low baseline which we beat by 17% F1.	authorization;baseline (configuration management);benchmark (computing);discriminative model;ibm notes;lemmatisation;question answering;realms of the haunting;supervised learning;textual entailment	Travis Wolfe;Benjamin Van Durme;Mark Dredze;Nicholas Andrews;Charley Beller;Chris Callison-Burch;Jay DeYoung;Justin Snyder;Jonathan Weese;Tan Xu;Xuchen Yao	2013			computer science;artificial intelligence;data mining;algorithm	NLP	-24.069077928880816	-73.13799856392484	162834
3bad279df8e04c80bd9f8d9295d93d1a191626da	an automated learner-based reading ability estimation strategy using concept indexing with integrated part-of-speech n-gram features	lg individual institutions asia africa;pe english;l education general;qa75 electronic computers computer science	This study is about the development of a retrainable reading ability estimation system based on concepts from the Text Readability Indexing (TRI) domain. This system aims to promote self-directed language learning and to serve as an educational reinforcement tool for English language learners. Student essays were used to calibrate the system which provided realistic approximations of their actual reading levels. In this thesis, we compared the performance of two vector semantics-based algorithms, namely, Latent Semantic Indexing (LSI) and Concept Indexing (CI) for content analysis. Since these algorithms rely on the bag-of-words approach and inherently lack grammatical analysis, we augmented them using Part-of-Speech (POS) n-gram features to approximate the syntactic complexity of text documents. Results show that directly combining the contentand grammar-based feature sets yielded lower classification accuracies than utilising each feature set alone. Using a sparsification strategy, we were able to optimise the combination process and, with the integration of POS bi-grams, we achieved our overall highest mean exact agreement accuracies (MEAA) of 0.924 and 0.952 for LSI and CI, respectively. We have also conducted error analyses on our results where we examined overestimation and underestimation error types to uncover the probable causes for the systems’ misclassifications.	approximation algorithm;bag-of-words model in computer vision;grams;mean squared error;n-gram;part-of-speech tagging	Abigail R. Razon	2017			natural language processing;speech recognition;computer science;artificial intelligence	NLP	-22.0836293432778	-73.16850358109444	163194
0963dc781b915feac3ea225ed723380299700537	can human verb associations help identify salient features for semantic verb classification?		This paper investigates whether human associations to verbs as collected in a web experiment can help us to identify salient verb features for semantic verb classes. Assuming that the associations model aspects of verb meaning, we apply a clustering to the verbs, as based on the associations, and validate the resulting verb classes against standard approaches to semantic verb classes, i.e. GermaNet and FrameNet. Then, various clusterings of the same verbs are performed on the basis of standard corpus-based types, and evaluated against the association-based clustering as well as GermaNet and FrameNet classes. We hypothesise that the corpusbased clusterings are better if the instantiations of the feature types show more overlap with the verb associations, and that the associations therefore help to identify salient feature types.	cluster analysis;framenet;germanet	Sabine Schulte im Walde	2006				NLP	-25.240912680952846	-72.27400086313443	163521
f1d6b10a146205c1ce9a2e8b86a0be8a6dd4fefb	extracting drug-drug interactions from literature using a rich feature-based linear kernel approach.	drug drug interaction;linear kernel approach;biomedical literature	Identifying unknown drug interactions is of great benefit in the early detection of adverse drug reactions. Despite existence of several resources for drug-drug interaction (DDI) information, the wealth of such information is buried in a body of unstructured medical text which is growing exponentially. This calls for developing text mining techniques for identifying DDIs. The state-of-the-art DDI extraction methods use Support Vector Machines (SVMs) with non-linear composite kernels to explore diverse contexts in literature. While computationally less expensive, linear kernel-based systems have not achieved a comparable performance in DDI extraction tasks. In this work, we propose an efficient and scalable system using a linear kernel to identify DDI information. The proposed approach consists of two steps: identifying DDIs and assigning one of four different DDI types to the predicted drug pairs. We demonstrate that when equipped with a rich set of lexical and syntactic features, a linear SVM classifier is able to achieve a competitive performance in detecting DDIs. In addition, the one-against-one strategy proves vital for addressing an imbalance issue in DDI type classification. Applied to the DDIExtraction 2013 corpus, our system achieves an F1 score of 0.670, as compared to 0.651 and 0.609 reported by the top two participating teams in the DDIExtraction 2013 challenge, both based on non-linear kernel methods.		Sun Kim;Haibin Liu;Lana Yeganova;W. John Wilbur	2014	Journal of biomedical informatics	10.1016/j.jbi.2015.03.002	computer science;bioinformatics;artificial intelligence;machine learning;pattern recognition;data mining;database	NLP	-23.411434878512214	-67.77938624676649	163579
bf78d82330b5b2b3f96b685737a7426a4a2706c2	evaluating quality of word embeddings with sentiment polarity identification task		Neural word embeddings have been widely used in modern NLP applications as they provide vector representation of words and capture the semantic properties of words and the linguistic relationship between the words. Many research groups have released their own version of word embeddings. However, they are trained on generic corpora, which limits their direct use for domain specific tasks. In this paper, we evaluate a set of pretrained word embeddings which were provided to us, on a standard NLP task - Sentiment Polarity Identification Task.		Vijayasaradhi Indurthi;Subba Reddy Oota	2018		10.1007/978-3-030-00072-1_18	semantic property;sentiment analysis;natural language processing;artificial intelligence;computer science	NLP	-21.147056521042778	-72.59817665401407	163641
af387975297615b883703ce0c35984ac28f2092b	equidistant nodes clustering: a soft clustering algorithm applied for synset induction				Mikhail Chernoskutov;Dmitry Ustalov	2018				AI	-24.961097554049548	-67.48105867386732	164005
1908e93bfa8ee6f1707a2513095e48945823727a	fine-grained entity typing with high-multiplicity assignments		As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.	entity;semiconductor industry;structured prediction;testbed;text corpus;type system;typing;wikipedia	Maxim Rabinovich;Dan Klein	2017		10.18653/v1/P17-2052	machine learning;natural language processing;computer science;artificial intelligence;typing;multiplicity (mathematics)	NLP	-25.918781235811416	-71.50883709311584	164031
c56f6afaac1463120d2ba30579e9818c78f02971	conditional random fields for term extraction		In this paper, we describe how to construct a machine learning framework that utilizes syntactic information in extraction of biomedical terms. Conditional random fields (CRF), is used as the basis of this framework. We make an effort to find the appropriate use for syntactic information, including parent nodes, syntactic paths and term ratios under the machine learning framework. The experiment results show that syntactic paths and term ratios can improve precision of term extraction, including old terms and novel terms. However, the recall rate of novel terms still needs to be increased. This research serves as an example for constructing machine learning based term extraction systems that utilizes linguistic information.	conditional random field;machine learning;sensitivity and specificity;terminology extraction	Xing Zhang;Yan Song;Alex Chengyu Fang	2010			random field	NLP	-24.691773402793427	-69.19419497454179	164362
20cc937ec9a6a2c8abeb308241cd43f2d3608987	chinese word sense induction with basic clustering algorithms		Word Sense Induction (WSI) is an important topic in natural langage processing area. For the bakeoff task Chinese Word Sense Induction (CWSI), this paper proposes two systems using basic clustering algorithms, k-means and agglomerative clustering. Experimental results show that k-means achieves a better performance. Based only on the data provided by the task organizers, the two systems get FScores of 0.7812 and 0.7651 respectively.	algorithm;cluster analysis;k-means clustering;microsoft word for mac;wafer-scale integration;word sense;word-sense induction	Yuxiang Jia;Shiwen Yu;Zhengyan Chen	2010			cluster analysis;word-sense induction;artificial intelligence;computer science;pattern recognition	AI	-24.45984712246229	-68.40283826190603	164396
8504e70d21c443a538f2758a3e6f8f8fd2c4a173	supervised word sense disambiguation with support vector machines and multiple knowledge sources		We participated in the SENSEVAL-3 English lexical sample task and multilingual lexical sample task. We adopted a supervised learning approach with Support Vector Machines, using only the official training data provided. No other external resources were used. The knowledge sources used were partof-speech of neighboring words, single words in the surrounding context, local collocations, and syntactic relations. For the translation and sense subtask of the multilingual lexical sample task, the English sense given for the target word was also used as an additional knowledge source. For the English lexical sample task, we obtained fine-grained and coarse-grained score (for both recall and precision) of 0.724 and 0.788 respectively. For the multilingual lexical sample task, we obtained recall (and precision) of 0.634 for the translation subtask, and 0.673 for the translation and sense subtask.	collocation;precision and recall;supervised learning;support vector machine;web services for devices;word sense;word-sense disambiguation	Yoong Keok Lee;Hwee Tou Ng;Tee Kiah Chia	2004			natural language processing;support vector machine;artificial intelligence;machine learning;computer science;word-sense disambiguation	NLP	-23.97834436082086	-71.0379764869975	164460
8102a64a0f4b353f9a9e15ef671469270fdfd1f8	on the application of the cyc ontology to word sense disambiguation	higher order;first order;concept map	This paper describes a novel, unsupervised method of word sense disambiguation that is wholly semantic, drawing upon a complex, rich ontology and inference engine (the Cyc system). This method goes beyond more familiar semantic closeness approaches to disambiguation that rely on string cooccurrence or relative location in a taxonomy or concept map by 1) exploiting a rich array of properties, including higher-order properties, not available in merely taxonomic (or other first-order) systems, and 2) appealing to the semantic contribution a word sense makes to the content of the target text. Experiments show that this method produces results markedly better than chance when disambiguating word senses in a corpus of topically unrelated documents.	centrality;concept map;cyc;experiment;first-order predicate;inference engine;taxonomy (general);text corpus;word sense;word-sense disambiguation	Jon Curtis;John Cabral;David Baxter	2006			concept map;machine learning;artificial intelligence;natural language processing;word-sense disambiguation;ontology;semeval;computer science	NLP	-26.067844019996528	-71.42018578909152	164570
ceb4395396c5c676c65ab4aa5c12bf6625290534	performance evaluation of knowledge extraction methods		This paper shows the precision, the recall and the F-measure for the knowledge extraction methods (under Open Information Extraction paradigm): ReVerb, OLLIE and ClausIE. For obtaining these three measures a subset of 55 newswires corpus was used. This subset was taken from the Reuters-21578 text categorization and test collection database. A handmade relation extraction was applied for each one of these newswires.	categorization;document classification;f1 score;information extraction;performance evaluation;programming paradigm;relationship extraction	Juan M. Rodríguez;Hernán D. Merlino;Patricia Pesado;Ramón García-Martínez	2016		10.1007/978-3-319-42007-3_2	information extraction;knowledge extraction;information retrieval;relationship extraction;recall;categorization;computer science	NLP	-25.163869452152923	-67.4489502407191	164833
7990138a515db968ebe0c85e0c529ced3bc3bf67	exploring linguistic features for extremist texts detection (on the material of russian-speaking illegal texts)		In this paper we present results of a research on automatic extremist text detection. For this purpose an experimental dataset in the Russian language was created. According to the Russian legislation we cannot make it publicly available. We compared various classification methods (multinomial naive Bayes, logistic regression, linear SVM, random forest, and gradient boosting) and evaluated the contribution of differentiating features (lexical, semantic and psycholinguistic) to classification quality. The results of experiments show that psycholinguistic and semantic features are promising for extremist text detection.	experiment;gradient boosting;machine learning;multinomial logistic regression;naive bayes classifier;random forest;sentiment analysis;text corpus;unbalanced circuit	Dmitry Devyatkin;Ivan Smirnov;Ananyeva Margarita;Kobozeva Maria;Chepovskiy Andrey;Solovyev Fyodor	2017	2017 IEEE International Conference on Intelligence and Security Informatics (ISI)	10.1109/ISI.2017.8004907	support vector machine;natural language processing;gradient boosting;naive bayes classifier;data mining;random forest;logistic regression;computer science;feature extraction;semantics;artificial intelligence;pragmatics;pattern recognition	Vision	-21.524552958653302	-67.9419040007182	164896
42bc19703f7aba8182eab8de2214d5fc47b1dafe	base noun phrase translation using web data and the em algorithm	tf-idf vector;web data;possible translation;base np;translation candidate;base noun phrase translation;em algorithm;new method;noun phrase	We consider here the problem of Base Noun Phrase translation. We propose a new method to perform the task. For a given Base NP, we first search its translation candidates from the web. We next determine the possible translation(s) from among the candidates using one of the two methods that we have developed. In one method, we employ an ensemble of Naïve Bayesian Classifiers constructed with the EM Algorithm. In the other method, we use TF-IDF vectors also constructed with the EM Algorithm. Experimental results indicate that the coverage and accuracy of our method are significantly better than those of the baseline methods relying on existing technologies.	baseline (configuration management);bayesian network;effective method;expectation–maximization algorithm;naive bayes classifier;tf–idf	Yunbo Cao;Hang Li	2002			natural language processing;noun phrase;speech recognition;expectation–maximization algorithm;computer science;pattern recognition;linguistics	NLP	-24.83779705322598	-67.33828068582292	165392
c8cf4d0d4d6276391ac98ef34f85cebcafbb328c	unsupervised concept categorization and extraction from scientific document titles		This paper studies the automated categorization and extraction of scientific concepts from titles of scientific articles, in order to gain a deeper understanding of their key contributions and facilitate the construction of a generic academic knowledgebase. Towards this goal, we propose an unsupervised, domain-independent, and scalable two-phase algorithm to type and extract key concept mentions into aspects of interest (e.g., Techniques, Applications, etc.). In the first phase of our algorithm we proposePhraseType, a probabilistic generative model which exploits textual features and limited POS tags to broadly segment text snippets into aspect-typed phrases. We extend this model to simultaneously learn aspect-specific features and identify academic domains in multi-domain corpora, since the two tasks mutually enhance each other. In the second phase, we propose an approach based on adaptor grammars to extract fine grained concept mentions from the aspect-typed phrases without the need for any external resources or human effort, in a purely data-driven manner. We apply our technique to study literature from diverse scientific domains and show significant gains over state-of-the-art concept extraction techniques. We also present a qualitative analysis of the results obtained.	algorithm;categorization;generative model;knowledge base;open road tolling;scalability;scientific literature;spec#;text corpus;two-phase commit protocol;unsupervised learning	Adit Krishnan;Aravind Sankar;Shi Zhi;Jiawei Han	2017		10.1145/3132847.3133023	information retrieval;data mining;computer science;categorization;probabilistic logic;generative model;scalability;statistical model;exploit;machine learning;artificial intelligence;rule-based machine translation	NLP	-25.83785275671392	-69.00747756406733	165521
d01bf5e7e9be7354c25c385f2e622d1a2ce445d2	annotating zero anaphora for question answering		We constructed a large annotated dataset of zero pronouns that correspond to adjuncts marked by -de (translated to English as in, at, by or with) in Japanese. Adjunct zero anaphora resolution plays an important role in extracting information such as location and means from a text. To our knowledge, however, there have been no large-scale dataset covering them. In this paper, focusing on the application of zero anaphora resolution to question answering (QA), we proposed two annotation schemes. The first scheme was designed to efficiently collect zero anaphora instances that are useful in QA. Instead of directly annotating zero anaphora, annotators evaluated QA instances whose correctness hinges on zero anaphora resolution. Over 20,000 instances of zero anaphora were collected with this scheme. We trained a multi-column convolutional neural network with the annotated data, achieving an average precision of 0.519 in predicting the correctness of QA instances of the same type. In the second scheme, zero anaphora is annotated in a more direct manner. A model trained with the results of the second annotation scheme performed better than the first scheme in identifying zero anaphora for sentences randomly sampled from a corpus, suggesting a tradeoff between application-specific and general-purpose annotation schemes.	anaphora (linguistics);artificial neural network;convolutional neural network;correctness (computer science);general-purpose markup language;information retrieval;question answering;randomness;software quality assurance	Yoshihiko Asao;Ryu Iida;Kentaro Torisawa	2018			speech recognition;artificial intelligence;natural language processing;question answering;computer science	NLP	-19.66221290040198	-72.36937319461711	166386
b3af2ea887f8df2a417c0ab97e96ae9e8f00fda9	recent advances in machine translation using comparable corpora		This paper highlights some of the recent developments in the field of machine translation using comparable corpora. We start by updating previous definitions of comparable corpora and then look at bilingual versions of continuous vector space models. Recently, neural networks have been used to obtain latent context representations with only few dimensions which are often called word embeddings. These promising new techniques cannot only be applied to parallel but also to comparable corpora. Subsequent sections of the paper discuss work specifically targeting at machine translation using comparable corpora, as well as work dealing with the extraction of parallel segments from comparable corpora. Finally, we give an overview on the design and the results of a recent shared task on measuring document comparability across languages.	machine translation;text corpus	Reinhard Rapp;Serge Sharoff;Pierre Zweigenbaum	2016	Natural Language Engineering	10.1017/S1351324916000115	natural language processing	NLP	-21.300483787112427	-72.72482293138444	166575
ddeab6770134141415e36d3f0427aea11bc62697	some empirical evidence for annotation noise in a benchmarked dataset	empirical evidence	A number of recent articles in computational linguistics venues called for a closer examination of the type of noise present in annotated datasets used for benchmarking (Reidsma and Carletta, 2008; Beigman Klebanov and Beigman, 2009). In particular, Beigman Klebanov and Beigman articulated a type of noise they call annotation noise and showed that in worst case such noise can severely degrade the generalization ability of a linear classifier (Beigman and Beigman Klebanov, 2009). In this paper, we provide quantitative empirical evidence for the existence of this type of noise in a recently benchmarked dataset. The proposed methodology can be used to zero in on unreliable instances, facilitating generation of cleaner gold standards for benchmarking.	best, worst and average case;computational linguistics;linear classifier	Beata Beigman Klebanov;Eyal Beigman	2010			speech recognition;empirical evidence;computer science;data science;machine learning;data mining;statistics	NLP	-21.024938505553408	-71.09313398452737	167338
f9e6bb818c7f83517e47ce8cf06552ad213d0161	recognizing textual entailment using inference phenomenon		Inference phenomena refer to inference relations in local fragments between two texts. Current research on inference phenomenon focuses on the construction of data annotation, whereas there are few research on how to identify those inference phenomena in texts, which will contributes to improving the performance of recognizing textual entailment. This paper proposes an approach, which uses inference phenomena to recognize entailment in texts. In the approach, the task of recognizing textual entailment is formalized as two problems, that is, inference phenomenon identification and entailment judgment, then a joint model is employed to combine such two related subtasks, which is helpful to avoid error propagation. Experimental results show that the approach performs efficiently for identifying inference phenomena and recognizing entailment at the same time.	textual entailment	Han Ren;Xia Li;Wenhe Feng;Jing Wan	2017		10.1007/978-3-319-73573-3_26	natural language processing;textual entailment;data annotation;inference;logical consequence;propagation of uncertainty;artificial intelligence;computer science;phenomenon	NLP	-19.74443469629162	-69.50067350497753	167546
a0d9e8fdd17ec96900529e59b56e69441152ff9b	extracting medication information from discharge summaries	discharge summary;statistical approach;hybrid system;statistical classifier;clinical record;good performance;top system;extracting medication information;additional resource;medication name list;medication information extraction	Extracting medication information from clinical records has many potential applications and was the focus of the i2b2 challenge in 2009. We present a hybrid system, comprised of machine learning and rule-based modules, for medication information extraction. With only a handful of template-filling rules, the system’s core is a cascade of statistical classifiers for field detection. It achieved good performance that was comparable to the top systems in the i2b2 challenge, demonstrating that a heavily statistical approach can perform as well or better than systems with many sophisticated rules. The system can easily incorporate additional resources such as medication name lists to further improve performance.	hybrid system;information extraction;logic programming;machine learning;statistical classification;statistical model	Scott R. Halgrim;Fei Xia;Imre Solti;Eithon Cadag;Özlem Uzuner	2010			computer science;data science;data mining;information retrieval	NLP	-23.80805378409203	-69.88437857676857	167559
248380e4b3cc91a87bfb11d29fb95125496dd2c9	towards language independent automated learning of text categorisation models		We describe the results of extensive machine learning experiments on large collections of Reuters’ English and German newswires. The goal of these experiments was to automatically discover classification patterns that can be used for assignment of topics to the individual newswires. Our results with the English newswire collection show a very large gain in performance as compared to published benchmarks, while our initial results with the German newswires appear very promising. We present our methodology, which seems to be insensitive to the language of the document collections, and discuss issues related to the differences in results that we have obtained for the two collections.	benchmark (computing);categorization;document classification;experiment;machine learning	Chidanand Apté;Fred J. Damerau;Sholom M. Weiss	1994				ML	-25.622114544832364	-68.69350580907395	167927
dd75bb21b4f6d5bebcdcf3afdfac7264abad494c	a feature extraction method based on word embedding for word similarity computing		In this paper, we introduce a new NLP task similar to word expansion task or word similarity task, which can discover words sharing the same semantic components (feature sub-space) with seed words. We also propose a Feature Extraction method based on Word Embeddings for this problem. We train word embeddings using state-of-the-art methods like word2vec and models supplied by Stanford NLP Group. Prior Statistical Knowledge and Negative Sampling are proposed and utilized to help extract the Feature Sub-Space. We evaluate our model on WordNet synonym dictionary dataset and compare it to word2vec on synonymy mining and word similarity computing task, showing that our method outperforms other models or methods and can significantly help improve language understanding.	dictionary;feature extraction;gibbs sampling;information retrieval;microsoft word for mac;natural language processing;natural language understanding;sampling (signal processing);word embedding;word2vec;wordnet	Weitai Zhang;Weiran Xu;Guang Chen;Jun Guo	2014		10.1007/978-3-662-45924-9_15	natural language processing;speech recognition;pattern recognition	NLP	-25.861117993989602	-67.66228343759491	168060
82001a554b72cd44707b78cae58f7a093ca07080	towards role-based filtering of disease outbreak reports	named entities;noun;information extraction;text mining;statistical significance;text classification;support vector machine;infectious disease;named entity;public health;disease outbreak;semantic roles	"""This paper explores the role of named entities (NEs) in the classification of disease outbreak report. In the annotation schema of BioCaster, a text mining system for public health protection, important concepts that reflect information about infectious diseases were conceptually analyzed with a formal ontological methodology and classified into types and roles. Types are specified as NE classes and roles are integrated into NEs as attributes such as a chemical and whether it is being used as a therapy for some infectious disease. We focus on the roles of NEs and explore different ways to extract, combine and use them as features in a text classifier. In addition, we investigate the combination of roles with semantic categories of disease-related nouns and verbs. Experimental results using naïve Bayes and Support Vector Machine (SVM) algorithms show that: (1) roles in combination with NEs improve performance in text classification, (2) roles in combination with semantic categories of noun and verb features contribute substantially to the improvement of text classification. Both these results were statistically significant compared to the baseline """"raw text"""" representation. We discuss in detail the effects of roles on each NE and on semantic categories of noun and verb features in terms of accuracy, precision/recall and F-score measures for the text classification task."""		Son Doan;Ai Kawazoe;Mike Conway;Nigel Collier	2009	Journal of biomedical informatics	10.1016/j.jbi.2008.12.009	natural language processing;noun;support vector machine;semantic role labeling;text mining;public health;infectious disease;computer science;pattern recognition;data mining;statistical significance;outbreak;information extraction	NLP	-23.6933787114591	-68.41411334339035	168472
daa9afe2865446b553c95c14ebc241288e7a87cb	directional distributional similarity for lexical inference	symmetric similarity measure;different nlp datasets;lexical inference;similarity measure;distributional word similarity;directional relation;directional measure;average precision;symmetric relation;directional distributional similarity;lexical semantics	Distributional word similarity is most commonly perceived as a symmetric relation. Yet, directional relations are abundant in lexical semantics and in many Natural Language Processing (NLP) settings that require lexical inference, making symmetric similarity measures less suitable for their identification. This paper investigates the nature of directional (asymmetric) similarity measures that aim to quantify distributional feature inclusion. We identify desired properties of such measures for lexical inference, specify a particular measure based on Average Precision that addresses these properties, and demonstrate the empirical benefit of directional measures for two different NLP datasets.	information retrieval;lexical substitution;natural language processing;operational semantics	Lili Kotlerman;Ido Dagan;Idan Szpektor;Maayan Zhitomirsky-Geffet	2010	Natural Language Engineering	10.1017/S1351324910000124	natural language processing;pattern recognition;data mining	NLP	-25.954637852943364	-70.6906660137261	168491
ba5d8ccb60fe5103299234c5c6443d1db4d2ffc7	the biomedical discourse relation bank	corpus annotation;software;discourse processing;text mining;evaluation method;semantics;data mining;computational biology bioinformatics;open access;algorithms;humans;combinatorial libraries;classification accuracy;computational biology;computer appl in life sciences;natural language processing;microarrays;bioinformatics	Identification of discourse relations, such as causal and contrastive relations, between situations mentioned in text is an important task for biomedical text-mining. A biomedical text corpus annotated with discourse relations would be very useful for developing and evaluating methods for biomedical discourse processing. However, little effort has been made to develop such an annotated resource. We have developed the Biomedical Discourse Relation Bank (BioDRB), in which we have annotated explicit and implicit discourse relations in 24 open-access full-text biomedical articles from the GENIA corpus. Guidelines for the annotation were adapted from the Penn Discourse TreeBank (PDTB), which has discourse relations annotated over open-domain news articles. We introduced new conventions and modifications to the sense classification. We report reliable inter-annotator agreement of over 80% for all sub-tasks. Experiments for identifying the sense of explicit discourse connectives show the connective itself as a highly reliable indicator for coarse sense classification (accuracy 90.9% and F1 score 0.89). These results are comparable to results obtained with the same classifier on the PDTB data. With more refined sense classification, there is degradation in performance (accuracy 69.2% and F1 score 0.28), mainly due to sparsity in the data. The size of the corpus was found to be sufficient for identifying the sense of explicit connectives, with classifier performance stabilizing at about 1900 training instances. Finally, the classifier performs poorly when trained on PDTB and tested on BioDRB (accuracy 54.5% and F1 score 0.57). Our work shows that discourse relations can be reliably annotated in biomedical text. Coarse sense disambiguation of explicit connectives can be done with high reliability by using just the connective as a feature, but more refined sense classification requires either richer features or more annotated data. The poor performance of a classifier trained in the open domain and tested in the biomedical domain suggests significant differences in the semantic usage of connectives across these domains, and provides robust evidence for a biomedical sublanguage for discourse and the need to develop a specialized biomedical discourse annotated corpus. The results of our cross-domain experiments are consistent with related work on identifying connectives in BioDRB.	biomedical text mining;body of uterus;causal filter;conferences;daisy digital talking book;discourse relation;eighty;elegant degradation;experiment;explicit substitution;f1 score;inter-rater reliability;logical connective;mind-body relations, metaphysical;sparse matrix;statistical classification;sublanguage;text corpus;treebank;uterine corpus carcinosarcoma;word-sense disambiguation	Rashmi Prasad;Susan McRoy;Nadya Frid;Aravind K. Joshi;Hong Yu	2010		10.1186/1471-2105-12-188	natural language processing;text mining;dna microarray;computer science;bioinformatics;data science;semantics;information retrieval	NLP	-24.57335889125926	-69.20245695476237	169131
def9c06279873361d448e86086015299c1c57034	sentimental analysis for aiml-based e-health conversational agents		Conversational agents or chat-bots are emerging in various applications including finance, education and e-health. Recent research has highlighted the importance of the consistency between the response of the chat-bot and the sentiment of the input utterance. This is quite challenging as detecting the sentiment of an utterance often depends on the context and timing of the conversation. Moreover, whereas humans have complex repair strategies, encoding these for human-computer interaction is problematic. This paper presents five sentiment prediction models for conversational agents that are trained on a large corpus of smartphone application reviews and their sentiment ranks obtained from the Google playstore. These models are tested on collected, real-life conversations between a human and a machine. It is found that positive utterances are classified with a high accuracy but classifying negative utterances is still challenging.		David Ireland;Hamed Hassanzadeh;Son N. Tran	2018		10.1007/978-3-030-04179-3_4	machine learning;conversation;natural language processing;encoding (memory);sentiment analysis;utterance;aiml;computer science;artificial intelligence	NLP	-19.448628675761828	-68.45877482432091	169655
3f257dfd148a7b28664139c3e0d550e8d2223fe1	rosemerry: a baseline message-level sentiment classification system		In this paper, we propose a baseline messagelevel sentiment classification method, as developed for SemEval-2015 Task 10, Subtask B. This system leverages both hand-crafted features and message-level embedding features, and uses an SVM classifier for messagelevel sentiment classification. In pre-training the embedding features, we use one million randomly-selected tweets. We present results over SemEval-2015 Task 10, Subtask B, as well as the Stanford Sentiment Treebank. Our experiments show the effectiveness of our method over both datasets.	baseline (configuration management);experiment;randomness;semeval;the australian;treebank	Huizhi Liang;Richard Fothergill;Timothy Baldwin	2015		10.18653/v1/S15-2092	computer science;pattern recognition;data mining;information retrieval	NLP	-21.69824355868312	-69.9218359332387	169721
f585a9a1de10967c8eb1d884bd1ea5bff49b87f0	using neutral examples for learning polarity	learning model;test bed;machine learning;sentiment analysis	Sentiment analysis is an example of polarity learning. Most research on learning to identify sentiment ignores “neutral” examples and instead performs training and testing using only examples of significant polarity. We show that it is crucial to use neutral examples in learning polarity for a variety of reasons and show how neutral examples help us obtain superior classification results in two sentiment analysis test-beds. Many machine-learning problems involve predicting an example’s polarity: is it (significantly) greater than or less than some standard. One canonical example of learning polarity is sentiment analysis, the determination of whether a particular text expresses positive or negative sentiment regarding some issue. The problem of how to exploit a labeled corpus to learn models for sentiment analysis has attracted a good deal of interest in recent years [Dave et al 2003, Pang et al 2002, Shanahan et al 2005]. One common characteristic of almost all this work has been the tendency to define the task as a two-category problem: positive versus negative. In almost all actual polarity problems, including sentiment analysis, there are, however, three categories that must be distinguished: positive, negative and neutral. Not every comment on a product or experience expresses purely positive or negative sentiment. Some – in many cases, most – comments might report objective facts without expressing any sentiment, while others might express mixed or conflicting sentiment. Researchers are aware, of course, of the existence of neutral documents. The rationale for ignoring them has been a reliance on two tacit assumptions: • Solving the binary positive vs. negative problem automatically solves the three-category problem since neutral documents will simply lie near the boundary of the binary model • There is less to learn from neutral documents than from documents with clearly defined sentiment The purpose of this paper is to show that there is no basis for either of those myths and that neutrals can be exploited in interesting ways to great effect. We consider two labeled corpora. The first consists of 1974 posts to chat groups devoted to popular U.S. television shows. The second consists of about 14,000 posts to shopping.com’s product evaluation pages. Both are equally distributed among positive, negative and neutral documents. Is it in fact the case that neutral documents lie near the boundary of a learned model that distinguishes positive and negative examples? To test this, we trained a linear SVM on all positive and negative documents in the TV corpus. In Figure 1, we show the signed distance from the boundary of the positive and negative training examples, in ascending order from left to right. (This SVM correctly classifies 79.1% of the training examples.) In addition, we show the signed distance from the boundary of all neutral examples. There is no band near the boundary in which the preponderance of examples is neutral. We indicate the band around the boundary that is optimal in terms of overall classification accuracy (positive, negative, or neutral) when all examples in the band are classed as neutral. Even using this optimal band, we attain accuracy of only 54.8%. (Note that simply using the SVM boundary to distinguish positive from negative and not classifying any examples as neutral would yield accuracy of 52.7%.) -3 -2 -1 0 1 2 3 pos	design rationale;document;machine learning;point of sale;sentiment analysis;sorting;text corpus;word lists by frequency	Moshe Koppel;Jonathan Schler	2005			computer science;artificial intelligence;machine learning;pattern recognition;sentiment analysis;testbed	NLP	-20.579977668070267	-67.43831969702649	170236
a7d0674d00270b4a6012abb9d32e97f6e746fe9c	classstruggle: a clustering based text segmentation	clustering;text segmentation	This paper describes ClassStruggle, an algorithm for linear text segmentation on general corpuses. It relies on an initial clustering of the sentences of the text. This preliminary partitioning provides a global view on the sentences relations existing in the text, considering the similarities in a group rather than individually. ClassStruggle is based on the distribution of the occurrences of the members of each class. During the process, the clusters then evolve, by considering a notion of proximity and of layout in the text, in the aim to create groups that contain only sentences related to a same topic development. Finally, boundaries are created between sentences belonging to two different classes. First experimental results are promising, ClassStruggle appears to be very competitive compared with existing methods.	algorithm;cluster analysis;text segmentation	Sylvain Lamprier;Tassadit Amghar;Bernard Levrat;Frédéric Saubion	2007		10.1145/1244002.1244140	natural language processing;text segmentation;computer science;machine learning;pattern recognition;data mining;cluster analysis	NLP	-25.085433037765174	-71.99762069641167	170387
12bb048034a2842b8197580606ea51021f154962	unsupervised document classification using sequential information maximization	cluster algorithm;n grams;multi document summarization;naive bayes classifier;passage similarity;term weights;clustering;clustering method;summary;space complexity;document classification;information bottleneck	We present a novel sequential clustering algorithm which is motivated by the Information Bottleneck (IB) method. In contrast to the agglomerative IB algorithm, the new sequential (sIB) approach is guaranteed to converge to a local maximum of the information with time and space complexity typically linear in the data size. information, as required by the original IB principle. Moreover, the time and space complexity are significantly improved. We apply this algorithm to unsupervised document classification. In our evaluation, on small and medium size corpora, the sIB is found to be consistently superior to all the other clustering methods we examine, typically by a significant margin. Moreover, the sIB results are comparable to those obtained by a supervised Naive Bayes classifier. Finally, we propose a simple procedure for trading cluster's recall to gain higher precision, and show how this approach can extract clusters which match the existing topics of the corpus almost perfectly.	cluster analysis;converge;dspace;document classification;expectation–maximization algorithm;maxima and minima;naive bayes classifier;text corpus	Noam Slonim;Nir Friedman;Naftali Tishby	2002		10.1145/564376.564401	n-gram;naive bayes classifier;information bottleneck method;multi-document summarization;computer science;machine learning;pattern recognition;data mining;cluster analysis;dspace	Web+IR	-24.013052962925904	-68.16499308035446	170459
d1c3ad3680f47b43a8d786c9c6334527c4fee1c5	explaining estimation of factor scores of question and answer statements	software engineering artificial intelligence distributed computing;multiple regression analysis;question answering information retrieval;distributed computing;software engineering;regression analysis feature extraction question answering information retrieval;closing sentence expressions factor scores estimation question and answer statements multiple regression analysis statements feature values statements syntactic information word imageability;factor score;feature extraction;multiple regression analysis question answer site factor score;artificial intelligence;regression analysis;question answer site	In order to avoid the problem of mismatch between the questioner and the respondent, we have conducted impression evaluation experiment and nine factors are obtained as a result. Then factor scores of any other statements have been tried to be estimated by using multiple regression analysis from feature values of statements. By adopting syntactic information of statements, word imageability and closing sentence expressions as the feature values, all the factor scores were well estimated. This paper tries to explain the estimation result with major feature values. The explanation is confirmed by comparing the features of the statements having high and low factor scores with the major features.	closing (morphology)	Yuya Yokoyama;Teruhisa Hochin;Hiroki Nomiya;Tetsuji Satoh	2012	2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2012.52	natural language processing;computer science;machine learning;pattern recognition;data mining;database;regression analysis	SE	-22.884622318853907	-66.26308650712511	170640
0a9f8a6888c1b989d0df779292adc2e1e400073d	uom: using explicit semantic analysis for classifying sentiments		In this paper, we describe our system submitted for the Sentiment Analysis task at SemEval 2013 (Task 2). We implemented a combination of Explicit Semantic Analysis (ESA) with Naive Bayes classifier. ESA represents text as a high dimensional vector of explicitly defined topics, following the distributional semantic model. This approach is novel in the sense that ESA has not been used for Sentiment Analysis in the literature, to the best of our knowledge.	esa;explicit semantic analysis;naive bayes classifier;semeval;sentiment analysis	Sapna Negi;Mike Rosner	2013			natural language processing;explicit semantic analysis;computer science;pattern recognition;data mining	NLP	-22.554745050090297	-69.50559579360696	170900
b1b3c23882818b70cd3e39fb699f2a944aa6de8a	an approach to detecting text autorship in the spanish language	social network services;measurement;bayes methods;syntactics;feature extraction;principal component analysis;writing	Authors tend to express themselves using language in ways that reflect particular styles, vocabularies, biases, idioms, etc. These features can be captured in the so-called firm or stylone. Although capturing these attributes with high fidelity has proven to be very challenging, some advances have been made. Stylometry is the analysis of the unique attributes that are expressed by an author unconsciously through his or her publications. In this paper we investigate techniques for the detection of authorship patterns from the text content of a large number of digital documents, including e-mails, academic notes and free redaction in the Spanish language. A mechanism based on pondering parameters, including statistical observations, extracting a pattern is proposed. We defined 150 stylistic criteria parameters adapted to the Spanish language to compute our metric. Extensive experiment results are also presented.	email;permanent brain;programming idiom;quasiperiodicity;sanitization (classified information);sensor;social network;stylometry;vocabulary;whole earth 'lectronic link	Mauricio Iturralde;Roberto Maldonado;Daniel Fellig	2016	2016 8th IFIP International Conference on New Technologies, Mobility and Security (NTMS)	10.1109/NTMS.2016.7792433	natural language processing;speech recognition;feature extraction;computer science;data mining;writing;measurement;statistics;principal component analysis	DB	-21.38750543388031	-67.68059858883778	171084
16a374a502afd8e2d18f02aac9289c2a33fe087d	cross-lingual annotation projection for weakly-supervised relation extraction	relation extraction;cross lingual annotation projection;weakly supervised learning	Although researchers have conducted extensive studies on relation extraction in the last decade, statistical systems based on supervised learning are still limited, because they require large amounts of training data to achieve high performance level. In this article, we propose cross-lingual annotation projection methods that leverage parallel corpora to build a relation extraction system for a resource-poor language without significant annotation efforts. To make our method more reliable, we introduce two types of projection approaches with noise reduction strategies. We demonstrate the merit of our method using a Korean relation extraction system trained on projected examples from an English-Korean parallel corpus. Experiments show the feasibility of our approaches through comparison to other systems based on monolingual resources.	noise reduction;parallel text;relationship extraction;supervised learning;text corpus	Seokhwan Kim;Minwoo Jeong;Jonghoon Lee;Gary Geunbae Lee	2014	ACM Trans. Asian Lang. Inf. Process.	10.1145/2529994	natural language processing;relationship extraction;speech recognition;computer science;machine learning;pattern recognition;data mining;information retrieval	NLP	-21.23496301647082	-72.18644493035022	171574
eb777c2af56c8315dc743338e31d6240345ef53a	open relation extraction based on core dependency phrase clustering		Relation extraction is very useful for many applications and has attracted much attention. The dominant prior methods for relation extraction were supervised methods which are relation-specific and limited by the availability of annotated training data. In this paper, we propose a method using hierarchical clustering to extract unbounded relations without relying on training data. The relation among entities in a sentence depends on the terms associated with the entities. Terms on the expandPath capture the relations between the entities. Given a relation, though an expandPath may have more than one dependency phrase, only the core dependency phrase describes the specific relation between the subject and the object. Our method uses heuristic rules to select the core dependency phrases and clusters entity pairs according to the similarity of the core dependency phrases in order to avoid irrelevant information and capture the semantics of the relation between entities more precisely. At last, our method automatically labels the relation clusters on basis of the semantics of core dependency phrases. The experimental results show that our method can cluster entity pairs which have the same relations more accurately and generate appropriate labels for the relations.	cluster analysis;entity;heuristic;hierarchical clustering;relationship extraction;relevance;supervised learning;unsupervised learning	Chengsen Ru;Shasha Li;Jintao Tang;Yi Gao;Ting Wang	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.91	semantics;multivalued dependency;feature extraction;relationship extraction;cluster analysis;hierarchical clustering;artificial intelligence;phrase;computer science;pattern recognition;sentence	DB	-24.15564607791557	-68.04036466781437	172045
7522d2774a2947665fdec9c0021053e98ddbf73d	integration of word and semantic features for theme identification in telephone conversations		The paper describes a research about the possibility of integrating different types of word and semantic features for automatically identifying themes of reallife telephone conversations in a customer care service. Features are all the words of the application vocabulary, the probabilities obtained with Latent Dirichelet Allocation (LDA) of selected discriminative words and semantic features obtained with a limited human supervision of words and patterns expressing entities and relations of the application ontology. A Deep Neural Network (DNN) is proposed for integrating these features. Experimental results on manual and automatic conversation transcriptions are presented showing the effective contribution of the integration. The results show how to automatically select a large subset of the test corpus with high precision and recall, making it possible to automatically obtain theme mention proportions in different time periods.	deep learning;entity;information;latent semantic analysis;precision and recall;vocabulary	Yannick Estève;Mohamed Bouallegue;Carole Lailler;Mohamed Morchid;Richard Dufour;Georges Linarès;Driss Matrouf;Renato De Mori	2015		10.1007/978-3-319-19291-8_21	natural language processing;speech recognition;computer science;communication	AI	-24.05732245353617	-71.61355349968044	173142
37a4f41d7f4e090f414fc410f343089df5f21bde	asapp: alinhamento semântico automático de palavras aplicado ao português		We present two distinct approaches to the ASSIN shared evaluation task where, given a collection with pairs of sentences, in Portuguese, poses the following challenges: (a)~computing the semantic similarity between the sentences of each pair; and (b)~testing whether one sentence paraphrases or entails the other. The first approach, dubbed Reciclagem, is exclusively based on heuristics computed on Portuguese semantic networks. The second, dubbed ASAPP, is based on supervised machine learning. The results of Reciclagem enable an indirect comparison of Portuguese semantic networks. They were then used as features of the ASAPP approach, together with lexical and syntactic features. After comparing our results with those in the gold collection, it is clear that ASAPP consistently outperforms Reciclagem. This happens both for European Portuguese and Brazilian Portuguese, where the entailment performance reaches an accuracy of 80.28% +- 0.019, and the semantic similarity scores are 66.5% +- 0.021 correlated with those given by humans.		Ana Oliveira Alves;Ricardo Rodrigues;Hugo Gonçalo Oliveira	2016	Linguamática		natural language processing;computer science;data mining;linguistics	Logic	-25.45504708373045	-71.76564208864056	173371
1de8d4c1d5ae11685c25bdc45968fecfeef8ad40	semantic features based on word alignments for estimating quality of text simplification		This paper examines the usefulness of semantic features based on word alignments for estimating the quality of text simplification. Specifically, we introduce seven types of alignment-based features computed on the basis of word embeddings and paraphrase lexicons. Through an empirical experiment using the QATS dataset (Štajner et al., 2016b), we confirm that we can achieve the state-of-the-art performance only with these features.	lexicon;semantic similarity;text simplification;word embedding	Tomoyuki Kajiwara;Atsushi Fujita	2017			natural language processing;artificial intelligence;pattern recognition;computer science;text simplification	NLP	-21.952976158335385	-72.84613950467204	173770
b19691da1c90d2424d155f20fd70778693ca5deb	nul system at ntcir rite-val tasks.		This paper describes the submitted strategy and the methods of NUL team on NTCIR-11 RITE-VAL[1] fact validation (FV) and system validation (SV) tasks. We started to follow the shallow approach by Tian et al[3]. Then, we improved the named entity recognition accuracy and transformed some variables by the cross validation score of training sets. Especially, in the FV tasks, we used Apache Solr as the base search system. We compared several units of chunk to the texts index and the weighting of the ranking score for search results. After several modification, we achieved the highest cross validation score to the RITE-10 Exam bc and Exam Search tasks. Our final submitted system achieved Macro-f1 score 61.93 in FV and 69.59 in SV respectively.	cross-validation (statistics);f1 score;farmville;named-entity recognition;null character;solr;systemverilog;variable assembly language	Ai Ishii;Hiroshi Miyashita;Mio Kobayashi;Chikara Hoshino	2014			cross-validation;pattern recognition;artificial intelligence;ranking;rite;named-entity recognition;computer science;weighting	AI	-23.130318293521718	-71.36563093845722	174373
2176d465f959e666e5c3eca3163381e6e788fd8e	semantic naïve bayes classifier for document classification		In this paper, we propose a semantic naïve Bayes classifier (SNBC) to improve the conventional naïve Bayes classifier (NBC) by incorporating “document-level” semantic information for document classification (DC). To capture the semantic information from each document, we develop semantic feature extraction and modeling algorithms. For semantic feature extraction, we first apply a log-Bilinear document modeling (LBDM) algorithm to transform each word into a semantic vector, and then apply principal component analysis (PCA) to the semantic space formed by the word vectors to extract a set of semantic features for each document. For semantic modeling, a semantic model is constructed using the semantic features of the training documents. In the testing phase, SNBC systematically integrates the semantic model and the conventional NBC to perform DC. The results of experiments on the 20 News-groups and WebKB datasets confirm that, with the semantic score, SNBC consistently outperforms NBC with various language modeling approaches.	algorithm;bilinear transform;document classification;experiment;feature extraction;language model;naive bayes classifier;naivety;principal component analysis;probabilistic latent semantic analysis;word embedding	How Jing;Yu Tsao;Kuan-Yu Chen;Hsin-Min Wang	2013				Web+IR	-20.969355967723967	-68.0904446751383	174874
89552c3849acc4f46e46f04908cbe609b9d23fee	evaluating neighbor rank and distance measures as predictors of semantic priming		This paper summarizes the results of a large-scale evaluation study of bag-ofwords distributional models on behavioral data from three semantic priming experiments. The tasks at issue are (i) identification of consistent primes based on their semantic relatedness to the target and (ii) correlation of semantic relatedness with latency times. We also provide an evaluation of the impact of specific model parameters on the prediction of priming. To the best of our knowledge, this is the first systematic evaluation of a wide range of DSM parameters in all possible combinations. An important result of the study is that neighbor rank performs better than distance measures in predicting semantic priming.	experiment;semantic similarity	Gabriella Lapesa;Stefan Evert	2013			latency (engineering);distance measures;priming (psychology);semantic similarity;artificial intelligence;pattern recognition;mathematics	NLP	-19.151520878450516	-69.89310639572129	175144
27249b30611786914fa7929605a7f55f42cfb935	uwm: disorder mention extraction from clinical text using crfs and normalization using learned edit distance patterns		This paper describes Team UWM’s system for the Task 7 of SemEval 2014 that does disorder mention extraction and normalization from clinical text. For the disorder mention extraction (Task A), the system was trained using Conditional Random Fields with features based on words, their POS tags and semantic types, as well as features based on MetaMap matches. For the disorder mention normalization (Task B), variations of disorder mentions were considered whenever exact matches were not found in the training data or in the UMLS. Suitable types of variations for disorder mentions were automatically learned using a new method based on edit distance patterns. Among nineteen participating teams, UWM ranked third in Task A with 0.755 strict F-measure and second in Task B with 0.66 strict accuracy.	approximation algorithm;brown corpus;conditional random field;database normalization;edit distance;f1 score;http 404;regular expression;semeval	Omid Ghiasvand;Rohit J. Kate	2014			speech recognition;computer science;pattern recognition;data mining	NLP	-23.61005828396471	-70.1879030075166	175455
c3774630910621bba7f0e246bbcff32f04ced7e8	relation linking for wikidata using bag of distribution representation		Knowledge graphs (KGs) are essential repositories of structured and semi-structured knowledge which benefit various NLP applications. To utilize the knowledge in KGs to help machines to better understand plain texts, one needs to bridge the gap between knowledge and texts. In this paper, a Relation Linking System for Wikidata (RLSW) is proposed to link the relations in KGs to plain texts. The proposed system uses the knowledge in Wikidata as seeds and clusters relation mentions in text with a novel phrase similarity algorithm. To enhance the system’s ability of handling unseen expressions and make use of the location information of words to reduce false positive rate, a bag of distribution pattern modeling method is proposed. Experimental results show that the proposed approach improves traditional methods, including word based pattern and syntax feature enriched system such as OLLIE.	wikidata	Xi Yang;Shiya Ren;Yuan Li;Ke Shen;Zhixing Li;Guoyin Wang	2017		10.1007/978-3-319-73618-1_55	false positive rate;syntax;expression (mathematics);phrase;artificial intelligence;pattern recognition;computer science;graph	NLP	-26.181053510702	-69.63170516761954	175687
4404a5ba8e9b6619f88bca58dc6f281e4a0dde49	ricoh at semeval-2016 task 1: ir-based semantic textual similarity estimation		This paper describes our IR (Information Retrieval) based method for SemEval 2016 task 1, Semantic Textual Similarity (STS). The main feature of our approach is to extend a conventional IR-based scheme by incorporating word alignment information. This enables us to develop a more fine-grained similarity measurement. In the evaluation results, we have seen that the proposed method improves upon a conventional IR-based method on average. In addition, one of our submissions achieved the best performance for the “postediting” data set.	bitext word alignment;coefficient of determination;data structure alignment;information retrieval;overlap–add method;postediting;semeval;semantic similarity;similarity measure;sørensen–dice coefficient	Hideo Itoh	2016			artificial intelligence;natural language processing;computer science;semeval;information retrieval	NLP	-22.01385168466309	-72.54269966066319	175890
d2d8b52f59945b4a3ef9d20ab44e108319eead6f	mining wordnet for a fuzzy sentiment: sentiment tag extraction from wordnet glosses.	col	Many of the tasks required for semantic tagging of phrases and texts rely on a list of words annotated with some semantic features. We present a method for extracting sentiment-bearing adjectives from WordNet using the Sentiment Tag Extraction Program (STEP). We did58 STEP runs on unique non-intersecting seed lists drawn from manually annotated list of positive and negative adjectives and evaluated the results against other manually annotated lists. The58 runs were then collapsed into a single set of 7, 813 unique words. For each word we computed a Net Overlap Score by subtracting the total number of runs assigning this word a negative sentiment from the total of the runs that consider it positive. We demonstrate that Net Overlap Score can be used as a measure of the words degree of membership in the fuzzy category of sentiment: the core adjectives, which had the highest Net Overlap scores, were identified most accurately both by STEP and by human annotators, while the words on the periphery of the category had the lowest scores and were associated with low rates of inter-annotator agreement.	fuzzy set;inter-rater reliability;lexical definition;overlap–add method;wordnet	Alina Andreevskaia;Sabine Bergler	2006			natural language processing;artificial intelligence;fuzzy logic;computer science;information retrieval;data mining;wordnet	NLP	-25.336421292986074	-69.59851490355739	176042
0c3cd81111567b2716f37ac1ddcd145b32b2f041	an unsupervised automated essay scoring system	unsupervised learning;supervised learning;voting algorithm;intelligence artificielle;apprentissage non supervise;supervised learning approaches;supervised learning approaches unsupervised automated essay scoring system voting algorithm;aprendizaje no supervisado;unsupervised automated essay scoring system;unsupervised learning natural language processing;intelligent systems machine learning natural language processing;machine learning;voting;intelligent systems;intelligent system;humans supervised learning unsupervised learning writing training data text analysis natural language processing machine learning learning systems large scale systems;clustering algorithms;artificial intelligence;voto;inteligencia artificial;apprentissage supervise;vote;aprendizaje supervisado;article;natural language processing;gaussian distribution;scoring system	The proposed automated essay-scoring system uses an unsupervised-learning approach based on a voting algorithm. Experiments show that this approach works well compared to supervised-learning approaches.	algorithm;automated essay scoring;supervised learning;unsupervised learning	Yen-Yu Chen;Chien-Liang Liu;Chia-Hoang Lee;Tao-Hsing Chang	2010	IEEE Intelligent Systems	10.1109/MIS.2010.3	normal distribution;natural language processing;unsupervised learning;voting;intelligent decision support system;computer science;artificial intelligence;machine learning;pattern recognition;cluster analysis;supervised learning;electoral-vote.com	Robotics	-22.828684295449943	-68.26741850606405	176252
153b67ed0459d259a66206f42d443be14749bf0b	using entity information from a knowledge base to improve relation extraction		Relation extraction is the task of extracting predicate-argument relationships between entities from natural language text. This paper investigates whether background information about entities available in knowledge bases such as FreeBase can be used to improve the accuracy of a state-of-the-art relation extraction system. We describe a simple and effective way of incorporating FreeBase’s notable types into a state-of-the-art relation extraction system (Riedel et al., 2013). Experimental results show that our notable typebased system achieves an average 7.5% weighted MAP score improvement. To understand where the notable type information contributes the most, we perform a series of ablation experiments. Results show that the notable type information improves relation extraction more than NER labels alone across a wide range of entity types and relations.	entity;experiment;freebase;knowledge base;monadic predicate calculus;named-entity recognition;natural language;relationship extraction	Lan Du;Anish Kumar;Mark Johnson;Massimiliano Ciaramita	2015			knowledge management;pattern recognition;data mining;knowledge extraction	NLP	-25.564369709377047	-70.34292156090396	176266
0a1d3edb53e429e890b68ebcab26f36dbbf11b29	cu-gwu perspective at semeval-2016 task 6: ideological stance detection in informal text		We present a supervised system that uses lexical, sentiment, semantic dictionaries and latent and frame semantic features to identify the stance of a tweeter towards an ideological target. We evaluate the performance of the proposed system on subtask A in SemEval2016 Task 6: “Detecting Stance in Tweets”. The system yields an average Fβ=1 score of 63.6% on the task’s test set and has been ranked the 6 by the task organizers out of 19 judged systems.	dictionary;supervised learning;test set;tip (unix utility);tweeter	Heba Elfardy;Mona T. Diab	2016			computer science;artificial intelligence;natural language processing;ideology;semeval	NLP	-22.100182429369752	-69.72319954747847	176315
f27be01ce79e086c02547137524db0a428239ff7	incomplete follow-up question resolution using retrieval based sequence to sequence learning		Intelligent personal assistants (IPAs) and interactive question answering (IQA) systems frequently encounter incomplete follow-up questions. The incomplete follow-up questions only make sense when seen in conjunction with the conversation context: the previous question and answer. Thus, IQA and IPA systems need to utilize the conversation context in order to handle the incomplete follow-up questions and generate an appropriate response. In this work, we present a retrieval based sequence to sequence learning system that can generate the complete (or intended) question for an incomplete follow-up question (given the conversation context). We can train our system using only a small labeled dataset (with only a few thousand conversations), by decomposing the original problem into two simpler and independent problems. The first problem focuses solely on selecting the candidate complete questions from a library of question templates (built offline using the small labeled conversations dataset). In the second problem, we re-rank the selected candidate questions using a neural language model (trained on millions of unlabelled questions independently). Our system can achieve a BLEU score of 42.91, as compared to 29.11 using an existing generation based approach. We further demonstrate the utility of our system as a plug-in module to an existing QA pipeline. Our system when added as a plug-in module, enables Siri to achieve an improvement of 131.57% in answering incomplete follow-up questions.	bleu;entity;language model;online and offline;plug-in (computing);question answering;siri;well-formed element	Vineet Kumar;Sachindra Joshi	2017		10.1145/3077136.3080801	computer science;bleu;information retrieval;data mining;conversation;question answering;sequence learning;machine learning;artificial intelligence;language model	NLP	-23.70215070784133	-72.1205243676503	176942
15792093c6570892aac920c395ea936371ec0384	unsupervised relation disambiguation with order identification capabilities	unsupervised learning;hierarchical clustering;high dimensionality;spectral clustering;clustering method;user experience;k means clustering;named entity;eigenvectors	We present an unsupervised learning approach to disambiguate various relations between name entities by use of various lexical and syntactic features from the contexts. It works by calculating eigenvectors of an adjacency graph’s Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors. This method can address two difficulties encoutered in Hasegawa et al. (2004)’s hierarchical clustering: no consideration of manifold structure in data, and requirement to provide cluster number by users. Experiment results on ACE corpora show that this spectral clustering based approach outperforms Hasegawa et al. (2004)’s hierarchical clustering method and a plain k-means clustering method.	ace;cluster analysis;cosine similarity;entity;hierarchical clustering;k-means clustering;relationship extraction;similarity measure;spectral clustering;text corpus;unsupervised learning;word-sense disambiguation	Jinxiu Chen;Dong-Hong Ji;Chew Lim Tan;Zheng-Yu Niu	2006		10.3115/1610075.1610154	unsupervised learning;correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;eigenvalues and eigenvectors;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;dendrogram;spectral clustering;hierarchical clustering of networks;k-means clustering;clustering high-dimensional data;conceptual clustering	NLP	-25.24237316621026	-66.84041289055267	177119
1fb20cce2a8a872d98ad19d6ed33b222e31a377b	the clustering of author's texts of english fiction in the vector space of semantic fields		This paper describes the analysis of possible differentiation of the author’s idiolect in the space of semantic fields; it also analyzes the clustering of text documents in the vector space of semantic fields and in the semantic space with orthogonal basis. The analysis showed that using the vector space model on the basis of semantic fields is efficient in cluster analysis algorithms of author’s texts in English fiction. The study of the distribution of authors' texts in the cluster structure showed the presence of the areas of semantic space that represent the idiolects of individual authors. Such areas are described by the clusters where only one author dominates. The clusters, where the texts of several authors dominate, can be considered as areas of semantic similarity of author’s styles. SVD factorization of the semantic fields matrix makes it possible to reduce significantly the dimension of the semantic space in the cluster analysis of author’s texts. Using the clustering of the semantic field vector space can be efficient in a comparative analysis of author's styles and idiolects. The clusters of some authors' idiolects are semantically invariant and do not depend on any changes in the basis of the semantic space and clustering method.	algorithm;cluster analysis;qualitative comparative analysis;semantic similarity;singular value decomposition	Bohdan Pavlyshenko	2012	CoRR		natural language processing;semantic similarity;semantic computing;computer science;theoretical computer science;machine learning;semantic compression;information retrieval	Web+IR	-26.144248033233346	-66.66072732331891	177499
6becdd728cff8fac21387db85a529597d112763e	ntt's question answering system for ntcir-6 qac-4	search engine;information retrieval;supervised machine learning;question answering system;question answering	NTCIR-6 QAC-4 organizers announced that there would be no restriction (such as factoid) on QAC4 questions, but they plan to include many ‘definition’ questions and ‘why’ questions. Therefore, we focused on these two question types. For ‘definition’ questions, we used a simple pattern-based approach. For ‘why’ questions, hand-crafted rules were used in previous work for answer candidate extraction [5]. However, such rules greatly depend on developers’ intuition and are costly to make. We adopt a supervised machine learning approach. We collected causal expressions from the EDR corpus and trained a causal expression classifier, integrating lexical, syntactic, and semantic features. The experimental results show that our system is effective for ‘why’ and ‘definition’ questions.	bluetooth;causal filter;expression (computer science);machine learning;question answering;supervised learning	Ryuichiro Higashinaka;Hideki Isozaki	2007			natural language processing;computer science;data mining;information retrieval	NLP	-25.182580566191334	-70.90810217286274	177737
6a04d1102b5a4c39b643746e96848192b8294c53	a hybrid approach for robust multilingual toponym extraction and disambiguation		Toponym extraction and disambiguation are key topics recently addressed by fields of Information Extraction and Geographical Information Retrieval. Toponym extraction and disambiguation are highly dependent processes. Not only toponym extraction effectiveness affects disambiguation, but also disambiguation results may help improving extraction accuracy. In this paper we propose a hybrid toponym extraction approach based on Hidden Markov Models (HMM) and Support Vector Machines (SVM). Hidden Markov Model is used for extraction with high recall and low precision. Then SVM is used to find false positives based on informativeness features and coherence features derived from the disambiguation results. Experimental results conducted with a set of descriptions of holiday homes with the aim to extract and disambiguate toponyms showed that the proposed approach outperform the state of the art methods of extraction and also proved to be robust. Robustness is proved on three aspects: language independence, high and low HMM threshold settings, and limited training data.	ambiguous grammar;experiment;hidden markov model;information extraction;information retrieval;markov chain;named entity;named-entity recognition;robustness (computer science);support vector machine;toponym resolution;word-sense disambiguation	Mena B. Habib;Maurice van Keulen	2013		10.1007/978-3-642-38634-3_1	speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-24.12467966435279	-68.92430729967353	178161
f2aa359d4cb43c85c3010e22eac1b7fb4ffa3ca8	a language independent method for generating large scale polarity lexicons		Sentiment Analysis systems aims at detecting opinions and sentiments that are expressed in texts. Many approaches in literature are based on resources that model the prior polarity of words or multi-word expressions, i.e. a polarity lexicon. Such resources are defined by teams of annotators, i.e. a manual annotation is provided to associate emotional or sentiment facets to the lexicon entries. The development of such lexicons is an expensive and language dependent process, making their coverage of linguistic sentiment phenomena limited. Moreover, once a lexicon is defined it can hardly be adopted in a different language or even a different domain. In this paper, we present several Distributional Polarity Lexicons (DPLs), i.e. large-scale polarity lexicons acquired with an unsupervised methodology based on Distributional Models of Lexical Semantics. Given a set of heuristically annotated sentences from Twitter, we transfer the sentiment information from sentences to words. The approach is mostly unsupervised, and experimental evaluations on Sentiment Analysis tasks in two languages show the benefits of the generated resources. The generated DPLs are publicly available in English and Italian.	distributional semantics;heuristic;lexicon;sensor;sentiment analysis	Giuseppe Castellucci;Danilo Croce;Roberto Basili	2016			natural language processing;artificial intelligence;speech recognition;computer science	NLP	-26.384543883673498	-71.88889662556294	178203
487ed99e00bf6803a53a6059ceccd1510a63e72d	an analysis of active learning strategies for sequence labeling tasks	active learning strategy;active learning approach;natural language processing;active learning;large-scale empirical comparison;multiple corpus;novel algorithm;document segmentation;information extraction;sequence model	Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.	algorithm;document layout analysis;fisher information;information design;information extraction;natural language processing;sequence labeling;text corpus	Burr Settles;Mark Craven	2008			semi-supervised learning;natural language processing;computer science;machine learning;data mining;active learning;active learning;information extraction	NLP	-21.357101767915708	-71.77142525035927	178272
01e0b215b190f2df531d78e0ee67e9182cc90261	symmetric pattern based word embeddings for improved word similarity prediction		We present a novel word level vector representation based on symmetric patterns (SPs). For this aim we automatically acquire SPs (e.g., “X and Y”) from a large corpus of plain text, and generate vectors where each coordinate represents the cooccurrence in SPs of the represented word with another word of the vocabulary. Our representation has three advantages over existing alternatives: First, being based on symmetric word relationships, it is highly suitable for word similarity prediction. Particularly, on the SimLex999 word similarity dataset, our model achieves a Spearman’s ρ score of 0.517, compared to 0.462 of the state-of-the-art word2vec model. Interestingly, our model performs exceptionally well on verbs, outperforming stateof-the-art baselines by 20.2–41.5%. Second, pattern features can be adapted to the needs of a target NLP application. For example, we show that we can easily control whether the embeddings derived from SPs deem antonym pairs (e.g. (big,small)) as similar or dissimilar, an important distinction for tasks such as word classification and sentiment analysis. Finally, we show that a simple combination of the word similarity scores generated by our method and by word2vec results in a superior predictive power over that of each individual model, scoring as high as 0.563 in Spearman’s ρ on SimLex999. This emphasizes the differences between the signals captured by each of the models.	bag-of-words model;baseline (configuration management);gene prediction;n-gram;natural language processing;provable security;sentiment analysis;vocabulary;word embedding;word2vec	Roy Schwartz;Roi Reichart;Ari Rappoport	2015			speech recognition;machine learning;pattern recognition	NLP	-20.898020226261913	-72.77106350403699	178311
e5eb2e3360525d2c35acbeb7ff42e9d1deef28d3	detecting factual and non-factual content in news articles		News articles are a major source of facts about the current state and events of our surrounding world. However, not all news articles are equally rich in presenting the facts. In this paper, we consider the problem of detecting factual and non-factual parts in news articles. We present a comprehensive survey on the existing literature on fact classification on news articles as well as a related and more widely studied problem of subjectivity vs objectivity classification of statements. Combining these techniques and some new features we design a framework for classifying facts and non-facts in news articles. We present extensive experiments on this task using several features and combinations of those on two datasets, one of which was used for subjectivity classification in previous works. We show that standard textual dataset dependent features such as n-grams produce good results on both datasets, but more general features such as part of speech tags and entity types produce inconsistent results. We analyze the results based on the nature of the datasets to present insights on the usefulness of the features and their applicability in the classification task we are considering.	experiment;grams;n-gram;objectivity/db;part-of-speech tagging;sensor;statistical classification	Ishan Sahu;Debapriyo Majumdar	2017		10.1145/3041823.3041837	part of speech;deep learning;information retrieval;unsupervised learning;objectivity (philosophy);data mining;subjectivity;computer science;artificial intelligence	Web+IR	-22.385023786672672	-67.64549121596986	179040
0b717855713f11280668d0afd0d3004466c50730	animacy detection with voting models		Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error.	n-gram	Joshua L. Moore;Christopher J. C. Burges;Erin Renshaw;Wen-tau Yih	2013			computer vision;computer science	ML	-20.461220678324796	-67.13693557067037	179377
49f1aac2a34093f901e3a27a0a8749cb7a0904de	entity extraction via ensemble semantics	ensemble semantics;entity extraction;large set;web-scale extraction;large gain;higher quality resource;average precision score;current state;combining information extraction system;information extraction	Combining information extraction systems yields significantly higher quality resources than each system in isolation. In this paper, we generalize such a mixing of sources and features in a framework called Ensemble Semantics. We show very large gains in entity extraction by combining state-of-the-art distributional and patternbased systems with a large set of features from a webcrawl, query logs, and Wikipedia. Experimental results on a webscale extraction of actors, athletes and musicians show significantly higher mean average precision scores (29% gain) compared with the current state of the art.	algorithm;entity;html element;information extraction;information retrieval;named-entity recognition;scalability;supervised learning;web page;web search engine;wikipedia;wordnet	Marco Pennacchiotti;Patrick Pantel	2009			natural language processing;computer science;pattern recognition;data mining;information extraction;information retrieval	NLP	-26.13350124212482	-67.06878030751353	179501
f478438a7e411126a3aaa2b7e24fa5c65e9bd84d	gplsiua team at the diaan 2018 task		This paper describes our participation in DIANN 2018 Task: DIsability ANNotation in English and Spanish documents. Our proposal detects disabilities as well as recognizes negated disabilities. To that end, our entity typing system is applied without tuning and it does not require any external knowledge. It consists of a Random Forest machine learning classifier whose feature set includes local entity information and profiles, generated unsupervisedly. Two experiments are presented in order to investigate performance of two types of profiles. Both proposals are able to reach promising and reasonable results, obtaining a partial-matching precision greater than 87% for disabilities and negated disabilities regardless of the language. Thus demonstrating the portability and adequateness of our approach regardless of type of profile.	experiment;machine learning;random forest;software portability	Isabel Moreno;María Teresa Romá-Ferri;Paloma Moreda	2018				NLP	-23.535106603093066	-71.42963406737819	179503
5f9455eea572d7b341ac6016f71de9e7c5384b85	lexical substitution for evaluating compositional distributional models		Compositional Distributional Semantic Models (CDSMs) model the meaning of phrases and sentences in vector space. They have been predominantly evaluated on limited, artificial tasks such as semantic sentence similarity on hand-constructed datasets. This paper argues for lexical substitution as a means to evaluate CDSMs. Lexical substitution is a more natural task, enables us to evaluate meaning composition at the level of individual words, and provides a common ground to compare CDSMs with dedicated lexical substitution models. We create a lexical substitution dataset for CDSM evaluation from an Englishlanguage corpus with manual “all-words” lexical substitution annotation. Our experiments indicate that the Practical Lexical Function CDSM outperforms simple component-wise CDSMs and performs on par with the context2vec lexical substitution model using the same context.	baseline (configuration management);experiment;least squares;lexical function;lexical substitution;modulation;substitution model	Maja Buljan;Sebastian Padó;Jan Snajder	2018			natural language processing;machine learning;artificial intelligence;computer science	NLP	-20.967390980252535	-72.88818907163184	179570
ae5cb7b148f9d498404444f4167084943afba8e2	hate speech classification in social media using emotional analysis		In this paper, we examine methods to classify hate speech in social media. We aim to establish lexical baselines for this task by applying classification methods using a dataset annotated for this purpose. As features, our system uses Natural Language Processing (NLP) techniques in order to expand the original dataset with emotional information and provide it for machine learning classification. We obtain results of 80.56% accuracy in hate speech identification, which represents an increase of almost 100% from the original analysis used as a reference.		Ricardo Martins;Marco Gomes;José João Almeida;Paulo Novais;Pedro Rangel Henriques	2018	2018 7th Brazilian Conference on Intelligent Systems (BRACIS)	10.1109/BRACIS.2018.00019	voice activity detection;social media;statistical classification;pattern recognition;computer science;artificial intelligence	NLP	-21.33945946724467	-68.48126043047294	180423
11b65832aa32cfda869d75aea38d245d4a7c067d	twise at semeval-2017 task 4: five-point twitter sentiment classification and quantification		The paper describes the participation of the team “TwiSE” in the SemEval-2017 challenge. Specifically, I participated at Task 4 entitled “Sentiment Analysis in Twitter” for which I implemented systems for five-point tweet classification (Subtask C) and five-point tweet quantification (Subtask E) for English tweets. In the feature extraction steps the systems rely on the vector space model, morpho-syntactic analysis of the tweets and several sentiment lexicons. The classification step of Subtask C uses a Logistic Regression trained with the one-versus-rest approach. Another instance of Logistic Regression combined with the classify-and-count approach is trained for the quantification task of Subtask E. In the official leaderboard the system is ranked 5/15 in Subtask C and 2/12 in Subtask E.	feature extraction;lexicon;logistic regression;parsing;sentiment analysis;test data	Georgios Balikas	2017		10.18653/v1/S17-2127	natural language processing;computer science;artificial intelligence;semeval	NLP	-22.147381312827104	-69.70185169588228	180994
b9eef66d56a1c616c4e6d418c706b1364a75f9fc	unsupervised and knowledge-poor approaches to sentiment analysis	qz psychology	Sentiment analysis focuses upon automatic classiffication of a document's sentiment (and more generally extraction of opinion from text). Ways of expressing sentiment have been#R##N#shown to be dependent on what a document is about (domain-dependency). This complicates supervised methods for sentiment analysis which rely on extensive use of training data or linguistic resources that are usually either domain-specific or generic. Both kinds of resources prevent classiffiers from performing well across a range of domains, as this requires appropriate in-domain (domain-specific) data.#R##N##R##N#This thesis presents a novel unsupervised, knowledge-poor approach to sentiment analysis aimed at creating a domain-independent and multilingual sentiment analysis system.#R##N#The approach extracts domain-specific resources from documents that are to be processed, and uses them for sentiment analysis. This approach does not require any training corpora, large sets of rules or generic sentiment lexicons, which makes it domain- and languageindependent but at the same time able to utilise domain- and language-specific information.#R##N##R##N#The thesis describes and tests the approach, which is applied to diffeerent data, including customer reviews of various types of products, reviews of films and books, and news items; and to four languages: Chinese, English, Russian and Japanese. The approach is applied not only to binary sentiment classiffication, but also to three-way sentiment classiffication (positive, negative and neutral), subjectivity classifiation of documents and sentences, and to the extraction of opinion holders and opinion targets. Experimental results suggest that the approach is often a viable alternative to supervised systems, especially when applied to large document collections.	sentiment analysis;unsupervised learning	Taras Zagibalov	2010			natural language processing;computer science;data mining;information retrieval;sentiment analysis	NLP	-24.685042898048376	-71.09186517226571	181296
16b380ae0effdf47e1372bc525f4c65e0ced1ef1	combining statistical machine learning with transformation rule learning for vietnamese word sense disambiguation	statistical machine learning;training;niobium;rule learning;word sense disambiguation;statistical analysis learning artificial intelligence natural language processing;data model;learning systems;learning system;accuracy;statistical analysis;machine learning;statistical machine learning model transformation rule learning vietnamese word sense disambiguation general statistical model transformation based learning;context training niobium machine learning accuracy learning systems data models;learning artificial intelligence;natural language processing;context;data models	Word Sense Disambiguation (WSD) is the task of determining the right sense of a word depending on the context it appears. Among various approaches developed for this task, statistical machine learning methods have been showing their advantages in comparison with others. However, there are some cases which cannot be solved by a general statistical model. This paper proposes a novel framework, in which we use the rules generated by transformation based learning (TBL) to improve the performance of a statistical machine learning model. This framework can be considered as a combination of a rule-based method and statistical based method. We have developed this method for the problem of Vietnamese WSD and achieved some promising results.	logic programming;machine learning;naive bayes classifier;natural language processing;newton's method;parsing;part-of-speech tagging;rule induction;statistical model;support vector machine;web services for devices;word sense;word-sense disambiguation	Phu-Hung Dinh;Ngoc-Khuong Nguyen;Anh-Cuong Le	2012	2012 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future	10.1109/rivf.2012.6169827	natural language processing;algorithmic learning theory;semeval;computer science;machine learning;pattern recognition	AI	-22.810644061673766	-72.6332411419975	181343
905045791944fc76f4103ad0355330ef61545407	desrl: a linear-time semantic role labeling system	classifier error;desrl system;complete problem evaluation;following score;research barcelona;di pisa;efficient pipeline;linear-time semantic role;different sub-task;conll-2008 shared task;closed challenge;system architecture;semantic role labeling;linear time	This paper describes the DeSRL system, a joined effort of Yahoo! Research Barcelona and Università di Pisa for the CoNLL-2008 Shared Task (Surdeanu et al., 2008). The system is characterized by an efficient pipeline of linear complexity components, each carrying out a different sub-task. Classifier errors and ambiguities are addressed with several strategies: revision models, voting, and reranking. The system participated in the closed challenge ranking third in the complete problem evaluation with the following scores: 82.06 labeled macro F1 for the overall task, 86.6 labeled attachment for syntactic dependencies, and 77.5 labeled F1 for semantic dependencies. 1 System description DeSRL is implemented as a sequence of components of linear complexity relative to the sentence length. We decompose the problem into three subtasks: parsing, predicate identification and classification (PIC), and argument identification and classification (AIC). We address each of these subtasks with separate components without backward feedback between sub-tasks. However, the use of multiple parsers at the beginning of the process, and re-ranking at the end, contribute beneficial stochastic aspects to the system. Figure 1 summarizes the system architecture. We detail the parsing ∗All authors contributed equally to this work. ∗ c © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. sub-task in Section 2 and the semantic sub-tasks (PIC and AIC) in Section 3.	attachments;parsing;semantic role labeling;stochastic process;systems architecture;time complexity	Massimiliano Ciaramita;Giuseppe Attardi;Felice Dell'Orletta;Mihai Surdeanu	2008			natural language processing;computer science;theoretical computer science;communication	NLP	-22.981639276954418	-69.93162752127509	181545
c76d7867e3146f393e0ad8db790a33f00f8f8758	lstms with attention for aggression detection		In this paper, we describe the system submitted for the shared task on Aggression Identification in Facebook posts and comments by the team Nishnik. Previous works demonstrate that LSTMs have achieved remarkable performance in natural language processing tasks. We deploy an LSTM model with an attention unit over it. Our system ranks 6th and 4th in the Hindi subtask for Facebook comments and subtask for generalized social media data respectively. And it ranks 17th and 10th in the corresponding English subtasks.	dropout (neural networks);long short-term memory;natural language processing;social media;word embedding	Nishant Nikhil;Ramit Pahwa;Mehul Kumar Nirala;Rohan Khilnani	2018	CoRR		artificial intelligence;natural language processing;computer science;hindi;aggression;social media	NLP	-21.525108796713187	-69.7455884019213	181637
e7f7eb2a383903501aeda698227f121b9a90be73	inferring discourse relations from pdtb-style discourse labels for argumentative revision classification.		Penn Discourse Treebank (PDTB)-style annotation focuses on labeling local discourse relations between text spans and typically ignores larger discourse contexts. In this paper we propose two approaches to infer discourse relations in a paragraph-level context from annotated PDTB labels. We investigate the utility of inferring such discourse information using the task of revision classification. Experimental results demonstrate that the inferred information can significantly improve classification performance compared to baselines, not only when PDTB annotation comes from humans but also from automatic parsers.	belief revision;daisy digital talking book;domain of discourse;intel matrix raid;shallow parsing;statistical classification;treebank	Fan Zhang;Diane J. Litman;Katherine Forbes-Riley	2016			natural language processing;linguistics	NLP	-21.68351606633646	-71.222695106482	181711
411b23ee925ba57f779fef72424bf4341eb6b14e	extractive spoken document summarization with representation learning techniques		The rapidly increasing availability of multimedia associated with spoken documents on the Internet has prompted automatic spoken document summarization to be an important research subject. Thus far, the majority of existing work has focused on extractive spoken document summarization, which selects salient sentences from an original spoken document according to a target summarization ratio and concatenates them to form a summary concisely, in order to convey the most important theme of the document. On the other hand, there has been a surge of interest in developing representation learning techniques for a wide variety of natural language processing (NLP)-related tasks. However, to our knowledge, they are largely unexplored in the context of extractive spoken document summarization. With the above background, this study explores a novel use of both word and sentence representation techniques for extractive spoken document summarization. In addition, three variants of sentence ranking models building on top of such representation techniques are proposed. Furthermore, extra information cues like the prosodic features extracted from spoken documents, apart from the lexical features, are also employed for boosting the summarization performance. A series of experiments conducted on the MATBN broadcast news corpus indeed reveal the performance merits of our proposed summarization methods in relation to several state-of-the-art baselines.	automatic summarization;experiment;feature learning;machine learning;natural language processing	Kai-Wun Shih;Kuan-Yu Chen;Shih-Hung Liu;Hsin-Min Wang;Berlin Chen	2015	IJCLCLP		automatic summarization;the internet;boosting (machine learning);natural language processing;artificial intelligence;ranking;multi-document summarization;computer science;feature learning;sentence	Web+IR	-21.91478431035848	-66.23408002953782	181766
65f816efac6f590ded3c7f7cdcbab0ee6553aeac	named entity recognition for telugu	noun;rule based;named entity recognition;conditional random field;named entity	This paper is about Named Entity Recognition (NER) for Telugu. Not much work has been done in NER for Indian languages in general and Telugu in particular. Adequate annotated corpora are not yet available in Telugu. We recognize that named entities are usually nouns. In this paper we therefore start with our experiments in building a CRF (Conditional Random Fields) based Noun Tagger. Trained on a manually tagged data of 13,425 words and tested on a test data set of 6,223 words, this Noun Tagger has given an F-Measure of about 92%. We then develop a rule based NER system for Telugu. Our focus is mainly on identifying person, place and organization names. A manually checked Named Entity tagged corpus of 72,157 words has been developed using this rule based tagger through bootstrapping. We have then developed a CRF based NER system for Telugu and tested it on several data sets from the Eenaadu and Andhra Prabha newspaper corpora developed by us here. Good performance has been obtained using the majority tag concept. We have obtained overall F-measures between 80% and 97% in various experiments.	bootstrapping (compilers);brill tagger;conditional random field;experiment;heuristic;morphological parsing;named entity;part-of-speech tagging;phrase chunking;test data;text corpus	P. Srikanth;Kavi Narayana Murthy	2008			rule-based system;natural language processing;noun;speech recognition;computer science;machine learning;pattern recognition;entity linking;linguistics;conditional random field	NLP	-24.066204830519794	-71.82447730335447	181817
e1de3705b14c8979847a72dce66512cf0adfff34	discourse connective - a marker for identifying featured articles in biological wikipedia		Wikipedia is a free-content Internet encyclopedia that can be edited by anyone who accesses it. As a result, Wikipedia contains both featured and non-featured articles. Featured articles are high-quality articles and nonfeatured articles are poor quality articles. Since there is an exponential growth of Wikipedia articles, the need to identify the featured Wikipedia articles has become indispensable so as to provide quality information to the users. As very few attempts have been carried out in the biology domain of English Wikipedia articles, we present our study to automatically measure the information quality in biological Wikipedia articles. Since the coherence shows representational information quality of a text, we have used the discourse connective count measure for our study. We compare this novel measure with two other popular approaches word count measure and explicit document model method that have been successfully applied to the task of quality measurement in Wikipedia articles. We organized the Wikipedia articles into balanced and unbalanced set. The balanced set contains featured and non-featured articles of equal length and the unbalanced set contains randomly selected featured and non-featured articles. The best result for the balanced set is obtained with F-measure of 83.2%, while using Support Vector Machine classifier with 4-gram representation and Term Frequency-Inverse Document Frequency weighting scheme. Meanwhile, the best result for unbalanced corpus is obtained using the discourse connective count measure with an F measure of 98.06%.	bag-of-words model;baseline (configuration management);binary prefix;categorization;experiment;information quality;internet;logical connective;machine learning;n-gram;nec shun-ei;naive bayes classifier;randomness;support vector machine;text corpus;tf–idf;time complexity;unbalanced circuit;wikipedia	Sindhuja Gopalan;Paolo Rosso;Sobha Lalitha Devi	2016	Research in Computing Science		natural language processing;computer science;linguistics;communication	NLP	-24.92008874055787	-67.70362531621632	182286
7b7c71d7f75da8592f24ef4da0e8708ba89f6217	semantic and logical inference model for textual entailment	logical inference model;pascal rte challenge;relation-based model;lexical matching algorithm;entailment pair;lexical similarity resource;lexical method;relation-based approach;compositional approach;local entailment problem;textual entailment;entailment recognition	We compare two approaches to the problem of Textual Entailment: SLIM, a compositional approach modeling the task based on identifying relations in the entailment pair, and BoLI, a lexical matching algorithm. SLIM’s framework incorporates a range of resources that solve local entailment problems. A search-based inference procedure unifies these resources, permitting them to interact flexibly. BoLI uses WordNet and other lexical similarity resources to detect correspondence between related words in the Hypothesis and the Text. In this paper we describe both systems in some detail and evaluate their performance on the 3rd PASCAL RTE Challenge. While the lexical method outperforms the relation-based approach, we argue that the relation-based model offers better long-term prospects for entailment recognition.	algorithm;textual entailment;wordnet	Dan Roth;Mark Sammons	2007			natural language processing;textual entailment;computer science;machine learning;data mining;preferential entailment;algorithm	NLP	-24.463896893876456	-73.114997433916	182517
927ee5e4884cacba9a7a8a25332f48767216a197	optimizing features for dialogue act classification	natural language interfaces;pattern clustering;human computer interaction;function words;meeting summarization feature optimization dialogue act classification natural language dialogue short text semantic similarity algorithm ordinary user complex computer application interaction dialogue act classifier slim function word classifier sfwc computational simplicity function word feature clustering grammatical principles classification accuracy sentence form selection intelligent text processing applications question answering;text analysis;text analysis grammars human computer interaction interactive systems natural language processing pattern classification pattern clustering;grammars;dialogue systems;pattern classification;pattern recognition;decision trees;interactive systems;natural language processing;feature extraction accuracy decision trees taxonomy training optimization educational institutions;function words natural language interfaces interactive systems decision trees pattern recognition dialogue systems	Natural language dialogue is an important component of interaction between ordinary users and complex computer applications. Short Text Semantic Similarity algorithms have been developed to improve the efficiency of producing sophisticated dialogue systems. Such algorithms are currently unable to discriminate between different dialogue acts (assertions, questions, instructions etc.), requiring the addition of efficient dialogue act classifiers to enhance them. The Slim Function Word Classifier (SFWC) has proved promising, particularly in its computational simplicity. This study optimizes the SFWC by clustering function word features using grammatical principles. Experiments show a significant improvement in classification accuracy for a selection of sentence forms which were challenging for the unoptimized SFWC. Results are expected to be applicable to many intelligent text processing applications ranging from question answering to meeting summarization.	algorithm;assertion (software development);cluster analysis;computer;context-free grammar;dialog system;natural language;optimizing compiler;question answering;semantic similarity	James O'Shea;Zuhair Bandar;Keeley A. Crockett	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.87	natural language processing;text mining;speech recognition;computer science;artificial intelligence;machine learning;decision tree;pattern recognition	NLP	-23.472822606598488	-71.131162411776	182629
2b67dfc9f9688eb2b53cdd9cd2ab3dcd373c82f0	frame semantic tree kernels for social network extraction from text		In this paper, we present work on extracting social networks from unstructured text. We introduce novel features derived from semantic annotations based on FrameNet. We also introduce novel semantic tree kernels that help us improve the performance of the best reported system on social event detection and classification by a statistically significant margin. We show results for combining the models for the two aforementioned subtasks into the overall task of social network extraction. We show that a combination of features from all three levels of abstractions (lexical, syntactic and semantic) are required to achieve the best performing system.	data (computing);framenet;lexicon;social network	Apoorv Agarwal;Sriramkumar Balasubramanian;Anup Kotalwar;Jiehan Zheng;Owen Rambow	2014		10.3115/v1/E14-1023	natural language processing;semantic computing;computer science;pattern recognition;data mining	NLP	-21.887772898435546	-68.073138822384	182776
942d9ad8e1276c46d6aa200e5377db4dfb76b3ce	evaluation of internal validity measures in short-text corpora	actividad text mess 1 solo si referencia este proyecto	Short texts clustering is one of the most difficult tasks in natural language processing due to the low frequencies of the document terms. We are interested in analysing these kind of corpora in order to develop novel techniques that may be used to improve results obtained by classical clustering algorithms. In this paper we are presenting an evaluation of different internal clustering validity measures in order to determine the possible correlation between these measures and that of the F -Measure, a well-known external clustering measure used to calculate the performance of clustering algorithms. We have used several short-text corpora in the experiments carried out. The obtained correlation with a particular set of internal validity measures let us to conclude that some of them may be used to improve the performance of text clustering algorithms.	algorithm;british informatics olympiad;cluster analysis;document classification;experiment;internal validity;natural language processing;text corpus	Diego Ingaramo;David Pinto;Paolo Rosso;Marcelo Luis Errecalde	2008		10.1007/978-3-540-78135-6_48	speech recognition;fuzzy clustering;computer science;artificial intelligence;data mining	ML	-25.617560233094128	-66.66702124141332	182975
48a4c01c1224b12e5f583305f56ffac858814906	explaining topical distances using word embeddings	convergence;electronic mail;text mining;approximation algorithms;vocabulary;semantics;economics	Word and document embeddings have gained a lot of attention recently, because they tend to work well in text mining tasks. Yet, they elude humans intuition. In this paper we are making the attempt to explain the arithmetic difference between two document embeddings by a series of word embeddings. We present an algorithm that iteratively picks words from a vocabulary that closes the topical gap between the documents. Moreover, we present the Econstor16 corpus that was used for the experiments. Although not all words that are found are great matches, the algorithm is able to find sets of words that are reasonable to a human that reads both documents. Remarkably, some of the well-explaining words are mentioned in neither documents.	algorithm;document;experiment;microsoft word for mac;text mining;vocabulary;whole earth 'lectronic link;word embedding	Nils Witt;Christin Seifert;Michael Granitzer	2016	2016 27th International Workshop on Database and Expert Systems Applications (DEXA)	10.1109/DEXA.2016.052	natural language processing;text mining;speech recognition;convergence;computer science;artificial intelligence;machine learning;data mining;database;semantics;world wide web	NLP	-25.67114932252447	-67.33658042515486	183780
4e5b5f7c96764a3670b2689d44606ef7f45b2cf3	event-time relation identification using machine learning and rules	tempeval 2007 task b;rule based approach;rule based system;rule based;temporal information;machine learning;conditional random field;temporal relation identification;natural language processing	Temporal information extraction is a popular and interesting research field in the area of Natural Language Processing (NLP). In this paper, we report our works on temporal relation identification within the TimeML framework. We worked on TempEval-2007 Task B that involves identification of relations between events and document creation time. Two different systems, one based on machine learning and the other based on handcrafted rules, are developed. The machine learning system is based on Conditional Random Field (CRF) that makes use of only some of the features available in TimeBank corpus in order to infer temporal relations. The second system is developed using a set of manually constructed handcrafted rules. Evaluation results show that the rule-based system performs better compared to the machine learning based system with the precision, recall and F-score values 75.9%, 75.9% and 75.9%, respectively under the strict evaluation scheme and 77.1%, 77.1% and 77.1%, respectively under the relaxed evaluation scheme. In contrast, CRF based system yields precision, recall and F-score values 74.1%, 73.6% and 73.8%, respectively under the strict evaluation scheme and 75.1%, 74.6% and 74.8%, respectively under the relaxed evaluation scheme.	machine learning	Anup Kumar Kolya;Asif Ekbal;Sivaji Bandyopadhyay	2010		10.1007/978-3-642-15760-8_16	rule-based system;speech recognition;computer science;artificial intelligence;machine learning;pattern recognition;data mining;conditional random field	ML	-23.9640364467642	-70.32635462082156	185153
2bfd3b9a4c9743660a3711c36f4c001afb6652c3	hearst patterns revisited: automatic hypernym detection from large text corpora		Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.	benchmark (computing);categorization;corpus linguistics;text corpus;unsupervised learning	Stephen Roller;Douwe Kiela;Maximilian Nickel	2018			computer science;natural language processing;artificial intelligence;text corpus	NLP	-21.377534144862704	-71.63857825173577	185350
4944bd1a531d8580344ed30a1105372d439f3c79	tuning support vector machines for biomedical named entity recognition	svm-based recognition system;non-entity class;class splitting technique;tuning support vector machine;entity recognition;genia corpus;proposed new feature;svm training;new feature;available largest corpus;genia corpus show;unsupervised learning;support vector machine;maximum entropy;part of speech;support vector	We explore the use of Support Vector Machines (SVMs) for biomedical named entity recognition. To make the SVM training with the available largest corpus – the GENIA corpus – tractable, we propose to split the non-entity class into sub-classes, using part-of-speech information. In addition, we explore new features such as word cache and the states of an HMM trained by unsupervised learning. Experiments on the GENIA corpus show that our class splitting technique not only enables the training with the GENIA corpus but also improves the accuracy. The proposed new features also contribute to improve the accuracy. We compare our SVMbased recognition system with a system using Maximum Entropy tagging method.	cobham's thesis;data pre-processing;entity–relationship model;experiment;hidden markov model;named-entity recognition;neural coding;polynomial kernel;principle of maximum entropy;support vector machine;text corpus;unsupervised learning	Jun'ichi Kazama;Takaki Makino;Yoshihiro Ohta;Jun'ichi Tsujii	2002			unsupervised learning;support vector machine;speech recognition;computer science;machine learning;pattern recognition	NLP	-22.277335506406722	-71.78393032290425	185367
112730b44c8acbf8736ef017d07be4f5a9a56e72	uiowa at ntcir-9 rite: using the power of the crowd to establish inference rules		We participated in the Binary Classification (BC), Multiple Classification (MC), and Question and Answer (RITE4QA) subtasks for both Simplified Chinese and Traditional Chinese in NTCIR-9 RITE. In this paper, we describe our procedure to establish inference rules using crowdsourcing, refine and weigh them, and apply these rules to a test collection.	binary classification;crowdsourcing	Christopher G. Harris	2011			rule of inference;data mining;rite;binary classification;engineering;crowdsourcing	NLP	-23.055237475390314	-69.3812464194108	186236
f0e1225a5efb6496ab08469f4c29e9da7f5d6c54	neurosent-pdi at semeval-2018 task 1: leveraging a multi-domain sentiment model for inferring polarity in micro-blog text		This paper describes the NeuroSent system that participated in SemEval 2018 Task 1. Our system takes a supervised approach that builds on neural networks and word embeddings. Word embeddings were built by starting from a repository of user generated reviews. Thus, they are specific for sentiment analysis tasks. Then, tweets are converted in the corresponding vector representation and given as input to the neural network with the aim of learning the different semantics contained in each emotion taken into account by the SemEval task. The output layer has been adapted based on the characteristics of each subtask. Preliminary results obtained on the provided training set are encouraging for pursuing the investigation into this direction.	artificial neural network;blog;portable database image;semeval;sensor;sentiment analysis;supervised learning;test set;word embedding	Mauro Dragoni	2018			natural language processing;machine learning;microblogging;semeval;artificial intelligence;computer science;social media	NLP	-19.335201788505223	-70.84971874255018	186327
8eb4a392e2a15c512d98f0f83216ff18583d0ccc	four methods for supervised word sense disambiguation	vector space model;naive bayes;statistical method;indexing terms;word sense disambiguation;automatic generation;naive bayes classifier;machine learning;term weighting;text retrieval;automatic indexing;latent semantic analysis;natural language processing	Word sense disambiguation is the task to identify the intended meaning of an ambiguous word in a certain context, one of the central problems in natural language processing. This paper describes four novel supervised disambiguation methods which adapt some familiar algorithms. They built on the Vector Space Model using an automatically generated stop list and two different statistical methods of finding	adaptive system;algorithm;mathematical model;naive bayes classifier;natural language processing;scalability;table (database);word sense;word-sense disambiguation	Kinga Schumacher	2007		10.1007/978-3-540-73351-5_28	natural language processing;naive bayes classifier;semeval;computer science;machine learning;pattern recognition	NLP	-24.784266310924014	-66.56387588555125	186362
2c851cabfa7682b736bf5814271601eaf4c1408d	language of vandalism: improving wikipedia vandalism detection via stylometric analysis	stylometric analysis;wikipedia vandalism detection;unique genre;textual vandalism;collective intelligence;community-based knowledge forum;unique language style;shallow lexico-syntactic pattern;discriminate vandalism;vandalism detection;deep syntactic pattern	Community-based knowledge forums, such as Wikipedia, are susceptible to vandalism, i.e., ill-intentioned contributions that are detrimental to the quality of collective intelligence. Most previous work to date relies on shallow lexico-syntactic patterns and metadata to automatically detect vandalism in Wikipedia. In this paper, we explore more linguistically motivated approaches to vandalism detection. In particular, we hypothesize that textual vandalism constitutes a unique genre where a group of people share a similar linguistic behavior. Experimental results suggest that (1) statistical models give evidence to unique language styles in vandalism, and that (2) deep syntactic patterns based on probabilistic context free grammars (PCFG) discriminate vandalism more effectively than shallow lexicosyntactic patterns based on n-grams.	collective intelligence;grams;lexico;n-gram;statistical model;stochastic context-free grammar;stylometry;wikipedia	Manoj Harpalani;Michael Hart;Sandesh Signh;Rob Johnson;Yejin Choi	2011			natural language processing;world wide web	NLP	-21.492141387798313	-67.97229289604998	186458
15c70c9124ff1f83cb309ce5f95d293c5894e277	inf-ufrgs-opinion-mining at semeval-2016 task 6: automatic generation of a training corpus for unsupervised identification of stance in tweets		This paper describe a weakly supervised solution for detecting stance in tweets, submitted to the SemEval 2016 Stance Task. Our approach is based on the premise that stance can be exposed as positive or negative opinions, although not necessarily about the stance target itself. Our system receives as input ngrams representing opinion targets and common terms used to denote stance (e.g. hashtags), and use these features, together with the sentiment detection solutions, to automatically compose a large training corpus. Then, it applies a supervised learning algorithm to develop a stance prediction model.	algorithm;f1 score;hashtag;n-gram;semeval;sensor;supervised learning;text corpus	Marcelo Dias;Karin Becker	2016			computer science;sentiment analysis;artificial intelligence;natural language processing;semeval;speech recognition	NLP	-22.09148681276823	-69.88567696663326	186465
f6bc340af859e8a9ec2d97d77042eb14084c0446	improving the estimation of word importance for news multi-document summarization		We introduce a supervised model for predicting word importance that incorporates a rich set of features. Our model is superior to prior approaches for identifying words used in human summaries. Moreover we show that an extractive summarizer using these estimates of word importance is comparable in automatic evaluation with the state-of-the-art.	automatic summarization;experiment;ibm notes;internet information services;multi-document summarization	Kai Hong;Ani Nenkova	2014			natural language processing;computer science;pattern recognition;information retrieval	NLP	-21.75281958550341	-67.06631870690182	186828
80d0ea9eb5f632c545236d40623f32a6952a55b2	wisebe: window-based sentence boundary evaluation		Sentence Boundary Detection (SBD) has been a major research topic since Automatic Speech Recognition transcripts have been used for further Natural Language Processing tasks like Part of Speech Tagging, Question Answering or Automatic Summarization. But what about evaluation? Do standard evaluation metrics like precision, recall, F-score or classification error; and more important, evaluating an automatic system against a unique reference is enough to conclude how well a SBD system is performing given the final application of the transcript? In this paper we propose Window-based Sentence Boundary Evaluation (WiSeBE), a semi-supervised metric for evaluating Sentence Boundary Detection systems based on multi-reference (dis)agreement. We evaluate and compare the performance of different SBD systems over a set of Youtube transcripts using WiSeBE and standard metrics. This double evaluation gives an understanding of how WiSeBE is a more reliable metric for the SBD task.	automatic summarization;evaluation function;f1 score;natural language processing;part-of-speech tagging;question answering;semiconductor industry;smart battery;speech recognition	Carlos-Emiliano González-Gallardo;Juan-Manuel Torres-Moreno	2018		10.1007/978-3-030-04497-8_10	automatic summarization;natural language processing;artificial intelligence;question answering;recall;sentence;part-of-speech tagging;computer science	NLP	-23.017306517318783	-70.05914856510972	186926
defc0ae74a38367fb1033108cf7b8baaab3770f7	can recognising multiword expressions improve shallow parsing?	multiword expression;adjective-noun construction;unannotated data;shallow parsing accuracy;human annotation;textual data;classification method;experimental study;compound nominal;shallow parsing	There is significant evidence in the literature that integrating knowledge about multiword expressions can improve shallow parsing accuracy. We present an experimental study to quantify this improvement, focusing on compound nominals, proper names and adjectivenoun constructions. The evaluation set of multiword expressions is derived from WordNet and the textual data are downloaded from the web. We use a classification method to aid human annotation of output parses. This method allows us to conduct experiments on a large dataset of unannotated data. Experiments show that knowledge about multiword expressions leads to an increase of between 7.5% and 9.5% in accuracy of shallow parsing in sentences containing these multiword expressions.	experiment;shallow parsing;text corpus;wordnet	Ioannis Korkontzelos;Suresh Manandhar	2010				NLP	-24.824341961295442	-73.10282874969421	187034
8b2254927e35b33585dd3f61a27a9479a51dfda5	on the contribution of word embeddings to temporal relation classification		Temporal relation classification is a challenging task, especially when there are no explicit markers to characterise the relation between temporal entities. This occurs frequently in intersentential relations, whose entities are not connected via direct syntactic relations making classification even more difficult. In these cases, resorting to features that focus on the semantic content of the event words may be very beneficial for inferring implicit relations. Specifically, while morpho-syntactic and context features are considered sufficient for classifying event-timex pairs, we believe that exploiting distributional semantic information about event words can benefit supervised classification of other types of pairs. In this work, we assess the impact of using word embeddings as features for event words in classifying temporal relations of event-event pairs and event-DCT (document creation time) pairs.	algorithm;discrete cosine transform;entity;general-purpose modeling;machine learning;performance;statistical classification;supervised learning;timex sinclair;word embedding;word lists by frequency	Paramita Mirza;Sara Tonelli	2016			natural language processing;artificial intelligence;pattern recognition;computer science	NLP	-19.360815509912666	-69.80651343427596	187514
8645c2dcfc96d356d757090acb63093bf13fb8fe	two-stage approach to named entity recognition using wikipedia and dbpedia	dbpedia;wikipedia;information extraction;named entity recognition;question answering	In natural language understanding, extraction of named entity (NE) mentions in given text and classification of the mentions into pre-defined NE types are important processes. Most NE recognition (NER) relies on resources such as a training corpus or NE dictionary, but collecting them manually is laborious and time-consuming. This paper proposes a two-stage approach based on nothing but Wikipedia and DBpedia to implement NER. This paper also addresses technical problems in developing Korean NER. In experiments, the proposed method can recognize NEs in short question sentences with 14.2% errors.	dbpedia;dictionary;experiment;named entity;natural language understanding;text corpus;wikipedia	Seonghan Ryu;Hwanjo Yu;Gary Geunbae Lee	2017		10.1145/3022227.3022299	natural language processing;question answering;computer science;data mining;brand;entity linking;information extraction;information retrieval	NLP	-24.305656194415157	-71.19567128474105	187623
01fa2a363aff2ef9afc8b4049e516d12b5610685	a critique of word similarity as a method for evaluating distributional semantic models	qa0075 electronic computers computer science;p0098 computational linguistics natural language processing	This paper aims to re-think the role of the word similarity task in distributional semantics research. We argue while it is a valuable tool, it should be used with care because it provides only an approximate measure of the quality of a distributional model. Word similarity evaluations assume there exists a single notion of similarity that is independent of a particular application. Further, the small size and low inter-annotator agreement of existing data sets makes it challenging to find significant differences between models.	algorithmic efficiency;approximation algorithm;distributional semantics;document classification;expectation propagation;garrus vakarian;inter-rater reliability;natural language processing;unified model	Miroslav Batchkarov;Thomas Kober;Jeremy Reffin;Julie Weeds;David J. Weir	2016		10.18653/v1/W16-2502	natural language processing;semantic similarity;computer science;theoretical computer science;linguistics	NLP	-26.24183440915586	-71.88590683524181	187972
500cbf0dd48828b80238b30d47140e4a7b4e47bb	an information retrieval-based approach to determining contextual opinion polarity of words		The paper presents a novel method for determining contextual polarity of ambiguous opinion words. The task of categorizing polarity of opinion words is cast as an information retrieval problem. The advantage of the approach is that it does not rely on hand-crafted rules and opinion lexicons. Evaluation on a set of polarity-ambiguous adjectives as well as a set of both ambiguous and unambiguous adjectives shows improvements compared to a context-independent me-	categorization;dependency relation;information retrieval;lexicon;text corpus	Olga Vechtomova;Kaheer Suleman;Jack Thomas	2014		10.1007/978-3-319-06028-6_56	natural language processing;computer science;data mining	NLP	-25.412685635729456	-70.72803957370708	188801
3201164a38686a73ff8d4bf41a75bc8739b5f210	exploiting social network structure for person-to-person sentiment analysis		Person-to-person evaluations are prevalent in all kinds of discourse and important for establishing reputations, building social bonds, and shaping public opinion. Such evaluations can be analyzed separately using signed social networks and textual sentiment analysis, but this misses the rich interactions between language and social context. To capture such interactions, we develop a model that predicts individual A’s opinion of individual B by synthesizing information from the signed social network in which A and B are embedded with sentiment analysis of the evaluative texts relating A to B. We prove that this problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss Markov random field, and we show that this implementation outperforms text-only and network-only versions in two very different datasets involving community-level decision-making: the Wikipedia Requests for Adminship corpus and the Convote U.S. Congressional speech corpus.	emoticon;graph (discrete mathematics);sentiment analysis;signed graph;social network;unordered associative containers (c++)	Robert West;Hristo S. Paskov;Jure Leskovec;Christopher Potts	2014	Transactions of the Association for Computational Linguistics	10.1162/tacl_a_00184	natural language processing;computer science;machine learning;social psychology;world wide web	NLP	-20.846870131796905	-66.52548443090997	189018
79f3717023011bdbe4b18464c43706054ebb35f6	aspie96 at ironita (evalita 2018): irony detection in italian tweets with character-level convolutional rnn		English. Irony is characterized by a strong contrast between what is said and what is meant: this makes its detection an important task in sentiment analysis. In recent years, neural networks have given promising results in different areas, including irony detection. In this report, I describe the system used by the Aspie96 team in the IronITA competition (part of EVALITA 2018) for irony and sarcasm detection in Italian tweets. Italiano. L’ironia è caratterizzata da un forte contrasto tra ciò che viene detto e ciò che si intende: questo ne rende la rilevazione un importante task nell’analisi del sentimento. In anni recenti, le reti neurali hanno prodotto risultati promettenti in aree diverse, tra cui la rilevazione dell’ironia. In questo report, descrivo il sistema utilizzato dal team Aspie96 nella competizione di IronITA (parte di EVALITA 2018) per la rilevazione dell’ironia e del sarcasmo in tweet italiani.		Valentino Giudice	2018				NLP	-21.086923741910873	-70.05091598622212	189277
3e9efb127aa9079b1502e1dfbe3cd9f8e6c4dae2	author clustering with the aid of a simple distance measure		A simple distance measure has been applied to the author clustering problem to determine which documents are written by the same author. This simple distance measure works with the probability distribution of character sequences of a document, making it insensitive to language differences. The top most frequent features k, where k is chosen to be 300, determine the distribution where punctuation is present. Also, the uppercase letters are transformed to lowercase symbols, while a threshold of 3.0 remains for the symmetric distance score. In addition, character 2-grams are chosen due to their best outcomes. Using the BCubed F-score provided, it achieves a score of 0.54 on the training set and a score of 0.53 on the test set with a relative low MAP score. Obtaining clusters from links still shows problems.	cluster analysis;f1 score;test set	Houda Alberts	2017			cluster analysis;computer science;pattern recognition;artificial intelligence	ML	-26.11785689184547	-68.36773442136533	189473
68f8d0eef76797a098006b78135cebaf68f9d765	determining senses for word sense disambiguation in turkish	conference;meeting	Word sense disambiguation is an important intermediate stage for many natural language processing applications. The senses of an ambiguous word are the classification of usages for that specific word. This paper deals with the methodologies of determining the senses for a given word if they can not be obtained from an already available resource like WordNet. We offer a method that helps us to determine the sense boundaries gradually. In this method, first we decide on some features that are thought to be effective on the senses and divide the instances first into two, then according to the results of evaluations we continue dividing instances gradually. In a second method we use the pseudo words. We devise artificial words depending on some criteria and evaluate classification algorithms on these previously classified words. Keywords—Word sense disambiguation, sense determination, pseudo words, sense granularity.	algorithm;coherence (physics);computation;correctness (computer science);entity;natural language processing;point of sale;simulation;text corpus;word sense;word-sense disambiguation;wordnet	Zeynep Orhan;Zeynep Altan	2005			natural language processing;speech recognition;semeval;linguistics	NLP	-25.924899183946415	-72.22416638343357	189524
3ac6048987af33cbccb0b5d3419df6092b09f6d2	choosing the word most typical in context using a lexical co-occurrence network	lexical choice program;partial solution;new statistical approach;lexical co-occurrence network;second-order co-occurrence relation;large corpus;lexical choice;second order	This paper presents a partial solution to a component of the problem of lexical choice: choosing the synonym most typical, or expected, in context. We apply a new statistical approach to representing the context of a word through lexical co-occurrence networks. The implementation was trained and evaluated on a large corpus, and results show that the inclusion of second-order co-occurrence relations improves the performance of our implemented lexical choice program.	co-occurrence networks;lexical choice;text corpus	Philip Edmonds	1997			natural language processing;lexical density;speech recognition;computer science;lexical chain;linguistics;lexical choice;second-order logic	NLP	-25.512381595572798	-72.97061390005632	189622
969b7cbbe338cfda1b3f48024c82f387e03c2388	clustering based approach to learning regular expressions over large alphabet for noisy unstructured text	regular expression learning;information extraction;rule based;regular expression;clustering in noisy text;approaches to learning;rule based information extraction	Regular Expressions have been used for Information Extraction tasks in a variety of domains. The alphabet of the regular expression can either be the relevant tokens corresponding to the entity of interest or individual characters in which case the alphabet size becomes very large. The presence of noise in unstructured text documents along with increased alphabet size of the regular expressions poses a significant challenge for entity extraction tasks, and also for algorithmically learning complex regular expressions. In this paper, we present a novel algorithm for regular expression learning which clusters similar matches to obtain the corresponding regular expressions, identifies and eliminates noisy clusters, and finally uses weighted disjunction of the most promising candidate regular expressions to obtain the final expression. The experimental results demonstrate high value of both precision and recall of this final expression, which reinforces the applicability of our approach in entity extraction tasks of practical importance.	algorithm;induction of regular languages;information extraction;named-entity recognition;precision and recall;regular expression	Rohit Babbar;Nidhi Singh	2010		10.1145/1871840.1871848	computer science;machine learning;pattern recognition;data mining	PL	-25.989111074203212	-69.37195738179298	189646
c58831d1978e4812f00b51f1e2ed53f9df9c307c	a deep learning approach to bilingual lexicon induction in the biomedical domain	bilingual lexicon induction;biomedical text mining;medical terminology;representation learning	Bilingual lexicon induction (BLI) is an important task in the biomedical domain as translation resources are usually available for general language usage, but are often lacking in domain-specific settings. In this article we consider BLI as a classification problem and train a neural network composed of a combination of recurrent long short-term memory and deep feed-forward networks in order to obtain word-level and character-level representations. The results show that the word-level and character-level representations each improve state-of-the-art results for BLI and biomedical translation mining. The best results are obtained by exploiting the synergy between these word-level and character-level representations in the classification model. We evaluate the models both quantitatively and qualitatively. Translation of domain-specific biomedical terminology benefits from the character-level representations compared to relying solely on word-level representations. It is beneficial to take a deep learning approach and learn character-level representations rather than relying on handcrafted representations that are typically used. Our combined model captures the semantics at the word level while also taking into account that specialized terminology often originates from a common root form (e.g., from Greek or Latin).	artificial neural network;biological neural networks;deep learning;jargon;lexicon;long short-term memory;nomenclature;synergy;benefit	Geert Heyman;Ivan Vulic;Marie-Francine Moens	2018		10.1186/s12859-018-2245-8	medical terminology;biomedical text mining;semantics;biology;natural language processing;deep learning;bioinformatics;artificial neural network;bilingual lexicon;terminology;artificial intelligence;feature learning	NLP	-19.173521921899933	-71.07983609351542	189802
4847321fed06695d729b3335f1e036fd6e319514	overview of the nlpcc 2015 shared task: chinese word segmentation and pos tagging for micro-blog texts		Word segmentation and Part-of-Speech (POS) tagging are two fundamental tasks for Chinese language processing. In recent years, word segmentation and POS tagging have undergone great development. The popular method is to regard these two tasks as sequence labeling problem, which can be handled with supervised learning algorithms such as Conditional Random Fields (CRF)[1]. However, the performances of the state-of-the-art systems are still relatively low for the informal texts, such as micro-blogs, forums. In this shared task, we wish to investigate the performances of Chinese word segmentation and POS tagging for the micro-blog texts.	algorithm;blog;conditional random field;machine learning;part-of-speech tagging;performance;sequence labeling;supervised learning;text segmentation	Xipeng Qiu;Peng Qian;Liusong Yin;Xuanjing Huang	2015		10.1007/978-3-319-25207-0_50	natural language processing;speech recognition;computer science;artificial intelligence	NLP	-21.358768058162617	-72.14511254449863	190379
b1fdb8b765178866ff7d9f7cba3ae01cc1a047f9	emotion in code-switching texts: corpus construction and analysis		Previous researches have focused on analyzing emotion through monolingual text, when in fact bilingual or code-switching posts are also common in social media. Despite the important implications of code-switching for emotion analysis, existing automatic emotion extraction methods fail to accommodate for the code-switching content. In this paper, we propose a general framework to construct and analyze the code-switching emotional posts in social media. We first propose an annotation scheme to identify the emotions associated with the languages expressing them in a Chinese-English code-switching corpus. We then make some observations and generate statistics from the corpus to analyze the linguistic phenomena of code-switching texts in social media. Finally, we propose a multiple-classifier-based automatic detection approach to detect emotion in the codeswitching corpus for evaluating the effectiveness of both Chinese and English texts.	emotion recognition;inter-rater reliability;sensor;social media;statistical classification;text corpus	Sophia Yat Mei Lee;Zhongqing Wang	2015		10.18653/v1/W15-3116	psychology;natural language processing;linguistics;communication	NLP	-21.499991804062237	-68.00850159582102	190557
ffad89234de4222b958bcb70afb59ed1cfc2982d	feature-based models for improving the quality of noisy training data for relation extraction	information extraction;pattern learning;distant supervision;machine learning;topic models	Supervised relation extraction from text relies on annotated data. Distant supervision is a scheme to obtain noisy training data by using a knowledge base of relational tuples as the ground truth and finding entity pair matches in a text corpus. We propose and evaluate two feature-based models for increasing the quality of distant supervision extraction patterns.  The first model is an extension of a hierarchical topic model that induces background, relation specific and argument-pair specific feature distributions. The second model is a perceptron, trained to match an objective function that enforces two constraints: 1) an at-least-one semantics, i.e. at least one training example per relational tuple is assumed to be correct; 2) high scores for a dedicated NIL label that accounts for the noise in the training data. For both algorithms, neither explicit negative data nor the ratio of negatives has to be provided. Both algorithms give improvements over a maximum likelihood baseline as well as over a previous topic model without features, evaluated on TAC KBP data.	algorithm;baseline (configuration management);feature model;ground truth;knowledge base;loss function;optimization problem;perceptron;relationship extraction;text corpus;topic model	Benjamin Roth;Dietrich Klakow	2013		10.1145/2505515.2507850	natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;data mining;database;topic model;world wide web;information extraction;information retrieval	NLP	-19.353329251684052	-68.98724936743038	190649
d3e836cfa86f8b695512a2be42b13d6625049940	fine-grained analysis of explicit and implicit sentiment in financial news articles	returns;volatility;stock market;automatic sentiment analysis;agreement;investor sentiment;languages and literatures;prediction;information;financial news	This paper focuses on topic-specific and more specifically company-specific sentiment analysis in financial newswire text. This application is of great use to researchers in the financial domain who study the impact of news (media) on the stock markets. We investigate the viability of a new fine-grained sentiment annotation scheme. Most of the current approaches to sentiment analysis focus on the detection of explicit sentiment. As news text often contains implicit sentiment, i.e. factual information implying positive or negative sentiment, our approach aims to identify both explicit and implicit sentiment. Furthermore, this sentiment is analyzed on a fine-grained level by detecting the topic of the sentiment, as sentiment is not always expressed towards the topics one is interested in. In order to test our approach, we assembled a corpus of company-specific news articles, which was manually labeled by four annotators to create a gold standard. We compare the results of our method to the performance of two coarse-grained baseline systems: a lexicon-based approach and a supervised machine learning approach that makes use of lexical features. Our fine-grained approach outperforms both baselines, and its output shows substantial to almost perfect agreement with the gold standard sentiment labels. Using our annotation scheme, we are able to filter out irrelevant sentiment expressions and detect explicit and implicit sentiment in a reliable way. 2015 Published by Elsevier Ltd.	baseline (configuration management);expert system;expression (computer science);lexicon;machine learning;off topic;regular expression;relevance;sensor;sentiment analysis;supervised learning;text corpus	Marjan Van de Kauter;Diane Breesch;Véronique Hoste	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.02.007	information;volatility;prediction;computer science;data mining;world wide web;sentiment analysis;statistics	NLP	-22.361917273589025	-67.31904686313388	190701
39b38835020ba2fb811112b8562b2770c57c4553	sentiment analysis on tweets about diabetes: an aspect-level approach		In recent years, some methods of sentiment analysis have been developed for the health domain; however, the diabetes domain has not been explored yet. In addition, there is a lack of approaches that analyze the positive or negative orientation of each aspect contained in a document (a review, a piece of news, and a tweet, among others). Based on this understanding, we propose an aspect-level sentiment analysis method based on ontologies in the diabetes domain. The sentiment of the aspects is calculated by considering the words around the aspect which are obtained through N-gram methods (N-gram after, N-gram before, and N-gram around). To evaluate the effectiveness of our method, we obtained a corpus from Twitter, which has been manually labelled at aspect level as positive, negative, or neutral. The experimental results show that the best result was obtained through the N-gram around method with a precision of 81.93%, a recall of 81.13%, and an F-measure of 81.24%.	body of uterus;contain (action);diabetes mellitus;experiment;lexicon;n-gram;ontology (information science);programming languages;sentiment analysis;text corpus;gram;interest	María del Pilar Salas-Zárate;José Medina-Moreira;Katty Lagos-Ortiz;Harry Luna-Aveiga;Miguel Ángel Rodríguez-García;Rafael Valencia-García	2017		10.1155/2017/5140631	computer science;data mining;world wide web;information retrieval	NLP	-23.606154445246418	-67.6212397881044	190977
0111538e86c326ef8fc11b787fcb2e3b7b3cd158	supervised recognition of age-related spanish temporal phrases	decision tree;learning methods;temporal expressions;learning supervised method	This paper reports research on temporal expressions shaped by a common temporal expression for a period of years modified by an adverb of time. From a Spanish corpus we found that some of those phrases are agerelated expressions. To determine automatically the temporal phrases with such meaning we analyzed a bigger sample obtained from the Internet. We analyzed these examples to define the relevant features to support a learning method. We present some preliminary results when a decision tree is applied.	algorithm;automatic identification and data capture;compiler;decision tree;internet;machine learning;natural language processing;temporal expressions;text corpus	Sofía N. Galicia-Haro;Alexander F. Gelbukh	2009		10.1007/978-3-642-05258-3_13	natural language processing;computer science;machine learning;decision tree;pattern recognition	NLP	-24.86408243570898	-72.12132348265759	191232
88765d49433cbfa7d8716c469a0fb30c38bfe75b	cross-argument inference for implicit discourse relation recognition	implicit discourse relation;pair to pair inference	Motivated by the critical importance of connectives in recognizing discourse relations, we present an unsupervised cross-argument inference mechanism to implicit discourse relation recognition. The basic idea is to infer the implicit discourse relation of an argument pair from a large number of comparable argument pairs, which are automatically retrieved from the web in an unsupervised way. In this way, the inference proceeds from explicit relations to implicit ones via connective as bridge. This kind of pair-to-pair inference is based on the assumption that two argument pairs with high content similarity (i.e. comparable argument pairs) should have similar discourse relationship. Evaluation on PDTB proves the effectiveness of our inference mechanism in implicit relation recognition to the four level-1 relations. It also shows that our mechanism significantly outperforms other alternatives.	daisy digital talking book;discourse relation;logical connective;unsupervised learning	Yu Hong;Xiaopei Zhou;Tingting Che;Jian-Min Yao;Qiaoming Zhu;Guodong Zhou	2012		10.1145/2396761.2396801	natural language processing;computer science;artificial intelligence	NLP	-25.260190865812696	-72.02348952147702	191564
1f229e5ab17bfb207a517156eb748487f5c6e5a4	detecting the need for resources and their availability		In this working note we describe our submission to the FIRE2017 IRMiDiS track. We participated in both sub-tasks, the first of which was directed at identifying the need or availability of specific resources and the second at matching tweets expressing the need for a resource with tweets mentioning their availability. Our linguistically motivated approach using pattern matching of word n-grams achieved an overall average MAP score of 0.2458 for sub-task (1), outperforming ourmachine-learning approach (MAP 0.1739)while being surpassed by two other (automatic) systems. The linguistic approach was also used in sub-task (2). There it was the bestperforming approach with an f-score of 0.3793.	f1 score;grams;n-gram;pattern matching;sensor	Nelleke Oostdijk;Ali Hurriyetoglu	2017			computer science;data mining	NLP	-23.65949267655559	-70.73562275096225	191850
1d4e15c2639e37ba1e0be7418b78dc793d27e1e4	extracting technology and effect entities in patents and research papers		This paper describes our approach to tackling the task of Technical Trend Map Creation as posed in NTCIR-8. The basic method is Conditional Random Fields, which is considered as the most advanced method in Named Entity Recognition. In order to improve the performance, we further resort a tag modification approach and pattern-based method. Our system performed competitively, achieving the top F-measure among participants in the formal run.	conditional random field;entity;named-entity recognition;tag system	Jingjing Wang;Han Tong Loh;Wen Feng Lu	2010			computer science;artificial intelligence;data science;data mining	NLP	-24.298096829579677	-69.50390519942164	192021
69125bf0842ebd8f92d438d6065f4d7cda57ffb7	topic intrusion for automatic topic model evaluation			topic model	Shraey Bhatia;Jey Han Lau;Timothy Baldwin	2018			natural language processing;artificial intelligence;topic model;computer science;intrusion	NLP	-22.664148054611946	-68.38289761691466	192212
d42d6aa4408de325b0cd970981d1d15fd1f7f52d	representation of word sentiment, idioms and senses		Distributional Semantic Models (DSM) that represent words as vectors of weights over a high dimensional feature space have proved very effective in representing semantic or syntactic word similarity. For certain tasks however it is important to represent contrasting aspects such as polarity, different senses or idiomatic use of words. We present two methods for creating embeddings that take into account such characteristics: a feed-forward neural network for learning sentiment specific and a skip-gram model for learning sense specific embeddings. Sense specific embeddings can be used to disambiguate queries and other classification tasks. We present an approach for recognizing idiomatic expressions by means of the embeddings. This can be used to segment queries into meaningful chunks. The implementation is available as a library implemented in Python with core numerical processing written in C++, using a parallel linear algebra library for efficiency and scalability.	artificial neural network;c++;comparison of linear algebra libraries;feature vector;feedforward neural network;n-gram;numerical analysis;programming idiom;python;scalability;word sense	Giuseppe Attardi	2015			syntax;artificial neural network;machine learning;scalability;linear algebra;python (programming language);linguistics;artificial intelligence;feature vector;expression (mathematics);computer science	NLP	-20.318554119481632	-71.9042686526504	192737
dffca118ff5d76b7a3ed5eee90ed4796ffe9a9c0	codeswitching detection via lexical features in conditional random fields		Half of the world’s population is estimated to be at least bilingual. Due to this fact many people use multiple languages interchangeably for effective communication. At the Second Workshop on Computational Approaches to Code Switching, we are presented with a task to label codeswitched, Spanish-English (ES-EN) and Modern Standard Arabic-Dialect Arabic (MSA-DA), tweets. We built a Conditional Random Field (CRF) using wellrounded features to capture not only the two languages but also the other classes. On the Spanish-English(ES-EN) classification task, we obtained weighted F1-score of 0.88 on the tweet level and an accuracy of 96.5% on the token level. On the MSA-DA classification task, our system managed to obtain F1-score of 0.66 on tweet level and overall token level accuracy of 74.7%.	computation;conditional random field;f1 score;named entity;named-entity recognition	Prajwol Shrestha	2016		10.18653/v1/W16-5816	speech recognition;database;programming language	NLP	-21.758700307614834	-70.4041065087362	192775
1b391906b8a22d24a7609b0231c5015dfcee7c82	using kohonen maps of chinese morphological families to visualize the interplay of morphology and semantics in chinese	判解;self organizing maps;semantics;法律詞典;論文;大陸法學;computational morphology;法規;月旦法學;法律題庫;裁判時報;月旦知識庫;法學資料庫;tssci;bruno galmar;教學	A morphological family in Chinese is the set of compound words embedding a common morpheme. Self-organizing maps (SOM) of Chinese morphological families are built. Computation of the unified-distance matrices for the SOMs allows us to perform a semantic clustering of the members of the morphological families. Such a semantic clustering shed light on the interplay between morphology and semantics in Chinese. Then, we studied how the word lists used in a lexical decision task (LDT) [1] are mapped onto the clusters of the SOMs. We showed that such a mapping is helpful to predict whether in a LDT repetitive processing of members of a morphological family would elicit a satiation habituation of both morphological and semantic units of the shared morpheme. In their LDT experiment, [1] found evidence for morphological satiation but not for semantic satiation. Conclusions drawn from our computational experimentations and calculations are concordant with [1] behavioral experimental results. We finally showed that our work could be helpful to linguists to prepare adequate word lists for the behavioral study of Chinese morphological families.	cluster analysis;computation;dictionary attack;galaxy morphological classification;global descriptor table;lejos rcx;mathematical morphology;organizing (structure);self-organizing map	Bruno Galmar	2011	IJCLCLP		natural language processing;computer science;artificial intelligence;communication	NLP	-25.599706012348417	-66.67673546496385	192867
4bfcf3c91d6d36e3910d252809c8656696f104f2	evaluating the quality of web-mined bilingual sentences using multiple linguistic features	grammar;pragmatics;support vector machines;english as second language;statistical machine translation;link grammar parser web mined bilingual sentences multiple linguistic features statistical machine translation english as second language;natural languages;data mining;classification;noise measurement;grammar feature extraction pragmatics support vector machines information filters noise measurement;linguistic quality evaluation;grammars;internet;web mined bilingual sentences;second language;quality evaluation;feature extraction;web mining;link grammar parser;bilingual corpus;multiple linguistic features;information filters;natural languages data mining grammars internet;bilingual sentence pairs;classification linguistic quality evaluation bilingual sentence pairs	We raise the problem of evaluating the quality of bilingual sentences mined from the web, which is critical for such applications as statistical machine translation (SMT) and English as Second Language (ESL) learning. To tackle this problem, we propose a novel method that integrates multiple linguistic features related to spelling, grammar, and alignment, particularly the sentence type feature that indicates if a sentence can be parsed by the Link Grammar Parser (LGP). Promising results are achieved on a bilingual corpus of about 6 million English-Chinese sentences mined from the web, indicating the effectiveness of our proposed method.	correctness (computer science);experiment;link grammar parser;mined;parsing;statistical machine translation;text corpus	Xiaohua Liu;Ming Zhou	2010	2010 International Conference on Asian Language Processing	10.1109/IALP.2010.12	natural language processing;support vector machine;web mining;the internet;speech recognition;feature extraction;biological classification;computer science;noise measurement;grammar;linguistics;natural language;pragmatics	NLP	-24.623292617148444	-66.42101017532644	193101
382b8fa02871d6c30e103cf35fa8868b167424f1	cusat_team@iecsil-fire-2018: a named entity recognition system for indian languages		Named Entity Recognition is the process of classifying the elementary units in a text document into meaningful categories such as person, location, organization, etc. It is a significant preprocessing step in the semantic analysis of natural language text. There is an enormous growth of Indian language content on various media types such as websites, blogs, email, chats, etc. over the past decade. Automatic processing of this huge unstructured data is a challenging task especially when the companies are interested to ascertain public view on their products and processes. NER is one of the subtasks of Information Extraction. Extracting structured information from the natural language text is the ultimate goal of IE systems. Different methods are proposed and experimented for NER. In this paper, we propose a Named Entity Recognition system for Indian languages using Conditional Random Fields. Training and testing are conducted using the shared corpus provided by ’ARNEKT-IECSIL 2018’ competition organizers. The evaluation results show that the proposed system is able to outperform most of the reported methods in the competition.		P AjeesA.;Sumam Mary Idicula	2018				NLP	-23.22615370228578	-67.4281078412195	193123
da7e63cb6ac698aaf0ca3e5c3597a5ed69503deb	whose nickname is this? recognizing politicians from their aliases		Using aliases to refer to public figures is one way to make fun of people, to express sarcasm, or even to sidestep legal issues when expressing opinions on social media. However, linking an alias back to the real name is difficult, as it entails phonemic, graphemic, and semantic challenges. In this paper, we propose a phonemic-based approach and inject semantic information to align aliases with politicians’ Chinese formal names. The proposed approach creates an HMM model for each name to model its phonemes and takes into account document-level pairwise mutual information to capture the semantic relations to the alias. In this work we also introduce two new datasets consisting of 167 phonemic pairs and 279 mixed pairs of aliases and formal names. Experimental results show that the proposed approach models both phonemic and semantic information and outperforms previous work on both the phonemic and mixed datasets with the best top-1 accuracies of 0.78 and 0.59 respectively.	align (company);big data;entity;experiment;hidden markov model;mutual information;social media	Wei-Chung Wang;Hung-Chen Chen;Zhi-Kai Ji;Hui-I Hsiao;Yu-Shian Chiu;Lun-Wei Ku	2016			natural language processing;computer science;artificial intelligence	NLP	-24.173156869014964	-67.51695848339311	193507
42dd62d05911aa689b879adb6ffe55d724726bf4	recognizing the absence of opposing arguments in persuasive essays		In this paper, we introduce an approach for recognizing the absence of opposing arguments in persuasive essays. We model this task as a binary document classification and show that adversative transitions in combination with unigrams and syntactic production rules significantly outperform a challenging heuristic baseline. Our approach yields an accuracy of 75.6% and 84% of human performance in a persuasive essay corpus with various topics.	baseline (configuration management);document classification;heuristic;human reliability;persuasive technology;text corpus	Christian Stab;Iryna Gurevych	2016			machine learning;natural language processing;artificial intelligence;computer science;syntax;binary number;heuristic;document classification	NLP	-21.212519620175385	-71.36228498810665	193756
758cc85cdf23e31b111f6367de2dbb7f3dfdd166	tmunsw: identification of disorders and normalization to snomed-ct terminology in unstructured clinical notes		Unstructured clinical notes are rich sources for valuable patient information. Information extraction techniques can be employed to extract this valuable information, which in turn can be used to discover new knowledge. Named entity recognition and normalization are the basic tasks involved in information extraction. In this paper, identification of disorder named entities and the mapping of identified disorder entities to SNOMED-CT terminology using UMLS Metathesaurus is presented. A supervised linear chain conditional random field model based on sets of features to predict disorder mentions is used in conjunction with MetaMap to identify and normalize disorders. Error analysis conclude that recall of the developed system can be significantly increased by adding more features during model development and also by using a frame based approach for handling disjoint entities.	conditional random field;dictionary;information extraction;named entity;semi-supervised learning;semiconductor industry;supervised learning;systematized nomenclature of medicine	Jitendra Jonnagaddala;Siaw-Teng Liaw;Pradeep Kumar Ray;Manish Kumar;Hong-Jie Dai	2015			natural language processing;computer science;data mining;information retrieval	NLP	-24.63925967731241	-68.69086471143554	193798
2ff53374f26af5b555f795aa22944abc9229245c	expanding domain sentiment lexicon through double propagation	opinion mining;sentiment lexicon expansion;sentiment analysis;propagation	In most sentiment analysis applications, the sentiment lexicon plays a key role. However, it is hard, if not impossible, to collect and maintain a universal sentiment lexicon for all application domains because different words may be used in different domains. The main existing technique extracts such sentiment words from a large domain corpus based on different conjunctions and the idea of sentiment coherency in a sentence. In this paper, we propose a novel propagation approach that exploits the relations between sentiment words and topics or product features that the sentiment words modify, and also sentiment words and product features themselves to extract new sentiment words. As the method propagates information through both sentiment words and features, we call it double propagation. The extraction rules are designed based on relations described in dependency trees. A new method is also proposed to assign polarities to newly discovered sentiment words in a domain. Experimental results show that our approach is able to extract a large number of new sentiment words. The polarity assignment method is also effective.	dependency grammar;lexicon;sentiment analysis;software propagation	Guang Qiu;Bing Liu;Jiajun Bu;Chun Chen	2003			natural language processing;speech recognition;computer science;chain propagation;sentiment analysis	AI	-24.439858215697683	-67.67541142584012	193802
57d9590ab8b6376a74eebfefcea88579af4fd4b1	word co-occurrence augmented topic model in short text		Topic models learn topics base on the amount of the word co-occurrence in the documents. The word co-occurrence is a degree which describes how often the two words appear together. BTM, discovers topics from bi-terms in the whole corpus to overcome the lack of local word co-occurrence information. However, BTM will make the common words be performed excessively because BTM identifies the word co-occurrence information by the bi-term The 2015 Conference on Computational Linguistics and Speech Processing ROCLING 2015, pp. 164-166  The Association for Computational Linguistics and Chinese Language Processing	computation;computational linguistics;speech processing;topic model	Guan-Bin Chen;Hung-Yu Kao	2015	IJCLCLP		natural language processing;speech recognition;information retrieval	NLP	-25.096919152671738	-68.65075289777084	194276
06b6436a6f5bece64214e91915cc0e176c90691e	authorship attribution and verification with many authors and limited data	learning methods;machine learning;feature extraction;feature selection;memory based learning;maximum entropy	Most studies in statistical or machine learning based authorship attribution focus on two or a few authors. This leads to an overestimation of the importance of the features extracted from the training data and found to be discriminating for these small sets of authors. Most studies also use sizes of training data that are unrealistic for situations in which stylometry is applied (e.g., forensics), and thereby overestimate the accuracy of their approach in these situations. A more realistic interpretation of the task is as an authorship verification problem that we approximate by pooling data from many different authors as negative examples. In this paper, we show, on the basis of a new corpus with 145 authors, what the effect is of many authors on feature selection and learning, and show robustness of a memory-based learning approach in doing authorship attribution and verification with many authors and limited training data when compared to eager learning methods such as SVMs and maximum entropy learning.	approximation algorithm;computer forensics;eager learning;experiment;feature selection;instance-based learning;machine learning;mathematical optimization;national fund for scientific research;stylometry;test case;text corpus	Kim Luyckx;Walter Daelemans	2008		10.3115/1599081.1599146	semi-supervised learning;natural language processing;feature extraction;computer science;principle of maximum entropy;machine learning;pattern recognition;data mining;feature selection;active learning	AI	-20.97918078737559	-70.37346716632223	194550
04ff60fe1940da8d78063e1cdd4c94a3f0752284	twitter trends detection by identifying grammatical relations		The problem considered in this paper relates to identification of trends in a given area based on analysis of Twitter messages. The approaches currently used for Twitter trends detection are based on n-grams. We propose another approach of trend detection based on identifying trend as grammatical relation and perform the identification of trending relations on the basis of their frequency change dynamics. This paper describes our method, which evaluates grammatical relations in a flow of messages on a particular subject taking into consideration both their frequency and semantic similarity among the pairs of relations. We conducted experiments to compare the outcomes provided by our method with the trends detected by conventional Twitter algorithms. The results confirmed the effectiveness of our method. The trends identified from the application of our method are easier for human interpretation.	algorithm;experiment;grams;n-gram;semantic similarity	Mikhail Aleksandrovich Dykov;Pavel Nikolaevich Vorobkalov	2013			natural language processing;semantic similarity;artificial intelligence;computer science;data mining;grammatical relation	NLP	-23.25473767790323	-67.04657846474488	194895
42e08bac75c1ad1a6a812f7b0e33004601423148	mixture of topic-based distributional semantic and affective models		Typically, Distributional Semantic Models (DSMs) estimate semantic similarity between words using a single-model, where the multiple senses of polysemous words are conflated in a single representation. Similarly, in textual affective analysis tasks, ambiguous words are usually not treated differently when estimating word affective scores. In this work, a semantic mixture model is proposed enabling the combination of word similarity scores estimated across multiple topic-specific DSMs (TDSMs). Based on the assumption that semantic similarity implies affective similarity, we extend this model to perform sentence-level affect estimation. The proposed model outperforms the baseline approach achieving state-of-the-art results for semantic similarity estimation and sentence-level polarity detection.	baseline (configuration management);computation;experiment;mixture model;semantic similarity;text corpus;topic model;universality probability;word embedding	Fenia Christopoulou;Eleftheria Briakou;Elias Iosif;Alexandros Potamianos	2018	2018 IEEE 12th International Conference on Semantic Computing (ICSC)	10.1109/ICSC.2018.00036	semantic similarity;mixture model;task analysis;semantics;feature extraction;affect (psychology);mathematics;context model;pattern recognition;artificial intelligence	NLP	-19.39099344230751	-70.00182212554324	195070
3c9b142b2d7f8e4cb38a1292d980166ebb09dfe1	usfd's phrase-level quality estimation systems		We describe the submissions of the University of Sheffield (USFD) for the phraselevel Quality Estimation (QE) shared task of WMT16. We test two different approaches for phrase-level QE: (i) we enrich the provided set of baseline features with information about the context of the phrases, and (ii) we exploit predictions at other granularity levels (word and sentence). These approaches perform closely in terms of multiplication of F1-scores (primary evaluation metric), but are considerably different in terms of the F1scores for individual classes.	baseline (configuration management);quadratic equation	Varvara Logacheva;Frédéric Blain;Lucia Specia	2016			granularity;phrase;multiplication;artificial intelligence;exploit;sentence;pattern recognition;computer science	NLP	-21.767575372964465	-72.75156967481723	195103
538269f83316920dcbc115a3a870fab2f75a992a	mobile phone name extraction from internet forums: a semi-supervised approach	journal article;mobile phone;internet forum;name recognition and normalization	Collecting users’ feedback on products from Internet forums is challenging because users often mention a product with informal abbreviations or nicknames. In this paper, we propose a method named Gren to recognize and normalize mobile phone names from domain-specific Internet forums. Instead of directly recognizing phone names from sentences as in most named entity recognition tasks, we propose an approach to generating candidate names as the first step. The candidate names capture short forms, spelling variations, and nicknames of products, but are not noise free. To predict whether a candidate name mention in a sentence indeed refers to a specific phone model, a Conditional Random Field (CRF)-based name recognizer is developed. The CRF model is trained by using a large set of sentences obtained in a semi-automatic manner with minimal manual labeling effort. Lastly, a rule-based name normalization component maps a recognized name to its formal form. Evaluated on more than 4000 manually labeled sentences with about 1000 phone name mentions, Gren outperforms all baseline methods. Specifically, it achieves precision and recall of 0.918 and 0.875 respectively, with the best feature setting. We also provide detailed analysis of the intermediate results obtained by each of the three components in Gren.	baseline (configuration management);conditional random field;database normalization;entity framework;experiment;focal (programming language);feedback;finite-state machine;logic programming;moe;map;mobile phone;named entity;precision and recall;semi-supervised learning;semiconductor industry;tier 2 network;user-generated content;weatherstar	Yangjie Yao;Aixin Sun	2015	World Wide Web	10.1007/s11280-015-0361-1	speech recognition;computer science;data mining;database;world wide web	Web+IR	-23.94902759832729	-71.11823859031763	195182
d006291d5ed9009e66c42f49c44ac34c760c35b1	proceedings of the 17th dutch-belgian information retrieval workshop		In the medical domain, user-generated social media text is increasingly used as a valuable complementary knowledge source to scientific medical literature: it contains the unprompted experiences of the patient. Yet, lexical normalization of such data has not been addressed properly. This paper presents a sequential, unsupervised pipeline for automatic lexical normalization of domain-specific abbreviations and spelling mistakes. This pipeline led to an absolute reduction of out-of-vocabulary terms of 0.82% and 0.78% in two cancer-related forums. Our approach mainly targeted, and thus corrected, medical concepts. Consequently, our pipeline may significantly improve downstream IR tasks.		Alex Brandsen;Anne Dirkson;Wessel Kraaij;Wout H. Lamers;Suzan Verberne;Hugo De Vos;Gineke Wiggers	2018	CoRR			NLP	-24.440195183334914	-69.77534789296487	195650
2d9855ba2ee2d911c3be9792dc60c8dc1ae60198	utilizing the one-sense-per-discourse constraint for fully unsupervised word sense induction and disambiguation		Recent advances in word sense induction rely on clustering related words. In this paper, instead of using a clustering algorithm, we suggest to perform a Singular Value Decomposition (SVD) which can be guaranteed to always find a global optimum. However, in order to apply this method to the problem of word sense induction, a semantic interpretation of the dimensions computed by the SVD is required. Our finding is that in our specific setting the first dimension relates to semantic similarities between words, and the second dimension distinguishes between the two main senses of an ambiguous word. Based on this result we present an algorithm for fully unsupervised word sense induction and disambiguation.	algorithm;cluster analysis;global optimization;semantic interpretation;singular value decomposition;word sense;word-sense disambiguation;word-sense induction	Reinhard Rapp	2004			natural language processing;artificial intelligence;semeval;semantic interpretation;cluster analysis;word-sense induction;global optimum;singular value decomposition;pattern recognition;computer science	AI	-24.643259191614597	-68.22226461547174	195667
c04bae9e44cf98019ed47b5d93cef115ea9b5838	icl00 at the ntcir-12 stc task: semantic-based retrieval method of short texts		We take part in the short text conversation task at NTCIR-12. We employ a semantic-based retrieval method to tackle this problem, by calculating textual similarity between posts and comments. Our method applies a rich-feature model to match post-comment pairs, by using semantic, grammar, n-gram and string features to extract high-level semantic meanings of text.	feature model;high- and low-level;n-gram	Weikang Li;Yixiu Wang;Yunfang Wu	2016			information retrieval;computer science	NLP	-26.081196930803	-66.30256400904005	195750
19ecf983e32e8674d594015f15dd0c72a4885a50	generalizing over lexical features: selectional preferences for semantic role classification	lexical sparseness;lexical feature;positive effect;semantic role;full system;best result;novel second-order distributional similarity;selectional preference;semantic role classification;large dataset;feature selection	This paper explores methods to alleviate the effect of lexical sparseness in the classification of verbal arguments. We show how automatically generated selectional preferences are able to generalize and perform better than lexical features in a large dataset for semantic role classification. The best results are obtained with a novel second-order distributional similarity measure, and the positive effect is specially relevant for out-of-domain data. Our findings suggest that selectional preferences have potential for improving a full system for Semantic Role Labeling.	neural coding;semantic role labeling;semantic similarity;similarity measure	Beñat Zapirain;Eneko Agirre;Lluís Màrquez i Villodre	2009			natural language processing;computer science;pattern recognition;information retrieval	NLP	-24.596818206133428	-71.73555450926187	196098
552430567941100b1700059fa6ad2c8c9d4c3fa9	verse: event and relation extraction in the bionlp 2016 shared task		We present the Vancouver Event and Relation System for Extraction (VERSE)1 as a competing system for three subtasks of the BioNLP Shared Task 2016. VERSE performs full event extraction including entity, relation and modification extraction using a feature-based approach. It achieved the highest F1-score in the Bacteria Biotope (BB3) event subtask and the third highest F1-score in the Seed Development (SeeDev) binary subtask.	biomedical text mining;f1 score;feature selection;mathematical optimization;overfitting;relationship extraction;sparse matrix;statistical classification;stochastic optimization	Jake Lever;Steven J. Jones	2016		10.18653/v1/W16-3005	natural language processing;parallel computing;communication	NLP	-22.009568778186637	-70.09425767806493	196307
aa8e33d7c4540ca5d16778f65c6bd9d2df25eb08	probabilistic topic and syntax modeling with part-of-speech lda		This article presents a probabilistic generative model for text based on semantic topics and syntactic classes called Part-of-Speech LDA (POSLDA). POSLDA simultaneously uncovers short-range syntactic patterns (syntax) and long-range semantic patterns (topics) that exist in document collections. This results in word distributions that are specific to both topics (sports, education, ...) and parts-of-speech (nouns, verbs, ...). For example, multinomial distributions over words are uncovered that can be understood as “nouns about weather” or “verbs about law”. We describe the model and an approximate inference algorithm and then demonstrate the quality of the learned topics both qualitatively and quantitatively. Then, we discuss an NLP application where the output of POSLDA can lead to strong improvements in quality: unsupervised partof-speech tagging. We describe algorithms for this task that make use of POSLDA-learned distributions that result in improved performance beyond the state of the art.	approximation algorithm;automatic summarization;coherence (physics);generative model;hidden markov model;language model;multinomial logistic regression;natural language generation;natural language processing;part-of-speech tagging;perplexity;point of sale;text segmentation;text-based (computing);unsupervised learning	William M. Darling;Fei Song	2013	CoRR		natural language processing;speech recognition;computer science;artificial intelligence;machine learning;pattern recognition;linguistics	NLP	-19.129666123204352	-68.20270901565384	196467
997bfd6c322909ac66fae417c553f104a6dcf18b	buap: evaluating compositional distributional semantic models on full sentences through semantic relatedness and textual entailment		The results obtained by the BUAP team at Task 1 of SemEval 2014 are presented in this paper. The run submitted is a supervised version based on two classification models: 1) We used logistic regression for determining the semantic relatedness between a pair of sentences, and 2) We employed support vector machines for identifying textual entailment degree between the two sentences. The behaviour for the second subtask (textual entailment) obtained much better performance than the one evaluated at the first subtask (relatedness), ranking our approach in the 7th position of 18 teams that participated at the competition.	logistic regression;semeval;semantic similarity;support vector machine;textual entailment	Saúl León;Darnes Vilariño Ayala;David Pinto;Mireya Tovar;Beatríz Beltrán	2014		10.3115/v1/S14-2021	natural language processing;computer science;pattern recognition;linguistics	NLP	-22.309263767252645	-69.8275830078076	196610
4e4c29f1638c63e294f9e29a20602d58e0c8e212	rank learning for factoid question answering with linguistic and semantic constraints	annotation graphs;committee perceptron;semantic role labeling;passage retrieval;text annotations;retrieval model;learning to rank;mean average precision;named entity;question answering	This work presents a general rank-learning framework for passage ranking within Question Answering (QA) systems using linguistic and semantic features. The framework enables query-time checking of complex linguistic and semantic constraints over keywords. Constraints are composed of a mixture of keyword and named entity features, as well as features derived from semantic role labeling. The framework supports the checking of constraints of arbitrary length relating any number of keywords. We show that a trained ranking model using this rich feature set achieves greater than a 20% improvement in Mean Average Precision over baseline keyword retrieval models. We also show that constraints based on semantic role labeling features are particularly effective for passage retrieval; when they can be leveraged, an 40% improvement in MAP over the baseline can be realized.	baseline (configuration management);information retrieval;mean squared error;named entity;question answering;semantic role labeling	Matthew W. Bilotti;Jonathan L. Elsas;Jaime G. Carbonell;Eric Nyberg	2010		10.1145/1871437.1871498	natural language processing;semantic role labeling;question answering;computer science;machine learning;pattern recognition;data mining;database;information retrieval;learning to rank	NLP	-22.606177277836537	-72.51201344918614	197292
c1f4d30216eff5fc8fb0a0442c3bd2c5e4d1fb61	literal or idiomatic? identifying the reading of single occurrences of german multiword expressions using word embeddings		Non-compositional multiword expressions (MWEs) still pose serious issues for a variety of natural language processing tasks and their ubiquity makes it impossible to get around methods which automatically identify these kind of MWEs. The method presented in this paper was inspired by Sporleder and Li (2009) and is able to discriminate between the literal and non-literal use of an MWE in an unsupervised way. It is based on the assumption that words in a text form cohesive units. If the cohesion of these units is weakened by an expression, it is classified as literal, and otherwise as idiomatic. While Sporleder an Li used Normalized Google Distance to model semantic similarity, the present work examines the use of a variety of different word embeddings.	experiment;jason;lexicon;literal (mathematical logic);minimal working example;natural language processing;normalized google distance;programming idiom;semantic similarity;unsupervised learning;word embedding	Rafael Ehren	2017			artificial intelligence;natural language processing;computer science;linguistics;expression (mathematics);german	NLP	-25.558440972980314	-72.60525638537406	198286
911e40ed7784d0b6b1f0cf0ace58c850c8e861e5	tuning context features with genetic algorithms	genetic algorithm	In this paper we present an approach to tuning of context features acquired from corpora. The approach is based on the idea of a genetic algorithm (GA). We analyse a whole population of contexts surrounding related linguistic entities in order to find a generic property characteristic of such contexts. Our goal is to tune the context properties so as not to lose any correct feature values, but also to minimise the presence of ambiguous values. The GA implements a crossover operator based on dominant and recessive genes, where a gene corresponds to a context feature. A dominant gene is the one that, when combined with another gene of the same type, is inevitably reflected in the offspring. Dominant genes denote the more suitable context features. In each iteration of the GA, the number of individuals in the population is halved, finally resulting in a single individual that contains context features tuned with respect to the information contained in the training corpus. We illustrate the general method by using a case study concerned with the identification of relationships between verbs and terms complementing them. More precisely, we tune the classes of terms that are typically selected as arguments for the considered verbs in order to acquire their semantic features. * This research is a part of the BioPATH research project coordinated by LION BioScience (http://www.lionbioscience.com) and funded by German Ministry of Research	entity;genetic algorithm;iteration;software release life cycle;text corpus;the offspring	Irena Spasić;Goran Nenadic;Sophia Ananiadou	2002			genetic algorithm;natural language processing;artificial intelligence;meta-optimization;population-based incremental learning;crossover;genetic representation;cultural algorithm;computer science;population;quality control and genetic algorithms	NLP	-25.73832600248378	-71.73358455806394	198321
1cc67894c702ca2405b65f972c80742bcbbf9b1d	automated document indexing via intelligent hierarchical clustering: a novel approach	hierarchical clustering;text analysis indexing pattern clustering statistical analysis;f measure automated document indexing textual data electronic format document organization framework intelligent hierarchical clustering algorithm hierarchy attributes statistical methods pre processing stage 20 newsgroups dataset;topic modeling;latent dirichlet allocation;indexing;latent semantic indexing;topic modeling hierarchical clustering indexing latent dirichlet allocation latent semantic indexing	With the rising quantity of textual data available in electronic format, the need to organize it become a highly challenging task. In the present paper, we explore a document organization framework that exploits an intelligent hierarchical clustering algorithm to generate an index over a set of documents. The framework has been designed to be scalable and accurate even with large corpora. The advantage of the proposed algorithm lies in the need for minimal inputs, with much of the hierarchy attributes being decided in an automated manner using statistical methods. The use of topic modeling in a pre-processing stage ensures robustness to a range of variations in the input data. For experimental work 20-Newsgroups dataset has been used. The F-measure of the proposed approach has been compared with the traditional K-Means and K-Medoids clustering algorithms. Test results demonstrate the applicability, efficiency and effectiveness of our proposed approach. After extensive experimentation, we conclude that the framework shows promise for further research and specialized commercial applications.	algorithm;categorization;cluster analysis;hierarchical clustering;k-means clustering;k-medoids;library (computing);medoid;preprocessor;python;scalability;text corpus;topic model	Rajendra Kumar Roul;Shubham Rohan Asthana;Sanjay Kumar Sahay	2014	2014 International Conference on High Performance Computing and Applications (ICHPCA)	10.1109/ICHPCA.2014.7045347	latent dirichlet allocation;correlation clustering;search engine indexing;latent semantic indexing;document clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;hierarchical clustering;topic model;cluster analysis;brown clustering;information retrieval;hierarchical clustering of networks	DB	-25.7080325270417	-66.15132353238627	198437
05b25fd71b69714594d68c093f00fbc1b189fae4	novel unsupervised features for czech multi-label document classification		This paper deals with automatic multi-label document classification in the context of a real application for the Czech News Agency. The main goal of this work consists in proposing novel fully unsupervised features based on an unsupervised stemmer, Latent Dirichlet Allocation and semantic spaces (HAL and COALS). The proposed features are integrated into the document classification task. Another interesting contribution is that these two semantic spaces have never been used in the context of document classification before. The proposed approaches are evaluated on a Czech newspaper corpus. We experimentally show that almost all proposed features significantly improve the document classification score. The corpus is freely available for research purposes.	baseline (configuration management);document classification;experiment;hal;latent dirichlet allocation;multi-label classification;text corpus;ultra-wideband;unsupervised learning	Tomas Brychcin;Pavel Král	2014		10.1007/978-3-319-13647-9_8	latent dirichlet allocation;principle of maximum entropy;pattern recognition;artificial intelligence;czech;computer science;document classification	NLP	-23.926453664390035	-69.35846496978293	198868
20073d0b19dad48fc1c195cd28076c06ad9848f4	ubc: cubes for english semantic textual similarity and supervised approaches for interpretable sts		In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment.	cubes;data structure;semantic similarity;supervised learning;turing completeness	Eneko Agirre;Aitor Gonzalez-Agirre;Iñigo Lopez-Gazpio;Montse Maritxalar;German Rigau;Larraitz Uria	2015			natural language processing;semantic similarity;speech recognition;computer science;machine learning;data mining;information retrieval	NLP	-22.356887713797843	-72.49324555633476	199830
