id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e3c9de5b526bbdf0e5b09baba1106dda474f035a	on checkpoint overhead in distributed systems providing session guarantees	distributed system;performance evaluation;checkpointing;consistency model;mobile environment;distributed mobile systems;rollback recovery;session guarantees;mobile systems	This paper presents the performance evaluation of the checkpointing and rollback-recovery protocols for distributed mobile systems, guarantying client-centric consistency models, despite failures of servers. The performance of considered protocols is evaluated by estimating the checkpointing overhead. Additionally, the paper disscusses the influence of the consistency models provided by the system on the moments of taking checkpoints. The impact of the obtained results on strategies for checkpointing in the mobile environment guarantying client-centric consistency models is assessed.	distributed computing;overhead (computing);transaction processing system	Arkadiusz Danilecki;Anna Kobusinska;Marek Libuda	2007		10.1007/978-3-540-68111-3_2	parallel computing;real-time computing;computer science;consistency model;distributed computing	OS	-20.7157776264567	48.15648016811061	178592
36d9ce3ee250b0805f8babfa7aeba3cc29654cb4	on using similarity for concurrency control in real-time database systems	phase locking;database system;real time;satisfiability;real time database systems;high priority;concurrency control;similarity;applications;time constraint	Most of the proposed concurrency control protocols for real-time database systems (RTDBS) are based on serializability theorem. Owing to the unique characteristics of real-time database applications and the importance of satisfying the timing constraint of the transactions, serializable concurrency control protocols are not suitable for RTDBS for most cases. In this paper, another notion of correctness, similarity, is used for concurrency control in a RTDBS, for instance, a stock trading database system. Similarity is a less restrictive notion comparing with serializability. By studying the correctness requirements of the stock trading database applications, a real-time two phase locking protocol, High Priority 2 Phase Locking (H2PL) is re-defined based on similarity. Although the new protocol cannot ensure serializability, the concurrency of the system is higher and the amount of inconsistency in the database is tolerable. On the other hand, the performance of the whole system can be much improved.	concurrency control;correctness (computer science);database transaction;global serializability;lock (computer science);real-time clock;real-time locating system;real-time operating system;real-time transcription;requirement;simulation;two-phase locking	Kam-yiu Lam;Wai-cheong Yau	1998	Journal of Systems and Software	10.1016/S0164-1212(98)10035-3	global serializability;timestamp-based concurrency control;optimistic concurrency control;real-time computing;isolation;similarity;commitment ordering;rollback;database tuning;computer science;two-phase locking;concurrency control;database;distributed computing;multiversion concurrency control;non-lock concurrency control;serializability;information technology;satisfiability;schedule;distributed concurrency control	DB	-23.68596416678503	48.02776849680576	178674
00e2a25b93a59d28b3eedb8808c5b9e4b0f1ffd7	okeanos: wasteless journaling for fast and reliable multistream storage	storage stack;synchronous concurrent small writes;subpage writes;storage bandwidth requirement;selective journaling;reliable multistream storage;data writes;wasteless journaling;synchronous small writes;storage bandwidth waste;data transfer	Synchronous small writes play a critical role in the reliability and availability of file systems and applications that use them to safely log recent state modifications and quickly recover from failures. However, storage stacks usually enforce page-sized granularity in their data transfers from memory to disk. We experimentally show that subpage writes may lead to storage bandwidth waste and high disk latencies. To address the issue in a journaled file system, we propose wasteless journaling as a mount mode that coalesces synchronous concurrent small writes of data into full page-sized blocks before transferring them to the journal. Additionally, we propose selective journaling that automatically applies wasteless journaling on data writes whose size lies below a fixed preconfigured threshold. In the Okeanos prototype implementation that we developed, we use microbenchmarks and application-level workloads to show substantial improvements in write latency, transaction throughput and storage bandwidth requirements.	experiment;prototype;reliability engineering;requirement;solid-state drive;subpage;throughput	Stergios V. Anastasiadis	2011			parallel computing;real-time computing;computer hardware;computer science;operating system;journaling file system	OS	-19.885556678836508	50.36989341322162	179442
250a6505453fa5821ffeea1831e36dffd62e019c	gmu: genuine multiversion update-serializable partial data replication	protocols proposals scalability semantics clocks history benchmark testing;multiversioning;protocols;partial data replication;history;clocks;semantics;non serializable semantics gmu genuine multiversion update serializable partial data replication genuine partial replication protocol transactional systems distributed multiversioning scheme multiversion based solutions global logical clock system scalability distributed validation schemes read intensive workloads extended update serializability isolation level eus isolation level consistency criterion consistency models attractive consistency model ordinary programmers in memory transactional data grid infinispan heterogeneous platforms industry standard benchmarks linear scalability;fault tolerance;transactional systems;fault tolerance partial data replication multiversioning transactional systems;scalability;data handling;proposals;benchmark testing	In this article we introduce GMU, a genuine partial replication protocol for transactional systems, which exploits an innovative, highly scalable, distributed multiversioning scheme. Unlike existing multiversion-based solutions, GMU does not rely on any global logical clock, which may represent a contention point and a major impairment to system scalability. Also, GMU never aborts read-only transactions and spares them from undergoing distributed validation schemes. This makes GMU particularly efficient in presence of read-intensive workloads, as typical of a wide range of real-world applications. GMU guarantees the Extended Update Serializability (EUS) isolation level. This consistency criterion is particularly attractive as it is sufficiently strong to ensure correctness even for very demanding applications (such as TPC-C), but is also weak enough to allow efficient and scalable implementations, such as GMU. Further, unlike several relaxed consistency models proposed in literature, EUS shows simple and intuitive semantics, thus being an attractive consistency model for ordinary programmers. We integrated GMU in a popular open source in-memory transactional data grid, namely Infinispan. On the basis of a wide experimental study performed on heterogeneous platforms and using industry standard benchmarks (namely TPC-C and YCSB), we show that GMU achieves almost linear scalability and that it introduces reduced overhead, with respect to solutions ensuring non-serializable semantics, in a wide range of workloads.	algorithm;attribute–value pair;causal filter;concurrency (computer science);consistency model;correctness (computer science);dynamic data;experiment;ibm tivoli storage productivity center;in-memory database;infinispan;isolation (database systems);key-value database;logical clock;multiversion concurrency control;online transaction processing;open-source software;overhead (computing);programmer;read-only memory;replication (computing);scalability;serializability;tpc-w;technical standard;usb flash drive;vector clock;ycsb	Sebastiano Peluso;Pedro Ruivo;Paolo Romano;Francesco Quaglia;Luís E. T. Rodrigues	2016	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2015.2510998	communications protocol;benchmark;fault tolerance;parallel computing;real-time computing;scalability;computer science;operating system;group method of data handling;database;distributed computing;semantics	DB	-21.682807992299995	49.0773279413117	179513
9a18e590cc5ebac6b274c62c1e1e3900d7d41341	an enhanced distributed database design over the cloud environment		The design of a distributed database is one of the major research issues within the distributed database system area. The main challenges facing distributed database systems (DDBS) design are fragmentation, allocation and replication. In this paper, we present an enhanced distributed database design over the cloud environment. It minimizes the execution time needed for the transactions and for noticing an error when an invalid query or data cannot found occurs. It also allows users to access the distributed database from anywhere. Moreover, it allows fragmentation, allocation and replication decisions to be taken statically at the initial stage of designing the distributed database. Experimental results show that the proposed system design, results in a significant reduction of the execution time needed for the transactions to reach the data in an appropriate site and the time taken to notice an error when an invalid query or data not found occurs.	database design;distributed database	Ahmed E. Abdel Raouf;Nagwa Lotfy Badr;Mohamed F. Tolba	2016		10.1007/978-3-319-48308-5_28	database;distributed computing;world wide web;distributed database	DB	-21.396231903435183	49.82344908085657	179907
199e1ad2b34a3e2269e872e79260a45d4ba38f98	a multiprogramming, virtual memory system for a small computer	virtual memory;real time processing;batch process;communication cost	The specific objective of this small computer system is to interface six to eight small graphical terminals to a large batch-processing computer. The small computer provides the graphical terminals with real-time processing for generating, editing and manipulating graphical or text files. The small computer passes along to the large computer requests for large tasks. Access to the data base in the large computer is provided. Another aspect of this objective is remote concentration. The terminals are connected to the small computer directly or through several DATA-PHONE® 103 data sets. The small computer is connected to the large computer through a single DATA-PHONE® 201 data set. This configuration reduces communication costs for a group of terminals located remotely from the large computation center.	batch processing;compatible time-sharing system;computation;computer multitasking;database;graphical user interface;openvms;real-time clock	C. Christensen;A. D. Hause	1970		10.1145/1476936.1477039	computer hardware;computer science;theoretical computer science;distributed computing;computer network programming	Graphics	-20.507315428146534	51.81250571290609	180040
0844628d3c003acb1710cede1edc531acc7f72b1	optimistic algorithms for partial database replication	distributed system;partial database replication;replication;tratamiento transaccion;partial replication;systeme reparti;duplication partielle;certification;database replication;database;base dato;state machine;replicacion;sistema repartido;certificacion;base de donnees;transaction processing;duplicacion parcial;traitement transaction	In this paper, we study the problem of partial database replication. Numerous previous works have investigated database replication, however, most of them focus on full replication. We are here interested in genuine partial replication protocols, which require replicas to permanently store only information about data items they replicate. We define two properties to characterize partial replication. The first one, QuasiGenuine Partial Replication, captures the above idea; the second one, Non-Trivial Certification, rules out solutions that would abort transactions unnecessarily in an attempt to ensure the first property. We also present two algorithms that extend the Database State Machine [8] to partial replication and guarantee the two aforementioned properties. Our algorithms compare favorably to existing solutions both in terms of number of messages and communication steps.	algorithm;replication (computing);self-replicating machine	Nicolas Schiper;Rodrigo Schmidt;Fernando Pedone	2006		10.1007/11945529_7	replication;transaction processing;computer science;database;distributed computing;finite-state machine;certification;algorithm;replication	DB	-23.545661167434105	46.91176081712185	181339
49bc2e4e9fc7e04bb4081497a556e72e05d4707c	on optimal scheduling of integrity assertions in a transaction processing system	semantic integration;optimal scheduling;transaction processing;database management system	Semantic integrity of a database is guarded by a set of integrity assertions expressed as predicates on database values. The problem of efficient evaluation of integrity assertions in transaction processing systems is considered. Three methods of validation (compile-time, run-time, and post-execution validations) are analyzed in terms of database access costs. The results show that if transactions are executed independently of each other, the cost of compile-time validation is never higher than the cost of run-time validation; in turn the cost of the latter is never higher than the cost of post-execution validation.	compile time;compiler;predicate (mathematical logic);scheduling (computing);semantic web;transaction processing system	Bharat K. Bhargava;Leszek Lilien	1981	International Journal of Computer & Information Sciences	10.1007/BF00993150	cost database;parallel computing;semantic integration;database transaction;transaction processing;rollback;distributed transaction;computer science;data mining;database;online transaction processing;transaction processing system	DB	-22.8670288213274	47.98248324380574	182924
304b57796186bb3219fff4b5d0f7827c6ed7dfc4	portable and scalable mpi shared file pointers	shared location;file system;shared file pointer;scalable mpi;native shared file pointer;file pointer support;file lock;o capability;file pointer;file content;separate file	While the I/O functions described in the MPI standard included shared file pointer support from the beginning, the performance and portability of these functions have been subpar at best. ROMIO [1], which provides the MPI-IO functionality for most MPI libraries, to this day uses a separate file to manage the shared file pointer. This file provides the shared location that holds the current value of the shared file pointer. Unfortunately, each access to the shared file pointer involves file lock management and updates to the file contents. Furthermore, support for shared file pointers is not universally available because few file systems support native shared file pointers [5] and a few file systems do not support file locks [3]. Application developers rarely use shared file pointers, even though many applications can benefit from this file I/O capability. These applications are typically loosely coupled and rarely exhibit application-wide synchronization. Examples include application tracing toolkits [8,4] and many-task computing applications [10]. Other approaches to the shared file pointer I/O models frequently used by these application classes include file-per-process, file-per-thread, and file-perrank approaches. While these approaches work relatively well at smaller scales, they fail to scale to leadership-class computing systems because of the intense metadata loads generated they generate. Recent research identified significant improvements from using shared-file I/O instead of multifile I/O patterns on leadership-class systems [6]. In this paper, we propose integrating shared file support into the I/O forwarding layer commonly found on leadership-class computing systems. I/O forwarding middleware, such as the I/O Forwarding Scalability Layer (IOFSL) [9,2], bridges the compute and I/O subsystems of leadership-class computing systems. This middleware layer captures all file I/O requests generated by applications executing on compute nodes and forwards them to dedicated I/O nodes. These I/O nodes, a common hardware feature of leadership-class computing systems, execute the I/O requests on behalf of the application. The I/O forwarding layer on these system is best suited to provide and manage shared file pointers because it has access to all application I/O requests and can provide enhanced file I/O capabilities independent of the system and I/O software stack. By embedding this capability into the I/O forwarding layer, applications developers can utilize shared file pointers for a variety of file I/O APIs (MPI-IO, POSIX, and ZOIDFS), synchronization levels (collective and independent I/O), and computing systems (IBM Blue Gene and Cray XT systems). We are adding several features to IOFSL and ROMIO to enable portable MPI-IO shared file pointer access. In prior work, we extended the ZOIDFS API [2] to provide a distributed atomic append capability. Our current work extends and generalizes this capability to provide shared file pointers as defined by the MPI standard. First, we created a per file shared (key,value) storage space. This capability allows users of the API to instantiate an instance of a ZOIDFS file handle and associate file state with the handle (such as the current position of a file pointer). Since a ZOIDFS file handle is a persistent, globally unique identifier linked to a specific file, this does not result in extra state for the client. To limit the amount of state stored within the I/O node and to enable recovery from faults, we are integrating purge policies for the key value store. Example policies include flushing data to other IOFSL servers or persistently storing this data in extended attribute fields of the target file. In prior work, we implemented a distributed atomic append by essentially implementing a per file, system wide shared file pointer. In our current work, we instead require a shared file pointer per MPI file handle. This is easily implemented by storing the current value of the shared file pointer in a key uniquely derived from the MPI file handle. We modified ROMIO to generate this unique key. When a file is first opened, a sufficiently large, random identifier is generated. This identifier is subsequently used to retrieve or update the current value of the shared file pointer. To avoid collisions, we rely on the fact that the key space provided by IOFSL supports an exclusive create operation. In the unlikely event that the generated identifier already exists for the file, ROMIO simply generates another one. By providing set, get, and atomic increment operations, the IOFSL server is responsible for shared file pointer synchronization. This precludes the need for explicit file lock management for shared file pointer support. Overall, few modifications to ROMIO were required. Before executing a shared read or write, ROMIO uses the key store to atomically increment and retrieve the shared file pointer. It then subsequently accesses the file using an ordinary independent I/O operation. To simplify fault tolerance, we plan to combine the I/O access and the key update into one operation. ROMIO’s MPI_File_close method removes the shared file pointer key in order to limit the amount of state held by the I/O nodes. For systems such as the Cray XT series, where I/O nodes are shared among multiple jobs, we automatically purge any keys left by applications that failed to clean up the shared file pointer, for example because of unclean application termination. On systems employing a dedicated I/O node, no cleanup is necessary, since the I/O node (and the IOFSL server) is restarted between jobs. These modifications provide a low-overhead, file-system-independent, shared file pointer implementation for MPI-IO on those systems supported by IOFSL. Unlike other solutions, our implementation does not require a progress thread or hardware-supported remote memory access functionality [7].	append;application programming interface;attribute–value pair;blue gene;fault tolerance;ibm personal computer xt;identifier;include directive;input/output;key space (cryptography);library (computing);list of toolkits;lock (computer science);loose coupling;many-task computing;message passing interface;middleware;overhead (computing);posix;pointer (computer programming);scalability;server (computing);software portability;task computing;unique key	Jason Cope;Kamil Iskra;Dries Kimpe;Robert B. Ross	2011		10.1007/978-3-642-24449-0_35	fork;self-certifying file system;parallel computing;torrent file;indexed file;memory-mapped file;device file;computer file;computer science;class implementation file;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;journaling file system;database;open;everything is a file;file system fragmentation;global namespace;file control block;virtual file system	OS	-20.884433830986985	51.17531824122344	183866
586a6f468ad9bd6d63b4caa8fe7dac183522bb8e	scalable deferred update replication	database replication;scalable data store;transactional systems database replication scalable data store fault tolerance high performance;fault tolerant computing;read only transactions scalable deferred update replication data management systems transaction execution update transactions;fault tolerance;transactional systems;data handling;high performance;fault tolerant computing data handling;servers databases protocols throughput radiation detectors computer crashes partitioning algorithms	Deferred update replication is a well-known approach to building data management systems as it provides both high availability and high performance. High availability comes from the fact that any replica can execute client transactions; the crash of one or more replicas does not interrupt the system. High performance comes from the fact that only one replica executes a transaction; the others must only apply its updates. Since replicas execute transactions concurrently, transaction execution is distributed across the system. The main drawback of deferred update replication is that update transactions scale poorly with the number of replicas, although read-only transactions scale well. This paper proposes an extension to the technique that improves the scalability of update transactions. In addition to presenting a novel protocol, we detail its implementation and provide an extensive analysis of its performance.	best, worst and average case;high availability;read-only memory;scalability;throughput;windows update	Daniele Sciascia;Fernando Pedone;Flavio Paiva Junqueira	2012	IEEE/IFIP International Conference on Dependable Systems and Networks (DSN 2012)	10.1109/DSN.2012.6263931	fault tolerance;parallel computing;real-time computing;database transaction;computer science;operating system;group method of data handling;database;distributed computing;replication	DB	-21.64029050116643	49.19326755146439	183992
9d780de6371404cbdbfccfc9776d84f4491424c0	a unified peer-to-peer database framework for scalable service and resource discovery	estensibilidad;distributed system;query language;database system;base donnee repartie;reseau pair;systeme reparti;distributed database;resource discovery;availability;disponibilidad;interrogation base donnee;distributed computing;base repartida dato;interrogacion base datos;p2p;data type;grid;peer to peer p2p;sistema repartido;rejilla;grille;calculo repartido;extensibilite;scalability;dynamic content;service discovery;peer to peer;disponibilite;calcul reparti;database query;data grid	In a large distributed system spanning many administrative domains such as a Data Grid, it is desirable to maintain and query dynamic and timely information about active participants such as services, resources and user communities. However, in such a database system, the set of information tuples in the universe is partitioned over many distributed nodes, for reasons including autonomy, scalability, availability, performance and security. It is not obvious how to enable general-purpose discovery query support and collective collaborative functionality that operate on the distributed system as a whole, rather than on a given part of it. Further, it is not obvious how to allow for search results that are fresh, allowing dynamic content. It appears that a Peer-to-Peer (P2P) database network may be well suited to support dynamic distributed database search, for example for service discovery. In this paper, we devise the Unified Peer-to-Peer Database Framework (UPDF), which allows to express specific applications for arbitrary query languages (e.g. XQuery, SQL) and node topologies, and a wide range of data types, query response modes (e.g. Routed, Direct and Referral Response), neighbor selection policies, pipelining characteristics, timeout and other scope options.	autonomy;circuit complexity;cycle detection;distributed computing;distributed database;dynamic web page;file spanning;freenet;general-purpose markup language;general-purpose modeling;gnutella;lightweight directory access protocol;lookup table;multi-purpose viewer;network topology;peer-to-peer;pipeline (computing);prototype;query language;routing;sql;scalability;scope (computer science);service discovery;tree network;xquery	Wolfgang Hoschek	2002		10.1007/3-540-36133-2_12	availability;query optimization;scalability;data type;computer science;query by example;operating system;dynamic web page;peer-to-peer;data grid;data mining;database;distributed computing;service discovery;grid;view;world wide web;distributed database;query language	DB	-26.326725451079433	48.59451090048551	184690
3bbdb1ab04c716c84d780be8263c95deaba00d09	a remote execution mechanism for distributed homogeneous stable stores	dynamic binding	Persistent languages and systems provide the ability to create and manipulate all data in a uniform manner regardless of how long it persists. Such systems are usually implemented above a stable persistent store which supports reliable long-term storage of persistent data. In this paper we consider the issue of distribution of the persistent store across nodes. A number of existing persistent languages with support for distribution are described in terms of a taxonomy of distributed stores. It is shown that there are considerable difficulties with these systems, particularly in terms of scalability. A new mechanism based on the exportation and remote execution of procedures is then described. A key feature of this mechanism is that an exported procedure may dynamically bind to data in the remote store. It is shown that the mechanism alleviates most of the problems of existing systems and provides considerable flexibility. The paper concludes with some examples of practical use of the proposed mechanism.	first-class function;inter-process communication;name binding;napier88;parametric polymorphism;persistence (computer science);scalability;server (computing);type safety;type system;warez	Alan Dearle;John Rosenberg;Francis Vaughan	1991			real-time computing;computer science;data mining;database;distributed computing;programming language;late binding	OS	-23.331659136553476	49.32400970152581	184821
f2b02fb8cdea73dea98e9c1368f843daa6054045	endurable transient inconsistency in byte-addressable persistent b+-tree			b+ tree;byte	Deukyeon Hwang;Wook-Hee Kim;Youjip Won;Beomseok Nam	2018			computer science;parallel computing;byte;b-tree	OS	-20.065744871969134	48.676038797192106	185561
fd7a45acb2748d9195a47001e1eb401fb6a10c8a	efficient algorithms for improving the performance of read operations in distributed file system	distributed system;performance;concurrency;speculation;blobseer	"""Distributed file systems (DFSs) are used in the modern cloud-based systems to store and process a large amount of data. File sharing semantics is used by the DFS for sharing the data in a consistent manner among the authorized users of the system. The limitation of popular session semantics is that read client processes cannot read the modifications done by a concurrent write client process on the same shared file in the same session. Linearizability semantics followed in the BlobSeer DFS permits the read client processes to read the previous version of a binary large object (blob) while write operation is carried out on that blob concurrently. In this paper, we have proposed a new type of semantics and metric namely """"Speculative Semantics"""" and """"Currency"""" respectively. Speculative semantics permits the read client processes to read the modifications done by the concurrent write client process in the same session. Currency is the metric used for measuring the performance of the read algorithms. In this paper, we have proposed two new read algorithms based on speculative semantics. We have conducted experiments on Blobseer DFS to measure the performance of these read algorithms and the results obtained indicate that the proposed algorithms perform better than existing read algorithm of the BlobSeer DFS."""		Talluri Lakshmi Siva Rama Krishna;Thirumalaisamy Ragunathan;Sudheer Kumar Battula	2015			speculation;parallel computing;real-time computing;concurrency;performance;computer science;operating system;database;distributed computing;programming language	HPC	-19.282896777130805	48.292918385896044	186054
c85430d4b31aa4efe9d0b873ba9e1b72162c9a3c	undo-based access control for distributed collaborative editors	performance measure;general solution;collaborative editors;access control;operational transformation;selective undo;distributed collaboration	When adding access control layer to a replication based Distributed Collaborative Editor (DCE), ensuring convergence to the same copy of the shared document becomes a challenging problem. We consider here an optimistic access control in the sense that temporarily access right violation is tolerated [3]. This leads to data divergence. To maintain convergence, updates violating access rightsmust be undone.However, undo approachmay itself lead to divergence cases called undo puzzles [6]. In this paper, we address undo as themain feature in an optimistic access-control-basedDCE.We also showhowwe can avoid several known undo puzzles and present additional ones. We propose a new generic solution for these puzzles and provide performance measurements of our undo command.	access control;undo	Asma Cherif;Abdessamad Imine	2009		10.1007/978-3-642-04265-2_14	real-time computing;computer science;access control;data mining;database	Crypto	-25.2821552813489	47.29764867585205	186358
e0d5f16a98e91976319c5016c22b526075b33386	distributed storage of network measurement data on hbase	storage management;distributed data storage network measurement cloud computing hadoop hbase;storage management cloud computing parallel processing;memory distributed databases file systems cloud computing servers probes scalability;distributed storage scheme performance demands network measurement data storage hbase open source cloud computing framework hadoop framework hbase computing resource storage capacity relational databases network measurement systems;parallel processing;cloud computing	As traditional network measurement systems store data in relational databases, the storage capacity and computing resource are limited, besides, the extendibility and elasticity are low. The centralized storage scheme will not meet the needs of big data analysis as the data is growing rapidly and continuously. In this paper, we adopt Hadoop and HBase-open source Cloud Computing Frameworks to store network measurement data of non-realtime and realtime applications. The distributed storage scheme is designed to meet the performance demands of storage capacity and computing resource for the mass data.	apache hbase;apache hadoop;big data;centralized computing;cloud computing;clustered file system;elasticity (data store);open-source software;real-time computing;relational database;system of measurement	Haijie Ding;Yuehui Jin;Yidong Cui;Tan Yang	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664268	parallel processing;storage area network;converged storage;cloud computing;computer science;operating system;data-intensive computing;database;distributed computing;utility computing;information repository	HPC	-20.04147747979561	53.19401429759933	187045
82829cb09263ff9594425ba90c44f6bfab6ed678	management of composite event for active database rule scheduling	database system;priority graph generation;active database rule scheduling;composite event;active database;database integrity checking;data management;construction industry;confluence;database integration;data mining;scheduling algorithm;active rules;event management;database systems;data access;scheduling algorithm algorithm design and analysis database systems processor scheduling project management computer science usa councils;schedules;priority graph generation composite event active database rule scheduling event management data management database integrity checking database integration;active databases;algorithm design and analysis;composite events active rules confluence;composite events	Active database rules provide event management capability to database systems by signaling events and handling events automatically. Active rules play important roles in data management such as database integrity checking and database integration. Our past research has reported an active rule scheduling algorithm, named IRS, to schedule the execution of concurrently triggered rules to achieve the confluence property. The confluence property allows rule execution to produce the same final result regardless of the execution order of simultaneously triggered rules. The IRS algorithm schedules rules at static time with rules triggered by primitive events. This paper describes our research on extending the IRS algorithm, named CIRS algorithm, to incorporate composite events. We define a new triggering graph to represent composite events, and convert the new graph to apply the data access sub-algorithm and priority graph generation sub-algorithm. Using the CIRS algorithm, rules triggered by composite events can be scheduled at static time that guarantees the confluent execution of simultaneously triggered rules.	active database;algorithm;confluence;data access;data integrity;event condition action;heterogeneous database system;scheduling (computing)	Ying Jin	2009	2009 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2009.5211569	data access;algorithm design;real-time computing;schedule;data management;computer science;data integration;data mining;database;scheduling;world wide web;confluence	DB	-24.626569312592714	48.87979110911821	187309
75e2488d7feb47b796a6edd044655489f03f5f6c	a bare pc mass storage usb driver		Today’s device drivers are dependent on a given operating system, kernel, or an embedded system platform that provides a higher level of abstraction for its use. We present a USB mass storage device driver that does not depend on any operating system, or kernel, or embedded application. The device driver runs on a bare PC and directly interfaces with an application written in C/C++. The application programmer controls and manages all driver facilities. We describe a step-by-step approach to design the driver and provide code snippets of its key elements that can be used to build a device driver that runs on a bare PC, or a similar computing device. The bare PC API can be used to build a file management system that is independent of standard file system specifications. A bare device driver inherently provides simplicity, total control, flexibility, portability, platform independence, and better performance compared to a standard device driver. The performance measurements for read and write operations show a performance gain over a Linux system with significant improvements in stress tests. The experiments suggest that large transfers are optimized for groups consisting of a particular number of sectors. Bare device drivers can be used for building bare applications or as a foundation for building other types of USB drivers that can run directly on a variety of devices with no system support.	application programming interface;c++;central processing unit;computer;device driver;disk sector;embedded system;experiment;kernel (operating system);linux;operating system;programmer;software portability;stress testing (software);usb mass storage device class;x86	Ramesh K. Karne;Songjie Liang;Alexander L. Wijesinha;Patrick Appiah-Kubi	2013	I. J. Comput. Appl.		usb;mass storage;computer hardware;computer science	OS	-26.346963002305788	50.782556975318826	187615
58dc47c1a061d5f6c55ec4b4944bc9eb7feddbcd	addrol: a method for on-line reorganization in a multisystem dbms with shared disks	shared disks addrol algorithm for distributed database reorganization on line on line reorganization biconnected components graph edge insertion edge deletion multisystem dbms;clustering algorithms spatial databases distributed computing distributed databases switches monitoring application software design methodology algorithm design and analysis history;distributed database;statistical profile of access patterns;dynamic clustering;performance analysis;distributed databases;buffering;on line reorganization;object database systems	This paper describes the design of a method, called ADDROL Algorithm for Distributed Database Reorganization On-Line, which supports on-line reorganization of the distributed database. The contributions of this paper are both of theoretical and of experimental nature. We present an algorithm for maintaining the biconnected components of a graph during a sequence of edge insertions and deletions.	algorithm;biconnected component;biconnected graph;clustered file system;computer data storage;distributed database;division by zero;online and offline	Vlad Ingar Wietrzyk;Mehmet A. Orgun	2000		10.1109/PADSW.2000.884652	parallel computing;computer science;data buffer;theoretical computer science;database;distributed computing;distributed database	DB	-19.212806713608774	47.500866514575726	187703
010b6def0a2afa42bfab3fd27bfe19463a772d1e	slow advances in fault-tolerant real-time distributed computing	fault tolerant;network operating systems;real time;distributed computing;fault tolerant computing;real time computing;automated recovery fault tolerant computing real time computing distributed computing ft rt computing;fault tolerance distributed computing real time systems fault tolerant systems environmental economics humans redundancy computer industry fuel processing industries process design;real time systems fault tolerant computing network operating systems;real time systems	Is fault-tolerant (FT) real-time computing a specialized branch of FT computing? The key issue in real-time (RT) computing is to economically produce systems that yield temporal behavior which is relatively easily analyzable and acceptable in given application environments. Fault-tolerant (FT) RT computing has been treated by the predominant segment of the FT computing research community as a highly specialized branch of FT computing. This author believes that the situation should be changed. It seems safe to say that FT techniques for which useful characterizations of temporal behavior have not been or cannot be developed, are at best immature, if not entirely useless. This means that FT RT computing is at the core of FT computing.	distributed computing;fault tolerance;real-time computing;windows rt	K. H. Kim	2004	Proceedings of the 23rd IEEE International Symposium on Reliable Distributed Systems, 2004.	10.1109/RELDIS.2004.1353009	embedded system;fault tolerance;real-time computing;computer science;operating system;distributed computing	HPC	-23.871019991368506	49.27725500488608	188163
741b56f8a5cb42e9a99529eb8c4aab01a0212862	a space-efficient approach to consistency check of firewall rules			firewall (computing)	Wei Li	2009			firewall (construction);causal consistency;consistency model;sequential consistency;distributed computing;computer science	NLP	-21.93681813636891	48.18769616006771	188342
42769ccc7c5bb1705e5ac9c2a8b7c084bc4ea218	durability of replicated distributed storage systems	replication;design automation;model based approach;distributed storage system;durability;file system;distributed file system;peer to peer	We study the problem of guaranteeing data durability [2] in distributed storage systems based on replication. Our work is motivated by several several recent efforts [3, 5, 1] to build such systems in a peer-to-peer environment. The key features of this environment which make achieving durability difficult are (1) data lifetimes may be several orders of magnitude larger than the lifetimes of individual storage units, and (2) the system may have little or no control over the participation of these storage units in the system. We use a model-based approach to develop engineering principles for designing automated replication and repair mechanisms to implement durability in such systems.	clustered file system;durability (database systems);peer-to-peer	Sriram Ramabhadran;Joseph Pasquale	2008		10.1145/1375457.1375514	replication;real-time computing;electronic design automation;computer science;operating system;durability;database;distributed computing;distributed file system;replication	OS	-25.208416682842994	52.610502795827486	188859
0596edcf2202b4d3992cd4b5573aac2d9ba9e9d6	on-demand recovery in middleware storage systems	middleware storage systems;in memory data management systems;protocols;storage system;recovery architecture;computer crashes;database management systems;database;recovery architecture in memory data management systems middleware storage systems on demand recovery technique;recovery;indexes;software architecture;servers;engines;software architecture database management systems middleware;middleware;on demand recovery technique;optimistic techniques;optimistic techniques recovery database;data management system;servers indexes computer crashes protocols engines hardware;hardware	This paper presents a recovery architecture for in-memory data management systems. Recovery in such systems boils down to solving two problems: retrieving and installing the last committed image of the crashed database on a new server and replaying the updates missing from the image. We improve recovery time with a novel technique called On-Demand Recovery, which removes the need to replay all missing updates before new transactions can be accepted. We have implemented and thoroughly evaluated the technique. We show in the paper that in some cases On-Demand Recovery can reduce recovery time by more than 50%.	in-memory database;middleware;regular expression;server (computing)	Lásaro J. Camargos;Fernando Pedone;Alex Pilchin;Marcin Wieloch	2010	2010 29th IEEE Symposium on Reliable Distributed Systems	10.1109/SRDS.2010.31	database index;communications protocol;software architecture;real-time computing;recovery;computer science;operating system;middleware;database;server	DB	-23.72263000182474	49.71173640620773	189762
023d75615404a8248dacb168774de03ec8750cef	causality and proactive cancellation	proactive cancellation mechanism;time warp simulation clocks distributed processing;time warp;optimistic time warp simulators;computational steering;clocks;distributed processing;static inter connection topology;timestamp mechanism;clocks computational modeling discrete event simulation time warp simulation out of order counting circuits topology laboratories time measurement scalability;logical processes proactive cancellation mechanism optimistic time warp simulators distributed cancellation mechanism event counter range plausible total clocks timestamp mechanism constant size vectors fifo communication layer static inter connection topology;out of order;out of order execution;fifo communication layer;distributed cancellation mechanism;plausible total clocks;event counter range;parallel visualization;proof of correctness;causal relation;time warp simulation;data redistribution;constant size vectors;logical process;logical processes;numerical simulation	Optimistic Time Warp simulators should stop the rapid propagation of incorrect events to avoid reaching a catastrophic state (a state where out of order event execution is always a step ahead of its corrective measures). A distributed cancellation mechanism using Total Clocks was proposed earlier to avoid such catastrophic states. In this paper, we present a proactive cancellation mechanism using a vector of event counter range to address the scalability issues with the previously defined solution. As opposed to total clocks, this timestamp mechanism (a.k.a. Plausible Total Clocks) consists of constant size vectors and are independent of the number of simulation objects in the simulation. The events generated due to an out of order execution are pro-actively canceled by determining its causality relation with the already annihilated events. We present a proof of correctness of the distributed cancellation mechanism and also show that catastrophic states are avoided with this proactive cancellation mechanism. This cancellation mechanism assumes FIFO communication layer, static inter-connection topology, and Logical Processes consisting of several simulation objects.	causality;computation;correctness (computer science);emoticon;fifo (computing and electronics);integrated circuit layout design protection;loss of significance;out-of-order execution;proactive parallel suite;requirement;rollback (data management);run time (program lifecycle phase);scalability;simulation;software propagation	Malolan Chetlur;Philip A. Wilsey	2006	2006 Tenth IEEE International Symposium on Distributed Simulation and Real-Time Applications	10.1109/DS-RT.2006.12	real-time computing;computer science;theoretical computer science;distributed computing	Embedded	-21.441017688366806	47.818657469158374	190730
1055ec63e403386f19ed2404e36635ae450a540c	concurrency control in multi-role association	phase locking;concurrency control permission processor scheduling access control access protocols information systems relational databases system recovery authorization distributed computing;information systems;processor scheduling;distributed computing;system recovery;permission;access control models;concurrency control;access protocols;authorization;relational databases;access control	A role shows a job function in an enterprise. In a rolebased access control model, a role is a set of access rights. A subject doing jobs is granted roles showing the jobs. In addition, objects have to be consistent in presence of multiple conflicting transactions. A transaction issued by a subject is associated with a subset of roles granted to the subject, which is named purpose. A method with a more significant purpose is performed before another method with a less significant purpose. We discuss which purpose is more significant than another purpose. We discuss general role-ordering (GRO) schedulers so that multiple conflicting transactions are serializable in the significant order of subjects and purposes. We evaluate the GRO scheduler compared with the traditional two-phase locking protocol.	access control list;concurrency control;job stream;lock (computer science);scheduling (computing);serializability;two-phase commit protocol;two-phase locking	Tomoya Enokido;Makoto Takizawa	2006	26th IEEE International Conference on Distributed Computing Systems Workshops (ICDCSW'06)	10.1109/ICDCSW.2006.31	optimistic concurrency control;real-time computing;computer access control;discretionary access control;relational database;computer science;access control;operating system;concurrency control;database;distributed computing;authorization;non-lock concurrency control;serializability;computer security;information system;computer network;distributed concurrency control	DB	-23.540112003311638	47.80571946215117	191464
ab42ca2017c534d09e55c077a2edf8a293ffb72d	database support for efficiently maintaining derived data	database system;conference_paper;active databases;high performance	Derived data is maintained in a database system to correlate and summarize base data which record real world facts. As base data changes, derived data needs to be recomputed. A high performance system should execute all these updates and recomputations in a timely fashion so that the data remains fresh and useful, while at the same time executing user transactions quickly. This paper studies the intricate balance between recomputing derived data and transaction execution. Our focus is on e cient recomputation strategies | how and when recomputations should be done to reduce their cost without jeopardizing data timeliness. We propose the Forced Delay recomputation algorithm and show how it can exploit update locality to improve both data freshness and transaction response time.	algorithm;database;general-purpose modeling;incremental backup;lazy evaluation;locality of reference;programmer;real-time clock;real-time operating system;replay attack;response time (technology);triangle strip	Brad Adelberg;Ben Kao;Hector Garcia-Molina	1996		10.1007/BFb0014155	real-time computing;computer science;data mining;database	DB	-20.986924313381223	50.195951788612476	191665
26b2ade8c9595ade03928225396a8a9b52d4e43c	towards analyzing the fault-tolerant operation of server-can	fault tolerant;fault tolerance scheduling bandwidth network servers real time systems computer science communication standards embedded system communication system control control systems;fault tolerant operation;controller area networks;wip;server can;fault tolerance;telecommunication security;wip fault tolerant operation server can work in progress;point of view;telecommunication security controller area networks fault tolerance;work in progress	This work-in-progress (WIP) paper presents server-CAN and highlights its operation and possible vulnerabilities from a fault tolerance point of view. The paper extends earlier work on server-CAN by investigating the behaviour of server-CAN in faulty conditions. Different types of faults are described, and their impact on sever-CAN is discussed, which is the subject of on-going research	care-of address;dependability;fault model;fault tolerance;server (computing);vulnerability (computing)	Thomas Nolte;Guillermo Rodríguez-Navas;Julian Proenza;Sasikumar Punnekkat;Hans A. Hansson	2005	2005 IEEE Conference on Emerging Technologies and Factory Automation	10.1109/ETFA.2005.1612590	embedded system;fault tolerance;real-time computing;computer science;engineering;work in process;fault model;distributed computing;software fault tolerance	Visualization	-25.104408722635288	50.03141370013619	192544
e1d20ea9a23d54736619ab76b2b3778a091fa828	real-time commit protocol for distributed real-time database systems	database system;real time;satisfiability;two phase commit protocol real time commit protocol distributed real time database program correctness remote transactions rcp timely completion fast computing voting phase message reduction;real time systems protocols database systems transaction databases cities and towns electronic mail voting feedback communications time factors;real time systems	This paper proposes a new commit protocol. The basic idea is that the commit procedures for correctness rely on the results of remote transactions that are timely completed. We call the new protocol real-time commit protocol (RCP). RCP satisfies both the correct and the timely completion and produces several desirable effects for fast computing like the elimination of the voting phase and the reduction of the number of messages in the two phase commit protocol.	real-time transcription	Yong-Ik Yoon;Mikyung Han;Ju-Hyun Cho	1996		10.1109/ICECCS.1996.558417	three-phase commit protocol;commit;real-time computing;two-phase commit protocol;computer science;x/open xa;database;distributed computing;compensating transaction;satisfiability	Embedded	-23.543515980120308	48.11344397361099	192779
f3d058bd7a2097c66308902610ed9d3adaed0457	enhancing pre-existing data managers with atomicity and durability	spatial data;data management	Many of currently available managers of persistent data do not provide support for updates that are atomic and durable. Developing from scratch the requisite software for incorporating the atomicity and durability properties into each of these pre-existing data managers is a time-consuming and formidable task. It is hence desirable to design and build a general purpose recovery manager that can be easily interfaced with pre-existing data managers. In this paper, we consider four recovery manager architectures that are based on recovery algorithms proposed in the literature and recovery designs adopted in existing products. We evaluate the various architectures with respect to 1) the extent of the modifications they cause to the software of pre-existing data managers, and 2) the degree to which they affect the performance of the data managers. Finally, we relate the experience we had in integrating a spatial data manager with a general purpose recovery manager.	atomicity (database systems);durability (database systems)	Rajeev Rastogi;Marie-Anne Neimat	1994		10.1007/3-540-58183-9_64	data mining;database;world wide web	Logic	-23.942255997958416	50.1143464211957	192981
eef137f16b09a5db4d67afce6ba16f6c23f22de5	a real-time scheduling strategy based on priority in data stream system	priority tree data stream query priority scheduling strategy;scheduling real time systems;query processing;real time systems hybrid intelligent systems information science conference management technology management databases monitoring data processing;processor scheduling;data stream;continuous query;real time scheduling strategy;continuous query scheduling real time scheduling strategy data stream management system real time system priority tree unique execution sequence hit value ratio;data mining;priority scheduling;query priority;continuous query scheduling;scheduling;real time scheduling;scheduling strategy;real time system;priority tree;unique execution sequence;data stream management system;real time systems;hit value ratio	In this paper, some concepts of real-time system (e.g., dealine, slack and criticalness) are introduced into some high-critical specific applications of Data Stream Management System (DSMS). In light of the features of continuous queries in DSMS, they are given new meaning. According to these concepts, a real-time scheduling strategy based on priority is proposed. In this strategy, the earlier dealine is, the shorter slack is and the higher criticalness is, the higher priopirty of a query is. Meanwhile, a structure of priority tree is presented in order to realize the unique execution sequence of queries based on their priorities. The experimental results show that the Hit Value Ratio (HVR) is improved greatly by applying this strategy, and the successful possibility of continuous query scheduling is improved as well.	management system;real-time clock;real-time computing;real-time operating system;real-time transcription;scheduling (computing);slack variable	Yan Wang;Weihong Xuan;Wei Li;Baoyan Song;Xiaoguang Li	2009	2009 Ninth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2009.269	real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;database;distributed computing;least slack time scheduling	Robotics	-24.460167393748666	48.834615374250255	193738
3b0182ef64d6d97c8ced044b26cc87e682edd57c	a case for epidemic fault detection and group membership in hpc storage systems		Fault response strategies are crucial to maintaining performance and availability in HPC storage systems, and the first responsibility of a successful fault response strategy is to detect failures and maintain an accurate view of group membership. This is a nontrivial problem given the unreliable nature of communication networks and other system components. As with many engineering problems, trade-offs must be made to account for the competing goals of fault detection efficiency and accuracy. Today’s production HPC services typically rely on distributed consensus algorithms and heartbeat monitoring for group membership. In this work, we investigate epidemic protocols to determine whether they would be a viable alternative. Epidemic protocols have been proposed in previous work for use in peer-to-peer systems, but they have the potential to increase scalability and decrease fault response time for HPC systems as well. We focus our analysis on the Scalable Weakly-consistent Infection-style Process Group Membership (SWIM) protocol. We begin by exploring how the semantics of this protocol differ from those of typical HPC group membership protocols, and we discuss how storage systems might need to adapt as a result. We use existing analytical models to choose appropriate SWIM parameters for an HPC use case. We then develop a new, high-resolution parallel discrete event simulation of the protocol to confirm existing analytical models and explore protocol behavior that cannot be readily observed with analytical models. Our preliminary results indicate that the SWIM protocol is a promising alternative for group membership in HPC storage systems, offering rapid convergence, tolerance to transient network failures, and minimal network load.	algorithm;computer data storage;consensus (computer science);disk controller;eventual consistency;fault detection and isolation;image resolution;peer-to-peer;process group;response time (technology);scalability;simulation;system wide information management;telecommunications network	Shane Snyder;Philip H. Carns;Jonathan Jenkins;Kevin Harms;Robert B. Ross;Misbah Mubarak;Christopher D. Carothers	2014		10.1007/978-3-319-17248-4_12	real-time computing;distributed computing;computer security	HPC	-22.714465480536596	53.0460400461217	193850
3da841e78c89f2a988f8b68339cf61847ebf3464	incremental garbage collection considering the objects' lifetime	garbage collection		garbage collection (computer science)	Tatsunobu Koike;Teruo Iwai;Masakazu Nakanishi	1999			computer science;garbage;distributed computing;garbage collection	HCI	-20.057158340789673	48.51481214991838	194056
0d66bc362f31c3599d46db1726ffa9f8d8bc865d	the second trans-pacific grid datafarm testbed and experiments for sc2003	file servers;fault tolerant;lan interconnection;data analysis;large scale;parallel architectures;file system;parallel file system;testing file systems petascale computing bandwidth computer architecture grid computing secure storage parallel processing data security fault tolerance;data analysis parallel architectures file servers workstation clusters lan interconnection;grid security infrastructure;it management;load balance;multiple cluster nodes trans pacific grid datafarm testbed grid datafarm architecture global petascale data intensive computing global parallel file system online petascale storage scalable i o bandwidth scalable parallel processing clusters grid grid security infrastructure filesystem metadata fault tolerance load balancing world wide largescale data analysis gfarm file system;workstation clusters;data intensive computing;parallel processing	The Grid Datafarm architecture is designed for global petascale data-intensive computing. It provides a global parallel file system (Gfarm file system) with online petascale storage, scalable I/O bandwidth, and scalable parallel processing by federating thousands of local file systems in a grid of clusters securely using Grid security infrastructure. One of features is that it manages file replicas in filesystem metadata for fault tolerance and load balancing. Here, we present an overview of our planned experiment performed as the SC2003 Bandwidth Challenge at the Supercomputing 2003 site in Phoenix, Arizona, USA. In the experiment, five clusters in Japan and three clusters in US comprise a Gfarm file system, on which world-wide largescale data analysis is performed. In the Gfarm file system, a file is dispersed in several cluster nodes, each of which is replicated independently and in parallel by multiple third-party transfers between multiple cluster nodes. For the Challenge, terabyte-scale experimental data is replicated between US and Japan via APAN/TransPAC and SuperSINET (about 10,000 km or 6,000 miles). At the workshop we present the full detail of the experiment.	acm/ieee supercomputing conference;clustered file system;data-intensive computing;experiment;fault tolerance;gfarm file system;grid security infrastructure;input/output;load balancing (computing);parallel computing;petascale computing;scalability;source-to-source compiler;terabyte;testbed	Osamu Tatebe;Hirotaka Ogawa;Yuetsu Kodama;Tomohiro Kudoh;Satoshi Sekiguchi;Satoshi Matsuoka;Kento Aida;Taisuke Boku;Mitsuhisa Sato;Youhei Morita;Yoshinori Kitatsuji;Jim Williams;John Hicks	2004	2004 International Symposium on Applications and the Internet Workshops. 2004 Workshops.	10.1109/SAINTW.2004.1268694	self-certifying file system;file server;parallel processing;fault tolerance;grid file;parallel computing;computer science;load balancing;operating system;data-intensive computing;database;distributed computing;data analysis	HPC	-26.407432417992016	52.123727927825065	194189
c1edae7ec6a9198f610899e3104e812cf6440a83	a recyclable resource management method for fast process creation and reduced memory consumption	information service environment;vector space model;information filtering;operating systems recyclable resource management method reduced memory consumption process creation mechanism process termination mechanism program execution adaptive control mechanism concurrent execution environment;information needs;user feedback;resource management memory management recycling costs operating systems frequency degradation web server pervasive computing adaptive control;feedback;recommender system;feature extraction;resource allocation concurrency control power aware computing;personalized information recommendation system;information society;adaptive filtering algorithm;feature selection;information service;information need;information filters;pseudo feedback;adaptive filter;training algorithm	The costs involved in process creation and termination make this procedure expensive. This procedure expensive, thus degrading the performance of program execution. To solve this problem, a fast process creation and termination mechanism is proposed. This mechanism is implemented by recycling process resources. In order to improve the efficiency of recycling, the management of preserved process resources for recycling is an important factor. In this paper, we propose an improved resource management method for recycling process resources and an adaptive control mechanism. In the method, only one process resource with a program image is preserved for each program that occurs with high frequency of program execution. The proposed method can reduce the amount of memory consumption for preserved process resources in a concurrent execution environment. We also describe the implementation of the proposed method on the tender operating system and report the results of our experiments.	algorithm;executable;experiment;newman's lemma;operating system	Toshihiro Tabata;Hideo Taniguchi	2007	The 2007 International Conference on Intelligent Pervasive Computing (IPC 2007)	10.1109/IPC.2007.83	real-time computing;computer science;information filtering system;distributed computing;world wide web	HPC	-25.950652695097315	47.57766732890757	194729
8554e7822e49f5fbb551dd20cf2a0f35b9fb6c5b	object-oriented serializability in real-time concurrency control	real time;concurrency control;object oriented		concurrency control;real-time cmix;serializability	Kenichi Asai;Junpei Nishibayashi;Kouji Yoshihara;Motoyasu Nagata	1996			distributed concurrency control;non-lock concurrency control;global serializability;isolation (database systems);two-phase locking;distributed computing;commitment ordering;timestamp-based concurrency control;serializability;computer science	DB	-23.684550939306057	47.41765077737583	195098
0c27347882b812800ad950533e13246403229b3c	gargamel: boosting dbms performance by parallelising write transactions	scheduing alghorithms cloud computing distributed dbmses;scheduling alghorithms gargamel partitions dbms performance boosting write transaction parallelization parallel transactions distributed dbms concurrency control aborts conflicting transaction pre serialization nonconflicting update transaction parallelization database replica mutual synchronisation response time improvement load improvement cloud computing;distributed dbmses;throughput databases time factors benchmark testing load modeling resource management numerical models;bases de donnees reparties;parallel databases;scheduing alghorithms;scheduling;concurrency control;optimistic approach;scheduling cloud computing concurrency control parallel databases replicated databases;distributed databases;distributed systems;distributed dbms;replicated databases;cloud computing	Parallel transactions in distributed DBs incur high overhead for concurrency control and aborts. We propose an alternative approach by pre-serializing possibly conflicting transactions, and parallelizing non-conflicting update transactions to different replicas. Our system provides strong transactional guarantees. In effect, Gargamel partitions the database dynamically according to the update workload. Each database replica runs sequentially, at full bandwidth, mutual synchronisation between replicas remains minimal. Our simulations show that Gargamel improves both response time and load by an order of magnitude when contention is high (highly loaded system with bounded resources), and that otherwise slow-down is negligible.	acid;concurrency (computer science);concurrency control;overhead (computing);parallel computing;response time (technology);serialization;simulation	Pierpaolo Cincilla;Sébastien Monnet;Marc Shapiro	2012	2012 IEEE 18th International Conference on Parallel and Distributed Systems	10.1109/ICPADS.2012.83	parallel computing;cloud computing;computer science;operating system;concurrency control;database;distributed computing;scheduling;distributed database	DB	-21.68854671614543	49.10159637312619	195405
9438cd54cd4dda209080f87594ec0515929cac65	adaptive protocols for managing replicated distributed databases	control systems;dynamic methods;distributed database;concurrent computing;system transactions;adaptive protocols;system configuration;transaction processing concurrency control database theory distributed databases;accessibility threshold protocol;adaptive protocol;user transactions adaptive protocols quorum consensus locking policy timestamp time stamp accessibility threshold protocol replicated distributed databases replicated databases system transactions view objects dynamic methods;view objects;transaction databases;user transactions;fault tolerant systems;replicated distributed databases;system design;database systems;concurrency control;distributed databases;access protocols;computer science;timestamp;quorum consensus locking policy;transaction processing;distributed databases access protocols costs database systems transaction databases fault tolerant systems concurrency control control systems computer science concurrent computing;database theory;replicated databases;time stamp	In this paper we propose a family of adaptive protocols for managing replicated databases. Adaptive protocols use information about the system configuration to determine how operations are executed. Special system transactions are executed to maintain this information in view objects. We propose using static as well as dynamic methods to execute both system and user transactions. This results in a modular methodology for designing four different protocols. Two of these protocols are new and present the system designer with several alternatives when compared with the previously known	adaptive grammar;architecture of btrieve;distributed database;system configuration;systems design	Amr El Abbadi	1991		10.1109/SPDP.1991.218299	timestamp;real-time computing;computer science;database;distributed computing;distributed database	DB	-24.43820215437132	47.60234818350299	195916
78ac7854245d563bc13d3bf12e8b46af33832247	an integrated approach to fault tolerance	distributed databases;fault tolerant computing;protocols;manetho;client processes;integrated approach;process replication;protocols;rollback-recovery;transparent fault tolerance	Manetho is an experiment’ system whose god is to explore the extent to which transpurent fault tolerance can be added to long-running tlistrihiitetl applications [5, 61. Transparent techniques are attractive because they can automatically add fault tolerance to existing applications that were written without .consitleration for reliability. Previous techniques for providing transparent faulttolerance relied on rollback-recovery [7, 131. However, rollback-recovery is not appropriate for server processes where the lack of service during rollback is intolerable. Furthermore, rollhack-recovery assumes that a process can be restarted on any available host. As a result, extended downtime cannot be tolerated for example in file servers, which have to run on tlie host where the disks reside. Manetho solves these prohlexiis with an integrated approach by using process replication for server processes and rollback-recovery for client processes. The iliain features of Manetho are:	downtime;fault tolerance;server (computing)	E. N. Elnozahy;Willy Zwaenepoel	1992			real-time computing;computer science;operating system;distributed computing	OS	-22.53849502258818	50.0228064047905	196519
797025fcc7bf32bb711f6ec8828ba456bfa6ca8d	on the self contained modelling of db/dc systems	decomposition of linear programs;analytic solution techniques;data base management system and subsystem;system performance;memory hierarchies;branch and bound algorithms;integer programming;computer networks and distributed subsystems;linear programming;simultaneous equations	This paper discusses the modelling of DB/DC systems operating in a demand paging environment. As an extension of an earlier definition of Denning a class of systems is defined, those operating with “working set integrity”, for which one can obtain system response parameters solely from the resource requirements of the input processes when running in isolation. No external information such as a miss ratio curve from another systems environment is required. The interdependence between the resource requirements of processes when running in a systems environment and the system performance parameters is faced directly and expressed as a set of simultaneous equations. The emphasis is on the effect of the sharing of a common set of pages among the active set of processes. Both unrestricted shared usage and usage with locking are treated.	active set method;dorothy e. denning;interdependence;lock (computer science);paging;requirement;working set	Peter D. Welch	1976		10.1145/800200.806178	real-time computing;integer programming;simultaneous equations;computer science;linear programming;theoretical computer science;operating system;distributed computing;computer performance	Embedded	-19.64414469216165	46.507224162160696	197467
741a04ef3a0c3953a3d37726bf4d6170eaa68a55	online-abft: an online algorithm based fault tolerance scheme for soft error detection in iterative methods	online error detection;iterative methods;algorithm based fault tolerance abft;checkpoint;soft error	Soft errors are one-time events that corrupt the state of a computing system but not its overall functionality. Large supercomputers are especially susceptible to soft errors because of their large number of components. Soft errors can generally be detected offline through the comparison of the final computation results of two duplicated computations, but this approach often introduces significant overhead. This paper presents Online-ABFT, a simple but efficient online soft error detection technique that can detect soft errors in the widely used Krylov subspace iterative methods in the middle of the program execution so that the computation efficiency can be improved through the termination of the corrupted computation in a timely manner soon after a soft error occurs. Based on a simple verification of orthogonality and residual, Online-ABFT is easy to implement and highly efficient. Experimental results demonstrate that, when this online error detection approach is used together with checkpointing, it improves the time to obtain correct results by up to several orders of magnitude over the traditional offline approach.	application checkpointing;computation;error detection and correction;fault tolerance;iterative method;krylov subspace;online algorithm;online and offline;overhead (computing);soft error;supercomputer	Zizhong Chen	2013		10.1145/2442516.2442533	parallel computing;real-time computing;soft error;computer science;theoretical computer science;distributed computing;iterative method	HPC	-19.125856361272543	48.77723888402792	197600
afe6326a67a48172c6cddee25edf09b11ed4d2d1	a late join approach for distributed dynamic-locking in real-time collaborative editing systems	phase locking;systeme temps reel;groupware;resolucion conflicto;verrouillage phase;maintenance;real time;customization;personnalisation;collaborative editing;enganche de fase;dominio trabajo;internet;resolution conflit;domaine travail;temps reel;concurrency control;personalizacion;mantenimiento;tiempo real;distribution dynamics;real time system;sistema tiempo real;workspace;controle concurrence;control concurrencia;information system;consistency maintenance;conflict resolution;collecticiel;shared workspace;systeme information;sistema informacion	This paper proposes a novel late join approach for dynamic locking mechanism (DLM) in Internet-based real-time collaborative editing systems. The main idea is that: in the process of late join, four cases are classified according to the situations of whether the user has enabled the DLM or not and whether the user is in the session or not. And different transmission steps are adopted for each case in order to achieve the goal that the dynamic locking data can be synchronized. This approach can maintain the consistency of locking data in the distributed collaborative environment more efficiently. And the process of late join for multiple users and the error control method in the network transmission are also presented in this paper.	collaborative real-time editor;distributed lock manager;error detection and correction;lock (computer science);mega man network transmission;multi-user;real-time clock;real-time transcription	Xianghua Xu;Jiajun Bu;Chun Chen;Yong Li	2004	TENCON 2005 - 2005 IEEE Region 10 Conference	10.1007/978-3-540-30112-7_23	double-checked locking;real-time computing;the internet;simulation;computer science;operating system;concurrency control;conflict resolution;index locking;database;distributed computing;information system;workspace	DB	-24.331979624314634	47.322875751392864	197975
13877626d21f8dd731e23533ce599a42fa2902d5	managing missed interactions in distributed virtual environments	and virtual realities c 2 4 distributed systems distributed applications;augmented;distributed virtual environment;categories and subject descriptors according to acm ccs h 5 1 information interfaces and presentation multimedia information systems artificial	A scalable distributed virtual environment (DVE) may be achieved by ensuring virtual world objects communicate their actions to only those objects that fall within their influence, reducing the need to send and process unnecessary messages. A missed interaction may be defined as a failure to exchange messages to appropriately model object interaction. A number of parameters under the control of a DVE developer may influence the possibility of missed interactions occurring (e.g., object velocities, area of influence). However, due to the complexities associated with object movement and the deployment environment (e.g., non-deterministic object movement, network latency), identifying the value for such parameters to minimise missed interactions while maintaining scalability (minimal message passing) is not clear. We present in this paper a tool which simulates a DVE and provides developers with an indication of the appropriate values for parameters when balancing missed interactions against scalability.	anomaly detection;deployment environment;digital video effect;display resolution;domain-driven design;graph (discrete mathematics);interaction;message passing;middleware;norm (social);scalability;simulation;software deployment;virtual reality;virtual world;warez	Simon Edward Parkin;Peter Andras;Graham Morgan	2006		10.2312/EGVE/EGVE06/027-034	simulation;computer science;theoretical computer science;distributed computing	Visualization	-25.605760840438204	51.73082984674811	198503
f6c9d49bc655c590e1fad96757539172ed2a2165	application-aware reliability and security: the trusted illiac approach	integrated approach;cluster computing;hardware software resource sharing;middleware application aware reliability trusted illiac approach secure cluster computing on demand utility computing adaptive enterprise computing hardware software resource sharing reprogrammable hardware configurable operating system;on demand utility computing;secure cluster computing;resource use;electrical and computer engineering;workstation clusters computer network reliability middleware security of data;operating system;application aware reliability;adaptive enterprise computing;middleware;reprogrammable hardware;workstation clusters;utility computing;trusted illiac approach;security of data;configurable operating system;computer network reliability	Security and reliability are the key attributes in building highly trusted systems. System security violations (e.g., unauthorized privileged access or the compromising of data integrity) and reliability failures can be caused by hardware problems (transient or intermittent), software bugs, resource exhaustion, environmental conditions, or any complex interaction among these factors. To build a truly trustworthy system, the designer must find ways to mitigate (avoid and tolerate) against accidental errors and malicious attacks. Trusted ILLIAC 1 is a reliable and secure clustercomputing platform being built at the University of Illinois Coordinated Science Laboratory (CSL) and Information Trust Institute (ITI), involving faculty from Electrical and Computer Engineering and Computer Science Departments. Trusted ILLIAC is intended to be a large, demonstrably trustworthy cluster-computing system to support what is variously referred to as on-demand/utility computing or adaptive enterprise computing. Such systems require that a significant number of applications co-exist and share hardware/software resources using a variety of containment boundaries. Current solutions aim at providing hardware and software solutions that can only be described as a one-size-fits-all approaches. Today’s environments are complex, expensive to implement, and nearly impossible to validate. The challenge is to provide an application-specific level of reliability and security in a totally transparent manner, while delivering optimal performance. A promising approach lies in developing a new set of application-aware methods that provide customized levels of trust (specified by the application) enforced using an integrated approach 1 Trusted ILLIAC is based on research and support provided by, among others, The National Science Foundation, MARCO/GSRC (SRC and DARPA), IBM, HP, AT&T, AMD, Intel, Motorola, XILINX, Nallatech, and the University of Illinois. involving reprogrammable hardware, enhanced compiler methods to extract security and reliability properties, and the support of configurable operating system and middleware. Our approach is to demonstrate such a set of integrated techniques that span entire system hierarchy: processor hardware, operating system, middleware, and application. At the processor level, a Reliability and Security Engine (RSE) provides a hardware framework that enables embedding low-cost, programmable hardware modules to provide application-aware error detection and security services (e.g., process hang detection, selective duplication of the instruction stream, and detection of memory-corruption	authorization;compiler;computer cluster;computer engineering;computer science;data integrity;enterprise software;error detection and correction;error-tolerant design;fits;illiac;information trust institute;malware;memory corruption;middleware;nallatech;privileged access;reliability (computer networking);resource exhaustion attack;semiconductor research corporation;software bug;trusted operating system;trusted system;utility computing;verification and validation	Ravishankar K. Iyer	2006		10.1109/NCA.2006.15	embedded system;real-time computing;computer cluster;computer science;operating system;trusted computing base;middleware;utility computing;computer network	Arch	-24.558012519822224	51.25134747641311	198878
4901c1caa57c39b96c504c4d98c31ea70452cb89	consistent synchronization schemes for workload replay		Oracle Database Replay has been recently introduced in Oracle 11g as a novel tool to test relational database systems [9]. It involves recording the workload running on the database server in a production system, and subsequently replaying it on the database server in a test system. A key feature of workload replay that enables realistic reproduction of a real workload is synchronization. It is a mechanism that enforces specific ordering on the replayed requests that comprise the workload. It affects the level of request concurrency and the consistency of the replay results when compared to the captured workload. In this paper, we define the class of consistent replay synchronization schemes and study, for the first time, the spectrum they cover and the tradeoffs they present. We place the only scheme proposed so far [9], the one implemented in Oracle 11g Release 1, within the aforementioned spectrum and show that it is coarse-grained and more restrictive than necessary, often enforcing dependencies between calls that are independent. By enforcing needless waits, it decreases the level of possible concurrency and degrades performance. To overcome these drawbacks, we identify the best scheme within the spectrum; it is finer-grained than its counterparts and strikes the right balance across different tradeoffs: it enforces a partial ordering on the replayed calls that minimizes the number of required waits and maximizes the level of concurrency, without compromising consistency of the replay results. We have implemented the new scheme in Oracle 11g Release 2. Our experiments indicate that it produces better quality replays than the pre-existing one for major classes of workload.	concurrency (computer science);database server;experiment;oracle database;production system (computer science);relational database;replay attack;server (computing);synchronization (computer science);waits	Konstantinos Morfonios;Romain Colle;Leonidas Galanis;Supiti Buranawatanachoke;Benoît Dageville;Karl Dias;Yujun Wang	2011	PVLDB		database;oracle;relational database management system;partially ordered set;workload;computer science;database server;concurrency;synchronization;real-time computing;distributed computing	DB	-22.799306292061065	48.45735779946131	199408
1f91349cd0377b1f3a2099470f83b463143b876b	a flexible system architecture for acquisition and storage of naturalistic driving data	software;data acquisition software computer architecture data visualization vehicles software architecture;in vehicle sensors;data communications;data fusion;intelligent transportation systems programs;computer architecture;data storage;software architecture;visualization;middleware data acquisition intelligent transportation systems message passing;data visualization;communication library flexible system architecture naturalistic driving data storage naturalistic driving data acquisition intelligent transportation systems data management infrastructure message passing publish subscribe network;automatic data collection systems;vehicles;ros intelligent transportation systems naturalistic driving data data acquisition data storage visualization communication lcm zmq rabbitmq;traffic data;data acquisition	Innovation in intelligent transportation systems relies on analysis of high-quality data. In this paper, we describe the design principles behind our data management infrastructure. The principles we adopt place an emphasis on flexibility and maintainability. This is achieved by breaking up code into a modular design that can be run on many independent processes. Message passing over a publish-subscribe network enables interprocess communication and promotes data-driven execution. By following these principles, rapid prototyping and experimentation with new sensing modalities and algorithms are possible. The communication library underpinning our proposed architecture is compared against several popular communication libraries. Features designed into the system make it decentralized, robust to failure, and amenable to scaling across multiple machines with minimal configuration. Code written using the proposed architecture is compact, transparent, and easy to maintain. Experimentation shows that our proposed architecture offers a high performance when compared against alternative communication libraries.	algorithm;big data;computer;data acquisition;data logger;database;image scaling;inter-process communication;latent class model;library (computing);message passing;modular design;multicast;programming language;programming paradigm;publish–subscribe pattern;rabbitmq;rapid prototyping;robot operating system;server (computing);software architecture;upload;zeromq	Asher Bender;James R. Ward;Stewart Worrall;Marcelo L. Moreyra;Santiago Gerling Konrad;Favio R. Masson;Eduardo Mario Nebot	2016	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2016.2524523	embedded system;software architecture;real-time computing;visualization;computer science;operating system;computer data storage;sensor fusion;data acquisition;data visualization	Arch	-20.72088269698673	52.80065319599596	199878
