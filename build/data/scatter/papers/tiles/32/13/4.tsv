id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
71803f100facf5388c700015e9feb5ed6d29ceea	a dependency parser for tweets		We describe a new dependency parser for English tweets, TWEEBOPARSER. The parser builds on several contributions: new syntactic annotations for a corpus of tweets (TWEEBANK), with conventions informed by the domain; adaptations to a statistical parsing algorithm; and a new approach to exploiting out-of-domain Penn Treebank data. Our experiments show that the parser achieves over 80% unlabeled attachment accuracy on our new, high-quality test set and measure the benefit of our contributions. Our dataset and parser can be found at http://www.ark.cs.cmu.edu/TweetNLP.	algorithm;attachments;experiment;parser;statistical parsing;test set;treebank	Lingpeng Kong;Nathan Schneider;Swabha Swayamdipta;Archna Bhatia;Chris Dyer;Noah A. Smith	2014			computer science;natural language processing;artificial intelligence;parsing;statistical parsing;treebank;syntax;dependency grammar;test set	NLP	-22.537744043115246	-74.59276185132309	130609
c85722e4d1b24ec4746a82ce2c4b1434d89da646	chinese named entity recognition with character-word mixed embedding		Named Entity Recognition (NER) is an important basis for the tasks in natural language processing such as relation extraction, entity linking and so on. The common method of existing Chinese NER systems is to use the character sequence as the input, and the intention is to avoid the word segmentation. However, the character sequence cannot express enough semantic information, so that the recognition accuracy of Chinese NER is not as good as western language such as English. To solve this issue, we propose a Chinese NER method based on Character-Word Mixed Embedding (CWME), and the method is in accord with the pipeline of Chinese natural language processing. Our experiments show that incorporating CWME can effectively improve the performance for the Chinese corpus with state-of-the-art neural architectures widely used in NER, and the proposed method yields nearly 9% absolute improvement over previously results.	entity linking;experiment;named entity;named-entity recognition;natural language processing;relationship extraction;text segmentation	E Shijia;Yang Xiang	2017		10.1145/3132847.3133088	word embedding;data mining;entity linking;computer science;relationship extraction;embedding;named-entity recognition;artificial intelligence;text segmentation;pattern recognition	NLP	-20.95924396811688	-73.22385865478034	131044
2a9aef56aa259ccf706202ac042def1df8339c63	urdu word segmentation	rule based;maximum matching;word segmentation;error detection	Word Segmentation is the foremost obligatory task in almost all the NLP applications where the initial phase requires tokenization of input into words. Urdu is amongst the Asian languages that face word segmentation challenge. However, unlike other Asian languages, word segmentation in Urdu not only has space omission errors but also space insertion errors. This paper discusses how orthographic and linguistic features in Urdu trigger these two problems. It also discusses the work that has been done to tokenize input text. We employ a hybrid solution that performs an n-gram ranking on top of rule based maximum matching heuristic. Our best technique gives an error detection of 85.8% and overall accuracy of 95.8%. Further issues and possible future directions are also discussed.	error detection and correction;foremost;heuristic;lexical analysis;matching (graph theory);n-gram;natural language processing;orthographic projection;regular expression;text segmentation;tokenization (data security)	Nadir Durrani;Sarmad Hussain	2010			rule-based system;natural language processing;text segmentation;error detection and correction;speech recognition;computer science;pattern recognition;matching	NLP	-25.220533684137227	-74.00425013715422	131174
9254f8d53fdabb07cb6fc9b521882f256f8e88ed	generative modeling of coordination by factoring parallelism and selectional preferences		We present a unified generative model of coordination that considers parallelism of conjuncts and selectional preferences. Parallelism of conjuncts, which frequently characterizes coordinate structures, is modeled as a synchronized generation process in the generative parser. Selectional preferences learned from a large web corpus provide an important clue for resolving the ambiguities of coordinate structures. Our experiments of Japanese dependency parsing indicate the effectiveness of our approach, particularly in the domains of newspapers and patents.	experiment;generative modelling language;generative model;integer factorization;parallel computing;parsing expression grammar;text corpus	Daisuke Kawahara;Sadao Kurohashi	2011			natural language processing;speech recognition;computer science;theoretical computer science;machine learning;algorithm	NLP	-20.65370199284316	-73.41780714146383	131208
6f554e35620ac0f411c387806d34dbc1f2f9f70c	learning clusters of bilingual suffixes using bilingual translation lexicon		By learning bilingual suffixation operations from translations using an existing bilingual lexicon with near translation forms we can improve its coverage and hence deal with the OOV entries. From this perspective, we identify bilingual stems, their bilingual morphological extensions bilingual suffixes and subsequently clusters of bilingual suffixes using known translation forms seen in an existing bilingual translation lexicon. We rely on clustering to enable safer translation generalisations. The degree of co-occurrence between two bilingual morphological extensions with reference to common bilingual stems determines if each of them should fall in the same cluster. Results are discussed for language pairs English-Portuguese EN-PT and English-Hindi EN-HI.	bilingual dictionary;lexicon	K. M. Kavitha;Luís Gomes;José Gabriel Pereira Lopes	2015		10.1007/978-3-319-26832-3_57	natural language processing;speech recognition;computer science;linguistics	NLP	-25.509521205759096	-75.4355888800087	132990
318d501ab5a164a9fd7ac5e2a8e33ceee22c9fe8	sentence segmentation and disfluency detection in narrative transcripts from neuropsychological tests		Natural Language Processing (NLP) tools aiming at the diagnosis of language impairing dementias generally extract several textual metrics of narrative transcripts. However, the absence of sentence boundary segmentation in transcripts prevents the direct application of NLP methods which rely on these marks to work properly, such as taggers and parsers. We present a method to segment the transcripts into sentences and another to detect the disfluencies present in them, to serve as a preprocessing step for the application of subsequent NLP tools. Our methods use recurrent convolutional neural networks with prosodic, morphosyntactic features, and word embeddings. We evaluated both tasks intrinsically, analyzing the most important features, comparing the proposed methods to simpler ones, and identifying the main hits and misses. In addition, a final method was created to combine all tasks and it was evaluated extrinsically using 9 syntactic metrics of Coh-Metrix-Dementia. In the intrinsic evaluations, we showed that our method achieved (i) state-of-the-art results for the sentence segmentation task on impaired speech, and (ii) results that are similar to related works for the English language for disfluency detection tasks. Regarding the extrinsic evaluation, only 3 metrics showed a statistically significant difference between manual MCI transcripts and those generated by our method, suggesting that our method is capable to preprocess transcriptions to be further analyzed by NLP tools.		Marcos Vinícius Treviso;Sandra M. Aluísio	2018		10.1007/978-3-319-99722-3_41	convolutional neural network;natural language processing;transcription (linguistics);parsing;preprocessor;syntax;narrative;sentence;computer science;segmentation;artificial intelligence	NLP	-25.91268587252149	-74.70262853732639	133225
f3b2fd0a89341087385540775997e93fd02f0367	strategies of processing japanese names and character variants in traditional chinese text	判解;chien wei pao;chuan jie lin;yen heng chen;法律詞典;character variants;論文;大陸法學;法規;月旦法學;法律題庫;裁判時報;semantic chinese word segmentation;月旦知識庫;法學資料庫;jia cheng zhan;tssci;教學;japanese name identification	This paper proposes an approach to identify word candidates that are not Traditional Chinese, including Japanese names (written in Japanese Kanji or Traditional Chinese characters) and word variants, when doing word segmentation on Traditional Chinese text. When handling personal names, a probability model concerning formats of names is introduced. We also propose a method to map Japanese Kanji into the corresponding Traditional Chinese characters. The same method can also be used to detect words written in character variants. After integrating generation rules for various types of special words, as well as their probability models, the F-measure of our word segmentation system rises from 94.16% to 96.06%. Another experiment shows that 83.18% of the 862 Japanese names in a set of 109 human-annotated documents can be successfully detected.	bigram;chien search;computation;computational complexity theory;computational linguistics;conditional random field;entity–relationship model;group identifier;hash table;heuristic (computer science);information processing;information retrieval;international joint conference on artificial intelligence;japanese input methods;keyword extraction;lu decomposition;language model;machine learning;message understanding conference;microsoft word for mac;named entity;network interface device;sequence labeling;smoothing;statistical model;syllable;text segmentation;utf-8;wikipedia	Chuan-Jie Lin;Jia-Cheng Zhan;Yen-Heng Chen;Chien-Wei Pao	2012	IJCLCLP		psychology;arithmetic;speech recognition;kanji;linguistics	NLP	-24.63434118684774	-78.3401966946766	133506
3614464a6a1ae5d56c1a7537cf80e55aee677b50	using eigenvectors of the bigram graph to infer morpheme identity	automatic learning;bigram graph;unsupervised fashion;raw corpus;certain point;statistical technique;consistent syntactic function;unsupervised learning;morpheme identity;brown et;syntactic category;statistical method;nearest neighbor graph;eigenvectors;derived category;noun	This paper describes the results of some experiments exploring statistical methods to infer syntactic categories from a raw corpus in an unsupervised fashion. It shares certain points in common with Brown et at (1992) and work that has grown out of that: it employs statistical techniques to derive categories based on what words occur adjacent to a given word. However, we use an eigenvector decomposition of a nearest-neighbor graph to produce a two-dimensional rendering of the words of a corpus in which words of the same syntactic category tend to form clusters and neighborhoods. We exploit this technique for extending the value of automatic learning of morphology. In particular, we look at the suffixes derived from a corpus by unsupervised learning of morphology, and we ask which of these suffixes have a consistent syntactic function (e.g., in English, -ed is primarily a mark of verbal past tense, does but –s marks both noun plurals and 3 person present on verbs).	bigram;download;experiment;galaxy morphological classification;graphical user interface;heuristic;inferring horizontal gene transfer;mathematical morphology;telephone exchange;text corpus;unsupervised learning	Mikhail Belkin;John A. Goldsmith	2002	CoRR		natural language processing;unsupervised learning;noun;nearest neighbor graph;speech recognition;eigenvalues and eigenvectors;machine learning;pattern recognition;linguistics;derived category	NLP	-25.09864498844218	-79.81293365992875	134008
d9b929b03c69f41f621f2a0e049fdc09f508e39a	investigation on language modelling approaches for open vocabulary speech recognition		By definition, words that are not present in a recognition vocabulary are called out-of-vocabulary (OOV) words. Recognition of unseen or new words is an important feature that is always desired in any real-world large vocabulary continuous speech recognition (LVCSR) system. However, human languages are complex in nature due to wide varieties of morphological richness such as inflections, derivations and compounding. For instance, language models for morphologically rich languages like German, Polish, Slovene, etc, often have high OOV rates, data sparsity and rather poor generalization of unseen sequences. In spite of the substantial amount of work that has been carried out to recognize unseen words in recent decades, many issues related to open vocabulary problem still exist, especially, under large vocabulary conditions. This dissertation addresses some of the core issues and makes an attempt to solve them by investigating and introducing different types of hybrid and hierarchical language models, supported by detailed experimental analysis. Careful selection of sub-word unit is necessary in a hybrid language model, as it has a large impact on OOV rate, data sparsity and recognition issues. Different types of sub-word unit, such as morphemes, syllables and graphones are investigated on selected morphologically rich languages. The traditional hybrid approach uses only sub-words, which is not robust in-terms of reducing word error rates on large vocabulary tasks. This work investigates different types of count-based hybrid language models. One method is to use an optimal number of full words and sub-words. Further extensions include the use of an optimal number of full words, sub-words and sub-word graphones based on word frequencies. The advantage of using two or three different types of units in a hybrid language model is that it helps improve recognition of OOVs and also compensates for weaker contexts, and reduces data sparsity to some extent. In addition, this work also investigates maximum entropy and long short-term memory network hybrid language models. A maximum entropy approach is combined within class-based language modelling framework. Additionally, novel extensions are proposed in the hierarchical language modelling approach, where a full word language model and a character level language model are directly used during decoding in a hierarchical manner to recognize in-vocabulary and OOV words, respectively, for LVCSR tasks. Sequence normalization using a prefix tree approach is applied to hierarchical language models. Variants of the hierarchical approach are introduced by incorporating weighted and non-weighted character language models, multi class character language models, and grapheme to phoneme models. These types of language model guarantee zero OOV rate. Alternatively, a properly normalized combined interpolated language model is introduced that also uses a full word language model and a character level language model during decoding, exploiting within word context or across word context at a character level for OOV recognition.		M. Ali Basha Shaik	2016			speech recognition;vocabulary;computer science;on language	NLP	-21.328847925709287	-79.39031356723073	134308
241a66e0baf3c6afe863ffe7c9637d2be64247bb	bidirectional decoding for statistical machine translation	statistical machine translation;english-to-japanese translation;englith-to-japanese translation;bidirectional decoding;right-to-left decoding method;english translation;input string;bidirectional decoding method;right-to-left direction;bidirectional method;japanese-to-english translation	This paper describes the right-to-left decoding method, which translates an input string by generating in right-to-left direction. In addition, presented is the bidirectional decoding method, that can take both of the advantages of left-to-right and right-to-left decoding method by generating output in both ways and by merging hypothesized partial outputs of two directions. The experimental results on Japanese and English translation showed that the right-to-left was better for Englith-to-Japanese translation, while the left-to-right was suitable for Japanese-to-English translation. It was also observed that the bidirectional method was better for English-to-Japanese translation.	bi-directional text;bidirectional transformation;decoding methods;right-to-left;statistical machine translation;text corpus	Taro Watanabe;Eiichiro Sumita	2002			speech recognition;transfer-based machine translation;computer science;theoretical computer science;algorithm	NLP	-21.72151944522603	-77.9083415069401	134492
1f05380f9d149a5a50eb71076f4f01629d548a10	unsupervised topic modelling for multi-party spoken discourse	bayesian inference;previous unsupervised segmentation-only method;topic segmentation;unsupervised topic;human judge;unsupervised topic modelling;generative model;multi-party meeting;document classification;unsegmented multi-party discourse transcript;topic identification;speech recognition	Syntactic knowledge is important for pronoun resolution. Traditionally, the syntactic information for pronoun resolution is represented in terms of features that have to be selected and defined heuristically. In the paper, we propose a kernel-based method that can automatically mine the syntactic information from the parse trees for pronoun resolution. Specifically, we utilize the parse trees directly as a structured feature and apply kernel functions to this feature, as well as other normal features, to learn the resolution classifier. In this way, our approach avoids the efforts of decoding the parse trees into the set of flat syntactic features. The experimental results show that our approach can bring significant performance improvement and is reliably effective for the pronoun resolution task.	ace;anaphora (linguistics);heuristic (computer science);kernel (operating system);parse tree;parsing	Matthew Purver;Konrad P. Körding;Thomas L. Griffiths;Joshua B. Tenenbaum	2006			natural language processing;speech recognition;computer science;pattern recognition;bayesian inference	NLP	-22.241591548003164	-74.71614449009164	134712
6428b8940fe8d44f6519c0d8ae8a3c41842e3c84	dependencias no dirigidas para el análisis basado en transiciones	syntax;analisis sintactico;computacion informatica;filologias;dependency grammar;info eu repo semantics article;informacion documentacion;parsing;gramaticas de dependencias;linguistica;analisis sintactico de dependencias;ciencias basicas y experimentales;dependency parsing;sintaxis;grupo a;ciencias sociales;grupo b	In this paper we introduce a new approach to transition-based dependency parsing. We propose that the parser construct an undirected graph during the parsing process, instead of a standard directed dependency structure. A posteriori, the output undirected structure is converted into a dependency tree. This alleviates error propagation, a characteristic problem of these systems. We apply this approach to obtain undirected variants of the Planar and 2-Planar parsers and of Covington’s non-projective parser. We perform experiments on several treebanks from the CoNLL-X shared task, showing that these variants outperform the original directed algorithms in most of the cases.	algorithm;experiment;graph (discrete mathematics);parsing;propagation of uncertainty;software propagation;treebank	Carlos Gómez-Rodríguez;Daniel Fernández-González	2012	Procesamiento del Lenguaje Natural		computer science;linguistics;algorithm;dependency grammar	NLP	-26.230724836840846	-78.33511939429054	135150
787837d2984bf7f78535467f087e6618ae8ecc87	english multiword expression-aware dependency parsing including named entities		Because syntactic structures and spans of multiword expressions (MWEs) are independently annotated in many English syntactic corpora, they are generally inconsistent with respect to one another, which is harmful to the implementation of an aggregate system. In this work, we construct a corpus that ensures consistency between dependency structures and MWEs, including named entities. Further, we explore models that predict both MWEspans and an MWE-aware dependency structure. Experimental results show that our joint model using additional MWEspan features achieves an MWE recognition improvement of 1.35 points over a pipeline model.	aggregate data;dependency grammar;experiment;expression (computer science);minimal working example;named entity;named-entity recognition;parsing expression grammar;text corpus	Akihiko Kato;Hiroyuki Shindo;Yuji Matsumoto	2017		10.18653/v1/P17-2068	machine learning;computer science;artificial intelligence;natural language processing;multiword expression;dependency grammar;s-attributed grammar	NLP	-24.13548139716753	-74.4920910911532	136740
221a21e0223bcb0c825506e3e245858e491d824c	working with a small dataset - semi-supervised dependency parsing for irish	irish language;computational linguistics;language parsing	We present a number of semi-supervised parsing experiments on the Irish language carried out using a small seed set of manually parsed trees and a larger, yet still relatively small, set of unlabelled sentences. We take two popular dependency parsers – one graph-based and one transition-based – and compare results for both. Results show that using semisupervised learning in the form of self-training and co-training yields only very modest improvements in parsing accuracy. We also try to use morphological information in a targeted way and fail to see any improvements.	co-training;experiment;parsing;semi-supervised learning;semiconductor industry	Teresa Lynn;Jennifer Foster;Mark Dras	2013			natural language processing;parser combinator;speech recognition;top-down parsing language;computer science;bottom-up parsing;s-attributed grammar;linguistics;top-down parsing	NLP	-22.470126111977965	-75.49625443273713	136943
f1f9ecc38bbd3251e2152fe5fafa4773b51f6131	о влиянии семантики на точность определения парафраз в русскоязычных текстах (effect of semantic parsing depth on the identification of paraphrases in russian texts)		As a tool to solve the problem of identification of paraphrases in Russian texts, the paper proposes the semantic-syntactic parser SemSin and a semantic classifier. Several alternative methods for evaluating the similarity of sentence pairs – by words, by lemmas, by classes, by semantically related concepts, by predicate groups – have been analyzed. Advantages and drawbacks of the methods are discussed. The paraphrase identification quality has been shown to rise with increasing depth of using the semantic information. Yet, complementing the analysis with predicate groups, identified by the dependency tree, may even cause the identification to degrade due to the growing number of false positive decisions.	parsing	Kirill Boyarsky;Eugeny Kanevsky	2017			parsing;mathematics;linguistics	AI	-25.735488698863463	-73.62082399861563	137303
f0a82f03436735733bdecf602974822a38e94a90	data pre-processing to train a better lithuanian-english mt system		Pried -as ir Protokol -as yr -a neatskiriam -a ši -o Susitar -imo dal -is. Prefixes separated, endings replaced by tense and number feature values System #2 Prefixes separated, all endings replaced by number feature values and verb endings also by time feature values System #3 Prefixes separated, endings deleted System #4 As Lithuanian is highly inflected language, the words change the form according to grammatical function. That means that the endings of nouns, pronouns, adjectives, numerals and verbs change depending on certain features. English instead does not have such a rich feature system. This difference between languages significantly impacts word and phrase alignment when training an SMT system. Typically one or two forms of an English word have to be aligned to more than ten different surface forms of a corresponding Lithuanian word. Lithuanian verbs have prefixes indicating negation and other semantic features while English verbs do not have prefixes and such information is expressed using modifying words. Many word forms are not as common as others in the corpus, therefore a Lithuanian-English SMT system does not translate all word forms equally well. It is very common to get many out of vocabulary words when translating from Lithuanian into English. Chosen approach	command & conquer:yuri's revenge;data pre-processing;vocabulary	Daiga Deksne;Raivis Skadins	2012		10.3233/978-1-61499-133-5-36	natural language processing;data pre-processing;lithuanian;artificial intelligence;vocabulary;computer science	NLP	-26.073077993636044	-76.291012208035	137767
641d5ef16add7242ed50a544328436a6536a89ec	a transition-based directed acyclic graph parser for ucca		We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.	adaptive multi-rate audio codec;directed acyclic graph;machine translation;naive bayes classifier;parser;randomness extractor;reentrancy (computing);semantic web;text simplification	Daniel Hershcovich;Omri Abend;Ari Rappoport	2017		10.18653/v1/P17-1104	programming language;reentrancy;directed acyclic graph;computer science;parsing;graph;annotation	NLP	-20.498193047597926	-74.69808668222633	137845
0fc5ecf76fd48eed8c114f39d7855a9860dcaa91	the unsupervised acquisition of a lexicon from continuous speech	unsupervised learning;syntax;data compression;phrase structure grammars;ai;text processing;efficiency;speech analysis;vocabulary;words language;acoustic signals;data bases;learning machines;hierarchies;computer programs;language acquisition;hierarchical representation;context sensitive grammars;induction;mathematical models;lexical acquisition;lexicography;continuous speech;natural language;word recognition;input output processing;pattern recognition;speech recognition;artificial intelligence;algorithms;optimization;phonetics;computational linguistics;mit;data acquisition;speech compression;language model;grammar induction;machine translation;phonemes	We present an unsupervised learning algorithm that acquires a natural-language lexicon from raw speech. The algorithm is based on the optimal encoding of symbol sequences in an MDL framework, and uses a hierarchical representation of language that overcomes many of the problems that have stymied previous grammar-induction procedures. The forward mapping from symbol sequences to the speech stream is modeled using features based on articulatory gestures. We present results on the acquisition of lexicons and language models from raw speech, text, and phonetic transcripts, and demonstrate that our algorithm compares very favorably to other reported results with respect to segmentation performance and statistical eeciency.	algorithm;grammar induction;language model;lexicon;mdl (programming language);natural language;unsupervised learning	Carl de Marcken	1995	CoRR		data compression;language acquisition;natural language processing;phonetics;unsupervised learning;speech recognition;syntax;grammar induction;word recognition;computer science;computational linguistics;mathematical model;lexicography;linguistics;efficiency;machine translation;natural language;data acquisition;hierarchy;language model	NLP	-22.720561487597738	-79.11550630768174	138148
c880ed5ef0870890762c1e76203118559c254749	wolvesaar at semeval-2016 task 1: replicating the success of monolingual word alignment and neural embeddings for semantic textual similarity.		This paper describes the WOLVESAAR systems that participated in the English Semantic Textual Similarity (STS) task in SemEval2016. We replicated the top systems from the last two editions of the STS task and extended the model using GloVe word embeddings and dense vector space LSTM based sentence representations. We compared the difference in performance of the replicated system and the extended variants. Our variants to the replicated system show improved correlation scores and all of our submissions outperform the median scores from all participating systems.	data structure alignment;long short-term memory;semeval;stellar (payment network)	Hanna Béchara;Rohit Gupta;Liling Tan;Constantin Orasan;Ruslan Mitkov;Josef van Genabith	2016		10.18653/v1/S16-1096	natural language processing;speech recognition;linguistics	NLP	-21.33081861495067	-75.07864723826746	138439
33e5a26842bcc37f5333bf808b2b6c9f56151d80	partial matching strategy for phrase-based statistical machine translation	training corpus;bilingual corpus;source phrase;partial matching strategy;data sparseness problem;state-of-the-art pbsmt system moses;word substitution;significant improvement;large corpus;phrase-based statistical machine translation;statistical significance	This paper presents a partial matching strategy for phrase-based statistical machine translation (PBSMT). Source phrases which do not appear in the training corpus can be translated by word substitution according to partially matched phrases. The advantage of this method is that it can alleviate the data sparseness problem if the amount of bilingual corpus is limited. We incorporate our approach into the state-of-the-art PBSMT system Moses and achieve statistically significant improvements on both small and large corpora.	bitext word alignment;microsoft word for mac;moses;neural coding;statistical machine translation;text corpus	Zhongjun He;Qun Liu;Shouxun Lin	2008			natural language processing;speech recognition;example-based machine translation;computer science;pattern recognition;statistical significance;machine translation	NLP	-21.498366680149083	-76.51233736101179	138966
2afa70bd8c3bdb922586924db306941b0b1656db	multimodal comparable corpora as resources for extracting parallel data: parallel phrases extraction		Discovering parallel data in comparable corpora is a promising approach for overcoming the lack of parallel texts in statistical machine translation and other NLP applications. In this paper we propose an alternative to comparable corpora of texts as resources for extracting parallel data: a multimodal comparable corpus of audio and texts. We present a novel method to detect parallel phrases from such corpora based on splitting comparable sentences into fragments, called phrases. The audio is transcribed by an automatic speech recognition system, split into fragments and translated with a baseline statistical machine translation system. We then use information retrieval in a large text corpus in the target language, split also into fragments, and extract parallel phrases. We compared our method with parallel sentences extraction techniques. We evaluate the quality of the extracted data on an English to French translation task and show significant improvements over a state-ofthe-art baseline.	baseline (configuration management);compiler;experiment;information retrieval;multimodal interaction;natural language processing;redundancy (engineering);speech recognition;statistical machine translation;streaming media;text corpus	Haithem Afli;Loïc Barrault;Holger Schwenk	2013			natural language processing;speech recognition;computer science;information retrieval	NLP	-22.384335405730678	-76.47448465940799	139376
026286be21945cd2c55dedbebe2c1ffc68c8eb9f	a deep-parsing approach to natural language understanding in dialogue system: results of a corpus-based evaluation	syntax;deep parsing;natural language understanding;semantic;ontology	This paper presents an approach to dialogue underst anding based on a deep parsing and rule-based seman tic analysis. Its performance in the semantic evaluation performed in the framew ork of the EVALDA/MEDIA campaign is encouraging. T he MEDIA project aims to evaluate natural language understanding systems for French on a hotel reservation task (Devillers t al., 2004). For the evaluation, five participating teams had to produce an annotat ed version of the input utterances in compliance w ith a commonly agreed format (the MEDIA formalism). An approach based on symbol ic processing was not straightforward given the co nditions of the evaluation but we achieved a score close to that of statistic al systems, without needing an annotated corpus. De pite the architecture has been designed for this campaign, exclusively dedicated to spoken dialogue understanding, we believe that our approach based on a LTAG parser and two ontologies can be used in real dial ogue systems, providing quite robust speech unders tanding and facilities for interfacing with a dialogue manager and the applic ation itself.	dialog system;dialog tree;logic programming;natural language understanding;ontology (information science);ork;parsing;semantics (computer science)	Alexandre Denis;Matthieu Quignard;Guillaume Pitel	2006			natural language processing;speech recognition;syntax;computer science;ontology;linguistics	NLP	-25.45881194297406	-75.98571686872893	139582
32d3e513cbf33f43e91a7145055409091ac31fad	density-driven cross-lingual transfer of dependency parsers		We present a novel method for the crosslingual transfer of dependency parsers. Our goal is to induce a dependency parser in a target language of interest without any direct supervision: instead we assume access to parallel translations between the target and one or more source languages, and to supervised parsers in the source language(s). Our key contributions are to show the utility of dense projected structures when training the target language parser, and to introduce a novel learning algorithm that makes use of dense structures. Results on several languages show an absolute improvement of 5.51% in average dependency accuracy over the state-of-the-art method of (Ma and Xia, 2014). Our average dependency accuracy of 82.18% compares favourably to the accuracy of fully supervised methods.	algorithm;bloomberg terminal;compiler;iterative method;knowledge engineering;parser combinator;parsing;supervised learning	Mohammad Sadegh Rasooli;Michael Collins	2015			natural language processing;speech recognition;computer science;programming language;lr parser	NLP	-19.774168405988533	-75.44227664023087	139606
66b64087bfd9a59805a24ec06add72dac6e86fc9	exploring the effects of word roots for arabic sentiment analysis		The inherent morphological complexity of languages such as Arabic entails the exploration of language traits that could be valuable to the task of detecting and classifying sentiment within text. This paper investigates the relevance of using the roots of words as input features into a sentiment analysis system under two distinct domains, in order to tailor the task more suitably to morphologically-rich languages such as Arabic. Different wordrooting solutions are employed in conjunction with a basic sentiment classifier, in order to demonstrate the potential of mapping Arabic words to basic roots for a language-specific development to the sentiment analysis task, showing a noteworthy improvement to baseline performance.	baseline (configuration management);galaxy morphological classification;library (computing);naive bayes classifier;natural language processing;relevance;roots;sensor;sentiment analysis	Shereen Oraby;Yasser El-Sonbaty;Mohamad Abou El-Nasr	2013			natural language processing;speech recognition;computer science;sentiment analysis	NLP	-25.43840429523087	-73.20896954317367	139755
0607e7d8188437136bcda06625c31f69e954c1b6	a structured language model	structured language model;long distance;language model	The paper presents a language model that develops syntactic structure and uses it to extract meaningful information from the word history, thus enabling the use of long distance dependencies. The model assigns probability to every joint sequence of words–binary-parse-structure with headword annotation. The model, its probabilistic parametrization, and a set of experiments meant to evaluate its predictive power are presented.	experiment;headword;language model;parsing	Ciprian Chelba	1997		10.3115/976909.979681	natural language processing;cache language model;speech recognition;data control language;computer science;machine learning;language model	NLP	-23.168961177070347	-78.75766244012438	139889
9d92735a9ae85d985bf18ffb903cb79118406250	go climb a dependency tree and correct the grammatical errors		State-of-art systems for grammar error correction often correct errors based on word sequences or phrases. In this paper, we describe a grammar error correction system which corrects grammatical errors at tree level directly. We cluster all error into two groups and divide our system into two modules correspondingly: the general module and the special module. In the general module, we propose a TreeNode Language Model to correct errors related to verbs and nouns. The TreeNode Language Model is easy to train and the decoding is efficient. In the special module, two extra classification models are trained to correct errors related to determiners and prepositions. Experiments show that our system outperforms the state-of-art systems and improves the F1 score.	error detection and correction;experiment;f1 score;language model;parsing;test set;treebank	Longkai Zhang;Houfeng Wang	2014			natural language processing;computer science;machine learning;linguistics;algorithm	NLP	-23.462726876235852	-75.90606999690988	140305
29c7fb312431f77aa369937959f26f292dfa27d6	an effective two-stage model for exploiting non-local dependencies in named entity recognition	tractable inference;ner system;error reduction;non-local dependency;approach yield;approximate inference;relative error reduction;inference time;effective two-stage model;entity recognition;state-of-the-art ner system;non-local information	This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efficient. NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text. We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF. Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-theart NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information. Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone. Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference.	algorithmic efficiency;approximation algorithm;approximation error;cobham's thesis;conditional random field;f1 score;graphical model;inference engine;named entity;named-entity recognition;quantum nonlocality;resultant;time complexity	Vijay Krishnan;Christopher D. Manning	2006			natural language processing;computer science;machine learning;pattern recognition;data mining	NLP	-20.84015815967844	-75.75053475221492	141041
1dbcbeb40e83a864423347b445fb9b18f943002d	constrained semantic forests for improved discriminative semantic parsing		In this paper, we present a model for improved discriminative semantic parsing. The model addresses an important limitation associated with our previous stateof-the-art discriminative semantic parsing model – the relaxed hybrid tree model by introducing our constrained semantic forests. We show that our model is able to yield new state-of-the-art results on standard datasets even with simpler features. Our system is available for download from http://statnlp.org/research/sp/.	download;experiment;mark steedman;n-gram;parsing;semantic analysis (machine learning);strongly regular graph	Wei Lu	2015			semantic computing;computer science	NLP	-21.412096842402534	-74.1837675258403	141158
2648926c90f6cc9c9fa2e9590bc9b3df88ac9cd2	training conditional random fields with multivariate evaluation measures	conditional random field;segmentation f-score;proposed framework;sequential segmentation task;evaluation measure;standard crf training;multivariate evaluation measure;entity recognition;non-linear measure;target evaluation measure;conditional random fields	This paper proposes a framework for training Conditional Random Fields (CRFs) to optimize multivariate evaluation measures, including non-linear measures such as F-score. Our proposed framework is derived from an error minimization approach that provides a simple solution for directly optimizing any evaluation measure. Specifically focusing on sequential segmentation tasks, i.e. text chunking and named entity recognition, we introduce a loss function that closely reflects the target evaluation measure for these tasks, namely, segmentation F-score. Our experiments show that our method performs better than standard CRF training.	approximation algorithm;conditional random field;experiment;f1 score;linuxmce;loss function;mathematical optimization;named-entity recognition;nonlinear system;shallow parsing	Jun Suzuki;Erik McDermott;Hideki Isozaki	2006			computer science;machine learning;pattern recognition;data mining;conditional random field	Vision	-20.06140507683238	-76.86036263529797	142694
21fce186f2aeace4e0f90531038e043b0e451241	semantic translation error rate for evaluating translation systems	software metrics;statistical machine translation automated metric;performance evaluation;automated metric;software metrics error statistics language translation performance evaluation;statistical machine translation;meteor semantic translation error rate translation systems evaluation machine translation systems word equivalence measures wordnet porter stemming ster alignments human driven analysis;language translation;statistical properties;error analysis humans costs natural languages surface mount technology information technology web sites globalization investments information analysis;error rate;error statistics;machine translation	In this paper, we introduce a new metric which we call the semantic translation error rate, or STER, for evaluating the performance of machine translation systems. STER is based on the previously published translation error rate (TER) (Snover et al., 2006) and METEOR (Banerjee and Lavie, 2005) metrics. Specifically, STER extends TER in two ways: first, by incorporating word equivalence measures (WordNet and Porter stemming) standardly used by METEOR, and second, by disallowing alignments of concept words to non-concept words (aka stop words). We show how these features make STER alignments better suited for human-driven analysis than standard TER. We also present experimental results that show that STER is better correlated to human judgments than TER. Finally, we compare STER to METEOR, and illustrate that METEOR scores computed using the STER alignments have similar statistical properties to METEOR scores computed using METEOR alignments.	algorithm;machine translation;meteor;semantic similarity;semantic translation;stemming;turing completeness;wordnet	Krishna Subramanian;David Stallard;Rohit Prasad;Shirin Saleem;Premkumar Natarajan	2007	2007 IEEE Workshop on Automatic Speech Recognition & Understanding (ASRU)	10.1109/ASRU.2007.4430144	natural language processing;speech recognition;word error rate;computer science;theoretical computer science;machine learning;machine translation;rule-based machine translation;machine translation software usability;software metric	NLP	-22.620421358503204	-77.8705279158501	142755
1c61cb18b15a02cbb946eccc99a5bcab470c422b	marathi parts-of-speech tagger using supervised learning	supervised learning;part of speech	In this paper, we present a parts-of-speech tagger for inflectional and derivational morphologically rich language Marathi. Marathi is spoken by the native people of Maharashtra. The general approach used for the development of tagger is statistical-based hidden Markov model (HMM). We establish a methodology of parts-of-speech (POS) tagging for Marathi using HMM. The main concept of HMM is to calculate probabilities to determine which is the best sequence of tags that correspond to observation sequence of words. In this paper, we show the development of the tagger. Moreover, we have also shown the evaluation done.		Jyoti Singh;Nisheeth Joshi;Iti Mathur	2013		10.1007/978-81-322-1665-0_24	unsupervised learning;pattern recognition	ML	-24.4535914730744	-78.39221173971798	142831
32f827a388c63038997249d03375905de694a971	delexicalized word embeddings for cross-lingual dependency parsing		This paper presents a new approach to the problem of cross-lingual dependency parsing, aiming at leveraging training data from different source languages to learn a parser in a target language. Specifically, this approach first constructs word vector representations that exploit structural (i.e., dependency-based) contexts but only considering the morpho-syntactic information associated with each word and its contexts. These delexicalized word embeddings, which can be trained on any set of languages and capture features shared across languages, are then used in combination with standard language-specific features to train a lexicalized parser in the target language. We evaluate our approach through experiments on a set of eight different languages that are part the Universal Dependencies Project. Our main results show that using such delexicalized embeddings, either trained in a monolingual or multilingual fashion, achieves significant improvements over monolingual baselines.	algorithm;baseline (configuration management);biological anthropology;cluster analysis;compiler;experiment;lexicon;parsing;principal component analysis;word embedding	Pascal Denis;Mathieu Dehouck	2017			machine learning;computer science;artificial intelligence;word embedding;natural language processing;dependency grammar;parsing;training set;exploit	NLP	-20.317353700667706	-73.8915717682758	142891
1711adbd0b62a5d564cb69e992dfa80b652ef56b	improved parsing and pos tagging using inter-sentence consistency constraints	mst parser;training data;consistency constraint;pos taggers;in-domain data;state-of-the-art statistical parsers;inter-sentence consistency constraint;pos tagging;available training data;stanford part-of-speech tagger;improved parsing	State-of-the-art statistical parsers and POS taggers perform very well when trained with large amounts of in-domain data. When training data is out-of-domain or limited, accuracy degrades. In this paper, we aim to compensate for the lack of available training data by exploiting similarities between test set sentences. We show how to augment sentencelevel models for parsing and POS tagging with inter-sentence consistency constraints. To deal with the resulting global objective, we present an efficient and exact dual decomposition decoding algorithm. In experiments, we add consistency constraints to the MST parser and the Stanford part-of-speech tagger and demonstrate significant error reduction in the domain adaptation and the lightly supervised settings across five languages.	algorithm;brill tagger;domain adaptation;experiment;lagrangian relaxation;markov random field;parsing;part-of-speech tagging;test set	Alexander M. Rush;Roi Reichart;Michael Collins;Amir Globerson	2012			speech recognition;computer science;machine learning;pattern recognition;data mining	NLP	-20.772560843596736	-76.09123630834627	143033
a38c474d44c4ebf86fec6c1bb8dc16e4bc76d5b8	overview for the second shared task on language identification in code-switched data		We present an overview of the second shared task on language identification in codeswitched data. For the shared task, we had code-switched data from two different language pairs: Modern Standard ArabicDialectal Arabic (MSA-DA) and SpanishEnglish (SPA-ENG). We had a total of nine participating teams, with all teams submitting a system for SPA-ENG and four submitting for MSA-DA. Through evaluation, we found that once again language identification is more difficult for the language pair that is more closely related. We also found that this year’s systems performed better overall than the systems from the previous shared task indicating overall progress in the state of the art for this task.	algorithm;baseline (configuration management);cs games;cs-blast;conditional random field;deep learning;electronic news-gathering;formal language;language identification;machine learning;sequence labeling;text corpus	Giovanni Molina;Fahad AlGhamdi;Mahmoud Ghoneim;Abdelati Hawwari;Nicolas Rey-Villamizar;Mona T. Diab;Thamar Solorio	2016		10.18653/v1/W16-5805	natural language processing;speech recognition;computer science;communication	NLP	-23.46312838633424	-75.29992729209488	143191
5c19403ebccc5b7ccf95779ec393a98227f61220	building multiword expressions bilingual lexicons for domain adaptation of an example-based machine translation system		We describe in this paper a hybrid approach to build automatically bilingual lexicons of Multiword Expressions (MWEs) from parallel corpora. We more specifically investigate the impact of using a domain-specific bilingual lexicon of MWEs on domain adaptation of an Example-Based Machine Translation (EBMT) system. We conducted experiments on the English-French language pair and two kinds of texts: in-domain texts from Europarl (European Parliament proceedings) and out-of-domain texts from Emea (European Medicines Agency documents) and Ecb (European Central Bank corpus). The obtained results indicate that integrating domain-specific bilingual lexicons of MWEs improves translation quality of the EBMT system when texts to translate are related to the specific domain and induces a relatively slight deterioration of translation quality when translating general-	artificial neural network;bilingual dictionary;domain adaptation;example-based machine translation;experiment;general-purpose markup language;language model;lexicon;moses;parallel text;perplexity;recurrent neural network;text corpus;vocabulary	Nasredine Semmar;Meriama Laïb	2017		10.26615/978-954-452-049-6_085	machine learning;natural language processing;artificial intelligence;domain adaptation;computer science;expression (mathematics);example-based machine translation	NLP	-23.1593202162055	-76.03898481706024	143223
67a38d69f5f2005bcf524bc38d80a501b523724d	neural machine translation for low-resource languages without parallel corpora		The problem of a total absence of parallel data is present for a large number of language pairs and can severely detriment the quality of machine translation. We describe a language-independent method to enable machine translation between a low-resource language (LRL) and a third language, e.g. English. We deal with cases of LRLs for which there is no readily available parallel data between the low-resource language and any other language, but there is ample training data between a closely-related high-resource language (HRL) and the third language. We take advantage of the similarities between the HRL and the LRL in order to transform the HRL data into data similar to the LRL using transliteration. The transliteration models are trained on transliteration pairs extracted from Wikipedia article titles. Then, we automatically back-translate monolingual LRL data with the models trained on the transliterated HRL data and use the resulting parallel corpus to train our final models. Our method achieves significant improvements in translation quality, close to the results that can be achieved by a general purpose neural machine translation system trained on a significant amount of parallel data. Moreover, the method does not rely on the existence of any parallel data for training, but attempts to bootstrap already existing resources in a related language.	booting;language-independent specification;neural machine translation;parallel text;programming language;text corpus;wikipedia	Alina Karakanta;Jon Dehdari;Josef van Genabith	2017	Machine Translation	10.1007/s10590-017-9203-5	artificial intelligence;natural language processing;computer science;machine translation;synchronous context-free grammar;transfer-based machine translation;transliteration;training set;example-based machine translation;rule-based machine translation	NLP	-22.767179628551986	-75.28933380159134	143580
b31ccd41140a06c53b6f5c9227ed40cc2944867b	a tabular method for island-driven context-free grammar parsing	context free grammar	Island-driven parsing is of great relevance for speech recognition/understanding and other natural language processing applications. A bidirectional algorithm is presented that efficiently solves this problem, allowing both any possible determination of the starting words in the input sentence and flexible control. In particular, a mixed bottom-to-top and top-down approach is followed, without leading to redundant. partial analyses. The algorithm performance is discussed.	context-free grammar;parsing;table (information)	Giorgio Satta;Oliviero Stock	1991			natural language processing;parser combinator;top-down parsing language;computer science;parsing;s-attributed grammar;context-free grammar;programming language;top-down parsing	NLP	-22.70922978750917	-78.64748557118942	143636
68d93931cf2e9806f101506b221ee8ac18afc734	automatic mt error analysis: hjerson helping addicter		We present a complex, open source tool for detailed machine translation error analysis providing the user with automatic error detection and classification, several monolingual alignment algorithms as well as with training and test corpus browsing. The tool is the result of a merge of automatic error detection and classification of Hjerson (Popović, 2011) and Addicter (Zeman et al., 2011) into the pipeline and web visualization of Addicter. It classifies errors into categories similar to those of Vilar et al. (2006), such as: morphological, reordering, missing words, extra words and lexical errors. The graphical user interface shows alignments in both training corpus and test data; the different classes of errors are colored. Also, the summary of errors can be displayed to provide an overall view of the MT system’s weaknesses. The tool was developed in Linux, but it was tested on Windows too.	algorithm;error analysis (mathematics);error detection and correction;graphical user interface;linux;machine translation;microsoft windows;open-source software;test data	Jan Berka;Ondrej Bojar;Mark Fishel;Maja Popovic;Daniel Zeman	2012			machine translation;merge (version control);speech recognition;error detection and correction;computer science;artificial intelligence;pattern recognition;visualization;test data;graphical user interface	NLP	-23.553624013257167	-76.38910396680474	144258
654eaf1024fcba4d27ec7e79926e1a903eee4c42	a combination of statistical and rule-based approach for mongolian lexical analysis	chinese mongolian machine translation;statistical approach;analytical models;pragmatics;rule based approach;probability;rule based;mongolian word segment;mongolian word segmentation;word level accuracy;joints;tree frame;statistical model;word segmentation;accuracy;tagging dictionaries joints accuracy pragmatics analytical models probability;joint segmentation and pos tagging;statistical analysis;part of speech tagging;dictionaries;information processing;identification technology;joint segmentation and pos tagging mongolian information processing mongolian word segment mongolian part of speech tagging;mongolian information processing;mongolian lexical analysis;mongolian part of speech tagging;word processing identification technology knowledge based systems natural language processing statistical analysis;natural language processing;lexical analysis;knowledge based systems;machine translation;word processing;word level accuracy mongolian lexical analysis mongolian information processing chinese mongolian machine translation statistical approach rule based approach mongolian word segmentation pos tagging tree frame;tagging;pos tagging	Mongolian lexical analysis is the first step in Mongolian information processing such as Chinese-Mongolian machine translation. In this paper, we introduce a statistic and rule based approach to solving the Mongolian word segmentation & POS tagging all at once. In this method, we use tree frame as basic statistical model. And then we combine the model with some rules to improve the lexical analysis system accuracy. The experiment results show that the word-level accuracy of joint segmentation and POS tagging is 95.2%, stem / postfix-level accuracy is 94.6%.	frame language;information processing;lexical analysis;machine translation;part-of-speech tagging;point of sale;statistical model;text segmentation	Lili Zhao;Jia Men;Congpin Zhang;Qun Liu;Wenbin Jiang;Jinxing Wu;Qing Chang	2010	2010 International Conference on Asian Language Processing	10.1109/IALP.2010.79	natural language processing;text segmentation;statistical model;speech recognition;information processing;lexical analysis;computer science;probability;accuracy and precision;linguistics;machine translation;statistics;pragmatics	NLP	-23.485266312992145	-77.21856326910245	144607
e0dda8a7cc783ed97de0c084e21e86ace27e067d	image-based natural language understanding using 2d convolutional neural networks		We propose a new approach to natural language understanding in which we consider the input text as an image and apply 2D Convolutional Neural Networks to learn the local and global semantics of the sentences from the variations of the visual patterns of words. Our approach demonstrates that it is possible to get semantically meaningful features from images with text without using optical character recognition and sequential processing pipelines, techniques that traditional Natural Language Understanding algorithms require. To validate our approach, we present results for two applications: text classification and dialog modeling. Using a 2D Convolutional Neural Network, we were able to outperform the stateof-art accuracy results of non-Latin alphabet-based text classification and achieved promising results for eight text classification datasets. Furthermore, our approach outperformed the memory networks when using out of vocabulary entities from task 4 of the bAbI dialog dataset.	algorithm;automatic summarization;color;computer vision;convolutional neural network;deep learning;dialog system;document classification;entity;language-independent specification;natural language generation;natural language processing;natural language understanding;optical character recognition;pipeline (computing);preprocessor;sentiment analysis;spell checker;stemming;text corpus;tokenization (data security);vocabulary;web colors;dialog	Erinc Merdivan;Anastasios Vafeiadis;Dimitrios Kalatzis;Lara Prien;Johannes Kropf;Konstantinos Votis;Dimitrios Giakoumis;Dimitrios Tzovaras;Liming Chen;Raouf Hamzaoui;Matthieu Geist	2018	CoRR		optical character recognition;machine learning;convolutional neural network;natural language processing;natural language understanding;semantics;artificial intelligence;computer science;dialog box;vocabulary;alphabet	NLP	-21.125842097609492	-73.51589552296223	144619
68f5be1e505823b0dab65f04348be558d4b7ff72	a technique to automatically assign parts-of-speech to words taking into account word-ending information through a probabilistic model	part of speech;probabilistic model		statistical model	Giulio Maltese;Federico Mancini	1991			artificial intelligence;part of speech;divergence-from-randomness model;speech recognition;machine learning;pattern recognition;computer science;probabilistic relevance model;statistical model	Logic	-24.990675668241273	-78.77192309266024	144989
e68042381b7cb5e7d0e7caa18ca3c964c3889b33	a framework for language-independent analysis and prosodic feature annotation of text corpora	noun;speech synthesis;noun phrase;hybrid method;part of speech tagging;natural language generation;part of speech;automatic classification;concept to speech	Concept-to-Speech systems include Natural Language Generators that produce linguistically enriched text descriptions which can lead to significantly improved quality of speech synthesis. There are cases, however, where either the generator modules produce pieces of non-analyzed, non-annotated plain text, or such modules are not available at all. Moreover, the language analysis is restricted by the usually limited domain coverage of the generator due to its embedded grammar. This work reports on a language-independent framework basis, linguistic resources and language analysis procedures (word/sentence identification, part-of-speech, prosodic feature annotation) for text annotation/processing for plain or enriched text corpora. It aims to produce an automated XMLannotated enriched prosodic markup for English and Greek texts, for improved synthetic speech. The markup includes information for both training the synthesizer and for actual input for synthesising. Depending on the domain and target, different methods may be used for automatic classification of entities (words, phrases, sentences) to one or more preset categories such as “emphatic event”, “new/old information”, “second argument to verb”, “proper noun phrase”, etc. The prosodic features are classified according to the analysis of the speechspecific characteristics for their role in prosody modelling and passed through to the synthesizer via an extended SOLE-ML description. Evaluation results show that using selectable hybrid methods for part-of-speech tagging high accuracy is achieved. Annotation of a large generated text corpus containing 50% enriched text and 50% canned plain text produces a fully annotated uniform SOLE-ML output containing all prosodic features found in the initial enriched source. Furthermore, additional automatically-derived prosodic feature annotation and speech synthesis related values are assigned, such as word-placement in sentences and phrases, previous and next word entity relations, emphatic phrases containing proper nouns, and more.	canned response;embedded system;entity;language-independent specification;markup language;natural language;part-of-speech tagging;semantic prosody;speech synthesis;synthetic intelligence;text corpus;xml	Dimitris Spiliotopoulos;Georgios Petasis;Georgios Kouroupetroglou	2008		10.1007/978-3-540-87391-4_66	natural language processing;noun;noun phrase;speech recognition;part of speech;computer science;linguistics;speech synthesis	NLP	-26.29876635356236	-75.8467783678706	145253
e309bbc95d836ffa2314c59103360cfe642da275	non-dictionary-based thai word segmentation using decision trees	segmentation accuracy;non-dictionary-based thai word segmentation;experiment result;longest matching method;word segmentation without a dictionary;syntactic attribute;thai word;specific information;decision trees;great number;decision tree model;thai corpus;unrecognized word;decision tree;word segmentation	For languages without word boundary delimiters, dictionaries are needed for segmenting running texts. This figure makes segmentation accuracy depend significantly on the quality of the dictionary used for analysis. If the dictionary is not sufficiently good, it will lead to a great number of unknown or unrecognized words. These unrecognized words certainly reduce segmentation accuracy. To solve such problem, we propose a method based on decision tree models. Without use of a dictionary, specific information, called syntactic attribute, is applied to identify the structure of Thai words. C4.5 is used as a tool for this purpose. Using a Thai corpus, experiment results show that our method outperforms some well-known dictionary-dependent techniques, maximum and longest matching methods, in case of no dictionary.	c4.5 algorithm;decision tree model;delimiter;dictionary;text corpus;text segmentation	Thanaruk Theeramunkong;Sasiporn Usanavasin	2001			natural language processing;speech recognition;computer science;pattern recognition	NLP	-24.56072950471003	-77.50697601327931	145501
c25fb96009b5f72450d0307570376012af2f7f3a	no more beating about the bush : a step towards idiom handling for indian language nlp		One of the major challenges in the field of Natural Language Processing (NLP) is the handling of idioms; seemingly ordinary phrases which could be further conjugated or even spread across the sentence to fit the context. Since idioms are a part of natural language, the ability to tackle them brings us closer to creating efficient NLP tools. This paper presents a multilingual parallel idiom dataset for seven Indian languages in addition to English and demonstrates its usefulness for two NLP applications Machine Translation and Sentiment Analysis. We observe significant improvement for both the subtasks over baseline models trained without employing the idiom dataset.	baseline (configuration management);machine translation;natural language processing;sentiment analysis	Ruchit Agrawal;Vighnesh Chenthil Kumar;Vigneshwaran Muralidaran;Dipti Misra Sharma	2018			artificial intelligence;speech recognition;natural language processing;computer science	NLP	-22.637828987536256	-75.05699211991565	145993
538b69a70a8417e564cd44c1c8e59b9bb08fbb12	the howard university system submission for the shared task in language identification in spanish-english codeswitching		This paper describes the Howard University system for the language identification shared task of the Second Workshop on Computational Approaches to Code Switching. Our system is based on prior work on SwahiliEnglish token-level language identification. Our system primarily uses character n-gram, prefix and suffix features, letter case and special character features along with previously existing tools. These are then combined with generated label probabilities of the immediate context of the token for the final system.	computation;computer cluster;language identification;n-gram	Rouzbeh A. Shirvani;Mario Piergallini;Gauri Shankar Gautam;Mohamed F. Chouikha	2016		10.18653/v1/W16-5815	natural language processing;speech recognition;computer science;communication	NLP	-23.949189010537857	-74.95377910839493	146395
dae7f83c8a9f51559d4c926e4cfa6fc51b36e956	statistical machine translation without long parallel sentences for training data		In this study, we paid attention to the reliability of phrase table. We have been used the phrase table using Och’s method[2]. And this method sometimes generate completely wrong phrase tables. We found that such phrase table caused by long parallel sentences. Therefore, we removed these long parallel sentences from training data. Also, we utilized general tools for statistical machine translation, such as ”Giza++”[3], ”moses”[4], and ”training-phrasemodel.perl”[5]. We obtained a BLEU score of 0.4047 (TEXT) and 0.3553(1-BEST) of the Challenge-EC task for our proposed method. On the other hand, we obtained a BLEU score of 0.3975(TEXT) and 0.3482(1-BEST) of the Challenge-EC task for a standard method. This means that our proposed method was effective for the Challenge-EC task. However, it was not effective for the BTECT-CE and Challenge-CE tasks. And our system was not good performance. For example, our system was the 7th place among 8 system for Challenge-EC task.	bleu;decision table;statistical machine translation	Jin'ichi Murakami;Masato Tokuhisa;Satoru Ikehara	2008			machine translation;bleu;training set;artificial intelligence;speech recognition;example-based machine translation;rule-based machine translation;pattern recognition;computer science;phrase	NLP	-23.60035098594894	-76.80024968610475	147382
70042cc47e947efb508816e2358e4cf5895d8451	extracting coordinate word pairs for dependency parsing	indexes;feature extraction	The subtask of identifying coordinate structures in Chinese dependency analysis is a challenging problem. The accuracy of coordinate word recognition remains below the average. To address this problem, we propose an automatic identification method based on large-scale unlabeled corpus. We then integrate a set of new features corresponding to the collected word pairs into the dependency parser. Specifically, our proposed method is based on the presence of easy-to-identify coordinate fragments. Our method can be divided into two steps. In the first step, we leverage two hand-crafted rules to extract highly accurate coordinate word pairs as seed words. The second step is to utilize seed words to extract coordinate structures in the corpus for further use of coordinate word pair extraction. Experimental results show that the extracted coordinate word pairs can significantly improve the accuracy on coordinate structure dependency analysis.	automatic identification and data capture;dependence analysis;parsing	Junjie Yu;Wenliang Chen	2015	2015 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2015.7451536	natural language processing;database index;speech recognition;feature extraction;computer science;pattern recognition	NLP	-23.836816359321357	-74.30399437794289	147425
d010fd9e5343ead0a5413b9e3d88fff611662bee	icfhr 2018 – competition on vietnamese online handwritten text recognition using hands-vnondb (vohtr2018)		This paper presents the results of the VOHTR 2018 competition on Vietnamese Online Handwritten Text Recognition. The goal of this competition is to evaluate and compare recent online handwritten text recognition systems on Vietnamese online handwritten text which contains many delayed strokes caused by the diacritic marks. Besides, the general objective is to encourage the studies on Vietnamese online handwritten text recognition based on the large Vietnamese handwriting database collected from 200 writers. In this competition, we introduce three tasks consisting of word recognition (task 1), text-line recognition (task 2) and paragraph recognition (task 3) which are described in details. Subsequently, we describe the evaluation metrics and give comparative results of competitors along with the brief descriptions of the respective methods.		Hung Tuan Nguyen;Cuong Tuan Nguyen;Masaki Nakagawa	2018	2018 16th International Conference on Frontiers in Handwriting Recognition (ICFHR)	10.1109/ICFHR-2018.2018.00092	vietnamese;speech recognition;artificial intelligence;pattern recognition;paragraph;word recognition;diacritic;computer science;handwriting	Robotics	-23.470992255250067	-73.61395068189681	147880
f8c8b9bfce7b6282cca00e02764889a280bf7b40	aligning very small parallel corpora using cross-lingual word embeddings and a monogamy objective		Count-based word alignment methods, such as the IBM models or fast-align, struggle on very small parallel corpora. We therefore present an alternative approach based on cross-lingual word embeddings (CLWEs), which are trained on purely monolingual data. Our main contribution is an unsupervised objective to adapt CLWEs to parallel corpora. In experiments on between 25 and 500 sentences, our method outperforms fast-align. We also show that our fine-tuning objective consistently improves a CLWE-only baseline.	align (company);baseline (configuration management);bitext word alignment;declaration (computer programming);experiment;parallel text;text corpus;unsupervised learning	Nina Pörner;Masoud Jalili Sabet;Benjamin Roth;Hinrich Schutze	2018	CoRR			NLP	-20.455953164862496	-75.70713265928005	148415
7715d8a3b84476a6e9803ffe9bb8d47fd5231d5c	more than words: using token context to improve canonicalization of historical german		Historical text presents numerous challenges for contemporary natural language processing techniques. In particular, the absence of consistent orthographic conventions in historical text presents difficulties for any system requiring reference to a static lexicon indexed by orthographic form. Canonicalization approaches seek to address these issues by associating one or more extant “canonical cognates” with each word of the input text and deferring application analysis to these canonical forms. Typewise conflation techniques treating each input word in isolation often suffer from a pronounced precision–recall trade-off pattern: high-precision techniques such as conservative transliteration have comparatively poor recall, whereas high-recall techniques such as phonetic conflation tend to be disappointingly imprecise. In this paper, we present a technique for disambiguation of type conflation sets at the token level using a Hidden Markov Model whose lexical probability matrix is dynamically computed from the candidate conflations, and evaluate its performance on a manually annotated corpus of historical German.	heuristic;hidden markov model;information retrieval;language-independent specification;lexicon;markov chain;natural language processing;orthographic projection;precision and recall;rewrite (programming);smoothing;sparse matrix;stochastic matrix;text corpus;transduction (machine learning);verification and validation;word-sense disambiguation	Bryan Jurish	2010	JLCL		computer science;natural language processing;canonicalization;artificial intelligence;canonical form;security token;extant taxon;composition (visual arts);lexicon;german;orthographic projection	NLP	-24.737739910415623	-76.47751330110543	148420
18a2eb6eb98f5ff53aedc473c9167e9fae0a488f	experiments with generative models for dependency tree linearization	article	We present experiments with generative models for linearization of unordered labeled syntactic dependency trees (Belz et al., 2011; Rajkumar and White, 2014). Our linearization models are derived from generative models for dependency structure (Eisner, 1996). We present a series of generative dependency models designed to capture successively more information about ordering constraints among sister dependents. We give a dynamic programming algorithm for computing the conditional probability of word orders given tree structures under these models. The models are tested on corpora of 11 languages using test-set likelihood, and human ratings for generated forms are collected for English. Our models benefit from representing local order constraints among sisters and from backing off to less sparse distributions, including distributions not conditioned on the head.	algorithm;dynamic programming;experiment;generative model;sparse matrix;text corpus	Richard Futrell;Edward Gibson	2015			mathematical optimization;machine learning;pattern recognition;mathematics	NLP	-20.102095693301642	-76.88489422112961	148763
fc313fe3a2acdcc18e8944ff5112dc28f4c491e2	a comparison of rnn lm and flm for russian speech recognition		In the paper, we describe a research of recurrent neural net- work (RNN) language model (LM) for N-best list rescoring for automatic continuous Russian speech recognition and make a comparison of it with factored language model (FLM). We tried RNN with different number of units in the hidden layer. For FLM creation, we used five linguistic factors: word, lemma, stem, part-of-speech, and morphological tag. All models were trained on the text corpus of 350M words. Also we made linear interpolation of RNN LM and FLM with the baseline 3-gram LM. We achieved the relative WER reduction of 8 % using FLM and 14 % relative WER reduction using RNN LM with respect to the baseline model.		Irina S. Kipyatkova;Alexey Karpov	2015		10.1007/978-3-319-23132-7_5	natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.724532830888176	-75.96659493008282	149024
5903fe051d9f55aefccd3dd3b0866983bd26da26	a syntax-free approach to japanese sentence compression	human subjects;term weighting;language model	Conventional sentence compression methods employ a syntactic parser to compress a sentence without changing its meaning. However, the reference compressions made by humans do not always retain the syntactic structures of the original sentences. Moreover, for the goal of ondemand sentence compression, the time spent in the parsing stage is not negligible. As an alternative to syntactic parsing, we propose a novel term weighting technique based on the positional information within the original sentence and a novel language model that combines statistics from the original sentence and a general corpus. Experiments that involve both human subjective evaluations and automatic evaluations show that our method outperforms Hori’s method, a state-of-theart conventional technique. Because our method does not use a syntactic parser, it is 4.3 times faster than Hori’s method.	brill tagger;experiment;language model;parser combinator;parsing;part-of-speech tagging;point of sale	Tsutomu Hirao;Jun Suzuki;Hideki Isozaki	2009		10.3115/1690219.1690262	natural language processing;speech recognition;inverted sentence;computer science;linguistics;balanced sentence;language model	NLP	-21.842011393504464	-77.42319586809477	149305
bc9ac9459e9d51acd39a10dbc176fa137a9d41b2	modeling word relatedness in latent dirichlet allocation		Standard LDA model suffers the problem that the topic assignment of each word is independent and word correlation hence is neglected. To address this problem, in this paper, we propose a model called Word Related Latent Dirichlet Allocation (WR-LDA) by incorporating word correlation into LDA topic models. This leads to new capabilities that standard LDA model does not have such as estimating infrequently occurring words or multi-language topic modeling. Experimental results demonstrate the effectiveness of our model compared with standard LDA.	latent dirichlet allocation;local-density approximation;topic model	Xun Wang	2014	CoRR		latent dirichlet allocation;natural language processing;dynamic topic model;speech recognition;computer science;pattern recognition	NLP	-19.679044967239047	-78.56943243213568	149425
29b440a9a9f5808a9411a6376ec35969d1c359e0	efficient solutions for word reordering in german-english phrase-based statistical machine translation		Despite being closely related languages, German and English are characterized by important word order differences. Longrange reordering of verbs, in particular, represents a real challenge for state-of-theart SMT systems and is one of the main reasons why translation quality is often so poor in this language pair. In this work, we review several solutions to improve the accuracy of German-English word reordering while preserving the efficiency of phrase-based decoding. Among these, we consider a novel technique to dynamically shape the reordering search space and effectively capture long-range reordering phenomena. Through an extensive evaluation including diverse translation quality metrics, we show that these solutions can significantly narrow the gap between phrase-based and hierarchical SMT.	distortion;satisfiability modulo theories;statistical machine translation	Arianna Bisazza;Marcello Federico	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-21.580897152380903	-76.81033999321944	149616
5b651ebcbe44e1133d760df623dcb65431a3accb	exploiting semantic and topic context to improve recognition of proper names in diachronic audio documents. (exploitation de contexte sémantique pour améliorer la reconnaissance des noms propres dans les documents audio diachroniques)		The diachronic nature of broadcast news causes frequent variations in the linguistic content and vocabulary, leading to the problem of Out-Of-Vocabulary (OOV) words in automatic speech recognition. Most of the OOV words are found to be proper names whereas proper names are important for automatic indexing of audio-video content as well as for obtaining reliable automatic transcriptions. New proper names missed by the speech recognition system can be recovered by a dynamic vocabulary multi-pass recognition approach in which new proper names are added to the speech recognition vocabulary based on the context of the spoken content. Existing methods for vocabulary selection rely on web search engines and adaptation corpora and choose the new vocabulary words using term-document frequency and co-occurrence based features. Open vocabulary systems based on sub-word units are an interesting solution but they face the problem of producing a reliable text transcription. The goal of this thesis is to model the semantic and topical context of new proper names in order to retrieve those which are relevant to the spoken content in the audio document. Training semantic/topic models is a challenging problem in this task because (a) several new proper names come with a low amount of data and (b) the context model should be robust to word errors in the automatic transcription. Probabilistic topic models and word embeddings from neural network models are explored for the task of retrieval of relevant proper names. A thorough evaluation of contextual representations from these models is performed. It is argued that these representations, which are learned in an unsupervised manner, are not the best for the given retrieval task. Neural network context models trained with an objective to maximise the retrieval performance are proposed. A Neural Bag-of-Words (NBOW) model trained to learn context vector representations at a document level is shown to outperform the generic representations. The proposed Neural Bag-of-Weighted-Words (NBOW2) model learns to assign a degree of importance to input words and has the ability to capture task specific key-words. Experiments on automatic speech recognition on French broadcast news videos demonstrate the effectiveness of the proposed models. Further evaluation of the NBOW2 model on standard text classification tasks, including movie review sentiment classification and newsgroup topic classification, shows that it learns interesting information about the task and gives the best classification accuracies among the bag-of-words models.	artificial neural network;bag-of-words model;content adaptation;digital video;document classification;emoticon;linear algebra;speech recognition;statistical classification;text corpus;transcription (software);unsupervised learning;vocabulary;web search engine;word embedding	Imran A. Sheikh	2016				NLP	-20.20711467434246	-73.49738570719312	149642
197da793a7bce43849f9237374c4f751a54266a2	maximal match chinese segmentation augmented by resources generated from a very large dictionary for post-processing		We used a production segmentation system, which draws heavily on a large dictionary derived from processing a large amount (over 150 million Chinese characters) of synchronous textual data gathered from various Chinese speech communities, including Beijing, Hong Kong, Taipei, and others. We run this system in two tracks in the Second International Chinese Word Segmentation Bakeoff, with Backward Maximal Matching (right-to-left) as the primary mechanism. We also explored the use of a number of supplementary features offered by the large dictionary in postprocessing, in an attempt to resolve ambiguities and detect unknown words. While the results might not have reached their fullest potential, they nevertheless reinforced the importance and usefulness of a large dictionary as a basis for segmentation, and the implication of following a uniform standard on the segmentation performance on data from various sources.	business motivation model;dictionary;matching (graph theory);maximal set;right-to-left;text corpus;text segmentation;video post-processing	Ka-Po Chow;Andy C. Chin;Wing Fu Tsoi	2005			matching (graph theory);chinese characters;text segmentation;segmentation;pattern recognition;artificial intelligence;computer science	NLP	-24.168452811152157	-73.70730439123093	149964
0da72c7287f1edc3f0f11681ac665c2dbfb6ee4d	automatic lemmatizer construction with focus on oov words lemmatization	lemmatizace;lenguaje natural;vocabulaire;lemmatization;katedra kybernetiky;kybernetika;etude experimentale;out of vocabulary;lexicon;speech processing;vocabulary;langage naturel;tratamiento palabra;informacni a řidici systemy;traitement parole;automaticke řizeni;intelligence artificielle;vocabulario;oov slova;lematizacion;publikace automatic lemmatizer construction with focus on oov words lemmatization;lemmatisation;dictionnaire;natural language;dictionaries;uměla inteligence;artificial intelligence;inteligencia artificial;lexico;clanek;article;estudio experimental;diccionario;publications automatic lemmatizer construction with focus on oov words lemmatization;lexique	This paper deals with the automatic construction of a lemmatizer from a Full Form Lemma (FFL) training dictionary and with lemmatization of new, in the FFL dictionary unseen, i.e. out-ofvocabulary (OOV) words. Three methods of lemmatization of three kinds of OOV words (missing full forms, unknown words, and compound words) are introduced. These methods were tested on Czech test data. The best result (recall: 99.3 % and precision: 75.1 %) has been achieved by a combination of these methods. The lexicon-free lemmatizer based on the method of lemmatization of unknown words (lemmatization patterns method) is introduced too.	dictionary;lemmatisation;lexicon;test data;the final fantasy legend	Jakub Kanis;Ludek Müller	2005		10.1007/11551874_17	natural language processing;lemmatisation;speech recognition;computer science;artificial intelligence;speech processing;linguistics	NLP	-26.332466956718243	-78.64152254963784	150019
f685b73ce149fd4d0964f5df0665237dfabf30c1	fast btg-forest-based hierarchical sub-sentential alignment		In this paper, we propose a novel BTGforest-based alignment method. Based on a fast unsupervised initialization of parameters using variational IBM models, we synchronously parse parallel sentences top-down and align hierarchically under the constraint of BTG. Our twostep method can achieve the same run-time and comparable translation performance as fast align while it yields smaller phrase tables. Final SMT results show that our method even outperforms in the experiment of distantly related languages, e.g., English–Japanese.	align (company);bitext word alignment;open-source software;top-down and bottom-up design;top-down parsing;unsupervised learning;variational principle;word lists by frequency	Hao Wang;Yves Lepage	2017	CoRR		initialization;parsing;ibm;phrase;artificial intelligence;pattern recognition;computer science	NLP	-21.016051346177957	-77.0297897809695	150467
069bcea5c01627583f564a349c285eb30083a5f0	a modified joint source-channel model for transliteration	bengali-english machine transliteration system;english transliteration unit;machine transliteration system;aligned transliteration unit;modified joint source-channel model;pattern c;form c;bengali word;transliteration unit;vowel modifier	Most machine transliteration systems transliterate out of vocabulary (OOV) words through intermediate phonemic mapping. A framework has been presented that allows direct orthographical mapping between two languages that are of different origins employing different alphabet sets. A modified joint source–channel model along with a number of alternatives have been proposed. Aligned transliteration units along with their context are automatically derived from a bilingual training corpus to generate the collocational statistics. The transliteration units in Bengali words take the pattern C + M where C represents a vowel or a consonant or a conjunct and M represents the vowel modifier or matra. The English transliteration units are of the form C*V* where C represents a consonant and V represents a vowel. A Bengali-English machine transliteration system has been developed based on the proposed models. The system has been trained to transliterate person names from Bengali to English. It uses the linguistic knowledge of possible conjuncts and diphthongs in Bengali and their equivalents in English. The system has been evaluated and it has been observed that the modified joint source-channel model performs best with a Word Agreement Ratio of 69.3% and a Transliteration Unit Agreement Ratio of 89.8%.	channel (communications);modifier key;text corpus;vocabulary	Asif Ekbal;Sudip Kumar Naskar;Sivaji Bandyopadhyay	2006			natural language processing;speech recognition;computer science;linguistics	NLP	-22.730452496798364	-77.96051931582437	150526
0dac616646bca966a85d5f3cec691c9371e00677	classification of german verbs using nouns in argument positions and aspectual features		This paper provides evidence that aspectual verb classes (Vendler, 1967) can be induced from nominal fillers in argument positions and aspectual features. We classified 35 German verbs in a supervised learning procedure using a support vector machine classifier and a classification into five aspectual classes (Richter and van Hout, 2015) as gold standard and observed excellent and substantial agreements.	supervised learning;support vector machine	Michael Richter;Jürgen Hermes	2015			support vector machine;supervised learning;noun;german verbs;natural language processing;mathematics;verb;artificial intelligence;classifier (linguistics)	NLP	-24.645713761080792	-73.93503099606679	150785
a28114478f829a8b6134455120237d8c4362e19b	khmer pos tagging using conditional random fields		The transformation-based approach with hybrid of rule-based and tri-gram have already been introduced for Khmer part-of-speech (POS) tagging. In this study, in order to further explore this topic, we present an alternative approach to Khmer POS tagging using Conditional Random Fields (CRFs). Since the features greatly affect the tagging accuracy, we investigate five groups of features and use them with the CRF model. First, we study different contextual information and use it as our baseline model. We then analyze the characteristics of Khmer and come up with three additional groups of language-related features including morphemes, word-shapes and name-entities. We also explore the use of lexicon as features to further improve the accuracy of our tagger. Our proposed approach has been evaluated on a corpus of 41,058 words and 27 POS tags. The comparative study has shown that our proposed approach produces a competitive accuracy compared to other Khmer POS tagging approaches.	conditional random field;part-of-speech tagging	Sokunsatya Sangvat;Charnyote Pluempitiwiriyawej	2017		10.1007/978-981-10-8438-6_14	crfs;lexicon;conditional random field;morpheme;pattern recognition;computer science;artificial intelligence;part-of-speech tagging	NLP	-22.086061657917025	-73.91965521303537	150798
b92f17c3df18a2c2a075e11b6bc53d5901cb3092	from treebank conversion to automatic dependency parsing for vietnamese		This paper presents a new conversion method to automatically transform a constituent-based Vietnamese Treebank into dependency trees. On a dependency Treebank created according to our new approach, we examine two stateof-the-art dependency parsers: the MSTParser and the MaltParser. Experiments show that the MSTParser outperforms the MaltParser. To the best of our knowledge, we report the highest performances published to date in the task of dependency parsing for Vietnamese. Particularly, on gold standard POS tags, we get an unlabeled attachment score of 79.08% and a labeled attachment score of 71.66%.	attachments;brown corpus;dependency grammar;parsing;performance;treebank;usb attached scsi	Dat Quoc Nguyen;Dai Quoc Nguyen;Son Bao Pham;Phuong-Thai Nguyen;Minh Le Nguyen	2014		10.1007/978-3-319-07983-7_26	natural language processing;speech recognition;computer science;linguistics	NLP	-22.745876769475686	-75.20885674216136	150836
59c935256a25dbf494620fd69e871af201aeac8f	bridging languages through etymology: the case of cross language text categorization		We propose the hypothesis that word etymology is useful for NLP applications as a bridge between languages. We support this hypothesis with experiments in crosslanguage (English-Italian) document categorization. In a straightforward bag-ofwords experimental set-up we add etymological ancestors of the words in the documents, and investigate the performance of a model built on English data, on Italian test data (and viceversa). The results show not only statistically significant, but a large improvement – a jump of almost 40 points in F1-score – over the raw (vanilla bag-ofwords) representation.	bag-of-words model;bridging (networking);categorization;document classification;experiment;f1 score;interaction;natural language processing;test data;text corpus;textual entailment	Vivi Nastase;Carlo Strapparava	2013			natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-22.68498614589303	-74.83438512095401	151312
9fc06a669eba2dafd4e3603de6e441ca9825d44d	consistency checking for treebank alignment	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;computational linguistics;language technology computational linguistics	This paper explores ways to detect errors in aligned corpora, using very little technology. In the first method, applicable to any aligned corpus, we consider alignment as a string-to-string mapping. Treating the target string as a label, we examine each source string to find inconsistencies in alignment. Despite setting up the problem on a par with grammatical annotation, we demonstrate crucial differences in sorting errors from legitimate variations. The second method examines phrase nodes which are predicted to be aligned, based on the alignment of their yields. Both methods are effective in complementary ways.	compiler;heuristic (computer science);machine translation;microsoft outlook for mac;point of sale;sorting;text corpus;treebank	Markus Dickinson;Yvonne Samuelsson	2010			natural language processing;speech recognition;quantitative linguistics;computer science;computational linguistics;treebank;linguistics	NLP	-25.178988653954804	-76.16449195741573	151905
b69bb95da1a6097db753d5a263f1361d582fd172	expanding paraphrase lexicons by exploiting lexical variants.		This study tackles the problem of paraphrase acquisition: achieving high coverage as well as accuracy. Our method first induces paraphrase patterns from given seed paraphrases, exploiting the generality of paraphrases exhibited by pairs of lexical variants, e.g., “amendment” and “amending,” in a fully empirical way. It then searches monolingual corpora for new paraphrases that match the patterns. This can extract paraphrases comprising words that are completely different from those of the given seeds. In experiments, our method expanded seed sets by factors of 42 to 206, gaining 84% to 208% more coverage than a previous method that generalizes only identical word forms. Human evaluation through a paraphrase substitution test demonstrated that the newly acquired paraphrases retained reasonable quality, given substantially high-quality seeds.	automatic summarization;experiment;level of detail;lexicon;machine translation;natural language processing;raw image format;text corpus;text simplification	Atsushi Fujita;Pierre Isabelle	2015		10.3115/v1/N15-1065	natural language processing;generality;artificial intelligence;paraphrase;computer science	NLP	-21.15865244330963	-75.40768610753193	152022
4f0c91ec7227902075faf518b8d6acd1e8d7894d	features for phrase-structure reranking from dependency parses	phrase-structure reranking;dependency parsers;discriminative phrase-structure parser;automatic dependency parses;different approach;significant improvement;last decade;state-of-the-art german discriminative constituent	Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser.	blue (queue management algorithm);discriminative model;experiment;parsing;vergence	Richárd Farkas;Bernd Bohnet;Helmut Schmid	2011			natural language processing;speech recognition;computer science;pattern recognition	NLP	-21.345629433909174	-75.77931251214189	153414
b220d593e56cb48048ef69dc3d45286119578aab	shrinkage based features for slot tagging with conditional random fields		In this paper we propose a set of class-based features that are generated in an unsupservised fashion to improve slot tagging with Conditional Random Fields (CRFs). The feature generation is based on the idea behind shrinkage based language models, where shrinking the sum of parameter magnitudes in an exponential model tends to improve performance. We use these features with CRFs and show that they consistently improve the slot tagging performance against baselines on several natural language understanding tasks. Since the proposed features are generated in an unsupervised manner without significant computational overhead, the improvements in performance comes for free and we expect that the same features may result in gains in other tagging tasks.	baseline (configuration management);computation;conditional random field;language model;natural language understanding;overhead (computing);time complexity;unsupervised learning	Ruhi Sarikaya;Asli Çelikyilmaz;Anoop Deoras;Minwoo Jeong	2014			crfs;pattern recognition;artificial intelligence;overhead (computing);natural language understanding;shrinkage;language model;conditional random field;computer science	NLP	-20.322749859333523	-75.78554108817605	154057
c6533fe4bcc6660882ab4f67546f6e4e44c9e3c5	accurate part-of-speech tagging via conditional random field		POS tagging (i.e. part-of-speech tagging) is an important component of syntactic parsing in the field of natural language processing. While CRF (i.e. conditional random field) is a class of statistical modelling method often applied in pattern recognition and machine learning, where it is used for structured prediction. As POS tagging can be considered as a structured prediction task to some extent, so in this paper, we proposed to utilize the inherent advantages of CRF, and apply it to POS tagging task to get more accurate. The subsequent experiments are introduced to validate our proposed method.	conditional random field;part-of-speech tagging	Jinmei Zhang;Yucheng Zhang	2016		10.1007/978-3-319-51969-2_18	syntax;part of speech;parsing;conditional random field;artificial intelligence;statistical model;structured prediction;pattern recognition;computer science;part-of-speech tagging	NLP	-21.560633645894526	-75.3783374591585	154231
5fc7b4dbc154bbbf26d8cee2f18f31ecbf286bcf	generalizing word embeddings using bag of subwords		We approach the problem of generalizing pre-trained word embeddings beyond fixed-size vocabularies without using additional contextual information. We propose a subword-level word vector generation model that views words as bags of character $n$-grams. The model is simple, fast to train and provides good vectors for rare or unseen words. Experiments show that our model achieves state-of-the-art performances in English word similarity task and in joint prediction of part-of-speech tag and morphosyntactic attributes in 23 languages, suggesting our modelu0027s ability in capturing the relationship between wordsu0027 textual representations and their embeddings.	experiment;grams;n-gram;natural language processing;part-of-speech tagging;performance;substring;vocabulary;word embedding	Jinman Zhao;Sidharth Mudgal;Yingyu Liang	2018			machine learning;artificial intelligence;computer science;generalization	NLP	-19.77187966294794	-74.1477686655835	154446
32ae2f0c0af70299ff1cf224165a553d62355961	granska-an efficient hybrid system for swedish grammar checking	hybrid system	This article describes how Granska a surface-oriented system for checking Swedish grammar is constructed. With the use of special error detection rules, the system can detect and suggest corrections for a number of grammatical errors in Swedish texts. Specifically, we focus on how erroneously split compounds and noun phrase agreement are handled in the rules. The system combines probabilistic and rule-based methods to achieve high efficiency and robustness. This is a necessary prerequisite for a grammar checker that will be used in real lime in direct interaction with users. We hope to show that the Granska system with higher efficiency can achieve the same or better results than systems that use rule-based parsing alone.	error detection and correction;grammar checker;hybrid system;logic programming;parsing;lime	Rickard Domeij;Ola Knutsson;Johan Carlberger;Viggo Kann	1999			robustness (computer science);error detection and correction;swedish grammar;noun phrase;natural language processing;probabilistic logic;parsing;hybrid system;computer science;grammar;artificial intelligence	AI	-25.880000126336324	-76.03990720808174	154620
b2757fea9cd44a568e5a8d249b16b8172fe28312	left-to-right tree-to-string decoding with prediction	left-to-right decoding;tree-to-string translation;left-to-right tree-to-string decoding;partial translation;language model;left-to-right method;decoding efficiency;language model evaluation;right decoding algorithm;decoding algorithm;machine translation	Decoding algorithms for syntax based machine translation suffer from high computational complexity, a consequence of intersecting a language model with a context free grammar. Left-to-right decoding, which generates the target string in order, can improve decoding efficiency by simplifying the language model evaluation. This paper presents a novel left to right decoding algorithm for tree-to-string translation, using a bottom-up parsing strategy and dynamic future cost estimation for each partial translation. Our method outperforms previously published tree-to-string decoders, including a competing left-to-right method.	algorithm;bleu;bottom-up parsing;computational complexity theory;context-free grammar;experiment;language model;machine translation;test set;tree traversal	Yang Feng;Yang Liu;Qun Liu;Trevor Cohn	2012			list decoding;speech recognition;sequential decoding;computer science;theoretical computer science;machine learning	NLP	-21.507947913885396	-78.11673607717185	154811
79a4e80ea5a36e026359b84cbf8e72195bab9d37	named entity recognition in vietnamese text using label propagation	supervised learning;semantics;words similarity named entity recognition labeled propagation semi supervised learning;frequency measurement;labeled documents named entity recognition vietnamese text label propagation noun phrases named entity candidates word similarity measurement;accuracy;text analysis learning artificial intelligence natural language processing;text recognition;context;semisupervised learning;context semantics text recognition accuracy supervised learning semisupervised learning frequency measurement	This paper presents our named entity recognition system for Vietnamese text using labeled propagation. In here we propose: (i) a method of choosing noun phrases as the named entity candidates; (ii) a method to measure the word similarity; and (iii) a method of decreasing the effect of high frequency labels in labeled documents. Experimental results show that our labeled propagate method achieves higher accuracy than the old one [12]. In addition, when the number of the labeled data is small, its accuracy is higher than when using conditional random fields.	algorithm;computation;conditional random field;experiment;named entity;newton's method;semi-supervised learning;semiconductor industry;software propagation;supervised learning;text corpus;time complexity	Huong Thanh Le;Rathany Chan Sam;Hoan Cong Nguyen;Thuy Thanh Nguyen	2013	2013 International Conference on Soft Computing and Pattern Recognition (SoCPaR)	10.1109/SOCPAR.2013.7054160	semi-supervised learning;natural language processing;speech recognition;computer science;machine learning;pattern recognition;semantics;accuracy and precision;supervised learning	NLP	-22.44642550851988	-74.89000454316276	155081
674be9f6a12ebe8b6fbc6b759fd3b3d1300ed21c	improving translation fluency with search-based decoding and a monolingual statistical machine translation model for automatic post-editing		The BLEU scores and translation fluency for the current state-of-the-art SMT systems based on IBM models are still too low for publication purposes. The major issue is that stochastically generated sentences hypotheses, produced through a stack decoding process, may not strictly follow the natural target language grammar, since the decoding process is directed by a highly simplified translation model and n-gram language model, and a large number of noisy phrase pairs may introduce significant search errors. This paper proposes a statistical post-editing (SPE) model, based on a special monolingual SMT paradigm, to “translate”disfluent sentences into fluent sentences. However, instead of conducting a stack decoding process, the sentence hypotheses are searched from fluent target sentences in a large target language corpus or on the Web to ensure fluency. Phrase-based local editing, if necessary, is then applied to correct weakest phrase alignments between the disfluent and searched hypotheses using fluent target language phrases; such phrases are segmented from a large target language corpus with a global optimization criterion to maximize the likelihood of the training sentences, instead of using noisy phrases combined from bilingually wordaligned pairs. With such search-based decoding, the absolute BLEU scores are much higher than automatic post editing systems that conduct a classical SMT decoding process. We are also able to fully correct a significant number of disfluent sentences into completely fluent versions. The BLEU scores are significantly improved. The evaluation shows that on average 46% of translation errors can be fully recovered, and the BLEU score can be improved by about 26%.	bleu;compiler;global optimization;language model;mathematical optimization;n-gram;postediting;programming paradigm;statistical machine translation;text corpus;world wide web	Jing-Shin Chang;Sheng-Sian Lin	2009			machine translation;bleu;fluency;language model;speech recognition;sentence;phrase;rule-based machine translation;evaluation of machine translation;computer science	NLP	-21.118528861499918	-76.61746856439564	155208
6c1ed847585140e5896f88b6033b91459c9d99b4	a reranking method for syntactic parsing with heterogeneous treebanks	disparate treebanks;syntactic parsing;standards;reranking;reranking method;chinese treebanks;equations standards artificial neural networks accuracy;annotation standards;tree data structures natural language processing;tree data structures;treebank conversion;accuracy;artificial neural networks;information exchange;heterogeneous treebanks syntactic parsing reranking;chinese treebanks reranking method syntactic parsing heterogeneous treebanks natural language processing annotation standards disparate treebanks treebank conversion;heterogeneous treebanks;natural language processing	In the field of natural language processing (NLP), there often exist multiple corpora with different annotation standards for the same task. In this paper, we take syntactic parsing as a case study and propose a reranking method which is able to make direct use of disparate treebanks simultaneously without using techniques such as treebank conversion. The method proceeds in three steps: 1) build parsers on individual treebanks; 2) use parsers independently to generate n-best lists for each sentence in test set; 3) rerank individual n-best lists which correspond to the same sentence by using consensus information exchanged among these n-best lists. Experimental results on two open Chinese treebanks show that our method significantly outperforms the baseline system by 0.84% and 0.53% respectively.	baseline (configuration management);coding tree unit;natural language processing;parsing;test set;text corpus;the coroner's toolkit;treebank	Haibo Ding;Muhua Zhu;Jingbo Zhu	2010	Proceedings of the 6th International Conference on Natural Language Processing and Knowledge Engineering(NLPKE-2010)	10.1109/NLPKE.2010.5587842	natural language processing;speech recognition;computer science;information retrieval	NLP	-22.350665736320444	-75.3435138385007	155219
b6ef2a03370d62e92dfceec7a90a00f10a0be289	soochow university word segmenter for sighan 2012 bakeoff		This paper presents a Chinese Word Segmentation system on MicroBlog corpora for the CIPS-SIGHAN Word Segmentation Bakeoff 2012. Our system employs Conditional Random Fields (CRF) as the segmentation model. To make our model more adaptive to MicroBlog, we manually analyze and annotate many MicroBlog messages. After manually checking and analyzing the MicroBlog text, we propose several pre-processing and post-processing rules to improve the performance. As a result, our system obtains a competitive F-score in comparison with other participating systems.	algorithm;conditional random field;f1 score;microsoft word for mac;preprocessor;text corpus;text segmentation;video post-processing	Yan Fang;Zhongqing Wang;Shoushan Li;Zhongguo Li;Richen Xu;Leixin Cai	2012			speech recognition;computer science;communication;world wide web	NLP	-23.33896243463812	-76.25242243705509	155685
67ad45d20bd17c5f1be45f9f2029f815c9873379	encoding sentences with graph convolutional networks for semantic role labeling		Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.	amazon web services;anton (computer);artificial neural network;benchmark (computing);encoder;graphics core next;long short-term memory;natural language processing;realms of the haunting;semantic role labeling;tridiagonal matrix algorithm	Diego Marcheggiani;Ivan Titov	2017			natural language processing;speech recognition;computer science;machine learning;linguistics;programming language	NLP	-19.25869845802193	-74.16562600169881	155687
3995bd289702d7291ac92c426ce28d2481d48e05	statistical properties of overlapping ambiguities in chinese word segmentation and a strategy for their disambiguation	statistical property;overlapping ambiguity;statistical properties;domain specific corpora;word segmentation;chinese word segmentation;domain specificity;disambiguation strategy	Overlapping ambiguity is a major ambiguity type in Chinese word segmentation. In this paper, the statistical properties of overlapping ambiguities are intensively studied based on the observations from a very large balanced general-purpose Chinese corpus. The relevant statistics are given from different perspectives. The stability of high frequent maximal overlapping ambiguities is tested based on statistical observations from both general-purpose corpus and domain-specific corpora. A disambiguation strategy for overlapping ambiguities, with a predefined solution for each of the 5,507 pseudo overlapping ambiguities, is proposed consequently, suggesting that over 42% of overlapping ambiguities in Chinese running text could be solved without making any error. Several state-of-the-art word segmenters are used to make comparisons on solving these overlapping ambiguities. Preliminary experiments show that about 2% of the 5,507 pseudo ambiguities which are mistakenly segmented by these segmenters can be properly treated by the proposed strategy.	chinese wall;domain-specific language;experiment;general-purpose markup language;general-purpose modeling;maximal set;text corpus;text segmentation;word-sense disambiguation	Wei Qiao;Maosong Sun;Wolfgang Menzel	2008		10.1007/978-3-540-87391-4_24	natural language processing;text segmentation;speech recognition;computer science;pattern recognition	NLP	-24.420547438022325	-76.78790312686603	155956
1b4f7491108070cb0909b9966cbf6967225bee83	tree sequence kernel for natural language		We propose Tree Sequence Kernel (TSK), which implicitly exhausts the structure features of a sequence of subtrees embedded in the phrasal parse tree. By incorporating the capability of sequence kernel, TSK enriches tree kernel with tree sequence features so that it may provide additional useful patterns for machine learning applications. Two approaches of penalizing the substructures are proposed and both can be accomplished by efficient algorithms via dynamic programming. Evaluations are performed on two natural language tasks, i.e. Question Classification and Relation Extraction. Experimental results suggest that TSK outperforms tree kernel for both tasks, which also reveals that the structure features made up of multiple subtrees are effective and play a complementary role to the single tree structure.	abstract syntax tree;algorithm;dynamic programming;embedded system;kernel (operating system);machine learning;maximal set;natural language;parse tree;parsing;relationship extraction;tree (data structure);tree structure	Jun Sun;Min Zhang;Chew Lim Tan	2011			trie;order statistic tree;machine learning;pattern recognition;incremental decision tree;data mining;k-ary tree;interval tree;fractal tree index;tree structure;search tree;tree;tree kernel;tree traversal	ML	-20.66894880971909	-74.52604117322194	156213
acc4a0dc5f3a6c30e46f98b53fd688b3c9797aaa	regression and ranking based optimisation for sentence level mt evaluation		Automatic evaluation metrics are fundamentally important for Machine Translation, allowing comparison of systems performance and efficient training. Current evaluation metrics fall into two classes: heuristic approaches, like BLEU, and those using supervised learning trained on human judgement data. While many trained metrics provide a better match against human judgements, this comes at the cost of including lots of features, leading to unwieldy, non-portable and slow metrics. In this paper, we introduce a new trained metric, ROSE, which only uses simple features that are easy portable and quick to compute. In addition, ROSE is sentence-based, as opposed to document-based, allowing it to be used in a wider range of settings. Results show that ROSE performs well on many tasks, such as ranking system and syntactic constituents, with results competitive to BLEU. Moreover, this still holds when ROSE is trained on human judgements of translations into a different language compared with that use in testing.	bleu;evaluation function;evaluation of machine translation;heuristic;hindmarsh–rose model;image noise;information;mathematical optimization;overfitting;rose;simple features;smoothed analysis;smoothing;string searching algorithm;supervised learning;teaching method	Xingyi Song;Trevor Cohn	2011			natural language processing;machine learning;pattern recognition	NLP	-19.323286284191017	-77.45211615290383	156414
b9598c3596ccb952114e1058da0abc4224a6626d	distributed feature representations for dependency parsing	distributed feature representations english data sets chinese data sets hidden class representations graph based dependency parsing models surrounding context representation dependency trees baseline system sentence parsing feature embeddings word embeddings borrowing terminologies feature sparseness problem distributed representation learning;context adaptation models data models training vectors predictive models context modeling;trees mathematics grammars learning artificial intelligence natural language processing;feature embeddings;dependency parsing;semi supervised approach;semi supervised approach natural language processing dependency parsing feature embeddings;natural language processing	This paper presents an approach to automatically learning distributed representations for features to address the feature sparseness problem for dependency parsing. Borrowing terminologies from word embeddings, we call the feature representation feature embeddings. In our approach, the feature embeddings are inferred from large amounts of auto-parsed data. First, the sentences in raw data are parsed by a baseline system and we obtain dependency trees. Then, we represent each model feature using the surrounding features on the dependency trees. Based on the representation of surrounding context, we proposed two learning methods to infer feature embeddings. Finally, based on feature embeddings, we present a set of new features for graph-based dependency parsing models. The new parsers can not only make full use of well-established hand-designed features but also benefit from the hidden-class representations of features. Experiments on the standard Chinese and English data sets show that the new parser achieves significant performance improvements over a strong baseline.	baseline (configuration management);dependency grammar;experiment;feature model;feature vector;neural coding;parsing;word embedding	Wenliang Chen;Min Zhang;Yue Zhang	2015	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2014.2365359	natural language processing;computer science;machine learning;pattern recognition;feature;dependency grammar	NLP	-19.7098918237897	-73.87693441476631	156586
d34c6e1218485938b211e8a1881220e823f84754	removing biases from trainable mt metrics by using self-training		Most trainable machine translation (MT) metrics train their weights on human judgments of state-of-the-art MT systems outputs. This makes trainable metrics biases in many ways. One of them is preferring longer translations. These biased metrics when used for tuning are evaluating different types of translations -- n-best lists of translations with very diverse quality. Systems tuned with these metrics tend to produce overly long translations that are preferred by the metric but not by humans. This is usually solved by manually tweaking metric's weights to equally value recall and precision. Our solution is more general: (1) it does not address only the recall bias but also all other biases that might be present in the data and (2) it does not require any knowledge of the types of features used which is useful in cases when manual tuning of metric's weights is not possible. This is accomplished by self-training on unlabeled n-best lists by using metric that was initially trained on standard human judgments. One way of looking at this is as domain adaptation from the domain of state-of-the-art MT translations to diverse n-best list translations.		Milos Stanojevic	2015	CoRR		speech recognition;computer science;artificial intelligence;machine learning	NLP	-19.392831407911213	-77.49850767353443	156664
aa72de0c837a32566fcf013579a6b99397376d0e	a generic sentence trimmer with crfs	generic model;con ditional random fields	The paper presents a novel sentence trimmer in Japanese, which combines a non-statistical yet generic tree generation model and Conditional Random Fields (CRFs), to address improving the grammaticality of compression while retaining its relevance. Experiments found that the present approach outperforms in grammaticality and in relevance a dependency-centric approach (Oguro et al., 2000; Morooka et al., 2004; Yamagata et al., 2006; Fukutomi et al., 2007)− the only line of work in prior literature (on Japanese compression) we are aware of that allows replication and permits a direct comparison.	conditional random field;experiment;local optimum;margin (machine learning);relevance	Tadashi Nomoto	2008			natural language processing;speech recognition;computer science	NLP	-20.742831697885478	-76.01634636405937	156788
584c93b423841e24b16c7d5ff1375e59f0da6520	context-sensitive statistics for improved grammatical language models	probabilistic context free grammar;high performance;language model	We develop a language model using probabilistic context-free grammars (PCFGs) that is “pseudo context-sensitive” in that the probability that a nonterminal N expands using a rule T depends on N’s parent. We give the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction.	context-free language;context-sensitive grammar;inside–outside algorithm;language model;mathematical induction;stochastic context-free grammar;terminal and nonterminal symbols	Eugene Charniak;Glenn Carroll	1994			natural language processing;computer science;artificial intelligence;machine learning;algorithm;language model	AI	-21.39272835303802	-78.36793355448084	156830
331b41aab0e6de601d48c1dfd3c7cd1684ba9e95	rule based transliteration scheme for english to punjabi	rule based;statistical machine translation;proper names;artificial intelligence;computational linguistics;named entity;machine translation	Machine Transliteration has come out to be an emerging and a very important research area in the field of machine translation. Transliteration basically aims to preserve the phonological structure of words. Proper transliteration of name entities plays a very significant role in improving the quality of machine translation. In this paper we are doing machine transliteration for English-Punjabi language pair using rule based approach. We have constructed some rules for syllabification. Syllabification is the process to extract or separate the syllable from the words. In this we are calculating the probabilities for name entities (Proper names and location). For those words which do not come under the category of name entities, separate probabilities are being calculated by using relative frequency through a statistical machine translation toolkit known as MOSES. Using these probabilities we are transliterating our input text from English to Punjabi.	entity;moses;statistical machine translation;syllable	Deepti Bhalla;Nisheeth Joshi;Iti Mathur	2013	CoRR	10.5121/ijnlc.2013.2207	natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;artificial intelligence;computational linguistics;proper noun;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-25.31231228179327	-77.24212881337414	156868
a1ef2438fe0d56889961a92ce3fb2d0a7ae903aa	a modular cascaded approach to complete parsing	object recognition;language use;identifiable linguistic unit;training;data driven dependency parsing;data mining;word order;natural languages tagging robustness speech recognition data mining information analysis speech analysis availability;hyderabad dependency treebank;accuracy;grammars;gold;mst parser;dependency label sets;modular cascaded approach;feature extraction;state of the art data driven hindi parser;dependence structure;state of the art data driven hindi parser modular cascaded approach data driven dependency parsing artificial root node dependency label sets identifiable linguistic unit morphologically rich free word order language mst parser hyderabad dependency treebank;dependency parsing;artificial root node;natural language processing;morphologically rich free word order language;natural language processing grammars linguistics;tagging;linguistics	In this paper, we propose a modular cascaded approach to data driven dependency parsing. Each module or layer leading to the complete parse produces a linguistically valid partial parse. We do this by introducing an artificial root node in the dependency structure of a sentence and by catering to distinct dependency label sets that reflect the function of the set internal labels vis-à-vis a distinct and identifiable linguistic unit, at different layers. The linguistic unit in our approach is a clause. Output (partial parse) from each layer can be accessed independently. We applied this approach to Hindi, a morphologically rich free word order language using MST Parser. We did all our experiments on a part of Hyderabad Dependency Treebank. The final results show an increase of 1.35% in unlabeled attachment and 1.36% in labeled attachment accuracies over state-of-the-art data driven Hindi parser.	attachments;experiment;parsing;tree (data structure);treebank	Samar Husain;Phani Gadde;Bharat Ram Ambati;Dipti Misra Sharma;Rajeev Sangal	2009	2009 International Conference on Asian Language Processing	10.1109/IALP.2009.37	gold;word order;natural language processing;parser combinator;speech recognition;feature extraction;computer science;cognitive neuroscience of visual object recognition;accuracy and precision;linguistics;top-down parsing;dependency grammar	NLP	-20.562189170516383	-74.0871582179272	157567
ac932ad6dbad3dd978fc5abdfe9035a9a538cb0f	creating chinese-english comparable corpora	comparable corpora;cross language information retrieval;document alignment;keyword extraction		chinese wall;text corpus	Degen Huang;Shanshan Wang;Fuji Ren	2013	IEICE Transactions		natural language processing;speech recognition;information retrieval	Vision	-23.580771622187882	-76.24238197422503	157590
04286371d9d70e99b147c0740cfcc7690841772d	cogalex-v shared task: mach5 - a traditional dsm approach to semantic relatedness		This contribution provides a strong baseline result for the CogALex-V shared task using a traditional “count”-type DSM (placed in rank 2 out of 7 in subtask 1 and rank 3 out of 6 in subtask 2). Parameter tuning experiments reveal some surprising effects and suggest that the use of random word pairs as negative examples may be problematic, guiding the parameter optimization in an undesirable direction.	baseline (configuration management);experiment;mathematical optimization;microsoft word for mac;semantic similarity	Stefan Evert	2016			artificial intelligence;natural language processing;computer science;semantic similarity	NLP	-26.361648755891785	-73.768045022536	157883
3afc3462f49d5750678fb83be8a3e6f4880f4c0a	topic-based language models using em	language model;em algorithm;latent variable	In this paper, we propose a novel statistical language model to capture topic-related long-range dependencies. Topics are modeled in a latent variable framework in which we also derive an EM algorithm to perform a topic factor decomposition based on a segmented training corpus. The topic model is combined with a standard language model to be used for on-line word prediction. Perplexity results indicate an improvement over previously proposed topic models, which unfortunately has not translated into lower word error.	expectation–maximization algorithm;language model;latent variable;online and offline;perplexity;topic model	Daniel Gildea;Thomas Hofmann	1999			topic model;speech recognition;standard language;artificial intelligence;language model;perplexity;latent variable;computer science;pattern recognition;expectation–maximization algorithm	ML	-20.23329343434545	-76.69195325154998	158294
22d7a53affb740602da71b311f73623140c352bc	language identification with confidence limits	classification algorithm;confidence limit;success rate;language identification	A statistical classification algorithm and its application to language identification from noisy input are described. The main innovation is to compute confidence limits on the classification, so that the algorithm terminates when enough evidence to make a clear decision has been made, and so avoiding problems with categories that have similar characteristics. A second application, to genre identification, is briefly examined. The results show that some of the problems of other language identification techniques can be avoided, and illustrate a more important point: that a statistical language process can be used to provide feedback about its own success rate. 1 I n t r o d u c t i o n Language identification is an example of a general class of problems in which we want to assign an input data stream to one of several categories as quickly and accurately as possible. It can be solved using many techniques, including knowledge-poor statistical approaches. Typically, the distribution of n-grams of characters or other objects is used to form a model. A comparison of the input against the model determines the language which matches best. Versions of this simple technique can be found in Dunning (1994) and Cavnar and Trenkle (1994), while an interesting practical implementat ion is described by Adams and Resnik (1997). A variant of the problem is considered by Sibun and Spitz (1994), and Sibun and Reynar (1996), who look at it from the point of view of Optical Character Recognition (OCR). Here, the language model for the OCR system cannot be selected until the language has been identified. They therefore work with so-called shape tokens, which give a very approximate encoding of the characters' shapes on the printed page without needing full-scale OCR. For example, all upper case letters are t reated as being one character shape, all characters with a descender are another, and so on. Sequences of character shape codes separated by white space are assembled into word shape tokens. Sibun and Spitz then determine the language on the basis of linear discriminant analysis (LDA) over word shape tokens, while Sibun and Reynar explore the use of entropy relative to training data for character shape unigrams, bigrams and trigrams. Both techniques are capable of over 90% accuracy for most languages. However, the LDA-based technique tends to perform significantly worse for languages which are similar to one another, such as the Norse languages. Relative entropy performs better, but still has some noticeable error clusters, such as confusion between Croatian, Serbian and Slovenian. What these techniques lack is a measure of when enough information has been accumulated to distinguish one language from another reliably: they examine all of the input data and then make the decision. Here we will look at a different approach which a t t empt s to overcome this by maintaining a measure of the total evidence accumulated for each language and how much confidence there is in the measure. To outline the approach: 1. The input is processed one (word shape) token at a time. For each language, we determine the probability that the token is in that language, expressed as a 95% confidence range. 2. The values for each word are accumulated into an overall score with a confidence range for the input to date, and compared both to an absolute threshold, and with	approximation algorithm;bigram;code;full scale;grams;kullback–leibler divergence;language identification;language model;linear discriminant analysis;n-gram;optical character recognition;point of view (computer hardware company);printing;shape context;statistical classification;trigram;windows 95	David Elworthy	1998	CoRR		natural language processing;language identification;speech recognition;confidence interval;computer science;artificial intelligence;machine learning;pattern recognition;statistics	NLP	-25.70038162221911	-80.09354846762135	158685
209ff48cb783fd01f4aa3073685686d608b7e891	dcg induction using mdl and parsed corpora	lenguaje natural;syntax;learning;langage naturel;syntaxe;aprendizaje;apprentissage;natural language;estimacion parametro;analizador sintaxico;parser;parameter estimation;estimation parametre;sintaxis;analyseur syntaxique;empirical evaluation	We show how partial models of natural language syntax (manually written DCGs, with parameters estimated from a parsed corpus) can be automatically extended when trained upon raw text (using MDL). We also show how we can use a parsed corpus as an alternative constraint upon estimation. Empirical evaluation suggests that a parsed corpus is more informative than a MDL-based prior. However , best results are achieved when the learner is supervised with a compression-based prior and a parsed corpus.	definite clause grammar;information;mdl (programming language);natural language;supervised learning;text corpus;treebank	Miles Osborne	1999		10.1007/3-540-40030-3_12	natural language processing;speech recognition;syntax;computer science;natural language;estimation theory	NLP	-23.601705111354352	-79.30314848030228	158785
eac0b4f51143d5e8fb1c37adfcf1e6469c01f5fc	generación de múltiples hipótesis ponderadas de reordenamiento para un sistema de traducción automática estadística	computacion informatica;statistical machine translation;filologias;info eu repo semantics article;informacion documentacion;reordering graph;linguistica;ciencias basicas y experimentales;tuples;traduccion automatica estadistica;grupo a;ciencias sociales;grupo b;grafo de reordenamiento;tuplas	Reordering is one of the most important challenges in Statistical Machine Translation (SMT) systems. This paper describes a novel strategy to face it: Statistical Machine Reordering (SMR). It consists in using the powerful techniques developed for Statistical Machine Translation (SMT) in order to translate the source language (S) into a reordered source language (S'), which allows for an improved translation into the target language (T). This technique allows to extract a weighted reordering graph which is used as SMT input. In addition, the use of classes in SMR helps to generalize word reorderings. Experiments are reported in the EPPS task in the direction English to Spanish showing a 2.4 point BLEU improvement in translation quality.		Marta R. Costa-Jussà;José A. R. Fonollosa	2008	Procesamiento del Lenguaje Natural		speech recognition;tuple;computer science;artificial intelligence;algorithm	Crypto	-26.168085590648154	-77.87441971651067	158801
7490cc8821405788bf899213d2b1470a48f7fc62	rtm results for predicting translation performance		With improved prediction combination using weights based on their training performance and stacking and multilayer perceptrons to build deeper prediction models, RTMs become the 3rd system in general at the sentence-level prediction of translation scores and achieve the lowest RMSE in English to German NMT QET results. For the document-level task, we compare document-level RTM models with sentence-level RTM models obtained with the concatenation of document sentences and obtain similar results.	concatenation;multilayer perceptron;stacking	Ergun Biçici	2018				NLP	-20.56557868266929	-75.04257199437855	159456
bb52e43fccc77b6c122d264fc79daa0a6e6a62e9	instant translation model adaptation by translating unseen words in continuous vector space		In statistical machine translation (SMT), differences between domains of training and test data result in poor translations. Although there have been many studies on domain adaptation of language models and translation models, most require supervised in-domain language resources such as parallel corpora for training and tuning the models. The necessity of supervised data has made such methods difficult to adapt to practical SMT systems. We thus propose a novel method that adapts translation models without in-domain parallel corpora. Our method infers translation candidates of unseen words by nearest-neighbor search after projecting their vector-based semantic representations to the semantic space of the target language. In our experiment of out-of-domain translation from Japanese to English, our method improved BLEU score by 0.5-1.5.	bleu;compiler;domain adaptation;language model;nearest neighbor search;statistical machine translation;supervised learning;test data;text corpus;vocabulary	Shonosuke Ishiwatari;Naoki Yoshinaga;Masashi Toyoda;Masaru Kitsuregawa	2016		10.1007/978-3-319-75487-1_5	natural language processing;machine translation;computer science;language model;machine learning;bleu;vector space;domain adaptation;test data;instant;artificial intelligence	NLP	-20.10790183480058	-78.15748059282542	159496
7c3b0b7d465da338e11a311db9012d2a9c2baf56	dual decomposition method for chinese predicate-argument structure analysis	argument structure;classification algorithm;dual decomposition semantic srl semantic role labeling s predicateargument structure;semantics;text analysis;joints;decomposition method;semantics joints pipelines inference algorithms classification algorithms optimization vehicles;error propagation;feature extraction;pipelines;semantic role labeling;classification algorithms;pattern classification;semantic;feature selection;inference algorithms;optimization;vehicles;computational linguistics;feature selection dual decomposition method chinese predicate argument structure analysis pipeline strategy error propagation predicate sense disambiguation argument classification argument identification syntactic dependency handling semantic dependency;dual decomposition;natural language processing;text analysis computational linguistics feature extraction natural language processing pattern classification;structure analysis;predicateargument structure;srl semantic role labeling s	Most of the previous work on predicate-argument(PA) structure analysis utilized pipeline strategies with several subtasks. However, when the subtasks are sequentially processed, the errors propagate from the preceding subtasks to consecutive subtasks. In this paper, the PA structure analysis is divided into three subtasks: predicate sense disambiguation, argument classification and argument identification. A dual decomposition method is used to join the latter two subtasks, which alleviates the error propagation by making the decisions from the two subtasks as consistent as possible. Furthermore the predicate sense disambiguation task is integrated into the argument classification task by handling syntactic dependencies and semantic dependencies between a predicate and its arguments. Experiments show that our approach achieves competitive results compared to the state-of-the-art systems without special feature selection procedures.	feature selection;lagrangian relaxation;monadic predicate calculus;pipeline (computing);propagation of uncertainty;software propagation;word-sense disambiguation	Yanyan Luo;Masayuki Asahara;Yuji Matsumoto	2011	2011 7th International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2011.6138234	natural language processing;computer science;machine learning;pattern recognition	NLP	-22.78959563845508	-73.33972694314	159830
09e63ccd11bd290c8f84d7e007d6afc2f4d541a7	an unsupervised method for uncovering morphological chains	article	Most state-of-the-art systems today produce morphological analysis based only on orthographic patterns. In contrast, we propose a model for unsupervised morphological analysis that integrates orthographic and semantic views of words. We model word formation in terms of morphological chains, from base words to the observed words, breaking the chains into parent-child relations. We use log-linear models with morpheme and word-level features to predict possible parents, including their modifications, for each word. The limited set of candidate parents for each word render contrastive estimation feasible. Our model consistently matches or outperforms five state-of-the-art systems on Arabic, English and Turkish.	ascii stereogram;authorization;discriminative model;dod ipv6 product certification;error analysis (mathematics);experiment;linear model;log-linear model;natural language processing;orthographic projection;tacl;unsupervised learning	Kumaravelu Narasimhan;Regina Barzilay;Tommi S. Jaakkola	2015	Transactions of the Association for Computational Linguistics	10.1162/tacl_a_00130	engineering;data science;management;operations research	NLP	-22.67201351731146	-77.08079610067549	160076
f818447ce70181ca3cdbc129de82cb36acee0cbc	projecting embeddings for domain adaption: joint modeling of sentiment analysis in diverse domains		Domain adaptation for sentiment analysis is challenging due to the fact that supervised classifiers are very sensitive to changes in domain. The two most prominent approaches to this problem are structural correspondence learning and autoencoders. However, they either require long training times or suffer greatly on highly divergent domains. Inspired by recent advances in cross-lingual sentiment analysis, we provide a novel perspective and cast the domain adaptation problem as an embedding projection task. Our model takes as input two mono-domain embedding spaces and learns to project them to a bi-domain space, which is jointly optimized to (1) project across domains and to (2) predict sentiment. We perform domain adaptation experiments on 20 source-target domain pairs for sentiment classification and report novel state-of-the-art results on 11 domain pairs, including the Amazon domain adaptation datasets and SemEval 2013 and 2016 datasets. Our analysis shows that our model performs comparably to state-of-the-art approaches on domains that are similar, while performing significantly better on highly divergent domains. Our code is available at https://github.com/jbarnesspain/domain_blse Title and Abstract in Basque Domeinu-Egokitzapenerako Bektore Proiekzioa: Domeinu Urrunetarako Sentimenduen Analisiko Eredu Bateratua Sentimenduen analisirako domeinu-egokitzapena erronka handi bat da oraindik, domeinu arteko ezberdintasunek ondorio esanguratsuak izan baititzakete sailkatzaile gainbegiratuentzat. Arazo honi aurre egiteko bi hurbilpen arrakastatsuenak egiturazko kidetasunaren ikasketa (structural correspondence learning) eta autoencoder-ak dira. Hala ere, denbora asko behar dute sistema entrenatzeko edo, domeinu arteko distantzia handia denean, ez dituzte emaitza onak lortzen. Hizkuntza-arteko sentimenduen analisian egindako azken lanetan oinarrituta, ikuspuntu berri bat eskaintzen dugu, domeinuaren egokitzapen ataza bektore proiekzio ataza gisa planteatuta. Gure sistemaren sarrera domeinu banako bi bektore espazio dira, zeinak sistemak espazio berri batera proiektatzen ikasten duen. Sistema hau optimizatuta dago (1) domeinu batetik besterako proiekzioa egiteko eta (2) esaldi baten sentimendua aurresateko. 20 jatorri-xede domeinu pareetan esperimentuak burutu ditugu eta 11 kasutan artearen egoerako emaitzarik onenak lortzen ditugu Amazon-eko domeinu-egokitzapeneko eta SemEval 2013 eta 2016 datu-multzoetan. Gure analisian ikus daitekeenez, gure hurbilpena artearen egoerako sistemen pareko moldatzen da antzeko domeinuetan, baina emaitza hobeak lortzen ditu oso domeinu ezberdinetan. Gure kodea eskuragarri dago helbide honetan: https://github.com/jbarnesspain/domain_blse. This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/	autoencoder;book;dago dazzler;domain adaptation;experiment;lexicon;n-gram;semeval;sentiment analysis;skip list;supervised learning;the movies;word lists by frequency	Jeremy Barnes;Roman Klinger;Sabine Schulte im Walde	2018			sentiment analysis;artificial intelligence;semeval;machine learning;domain adaptation;computer science;pattern recognition;embedding	NLP	-19.83920982754315	-73.6891106201104	160118
d437b45e8b9bc3008c06fc70854d8e495751bfac	statistical sandhi splitter and its effect on nlp applications		This paper revisits the work of (Kuncham et al., 2015) which developed a statistical sandhi splitter (SSS) for agglutinative languages that was tested for Telugu and Malayalam languages. Handling compound words is a major challenge for Natural Language Processing (NLP) applications for agglutinative languages. Hence, in this paper we concentrate on testing the effect of SSS on the NLP applications like Machine Translation, Dialogue System and Anaphora Resolution and show that the accuracy of these applications is consistently improved by using SSS. We shall also discuss in detail the performance of SSS on these applications.	anaphora (linguistics);demultiplexer (media file);dialog system;freenet;lumpers and splitters;machine translation;natural language processing	Prathyusha Kuncham;Kovida Nelakuditi;Radhika Mamidi	2015			machine translation;malayalam;artificial intelligence;natural language processing;splitter;computer science;sandhi;sss*;compound;speech recognition;agglutinative language	NLP	-24.521718975263614	-79.80367423265476	160222
dcb193bdf23622645452cba7f789c75157eda253	multi-span statistical language modeling for large vocabulary speech recognition	speech recognition	The goal of multi-span language modeling is to integrate the various constraints, both local and global, that are present in the language. In this paper, local constraints are captured via the usual n-gram approach, while global constraints are taken into account through the use of latent semantic analysis. An integrative formulation is derived for the combination of these two paradigms, resulting in an entirely data-driven, multi-span framework for large vocabulary speech recognition. Because of the inherent complementarity in the two types of constraints, the performance of the integrated language model compares favorably with the corresponding n-gram performance. Both perplexity and average word error rate gures are reported and discussed.	complementarity theory;language model;latent semantic analysis;n-gram;perplexity;speech recognition;vocabulary;word error rate	Jerome R. Bellegarda	1998			cued speech;speech recognition;speech corpus;language identification;acoustic model;language model;natural language processing;computer science;computational linguistics;artificial intelligence;cache language model;vocabulary	NLP	-21.827310989824543	-79.91404523800966	160559
ca3fa5cbddf6ab103531ce136ab27648d0603e13	automatic acquisition of class-based rules for word alignment	conference paper	In this paper, we describe an algorithm for aligning words with their translation in a bilingual corpus. Existing algorithms require enormous bilingual data to train statistical word-to-word translation models. Using word-based approach, frequent words with consistent translation can be aligned at a high precision rate. However, less frequent words or words with diverse translations usually do not have statistically significant evidence for confident alignment. Incomplete or incorrect alignments consequently result. Our algorithm attempts to handle the problem using a hierarchical class-based approximation of translation probabilities. The translation probabilities are estimated using class-based models on 3 levels of specificity. We found that the algorithm can provide translation probability for more word pairs at the cost of slightly lower degree of precision, even when a small corpus was used in training. We have achieved an application rate of 81.8% and precision rate of 93.3%. The algorithm also offer the advantage of producing word-sense disambiguation information.	algorithm;approximation;data structure alignment;sensitivity and specificity;word sense;word-sense disambiguation	Sur-Jin Ker;Jason J. S. Chang	1995			natural language processing;speech recognition;computer science;information retrieval	NLP	-22.608555007124316	-77.13963752561332	160756
37c044c7f7237213601106214909a95b5a827c67	clustering comparable corpora for bilingual lexicon extraction	homogeneity feature;comparable corpus;bilingual lexicon;bilingual corpus;bilingual lexicon extraction;corpus comparability;clustering-based approach;original corpus;homogeneous corpus;better quality;previous approach	We study in this paper the problem of enhancing the comparability of bilingual corpora in order to improve the quality of bilingual lexicons extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches.	cluster analysis;experiment;lexicon;text corpus;vocabulary;whole earth 'lectronic link	Bo Li;Éric Gaussier;Akiko Aizawa	2011			natural language processing;speech recognition;computer science;linguistics	NLP	-25.316277932404805	-74.1091052434772	160784
c3fa509e53d94c996340e015b11e415fd1ac56db	training set similarity based parameter selection for statistical machine translation		Log-linear model based statistical machine translation systems (SMT) are usually composed of multiple feature functions. Each feature function is assigned a weight as a model parameter. In this paper, we consider that different input source sentences may have discrepant needs for model parameters. To adapt the model to different inputs, we propose a model parameters selection method for log-linear model based SMT systems. The method is mainly based on the characteristics of different feature functions themselves without any assumption on unseen test sets. Experimental results on two language pairs (Zh-En and Ug-Zh) show that our method leads to the improvements up to 2.4 and 2.2 BLEU score respectively, and it also shows the good interpretability of our proposed method.	statistical machine translation	Xuewen Shi;Heyan Huang;Ping Jian;Yi-Kun Tang	2018		10.1007/978-3-319-96890-2_6	machine learning;log-linear model;machine translation;artificial intelligence;computer science;bleu;interpretability;training set	NLP	-19.862883484636452	-77.73701659409383	160908
d9afed7921cc91ce6b70176acc5937b80550f1b1	full-coverage identification of english light verb constructions		The identification of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates.	information extraction;lvcmos;machine learning;machine translation;minimal working example;natural language processing;preprocessor;text corpus	T. Nagy IstvánNagy;Veronika Vincze;Richárd Farkas	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-25.456356994371436	-73.20878740621438	161077
08c01db71257087d45716d99d3c9549b97b79442	an unsupervised hindi stemmer with heuristic improvements	heuristics and optimal word segment;information retrieval;unsupervised morphological analyzer;hindi;word segmentation;indexation	Stemmers are used to convert inflected words into their root or stem. Stem does not necessarily correspond to linguistic root of a word. Stemming improve performance by reducing morphologically variants into same words. This paper presents an approach is to develop unsupervised Hindi stemmer. This paper focus on the development of an unsupervised stemmer for Hindi and evaluation of approach using manually segmented words. We evaluate our approach on 1000-1000 words randomly extracted words (only) from Hindi WordNet1 data base. The training data has been constructed by extracting 106403 words extracted from EMILLE2 corpus. The observed accuracy was found to be 89.9% after applying some heuristic measures. The F-score was 94.96%. As the algorithm does not require any language specific information, it can be applied to other Indian languages as well. We also evaluate the effect of stemmer in terms of reducing size of index for Hindi information retrieval task. The results have been compared with light weight stemmer [10] and UMass stemmer [17]. Test run shows that our stemmer outperforms both the stemmer.	algorithm;database;heuristic;information retrieval;randomness;stemming	Amaresh Kumar Pandey;Tanveer J. Siddiqui	2008		10.1145/1390749.1390765	natural language processing;speech recognition;hindi;computer science;pattern recognition;information retrieval	NLP	-24.398227686435508	-77.38853263130751	161143
4a27822c8718bcfdd01fd5cc9e75dd1df9f9add5	independence and commitment: assumptions for rapid training and execution of rule-based pos taggers	decision list model;rapid tagger execution;rule-based pos tagging method;large data set;brill method;rapid training;rule-based pos taggers;rule interaction;tagging accuracy;rule based	This paper addresses the rule-based POS tagging method of Brill, and questions the importance of rule interactions to its performance. Adopting two assumptions that serve to exclude rule interactions during tagging and training, we arrive at some variants of Brill's approach that are instances of decision list models. These models allow for both rapid training on large data sets and rapid tagger execution, giving tagging accuracy that is comparable to, or better than the Brill method.	brill tagger;decision list;interaction;logic programming;part-of-speech tagging;rule 90;tag (metadata)	Mark Hepple	2000			rule-based system;computer science;artificial intelligence;data mining;algorithm	AI	-25.08485443105205	-78.21037450556176	161152
b128d69fa7ccba2a80a6029d2208feddb246b6bd	applying machine learning to text segmentation for information retrieval	cross lingual information retrieval;busqueda informacion;evaluation performance;performance evaluation;learning;information retrieval system;information retrieval;evaluacion prestacion;retrieval performance;systeme recherche;segmentation;chino;search system;aprendizaje;word segmentation;apprentissage;machine learning;chinese information retrieval;sistema investigacion;recherche information;algorithme em;mutual information;algoritmo em;multilinguisme;chinois;performance recherche;chinese;em algorithm;segmentacion;text segmentation;multilingualism;multilinguismo	We propose a self-supervised word segmentation technique for text segmentation in Chinese information retrieval. This method combines the advantages of traditional dictionary based, character based and mutual information based approaches, while overcoming many of their shortcomings. Experiments on TREC data show this method is promising. Our method is completely language independent and unsupervised, which provides a promising avenue for constructing accurate multi-lingual or cross-lingual information retrieval systems that are flexible and adaptive. We find that although the segmentation accuracy of self-supervised segmentation is not as high as some other segmentation methods, it is enough to give good retrieval performance. It is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in Chinese information retrieval. However, for Chinese, we find that the relationship between segmentation and retrieval performance is in fact nonmonotonic; that is, at around 70% word segmentation accuracy an over-segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance. We demonstrate this effect by presenting an empirical investigation of information retrieval on Chinese TREC data, using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44% to 95%, including 70% word segmentation accuracy from our self-supervised word-segmentation approach. It appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters, while they are broken up by less accurate (but reasonable) segmenters, to a surprising advantage. This suggests that words themselves might be too broad a notion to conveniently capture the general semantic meaning of Chinese text. Our research suggests machine learning techniques can play an important role in building adaptable information retrieval systems and different evaluation standards for word segmentation should be given to different applications.	algorithm;collocation;cross-language information retrieval;dictionary;experiment;machine learning;mutual information;text retrieval conference;text segmentation;unsupervised learning	Xiangji Huang;Fuchun Peng;Dale Schuurmans;Nick Cercone;Stephen E. Robertson	2003	Information Retrieval	10.1023/A:1026028229881	natural language processing;text segmentation;visual word;speech recognition;computer science;machine learning;segmentation-based object categorization;world wide web;information retrieval	Web+IR	-24.837274948393222	-77.35667960804737	161427
1b98ecc801d4435fd57c6ef87b4c24cf6b3b58ce	document summarization via guided sentence compression		Joint compression and summarization has been used recently to generate high quality summaries. However, such word-based joint optimization is computationally expensive. In this paper we adopt the ‘sentence compression + sentence selection’ pipeline approach for compressive summarization, but propose to perform summary guided compression, rather than generic sentence-based compression. To create an annotated corpus, the human annotators were asked to compress sentences while explicitly given the important summary words in the sentences. Using this corpus, we train a supervised sentence compression model using a set of word-, syntax-, and documentlevel features. During summarization, we use multiple compressed sentences in the integer linear programming framework to select salient summary sentences. Our results on the TAC 2008 and 2011 summarization data sets show that by incorporating the guided sentence compression model, our summarization system can yield significant performance gain as compared to the state-of-the-art.	analysis of algorithms;automatic summarization;display resolution;experiment;integer programming;linear programming;mathematical optimization;text corpus	Chen Li;Fei Liu;Fuliang Weng;Yang Liu	2013			natural language processing;speech recognition;information retrieval	NLP	-19.8671349446656	-75.96150001127826	162492
5e15d72789b26b5c4426580d6bbbf8352061233e	successfully detecting and correcting false friends using channel profiles	text;analisis estadistico;correction erreur;optical character recognition;texte;probabilistic approach;correction automatique;desambiguisacion;automatic correction;dictionnaire;false friends;reconnaissance caractere;statistical analysis;error dictionaries;enfoque probabilista;approche probabiliste;error correction;correccion automatica;dictionaries;analyse statistique;disambiguation;reconocimento optico de caracteres;correccion error;desambiguisation;texto;character recognition;diccionario;reconocimiento caracter;reconnaissance optique caractere	The detection and correction of false friends—also called real-word errors—is a notoriously difficult problem. On realistic data, the break-even point for automatic correction so far could not be reached: the number of additional infelicitous corrections outnumbered the useful corrections. We present a new approach where we first compute a profile of the error channel for the given text. During the correction process, the profile (1) helps to restrict attention to a small set of “suspicious” lexical tokens of the input text where it is “plausible” to assume that the token represents a false friend. In this way, recognition of false friends is improved. Furthermore, the profile (2) helps to isolate the “most promising” correction suggestion for “suspicious” tokens. Using a conventional word trigram statistics for disambiguation we obtain a correction method that can be successfully applied to unrestricted text. In experiments for OCR documents, we show significant accuracy gains by fully automatic correction of false friends.	experiment;sensor;trigram;word-sense disambiguation	Ulrich Reffle;Annette Gotscharek;Christoph Ringlstetter;Klaus U. Schulz	2009	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-009-0091-y	error detection and correction;speech recognition;computer science;artificial intelligence;optical character recognition;algorithm;statistics	NLP	-26.02468408163999	-78.65081638617869	162959
5373977a25077657c45a9f475a58e4d3755e2ae6	improving phrase-based translation with prototypes of short phrases	translation task;improved sentence translation;traditional decoder;phrase-based decoder;improving phrase-based translation;source text;phrase pair;linguistically rich feature;short sequence;short phrase;additional bilingual phrase pair	We investigate methods of generating additional bilingual phrase pairs for a phrasebased decoder by translating short sequences of source text. Because our translation task is more constrained, we can use a model that employs more linguistically rich features than a traditional decoder. We have implemented an example of this approach. Experimental results suggest that the phrase pairs produced by our method are useful to the decoder, and lead to improved sentence translations.	statistical machine translation	Frank Liberato;Behrang Mohit;Rebecca Hwa	2010			natural language processing;speech recognition;computer science;phrase search;linguistics	NLP	-21.96620935415604	-77.30574963576998	163125
9af5c320f1ab4e881c1aa4e35d7c7f10d5d2405d	unsupervised models for morpheme segmentation and morphology learning	morpheme lexicon and segmentation;finnish data;model family;unsupervised model;english data;highly-inecting and compounding lan- guages;raw text data;efficient storage;benchmark algorithm;lexicon stores information;probabilistic maximum;language independent methods;posteriori framework;morphology learning;languages;highly inflecting and compounding languages;morpheme segmentation task;categories and subject descriptors: ... ...: ... general terms: algorithms;maximum a posteriori map estimation;experimentation;language independence;performance additional key words and phrases: ecien t storage;unsupervised learning;lengthy sequence;word segmentation;algorithms	We present a model family called Morfessor for the unsupervised induction of a simple morphology from raw text data. The model is formulated in a probabilistic maximum a posteriori framework. Morfessor can handle highly inflecting and compounding languages where words can consist of lengthy sequences of morphemes. A lexicon of word segments, called morphs, is induced from the data. The lexicon stores information about both the usage and form of the morphs. Several instances of the model are evaluated quantitatively in a morpheme segmentation task on different sized sets of Finnish as well as English data. Morfessor is shown to perform very well compared to a widely known benchmark algorithm, in particular on Finnish data.	galaxy morphological classification;unsupervised learning	Mathias Creutz;Krista Lagus	2007	TSLP	10.1145/1217098.1217101	natural language processing;unsupervised learning;text segmentation;speech recognition;computer science;machine learning;pattern recognition	Vision	-20.787245155222873	-77.28473023855848	163137
8f0fa6de779ead2efe478dd1903bb5c9e2c98f0a	experiments on the use of corpus-based word bi-gram in chinese word segmentation	word frequency;natural languages character recognition image segmentation statistical analysis;image segmentation;statistical method;natural languages;maximum matching;discriminant function;statistical analysis;language processing;chinese word segmentation;natural languages frequency measurement natural language processing dictionaries statistical analysis particle separators probability;ambiguity strings corpus based word bi gram chinese word segmentation chinese sentence segmentation corpus based statistical method word bi gram statistical measures segmentation ambiguities bidirectional maximum matching method word frequency discriminate function;character recognition	The first step of Chinese language processing is to segment a Chinese sentence into a sequence of words due to the fact that there is no original separation between adjacent words. An efficient corpus-based statistical method is adopted here to address such a problem. In this paper, some word BI-gram statistical measures derived from corpus are employed to remove the segmentation ambiguities. To segment a Chinese sentence, a bidirectional maximum matching method is firstly used to do pre-matching in order to get segmentation candidates and locate possible ambiguities. The statistical measures based on word BI-gram information and word frequency will be used to construct a discriminate function, which is applied to ambiguity strings in order to get an utmost correct segmentation. Experimental results are analyzed to describe the features and limitations of this approach, and preliminary results indicate that our approach is compared favorably to other existing techniques.	text corpus;text segmentation	Ruifeng Xu;Daniel S. Yeung	1998		10.1109/ICSMC.1998.727508	natural language processing;text segmentation;speech recognition;computer science;segmentation-based object categorization;pattern recognition;discriminant function analysis;word lists by frequency;image segmentation;speech segmentation;natural language;scale-space segmentation;matching	NLP	-24.590531616281883	-77.53458541261038	163308
3b211ae088f757a15a050f8e58f7c9a6514a707d	a unified annotation scheme for the semantic/pragmatic components of definiteness	definiteness;annotation scheme;communicative functions	We present a definiteness annotation scheme that captures the semantic, pragmatic, and discourse information associated with noun phrases, which we call communicative functions. A survey of the linguistics literature suggests that definiteness does not express a single communicative function but is a grammaticalization of many such functions, for example, identifiability, familiarity, uniqueness, and specificity. Our annotation scheme unifies ideas from previous research on definiteness while attempting to remove redundancy. The scheme encodes the communicative functions of definiteness rather than the grammatical forms of definiteness. We assume that the communicative functions are largely maintained across languages while the grammaticalization of this information may vary. Corpora that are annotated using communicative functions can be used to train classifiers, offering data-driven insights into the grammaticalization of definiteness in different languages. We release our annotated corpora for English and Hindi as well as sample annotations for Hebrew and Russian, together with an annotation manual.	sensitivity and specificity;text corpus	Archna Bhatia;Mandy Simons;Lori S. Levin;Yulia Tsvetkov;Chris Dyer;Jordan Bender	2014			natural language processing;linguistics	NLP	-26.36939686108016	-74.99586201156457	163360
39ecd41c3a8d6f113218007ce30d3d4be55f36e0	deriving phonetic transcriptions and discovering word segmentations for speech-to-speech translation in low-resource settings		We investigate speech-to-speech translation where one language does not have a well-defined written form. We use English-Spanish and Mandarin-English bitext corpora in order to provide both gold-standard text-based translations and experimental results for different levels of automatically derived symbolic representations from speech. We constrain our experiments such that the methods developed can be extended to low-resource languages. We derive different phonetic representations of the source texts in order to model the kinds of transcriptions that can be learned from low-resource-language speech data. We experiment with different methods of clustering the elements of the phonetic representations together into word-like units. We train MT models on the resulting texts, and report BLEU scores for the different representations and clustering methods in order to compare their effectiveness. Finally, we discuss our findings and suggest avenues for future research.	bleu;cluster analysis;experiment;machine translation;parallel text;super robot monkey team hyperforce go!;text corpus;text-based (computing)	Andrew Wilkinson;Tiancheng Zhao;Alan W. Black	2016		10.21437/Interspeech.2016-1319	linguistics	NLP	-25.38311309297892	-75.15511725920015	163921
28c938d64d360cdf97c3584d0452a088b56e85ca	touch-based pre-post-editing of machine translation output		We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach.	bleu;experiment;interactive machine translation;postediting	Benjamin Marie;Aurélien Max	2015			bleu;simulation;transfer-based machine translation;computer science;theoretical computer science;machine learning;algorithm	NLP	-20.638487295176283	-78.07610539279901	164041
088659e02440088bb9b65258ef80275040456154	chinese pos disambiguation and unknown word guessing with lexicalized hmms	human information systems;natural languages;human computer systems;natural language interface;natural language processors;article;text processing software	This article presents a lexicalized HMM-based approach to Chinese part-of-speech (POS) disambiguation and unknown word guessing (UWG). In order to explore word-internal morphological features for Chinese POS tagging, four types of pattern tags are defined to indicate the way lexicon words are used in a segmented sentence. Such patterns are combined further with POS tags. Thus, Chinese POS disambiguation and UWG can be unified as a single task of assigning each known word to input a proper hybrid tag. Furthermore, a uniformly lexicalized HMM-based tagger also is developed to perform this task, which can incorporate both internal word-formation patterns and surrounding contextual information for Chinese POS tagging under the framework of HMMs. Experiments on the Peking University Corpus indicate that the tagging precision can be improved with efficiency by the proposed approach.	brill tagger;brown corpus;hidden markov model;lexicon;microsoft word for mac;part-of-speech tagging;word-sense disambiguation	Guohong Fu;Kang-Kwong Luke	2006	IJTHI	10.4018/jthi.2006010103	natural language processing;language identification;natural language programming;speech recognition;natural language user interface;computer science;linguistics;natural language	NLP	-25.655938377151326	-77.17697192945087	164136
54ad4be7950f54586d9051a171529a05adaa70f7	description of the ncu chinese word segmentation and part-of-speech tagging for sighan bakeoff 2007		In Chinese, most of the language processing starts from word segmentation and part-of-speech (POS) tagging. These two steps tokenize the word from a sequence of characters and predict the syntactic labels for each segmented word. In this paper, we present two distinct sequential tagging models for the above two tasks. The first word segmentation model was basically similar to previous work which made use of conditional random fields (CRF) and set of predefined dictionaries to recognize word boundaries. Second, we revise and modify support vector machine-based chunking model to label the POS tag in the tagging task. Our method in the WS task achieves moderately rank among all participants, while in the POS tagging task, it reaches very competitive results.	categorization;conditional random field;dictionary;document classification;information retrieval;lexical analysis;part-of-speech tagging;shallow parsing;support vector machine;text segmentation;unsupervised learning	Yu-Chieh Wu;Jie-Chi Yang;Yue-Shi Lee	2008			computer science;syntax;pattern recognition;support vector machine;artificial intelligence;chunking (psychology);conditional random field;part-of-speech tagging;text segmentation	NLP	-22.98067884269753	-73.53387658545182	164155
f3fb4cdd769553cdf75ce31ad866634b2022d045	an intelligent sample selection approach to language model adaptation for hand-written text recognition	sample selection domain adaptation language modeling handwritten text recognition;document image intelligent sample selection language model adaptation handwritten text recognition;training adaptation models data models interpolation measurement dictionaries text recognition;text detection document image processing feature selection handwritten character recognition natural language processing text analysis	We present an intelligent sample selection approach to language model adaptation for handwritten text recognition, which exploits a combination of in-domain and out-of-domain data for construction of language models. In comparison to approaches proposed in the literature, our approach is characterized by a careful consideration of the criteria used for ranking samples and an innovative approach to sample selection which iteratively extends the training set for two language models. We propose two methods, in which agreement or disagreement of two ranking criteria (one for each language model) guides the selection of samples to add to the training sets of the models. Both approaches are shown to clearly outperform a strong baseline consisting of a carefully tuned interpolation of in-domain and out-of-domain language models.	baseline (configuration management);co-training;domain adaptation;information;interpolation;language model;optical character recognition;test set	Jafar Tanha;Jesse de Does;Katrien Depuydt	2014	2014 14th International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2014.65	natural language processing;text graph;language identification;speech recognition;document processing;intelligent character recognition;computer science;intelligent word recognition;noisy text analytics;pattern recognition	NLP	-21.490503257554316	-74.64766262367294	164307
268e6dffed8d5169ee26ff0c63b37a94a1011780	statistical sandhi splitter for agglutinative languages		Sandhi splitting is a primary and an important step for any natural language processing (NLP) application for languages which have agglutinative morphology. This paper presents a statistical approach to build a sandhi splitter for agglutinative languages. The input to the model is a valid string in the language and the output is a split of that string into meaningful word/s. The approach adopted comprises of two stages namely Segmentation and Word generation, both of which use conditional random fields (CRFs). Our approach is robust and language independent. The results for two Dravidian languages viz. Telugu and Malayalam show an accuracy of 89.07% and 90.50% respectively.	conditional random field;demultiplexer (media file);lumpers and splitters;mathematical morphology;natural language processing;robustness (computer science);string (computer science);variable splitting;viz: the computer game	Prathyusha Kuncham;Kovida Nelakuditi;Sneha Nallani;Radhika Mamidi	2015		10.1007/978-3-319-18111-0_13	natural language processing;artificial intelligence;dravidian languages;malayalam;computer science;telugu;sandhi;compound;text segmentation;conditional random field;agglutinative language	NLP	-23.73585119602167	-77.03761047048279	165118
5f06240ed072915012ba0e8ef9b09e8119a9bd31	a fast, accurate deterministic parser for chinese	accurate deterministic parser;best model;state-of-the-art parsers;gold-standard part-of-speech tag;parser compute;svm parser;deterministic parser;parse tree;linear time;accurate result;chinese constituency parsing;bottom up;gold standard	We present a novel classifier-based deterministic parser for Chinese constituency parsing. Our parser computes parse trees from bottom up in one pass, and uses classifiers to make shift-reduce decisions. Trained and evaluated on the standard training and test sets, our best model (using stacked classifiers) runs in linear time and has labeled precision and recall above 88% using gold-standard part-of-speech tags, surpassing the best published results. Our SVM parser is 2-13 times faster than state-of-the-art parsers, while producing more accurate results. Our Maxent and DTree parsers run at speeds 40-270 times faster than state-of-the-art parsers, but with 5-6% losses in accuracy.	brown corpus;deterministic parsing;natural language;parse tree;parser;part-of-speech tagging;precision and recall;time complexity;top-down and bottom-up design	Mengqiu Wang;Kenji Sagae;Teruko Mitamura	2006			natural language processing;time complexity;parser combinator;speech recognition;canonical lr parser;gold standard;computer science;glr parser;top-down and bottom-up design;programming language;lr parser;simple lr parser	NLP	-22.433603060044444	-76.01936176357044	165204
59ace2e5456e5b69eec49c95e352ff65926a889f	the crotal srl system : a generic tool based on tree-structured crf	generic tool;tree-structured crf;account hierarchical relation;configurable crf library;crotal system;conll09 shared task;crotal srl system;tree structure	We present the Crotal system, used in the CoNLL09 Shared Task. It is based on XCRF, a highly configurable CRF library which can take into account hierarchical relations. This system had never been used in such a context thus the performance is average, but we are confident that there is room for progression.	color gradient;conditional random field	Erwan Moreau;Isabelle Tellier	2009			simulation;speech recognition;computer science;communication	NLP	-22.274569431143462	-74.43238382594807	165580
83a6fb9ef05577b4f34aa889cdde87da928df939	rule-based translation of spanish verb-noun combinations into basque		This paper presents a method to improve the translation of Verb-Noun Combinations (VNCs) in a rule-based Machine Translation (MT) system for SpanishBasque. Linguistic information about a set of VNCs is gathered from the public database Konbitzul, and it is integrated into the MT system, leading to an improvement in BLEU, NIST and TER scores, as well as the results being significantly better according to human evaluators.	bleu;database;logic programming;minimal working example;nist (metric);rule-based machine translation;text corpus	Uxoa Iñurrieta Urmeneta;Itziar Aduriz;Arantza Díaz de Ilarraza;Gorka Labaka;Kepa Sarasola	2017			rule-based system;linguistics;noun;verb;mathematics	NLP	-26.229476690685757	-75.95655670255726	165843
87a06007aa231a4ad262a1c1aba648b418391744	combining pcfg-la models with dual decomposition: a case study with function labels and binarization	parsing models;machine learning;computational linguistics;dual decomposition	It has recently been shown that different NLP models can be effectively combined using dual decomposition. In this paper we demonstrate that PCFG-LA parsing models are suitable for combination in this way. We experiment with the different models which result from alternative methods of extracting a grammar from a treebank (retaining or discarding function labels, left binarization versus right binarization) and achieve a labeled Parseval F-score of 92.4 on Wall Street Journal Section 23 – this represents an absolute improvement of 0.7 and an error reduction rate of 7% over a strong PCFG-LA product-model baseline. Although we experiment only with binarization and function labels in this study, there is much scope for applying this approach to other grammar extraction strategies.	algorithm;baseline (configuration management);binary image;experiment;lagrangian relaxation;natural language processing;parsing;stochastic context-free grammar;the wall street journal;treebank	Joseph Le Roux;Antoine Rozenknop;Jennifer Foster	2013			natural language processing;speech recognition;computer science;artificial intelligence;computational linguistics;machine learning;algorithm	NLP	-21.804263030587073	-75.91459431958778	166100
7cd4378a2b3ffb1b35a455c500cc1f8c51ebf76b	fusion of word and letter based metrics for automatic mt evaluation	linguistic feature;mt evaluation metric;automatic mt evaluation;time-split data validation;language independent feature;independent metrics;current effort;wmt data;human translation;current language;word level metrics	With the progress in machine translation, it becomes more subtle to develop the evaluation metric capturing the systems’ differences in comparison to the human translations. In contrast to the current efforts in leveraging more linguistic information to depict translation quality, this paper takes the thread of combining language independent features for a robust solution to MT evaluation metric. To compete with finer granularity of modeling brought by linguistic features, the proposed method augments the word level metrics by a letter based calculation. An empirical study is then conducted over WMT data to train the metrics by ranking SVM. The results reveal that the integration of current language independent metrics can generate well enough performance for a variety of languages. Time-split data validation is promising as a better training setting, though the greedy strategy also works well.	cross-validation (statistics);data validation;feature selection;greedy algorithm;ibm notes;information gain in decision trees;kullback–leibler divergence;machine translation;principal component analysis;ranking svm;sting;string metric	Muyun Yang;Junguo Zhu;Sheng Li;Tiejun Zhao	2013			natural language processing;speech recognition;computer science;artificial intelligence;machine learning;data mining	AI	-20.50332220571087	-74.69421131062404	167082
13fbf3d657dd8d684d8e0f7ed30488c23a1073da	offline bilingual word vectors, orthogonal transformations and the inverted softmax		Usually bilingual word vectors are trained “online”. Mikolov et al. (2013a) showed they can also be found “offline”; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel “inverted softmax” for identifying translation pairs, with which we improve the precision @1 of Mikolov’s original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a “pseudo-dictionary” from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.	compiler;dictionary;online and offline;singular value decomposition;softmax function;test set;text corpus;word embedding	Samuel L. Smith;David H. P. Turban;Steven Hamblin;Nils Y. Hammerla	2017	CoRR		transfer of learning;artificial intelligence;natural language processing;machine learning;computer science;singular value decomposition;linear map;speech recognition;test set;softmax function	NLP	-19.999221934081344	-78.14356012882078	167512
0c2ed3dd5db0944e412e2d9104daaf208019014a	broadcast news lm adaptation using contemporary texts.	broadcast news;word error rate;out of vocabulary;probability distribution;adaptive method;speech recognition;language model	This paper investigates the problem of dynamically updating the language model (LM) of a broadcast news speech recognition system, in order to cope with language and topic changes, typical of the news domain. Statistical adaptation methods are proposed that exploit written news sources which are daily available on the Internet, i.e. newswires and newspapers. Specifically, LM adaptation is performed by extending the basic lexicon, in order to minimize the out-of-vocabulary (OOV) rate, and by adapting the word probability distribution on the contemporary data. Experiments performed on 19 newscasts showed relative reductions of 58% on the OOV rate, 16% on the perplexity, and 4% on the word error rate.	experiment;language model;lexicon;perplexity;speech recognition;vocabulary;word error rate	Marcello Federico;Nicola Bertoldi	2001			natural language processing;probability distribution;speech recognition;word error rate;computer science;language model	NLP	-22.157655782147216	-79.88679159204014	168050
e90db5f87d782f4d2fb2ba3e6888925a50c3bc60	handwriting recognition with multigrams		We introduce a novel handwriting recognition approach based on sub-lexical units known as multigrams of characters, that are variable lengths characters sequences. A Hidden Semi Markov model is used to model the multigrams occurrences within the target language corpus. Decoding the training language corpus with this model provides an optimized multigram lexicon of reduced size with high coverage rate of OOV compared to the traditional word modeling approach. The handwriting recognition system is composed of two components: the optical model and the statistical n-grams of multigrams language model. The two models are combined together during the recognition process using a decoding technique based on Weighted Finite State Transducers (WFST). We experiment the approach on two Latin language datasets (the French RIMES and English IAM datasets) and we show that it outperforms words and character models language models for high Out Of Vocabulary (OOV) words rates, and that it performs similarly to these traditional models for low OOV rates, with the advantage of a reduced complexity.	compiler;decoding methods;finite-state transducer;grams;handwriting recognition;hidden markov model;hidden semi-markov model;identity management;language model;lexicon;markov chain;n-gram;regular expression;semiconductor industry;statistical model;text corpus;vocabulary	Wassim Swaileh;Thierry Paquet;Yann Soullard;Pierrick Tranouez	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.31	pattern recognition;computer science;artificial intelligence;lexicon;hidden semi-markov model;handwriting recognition;data modeling;hidden markov model;markov process;language model;vocabulary	NLP	-21.49779939733489	-78.12105673578456	169259
dfc2245c61a634feea35edd0ffe6360a316fe4a3	evaluating a pivot-based approach for bilingual lexicon extraction	translating;vocabulary;semantics;humans;information storage and retrieval;natural language processing;multilingualism	A pivot-based approach for bilingual lexicon extraction is based on the similarity of context vectors represented by words in a pivot language like English. In this paper, in order to show validity and usability of the pivot-based approach, we evaluate the approach in company with two different methods for estimating context vectors: one estimates them from two parallel corpora based on word association between source words (resp., target words) and pivot words and the other estimates them from two parallel corpora based on word alignment tools for statistical machine translation. Empirical results on two language pairs (e.g., Korean-Spanish and Korean-French) have shown that the pivot-based approach is very promising for resource-poor languages and this approach observes its validity and usability. Furthermore, for words with low frequency, our method is also well performed.	bitext word alignment;data structure alignment;dictionary [publication type];equivalent weight;estimated;large;lexicon;linguistics;numerous;parallel text;pivot device component;programming languages;random seed;statistical machine translation;text corpus;usability;interest;poly(tetramethylene succinate-co-tetramethylene adipate)	Jae-Hoon Kim;Hong-Seok Kwon;Hyeong-Won Seo	2015		10.1155/2015/434153	natural language processing;speech recognition;computer science;semantics	NLP	-25.415183010246018	-75.20560624073909	169439
98b26ed90215c6815809adfc66848d09752cdd7d	what do we need to know about an unknown word when parsing german		We propose a new type of subword embedding designed to provide more information about unknown compounds, a major source for OOV words in German. We present an extrinsic evaluation where we use the compound embeddings as input to a neural dependency parser and compare the results to the ones obtained with other types of embeddings. Our evaluation shows that adding compound embeddings yields a significant improvement of 2% LAS over using word embeddings when no POS information is available. When adding POS embeddings to the input, however, the effect levels out. This suggests that it is not the missing information about the semantics of the unknown words that causes problems for parsing German, but the lack of morphological information for unknown words. To augment our evaluation, we also test the new embeddings in a language modelling task that requires both syntactic and semantic information.	language model;need to know;parsing;substring;text-based (computing);word embedding	Bich-Ngoc Do;Ines Rehbein;Anette Frank	2017			artificial intelligence;computer science;natural language processing;parsing;need to know;speech recognition;german	NLP	-20.825349235474164	-79.3893438393148	170198
a96cb853a6fc8f340f7660648556fa86cced8fea	a morphology-driven string matching approach to arabic text searching	performance indicator;morphological matching;text searching;searching;arabic;pattern matching;string matching;stemming	In this paper, a morphological matching approach based on the lexical structure of Arabic words is introduced. Given a root R, the principle underlying morphological matching is to search for R in a text T to find all its occurrences. An algorithm is presented and its empirical and running time performance is analyzed. The results of searching experiments show that morphological matching provides high precision and recall ratios. But, since morphological searching is probabilistic, the results of experiments involved a number of errors caused by weak verbs. The analysis of running time performance indicates that morphological searching can be as efficient as pattern matching, if the root is entered by the user.	galaxy morphological classification;string searching algorithm	Suleiman H. Mustafa	2003	Journal of Systems and Software	10.1016/S0164-1212(02)00089-4	speech recognition;commentz-walter algorithm;computer science;performance indicator;machine learning;pattern matching;arabic;pattern recognition;stemming;programming language	SE	-24.8057640216626	-77.56987222700738	170277
b9693802290148843f42d40fb197717e218b3212	lebleu: n-gram-based translation evaluation score for morphologically complex languages		This paper describes the LeBLEU evaluation score for machine translation, submitted to WMT15 Metrics Shared Task. LeBLEU extends the popular BLEU score to consider fuzzy matches between word n-grams. While there are several variants of BLEU that allow to non-exact matches between words either by character-based distance measures or morphological preprocessing, none of them use fuzzy comparison between longer chunks of text. The results on WMT data sets show that fuzzy n-gram matching improves correlations to human evaluation especially for highly compounding languages.	bleu;grams;logic programming;machine translation;n-gram;preprocessor;text-based (computing);usability	Sami Virpioja;Stig-Arne Grönroos	2015			natural language processing;speech recognition;computer science;pattern recognition;rule-based machine translation	NLP	-23.670279344789417	-76.69776939630293	170364
243cbb84556a7ee6d54e83524dac3689a9749a2a	discriminative phrase embedding for paraphrase identification		This work, concerning paraphrase identification task, on one hand contributes to expanding deep learning embeddings to include continuous and discontinuous linguistic phrases. On the other hand, it comes up with a new scheme TF-KLD-KNN to learn the discriminative weights of words and phrases specific to paraphrase task, so that a weighted sum of embeddings can represent sentences more effectively. Based on these two innovations we get competitive state-of-the-art performance on paraphrase identification.	deep learning;domain adaptation;experiment;k-nearest neighbors algorithm;weight function	Wenpeng Yin;Hinrich Schütze	2015			natural language processing;speech recognition;pattern recognition	NLP	-19.504821710191482	-74.09675369159582	170643
d09c09104e7208ccae3d6dde057fa42b8e603bed	constructing morpheme-based translation model for mongolian-chinese smt	low resource language statistical machine translation phrase table pivot language morpheme multiple decoding paths;hidden markov models;1 37 bleu points morpheme based translation model mongolian chinese smt data sparsity morphological difference mongolian chinese statistical machine translation mongolian morpheme smt morpheme chinese smt system mongolian morpheme smt system language pair morphology phrase table translation knowledge bilingual resources low resource language pairs multiple decoding paths;natural language processing language translation	The data sparsity and the morphological difference between Chinese and Mongolian are the main problems in Mongolian-Chinese statistical machine translation (SMT). In this paper, we propose a novel method to construct morpheme-based translation model by using Mongolian morpheme as the pivot language. First, we train Mongolian-Morpheme SMT and Morpheme-Chinese SMT system, achieving a balance in the morphology of the language pair. Then we construct a new phrase table via these two systems to enrich translation knowledge without any additional bilingual resources, which is suitable for low-resource language pairs. Finally we incorporate this phrase table to our Mongolian-Chinese SMT system in two ways: by using multiple decoding paths and by the combination of two phrase tables. Experimental results demonstrate the effectiveness of our method with a maximum improvement of 1.37 BLEU points increment over baseline.	bleu;baseline (configuration management);mathematical morphology;sparse matrix;statistical machine translation	Zhenxin Yang;Miao Li;Lei Chen;Linyu Wei;Jinxing Wu;Sheng Chen	2015	2015 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2015.7451523	natural language processing;speech recognition;computer science;machine learning;linguistics;machine translation;hidden markov model	NLP	-21.74613300438496	-77.05144578833293	170822
e2446cd0a16facef20dfc953357f143607635f10	limsi's contribution to the wmt'16 biomedical translation task		The article describes LIMSI’s submission to the first WMT’16 shared biomedical translation task, focusing on the sole English-French translation direction. Our main submission is the output of a MOSES-based statistical machine translation (SMT) system, rescored with Structured OUtput Layer (SOUL) neural network models. We also present an attempt to circumvent syntactic complexity: our proposal combines the outputs of PBSMT systems trained either to translate entire source sentences or specific syntactic constructs extracted from those sentences. The approach is implemented using Confusion Network (CN) decoding. The quality of the combined output is comparable to the quality of our main system.	artificial neural network;baseline (configuration management);experiment;moses;scientific literature;statistical machine translation;text corpus	Julia Ive;Aurélien Max;François Yvon	2016		10.18653/v1/W16-2337	confusion;machine translation;natural language processing;decoding methods;syntax;artificial neural network;artificial intelligence;computer science	NLP	-19.604372279238703	-73.78069635148158	170940
34b79993a0af771b984dc7addf74caa4dc81196d	two-step validation in character-based ingredient normalization		Although ingredients are important items of information in recipes, it is difficult to process them, especially for computers, because they are user-generated informal text. To normalize ingredients, we can use a character-based encoder-decoder model that takes the character sequence of an ingredient as an input and outputs its canonical form. However, the model still has two problems: The first is that the model often generates unnatural sequences as outputs. The second problem is that the generated sequences are sometimes unrelated to the original ingredient. Therefore, we propose a two-step validation to generate better normalizations. In the first validation step, we use a trie to limit the normalization candidates to existing sequences. In the second validation step, we rerank the normalization candidates based on their similarity to the original ingredient. We conducted experiments using a corpus that includes approximately 10 thousand pairs of ingredients and their canonical forms and showed that our proposed validation improved the performance of encoder-decoder models.	beam search;computer;experiment;priority encoder;regular expression;text corpus;text-based (computing);trie;user-generated content;verification and validation	Jun Harashima;Yoshiaki Yamada	2018		10.1145/3230519.3230589	computer science;artificial intelligence;canonical form;trie;normalization (statistics);machine learning;ingredient	NLP	-22.40821622666455	-80.13474180941351	171115
0a1885b0a16fbab2a8b9920090ef470e2f76dd3a	applications of statistical machine translation approaches to spoken language understanding	analyse parole;sistema interactivo;lenguaje natural;evaluation performance;metodo estadistico;word error rate;german inhouse corpora;traduccion automatica;performance evaluation;metodo entropia maxima;learning;spoken language understanding combined approach machine translation maximum entropy minimum error rate training speech recognition;aleman;taux erreur;man machine dialogue;analisis palabra;frase;evaluacion prestacion;speech processing;speech analysis;statistical machine translation;langage naturel;tratamiento palabra;language translation;traitement parole;combined approach;formal languages;maximum entropy framework;speech;tratamiento lenguaje;statistical method;spoken dialogue systems statistical machine translation spoken language understanding source channel paradigm maximum entropy framework natural language understanding formal language target sentence alignment models direct maximum entropy approach speech recognition errors minimum error rate training final evaluation criterion german inhouse corpora;minimum error rate training;systeme conversationnel;spoken dialogue system;aprendizaje;speech recognition errors;natural languages entropy error analysis automatic speech recognition speech recognition surface mount technology computer science statistical analysis formal languages humans;error analysis;sentence;apprentissage;natural language understanding;reconocimiento voz;traduction automatique;statistical analysis;language processing;interactive system;final evaluation criterion;methode statistique;formal language target sentence;natural language;traitement langage;source channel paradigm;allemand;error rate;speech recognition;spoken dialogue systems;dialogo hombre maquina;phrase;entropy;alignment models;reconnaissance parole;methode entropie maximum;german;direct maximum entropy approach;indice error;method of maximum entropy;lenguaje formal;natural language processing;maximum entropy;formal language;machine translation	In this paper, we investigate two statistical methods for spoken language understanding based on statistical machine translation. The first approach employs the source-channel paradigm, whereas the other uses the maximum entropy framework. Starting with an annotated corpus, we describe the problem of natural language understanding as a translation from a source sentence to a formal language target sentence. We analyze the quality of different alignment models and feature functions and show that the direct maximum entropy approach outperforms the source channel-based method. Furthermore, we investigate how both methods perform if the input sentences contain speech recognition errors. Finally, we investigate a new approach to combine speech recognition and spoken language understanding. For this purpose, we employ minimum error rate training which directly optimizes the final evaluation criterion. By combining all knowledge sources in a log-linear way, we show that we can decrease both the word error rate and the slot error rate. Experiments were carried out on two German inhouse corpora for spoken dialogue systems.	automated system recovery;categorization;dialog system;erdős–rényi model;formal language;log-linear model;mathematical optimization;modality (human–computer interaction);natural language understanding;principle of maximum entropy;programming paradigm;resource description framework;speech recognition;statistical machine translation;text corpus;windows me;word error rate	Klaus Macherey;Oliver Bender;Hermann Ney	2009	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2009.2014262	natural language processing;cache language model;formal language;speech recognition;word error rate;computer science;speech processing;linguistics;machine translation	NLP	-23.74898954241896	-79.76964136128537	171361
31299844001b0322b4262c3f840ecd1d6a98149c	cross-lingual transfer of named entity recognizers without parallel corpora		We propose an approach to cross-lingual named entity recognition model transfer without the use of parallel corpora. In addition to global de-lexicalized features, we introduce multilingual gazetteers that are generated using graph propagation, and cross-lingual word representation mappings without the use of parallel data. We target the e-commerce domain, which is challenging due to its unstructured and noisy nature. The experiments have shown that our approaches beat the strong MT baseline, where the English model is transferred to two languages: Spanish and Chinese.	baseline (configuration management);e-commerce;experiment;finite-state machine;ner model;named entity;parallel text;software propagation;text corpus	Ayah Zirikly	2015			pattern recognition	NLP	-23.131637139007896	-75.87897228772982	171369
7ae34df8c1df79584586aa9b2595ff8078483a1b	chord recognition in symbolic music: a segmental crf model, segment-level features, and comparative evaluations on classical and popular music		We present a new approach to harmonic analysis that is trained to segment music into a sequence of chord spans tagged with chord labels. Formulated as a semi-Markov Conditional Random Field (semi-CRF), this joint segmentation and labeling approach enables the use of a rich set of segment-level features, such as segment purity and chord coverage, that capture the extent to which the events in an entire segment of music are compatible with a candidate chord label. The new chord recognition model is evaluated extensively on three corpora of classical music and a newly created corpus of rock music. Experimental results show that the semi-CRF model performs substantially better than previous approaches when trained on a sufficient number of labeled examples and remains competitive when the amount of training data is limited.	conditional random field;markov chain;pure function;semiconductor industry;text corpus	Kristen Masada;Razvan C. Bunescu	2018	CoRR		artificial intelligence;machine learning;harmonic analysis;computer science;training set;popular music;chord (music);classical music;conditional random field	ML	-23.690050856379308	-74.46550491594975	171734
20a599c5f152c3f135b9b55a667e64c93ec8d477	multi-way, multilingual neural machine translation with a shared attention mechanism		We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT’15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.	artificial neural network;estimation theory;experiment;neural machine translation;parallel text;programming language;shortest path problem;text corpus;vocabulary	Orhan Firat;Kyunghyun Cho;Yoshua Bengio	2016			natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;artificial intelligence;machine learning;rule-based machine translation;programming language	NLP	-19.574423730860673	-76.11731084298573	172082
28487f55028b6cd974cda73725bd80bf4e20f890	robust methods in analysis of natural language data	natural language;robust method	Modern statistical parsers are robust and quite fast, but their output is relatively shallow when compared to formal grammar parsers. We suggest to extend statistical approaches to a more deep-linguistic analysis while at the same time keeping the speed and low complexity of a statistical parser. The resulting parsing architecture suggested, implemented and evaluated here is highly robust and hybrid on a number of levels, combining statistical and rule-based approaches, constituency and dependency grammar, shallow and deep processing, full and nearfull parsing. With its parsing speed of about 300,000 words per hour and state-of-the-art performance the parser is reliable for a number of large-scale applications discussed in the article.	deep linguistic processing;dependency grammar;formal grammar;gnu indent;international symposium on fundamentals of computation theory;logic programming;natural language;parsing	Afzal Ballim;Vincenzo Pallotta	2002	Natural Language Engineering	10.1017/S1351324902002942	natural language processing;language identification;speech recognition;computer science;linguistics;natural language	NLP	-22.44530860837513	-76.35623098360426	172150
827316f6e7169b98d7a2374054062b34309ca1f1	learning online alignments with continuous rewards policy gradient		Sequence-to-sequence models with soft attention had significant success in machine translation, speech recognition, and question answering. Though capable and easy to use, they require that the entirety of the input sequence is available at the beginning of inference, an assumption that is not valid for instantaneous translation and speech recognition. To address this problem, we present a new method for solving sequence-to-sequence problems using hard online alignments instead of soft offline alignments. The online alignments model is able to start producing outputs without the need to first process the entire input sequence. A highly accurate online sequence-to-sequence model is useful because it can be used to build an accurate voice-based instantaneous translator. Our model uses hard binary stochastic decisions to select the timesteps at which outputs will be produced. The model is trained to produce these stochastic decisions using a standard policy gradient method. In our experiments, we show that this model achieves encouraging performance on TIMIT and Wall Street Journal (WSJ) speech recognition datasets.	experiment;gradient method;machine translation;online and offline;question answering;speech recognition;timit;the wall street journal	Yuping Luo;Chung-Cheng Chiu;Navdeep Jaitly;Ilya Sutskever	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952667	speech recognition;computer science;artificial intelligence;machine learning;statistics	NLP	-19.439100317443398	-78.59390043851141	172480
df939d826bc8b078fe6f0f558fe3957c8dc4e9b2	multi-criterion active learning in conditional random fields	costs training data laboratories supervised learning natural language processing computational efficiency learning systems markov random fields labeling tagging;manual annotation;text documents;supervised learning;active learning;text analysis;text analysis generalisation artificial intelligence learning artificial intelligence natural language processing random processes;conditional random fields;supervised learning models;training sets;multicriterion active learning;training sets multicriterion active learning conditional random fields supervised learning models natural language processing manual annotation text documents;random processes;conditional random field;generalisation artificial intelligence;learning artificial intelligence;natural language processing	Conditional random fields (CRFs), which are popular supervised learning models for many natural language processing (NLP) tasks, typically require a large collection of labeled data for training. In practice, however, manual annotation of text documents is quite costly. Furthermore, even large labeled training sets can have arbitrarily limited performance peaks if they are not chosen with care. This paper considers the use of multi-criterion active learning for identification of a small but sufficient set of text samples for training CRFs. Our empirical results demonstrate that our method is capable of reducing the manual annotation costs, while also limiting the retraining costs that are often associated with active learning. In addition, we show that the generalization performance of CRFs can be enhanced through judicious selection of training examples	active learning (machine learning);artificial intelligence;bibliothèque de l'école des chartes;broyden–fletcher–goldfarb–shanno algorithm;code;computation;conditional random field;experiment;feature selection;gradient;limited-memory bfgs;mathematical optimization;natural language processing;open-source software;overfitting;potential method;requirement;sourceforge;supervised learning;test set	Christopher T. Symons;Nagiza F. Samatova;Ramya Krishnamurthy;Byung-Hoon Park;Tarik Umar;David Buttler;Terence Critchlow;David Hysom	2006	2006 18th IEEE International Conference on Tools with Artificial Intelligence (ICTAI'06)	10.1109/ICTAI.2006.90	semi-supervised learning;natural language processing;computer science;artificial intelligence;machine learning;pattern recognition;supervised learning;conditional random field	Vision	-21.64337798378974	-75.32254359162711	172738
31ea50a9a54050decad35d775788afa7a4615ced	corpus based pp attachment ambiguity resolution with a semantic dictionary	supervised learning;word sense disambiguation;natural language;ambiguity resolution;prepositional phrase	This paper deals with two important ambiguities of natural language: prepositional phrase attachment and word sense ambiguity. We propose a new supervised learning method for PPattachment based on a semantically tagged corpus. Because any sufficiently big sense-tagged corpus does not exist, we also propose a new unsupervised context based word sense disambiguation algorithm which amends the training corpus for the PP attachment by word sense tags. We present the results of our approach and evaluate the achieved PP attachment accuracy in comparison with other methods.	algorithm;attachments;dictionary;experiment;human reliability;natural language processing;speech corpus;supervised learning;text corpus;word sense;word-sense disambiguation	Jiri Stetina;Makoto Nagao	1997			natural language processing;speech recognition;semeval;computer science;linguistics	NLP	-23.920480795614317	-73.7708078192494	172968
5ee7f2e3eb6d93077350e229341254fd26617c5d	uofr at semeval-2016 task 8: learning synchronous hyperedge replacement grammar for amr parsing		In this paper, we apply a synchronous-graphgrammar-based approach to SemEval-2016 Task 8, Meaning Representation Parsing. In particular, we learn Synchronous Hyperedge Replacement Grammar (SHRG) rules from aligned pairs of sentences and AMR graphs. Then we use Earley algorithm with cubepruning for AMR parsing given new sentences and the learned SHRG. Experiments on the evaluation dataset demonstrate that competitive results can be achieved using a SHRGbased approach.	adaptive multi-rate audio codec;algorithm;discriminative model;earley parser;language model;parsing;semeval;synchronization (computer science)	Xiaochang Peng;Daniel Gildea	2016			computer science;natural language processing;artificial intelligence;semeval;parsing;speech recognition;top-down parsing;grammar	NLP	-19.898548898953642	-74.9613050442167	172995
beafd7ecbfeba87eb153546f9c9920aa12aac23a	extraction of broad-scale, high-precision japanese-english parallel translation expressions using lexical information and rules	conference paper	Extraction was attempted of broad-scale, high-precision Japanese-English parallel translation expressions from large aligned parallel corpora. To acquire broad-scale parallel translation expressions, a new method was used to extract single Japanese and English word n-grams, by which as many parallel translation expressions as possible could then be extracted. To achieve high extraction precision, first, hand-crafted rules were used to prune the unnecessary words often found in expressions extracted on the basis of word n-grams, and lexical information was used to refine the parallel translation expressions. Computer experiments with aligned parallel corpora consisting of about 280,000 pairs of Japanese-English parallel sentences found that more than 125,000 pairs of parallel translation expressions could be extracted with a precision of 0.96. These figures show that the proposed methods for extracting a broad range of parallel translation expressions have reached a level high enough for practical use.	computer experiment;dictionary;grams;machine translation;n-gram;parallel text;text corpus	Qing Ma;Shinya Sakagami;Masaki Murata	2011			natural language processing;speech recognition;computer science;communication	NLP	-24.91707895785247	-75.7657080537518	173130
98437b48537e2a915b85e02b9f8eab8a0e846329	collocation extraction in turkish texts using statistical methods	statistical method;word sense disambiguation;frequency of occurrence;part of speech tagging;collocation;mutual information;collocation extraction;statistical techniques;natural language processing;extraction method;machine translation;hypothesis test	Collocation is the combination of words in which words appear together more often than by chance. Since collocations are blocks of meaning, they play an important role in natural language processing applications (word sense disambiguation, part of speech tagging, machine translation, etc). In this study, a corpus of Turkish is subjected to the following statistical techniques: frequency of occurrence, mutual information and hypothesis tests. We have utilized both stemmed and surface form of corpus to explore the effect of stemming in collocation extraction. The techniques are evaluated by recall and precision measures. Chi-square hypothesis test and mutual information methods have produced better results compared to other methods on Turkish corpus. In addition, we have found that a stemmed corpus facilitates discrimination between successful and unsuccessful collocation extraction methods.	chi;collocation extraction;machine translation;mutual information;natural language processing;part-of-speech tagging;precision and recall;randomness;stemming;text corpus;word sense;word-sense disambiguation	Senem Kumova Metin;Bahar Karaoglan	2010		10.1007/978-3-642-14770-8_27	natural language processing;speech recognition;computer science;pattern recognition	NLP	-25.669915520634337	-77.00585924512582	174139
4ddb7f7a9ca8f95063725bfc04042782c591a454	improving statistical natural language translation with categories and rules	translation rule;word class;spontaneous speech corpus;level approach;overall statistical approach;parallel corpus;statistical translation lexicon;improving statistical natural language;translation process;statistical natural language translation;word alignment	"""This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a generalized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based techniques enter an overall statistical approach leading to about 50 percent correctly translated sentences applied to the very ditficult EnglishGerman VERBMOBIL spontaneous speech corpus. 1 I n t r o d u c t i o n In SNLT the transfer itself is realized as a maximization process of the form rI~'ans(d) = argmax c P(eld) (1) tIere d is a given source language (SL) sentence which has to be translated into a target language (TL) sentence e. In order to model the distributions P(e[d) all approaches in SNLT use a """"divide and conquer"""" strategy of approximating P (e ld ) by a combination of simpler models. The problem is to reduce parameters in a sufficient way but end up with a model still able to describe the linguistic facts of natural language translation. The work presented here uses two approximations for P(e ld ). One approximation is used for to gain the relevant parameters in training while a modified formula is subject of decoding translations. In detail, we impose the following modifications with respect to approaches published in the last decade: 1. A refined distance weight for the STL probabilities is used which allows for a good modeling of tile effects caused by syntactic phrases. 2. In order to account for collocations a WA technique is used, where oneto-n and n-to-one WAs are allowed. 3. For the translation WCs are used which are coilstrutted using clustering techniques, where the STL forms a part of the optimization criterion. 4. A set of TRs is learned mapping sequences of SL WCs to sequences of TL WCs. Throughout the paper the four topics above are described in more detail. Finally we report on experimental results produced on the VERBMOBIL corpus. 2 L e a r n i n g o f t h e T r a n s l a t i o n"""	approximation;beam search;bilingual dictionary;bitext word alignment;cluster analysis;collocation;compiler;decoding methods;expectation–maximization algorithm;lexicon;mathematical optimization;microsoft word for mac;natural language;parallel text;sl (complexity);speech corpus;spontaneous order;statistical machine translation;transform, clipping, and lighting;verbmobil	Franz Josef Och;Hans Weber	1998			natural language processing;speech recognition;example-based machine translation;computer science;linguistics;machine translation;natural language	NLP	-22.77295867545875	-78.756535556986	175337
fee7750d693afa2f820c0c5384907d46c4110d80	speech recognition using function-word n-grams and content-word n-grams	speech recognition;n gram		grams;microsoft word for mac;n-gram;speech recognition	Ryosuke Isotani;Shoichi Matsunaga;Shigeki Sagayama	1995	IEICE Transactions			Vision	-24.81203060491385	-78.9390699453058	175770
427c7155ec1c84a21903e32c37befc2d8f817b9d	a phrase orientation model for hierarchical machine translation		We introduce a lexicalized reordering model for hierarchical phrase-based machine translation. The model scores monotone, swap, and discontinuous phrase orientations in the manner of the one presented by Tillmann (2004). While this type of lexicalized reordering model is a valuable and widely-used component of standard phrase-based statistical machine translation systems (Koehn et al., 2007), it is however commonly not employed in hierarchical decoders. We describe how phrase orientation probabilities can be extracted from wordaligned training data for use with hierarchical phrase inventories, and show how orientations can be scored in hierarchical decoding. The model is empirically evaluated on the NIST Chinese→English translation task. We achieve a significant improvement of +1.2 %BLEU over a typical hierarchical baseline setup and an improvement of +0.7 %BLEU over a syntax-augmented hierarchical setup. On a French→German translation task, we obtain a gain of up to +0.4 %BLEU.	bleu;baseline (configuration management);experiment;inventory;jane (software);open-source software;out-of-order execution;programming paradigm;quantum gate;statistical machine translation;word lists by frequency;monotone	Matthias Huck;Joern Wuebker;Felix Rietig;Hermann Ney	2013			natural language processing;bleu;speech recognition;computer science;pattern recognition	NLP	-20.9993478142428	-77.03307371316026	176014
26040ab24407312bb4d244178f8f0f2793c0f5a2	building a topic-dependent maximum entropy model for very large corpora	broadcast news;entropy computational modeling;n gram model;computational modeling;speech recognition;maximum entropy model;entropy;divide and conquer;maximum entropy;language model	Maximum entropy (ME) techniques have been successfully used to combine different sources of linguistically meaningful constraints in language models. However, most of the current ME models can only be used for small corpora, since the computational load in training ME models for large corpora is unbearable. This problem is especially severe when non-local dependencies are considered. In this paper, we show how to train and use topic-dependent ME models efficiently for a very large corpus, Broadcast News (BN). The training time is greatly reduced by hierarchical training and divide-and-conquer approaches. The computation in using the model is also simplified by pre-normalizing the denominators of the ME model. We report new speech recognition results showing improvement with the topic model relative to the standard N-gram model for the Broadcast News task.	computation;language model;maximum entropy spectral estimation;n-gram;principle of maximum entropy;speech recognition;text corpus;topic model	Jun Wu;Sanjeev Khudanpur	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743833	natural language processing;maximum-entropy markov model;speech recognition;computer science;principle of maximum entropy;machine learning;statistics;language model	NLP	-20.956610321743803	-79.3551016309255	176410
003ca592afffc2bb13886c67021d1af578650053	unsupervised methods for head assignments	parsing accuracy;comparative parsing accuracy;different linguistic intuition;phrase structure tree;unsupervised method;implicit lexicalized tree grammar;assigning head;unique lexicalized tree substitution;lexicalized grammar;head assignment;implied lexicalized grammars score;hand-designed heuristics;natural language;objective function	We present several algorithms for assigning heads in phrase structure trees, based on different linguistic intuitions on the role of heads in natural language syntax. Starting point of our approach is the observation that a head-annotated treebank defines a unique lexicalized tree substitution grammar. This allows us to go back and forth between the two representations, and define objective functions for the unsupervised learning of head assignments in terms of features of the implicit lexicalized tree grammars. We evaluate algorithms based on the match with gold standard head-annotations, and the comparative parsing accuracy of the lexicalized grammars they give rise to. On the first task, we approach the accuracy of handdesigned heuristics for English and interannotation-standard agreement for German. On the second task, the implied lexicalized grammars score 4% points higher on parsing accuracy than lexicalized grammars derived by commonly used heuristics.	algorithm;attribute grammar;dos;heuristic (computer science);natural language;parsing;phrase structure rules;treebank;unsupervised learning	Federico Sangati;Willem H. Zuidema	2009			natural language processing;tree-adjoining grammar;l-attributed grammar;speech recognition;phrase structure grammar;computer science;s-attributed grammar;linguistics;natural language	NLP	-26.331623865491355	-76.44094769915228	176510
6131f27c1f50d9784d0c0b4082a826b92d81bf31	a maximum entropy approach to combining word alignments	phrase-based mt system;alignment link;maximum entropy approach;feature function;word alignment system;new approach;alignment decision;input alignment;mt quality;word alignment;maximum entropy	This paper presents a new approach to combining outputs of existing word alignment systems. Each alignment link is represented with a set of feature functions extracted from linguistic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system.	acme;approximation error;bleu;bitext word alignment;brill tagger;data structure alignment;experiment;heuristic;part-of-speech tagging;point of sale;principle of maximum entropy;supervised learning	Necip Fazil Ayan;Bonnie J. Dorr	2006			speech recognition;computer science;principle of maximum entropy;machine learning;pattern recognition;statistics	NLP	-20.881158501955266	-77.44101302966175	176889
47af073a0bd45d294c32fffb0e5b4ecf3487a595	automatic identification of non-anaphoric anaphora in spoken dialog	machine learning algorithms;support vector machines;probability density function;anaphora resolution;dependency grammar;text analysis;data mining;non anaphoric anaphora identification;automatic nonanaphoric anaphora identification anaphora resolution spoken dialog part of speech tagger;non anaphoric anaphora identification spoken dialog anaphora resolution;machine learning;feature extraction;statistics computer science machine learning algorithms support vector machines decision trees feature extraction pattern matching support vector machine classification classification tree analysis;classification algorithms;automatic nonanaphoric anaphora identification;part of speech;text analysis computational linguistics;computational linguistics;spoken dialog;part of speech tagger	Identification of non-anaphoric anaphora is an important step towards a full anaphora resolution. In this paper, we present an automatic identification approach for this task. In our work, some novel features are proposed, which are based on dependency grammars, surrounding words and their POS tags. All the features are automatically extracted using a part-of-speech (POS) tagger and a dependency parser. Our experiments are on a commonly available dialogue corpus, Trains-93. Several machine learning algorithms are used in the experiments, including CME, CRF and SVM. Results show that compared to the approaches used in the previous work, our algorithm is simpler and achieves a higher accuracy.	algorithm;anaphora (linguistics);automatic identification and data capture;brill tagger;brown corpus;conditional random field;dependency grammar;experiment;linguistic data consortium;machine learning;part-of-speech tagging;robert;text corpus;dialog	Zhongchao Fei;Xuanjing Huang;Fuliang Weng	2008	2008 International Conference on Natural Language Processing and Knowledge Engineering	10.1109/NLPKE.2008.4906761	natural language processing;speech recognition;computer science;pattern recognition	NLP	-24.40657940913045	-76.96092202527801	176941
8e2e756620214ebbefe124ef9915fa07bdd8c3a6	formatting time-aligned asr transcripts for readability	automatic speech recognition;formatting time-aligned asr transcript;hand-crafted grammar;formatting task;class-based language model;asr transcript;written text;numeric entity;word-level timing information;end time;weighted finite state transducers	We address the problem of formatting the output of an automatic speech recognition (ASR) system for readability, while preserving wordlevel timing information of the transcript. Our system enriches the ASR transcript with punctuation, capitalization and properly written dates, times and other numeric entities, and our approach can be applied to other formatting tasks. The method we describe combines hand-crafted grammars with a class-based language model trained on written text and relies on Weighted Finite State Transducers (WFSTs) for the preservation of start and end time of each word.	algorithm;automated system recovery;brown clustering;entity;finite-state transducer;language model;linguistic data consortium;numeric character reference;question answering;sparse matrix;speech recognition;transcription (software);usability testing	Maria Shugrina	2010			natural language processing;speech recognition;computer science	NLP	-22.290221911753083	-79.21208929168509	177518
57a71c6ab7de3cb98725a4ae17a72ceda0397aba	detecting new words from chinese text using latent semi-crf models	conditional random fields;latent semi-crf;latent-dynamic crf;natural language processing;new word detection;new words pos tagging;semi-crf-;conditional random field;low frequency;information technology;hidden variables;part of speech	Chinese new words and their part-of-speech (POS) are particularly problematic in Chinese natural language processing. With the fast development of internet and information technology, it is impossible to get a complete system dictionary for Chinese natural language processing, as new words out of the basic system dictionary are always being created. A latent semi-CRF model, which combines the strengths of LDCRF (Latent-Dynamic Conditional Random Field) and semi-CRF, is proposed to detect the new words together with their POS synchronously regardless of the types of the new words from the Chinese text without being pre-segmented. Unlike the original semi-CRF, the LDCRF is applied to generate the candidate entities for training and testing the latent semi-CRF, which accelerates the training speed and decreases the computation cost. The complexity of the latent semi-CRF could be further adjusted by tuning the number of hidden variables in LDCRF and the number of the candidate entities from the Nbest outputs of the LDCRF. A new-words-generating framework is proposed for model training and testing, under which the definitions and distributions of the new words conform to the ones existing in real text. Specific features called “Global Fragment Information” for new word detection and POS tagging are adopted in the model training and testing. The experimental results show that the proposed method is capable of detecting even low frequency new words together with their POS tags. The proposed model is found to be performing competitively with the state-ofthe-art models presented. key words: natural language processing, new word detection, new words POS tagging, conditional random fields, latent-dynamic CRF, semi-CRF, latent semi-CRF	brown corpus;computation;conditional random field;dictionary;entity;hidden variable theory;natural language processing;part-of-speech tagging;protologism;semiconductor industry;sensor;words (unix)	Xiao Sun;Degen Huang;Fuji Ren	2010	IEICE Transactions		natural language processing;speech recognition;computer science;artificial intelligence;machine learning;information technology;conditional random field;statistics	AI	-22.882883503498153	-75.62993986504198	177565
566b1844a75b6e56a75fcc74d56cf34b06901e5e	extracting medical events from clinical records using conditional random fields and parameter tuning for hidden markov models			conditional random field;hidden markov model;markov chain	Carolina Fócil Arias;Grigori Sidorov;Alexander F. Gelbukh;Fernando Arce	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169479	machine learning;artificial intelligence;mathematics;conditional random field;hidden markov model	ML	-24.90407560832117	-79.27196321883659	178105
9388faff80c6b13e39ce27cd66c3f02fa0ecb915	an efficient ocr error correction method for japanese text recognition	error correction	OCR error correction using Japanese morphological analysis contains two time-consuming procedures: extraction of candidate words from combinations of candidate characters, and finding the most plausible word sequence in combinations of the candidate words. In this paper an optimal word extraction technique, and the use of lexical entries that are tailored for Japanese verb inflection, are investigated and developed. Compared to a standard method, the new method requires 84% less computation, and captures 2.6% more candidate words. The new design of lexical entries reduces the chart parsing computation by 20%. The error correction rate of the system is 86.9%, which is 19.6% higher than that of the standard one.	chart parser;computation;error detection and correction;experiment;optical character recognition;parsing;qr code	Toru Hisamitsu;Katsumi Marukawa;Yoshihiro Shima;Hiromichi Fujisawa;Yoshihiko Nitta	1994			natural language processing;error detection and correction;speech recognition;computer science;pattern recognition	NLP	-23.030754382869667	-80.1708750232266	178118
bee97f419057ebe3826e17384387d030faf37ef8	dirichlet processes for joint learning of morphology and pos tags		This paper presents a joint model for learning morphology and part-of-speech (PoS) tags simultaneously. The proposed method adopts a finite mixture model that groups words having similar contextual features thereby assigning the same PoS tag to those words. While learning PoS tags, words are analysed morphologically by exploiting similar morphological features of the learned PoS tags. The results show that morphology and PoS tags can be learned jointly in a fully unsupervised setting.	brown corpus;galaxy morphological classification;mathematical morphology;mixture model;unsupervised learning	Burcu Can;Suresh Manandhar	2013			speech recognition;pattern recognition	NLP	-23.084400314419018	-73.85343678919091	178276
a72d1b50797cee5ae7a783925b0f9be6318c24d3	end-to-end coreference resolution via hypergraph partitioning	hypergraph partitioning;coreference resolution	We describe a novel approach to coreference resolution which implements a global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step. Our hypergraph-based global model implemented within an endto-end coreference resolution system outperforms two strong baselines (Soon et al., 2001; Bengtson & Roth, 2008) using sys-	cluster analysis;graph partition;realms of the haunting	Jie Cai;Michael Strube	2010			computer science;machine learning;pattern recognition;data mining	NLP	-20.337530300203817	-73.7458183458641	178335
0c5d639cc60c66e473f90b3f8ecd8249c557a8e8	character segmentation of hindi unconstrained handwritten words	character segmentation;handwritten word;header line detection;upper modifier;hindi language;structural approach;ocr;lower modifier	The proper character level segmentation of printed or handwritten text is an important preprocessing step for optical character recognition OCR. It is noticed that the languages having cursive nature in writing make the segmentation problem much more complicated. Hindi is one of the well known language in India having this cursive nature in writing style. The main challenge in handwritten character segmentation is to handle the inherent variability in the writing style of different individuals. In this paper, we present an efficient character segmentation method for handwritten Hindi words. Segmentation is performed on the basis of some structural patterns observed in the writing style of this language. The proposed method can cope with high variations in writing style and skewed header lines as input. The method has been tested on our own database for both printed and handwritten words. The average success rate is 96.93i¾?%. The method yields fairly good results for this database comparing with other existing methods. We foresee that the proposed character segmenattion technique can be used as a part of an OCR system for cursive handwritten Hindi language.		Soumen Bag;Ankit Krishna	2015		10.1007/978-3-319-26145-4_18	natural language processing;speech recognition;hindi;computer science;intelligent word recognition	Vision	-24.644658925868015	-77.81518980007957	178881
05a655ee8feff673ce15f559a785894441266652	triangular architecture for rare language translation		Neural Machine Translation (NMT) performs poor on the low-resource language pair $(X,Z)$, especially when $Z$ is a rare language. By introducing another rich language $Y$, we propose a novel triangular training architecture (TA-NMT) to leverage bilingual data $(Y,Z)$ (may be small) and $(X,Y)$ (can be rich) to improve the translation performance of low-resource pairs. In this triangular architecture, $Z$ is taken as the intermediate latent variable, and translation models of $Z$ are jointly optimized with a unified bidirectional EM algorithm under the goal of maximizing the translation likelihood of $(X,Y)$. Empirical results demonstrate that our method significantly improves the translation quality of rare languages on MultiUN and IWSLT2012 datasets, and achieves even better performance combining back-translation methods.	entity–relationship model;expectation–maximization algorithm;latent variable;neural machine translation;unsupervised learning	Shuo Ren;Wenhu Chen;Shujie Liu;Mu Li;Ming Zhou;Shuai Ma	2018			machine translation;computer science;machine learning;expectation–maximization algorithm;architecture;artificial intelligence;latent variable	NLP	-19.840358773468758	-76.28345704773939	179069
257879b7d0e8b26d68f20f526f235411f8e6edf1	language identification: the long and the short of the matter	language identification	Language identification is the task of identifying the language a given document is written in. This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokenisation strategies. We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents. We also show that it is possible to perform language identification without having to perform explicit character encoding detection.	bigram;byte;character encoding;charset detection;cosine similarity;document;experiment;language identification;n-gram;trigram	Timothy Baldwin;Marco Lui	2010			natural language processing;language identification;speech recognition;computer science	NLP	-25.785697457382927	-79.66274623590948	179727
1dbbe9c0077c305c9600eb2ff0e261735d6200e6	multimodal pivots for image caption translation		We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-ofthe-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.	artificial neural network;bleu;compiler;convolutional neural network;image retrieval;multimodal interaction;statistical machine translation	Julian Hitschler;Shigehiko Schamoni;Stefan Riezler	2016	CoRR		computer vision;speech recognition;computer science;pattern recognition	NLP	-20.197365355497347	-74.67579375351113	180136
835b7d954e81bed9b83cfb24d07a79776ab02f4e	hungarian and czech stemming using yass		This is the second year in a row we are participating in CLEF. Our aim is to test the performance of a statistical stemmer on various languages. Last year, we tried the stemmer on French; this year, we opted for Hungarian, Bulgarian and Czech. We were unable to complete the Bulgarian task, but submitted official runs for the adhoc monolingual Hungarian and Czech tasks. We find that, for both languages, the performance of the statistical stemmer is comparable to that of an available rule-based stemmer.	logic programming;stemming;yass	Prasenjit Majumder;Mandar Mitra;Dipasree Pal	2007			artificial intelligence;bulgarian;natural language processing;computer science;czech;clef	NLP	-23.913484207820275	-75.78941369971768	180293
01aa0b5c8f1bbf7e3c535a6764569ba51a237e7b	feature-based approach combined with hierarchical classifying strategy to relation extraction	pattern classification learning artificial intelligence ontologies artificial intelligence;dependency features;dependency tree relation extraction hierarchical classifying strategy correction mechanism;lexical feature;hierarchical classifying strategy;ontologies artificial intelligence;relation extraction;syntactic feature;hierarchical classification strategy;variable speed drives;dependency tree;pattern classification;relation extraction task;feature based approach;learning artificial intelligence;correction mechanism;correction mechanism feature based approach hierarchical classification strategy relation extraction task lexical feature syntactic feature dependency features	This paper proposes a novel feature-based method for relation extraction task. Diverse lexical and syntactic features are defined to describe the context of the pair of entities. Dependency features are selected to capture the structure and dependency information of sentence. Hierarchical classifying strategy is used to reduce the weakness of the traditional approach, which treats training examples in different classes equally and independently, At the same time, correction mechanism is used to improve the performance of the system.	class hierarchy;entity;neural coding;relationship extraction;support vector machine;top-down and bottom-up design	Jing Qiu;Jun-Kang Hao	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580642	relationship extraction;computer science;artificial intelligence;machine learning;pattern recognition;data mining	NLP	-22.91034416820409	-73.24206477163065	180428
b2688b811c28f69665732a5a68680c379a339fc9	adaptive clustering for coreference resolution with deterministic rules and web-based language models	significant advantage;semantic compatibility feature;deterministic rule;new semantic feature;web-based language model;overall performance improvement;coreference resolution;novel adaptive;new feature;f1 measure;new approach;adaptive clustering approach result;expert rule	We present a novel adaptive clustering model for coreference resolution in which the expert rules of a state of the art deterministic system are used as features over pairs of clusters. A significant advantage of the new approach is that the expert rules can be easily augmented with new semantic features. We demonstrate this advantage by incorporating semantic compatibility features for neutral pronouns computed from web n-gram statistics. Experimental results show that the combination of the new features with the expert rules in the adaptive clustering approach results in an overall performance improvement, and over 5% improvement in F1 measure for the target pronouns when evaluated on the ACE 2004 newswire corpus.	ace;cluster analysis;f1 score;language model;n-gram	Razvan C. Bunescu	2012			natural language processing;computer science;machine learning;pattern recognition;data mining	NLP	-22.187355321075657	-74.06729495554231	180457
26954348cad401586b8940dec0c362365714c9cc	weight initialization in neural language models		Semantic Similarity is an important application which finds its use in many downstream NLP applications. Though the task is mathematically defined, semantic similaritys essence is to capture the notions of similarity impregnated in humans. Machines use some heuristics to calculate the similarity between words, but these are typically corpus dependent or are useful for specific domains.The difference between Semantic Similarity and Semantic Relatedness motivates the development of new algorithms. For a human, the word car and road are probably as related as car and bus. But this may not be the case for computational methods. Ontological methods are good at encoding Semantic Similarity and Vector Space models are better at encoding Semantic Relatedness. There is a dearth of methods which leverage ontologies to create better vector representations. The aim of this proposal is to explore in the direction of a hybrid method which combines statistical/vector space methods like Word2Vec and Ontological methods like WordNet to leverage the advantages provided by both.	algorithm;dictionary;downstream (software development);experiment;feature vector;heuristic (computer science);language model;mike lesser;natural language processing;ontology (information science);semantic similarity;text corpus;word embedding;word2vec;wordnet	Ameet Deshpande;Vedant Somani	2018	CoRR		semantic similarity;machine learning;word2vec;natural language processing;encoding (memory);initialization;artificial intelligence;ontology (information science);heuristics;computer science;wordnet;language model	NLP	-19.617691340076362	-73.38859441346747	180836
aeacba3c85b22ee3d85011e0a472bc246a5bba45	effective selection of translation model training data		Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest. Most current data selection methods solely use language models trained on a small scale in-domain data to select domain-relevant sentence pairs from general-domain parallel corpus. By contrast, we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model. In this paper, we study and experiment with novel methods that apply translation models into domain-relevant data selection. The results show that our methods outperform previous methods. When the selected sentence pairs are evaluated on an end-to-end MT task, our methods can increase the translation performance by 3 BLEU points. *	bleu;end-to-end principle;language model;parallel text;relevance;statistical machine translation;test set	Le Yu Liu;Yu Hong;Hao Liu;Xing Wang;Jian-Min Yao	2014			natural language processing;speech recognition;transfer-based machine translation;computer science;machine learning;evaluation of machine translation;rule-based machine translation	NLP	-20.028768042584975	-75.33833295157528	181512
7e4a9360bc4d825cd5a1bdf3ba590de27580eb53	cross-lingual task-specific representation learning for text classification in resource poor languages		Neural network models have shown promising results for text classification. However, these solutions are limited by their dependence on the availability of annotated data. The prospect of leveraging resource-rich languages to enhance the text classification of resource-poor languages is fascinating. The performance on resource-poor languages can significantly improve if the resource availability constraints can be offset. To this end, we present a twin Bidirectional Long Short Term Memory (Bi-LSTM) network with shared parameters consolidated by a contrastive loss function (based on a similarity metric). The model learns the representation of resource-poor and resource-rich sentences in a common space by using the similarity between their assigned annotation tags. Hence, the model projects sentences with similar tags closer and those with different tags farther from each other. We evaluated our model on the classification tasks of sentiment analysis and emoji prediction for resource-poor languages Hindi and Telugu and resource-rich languages English and Spanish. Our model significantly outperforms the state-of-the-art approaches in both the tasks across all metrics.	document classification;emoji;feature learning;long short-term memory;loss function;machine learning;problem domain;sentiment analysis	Nurendra Choudhary;Rajat Singh;Manish Shrivastava	2018	CoRR		emoji;natural language processing;artificial intelligence;machine learning;long short term memory;artificial neural network;sentiment analysis;hindi;offset (computer science);computer science;feature learning;annotation	NLP	-19.31998261345689	-73.48659686443652	181795
9816380b6987edfd52f10167672d2a632c5a06cb	machine translation by modeling predicate-argument structure transformation		Machine translation aims to generate a target sentence that is semantically equivalent to the source sentence. However, most of current statistical machine translation models do not model the semantics of sentences. In this paper, we propose a novel translation framework based on predicate-argument struct re (PAS) for its capacity on grasping the semantics and skeleton structure of sentences. By usin g PAS, the framework effectively models both semantics of languages and global reordering for translation. In the framework, we divide the translation process into 3 steps: (1) PAS acquisition : perform semantic role labeling (SRL) on the input sentences to acquire source-side PASs; (2) Transformation: convert source-sid e PASs to their target counterparts by predicate-aw are PAS transformation rules; (3) Translation: first translate the predicate and arguments of PAS and then a dopt a CKY-style decoding algorithm to translate the entire PAS. Experimental results show that our PAS-based translation framework significantly improves the translation performance.	algorithm;experiment;operational semantics;oxford spelling;semantic role labeling;sparse voxel octree;statistical machine translation;struct (c programming language);structural integrity and failure	Feifei Zhai;Jiajun Zhang;Yu Zhou;Chengqing Zong	2012			dynamic and formal equivalence;natural language processing;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;rule-based machine translation	NLP	-21.899564383629844	-76.94642357493586	182035
4fb7fbfad0d50af17db260e9da9fc68762b37eac	vocabulary manipulation for neural machine translation		In order to capture rich language phenomena, neural machine translation models have to use a large vocabulary size, which requires high computing time and large memory usage. In this paper, we alleviate this issue by introducing a sentence-level or batch-level vocabulary, which is only a very small sub-set of the full output vocabulary. For each sentence or batch, we only predict the target words in its sentencelevel or batch-level vocabulary. Thus, we reduce both the computing time and the memory usage. Our method simply takes into account the translation options of each word or phrase in the source sentence, and picks a very small target vocabulary for each sentence based on a wordto-word translation model or a bilingual phrase library learned from a traditional machine translation model. Experimental results on the large-scale English-toFrench task show that our method achieves better translation performance by 1 BLEU point over the large vocabulary neural machine translation system of Jean et al. (2015).	bleu;jean;library (computing);neural machine translation;simulation;softmax function;speedup;the sentence;vocabulary;vyos	Haitao Mi;Zhiguo Wang;Abe Ittycheriah	2016	CoRR		natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation	NLP	-19.616295216987766	-76.03970689715447	182253
9f35a94cdcb2816cfd11eb5d0011744468f85dbf	inducing implicit arguments via cross-document alignment: a framework and its applications		Natural language texts frequently contain related information in different positions in discourse. As human readers, we can recognize such information across sentence boundaries and correctly infer relations between them. Given this inference capability, we understand texts that describe complex dependencies even if central aspects are not repeated in every sentence. In linguistics, certain omissions of redundant information are known under the term ellipsis and have been studied as cohesive devices in discourse (Halliday and Hasan, 1976). For computational approaches to semantic processing, such cohesive devices are problematic because methods are traditionally applied on the sentence level and barely take surrounding context into account. In this dissertation, we investigate omission phenomena on the level of predicateargument structures. In particular, we examine instances of structures involving arguments that are not locally realized but inferable from context. The goal of this work is to automatically acquire and process such instances, which we also refer to as implicit arguments, to improve natural language processing applications. Our main contribution is a framework that identifies implicit arguments by aligning and comparing predicateargument structures across pairs of comparable texts. As part of this framework, we develop a novel graph-based clustering approach, which detects corresponding predicateargument structures using pairwise similarity metrics. To find discourse antecedents of implicit arguments, we further design a heuristic method that utilizes automatic annotations from various linguistic pre-processing tools. We empirically validate the utility of automatically induced instances of implicit arguments and discourse antecedents in three extrinsic evaluation scenarios. In the first scenario, we show that our induced pairs of arguments and antecedents can successfully be applied to improve a pre-existing model for linking implicit arguments in discourse. In two further evaluation settings, we show that induced instances of implicit arguments, together with their aligned explicit counterparts, can be used as training material for a novel model of local coherence. Given discourse-level and semantic features, this model can predict whether a specific argument should be explicitly realized to establish local coherence or whether it is inferable and hence redundant in context.	cluster analysis;heuristic;natural language processing;preprocessor	Michael Roth	2013			natural language processing;computer science;artificial intelligence;communication	NLP	-25.81661294760583	-73.37266267350945	183031
c6bdde7409fd970a02ab871db3e21cdc5a0c38f4	natural language generation in an mt system based on a multi-level processing strategy	natural language generation	This paper describes how to generate high quality Korean sentences from intermediate meaning representations. In previous research, there is no clear-cut separation between syntactic and morphological processing, and lexical information and rules are so tightly combined. This creates difficulties, such as attempting to enhance portability and extensibility, or handling complex linguistic phenomena in a systematic manner. We adopted Mel'cuk's meaning-text model as a linguistic model in Korean generation part, in which the complicated task of Korean generation can be broken up into logically independent subtasks, making the system highly modularized and robust. In a Korean generation experiment, the system showed a fidelity rate of 90% and an intelligibility rate of 85%, which is a promising result considering the difficulties with generating various linguistic phenomena.	natural language generation	Hui-Feng Li;Sin-Jae Kang;Jong-Hyeok Lee	2001	Int. J. Comput. Proc. Oriental Lang.	10.1142/S021942790100028X	natural language processing;speech recognition;computer science;linguistics	NLP	-22.519476462348308	-79.55397880587795	183045
fde98e6b1e96ed7f40c9b40063203fff810a35e8	supervised all-words lexical substitution using delexicalized features		We propose a supervised lexical substitution system that does not use separate classifiers per word and is therefore applicable to any word in the vocabulary. Instead of learning word-specific substitution patterns, a global model for lexical substitution is trained on delexicalized (i.e., non lexical) features, which allows to exploit the power of supervised methods while being able to generalize beyond target words in the training set. This way, our approach remains technically straightforward, provides better performance and similar coverage in comparison to unsupervised approaches. Using features from lexical resources, as well as a variety of features computed from large corpora (n-gram counts, distributional similarity) and a ranking method based on the posterior probabilities obtained from a Maximum Entropy classifier, we improve over the state of the art in the LexSub Best-Precision metric and the Generalized Average Precision measure. Robustness of our approach is demonstrated by evaluating it successfully on two different datasets.	baseline (configuration management);boolean algebra;digital humanities;hessian;information retrieval;lexical substitution;machine learning;microsoft word for mac;multinomial logistic regression;n-gram;natural language processing;precision and recall;principle of maximum entropy;rewriting;robustness (computer science);semeval;steganography;supervised learning;test set;text corpus;thesaurus;unsupervised learning;utility functions on indivisible goods;vocabulary;word lists by frequency;wordnet	György Szarvas;Christian Biemann;Iryna Gurevych	2013			multinomial logistic regression;robustness (computer science);machine learning;artificial intelligence;computer science;ranking;training set;exploit;posterior probability;vocabulary	NLP	-21.690926766270113	-75.07028452057968	183227
9a85ce5b1aee81e8a45fc2bd76dac968a139a8f7	character-cluster-based segmentation using monolingual and bilingual information for statistical machine translation		We present a novel segmentation approach for Phrase-Based Statistical Machine Translation (PB-SMT) to languages where word boundaries are not obviously marked by using both monolingual and bilingual information and demonstrate that (1) unsegmented corpus is able to provide the nearly identical result compares to manually segmented corpus in PB-SMT task when a good heuristic character clustering algorithm is applied on it, (2) the performance of PB-SMT task has significantly increased when bilingual information are used on top of monolingual segmented result. Our technique, instead of focusing on word separation, mainly concentrate on character clustering. First, we cluster each character from the unsegmented monolingual corpus by employing character co-occurrence statistics and orthographic insight. Secondly, we enhance the segmented result by incorporating the bilingual information which are character cluster alignment, co-occurrence frequency and alignment confidence into that result. We evaluate the effectiveness of our method on PB-SMT task using English-Thai language pair and report the best improvement of 8.1% increase in BLEU score. There are two main advantages of our approach. First, our method requires less effort on developing the corpus and can be applied to unsegmented corpus or poor-quality manually segmented corpus. Second, this technique does not only limited to specific language pair but also capable of automatically adjust the character cluster boundaries to be suitable for other language pairs.	algorithm;bleu;cluster analysis;heuristic;orthographic projection;statistical machine translation	Vipas Sutantayawalee;Peerachet Porkaew;Thepchai Supnithi;Prachya Boonkwan;Sitthaa Phaholphinyo	2014		10.3115/v1/W14-5513	natural language processing;speech recognition;computer science;pattern recognition	NLP	-24.217985752841514	-76.86622071916561	183257
182eb580bc04d605c91fc77e6fa6a270b6ef1008	adaptive development data selection for log-linear model in statistical machine translation	statistical machine translation;log linear model	This paper addresses the problem of dynamic model parameter selection for loglinear model based statistical machine translation (SMT) systems. In this work, we propose a principled method for this task by transforming it to a test data dependent development set selection problem. We present two algorithms for automatic development set construction, and evaluated our method on several NIST data sets for the Chinese-English translation task. Experimental results show that our method can effectively adapt log-linear model parameters to different test data, and consistently achieves good translation performance compared with conventional methods that use a fixed model parameter setting across different data sets.	cluster analysis;linear model;log-linear model;mathematical model;model selection;selection algorithm;statistical machine translation;test data;test set;tweaking	Mu Li;Yinggong Zhao;Dongdong Zhang;Ming Zhou	2010			computer science;machine learning;pattern recognition;data mining;model selection;log-linear model	NLP	-19.801335819318876	-77.8984585125652	183389
2ec7114612aa192cea559545ddb53dc58fadb25a	use of direct modeling in natural language generation for chinese and english translation	grammars language translation maximum entropy methods natural languages linguistics;maximum entropy methods;bleu score automatic speech to speech translation maximum entropy probability model natural language generation direct modeling chinese english translation interlingua based speech translation system linguistic constituent information concept padding scheme semantic parse tree concept sequences concept error rate cer reduction;language translation;model based approach;natural languages;grammars;speech translation;target language;natural language generation;error rate;natural languages speech recognition speech synthesis entropy humans knowledge representation error analysis testing natural language processing protection;maximum entropy;linguistics	This paper proposes a new direct-modeling-based approach to improve the maximum entropy based natural language generation (NLG) in the IBM MASTOR system, an interlingua-based speech translation system. Due to the intrinsic disparity between Chinese and English sentences, the previous method employed only linguistic constituents from output language sentences to train the NLG model. The new algorithm exploits a direct-modeling scheme to admit linguistic constituent information from both source and target languages into the training process seamlessly when incorporating a concept padding scheme. When concept sequences from the top level of semantic parse trees are considered, the concept error rate (CER) is significantly reduced to 14.3%, compared to 23.9% in the baseline NLG. Similarly, when concept sequences from all levels of semantic parse trees are tested, the direct-modeling scheme yields a CER of 10.8% compared to 17.8% in the baseline. A sensible improvement on the overall translation is made when the direct-modeling scheme improves the BLEU score from 0.252 to 0.294.	algorithm;bleu;baseline (configuration management);binocular disparity;explicit modeling;machine translation;natural language generation;padding (cryptography);parse tree;parsing	Fu-Hua Liu;Yuqing Gao	2004	2004 International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2004.1409650	natural language processing;cache language model;speech recognition;transfer-based machine translation;universal networking language;word error rate;computer science;principle of maximum entropy;linguistics;machine translation;rule-based machine translation;natural language	NLP	-21.882453982385513	-78.00115899157261	183466
4573a656ce383ddfe3066f91ce0a5c18d4a29d57	thai grapheme-to-phoneme using probabilistic glr parser.	decision tree;rule based;probabilistic model	Many difficulties in the Thai language such as the absence of boundary word, linking syllables in pronunciation, and homographs are challenging us in developing a Thai Grapheme-to-Phoneme (G2P) converter. Presently there are a couple Thai G2P systems which are proposed in ruled-based and decision-tree approach. The rule-based approach has a drawback in the limitation of employing the context. The decision-tree approach is somehow able to capture the local context for making the decision. On the contrary, the Probabilistic Generalized LR (PGLR) approach is reported that both the global and local context are efficiently captured in the probabilistic model. In this paper, we implement a Thai G2P system based on the PGLR approach. The result of experiment shows 90.44% of word accuracy in case of ignoring vowels length and 72.87% of word accuracy in case of exact match evaluation. These results are superior to those of rule-based and decision-tree approaches.	decision tree;glr parser;lr parser;lattice model (finance);logic programming;statistical model	Pongthai Tarsaku;Virach Sornlertlamvanich;Rachod Thongprasirt	2001			rule-based system;natural language processing;statistical model;probabilistic ctl;probabilistic relevance model;computer science;machine learning;decision tree;glr parser;pattern recognition	AI	-22.056156852188497	-78.52718258451573	184335
d38235a812ac41ac583259ede58e664d0bba9c2f	relation mention extraction from noisy data with hierarchical reinforcement learning		In this paper we address a task of relation mention extraction from noisy data: extracting representative phrases for a particular relation from noisy sentences that are collected via distant supervision. Despite its significance and value in many downstream applications, this task is less studied on noisy data. The major challenges exists in 1) the lack of annotation on mention phrases, and more severely, 2) handling noisy sentences which do not express a relation at all. To address the two challenges, we formulate the task as a semi-Markov decision process and propose a novel hierarchical reinforcement learning model. Our model consists of a top-level sentence selector to remove noisy sentences, a low-level mention extractor to extract relation mentions, and a reward estimator to provide signals to guide data denoising and mention extraction without explicit annotations. Experimental results show that our model is effective to extract relation mentions from noisy data.	approximation algorithm;baseline (configuration management);concatenation;downstream (software development);experiment;high- and low-level;markov chain;markov decision process;n-gram;noise reduction;randomness extractor;reinforcement learning;relationship extraction;semiconductor industry;signal-to-noise ratio;statistical classification	Jun Feng;Minlie Huang;Yijie Zhang;Yang Yang;Xiaoyan Zhu	2018	CoRR			NLP	-21.147741144491537	-73.4800753436312	184515
a48affd9a3bf680e810a0c0b9aa37d419a4cb09c	annotating noun argument structure for nombank		When complete, NomBank will provide annotation of noun arguments in Penn Treebank II (PTB). In PropBank, University of Pennsylvania annotators provide similar information for verbs. Given nominalization/verb mappings, the combination of NomBank and PropBank allows for generalization of arguments across parts of speech. This paper describes our annotation task including factors which make assigning role labels to noun arguments a challenging task.	propbank;treebank	Adam Meyers;Ruth Reeves;Catherine Macleod;Rachel Szekely;Veronika Zielinska;Brian Young;Ralph Grishman	2004			nominalization;part of speech;propbank;treebank;artificial intelligence;natural language processing;noun;computer science;verb;annotation	NLP	-25.737674189421444	-74.81526643958243	184580
33fd841560ad20ae25e6ffb852b352973cc58a31	improving supervised learning for meeting summarization using sampling and regression	corpus annotation;statistique;supervised learning;information extraction;modele markov cache;imbalanced data;automatic summarization;echantillonnage;supervised classification;regression model;information access;annotation de corpus;sampling;performance improvement;regression;corpus;meeting summarization;transcription;resume automatique;modele regression;reconnaissance de la parole;entropie;statistics;speech recognition;binary classification;entropy;computational linguistics;apprentissage supervise;extraction d information;sampling methods;linguistique informatique;champ aleatoire	Meeting summarization provides a concise and informative summary for the lengthy meetings and is an effective tool for efficient information access. In this paper, we focus on extractive summarization, where salient sentences are selected from the meeting transcripts to form a summary. We adopt a supervised learning approach for this task and use a classifier to determine whether to select a sentence in the summary based on a rich set of features. We address two important problems associated with this supervised classification approach. First we propose different sampling methods to deal with the imbalanced data problem for this task where the summary sentences are the minority class. Second, in order to account for human disagreement for summary annotation, we reframe the extractive summarization task using a regression scheme instead of binary classification. We evaluate our approaches using the ICSI meeting corpus on both the human transcripts and speech recognition output, and show performance improvement using different sampling methods and regression model. 2009 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;automated system recovery;automatic summarization;binary classification;cosine similarity;experiment;feature selection;ibm notes;information access;lexicon;loss function;michel hénon;numerical analysis;optimization problem;sampling (signal processing);segmentation fault;speech recognition;statistical classification;statistical model;supervised learning;text segmentation;tf–idf;x image extension;dialog	Shasha Xie;Yang Liu	2010	Computer Speech & Language	10.1016/j.csl.2009.04.007	natural language processing;sampling;entropy;speech recognition;multi-document summarization;computer science;computational linguistics;automatic summarization;machine learning;pattern recognition;data mining;supervised learning;information extraction;statistics	NLP	-21.207254547858057	-79.72353080528967	185177
0915ce48dca94cffa975d924e84a751b9e535786	segmentation-free word embedding for unsegmented languages.		In this paper, we propose a new pipeline of word embedding for unsegmented languages, called segmentation-free word embedding, which does not require word segmentation as a preprocessing step. Unlike space-delimited languages, unsegmented languages, such as Chinese and Japanese, require word segmentation as a preprocessing step. However, word segmentation, that often requires manually annotated resources, is difficult and expensive, and unavoidable errors in word segmentation affect downstream tasks. To avoid these problems in learning word vectors of unsegmented languages, we consider word co-occurrence statistics over all possible candidates of segmentations based on frequent character n-grams instead of segmented sentences provided by conventional word segmenters. Our experiments of noun category prediction tasks on raw Twitter, Weibo, and Wikipedia corpora show that the proposed method outperforms the conventional approaches that require word segmenters.	delimiter;downstream (software development);experiment;grams;n-gram;null character;preprocessor;text corpus;text segmentation;wikipedia;word embedding	Takamasa Oshikiri	2017		10.18653/v1/d17-1080	word embedding;natural language processing;computer science;artificial intelligence;segmentation	NLP	-23.74365558440136	-76.11027948438455	185237
7247811f4a994b91f3af530bcc41924a4e8f6be5	an experiment in spanish-portuguese statistical machine translation	statistical approach;rule based system;statistical machine translation;statistical ma chine translation;machine translation	Statistical approaches to machine translation have long been successfully applied to a number of „distant‟ language pairs such as English-Arabic and English-Chinese. In this work we describe an experiment in statistical machine translation between two „related‟ languages: European Spanish and Brazilian Portuguese. Preliminary results suggest not only that statistical approaches are comparable to a rule-based system, but also that they are more adaptive and take considerably less effort to be developed.	experiment;rule-based system;statistical machine translation	Wilker Ferreira Aziz;Thiago Alexandre Salgueiro Pardo;Ivandré Paraboni	2008		10.1007/978-3-540-88190-2_30	natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;artificial intelligence;data science;evaluation of machine translation;machine translation;rule-based machine translation;machine translation software usability	NLP	-23.087996608741495	-76.72381713394206	185344
8e9d0eea4e853fb83031df2bee302603271a944f	smt from agglutinative languages: use of suffix separation and word splitting		Marathi and Hindi both being Indo-Aryan family members and using Devanagari script are similar to a great extent. Both follow SOV sentence structure and are equally liberal in word order. The translation for this language pair appears to be easy. But experiments show this to be a significantly difficult task, primarily due to the fact that Marathi is morphologically richer compared to Hindi. We propose a Marathi to Hindi Statistical Machine Translation (SMT) system which makes use of compound word splitting to tackle the morphological richness of Marathi.	baseline (configuration management);dictionary;experiment;indo;statistical machine translation;text segmentation	Prakash B. Pimpale;Raj Nath Patel;M Sasikumar	2014			natural language processing;suffix;distributed computing;computer science;artificial intelligence;agglutinative language	NLP	-26.350648233870253	-74.64120107235796	185490
42d26738ff9624d6a23574c6cb2e6895808cd343	lexicalized semi-incremental dependency parsing	blow up;search space;look ahead;universiteitsbibliotheek;left to right;linear time;machine translating;speech recognition;dependence structure;dependency parsing;machine translation	Even leaving aside concerns of cognitive plausibility, incremental parsing is appealing for applications such as speech recognition and machine translation because it could allow the incorporation of syntactic features into the decoding process without blowing up the search space. Nevertheless, incremental parsing is often associated with greedy parsing decisions and intolerable loss of accuracy. Would the use of lexicalized grammars provide a new perspective on incremental	bottom-up parsing;combinatory categorial grammar;greedy algorithm;language model;local search (optimization);machine translation;parsing expression grammar;plausibility structure;semiconductor industry;speech recognition;speedup	Hany Hassan;Khalil Sima'an;Andy Way	2009			natural language processing;time complexity;parser combinator;speech recognition;parsing expression grammar;top-down parsing language;computer science;bottom-up parsing;machine learning;s-attributed grammar;linguistics;machine translation;programming language;top-down parsing;dependency grammar	NLP	-21.94672529364059	-78.17080297030634	185517
19bf44c31eca4c8163d2bdc9f700339b87b6b236	scalable micro-planned generation of discourse from structured data		We present a framework for generating natural language description from structured data such as tables. Motivated by the need to approach this problem in a manner that is scalable and easily adaptable to newer domains, unlike existing related systems, our system does not require parallel data; it rather relies on monolingual corpora and basic NLP tools which are easily accessible. The system employs a 3-staged pipeline that: (i) converts entries in the structured data to canonical form, (ii) generates simple sentences for each atomic entry in the canonicalized representation, and (iii) combines the sentences to produce a coherent, fluent and adequate paragraph description through sentence compounding and co-reference replacement modules. Experiments on a benchmark mixeddomain dataset curated for paragraph description from tables reveals the superiority of our system over existing data-to-text approaches. We also demonstrate the robustness of our system in accepting other data types such as Knowledge-Graphs and Key-Value dictionaries.	attribute–value pair;automatic summarization;benchmark (computing);coherence (physics);data model;dictionary;emoticon;experiment;knowledge graph;natural language processing;scalability;table (database);table (information);text corpus	Anirban Laha;Parag Jain;Abhijit Mishra;Karthik Sankaranarayanan	2018	CoRR		paragraph;robustness (computer science);data type;canonical form;natural language processing;machine learning;artificial intelligence;natural language;scalability;data model;computer science;sentence	NLP	-25.132584849523912	-74.95056026538838	186090
69669af61ba7152a6fadfe360d767649275626f5	time series analysis using noc	latent variable models;nonparametric models;n gram topic model;time series analysis;topic models	We present a time series analysis employing natural language processing (NLP) techniques, and show the effect of N -gram over Context (NOC), that is a one of topic models that enjoy success in NLP, in this analysis.	natural language processing;network on a chip;time series;topic model	Noriaki Kawamae	2016		10.1145/2872518.2889396	natural language processing;speech recognition;computer science;machine learning;time series;topic model	NLP	-19.51804437018205	-78.2106058810142	186186
039c572799e3ba31a9128db108dbdc2d8285d51a	an approach to word sense disambiguation in english-vietnamese-english statistical machine translation	bleu scores word sense disambiguation english vietnamese english statistical machine translation polysemous words text context text topic part of speech morphology;support vector machines;training;statistical machine translation;language translation;semantics;text analysis;testing;word sense disambiguation;vectors;text analysis language translation natural language processing;support vector machine;buildings testing vectors training tagging semantics support vector machines;natural language processing;buildings;tagging	The most difficult problem of machine translation (MT) in general and statistical machine translation (SMT) in particular is to select the correct meaning of the polysemous words. Their correct meaning mainly depends on the context and the topic of the text. Therefore, to improve the quality of SMT by resolving semantic ambiguity of words, we integrate more knowledge about the topic of the text, part-of-speech (POS) and morphology. We applied this model to English-Vietnamese- English SMT system and BLEU scores increased over 6% compared with the baseline general SMT system, which was not integrated information about the topic or other language knowledge.	bleu;baseline (configuration management);galaxy morphological classification;mathematical morphology;part-of-speech tagging;preprocessor;statistical machine translation;word sense;word-sense disambiguation	Quy Nguyen;An Nguyen;Dinh Dien	2012	2012 IEEE RIVF International Conference on Computing & Communication Technologies, Research, Innovation, and Vision for the Future	10.1109/rivf.2012.6169839	natural language processing;statistical semantics;speech recognition;example-based machine translation;semeval;word error rate;computer science;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-23.21102239990659	-76.5355381778815	186316
d2721ae72da23ead4e475e46beec584c701cb0ee	interleaving syntax and semantics in an effecient bottom-up parser		z Abstract We describe an eecient bottom-up parser that in-terleaves syntactic and semantic structure building. Two techniques are presented for reducing search by reducing local ambiguity: Limited left-context constraints are used to reduce local syntactic ambiguity, and deferred sortal-constraint application is used to reduce local semantic ambiguity. We experimentally evaluate these techniques , and show dramatic reductions in both number of chart edges and total parsing time. The robust processing capabilities of the parser are demonstrated in its use in improving the accuracy of a speech recognizer.	bottom-up parsing;experiment;finite-state machine;forward error correction;parser;speech recognition	John Dowding;Robert C. Moore;François Andry;Douglas B. Moran	1994				NLP	-22.40476513749534	-78.60042952391339	187043
441e94ef94acf2fe1ef8809138738c927528c1c5	stochastic parse tree selection for an existing rbmt system	analysis parse tree;stochastic parse tree selection;rbmt engine;rule-based translation system;wmt11 shared translation task;hybrid machine translation system;analysis phase;existing rbmt system;machine translation quality;rule-based mt system;rbmt system;rbmt baseline	In this paper we describe our hybrid machine translation system with which we participated in the WMT11 shared translation task for the English→German language pair. Our system was able to outperform its RBMT baseline and turned out to be the best-scored participating system in the manual evaluation. To achieve this, we extended an existing, rule-based MT system with a module for stochastic selection of analysis parse trees that allowed to better cope with parsing errors during the system’s analysis phase. Due to the integration into the analysis phase of the RBMT engine, we are able to preserve the benefits of a rule-based translation system such as proper generation of target language text. Additionally, we used a statistical tool for terminology extraction to improve the lexicon of the RBMT system. We report results from both automated metrics and human evaluation efforts, including examples which show how the proposed approach can improve machine translation quality.	baseline (configuration management);compiler;hybrid machine translation;lexicon;logic programming;parse tree;parsing;rule-based machine translation;terminology extraction	Christian Federmann;Sabine Hunsicker	2011			natural language processing;speech recognition;computer science;data mining;rule-based machine translation	NLP	-22.683732387366696	-77.11295393089175	187077
e779f0429d09c74c6c8a7b9f2ab4fbfe873b08af	formal multiple-bernoulli models for language modeling	multinomial distribution;information retrieval;language modeling;formal method;language model	Statistical language modeling allows formal methods to be applied to information retrieval. As a result, such methods are preferred over their heuristic tf.idf -based counterparts. In language modeling, a statistical model is estimated for each document in the corpus. Documents are then scored by the likelihood the query was generated by the document’s model. Typically, the underlying model is assumed to be of a specific parametric form. In the past, a number of different assumptions have been made about this distribution. In [1], documents were modeled by a multiple-Bernoulli distribution. However, the estimation and smoothing techniques used to estimate the model were non-standard and somewhat heuristic. The predominant modeling assumption used today, as described in [2], is to model documents by a multinomial distribution. Such models may be smoothed in a number of ways [4]. Among these is Bayesian (Dirichlet) smoothing that takes a formal, Bayesian approach to smoothing by assuming a Dirichlet prior over the document model. Unlike Ponte and Croft’s multiple-Bernoulli estimation techniques, the multinomial assumption combined with Bayesian smoothing results in a completely formal statistical model. In this paper, we revisit the multiple-Bernoulli assumption and formalize it by taking a Bayesian approach to estimating smoothed document models.	bernoulli polynomials;formal methods;heuristic;information retrieval;language model;multinomial logistic regression;smoothing;statistical model;text corpus;tf–idf	Donald Metzler;Victor Lavrenko;W. Bruce Croft	2004		10.1145/1008992.1009110	grammar systems theory;natural language processing;language identification;formal methods;universal networking language;formal semantics;object language;specification language;computer science;pattern recognition;modeling language;programming language;information retrieval;multinomial distribution;object constraint language;statistics;language model	Web+IR	-21.55129809505765	-79.80053886900576	187247
78e592524e336afdf586c51ac228488a71e31340	sentence embedding for neural machine translation domain adaptation		Although new corpora are becoming increasingly available for machine translation, only those that belong to the same or similar domains are typically able to improve translation performance. Recently Neural Machine Translation (NMT) has become prominent in the field. However, most of the existing domain adaptation methods only focus on phrase-based machine translation. In this paper, we exploit the NMT’s internal embedding of the source sentence and use the sentence embedding similarity to select the sentences which are close to in-domain data. The empirical adaptation results on the IWSLT English-French and NIST Chinese-English tasks show that the proposed methods can substantially improve NMT performance by 2.4-9.0 BLEU points, outperforming the existing state-of-the-art baseline by 2.3-4.5 BLEU points.	bleu;baseline (configuration management);domain adaptation;neural machine translation;performance;text corpus	Rui Wang;Andrew M. Finch;Masao Utiyama;Eiichiro Sumita	2017		10.18653/v1/P17-2089	machine learning;artificial intelligence;computer science;machine translation;natural language processing;domain adaptation;transfer-based machine translation;embedding;rule-based machine translation;example-based machine translation;sentence	NLP	-19.52006967746957	-74.10132261213874	187786
f4c5b1ab90144cb25803fa07923f86a866d0581d	bleus-syn: cilin-based smoothed bleu		Machine Translation (MT) evaluation is very important for a MT system. In this paper, we investigate an improved Cilin-based smoothed BLEU (BLEUS-syn). As the possible cases that the short translation or English abbreviations in candidate may cause unigram have no matches, this evaluation metric smoothed the traditional BLEUS n-gram. It applied synonym substitution in unigram matching, and calculated the other 2–4-gram. It performed experiments in Russian and Chinese bilingual sentence data set and evaluated the output translations of online translation systems such as Google, Baidu, Bing and Youdao. The experimental results show that the effectiveness of our BLEUS-syn and traditional BLEUS are consistent. The performance of Baidu is the best, that of Youdao is the second, and that of Bing is the worst. Using BLEUS-syn can greatly enhance the performance of traditional BLEUS evaluation. It makes the Baidu BLEUS value improve 6.81%, Youdao improve 6.98%, Google 7.82%, and Bing 7.55%.	bleu;smoothed analysis;smoothing	Junting Yu;Wuying Liu;Hongye He;Mianzhu Yi	2016		10.1007/978-981-10-3635-4_9	machine translation;bleu;synonym;computer science;sentence;pattern recognition;artificial intelligence	NLP	-23.056515786722397	-76.24017276660958	187804
659c6b4819d2afea0bdc06e01cefd3e274219530	nonparametric model for inupiaq word segmentation		We present how to use English translation for unsupervised word segmentation of low resource languages. The inference uses a dynamic programming algorithm for efficient blocked Gibbs sampling. We apply the model to Inupiaq morphology analysis and get better results than monolingual model as well as Morfessor output.	algorithm;dynamic programming;gibbs sampling;mathematical morphology;sampling (signal processing);text segmentation;unsupervised learning	ThuyLinh Nguyen;Stephan Vogel	2012			pattern recognition	NLP	-20.69741160310921	-77.11883446830107	187922
c1213cb01ada6376574677566ada9379f8c5732d	modern natural language interfaces to databases: composing statistical parsing with semantic tractability	benchmark atis data;natural language interfaces;statistical parsing;modern natural language interface;parser error;semantic tractability;paper report;database-independent nlis;corresponding sql query;statistical parser;statistical parsers;precise nli;semantic model;natural language interface	Natural Language Interfaces to Databases (NLIs) can benefit from the advances in statistical parsing over the last fifteen years or so. However, statistical parsers require training on a massive, labeled corpus, and manually creating such a corpus for each database is prohibitively expensive. To address this quandary, this paper reports on the PRECISE NLI, which uses a statistical parser as a “plug in”. The paper shows how a strong semantic model coupled with “light re-training” enables PRECISE to overcome parser errors, and correctly map from parsed questions to the corresponding SQL queries. We discuss the issues in using statistical parsers to build database-independent NLIs, and report on experimental results with the benchmark ATIS data set where PRECISE achieves 94% accuracy.	automatic transmitter identification system (television);benchmark (computing);native-language identification;natural language;sql;statistical parsing;text corpus	Ana-Maria Popescu;Alex Armanasu;Oren Etzioni;David Ko;Alexander Yates	2004			semantic data model;natural language processing;natural language user interface;computer science;database;linguistics;programming language;lr parser	NLP	-24.548938494633298	-74.35988728049958	188139
48b915e2d54e93c3a66e9ddc35244ebdef22c101	computation of the n best parse trees for weighted and stochastic context-free grammars	gorithme cocke younger kasami cyk;stochastic context free grammar;wall street journal;efficient algorithm;optical character recognition;gramatica cf;grammaire cf;context free grammar;natural language;reconocimento optico de caracteres;pattern recognition;analizador sintaxico;speech recognition;parser;reconnaissance forme;reconocimiento patron;computational biology;analyseur syntaxique;language model;reconnaissance optique caractere	Context-Free Grammars are the object of increasing interest in the pattern recognition research community in an attempt to overcome the limited modeling capabilities of the simpler regular grammars, and have application in a variety of fields such as language modeling, speech recognition, optical character recognition, computational biology, etc. This paper proposes an efficient algorithm to solve one of the problems associated to the use of weighted and stochastic Context-Free Grammars: the problem of computing the N best parse trees of a given string. After the best parse tree has been computed using the CYK algorithm, a large number of alternative parse trees are obtained, in order by weight (or probability), in a small fraction of the time required by the CYK algorithm to find the best parse tree. This is confirmed by experimental results using grammars from two different domains: a chromosome grammar, and a grammar modeling natural language sentences from the Wall Street Journal corpus.	cyk algorithm;computation;computational biology;context-free grammar;enumerated type;language model;natural language;optical character recognition;order by;parse tree;parsing;pattern recognition;regular grammar;speech recognition;the wall street journal	Víctor M. Jiménez;Andrés Marzal	2000		10.1007/3-540-44522-6_19	natural language processing;context-sensitive grammar;tree-adjoining grammar;l-attributed grammar;speech recognition;parsing expression grammar;computer science;artificial intelligence;machine learning;extended affix grammar;database;mathematics;natural language;context-free grammar;programming language;optical character recognition;ambiguous grammar;stochastic context-free grammar;algorithm;language model	NLP	-24.63363378833391	-79.12624263979295	188378
83357a5c5b8bd51b3bf7d95fbe04cd7d2c954b57	a comparative study of extremely low-resource transliteration of the world's languages		Transliteration from low-resource languages is difficult, in large part due to the small amounts of data available for training transliteration systems. In this paper, we evaluate the effectiveness of several translation methods in the task of transliterating around 1000 Bible names from 591 languages into English. In this extremely low-resource task, we found that a phrase-based MT system performs much better than other methods, including a g2p system and a neural MT system. However, by combining the data and training a single neural system, we discovered significant gains over single-language systems. We release the output from each system for comparative analysis.	qualitative comparative analysis	Winston Wu;David Yarowsky	2018			natural language processing;speech recognition;artificial intelligence;computer science;transliteration	NLP	-20.235435950440948	-74.28891112199165	188536
6810e6e5ded8400d07672b4c583360e3cfbea02e	crucial combinations for the recognition of handwritten letters	clarification discrimination;character models;partition combination;crucial combination;character recognition	Crucial features are important for the identi®cation of patterns. Great eorts have been devoted to discover the most distinctive features. This paper presents the ideas of splitting patterns into 4or 6-parts, integration for recognition, and selection of crucial combinations. The crucial combinations are explored with particular reference to 26 handprinted English letters. This paper proposes the ®rst simple algorithms to perform the crucial combinations. Also, the largest confusion regions and algorithms for ®nding them are provided. The most useful crucial combinations and the largest confusion combinations are listed in this paper. Moreover, we extend the crucial combinations and largest confusion regions for the threshold recognition rates qij Pq0, say q0  100%; 80% and 50%. The second algorithms are developed, and numerical results for 4-partitions are provided. Ó 2000 Elsevier Science B.V. All rights reserved.	algorithm;numerical analysis	Zi-Cai Li;Ching Y. Suen	2000	Pattern Recognition Letters	10.1016/S0167-8655(00)00047-7	arithmetic;artificial intelligence	Vision	-26.353869419677	-78.83867527003946	189236
5322edd46ea81a2a93d4c6b2049f1684713cf8b6	adaptive text correction with web-crawled domain-dependent dictionaries	web pages;adaptive techniques;error correction;web crawling;dictionaries;error rate;domains	For the success of lexical text correction, high coverage of the underlying background dictionary is crucial. Still, most correction tools are built on top of static dictionaries that represent fixed collections of expressions of a given language. When treating texts from specific domains and areas, often a significant part of the vocabulary is missed. In this situation, both automated and interactive correction systems produce suboptimal results. In this article, we describe strategies for crawling Web pages that fit the thematic domain of the given input text. Special filtering techniques are introduced to avoid pages with many orthographic errors. Collecting the vocabulary of filtered pages that meet the vocabulary of the input text, dynamic dictionaries of modest size are obtained that reach excellent coverage values. A tool has been developed that automatically crawls dictionaries in the indicated way. Our correction experiments with crawled dictionaries, which address English and German document collections from a variety of thematic fields, show that with these dictionaries even the error rate of highly accurate texts can be reduced, using completely automated correction methods. For interactive text correction, more sensible candidate sets for correcting erroneous words are obtained and the manual effort is reduced in a significant way. To complete this picture, we study the effect when using word trigram models for correction. Again, trigram models from crawled corpora outperform those obtained from static corpora.	dictionary;experiment;orthographic projection;text corpus;trigram;vocabulary;web page	Christoph Ringlstetter;Klaus U. Schulz;Stoyan Mihov	2007	TSLP	10.1145/1289600.1289602	natural language processing;error detection and correction;speech recognition;word error rate;computer science;web crawler;machine learning;web page;information retrieval	NLP	-23.806361481593232	-79.79267243768173	189419
0af5109626bfae9379879c7fe1848a0df1b9e313	dependency link embeddings: continuous representations of syntactic substructures		We present a simple method to learn continuous representations of dependency substructures (links), with the motivation of directly working with higher-order, structured embeddings and their hidden relationships, and also to avoid the millions of sparse, template-based word-cluster features in dependency parsing. These link embeddings allow a significantly smaller and simpler set of unary features for dependency parsing, while maintaining improvements similar to state-of-the-art, n-ary word-cluster features, and also stacking over them. Moreover, these link vectors (made publicly available) are directly portable as offthe-shelf, dense, syntactic features in various NLP tasks. As one example, we incorporate them into constituent parse reranking, where their small feature set again matches the performance of standard non-local, manuallydefined features, and also stacks over them.	downstream (software development);feature learning;machine learning;natural language processing;parse tree;parsing;sparse matrix;stacking;structured programming;unary operation	Mohit Bansal	2015			machine learning;stack (abstract data type);syntax;dependency grammar;computer science;artificial intelligence;parsing;unary operation	NLP	-19.236905724366768	-74.43840691016642	189570
2bfed7386785cd9d5ae5fc9ab733154bdee3d582	dialogue act tagging and segmentation with a single perceptron		In this paper we present a simultaneous automatic Dialogue Act (DA) tagger and segmenter. The model employed is based on the well-known single layer perceptron algorithm used successfully in other Computational Linguistic tasks. A decoding process was developed for searching the sequence of segments and DA tags from all the possible exponential possibilities. A set of features based on combination of words and DA tags were empirically selected. Models were tested over transcriptions of two corpora of dialogues (Switchboard and Dihana) and transcriptions and ASR output of a third corpus composed by meetings (AMI corpus). The results obtained for such a simple but powerful model are for some of the evaluation metrics equal or better than much more complex models presented in recent studies for the same experiments.	algorithm;brill tagger;computation;evaluation function;experiment;feedforward neural network;perceptron;telephone switchboard;text corpus;time complexity;whole earth 'lectronic link	Ramón Granell;Stephen G. Pulman;Carlos D. Martínez-Hinarejos;José-Miguel Benedí	2010			perceptron;speech recognition;pattern recognition;machine learning;computer science;segmentation;artificial intelligence	NLP	-22.162810546306055	-76.05192891115152	190910
e5d9bd94a65f3ed497a4d36e089b1393b0e5520d	probabilistic tree-edit models with structured latent variables for textual entailment and question answering	latent variable;question answering	A range of Natural Language Processing tasks involve making judgments about the semantic relatedness of a pair of sentences, such as Recognizing Textual Entailment (RTE) and answer selection for Question Answering (QA). A key challenge that these tasks face in common is the lack of explicit alignment annotation between a sentence pair. We capture the alignment by using a novel probabilistic model that models tree-edit operations on dependency parse trees. Unlike previous tree-edit models which require a separate alignment-finding phase and resort to ad-hoc distance metrics, our method treats alignments as structured latent variables, and offers a principled framework for incorporating complex linguistic features. We demonstrate the robustness of our model by conducting experiments for RTE and QA, and show that our model performs competitively on both tasks with the same set of general features.	conditional random field;dependency grammar;discriminative model;experiment;hoc (programming language);latent variable;natural language processing;parsing;question answering;semantic similarity;software quality assurance;statistical model;ted;textual entailment	Mengqiu Wang;Christopher D. Manning	2010			latent variable;natural language processing;question answering;computer science;pattern recognition;data mining	NLP	-23.23511275817163	-73.81243422364463	191252
7553c3f298ba0633fd2d85309d24f814d1a7772b	on wordsense disambiguation through morphological transformation and semantic distance and domain link knowledge		Despite the advances in information processing systems, word-sense disambiguation tasks are far to be satisfactory as testified by numerous limitations of current translation systems and text inference systems. This paper attempts to investigate new techniques in knowledge based word-sense disambiguation field. First, by exploring the WordNet lexical database and part-of-speech conversion through the established CatVar database that translates all non-noun words into their noun counterparts, and following the spirit of Lesk's disambiguation algorithm, a new disambiguation algorithm that maximizes the overall semantic similarity in the sense of Wu and Palmer measure between each sense of the target word and synsets of words of the context, is established. Second, motivated by the existence of WordNet domains for individual synsets, an overlapping based approach that quantifies the set intersection of synset domains, if not empty, or the hierarchy structure of the domains links through a simple path-length measure is put forward. Third, instead of exploring the whole set of words involved in the context, a selective approach that uses syntactic feature as outputted by Stanford Parser and a fixed length windowing is developed. The developed algorithms are evaluated according to two commonly employed dataset where a clear improvement to the baseline algorithm has been acknowledged.	baseline (configuration management);information processing;lesk algorithm;lexical database;memory disambiguation;parser;path (graph theory);semeval;semantic similarity;synonym ring;word sense;word-sense disambiguation;wordnet	Mourad Oussalah;Mikko Vaisanen;Ekaterina Gilman	2018	2018 IEEE International Conference on Information Reuse and Integration (IRI)	10.1109/IRI.2018.00024	semantic similarity;machine learning;artificial intelligence;semantics;noun;parsing;information processing;wordnet;computer science;lexical database;inference	NLP	-19.429622114881543	-73.31013049759703	191987
9bbafa13ae8fe107aeded459c966608bfb8efc80	development of part of speech tagger for assamese using hmm			brill tagger;hidden markov model;part-of-speech tagging	Surjya Kanta Daimary;Vishal Goyal;Madhumita Barbora;Umrinderpal Singh	2018	IJSE	10.4018/IJSE.2018010102	speech recognition;artificial intelligence;machine learning;hidden markov model;psychology;part of speech;assamese	NLP	-24.843691652623274	-78.89526866340685	194094
fb4e742db733157036ab7b54d2509a3865e0c4e4	chinese text segmentation with mbdp-1: making the most of training corpora	mbdp-1 algorithm;knowledge-free segmentation algorithm;mbdp-1 converges;chinese text segmentation;common event;chinese text;available hand-segmented training corpus;previous algorithm;hand-segmented training corpus;new genre;small corpus;text segmentation	This paper describes a system for segmenting Chinese text into words using the MBDP-1 algorithm. MBDP-1 is a knowledge-free segmentation algorithm that bootstraps its own lexicon, which starts out empty. Experiments on Chinese and English corpora show that MBDP-1 reliably outperforms the best previous algorithm when the available hand-segmented training corpus is small. As the size of the hand-segmented training corpus grows, the performance of MBDP-1 converges toward that of the best previous algorithm. The fact that MBDP-1 can be used with a small corpus is expected to be useful not only for the rare event of adapting to a new language, but also for the common event of adapting to a new genre within the same language.	text corpus;text segmentation	Michael R. Brent;Xiaopeng Tao	2001			natural language processing;text segmentation;speech recognition;performance;computer science;computational linguistics;corpus linguistics;text corpus;linguistics;segmentation;chinese	NLP	-23.792848695061956	-77.44178417622065	194379
4e1be2c65cacb5772424ac9ba19f61d45e54581e	statistical recognition of noun phrases in unrestricted text	grammar;modelizacion;text;analisis estadistico;analisis datos;noun phrase;transition probability;speech processing;tratamiento palabra;traitement parole;intelligence artificielle;texte;probabilistic approach;algoritmo genetico;modelisation;data analysis;statistical analysis;grammaire;enfoque probabilista;approche probabiliste;part of speech tagging;analyse statistique;probabilidad transicion;algorithme genetique;artificial intelligence;algorithme evolutionniste;analyse donnee;genetic algorithm;algoritmo evolucionista;inteligencia artificial;evolutionary algorithm;texto;modeling;gramatica;probabilite transition	This paper presents a new model for flexible noun phrase detection, which is able to recognize noun phrases similar enough to the ones given by the inferred noun phrase grammar. To allow this flexibility, we use a very accurate set of probabilities for the transitions between the part-of-speech tag sequence which defines a noun phrase. These accurate probabilities are obtained by means of an evolutionary algorithm, which works with both, positive and negative examples of the language, thus improving the system coverage, while maintaining its precision. We have tested the system on different corpora and compare the results with other systems, what has revealed a clear improvement of the performance.		Jose Ignacio Serrano;Lourdes Araujo	2005		10.1007/11552253_36	natural language processing;markov chain;noun phrase;speech recognition;systems modeling;genetic algorithm;computer science;artificial intelligence;evolutionary algorithm;speech processing;grammar;determiner phrase;data analysis;statistics	NLP	-25.283642330832944	-78.96048022268673	194637
e7dae8b7a56c0a66196e050fbc543791597af816	the 2018 shared task on extrinsic parser evaluation: on the downstream utility of english universal dependency parsers		We summarize empirical results and tentative conclusions from the Second Extrinsic Parser Evaluation Initiative (EPE 2018). We review the basic task setup, downstream applications involved, and end-to-end results for seventeen participating parsers. Based on both quantitative and qualitative analysis, we correlate intrinsic evaluation results at different layers of morphsyntactic analysis with observed downstream behavior. 1 Background and Motivation The Second Extrinsic Parser Evaluation Initiative (EPE 2018) was organized as an optional track of the 2018 Shared Task on Multilingual Parsing from Raw Text to Universal Dependencies (Zeman et al., 2018) at the Conference on Computational Natural Language Learning (CoNLL 2018). In the following, we distinguish the tracks as the EPE vs. the ‘core’ UD parsing tasks, respectively. One focus of the UD parsing task in 2018 was on different intrinsic evaluation metrics, such that the connection to the EPE framework provides new opportunities for correlating intrinsic metrics with downstream utility to three relevant applications, viz. biological event extraction, fine-grained opinion analysis, and negation resolution. Unlike the strongly multilingual core task, the EPE framework for the time being is limited to English. A previous instance of the EPE initiative (see § 2 below) embraced diversity and accepted submissions of parser outputs that varied along several dimensions, including different types of syntactic or semantic dependency representations, variable parser training data in type and volume, and of course diverse approaches to input segmentation and parsing. In contrast, the association of EPE 2018 with the UD parsing task ‘fixes’ two of these dimensions: All submitted systems output basic Universal Dependency (UD; McDonald et al., 2013; Nivre et al., 2016) trees (following the conventions of UD version 2.x) and parser training data was limited to the English UD treebanks provided for the core task. 2 History: The EPE 2017 Infrastructure What we somewhat interchangeably refer to as the EPE framework or the EPE infrastructure was originally assembled in mid-2017, to enable the First Shared Task on Extrinsic Parser Evaluation (EPE 2017; Oepen et al., 2017), which was organized as a joint event by the Fourth International Conference on Dependency Linguistics (DepLing 2017) and the 15th International Conference on Parsing Technologies (IWPT 2017). The framework is characterized by a collection of ‘downstream’ natural language ‘understanding’ applications that are assumed to depend on the analysis of grammatical structure. For each downstream application, there are commonly used reference data sets (often from past shared tasks) and evaluation metrics. In the EPE context, state-ofthe-art systems for these applications have been generalized to accept as inputs a broad variety of syntactico-semantic dependency representations (i.e. parser outputs submitted for extrinsic evaluation) and to automatically retrain (and tune, to some degree) for each specific parser. The following paragraphs briefly summarize each of the downstream systems and main results from the EPE 2017 competition. Dependency Representations For compatibility with different linguistic schools in syntacticosemantic analysis, the EPE framework assumes a comparatively broad definition of suitable interface representations to grammatical analysis (Oepen et al., 2017; p. 6): The term (bi-lexical) dependency representation in the context of EPE 2017 is interpreted as a graph whose nodes are anchored in surface lexical units, and whose edges represent labeled directed relations between two nodes. Each node corresponds to a sub-string of the underlying linguistic signal (input string), identified by character stand-off pointers. Node labels can comprise a non-recursive attribute–value matrix (or ‘feature structure’), for example to encode lemma and part of speech information. Each graph can optionally designate one or more ‘top’ nodes, broadly interpreted as the root-level head or highest-scoping predicate (Kuhlmann and Oepen, 2016). In principle, this notion of dependency representations is broad in that it allows nodes that do not correspond to (full) surface tokens, partial or full overlap of nodes, as well as graphs that transcend fully connected rooted trees. Participating teams in the original EPE 2017 initiative did in fact take advantage of all these degrees of freedom, whereas in connection to the 2018 UD parsing task such variation is excluded by design. Biological Event Extraction The Turku Event Extraction System (TEES) (Björne, 2014) is a program developed for the automated extraction of events, complex relations used to define the semantic structure of a sentence. These events differ from pairwise binary relations in that they have a defined trigger node, usually a verb, they can have multiple arguments, and other events can be used as event arguments, forming complex nested relations. Events can be seen as graphs, where named entities and triggers are the nodes and the arguments linking these are the edges. In this graph model, an event is implicitly defined as a trigger node and its set of outgoing edges. The TEES system approaches event extraction as a task of graph generation, modelling it as a pipeline of consecutive, atomic classification tasks. The first step is entity detection where each token in the sentence is predicted as an entity node or as negative. In the second step of edge detection, argument edges are predicted for all valid, directed pairs of nodes. In the third, unmerging step, overlapping events are ‘pulled apart’ by duplicating trigger nodes. In the optional fourth step of modifier detection, binary modifiers (such as speculation or negation) can be predicted for the detected events. All of the classification steps in the TEES system rely on rich feature representations generated to a large degree from syntactic dependency parses. All classification tasks are implemented using the SVMmulticlass classifier (Joachims, 1999). TEES has been developed using corpora from the Biomedical Natural Language Processing (BioNLP) domain, in particular the event corpora from the BioNLP Shared Tasks. These tasks define their own annotation schemes and provide standardized evaluation services. In the context of the EPE challenge we use the BioNLP 2009 GENIA corpus and its associated evaluation program to measure the impact of different parses on event extraction performance (Kim et al., 2009). The metric used for comparing the EPE submissions is the primary ‘approximate span and recursive mode’ metric of the original Shared Task, a micro-averaged F1 score for the nine event classes of the corpus. The specialized domain language presents unique challenges for parsers not specifically optimized for this domain, so using this data set to evaluate open-domain parses may result in overall lower performance than with parsers specifically trained on e.g. the GENIA treebank (Tateisi et al., 2005). When using the EPE parse data, TEES features encompass the type and direction for the dependencies combined wit the text span and a single part of speech for the tokens; lemmas are not	approximation algorithm;biomedical text mining;computation;degree (graph theory);downstream (software development);encode;edge detection;end-to-end principle;f1 score;graph (discrete mathematics);modifier key;named entity;natural language processing;parser;recursion;scope (computer science);speculative execution;text corpus;treebank;urban dictionary;viz: the computer game;word lists by frequency	Murhaf Fares;Stephan Oepen;Lilja Øvrelid;Jari Björne;Richard Johansson	2018			natural language processing;parsing;artificial intelligence;computer science	NLP	-22.862497118917982	-73.73249343018969	195201
798957b4bbe99fcf9283027d30e19eb03ce6b4d5	dependency parsing and domain adaptation with lr models and parser ensembles	dependency parsing	We present a data-driven variant of the LR algorithm for dependency parsing, and extend it with a best-first search for probabilistic generalized LR dependency parsing. Parser actions are determined by a classifier, based on features that represent the current state of the parser. We apply this parsing framework to both tracks of the CoNLL 2007 shared task, in each case taking advantage of multiple models trained with different learners. In the multilingual track, we train three LR models for each of the ten languages, and combine the analyses obtained with each individual model with a maximum spanning tree voting scheme. In the domain adaptation track, we use two models to parse unlabeled data in the target domain to supplement the labeled out-ofdomain training set, in a scheme similar to one iteration of co-training.	algorithm;best-first search;co-training;consistency model;domain adaptation;earley parser;experiment;file spanning;glr parser;iteration;lr parser;parsing;principle of maximum entropy;spanning tree;statistical classification;test set;treebank	Kenji Sagae;Jun'ichi Tsujii	2007			natural language processing;parser combinator;speech recognition;lalr parser;canonical lr parser;parsing expression grammar;computer science;bottom-up parsing;parsing;glr parser;s-attributed grammar;linguistics;programming language;top-down parsing;lr parser;simple lr parser;dependency grammar	NLP	-21.83044815402715	-75.48982326474925	195216
cff1ce1f325c5b2535878f1b16605892404ad5f4	improving the naturalness and expressivity of language generation for spanish		We present a flexible Natural Language Generation approach for Spanish, focused on the surface realisation stage, which integrates an inflection module in order to improve the naturalness and expressivity of the generated language. This inflection module inflects the verbs using an ensemble of trainable algorithms whereas the other types of words (e.g. nouns, determiners, etc) are inflected using hand-crafted rules. We show that our approach achieves 2% higher accuracy than two stateof-art inflection generation approaches. Furthermore, our proposed approach also predicts an extra feature: the inflection of the imperative mood, which was not taken into account by previous work. We also present a user evaluation, where we demonstrate that the proposed method significantly improves the perceived naturalness of the generated language.	algorithm;imperative programming;natural language generation	Cristina Barros;Dimitra Gkatzia;Elena Lloret	2017			natural language processing;data mining;artificial intelligence;expressivity;computer science;naturalness	NLP	-25.433136537059937	-76.32507277222285	195367
cdbb107911aaa4272ddc9880a9ecb4ce026481c6	leveraging frame semantics and distributional semantics for unsupervised semantic slot induction in spoken dialogue systems	natural language processing computational linguistics interactive systems;distributional semantics;semantics vectors google manuals probabilistic logic coherence vocabulary;frame semantics task oriented spoken dialogue systems asr decodings framenet style parses distributional word representations real world spoken dialogue dataset probabilistic frame semantic parser continuous valued word embeddings unlabeled audio files semantic slots language understanding unsupervised semantic slot induction distributional semantics;unsupervised slot induction;frame semantics unsupervised slot induction distributional semantics;frame semantics	Distributional semantics and frame semantics are two representative views on language understanding in the statistical world and the linguistic world, respectively. In this paper, we combine the best of two worlds to automatically induce the semantic slots for spoken dialogue systems. Given a collection of unlabeled audio files, we exploit continuous-valued word embeddings to augment a probabilistic frame-semantic parser that identifies key semantic slots in an unsupervised fashion. In experiments, our results on a real-world spoken dialogue dataset show that the distributional word representations significantly improve the adaptation of FrameNet-style parses of ASR decodings to the target semantic space; that comparing to a state-of-the-art baseline, a 13% relative average precision improvement is achieved by leveraging word vectors trained on two 100-billion words datasets; and that the proposed technology can be used to reduce the costs for designing task-oriented spoken dialogue systems.	align (company);baseline (configuration management);dialog system;distributional semantics;experiment;frame language;framenet;information retrieval;natural language understanding;unsupervised learning;word embedding	Yun-Nung Chen;William Yang Wang;Alexander I. Rudnicky	2014	2014 IEEE Spoken Language Technology Workshop (SLT)	10.1109/SLT.2014.7078639	natural language processing;statistical semantics;speech recognition;computer science;linguistics;computational semantics;frame semantics	NLP	-19.9491058107552	-74.24583832841225	195530
cfdeccc0a94df2e50316fbbd31b508eac14c9b15	smoothing a tera-word language model	kneser-ney discounting;standard smoothing method;tera-word language model;new smoothing algorithm;frequency count;brown corpus;low frequency n-gram count;large datasets;dirichlet prior form;baseline implementation;large corpus;dirichlet prior;low frequency;language model	Frequency counts from very large corpora, such as the Web 1T dataset, have recently become available for language modeling. Omission of low frequency n-gram counts is a practical necessity for datasets of this size. Naive implementations of standard smoothing methods do not realize the full potential of such large datasets with missing counts. In this paper I present a new smoothing algorithm that combines the Dirichlet prior form of (Mackay and Peto, 1995) with the modified back-off estimates of (Kneser and Ney, 1995) that leads to a 31% perplexity reduction on the Brown corpus compared to a baseline implementation of Kneser-Ney discounting.	algorithm;baseline (configuration management);brown corpus;dirichlet kernel;entity–relationship model;interpolation;language model;n-gram;perplexity;smoothing;teh;text corpus;world wide web	Deniz Yuret	2008			dirichlet distribution;computer science;machine learning;pattern recognition;data mining;low frequency;statistics;language model	NLP	-20.792989187352973	-75.01767318396894	195925
35640547b3ba7989b5abbb9d269055e736d9dff3	chinese part-of-speech tagging: one-at-a-time or all-at-once? word-based or character-based?	word segmentation;chinese word segmentation;part of speech tagging;part of speech;maximum entropy	Chinese part-of-speech (POS) tagging assigns one POS tag to each word in a Chinese sentence. However, since words are not demarcated in a Chinese sentence, Chinese POS tagging requires word segmentation as a prerequisite. We could perform Chinese POS tagging strictly after word segmentation (one-at-a-time approach), or perform both word segmentation and POS tagging in a combined, single step simultaneously (all-atonce approach). Also, we could choose to assign POS tags on a word-by-word basis, making use of word features in the surrounding context (word-based), or on a character-by-character basis with character features (character-based). This paper presents an in-depth study on such issues of processing architecture and feature representation for Chinese POS tagging, within a maximum entropy framework. We found that while the all-at-once, characterbased approach is the best, the one-at-a-time, character-based approach is a worthwhile compromise, performing only slightly worse in terms of accuracy, but taking shorter time to train and run. As part of our investigation, we also built a state-of-the-art Chinese word segmenter, which outperforms the best SIGHAN 2003 word segmenters in the closed track on 3 out of 4 test corpora.	brown corpus;chinese wall;encode;microsoft word for mac;part-of-speech tagging;text corpus;text segmentation;text-based (computing)	Hwee Tou Ng;Jin Kiat Low	2004			natural language processing;text segmentation;part of speech;computer science;principle of maximum entropy;linguistics;speech segmentation	NLP	-24.152610707910494	-77.32562809201966	196853
18178566965555152b399f316dd194fec3d6f0fb	syntactic parse fusion		Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains.	algorithmic efficiency;computation;discriminative model;early access;marginal model;parsing;structured prediction;text corpus;the wall street journal	Do Kook Choe;David McClosky;Eugene Charniak	2015			fusion;discriminative model;artificial intelligence;syntax;computer science;machine learning;parsing	NLP	-21.029158687022516	-75.9293981366783	198073
23af9581323a5cdb652311b6b47b78a4610b593b	comparing rnns and log-linear interpolation of improved skip-model on four babel languages: cantonese, pashto, tagalog, turkish	interpolation;smoothing methods;log linear interpolation turkish language tagalog language pashto language cantonese language babel language dirichlet smoothing kn smoothing technique kneser ney smoothing word level advanced language modeling technique recurrent neural network skip model rnn;smoothing methods context context modeling interpolation computational modeling recurrent neural networks standards;recurrent neural nets;under researched languages rnns log linear interpolation skip models smoothing;natural language processing;smoothing methods interpolation natural language processing recurrent neural nets	Recurrent neural networks (RNNs) are a very recent technique to model long range dependencies in natural languages. They have clearly outperformed trigrams and other more advanced language modeling techniques by using non-linearly modeling long range dependencies. An alternative is to use log-linear interpolation of skip models (i.e. skip bigrams and skip trigrams). The method as such has been published earlier. In this paper we investigate the impact of different smoothing techniques on the skip models as a measure of their overall performance. One option is to use automatically trained distance clusters (both hard and soft) to increase robustness and to combat sparseness in the skip model. We also investigate alternative smoothing techniques on word level. For skip bigrams when skipping a small number of words Kneser-Ney smoothing (KN) is advantageous. For a larger number of words being skipped Dirichlet smoothing performs better. In order to exploit the advantages of both KN and Dirichlet smoothing we propose a new unified smoothing technique. Experiments are performed on four Babel languages: Cantonese, Pashto, Tagalog and Turkish. RNNs and log-linearly interpolated skip models are on par if the skip models are trained with standard smoothing techniques. Using the improved smoothing of the skip models along with distance clusters, we can clearly outperform RNNs by about 8-11 % in perplexity across all four languages.	artificial neural network;bigram;dirichlet kernel;language model;linear interpolation;log-linear model;natural language;neural coding;perplexity;recurrent neural network;smoothing;trigram	Mittul Singh;Dietrich Klakow	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639307	speech recognition;interpolation;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	-20.990230359232523	-79.3774371665215	198079
3a722dbcf5842be1d71a08e24b236c33f3982878	adding more languages improves unsupervised multilingual part-of-speech tagging: a bayesian non-parametric approach	article	We investigate the problem of unsupervised part-of-speech tagging when raw parallel data is available in a large number of languages. Patterns of ambiguity vary greatly across languages and therefore even unannotated multilingual data can serve as a learning signal. We propose a non-parametric Bayesian model that connects related tagging decisions across languages through the use of multilingual latent variables. Our experiments show that performance improves steadily as the number of languages increases.	bayesian network;experiment;graceful exit;latent variable;parallel text;part-of-speech tagging;supervised learning;unsupervised learning	Benjamin Snyder;Tahira Naseem;Jacob Eisenstein;Regina Barzilay	2009			latent variable;natural language processing;speech recognition;computer science;machine learning;pattern recognition;bayesian inference;statistics	NLP	-19.91509731096057	-76.31614838749347	198100
57501cf1bb17f9cd7f707c3ebd80262a59b6c9a8	improved chart parsing applied kazakh graph analysis	grammar;active edge kazakh syntactic analysis chart analysis rule base;grammar optimization accuracy;accuracy;optimization;syntax tree projects chart parsing kazakh graph analysis kazakh syntactic analysis chart analysis method top down chart analysis bottom up analysis algorithms optimization strategy;graph theory graph grammars	To make faster and more complete Kazakh syntactic analysis, the improved algorithm analysis Chart analysis method is presented. First introduced the tradition of bottom-up and top-down chart analysis, focusing on bottom-up analysis algorithms applications statement and found that the algorithm increases the length of the sentence lower efficiency of the algorithm. For a long sentence Kazakh left recursive rules phenomenon more optimization strategy proposed rules, while adding active edge ways to optimize storage. By comparing the experimental results showed that: The combination of these two strategies to reduce the storage space makes the parsing while improving the speed, efficiency and time is eight times more efficient, reducing by 20% the number of active edges, unreasonable syntax tree projects has decreased.	algorithm;analysis of algorithms;bottom-up parsing;bottom-up proteomics;chart parser;knowledge base;left recursion;mathematical optimization;optimization mechanism;parse tree;parsing;prototype;text corpus;top-down and bottom-up design;treebank	Niu Na;Gulila Altenbek	2014	2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2014.7175777	natural language processing;speech recognition;computer science;grammar;accuracy and precision;algorithm	Robotics	-22.69934144764349	-78.68501936654187	198247
74cd94d0a95da059831717c7b6b680edd331d367	towards quantum language models		This paper presents a new approach for building Language Models using the Quantum Probability Theory, a Quantum Language Model (QLM). It mainly shows that relying on this probability calculus it is possible to build stochastic models able to benefit from quantum correlations due to interference and entanglement. We extensively tested our approach showing its superior performances, both in terms of model perplexity and inserting it into an automatic speech recognition evaluation setting, when compared with state-of-theart language modelling techniques.	interference (communication);language model;performance;perplexity;quantum entanglement;quantum probability;speech recognition;stochastic process	Ivano Basile;Fabio Tamburini	2017		10.18653/v1/d17-1196	computer science;natural language processing;artificial intelligence;quantum;language model	NLP	-20.72814571039037	-78.35870027737977	198253
8c85abab8febf68332da62da1b53689c8518ff88	identification and transformation of comparative sentences in patent chinese-english machine translation	libraries;semantics patents libraries information processing pragmatics educational institutions labeling;comparative sentence;pragmatics;language translation;semantics;text analysis;hierarchical network of concepts comparative sentence identification comparative sentence transformation patent chinese english machine translation patent texts statistical methods hnc theory;statistical analysis;patents;identification;information processing;text analysis language translation natural language processing patents statistical analysis;transformation;rule;natural language processing;labeling;machine translation;rule machine translation identification transformation comparative sentence	Machine translation for patent texts is a worldwide concern and more than 25% of the sentences in patent texts are comparative sentences. So the research on identification and transformation of comparative sentences in patent texts is very significant and can contribute to the patent Chinese-English machine translation. On the issue of identification of comparative sentences, there are some studies basically using statistical methods, but almost no study applies the rule-based method and no study uses the results of identification for Chinese-English machine translation. In this paper, based on HNC (Hierarchical Network of Concepts) theory, we applied the method based on rules to study the identification and transformation of comparative sentences in patent texts. Experiment shows us that the method can improve the performance of patent machine translation effectually.	information processing;logic programming;machine translation;point of view (computer hardware company);precision and recall;text corpus	Runxiang Zhang;Yaohong Jin	2012	2012 International Conference on Asian Language Processing	10.1109/IALP.2012.41	transformation;identification;natural language processing;labeling theory;text mining;speech recognition;example-based machine translation;information processing;computer science;semantics;linguistics;machine translation;rule-based machine translation;pragmatics	AI	-24.739794817771642	-78.17305057517225	198408
e3096cea2f97db46839ebaa39eaa58bf409ee43b	coupling an annotated corpus and a lexicon for state-of-the-art pos tagging	french;morphosyntactic lexicon;part of speech tagging;maximum entropy models;language resource development	This paper investigates how to best couple hand-annotated d ta with information extracted from an external lexical resource to improve POS t agging performance. Focusing mostly on French tagging, we introduce a Maximum Entropy Mar kov Model-based tagging system that is enriched with information extracted from a mo rph logical resource. This system gives a97.75% accuracy on the French Treebank, an error reduction of 25% (38% on unknown words) over the same tagger without lexical infor mation. We perform a series of experiments that help understanding how this lexical infor mation helps improving tagging accuracy. We also conduct experiments on datasets and lexic ons of varying sizes in order to assess the best trade-off between annotating data vs. dev eloping a lexicon. We find that the use of a lexicon improves the quality of the tagger at any s tage of development of either resource, and that for fixed performance levels the availabi lity of the full lexicon consistently reduces the need for supervised data by at least one half.	baseline (configuration management);brill tagger;experiment;lexicon;parsing;part-of-speech tagging;point of sale;principle of maximum entropy;treebank	Pascal Denis;Benoît Sagot	2012	Language Resources and Evaluation	10.1007/s10579-012-9193-0	natural language processing;speech recognition;french;computer science;linguistics	NLP	-22.849451794049482	-74.6755044796102	198551
0cf5e9c17d5ca6eb3e6dc4cb31c33385add677a8	unsupervised tokenization for machine translation	statistical machine translation;morphologically rich language;tokenization problem;optimal token boundary;statistical solution;bigger challenge;parallel corpus;machine translation;unsupervised tokenization;rule-based solution;good tokenization;rule based	Training a statistical machine translation starts with tokenizing a parallel corpus. Some languages such as Chinese do not incorporate spacing in their writing system, which creates a challenge for tokenization. Moreover, morphologically rich languages such as Korean present an even bigger challenge, since optimal token boundaries for machine translation in these languages are often unclear. Both rule-based solutions and statistical solutions are currently used. In this paper, we present unsupervised methods to solve tokenization problem. Our methods incorporate information available from parallel corpus to determine a good tokenization for machine translation.	bitext word alignment;data structure alignment;lexical analysis;logic programming;parallel text;statistical machine translation;synthetic intelligence;tokenization (data security);unsupervised learning	Tagyoung Chung;Daniel Gildea	2009			natural language processing;tokenization;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;machine learning;pattern recognition;machine translation;rule-based machine translation;machine translation software usability	NLP	-22.777564235499476	-78.24599382690172	199834
6555bde589a5b0a0a3bae53db8c09f4151914e56	a model of back-channel acknowledgements in spoken dialogue		Spoken dialogue systems would be more acceptable if they were able to produce backchannel continuers such as mm-hmm in naturalistic locations during the user's utterances. Using the HCRC Map Task Corpus as our data source, we describe models for predicting these locations using only limited processing and features of the user's speech that are commonly available, and which therefore could be used as a lowcost improvement for current systems. The baseline model inserts continuers after a predetermined number of words. One further model correlates back-channel continuers with pause duration, while a second predicts their occurrence using trigram POS frequencies. Combining these two models gives the best results.	backchannel;baseline (configuration management);dialog system;f1 score;kerrison predictor;language model;natural language;part-of-speech tagging;statistical parsing;test data;trigram	Nicola Cathcart;Jean Carletta;Ewan Klein	2003			computer science;natural language processing;artificial intelligence;communication channel	NLP	-20.070051884516683	-79.90708088216122	199863
617901fb86d2da4e34250fa5df59dc66acef90c4	deepgttm-i&ii: local boundary and metrical structure analyzer based on deep learning technique		This paper describes an analyzer for detecting local grouping boundaries and generating metrical structures of music pieces based on a generative theory of tonal music (GTTM). Although systems for automatically detecting local grouping boundaries and generating metrical structures, such as the full automatic time-span tree analyzer, have been proposed, musicologists have to correct the boundaries or strong beat positions due to numerous errors. In light of this, we use a deep learning technique for detecting local boundaries and generating metrical structures of music pieces based on a GTTM. Because we only have 300 pieces of music with the local grouping boundaries and metrical structures analyzed by musicologist, directly learning the relationship between the scores and metrical structures is difficult due to the lack of training data. To solve this problem, we propose a multi-task learning analyzer called deepGTM-Iu0026II based on the above deep learning technique to learn the relationship between scores and metrical structures in the following three steps. First, we conduct unsupervised pre-training of a network using 15,000 pieces of music in a non-labeled dataset. After pre-training, the network involves supervised fine-tuning by back propagation from output to input layers using a half-labeled dataset, which consists of 15,000 pieces of music labeled with an automatic analyzer that we previously constructed. Finally, the network involves supervised fine-tuning using a labeled dataset. The experimental results indicate that deepGTTM-Iu0026II outperformed previous analyzers for a GTTM in terms of the F-measure for generating metrical structures.	deep learning	Masatoshi Hamanaka;Keiji Hirata;Satoshi Tojo	2016		10.1007/978-3-319-67738-5_1	spectrum analyzer;machine learning;beat (music);speech recognition;deep learning;pure mathematics;training set;computer science;musicology;backpropagation;artificial intelligence;generative theory of tonal music	Robotics	-19.4651766798842	-79.51934274642436	199875
