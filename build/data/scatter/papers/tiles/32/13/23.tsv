id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
9c807e6cac67f7a5ca83deeeb611d362c9394a68	modeling data flow execution in a parallel environment		Although the modern data flows are executed in parallel and distributed environments, e.g. on a multi-core machine or on the cloud, current cost models, e.g., those considered by state-of-the-art data flow optimization techniques, do not accurately reflect the response time of real data flow execution in these execution environments. This is mainly due to the fact that the impact of parallelism, and more specifically, the impact of concurrent task execution on the running time is not adequately modeled. In this work, we propose a cost modeling solution that aims to accurately reflect the response time of a data flow that is executed in parallel. We focus on the single multi-core machine environment provided by modern business intelligence tools, such as Pentaho Kettle, but our approach can be extended to massively parallel and distributed settings. The distinctive features of our proposal is that we model both time overlaps and the impact of concurrency on task running times in a combined manner; the latter is appropriately quantified and its significance is exemplified.	approximation algorithm;benchmark (computing);concurrency (computer science);dataflow architecture;experiment;high- and low-level;ibm tivoli storage productivity center;mathematical optimization;multi-core processor;norm (social);parallel computing;response time (technology);scheduling (computing);simulation;the pot calling the kettle black;time complexity	Georgia Kougka;Anastasios Gounaris;Ulf Leser	2017		10.1007/978-3-319-64283-3_14	massively parallel;cloud computing;business intelligence;data modeling;flow (psychology);computer science;real-time computing;concurrency;response time;data flow diagram	DB	-21.20007782258562	57.52812320001198	179828
1e94a4a12f2848dc1f6dadb38dbfac4866550b0c	adaptive overload control for busy internet servers	complex dynamics;overload control;service differentiation;internet services;admission control	As Internet services become more popular and pervasive, a critical problem that arises is managing the performance of services under extreme overload. This paper presents a set of techniques for managing overload in complex, dynamic Internet services. These techniques are based on an adaptive admission control mechanism that attempts to bound the 90th-percentile response time of requests flowing through the service. This is accomplished by internally monitoring the performance of the service, which is decomposed into a set of event-driven stages connected with request queues. By controlling the rate at which each stage admits requests, the service can perform focused overload management, for example, by filtering only those requests that lead to resource bottlenecks. We present two extensions of this basic controller that provide class-based service differentiation as well as application-specific service degradation. We evaluate these mechanisms using a complex Webbased e-mail service that is subjected to a realistic user load, as well as a simpler Web server benchmark.	benchmark (computing);bottleneck (software);elegant degradation;email;event-driven programming;pervasive informatics;response time (technology);server (computing);web server benchmarking;web service	Matt Welsh;David E. Culler	2003			real-time computing;differentiated service;computer science;computer security;computer network	Networks	-24.64085084753439	58.16451359065265	180245
a95d8f1aa00e5544f471a079b282eb48b8da2478	a framework for scalable, parallel performance monitoring	performance;monitoring;tree-based;overlay;tau;mrnet	Performance monitoring of HPC applications offers opportunities for adaptive optimization based on dynamic performance behavior, unavailable in purely post-mortem performance views. However, a parallel performance monitoring system must have low overhead and high efficiency to make these opportunities tangible. We describe a scalable parallel performance monitor called TAUoverMRNet (ToM), created from the integration of the TAU performance system and the Multicast Reduction Network (MRNet). The integration is achieved through a plug-in architecture in TAU that allows selection of different transport substrates to offload online performance data. A method to establish the transport overlay structure of the monitor from within TAU, one that requires no added support from the job manager or application, is presented. We demonstrate the distribution of performance analysis from the sink to the overlay nodes and the reduction in large-scale profile data that could otherwise overwhelm any single sink. Results show low perturbation and significant savings accrued from reduction at large processor-counts.	adaptive optimization;debugging;fan-out;graphical user interface;ibm websphere extreme scale;load balancing (computing);mathematical optimization;multicast;overhead (computing);plug-in (computing);prototype;real-time transcription;scalability	Aroon Nataraj;Allen D. Malony;Alan Morris;Dorian C. Arnold;Barton P. Miller	2010	Concurrency and Computation: Practice and Experience	10.1002/cpe.1544	embedded system;adaptive optimization;parallel computing;real-time computing;simulation;performance;computer science;operating system;overlay;programming language	HPC	-19.391977022403484	56.921486179771115	180521
f9fcc97b16ccbbafbe1ce35ea7b95ceb85a1ec92	a neural network approach to forecasting computing-resource exhaustion with workload	workload;software;forecasting;computing resource usage;neural nets;neural networks computer networks aging application software degradation computer errors software systems predictive models software performance software quality;computing resource exhaustion;software maintenance;resource allocation;data collection;workload parameters;free memory;aging;lan;backpropagation;back propagation neural network;artificial neural networks;computational modeling;system recovery;workload parameters software aging neural network computing resource exhaustion;close relationships;workstations;used swap;failure rate;non parametric statistics;system recovery backpropagation neural nets resource allocation software maintenance;neurons;performance degradation;multilayer back propagation neural network;used swap computing resource exhaustion software aging failure rate performance degradation lan workload multilayer back propagation neural network computing resource usage free memory;neural network;software aging;time series model	Software aging refers to the phenomenon that applications will show growing failure rate or performance degradation after longtime execution. It is reported that this phenomenon usually has close relationship with computing-resource exhaustion. This paper analyzes computing-resource usage data collected on a LAN, and quantitatively investigates the relationship between computing-resource exhaustion trend and workload. First, we discuss the definition of workload, and then a Multi-Layer Back propagation neural network is trained to construct the nonlinear relationship between input (workload) and output (computing-resource usage). Then we use the trained neural network to forecast the computing-resource usage, i.e., free memory and used swap, with workload as its input. Finally, the results were benchmarked against those obtained without regard to influence of workload reported in the literatures, such as non-parametric statistical techniques or parametric time series models.	approximation;artificial neural network;backpropagation;computer engineering;elegant degradation;failure rate;hudson;network model;nonlinear system;paging;resource exhaustion attack;server (computing);software aging;software propagation;time series;usage data;web traffic;workstation	Ke-Xian Xue;Liang Su;Yun-Fei Jia;Kai-Yuan Cai	2009	2009 Ninth International Conference on Quality Software	10.1109/QSIC.2009.48	local area network;nonparametric statistics;real-time computing;simulation;workstation;forecasting;resource allocation;computer science;backpropagation;failure rate;time series;software aging;distributed computing;software maintenance;computational model;artificial neural network;data collection	HPC	-22.215928450321687	55.05986005456856	180739
652991f2bbf4b5133b0bea9a4b870ac4ec929e54	iotabench: an internet of things analytics benchmark	benchmarking;performance evaluation;internet of things;big data	"""The commoditization of sensors and communication networks is enabling vast quantities of data to be generated by and collected from cyber-physical systems. This ``Internet-of-Things"""" (IoT) makes possible new business opportunities, from usage-based insurance to proactive equipment maintenance. While many technology vendors now offer ``Big Data"""" solutions, a challenge for potential customers is understanding quantitatively how these solutions will work for IoT use cases. This paper describes a benchmark toolkit called IoTAbench for IoT Big Data scenarios. This toolset facilitates repeatable testing that can be easily extended to multiple IoT use cases, including a user's specific needs, interests or dataset. We demonstrate the benchmark via a smart metering use case involving an eight-node cluster running the HP Vertica analytics platform. The use case involves generating, loading, repairing and analyzing synthetic meter readings. The intent of IoTAbench is to provide the means to perform ``apples-to-apples"""" comparisons between different sensor data and analytics platforms. We illustrate the capabilities of IoTAbench via a large experimental study, where we store 22.8 trillion smart meter readings totaling 727 TB of data in our eight-node cluster."""	benchmark (computing);big data;cyber-physical system;experiment;internet of things;sensor;smart meter;synthetic intelligence;telecommunications network;terabyte	Martin F. Arlitt;Manish Marwah;Gowtham Bellala;Amip Shah;Jeff Healey;Ben Vandiver	2015		10.1145/2668930.2688055	big data;computer science;engineering;data science;data mining;world wide web;internet of things;benchmarking	Robotics	-24.350617714926546	56.543170871136326	180846
cfb3c5fabef7a602cb5c8a02b5b516d38e3fe991	improving scalability of software cloud for composite web services	software;composite web service;software cloud cloud computing communication patterns composite services scalability service assignment and deployment service based applications;communication patterns software cloud composite web services resource assignment service based applications;software cloud;scalability web services application software cloud computing pattern analysis computer science management information systems information management access control educational institutions;service based applications;service operation;composite services;servers;resource assignment;web services;composite web services;bandwidth;scalability;productivity;service assignment and deployment;meteorology;communication pattern;communication patterns;cloud computing	Most of the work on cloud scalability has focused on the granularity of applications and systems deployed on the cloud and on how to adjust their resource assignment according to the scale or volume of the requests. In this paper, we present a scheme for improving the scalability of service-based applications in a cloud from the granularity of the constituent services and their individual placement in the cloud. The approach we take is to analyze the communication patterns among the service operations of service-based applications and to analyze the assignment of the involved services to the available servers. We define a notion of scalability for service-based applications in a cloud and a framework to measure the scalability. We then propose an optimized assignment strategy to improve the scalability of composite Web services in terms of the productivity of such services. We report preliminary simulation experimental results that show the effectiveness of our scheme.	cloud computing;experiment;image scaling;scalability;server (computing);simulation;web service	Jian Wu;Qianhui Althea Liang;Elisa Bertino	2009	2009 IEEE International Conference on Cloud Computing	10.1109/CLOUD.2009.75	web service;productivity;scalability;cloud computing;computer science;operating system;database;distributed computing;scalability testing;world wide web;bandwidth;server	HPC	-26.180025900418315	58.275109779642044	181400
b9ebd4c9389c55373794491379ac6aa4dd837fa3	load balancing in xen virtual machine monitor	virtual machine;virtual machine monitor;multiprocessor systems;earliest deadline first;low latency;interactive application;load balance;algorithm design	Global load balancing across all the available physical pro- cessors is an important characteristic of a virtual machine scheduler. Xen's Simple Earliest Deadline First Scheduler (SEDF) serves the pur- pose for interactive applications and low latency applications. SEDF scheduler can not be used in multiprocessor environments due to un- availability of load balancing. This paper investigates requirement of this feature and discusses algorithmic design and implementation of an user space load balancing program. Experiment results show a balance among number of physical processors with better utilization of resources in mul- tiprocessor systems.	hypervisor;load balancing (computing);virtual machine	Gaurav Somani;Sanjay Chaudhary	2010		10.1007/978-3-642-14825-5_6	algorithm design;network load balancing services;parallel computing;real-time computing;earliest deadline first scheduling;computer science;virtual machine;load balancing;operating system;hypervisor;low latency	OS	-19.750224560475584	59.39212065854659	181449
304265f68a77c2f60b3b29a8234e74624f02a24d	a hybrid distributed architecture for indexing	search engine;info eu repo semantics conferenceobject πρακτικά συνeδρίου;digital library;hardware architecture;large scale;indexation;cost effectiveness;distributed architecture	This paper presents a hybrid scavenger grid as an underlying hardware architecture for search services within digital libraries. The hybrid scavenger grid consists of both dedicated servers and dynamic resources in the form of idle workstations to handle mediumto large-scale search engine workloads. The dedicated resources are expected to have reliable and predictable behaviour. The dynamic resources are used opportunistically without any guarantees of availability. Test results confirmed that indexing performance is directly related to the size of the hybrid grid and intranet networking does not play a major role. A systemefficiency and cost-effectiveness comparison of a grid and a multiprocessor machine showed that for workloads of modest to large sizes, the grid architecture delivers better throughput per unit cost than the multiprocessor, at a system efficiency that is comparable to that of the multiprocessor.	best practice;dedicated hosting service;digital library;distributed computing;grid computing;intranet;library (computing);multiprocessing;scalability;terabyte;throughput;web search engine;workstation	Ndapandula Nakashole;Hussein Suleman	2009		10.1007/978-3-642-04346-8_25	embedded system;real-time computing;digital library;cost-effectiveness analysis;computer science;operating system;hardware architecture;world wide web;search engine	HPC	-20.253042113449407	58.046261661992496	181762
bb292a69117d0f797f9e7c818ed9b06ce41cf953	high-responsive scheduling with mapreduce performance prediction on hadoop yarn	yarn;software performance evaluation big data data analysis parallel processing public domain software resource allocation scheduling;processor scheduling;job size prediction mechanism hadoop yarn scheduling mapreduce performance prediction open source big data analysis platform resource management fair sojourn protocol fspy;resource management;time factors;yarn containers time factors resource management programming processor scheduling;programming;containers;job size prediction yarn scheduling responsiveness fairness	Hadoop is an open-source big data analysis platform that is widely used in both academia and industry. Decoupling of resource management and programming framework, the next generation of Hadoop, namely Hadoop YARN, is accommodated to various programming frameworks and capable of handling more kinds of workload, such as interactive analysis and stream processing. However, most existent schedulers in YARN are designed for batch processing and they do not value per-job response time, which results in low responsiveness of the Hadoop platform. This paper proposes a FSPY (Fair Sojourn Protocol in YARN) scheduler to improve responsiveness with guaranteeing fairness. FSPY relies on job sizes which are unknown a priori. Consequently, we also present a job size prediction mechanism for MapReduce. Experimental results show that our scheduler outperforms Fair scheduler by 10x with respect to responsiveness under heavy workloads. Meanwhile, our prediction mechanism reaches an R2 prediction accuracy of 0.97.	algorithm;apache hadoop;batch processing;big data;coupling (computer programming);emoticon;fairness measure;job stream;mapreduce;next-generation network;open-source software;performance prediction;response time (technology);responsiveness;spark;scheduling (computing);stream processing	Yang Liu;Yukun Zeng;Xuefeng Piao	2016	2016 IEEE 22nd International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA)	10.1109/RTCSA.2016.51	embedded system;programming;parallel computing;real-time computing;computer science;resource management;operating system;distributed computing	HPC	-20.244179111855978	60.021432278608216	181843
9246ebb2c6c2004cdbb2df420adc892672586344	load balancing voice applications with piranha.	mobile device;fault tolerant;load balance;distributed algorithm	In this paper, we investigate the load balancing problem among a cluster of mobile and fixed devices in a voice enabled interface. We consider a design approach. The voice interface has to support up to a hundred simultaneous users. The load balancing criteria we consider are defined, on the one hand in terms of network, CPU and memory resources, and on the other hand in terms of the boundary between fixed and mobile devices. The solution we propose is based on a derivate of the Linux Virtual Server: the Piranha system. Piranha enhances the Linux Virtual Server with several features, in particular with monitoring aspects and fault tolerance. We describe precisely the proposed architecture of the application. We present a new scheduling technique which has been designed to take into account dynamically resource loads	central processing unit;fault tolerance;linux virtual server;load balancing (computing);mobile device;scheduling (computing)	Mustapha Hadim;Pierre Manneback;Michel Bagein;Pierre Maon	2003			embedded system;distributed algorithm;fault tolerance;real-time computing;computer science;load balancing;mobile device;distributed computing	Embedded	-19.809881809043514	59.26488718759869	181983
6cd58cb922f9d210905af441741a24d8ee6b61ca	a survey on optimal utilization of preemptible vm instances in cloud computing		In this article, an extensive survey on optimal utilization of preemptible instances in cloud is presented. Different techniques used in state-of-the-art research for efficient utilization of spot instances have been classified and categorized in the paper. To the best of our knowledge, this is the first attempt of its kind. With the continuing growths in cloud computing, researchers and business personnels are exploiting the services provided by the cloud computing to reduce their operational cost. Users can share resources in the cloud with the help of virtualization. Virtualization provides abstraction of cloud to the users by hiding the complexity of inherent software and hardware present in the cloud. It increases the likelihood of running multiple operating systems (OSs) on a single physical machine with sharing of hardware resources. Each OS can be considered as a virtual machine (VM) installed on a physical machine. Based on subscription model, VMs can be classified into three types: reserved VMs, on-demand VMs and spot VMs. Spot instances are also known as preemptible VM instances. Spot instances are used as reduced cost resources at the risk of reliability. To utilize spot instances, users have to bid for them. Users will able to get the spot instances only if the biding price is greater than the spot instance price. As soon as the bid price becomes less than the spot price, the cloud provider will revoke the VMs (SIs). This survey aims to find the ways, one can efficiently utilize spot instances for executing the tasks with optimized cost and time.	application checkpointing;categorization;cloud computing;fault tolerance;iterative method;mapreduce;openvms;operating system;overhead (computing);preemption (computing);q-learning;real-time clock;reduced cost;representational state transfer;run time (program lifecycle phase);system migration;virtual machine;web service	Ashish Kumar Mishra;Brajesh Kumar Umrao;Dharmendra K. Yadav	2018	The Journal of Supercomputing	10.1007/s11227-018-2509-0	computer science;virtualization;distributed computing;reduced cost;cloud computing;software;fault tolerance;bid price;spot contract;virtual machine	HPC	-24.383478336544044	60.256687398700684	182463
317ff029697a2436ac9bec5b162af668e566aafd	[journal first] empirical study on the discrepancy between performance testing results from virtual and physical environments		Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving field requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide flexible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O traffic) to the testing environment and lead to unrealistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities.  To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a case study on two open source systems - namely Dell DVD Store (DS2) and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) single performance metric (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) A single metric from virtual and physical environments do not follow the same distribution, hence practitioners cannot simply use a scaling factor to compare the performance between environments, 2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance metrics from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we investigate ways to reduce the discrepancy. We find that such discrepancy can be reduced by normalizing performance metrics based on deviance. Overall, we suggest that practitioners should not use the performance testing results from virtual environment with the simple assumption of straightforward performance overhead. Instead, practitioners should consider leveraging normalization techniques to reduce the discrepancy before examining performance testing results from virtual and physical environments.	central processing unit;cloudstore;control theory;database normalization;discrepancy function;image scaling;input/output;open-source software;overhead (computing);software performance testing;software system;statistical model;virtual machine;virtual reality	Muhammad Moiz Arif;Weiyi Shang;Emad Shihab	2018	2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE)	10.1145/3180155.3182527	software system;empirical research;real-time computing;cpu time;computer science;normalization (statistics);virtual machine;performance metric;statistical model;software performance testing	HPC	-22.793061800694773	57.10230876117022	182718
f4d64266de8bf47b1b45c26bdfb40a515a8fee74	exploring dynamic load imbalance solutions with the comd proxy application		Abstract Proxy applications are developed to simplify studying parallel performance of scientific simulations and to test potential solutions for performance problems. However, proxy applications are typically too simple to allow work migration or to represent the load imbalance of their parent applications. To study the ability of load balancing solutions to balance work effectively, we enable work migration in one of the Exascale Co-design Center for Materials in Extreme Environments (ExMatEx) [ 1 ] applications, CoMD. We design a methodology to parameterize three key aspects necessary for studying load imbalance correction: (1) the granularity with which work can be migrated; (2) the initial load imbalance; (3) the dynamic load imbalance (how quickly the load changes over time). We present a study of the impact of flexibility in work migration in CoMD on load balance and the associated rebalancing costs for a wide range of initial and dynamic load imbalance scenarios.		Olga Pearce;Hadia Ahmed;Rasmus W. Larsen;Peter Pirkelbauer;David F. Richards	2019	Future Generation Comp. Syst.	10.1016/j.future.2017.12.010	initial load;granularity;distributed computing;computer science;dynamic load testing;proxy (climate);load balancing (computing)	Arch	-20.10477416891036	58.44200951659387	182814
b0fefa99c5955e10abc6dbd78ae44ada06c8e862	towards a performance characterization of a parallel file system over virtualized environments	computers;virtualization;virtualisation cloud computing disc storage file organisation parallel processing resource allocation;performance evaluation;cloud computing throughput virtualization performance evaluation computers program processors;i o node performance characterization virtualized environment performance impact parallel file system environment pfs environment computing platform system performance write throughput disk contention performance loss computing node microsoft azure cloud infrastructure balanced resource sharing network contention;program processors;cloud computing;throughput	This research work investigates the performance impact of virtualizing parallel file systems' (PFS) environments. Through an extensive experimental analysis, using three distinct computing platforms and considering 11 factors with relevant impact on systems' performance, we demonstrate how PFSs' write throughput is affected by network and disk contentions on virtualized environments. Results indicate that on extreme cases, in which the whole PFS environment executes over a single host, performance losses of up to 93% are observed when the number of I/O and computing nodes increases. On a PFS's environment deployed over Microsoft Azure cloud infrastructure, where it is expected a more balanced resource sharing, network contention proved more critical causing performance losses of up to 80% with the increase of the number of I/O nodes. Disk contention in this environment, however, was not an issue, behaving more like an environment deployed over a dedicated cluster.	cloud computing;clustered file system;forward secrecy;input/output;microsoft azure;throughput	Eduardo Camilo Inacio;Mario A. R. Dantas;Douglas Dyllon Jeronimo de Macedo	2015	2015 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2015.7405579	throughput;parallel computing;virtualization;cloud computing;computer science;operating system;distributed computing;computer network	HPC	-21.67292575144937	59.172672634487974	183950
f16990dbf6235fb9336fe89101f9df487f230b4e	model-based performance analysis using block coverage measurements	software testing;performance estimation;code coverage;network routing;model based performance analysis;test coverage;performance analysis;performance model;profile data	The primary advantage of model-based performance analysis is its ability to facilitate sensitivity and predictive analysis, in addition to providing an estimate of the application performance. To conduct model-based analysis, it is necessary to build a performance model of an application which represents the application structure in terms of the interactions among its components, using an appropriate modeling paradigm. While several research efforts have been devoted to the development of the theoretical aspects of model-based analysis, its practical applicability has been limited despite the advantages it offers. This limited practical applicability is due to the lack of techniques available to estimate the parameters of the performance model of the application. Since the model parameters cannot be estimated in a realistic manner, the results obtained from model-based analysis may not be accurate. In this paper, we present an empirical approach in which profile data in the form of block coverage measurements is used to parameterize the performance model of an application. We illustrate the approach using a network routing simulator called Maryland routing simulator (MaRS). Validation of the performance estimate of MaRS obtained from the performance model parameterized using our approach demonstrates the viability of our approach. We then illustrate how the model could be used for predictive performance analysis using two scenarios. By the virtue of using code coverage measurements to parameterize a performance model, we integrate two mature, yet independent research areas, namely, software testing and model-based performance analysis.	profiling (computer programming)	Swapna S. Gokhale	2009	Journal of Systems and Software	10.1016/j.jss.2008.06.051	simulation;computer science;engineering;software engineering;data mining;code coverage;operations research;statistics	OS	-22.20746720289924	56.390432028735354	184105
ce89d24d68c0d919b1ca641e8195b359e431823a	performance evaluation of commodity iscsi-based storage systems	storage system;peripheral interfaces storage management performance evaluation disc storage local area networks operating system kernels linux storage area networks;performance evaluation;peripheral interfaces;next generation network;perforation;disc storage;storage management;application server;null;system performance;storage area networks;gigabit ethernet;linux;operating system kernels;performance evaluation commodity iscsi based future storage systems system i o behavior iscsi initiator directly attached storage commodity pc disks storage nodes gigabit ethernet network storage network application server linux kernel network based i o architecture buffer cache network bandwidth;local area networks;buildings network servers kernel system performance personal communication networks ethernet networks instruments linux performance analysis next generation networking	iSCSI is proposed as a possible solution to building future storage systems. However, using iSCSI raises numerous questions about its implications on system performance. This lack of understanding of system I/O behavior in modern and future systems inhibits providing solutions at the architectural and system levels. Our main goals in this work are to understand the behavior of the application server (iSCSI initiator), to evaluate the overhead introduced by iSCSI compared to systems with directly-attached storage, and to provide insight about how future storage systems may be improved. We examine these questions in the context of commodity iSCSI systems that can benefit most from using iSCSI. We use commodity PCs with several disks as storage nodes and a Gigabit Ethernet network as the storage network. On the application server side we use a broad range of benchmarks and applications to evaluate the impact of iSCSI on application and server performance. We instrument the Linux kernel to provide detailed information about I/O activity and the various overheads of kernel I/O layers. Our analysis reveals how iSCSI affects application performance and shows that building next generation, network-based I/O architectures, requires optimizing I/O latency, reducing network and buffer cache related processing in the host CPU, and increasing the sheer network bandwidth to account for consolidation of different types of traffic.	application server;benchmark (computing);central processing unit;client–server model;clustered file system;gigabit;ibm system i;ibm tivoli storage productivity center;iscsi;input/output;linux;mike lesser;next-generation network;overhead (computing);page cache;performance evaluation;personal computer;real life;reduced cost;scsi initiator and target;semiconductor consolidation;server (computing);server-side;simple features;spec#;storage area network;throughput	Dimitrios Xinidis;Angelos Bilas;Michail Flouris	2005	22nd IEEE / 13th NASA Goddard Conference on Mass Storage Systems and Technologies (MSST'05)	10.1109/MSST.2005.23	hyperscsi;embedded system;real-time computing;engineering;operating system	HPC	-23.416861636861107	57.26659532140351	184449
f24fb91815e9015b0d824844558125d3af639955	implementing a resilient application architecture for state management on a paas cloud	application architecture;fault tolerance cloud computing paas session state management application architecture;session state management;java heart beat context computer architecture servers time factors databases;resilient user state management resilient application architecture state management paas cloud platform as a service application state management clustering techniques platform configuration session state management architecture reloc architecture loosely coupled services platform agnostic scalable messaging technology session states propagation session states savings heroku application instances;fault tolerance;paas;cloud computing	Platform as a Service Clouds typically lack direct support for application state management, and traditional state management techniques like clustering are not applicable as PaaS platforms offer little support for changing the underlying platform configuration. In this paper we build upon our earlier work where we proposed a session-state management architecture for Cloud called ReLoC, that uses loosely-coupled services and platform agnostic scalable messaging technology to propagate and save session states. Here, we present an actual implementation of the ReLoC onto a PaaS platform and an empirical evaluation of the original hypotheses of scalability and resilience of the proposed application architecture. We also present the challenges faced in implementing ReLoC on Heroku. The results indicate that ReLoC indeed allows applications to scale well and mitigates failures in individual application instances while maintaining state and hiding such failures from the users. The results also indicate that the performance degradation due to use of ReLoC is minimal and it is thus a promising approach for resilient user state management on PaaS Clouds.	asp.net;applications architecture;cluster analysis;elegant degradation;loose coupling;message queue;platform as a service;scalability;server (computing);session (computer science);software propagation;state (computer science);state management	Vibhu Saujanya Sharma;Aravindan Santharam	2013	2013 IEEE 5th International Conference on Cloud Computing Technology and Science	10.1109/CloudCom.2013.26	fault tolerance;real-time computing;cloud computing;computer science;applications architecture;operating system;distributed computing	DB	-25.197863124069272	54.79809666703111	186337
1b52cfa93f64071c1a6ac2dee3fed6ed9e144679	efficient execution plans for distributed skyline query processing	distributed system;execution plan;query processing;distributed processing;in network processing;skyline query;distributed environment;distributed systems	In this paper, we study the generation of efficient execution plans for skyline query processing in large-scale distributed environments. In such a setting, each server stores autonomously a fraction of the data, thus all servers need to process the skyline query. An execution plan defines the order in which the individual skyline queries are processed on different servers, and influences the performance of query processing. Querying servers consecutively reduces the amount of transferred data and the number of queried servers, since skyline points obtained by one server prune points in the subsequent servers, but also increases the latency of the system. To address this trade-off, we introduce a novel framework, called SkyPlan, for processing distributed skyline queries that generates execution plans aiming at optimizing the performance of query processing. Thus, we quantify the gain of querying consecutively different servers. Then, execution plans are generated that maximize the overall gain, while also taking into account additional objectives, such as bounding the maximum number of hops required for the query or balancing the load on different servers fairly. Finally, we present an algorithm for distributed processing based on the generated plan that continuously refines the execution plan during in-network processing. Our framework consistently outperforms the state-of-the-art algorithm.	algorithm;computation;database;distributed computing;map;network processor;parallel computing;pareto efficiency;query optimization;query plan;server (computing)	João B. Rocha-Junior;Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg	2011		10.1145/1951365.1951399	query optimization;real-time computing;computer science;database;distributed computing;distributed computing environment	DB	-19.23043150114255	55.998635957163714	188123
627d5587dd1dcd0a0642d6765f57792da89e8e01	research and implementation on middleware of database workload autonomic adaptation	databases;software;database system;middleware databases time factors software database systems monitoring servers;database management systems;system performance;servers;time factors;monitoring;schedule middleware database workload autonomic adaptation;database systems;database workload;schedule;network system environment;autonomic adaptation;middleware;middleware database management systems;networked systems;autonomic adaptation middleware database workload network system environment	The workload of the database system is unpredictable and fluctuated rapidly in the network system environment. The load to accept the request and response it is gradually increasing for the database system. This paper proposes and implements a kind of middleware prototype used to adapt the workload of the database system autonomically. The middleware schedules and controls the workload based on the current system performance. It achieves the purpose that it enhances the stability of database system and the total performance of application system. The experiment testified the validity of this middleware.	algorithm;autonomic computing;database;environment variable;middleware;multi-user;network operating system;prototype;scheduling (computing)	Fu Duan;Yuxing Wang;Chanchan Zhao;Yueqin Zhang	2008	2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2008.225	middleware;real-time computing;database tuning;computer science;operating system;middleware;database;computer performance;schedule;server	DB	-20.521906043639188	57.185207342256966	188295
b0b32a0102e184ae42627d13e184896c5cabe559	mobile agent systems integration into parallel environments		In this work MASIPE, a tool for monitoring parallel applications, is presented. MASIPE is a distributed tool that gives support to user-defined mobile agents, including functionalities for creating and transferring these agents through different compute nodes. In each node, the mobile agent can access the node information as well as the memory space of the parallel program that is being monitored. In addition, MASIPE includes functionalities for managing and graphically displaying the agent data. In this work, its internal structure is detailed and an example of a monitored scientific application is shown. We also perform a study of the MASIPE requirements (in terms of CPU and memory) and we evaluate its overhead during the program execution. Experimental results show that MASIPE can be efficiently used with minimum impact on the program performance.	central processing unit;dspace;mobile agent;overhead (computing);requirement;system integration	David E. Singh;Alejandro Miguel;Félix García;Jesús Carretero	2008	Scalable Computing: Practice and Experience		embedded system;real-time computing;computer science;distributed computing	HPC	-20.294092625268508	53.762335116694956	189034
9db757fa4670b1348c00304948bb817b57c54bfb	durango: scalable synthetic workload generation for extreme-scale application performance modeling and simulation		Performance modeling of extreme-scale applications on accurate representations of potential architectures is critical for designing next generation supercomputing systems because it is impractical to construct prototype systems at scale with new network hardware in order to explore designs and policies. However, these simulations often rely on static application traces that can be difficult to work with because of their size and lack of flexibility to extend or scale up without rerunning the original application. To address this problem, we have created a new technique for generating scalable, flexible workloads from real applications, we have implemented a prototype, called Durango, that combines a proven analytical performance modeling language, Aspen, with the massively parallel HPC network modeling capabilities of the CODES framework.  Our models are compact, parameterized and representative of real applications with computation events. They are not resource intensive to create and are portable across simulator environments. We demonstrate the utility of Durango by simulating the LULESH application in the CODES simulation environment on several topologies and show that Durango is practical to use for simulation without loss of fidelity, as quantified by simulation metrics. During our validation of Durango's generated communication model of LULESH, we found that the original LULESH miniapp code had a latent bug where the MPI_Waitall operation was used incorrectly. This finding underscores the potential need for a tool such as Durango, beyond its benefits for flexible workload generation and modeling.  Additionally, we demonstrate the efficacy of Durango's direct integration approach, which links Aspen into CODES as part of the running network simulation model. Here, Aspen generates the application-level computation timing events, which in turn drive the start of a network communication phase. Results show that Durango's performance scales well when executing both torus and dragonfly network models on up to 4K Blue Gene/Q nodes using 32K MPI ranks, Durango also avoids the overheads and complexities associated with extreme-scale trace files.	analytical performance modeling;blue gene;computation;ibm websphere extreme scale;modeling language;networking hardware;performance prediction;prototype;scalability;simulation;software portability;supercomputer;synthetic intelligence;tracing (software)	Christopher D. Carothers;Jeremy S. Meredith;Mark P. Blanco;Jeffrey S. Vetter;Misbah Mubarak;Justin M. LaPre;Shirley Moore	2017		10.1145/3064911.3064923	real-time computing;simulation;engineering;computer graphics (images)	HPC	-23.917999605458096	56.95103175410954	189692
96c66a3d6884f027bb820398b5af6e0542502866	orion: online resource negotiator for multiple big data analytics frameworks		In recent years we observe the rapid growth of large-scale analytics applications in a wide range of domains – from healthcare infrastructures to traffic management. The high volume of data that need to be processed has stimulated the development of special purpose frameworks which handle the data deluge by parallelizing data processing and concurrently using multiple computing nodes. These frameworks differentiate significantly in terms of the policies they follow to decompose their workloads into multiple tasks and also on the way they exploit the available computing resources. As a result, based on the framework that applications have been implemented in, we observe significant variations in their resource utilization and execution times. Therefore, determining the appropriate framework for executing a big data application is not trivial. In this work we propose Orion, a novel resource negotiator for cloud infrastructures that support multiple big data frameworks such as Apache Spark, Apache Flink and TensorFlow. More specifically, given an application, Orion determines the most appropriate framework to assign it to. Additionally, Orion reserves the required resources so that the application is able to meet its performance requirements. Our negotiator exploits state-of-the-art prediction techniques for estimating the application's execution time when it is assigned to a specific framework with varying configuration parameters and processing resources. Finally, our detailed experimental evaluation, using practical big data workloads on our local cluster, illustrates that our approach outperforms its competitors.	apache flink;apache spark;big data;decision tree;information explosion;norm (social);parallel computing;requirement;run time (program lifecycle phase);scheduling (computing);traverse;tensorflow	Nikos Zacheilas;Nikolaos Chalvantzis;Ioannis Konstantinou;Vana Kalogeraki;Nectarios Koziris	2018	2018 IEEE International Conference on Autonomic Computing (ICAC)	10.1109/ICAC.2018.00011	task analysis;competitor analysis;big data;cloud computing;negotiation;distributed computing;exploit;computer science;analytics;data processing	DB	-21.34921259420345	58.41200219358355	189721
fe4a446e492bbf330ea02ee56643fc46f7eac177	optimization of network bandwidth allocation in xen	virtualization;bandwidth allocation;virtual machining;resource management;bandwidth channel allocation resource management virtual machining virtualization linux servers;servers;xen;bandwidth;linux;channel allocation;bandwidth allocation xen	Through virtualization, a plurality of servers can be simulated on a single physical machine. The sharing of hardware resources of one physical machine by multiple virtual servers greatly improves resource utilization. The key to virtualization is how to handle the allocation and management of hardware resources among competing virtual machines. One type of hardware resource that has not been fully studied in this regard is network bandwidth, which, if dealt poorly, can have significant negative impact on the performance of a server cluster. This paper analyzes the bandwidth allocation strategy of Xen, an open source hypervisor. We find that Xen uses a static network allocation approach that is based on the parameters specified in a configuration file. Such approach greatly limits the utilization of network resource. To overcome this drawback, this paper presents a dynamic bandwidth allocation strategy based on the demand and priority of each virtual machine. Our evaluation demonstrates that the proposed solution makes much better use of network resources with relatively low overhead.	computer cluster;hypervisor;open-source software;overhead (computing);server (computing);virtual machine	LiRong Mei	2015	2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems	10.1109/HPCC-CSS-ICESS.2015.35	full virtualization;parallel computing;real-time computing;virtualization;computer science;virtual machine;resource management;operating system;linux kernel;bandwidth;server;dynamic bandwidth allocation;computer network;bandwidth allocation	Arch	-21.874292505172292	60.2144130666917	190427
74911278247239eef9e591575822ab2fbd78ad8c	dcsim: a data centre simulation tool for evaluating dynamic virtualized resource management	virtualization;resource allocation;data centre;resource management;cloud;resource management servers data models load modeling heuristic algorithms computational modeling;simulator;computer centres;servers;computational modeling;virtual machines;infrastructure as a service;heuristic algorithms;virtualisation cloud computing computer centres resource allocation virtual machines;infrastructure as a service cloud data centre simulator virtualization;infrastructure as a service cloud dcsim data centre simulation tool dynamic virtualized resource management virtualization virtual machines server utilization vm reallocation dynamic resource provisioning;load modeling;virtualisation;cloud computing;data models	Computing today is shifting from hosting services in servers owned by individual organizations to data centres providing resources to a number of organizations on a shared infrastructure. Managing such a data centre presents a unique set of goals and challenges. Through the use of virtualization, multiple users can run isolated virtual machines (VMs) on a single physical host, allowing for a higher server utilization. By consolidating VMs onto fewer physical hosts, infrastructure costs can be reduced in terms of the number of servers required, power consumption, and maintenance. To meet constantly changing workload levels, running VMs may need to be migrated (moved) to another physical host. Algorithms to perform dynamic VM reallocation, as well as dynamic resource provisioning on a single host, are open research problems. Experimenting with such algorithms on the data centre scale is impractical. Thus, there is a need for simulation tools to allow rapid development and evaluation of data centre management techniques. We present DCSim, an extensible simulation framework for simulating a data centre hosting an Infrastructure as a Service cloud. We evaluate the scalability of DCSim, and demonstrate its usefulness in evaluating VM management techniques.	algorithm;central processing unit;cloud computing;computer data storage;data center;experiment;fault tolerance;http 404;image scaling;interaction;multi-user;multitier architecture;open research;provisioning;scalability;scheduling (computing);server (computing);simulation;virtual machine	Michael Tighe;Gastón Keller;Michael Bauer;Hanan Lutfiyya	2012	2012 8th international conference on network and service management (cnsm) and 2012 workshop on systems virtualiztion management (svm)		real-time computing;cloud computing;computer science;resource management;operating system;distributed computing;computer network	DB	-21.092087898828034	59.37171266439158	190594
dabe322ca89b8cc28bbea3b274bca30e2ca57810	end-to-end cache system for grid computing: design and efficiency analysis of a high-throughput bioinformatic docking application	transfer operator;protein protein docking;scheduler;high throughput application;efficiency analysis;task assignment;cache system;high throughput;grid computing;grid system;high throughput computing	Cache techniques are an efficient tool to reduce latency times in transfer operations through Grid systems. Although different approximations to introduce cache facilities into Grid computing have already been studied, they require intrusive modifications of Grid software and hardware. Here, we propose an end-to-end cache system that is implemented over scheduling services. This cache system requires neither changes in the Grid software nor introduction of new software in the Grid resources. Parallel Grid adaptation of many high-throughput computing applications that use the same data intensively could enjoy great benefits from our cache system. The maintenance of cacheable data in the resources of already-executed tasks allows faster executions of future tasks assigned to the same resources. To analyze the performance of our endto-end cache system, we tested it with a new protein–protein docking application. The obtained results confirm our cache system’s robustness and efficiency gain for this kind of high-throughput application.	approximation;bioinformatics;cpu cache;docking (molecular);end-to-end principle;grid computing;high-throughput computing;macromolecular docking;scheduling (computing);throughput	José Ignacio Garzón;Eduardo Huedo;Rubén S. Montero;Ignacio Martín Llorente;Pablo Chacón	2010	IJHPCA	10.1177/1094342009350469	high-throughput screening;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cache;computer science;cache invalidation;operating system;smart cache;cache algorithms;cache pollution;grid computing	HPC	-19.6497218787114	58.90398506808477	190705
c5b8dec5e7178dff4c87672611d66c26cc655c2d	enhanced inferencing: estimation of a workload dependent performance model	software system;modern software system;enhanced inferencing;performance modeling;quantitative performance difference;capacity planning;powerful inferencing technique;workload dependent performance model;inferencing technique;performance model;robust performance prediction;queueing theory;estimation	Performance modeling of software systems is vital for predictive analysis of their performance and capacity planning of the host environment. Robust performance prediction and efficient capacity planning highly depend on an accurate estimation of the underlying model parameters. AMBIENCE, which is a prototype tool developed at IBM Research, makes use of the powerful Inferencing technique to generate a workload-independent parameters based performance model. However, modern software systems are quite complex in design and may exhibit variable service times and overheads at changing workloads. In this work, we extend the Inferencing technique for generating workload-dependent service time and CPU overhead based performance models. We call this extended form as Enhanced Inferencing. Implementation of Enhanced Inferencing in AMBIENCE shows significant improvement of the order of 26 times over Inferencing. We further present a case study where Enhanced Inferencing provides a quantitative performance difference between consolidated and partitioned software system installations. Ability to carry out such evaluations can have significant impact on capacity planning of software systems that are characterized by workload-dependent model parameters.		Dinesh Kumar;Li Zhang;Asser N. Tantawi	2009			embedded system;estimation;real-time computing;simulation;computer science;mathematics;queueing theory;statistics	SE	-22.098860941045913	56.34956625648093	191585
a75f47cf1f047fdc823abb7c77e36c7f88fc2729	dress: dynamic resource-reservation scheme for congested data-intensive computing platforms		In the past few years, we have envisioned an increasing number of businesses start driving by big data analytics, such as Amazon recommendations and Google Advertisements. At the back-end side, the businesses are powered by big data processing platforms to quickly extract information and make decisions. Running on top of a computing cluster, those platforms utilize scheduling algorithms to allocate resources. An efficient scheduler is crucial to the system performance due to limited resources, e.g. CPU and Memory, and a large number of user demands. However, besides requests from clients and current status of the system, it has limited knowledge about execution length of the running jobs, and incoming jobs' resource demands, which make assigning resources a challenging task. If most of the resources are occupied by a long-running job, other jobs will have to keep waiting until it releases them. This paper presents a new scheduling strategy, named DRESS that particularly aims to optimize the allocation among jobs with various demands. Specifically, it classifies the jobs into two categories based on their requests, reserves a portion of resources for each of category, and dynamically adjusts the reserved ratio by monitoring the pending requests and estimating release patterns of running jobs. The results demonstrate DRESS significantly reduces the completion time for one category, up to 76.1% in our experiments, and in the meanwhile, maintains a stable overall system performance.	algorithm;apache hadoop;big data;central processing unit;computer cluster;data-intensive computing;experiment;job (computing);job stream;mapreduce;scheduling (computing);xfig	Ying Mao;Victoria Green;Jiayin Wang;Haoyi Xiong;Zhishan Guo	2018	2018 IEEE 11th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2018.00095	real-time computing;computer science;task analysis;big data;scheduling (computing);dynamic priority scheduling;data-intensive computing;distributed computing;computer cluster	HPC	-21.497502217278807	59.40055296230431	191904
640665988751e5629d14a6bbaa8387e8a1de8bdf	on qos-aware scheduling of data stream applications over fog computing infrastructures	digital signal processing;topology;quality of service qos aware scheduling fog computing infrastructures distributed computing cloud computing data stream processing applications dsp applications local computing resources storm open source dsp system scalable dsp system fault tolerant dsp system locally distributed clusters geographically distributed environment distributed qos aware scheduler self adaptation capabilities runtime adaptation capabilities;storms;monitoring;settore ing inf 05 sistemi di elaborazione delle informazioni;quality of service storms digital signal processing topology cloud computing monitoring fasteners;fasteners;quality of service;scheduling cloud computing data handling fault tolerant computing public domain software quality of service;cloud computing	Fog computing is rapidly changing the distributed computing landscape by extending the Cloud computing paradigm to include wide-spread resources located at the network edges. This diffused infrastructure is well suited for the implementation of data stream processing (DSP) applications, by possibly exploiting local computing resources. Storm is an open source, scalable, and fault-tolerant DSP system designed for locally distributed clusters. We made it suitable to operate in a geographically distributed and highly variable environment; to this end, we extended Storm with new components that allow to execute a distributed QoS-aware scheduler and give self-adaptation capabilities to the system. In this paper we provide a thorough experimental evaluation of the proposed solution using two sets of DSP applications: the former is characterized by a simple topology with different requirements; the latter comprises some well known applications (i.e., Word Count, Log Processing). The results show that the distributed QoS-aware scheduler outperforms the centralized default one, improving the application performance and enhancing the system with runtime adaptation capabilities. However, complex topologies involving many operators may cause some instability that can decrease the DSP application availability.	algorithm;centralized computing;cloud computing;computer cluster;distributed computing;fault tolerance;fog computing;instability;mobile device;open-source software;programming paradigm;quality of service;reference architecture;requirement;scalability;scheduling (computing);software-defined networking;stream processing	Valeria Cardellini;Vincenzo Grassi;Francesco Lo Presti;Matteo Nardelli	2015	2015 IEEE Symposium on Computers and Communication (ISCC)	10.1109/ISCC.2015.7405527	embedded system;distributed algorithm;real-time computing;quality of service;cloud computing;computer science;operating system;digital signal processing;distributed computing;utility computing;distributed design patterns;storm;computer security;computer network	HPC	-25.312751880545623	57.26668246295234	192356
07047bbd497d976392acd4df669fa34546f5a0db	microsharding: a declarative approach to support elastic oltp workloads	large scale	The paper proposes microsharding, a relational alternative for the recent procedural approaches with large-scale data stores to support OLTP workloads elastically. It employs a declarative specification, called transaction classes, of constraints applied on the transactions in a workload. The declarative specification enables a principled approach to design and analyze OLTP workloads. We discuss the current framework as well as identify research directions.	data store;declarative programming;online transaction processing	Jun'ichi Tatemura;Oliver Po;Hakan Hacigümüs	2012	Operating Systems Review	10.1145/2146382.2146385	parallel computing;real-time computing;computer science;database;online transaction processing	DB	-21.6240260687848	54.012896553680605	193396
d86e02145aa14bd2a4444120e13c21a2eb11b67a	on design of cluster and grid computing environment toolkit for bioinformatics applications	grid computing environment;biological data;grid computing;high speed;parallel applications	In this paper, we present BioGrid, a novel computing resource that combines advantages of grid computing technology with bioinformatics parallel applications. The grid environment permits the sharing of a large amount of idle computing resources and data. The investigation of bioinformatics demands orders of magnitude of computing resources and in parallel, the biological data are growing in such a high speed. By using BioGrid, we plan to integrate and share resources and biological data using the grid computing technology. Still in this paper, the well-known FASTA application is performed as matter of testing and analysis in our grid computing platform, as goal to show the improved efficiency and reduced computation time.	bioinformatics;grid computing	Chao-Tung Yang;Yu-Lun Kuo;Kuan-Ching Li;Jean-Luc Gaudiot	2004		10.1007/978-3-540-30536-1_10	computational science;semantic grid;computer science;theoretical computer science;end-user computing;data-intensive computing;distributed computing;utility computing;drmaa;grid computing	HPC	-21.118076495357652	54.28903248708985	193460
5d80167cb263e3942c2a88a06965f3d1d0a1eacb	cloudpt: performance testing for identifying and detecting bottlenecks in iaas		This work addresses performance testing for monitoring mass quantities of large-dataset measurements in infrastructure-as-a-Service (IaaS). Physical resources are not virtualized in sharing dynamic clouds; thus, shared resources compete for access to system resources. This competition introduces significant new challenges when assessing the performance of IaaS. A bottleneck may occur if one system resource is critical to IaaS; this may shut down the system and services, which would reduce the workflow performance by a large margin. To protect against bottlenecks, we propose CloudPT, a performance test management framework for IaaS. CloudPT has many advantages: (I) high-efficiency detection; (II) a unified end-to-end feedback loop to collaborate with cloud-ecosystems management; and (III) a troubleshooting performance test. This paper shows that CloudPT efficiently identifies and detects bottlenecks with a minimal false-positive rate (u003c13%) and it correlates high accuracy using the failure of a host virtual machine (host VM) to start-up with both cloud illustrative batches and transactional workloads such as the Spark, and Kafka framework for a data partitioning and collecting events on an each server. In a framework based on a trace case study, CloudPT diagnosed performance bottlenecks in 20 s with a precision rate of 86%, confirming its real-time efficiency.		Ameen Alkasem;Hongwei Liu;De-Cheng Zuo	2018		10.1007/978-3-030-05057-3_33	distributed computing;computer science;troubleshooting;test management;cloud computing;transactional leadership;workflow;virtual machine;bottleneck;feedback loop	SE	-24.068933730378866	58.988270477250225	194212
4520963b62d6992fa2caaff2e60f083bc5dcba69	recent trends in energy-efficient cloud computing	energy efficiency;cloud management system;appliances;data management;datacenter;computer architecture;servers;energy efficiency techniques energy efficient cloud computing online user information and communication technology ict paradigm datacenters cloud management systems;computer centres cloud computing;cloud computing energy efficiency data centers data management computer architecture servers;appliances cloud computing energy efficiency datacenter network servers cloud management system;network;cloud computing;data centers	Almost every online user directly or indirectly uses cloud computing, which is the most promising information and communication technology (ICT) paradigm. However, cloud computing's ultrascale size requires large datacenters comprising thousands of servers and other supporting equipment. The power consumption share of such infrastructures reaches 1.1 percent to 1.5 percent of the total electricity use worldwide, and is projected to rise even more. In this article, the authors describe recent trends in cloud computing regarding the energy efficiency of its supporting infrastructure. They present state-of-the-art approaches found in literature and in practice covering servers, networking, cloud management systems, and appliances (user software). They also describe benefits and trade-offs when applying energy-efficiency techniques, and discuss existing challenges and future research directions.	cloud computing;end-user development;programming paradigm;server (computing)	Toni Mastelic;Ivona Brandic	2015	IEEE Cloud Computing	10.1109/MCC.2015.15	panorama9;cloud computing security;data center;real-time computing;cloud computing;data management;computer science;operating system;cloud testing;distributed computing;utility computing;world wide web	HPC	-25.685488625959252	60.29506912552251	194739
39358d91e9e9322bf1d8eeebbbb710d5a38bc921	distributed slicing in dynamic systems	automatic control;p2p system;churn;cluster computing;application specific architectures;sorting;service orientation;peer to peer systems;resource allocation;resource management;automatic partitioning;gossip;p2p;gossip based algorithms distributed slicing dynamic systems peer to peer systems application specific architectures service oriented design philosophy p2p middleware services resource assignment resource management slicing service automatic partitioning p2p networks;dynamic system;dynamic systems;slice;aggregation;partitioning algorithms middleware environmental management resource management large scale systems automatic control sorting peer to peer computing scalability resilience;large scale;slicing;resilience;gossip based algorithms;resource assignment;middleware;scalability;p2p networks;peer to peer computing;program slicing;slicing service;random numbers;p2p middleware services;peer to peer;environmental management;resource allocation middleware peer to peer computing program slicing;distributed slicing;large scale systems;service oriented design philosophy;partitioning algorithms	"""Peer to peer (P2P) systems have moved from application specific architectures to a generic service oriented design philosophy. This raised interesting problems in connection with providing useful P2P middleware services capable of dealing with resource assignment and management in a large-scale, heterogeneous and unreliable environment. The slicing problem consists of partitioning a P2P network into <inline-formula><tex-math notation=""""LaTeX"""">$k$ </tex-math><alternatives><inline-graphic xlink:type=""""simple"""" xlink:href=""""gramoli-ieq1-2430856.gif""""/></alternatives></inline-formula> groups (slices) of a given portion of the network nodes that share similar resource values. As the network is large and dynamic this partitioning is continuously updated without any node knowing the network size. In this paper, we propose the first algorithm to solve the slicing problem. We introduce the metric of slice disorder and show that the existing ordering algorithm cannot nullify this disorder. We propose a new algorithm that speeds up the existing ordering algorithm but that suffers from the same inaccuracy. Then, we propose another algorithm based on ranking that is provably convergent under reasonable assumptions. In particular, we notice experimentally that ordering algorithms suffer from resource-correlated churn while the ranking algorithm can cope with it. These algorithms are proved viable theoretically and experimentally."""	algorithm;array slicing;dynamical system;experiment;middleware;peer-to-peer;xlink	Antonio Fernández;Vincent Gramoli;Ernesto Jiménez;Anne-Marie Kermarrec;Michel Raynal	2007	IEEE Transactions on Parallel and Distributed Systems	10.1109/ICDCS.2007.102	parallel computing;real-time computing;computer science;resource management;operating system;dynamical system;automatic control;database;distributed computing	HPC	-25.514755901537825	56.5298844488915	196101
602f8382e75247264a1cf6d55bfc2a1c55a5881a	experimental study of a self-tuning algorithm for dbms buffer pools	resource management;dbmss;buffer management;database systems performance	The tasks of configuring and tuning large database management systems (DBMSs) have always been both complex and time-consuming. They require knowledge of the characteristics of the system, the data, and the workload, and of the interrelationships between them. The increasing diversity of the data and the workloads handled by today’s systems is making manual tuning by database administrators almost impossible. Self-tuning DBMSs, which dynamically reallocate resources in response to changes in their workload in order to maintain predefined levels of performance, are one approach to handling the tuning problem. In this paper, we apply selftuning technology to managing the buffer pools, which are a key resource in a DBMS. Tuning the size of the buffer pools to a workload is crucial to achieving good performance. We describe a Buffer Pool Tuning Wizard that can be used by database administrators to determine effective buffer pool sizes. The wizard is based on a self-tuning algorithm called the Dynamic Reconfiguration algorithm (DRF), which uses the principle of goal-oriented resource management. It is an iterative algorithm that uses greedy heuristics to find a reallocation that benefits a target transaction class. We define and motivate the cost estimate equations used in the algorithm. We present the results of a set of experiments to investigate the performance of the algorithm.	experiment;greedy algorithm;heuristic (computer science);iterative method;oracle database;referring expression generation;self-tuning	Patrick Martin;Wendy Powley;Min Zheng	2005	J. Database Manag.	10.4018/jdm.2005040101	real-time computing;computer science;resource management;data mining;database;distributed computing	DB	-20.35938880362104	57.229873881992	196364
55beed79133698f09f181438229fbcbef8614b18	monitoring continuous state violation in datacenters: exploring the time dimension	performance optimization techniques continuous state violation violation monitoring data centers time dimension state monitoring instantaneous state monitoring approach unpredictable monitored value bursts momentary outliers window based state monitoring filtering time windows performance tuning techniques;continuous state violation;optimisation;momentary outliers;application software;information filtering;performance optimization techniques;distributed computing;time window;state monitoring;filtering time windows;window based state monitoring;computer centres;computerized monitoring;large scale;servers;violation monitoring;internet;tuning;monitoring;statistics;communication cost;statistics computer centres computerised monitoring optimisation;algorithms;optimization;scalability;internet application;time dimension;unpredictable monitored value bursts;computerised monitoring;information filters;performance optimization;performance tuning techniques;performance tuning;cost model;large scale systems;application software costs internet scalability large scale systems computerized monitoring educational institutions distributed computing information filtering information filters;instantaneous state monitoring approach;data centers	Monitoring global states of an application deployed over distributed nodes becomes prevalent in today's datacenters. State monitoring requires not only correct monitoring results but also minimum communication cost for efficiency and scalability. Most existing work adopts an instantaneous state monitoring approach, which triggers state alerts whenever a constraint is violated. Such an approach, however, may cause frequent and unnecessary state alerts due to unpredictable monitored value bursts and momentary outliers that are common in large-scale Internet applications. These false alerts may further lead to expensive and problematic counter-measures. To address this issue, we introduce window-based state monitoring in this paper. Window-based state monitoring evaluates whether state violation is continuous within a time window, and thus, gains immunity to short-term value bursts and outliers. Furthermore, we find that exploring the monitoring time window at distributed nodes achieves significant communication savings over instantaneous monitoring. Based on this finding, we develop WISE, a system that efficiently performs WIndow-based StatE monitoring at datacenter-scale. WISE is highlighted with three sets of techniques. First, WISE uses distributed filtering time windows and intelligently avoids global information collecting to achieve communication efficiency, while guaranteeing monitoring correctness at the same time. Second, WISE provides a suite of performance tuning techniques to minimize communication cost based on a sophisticated cost model. Third, WISE also employs a set of novel performance optimization techniques. Extensive experiments over both real world and synthetic traces show that WISE achieves a 50%–90% reduction in communication cost compared with existing instantaneous monitoring approaches and simple alternative schemes.	advanced configuration and power interface;analysis of algorithms;correctness (computer science);data center;distributed computing;experiment;filter (signal processing);internet;mathematical optimization;microsoft windows;performance tuning;scalability;subroutine;synthetic intelligence;tracing (software)	Shicong Meng;Ting Wang;Ling Liu	2010	2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)	10.1109/ICDE.2010.5447923	multiple time dimensions;data center;application software;real-time computing;scalability;the internet;simulation;computer science;data mining;database;server	DB	-20.515490383514972	55.2263778213273	196695
9bc33b7beb40a72a2699023568904355b95906c6	daphne: a flexible and hybrid scheduling framework in multi-tenant clusters		Distributed computing technologies, as popularized by Hadoop, have been proliferating in Cloud and enterprise computing over ten years, with the capability of processing data across thousands of machines. There is a wide diversity of workloads in such large scale clusters shared by multi-tenant. Hence, resource utilization and task scheduling become vital to performance and bring challenges to architecture designers. We present Daphne, a hybrid scheduling framework that strikes a tradeoff among three universal scheduling frameworks: 1) centralized scheduling; 2) loose coordination scheduling; and 3) fully distributed scheduling. Daphne defines a matching tree that forwards the application to the fittest scheduler according to the characteristic and priority of the application. Besides, Daphne utilizes resource prediction to increase task throughput significantly while it does not interfere with any running workload. We implement Daphne based on YARN and demonstrate that Daphne improves task throughput by nearly 17%.	apache hadoop;centralized computing;cloud computing;distributed computing;enterprise software;multitenancy;scheduling (computing);throughput	Yiqian Xia;Rui Ren;Hongming Cai;Athanasios V. Vasilakos;Zheng Lv	2018	IEEE Transactions on Network and Service Management	10.1109/TNSM.2017.2777885	throughput;workload;architecture;scheduling (computing);cloud computing;distributed computing;computer science;resource management;hybrid scheduling	HPC	-21.150328198106003	60.175512949919295	196757
dbcbdec1c13493de20888c5cf59e091e776e44a4	ripple: a scalable framework for distributed processing of rank queries	database management;data structures	We introduce a generic framework, termed RIPPLE, for processing rank queries in decentralized systems. Rank queries are particularly challenging, since the search area (i.e., which tuples qualify) cannot be determined by any peer individually. While our proposed framework is generic enough to apply to all decentralized structured systems, we show that when coupled with a particular distributed hash table (DHT) topology, it offers guaranteed worst-case performance. Specifically, rank query processing in our framework exhibits tunable polylogarithmic latency, in terms of the network size. Additionally we provide a means to trade-off latency for communication and processing cost. As a proof of concept, we apply RIPPLE for top-k query processing. Then, we consider skyline queries, and demonstrate that our framework results in a method that has better latency and lower overall communication cost than existing approaches over DHTs. Finally, we provide a RIPPLEbased approach for constructing a k-diversified set, which, to the best of our knowledge, is the first distributed solution for this problem. Extensive experiments with real and synthetic datasets validate the effectiveness of our framework.	best, worst and average case;database;distributed algorithm;distributed computing;distributed hash table;diversification (finance);experiment;learning to rank;network congestion;polylogarithmic function;skyline operator;synthetic intelligence;unified framework;universal instantiation	George Tsatsanifos;Dimitris Sacharidis;Timos K. Sellis	2014		10.5441/002/edbt.2014.25	data structure;computer science;data mining;database;distributed computing	DB	-19.236002507057215	55.631061360110635	196954
9d495bd3b99280f913251d10fa19f1735e1fd0d3	distributed scheduling algorithm for highly available component based applications	reliability;overlay networks;virtual machines cloud computing distributed algorithms overlay networks scheduling;distributed scheduling;connectors;communication overhead distributed scheduling algorithm highly available component based applications multiclouds infrastructure reliability cloud providers application level application aware algorithms cloud providers distributed management algorithm decentralized autonomic algorithm scalable component based applications overlay networks multicloud environment cloud provider independence global application availability centralized approach;scheduling algorithms;highly available applications;scheduling;multi clouds;relays reliability scheduling algorithms peer to peer computing connectors overlay networks scheduling;peer to peer computing;relays;multi clouds distributed scheduling highly available applications	The emergence of multi-clouds makes it difficult for application providers to offer reliable applications to end users. The different levels of infrastructure reliability offered by various cloud providers need to be abstracted at application level through application-aware algorithms for high availability. This task is challenging due to the closed world approach taken by the various cloud providers. In the face of different access and management policies orchestrated distributed management algorithms are needed instead of centralized solutions. In this paper we present a decentralized autonomic algorithm for achieving application high availability by harnessing the properties of scalable component-based applications and the advantage of overlay networks to communicate between peers. In a multi-cloud environment the algorithm maintains cloud provider independence while achieving global application availability. The algorithm was tested on a simulator and results show that it gives similar results to a centralized approach without inducing much communication overhead.	algorithm;autonomic computing;autonomous robot;centralized computing;cloud computing;component-based software engineering;decentralized autonomous organization;emergence;high availability;inductive reasoning;intelligent agent;openvms;overhead (computing);overlay network;provisioning;scalability;scheduling (computing);simulation;throughput	Marc Frîncu	2015	2015 IEEE International Parallel and Distributed Processing Symposium Workshop	10.1109/IPDPSW.2015.114	fair-share scheduling;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;scheduling;computer network	Embedded	-25.76834915599637	56.65210449392948	197478
007cce60f60ac5ef11ba89b3a38e6afa621273d4	a survey of distributed message broker queues		This paper surveys the message brokers that are in vogue today for distributed communication. Their primary goal is to facilitate the construction of decentralized topologies without single points of failure, enabling fault tolerance and high availability. These characteristics make them optimal for usage within distributed architectures. However, there are multiple protocols built to achieve this, and it would be beneficial to have a empirical comparison between their features and performance to determine their real-world applicability. This paper focuses on two popular protocols (Kafka and AMQP) and explores the divergence in their features as well as their performance under varied testing workloads.	advanced message queuing protocol;distributed operating system;fault tolerance;high availability;message broker;reliability engineering;single point of failure;vergence	Vineet John;Xia Liu	2017	CoRR		object request broker;message broker;mqtt	Networks	-24.503340416783217	53.60964588648937	197760
17c9bd881f408095080192285b1416fc73c5d524	a methodology for auto-recognizing dbms workloads	decision support system;difference set;transaction processing;database management system	The type of the workload on a database management system (DBMS) is a key consideration in tuning the system. Allocations for resources such as main memory can be very different depending on whether the workload type is Online Transaction Processing (OLTP) or Decision Support System (DSS). A DBMS also typically experiences changes in the type of workload it handles during its normal processing cycle. Database administrators must, therefore, recognize the significant shifts of workload type that demand reconfiguring the system in order to maintain acceptable levels of performance. We envision autonomous, selftuning DBMSs that have the capability to manage their own performance by automatically recognizing the workload type and then reconfiguring their resources accordingly. In this paper, we present an approach to automatically identifying a DBMS workload as either OLTP or DSS. We build a classification model based on the most significant workload characteristics that differenti ate OLTP from DSS and then use the model to identify any change in the workload type. We construct and compare classifiers built from two different sets of industry-standard workloads, namely the TPC-C and TPC-H benchmarks, and the Browsing and Ordering profiles from the TPC-W benchmark. We conduct various sets of experiments that show that our workload classifiers are reliable, and have high accuracy in recognizing the type of the workload mix and in estimating the degree of its concentration.	autonomous robot;benchmark (computing);computer data storage;database;decision support system;experiment;ibm tivoli storage productivity center;linear classifier;online transaction processing;performance tuning;tpc-w	Said Elnaffar	2002		10.1145/782115.782117	real-time computing;decision support system;transaction processing;computer science;operating system;data mining;database;distributed computing;difference set	DB	-20.7171449800617	57.06386899765055	197826
db44f63a98ad9437b10d20c43c01b5b68c9a6ef8	scale virtual worlds through dynamic load balancing	binary space partitioning virtual worlds distrbuted simulation load balancing;dynamic load balancing;space based load partitioning;resource allocation;prototypes;distributed processing;binary space partitioning;virtual reality;simulator centric architecture virtual world dynamic load balancing dynamic allocation space based load partitioning distributed binary space partitioning open simulator second life distributed bsp;distributed bsp;simulator centric architecture;computer architecture;servers;engines;dynamic allocation;binary space partition;virtual reality distributed processing resource allocation;open simulator;load management;load balancing;second life;load modeling servers load management engines second life computer architecture prototypes;distrbuted simulation;load balance;load modeling;distributed binary space partitioning;virtual world;virtual worlds	Dynamic load balancing holds the potential to scale virtual worlds flexibly by dynamic allocation of hardware to match load. In this paper, we study the benefits and overheads of space based load partitioning, in particular, distributed binary space partitioning (BSP). Our evaluation is based on Open Simulator, a virtual world system compatible with Second Life® viewers. Our work reveals that although simple and effective, distributed BSP has several limitations and suffers from high overhead. We then analyze the fundamental reasons of these limitations. To overcome the limitations, we argue that it is necessary to break away from the simulator-centric architecture used in today’s virtual worlds, and present potential new directions.	binary space partitioning;feasible region;fragmentation (computing);load balancing (computing);memory management;open research;opensimulator;overhead (computing);scene graph;second life;simulation;virtual world;world-system	Huaiyu Liu;Mic Bowman	2010	2010 IEEE/ACM 14th International Symposium on Distributed Simulation and Real Time Applications	10.1109/DS-RT.2010.14	parallel computing;real-time computing;computer science;distributed computing	Arch	-20.583364247138476	59.06198457161798	198256
0ed311d0276f0aa98673e0f5108878688bc4e2c9	client-centric benchmarking of eventual consistency for cloud storage systems	benchmarking;staleness;key value storage;benchmarking distributed systems key value storage eventual consistency staleness;clocks clustering algorithms databases tuning distributed computing benchmark testing;distributed systems;information retrieval systems cloud computing contracts;cassandra key value store client centric benchmarking cloud storage system key value storage system acid semantics application developers system administrators consistency aware service level agreements generation storage system client observed consistency;eventual consistency	Eventually-consistent key-value storage systems sacrifice the ACID semantics of conventional databases to achieve superior latency and availability. However, this means that client applications, and hence end-users, can be exposed to stale data. The degree of staleness observed depends on various tuning knobs set by application developers (customers of key-value stores) and system administrators (providers of key-value stores). Both parties must be cognizant of how these tuning knobs affect the consistency observed by client applications in the interest of both providing the best end-user experience and maximizing revenues for storage providers. Quantifying consistency in a meaningful way is a critical step toward both understanding what clients actually observe, and supporting consistency-aware service level agreements (SLAs) in next generation storage systems. This paper proposes a novel consistency metric called Gamma that captures client-observed consistency. This metric provides quantitative answers to questions regarding observed consistency anomalies, such as how often they occur and how bad they are when they do occur. We argue that Gamma is more useful and accurate than existing metrics. We also apply Gamma to benchmark the popular Cassandra key-value store. Our experiments demonstrate that Gamma is sensitive to both the workload and client-level tuning knobs, and is preferable to existing techniques which focus on worst-case behavior.		Wojciech M. Golab;Muntasir Raihan Rahman;Alvin AuYoung;Kimberly Keeton;Indranil Gupta	2014		10.1109/ICDCS.2014.57	real-time computing;computer science;consistency model;operating system;data mining;database;distributed computing;eventual consistency;computer security;computer network;benchmarking	OS	-23.668654222958693	57.881315764417295	198605
12ce9d2fca07c2189a18127a30fc432cdfc736ff	modeling the performance of ultra-large-scale systems using layered simulations	analytical models;software performance evaluation cloud computing large scale systems software architecture;servers analytical models unified modeling language software systems hardware blogs;layered simulation models cloud computing amazon s3 salesforce ultra large scale system globally distributed infrastructure software nodes hardware nodes uls system service demand system performance behaviour software developers software components 4 1 view model software architecture performance analysis models simulation models open source rss cloud system really simple syndication system industry inspired performance monitor;really simple syndication;performance monitoring;layered performance simulation;system configuration;4 1 view;software performance evaluation;software systems;indexing terms;proof of concept;software architecture;servers;building simulation;software development;unified modeling language;performance analysis;software component;performance model;4 1 view uls layered performance simulation;uls;simulation model;blogs;ultra large scale;analytical model;large scale systems;cloud computing;hardware;open source	The backbone of cloud computing platforms like Amazon S3 and Salesforce is formed by Ultra-Large-Scale (ULS) systems, i.e., complex, globally distributed infrastructure consisting of heterogeneous sets of software and hardware nodes. To ensure that a ULS system can scale to handle increasing service demand, it is important to understand the system's performance behaviour, for example to pro-actively plan for hardware upgrades. A good performance model should address concerns from all stakeholders at the level appropriate to their knowledge, interest, and experience. However, this is not straightforward, since stakeholders of ULS systems have a wide range of backgrounds and concerns: software developers are more interested in the performance of individual software components in the system, whereas managers are concerned about the performance of the entire system in different configurations. In this paper, we adapt the “4+1 View” model for software architecture to performance analysis models by building simulation models with multiple layers of abstraction. As a proof-of-concept, we conducted case studies on an open source RSS (Really Simple Syndication) Cloud system that actively delivers notifications of newly published content to subscribers, and on a hypothetical, industry-inspired performance monitor for ULS systems. We show that our layered simulation models are effective in identifying performance bottlenecks and optimal system configurations, balancing across performance objectives.	4+1 architectural view model;abstraction layer;amazon simple storage service;bottleneck (software);cloud computing;component-based software engineering;internet backbone;like button;open-source software;rss;service layer;simulation;software architecture;software developer;software project management;software system;top-down and bottom-up design;ultra-large-scale systems;user location service;web syndication	King Chun Foo;Zhen Ming Jiang;Bram Adams;Ahmed E. Hassan;Ying Zou;Kim Martin;Parminder Flora	2011	2011 International Workshop on the Maintenance and Evolution of Service-Oriented and Cloud-Based Systems	10.1109/MESOCA.2011.6049040	real-time computing;simulation;computer science;distributed computing	Metrics	-24.797649286630538	56.764459386570785	199768
4e906e2784179316af68152af322269b0029516a	vdismodel: a display latency performance model in virtualization environment	virtual machine;performance evaluation;virtual machining;vdismodel;benchmark testing servers virtual machining computational modeling driver circuits virtual environment performance evaluation;virtualization environment;system performance;virtual machines performance evaluation;xen based virtual machine vdismodel display latency performance model virtualization environment computer hardware performance evaluation system domain u;servers;computational modeling;virtual machines;performance model;driver circuits;computer hardware;performance evaluation system;virtual environment;domain u;xen based virtual machine;display latency performance model;benchmark testing	With the rapid development of virtualization technology as well as the good performance of computer hardware, virtual machine is beginning to gradually increase the performance of display to meet the needs of desktop users. However, there is a difference between the display mechanism in the virtual environment and the traditional environment. At present, more is focused on how to effectively and pertinently measure and assess the performance of display in virtualization environment. Taking consideration of the characteristics of display in the virtual environment, the virtual machine display performance evaluation system has following advantages: it can select a representative workload running on the VM in Domain U, make use of Tracing and Instrumentation to insert the record in the kernel of VM, log the relevant information, and then finally draw the performance-related parameters by the analysis of the recorder. This paper mainly depicts the underlying implementation details of the virtual display system in Xen and analyzes the display principle of Xen-based virtual machine. It also proposes a display latency performance model in Xen environment, which is named Vdismodel by verifying the reasonableness of the Vdismodel through a series of experiments and comparing the theoretical data and the measured data which are accessed by the virtual machine system performance evaluation.	computer hardware;desktop computer;experiment;hardware virtualization;hardware-assisted virtualization;interrupt latency;performance evaluation;verification and validation;virtual machine;virtual reality;x86 virtualization	Bin Zhou;Hai Jin;Pingpeng Yuan	2010	2010 Sixth International Conference on Semantics, Knowledge and Grids	10.1109/SKG.2010.17	embedded system;full virtualization;real-time computing;computer science;virtual machine;operating system;hardware virtualization;computer performance;virtual finite-state machine	HPC	-22.331606606225982	55.99503287613425	199900
