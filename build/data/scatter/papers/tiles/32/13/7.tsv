id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c031b5f32a2e6fb907080c5cc9efb35e2da9fcc4	toward formal reasoning with epistemic policies about information quality in the twittersphere	twittersphere;google;reliability;measurement;twitter streams;deliberate disinformation;twitter reliability measurement blogs resource description framework google;data fusion;information credibility;resource description framework;data mining;information evaluation;mideast;social network analysis twitter soft data fusion situation awareness information evaluation reliability credibility source independence;formal reasoning;epistemic policies;source independence;mining traffic;misinformation;information quality;social networking online;situation awareness;social network analysis;soft data fusion;twitter;twitter source reliability;deliberate disinformation formal reasoning epistemic policies information quality twittersphere high level situational awareness mining traffic twitter streams information credibility mideast twitter source reliability misinformation;credibility;blogs;high level situational awareness;social networking online data mining	Some recent systems accurately produce high-level situational awareness by mining traffic in Twitter. Where these systems have been successful, there has been no attempt to evaluate Twitter streams for source reliability and information credibility because the situations have not been adversarial. The use of Twitter in recent political dissent in the Mideast makes the need for computationally tractable approaches to evaluating Twitter source reliability and information credibility more acute in order to produce accurate situation awareness in the face of misinformation or deliberate disinformation.	cobham's thesis;high- and low-level;information quality;reason	Brian Ulicny;Mieczyslaw M. Kokar	2011	14th International Conference on Information Fusion		political science;data mining;internet privacy;world wide web	DB	-20.860174237953515	-55.437415446474894	149315
77276366906fc746098dd5dfe425e180827aac10	determining language variant in microblog messages	search intent;log analysis;user behavior	It is difficult to determine the country of origin of the author of a short message based only on the text. This is an even more complex problem when more than one country uses the same native language. In this paper, we address the specific problem of detecting the two main variants of the Portuguese language --- European and Brazilian --- in Twitter micro-blogging data, by proposing and evaluating a set of high-precision features. We follow an automatic classification approach using a Naïve Bayes classifier, achieving 95% accuracy. We find that our system is adequate for real-time tweet classification.	blog;naive bayes classifier;real-time clock;sensor	Gustavo Laboreiro;Matko Bosnjak;Luís Sarmento;Eduarda Mendes Rodrigues;Eugénio C. Oliveira	2013		10.1145/2480362.2480535	computer science;data science;machine learning;data mining;database;world wide web	NLP	-23.495047205509774	-55.45606276795877	149503
dc2e1c214acccdf57e83bdce25cfb47ee716a671	finding and classifying product relationships using information from the public web.		Relationships between products such as accessory or successor products are hard to find on the Web or have to be inserted manually in product information systems. Finding and classifying such relations automatically using information from the public Web only offers great value for customers and vendors as it helps to improve the buying process at low cost. We present and evaluate algorithms and methods for product relationship extraction on the Web requiring only a set of clustered product names as input. The solution can be easily implemented in different product information systems most useful in but not necessarily restricted to the application domain of online shopping.	algorithm;application domain;artificial neural network;brute-force search;computation;decision tree;defense in depth (computing);entity;information extraction;information system;internationalized domain name;linear function;online shopping;precision and recall;relationship extraction;scalability;statistical classification;support vector machine;test set;world wide web	Daniel Schuster;Till M. Juchheim;Alexander Schill	2010			data science;data mining	Web+IR	-19.147020360177706	-53.250225658073525	150288
026053d616a6eb87fb30307557204b78587c25d3	using aspect-based sentiment analysis to evaluate arabic news affect on readers	text analysis feature selection pattern classification sentiment analysis;cloud computing high definition video;aspect based sentiment analysis polarity identification j48 classifier crf classifier conditional random field classifier text classification feature selection text processing automated emotional affect analysis method automated sentiment analysis method automated post tone analysis method political views communication channel social media automated content analvsis digital information arabic news affect evaluation;high definition video;cloud computing	The rapid increase in digital information has raised great challenges especially when it comes to automated content analysis. The adoption of social media as a communication channel for political views demands automated methods for posts' tone analysis, sentiment analysis, and emotional affect. This paper proposes a novel approach of using aspect-based sentiment analysis in evaluating Arabic news posts affect on readers. The approach adopts several phases of text processing, features selection, and text classification. Two widely used classifiers, namely Conditional Random Fields (CRF) and J48, are tested. Experimentation results show that J48 outperforms CRF in aspect terms extraction whereas CRF is slightly better in aspect terms polarity identification.	channel (communications);conditional random field;digital data;document classification;sentiment analysis;social media	Mohammad Al-Smadi;Mahmoud Al-Ayyoub;Huda Al-Sarhan;Yaser Jararweh	2015	2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing (UCC)	10.1109/UCC.2015.78	speech recognition;cloud computing;computer science;operating system;pattern recognition;data mining;sentiment analysis	NLP	-21.08613831853805	-58.85867747488842	151236
4fb185af162e66d5b89d624c9b0d2ee159ef1226	privacy-aware image classification and search	user evaluation;image classification;classification;large scale;automatic detection;image search;visual features;image analysis;diversification;privacy	Modern content sharing environments such as Flickr or YouTube contain a large amount of private resources such as photos showing weddings, family holidays, and private parties. These resources can be of a highly sensitive nature, disclosing many details of the users' private sphere. In order to support users in making privacy decisions in the context of image sharing and to provide them with a better overview on privacy related visual content available on the Web, we propose techniques to automatically detect private images, and to enable privacy-oriented image search. To this end, we learn privacy classifiers trained on a large set of manually assessed Flickr photos, combining textual metadata of images with a variety of visual features. We employ the resulting classification models for specifically searching for private photos, and for diversifying query results to provide users with a better coverage of private and public content. Large-scale classification experiments reveal insights into the predictive performance of different visual and textual features, and a user evaluation of query result rankings demonstrates the viability of our approach.	computer vision;experiment;flickr;image retrieval;privacy;world wide web	Sergej Zerr;Stefan Siersdorfer;Jonathon S. Hare;Elena Demidova	2012		10.1145/2348283.2348292	diversification;contextual image classification;web query classification;image analysis;biological classification;computer science;data mining;privacy;world wide web;information retrieval	Web+IR	-25.20204924067519	-52.29720097934295	151269
bb769677808c3e907535d3c077bce1319748e539	user tweets based genre prediction and movie recommendation using lsi and svd	databases;motion pictures;semantics;data mining;large scale integration;dictionaries;twitter	The emerging popularity and raise in users' posts on social media gave birth to numerous research challenges. Out of all challenges users' centric context information based recommendation is one prime research area to recommend jobs, events and movies. Here in this research work we focus on movie context aware recommendation and for this purpose, we analyze users' posted movie tweets to understand their intentions for the same. Therefore, the objective of this research work is to predict genre of movies based on user's posted movie tweets and recommending movies to users' according to predicted genre. For this purpose, we pre-processed twitter extracted movie tweets using tokenization, porter stemming, stop word removal and use Word-Net dictionary for synonym matching. Further, we apply Latent Semantic Indexing technique which in turn involves Singular Value Decomposition on this pre-processed data and predicts genre on the basis of IMDb movie genre categorization. The predicted genre conveys the movie interest of the user and to recommend movie on the basis of predicted genre which is measured through euclidean distance. We have extracted IMdb given movie data and further predicted genre using our proposed technique. To validate this we divided our dataset using pareto principle and matched with IMDb given genre data set and achieved approximate 70% accuracy using our approach.	approximation algorithm;categorization;dictionary;euclidean distance;integrated circuit;internet movie database (imdb);pareto efficiency;singular value decomposition;social media;stemming;tokenization (data security)	Sakshi Bansal;Chetna Gupta;Anuja Arora	2016	2016 Ninth International Conference on Contemporary Computing (IC3)	10.1109/IC3.2016.7880220	film;computer science;data mining;semantics;multimedia;world wide web	ML	-21.9817023984511	-52.80241053131852	151814
a5925be5deed45aab67b32c3e43c56eb339075fa	analyzing and detecting review spam	unsolicited electronic mail web pages data mining blogs manufacturing search engines user generated content data analysis computer science application software;electronic commerce;web page spam;web pages;spam review categorization;data mining;internet data mining electronic commerce;product review spam detection;internet;email spam;product reviews opinion mining;spam review categorization product review spam detection product reviews opinion mining web page spam email spam	Mining of opinions from product reviews, forum posts and blogs is an important research topic with many applications. However, existing research has been focused on extraction, classification and summarization of opinions from these sources. An important issue that has not been studied so far is the opinion spam or the trustworthiness of online opinions. In this paper, we study this issue in the context of product reviews. To our knowledge, there is still no published study on this topic, although Web page spam and email spam have been investigated extensively. We will see that review spam is quite different from Web page spam and email spam, and thus requires different detection techniques. Based on the analysis of 5.8 million reviews and 2.14 million reviewers from amazon.com, we show that review spam is widespread. In this paper, we first present a categorization of spam reviews and then propose several techniques to detect them.	blog;categorization;email;sensor;spamming;statistical classification;trust (emotion);web page	Nitin Jindal;Bing Liu	2007	Seventh IEEE International Conference on Data Mining (ICDM 2007)	10.1109/ICDM.2007.68	spam blog;e-commerce;content farm;forum spam;the internet;computer science;spamming;social spam;spambot;web page;email address harvesting;internet privacy;spamtrap;world wide web;information retrieval	DB	-22.3695790202981	-56.774801155642265	152216
e2a14d4aa41bcf771a2221fdd138e87f0d4194bc	trollspot: detecting misbehavior in commenting platforms		Commenting platforms, such as Disqus, have emerged as a major online communication platform with millions of users and posts. Their popularity has also attracted parasitic and malicious behaviors, such as trolling and spamming. There has been relatively little research on modeling and safeguarding these platforms. As our key contribution, we develop a systematic approach to detect malicious users on commenting platforms. Our work provides two key novelties: (a) we provide a fine-grained classification of malicious behaviors, and (b) we use a comprehensive set of 73 features that span four dimensions of information. We use 7 million comments during a 9 month period, and we show that our classification methods can distinguish between benign, and malicious roles (spammers, trollers, and fanatics) with a 0.904 AUC. Our work is a solid step towards ensuring that commenting platforms are a safe and pleasant medium for the exchange of ideas.	comment (computer programming);computer-mediated communication;emoticon;internet troll;knowledge spillover;malware;spamming	Tai-Ching Li;Joobin Gharibshah;Evangelos E. Papalexakis;Michalis Faloutsos	2017		10.1145/3110025.3110057	convolutional neural network;data mining;semantic similarity;computer science;popularity;spamming	Security	-20.21641319434198	-55.18867391540176	152563
4b560ed6b5cc8fb20abc8458053ed7e3be6a133e	richness evaluation of blogs on its topics using a generative model and probabilistic analysis	latent dirichlet analysis richness evaluation generative model probabilistic analysis web service blog classification blog retrieval blogosphere web context blog article clustering lda;text mining information retrieval semantic web data mining;web sites information retrieval pattern classification pattern clustering probability web services	Nowadays, blogs are one of important web services to publish and share various information. Accordingly, evaluation of various keywords in blogs is one of the important research topics for effective and efficient classification and retrieval of blogs in the blogosphere. In this paper, we propose a method to identify important keywords in a blog. In order to identify such keywords, we consider web context, assuming that the blogs documents are generated from web contexts by proposed generative model. Therefore, if the contexts of keyword on the web are reflected well in the blog, then we may regard the keyword is essential because the blog is rich on the keyword. We clustered the blog articles on the given keyword by several subtopics using LDA (Latent Dirichlet Analysis), and compared the clusters with the web context documents obtained by web search. Finally, we evaluated the richness of blog on each keyword.	blog;blogosphere;generative model;latent dirichlet allocation;probabilistic analysis of algorithms;web search engine;web service	Jinhee Park;Jaedong Lee;Hye-Wuk Jung;Jee-Hyong Lee	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505393	web mining;computer science;data mining;web intelligence;world wide web;information retrieval	DB	-26.333032691273473	-56.999085497822314	152843
18ceea5bb8e369c58c2d16866585e4a69bddc78d	aspect-based twitter sentiment classification	opinion mining;twitter sentiment analysis;data mining;aspect based sentiment analysis;social networking online;sentiment analysis;pattern classification	Due to the popularity of Twitter, sentiment classification for Twitter has become a hot research topic. Previous studies have approached the problem as a tweet-level classification task where each tweet is classified as positive, negative or neutral. However, getting an overall sentiment might not be useful to organizations which are using twitter for monitoring consumer opinion of their products/services. Instead, it is more useful to determine specifically which aspects of the products/services the users are happy or unhappy about. This paper proposes an aspect-based sentiment classification approach to analyze sentiments for tweets. To the best of our knowledge, we are the first to perform sentiment analysis for Twitter in this manner. We conducted several experiments and show that by incorporating results from the aspect-based sentiment classifier, we are able to improve existing tweet-level classifiers. The experimental results also demonstrated that our approach outperforms existing state-of-the-art approaches.	brill tagger;experiment;lexicon;naive bayes classifier;part-of-speech tagging;point of sale;regular expression;sentiment analysis;statistical classification;tier 1 network	Hsiang Hui Lek;Danny Chiang Choon Poo	2013	2013 IEEE 25th International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2013.62	computer science;data mining;internet privacy;world wide web;sentiment analysis	NLP	-21.222970925323732	-56.80414021184412	153160
95f166c53c0deaa2733f458cedcf953478e661b2	extracting opinionated (sub)features from a stream of product reviews		We propose a stream mining method that learns opinion- ated product features from a stream of reviews. Monitoring the attitude of customers towards products is a field of much interest, but the prod- ucts themselves may come in and out of the market. We rather inves- tigate which (implicit) features of the products are important for the customers, and monitor how customer attitude towards such features evolves. To this purpose, we use a two-level stream clustering algorithm that extracts features and subfeatures from an opinionated stream, and couple it with dedicated feature-specific classifiers that assess the polar- ity of each extracted (sub)feature. We evaluate our method on a stream of reviews and we elaborate on how changes in the arrival rate of features (drift) affects algorithm performance.		Max Zimmermann;Eirini Ntoutsi;Myra Spiliopoulou	2013		10.1007/978-3-642-40897-7_23	computer science;data science;data mining;world wide web	NLP	-23.672488476070967	-55.25363914601828	153713
81c54408dd09e38c888b29f489b9257dfafa0205	did you expect your users to say this?: distilling unexpected micro-reviews for venue owners	unexpected;ranking;tip;micro review;foursquare	With social media platforms such as Foursquare, users can now generate concise reviews, i.e. micro-reviews, about entities such as venues (or products). From the venue owner's perspective, analysing these micro-reviews will offer interesting insights, useful for event detection and customer relationship management. However not all micro-reviews are equally important, especially since a venue owner should already be familiar with his venue's primary aspects. Instead we envisage that a venue owner will be interested in micro-reviews that are unexpected to him. These can arise in many ways, such as users focusing on easily overlooked aspects (by the venue owner), making comparisons with competitors, using unusual language or mentioning rare venue-related events, e.g. a dish being contaminated with bugs. Hence in this study, we propose to discover unexpected information in micro-reviews, primarily to serve the needs of venue owners.  Our proposed solution is to score and rank micro-reviews, for which we design a novel topic model, Sparse Additive Micro-Review (SAMR). Our model surfaces micro-review topics related to the venues. By properly offsetting these topics, we then derive unexpected micro-reviews. Qualitatively, we observed reasonable results for many venues. We then evaluate ranking accuracy using both human annotation and an automated approach with synthesized data. Both sets of evaluation indicate that our novel topic model, Sparse Additive Micro-Review (SAMR) has the best ranking accuracy, outperforming baselines using chi-square statistics and the vector space model.	additive model;customer relationship management;entity;expect;social media;software bug;sparse;topic model;venue (sound system)	Wen-Haw Chong;Bing Tian Dai;Ee-Peng Lim	2015		10.1145/2700171.2791024	ranking;data mining;world wide web	ML	-22.413514735884124	-52.78203301900198	153828
1cf24e9d97e9a2c2e89384e6219a43c532373ba5	real-time event localization and detection over social networks using apache intelligence		Today, the event detection activity is becoming increasingly more complex with the emergence of the Big Data phenomenon. More specifically, different social networks and microblogging services are leading to the massive explosion of data describing the real world. Multiple conducted works highlight the role of textual data as the simplest form to report actualities and to describe real-time events. But, the variety of the proposed solutions with high complexity degrees reveals big challenges that are hidden behind the nature of those data. A concentrated study around some established works for event detection over textual data published on social networks allows us to meet limits of proposed solutions. These limits are the main areas of research leading to the proposal of a new approach for a real-time localization and detection of events over textual data posted on social networks. This approach takes benefit from a set of Apache platforms for efficient treatment of the huge streams of data.	automatic summarization;big data;data acquisition;emergence;geolocation;machine learning;real-time clock;real-time transcription;social network;supervised learning;text corpus	Sarra Hasni;Sami Faïz	2017	2017 IEEE/ACS 14th International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2017.135	real-time computing;streams;computer science;big data;data mining;microblogging;social media;social network;phenomenon	DB	-23.21734993255776	-53.88725624944618	154488
39259a53fb50295a3927e28be471eff87545f6f1	automatic general-purpose sanitization of textual documents	text analysis data privacy information theory publishing security of data;publishing;text analysis;privacy data publishing document sanitization information theory;data privacy;data publishing automatic general purpose sanitization method textual documents information sharing technologies confidential information document sanitization goal information theory knowledge bases sensitive textual information detection;knowledge based systems context companies data privacy proposals government manuals;security of data;information theory	The advent of new information sharing technologies has led society to a scenario where thousands of textual documents are publicly published every day. The existence of confidential information in many of these documents motivates the use of measures to hide sensitive data before being published, which is precisely the goal of document sanitization. Even though methods to assist the sanitization process have been proposed, most of them are focused on the detection of specific types of sensitive entities for concrete domains, lacking generality and and requiring user supervision. Moreover, to hide sensitive terms, most approaches opt to remove them, a measure that hampers the utility of the sanitized document. This paper presents a general-purpose sanitization method that, based on information theory and exploiting knowledge bases, detects and hides sensitive textual information while preserving its meaning. Our proposal works in an automatic and unsupervised way and it can be applied to heterogeneous documents, which make it specially suitable for environments with massive and heterogeneous information-sharing needs. Evaluation results show that our method outperforms strategies based on trained classifiers regarding the detection recall, whereas it better retains the document's utility compared to term-suppression methods.	confidentiality;entity;general-purpose markup language;information theory;knowledge base;sanitization (classified information);unsupervised learning;zero suppression	David Sánchez;Montserrat Batet;Alexandre Viejo	2013	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2013.2239641	information theory;computer science;data mining;publishing;world wide web;computer security;information retrieval;statistics	Web+IR	-19.481543583776755	-59.05608022852736	154793
61816c876d46d7834d18afe9bfb5f74fa976c666	prediction of importance of figures in scholarly papers		This paper shows that the importance of a figure in scholarly papers can be predicted by a machine learning technique. The growing number of scholarly papers makes it difficult to keep pace with researches. The current scholarly paper format of the ACM / IEEE long paper contains eight pages. These eight pages include details that may not be necessary for most readers. In fact, even before we read the details, we try to obtain a vague understanding of the entire content by browsing through the eight pages. To address this issue, summarization techniques have been explored. However, these techniques are focused on texts, thus there is room for research to make the paper easier to read by summarizing the figure of the paper. A wide variety of figures can be found in scholarly papers. They include figures that depict the overview of a paper or highly contextualized figures that can be understood only after reading the detailed text. Therefore, selecting important figures is a key issue in the summarization of scholarly papers. This paper shows that a figure that should be presented first to the readers can be selected based on a comparison of the sizes, page numbers or color features of the figures. We also described how our result can be applied in more practical cases on searching, exploring and serendipitious encounter of digital documents.	color;information retrieval;machine learning;random forest;randomness;speech recognition;vagueness	Yui Kita;Jun Rekimoto	2017	2017 Twelfth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2017.8244648	automatic summarization;data mining;information retrieval;web page;feature extraction;computer science;search engine	DB	-25.11739706613175	-57.40133581598025	155082
37a6a9963357bbb764410f1e9f3ac4b9df8e9e0d	twitter topic modeling for breaking news detection		Social media platforms like Twitter have become increasingly popular for the dissemination and discussion of current events. Twitter makes it possible for people to share stories that they find interesting with their followers, and write updates on what is happening around them. In this paper we attempt to use topic models of tweets in real time to identify breaking news. Two different methods, Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Process (HDP) are tested with each tweet in the training corpus as a document by itself, as well as with all the tweets of a unique user regarded as one document. This second approach emulates Author-Topic modeling (AT-modeling). The evaluation of methods relies on manual scoring of the accuracy of the modeling by volunteered participants. The experiments indicate topic modeling on tweets in real-time is not suitable for detecting breaking news by itself, but may be useful in analyzing and describing news tweets.	best, worst and average case;cluster analysis;computer cluster;emulator;experiment;latent dirichlet allocation;news aggregator;real-time clock;sensor;social media;statistical model;text corpus;the new york times;topic model;unique user;user experience	Henning M. Wold;Linn Vikre;Jon Atle Gulla;Özlem Özgöbek;Xiaomeng Su	2016		10.5220/0005801902110218	internet privacy	ML	-23.43799382137255	-53.535466036588375	155731
a57014782c49b5c6bc3808eabccf38c021791d2a	social media alert and response to threats to citizens (smart-c)	data analysis;emergency management;emergency services;national security;sensor fusion;social networking (online);department of homeland security;mms message;smart-c system;twitter;blogs;cell phone;citizens;data cleaning;disaster life-cycle;effective emergency management;event detection;event level semantic information;event notification;incident awareness;information portal;information veracity estimation;multisensor fusion;pattern analysis;robust bidirectional communication;social media alert;social media initiative;spatiotemporal analysis;text message;threat response;trend analysis;uncertainty reduction;alerting;emergency management;robust data analytics;social media	Social media, such as blogs, Twitter, and information portals, have emerged as the dominant communication mechanism of society. Exploiting such input to gain awareness of an incident is a critical direction for research in effective emergency management. In this paper we present an overview of the SMART-C system, which is part of the social media initiative at the Department of Homeland Security. The system aims to enable robust bidirectional communication between emergency management and the public at large throughout the disaster life-cycle via a multitude of devices and modalities including cell phones, MMS messages, text messages, blogs, Twitter, etc. A discussion of the major components of SMART-C and related research challenges is included. These components include mechanisms to model event level semantic information, a platform for implementing multi-sensor fusion, mechanisms for estimating the veracity of information, data cleaning to reduce uncertainty and enhance accuracy of event detection and notification, and spatiotemporal analyses for pattern and trend analyses for higher level observations.	blog;coherence (physics);dalton (program);image;mobile phone;modal logic;plasma cleaning;plug and play;portals;smart tv;social media;veracity;video;word-sense disambiguation	Nabil Adam;Jayan Eledath;Sharad Mehrotra;Nalini Venkatasubramanian	2012	8th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom)		visualization;social media;computer science;speech;national security;operating system;noise reduction;reliability;database;sensor fusion;internet privacy;data analysis;world wide web;computer security;emergency management;computer network	HPC	-22.327922421014982	-55.5406969357647	155783
836f29edda1abfc288e6768eaf4515a0dea0c09f	constructing conceptual trajectory maps to trace the development of research fields	text mining	"""This study proposes a new method to construct and trace the trajectory of conceptual development of a research field by combining main path analysis, citation analysis, and text-mining techniques. Main path analysis, a method used commonly to trace the most critical path in a citation network, helps describe the developmental trajectory of a research field. This study extends the main path analysis method and applies text-mining techniques in the new method, which reflects the trajectory of conceptual development in an academic research field more accurately than citation frequency, which represents only the articles examined. Articles can be merged based on similarity of concepts, and by merging concepts the history of a research field can be described more precisely. The new method was applied to the """"h-index"""" and """"text mining"""" fields. The precision, recall, and F-measures of the h-index were 0.738, 0.652, and 0.658 and those of text-mining were 0.501, 0.653, and 0.551, respectively. Last, this study not only establishes the conceptual trajectory map of a research field, but also recommends keywords that are more precise than those used currently by researchers. These precise keywords could enable researchers to gather related works more quickly than before."""	map	Yi-Ning Tu;Shu-Lan Hsu	2016	JASIST	10.1002/asi.23522	text mining;computer science;data science;data mining;world wide web;information retrieval	Robotics	-25.295602116073248	-57.233727325173824	156967
e009ebd1b8162142695984ab71e2eeee759aff85	cofea: a novel approach to spam review identification based on entropy and co-training	spam review;cofea;co training	With the rapid development of electronic commerce, spam reviews are rapidly growing on the Internet to manipulate online customers’ opinions on goods being sold. This paper proposes a novel approach, called CoFea (Co-training by Features), to identify spam reviews, based on entropy and the co-training algorithm. After sorting all lexical terms of reviews by entropy, we produce two views on the reviews by dividing the lexical terms into two subsets. One subset contains odd-numbered terms and the other contains even-numbered terms. Using SVM (support vector machine) as the base classifier, we further propose two strategies, CoFea-T and CoFea-S, embedded with the CoFea approach. The CoFea-T strategy uses all terms in the subsets for spam review identification by SVM. The CoFea-S strategy uses a predefined number of terms with small entropy for spam review identification by SVM. The experiment results show that the CoFea-T strategy produces better accuracy than the CoFea-S strategy, while the CoFea-S strategy saves more computing time than the CoFea-T strategy with acceptable accuracy in spam review identification.	algorithm;co-training;e-commerce;embedded system;internet;sorting;spamming;support vector machine	Wen Zhang;Chaoqi Bu;Taketoshi Yoshida;Siguang Zhang	2016	Entropy	10.3390/e18120429	computer science;machine learning;data mining;world wide web	AI	-22.520482693653758	-57.03888391014667	156990
4b4b309865908da184e0d7e6c937e5906c196948	personalized microblogs corpus recommendation based on dynamic users interests	publisher authority personalized microblog corpus recommendation dynamic user interests virtual social network web based applications time consuming process user productivity user dynamic level of interest user social relationship tweet content;dynamic level of interest microblogs recommendation;vectors mathematical model feature extraction equations history runtime training;social networking online;recommendation;dynamic level of interest;social networking online recommender systems;recommender systems;microblogs	Microblogs are specialized virtual social network web-based applications. Nowadays, following the microblogs is becoming more challenging as users can receive thousands of corpus updates every day. Going through all the corpuses updates is a time consuming process and affects the user's productivity in real life, especially for the users who have a lot of followees and thousands of tweets arriving at their timelines everyday. In this paper, we propose a personalized recommendation system that aims at giving the user a summary of all received corpuses. Considering the fact that the user interests changes over time, this summary should be based on the user's level of interest in the topic of the corpus at the time of reception. Our method considers three major elements: users's dynamic level of interest in a topic, user's social relationship such as the number of followers, their real geographical neighborhood, and other explicit features related to the publishers authority and the tweet's content.	information retrieval;naive bayes classifier;personalization;real life;recommender system;social network;text corpus;timeline;web application	Shaymaa Khater;Hicham G. Elmongui;Denis Gracanin	2013	2013 International Conference on Social Computing	10.1109/SocialCom.2013.156	computer science;microblogging;data mining;multimedia;internet privacy;world wide web;recommender system	DB	-23.78607059818235	-52.83675215788598	157161
7dd7c8313d9f0225343e2153a22b75e5382d8e32	intelligent discovery of notable product features by mining large scale online reviews		The large, publicly accessible online product reviews have become a significant information resource for enterprise to discover preferences of the public and market trend. In this paper, we propose the text mining driven information gain model for identifying notable product features to enable enterprise understand what product features determine the customers' satisfaction about the given products. This paper collects a large scale online product reviews about the automobile to empirically evaluate the effectiveness of the proposed mathematical model.	information gain in decision trees;kullback–leibler divergence;mathematical model;text mining	Xin Ni;Yinghui Sai;Anning Wang;Qiang Zhang	2017	2017 IEEE Second International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2017.69	market research;feature extraction;data mining;business;market trend;data science;text mining	DB	-21.16603219186888	-52.26560608825919	157807
0791f5bb64c67b07c6c9d7a2047f209046838ad1	research on micro-blog sentiment analysis	micro-blog;parsing;polar word;sentiment analysis	Micro-Blog is a kind of important media on the Internet, which conveys the users' point of view in simple and convenient ways. The research related to micro-blog has got extensive attention from the academic and industrial areas. This paper aims to study on the analysis of Chinese micro-blog emotion, proposing a template-based algorithm for the automatic discovery of micro-blog emotional neologisms. It uses a combination method of dictionaries and rules on Chinese micro-blog sentiment analysis.		Xinpo Lou;Yumei Chai;Hongying Zan;Ruicong Xu;Yingjie Han;Kunli Zhang	2013		10.1007/978-3-642-45185-0_49	computer science;data mining;advertising;world wide web	NLP	-22.597033096701182	-57.69516287105397	158281
3695e7795944155262e1c4396ad28835f7e8a586	a survey of location inference techniques on twitter	twitter analytics;information retrieval;location inference	The increasing popularity of the social networking service, Twitter, has made it more involved in day-to-day communications, strengthening social relationships and information dissemination. Conversations on Twitter are now being explored as indicators within early warning systems to alert of imminent natural disasters such earthquakes and aid prompt emergency responses to crime. Producers are privileged to have limitless access to market perception from consumer comments on social media and microblogs. Targeted advertising can be made more effective based on user profile information such as demography, interests and location. While these applications have proven beneficial, the ability to effectively infer the location of Twitter users has even more immense value. However, accurately identifying where a message originated from or author’s location remains a challenge thus essentially driving research in that regard. In this paper, we survey a range of techniques applied to infer the location of Twitter users from inception to state-of-the-art. We find significant improvements over time in the granularity levels and better accuracy with results driven by refinements to algorithms and inclusion of more spatial features.	algorithm;location inference;social media;user profile	Oluwaseun Ajao;Jun Hong;Weiru Liu	2015	J. Information Science	10.1177/0165551515602847	computer science;data mining;database;internet privacy;world wide web;information retrieval	Web+IR	-24.464511140174515	-52.79895998786931	158950
7138a8d67a1bf206a34864f8eb994672eb363b0f	topic chains for understanding a news corpus	similarity metric	The Web is a great resource and archive of news articles for the world. We present a framework, based on probabilistic topic modeling, for uncovering the meaningful structure and trends of important topics and issues hidden within the news archives on the Web. Central in the framework is a topic chain, a temporal organization of similar topics. We experimented with various topic similarity metrics and present our insights on how best to construct topic chains. We discuss how to interpret the topic chains to understand the news corpus by looking at long-term topics, temporary issues, and shifts of focus in the topic chains. We applied our framework to nine months of Korean Web news corpus and present our findings.	archive;markov chain;topic model;world wide web	Dongwoo Kim;Alice H. Oh	2011		10.1007/978-3-642-19437-5_13	computer science;data science;data mining;information retrieval	NLP	-24.182772644353083	-55.53587700805684	159000
ea8ab14b78beaba5fdc8bbfee28307d9cbfc980f	towards social signal separation based on reconstruction independent component analysis		We all know that the ratio of social data noise is pretty significant. Therefore, tackling with noise problem is always obtained attention from data scientists. In this paper, we present a research of using reconstruction independent component analysis algorithm for blind separation social event signals from their mixtures (i.e., mixture is the combination of source signal and noise). This issue can be categorized as cocktail party problem. Despite cocktail party problem is a classical topic, however, dealing with social media data can be considered as a new research trend. From the case study with two events on Twitter, we demonstrate that our approach is quite promising. Further, our work can be applied for recommendation systems, or is used as a pre-processing step for other studies (e.g., focus search and event detection).	independent component analysis	Hoang Long Nguyen;Khac-Hoai Nam Bui;Nayoung Jo;Jason J. Jung;David Camacho	2018		10.1007/978-3-030-02738-4_16	recommender system;data mining;independent component analysis;cocktail party effect;data noise;social media;computer science	ML	-21.473705606140044	-53.90604107018543	159008
f54c515534e04b5ba8dc879b106022f404718870	approaches to develop oracle for detecting deception in online chatting software	social network services;software;detectors;software ontologies data mining internet social network services blogs detectors;expert systems;early warning software;social networking services;detecting lies;text analysis;data mining;ontologies artificial intelligence;text analysis data mining expert systems ontologies artificial intelligence security of data social networking online;early warning;text processing oracle development approach deception detection online chatting software expert rule data mining ontology;detecting lies online chatting chatting dangers early warning software;internet;social networking online;ontologies;online chatting;blogs;security of data;chatting dangers	In this paper we will discuss several approaches that can be used to develop an oracle in chatting software. This oracle is meant to give early warning to the chatting software user about their chatting partner based on their communication. The approaches include expert rule, data mining, ontology as well as simple text processing. Even though the issue of privacy and free-information in cyber world is unavoidable, we believe that choices must be given so that stack holder such as parents can choose to have save environment for their children.	data mining;online chat;oracle database;oracle machine;privacy;sensor;simpletext	Z. Shukur;Arbi Haza Nasution;A. A. Wibowo	2011	Proceedings of the 2011 International Conference on Electrical Engineering and Informatics	10.1109/ICEEI.2011.6021557	detector;text mining;the internet;computer science;ontology;artificial intelligence;warning system;data mining;database;internet privacy;world wide web;expert system	DB	-20.610013940056884	-56.48122470520616	159160
be9177d5eacf02dae53d048bebb4322cd343a4a1	text-based emotion classification using emotion cause extraction	weibo;emotion classification;microblogging;emotion cause extraction	In recent years, increasing impact of social networks on people's opinions and decision making has attracted lots of attention. Microblogging, one of the most popular social network applications that allows people to share ideas and discuss over various topics, is taken as a rich resource of opinion and emotion data. In this paper, we propose and implement a novel method for identifying emotions in microblog posts. Unlike traditional approaches which are mostly based on statistical methods, we try to infer and extract the reasons of emotions by importing knowledge and theories from other fields such as Sociology. Based on the theory that a triggering cause event is an integral part of emotion, the technique of emotion cause extraction is used as a crucial step to improve the quality of selected features. First, after thorough analysis on sample data we constructed an automatic rule-based system to detect and extract the cause event of each emotional post. We build an emotion corpus with Chinese microblog posts labeled by human annotators. Then a classifier is trained to classify emotions in microblog posts based on extracted cause events. The overall performance of our system is very promising. The experiment results show that our approach is effective in selecting informative features. Our system outperformed the baseline noticeably in most cases, suggesting its great potential. This exploration should provide a new way to look at the emotion classification task and lay the ground for future research on textual emotion processing.	baseline (configuration management);expert system;feature selection;logic programming;social network;statistical classification;text-based (computing);titcoin	Weiyuan Li;Hua Xu	2014	Expert Syst. Appl.	10.1016/j.eswa.2013.08.073	computer science;artificial intelligence;microblogging;machine learning;emotion classification;data mining	NLP	-21.73570235619092	-57.54018836114372	159297
d39f879aa866f5ccea44ae75a888a85243b33019	detection and fine-grained classification of cyberbullying events	classification;cyberbullying;languages and literatures;natural language processing	In the current era of online interactions, both positive and negative experiences are abundant on the Web. As in real life, negative experiences can have a serious impact on youngsters. Recent studies have reported cybervictimization rates among teenagers that vary between 20% and 40%. In this paper, we focus on cyberbullying as a particular form of cybervictimization and explore its automatic detection and fine-grained classification. Data containing cyberbullying was collected from the social networking site Ask.fm. We developed and applied a new scheme for cyberbullying annotation, which describes the presence and severity of cyberbullying, a post author’s role (harasser, victim or bystander) and a number of fine-grained categories related to cyberbullying, such as insults and threats. We present experimental results on the automatic detection of cyberbullying and explore the feasibility of detecting the more fine-grained cyberbullying categories in online posts. For the first task, an F-score of 55.39% is obtained. We observe that the detection of the fine-grained categories (e.g. threats) is more challenging, presumably due to data sparsity, and because they are often expressed in a subtle and implicit way.	cyberbullying;database normalization;experience;experiment;feature selection;google moderator;information;interaction;learnability;mathematical optimization;neural coding;orthographic projection;preprocessor;real life;sensor;social media;sparse matrix;threat (computer);world wide web	Cynthia Van Hee;Els Lefever;Ben Verhoeven;Julie Mennes;Bart Desmet;Guy De Pauw;Walter Daelemans;Véronique Hoste	2015			natural language processing;biological classification;computer science;world wide web;computer security	NLP	-20.847893825960906	-57.39763246986279	159827
3936ad8d61ce32e1a592ba5c813c0f4dbb8c7500	identifying microblogs for targeted contextual advertising	contextual advertising;classification;microblogs;microposts	Micro-blogging sites such as Facebook, Twitter, Google+ present a nice opportunity for targeting advertisements that are contextually related to the microblog content. By virtue of the sparse and noisy text makes identifying the microblogs suitable for advertising a very hard problem. In this work, we approach the problem of identifying the microblogs that could be targeted for advertisements as a twostep classification approach. In the first pass, microblogs suitable for advertising are identified. Next, in the second pass, we build a model to find the sentiment of the advertisable microblog. The systems use features derived from the Partof-speech tags, the tweet content and uses external resources such as query logs and n-gram dictionaries from previously labeled data. This work aims at providing a thorough insight into the problem and analyzing various features to assess which features contribute the most towards identifying the tweets that can be targeted for advertisements.	blog;contextual advertising;dictionary;google+;n-gram;noisy text;sparse matrix	Kushal S. Dave;Vasudeva Varma	2012			biological classification;computer science;microblogging;internet privacy;world wide web;contextual advertising	NLP	-22.793281537862875	-53.73314528504494	159851
13001c6f11409e6764f0ec94c55bc35b59755df0	text analytics: the convergence of big data and artificial intelligence	tecnologias generalidades;tecnologias	The analysis of the text content in emails, blogs, tweets, forums and other forms of textual communication constitutes what we call text analytics. Text analytics is applicable to most industries: it can help analyze millions of emails; you can analyze customers’ comments and questions in forums; you can perform sentiment analysis using text analytics by measuring positive or negative perceptions of a company, brand, or product. Text Analytics has also been called text mining, and is a subcategory of the Natural Language Processing (NLP) field, which is one of the founding branches of Artificial Intelligence, back in the 1950s, when an interest in understanding text originally developed. Currently Text Analytics is often considered as the next step in Big Data analysis. Text Analytics has a number of subdivisions: Information Extraction, Named Entity Recognition, Semantic Web annotated domain’s representation, and many more. Several techniques are currently used and some of them have gained a lot of attention, such as Machine Learning, to show a semisupervised enhancement of systems, but they also present a number of limitations which make them not always the only or the best choice. We conclude with current and near future applications of Text Analytics.	artificial intelligence;big data;blog;email;information extraction;machine learning;named-entity recognition;natural language processing;semantic web;semi-supervised learning;sentiment analysis;text mining	Antonio Moreno;Teófilo Redondo	2016	IJIMAI	10.9781/ijimai.2016.369	analytics;text mining;web analytics;computer science;data science;noisy text analytics;data mining;business intelligence;cultural analytics;software analytics;information retrieval;semantic analytics;speech analytics	AI	-23.700945908426263	-56.66467234889594	160178
0b37e91d852ed4d6f492c0fd2db742f73e833d83	learning semantic relationships between entities in twitter	semantic enrichment;social web;relation learning;twitter	In this paper, we investigate whether semantic relationships between entities can be learnt from analyzing microblog posts published on Twitter. We identify semantic links between persons, products, events and other entities. We develop a relation discovery framework that allows for the detection of typed relations that moreover may have temporal dynamics. Based on a large Twitter dataset, we evaluate different strategies and show that co-occurrence based strategies allow for high precision and perform particularly well for relations between persons and events achieving precisions of more than 80%. We further analyze the performance in learning relationships that are valid only for a certain time period and reveal that for those types of relationships Twitter is a suitable source as it allows for discovering trending topics with higher accuracy and with lower delay in time than traditional news media.	contextual advertising;dbpedia;entity;gene ontology term enrichment;linkage (software);ontology (information science);precision and recall;sensor	Ilknur Celik;Fabian Abel;Geert-Jan Houben	2011		10.1007/978-3-642-22233-7_12	social web;computer science;data science;data mining;world wide web	Web+IR	-24.04483289080204	-52.26467351545839	160820
4b6aba91164d41592b0c0a7a8fb46524eafe6934	challenge on context-aware movie recommendation: camra2011	context aware;contextual information;context aware recommendations;recommender system;group recommendations;recommender systems	This paper provides an overview of CAMRa2011, the second edition of the Challenge on Context-Aware Movie Recommendation. The challenge attracted a large number of participants to work on the challenge tracks, which this time focused on group related recommendation aspects.		Alan Said;Shlomo Berkovsky;Ernesto William De Luca;Jannis Hermanns	2011		10.1145/2043932.2044015	computer science;machine learning;multimedia;world wide web;information retrieval;recommender system	HCI	-25.878434879291092	-52.1816651414029	161671
30ed57d93e1ec033be43d6c75aceb334c89a0a3a	automated classification of extremist twitter accounts using content-based and network-based features	training;data mining;twitter;decision trees;tagging;harmonic analysis	The Islamic State of Iraq and Syria (ISIS) is a extremist militant group in the Middle East known to employ social media for propaganda and recruiting purposes. In particular, the social media website Twitter is well known to be exploited by ISIS supporters. To this end, we devise an effective and scalable classification scheme to filter out ISIS propaganda accounts from the rest of the Twitter accounts covering the general population in specific geographic regions. We take a data-driven approach to train and test a classifier using a combination of both content-based and network-based features on a large-scale dataset consisting of 10% random sample of all public Twitter posts in 2014. Such a dataset contains a total of 4,820 confirmed ISIS propaganda accounts and a large collection of general user accounts originated in nearby Egypt and Saudi Arabia regions. We show that high precision and specificity is obtained with the proposed classifier through cross-validation. In addition, we demonstrate that the use of network-based features derived from the Twitter @mention network is crucial on accurte classification of ISIS propaganda accounts.	cellular automaton;cross-validation (statistics);isis;scalability;sensitivity and specificity;social media;user (computing)	Daniel Xie;Jiejun Xu;Tsai-Ching Lu	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840895	geography;data mining;internet privacy;world wide web	ML	-22.932182784555817	-54.64577114386028	161718
d4bbd9c7d95f00a5acaef2f2a48f8543f52df18a	a survey on natural language processing for fake news detection		Fake news detection is a critical yet challenging problem in Natural Language Processing (NLP). The rapid rise of social networking platforms has not only yielded a vast increase in information accessibility but has also accelerated the spread of fake news. Given the massive amount of Web content, automatic fake news detection is a practical NLP problem required by all online content providers. This paper presents a survey on fake news detection. Our survey introduces the challenges of automatic fake news detection. We systematically review the datasets and NLP solutions that have been developed for this task. We also discuss the limits of these datasets and problem formulations, our insights, and recommended solutions.	accessibility;natural language processing;web content	Ray Oshikawa;Jing Qian;William Yang Wang	2018	CoRR		natural language processing;fake news;problem formulations;artificial intelligence;social network;web content;computer science	ML	-22.455191270391158	-53.58578268246609	161789
cc79d555108f70040288f16dcea75589c9bfd410	hierarchical emotion classification and emotion component analysis on chinese micro-blog posts	text mining;emotion component analysis;emotion classification;micro blog	Text emotion analysis has long been a hot topic. With the development of social network, text emotion analysis on micro-blog posts is a new trend. However, most researchers classify posts into coarse-grained emotion classes, which cannot depict the emotions accurately. Besides, flat classification is mostly adopted, which brings difficulty for classifiers when given a large dataset. In this paper, we classify Chinese micro-blog posts into fine-grained emotion classes, employing hierarchical classification to improve the performance of classifiers. Moreover, based on the regression values in classification procedure, we propose an algorithm to detect the principal emotions in posts and calculate their ratios.	algorithm;blog;social network	Hua Xu;Weiwei Yang;Jiushuo Wang	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.07.028	text mining;speech recognition;computer science;microblogging;emotion classification;pattern recognition;data mining	NLP	-21.679430795648138	-57.31824754365105	161826
6901b69887829597d4322fa72dd8cc0a3e1b5170	finding topic-level experts in scholarly networks	topic relevance;factor graph;expert finding;scholarly network	Expert finding is of vital importance for exploring scientific collaborations to increase productivity by sharing and transferring knowledge within and across different research areas. Expert finding methods, including content-based methods, link structure-based methods, and a combination of content-based and link structure-based methods, have been studied in recent years. However, most state-of-the-art expert finding approaches have usually studied candidates’ personal information (e.g. topic relevance and citation counts) and network information (e.g. citation relationship) separately, causing some potential experts to be ignored. In this paper, we propose a topical and weighted factor graph model that simultaneously combines all the possible information in a unified way. In addition, we also design the Loopy Max-Product algorithm and related message-passing schedules to perform approximate inference on our cycle-containing factor graph model. Information Retrieval is chosen as the test field to identify representative authors for different topics within this area. Finally, we compare our approach with three baseline methods in terms of topic sensitivity, coverage rate of SIGIR PC (e.g. Program Committees or Program Chairs) members, and Normalized Discounted Cumulated Gain scores for different rankings on each topic. The experimental results demonstrate that our factor graph-based model can definitely enhance the expert-finding performance.	approximation algorithm;baseline (configuration management);casio loopy;factor graph;information retrieval;message passing;personally identifiable information;relevance	Lili Lin;Zhuoming Xu;Ying Ding;Xiaozhong Liu	2013	Scientometrics	10.1007/s11192-013-0988-6	computer science;data science;factor graph;data mining;world wide web;information retrieval;statistics	Web+IR	-25.940368497493225	-58.501367754297846	161959
33cff42fa86e02031df0117749cb3ee9298ccc38	using sentiment representation learning to enhance gender classification for user profiling		User profiling means exploiting the technology of machine learning to predict attributes of users, such as demographic attributes, hobby attributes, preference attributes, etc. It’s a powerful data support of precision marketing. Existing methods mainly study network behavior, personal preferences, post texts to build user profile. Through our data analysis of micro-blog, we find that females show more positive and have richer emotions than males in online social platform. This difference is very conducive to the distinction between genders. Therefore, we argue that sentiment context is important as well for user profiling.This paper focuses on exploiting microblog user posts to predict one of the demographic labels: gender. We propose a Sentiment Representation Learning based Multi-Layer Perceptron(SRL-MLP) model to classify gender. First we build a sentiment polarity classifier in advance by training Long Short-Term Memory(LSTM) model on e-commerce review corpus. Next we transfer sentiment representation to a basic MLP network. Last we conduct experiments on gender classification by sentiment representation. Experimental results show that our approach can improve gender classification accuracy by 5.53%, from 84.20% to 89.73%.	blog;concatenation;e-commerce;experiment;feature extraction;long short-term memory;machine learning;memory-level parallelism;sentiment analysis;statistical classification;user profile	F.C. Caruso;Lin Li;Luo Zhong;Jianwei Zhang;Jinghang Liu	2018	CoRR		machine learning;artificial intelligence;perceptron;precision marketing;profiling (computer programming);microblogging;social media;hobby;feature learning;classifier (linguistics);computer science;user profile	AI	-19.683134761870082	-52.43071097867667	162375
7f8fde47e476405ec981205dceda67bb2a3c64f7	i don't think we've met: encouraging collaboration via topic-based search	icebreaker;authors;topic based search;collaboration;visualization;break the ice;topic modelling;natural language processing	We present PaperPilot1 (bit.ly/paperpilot) a new tool which performs smart collaborator search using research concepts automatically extracted from the CSCW domain, as characterized by 5,516 papers taken from four conferences in the area. PaperPilot infers how a paragraph of text (say an abstract or news article) relates to these research concepts and uses this information to retrieve the 100 most similar papers and identify the most relevant topic for each. These topics can be used both to obtain a quick overview of the papers and as an ice breaker for opening conversations with potential collaborators. To ensure the smart collaborator search is relevant to CSCW 2015 attendees, all accepted papers and authors will also be included.	computer-supported cooperative work;the 100	Thomas S. Methven;Stefano Padilla;Mike J. Chantler	2015		10.1145/2685553.2702678	visualization;human–computer interaction;computer science;data mining;multimedia;topic model;communication;management;world wide web;collaboration	NLP	-25.59874880283936	-53.160496960248	162527
e778686aa61be53ce8e95ac83b2af0f61979547b	identifying textual personal information using bidirectional lstm networks		Data-driven approaches based on the data collected from individuals are improving everyday life as a result of the developments in big data studies. Prior to developing such an approach, removal of personal information from the data is important since personal information contained in data would jeopardize people's privacy and may harm related individuals. Especially in the field of health sciences, identifying personal information in the collected data is a difficult task as most of the data collected in hospitals are in plain text format. In this work, a method for automatically identifying words which includes personal information is proposed. The proposed method uses natural language processing techniques and bi-directional long short term memory networks. Development of the proposed method is done by using a de-identification challenge dataset which is composed of discharge summaries of 889 patients. The proposed method in this study is able to identify words that include personal information from their surrounding words without using dictionaries such as name lists or city lists. The tests at the end of this study show that proposed method can identify words containing personal information with an accuracy of 99.43%.	big data;de-identification;dictionary;discharger;long short-term memory;name mangling;natural language processing;personally identifiable information	Memduh Cagri Demir;Seyda Ertekin	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404506	data mining;long short term memory;artificial intelligence;support vector machine;pattern recognition;computer science;big data;informatics;everyday life;personally identifiable information;biomedical sciences;plain text	NLP	-21.565899727384217	-56.23370921158286	162739
52855cd58c6021b39b5d9c0dffd5222f578074fe	machine learning approach to analyze and predict the popularity of tweets with images		Social Media platforms play a major role in spreading information. Twitter, is one such platform which is used by millions of people to share information every day. Twitter with the recent introduction of a feature that helps its users to attach images to a tweet has changed the dynamics of tweeting. Many people now prefer to tweet with images. This study tries to analyse and predict the popularity of such tweets. This study uses learning mechanisms like decision tree, neural networks and random forests to learn the tweets posted by people with a higher number of followers. Image parameters, network variables, transactional, and historical variables of a tweet are identified and are trained for predicting the test data. This study can help businesses to build better social media tools, which allows customers to tweet data at the right time. This study also identifies the contribution of various parameters that may help a tweet to go viral.	machine learning	Nimish Joseph;Amir Sultan;Arpan Kumar Kar;P. Vigneswara Ilavarasan	2018		10.1007/978-3-030-02131-3_49	data mining;decision tree;artificial neural network;transactional leadership;test data;popularity;social media;random forest;computer science	AI	-19.98907858461689	-52.52632240449964	162903
0a50d0e65f69ec2d14b6ffb1c99b81918b8ea85c	forecasting weekly crude oil using twitter sentiment of u.s. foreign policy and oil companies data		The drop in crude oil price during late 2014 has had a significant impact on all nations. While some countries have reaped the benefits of low oil prices, others have suffered greatly. As a result, it is no surprise that many academics have attempted to develop reliable models to forecast crude oil price. In the age of information and social media, the role of Twitter and Facebook has become increasingly more relevant in understanding our environment. Many academics have exploited this wealth of data to extract features including sentiment and word frequency to build reliable forecasting models for financial instruments such as stocks. These methodologies, however, remain unexplored for the prediction of crude oil prices. The purpose of this investigation to develop a novel model that uses sentiment of United States foreign policy and oil companies' to forecast the direction of weekly WTI crude oil prices. The investigation is divided into three parts: 1) a methodology of collecting tweets relevant to US foreign policy and oil companies'; 2) a statistical analysis of the novel features using Granger Causality Test; 3) the development and evaluation of three machine learning classifiers including Naïve Bayes, ANNs, and SVM to predict the direction of weekly WTI crude oil. The findings of the statistical analysis showed strong correlation between the novel inputs and WTI crude oil price. The results of the statistical tests were then used in the development of the predictive model. SVM was found to provide best forecasting performance. Furthermore, using these novel features, the predictive accuracy exceeded that of existing models mentioned in literature	causality;machine learning;naive bayes classifier;predictive modelling;social media;word lists by frequency	Mourad Oussalah;Ahmed Zaidi	2018	2018 IEEE International Conference on Information Reuse and Integration (IRI)	10.1109/IRI.2018.00037	finance;naive bayes classifier;statistical hypothesis testing;data mining;west texas intermediate;sentiment analysis;granger causality;foreign policy;social media;computer science;text mining	Web+IR	-20.15506659227365	-53.184120489655655	163389
36c274d0e27263dbd704b462cb2282ffe74d9abd	finding optimists and pessimists on twitter		Optimism is linked to various personality factors as well as both psychological and physical health, but how does it relate to the way a person tweets? We analyze the online activity of a set of Twitter users in order to determine how well machine learning algorithms can detect a person’s outlook on life by reading their tweets. A sample of tweets from each user is manually annotated in order to establish ground truth labels, and classifiers are trained to distinguish between optimistic and pessimistic users. Our results suggest that the words in people’s tweets provide ample evidence to identify them as optimists, pessimists, or somewhere in between. Additionally, several applications of these trained models are explored.	algorithm;bookmark (world wide web);ground truth;machine learning;microsoft outlook for mac;text-based (computing)	Xianzhi Ruan;Steven R. Wilson;Rada Mihalcea	2016			machine learning;data mining;world wide web	NLP	-20.645420374985036	-58.88067601509435	164068
020f9a10b92fadd12265494ef0894580e7e5901e	sentiment analysis based on mapreduce: a survey		Sentiment analysis is the process of analyzing people's sentiments, opinions, evaluations and emotions by studying their written text. It attracts the interest of many researchers, since it is useful for many applications, ranging from decision making to product evaluation to mention a few. Sentiment analysis can be conducted using machine-learning techniques, lexicon-based techniques or hybrid techniques that combines both. As people are more reliant on social networks such as Twitter, this has become a valuable source for sentiment analysis. However, the existence of big data frameworks require adaptation of these techniques to run within such frameworks. This paper reviews sentiment analysis techniques, focusing on the MapReduce-based analysis techniques. We found that the Naïve Bayes algorithm was the most used machine learning technique for extracting sentiments from big datasets because of its high accuracy rates. However, the dictionary-based techniques achieved better results in terms of execution time.		Mariam Khader;Arafat Awajan;Ghazi Al-Naymat	2018		10.1145/3291280.3291795	naive bayes classifier;sentiment analysis;big data;machine learning;social network;lexicon;artificial intelligence;computer science	Web+IR	-21.69022052628487	-57.71895162312231	164755
09e356dc597bef308e22b2a4eda581ec0f24f48c	detecting expressions of blame or praise in text		The growth of social networking platforms has drawn a lot of attentions to the need for social computing. Social computing utilises human insights for computational tasks as well as design of systems that support social behaviours and interactions. One of the key aspects of social computing is the ability to attribute responsibility such as blame or praise to social events. This ability helps an intelligent entity account and understand other intelligent entities’ social behaviours, and enriches both the social functionalities and cognitive aspects of intelligent agents. In this paper, we present an approach with a model for blame and praise detection in text. We build our model based on various theories of blame and include in our model features used by humans determining judgment such as moral agent causality, foreknowledge, intentionality and coercion. An annotated corpus has been created for the task of blame and praise detection from text. The experimental results show that while our model gives similar results compared to supervised classifiers on classifying text as blame, praise or others, it outperforms supervised classifiers on more finer-grained classification of determining the direction of blame and praise, i.e., self-blame, blame-others, self-praise or praise-others, despite not using labelled training data.	causality;intelligent agent;intentionality;interaction;sensor;social computing;supervised learning;text corpus;theory	Orizu Udochukwu;Yulan He	2016			artificial intelligence	AI	-21.56902049469532	-57.80336760408047	164842
97ed65e162a48ee994c27dcde374b17e266c371f	topic detection and tracking for threaded discussion communities	language use;computed tomography;circuit faults;current transformers;fault currents;online discussion;topic detection and tracking;online community;content analysis;internet;online discussion community;authorship analysis topic detection and tracking online discussion community content analysis;authorship analysis;user activity information topic detection threaded discussion communities online communities web users tracking method discussion data two level decision framework content similarity;communities;internet information analysis;information analysis;algorithm design and analysis;yarn communities intelligent agent laboratories pattern recognition automation data visualization search engines discussion forums internet	The threaded discussion communities are one of the most common forms of online communities, which are becoming more and more popular among web users. Everyday a huge amount of new discussions are added to these communities, which are difficult to summarize and search. In this paper, we propose a topic detection and tracking (TDT) method for the discussion threads. Most existing TDT methods deal with the news stories, but the language used in discussion data are much more casual, oral and informal compared with news data. To solve this problem, we design several extensions to the basic TDT framework, focusing on the very nature of discussion data, including a thread/post activity validation step, a term pos-weighting strategy, and a two-level decision framework considering not only the content similarity but also the user activity information. Experiment results show that our pro-posed method greatly improves current TDT methods in real discussion community environment. The discussion data can be better organized for searching and visualization with the help of TDT.	conversation threading;decision support system;online community;post-hartree–fock;time-domain reflectometry	Mingliang Zhu;Weiming Hu;Ou Wu	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.50	current transformer;algorithm design;the internet;content analysis;computer science;artificial intelligence;machine learning;data mining;multimedia;data analysis;law;world wide web	HCI	-26.301012846273757	-55.5865812551473	166305
4dc8f71d618f800b37bf3772cf7710228781b886	comparative study of authorship identification techniques for cyber forensics analysis	information retrieval;natural language processing;machine learning	Authorship Identification techniques are used to identify the most appropriate author from group of potential suspects of online messages and find evidences to support the conclusion. Cybercriminals make misuse of online communication for sending blackmail or a spam email and then attempt to hide their true identities to void detection.Authorship Identification of online messages is the contemporary research issue for identity tracing in cyber forensics. This is highly interdisciplinary area as it takes advantage of machine learning, information retrieval, and natural language processing. In this paper, a study of recent techniques and automated approaches to attributing authorship of online messages is presented. The focus of this review study is to summarize all existing authorship identification techniques used in literature to identify authors of online messages. Also it discusses evaluation criteria and parameters for authorship attribution studies and list open questions that will attract future work in this area. Keywords—cyber crime; Author Identification; SVM	blog;boolean algebra;common criteria;computer forensics;computer-mediated communication;cybercrime;email;information retrieval;machine learning;natural language processing;spamming;stylometry;test set	Smita Nirkhi;Rajiv V. Dharaskar	2013	CoRR		support vector machine;computer science;artificial intelligence;machine learning;data mining;internet privacy;world wide web;computer security	Security	-20.078751384027182	-57.02203569337334	167187
0bdb56c3e9141d654a4809f014d0369305f1e7fd	you are what you say: privacy risks of public mentions	k anonymity;re identification;mentions;k identification;datasets;egat;medical ethics;sparse relation space;privacy	In today's data-rich networked world, people express many aspects of their lives online. It is common to segregate different aspects in different places: you might write opinionated rants about movies in your blog under a pseudonym while participating in a forum or web site for scholarly discussion of medical ethics under your real name. However, it may be possible to link these separate identities, because the movies, journal articles, or authors you mention are from a sparse relation space whose properties (e.g., many items related to by only a few users) allow re-identification. This re-identification violates people's intentions to separate aspects of their life and can have negative consequences; it also may allow other privacy violations, such as obtaining a stronger identifier like name and address.This paper examines this general problem in a specific setting: re-identification of users from a public web movie forum in a private movie ratings dataset. We present three major results. First, we develop algorithms that can re-identify a large proportion of public users in a sparse relation space. Second, we evaluate whether private dataset owners can protect user privacy by hiding data; we show that this requires extensive and undesirable changes to the dataset, making it impractical. Third, we evaluate two methods for users in a public forum to protect their own privacy, suppression and misdirection. Suppression doesn't work here either. However, we show that a simple misdirection strategy works well: mention a few popular items that you haven't rated.	algorithm;blog;botnet;identifier;internet privacy;sparse matrix;the movies;zero suppression	Dan Frankowski;Dan Cosley;Shilad Sen;Loren G. Terveen;John Riedl	2006		10.1145/1148170.1148267	medical ethics;computer science;data mining;internet privacy;privacy;world wide web;information retrieval	Web+IR	-21.354522990042565	-53.746587309887936	167353
220943b35b4a674cbbfb98e8a4e2e8a996351271	a novel approach to compute pattern history for trend analysis	word processing pattern classification sorting text analysis;sorting;trend analysis;text analysis;pattern classification;scalability trend analysis pattern history retrospective corpus frequency distribution information analyst word sequence pattern extraction text analysis bucket like suffix sorting push pop stack operation;history lungs sorting cancer usa councils bioinformatics time frequency analysis;time frequency analysis;word processing	It is attractive to observe the history of one pattern in the retrospective corpus such that one might sense the trends related to that pattern efficiently, where one pattern history was defined as the frequency distribution of that pattern over time. Pattern history could provide information analysts with valuable information and clues for trend analysis. Note that one pattern could be a token or a sequence of words in this study. To extract significant patterns from a large amount of texts, and meanwhile compute the corresponding patterns histories, a scalable and external memory approach based on bucket-like suffixes sorting and push-pop stack operations is proposed. To highlight the scalability and robustness of this approach, experimental data consisted of 3, 225, 549 articles (about 4 GB) downloaded from the PubMed for 20 years from 1990 to 2009, and the total computation time of patterns histories was about 48 hours using only one PC. Experimental results showed that specific patterns histories did reveal the variations of some events and gave hints for trend analysis.	central processing unit;computation;debian;diagram;digital history;gnu;hard disk drive;keyword extraction;linux;pattern language;pubmed;random-access memory;scalability;sorting;terabyte;time complexity	Jing-Doo Wang	2011	2011 Eighth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2011.6019799	text mining;speech recognition;time–frequency analysis;trend analysis;computer science;sorting;artificial intelligence;machine learning;data mining;algorithm;statistics	HPC	-25.354521590755162	-55.19023346226964	167443
45527a1a733770cb00b5b8a90994039c6f0b17f5	tagspheres: visualizing hierarchical relations in tag clouds		Tag clouds are widely applied, popular visualization techniques as they illustrate summaries of textual data in an intuitive, lucid manner. Many layout algorithms for tag clouds have been developed in the recent years, but none of these approaches is designed to reflect the notion of hierarchical distance. For that purpose, we introduce a novel tag cloud layout called TagSpheres. By arranging tags on various hierarchy levels and applying appropriate colors, the importance of individual tags to the observed topic gets assessable. To explore relationships among various hierarchy levels, we aim to place related tags closely. Three usage scenarios from the digital humanities, sports and aviation, and an evaluation with humanities scholars exemplify the applicability and point out the benefit of TagSpheres.	algorithm;color;digital humanities;exemplification;lucid;qualitative comparative analysis;tag cloud;text corpus	Stefan Jänicke;Gerik Scheuermann	2016		10.5220/0005654100150026	data science;data mining;world wide web	HCI	-24.209775797509696	-58.978360283993936	167840
97c23bdfd9e0921048558ef6aeaa327f368ce8e1	robust spammer detection in microblogs: leveraging user carefulness		Microblogging Web sites, such as Twitter and Sina Weibo, have become popular platforms for socializing and sharing information in recent years. Spammers have also discovered this new opportunity to unfairly overpower normal users with unsolicited content, namely social spams. Although it is intuitive for everyone to follow legitimate users, recent studies show that both legitimate users and spammers follow spammers for different reasons. Evidence of users seeking spammers on purpose is also observed. We regard this behavior as useful information for spammer detection. In this article, we approach the problem of spammer detection by leveraging the “carefulness” of users, which indicates how careful a user is when she is about to follow a potential spammer. We propose a framework to measure the carefulness and develop a supervised learning algorithm to estimate it based on known spammers and legitimate users. We illustrate how the robustness of the detection algorithms can be improved with aid of the proposed measure. Evaluation on two real datasets from Sina Weibo and Twitter with millions of users are performed, as well as an online test on Sina Weibo. The results show that our approach indeed captures the carefulness, and it is effective for detecting spammers. In addition, we find that our measure is also beneficial for other applications, such as link prediction.	algorithm;sensor;socialization;spamming;supervised learning;web design	Hao Fu;Xing Xie;Yong Rui;Neil Zhenqiang Gong;Guangzhong Sun;Enhong Chen	2017	ACM TIST	10.1145/3086637	supervised learning;robustness (computer science);data mining;microblogging;internet privacy;spamming;social network;computer science;social media	Web+IR	-19.764839069416343	-54.97213474420665	168139
afe45eea3e954805df2dac45b55726773d221f1b	predictive analytics on public data - the case of stock markets	predictive analytics;data mining;sentiment analysis;financial markets;twitter;social media	This work examines the predictive power of public data by aggregating information from multiple online sources. Our sources include microblogging sites like Twitter, online message boards like Yahoo! Finance, and traditional news articles. The subject of prediction are daily stock price movements from Standard & Poor’s 500 index (S&P 500) during a period from June 2011 to November 2011. To forecast price movements we filter messages by stocks, apply state-of-the-art sentiment analysis to message texts, and aggregate message sentiments to generate trading signals for daily buy and sell decisions. We evaluate prediction quality through a simple trading model considering real-world limitations like transaction costs or broker commission fees. Considering 833 virtual trades, our model outperformed the S&P 500 and achieved a positive return on investment of up to ~0.49% per trade or ~0.24% when adjusted by market, depending on supposed trading costs.	aggregate data;google analytics;information privacy;sentiment analysis;technical standard	Stefan Nann;Jonas Krauss;Detlef Schoder	2013			analytics;predictive analytics;social media;computer science;data science;marketing;data mining;business analytics;financial market;sentiment analysis	ML	-20.139725963694207	-52.77174633114253	168151
38092fcd0d484fdbc09ad776fedca87d009c225b	an algorithm for local geoparsing of microtext	microtext;geo ir or gir;location;data mining;toponym;geographic information retrieval;twitter;social media;local search;microblogs;geoparse	The location of the author of a social media message is not invariably the same as the location that the author writes about in the message. In applications that mine these messages for information such as tracking news, political events or responding to disasters, it is the geographic content of the message rather than the location of the author that is important. To this end, we present a method to geo-parse the short, informal messages known as microtext. Our preliminary investigation has shown that many microtext messages contain place references that are abbreviated, misspelled, or highly localized. These references are missed by standard geo-parsers. Our geo-parser is built to find such references. It uses Natural Language Processing methods to identify references to streets and addresses, buildings and urban spaces, and toponyms, and place acronyms and abbreviations. It combines heuristics, open-source Named Entity Recognition software, and machine learning techniques. Our primary data consisted of Twitter messages sent immediately following the February 2011 earthquake in Christchurch, New Zealand. The algorithm identified location in the data sample, Twitter messages, giving an F statistic of 0.85 for streets, 0.86 for buildings, 0.96 for toponyms, and 0.88 for place abbreviations, with a combined average F of 0.90 for identifying places. The same data run through a geo-parsing standard, Yahoo! Placemaker, yielded an F statistic of zero for streets and buildings (because Placemaker is designed to find neither streets nor buildings), and an F of 0.67 for toponyms.	algorithm;geo (microformat);heuristic (computer science);machine learning;microprinting;named-entity recognition;natural language processing;open-source software;parsing;social media	Judith Gelernter;Shilpa Balaji	2013	GeoInformatica	10.1007/s10707-012-0173-8	social media;geography;computer science;local search;microblogging;data mining;database;internet privacy;toponymy;location;world wide web;cartography	NLP	-24.303916523324858	-54.414991669590705	168404
29cb2b615c075cfd59a90bac8fb571b77a324baa	visual analysis of online social media to open up the investigation of stance phenomena	text visualization;text and document data;sprakteknologi sprakvetenskaplig databehandling;visual linguistics;human computer interaction;online social media;manniska datorinteraktion interaktionsdesign;interaction;time series;text analytics;informations och programvisualisering;stance analysis;visualization;sprakstudier;sentiment analysis;specific languages;datavetenskap datalogi;language technology computational linguistics;computer science;visual analytics;data och informationsvetenskap;information and software visualization;sprak och litteratur	Online social media are a perfect text source for stance analysis. Stance in human communication is concerned with speaker attitudes, beliefs, feelings and opinions. Expressions of stance are associated with the speakers' view of what they are talking about and what is up for discussion and negotiation in the intersubjective exchange. Taking stance is thus crucial for the social construction of meaning. Increased knowledge of stance can be useful for many application fields such as business intelligence, security analytics, or social media monitoring. In order to process large amounts of text data for stance analyses, linguists need interactive tools to explore the textual sources as well as the processed data based on computational linguistics techniques. Both original texts and derived data are important for refining the analyses iteratively. In this work, we present a visual analytics tool for online social media text data that can be used to open up the investigation of stance phenomena. Our approach complements traditional linguistic analysis techniques and is based on the analysis of utterances associated with two stance categories: sentiment and certainty. Our contributions include (1) the description of a novel web-based solution for analyzing the use and patterns of stance meanings and expressions in human communication over time; and (2) specialized techniques used for visualizing analysis provenance and corpus overview/navigation. We demonstrate our approach by means of text media on a highly controversial scandal with regard to expressions of anger and provide an expert review from linguists who have been using our tool.	attitude;body of uterus;categories;complement system proteins;computation;computational linguistics;intersubjectivity;social media measurement;speech;text corpus;visual analytics;web application	Kostiantyn Kucher;Teri Schamp-Bjerede;Andreas Kerren;Carita Paradis;Magnus Sahlgren	2016		10.1177/1473871615575079	natural language processing;computer vision;visual analytics;interaction;visualization;computer science;artificial intelligence;machine learning;time series;data mining;multimedia;sentiment analysis;statistics	NLP	-22.824072931983338	-56.24057042629881	169122
c4d0605396bd40c879ce6a34da039dd1976b477b	unsupervised strategies for shilling detection and robust collaborative filtering	shilling;plsa;robust statistics;social system;user profile;dimensionality reduction;recommender system;collaborative filtering;principal component analysis;detection algorithm;dimensional reduction;pca	Collaborative filtering systems are essentially social systems which base their recommendation on the judgment of a large number of people. However, like other social systems, they are also vulnerable to manipulation by malicious social elements. Lies and Propaganda may be spread by a malicious user who may have an interest in promoting an item, or downplaying the popularity of another one. By doing this systematically, with either multiple identities, or by involving more people, malicious user votes and profiles can be injected into a collaborative recommender system. This can significantly affect the robustness of a system or algorithm, as has been studied in previous work. While current detection algorithms are able to use certain characteristics of shilling profiles to detect them, they suffer from low precision, and require a large amount of training data. In this work, we provide an in-depth analysis of shilling profiles and describe new approaches to detect malicious collaborative filtering profiles. In particular, we exploit the similarity structure in shilling user profiles to separate them from normal user profiles using unsupervised dimensionality reduction. We present two detection algorithms; one based on PCA, while the other uses PLSA. Experimental results show a much improved detection precision over existing methods without the usage of additional training time required for supervised approaches. Finally, we present a novel and highly effective robust collaborative filtering algorithm which uses ideas presented in the detection algorithms using principal component analysis.	algorithm;cartographic propaganda;dimensionality reduction;malware;personalization;preprocessor;principal component analysis;probabilistic latent semantic analysis;recommender system;robust collaborative filtering;robustness (computer science);security hacker;sensor;signal-to-noise ratio;social system;supervised learning;unsupervised learning;user (computing);user profile	Bhaskar Mehta;Wolfgang Nejdl	2008	User Modeling and User-Adapted Interaction	10.1007/s11257-008-9050-4	computer science;collaborative filtering;machine learning;data mining;world wide web;recommender system;principal component analysis	Web+IR	-19.444696592624243	-54.92878926623776	169341
0709b8badadb2f3d82dbd3d2aa7f8bd9fae55f93	topic segmentation via community detection in complex networks	small world networks;grupo de excelencia;text analysis;ciencias basicas y experimentales;matematicas;web sites;natural language processing	Many real systems have been modeled in terms of network concepts, and written texts are a particular example of information networks. In recent years, the use of network methods to analyze language has allowed the discovery of several interesting effects, including the proposition of novel models to explain the emergence of fundamental universal patterns. While syntactical networks, one of the most prevalent networked models of written texts, display both scale-free and small-world properties, such a representation fails in capturing other textual features, such as the organization in topics or subjects. We propose a novel network representation whose main purpose is to capture the semantical relationships of words in a simple way. To do so, we link all words co-occurring in the same semantic context, which is defined in a threefold way. We show that the proposed representations favor the emergence of communities of semantically related words, and this feature may be used to identify relevant topics. The proposed methodology to detect topics was applied to segment selected Wikipedia articles. We found that, in general, our methods outperform traditional bag-of-words representations, which suggests that a high-level textual representation may be useful to study the semantical features of texts.	bag-of-words model;community;complex network;emergence;high- and low-level;small-world experiment;text segmentation;von neumann universal constructor;wikipedia	Henrique Ferraz de Arruda;Luciano da Fontoura Costa;Diego R. Amancio	2016	Chaos	10.1063/1.4954215	natural language processing;text mining;computer science;artificial intelligence;mathematics;small-world network	AI	-25.081428896001714	-58.541074254388924	171812
a5d03ff4bac9e63457db396a5bd9ca31de100186	automatic categorization of questions from q&a sites	data mining;q a sites;classification algorithms;pattern recognition;bayes	Q&A sites are attracting growing interest of software developers. The categorization of questions in terms of user concerns would open new opportunities to extract valuable information from millions of posts.  This paper presents a comparison between different classification algorithms to find the one that best classifies questions from Q&A sites, such as, Stack Overflow. In the classification process, we used the following classification algorithms: Naive Bayes, Multilayer Perceptron, Support Vector Machine, K-Nearest Neighbors, J4.8 Decision Tree and Random Forests.  We conducted an experimental study with Stack Overflow questions with posts equally divided into three domain categories: How-to-do-it, Need-to-know and Seeking-something. The attributes were extracted from a textual analysis of the title and body of each question. We considered a total of 8 attributes to get the data for each question. We found a classifier with an overall success rate of 84.16% and 92.5% on How-to-do-it category.	categorization;decision tree;experiment;k-nearest neighbors algorithm;multilayer perceptron;naive bayes classifier;random forest;software developer;stack overflow;support vector machine	Eduardo Cunha Campos;Marcelo de Almeida Maia	2014		10.1145/2554850.2555117	statistical classification;computer science;machine learning;pattern recognition;data mining;database;bayes' theorem;world wide web	Web+IR	-21.747290552911057	-57.03897460511257	172267
0616559b91f152be4fcda373a524417f63df7936	a web surfer model incorporating topic continuity	iterative method;web documents;categorisation;page ranking;stochastic process;web pages;red www;probabilidad condicional;information retrieval;probabilite conditionnelle;reseau web;indexing terms;webbase web surfer model topic continuity text categorization web document iterative method probability matrix web intelligence;categorization index terms web intelligence probabilistic surfer history page ranking stochastic processes context identification;metodo iterativo;index terms web intelligence;iterative methods;categorizacion;hierarchical classification;web intelligence;internet;web surfer model;information retrieval internet iterative methods;stochastic processes;methode iterative;processus stochastique;classification hierarchique;world wide web;probabilistic surfer history;proceso estocastico;context identification;history context modeling web pages stochastic processes convergence mathematical model uniform resource locators internet state space methods;conditional probability;clasificacion jerarquizada;categorization	This paper describes a surfer model which incorporates information about topic continuity derived from the surfer's history. Therefore, unlike earlier models, it captures the interrelationship between categorization (context) and ranking of Web documents simultaneously. The model is mathematically formulated. A scalable and convergent iterative procedure is provided for its implementation. Its different characteristic features, as obtained from the joint probability matrix, and their significance in Web intelligence are mentioned. Experiments performed on Web pages obtained from WebBase confirm the superiority of the model.	categorization;iterative method;scalability;scott continuity;stochastic matrix;web intelligence;web page	Sankar K. Pal;B. Lakshmi Narayan;Soumitra Dutta	2005	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2005.69	stochastic process;computer science;artificial intelligence;machine learning;data mining;database;iterative method;world wide web;information retrieval;statistics	DB	-26.363214260995285	-56.84034626825142	172573
3929ab7cf6fd645f80413f3677a3b6cbc58ca097	musical track popularity mining dataset		Music Information Research requires access to real musical content in order to test efficiency and effectiveness of its methods as well as to compare developed methodologies on common data. Existing datasets do not address the research direction of musical track popularity that has recently received considerate attention. Existing sources of musical popularity do not provide easily manageable data and no standardised dataset exists. Accordingly, in this paper we present the Track Popularity Dataset (TPD) that provides different sources of popularity definition ranging from 2004 to 2014, a mapping between different track/ author/ album identification spaces that allows use of all different sources, information on the remaining, non popular, tracks of an album with a popular track, contextual similarity between tracks and ready for MIR use extracted features for both popular and non-popular audio tracks.	application programming interface;global variable;propagation delay;requirement;social network	Ioannis Karydis;Aggelos Gkiokas;Vassilios Katsouros	2016		10.1007/978-3-319-44944-9_50	data mining;internet privacy;world wide web	Web+IR	-25.501136924952704	-53.43404990886872	173157
585b99ba0855102a8c6896454e8777e6e9fdea41	competitive self-training technique for sentiment analysis in mass social media	semi supervised learning;self training technique;support vector machine twitter sentiment analysis semi supervised learning self training technique;sentiment analysis;f measure competitive self training technique sentiment analysis mass social media user emotion analysis twitter semisupervised learning techniques binary mixture perspectives;support vector machine;twitter;data models sentiment analysis analytical models support vector machines accuracy twitter training;social sciences computing data analysis learning artificial intelligence social networking online	"""This paper aims to analyze user's emotion automatically by analyzing Twitter using """"data without sentiment labels"""", not only """"data with sentiment labels"""", to increase accuracy of sentiment analysis through an improved Self-Training, one of Semi-supervised learning techniques. Self-Training has a weak point that a classification mistake can reinforce itself. Self-Training iteratively modifies the model based on the output of the model. Thus, if the model generates wrong output, the model can be wrongly modified. For alleviate this weak point, we propose a competitive Self-Training technique. We create three models based on the output of the model and choose the best. Three models are created by binary mixture perspectives: the threshold, the same number, and the maximum number for updates. We repeat step that creating model and choosing a best model highest to get F-measure. Finally, we can improve the performance of sentiment analysis model."""	effective method;f1 score;mixture model;semi-supervised learning;sentiment analysis;social media;supervised learning	Sola Hong;Jaedong Lee;Jee-Hyong Lee	2014	2014 Joint 7th International Conference on Soft Computing and Intelligent Systems (SCIS) and 15th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2014.7044857	support vector machine;computer science;data science;machine learning;data mining;sentiment analysis	NLP	-20.27708494357647	-56.896298976706355	173500
5c173a81e123513d4de9ea84d26106c81b658719	social event detection and retrieval in collaborative photo collections	retrieval;event detection;web service;classification;photo collections;photo collection;visual features;experience base	In this paper, we present an approach to detect social events and retrieve associated photos in collaboratively annotated photo collections. We combine data of various modalities such as time, location, and textual and visual features within a framework that has a classification model at its core. Compared to traditional approaches that mainly consider the photos only as a source of information, we also incorporate external information from datasets and online web services to further improve the performance. Experiments based on the MediaEval Social Event Detection Dataset demonstrate the effectiveness of our approach.	benchmark (computing);experiment;fits;flickr;information source;sed;social media;venue (sound system);web service	Markus Brenner;Ebroul Izquierdo	2012		10.1145/2324796.2324823	web service;biological classification;computer science;multimedia;world wide web;information retrieval	Web+IR	-26.007800860985753	-52.259576505414216	173847
ad20a7f7cf010b050cccab2828fe958befac79e3	early detection of promotion campaigns in community question answering		As is the case with many social media websites, the Community Question Answering (CQA) portal has become a target for spammers to disseminate promotion information. Previous works mainly focus on identifying low-quality answers or detecting spam information in question-answer (QA) pairs. However, these works suffer from long delay since they all rely on the information of answers or answerers while questions have been displayed on the websites for some time and attracted certain user traffic. As a matter of fact, spammers on CQA platforms also act as questioners and involve promotion information in their questions. So if they can be detected as early as possible, the questions will not appear on the websites and affect legitimate users. In this paper, we design a framework for early detection of promotion campaigns in CQA based on only question information and questioner profile. First, we propose a novel sampling method for identifying the questions that contain promotion information, which compose the positive dataset. We also sample an unlabeled dataset of unsolved questions during a certain period of time. Then, we compare the characteristics of question information and user profiles between the two datasets, which are also used as features in the learning process. Finally, we apply and compare several PU (Positive and Unlabeled examples) learning algorithms to find positive examples in the unlabeled dataset. In our approach, no answer side information is needed, which means that it can detect spamming activities as soon as the question is posted. Experimental results based on about 0.7 million questions derived from a popular Chinese CQA portal indicate that our approach can detect questions related to promotion campaigns as effectively as but more efficiently than the state-of-the-art QA pair level detection methods.	question answering	Xin Li;Yiqun Liu;Min Zhang;Shaoping Ma	2016		10.1007/978-981-10-2993-6_15	public relations;data mining;information retrieval	NLP	-22.900818499921705	-53.32336082266825	173953
385b18d92ec2395c203f66a76b86afca9844874a	tweet factors influencing trust and usefulness during both man-made and natural disasters		To this date, research on crisis informatics has focused on the detection of trust in Twitter data through the use of message structure, sentiment, propagation and author. Little research has examined the usefulness of these messages in the crisis response domain. Toward detecting useful messages in case of crisis, in this paper, we characterize tweets, which are perceived useful or trustworthy, and determine their main features. Our analysis is carried out on two datasets (one natural and one man made) gathered from Twitter concerning hurricane Sandy in 2012 and the Boston Bombing 2013. The results indicate that there is a high correlation and similar factors (support for the victims, informational data, use of humor and type of emotion used) influencing trustworthiness and usefulness for both disaster types. This could have impacts on how messages from social media data are analyzed for use in crisis response.	disaster informatics;sensor;social media;software propagation;trust (emotion);trustworthy computing	Shane Errol Halse;Andrea H. Tapia;Anna Cinzia Squicciarini;Cornelia Caragea	2016			knowledge management;natural disaster;computer science	HCI	-21.351710704084972	-54.615361251583295	174588
96216f89d7af4d16e541f8574f350c17571347d5	learning the sentiment of soccer fans from data on bets and social nets		In this paper we propose a Hidden Markov Model in order to predict the sentiment of soccer fans based on information regarding the result of matches. The model was constructed by data collected from a social network where fans of a soccer team periodically expressed feelings towards their team. We show that the choice of a HMM is justified due to the fact that the change in a fanu0027s sentiment is analogous to a Markovian process of change of state through time. A comparative evaluation will be performed between variations of the proposed models and also between the most accurate of them and classification algorithms. Second order HMM, considering the match results and fanu0027s gambling information, is the most accurate model.	social network	Rafael Aiello Bomfim;Vasco Furtado	2017	Web Intelligence	10.3233/WEB-170355	data modeling;simulation;speech recognition;variable-order bayesian network;computer science;machine learning;markov process;television;sentiment analysis;statistics;social network	AI	-19.76042692031596	-57.62786119111615	174872
4ed73089ecd5a3d65e3aabe4f1c84444dec92ff7	an enhanced topic modeling approach to multiple stance identification		People often publish online texts to express their stances, which reflect the essential viewpoints they stand. Stance identification has been an important research topic in text analysis and facilitates many applications in business, public security and government decision making. Previous work on stance identification solely focuses on classifying the supportive or unsupportive attitude towards a certain topic/entity. The other important type of stance identification, multiple stance identification, was largely ignored in previous research. In contrast, multiple stance identification focuses on identifying different standpoints of multiple parties involved in online texts. In this paper, we address the problem of recognizing distinct standpoints implied in textual data. As people are inclined to discuss the topics favorable to their standpoints, topics thus can provide distinguishable information of different standpoints. We propose a topic-based method for standpoint identification. To acquire more distinguishable topics, we further enhance topic model by adding constraints on document-topic distributions. We finally conduct experimental studies on two real datasets to verify the effectiveness of our approach to multiple stance identification.	text corpus;topic model	Junjie Lin;Wenji Mao;Yuhao Zhang	2017		10.1145/3132847.3133145	management science;data mining;topic model;viewpoints;computer science;publication;machine learning;government;text mining;artificial intelligence	NLP	-21.74006367272424	-55.6736663432712	175994
f0b6da5f31133d8197ba579cdfc1d2786e1e846b	finding the needle in a haystack: who are the most central authors within a domain?	semantic similarity;co citation;2 mode multilayered graph;conferenceobject;learning analytics;co authorship	The speed at which new scientific papers are published has increased dramatically, while the process of tracking the most recent publications having a high impact has become more and more cumbersome. In order to support learners and researchers in retrieving relevant articles and identifying the most central researchers within a domain, we propose a novel 2-mode multilayered graph derived from Cohesion Network Analysis (CNA). The resulting extended CNA graph integrates both authors and papers, as well as three principal link types: coauthorship, co-citation, and semantic similarity among the contents of the papers. Our rankings do not rely on the number of published documents, but on their global impact based on links between authors, citations, and semantic relatedness to similar articles. As a preliminary validation, we have built a network based on the 2013 LAK dataset in order to reveal the most central authors within the emerging Learning Analytics domain.	co-citation;netware;scientific literature;semantic similarity	Ionut Cristian Paraschiv;Mihai Dascalu;Danielle S. McNamara;Stefan Trausan-Matu	2016		10.1007/978-3-319-45153-4_79	semantic similarity;computer science;artificial intelligence;data science;data mining;management;world wide web	AI	-25.944533280661993	-57.788261290700426	176234
9b6481dd74f3f245332eeceeedf32735bedc88bc	exploring emergent semantic communities from dblp bibliography database	small community;complex networks;giant community;emergent semantic community;digital bibliography library project;network evolution;information retrieval;complex network;community discovery algorithm;text analysis bibliographic systems complex networks information retrieval;dblp bibliography record;cpm algorithm;text analysis;bibliographies databases complex networks software libraries computer science social network services web server frequency clustering algorithms information analysis;clique percolation method complex network semantic community evolution;data mining;word association network;cpmw algorithm;semantic community;emergent semantics;image edge detection;clique percolation method;feature extraction;clique percolation method emergent semantic community dblp bibliography database word association network dblp bibliography record community discovery algorithm cpm algorithm cpmw algorithm complex network giant community small community network evolution digital bibliography library project;continuous phase modulation;dblp bibliography database;bibliographic systems;correlation;communities;data models;evolution	In this paper, we construct a word association network from DBLP bibliography records and detect its evolution progress based on community discovery algorithm CPM and CPMw. We found that the network is of complex network characteristics, and the detected semantic communities can be classified into two categories: giant community and small community. They differ in size and content, and behave differently in the network evolution. Discovering the evolution of network and the emergent semantic communities can help researchers grasp the state of arts of the related field, identify emergent issues and thus inspire new idea to solve scientific questions.	algorithm;complex network;concurrence (quantum computing);dbl-browser;digital library;emergent;library (computing);scientific literature	Zhixing Huang;Yan Yan;Yuhui Qiu;Shuqiong Qiao	2009	2009 International Conference on Advances in Social Network Analysis and Mining	10.1109/ASONAM.2009.6	text mining;computer science;artificial intelligence;data science;machine learning;data mining;world wide web;complex network	ML	-26.0123065789176	-56.240046603654505	176828
5d0bc7b680e60c8f358372dab5845d3357e90f5b	sampling social media: supporting information retrieval from microblog data resellers with text, network, and spatial analysis		This paper presents a computationally assisted method for scaling researcher expertise to large, online social media datasets in which access is constrained and costly. Developed collaboratively between social and computer science researchers, this method is designed to be flexible, scalable, cost-effective, and to reduce bias in data collection. Online response to six case studies covering elections and election-related violence in Sub-Saharan African countries are explored using Twitter, a popular online microblogging platform. Results show: 1) automated query expansion can mitigate researcher bias, 2) machine learning models combining textual, social, temporal, and geographic features in social media data perform well in filtering data unrelated to the target event, and 3) these results are achievable while minimizing fee-based queries by bootstrapping with readily-available Twitter samples.		Cody Buntain;Erin McGrath;Brandon Behlendorf	2018			microblogging;data collection;computer science;information retrieval;sampling (statistics);social media	NLP	-24.374942935177824	-53.94868737639254	179517
4ff84f1026e71262277427205a6c115e803fec4a	p2v: effective website fingerprinting using vector space representations	web pages;web sites computer crime data privacy learning artificial intelligence;web pages feature extraction servers context mathematical model fingerprint recognition computational modeling;servers;computational modeling;fingerprint recognition;feature extraction;mathematical model;context;p2v word vector representations packet to vector approach features extraction web page destination online activists internet users anonymous communication users navigation privacy passive traffic analysis attack cyber security natural language processing machine learning real valued vector vsm language vector space models vector space representations web site fingerprinting attack	Language vector space models (VSMs) have recently proven to be effective across a variety of tasks. In VSMs, each word in a corpus is represented as a real-valued vector. These vectors can be used as features in many applications in machine learning and natural language processing. In this paper, we study the effect of vector space representations in cyber security. In particular, we consider a passive traffic analysis attack (Website Fingerprinting) that threatens users' navigation privacy on the web. By using anonymous communication, Internet users (such as online activists) may wish to hide the destination of web pages they access for different reasons such as avoiding tyrant governments. Traditional website fingerprinting studies collect packets from the users' network and extract features that are used by machine learning techniques to reveal the destination of certain web pages. In this work, we propose the packet to vector (P2V) approach where we model website fingerprinting attack using word vector representations. We show how the suggested model outperforms previous website fingerprinting works.	computer security;fingerprint (computing);machine learning;natural language processing;text corpus;traffic analysis;vii;web page;word embedding	Khaled Al-Naami;Gbadebo Ayoade;Asim Siddiqui;Nicholas Ruozzi;Latifur Khan;Bhavani M. Thuraisingham	2015	2015 IEEE Symposium Series on Computational Intelligence	10.1109/SSCI.2015.19	computer science;data mining;internet privacy;world wide web	Security	-19.92143826952253	-56.10466952504757	179694
c70e92ac841b4382480493d87f463ae200f27212	predicting churn of expert respondents in social networks using data mining techniques: a case study of stack overflow	social network services data mining classification algorithms prediction algorithms logistics measurement algorithm design and analysis;churn prediction;random forests expert respondents social networks data mining techniques stack overflow q a logistic regression neural networks support vector machines;data mining;social networks;data mining churn prediction social networks;support vector machines data mining logistics neural nets regression analysis social networking online	In Q&A social networks, the few respondents that answer most of the questions are an asset to that network. Being able to predict the churn of these expert respondents will enable the owners of such network put things in place in order to keep them. In this paper, we predicted the churn of expert respondents in Stack Overflow. We identified experts based on the InDegree of the respondents and the value of the incentives earned by these experts from the questions they have answered in the past. Using four data mining techniques: logistic regression, neural networks, support vector machines and random forests, we predicted user churn and evaluated our results with four evaluation metrics: percentage correctly classified, area under receiver operating characteristic curve, precision and recall. Of the four data mining algorithms, random forests performed best with PCC of 76%, ROC area of 0.82, precision of 0.76 and recall of 0.77.	algorithm;artificial neural network;data mining;directed graph;logistic regression;portable c compiler;precision and recall;random forest;receiver operating characteristic;social network;stack overflow;support vector machine	Ifeoma Adaji;Julita Vassileva	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.120	computer science;data science;machine learning;data mining;social network	ML	-19.996483103565172	-52.65228221668535	179715
a50e84c7724bff8be8f5a99fc396ed8999deda97	tag recommendations based on tracking social bookmarking systems	search engine;social bookmarking;information retrieval;document retrieval	The purpose of this paper is to describe our approach to Tag Recommendation Task during ECML PKDD 2009 Challenge. The organizers supplied a training set of tagged webpages and publications from BibSonomy portal. Our goal was to build a model which can predict tags for new users bookmarking digital resources. Our strategy was based on an assumption that users tend to tag the same resources in various systems. Therefore, have we developed a tracking engine, which was adjusted to the profile of BibSonomy users in selection of RSS feeds and utilized the training data to optimize the list of tracked URLs. We had over 90 days to collect the data from the feeds, but this period did not overlap with the dates of posts from the training set. As a result we had to set manually parameters responsible for a trade-off between recall and accuracy of the model. We stored all downloaded feed entries in a searching engine. The recommendation was based on tags attached to the documents retrieved from the engine by means of typical information retrieval query.	ecml pkdd;information retrieval;rss;recommender system;tag (metadata);test set	Szymon Chojnacki	2009			computer science;data mining;world wide web;information retrieval	Web+IR	-25.86736692846788	-53.12554450858902	180507
296bff007f69d398f3272e7dba2053a878b2bf3a	deep learning for financial sentiment analysis on finance news providers	forecasting;text mining;investment;media;machine learning;share prices	Investors have always been interested in stock price forecasting. Since the development of electronic media, hundreds pieces of financial news are released on different media every day. Numerous studies have attempted to examine whether the stock price forecasting through text mining technology and machine learning could lead to abnormal returns. However, few of them involved the discussion on whether using different media could affect forecasting results. Financial sentiment analysis is an important research area of financial technology (FinTech). This research focuses on investigating the influence of using different financial resources to investment and how to improve the accuracy of forecasting through deep learning. The experimental result shows various financial resources have significantly different effects to investors and their investments, while the accuracy of news categorization could be improved through deep learning.	categorization;deep learning;machine learning;sentiment analysis;text mining	Min-Yuh Day;Chia-Chou Lee	2016	2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1109/ASONAM.2016.7752381	text mining;media;actuarial science;forecasting;investment;computer science;machine learning;data mining;statistics	SE	-20.352249603946802	-52.90149697602904	180773
3cf0f1da8fe84ae8eb60e3d9c17d92048b97cd57	location-based event search in social texts	social network services;event detection;media;social network services event detection media context tornadoes estimation text recognition;estimation;text analysis query processing social networking online;text recognition;tornadoes;context;event query location based event search social media sites social media texts social media data keyword based search method geotagged users geotagged texts spatial search	A tremendous amount of information is being shared everyday on social media sites such as Facebook, Twitter, or Google+. Often times these social media texts include information that are valuable to others, such as activities (e.g., art fairs, jazz festivals, and gatherings), natural disaster occurrences (e.g., tornadoes, earthquakes), or incidents (e.g., traffic jams). However, current search on social media data is mostly keyword or hashtag based. The keyword based search method does not allow efficient search of events. In order to detect, scan, and search location based events from social media, users and social texts need to be effectively geotagged. Based on geotagged users and texts, effective and efficient algorithms need to be developed to integrate key word and spatial search to allow event query. This position paper summarize the current work, challenges, and research needs in location-event search in social texts.	algorithm;geotagging;google+;hashtag;location-based service;sensor;social media;social network	Yan Huang;Zhi Liu;Phuc Nguyen	2015	2015 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2015.7069425	social media optimization;computer science;data mining;internet privacy;world wide web	AI	-24.390418655144558	-53.47917165557725	181985
16c0fa88984db692721b1a21b4152d9e0a34e7f3	jitter search: a news-based real-time twitter search interface		In this demo we show how we can enhance real-time microblog search by monitoring news sources on Twitter. We improve retrieval through query expansion using pseudo-relevance feedback. However, instead of doing feedback on the original corpus we use a separate Twitter news index. This allows the system to find additional terms associated with the original query to find more “interesting” posts.	real-time transcription	Flávio Martins;João Magalhães;James P. Callan	2016		10.1007/978-3-319-30671-1_77	real-time computing;internet privacy;world wide web	AI	-26.320501061632584	-54.17015934639779	182134
a5fd142ca02336e809fd327ac18ab294a78a4814	twitter location (sometimes) matters: exploring the relationship between georeferenced tweet content and nearby feature classes	natural language processing data mining;data mining;mobile microblogging;openstreetmap;natural languageprocessing;correlation between location and content mobile microblogging natural language processing data mining twitter openstreetmap;twitter;correlation between location and content	In this paper, we investigate whether microblogging texts (tweets) produced on mobile devices are related to the geographical locations where they were posted. For this purpose, we correlate tweet topics to areas. In doing so, classified points of interest from OpenStreetMap serve as validation points. We adopted the classification and geolocation of these points to correlate with tweet content by means of manual, supervised, and unsupervised machine learning approaches. Evaluation showed the manual classification approach to be highest quality, followed by the supervised method, and that the unsupervised classification was of low quality. We found that the degree to which tweet content is related to nearby points of interest depends upon topic (that is, upon the OpenStreetMap category). A more general synthesis with prior research leads to the conclusion that the strength of the relationship of tweets and their geographic origin also depends upon geographic scale (where smaller scale correlations are more significant than those of larger scale).	geolocation;machine learning;mobile device;openstreetmap;point of interest;statistical classification;unsupervised learning	Stefan Hahmann;Ross Purves;Dirk Burghardt	2014	J. Spatial Information Science	10.5311/JOSIS.2014.9.185	geography;computer science;data mining;internet privacy;world wide web	Web+IR	-25.10577422218656	-52.61610330423677	183070
db8f52cc9341543e6b6c0b45032275e31eb09431	lexico semantic patterns for customer intentions analysis of microblogging	manuals;social networking online cognition data mining ontologies artificial intelligence sentiment analysis;owl;ontologies semantics twitter syntactics owl feature extraction manuals;semantics;cognitive computing customer intentions ontologies semantic patterns microblogging;syntactics;feature extraction;ontologies;ontologies lexico semantic patterns customer intention analysis twitter microblog posts user intention mining semantic techniques user intention analysis semantic level customer intention detection semantic patterns;twitter	Micro blogs, forums, such as Twitter constitute a powerful medium today that people use to express their thoughts and intentions. In order to take advantage of their phenomenon, user intentions analysis becomes a major contemporary challenge. Due to the nature of micro blogs posts, mining user intentions is still a challenging task. In this paper, we investigate semantic techniques which facilitate the user intentions analysis process at semantic level. We propose a generalized approach to address their issues. We conduct a case study of user intentions in the commercial field. Our experimental results show the importance and effectiveness of customer intentions detection using semantic patterns and ontologies.	baseline (configuration management);blog;interaction technique;lexico;national lidar dataset;ontology (information science)	Mohamed Hamroun;Mohamed Salah Gouider;Lamjed Ben Said	2015	2015 11th International Conference on Semantics, Knowledge and Grids (SKG)	10.1109/SKG.2015.40	semantic computing;feature extraction;computer science;knowledge management;ontology;artificial intelligence;data mining;semantics;world wide web;sentiment analysis	AI	-22.210474381139118	-57.22929435296177	183219
9730d3d3b0557034b7381000a7b6222d60d1eaa5	exploring significant interactions in live news		News monitoring is of interest to detect current news and track developing stories, but also to explore what is being talked about. In this article, we present an approach to monitoring live feeds of news articles and detecting significant (co-)occurrences of terms compared to a learning background corpus. We visualize the result as a graphstructured semantic word cloud that uses a stochastic neighbor embedding (SNE) based layout and visualizes edges between related terms. We give visual examples of our prototype that processes news as they are crawled from dozens of news sites.	interaction;prototype;sensor;tag cloud	Erich Schubert;Andreas Spitz;Michael Gertz	2018			environmental science	Web+IR	-25.343805761683875	-53.080923548473194	184585
7915b4cf81f991436be506a1c48bd5725f59a9f6	social summarization via automatically discovered social context		Heavy research has been done in recent years on tasks of traditional summarization. However, social context, which is critical in building high-quality social summarizer for web documents, is usually neglected. To address this issue, we propose a novel summarization approach based on social context. In this approach, social summarization is implemented by first employing the tripartite clustering algorithm to simultaneously discover document context and user context for a specified document. Then sentence relationships intra and inter documents plus intended user communities are taken into account to evaluate the significance of each sentence in different context views. Finally, a few sentences with highest overall scores are selected to form the summary. Experimental results demonstrate the effectiveness of the proposed approach and show the superior performance over several baselines.	algorithm;automatic summarization;baseline (configuration management);cluster analysis;hyperlink;mathematical optimization;web page	Po Hu;Cheng Sun;Longfei Wu;Dong-Hong Ji;Chong Teng	2011			multi-document summarization;computer science;automatic summarization;data mining;world wide web;information retrieval	AI	-26.06589623911946	-58.3957471160348	184987
e778c279a2b2825b5bf39b29dc105414981a12ca	image spam — ascii to the rescue!	software;unsolicited e mail image classification security of data;art;electronic mail;antispam techniques;optical filters;training;bayesian methods;image classification;ascii;bayesian methods computer science art image converters drives unsolicited electronic mail optical character recognition software optical filters character recognition mathematics;image spam detection;unsolicited e mail;antispam techniques ascii image spam detection;information filters;security of data;spam detection	We take an unorthodox approach to image spam detection, by applying existing software and decades-old technology: ASCII art. Our technique is straightforward and gets good levels of detection over a corpus with 1159 ham and 1492 spam images, with a tolerable amount of misclassifications. Furthermore, we only look at the images themselves, meaning that this method can be trivially enhanced by combining it with existing anti-spam techniques.	ascii art;anti-spam techniques;text corpus	Jordan Nielson;Daniel Medeiros Nunes de Castro;John Aycock	2008	2008 3rd International Conference on Malicious and Unwanted Software (MALWARE)	10.1109/MALWARE.2008.4690859	computer science;data mining;internet privacy;world wide web	SE	-19.39145041782678	-58.19568128931891	186129
ea037ddce31fed4797ececd34ab46dba7f665200	using the collective intelligence of sports fans to improve professional football league customer service	text mining;sports marketing;collective intelligence sports marketing critical incident techniques text mining cluster analysis;cluster analysis;trees mathematics customer services data mining ontologies artificial intelligence sport text analysis;fans games ontologies decision support systems text mining conferences computers;proceedings paper;critical incident techniques;collective intelligence;professional sports services collective intelligence professional football league customer service sports fan emotions sports fan feelings professional football game crowd interactions critical incident surveys salient positive phrases salient negative phrases football fan game experiences text mining technique data mining techniques ontological base ontology tree structure formal ontology structure green bay packers fan emotions green bay packers fan feelings event related incidences ontology schema green bay packers fan deep opinions	This research investigates sports fans' emotions and feelings toward a professional football game as a unique event offering related services, crowd interactions, performances, incidences, and outcomes. Critical incident surveys (open-ended, written text dialogues) were used to identify the most salient positive and negative phrases used to express football fans' game experiences. The key term frequencies were first analyzed using text and data mining techniques to form the ontological base (with a focus on emotions, feeling, and events); second by building a theory based ontology tree structure; and third by experts abstracting the dialogues into consistent key terms and phases related to a formal ontology structure. The collective intelligence of 37 Green Bay Packers fans' emotions, feelings, event related incidences, and outcomes were mapped to the derived ontology schema which in turn was re-submitted to the text mining algorithms. The ontology based, collective findings depicts the Green Bay Packers fans' deep opinions. Given the structured text data results, clusters form a theoretical base for creating initial causal models for future verification. The initial research provides a new means for effectively improving professional sports services, particularly in defining the interrelation of feelings, emotions, events, and the related object properties.	algorithm;causal filter;collective intelligence;data mining;experience;formal ontology;interaction;nonlinear gameplay;performance;structured text;text corpus;text mining;tree structure	Charles V. Trappey;Peter Smith;Shelby Trappey;Lynn W. L. Chen;Jasmine T. C. Tung	2014	Proceedings of the 2014 IEEE 18th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2014.6846861	text mining;simulation;computer science;knowledge management;artificial intelligence;marketing;multimedia;collective intelligence;cluster analysis;sports marketing;management	AI	-22.88316804990728	-56.21342265615435	186636
fef312ae1eaaa2312b130ba1910506405e1aa990	mobile service for reputation extraction from weblogs - public experiment and evaluation		In this paper, we introduce a mobile service that extracts reputations of a product from weblogs by cellular phones during shopping. If the user takes a photo of a product barcode on the package with a cellular phone camera, Ubiquitous Metadata Scouter first gets the product metadata (name, manufacturer, etc.) from the internet and collects blogs that review the product. Also, it analyzes the blog contents with NLP techniques and ontologies. Then, it indicates the overall reputation (positive or negative), and other related products that are the subject of much discussion in the blogs. This paper illustrates each function of this service and a public experiment and evaluation at a real consumer electronics store and book-store in Tokyo in March 2006.	barcode;blog;camera phone;mobile phone;natural language processing;ontology (information science)	Takahiro Kawamura;Shinichi Nagano;Masumi Inaba;Yumiko Mizoguchi	2007				NLP	-23.13964821379526	-56.80422700058439	187010
69968aa1deca444598efa968973dbd19f848069e	learning to geolocalise tweets at a fine-grained level		Fine-grained geolocation of tweets has become an important feature for reliably performing a wide range of tasks such as real-time event detection, topic detection or disaster and emergency analysis. Recent work adopted a ranking approach to return a predicted location based on content-based similarity to already available individual geotagged tweets. However, this work made use of the IDF weighting model to compute the ranking, which can diminish the quality of the Top-N retrieved tweets. In this work, we adopt a learning to rank approach towards improving the effectiveness of the ranking and increasing the accuracy of fine-grained geolocalisation. To this end we propose a set of features extracted from pairs of geotagged tweets generated within the same fine-grained geographical area (squared areas of size 1 km). Using geotagged tweets from two cities (Chicago and New York, USA), our experimental results show that our learning to rank approach significantly outperforms previous work based on IDF ranking, and improves accuracy of tweet geolocalisation at a fine-grained level.		Jorge David Gonzalez Paule;Yashar Moshfeghi;Craig MacDonald;Iadh Ounis	2018		10.1145/3269206.3269291	learning to rank;data mining;information retrieval;computer science;geolocation;ranking;weighting	Web+IR	-24.590630065935	-53.108207642160544	187685
8946c7b546a8e6802e8581c7bd072b18752f6035	a new spatio-temporal method for event detection and personalized retrieval of sports video	temporal sequential patterns;event detection;data mining;personalized retrieval;association rule mining;temporal information;sports video;web casting text;temporal pattern	In this paper, a new spatio-temporal method for adaptively detecting events based on Allen temporal algebra and external information support is presented. The temporal information is captured by presenting events as the temporal sequences using a lexicon of non-ambiguous temporal patterns. These sequences are then exploited to mine undiscovered sequences with external text information supports by using class associate rules mining technique. By modeling each pattern with linguistic part and perceptual part those work independently and connect together via transformer, it is easy to deploy this method to any new domain (e.g baseball, basketball, tennis, etc.) with a few changes in perceptual part and transformer. Thus the proposed method not only can work well in unwell structured environments but also can be able to adapt itself to new domains without the need (or with a few modification) for external re-programming, re-configuring and re-adjusting. Results of automatic event detection progress are tailored to personalized retrieval via click-and-see style using either conceptual or conceptual-visual query scheme. Experimental results carried on more than 30 hours of soccer video corpus captured at different broadcasters and conditions as well as compared with well-known related methods, demonstrated the efficiency, effectiveness, and robustness of the proposed method in both offline and online processes.	algorithm;database;event (computing);hash function;lexicon;online and offline;personalization;sensor;transformer	Minh-Son Dao;Noboru Babaguchi	2009	Multimedia Tools and Applications	10.1007/s11042-009-0379-4	computer vision;association rule learning;computer science;machine learning;data mining;multimedia;world wide web;computer security	Web+IR	-26.017447759580396	-54.343103921528126	187711
7c02d6f9b262c2c09b48061961c1e2d839deaf0a	a machine learning approach for classifying textual data in crowdsourcing	text mining;machine learning;automatization;crowdsourcing	Crowdsourcing represents an innovative approach that allows companies to engage a diverse network of people over the internet and use their collective creativity, expertise, or workforce for completing tasks that have previously been performed by dedicated employees or contractors. However, the process of reviewing and filtering the large amount of solutions, ideas, or feedback submitted by a crowd is a latent challenge. Identifying valuable inputs and separating them from low quality contributions that cannot be used by the companies is time-consuming and cost-intensive. In this study, we build upon the principles of text mining and machine learning to partially automatize this process. Our results show that it is possible to explain and predict the quality of crowdsourced contributions based on a set of textual features. We use these textual features to train and evaluate a classification algorithm capable of automatically filtering textual contributions in crowdsourcing.	algorithm;baseline (configuration management);crowdsourcing;feedback;machine learning;sensitivity and specificity;text corpus;text mining	Marcel Rhyn;Ivo Blohm	2017			text mining;computer science;data science;machine learning;pattern recognition;data mining;crowdsourcing	Web+IR	-21.101892930786864	-58.15159962739531	188043
652d93bd6e85fefffe6f2e47b35ed3c516d2d889	graph structure in the web: aggregated by pay-level domain	web graph;004 informatik;network analysis;graph analysis;web science;web mining;world wide web	Previous research on the overall graph structure of the World Wide Web mostly focused on the page level, meaning that the graph that directly results from hyperlinks between individual web pages was analyzed. This paper aims to provide additional insights about the macroscopic structure of the World Web Web by analyzing an aggregated version of a recent web graph. The graph covers over 3.5 billion web pages and 128 billion hyperlinks between pages. It was crawled in the first half of 2012. We aggregate this graph by pay-level domain (PLD), meaning that all pages that belong to the same pay-level domain are represented by a single node and that an arc exists between two nodes if there is at least one hyperlink between pages of the corresponding pay-level domains. The resulting PLD graph covers 43 million PLDs and contains 623 million arcs between PLDs. Analyzing this aggregated graph allows us to present findings about linkage patterns between complete websites and not only individual HTML pages. In this paper, we present basic statistics about the PLD graph, such as degree distributions, top-ranked PLDs, distances and diameter. We analyze whether the bow-tie structure introduced by Broder et al. can also be identified in our PLD graph and reveal a backbone of highly interlinked websites within the graph. We group the websites by top-level domain and report findings about the overall linkage within and between different top-level domains. In a last experiment, we use data from the Open Directory Project (DMOZ) to categorize websites by topic and report findings about linkage patterns between websites belonging to different topical categories.	aggregate data;andrei broder;apple open directory;categorization;degree distribution;html;hyperlink;internet backbone;linkage (software);programmable logic device;web page;webgraph;world wide web	Oliver Lehmberg;Robert Meusel;Christian Bizer	2014		10.1145/2615569.2615674	computer science;data mining;world wide web;graph database;information retrieval	Web+IR	-26.075697970534684	-57.50318848741782	188466
c174bb48dd25342860e691566aff2a8777b7b896	applying clustering and ensemble clustering approaches to phishing profiling	phishing;profiling;graph partitioning;clustering;respubid22603;institute of sport exercise and active living iseal;cluster ensembles;consensus functions;1005 communications technologies	This paper describes a novel approach to profiling phishing emails based on the combination of multiple independent clusterings of the email documents. Each clustering is motivated by a natural representation of the emails. A data set of 2048 phishing emails provided by a major Australian financial institution was pre-processed to extract features describing the textual content, hyperlinks and orthographic structure of the emails. Independent clusterings using different techniques were performed on each representation, and these clusterings were then ensembled using a variety of consensus functions. This paper concentrates on using several clustering approaches to determine the most likely number of phishing groups and explores ways in which individual and combined results relate. The approach suggests a number of phishing groups and the structure of the approach can aid the development of profiles based on the individual clusters. The actual profiling is not carried out in this paper.	algorithm;cluster analysis;email;hyperlink;k-means clustering;loss function;mutual information;non-maskable interrupt;optimization problem;orthographic projection;phishing;pure function;reality check (tv series);while	John Yearwood;Dean Webb;Liping Ma;Peter Vamplew;Bahadorreza Ofoghi;Andrei V. Kelarev	2009			phishing;computer science;graph partition;machine learning;data mining;profiling;internet privacy;cluster analysis;world wide web	ML	-19.846960205455662	-55.44475956062428	188516
65735396664f33c3200c4003ccc90e0b5ac88a81	emotion classification of social media posts for estimating people’s reactions to communicated alert messages during crises	data mining and knowledge discovery;information systems and communication service	One of the key factors influencing how people react to and behave during a crisis is their digital or non-digital social network, and the information they receive through this network. Publicly available online social media sites make it possible for crisis management organizations to use some of these experiences as input for their decision-making. We describe a methodology for collecting a large number of relevant tweets and annotating them with emotional labels. This methodology has been used for creating a training dataset consisting of manually annotated tweets from the Sandy hurricane. Those tweets have been utilized for building machine learning classifiers able to automatically classify new tweets. Results show that a support vector machine achieves the best results with about 60% accuracy on the multi-classification problem. This classifier has been used as a basis for constructing a decision support tool where emotional trends are visualized. To evaluate the tool, it has been successfully integrated with a pan-European alerting system, and demonstrated as part of a crisis management concept during a public event involving relevant stakeholders.	decision support system;machine learning;social media;social network;support vector machine	Joel Brynielsson;Fredrik Johansson;Carl Jonsson;Anders Westling	2014	Security Informatics	10.1186/s13388-014-0007-3	simulation;computer science;data science;machine learning;data mining;world wide web;computer security	HCI	-22.328615387859763	-54.90060011272949	188648
349b456eaf69fcaf1715aac264ebed2edc7af5db	listen to your customers: insights into brand image using online consumer-generated product reviews	consumer generated content;text mining;network analysis;brand image;online product reviews	ABSTRACTOnline consumer-generated product reviews are a growing phenomenon and have led to the posting of colossal amounts of data by consumers on the Web. These data include consumers’ thoughts, opinions, and feelings about brands and offer firms the opportunity to “listen in” on consumers to get a better understanding of the topics discussed about their brands. Using the human associative memory model as the theoretical framework, the authors introduce an approach to convert online product reviews into meaningful information about brand images using a novel combination of text mining and network analysis methodologies. Following a network-based understanding of brand image, the authors use online product reviews to extract consumers’ brand associations and their interconnections as well as to depict and characterize the network of brand associations. In an empirical study, the authors test the approach and illustrate its managerial usefulness. The suggested approach allows managers to effectively monito...		Sonja Gensler;Franziska Völckner;Marc Egger;Kai Fischbach;Detlef Schoder	2015	Int. J. Electronic Commerce	10.1080/10864415.2016.1061792	text mining;network analysis;computer science;artificial intelligence;marketing;brand management;brand;multimedia;advertising;brand awareness;world wide web	Web+IR	-21.866746772469014	-52.21175442158874	188733
589bd9bd320a3a8d58296a0809cb2cce3d5e6586	evaluating sentiment in financial news articles	text mining;financial prediction;sentiment analysis;business intelligence	Can the choice of words and tone used by the authors of financial news articles correlate to measurable stock price movements? If so, can the magnitude of price movement be predicted using these same variables? We investigate these questions using the Arizona Financial Text (AZFinText) system, a financial news article prediction system, and pair it with a sentiment analysis tool. Through our analysis, we found that subjective news articles were easier to predict in price direction (59.0% versus 50.0% of chance alone) and using a simple trading engine, subjective articles garnered a 3.30% return. Looking further into the role of author tone in financial news articles, we found that articles with a negative sentiment were easiest to predict in price direction (50.9% versus 50.0% of chance alone) and a 3.04% trading return. Investigating negative sentiment further, we found that our system was able to predict price decreases in articles of a positive sentiment 53.5% of the time, and price increases in articles of a negative	algorithmic trading;futures studies;machine learning;real-time clock;real-time operating system;sentiment analysis	Robert P. Schumaker;Yulei Zhang;Chunneng Huang;Hsinchun Chen	2012	Decision Support Systems	10.1016/j.dss.2012.03.001	text mining;economics;computer science;marketing;data mining;advertising;business intelligence;sentiment analysis;commerce	Web+IR	-20.19658586445094	-52.59174944084491	188783
52ecc23f042f9a078effc76542823a4150289066	the framework of network public opinion monitoring and analyzing system based on semantic content identification	public opinion	Network has become important public platform for the public to express the opinion, to discuss public affairs, to participate in economic social and political life. The spread of information network public opinion geometric progression growth, it is necessary to monitor and analyze network Public Opinion for the government to manage the public opinion information and to timely discover hot spots and to correctly guide public opinion trends. Therefore, Network Public Opinion monitoring and analyzing have become a hot issue in recent years. Now the main mature technology is the statistical analysis based on key word. However, there is still much room for improving its effectiveness. This paper describes a framework of network Public Opinion monitoring and analyzing system based on semantic content identification to solve some key problems of the public opinion.	color gradient;experiment;machine learning;netizen;vocabulary	Xian-Yi Cheng;Ling-ling Zhu;Qian Zhu;Zhongjing Wang	2010	JCIT		public opinion;computer science;data science;data mining	ML	-22.090859173883995	-55.5245180113073	189055
a13996d04b6794e9682893b333c4d6780ec18d8b	interest mining from user tweets	data processing;topic modeling;social networks;keyword keyphrase ranking;twitter;keyword keyphrase extraction	We build a system to extract user interests from Twitter messages. Specifically, we extract interest candidates using linguistic patterns and rank them using four different keyphrase ranking techniques: TFIDF, TextRank, LDA-TextRank, and Relevance-Interestingness-Rank (RI-Rank). We also explore the complementary relation between TFIDF and TextRank in ranking interest candidates. Top ranked interests are evaluated with user feedback gathered from an online survey. The results show that TFIDF and TextRank are both suitable for extracting user interests from tweets. Moreover, the combination of TFIDF and TextRank consistently yields the highest user positive feedback.	learning to rank;positive feedback;relevance;ti-nspire series;tf–idf;topic model	Thuy Vu;Victor Perez	2013		10.1145/2505515.2507883	data processing;computer science;machine learning;data mining;database;topic model;world wide web;information retrieval;social network	Web+IR	-24.832759677300906	-54.047596744250846	189232
8f0fa9b1d78eb9b88e645a75b3ae90f7352480d5	estimating online user location distribution without gps location	google;histograms;measurement;distribution estimation metric online user location distribution offline user location estimation online information tv segment advertising gps information english location description neural language model semantic similarity geodemographic histograms demographic gap gps free user location distribution estimation;location;gps;gps location online social network offline social network;offline social network;global positioning system;user interfaces mobile computing;online social network;tv;twitter;twitter histograms tv global positioning system measurement google sociology;sociology	We focus on the problem of offline user location estimation using online information, particularly for the application of TV segment advertising. Unlike previous works, the proposed method does not assume GPS information, but works with loosely structured information such as English location description. We propose to use a neural language model to capture the semantic similarity among the location descriptions. The language model can help reduce the otherwise expensive geolocating service lookups by internally resolving similar areas, neighborhoods, etc. Onto the same description. We also propose a metric for comparing geodemographic histograms. This metric considers the demographic gap between the online world and the offline world. In the experiments section, we demonstrate the recall and accuracy of our language-based, GPS-free user location distribution estimation. In addition, we illustrate the effectiveness of the proposed distribution estimation metric.	buffalo airstation;buffalo network-attached storage series;cryptographic hash function;direct memory access;experiment;geolocation;geotagging;global positioning system;heuristic;language model;online and offline;semantic similarity;social network;tag (metadata);text-based (computing);v-optimal histograms;windows legacy audio components	Yusheng Xie;Yu Cheng;Ankit Agrawal;Alok N. Choudhary	2014	2014 IEEE International Conference on Data Mining Workshop	10.1109/ICDMW.2014.30	global positioning system;data mining;mathematics;internet privacy;world wide web;statistics	Robotics	-25.87599111394257	-52.912391398544436	189472
e3092733f14058a7aed542c4dbb829e16d7c9818	facility detection and popularity assessment from text classification of social media and crowdsourced data	occupancy analysis;text classification;machine learning;geographic information systems;participatory sensing;crowd sourced data;social media	Advances in technology have continually progressed our understanding of where people are, how they use the environment around them, and why they are at their current location. Having a better knowledge of when various locations become popular through space and time could have large impacts on research fields like urban dynamics and energy consumption. In this paper, we discuss the ability to identify and locate various facility types (e.g. restaurant, airport, stadiums) using social media, and assess methods in determining when these facilities become popular over time. We use standard natural language processing tools and machine learning classifiers to interpret geotagged Twitter text and determine if a user is seemingly at a location of interest when the tweet was sent. On average our classifiers are approximately 85% accurate varying across multiple facility types, with a peak precision of 98%. By using these standard methods to classify unstructured text, geotagged social media data can be an extremely useful tool to better understanding the composition of places and how and when people use them.	crowdsourcing;document classification;geotagging;machine learning;natural language processing;social media	Kevin A. Sparks;Roger G. Li;Gautam S. Thakur;Robert N. Stewart;Marie L. Urban	2016		10.1145/3003464.3003466	social media;computer science;data science;machine learning;data mining;geographic information system;world wide web;information retrieval;cartography	HCI	-23.01903705872408	-54.478733659070855	190431
0e0206e3c7e5bcd6d9e269199d68f13462a50a33	supporting news article understanding by detecting subject-background event relations	support vector machines;information retrieval;frequency measurement;visualization;feature extraction;strips;context	Typically, news articles mention not just one but multiple events. These events can be classified into subject or background events. The former are events that the article is written about, while the latter are additional events referred to in order to explain the background of the subject events (e.g., causal relations, circumstances or the consequences of the main event). Background events are considered to play an important role in helping to understand articles. In this paper, we first propose to classify content of news articles into subject or background event descriptions. In the second part of the paper, we demonstrate a novel solution for improving the news article search. Based on the subject and background relationship structure between events and articles, our method outputs news articles that help with understanding of a given target article.	causality;sensor	Shotaro Tanaka;Adam Jatowt;Katsumi Tanaka	2016	2016 IEEE/WIC/ACM International Conference on Web Intelligence (WI)	10.1109/WI.2016.0044	support vector machine;strips;visualization;feature extraction;computer science;artificial intelligence;data science;machine learning;data mining;information retrieval	Web+IR	-23.085182660606094	-55.816803883138554	192101
50cdf17f45ad118553e99cc4e5e3313397148cf5	detecting spammers with changing strategies via a transfer distance learning method		Social spammers bring plenty of harmful influence to the social networking involving both social network sites and normal users. It is a consensus to detect and filter spammers. Existing social spammer detection approaches mainly focus on discovering discriminative features and organizing these features in a proper way to improve the detection performance, e.g., combining multiple features together. However, spammers are easy to escape being detected by using changing spamming strategies. Various spamming strategies bring differences in data distribution between training and testing data. Thus, previous fixed approaches are difficult to achieve desired performance in real applications. To address this, in this paper, we present a transfer distance learning approach, which combines distance learning and transfer learning to extract informative knowledge underlying training and testing instances in a unified framework. The proposed approach is validated on large real-world data. Empirical experiments results give the evidence that our method is efficient to detect spammers with changing spamming strategies.		Hao Chen;Jun Liu;Yanzhang Lv	2018		10.1007/978-3-030-05090-0_24	discriminative model;artificial intelligence;transfer of learning;machine learning;distance education;social network;computer science;test data;spamming	Vision	-19.313715041348907	-54.783345118106446	192982
5ac72373e2201a263c07423c6f2462663ba975a1	using targeted bayesian network learning for suspect identification in communication networks		This paper proposes a machine learning application to identify mobile phone users suspected of involvement in criminal activities. The application characterizes the behavioral patterns of suspect users versus non-suspect users based on usage metadata such as call duration, call distribution, interaction time preferences and text-to-call ratios while avoiding any access to the content of calls or messages. The application is based on targeted Bayesian network learning method. It generates a graphical network that can be used by domain experts to gain intuitive insights about the key features that can help identify suspect users. The method enables experts to manage the trade-off between model complexity and accuracy using information theory metrics. Unlike other graphical Bayesian classifiers, the proposed application accomplishes the task required of a security company, namely an accurate suspect identification rate (recall) of at least 50% with no more than a 1% false identification rate. The targeted Bayesian network learning method is also used for additional tasks such as anomaly detection, distinction between “relevant” and “irrelevant” anomalies, and for associating anonymous telephone numbers with existing users by matching behavioral patterns.	anomaly detection;audio crossover;automatic call distributor;bayesian network;behavioral pattern;binary prefix;directed acyclic graph;feature engineering;feature selection;graphical user interface;greedy algorithm;identity document forgery;information theory;iteration;machine learning;markov blanket;markov chain;mobile phone;recursion;relevance;subject matter expert turing test;subject-matter expert;telecommunications network;telephone number	Andrea Gruber;I. Ben-Gal	2017	International Journal of Information Security	10.1007/s10207-017-0362-4	information theory;computer security;data mining;metadata;computer science;anomaly detection;suspect;mobile phone;bayesian network;bayesian probability;behavioral pattern	ML	-19.158666021696433	-54.92260201656611	193575
3f645b6ef1a0369a96fdc8e311fdd53830a29bb6	opinion mining in hindi language: a survey		Opinions are very important in the life of human beings. These Opinions helped the humans to carry out the decisions. As the impact of the Web is increasing day by day, Web documents can be seen as a new source of opinion for human beings. Web contains a huge amount of information generated by the users through blogs, forum entries, and social networking websites and so on To analyze this large amount of information it is required to develop a method that automatically classifies the information available on the Web. This domain is called Sentiment Analysis and Opinion Mining. Opinion Mining or Sentiment Analysis is a natural language processing task that mine information from various text forms such as reviews, news, and blogs and classify them on the basis of their polarity as positive, negative or neutral. But, from the last few years, enormous increase has been seen in Hindi language on the Web. Research in opinion mining mostly carried out in English language but it is very important to perform the opinion mining in Hindi language also as large amount of information in Hindi is also available on the Web. This paper gives an overview of the work that has been done Hindi language.	blog;humans;natural language processing;sentiment analysis;web page;world wide web	Richa Sharma;Shweta Nigam;Rekha Jain	2014	CoRR		natural language processing;data mining;world wide web	ML	-22.813688910859717	-57.71230577900328	193600
904d6bcd1e5f80c422bc92690f7eedf1031c8b9b	big data and forensics: an innovative approach for a predictable jurisprudence		Nowadays, it is easy to trace a large amount of information on the Web, to access documents and produce a digital storage. The current work is submitted as an introduction to an innovative system for the investigation about notoriety of Web data which is based on the evaluation of judicial sentences and it is implemented to reduce the duration of all processes. This research also aims to open some new conjoint debates about the study and application of statistical and computational methods to web data on new forensics topics: text mining techniques enable us to obtain information which may be helpful to establish a statistical index in order to describe the quality and the efficiency in terms of law. It is also possible to develop an intelligent system about facts and judgments.	artificial intelligence;big data;computer forensics;text mining;world wide web	Massimiliano Giacalone;Carlo Cusatelli;Angelo Romano;Antonio Buondonno;Vito Santarcangelo	2018	Inf. Sci.	10.1016/j.ins.2017.10.036	forensic science;machine learning;data science;artificial intelligence;big data;data mining;text mining;computer science	Web+IR	-24.242307856800135	-57.11872233908609	193941
daf9ed5dc6c6bad5367d7fd8561527da30e9b8dd	practical lessons from predicting clicks on ads at facebook	audience targeting;logistic regression;online advertising;probability estimation	Online advertising allows advertisers to only bid and pay for measurable user responses, such as clicks on ads. As a consequence, click prediction systems are central to most online advertising systems. With over 750 million daily active users and over 1 million active advertisers, predicting clicks on Facebook ads is a challenging machine learning task. In this paper we introduce a model which combines decision trees with logistic regression, outperforming either of these methods on its own by over 3%, an improvement with significant impact to the overall system performance. We then explore how a number of fundamental parameters impact the final prediction performance of our system. Not surprisingly, the most important thing is to have the right features: those capturing historical information about the user or ad dominate other types of features. Once we have the right features and the right model (decisions trees plus logistic regression), other factors play small roles (though even small improvements are important at scale). Picking the optimal handling for data freshness, learning rate schema and data sampling improve the model slightly, though much less than adding a high-value feature, or picking the right model to begin with.	daily active users;decision tree;logistic regression;machine learning;online advertising;replay attack;sampling (signal processing)	Xinran He;Junfeng Pan;Ou Jin;Tianbing Xu;Bo Liu;Tao Xu;Yanxin Shi;Antoine Atallah;Ralf Herbrich;Stuart Bowers;Joaquin Quiñonero Candela	2014		10.1145/2648584.2648589	online advertising;simulation;computer science;machine learning;data mining;logistic regression;world wide web	ML	-19.67098491213679	-52.166672335054045	195240
3c9e1255bfdc007f9cc6550f0b60d0f4132a9bbe	an extraction and unification methodology for social networks data: an application to public security		Social media are invaded our daily life where millions of users are subscribed. Those online sites provide great tool for users to communicate with others from all over the world, share information, and express their opinion. Unfortunately, this ease of access and availability of information is exploited by malicious users to spread their radical ideas and prepare for terrorist attacks. In this paper, we have proposed a new methodology for data extraction, annotation, and unification in order to identify suspicious content from online social media threatening public security. To the best of our knowledge, there is no specific data collected method from social media devoted to suspicious user profile extraction. Also, implying an expert for data annotation for a security purposes remain a new research task that necessitate expertise and knowledge. In this paper, we tackle this research area by collecting suspicious content from different social media.	malware;social media;social network;sparse matrix;unification (computer science);user profile	Amal Abid;Hanen Ameur;Atika Mbarek;Amal Rekik;Salma Jamoussi;Abdelmajid Ben Hamadou	2017		10.1145/3151759.3151836	data collection;database;world wide web;social network;computer science;social media;data extraction;user profile;unification;ease of access;annotation	HCI	-21.395384096218375	-55.13955083259278	195392
13dce6263b052b35c581845498a96fa276bc8f1c	predicting news coverage of scientific articles		Journalists act as gatekeepers to the scientific world, controlling what information reaches the public eye and how it is presented. Analyzing the kinds of research that typically receive more media attention is vital to understanding issues such as the “science of science communication” (National Academies of Sciences, Engineering, and Medicine 2017), patterns of misinformation, and the “cycle of hype.” We track the coverage of 91,997 scientific articles published in 2016 across various disciplines, publishers, and news outlets using metadata and text data from a leading tracker of scientific coverage in social and traditional media, Altmetric. We approach the problem as one of ranking each day’s, or week’s, papers by their likely level of media attention, using the learning-to-rank model lambdaMART (Burges 2010). We find that ngram features from the title, abstract and press release significantly improve performance over the metadata features journal, publisher, and subjects.	altmetrics;learning to rank;n-gram;science communication;scientific literature;text corpus	Ansel MacLaughlin;John Wihbey;David A. Smith	2018			data mining;internet privacy;computer science	HPC	-20.71287277704005	-53.77206752294575	195434
763d735e5447ff0f577a98167aafaa6ba90221ac	a model for discovering unpopular research interests	probabilistic topic model;会议论文;unpopular topics;popular topics	Traditional topic models illustrate how documents can be modeled as mixtures of topic probability distributions, which provides a simple method for discovering author’s research interests from a collection of documents. However, existing topic models such as the simplest author model [1] and the author-topic model (ATM) [2] mainly detect popular research topics and largely neglect unpopular ones. In these models, general topical words are grouped into the shared word distribution of each topic, but the words contained in each author-specific distribution that best describe the authors’ research interests are not included. Thus, a novel author-topic model for discovering unpopular research interests (URI-ATM) is proposed, which incorporates a new control variable k that takes on different values when the words belong to different types of research topics. In the model, each topic is associated with two classes of word distributions: one is the popular class among all authors, and the other is the author-specific class from which the document comes. After the URI-ATM is explained, a variety of qualitative and quantitative evaluations are performed. The results demonstrate the advantage of our approach over comparative ones.		Shanshan Feng;Jian Cao;Yuwen Chen;Jing Qi	2015		10.1007/978-3-319-25159-2_35	computer science;artificial intelligence;data science;machine learning;data mining;topic model	ML	-25.004130557111576	-58.900809600374416	196162
280e836f4432c8b094340933a5ab2de6aeec9746	describing and classifying post-mortem content on social media		As the quantity of user profiles on social media grows, so does the number of post-mortem profiles. In this paper, we present a computational linguistic analysis of post-mortem social media profiles. Specifically, we provide an analysis of prevs. post-mortem language use, followed by a description of classifiers we developed that can accurately classify the mortality of social media profiles. These results shed initial lights into the ways in which people speak to the dead, and mark a first step toward accurately identifying mortality on a large scale.	social media;user profile	Jialun Aaron Jiang;Jed R. Brubaker	2018			internet privacy;computer science;social media	NLP	-22.14834192399713	-55.21770994654472	196256
728fe423925adf34c1271074f61645be36a1fe9e	a cognitive assistant for risk identification and modeling	risk analysis;semantic document representation;risk-related knowledge;risk-related queries;risk modeling	Economic systems are rife with heterogeneous risk events that have the potential to cause disruption. The diversity of risk types makes it challenging for companies to conduct comprehensive risk analysis for any chosen business opportunity. The current practice is laborious and expensive, involving internal risk analysts and external risk advisory services. In this paper, we present a cognitive system that augments human abilities, with the objective of drastic improvements in the productivity of risk analysis efforts. Our system is provided with a comprehensive risk taxonomy and its textual description along with an extensive corpus of textual data such as news articles. Using a series of textual analysis, knowledge extraction and machine learning techniques, the data corpus is annotated with risk-related information and indexed in a risk store for flexible query and retrieval. Our system interfaces with the risk analyst using a query orchestrator which translates analyst queries that are posed at a high level into lower level queries that are expanded to exploit the system's risk-related knowledge. It also enables formulating a graphical model and assessing the required probabilities; we introduce a particular family of models that can succinctly represent risk events modeled as stochastic processes over a long time horizon. We illustrate how a risk analyst can query the system to build a risk model with the help of a case study.	artificial intelligence;denial-of-service attack;financial risk modeling;graphical model;high-level programming language;it risk management;machine learning;rife;risk aversion;stochastic process;text corpus	Dharmashankar Subramanian;Debarun Bhattacharjya;Ruben Rodriguez Torrado;Jeffrey O. Kephart;Vijil Chenthamarakshan;Jesus Rios	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258091	data mining;knowledge extraction;business opportunity;semantics;computer science;time horizon;cognition;exploit;risk analysis (business);graphical model	ML	-22.02105349428157	-55.946148306704174	197003
49f5c13f2958b4eab818cde8fdaf8f0a3e3c94ec	discovering emerging entities with ambiguous names	knowledge base population;entity disambiguation;emerging entity discovery	Knowledge bases (KB's) contain data about a large number of people, organizations, and other entities. However, this knowledge can never be complete due to the dynamics of the ever-changing world: new companies are formed every day, new songs are composed every minute and become of interest for addition to a KB. To keep up with the real world's entities, the KB maintenance process needs to continuously discover newly emerging entities in news and other Web streams. In this paper we focus on the most difficult case where the names of new entities are ambiguous. This raises the technical problem to decide whether an observed name refers to a known entity or represents a new entity. This paper presents a method to solve this problem with high accuracy. It is based on a new model of measuring the confidence of mapping an ambiguous mention to an existing entity, and a new model of representing a new entity with the same ambiguous name as a set of weighted keyphrases. The method can handle both Wikipedia-derived entities that typically constitute the bulk of large KB's as well as entities that exist only in other Web sources such as online communities about music or movies. Experiments show that our entity discovery method outperforms previous methods for coping with out-of-KB entities (called unlinkable in entity linking).	entity linking;kilobyte;online community;wikipedia	Johannes Hoffart;Yasemin Altun;Gerhard Weikum	2014		10.1145/2566486.2568003	computer science;data mining;entity linking;entity;world wide web;weak entity;information retrieval;sgml entity	Web+IR	-24.868499772970186	-53.76232873066386	197076
7d0ab8168a24df38f7a2e897d1374c4d505979ed	learning to detect event-related queries for web search	events;query intent;temporal query classification	In many cases, a user turns to search engines to find information about real-world situations, namely, political elections, sport competitions, or natural disasters. Such temporal querying behavior can be observed through a significant number of event-related queries generated in web search. In this paper, we study the task of detecting event-related queries, which is the first step for understanding temporal query intent and enabling different temporal search applications, e.g., time-aware query auto-completion, temporal ranking, and result diversification. We propose a two-step approach to detecting events from query logs. We first identify a set of event candidates by considering both implicit and explicit temporal information needs. The next step further classifies the candidates into two main categories, namely, event or non-event. In more detail, we leverage different machine learning techniques for query classification, which are trained using the feature set composed of time series features from signal processing, along with features derived from click-through information, and standard statistical features. In order to evaluate our proposed approach, we conduct an experiment using two real-world query logs with manually annotated relevance assessments for 837 events. To this end, we provide a large set of event-related queries made available for fostering research on this challenging task.	diversification (finance);information needs;machine learning;relevance feedback;sensor;signal processing;time series;web query classification;web search engine	Nattiya Kanhabua;Tu Ngoc Nguyen;Wolfgang Nejdl	2015		10.1145/2740908.2741698	sargable;query optimization;query expansion;web query classification;ranking;computer science;data mining;web search query;world wide web;information retrieval;query language;spatial query	Web+IR	-24.688040439006894	-54.381136138762415	197778
8f745d9528c3bb1bc0909224a2ff2376c3a38ea6	similarity in patient support forums using tf-idf and cosine similarity metrics	measurement;medical computing document handling health care;media;tf idf;indexes;measurement medical services data models informatics conferences media indexes;medical services;healthcare forums;informatics;data annotation patient support forums tf idf cosine similarity metrics healthcare domain health inquiries social media term frequency inverse document frequency;conferences;data models;cosine similarity;healthcare forums tf idf cosine similarity	The IEEE International Conference on Healthcare Informatics 2015 (ICHI 2015) announced a challenge in healthcare domain that concerns the quality of health inquiries on social media. The problem of the challenge is to reduce the repetition of posts for patient support forums. This problem gradually becomes hard to control due to the increase of forum users and lack of research within the forum's older posts. To address this problem we used a model that finds the similarity of forum posts using cosine similarity metric over the term frequency-inverse document frequency (TF-IDF). We applied our model on data that are provided by the challenge committee. We used three graduate students to annotate the data for us and find the agreement vote of similarity. The results of our model using cosine similarity and TF-IDF were improved over existing models that primarily use topic modeling approaches such as Latent dirichlet allocation (LDA), and Latent Semantic Index (LSI).	cosine similarity;informatics;latent dirichlet allocation;social media;tf–idf;topic model	Mohammad Alodadi;Vandana Pursnani Janeja	2015	2015 International Conference on Healthcare Informatics	10.1109/ICHI.2015.99	computer science;data science;data mining;world wide web	SE	-25.82760617165243	-58.792300357930536	198308
e20eb9e12abccaa782cde1bd0bb653706e03bb8b	a visual analytics framework for identifying topic drivers in media events		Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored.	armed conflicts;causality;extraction;flow;frame (physical object);named-entity recognition;relevance;silo (dataset);text corpus;text mining;timeline;visual analytics	Yafeng Lu;Hong Wang;Steven Landis;Ross Maciejewski	2018	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2752166	data science;visual analytics;media event;data mining;computer science;semantic similarity;cultural analytics;social media;scale analysis (statistics);analytics;text mining	Visualization	-24.098723669053104	-55.24667976635198	198328
08e6d0e0adc5747f50e5c672522dfff14b2cb124	analyzing knowledge communities using foreground and background clusters	community evolution;citation analysis;text mining;diffuse background;knowledge communities;clustering;clustering method;prediction model;scientific knowledge	Insight into the growth (or shrinkage) of “knowledge communities” of authors that build on each other's work can be gained by studying the evolution over time of clusters of documents. We cluster documents based on the documents they cite in common using the Streemer clustering method, which finds cohesive foreground clusters (the knowledge communities) embedded in a diffuse background. We build predictive models with features based on the citation structure, the vocabulary of the papers, and the affiliations and prestige of the authors and use these models to study the drivers of community growth and the predictors of how widely a paper will be cited. We find that scientific knowledge communities tend to grow more rapidly if their publications build on diverse information and use narrow vocabulary and that papers that lie on the periphery of a community have the highest impact, while those not in any community have the lowest impact.	cluster analysis;computer cluster;embedded system;predictive modelling;vocabulary	Vasileios Kandylas;S. Phineas Upham;Lyle H. Ungar	2010	TKDD	10.1145/1754428.1754430	text mining;computer science;data science;machine learning;data mining;predictive modelling;cluster analysis;citation analysis;world wide web;sociology of scientific knowledge	Web+IR	-26.142141479263923	-58.017154172286865	199478
