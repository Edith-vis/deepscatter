id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
07ca0f0277f79c87ee165e5088b199617159c341	multivariate ar model based support vector machine for multispectral remote sensing image classification	remote sensing image;high dimensional vector measurement;ar model;remote sensing imaging;svm learning machine;image segmentation;high dimensionality;features extraction;support vector machines;white noise error covariance matrix;thermal mapper;pixel classification;terrain segmentation;schur complement;vector measure;image classification;contextual information;curse of dimensionality;support vector machines support vector machine classification remote sensing image classification autoregressive processes covariance matrix image segmentation multispectral imaging context modeling two dimensional displays;two dimensional displays;time series;statistical model;image texture;time series statistical model;region segmentation;time series geophysical signal processing image segmentation image texture radar imaging remote sensing remote sensing by radar support vector machines terrain mapping;cholesky factorization;remote sensing by radar;moving average;autoregressive moving average;computational modeling;sar;numerical analysis;2d autoregressive model;schur complement matrix;autoregressive processes;time series analysis;multispectral remote sensing image classification;geophysical signal processing;remote sensing;multispectral images;radar imaging;mathematical model;qr matrix;2d univariate time series based imaging;italy;support vector machine classification;2d system parameter matrix;texture information;2d multivariate vector ar time series model;terrain mapping;multivariate ar model;support vector machine;multi spectral;context modeling;image modeling;white noise;multispectral imaging;covariance matrix;lake mulargias region	"""Time series statistical models such as autoregressive moving average (ARMA) were considered useful in describing the texture and contextual information of an remote sensing image. To simplify the computation, we use a two-dimensional (2-D) autoregressive (AR) model instead. In our previous research, the 2-D univariate time series based imaging model was derived mathematically to extract the features for further terrain segmentations. The effectiveness of the model was demonstrated in region segmentation of a multispectral image of the Lake Mulargias region in Italy. Due to the nature of remote sensing images such as SAR (synthetic aperture radar) and TM (Thermal Mapper) which are mostly in multi-spectral image stack format, a 2-D Multivariate Vector AR (ARV) time series model with pixel vectors of multiple elements (i.e. 15 elements in the case of TM+SAR remote sensing) are examined. The 2-D system parameter matrix and white noise error covariance matrix are estimated for further classifications. To compute the time series ARV system parameter matrix and estimate the error covariance matrix efficiently, a new method based on modern numerical analysis is developed by introducing the Schur complement matrix, the QR (orthogonal, upper triangular) matrix and the Cholesky factorizations in the ARV model formulation. As for pixel classification, the powerful Support Vector Machine (SVM) kernel based learning machine is applied in conjunction with the 2-D time series ARV model. The SVM is particularly suitable for the high dimensional vector measurement as the """"curse of dimensionality"""" problem is avoided."""	autoregressive model;cholesky decomposition;computation;curse of dimensionality;multispectral image;numerical analysis;pixel;statistical model;support vector machine;synthetic intelligence;time series;triangular matrix;white noise;xm1219 armed robotic vehicle	Pei-Gee Peter Ho;Chi Hau Chen	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779694	computer vision;time series;pattern recognition;statistics;remote sensing	Robotics	32.11197038847973	-43.10797326313263	160462
c948fc59fce7f6171c043179e34290adf09320e2	using gaussian processes in bayesian robot programming	gaussian processes;robot navigation;machine learning;bayesian programming;gaussian process;robot programming	In this paper, we present an adaptation of Gaussian Processes for learning a joint probabilistic distribution using Bayesian Programming. More specifically, a robot navigation problem will be showed as a case of study. In addition, Gaussian Processes will be compared with one of the most popular techniques for machine learning: Neural Networks. Finally, we will discuss about the accuracy of these methods and will conclude proposing some future lines for this research.	gaussian process;robot	Fidel Aznar Gregori;Francisco Antonio Pujol López;Maria Del Mar Pujol López;Ramón Rizo Aldeguer	2009		10.1007/978-3-642-02481-8_79	robot learning;computer vision;computer science;artificial intelligence;machine learning;gaussian process;inductive programming	ML	36.51058604704476	-39.9874994069755	160901
e4797ddf8450e19f9cb586921310360207872a29	pattern detection with information-based maximum discrimination and error bootstrapping	optimisation;probability;maximum likelihood estimation;maximum likelihood estimation optimisation face recognition entropy learning artificial intelligence probability;pattern detection;face recognition;maximum likelihood detection face detection lighting entropy pattern analysis pattern recognition electrical capacitance tomography computer errors system testing markov processes;visual learning;maximum likelihood detection;optimality criteria;pattern recognition;pattern recognition algorithms pattern detection information based maximum discrimination error bootstrapping visual learning technique information theoretic entropy visual patterns probability model class discrimination maximum likelihood detection face detection facial feature detection optimization criteria;facial features;entropy;learning artificial intelligence;probability model;information theoretic	We have previously introduced a visual learning technique based on information-theoretic entropy. In that approach, positive and negative examples of a class of visual patterns were analyzed to obtain the probability model that best discriminate that class among others. Such models were tested in the context of maximum likelihood detection of faces and facial features. In this paper, we further improve on that technique by using other family of probability model and by extending the optimization criteria to allow for error bootstrapping. The results include a detail analysis of the improvements obtained and a comparison of these pattern recognition algorithms.	algorithm;information theory;mathematical optimization;pattern recognition;visual learning	Antonio Colmenarez;Thomas S. Huang	1998		10.1109/ICPR.1998.711121	facial recognition system;entropy;computer science;machine learning;pattern recognition;probability;maximum likelihood;statistics	Vision	32.69901162357876	-41.51491649282934	166493
33566c26fdc11f34bcbac796753ee8098dcace19	smartannotator: an interactive tool for annotating rgbd indoor images.		RGBD images with high quality annotations in the form of geometric (i.e., segmentation) and structural (i.e., how do the segments are mutually related in 3D) information provide valuable priors to a large number of scene and image manipulation applications. While it is now simple to acquire RGBD images, annotating them, automatically or manually, remains challenging especially in cluttered noisy environments. We present SmartAnnotator, an interactive system to facilitate annotating RGBD images. The system performs the tedious tasks of grouping pixels, creating potential abstracted cuboids, inferring object interactions in 3D, and comes up with various hypotheses. The user simply has to flip through a list of suggestions for segment labels, finalize a selection, and the system updates the remaining hypotheses. As objects are finalized, the process speeds up with fewer ambiguities to resolve. Further, as more scenes are annotated, the system makes better suggestions based on structural and geometric priors learns from the previous annotation sessions. We test our system on a large number of database scenes and report significant improvements over naive low-level annotation tools.	cuboid;display resolution;finalize (optical discs);high- and low-level;interaction;interactivity;pixel	Yu-Shiang Wong;Hung-Kuo Chu;Niloy Jyoti Mitra	2014	CoRR		machine learning;pixel;artificial intelligence;pattern recognition;computer vision;computer science;annotation	Graphics	35.3479588134199	-44.750635607020996	167248
285dde7f68697baae82d01dcf4a7673b79958aa4	classification for sar scene matching areas based on convolutional neural networks		The selection of scene matching areas is a difficult problem in the field of matching guidance. Compared with the traditional methods of matching feature extraction and pattern classification, this letter applies convolutional neural networks (CNN) to the extraction of synthetic aperture radar (SAR) scene matching regions for the first time. First of all, we match the SAR images of the same land taken by satellites from different angles and in different phases, and then automatically label the matching suitability of the images as the output of the network according to the matching results. Next, the digital elevation model data reflecting the elevation information and the SAR image grayscale information are fused as the input to the network. Finally, CNN is used to automatically extract the matching features and classify the suitability of the SAR images. The proposed method avoids the steps of extracting features manually and improves the classification performance of SAR scene matching area. Compared with the support vector machine method, the classification accuracy increases from 86.1% to 93.3%.	artificial neural network;convolutional neural network;digital elevation model;feature extraction;grayscale;support vector machine;synthetic data	Chengliang Zhong;Xiaodong Mu;Xiangchen He;Bichao Zhan;Ben Niu	2018	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2018.2840687	artificial intelligence;computer vision;mathematics;grayscale;elevation;convolutional neural network;feature extraction;support vector machine;synthetic aperture radar;digital elevation model	Vision	33.03876783478688	-44.920096536979955	168024
4005411d2570fa2199c541e212b583c468f6f979	source separation and density estimation by faithful equivariant som	density estimation;source separation	Jack D. Cowan Department of Math University of Chicago Chicago, IL 60637 j-cowan@uchicago.edu We couple the tasks of source separation and density estimation by extracting the local geometrical structure of distributions obtained from mixtures of statistically independent sources. Our modifications of the self-organizing map (SOM) algorithm results in purely digital learning rules which perform non-parametric histogram density estimation. The non-parametric nature of the separation allows for source separation of non-linear mixtures. An anisotropic coupling is introduced into our SOM with the role of aligning the network locally with the independent component contours. This approach provides an exact verification condition for source separation with no prior on the source distributions.	algorithm;learning rule;line source;loose coupling;modeling perspective;nonlinear system;organizing (structure);self-organization;self-organizing map;source separation	Juan K. Lin;Jack D. Cowan;David G. Grier	1996			density estimation;computer science;machine learning;pattern recognition;mathematics;statistics	ML	33.39941696181245	-38.94256466349565	168187
0ff9cf3407eb01bafd54a8ae5946e9ed520702eb	hand gesture recognition: self-organising maps as a graphical user interface for the partitioning of large training data sets	human gesture;computer vision;human hand;appropriate labelled image data;segmented hand;configuration space;graphical user interface;gesture recognition;large training data sets;hand gesture recognition;self-organising maps;labelled set;difficult task;classification system;degree of freedom;graphic user interface;graphical interface;graphical user interfaces;image classification	"""Gesture recognition is a difficult task in computer vision due to the numerous degrees of freedom of a human hand. Fortunately, human gesture covers only a small part of the theoretical """"configuration space"""" of a hand, so an appearance based representation of human gesture becomes tractable. A major problem, however, is the acquisition of appropriate labelled image data from which an appearance based representation can be built. In This work we apply self-organising maps for a visualisation of large amounts of segmented hands performing pointing gestures. Using a graphical interface, an easy labelling of the data set is facilitated. The labelled set is used to train a neural classification system, which is itself embedded in a larger architecture for the recognition of gestural reference to objects."""	cobham's thesis;computer vision;embedded system;gesture recognition;graphical user interface;self-organization;self-organizing map;signal-to-noise ratio	Gunther Heidemann;Holger Bekel;Ingo Bax;Axel Saalbach	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333817	computer vision;computer science;machine learning;gesture recognition;graphical user interface	Vision	37.680262450906426	-39.799879213837954	170671
d204eb34bd93a19ce6e42c2d4a1a1b45cb624d14	automatic radar target recognition using superresolution music 2d images and self-organizing neural network	decision making;image classification;image resolution;neural nets;radar imaging;radar target recognition;anechoic chamber;decision-making system;self-organizing neural network;superresolution music 2d images	The key problem in any decision-making system is to gather as much information as possible about the object or the phenomenon under study. In the case of the radar targets the frequency and angular information is integrated to form a radar image, which has high information content. A supper-resolution technique (MUSIC 2D) is used in the paper in order to reconstruct the target image. A supervised self-organizing neural network was developed to classify the images obtained in this way for ten different radar targets in an anechoic chamber.	algorithm;angularjs;artificial neural network;discretization;emoticon;organizing (structure);radar;self-information;self-organization;super-resolution imaging;supervised learning	Emanuel Radoi;André Quinquis;Felix Totir;Fabrice Pellen	2004	2004 12th European Signal Processing Conference		computer vision;speech recognition;geography;inverse synthetic aperture radar;remote sensing;automatic target recognition	Robotics	33.126737348372295	-44.0981241900684	171151
3bb69a1fa91b3b7c3faf58bcd8784d596d9e3d2b	automatic place detection and localization in autonomous robotics	unsupervised learning;learning;gaussian processes;hidden markov model;probability distribution place detection place localization autonomous robotics recognition learning gaussian mixture model mml em feature extraction hidden markov model;system performance;statistical distributions;recognition;gaussian mixture model;hidden markov models;feature extraction;probability distribution;place detection;robots;mml em;place localization;robotics and automation hidden markov models navigation probability distribution feature extraction data mining detectors computer vision system testing performance evaluation;unsupervised learning expectation maximisation algorithm feature extraction gaussian processes hidden markov models object detection robots statistical distributions;autonomous robotics;autonomous robot;object detection;expectation maximisation algorithm	This paper presents an approach for the simultaneous learning and recognition of places applied to autonomous robotics. While noteworthy results have been achieved with respect to off-line training process for appearance-based navigation, novel issues arise when recognition and learning are simultaneous and unsupervised processes. The approach adopted here uses a Gaussian mixture model estimated by a novel incremental MML-EM to model the probability distribution of features extracted by image-preprocessing. A place detector decides which features belong to which place integrating odometric information and a hidden Markov model. Tests demonstrate that the proposed system performs as well as the ones relying on batch off-line environmental learning.	approximation algorithm;autonomous robot;dynamic problem (algorithms);hidden markov model;machine learning;markov chain;mixture model;online and offline;preprocessor;robotics	Antonio Chella;Irene Macaluso;Lorenzo Riano	2007	2007 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2007.4399614	probability distribution;computer vision;computer science;machine learning;pattern recognition;hidden markov model	Robotics	38.266775914035016	-41.732576862459645	171735
cf936fcf05cd274cbd6726693ce28bbec358eeaa	a fingerprint identification system using adaptive fpga-based enhanced probabilistic convergent network	databases;probability;neural networks;human subject;hardware based system;training;biometrics;enhanced probabilistic convergent network;binarized fingerprint fingerprint identification system enhanced probabilistic convergent network biometric identification biometric verification human subject adaptive fpga based weightless neural network hardware based system;binarized fingerprint;biometric verification;human subjects;adaptive fpga based weightless neural network;qa75 electronic computers computer science;neural chips;artificial neural networks;fingerprint recognition;qa76 87 neural computers;probability field programmable gate arrays fingerprint identification neural chips;fingerprint recognition adaptive systems biometrics neural networks field programmable gate arrays image databases spatial databases prototypes bifurcation nasa;neurons;field programmable gate arrays;biometric identification;fingerprint identification system;fingerprint identification;neural network	This paper explores the biometric identification and verification of human subjects via fingerprints utilising an adaptive FPGA-based weightless neural networks. The exploration espoused here is a hardware-based system motivated by the need for accurate and rapid response to identification of fingerprints which may be lacking in other alternative systems such as software based neural networks. The fingerprints are pre-processed and binarized, and the binarized fingerprints are partitioned into train- and test-set for the FPGA-based neural network. The neural network employed in this exploration is known as Enhanced Convergent Network (EPCN). The results obtained are compared to other alternative systems. They demonstrate the suitability of the FPGA-based EPCN for such tasks.	artificial neural network;biometrics;field-programmable gate array;fingerprint;high- and low-level;performance;stochastic optimization;weightless (wireless communications)	Pierre Lorrentz;W. Gareth J. Howells;Klaus D. McDonald-Maier	2009	2009 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2009.8	computer science;artificial intelligence;theoretical computer science;machine learning;artificial neural network;biometrics	AI	35.28943938889078	-39.03927156111905	173850
18571be61f100dc4686f6baf659edc95347435cf	build-and-insert: anatomical structure generation for surgical simulators	surgical simulation;ultrasound;evaluation metric;virtual environment	Development of surgical simulators remains a complex task, especially when the virtual environment (VE) needs modification. In this paper, we describe a build-and-insert mechanism that allows for creation of new anatomical models and their insertion into an existing simulator while preserving the existing tasks (e.g., vessel clipping and cutting) and evaluation metrics. Tools used to generate virtual structures from the Visible Human and patient-specific (CT, MRI, ultrasound, etc.) datasets and insert them into a simulator are presented. A laparoscopic nephrectomy simulator is used as an example to show the feasibility of the build-and-insert mechanism. The nephrectomy simulator is a part of our haptic laparoscopic simulator, LapSkills, which allows a surgeon to master a set of fundamental skills such as instrument and camera navigation, handeye coordination, grasping, and applying clips to vessels and cutting them. By interfacing to our tools, existing simulators can take advantage of this dynamic anatomical structure generation and insertion capabilities.	clipping (computer graphics);haptic technology;simulation;virtual reality	Eric Acosta;Bharti Temkin	2004		10.1007/978-3-540-25968-8_26	computer vision;simulation;engineering;engineering drawing	Robotics	39.08627585495052	-38.64589070620937	175758
7a69aa011c79cedcb122e5c071a016b2a45f5aa3	parallelizing maximum likelihood classification on computer cluster and graphics processing unit for supervised image classification	parallel computing;supervised classification;maximum likelihood classification;graphics processing unit	Supervised image classification has been widely utilized in a variety of remote sensing applications. When large volume of satellite imagery data and aerial photos are increasingly available, high-performance image processing solutions are required to handle large scale of data. This paper introduces how maximum likelihood classification approach is parallelized for implementation on a computer cluster and a graphics processing unit to achieve high performance when processing big imagery data. The solution is scalable and satisfies the need of change detection, object identification, and exploratory analysis on large-scale high-resolution imagery data in remote sensing applications. ARTICLE HISTORY Received 15 July 2016 Accepted 18 October 2016	aerial photography;algorithm;automatic parallelization;cuda;central processing unit;computation;computer cluster;computer graphics;computer vision;data-intensive computing;embedded system;gpu cluster;graphics processing unit;image processing;image resolution;merge sort;message passing interface;multi-level cell;parallel computing;personal computer;real-time transcription;scalability;thread (computing);workstation	Xuan Shi;Bowei Xue	2017	Int. J. Digital Earth	10.1080/17538947.2016.1251502	image processing;satellite imagery;scalability;remote sensing application;change detection;graphics processing unit;contextual image classification;computer cluster;computer vision;artificial intelligence;pattern recognition;computer science	Visualization	38.37261672355162	-39.8104386183523	176685
70ea856291da428ad38944b1be948a8e7ac49cfd	on driver gaze estimation: explorations and fusion of geometric and data driven approaches	mirrors;automobiles;learning systems;estimation;solid modeling;head	Gaze direction is important in a number of applications such as active safety and driver's activity monitoring. However, there are challenges in estimating gaze robustly in real world driving situations. While performance of personalized gaze estimation models has improved significantly, performance improvement of universal gaze estimation is lagging behind; one reason being, learning based methods do not exploit the physical constraints of the car. In this paper, we propose a system to estimate driver's gaze from head and eye cues projected on a multi-plane geometrical environment and a system which fuses the geometric with data driven learning method. Evaluations are conducted on naturalistic driving data containing different drivers in different vehicles in order to test the generalization of the methods. Systematic evaluations on this data set are presented for the proposed geometric based gaze estimation method and geometric plus learning based hybrid gaze estimation framework, where exploiting the geometrical constraints of the car shows promising results of generalization.	activity tracker;device driver;personalization	Borhan Vasli;Sujitha Martin;Mohan Manubhai Trivedi	2016	2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2016.7795623	computer vision;simulation;computer science;communication	Robotics	37.499670591923625	-43.22016960667762	177861
8db84a6c13102e4badd2299175209b9cdb56578b	alpha-beta associative memories for gray level patterns	memoire associative;image segmentation;imagen nivel gris;image niveau gris;segmentation image;courbe niveau;associative memory;memoria asociativa;reseau neuronal;curva nivel;grey level image;red neuronal;contour line;neural network	In this paper, we show how the binary Alpha-Beta associative memories, created and developed by Yanez-Marquez, and introduced in [1-3], can be used to operate with gray level patterns (namely gray-level images), improving the results presented by Sossa et. al. in [4]. To achieve our goal, given a fundamental set of gray-level patterns, we find the binary representation of each entry, then we build a binary Alpha-Beta associative memory. After that, a given gray level pattern or a distorted version of it is recalled by converting its entries to a binary representation, then recalling it with the binary associative memory, and finally converting again this binary output pattern into a gray level pattern. Experimental results show the efficiency of the new memories. It is important to point out that this solution is more simple and elegant than that of the presented in [4].		Cornelio Yáñez-Márquez;Luis Pastor Sánchez Fernández;Itzamá López-Yáñez	2006		10.1007/11759966_120	arithmetic;computer science;artificial intelligence;machine learning;mathematics;image segmentation;artificial neural network;contour line	NLP	34.77033211263222	-39.863767469271515	178679
41efaabb674b56e68f87480d98c36ee36a61a3dd	centroid calculation using neural networks	neural networks;hopfield network;automatic target recognition;spatial filtering;hardware implementation;neural network	Centroid calculation provides a means of eliminating translation problems, which is useful for automatic target recognition. A neural network implementation of centroid calculation is described that uses a spatial filter and a Hopfield network to determine the centroid location of an object. Spatial filtering of a segmented window creates a result whose peak value occurs at the centroid of the input data set. A Hopfield network then finds the location of this peak and hence gives the location of the centroid. Hardware implementations of the networks are described and simulation results are provided.	artificial neural network	Glenn S. Himes;Rafael M. Inigo	1992	J. Electronic Imaging	10.1117/12.55180	stochastic neural network;cellular neural network;probabilistic neural network;types of artificial neural networks;computer science;recurrent neural network;theoretical computer science;machine learning;pattern recognition;physical neural network;time delay neural network;deep learning;hopfield network;artificial neural network;spatial filter;automatic target recognition	ML	34.41591843448968	-39.9995565218266	179270
413ffcba70bb029ada7756fb448a9b66073cdb1b	product recognition in store shelves as a sub-graph isomorphism problem		The arrangement of products in store shelves is carefully planned to maximize sales and keep customers happy. Verifying compliance of real shelves to the ideal layout, however, is a costly task currently routinely performed by the store personnel. In this paper, we propose a computer vision pipeline to recognize products on shelves and verify compliance to the planned layout. We deploy local invariant features together with a novel formulation of the product recognition problem as a sub-graph isomorphism between the items appearing in the given image and the ideal layout. This allows for auto-localizing the given image within aisles of the store and improves recognition dramatically.	authentication;categorization;computer vision;deep learning;graph isomorphism problem;planar (computer graphics);sensor	Alessio Tonioni;Luigi di Stefano	2017		10.1007/978-3-319-68560-1_61	isomorphism;graph isomorphism problem;artificial intelligence;computer science;machine learning;invariant (mathematics)	Vision	39.091576955132	-44.49150753332281	180361
93996d49204a9057da85e3cac4e0d0e9479f80e1	a committee machine scheme for feature map fusion under uncertainty: the face detection case	dynamic change;confidence level;image processing;map fusion;uncertainty;committee machine;prior knowledge;committee machines;face detection;visual attention	Feature map fusion in Visual Attention (VA) models is by definition an uncertain procedure. One of the major impediments in extending the static VA architecture proposed by Itti et al. (2000) to account for motion or other information is the lack of justification on how to integrate the various channels. We propose an innovative committee machine scheme that allows for dynamically changing the committee members (maps) and weighting them according to the confidence level of their estimation. Through this machine we handle the extensions on Itti’s model; we add a motion channel and a prior knowledge channel which accounts for the conscious search performed by humans when looking for faces in a scene. The experimental results, obtained when considering face detection, show that the map fusion, through the proposed committee machine, leads to significantly better statistical results when compared with the simple skin-based face detection method.	committee machine;experiment;face detection;global illumination;map (higher-order function);simulation;top-down and bottom-up design	Konstantinos Rapantzikos;Nicolas Tsapatsoulis	2006	IJISTA	10.1504/IJISTA.2006.009912	computer vision;face detection;confidence interval;uncertainty;image processing;computer science;artificial intelligence;machine learning;data mining;statistics	Vision	34.72278462537188	-43.56117433394631	181007
049020aa609d770bd11e01c08f350eb5b8631449	a comparative study for texture classification techniques on wood species recognition problem	cross section surface;texture classification;species recognition;training;cairo wood dataset;image classification;gabor filters;wood samples;wood samples texture classification techniques wood species recognition problem cross section surface grey level co occurrence matrices gabor filters combined glcm and gabor filters covariance matrix cairo wood dataset;image texture;accuracy;gabor filter;timber covariance matrices gabor filters image classification image texture;gabor filters covariance matrix testing computer vision surface texture signal processing pixel fast fourier transforms intelligent systems image generation;image generation;covariance matrices;feature extraction;classification algorithms;grey level co occurrence matrices;comparative study;wood species recognition problem;artificial intelligence;cross section;texture classification techniques;combined glcm and gabor filters;timber;covariance matrix	Wood species recognition is a texture classification problem that has yet to be well studied. The textures observed on the cross section surface of the wood samples can be used to identify the species of the wood. In this paper, we tested various texture classification techniques, i.e. grey level co-occurrence matrices (GLCM), Gabor filters, combined GLCM and Gabor filters as well as covariance matrix. The experiments are conducted on 512 × 512 images of the six wood species from the CAIRO wood dataset. The experimental results show that the covariance matrix produced using the feature images generated by the Gabor filters is 85% compared to 78.33% for the raw GLCM, 73.33% for the Gabor filters and 76.67% for the combined GLCM and Gabor filters. The experimental results show that the covariance matrix has the best recognition rate.	cross section (geometry);experiment;gabor filter;grayscale;statistical classification;cairo	Jing Yi Tou;Yong Haur Tay;Phooi Yee Lau	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.594	computer vision;speech recognition;engineering;pattern recognition	Robotics	32.145076118543756	-43.61831433255698	181878
0d5e357580ba62e1fc87bc599f382b2425042d8f	robust 3d scan point classification using associative markov networks	piecewise linear approximation;max margin optimization;supervised learning;data collection;markov random fields;mobile robots;robust control;maximum likelihood estimation;robustness markov random fields mobile robots training data labeling computer science laser modes piecewise linear approximation supervised learning maximum likelihood estimation;training data;robust control markov processes mobile robots;3d scanning;associative markov networks;kd tree;mobile outdoor robot robust 3d scan point classification associative markov networks max margin optimization;mobile outdoor robot;markov network;robustness;markov processes;computer science;laser modes;robust 3d scan point classification;labeling	In this paper we present an efficient technique to learn associative Markov networks (AMNs) for the segmentation of 3D scan data. Our technique is an extension of the work recently presented by Anguelov et al. (2005), in which AMNs are applied and the learning is done using max-margin optimization. In this paper we show that by adaptively reducing the training data, the training process can be performed much more efficiently while still achieving good classification results. The reduction is obtained by utilizing kd-trees and pruning them appropriately. Our algorithm does not require any additional parameters and yields an abstraction of the training data. In experiments with real data collected from a mobile outdoor robot we demonstrate that our approach yields accurate segmentations	algorithm;experiment;markov chain;markov random field;mathematical optimization;norm (social)	Rudolph Triebel;Kristian Kersting;Wolfram Burgard	2006	Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.	10.1109/ROBOT.2006.1642094	robust control;mobile robot;training set;mathematical optimization;labeling theory;computer science;machine learning;pattern recognition;k-d tree;maximum likelihood;markov process;supervised learning;statistics;robustness;data collection	Robotics	36.664036150913425	-44.14427683368991	182527
2a2048521862f46b3d89b5ff1da440fa38cfc668	human activity as a manifold-valued random process	analytical models;piecewise brownian motion manifold valued random process shape based human activity models linear assumption extrinsic interpretation shape space nonlinear intrinsic geometry nonlinear modeling stochastic modeling euclidean space video sequence sequence extraction;manifolds;video signal processing;manifolds shape stochastic processes humans geometry random processes analytical models;brownian motion;stationary incremental process;geometry;shape manifold;shape;stochastic processes;random processes;parallel transport;humans;stochastic development activity recognition parallel transport shape manifold stationary incremental process;video signal processing brownian motion geometry image sequences stochastic processes;stochastic development;image sequences;activity recognition	Most of the previous shape-based human activity models are built with either a linear assumption or an extrinsic interpretation of the nonlinear geometry of the shape space, both of which proved to be problematic on account of the nonlinear intrinsic geometry of the associated shape spaces. In this paper, we propose an intrinsic stochastic modeling of human activity on a shape manifold. More importantly, within an elegant and theoretically sound framework, our work effectively bridges the nonlinear modeling of human activity on a nonlinear space, with the classic stochastic modeling in a Euclidean space, and thereby provides a foundation for a more effective and accurate analysis of the nonlinear feature space of activity models. From a video sequence, human activity is extracted as a sequence of shapes. Such a sequence is considered as one realization of a random process on a shape manifold. Different activities are then modeled as manifold valued random processes with different distributions. To address the problem of stochastic modeling on a manifold, we first construct a nonlinear invertible map of a manifold valued process to a Euclidean process. The resulting process is then modeled as a global or piecewise Brownian motion. The mapping from a manifold to a Euclidean space is known as a stochastic development. The advantage of such a technique is that it yields a one-one correspondence, and the resulting Euclidean process intrinsically captures the curvature on the original manifold. The proposed algorithm is validated on two activity databases and compared with the related works on each of these. The substantiating results demonstrate the viability and high-accuracy of our modeling technique in characterizing and classifying different activities.	activity recognition;algorithm;brownian motion;classification;experiment;extraction;feature vector;human activities;increment;nonlinear system;performance;relational database;stationary process;stochastic modelling (insurance);stochastic process;manifold	Sheng Yi;Hamid Krim;Larry K. Norris	2012	IEEE Transactions on Image Processing	10.1109/TIP.2012.2197008	stochastic process;configuration space;mathematical optimization;parallel transport;topology;manifold;shape;machine learning;brownian motion;mathematics;geometry;simplicial manifold;manifold alignment	Vision	36.26596945714252	-43.15035553905625	182723
95c2f1f2b0485d10daee6b188e18ad00c05294d4	a subspace method for maximum likelihood target detection	eigenvalues and eigenfunctions;unsupervised learning;filtering;image recognition;high dimensionality;image processing;maximum likelihood;gaussian processes;image processing subspace method maximum likelihood target detection unsupervised technique visual target modeling density estimation high dimensional spaces eigenspace decomposition computationally efficient estimator optimal estimator multivariate gaussian distribution maximum likelihood estimation visual search learning technique probabilistic visual modeling facial features detection;image recognition maximum likelihood estimation maximum likelihood detection eigenvalues and eigenfunctions gaussian distribution gaussian processes unsupervised learning face recognition;optimal estimation;probabilistic visual modeling;maximum likelihood estimation;multivariate gaussian distribution;learning technique;density estimation;maximum likelihood detection object detection principal component analysis face detection karhunen loeve transforms covariance matrix maximum likelihood estimation space technology matched filters filtering;karhunen loeve transforms;maximum likelihood estimate;eigenspace decomposition;face recognition;space use;principal component analysis;visual search;facial features detection;maximum likelihood detection;subspace method;matched filters;facial features;space technology;unsupervised technique;high dimensional spaces;matched filter;computationally efficient estimator;face detection;target detection;maximum likelihood target detection;gaussian distribution;object detection;optimal estimator;visual target modeling;covariance matrix	We present an unsupervised technique for visual target modeling which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. A computationally eecient and optimal estimator for a multivariate Gaussian distribution is derived. This density estimate is then used to formulate a maximum likelihood estimation framework for visual search and target detection. Our learning technique is applied to the probabilistic visual modeling and subsequent detection of facial features and is shown to be superior to matched ltering.	unsupervised learning;visual modeling	Baback Moghaddam;Alex Pentland	1995		10.1109/ICIP.1995.537684	unsupervised learning;image processing;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;maximum likelihood sequence estimation;statistics	ML	32.31970912574511	-41.50417291749346	183745
0727fda3254316553edf4b51d060b13c4b698c4e	a model of shared grasp affordances from demonstration	statistical topic model;shared grasp affordances;robots grasping vocabulary context modeling laboratories sampling methods neuroscience fingers information retrieval probability distribution;grasping;grasp preshapes;gibbs sampling;training;hand orientation;training data;dexterous manipulators;visualization;computational modeling;position control;gibbs sampling shared grasp affordances statistical topic model grasp preshapes teleoperation hand position hand orientation visual appearance;robots;sampling methods dexterous manipulators position control;hand position;sampling methods;visual appearance;teleoperation;data models	This paper presents a hierarchical, statistical topic model for representing the grasp preshapes of a set of objects. Observations provided by teleoperation are clustered into latent affordances shared among all objects. Each affordance defines a joint distribution over position and orientation of the hand relative to the object and conditioned on visual appearance. The parameters of the model are learned using a Gibbs sampling method. After training, the model can be used to compute grasp preshapes for a novel object based on its visual appearance. The model is evaluated experimentally on a set of objects for its ability to generate grasp preshapes that lead to successful grasps, and compared to a baseline approach.	baseline (configuration management);experiment;gibbs sampling;object-based language;sampling (signal processing);social affordance;topic model	John Sweeney;Roderic A. Grupen	2007	2007 7th IEEE-RAS International Conference on Humanoid Robots	10.1109/ICHR.2007.4813845	robot;data modeling;sampling;computer vision;training set;teleoperation;simulation;visualization;gibbs sampling;computer science;artificial intelligence;computational model;visual appearance	Robotics	36.976892169879996	-40.81041357149623	185713
05a14f30cfe909182522d29de7ea0c455f6c8933	multilayer perceptron classification for envisat-asar imagery	fast learning;multipolarization envisat asar imagery;synthetic aperture radar geophysical signal processing geophysical techniques image classification learning artificial intelligence multilayer perceptrons remote sensing by radar;target classification;in situ tests;neural networks;multilayer perceptrons;image classification;multilayer perceptron;guangdong province;interconnection network;learning systems;remote sensing by radar;artificial neural networks;fully interconnected network;geophysical measurements;china multilayer perceptron classification neural networks target classification multipolarization envisat asar imagery fast learning fully interconnected network known truth data in situ test data zhaoqing guangdong province;geophysical signal processing;in situ test data;remote sensing;radar imaging;zhaoqing;learning artificial intelligence;china;known truth data;multilayer perceptron classification;geophysical techniques;multilayer perceptrons neural networks biological neural networks remote sensing multi layer neural network probability density function mathematical model polynomials training data testing;neural network;synthetic aperture radar	This paper describes the application of neural networks to targets classification from multi-polarization ENVISAT-ASAR imagery. The used neural network is multilayer perception (MLP) with fast learning (FL), which is fully interconnected network. Accordingly, the training data sets may be taken from a known truth data in the ground. And finally, the results of proposed method are compared with that of the other classification ones, the in situ test data are from Zhaoqing in Guangdong Province of China	artificial neural network;memory-level parallelism;multilayer perceptron;polarization (waves);test data	Feiya Zhu;Huadong Guo;Qing Dong;Changlin Wang	2004	IGARSS 2004. 2004 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2004.1370348	synthetic aperture radar;computer science;machine learning;data mining;multilayer perceptron;radar imaging;china;artificial neural network;remote sensing	Robotics	32.269707787241614	-43.896735130379085	186411
4a863ad392f984188dc3a9cceb6f8a45b3170ec4	"""commentary paper on """"a probabilistic template update method"""""""	image processing;special issues and sections;surveillance;physiological temperature constraints;computer vision;domain knowledge;infrared imaging;probabilistic logic;template tracking scheme;benchmark testing;tracking;thermal imagery;probabilistic template update method;conferences	This paper describes a novel approach to updating a template in a template tracking scheme. Specifically, the technique involves thermal imagery and makes appropriate use of domain knowledge to set thresholds based on physiological temperature constraints.		Alan J. Lipton	2008	2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2008.42	benchmark;computer vision;image processing;computer science;machine learning;data mining;tracking;probabilistic logic;domain knowledge	Robotics	37.87788831645875	-44.805568035291536	187855
a6b267bad4f40eb4f48d2dc1a079e51b37dbcf9f	synthetic aperture radar (sar) automatic target recognition (atr) using fuzzy co-occurrence matrix texture features		Synthetic aperture radar (SAR) image classification is one of the challenging problems because of the difficult characteristics of SAR images. In this chapter, we implement SAR image classification on three military vehicles types, i.e., T72 tank, BMP2 armored personnel carriers (APCs), and BTR70 APCs. The texture features generated from the fuzzy co-occurrence matrix (FCOM) are utilized with the multi-class support vector machine (MSVM) and the radial basis function (RBF) network. Finally, the ensemble average is implemented as a fusion tool as well. The best detection result is at 97.94 % correct detection from the fusion of twenty best FCOM with RBF network models (ten best RBF network models at d = 5 and other ten best RBF network models at d = 10). Whereas the best fusion result of FCOM with MSVM is at 95.37 % correct classification. This comes from the fusion of ten best MSVM models at d = 5 and other ten best MSVM models at d = 10. As a comparison we also generate features from the gray level co-occurrence matrix (GLCM). This feature set is implemented on the same classifiers. The results from FCOM are better than those from GLCM in all cases.	aperture (software);automatic target recognition;co-occurrence matrix;document-term matrix;synthetic data	Sansanee Auephanwiriyakul;Yutthana Munklang;Nipon Theera-Umpon	2016		10.1007/978-3-319-26450-9_18	computer vision	Vision	32.44265227780942	-44.2988768132605	189872
4c989676b701495221433b4c174ee6d04051348a	interactive framework for insect tracking with active learning	multi object tracking;video signal processing biology computing image classification interactive systems learning artificial intelligence sensor fusion target tracking;insect tracking;insect tracking multi object tracking;videos training benchmark testing insects target tracking joining processes;association optimization algorithm multitarget tracking honey bee antennae tracking honey bee mouth part tracking frame rate videos interactive framework key frame estimation incremental learning object classifier data association classification algorithm;inproceedings	Extracting motion trajectories of insects is an important prerequisite in many behavioral studies. Despite great efforts to design efficient automatic tracking algorithms, tracking errors are unavoidable. In this paper, we propose general principles that help to minimize the human effort required for accurate multi-target tracking in the form of applications that can track the antennae and mouthparts of a honey bee based on a set of low frame rate videos. This interactive framework estimates which key frames will require user correction, i.e. those that are used for user correction, which are used for 1) incrementally learning an object classifier and 2) data association based tracking. To this framework we apply a standard classification algorithm (i.e. naive Bayesian classification) and an association optimization algorithm (i.e. Hungarian algorithm). The precision of tracking results by our framework on real-world video data is above 98%.	bayesian network;correspondence problem;experiment;hungarian algorithm;intel dynamic acceleration;key frame;loss function;mathematical optimization;naive bayes classifier	Minmin Shen;Wei Huang;Paul Szyszka;C. Giovanni Galizia;Dorit Merhof	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.471	computer vision;simulation;tracking system;computer science;machine learning;video tracking	Vision	37.491311731082945	-44.800233629511276	190140
df8ddd67c340c5f4f6eea81c54b203dc36941de7	fatty tissue in a haptic illustration environment		Modeling soft tissue for surgery simulation is a challenging task due to the complex way that the tissue can deform and interact with virtual surgical tools manipulated by user. One soft tissue that is ubiquitous but often not modeled, is fatty tissue. Here we present a novel fatty tissue model based on the mass-spring system on the Graphics Processing Unit (GPU) as part of our Toolkit for Illustration of Procedures in Surgery (TIPS). The user can interact with the fatty tissue in real time via a handheld haptic stylus that represents a virtual surgical tool in TIPS environment. The currently available interactions are palpation, grasp, and cut.	adipose tissue;graphics processing unit;handheld game console;haptic device component;haptic technology;interaction;palpation;simulation;stylus (computing);transjugular intrahepatic portosystemic shunt procedure;soft tissue	Sukitti Punak;Minho Kim;Ashish Myles;Juan Cendan;Sergei Kurenov;Jörg Peters	2008	Studies in health technology and informatics		computer vision;knowledge management;haptic technology;adipose tissue;artificial intelligence;medicine	Robotics	39.15583139111765	-38.73332839711148	191677
93cbb3b3e40321c4990c36f89a63534b506b6daf	learning from examples in the small sample case: face expression recognition	sampling methods learning by example computer vision face recognition emotion recognition linear programming feature extraction bayes methods image representation filtering theory image classification;bayes methods;support vector machine adaboost bayes decision face expression recognition feature selection large margin classifiers learning by example linear programming gabor wavelets small sample case statistical learning;image classification;emotion recognition;bayes decision;small samples;computer vision;statistical learning;face recognition;large margin classifiers;learning by example;algorithms artificial intelligence cluster analysis computer graphics computer simulation face female humans image enhancement image interpretation computer assisted information storage and retrieval models biological models statistical numerical analysis computer assisted pattern recognition automated photography reproducibility of results sample size sensitivity and specificity signal processing computer assisted subtraction technique;image representation;feature extraction;computer aided software engineering face recognition face detection computer vision pattern recognition image recognition linear programming support vector machines support vector machine classification computer interfaces;learning from examples;adaboost;linear programming;linear program;feature selection;situated learning;support vector machine;sampling methods;bayes classifier;face expression recognition;small sample case;gabor wavelets;statistical learning small sample case face expression recognition example based learning computer vision linear programming feature selection classifier training pairwise framework bayes classifier support vector machine adaboost bayes decision large margin classifier gabor wavelets;filtering theory;large margin classifier	Example-based learning for computer vision can be difficult when a large number of examples to represent each pattern or object class is not available. In such situations, learning from a small number of samples is of practical value. To study this issue, the task of face expression recognition with a small number of training images of each expression is considered. A new technique based on linear programming for both feature selection and classifier training is introduced. A pairwise framework for feature selection, instead of using all classes simultaneously, is presented. Experimental results compare the method with three others: a simplified Bayes classifier, support vector machine, and AdaBoost. Finally, each algorithm is analyzed and a new categorization of these algorithms is given, especially for learning from examples in the small sample case.	adaboost;algorithm;categorization;class;computer vision;feature selection;linear programming;naive bayes classifier;support vector machine	Guodong Guo;Charles R. Dyer	2005	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2005.846658	adaboost;situated learning;support vector machine;sampling;computer vision;bayes classifier;contextual image classification;feature extraction;computer science;linear programming;machine learning;pattern recognition;supervised learning;stability	Vision	32.60512279205374	-41.60162082835079	197445
37cd1cb697748415999783f07c3971aed376ad09	the role of kl divergence in anomaly detection	kullback leibler divergence;anomaly detection;kl divergence	We study the role of Kullback-Leibler divergence in the framework of anomaly detection, where its abilities as a statistic underlying detection have never been investigated in depth. We give an in-principle analysis of network attack detection, showing explicitly attacks may be masked at minimal cost through 'camouflage'. We illustrate on both synthetic distributions and ones taken from real traffic.	anomaly detection;kullback–leibler divergence;synthetic intelligence	Lele Zhang;Darryl Veitch;Kotagiri Ramamohanarao	2011		10.1145/1993744.1993787	anomaly detection;speech recognition;computer science;pattern recognition;kullback–leibler divergence;statistics	AI	35.55403074686281	-44.065328655904096	199679
