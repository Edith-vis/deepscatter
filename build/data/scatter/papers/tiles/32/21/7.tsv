id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d9dbd5f222e9fbcd73237eb4b9801872d57e78ea	a multistage approach for image registration	frequency domain analysis;feature extraction;image registration;transforms;optimization;algorithm design and analysis	Successful image registration is an important step for object recognition, target detection, remote sensing, multimodal content fusion, scene blending, and disaster assessment and management. The geometric and photometric variations between images adversely affect the ability for an algorithm to estimate the transformation parameters that relate the two images. Local deformations, lighting conditions, object obstructions, and perspective differences all contribute to the challenges faced by traditional registration techniques. In this paper, a novel multistage registration approach is proposed that is resilient to view point differences, image content variations, and lighting conditions. Robust registration is realized through the utilization of a novel region descriptor which couples with the spatial and texture characteristics of invariant feature points. The proposed region descriptor is exploited in a multistage approach. A multistage process allows the utilization of the graph-based descriptor in many scenarios thus allowing the algorithm to be applied to a broader set of images. Each successive stage of the registration technique is evaluated through an effective similarity metric which determines subsequent action. The registration of aerial and street view images from pre- and post-disaster provide strong evidence that the proposed method estimates more accurate global transformation parameters than traditional feature-based methods. Experimental results show the robustness and accuracy of the proposed multistage image registration methodology.	aerial photography;algorithm;alpha compositing;canny edge detector;estimated;experiment;google street view;graph - visual representation;guided imagery;image registration;lightning;matching;map;mathematical morphology;multimodal interaction;multistage amplifier;musculoskeletal diseases;mutual information;natural disasters;neural correlates of consciousness;obstruction;outline of object recognition;phase correlation;photometry;physical object;pixel;question (inquiry);scale-invariant feature transform;speeded up robust features;test set;registration - actclass	Francis Bowen;Jianghai Hu;Yingzi Du	2016	IEEE Transactions on Cybernetics	10.1109/TCYB.2015.2465394	algorithm design;computer vision;simulation;feature extraction;computer science;image registration;kanade–lucas–tomasi feature tracker;data mining;frequency domain	Vision	38.520110559692974	-56.044756008595286	153423
62d738984a35b348faabb5efa9497ec44c36d528	adaptive color independent components based sift descriptors for image classification	image classification color independent components based sift descriptor termed cic sift;image databases;color space;color image representation;color;image color analysis image classification feature extraction color transforms image databases;image classification;independent component analysis;image color analysis;image colour analysis;image representation;feature extraction;transforms;adaptive color independent components;color image representation adaptive color independent components sift descriptors image classification feature extraction independent component analysis ica;independent component analysis feature extraction image classification image colour analysis image representation;sift descriptors;color independent components based sift descriptor termed cic sift;ica;independent component;color image	This paper proposes an adaptive color independent components based SIFT descriptor (termed CIC-SIFT) for image classification. Our motivation is to seek an adaptive and efficient color space for color SIFT feature extraction. Our work has two key contributions. First, based on independent component analysis (ICA), an adaptive and efficient color space is proposed for color image representation. Second, in this ICA-based color space, a discriminative CIC-SIFT descriptor is calculated for image classification. The experiment results indicate that (1) contrast between objects and background can be enhanced on the ICA-based color space and (2) the CIC-SIFT descriptor outperforms other conventional color SIFT descriptors on image classification.	color image;color space;computer vision;feature extraction;independent computing architecture;independent component analysis;scale-invariant feature transform	Danni Ai;Xian-Hua Han;Xiang Ruan;Yen-Wei Chen	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.596	color histogram;independent component analysis;computer vision;contextual image classification;color quantization;color normalization;color image;feature extraction;computer science;machine learning;pattern recognition;color space	Vision	35.173999666690996	-58.647548217231474	153860
8eb9aa6349db3dd1b724266fcd5fc39a83da022a	a novel feature extraction method using pyramid histogram of orientation gradients for smile recognition	gabor feature;mouth;facial expression recognition;support vector machines;training;feature extraction histograms cameras equations robustness image sequences computational complexity image reconstruction singular value decomposition boosting;gabor filters;histograms of oriented gradients;adaboost algorithm feature extraction method smile recognition happy mood detection gabor features facial expression recognition pyramid histogram of oriented gradients;support vector machine smile recognition pyramid histogram of oriented gradients gabor feature adaboost;face recognition;pyramid histogram of oriented gradients;image edge detection;feature extraction;adaboost;feature selection;support vector machine;feature extraction face recognition;smile recognition	Recognizing smiles is of much importance for detecting happy moods. Gabor features are conventionally widely applied to facial expression recognition, but the number of Gabor features is usually too large. We proposed to use Pyramid Histogram of Oriented Gradients (PHOG) as the features extracted for smile recognition in this paper. The comparisons between the PHOG and Gabor features using a publicly available dataset demonstrated that the PHOG with a significantly shorter vector length could achieve as high a recognition rate as the Gabor features did. Furthermore, the feature selection conducted by an AdaBoost algorithm was not needed when using the PHOG features. To further improve the recognition performance, we combined these two feature extraction methods and achieved the best smile recognition rate, indicating a good value of the PHOG features for smile recognitions.	adaboost;algorithm;feature extraction;feature selection;histogram of oriented gradients;image gradient;sensor;simplified molecular-input line-entry system	Yang Bai;Lihua Guo;Lianwen Jin;Qinghua Huang	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413938	support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;mathematics;feature selection	Vision	32.949925855837016	-57.20815977558741	154116
1bed38bc216f80a50617afa5c6d9cc4b2db72519	face recognition using early biologically inspired features	image matching;mobile face dataset face recognition early biologically inspired features biologically inspired model bim feature representation approach visual object categorization simple to complex hierarchical layers visual perception process primate visual cortex computational cost c1 layer real time object recognition tasks ebif pyramidal statistics standard deviation mean deviation max pooling scale tolerant feature condensation local normalization s1 layer incremental pca principal component analysis lda linear discriminant analysis discriminant subspace feature dimensionality reduction cosine similarity distance metric face matching public face datasets;visual perception face recognition feature extraction image matching object detection principal component analysis statistics visual databases;face recognition;feature extraction;principal component analysis;statistics;face face recognition principal component analysis visualization mobile communication training;visual perception;object detection;visual databases	Biologically inspired model (BIM) is proven to be an effective feature representation approach for visual object categorization. In BIM, two successive S(simple)-to-C(complex) hierarchical layers are performed to simulate the visual perception process of primate visual cortex. However, the intensive computational cost above C1 layer in BIM extremely limits its application in real-time object recognition tasks. This paper proposes to use a set of improved early biologically inspired features (EBIF, including S1 and C1) for face recognition, in which pyramidal statistics of mean and standard deviation rather than MAX pooling are used for scale-tolerant feature condensation and local normalization is performed on C1 layer. Incremental PCA (Principal Component Analysis) and LDA (Linear Discriminant Analysis) are then combined to efficiently learn a discriminant subspace for feature dimensionality reduction. In the matching stage, Cosine similarity is adopted as the distance metric for a given face pair. Experimental results on two public face datasets and a mobile face dataset show the effectiveness of the proposed method.	algorithm;algorithmic efficiency;authentication;bim;categorization;computation;cosine similarity;database normalization;dimensionality reduction;feret (facial recognition technology);facial recognition system;linear discriminant analysis;max;multilinear subspace learning;outline of object recognition;principal component analysis;real-time clock;relevance;return loss;simulation	Min Li;Shenghua Bao;Weihong Qian;Zhong Su;Nalini K. Ratha	2013	2013 IEEE Sixth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2013.6712711	facial recognition system;computer vision;visual perception;feature extraction;computer science;machine learning;pattern recognition;statistics;principal component analysis	Vision	34.123283703744086	-55.86296873998836	154479
c39e89cc9a5d15f8aeecf3b967278cc9b3a9e08e	utilizing dictionary learning and machine learning for blind quality assessment of 3-d images	three dimensional displays visualization measurement feature extraction retina dictionaries visual perception;local pattern 3d image quality assessment dictionary learning machine learning binocular vision	In recent years, perceptual objective quality assessment of 3-D images has become an intense focus of research. In this paper, we propose an efficient blind 3-D image quality assessment (IQA) metric that utilizes binocular vision-based dictionary learning (DL) and  ${k}$ -nearest-neighbors (KNN)-based machine learning (ML) to more accurately align with human subjective judgments. More specifically, in the DL stage, histogram representations from the local patterns of simple and complex cells are concatenated to form basic feature vectors. Then, by using a collaborative representation algorithm, the learned binocular quality-aware features of the distorted 3-D image can be efficiently represented by a linear combination of only a few of these basic feature vectors. In the ML stage, we intuitively simulate the complex high-level behaviors of human perceptual activity with KNN-based ML, which transfers the weighted human subjective quality scores from the annotated 3-D images to the query 3-D image. Our results using three standard subject-rated 3-D-IQA databases confirm that the proposed metric consistently aligns with the subjective ratings and outperforms many representative blind metrics.	align (company);binocular vision;concatenation;database;dictionary;distortion;feature vector;high- and low-level;image quality;interpreter (computing);k-nearest neighbors algorithm;machine learning;simulation;software quality	Wujie Zhou;Weiwei Qiu;Ming-Wei Wu	2017	IEEE Transactions on Broadcasting	10.1109/TBC.2016.2638620	computer vision;computer science;machine learning;pattern recognition	Vision	36.64525746110699	-52.3700808676118	154541
ea4120e3c1d78e49a29581cb94b2bff1ad3c5b9c	combination of high-level features with low-level features for detection of pedestrian		In this paper, we present two high-level features for combining with low-level features. The reason for our use of “high level” and “low level” terms is the ability of features in extracting global and local, respectively, specifications of the objects. We specify the detection result of each feature for a given sample by a score and then add the score of all features to make the final decision. Evaluation results over the cropped images of INRIA dataset for three low-level features including histogram of gradient (HOG), convolutional neural network and Haar, in combination with neural network and SVM as the classifier, show that combining the high-level features with different low-level features, on average, leads to 2.5–7 % increase in detection rate (DR). Also evaluation results on full images of INRIA dataset for two different detectors including: HOG \(+\) neural network and channel features \(+\) boosted decision tree reveal an increase of approximately 5 and 3 % in DR for these two detectors, respectively. Repeating the experiments on more challenging datasets such as Caltech and TUD-Brussels also show an increase of approximately 3 and 1 % for these two detectors, respectively. Overall, combining the high-level features with the low-level features yields at least an increase of 1 % in DR and in some cases, the increase value even reaches to a maximum of 5 %, while the surplus computational burden is only 8 % more than the original detectors.	high- and low-level	Fariba Takarli;Ali Aghagolzadeh;Hadi Seyedarabi	2016	Signal, Image and Video Processing	10.1007/s11760-014-0706-8	computer science;machine learning;pattern recognition;data mining	Vision	32.20300104065959	-55.68082427744949	154564
dd7ed20a65d811dcf863f796d6dcbe873f57e7c4	object detection via structural feature selection and shape model	part based shape model;foreground feature selection;object detection iterative method shape based detection methods discriminative foreground features pairwise image matching structural feature descriptors local contour features class specific codebook cluttered training images part based shape model structural feature selection;feature extraction shape analysis object detection robustness;image matching;institute for integrated and intelligent systems;faculty of science environment engineering and technology;part based shape model object detection foreground feature selection;computer vision;iterative methods;object detection image matching iterative methods learning artificial intelligence;learning artificial intelligence;080104;object detection	In this paper, we propose an approach for object detection via structural feature selection and part-based shape model. It automatically learns a shape model from cluttered training images without need to explicitly use bounding boxes on objects. Our approach first builds a class-specific codebook of local contour features, and then generates structural feature descriptors by combining context shape information. These descriptors are robust to both within-class variations and scale changes. Through exploring pairwise image matching using fast earth mover's distance, feature weights can be iteratively updated. Those discriminative foreground features are assigned high weights and then selected to build a part-based shape model. Finally, object detection is performed by matching each testing image with this model. Experiments show that the proposed method is very effective. It has achieved comparable performance to the state-of-the-art shape-based detection methods, but requires much less training information.	box;codebook;computation (action);experiment;feature model;feature selection;feature vector;genetic selection;image registration;matching;minimum bounding box;object detection;personnameuse - assigned;physical object;visual descriptor;weight	Huigang Zhang;Xiao Bai;Jun Zhou;Jian Cheng;Huijie Zhao	2013	IEEE Transactions on Image Processing	10.1109/TIP.2013.2281406	active shape model;computer vision;feature detection;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;mathematics;iterative method;feature	Vision	35.303130324968365	-53.63091228254422	154720
897bb75d61e72613f4da0bd39c5cbab29a7f5cbf	skin detection using spatial analysis with adaptive seed	adaptive skin color modeling skin detection skin segmentation;face recognition;image colour analysis;feature extraction;detected faces spatial analysis adaptive seed adaptive skin detection color images skin pixels skin color model seed extraction technique local dynamic skin model;object detection face recognition feature extraction image colour analysis;object detection	This paper introduces a new method for adaptive skin detection in color images combined with spatial analysis of skin pixels. It has been reported in many works that adaptation of a skin color model to a particular image may decrease the false positives, however the false negatives are considerably high unless a local model is combined with the global one. Another possibility for improvement is to analyze spatial properties of the pixels classified as skin, but this operation strongly depends on the seed extraction technique. Our contribution lies in using a local dynamic skin model learned from the detected faces to extract seeds for the spatial analysis. We present an extensive experimental study confirming that our method outperforms alternative skin detection techniques.	experiment;pixel;seed;spatial analysis	Michal Kawulok;Jolanta Kawulok;Jakub Nalepa;Maciej Papiez	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738767	facial recognition system;computer vision;feature extraction;computer science;machine learning;pattern recognition;computer graphics (images)	Vision	36.47146300479988	-58.61603948325361	154948
0504c46c80aba498434a81c8133c23cc458d42d3	real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques	grammar;object recognition;skin detection;pattern clustering;human computer interaction;support vector machines;support vector machine svm;real time;skin;training;k means;image classification;testing;hand gesture;gesture recognition feature extraction support vector machines real time systems grammar human computer interaction object detection object recognition;bag of features;scale invariant feature transform;feature extraction;support vector machine svm bag of features grammar hand gesture hand posture human computer interaction k means object detection object recognition scale invariant feature transform sift;scale invariant feature transform sift;hand posture;face;vector quantizer;lighting;support vector machine;cluster model;computer games;vector quantisation;bag of words;gesture recognition;k means clustering;multiclass svm training classifier hand gesture detection hand gesture recognition bag of features support vector machine techniques real time system video game skin detection hand posture contour comparison algorithm face subtraction scale invariance feature transform vector quantization technique unified dimensional histogram vector k means clustering keypoint extraction;object detection;real time systems;vector quantisation computer games gesture recognition image classification object detection pattern clustering skin support vector machines	This paper presents a novel and real-time system for interaction with an application or video game via hand gestures. Our system includes detecting and tracking bare hand in cluttered background using skin detection and hand posture contour comparison algorithm after face subtraction, recognizing hand gestures via bag-of-features and multiclass support vector machine (SVM) and building a grammar that generates gesture commands to control an application. In the training stage, after extracting the keypoints for every training image using the scale invariance feature transform (SIFT), a vector quantization technique will map keypoints from every training image into a unified dimensional histogram vector (bag-of-words) after K-means clustering. This histogram is treated as an input vector for a multiclass SVM to build the training classifier. In the testing stage, for every frame captured from a webcam, the hand is detected using our algorithm, then, the keypoints are extracted for every small image that contains the detected hand gesture only and fed into the cluster model to map them into a bag-of-words vector, which is finally fed into the multiclass SVM training classifier to recognize the hand gesture.	algorithm;bag-of-words model;cluster analysis;computer cluster;k-means clustering;pixel;poor posture;real-time clock;real-time computing;real-time locating system;real-time transcription;sensor;support vector machine;vector quantization;webcam	Nasser H. Dardas;Nicolas D. Georganas	2011	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2011.2161140	support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;gesture recognition;structured support vector machine;k-means clustering	Vision	31.92101341505233	-57.76725721985769	155841
478582a0be6faed630d88b7794ff0afa59627401	fast and robust short video clip search using an index structure	range query;video streaming;high dimensionality;index structure;video segmentation;sequential search;feature space;fast query;spatial temporal feature;video content identification;kd tree;video similarity search;multi resolution;similarity search;matching method	In this paper, we present an index structure-based method to fast and robustly search short video clips in large video collections. First we temporally segment a given long video stream into overlapped matching windows, then map extracted features from the windows into points in a high dimensional feature space, and construct index structures for these feature points for querying process. Different from linear-scan similarity matching methods, querying process can be accelerated by spatial pruning brought by an index structure. A multi-resolution kd-tree (mrkd-tree) is employed to complete exact K-NN Query and range query with the aim of fast and precisely searching out all short video segments having the same contents as the query. In terms of feature representation, rather than selecting representative key frames, we develop a set of spatial-temporal features in order to globally capture the pattern of a short video clip (e.g. a commercial clip, a lead in/out clip) and combine it with the color range feature to form video signatures. Our experiments have shown the efficiency and effectiveness of the proposed method that the very first instance of a given 10-sec query clip can be identified from a 10.5-hour video collection in tens of milliseconds. The proposed method has been also compared with the fast sequential search algorithm	antivirus software;database index;experiment;fast fourier transform;feature vector;k-nearest neighbors algorithm;key frame;linear search;microsoft windows;range query (database);search algorithm;streaming media;video clip	Junsong Yuan;Ling-yu Duan;Qi Tian;Changsheng Xu	2004		10.1145/1026711.1026722	video compression picture types;linear search;range query;computer vision;feature vector;computer science;machine learning;video tracking;pattern recognition;k-d tree;information retrieval	DB	37.02507454362054	-55.057572867913194	155888
680e051a457252e388ff4ac9a7334ea82e0c033e	hierarchical elastic graph matching for hand gesture recognition		This paper proposes a hierarchical scheme for elastic graph matching hand posture recognition. The hierarchy is expressed in terms of weights assigned to visual features scattered over an elastic graph. The weights in graph’s nodes are adapted according to their relative ability to enhance the recognition, and determined using adaptive boosting. A dictionary representing the variability of each gesture class is proposed, in the form of a collection of graphs (a bunch graph). Positions of nodes in the bunch graph are created using three techniques: manually, semi-automatic, and automatically. The recognition results show that the hierarchical weighting on features has significant discriminative power compared to the classic method (uniform weighting). Experimental results also show that the semi-automatically annotation method provides efficient and accurate performance in terms of two performance measures; cost function and accuracy.	gesture recognition;matching (graph theory)	Yu-Ting Li;Juan Pablo Wachs	2012		10.1007/978-3-642-33275-3_38	computer vision;machine learning;pattern recognition	Vision	38.249059069689416	-57.411287293203905	156254
9ba8c6123da2b16a5ac7edb566b97c71a3ebe812	geotopo: dynamic 3d facial expression retrieval using topological and geometric information	solid;object representations h 3 3 information storage and retrieval;curve;i 3 5 computer graphics;i 3 8 computer graphics;computational geometry and object modeling;surface;retrieval models;information search and retrieval;applications	Recently, a lot of research has been dedicated to address the problem of facial expression recognition in dynamic sequences of 3D face scans. On the contrary, no research has been conducted on facial expression retrieval using dynamic 3D face scans. This paper illustrates the first results on the area of dynamic 3D facial expression retrieval. To this end, a novel descriptor is created, namely GeoTopo, capturing the topological as well as the geometric information of the 3D face scans along time. Experiments have been implemented using the angry, happy and surprise expressions of the publicly available dataset BU -- 4DFE. The obtained retrieval results are very promising. Furthermore, a methodology which exploits the retrieval results, in order to achieve unsupervised dynamic 3D facial expression recognition, is presented. The aforementioned unsupervised methodology achieves classification accuracy comparable to the supervised dynamic 3D facial expression recognition state-of-the-art techniques.		Antonios Danelakis;Theoharis Theoharis;Ioannis Pratikakis	2014		10.2312/3dor.20141043	computer vision;visual word;computer science;machine learning;multimedia	Vision	36.33733679821801	-55.54372690858576	156434
b132abe621e597c1a4eca0507b349a79389cb744	the robust derivative code for object recognition	derivative code;gabor wavelet;local pattern;object recognition	This paper proposes new methods, named Derivative Code (DerivativeCode) and Derivative Code Pattern (DCP), for object recognition. The discriminative derivative code is used to capture the local relationship in the input image by concatenating binary results of the mathematical derivative value. Gabor based DerivativeCode is directly used to solve the palmprint recognition problem, which achieves a much better performance than the state-of-art results on the PolyU palmprint database. A new local pattern method, named Derivative Code Pattern (DCP), is further introduced to calculate the local pattern feature based on Dervativecode for object recognition. Similar to local binary pattern (LBP), DCP can be further combined with Gabor features and modeled by spatial histogram. To evaluate the performance of DCP and Gabor-DCP, we test them on the FERET and PolyU infrared face databases, and experimental results show that the proposed method achieves a better result than LBP and some state-of-the-arts. © 2017 KSII.	derivative code;outline of object recognition	Hainan Wang;Baochang Zhang;Hong Zheng;Yao Cao;Zhenhua Guo;Chengshan Qian	2017	TIIS	10.3837/tiis.2017.01.014	theoretical computer science;distributed computing;computer science;3d single-object recognition;cognitive neuroscience of visual object recognition	Robotics	35.46370337167415	-58.49271692851217	156700
e22848256badddc864f13578ac895fb096781df5	a comprehensive and comparative survey of the sift algorithm - feature detection, description, and characterization		The SIFT feature extractor was introduced by Lowe in 1999. This algorithm provides invariant features and the corresponding local descriptors. The descriptors are then used in the image matching process. We propose an overview of this algorithm: the methodology and the tricky steps of its implementation, properties of the detector and descriptor. We analyze the structure of detected features. We finally compare our implementation to others, including Lowe’s.	algorithm;feature detection (computer vision);image registration;randomness extractor;scale-invariant feature transform;synthetic intelligence	Lara Younes;Barbara Romaniuk;Eric Bittar	2012			pattern recognition;computer science;artificial intelligence;computer vision;feature detection;scale-invariant feature transform	Vision	37.868844941448124	-58.17183960098307	156802
c43474a5f910329b79d61a853d4a26b926103d87	pyramidal multi-level features for the robot vision@icpr 2010 challenge	databases;erbium;robot localization;histograms;histogram of oriented edge energy;kernel;support vector machines;robotvision icpr 2010 challenge;support vector machines robot vision;local binary pattern;robot localization pyramidal multilevel features robotvision icpr 2010 challenge multilevel spatial pyramidal features extended local binary pattern extended local binary orientation pattern histogram of oriented edge energy svm algorithms;extended local binary orientation pattern;pyramidal multilevel features;robot vision;indoor environment;robots;extended local binary pattern;lbp;svm algorithms;svm;svm vision lbp;encoding;vision;multilevel spatial pyramidal features;robots databases support vector machines histograms kernel erbium encoding	This paper combines and proposes two novel multi-level spatial pyramidal (sp) features: spELBP (Extended Local Binary Pattern), spELBOP (Extended Local Binary Orientation Pattern) and spHOEE (Histogram of Oriented Edge Energy). These features feed state-of-the-art SVM algorithms for the localization of a robot in indoor environments. Two tasks are associated with the RobotVision@ICPR 2010 Challenge, the first one uses only a frame of stereoscopic images, the second takes into account the dynamics of the robot for improving results. Our scores are ranked $3^{rd}$ for Task1 and $1^{st}$ for Task2	forward–backward algorithm;margin classifier;multiple kernel learning;performance;robot;smoothing;sparse matrix;stereoscopy;support vector machine	Sébastien Paris;Hervé Glotin	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.1143	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics	Robotics	34.1373771457866	-55.43599011149964	157266
72ff1a1dd7c24b51d40d2be7f337b9075bbb8d58	where and who? automatic semantic-aware person composition		Image compositing is a method used to generate realistic yet fake imagery by inserting contents from one image to another. Previous work in compositing has focused on improving appearance compatibility of a user selected foreground segment and a background image (i.e. color and illumination consistency). In this work, we instead develop a fully automated compositing model that additionally learns to select and transform compatible foreground segments from a large collection given only an input image background. To simplify the task, we restrict our problem by focusing on human instance composition, because human segments exhibit strong correlations with their background and because of the availability of large annotated data. We develop a novel branching Convolutional Neural Network (CNN) that jointly predicts candidate person locations given a background image. We then use pre-trained deep feature representations to retrieve person instances from a large segment database. Experimental results show that our model can generate composite images that look visually convincing. We also develop a user interface to demonstrate the potential application of our method.	compositing;convolutional neural network;user interface	Fuwen Tan;Crispin Bernier;Benjamin Cohen;Vicente Ordonez;Connelly Barnes	2018	2018 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2018.00170	branching (version control);convolutional neural network;task analysis;compositing;computer vision;artificial intelligence;computer science;pattern recognition;composition (visual arts);image segmentation;restrict;user interface	Vision	33.19128688617782	-52.234773834935105	157341
98ed0e2786c756bbd4c45d43fa652bf1b5abad06	contour grouping and abstraction using simple part models	shape abstraction;part vocabulary;perceptual grouping;ground truth;active shape model	We address the problem of contour-based perceptual grouping using a user-defined vocabulary of simple part models. We train a family of classifiers on the vocabulary, and apply them to a region oversegmentation of the input image to detect closed contours that are consistent with some shape in the vocabulary. Given such a set of consistent cycles, they are both abstracted and categorized through a novel application of an active shape model also trained on the vocabulary. From an image of a real object, our framework recovers the projections of the abstract surfaces that comprise an idealized model of the object. We evaluate our framework on a newly constructed dataset annotated with a set of ground truth abstract surfaces.	active shape model;categorization;contour line;ground truth;statistical classification;vocabulary	Pablo Sala;Sven J. Dickinson	2010		10.1007/978-3-642-15555-0_44	active shape model;natural language processing;computer vision;ground truth;computer science;machine learning	Vision	38.66997510940223	-55.630650829838466	157404
2106bb0fc4721d264c850acbe57efb492028d404	spotting fingerspelled words from sign language video by temporally regularized canonical component analysis	kernel;speech;shape;feature extraction;assistive technology;correlation;gesture recognition	"""A method for spotting specific words in sign language video is proposed. In classes and talks given using Japanese Sign Language, words that do not have a defined sign, such as the names of people, objects, and places, are represented by sets of multiple characters from the Japanese finger alphabet. The difficulty of recognizing these words has created strong demand for the ability to spot specific words in order to help interpreters and the audience to follow a talk. We address the spotting task by employing the basic idea of temporal regularized canonical correlation analysis (TRCCA), which can simultaneously handle shape and motion information about a 3D object. The classification accuracy of TRCCA is enhanced by incorporating two functions: 1) parallel processing with multiple time scales, 2) strong implicit feature mapping by nonlinear orthogonalization. The enhanced TRCCA is called """"kernel orthogonal TRCCA (KOTRCCA)"""". The effectiveness of the proposed method using KOTRCCA is demonstrated through experiments spotting eight different words in sign language videos."""	decorrelation;experiment;formal language;kernel (operating system);nonlinear system;parallel computing;performance;regularized canonical correlation analysis	Shohei Tanaka;Akio Okazaki;Nobuko Kato;Hideitsu Hino;Kazuhiro Fukui	2016	2016 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA)	10.1109/ISBA.2016.7477238	speech recognition;computer science;pattern recognition;communication	Vision	34.596873635053754	-57.055236055445604	157436
484143c0c2d52a8b6ac0752a6020583a092ba715	visual words selection for human action classification	histograms;object recognition;support vector machines;vocabulary;image classification;classification;computer vision;feature selection and extraction;accuracy;vocabulary visualization accuracy support vector machines feature extraction histograms humans;visualization;feature extraction;vocabulary computer vision feature extraction image classification object recognition support vector machines;humans;computer vision feature selection and extraction classification support vector machines;vocabulary representation visual words selection human action classification computer vision bag of words model spatio temporal features vocabulary size reduction video word ranking method kth dataset stip mosift classifiers knn svm recognition rate nondescriptive words	Human action classification is an important task in computer vision. The Bag-of-Words model uses spatio-temporal features assigned to visual words of a vocabulary and some classification algorithm to attain this goal. In this work we have studied the effect of reducing the vocabulary size using a video word ranking method. We have applied this method to the KTH dataset to obtain a vocabulary with more descriptive words where the representation is more compact and efficient. Two feature descriptors, STIP and MoSIFT, and two classifiers, KNN and SVM, have been used to check the validity of our approach. Results for different vocabulary sizes show an improvement of the recognition rate whilst reducing the number of words as non-descriptive words are removed. Additionally, state-of-the-art performances are reached with this new compact vocabulary representation.	bag-of-words model;computer vision;cross-validation (statistics);database;expect;feature model;k-nearest neighbors algorithm;performance;vocabulary;while	Julián Ramos Cózar;José María González-Linares;Nicolás Guil Mata;R. Hernández;Y. Heredia	2012	2012 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2012.6266910	support vector machine;contextual image classification;speech recognition;visualization;feature extraction;biological classification;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;histogram;accuracy and precision;bag-of-words model in computer vision	Vision	34.43829517113138	-55.740310309035905	157735
c4ecdf992f3914bd21a75e492298e3299b732376	salient object detection via structure extraction and region contrast		In this paper, we propose a novel salient object detection approach, which aims in suppressing distractions caused by the small scale pattern in the background and foreground. First, we employ a structure extraction algorithm as a pre-processing step to smooth the textures, eliminate high frequency components and retain the image’s main structure information. Second, we segment the texture-suppressed image into perceptually homogenous regions. Third, two saliency feature maps are computed and fused according to the color contrast and center prior cues. To better exploit each pixel’s color and position information, we refine the fused saliency map. Experiments on two popular benchmark datasets demonstrate that our proposed approach achieves state-of-the-art performance compared with sixteen other state-of-the-art methods in terms of three popular evaluation measures, i.e., Precision and Recall curve, Area Under ROC Curve and F-measure value.	algorithm;benchmark (computing);color;computation;f1 score;map;object detection;pixel;precision and recall;preprocessor;receiver operating characteristic	Qing Zhang;Jiajun Lin;Xiaodan Li	2016	J. Inf. Sci. Eng.		salient;distributed computing;computer vision;object detection;computer science;artificial intelligence	AI	36.54526894052135	-57.3190302178778	157846
e14ac7dea493f048b2b02900225fbc01705637c6	performance comparison among complex wavelet transforms based face recognition systems		  In this paper we investigate the recently developed dual tree complex wavelet transform (DT-CWT) and the single tree wavelet  transform (ST-CWT) and compared them with Gabor wavelet transform for the face recognition problem. Experiments are carried  out on standard databases. The resulting feature vectors of complex wavelets were applied to PCA and LDA for dimensionality  reduction. In all experiments, complex wavelets equaled or surpassed the performance of Gabor wavelets in recognition rate  when equal number of orientations and scales are used. Moreover, generally ST-CWT results outperformed DT-CWT. Obtained results  indicate that complex wavelets can provide a successful alternative to Gabor wavelets for face recognition both using PCA  and LDA.    	facial recognition system;wavelet transform	Alaa Eleyan;Hasan Demirel	2010		10.1007/978-3-642-16295-4_23	computer vision;pattern recognition;discrete wavelet transform	Vision	33.957359241427135	-58.99922378604477	158408
90b8fd0c408aca568d3114380c96f3bccc3dc203	feature optimality-based semi-supervised face recognition approach		In this paper, a novel approach is proposed that cope with challenges such as illuminations, expressions, poses, and occlusions. The proposed methodology is a non-domination-based optimization technique with a semi-supervised classifier for recognizing a known and unknown face based on different scenarios. The classification is a robust method attaining aptness at different stages resulting in identification of proper training set with actual face image. Different datasets Yale Face Database, Extended Yale Face Database B, ORL database has been considered for our experiments. The performance of the proposed method has been evaluated on several grounds. Results show that the proposed method attains a better performance than the statistical methods.	facial recognition system;semiconductor industry	Taqdir;Renu Dhir	2016		10.1007/978-981-10-3153-3_6	face detection;three-dimensional face recognition;3d single-object recognition;feature	Vision	33.2085372854316	-57.35046456654032	158497
25cb9420ae3c87b52675364dbb74f0ea3ee07183	recognition and learning with polymorphic structural components	primitive parts learning polymorphic structural components object recognition appearance based system learned decision tree image representation image component sequences part paths feature extraction image classification part compatibility graph;object recognition;decision tree;image recognition layout pattern recognition data mining computer vision decision trees shape laboratories classification tree analysis topology;local structure;polymorphism;object model	We address the problem of describing recognizing and learning generic free form objects in real world scenes The appearance based system rep resents objects in a graph form and uses weak structure and evidence accu mulation to implicitly encode object models in a learned decision tree after the method developed by Bischof and Caelli Pattern Recognition pp The decision tree is used to classify sequences of im age components or part paths extracted from the object to be recognized The part paths are in turn used to accumulate evidence for the classi ca tion of the entire object We introduce an improved method for generating part paths based upon the Part Compatibility Graph PCG a replacement for Bischof and Caelli s implicit use of the Part Adjacency Graph PAG A new formalism for extending the representation and recognition scheme to utilize multiple polymorphic types of primitive parts is presented and the approach is demonstrated on a selection of imagery	accu (organisation);decision tree;encode;pattern recognition;semantics (computer science)	Mark Burge;Wilhelm Burger;Winfried Mayr	1996		10.1109/ICPR.1996.545984	polymorphism;computer vision;object model;computer science;cognitive neuroscience of visual object recognition;machine learning;decision tree;pattern recognition;mathematics;3d single-object recognition	Vision	37.74835506304898	-55.40163425051281	158612
d60b97651d51b668a2854e21e2280d08c8070e97	real-time korean traffic sign detection and recognition	sign symbol recognition;sign character recognition;traffic sign;road sign;color segmentation;shape classification	In this paper, we propose a real-time Korean traffic sign detection and recognition method based on color properties and shape geometries of images. The proposed method supports detecting and recognizing various shapes of traffic signs in real-time. Our method consists of four stages: 1) color based image segmentation; 2) region of interest (ROI) detection; 3) shape classification; and 4) numeral recognition. The proposed method can classify even the signs that are partially occluded. In addition, we improve efficiency of shape classification by using simple shape geometry measurements. Our experiment shows that our approach can provide high classification accuracies for octagonal shape signs (92%) and speed-limit signs (94.5%).	image segmentation;real-time clock;real-time transcription;region of interest;sensor	Jihie Kim;Kwangyong Lim;Youngjung Uh;Seunggyu Kim;Yeongwoo Choi;Hyeran Byun	2013		10.1145/2448556.2448575	computer vision;speech recognition;pattern recognition	Vision	32.64444609814431	-57.26388170618821	158844
d5b37090422967018477439c25f672eed2f5110a	human ear recognition based on multi-scale local binary pattern descriptor and kl divergence	databases;image recognition;iris recognition;ear;feature extraction;pattern recognition	This paper presents a novel human ear recognition approach based on Multi-scale Local Binary Pattern (MLBP) descriptor to enhance the recognition performance. The proposed method includes the following two steps: (i) the feature extraction step that computes the MLBP descriptor-based features from human ear images, and (ii) the matching process that uses the Kullback Leibler (KL) distance to capture efficiently the similarities/dissimilarities between the feature vectors and then make a decision. The proposed method is performed using the IIT Delhi Ear database and then compared to the state-of-the-art methods. The results obtained have shown that the proposed method achieves satisfying identification performances up to 95% in terms of rank-1 identification rate.	biometrics;feature extraction;feature vector;integrated information theory;kullback–leibler divergence;performance	Zineb Youbi;Larbi Boubchir;Meriem Dorsaf Bounneche;Arab Ali Chérif;Abdelhani Boukrouche	2016	2016 39th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2016.7760971	computer vision;speech recognition;feature;feature extraction;computer science;pattern recognition;iris recognition	Robotics	33.73354257267837	-58.94040049627587	158966
7b8a808e1b17cb7eadca79cd2db44f60c13a85e8	a visual attention model for stereoscopic 3d images using monocular cues	interest points;stereoscopic images;monocular cues;visual attention	2D Visual saliency has been widely explored for decades. Several comprehensive and well performing models have been proposed, but they are not totally adapted to stereoscopic 3D content. To date only few tentatives of 3D saliency prediction can be found in the literature and most of them rely on binocular depth/disparity. The latter information cannot be correctly obtained in the case of asymmetric processing of the stereo-pair, exploiting the phenomenon of binocular suppression. Based on this aspect, we propose in this paper a new saliency model for stereoscopic 3D images. The proposed model considers two features: (1) spatial feature based on the characteristics of interest points and (2) depth feature based on monocular cues. The latter feature is adapted to asymmetric content and uses occlusions for predicting depth order of the image objects. A tunable fusion strategy is proposed in order to take advantage of different modalities of combining conspicuity maps. For the needs of performance evaluation, an eye-tracking database is created using stereo-pairs with different content. The proposed model gives very good performance in comparison to the literature. The results show that the use of monocular cues outperforms the use of disparity. HighlightsA saliency model for stereoscopic 3D images (asymmetric or symmetric) is proposed.Interest point is exploited for the construction of the spatial conspicuity map.Monocular cues (occlusion) are used for the construction of depth conspicuity map.Different fusion strategies are applied to combine spatial and depth features.An eye-tracking experiment is conducted for the validation of the proposed model.	depth perception;stereoscopy	Iana Iatsun;Mohamed-Chaker Larabi;Christine Fernandez-Maloigne	2015	Sig. Proc.: Image Comm.	10.1016/j.image.2015.05.009	computer vision;depth perception;multimedia;computer graphics (images)	Vision	38.2320404348616	-53.634029017398525	159165
76dab7d62eae67729c5b68db4b0351ee1c4c9425	a support vector machines network for traffic sign recognition	hierarchical structure;object recognition;support vector machines;image classification;histograms of oriented gradients;computer vision;feature extraction;traffic engineering computing;support vector machine;traffic engineering computing feature extraction image classification object recognition support vector machines;support vector machines reliability shape image color analysis classification algorithms algorithm design and analysis histograms;combining the results and assigning the labels procedure support vector machines traffic sign recognition computer vision pre processing operation traffic sign classifier feature extraction hue histogram feature histogram of oriented gradients feature one versus all methodology	The objective of this paper is to describe an algorithm able to solve the traffic sign recognition problem, based on a structure composed by a cascade of competing classifiers and some computer vision pre-processing operations. Traffic sign recognition is a very complex problem, involving a multiclass analysis with unbalanced class frequencies, most of them very similar to each other. With our system, that we are going to call Traffic Sign Classifier (TSC), during the competition promoted by the Institut für Neuroinformatik, Ruhr Universität Bochum, it was possible to recognize more than 40 classes of signs with an average error close to 3%. The algorithm, realized by our development team, consists basically of two modules: a preprocessing module, where the data are managed in order to extract some features, such as the Hue Histogram (HH) and the Histograms of Oriented Gradients (HOG); a second module, where the data coming from the first one are analyzed using a sequence of Support Vector Machines (SVM), implemented with the One Versus All (OVA) methodology. This module includes a couple of systems, composed of several SVMs; one of these systems consists of a hierarchical structure. The results coming out from both the systems are compared with each other in order to define which is the most reliable. This work is performed by the so called “Combining the Results and Assigning the Labels” procedure; calibrating the systems and the parameters employed inside the several analyses performed, it is possible to decrease the number of misclassifications and consequently increase the performance of the entire network.	algorithm;ccir system a;computation;computer vision;extrapolation;image gradient;image histogram;preprocessor;remote desktop services;support vector machine;traffic sign recognition;unbalanced circuit	Fabio Boi;Lorenzo Gagliardini	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033503	support vector machine;computer vision;computer science;machine learning;pattern recognition	Robotics	32.08953350498482	-56.62685251924008	159337
418a2647bd94d4a3fa735fe02f453e2514db5426	combination of image descriptors for the exploration of cultural photographic collections	sensors;image retrieval	The rapid growth of image digitization and collections in recent years makes it challenging and burdensome to organize, categorize, and retrieve similar images from voluminous collections. Content-based image retrieval (CBIR) is immensely convenient in this context. A considerable number of local feature detectors and descriptors are present in the literature of CBIR. We propose a model to anticipate the best feature combinations for image retrieval-related applications. Several spatial complementarity criteria of local feature detectors are analyzed and then engaged in a regression framework to find the optimal combination of detectors for a given dataset and are better adapted for each given image; the proposed model is also useful to optimally fix some other parameters, such as the k in k -nearest neighbor retrieval. Three public datasets of various contents and sizes are employed to evaluate the proposal, which is legitimized by improving the quality of retrieval notably facing classical approaches. Finally, the proposed image search engine is applied to the cultural photographic collections of a French museum, where it demonstrates its added value for the exploration and promotion of these contents at different levels from their archiving up to their exhibition in or ex situ. © 2016 SPIE and IS&T [DOI: 10.1117/1.JEI.26.1.XXXXXX]	archive;categorization;complementarity theory;content-based image retrieval;experiment;farthest-first traversal;independent set (graph theory);k-nearest neighbors algorithm;query by example;sensor;visual descriptor;web search engine	Neelanjan Bhowmik;Valérie Gouet-Brunet;Gabriel Bloch;Sylvain Besson	2017	J. Electronic Imaging	10.1117/1.JEI.26.1.011019	computer vision;visual word;image retrieval;computer science;sensor;multimedia;automatic image annotation;information retrieval	Vision	36.23786983184738	-55.163993579717314	159819
eb2333a759788903848c49111a03c4c00d188e70	color-based and rotation invariant self-similarities		One big challenge in computer vision is to extract robust and discriminative local descriptors. For many applications such as object tracking, image classification or image matching, there exist appearance-based descriptors such as SIFT or learned CNN-features that provide very good results. But for some other applications such as multimodal image comparison (infra-red versus color, color versus depth, ...) these descriptors failed and people resort to using the spatial distribution of self-similarities. The idea is to inform about the similarities between local regions in an image rather than the appearances of these regions at the pixel level. Nevertheless, the classical self-similarities are not invariant to rotation in the image space, so that two rotated versions of a local patch are not considered as similar and we think that many discriminative information is lost because of this weakness. In this paper, we present a method to extract rotation-invariant self similarities. In this aim, we propose to compare color descriptors of the local regions rather than the local regions themselves. Furthermore, since this comparison informs us about the relative orientations of the two local regions, we incorporate this information in the final image descriptor in order to increase the discriminative power of the system. We show that the self similarities extracted by this way are very discriminative.	color;computer vision;existential quantification;image registration;institute for operations research and the management sciences;multimodal interaction;pixel;scale-invariant feature transform;self-similarity;visual descriptor	Xiaohu Song;Damien Muselet;Alain Trémeau	2017		10.5220/0006107503440351	computer vision;computer science;pattern recognition;artificial intelligence;invariant (mathematics)	Vision	38.37381960663378	-55.46153804720125	160086
eaebb7f6eaf9dea82d4071aea0df4231db3b530f	a new descriptor for image matching based on bionic principles		After millions of years of evolution, nature has developed a wide variety of interesting structures, each with their own singularities and properties. Such structures provide several unique and innovative models which may be extended to solve complex engineering problems. One example of such structures is the so-called orb webs, built by many species of a spider as a part of their survival tactics. Orb webs are highly optimized structure, specifically devised to capture prey by efficiently covering a whole area with sticky threads. In this paper, a new feature descriptor called spider local image features (SLIF) is proposed. In the proposed approach, feature vectors are built by selectively extracting pictorial information from a set of previously detected interest point. This is achieved by considering a set of efficiently distributed sampling points, which emulate the intersection nodes formed by the threads of an orb web structure. The SLIF method produces simple low-dimensional feature descriptors, which are robust to several image transformation and distortions, such as scaling, rotation, bright shifts and viewpoint changes. In order to illustrate the proficiency and robustness of the proposed approach, it is compared to other well-known feature description methods, such as those presented on the scale-invariant feature transform, speeded-up robust features, binary robust scalable keypoints and fast retina keypoints. The comparison examines several different images, commonly considered as a benchmark within the image matching literature. Our experimental results evidence SLIF’s high performance and robustness against common image transformations and distortions and further show its viability for many of computer vision applications.	algorithm;benchmark (computing);computer vision;distortion;experiment;freak;feature vector;image processing;image registration;image scaling;object detection;prey;sampling (signal processing);scalability;scale-invariant feature transform;speeded up robust features;sticky bit;visual descriptor	Fernando Fausto;Erik Valdemar Cuevas Jiménez;Adrián Gonzales	2017	Pattern Analysis and Applications	10.1007/s10044-017-0605-z	feature (computer vision);orb (optics);artificial intelligence;robustness (computer science);machine learning;scalability;scaling;pattern recognition;thread (computing);computer vision;feature vector;computer science	Vision	38.5877440143237	-56.5029061892305	160329
ffb4dadda846e9d3d79c5b6aa5aad869d10720da	cross-view object identification using principal color transformation	object recognition;lab color space;learning algorithm;support vector machines image colour analysis object detection pattern classification principal component analysis;cross view object identification;color correction technique;support vector machines;color space;color;hybrid classifier;rgb channels cross view object identification principal color transformation color correction technique lab color space principal color axis principal component analysis hybrid classifier svm learning algorithm color classifier;image color analysis cameras color vehicles object recognition classification algorithms lighting;svm learning algorithm;sampling technique;image color analysis;image colour analysis;principal component analysis;classification algorithms;pattern classification;color correction;color classifier;polar coordinate;vehicles;lighting;principal color axis;object identification;rgb channels;cameras;object detection;principal color transformation	This paper presents a novel color correction technique for object identification across different cameras. First of all, we project the analyzed object onto the LAB color space and then find its principal color axis through the principal component analysis. Since the L axis corresponds to the intensity, we then rotate the found principal color axis for making it parallel to the L axis. After this rotation, the color distortions among different cameras can be reduced into minimum. Then, a hybrid classifier is designed for classifying objects into different categories even though they are captured under different lighting conditions. Based on a polar coordinate, a sampling technique is then proposed for extracting several important color features from AB plane. Then, using the SVM learning algorithm, a color classifier can be trained for classifying each object into different categories. For the non-color categories, we quantize the RGB channels into different levels. Then, another classifier is obtained for classifying each gray object into its corresponding category. Since the proposed color correction scheme reduce the problem of color distortions into a minimum, each object can be well classified and identified even though they are captured across different cameras and under lighting condition.	algorithm;apache axis;color space;distortion;principal component analysis;quantization (signal processing);sampling (signal processing)	Sin-Yu Chen;Jun-Wei Hsieh;Duan-Yu Chen	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580787	statistical classification;color histogram;support vector machine;sampling;computer vision;polar coordinate system;color normalization;color depth;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;lighting;mathematics;color balance;color space;principal component analysis	Robotics	34.77343725458716	-57.683870865635114	160477
97de43093fd16952fcb3119c0cb1bff00d8fdb85	bubble space and place representation in topological maps	support vector machines;visual place recognition;semantic description;topological maps	This paper presents bubble space based representation of “places” (nodes) in topological maps. Bubble space simultaneously provides for detailed (bubble surfaces) and holistic (bubble descriptors) representation of places. It is based on bubble memory where visual feature values and their local S2-metric relations from robot’s viewpoint are simultaneously encoded on a deformable spherical surface. Bubble surfaces extend bubble memory to accommodate varying robot pose and multiple features. They are transformed into bubble descriptors that are rotationally invariant with respect to heading changes while being computable in an incremental manner as each new set of visual observations is made. We use bubble descriptors for place learning and recognition with support vector machines in both indoor and outdoor environments and provide analysis results on recognition, recall and precision rates and time performance including a comparative study with the state-of-the-art descriptors.	bubble memory;computable function;course (navigation);dot-com bubble;holism;map;precision and recall;robot;support vector machine	Özgür Erkent;H. Isil Bozma	2013	I. J. Robotics Res.	10.1177/0278364913481393	support vector machine;computer vision;computer science;machine learning;mathematics;geometry	Robotics	36.762653079617884	-53.15095400387817	161121
1c8145b99a48284693090e9a3b95e87346f83bff	the rijksmuseum challenge: museum-centered visual recognition	cultural heritage;image classification;art dataset;universiteitsbibliotheek	This paper offers a challenge for visual classification and content-based retrieval of artistic content. The challenge is posed from a museum-centric point of view offering a wide range of object types including paintings, photographs, ceramics, furniture, etc. The freely available dataset consists of 112,039 photographic reproductions of the artworks exhibited in the Rijksmuseum in Amsterdam, the Netherlands. We offer four automatic visual recognition challenges consisting of predicting the artist, type, material and creation year. We include a set of baseline results, and make available state-of-the-art image features encoded with the Fisher vector. Progress on this challenge improves the tools of a museum curator while improving content-based exploration by online visitors of the museum collection.	baseline (configuration management);point of view (computer hardware company)	Thomas Mensink;Jan C. van Gemert	2014		10.1145/2578726.2578791	computer vision;contextual image classification;computer science;cultural heritage;multimedia;world wide web	Vision	32.70704597435101	-52.69335611345613	161322
055f35b540ac45bd0d37526ae6123eb19c52ddd6	face recognition based on gradient gabor feature	pre2009 pattern recognition;gaussian processes;280207;institute for integrated and intelligent systems;frequency domain analysis;conference output;faculty of science environment engineering and technology;image classification;gabor filters;local kernel mapping face recognition gradient gabor feature gradient gabor filter multiscale features multiorientation features face representation face classification gaussian function harmonic function spatial domains frequency domains orientation information scale information efficient kernel fisher analysis multiple subspaces magnitude features phase features;face recognition;image representation;feature extraction;280208;face recognition gabor filters kernel data mining feature extraction pattern recognition frequency domain analysis spatial databases image processing automation;image representation face recognition feature extraction frequency domain analysis gabor filters gaussian processes harmonic analysis image classification;pre2009 computer vision;frequency domain;harmonic function;harmonic analysis	In this paper, a novel gradient Gabor (GGabor) filter is proposed to extract multi-scale and multi-orientation features to represent and classify faces. Gradient Gabor combines the derivative of Gaussian functions and the harmonic functions to capture the features in both spatial and frequency domains to deliver orientation and scale information. The spatial positions are combined into Gaussian derivatives which allows it to provide more stable information. An efficient Kernel Fisher analysis method is proposed to find multiple subspaces based on both GGabor magnitude and phase features, which is a local kernel mapping method to capture the structure information in faces. Experiments on two face databases, FRGC Version 1 and FRGC Version 2, are conducted to compare the performances of the Gabor and GGabor features, which show that GGabor can also be a powerful tool to model faces, and the Efficient Kernel Fisher classifier can improve the efficiency of the original kernel fisher method.	database;facial recognition system;gaussian (software);gradient;kernel (operating system);performance	Baochang Zhang;Yongsheng Gao;Yu Qiao	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712152	computer vision;computer science;machine learning;harmonic analysis;pattern recognition;mathematics;frequency domain	Vision	35.5412878453818	-58.583889288072584	161323
fcd806c4de5e765135e78cf9de2ade422161e30d	an appearance based neural image processing algorithm for 3-d object recognition	object representation;translation invariant;image recognition;object recognition;resilient backpropagation appearance based neural image processing algorithm 3d object recognition 2d image wavelet transform feature extraction object representation translational invariance object pose estimators image center translation invariant features neural model illumination condition object identification real image recognition occlusions;image processing;neural model;neural nets;image processing object recognition image recognition image segmentation shape pattern recognition wavelet transforms feature extraction lighting target recognition;backpropagation object recognition image recognition neural nets image representation wavelet transforms lighting feature extraction;backpropagation;wavelet transforms;object segmentation;wavelet transform;image representation;feature extraction;lighting;neural network;pose estimation	We propose an appearance based neural image processing algorithm for the recognition of 3-D objects with arbitrary pose in a 2-D image. Instead of object segmentation we utilize the wavelet transform to extract compact features for object representation. Translational invariance is achieved by using two neural network based object pose estimators to translate objects automatically to the image center. Based on these translation-invariant features a neural model is built to identify objects taken at different viewpoint and under different illumination condition. Results for the recognition of real images under occlusions are shown.	algorithm;image processing;outline of object recognition	Chunrong Yuan;Heinrich Niemann	2000		10.1109/ICIP.2000.899388	computer vision;pose;3d pose estimation;computer science;machine learning;pattern recognition;3d single-object recognition;artificial neural network;wavelet transform	ML	35.528023755507604	-53.67619814555709	162087
7e33a9dbe7aac3675e51ae1e089dfb0e8200e31c	extracting key frames for surveillance video based on color spatial distribution histograms	key frame;color spatial distribution;spatial distribution;extraction method;surveillance video analyzing	This paper proposes a new key frame extraction method by using color spatial distribution histograms. Pixels in the same color are contiguous or non-contiguous to each other in an image, which is a noticeable spatial character. With this property, the pixels are classified as contiguous and non-contiguous ones. Therefore, refined histograms distinguish the small changes better than the original histograms between frames, and it allows fine distinctions that are similar to original histograms. With the evaluation of our experiments, this approach extract key frames more efficiently and accurately.		Jun Chang;Ruimin Hu;Zhongyuan Wang;Bo Hang	2009		10.1007/978-3-642-10467-1_96	color histogram;computer vision;information retrieval;computer graphics (images)	Vision	38.97951069350712	-54.56596309040783	162118
5253cf4f3b1eeb63a07982b0e9e42e7d267d767e	learning visual groups from co-occurrences in space and time		We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.	binary classification;cluster analysis;entity;supervised learning	Phillip Isola;Daniel Zoran;Dilip Krishnan;Edward H. Adelson	2015	CoRR		computer vision;machine learning;mathematics	Vision	32.35200282030994	-52.09833892046047	162303
71276bf93f8d014adc025c44739ec25d1da04e62	detection and recognition of speed limit sign from video		The proper identification of speed limit traffic sighs can alarm the drivers the highest speed allowed and can effictively reduce the number of traffic accidents. In this paper, we put forward an efficient detection method for speed limit traffic signs based on the fast radial symmetry transform with new Sobel operator. when we detected the speed limit traffic sign, we need to segment the digits. Digit segmentation is achieved by cropping the candidate traffic sign from the traffic scene, making use of Otsu thresholding algorithm to binary it, and normalizing it to a uniform size. Finally we recognize and classify the signs using DAG-SVMs classifier which is trained for this purpose. In cloudy weather conditions and dusk illumination condition, we tested 10 videos about 28min. The recognition rate of frames which contain speed limit sign is 90.48 %.		Lei Zhu;Chun-Sheng Yang;Jeng-Shyang Pan	2016		10.1007/978-3-662-49381-6_73	machine learning;artificial intelligence;computer science;thresholding;speed limit;binary number;sobel operator;pattern recognition	Vision	32.3758575560312	-56.83283775261868	162412
0bf87feca12f11598aeb61fb507f035288a4130d	spatio-temporal tube kernel for actor retrieval	video object;kernel on bags;support vector machines;video retrieval;spatio temporal tube kernel face recognition video object actor retrieval kernel on bags;indexing terms;face recognition;image representation;feature extraction;spatio temporal tube kernel;visual features;video retrieval content based retrieval face recognition feature extraction image representation support vector machines;actor retrieval;temporal coherence;kernel based svm learning framework spatio temporal tube kernel actor retrieval video retrieval system face video tubes extraction face video tubes representation coherent features visual features sift points video shot feature point chains;content based retrieval;kernel face detection data mining motion pictures machine learning content based retrieval image retrieval detectors feature extraction support vector machines	This paper presents an actor video retrieval system based on face video-tubes extraction and representation with sets of temporally coherent features. Visual features, SIFT points, are tracked along a video shot, resulting in sets of feature point chains (spatio-temporal tubes). These tubes are then classified and retrieved using a kernel-based SVM learning framework for actor retrieval in a movie. In this paper, we present optimized feature tubes, we extend our feature representation with spatial location of SIFT points and we describe the new Spatio-Temporal Tube Kernel (STTK) of our content-based retrieval system. Our approach has been tested on a real movie and proved to be faster and more robust for actor retrieval task.	actor model;coherence (physics);kernel (operating system);scale-invariant feature transform	Shuji Zhao;Frédéric Precioso;Matthieu Cord	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5413540	facial recognition system;support vector machine;computer vision;visual word;index term;feature extraction;computer science;machine learning;pattern recognition	Vision	35.536306940947185	-52.49601253801263	162863
7cd997f668cd8acaa8f7c885789f6a1d5e7d4bda	3d face verification across pose based on euler rotation and tensors		In this paper, we propose a new approach for 3D face verification based on tensor representation. Face challenges, such as illumination, expression and pose, are modeled as a multilinear algebra problem where facial images are represented as high order tensors. Particularly, to account for head pose variations, several pose scans are generated from a single depth image using Euler transformation. Multi-bloc local phase quantization (MB-LPQ) histogram features are extracted from depth face images and arranged as a third order tensor. The dimensionality of the tensor is reduced based on the higher-order singular value decomposition (HOSVD). HOSVD projects the input tensor in a new subspace in which the dimension of each tensor mode is reduced. To discriminate faces of different persons, we utilize the Enhanced Fisher Model (EFM). Experimental evaluations on CASIA-3D database, which contains large head pose variations, demonstrate the effectiveness of the proposed approach. A verification rate of 98.60% is obtained.	database;euler;singular value decomposition	Ammar Chouchane;Abdelmalik Ouamane;Elhocine Boutellaa;Mebarka Belahcene;Salah Bourennane	2017	Multimedia Tools and Applications	10.1007/s11042-017-5478-z	euler angles;pattern recognition;tensor;artificial intelligence;euler's formula;computer science;histogram;quantization (signal processing);multilinear algebra;singular value decomposition;subspace topology	Vision	35.51152680122733	-57.506797883484865	162876
c38eacf20a7b2e9b39ff4d62078247f2eb825e41	saliency and kaze features assisted object segmentation		In this paper, we propose an unsupervised salient object segmentation approach using saliency and object features. In the proposed method, we utilize occlusion boundaries to construct a region-prior map which is then enhanced using object properties. To reject the non-salient regions, a region rejection strategy is employed based on the amount of detail (saliency information) and density of KAZE keypoints contained in them. Using the region rejection scheme, we obtain a threshold for binarizing the saliency map. The binarized saliency map is used to form a salient superpixel cluster. Finally, an iterative grabcut segmentation is applied with salient texture keypoints (SIFT keypoints on the Gabor convolved texture map) supplemented with salient KAZE keypoints (keypoints inside saliency cluster) as the foreground seeds and the binarized saliency map (obtained using the region rejection strategy) as a probably foreground region. We perform experiments on several datasets and show that the proposed segmentation framework outperforms the state of the art unsupervised salient object segmentation approaches on various performance metrics.		Prerana Mukherjee;Brejesh Lall	2017	Image Vision Comput.	10.1016/j.imavis.2017.02.008	computer vision;machine learning;pattern recognition	Vision	36.53194976544416	-57.33516760066057	163075
2bf494cb7aca2519b99a8ec196e884fb6604d3bd	cnn-vwii: an efficient approach for large-scale video retrieval by image queries		This paper aims to solve the problem of large-scale video retrieval by a query image. Firstly, we define the problem of top-k image to video query. Then, we combine the merits of convolutional neural networks(CNN for short) and Bag of Visual Word(BoVW for short) module to design a model for video frames information extraction and representation. In order to meet the requirements of large-scale video retrieval, we proposed a visual weighted inverted index(VWII for short) and related algorithm to improve the efficiency and accuracy of retrieval process. Comprehensive experiments show that our proposed technique achieves substantial improvements (up to an order of magnitude speed up) over the state-of-the-art techniques with similar accuracy. c © 2018 Elsevier Ltd. All rights reserved.	algorithm;convolutional neural network;experiment;information extraction;requirement;speedup	Chengyuan Zhang;Yunwu Lin;Lei Zhu;Anfeng Liu;Zuping Zhang;Fang Huang	2018	CoRR		information extraction;convolutional neural network;artificial intelligence;mathematics;inverted index;computer vision;visual word;pattern recognition;speedup	Vision	36.69566533274004	-55.16252967115103	163351
cd5d1e54ed6b97550c7871050d512f21aae27f9d	image recognition based on discriminative models using features generated from separable lattice hmms	image recognition;separable lattice hmms;hidden markov hodels;derivative features;log linear models	This paper presents an image recognition technique based on discriminative models using features generated from separable lattice hidden Markov models (SL-HMMs). A major problem in image recognition is that the recognition performance is degraded by geometric variations such as that in position and size of the object to be recognized. SL-HMMs have been proposed to solve this problem. SL-HMMs are an extension of HMMs with size and locational invariances based on state transitions. An SL-HMM is a generative model and can represent generation processes of observations well. However, there is a possibility that the recognition performance of generative models is inferior to that of discriminative models because discriminative models are specialized to identification. In this paper, we propose image recognition based on log linear models (LLMs) using features extracted from SL-HMMs. The proposed method can extract features invariant to geometric variations by using SL-HMMs and built an accurate classifier based on discriminative models with the extracted features. Face recognition experiments showed that the proposed method obtained higher recognition rates than SL-HMMs and convolutional neural networks based methods.	artificial neural network;computer vision;convolutional neural network;discriminative model;experiment;facial recognition system;generative model;hidden markov model;linear model;markov chain;sl (complexity)	Yoshinari Tsuzuki;Kei Sawada;Kei Hashimoto;Yoshihiko Nankaku;Keiichi Tokuda	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952628	discrete mathematics;machine learning;pattern recognition	Vision	35.61107355876795	-53.09269207554797	163404
afaa607aa9ad0e9dad0ce2fe5b031eb4e525cbd8	towards an automatic face indexing system for actor-based video services in an iptv environment	database indexing;digital video broadcasting;internet protocol;internet protocol television iptv;fr engine;internet connection;iptv database indexing digital video broadcasting face recognition;face clustering technique;digital video content;iptv environment;indexing iptv face detection us department of transportation internet motion pictures telecommunications digital tv search engines face recognition;automatic face indexing system;face recognition;streaming media;image color analysis;feature extraction;indexation;actor based video services;face clustering technique automatic face indexing system actor based video services iptv environment internet protocol television digital video content internet connection stb fr engine;face indexing face recognition digital video content internet protocol television iptv;feature fusion;digital video;internet protocol television;article;face indexing;iptv;stb	Automatic face indexing is an important technique for realizing actor-based video services in an IPTV environment. This paper proposes a novel face indexing system that takes advantages of the internet connection of an STB to construct a FR engine that is equipped with a high number of training face images. In addition, we use a face clustering technique to obtain multiple face images of the same subject from a sequence of video frames. The clustered face images are combined using a weighted feature fusion scheme, resulting in a considerable enhancement in face indexing accuracy. The effectiveness of the proposed system is validated using more than 300,000 video frames, collected from five video clips containing drama or movie content. The experimental results show that the proposed system can achieve a face annotation accuracy that is feasible for practical applications.	algorithmic efficiency;analysis of algorithms;cluster analysis;computational complexity theory;digital video;frame (video);high memory;iptv;illumination (image);internet access;online and offline;randomness extractor;run time (program lifecycle phase);set-top box;software deployment;unsupervised learning;user (computing);video clip	Jae Young Choi;Wesley De Neve;Yong Man Ro	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5439138	internet protocol;facial recognition system;database index;internet access;telecommunications;feature extraction;computer science;multimedia;internet privacy;world wide web;digital video broadcasting	Vision	34.73974106617405	-52.668871526480594	164159
b6b071ad75cc8cd38157764e1834a6e30625fd50	spiking hierarchical neural network for corner detection		To enable fast reliable feature matching or tracking in scenes, features need to be discrete and meaningful, and hence corner detection is often used for this purpose. We present a new approach to corner detection inspired by the structure and behaviour of the human visual system, which uses spiking neural networks. Standard digital images are processed and converted to spikes in a manner similar to the processing that is performed in the retina. The spiking neural network performs edge and corner detection using receptive fields that are able to detect edges and corners of various orientations. The locations where neurons emit a spike indicate the positions of detected features. Results are presented using synthetic and real images.	artificial neural network;corner detection;digital image;spiking neural network;synthetic intelligence	Dermot Kerr;T. Martin McGinnity;Sonya A. Coleman;Qingxiang Wu;Marine Clogenson	2011			computer vision;computer science;artificial intelligence;machine learning	Vision	37.970138233457234	-55.24514900642537	164349
89196f7dd5dc1a9f8afb3c507f213ecaaeafd21a	vehicle logo recognition based on a weighted spatial pyramid framework	detection and identification technologies;mathematical methods;surveillance cameras vehicle logo recognition framework regions of interest detection adaboost based detector weighted spatial pyramid framework feature vector extraction roi sift descriptors scale invariant feature transform descriptors spatial distribution local features linear support vector machine classifier svm classifier logo classification max pooling local descriptors logo image capture;会议论文;vehicles accuracy detectors robustness feature extraction vectors licenses;automatic vehicle detection and identification systems;vehicle characteristics;automatic vehicle classification;traffic surveillance;transforms cameras feature extraction image capture image classification intelligent transportation systems learning artificial intelligence object detection support vector machines surveillance;character recognition	This paper proposes an effective vehicle logo recognition framework, which is robust when the logos are only roughly located but not well segmented. Regions of interest (ROI) are first detected by using an AdaBoost-based detector. The detector is tuned to have a low false negative rate so as to guarantee coverage of the vehicle logo as much as possible. A weighted spatial pyramid framework is introduced to extract feature vectors from these ROIs. In this framework, we consider the union of ROIs instead of processing the ROIs individually for robustness and efficiency. Dense SIFT descriptors are extracted from the ROIs for robust description of the image. The scale-invariant feature transform (SIFT) descriptors are weighted based on the corresponding ROIs, highlighting locations with high confidence. The spatial pyramid scheme is then implemented to exploit the spatial distribution of local features. Finally, we apply a linear support vector machine (SVM) classifier to classify the logos based on max pooling of local descriptors. Experiments show that the proposed method attains high recognition accuracies in decent time on logo images captured by surveillance cameras in the real-world scenario, which verifies the robustness and effectiveness of the proposed framework.	adaboost;closed-circuit television;code;codebook;convolutional neural network;discriminative model;experiment;feature vector;logo;region of interest;scale-invariant feature transform;sensor;spatial anti-aliasing;support vector machine;tree structure	Yuanchang Ou;Huicheng Zheng;Shuyue Chen;Jiangtao Chen	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6957857	computer vision;engineering;machine learning;pattern recognition	Vision	33.52336501946422	-56.459703466803894	164516
21f11b581842d48d9df7d2366acd5aedb86f6e12	saliency-svm: an automatic approach for image segmentation	training set selection;image segmentation;journal;期刊论文;support vector machine;visual saliency detection	Although there are some support vector machine (SVM) based methods for image segmentation, automatically and accurately segmenting objects that appeal to human perception is indeed a significant issue. One problem with these methods may be that the human visual attention is seldom taken into consideration. This paper proposes a novel visual saliency based SVM approach for automatic training data selection and object segmentation, namely Saliency-SVM. Firstly, a trimap of the given image is generated according to the saliency map in order to estimate the prominent object locations. Then, positive (salient object) and negative (background) training sets are automatically selected through histogram analysis on trimap for SVM training. Finally, the whole salient object is segmented using the trained SVM classifier. Experiment results on a benchmark dataset demonstrate the effectiveness of the	benchmark (computing);facial recognition system;high- and low-level;image retrieval;image segmentation;support vector machine	Xuefei Bai;Wenjian Wang	2014	Neurocomputing	10.1016/j.neucom.2014.01.008	support vector machine;computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	34.08264133342974	-54.59672639124651	164561
a8035ca71af8cc68b3e0ac9190a89fed50c92332	iiit-cfw: a benchmark database of cartoon faces in the wild		In this paper, we introduce the cartoon faces in the wild (IIIT-CFW) database and associated problems. This database contains 8,928 annotated images of cartoon faces of 100 public figures. It will be useful in conducting research on spectrum of problems associated with cartoon understanding. Note that to our knowledge, such realistic and large databases of cartoon faces are not available in the literature.	benchmark (computing);computer;database	Ashutosh Mishra;Shyam Nandan Rai;Anand Mishra;C. V. Jawahar	2016		10.1007/978-3-319-46604-0_3	computer vision;data mining;computer graphics (images)	Vision	35.447639835149324	-55.38445324001931	164610
0d78052030d17c1af75bcd1364e7e1c4bdf0fee1	scene perception by context-aware dominant surfaces	image segmentation;image segmentation robustness visualization indexes data mining context modeling ear;image segmentation scene perception context aware dominant surfaces computer vision algorithms feature extraction geometric context scene decomposition method context aware augmented reality system generated 3d scene;image classification;computer vision;visual perception scene understanding single image surface extraction image segmentation geometric cues;feature extraction;datavetenskap datalogi;datavetenskap;ubiquitous computing;computer science;augmented reality;ubiquitous computing augmented reality computer vision feature extraction image classification image segmentation	Most of the computer vision algorithms operate pixel-wise and process image in a small neighborhood for feature extraction. Such a feature extraction strategy ignores the context of an object in the real world. Taking geometric context into account while classifying various regions in a scene, we can discriminate the similar features obtained from different regions with respect to their context. A geometric context based scene decomposition method is proposed and is applied in a context-aware Augmented Reality (AR) system. The proposed system segments a single image of a scene into a set of semantic classes representing dominant surfaces in the scene. The classification method is evaluated on an urban driving sequence with labeled ground truths and found to be robust in classifying the scene regions into a set of dominant applicable surfaces. The classified dominant surfaces are used to generate a 3D scene. The generated 3D scene provides an input to the AR system. The visual experience of 3D scene through the contextually aware AR system provides a solution for visual touring from single images as well as an experimental tool for improving the understanding of human visual perception.	3d computer graphics;algorithm;augmented reality;autostereogram;bmc remedy action request system;computer vision;feature extraction;pixel;scene graph;system image	J. Rafid Siddiqui;Siamak Khatibi;S. Bitra;S. Tavakoli	2013	2013, 7th International Conference on Signal Processing and Communication Systems (ICSPCS)	10.1109/ICSPCS.2013.6723977	computer vision;image-based modeling and rendering;computer science;pattern recognition;multimedia	Robotics	38.029570645062165	-53.754212116193024	165061
42de53106949ac28452b806ddf6885308dd0e447	robust duplicate detection of 2d and 3d objects	duplicate detection;retrieval;computer vision;sift;computational complexity;feature extraction;graph;graph model;spatial information;object model;object duplicate detection	In this paper, the authors analyze their graph-based approach for 2D and 3D object duplicate detection in still images. A graph model is used to represent the 3D spatial information of the object based on the features extracted from training images to avoid explicit and complex 3D object modeling. Therefore, improved performance can be achieved in comparison to existing methods in terms of both robustness and computational complexity. Different limitations of this approach are analyzed by evaluating performance with respect to the number of training images and calculation of optimal parameters in a number of applications. Furthermore, effectiveness of object duplicate detection algorithm is measured over different object classes. The authors’ method is shown to be robust in detecting the same objects even when images with objects are taken from different viewpoints or distances. DOI: 10.4018/978-1-4666-1791-9.ch007	3d modeling;algorithm;computational complexity theory;object-based language;robustness (computer science);sensor	Peter Vajda;Ivan Ivanov;Lutz Goldmann;Jong-Seok Lee;Touradj Ebrahimi	2010	IJMDEM	10.4018/jmdem.2010070102	computer vision;object-class detection;object model;feature extraction;computer science;viola–jones object detection framework;machine learning;pattern recognition;scale-invariant feature transform;spatial analysis;graph;computational complexity theory	Vision	37.127258986920125	-55.929006169609295	165240
4429d386a23805dcaee631e52bf428d97ea679d6	a hog-based hand gesture recognition system on a mobile device	client server system;video databases client server systems gesture recognition image matching mobile computing mobile handsets sign language recognition;client server system hand gesture recognition hog feature;servers gesture recognition databases mobile handsets feature extraction conferences;network configurations hog based hand gesture recognition system mobile device video database hand gesture images rotation variation best matched image client server system american sign language alphabet recognition problem asl rotation change;hog feature;hand gesture recognition	We propose a HOG-based hand gesture recognition system running on a mobile device. Input data is a video of hand gesture taken by a mobile device. The input data is compared with a database storing hand gesture images, which was synthesized with rotation variation. The comparison is done based on their HOG features and the gesture corresponding to the best-matched image is returned as the result. The recognition algorithm is implemented on a client-server system. The proposed system is applied to American Sign Language (ASL) alphabet recognition problem. The experimental results show that the proposed recognition algorithm improves HOG's robustness under rotation change and compare processing time with different network configurations.	algorithm;client–server model;computation;experiment;gesture recognition;mobile device;server (computing)	Lukas Prasuhn;Yuji Oyamada;Yoshihiko Mochizuki;Hiroshi Ishikawa	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025807	computer vision;speech recognition;computer science;gesture recognition	Robotics	37.71280093860716	-52.96863407269527	165512
752eacf80071a94bcabde2e383d7bb12cfe0fd77	on-line signature recognition via fusion of dynamic features into dissimilarity space	dynamic features;handwriting recognition;image fusion;vectors feature extraction handwriting recognition image classification image fusion image representation;image classification;dissimilarity representation;feature fusion on line signature dissimilarity representation sparse representation based classification dynamic features;vectors;image representation;feature extraction;feature fusion;svc2004 online signature database online signature recognition dynamic feature fusion dissimilarity space collaborative sparse representation based classification scheme dissimilarity vectors informative dynamic feature extraction pairwise dissimilarities feature sequences dtw algorithm embedding space data structure susig online signature database;sparse representation based classification;on line signature	In this paper a method for on-line signature recognition that combines dynamic features, fused into dissimilarity space, with a powerful collaborative sparse representation-based classification scheme is proposed. Dissimilarity vectors are formed in two stages. Initially, a number of informative dynamic features are extracted and stored in sequences. Afterwards, pairwise dissimilarities among feature sequences, computed using the DTW algorithm, are used to construct the new representation. Based on collaborative sparse representation principles, a new embedding space is defined where classification can be implemented efficiently. According to this scheme, signatures are represented in terms of their position inside the data structure, resulting in high-level performance without utilizing optimal feature selection procedures. The proposed framework has been evaluated using the SUSIG and the SVC2004 on-line signature databases.	algorithm;antivirus software;comparison and contrast of classification schemes in linguistics and metadata;data structure;database;experiment;feature selection;high- and low-level;information;mathematical optimization;online and offline;signature recognition;sparse approximation;sparse matrix;spatial variability;type signature	Ilias Theodorakopoulos;George Economou;Spiros Fotopoulos;Apostolos Ifantis	2013	IEEE International Symposium on Signal Processing and Information Technology	10.1109/ISSPIT.2013.6781921	contextual image classification;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;handwriting recognition;image fusion;feature	Vision	35.37994591983127	-54.491242497813964	165981
820f44ea9d499a53e578c132d9ee6b493de0aabb	application of a decomposed support vector machine algorithm in pedestrian detection from a moving vehicle	pedestrian detection;support vector machine;无	For a shape-based pedestrian detection system [1], the critical requirement for pedestrian detection from a moving vehicle is to both quickly and reliably determine if a moving figure is a pedestrian. This can be achieved by comparing the candidate pedestrian figure with the given pedestrian templates. However, due to the vast number of templates stored, it is difficult to make the matching process fast and reliable. Therefore many pedestrian detection systems [2, 3, 4] re developed to help the matching process. In this paper, we apply a decomposed SVM algorithm in the matching process which can fulfill the recognition task efficiently.		Hong Qiao;Fei-Yue Wang;Xianbin Cao	2005		10.1007/11427995_97	support vector machine;computer vision;simulation;computer science;machine learning	AI	33.71438055800755	-56.31368479767458	166847
a3f6f1ad6e3c04f9a1f0ac0a4b70eecae5a50c3f	similarity-based object retrieval using appearance and geometric feature combination	geometric feature;indexation;content based image retrieval	This work presents a content-based image retrieval system of general purpose that deals with cluttered scenes containing a given query object. The system is flexible enough to handle with a single image of an object despite its rotation, translation and scale variations. The image content is divided in parts that are described with a combination of features based on geometrical and color properties. The idea behind the feature combination is to benefit from a fuzzy similarity computation that provides robustness and tolerance to the retrieval process. The features can be independently computed and the image parts can be easily indexed by using a table structure on every feature value. Finally a process inspired in the alignment strategies is used to check the coherence of the object parts found in a scene. Our work presents a system of easy implementation that uses an open set of features and can suit a wide variety of applications.		Agnès Borràs;Josep Lladós	2007		10.1007/978-3-540-72849-8_5	computer vision;feature detection;visual word;computer science;pattern recognition;automatic image annotation;feature;information retrieval	Vision	39.015854274810394	-58.4694271574983	166929
e372d9f90349b26064bb654ab97e36bd13c75f90	indexation et recherche d'images par arbres des coupes. (image indexing and retrieval using component trees)		This thesis explores component trees, hierarchical structures from Mathematical Morphology, and their application to image retrieval and related tasks. The distinct component trees are analyzed and a novel classification into two superclasses is proposed, as well as a contribution to indexing and representation of the hierarchies using dendrograms. The first contribution to the field of image retrieval is in developing a novel feature detector, built upon the well-established MSER detection. The tree-based implementation of the MSER detector allows for changing the underlying tree in order to produce features of different stability properties. This resulted in the Tree of Shapes based Maximally Stable Region detector, leading to improvements over MSER in retrieval performance. Focusing on feature description, we extend the concept of 2D pattern spectra and adapt their global variant to more powerful, local schemes. Computed on the components of Min/Max-tree, they are histograms holding the information on distribution of image region attributes. The rotation and translation invariance is preserved from the global descriptor, while special attention is given to achieving scale invariance. We report comparable results to SIFT in image classification, as well as outperforming Morphology-based descriptors in satellite image retrieval, with a descriptor shorter than SIFT. Finally, a preprocessing or simplification technique for component trees is also presented, allowing the user to reevaluate the measures of region level of aggregation imposed on a component tree. The thesis is concluded by outlining the future perspectives based on the content of the thesis.		Petra Bosilj	2016				Vision	38.224544584887845	-58.60336213178609	167022
4eab623435a4a8d9aae844a5a6e188508fb29267	fast gaussian mixture clustering for skin detection	image sampling;skin detection;pattern clustering;skin clustering algorithms parameter estimation face detection image databases bayesian methods detection algorithms histograms computer science computational efficiency;gaussian mixture;image segmentation;hand gesture analysis gaussian mixture clustering skin detection skin segmentation image subsampling incremental clustering hierarchical clustering parameter estimation skin database color space face detection;support vector machines;gaussian processes;edge detection;skin;image segmentation support vector machine;image classification;indexing terms;data mining;support vector;image segmentation support vector machine gaussian mixture clustering skin detection image classification;computer vision;face recognition;machine learning;image colour analysis;visual databases expectation maximisation algorithm face recognition gesture recognition image colour analysis image sampling image segmentation pattern clustering skin;support vector machine;skin support vector machines support vector machine classification machine learning data mining computational efficiency image segmentation iterative algorithms computer science computer vision;gesture recognition;support vector machines edge detection gaussian processes image classification image segmentation skin;visual databases;expectation maximisation algorithm	Support vector machine (SVM) is a hot topic in many areas, such as machine learning, computer vision, data mining, and so on, due to its powerful ability to perform classification. Though there exist a lot of approaches to improve the accuracy and the efficiency of the models of SVM, few of them address how to eliminate the redundant data from the input training vectors. As it is known, most of support vectors distributes in the boundary of the class, which means the vectors in the center of the class are useless. In the paper, we propose a new approach based on Gaussian model to preserve the training vectors in the boundary of the class and eliminate the training vectors in the center of the class. The experiments show that our approach can reduce most of the input training vectors and preserve the support vectors at the same time, which leads to a significant reduction in the computational cost and maintains the accuracy.	algorithmic efficiency;computation;computer vision;data mining;existential quantification;experiment;machine learning;support vector machine;vector graphics	Zhiwen Yu;Hau-San Wong	2006	2006 International Conference on Image Processing	10.1109/ICIP.2006.312967	correlation clustering;support vector machine;computer vision;data stream clustering;k-medians clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;gesture recognition;cluster analysis;brown clustering	ML	31.997228962985083	-57.36457283430582	167128
599363bcb3093af66eb5882f3f95ef75ca30140e	sparse representation for multi-label image annotation	databases;multi level decision;support vector machines;training vectors support vector machines databases feature extraction roads training data;text analysis image classification image representation image retrieval;training;multiobject image classification sparse representation multi level decision;image classification;text analysis;image annotation;multiobject image classification;training data;vectors;roads;image representation;feature extraction;object classification;support vector machine;sparse representation;multilevel decision method sparse representation multilabel image annotation keywords assignment multiobject image classification multilabel annotation algorithm;image retrieval	Image annotation is the process of assigning proper keywords to describe the content of a given image, which can be regarded as a problem of multi-object image classification. In this paper, a general multi-label annotation algorithm is proposed, which is based on sparse representation theory and employs a multi-level decision method to deal with the multi-object classification problem. The experimental results show that the proposed algorithm can provide more promising results compared with the traditional classification based image annotation methods.	algorithm;automatic image annotation;coefficient;computer vision;feature extraction;image segmentation;multi-label classification;norm (social);sparse approximation;sparse matrix;standard test image	Bingxin Xu;Ping Guo	2011	2011 Seventh International Conference on Computational Intelligence and Security	10.1109/CIS.2011.269	support vector machine;image retrieval;computer science;machine learning;pattern recognition;data mining;automatic image annotation	Vision	34.3452152393326	-54.57643528149845	167622
8e6fcb6fec3b89ef6d08d8609275fb670c0ee292	statistical summarization of content features for fast near-duplicate video detection	index structure;feature space;computational complexity;news video;algorithms;design;experimentation;video database;similarity measure;topic tracking	This paper outlines a system for detecting near-duplicate videos based on a novel summarization of content features for each clip. It captures the dominating content and content changing trends of a video, so this representation is very compact and effective. Unlike traditional frame-to-frame comparisons that involve quadratic computational complexity, the similarity measure of our method is only linear in dimensionality of feature space and independent of video length. To further improve the search efficiency for very large video databases, an effective indexing structure is deployed to significantly reduce the number of videos for comparison. This demo shows that our system can accurately find near-duplicates from a collection of tens of thousands of video clips extremely fast.	computational complexity theory;database;feature vector;sensor;similarity measure;video clip	Heng Tao Shen;Xiaofang Zhou;Zi Xuan Huang;Jie Shao	2007		10.1145/1291233.1291271	video compression picture types;computer vision;design;computer science;automatic summarization;video tracking;multimedia;world wide web;information retrieval	Vision	36.827550423253605	-54.50659434085741	168044
61fd8114f57eae22ebafeafb812287b51242bc8d	pavement texture segmentation using lbp and glcm for visually impaired person		This paper proposes about a method for region segmentation and texture extraction to classify pavement and roadway region in the image that acquired from cameras equipped to the visually impaired person during a walk. First, detect a road boundary line through the line detections technique using the Hough transform, and obtain candidate regions of pavement and roadway. Second, extract texture feature in segmented candidate region, and separated pavement and roadway regions as classified three levels according to perspective scope in triangular model. In this paper, used rotation invariant LBP and GLCM to compare the difference of texture feature that pavement with various precast pavers and relatively a roadway being monotonous. Proposed method in this paper was verified that the analytical performance nighttime did not deteriorate in comparison with the results from the daytime, and region segmentation performance was very well in complex image has various obstacles and pedestrians.	belief propagation;digital media;hough transform;local binary patterns;real-time clock;real-time computing;sensor	Sun-Hee Weon;Sung-Il Joo;Hyung Il Choi	2012			computer vision;multimedia	Vision	32.707910588049465	-57.36893460962043	168056
18791e9bc379742110514739bb2a33c54f31364e	learning local image descriptors using binary decision trees	training;geometry;materials;accuracy;decision trees training geometry robustness accuracy materials entropy;robustness;entropy;decision trees	In this paper we propose a unified framework for learning such local image descriptors that describe pixel neighborhoods using binary codes. The descriptors are constructed using binary decision trees which are learnt from a set of training image patches. Our framework generalizes several previously proposed binary descriptors, such as BRIEF, LBP and their variants, and provides a principled way to learn new constructions which have not been previously studied. Further, the proposed framework can utilize both labeled or unlabeled training data, and hence fits to both supervised and unsupervised learning scenarios. We evaluate our framework using varying levels of supervision in the learning phase. The experiments show that our descriptor constructions perform comparably to benchmark descriptors in two different applications, namely texture categorization and age group classification from facial images.	academy;benchmark (computing);binary code;categorization;decision tree model;experiment;fits;letter-quality printer;local binary patterns;pixel;supervised learning;unified framework;unsupervised learning;visual descriptor	Juha Ylioinas;Juho Kannala;Abdenour Hadid;Matti Pietikäinen	2014	IEEE Winter Conference on Applications of Computer Vision	10.1109/WACV.2014.6836079	entropy;computer science;machine learning;decision tree;pattern recognition;data mining;mathematics;geometry;accuracy and precision;robustness	Vision	32.99467854850397	-54.50066860459989	168513
0cb923d34e15a7cb03b85541b6a3896eced3ecee	generalized weber-face for illumination-robust face recognition	weber face;multi scale information;face recognition;illumination insensitive representation	Robust face recognition under uncontrolled illumination conditions is one of the key challenges for real-time face recognition systems. Weber-face (WF) is an illumination insensitive face representation based on Weber׳ law. In this letter, we develop a generalized Weber-face (GWF) which extracts the statistics of multi-scale information from face images. By assigning different weights to the inner-ground and outer-ground we further develop a weighted GWF (wGWF) version. Based on our experiments on the extended Yale-B and FERET face database we show that the proposed methods are robust to illumination variations and can obtain promising performance comparable with existing approaches.	facial recognition system	Yong Wu;Yinyan Jiang;Yicong Zhou;Weifeng Li;Zongqing Lu;Qingmin Liao	2014	Neurocomputing	10.1016/j.neucom.2014.01.006	facial recognition system;computer vision;computer science;artificial intelligence;machine learning;three-dimensional face recognition	Vision	34.99005627229235	-57.81129258813587	168575
1b5d97d515a121532ec6b892ddc4cee27ccf6fea	on transforming statistical models for non-frontal face verification	bayesian classifier;learning;280207;institute for integrated and intelligent systems;biometrics;prior information;linear regression;statistical methods;face verification;statistical model;journal article;error analysis;gaussian mixture model;face recognition;spatial relation;maximum likelihood linear regression;mathematical models;faculty of engineering and information technology;local features;database systems;face modeling;pattern recognition;pose mismatch;mixture of gaussians;model synthesis;keywords biometrics;gaussian fixture model;problem solving;face recognition biometrics	We address the pose mismatch problem which can occur in face verification systems that have only a single (frontal) face image available for training. In the framework of a Bayesian classifier based on mixtures of gaussians, the problem is tackled through extending each frontal face model with artificially synthesized models for non-frontal views. The synthesis methods are based on several implementations of maximum likelihood linear regression (MLLR), as well as standard multi-variate linear regression (LinReg). All synthesis techniques rely on prior information and learn how face models for the frontal view are related to face models for non-frontal views. The synthesis and extension approach is evaluated by applying it to two face verification systems: a holistic system (based on PCA-derived features) and a local feature system (based on DCT-derived features). Experiments on the FERET database suggest that for the holistic system, the LinReg-based technique is more suited than the MLLR-based techniques; for the local feature system, the results show that synthesis via a new MLLR implementation obtains better performance than synthesis based on traditional MLLR. The results further suggest that extending frontal models considerably reduces errors. It is also shown that the local feature system is less affected by view changes than the holistic system; this can be attributed to the parts based representation of the face, and, due to the classifier based on mixtures of gaussians, the lack of constraints on spatial relations between the face parts, allowing for deformations and movements of face areas. 2005 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	3d pose estimation;bayesian network;consistency model;dimensionality reduction;discrete cosine transform;feret (facial recognition technology);feret database;feature vector;google map maker;hidden markov model;holism;image scaling;information management;interpolation;markov chain;mixture model;multimodal interaction;naive bayes classifier;pattern recognition;principal component analysis;speech recognition;statistical classification;statistical model;synapomorphy;the australian	Conrad Sanderson;Samy Bengio;Yongsheng Gao	2006	Pattern Recognition	10.1016/j.patcog.2005.07.001	facial recognition system;computer vision;speech recognition;computer science;machine learning;pattern recognition;mixture model;statistics	Vision	34.74044239119599	-57.02472466036691	169196
01f72336bd8559a8c69bc1caf38b48f826a85c09	saliency detection using joint spatial-color constraint and multi-scale segmentation	spatial constraint;saliency model;similarity distribution;multi scale technique;visual attention;region detection;human fixation prediction;color double opponent;segmentation based method	In this paper, a novel method is proposed to detect salient regions in images. To measure pixel-level saliency, joint spatial-color constraint is defined, i.e., spatial constraint (SC), color double-opponent (CD) constraint and similarity distribution (SD) constraint. The SC constraint is designed to produce global contrast with ability to distinguish the difference between ‘‘center and surround’’. The CD constraint is introduced to extract intensive contrast of red-green and blue-yellow double opponency. The SD constraint is developed to detect the salient object and its background. A two-layer structure is adopted to merge the SC, CD and SD saliency into a saliency map. In order to obtain a consistent saliency map, the region-based saliency detection is performed by incorporating a multi-scale segmentation technique. The proposed method is evaluated on two image datasets. Experimental results show that the proposed method outperforms the state-of-the-art methods on salient region detection as well as human fixation	color space;color vision;computability in europe;eye tracking;image segmentation;map;pixel;weight function	Linfeng Xu;Hongliang Li;Liaoyuan Zeng;King Ngi Ngan	2013	J. Visual Communication and Image Representation	10.1016/j.jvcir.2013.02.007	computer vision;machine learning;pattern recognition	Vision	38.41762778985907	-54.459920994868895	169480
eae906e4375b70900192233c376ba014ec32cdf3	a survey of manifold learning for images	manifold learning	Many natural image sets are samples of a low-dimensional manifold in the space of all possible images. Understanding this manifold is a key first step in understanding many sets of images, and manifold learning approaches have recently been used within many application domains, including face recognition, medical image segmentation, gait recognition and hand-written character recognition. This paper attempts to characterize the special features of manifold learning on image data sets, and to highlight the value and limitations of these approaches.	computer vision;facial recognition system;gait analysis;image segmentation;nonlinear dimensionality reduction;optical character recognition;video tracking	Robert Pless;Richard Souvenir	2009	IPSJ Trans. Computer Vision and Applications	10.2197/ipsjtcva.1.83	computer vision;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;manifold alignment	Vision	35.55994801078317	-52.13385864864575	169821
35c73278dae36cb09e6ba8f438909dfb081e205c	illumination invariant face recognition based on the new phase features	hilbert transforms;hilbert huang transform;image classification face recognition frequency domain analysis hilbert transforms;image processing;frequency domain analysis;image classification;phase congruency;riesz transform;hilbert transform;face recognition;intrinsic mode function;local features;signal processing;bidimensional empirical mode decomposition;facial features;frequency domain;monogenic signal;illumination invariance;face recognition hilbert huang transform monogenic signal bidimensional empirical mode decomposition phase congruency;face classification illumination invariant face recognition phase features hilbert huang transform signal processing method nonstationary signals nonlinear signals bidimensional empirical mode decomposition hilbert transform intrinsic mode function intrinsic frequency component bidimensional version image processing monogenic signal frequency domain quantity phase congruency image illumination riesz transform;empirical mode decomposition	Hilbert-Huang transform (HHT) is a novel signal processing method which can efficiently handle non-stationary and nonlinear signals. Two key parts are included: Empirical Mode Decomposition (EMD) and Hilbert transform. EMD decomposes signals into a complete series of Intrinsic Mode Functions (IMFs), which capture the intrinsic frequency components of the original signals. Hilbert transform is adopted on the IMFs to get the analytical local features. Due to its efficiency in signal processing, the bidimensional version has been studied for the advanced image processing. EMD has been extended to bidimensional EMD (BEMD), and the corresponding monogenic signals are studied. Phase information is an important local feature of signals in frequency domain because it is robust to contrast, brightness, noise, shading in the image. The quantity Phase congruency (PC) is invariant to changes in image illumination. In this paper, we firstly proposed an improved BEMD method based on the novel evaluation of local mean, then the Riesz transform is applied to get the corresponding monogenic signals. Finally, PC was calculated based on the new phase information and it then has been adopted as facial features to classify faces under variant illumination conditions. The experimental results demonstrated the efficiency of the proposed approach.	facial recognition system;hilbert transform;hilbert–huang transform;image processing;nonlinear system;phase congruency;shading;signal processing;stationary process	Dan Zhang;Jianjia Pan;Yuan Yan Tang;Chunzhi Wang	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641808	computer vision;speech recognition;image processing;computer science;hilbert–huang transform;signal processing;pattern recognition;mathematics;frequency domain	Vision	35.65352262387108	-58.83837378395178	170005
27d9d00f8276007d0e0caa746b9fdbdd432116bd	histogram matching for music repetition detection	music acoustic signal detection;dynamic programming;histograms;repetition detection;complexity theory;theoretic analysis music repetition detection histogram matching music thumbnailing music summarization;music repetition detection;music summarization;music thumbnailing;data mining;histogram;histograms music laboratories computational efficiency computer vision euclidean distance dynamic programming feature extraction multiple signal classification intelligent systems;theoretical analysis;theoretic analysis;pattern matching;feature extraction;pattern matching repetition detection music structure analysis histogram;acoustic signal detection;histogram matching;experimental evaluation;computational efficiency;music structure analysis;music;structure analysis	Repetition detection is a fundamental issue for music thumbnailing and summarization. In this paper, we propose a new feature, called chroma histogram, which enables us to find out repetitive segments from popular songs accurately and quickly. The feature is robust to tempo variation, because sequential information is removed during the process. The low dimensional feature guarantees a very low computational cost, which is proved by theoretic analysis and experimental evaluation. The objective evaluation results demonstrate that our algorithm outperforms previous approaches in terms of both detecting accuracy and efficiency.	algorithm;algorithmic efficiency;chroma subsampling;computation;histogram matching;sensor;theory;thumbnail	Aibo Tian;Wen Li;Linxing Xiao;Jie Zhou;Tong Zhang	2009	2009 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2009.5202583	computer vision;speech recognition;computer science;machine learning;pattern recognition;histogram;statistics	Vision	38.86518193848372	-52.10411084732272	170045
0d44b57c5db0519443264a1c7ccebb5a8ab0ba5b	adding cues to binary feature descriptors for visual place recognition		In this paper we propose an approach to embed continuous and selector cues in binary feature descriptors used for visual place recognition. The embedding is achieved by extending each feature descriptor with a binary string that encodes a cue and supports the Hamming distance metric. Augmenting the descriptors in such a way has the advantage of being transparent to the procedure used to compare them. We present two concrete applications of our methodology, demonstrating the two considered types of cues. In addition to that, we conducted on these applications a broad quantitative and comparative evaluation covering five benchmark datasets and several state-of-the-art image retrieval approaches in combination with various binary descriptor types.	benchmark (computing);computation;feature model;feature vector;hamming distance;image retrieval;similarity search;visual descriptor	Dominik Schlegel;Giorgio Grisetti	2018	CoRR		engineering;control engineering;hamming distance;embedding;image retrieval;binary number;artificial intelligence;pattern recognition	Vision	36.49122731225442	-53.69196084615454	170151
9726dfef994ad7e617d4423a07304fe3df5fb539	symmetrical null space lda for face and ear recognition	symmetrical lda;null space;linear discriminate analysis;face recognition;ear recognition;linear discriminant analysis	Many natural objects such as face and ear manifest symmetry. The mirror images of symmetrical objects also encode significant discriminative information, which is of benefit to recognition performance. In this paper, a novel symmetrical null space method with the even-odd decomposition principle is proposed for face and ear recognition. By introducing mirror images, the two orthogonal even/odd eigenspaces are constructed. Then the discriminative features are, respectively, extracted from the two eigenspaces under the most suitable situation of the null space. Finally, all the features are combined for classification. Experimental results on both face database and ear database demonstrate the performance of the proposed method.	kernel (linear algebra)	Xiaoxun Zhang;Yunde Jia	2007	Neurocomputing	10.1016/j.neucom.2006.10.016	kernel;speech recognition;computer science;machine learning;pattern recognition;mathematics;linear discriminant analysis	Vision	34.34299128484724	-57.68459042321624	170449
7345bbb755df0e5a45ff7c036416dbe1c10298a1	multisensor biometric evidence fusion of face and palmprint for person authentication using particle swarm optimisation (pso)	image fusion;match score level fusion;pso;multisensor fusion;multimodal biometrics;feature extraction;face images;computer science;sensor fusion;particle swarm optimisation;palmprint images;wavelet coefficients	This paper presents a novel biometric sensor fusion technique for face and palmprint images using Particle Swarm Optimisation (PSO). The proposed method can be visualised in the following steps: we first decompose the face and palmprint image obtained from different sensors using wavelet transformation and then, we employ PSO to select most informative wavelet coefficients from face and palmprint to produce a new fused image. We then employed Kernel Direct Discriminant Analysis (KDDA) for feature extraction and the decision about accept/reject is carried out using Nearest Neighbour Classifier (NNC). Extensive experiments carried out on a virtual multimodal biometric database of 250 users indicate the efficacy of the proposed method.	authentication;biometrics;fingerprint;mathematical optimization;particle swarm optimization	Ramachandra Raghavendra;Ashok Rao;G. Hemantha Kumar	2010	IJBM	10.1504/IJBM.2010.030414	computer vision;speech recognition;feature extraction;computer science;machine learning;pattern recognition;sensor fusion;image fusion	Vision	34.03265217459846	-59.08129691613624	170800
284c0835578198204a24feef613f94e77ef7bec4	detection of salient objects in computer synthesized images based on object-level contrast	image color analysis feature extraction three dimensional displays computers painting graphics solid modeling;computer rendered images visual salient object detection method computer synthesized images object level contrast 3d mesh graphic saliency pixel level contrast dominant color extraction dominant color descriptor dcd surrounding objects;image colour analysis;feature extraction;object level contrast saliency detection computer synthesized images;rendering computer graphics;object detection;rendering computer graphics feature extraction image colour analysis object detection	In this work, we propose a method to detect visually salient objects in computer synthesized images from 3D meshes. Different from existing detection methods on graphic saliency which compute saliency based on pixel-level contrast, the proposed method computes saliency by measuring object-level contrast of each object to the other objects in a rendered image. Given a synthesized image, the proposed method first extracts dominant colors from each object, and represents each object with the dominant color descriptor (DCD). Saliency is measured as the contrast between the DCD of the object and the DCDs of its surrounding objects. We evaluate the proposed method on a data set of computer rendered images, and the results show that the proposed method obtains much better performance compared with existing related methods.	3d computer graphics;color;pixel	Lu Dong;Weisi Lin;Yuming Fang;Shiqian Wu;Seah Hock Soon	2013	2013 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2013.6706362	computer vision;object-class detection;feature extraction;computer science;kadir–brady saliency detector;machine learning;multimedia;computer graphics (images)	Vision	38.47398978022438	-54.14178478157111	170877
34641da04e2caee6cde8b1adda915adf298c788a	context aware keypoint extraction for robust image representation		We introduce a context-aware keypoint extractor, coined as CAKE, aimed at capturing the most informative image content. We find this algorithm particularly useful in tasks such as image retrieval, scene classification, and object (class) recognition, in which local features are mainly used to provide a robust and efficient image representation. We are motivated by the fact that the majority of local feature extractors are designed to respond to a reduced number of structures. Furthermore, we observe that the existent complementarity among feature sets is often neglected. Our context-aware algorithm is designed to respond to complementary features as long as they are informative. In the particular case of images with different types of structures, one can expect a high complementarity among the features retrieved by a context-aware extractor. By contrast, images with repetitive patterns will inhibit our method from retrieving a clear summarised description of the image content. Nonetheless, the extracted set of features can be complemented with a counterpart that retrieves the repetitive elements in the image. These two cases are depicted in Figure 1. The upper image shows a context-aware keypoint extraction on a well-structured scene, which retrieves the 100 most informative keypoints. This small number of features is sufficient to provide a good coverage of the content, which includes different types of structures. The lower image illustrates the advantages of combining context-aware keypoints with strictly local ones (SFOP keypoints [2]) to obtain a better coverage of images with repetitive patterns. An information theoretic framework is used to formulate our contextaware keypoint extraction. A keypoint will correspond to a certain image location within a structure with a low probability of occurrence (high information content). For each image location x, we consider w(x) ∈RD, any viable local representation (e.g, the Hessian matrix or the structure tensor matrix) as a “codeword” that represents the neighbourhood of x. To define the saliency measure, we regard the image codewords as samples of a multivariate probability density function. We compute the probability of a codeword w(y) using a Kernel Density Estimator [4] in which the kernel is a multidimensional Gaussian function with zero mean and standard deviation σk:	algorithm;code word;complementarity theory;feature extraction;hessian;image retrieval;information extraction;information theory;kernel (operating system);randomness extractor;self-information;statistical classification;structure tensor;the 100	Pedro Martins;Paulo Carvalho;Carlo Gatta	2012		10.5244/C.26.100	computer vision;machine learning;pattern recognition	Vision	37.94924253954733	-58.19154090796201	171178
72962038460e32b0dd01d083f7f4049be36a34b9	face recognition under occlusions and variant expressions with partial similarity	databases;general golden section rule;perception inspired nonmetric partial similarity measure;machine learning face recognition systems perception inspired nonmetric partial similarity measure general golden section rule maximum margin criterion;information security;similarity measure face recognition machine learning nonmetric similarity partial similarity pattern recognition self organizing map som;surveillance;image matching;distortion measurement;matrix algebra;face recognition systems;maximum margin criterion;partial similarity;self organizing map som;face recognition;machine learning;nonmetric similarity;pattern recognition;self organized map;humans;computer science;matrix algebra face recognition image matching learning artificial intelligence;learning artificial intelligence;extraterrestrial measurements;face recognition pattern recognition extraterrestrial measurements computer science humans databases machine learning distortion measurement information security surveillance;similarity measure;human perception	Recognition in uncontrolled situations is one of the most important bottlenecks for practical face recognition systems. In particular, few researchers have addressed the challenge to recognize noncooperative or even uncooperative subjects who try to cheat the recognition system by deliberately changing their facial appearance through such tricks as variant expressions or disguise (e.g., by partial occlusions). This paper addresses these problems within the framework of similarity matching. A novel perception-inspired nonmetric partial similarity measure is introduced, which is potentially useful in dealing with the concerned problems because it can help capture the prominent partial similarities that are dominant in human perception. Two methods, based on the general golden section rule and the maximum margin criterion, respectively, are proposed to automatically set the similarity threshold. The effectiveness of the proposed method in handling large expressions, partial occlusions, and other distortions is demonstrated on several well-known face databases.	anomaly detection;bottleneck (software);cluster analysis;database;distortion;experiment;facial recognition system;image segmentation;marginal model;semi-supervised learning;similarity measure;uncontrolled format string	Xiaoyang Tan;Songcan Chen;Zhi-Hua Zhou;Jun Liu	2009	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2009.2020772	computer vision;computer science;information security;machine learning;pattern recognition;mathematics;perception;computer security	Vision	34.19188404697863	-53.43840431513593	171191
93a675523d0711c81e8292e59a5396a114174866	coding of image data via correlation filters for invariant pattern recognition: some practical results	real time;discriminant function;invariant pattern recognition;target recognition;automatic target recognition;infrared;template matching	The National Optics Institute is currently carrying out a project in automatic target recognition (ATR) in order to locate, recognize, and track potential targets (e.g. tanks or troop carriers) acquired in real time by an infrared camera. The targets may be camouflaged or par­ tially occluded, in real scenes with high levels of clutters and noise. We present an algorithm based on the Dis­ tance Classifier Correlation Filter (DCCF), a subclass of the Synthetic Discriminant Functions (SDF) family which are often used to solve the ATR problem. We describe the general approach of DCCF-based template matching and the algorithms used, the effect of the train­ ing set on the discrimination performance, the optical implementation, and finally a few practical results. I DCCF algorithm	algorithm;answer to reset;automatic target recognition;discriminant;pattern recognition;template matching	Jonny Gauvin;Michel Doucet;Denis Gingras;Paul Chevrette	1995		10.1007/BFb0025149	computer vision;speech recognition;template matching;infrared;feature;computer science;pattern recognition;discriminant function analysis;3d single-object recognition;signature recognition;automatic target recognition	Vision	32.910062745645995	-58.45111460062976	171386
40b82e992263e74537b1c4c70bd9d9cf0c241e02	a novel multi-feature descriptor for human detection using cascaded classifiers in static images	hog;cascaded classifiers;multi feature;feature extraction;human detection	Combining multiple kinds of features is useful to achieve the state of the art performance for human detection. But combining more features will result in high dimensional feature descriptors, which is time-consuming for feature extraction and detection. How to exploit different kinds of features and reduce the dimension of feature descriptor are challenging problems. A novel multi-feature descriptor (MFD) combining Optimal Histograms of Oriented Gradients (OHOG), Local Binary Patterns (LBP) and Color SelfSimilarity in Neighbor (NCSS) is proposed. Firstly, a discriminative feature selection and combination strategy is introduced to obtain distinctive local HOGs and construct OHOG feature. OHOG combines local discriminative and correlated information, which improves the classification performance compared with HOG. Besides, LBP describes texture feature of human appearance. Finally, a compact and lower dimensional feature NCSS is proposed to encode the self-similarity of color histograms in limited neighbor sub-regions instead of global regions. The proposed MFD describes human appearance from gradient, texture and color features, which can complement each other and improve the robustness of human description. To further improve detection speed without decreasing accuracy, we cascade early stages of Adaboost based on selected local HOGs and SVM classifier based on MFD. The former part can reject most non-human detection windows quickly and the final SVM classifier can guarantee a high accuracy. Experimental results on public dataset show that the proposed MFD and cascaded classifiers framework can achieve promising results both in accuracy and detection speed.	adaboost;cascading style sheets;encode;feature extraction;feature model;feature selection;feature vector;image gradient;local binary patterns;microsoft windows;multiple encryption;self-similarity;sensor;speedup;texture mapping;visual descriptor	Hantian Liu;Tao Xu;Xiangdong Wang;Yueliang Qian	2015	Signal Processing Systems	10.1007/s11265-014-0960-6	computer vision;local binary patterns;feature extraction;computer science;machine learning;pattern recognition;feature	ML	33.73794924322486	-56.76093161629477	171401
07137100f41192393f54c736b3d065616e464d60	extended lbp operator to characterize event-address representation connectivity		Address-Event Representation is a flowering technology that can change the visual perception of the computer vision world. This paper proposes a methodology to associate the input data from this kind of sensors. A new descriptor computed using an extended LBP operator seeks to characterize the connectivity of the asynchronous incoming events in a two dimensional space. Those features can be organized on histograms and combined with others descriptors, as histograms of oriented events. They can be the input of traditional classifiers to detect or recognize objects from the scene.	local binary patterns	Pablo Negri	2016		10.1007/978-3-319-52277-7_30	operator (computer programming);computer vision;two-dimensional space;pattern recognition;local binary patterns;computer science;artificial intelligence;visual perception;asynchronous communication;histogram	Vision	37.67763765303329	-55.02422889647497	171616
bc900a41868e068362b87258b45db4f365f1a277	traffic sign recognition using discriminative local features	book chapter;real time;minimum distance;local features;image representation;principal component analysis;error rate;feature selection;distance transform	Real-time road sign recognition has been of great interest for many years. This problem is often addressed in a two-stage procedure involving detection and classification. In this paper a novel approach to sign classification is proposed. In many previous studies focus was put on deriving a possibly most discriminative set of features from a large amount of training data using global selection techniques e.g. Principal Component Analysis or AdaBoost. In our method we have chosen a simple yet robust image representation built on top of the Colour Distance Transform (CDT). Based on this representation, we introduce a feature selection algorithm which captures a variable-size set of local image regions ensuring maximum dissimilarity between each individual sign and all other signs. Experiments have shown that the discriminative local features extracted from template sign images enable simple minimumdistance classification with error rate not exceeding 7%.	adaboost;color image;distance transform;eclipse;experiment;feature selection;principal component analysis;real-time transcription;selection algorithm;spatial variability;statistical classification;traffic sign recognition	Andrzej Ruta;Yongmin Li;Xiaohui Liu	2007		10.1007/978-3-540-74825-0_32	speech recognition;word error rate;computer science;machine learning;pattern recognition;mathematics;distance transform;feature selection;principal component analysis	Vision	32.52784779117561	-57.11011982609718	171641
8a8177fd08a3a7f66005fdee77003bd4cf74b2b7	modified lnmf based color face recognition	databases;color face recognition;appearance based face recognition;image recognition;virtual face images;color face face recognition local nmf;algorithm learning lnmf color face recognition dimensionality reduction appearance based face recognition monochromatic face image color face image feature extraction virtual face images;image colour analysis face recognition feature extraction;image color analysis face face recognition databases three dimensional displays image recognition deformable models;deformable models;lnmf;face recognition;dimensionality reduction;color face image;three dimensional displays;image color analysis;image colour analysis;feature extraction;block diagonalization;algorithm learning;local nmf;monochromatic face image;facial features;face;color face;dimensional reduction;deformable model	Dimensionality reduction technique is very important to appearance-based face recognition algorithm. Compared with monochromatic face image, color face image which is composed of different color channels can provide more cues for recognition task. In this paper, a novel appearance-based recognition approach, modified local NMF based color face recognition, is proposed. Block diagonal matrix mode is employed to encode color information of different channels directly. Block diagonal constraint is plus into that of traditional local NMF to build modified local NMF for dimensionality reduction and feature extraction. 3D active deformable model is exploited to generate virtual face images for algorithm learning. Projective coefficients are adopted as facial features for recognition.	algorithm;channel (digital image);coefficient;database;dimensionality reduction;encode;facial recognition system;feature extraction;monochrome;non-negative matrix factorization	Xiaoming Bai;Chengzhang Wang	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583671	computer vision;speech recognition;pattern recognition;three-dimensional face recognition	Vision	34.732492459874564	-57.403040634939956	171790
6363af2d0a23c43e7f23f36c00f57c142c84ef08	a novel method for generation of motion saliency	visual saliency;analytical models;motion estimation feature extraction;saliency map;visualization computational modeling proposals feature extraction color analytical models video sequences;spatial saliency information;saliency map motion saliency feature extraction visual saliency;color;feature extraction motion saliency spatial saliency information;reference frame;motion estimation;video sequences;visualization;computational modeling;feature extraction;motion saliency;proposals	Motion saliency is the key component for the video saliency model, and attracts great research interest. However, there is few universal predictor of motion saliency. In this paper, a novel method for generation of motion saliency is proposed, in which motion saliency map is obtained through the multi reference frames, and enhanced by spatial saliency information. The proposal can obtain more detailed information about motion feature and extract the salient object more integrally. The experiment results shows that our proposal can achieve 95.4% of the ROC area of a human based control in motion channel, whereas the classical Itti's model achieves 78.5%.	kerrison predictor	Yang Xia;Ruimin Hu;Zhenkun Huang;Yin Su	2010	2010 IEEE International Conference on Image Processing	10.1109/ICIP.2010.5653175	reference frame;computer vision;visualization;feature extraction;computer science;kadir–brady saliency detector;machine learning;pattern recognition;motion estimation;computational model	Robotics	38.241543877002144	-53.573407425689304	172068
146a278546c5d1bd42621f3f0d36b3f5059840b8	optimal decision fusion and its application on 3d face recognition	3d face recognition;multiple classifiers;decision fusion;error rate	Fusion is a popular practice to combine multiple classifiers or multiple modalities in biometrics. In this paper, optimal decision fusion (ODF) by AND rule and OR rule is presented. We show that the decision fusion can be done in an optimal way such that it always gives an improvement in terms of error rates over the classifiers that are fused. Both the optimal decision fusion theory and the experimental results on the FRGC 2D and 3D face data are given. Experiments show that the optimal decision fusion effectively combines the 2D texture and 3D shape information, and boosts the performance of the system.	biometrics;facial recognition system;three-dimensional face recognition	Qian Tao;Robin van Rootseler;Raymond N. J. Veldhuis;Stefan Gehlen;Frank Weber	2007			speech recognition;word error rate;computer science;machine learning;pattern recognition	Vision	33.40972226777221	-58.78502762288689	172294
1b199f60d40d258e8848fa6c3f46e3ffc7aee96e	visual attention detection in video sequences using spatiotemporal cues	saliency map;video attention detection;spatiotemporal saliency map;visual attention	Human vision system actively seeks interesting regions in images to reduce the search effort in tasks, such as object detection and recognition. Similarly, prominent actions in video sequences are more likely to attract our first sight than their surrounding neighbors. In this paper, we propose a spatiotemporal video attention detection technique for detecting the attended regions that correspond to both interesting objects and actions in video sequences. Both spatial and temporal saliency maps are constructed and further fused in a dynamic fashion to produce the overall spatiotemporal attention model. In the temporal attention model, motion contrast is computed based on the planar motions (homography) between images, which is estimated by applying RANSAC on point correspondences in the scene. To compensate the non-uniformity of spatial distribution of interest-points, spanning areas of motion segments are incorporated in the motion contrast computation. In the spatial attention model, a fast method for computing pixel-level saliency maps has been developed using color histograms of images. A hierarchical spatial attention representation is established to reveal the interesting points in images as well as the interesting regions. Finally, a dynamic fusion technique is applied to combine both the temporal and spatial saliency maps, where temporal attention is dominant over the spatial model when large motion contrast exists, and vice versa. The proposed spatiotemporal attention framework has been applied on over 20 testing video sequences, and attended regions are detected to highlight interesting objects and motions present in the sequences with very high user satisfaction rate.	circuit complexity;computation;file spanning;homography (computer vision);map;object detection;pixel;random sample consensus;sensor	Yun Zhai;Mubarak Shah	2006		10.1145/1180639.1180824	computer vision;computer graphics (images)	Vision	38.44734070119189	-53.425162602047855	172917
d8a9b7ea0a0ec4c807b7d3a1ec6b0ce322b9e491	pedestrian detection based on hog-lbp feature	detectors;linear svm;hog lbp feature;support vector machines;support vector machines feature extraction image classification object detection pedestrians;training;image classification;computer vision;feature vector;pedestrians;vectors;feature extraction;pedestrian detection;feature extraction detectors humans computer vision training vectors support vector machines;linear svm pedestrian detection hog lbp feature;humans;support vector machine;histogram of oriented gradient descriptor pedestrian detection performance hog lbp feature variable size block feature vector linear svm pedestrian classifier training inria dataset;object detection	in this paper, we propose a new framework in pedestrian detection by combining the HOG and uniform LBP feature on blocks. Contrast experiment result shows that detector using combined features is more powerful than one single feature. To further improve the detection performance, we make a contrast experiment that the HOG-LBP features are calculated at variable-size blocks to find the most efficient feature vector. The linear SVM is used to train the pedestrian classifier. Results presented on the INRIA dataset show that our detector is more discriminative and robust than the state-of-the-art algorithms.	algorithm;belief propagation;discriminative model;experiment;feature vector;local binary patterns;pedestrian detection	Guolong Gan;Jian Cheng	2011	2011 Seventh International Conference on Computational Intelligence and Security	10.1109/CIS.2011.262	support vector machine;computer vision;computer science;machine learning;pattern recognition;feature	Vision	32.55374960917099	-55.99826518471539	173229
66fe32c2c6c953d5bbba0e20198180fd7f1705fb	text detection, localization and segmentation in compressed videos	semantic based video analysis;video coding data compression discrete cosine transforms image texture object detection text analysis;data compression;retrieval;video analysis;text analysis;foreground and background integrated text detection compressed videos video text segmentation video text localization video text information semantic based video analysis indexing retrieval mpeg coding techniques dct block texture constraint vertical aligned text lines determination;indexing and retrieval;image texture;video coding;video text localization;video text segmentation;indexing;discrete cosine transforms;adaptive method;compressed videos;block texture constraint;vertical aligned text lines determination;text detection;foreground and background integrated;video text information;mpeg coding techniques;text segmentation;dct;object detection;videos discrete cosine transforms layout image edge detection indexing optical character recognition software speech analysis asia information analysis information retrieval;compressed video	Video text information plays an important role in semantic-based video analysis, indexing and retrieval. Video texts are closely related to the content of a video. Text-based video analysis, browsing and retrieval are usually carried out in the following for steps: video text detection, localization, segmentation and recognition. Videos are commonly stored in compressed formats where MPEG coding techniques are adopted. In this paper, a DCT coefficient based multilingual video text detection and localization scheme for compressed videos is proposed. Candidate text blocks are detected in terms of block texture constraint. An adaptive method for the horizontal and vertical aligned text lines determination is then designed according to the run length of the horizontal and vertical block numbers. The remaining block regions are further verified by local block texture constraints. And the text block region can be localized by virtue of the horizontal and vertical block texture projections. Finally, a foreground and background integrated (FBI) video text segmentation approach is adopted in this paper to eliminate the complex background in text regions. The final experimental results show the effectiveness of our methods	coefficient;data compression;discrete cosine transform;image segmentation;internationalization and localization;moving picture experts group;run-length encoding;text segmentation;text-based (computing);video content analysis;video game localization	Xueming Qian;Guizhong Liu	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660360	data compression;image texture;text segmentation;computer vision;search engine indexing;text mining;speech recognition;computer science;discrete cosine transform;video tracking;multimedia	Vision	38.93297943755017	-52.3917228221472	173393
7b4c635993f68e4842f6db93457895fcb94dd389	dealing with non-linearity in shape modelling of articulated objects	independent component analysis;gaussian mixture model;human body;nearest neighbor;number of clusters;computer science and informatics;point of view;point distribution model	We address the problem of non-linearity in 2D Shape modelling of a particular articulated object: the human body. This issue is partially resolved by applying a different Point Distribution Model (PDM) depending on the viewpoint. The remaining non-linearity is solved by using Gaussian Mixture Models (GMM). A dynamic-based clustering is proposed and carried out in the Pose Eigenspace. A fundamental question when clustering is to determine the optimal number of clusters. From our point of view, the main aspect to be evaluated is the mean gaussianity. This partitioning is then used to fit a GMM to each one of the view-based PDM, derived from a database of Silhouettes and Skeletons. Dynamic correspondences are then obtained between gaussian models of the 4 mixtures. Finally, we compare this approach with other two methods we previously developed to cope with non-linearity: Nearest Neighbor (NN) Classifier and Independent Component Analysis (ICA).	cluster analysis;google map maker;independent computing architecture;independent component analysis;mixture model;non-functional requirement;non-repudiation;nonlinear system;point distribution model	Grégory Rogez;Jesús Martínez del Rincón;Carlos Orrite-Uruñuela	2007		10.1007/978-3-540-72847-4_10	independent component analysis;point distribution model;human body;computer science;machine learning;pattern recognition;mixture model;mathematics;k-nearest neighbors algorithm;statistics	ML	34.86203943727649	-56.92089852012747	173444
2849466611325897882d65d4ae07086193de52d3	image level fusion method for multimodal 2d + 3d face recognition	3d face recognition;3d imaging;face recognition;multimodal;3d representation;image level fusion	Most of the existing multimodal 2D + 3D face recognition approaches do not account for the dependency between 2D and 3D representations of a face. This dependency reduces the benefit of fusion at the late-stage feature or metric level. On the other hand, it is advantageous to fuse at the early stage. We propose an image-level fusion method that explores the dependency between modalities for face recognition. Facial cues from 2D and 3D images are fused into more independent and discriminating data by finding fusion axes that pass through the most uncorrelated information in the images. Experimental results based on our face database of 1280 2D + 3D facial samples from 80 adults show that our image-level fusion approach outperforms the pixel- and metric-level fusion approaches.		Gede Putra Kusuma Negara;Chin-Seng Chua	2008		10.1007/978-3-540-69812-8_98	facial recognition system;stereoscopy;computer vision;face detection;speech recognition;computer science;multimodal interaction;pattern recognition;three-dimensional face recognition	Vision	38.15970923164463	-54.816346557427586	173697
fd5ce681cb1e6d429e7f31789e36e00c0f34e36d	saliency analysis and region-of-interest extraction for satellite images by biological sparse modeling	geoscience and remote sensing;ieee multimedia;indexes;machine intelligence;multimedia communication;decision support systems;information processing	Traditional models for saliency analysis in satellite images cannot genuinely mimic the selection mechanism of human vision system. Furthermore, feature selection needs variant considering the complexity of data distribution of different satellite images thereby not being one-size-fits-all. Aiming at these problems, we propose a novel model based on sparse representation for saliency analysis with biological plausibility and preferably, our model only needs to decide the number of feature without considering feature complexity and massive parameters tuning in other feature learning algorithms. First, sparse filtering is adopted to learn a sparse dictionary for satellite images. Then, we use Incremental Coding Length (ICL) to measure the saliency contribution of every feature for the final saliency map. The region-of-interest (ROI) can be extracted based on saliency maps by thresholding segmentation. Experimental results show that our model achieves better performance compared with several traditional models for saliency analysis and ROIs extraction in satellite images.	algorithm;dictionary;fits;feature learning;feature selection;icl;machine learning;map;performance tuning;plausibility structure;region of interest;sparse approximation;sparse matrix;thresholding (image processing)	Libao Zhang;Xu Liang;Jie Chen	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532861	database index;computer vision;information processing;computer science;kadir–brady saliency detector;machine learning;pattern recognition	Robotics	35.082927959535716	-55.38870698314238	174036
2d818e0c9987dbe98bd1caebc2c87fc4c87ff4eb	image retrieval based on independent components of color histograms	high dimensionality;color histogram;independent component analysis;feature vector;principal component analysis;visual features;orthogonal transformation;independent component;spatial information;image retrieval	  Color histograms are effective for representing color visual features. However, the high dimensionality of feature vectors  results in high computational cost. Several transformations, including principal component analysis (PCA), have been proposed  to reduce the dimensionality. PCA reduce the dimensionality by projecting the data to a subspace which contains most of the  variance. It is restricted to an orthogonal transformation and may not be the optimal to represent the intrinsic features  of data. In this paper, we apply independent component analysis (ICA) to extract the features in color histograms. PCA is  applied to reduce the dimensionality and then ICA is performed on the low-dimensional PCA subspace. Furthermore, spatial information  is incoporated by performing ICA on a color coherent vector (CCV). The experimental results show that the proposed method  outperform other methods based on SVD of quadratic matrix or PCA, in terms of retrieval accuracy.    	image retrieval	Xiang-Yan Zeng;Yen-Wei Chen;Zensho Nakao;Jian Cheng;Hanqing Lu	2003		10.1007/978-3-540-45224-9_193	color histogram;computer vision;visual word;pattern recognition;information retrieval	Vision	35.690446803128985	-58.567910815516385	174079
b00ee92b0f8ec2007f4c1c559dc98028959babab	sparse feature maps in a scale hierarchy	scale hierarchy;edge detection;technology;teknikvetenskap;engineering and technology;pattern detection;teknik och teknologier;characteristic phase;image representation;lateral inhibition;view centered representation;sparse coding	This article describes an essential step towards what is called a view centered representation of the low-level structure in an image. Instead of representing low-level structure (lines and edges) in one compact feature map, we will separate structural information into several feature maps, each signifying features at a characteristic phase, in a specific scale. By characteristic phase we mean the phases 0, π, and ±π/2, corresponding to bright, and dark lines, and edges between different intensity levels, or colours. A lateral inhibition mechanism selects the strongest feature within each local region of scale represented. The scale representation is limited to maps one octave apart, but can be interpolated to provide a continous representation. The resultant image representation is sparse, and thus well suited for further processing, such as pattern detection.	color;high- and low-level;interpolation;lateral computing;lateral thinking;level structure;map;pattern recognition;resultant;sparse matrix	Per-Erik Forssén;Gösta H. Granlund	2000		10.1007/10722492_13	computer vision;feature detection;edge detection;machine learning;pattern recognition;mathematics;feature	Vision	37.879184026667005	-55.434288555988296	174237
82bd9bf10d4f646a46f8affbeb5581e34903d832	coarse-to-fine multiclass nested cascades for object detection	multiclass cascade;training;coarse to fine;training object detection face detection feature extraction robustness accuracy;computer vision;object detection computer vision;accuracy;ctf nested cascade;ctf nested cascade coarse to fine multiclass nested cascade object detection computer vision;feature extraction;coarse to fine multiclass nested cascade;adaboost;robustness;adaboost object detection multiclass cascade coarse to fine;face detection;object detection	Building robust and fast object detection systems is an important goal of computer vision. A problem arises when several object types are to be detected, because the computational burden of running several specific classifiers in parallel becomes a problem. In addition the accuracy and the training time can be greatly affected. Seeking to provide a solution to these problems, we extend cascade classifiers to the multiclass case by proposing the use of multiclass coarse-to-fine (CTF) nested cascades. The presented results show that the proposed system scales well with the number of classes, both at training and running time.	charge trap flash;computation;computer vision;object detection;time complexity;type system	Rodrigo Verschae;Javier Ruiz-del-Solar	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.93	adaboost;computer vision;face detection;feature extraction;computer science;machine learning;pattern recognition;accuracy and precision;robustness	Vision	32.490204774806486	-55.635783948928015	174244
dca59f5806f29c772f6ad3746343340191b1a670	mpeg-2 video copy detection method based on sparse representation of spatial and temporal features	histograms;mpeg 2 video copy detection sparse representation spatial features temporal features single feature video content matching video transformations visual saliency model global feature hsv color histograms local feature orb features sparse coding motion vector extraction video bitstreams mv angle histograms trecvid 2009 dataset;video coding content based retrieval feature extraction image matching image motion analysis;sparse coding video copy detection spatial and temporal features visual saliency model;transform coding;data mining;visual saliency model;video copy detection;feature extraction image color analysis histograms transform coding encoding robustness data mining;image color analysis;feature extraction;robustness;spatial and temporal features;encoding;sparse coding	Due to the wide variety of copy videos, the existing video copy detection methods using single feature face great challenges, especially for video content matching, which are difficult to deal with various copy video transformations. To overcome this problem, a video copy detection method based on sparse representation of MPEG-2 spatial and temporal features is proposed in this paper. Firstly, the key frames are extracted based on visual saliency model, Then the global feature (HSV color histograms) and local feature (ORB features) are extracted from the key frames, Meanwhile, the key frames are represented compactly by sparse coding which exploits ORB features, and motion vectors (MV) extracted from the video bitstreams are exploited to build MV angle histograms. Finally, spatial feature and temporal feature are compared respectively, and matching results are fused to generate the final copy detection judgement. The experimental results on dataset TRECVID 2009 show that the proposed method presents better robustness and higher time efficiency.	algorithm;digital video;h.262/mpeg-2 part 2;key frame;mpeg-2;neural coding;preprocessor;sparse approximation;sparse matrix;video copy detection	Dongyue Ren;Li Zhuo;Hai-Xia Long;Panling Qu;Jing Zhang	2016	2016 IEEE Second International Conference on Multimedia Big Data (BigMM)	10.1109/BigMM.2016.21	computer vision;speech recognition;computer science;pattern recognition;feature	Vision	37.824596554924085	-52.260587847897696	174704
a872d7283bb7857f6fc63b5fda1c53e252e85766	real-time hand detection based on multi-stage hog-svm classifier	speed performance realtime hand detection multistage hog svm classifier histogram of gradients support vector machines learning feature vector decomposition skin color information integral histogram;support vector machines;image classification;support vector machines feature extraction image classification image colour analysis learning artificial intelligence object detection statistical analysis;statistical analysis;image colour analysis;feature extraction;integral image human computer interaction hand detection hog svm classifier;learning artificial intelligence;object detection	In this paper, we propose a real-time hand detection method with multi-stage HOG-SVM classifier. Unlike traditional methods based on learning which make decomposition of feature vector or combination of different types of features or classifiers, upon the division of background into several categories, we propose a multi-stage classifier which combines several SVM classifies each of which is trained to distinguish corresponding divisions of background and target. Furthermore, in order to improve speed performance, skin color information and integral histogram are also applied. Experiment results demonstrate that the proposed algorithm works well under multiple challenging backgrounds in real-time speed (16 frames per second).	algorithm;feature vector;real-time clock;real-time transcription	Jiang Guo;Jun Cheng;Jianxin Pang;Yu Guo	2013	2013 IEEE International Conference on Image Processing	10.1109/ICIP.2013.6738846	margin classifier;support vector machine;computer vision;contextual image classification;feature detection;quadratic classifier;feature extraction;computer science;machine learning;linear classifier;pattern recognition;feature;structured support vector machine	Robotics	32.48684871177851	-57.167357148141555	175169
80be3d05573c4bd9ecb0d7ecd52997a7c7406c23	hyperspectral face recognition via feature extraction and crc-based classifier		Hyperspectral face recognition provides improved classification rates due to its abundant information in the face cubes of every subject in hyperspectral face databases. However, it is less popular in face recognition due to its difficulty in data acquisition, low signal-to-noise ratio, and high dimensionality. The authors compare five existing descriptors that are frequently used in 2D face recognition, and use collaborative representation classifier (CRC) with two voting techniques for hyperspectral face recognition. Experimental results demonstrate that, for PolyU-HSFD database, Gabor filter bank-based features are very robust to both Gaussian white noise and shot noise, and it achieves very competitive classification results. For CMU-HSFD database, when the noise level is low, histogram of oriented gradients (HOG) yields good classification results. In addition, when the noise level is high, raw facial images without feature extraction perform very well in term of correct classification rate. The local binary pattern and HOG descriptor are very sensitive to noise even though they achieve rather good classification rates if the facial images contain no noise. The best recognition result for the PolyU-HSFD is 96.4% ± 2.3 and for the CMU-HSFD is 98.0% ± 0.7.	binary pattern (image generation);cyclic redundancy check;data acquisition;database;facial recognition system;feature extraction;filter bank;gabor filter;gradient;histogram of oriented gradients;noise (electronics);olap cube;shot noise;signal-to-noise ratio;white noise	Guangyi Chen;Changjun Li;Wei Sun	2017	IET Image Processing	10.1049/iet-ipr.2016.0722	speech recognition;machine learning;pattern recognition	Vision	33.62158680503842	-58.48116019187837	175665
7cdda91b72950c9eaf8a27f16f41720703e31897	efficient rectangle feature extraction for real-time facial expression recognition based on adaboost	image classification face recognition feature extraction;facial expression recognition;pattern classification rectangle feature feature selection facial expression recognition adaboost;feature extraction face recognition face detection solid modeling active appearance model principal component analysis computer science pattern recognition pattern classification humans;conference;real time;image classification;face recognition;feature extraction;pattern classification feature extraction facial expression recognition rectangle feature face detection adaboost algorithm feature selection;pattern classification;feature selection;face detection	In this paper, we propose a method of selecting new types of rectangle features that are suitable for facial expression recognition. The basic concept in this paper is similar to Violar's approach, which is used for face detection. Instead of previous Haar-like rectangle features, we choose rectangle features for facial expression recognition among all possible rectangle types in a 3/spl times/3 matrix form using the AdaBoost algorithm. Also, the facial expression recognition system constituted with the proposed rectangle features is compared to that with previous rectangle features with regard to its capacity. The results show that the proposed approach has better performance in facial expression recognition in terms of simulation and experimental results.	adaboost;algorithm;face detection;feature extraction;haar wavelet;real-time clock;simulation	Sung-Uk Jung;Do Hyoung Kim;Kwang Ho An;Myung Jin Chung	2005	2005 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2005.1545534	computer vision;contextual image classification;face detection;feature;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;feature selection;feature;face hallucination	Robotics	32.463305229842696	-57.44868748161787	176419
db88343586648929db5fad6920f1ff626d94659c	cross grouping strategy based 2dpca method for face recognition	2dpca;singular value decomposition;cross covariance;facial image recognition;grouping strategy	Graphical abstractDisplay Omitted HighlightsAn algorithm reduces the redundancy among row and column vectors of all images.In this algorithm, the cross-covariance matrix (CCM) is calculated.CCM completely preserves the covariance information of PCA between local geometric structures in the image matrix which is partially maintained in 2DPCA and its variants.A new grouping strategy that reduces the intra-group correlation is proposed.Compared to 2DPCA this algorithm is more robust to changes in lighting conditions. Grouping strategy exactly specifies the form of covariance matrix, therefore it is very essential. Most 2DPCA methods use the original 2D image matrices to form the covariance matrix which actually means that the strategy is to group the random variables by row or column of the input image. Because of their grouping strategies these methods have two main drawbacks. Firstly, 2DPCA and some of its variants such as A2DPCA, DiaPCA and MatPCA preserve only the covariance information between the elements of these groups. This directly implies that 2DPCA and these variants eliminate some covariance information while PCA preserves such information that can be useful for recognition. Secondly, all the existing methods suffer from the relatively high intra-group correlation, since the random variables in a row, column, or a block are closely located and highly correlated. To overcome such drawbacks we propose a novel grouping strategy named cross grouping strategy. The algorithm focuses on reducing the redundancy among the row and the column vectors of the image matrix. While doing this the algorithm completely preserves the covariance information of PCA between local geometric structures in the image matrix which is partially maintained in 2DPCA and its variants. And also in the proposed study intra-group correlation is weak according to the 2DPCA and its variants because the random variables spread over the whole face image. These make the proposed algorithm superior to 2DPCA and its variants. In order to achieve this, image cross-covariance matrix is calculated from the summation of the outer products of the column and the row vectors of all images. The singular value decomposition (SVD) is then applied to the image cross-covariance matrix. The right and the left singular vectors of SVD of the image cross-covariance matrix are used as the optimal projective vectors. Further in order to reduce the dimension LDA is applied on the feature space of the proposed method that is proposed method+LDA. The exhaustive experimental results demonstrate that proposed grouping strategy for 2DPCA is superior to 2DPCA, its specified variants and PCA, and proposed method outperforms bi-directional PCA+LDA.	facial recognition system	Ü. Çigdem Turhal;Alpaslan Duysak	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.01.016	estimation of covariance matrices;mathematical optimization;cross-covariance;machine learning;pattern recognition;mathematics;singular value decomposition;statistics	Vision	34.76625537841004	-57.72999112339388	177085
f31d21c8be60efc60e9501e9f33adab6d2fcc361	object detection applied to indoor environments for mobile robot navigation	robot navigation;mobile robots;object classification;support vector machine;shapes descriptors;object detection	To move around the environment, human beings depend on sight more than their other senses, because it provides information about the size, shape, color and position of an object. The increasing interest in building autonomous mobile systems makes the detection and recognition of objects in indoor environments a very important and challenging task. In this work, a vision system to detect objects considering usual human environments, able to work on a real mobile robot, is developed. In the proposed system, the classification method used is Support Vector Machine (SVM) and as input to this system, RGB and depth images are used. Different segmentation techniques have been applied to each kind of object. Similarly, two alternatives to extract features of the objects are explored, based on geometric shape descriptors and bag of words. The experimental results have demonstrated the usefulness of the system for the detection and location of the objects in indoor environments. Furthermore, through the comparison of two proposed methods for extracting features, it has been determined which alternative offers better performance. The final results have been obtained taking into account the proposed problem and that the environment has not been changed, that is to say, the environment has not been altered to perform the tests.	autonomous robot;bag-of-words model;esthesia;mobile robot;object detection;physical object;shape analysis (digital geometry);support vector machine;biologic segmentation	Alejandra Carolina Hernández;Clara Gómez;Jonathan Crespo;Ramón Barber	2016		10.3390/s16081180	mobile robot;support vector machine;computer vision;simulation;computer science;mobile robot navigation	Robotics	32.60537792062118	-55.1562544193389	177108
2fcdb78b923cc7d8293de62ab9065a08a07421be	united zernike invariants for character images	t technology general;united moment invariant;qa75 electronic computers computer science;feature extraction;granular mining;zernike moment;pattern recognition;zernike moment invariant;intra class;moment invariant;inter class	Feature extraction is one of the major components in traditional pattern recognition. There are many methods for extracting the features, either structural approach or global approach. In this paper, we present integrated formulation of Zernike Moments and United Moment Invariant for extracting the character images accordingly. The extraction values are validated by measuring the Inter-class and intra-class analysis to illustrate the effectiveness of the proposed solution. The results yield that the proposed method are feasible and better for extracting the images for both inter-class and intra-class analysis.	feature extraction;image moment;pattern recognition	Norsharina Abu Bakar;Siti Mariyam Hj. Shamsuddin	2009		10.1007/978-3-642-05036-7_47	computer vision;machine learning;pattern recognition;mathematics	Vision	36.539294977651814	-58.58474431383235	177350
d22a8bac307e1550a9542c3d4e316496b968bf4f	advancing large scale object retrieval	visual search	The objective of this work is object retrieval in large scale image datasets, where the object is specified by an image query and retrieval should be immediate at run time. Such a system has a wide variety of applications including object or location recognition, video search, near duplicate detection and 3D reconstruction. The task is very challenging because of large variations in the imaged object appearance due to changes in lighting conditions, scale and viewpoint, as well as partial occlusions. A starting point of established systems which tackle the same task is detection of viewpoint invariant features, which are then quantized into visual words and efficient retrieval is performed using an inverted index. We make the following three improvements to the standard framework: (i) a new method to compare SIFT descriptors (RootSIFT) which yields superior performance without increasing processing or storage requirements; (ii) a novel discriminative method for query expansion; (iii) a new feature augmentation method. Scaling up to searching millions of images involves either distributing storage and computation across many computers, or employing very compact image representations on a single computer combined with memory-efficient approximate nearest neighbour search (ANN). We take the latter approach and improve VLAD, a popular compact image descriptor, using: (i) a new normalization method to alleviate the burstiness effect; (ii) vocabulary adaptation to reduce influence of using a bad visual vocabulary; (iii) extraction of multiple VLADs for retrieval and localization of small objects. We also propose a method, SCT, for extremely low bit-rate compression of descriptor sets in order to reduce the memory footprint of ANN. The problem of finding images of an object in an unannotated image corpus starting from a textual query is also considered. Our approach is to first obtain multiple images of the queried object using textual Google image search, and then use these images to visually query the target database. We show that issuing multiple queries significantly improves recall and enables the system to find quite challenging occurrences of the queried object. Current retrieval techniques work only for objects which have a light coating of texture, while failing completely for smooth (fairly textureless) objects best described by shape. We present a scalable approach to smooth object retrieval and illustrate it on sculptures. A smooth object is represented by its imaged shape using a set of quantized semi-local boundary descriptors (a bag-of-boundaries); the representation is suited to the standard visual word based object retrieval. Furthermore, we describe a method for automatically determining the title and sculptor of an imaged sculpture using the proposed smooth object retrieval system. This thesis is submitted to the Department of Engineering Science, University of Oxford, in fulfilment of the requirements for the degree of Doctor of Philosophy. This thesis is entirely my own work, and except where otherwise stated, describes my own research. Relja Arandjelović, Christ Church	2.5d;3d reconstruction;approximation algorithm;computation;computer;failure;image retrieval;inverted index;memory footprint;nearest neighbor search;query expansion;requirement;run time (program lifecycle phase);scalability;semiconductor industry;visual word;visual descriptor;vocabulary	Relja Arandjelovic	2013			visual word	Vision	38.9122356640663	-55.03700843040275	177524
e0e650ce4b9c891285501933b242b12047e2b143	traffic incident classification at intersections based on image sequences by hmm/svm classifiers	intelligent transport system;typical urban;hidden markov model;image database;hmm;its;feature extraction;signal processing;image sequence;svm;traffic incident detection;support vector machine;intersection;quick response	With the development of modern intelligent transportation systems (ITS), automatic traffic incident detection with quick response and high accuracy becomes one of the most important issues, especially for metropolitan streets that are full of signaled intersections. In this paper, we present our up-to-date research outcomes of the traffic incident detection system, which makes use of the image sequences gathered from a typical urban intersection. Basic image signal processing was used to extract image difference information for traffic image database construction. Feature extraction algorithms were then discussed and compared including PCA, FFT, and hybrid analysis of DCT-FFT. Finally, multi-classification of traffic signal logics (East–West, West–East, South–North, North–South) and accidents were realized by HMM (Hidden Markov Model) and SVM (Support Vector Machine) respectively. Experimental results showed that the hybrid DCT-FFT method gives the best features, and classification performance of SVM is superior to HMM with limited training samples, where the correction rate is 100% for SVM and 91% for HMM.	algorithm;automatic identification and data capture;experiment;feature extraction;hidden markov model;markov chain;signal processing;support vector machine	Yuexian Zou;Guangyi Shi;Hang Shi;He Zhao	2010	Multimedia Tools and Applications	10.1007/s11042-010-0466-6	support vector machine;speech recognition;computer science;machine learning;pattern recognition;hidden markov model	AI	31.981253058955712	-56.91525154907254	178346
2ee7d609c4e4730d623885b82bd1cf3db8b76a27	matching content-based saliency regions for partial-duplicate image retrieval	histograms;image segmentation;relative saliency ordering constraint;interest points;layout;visualization image retrieval image color analysis histograms image segmentation layout;visualization;content based saliency region;local features;image color analysis;relative saliency ordering constraint content based saliency region partial duplicate image retrieval;bag of visual words;visual attention;partial duplicate image retrieval;image retrieval	In traditional partial-duplicate image retrieval, images are commonly represented using the Bag-of-Visual-Words (BOV) model built from image local features, such as SIFT. Actually, there is only a small similar portion between partial-duplicate images so that such representation on the whole image is not adequate for the partial-duplicate image retrieval task. In this paper, we propose a novel perspective to retrieval partial-duplicate images with Contented-based Saliency Region (CSR). CSRs are such sub-regions with abundant visual content and high visual attention in the image. The content of CSR is represented with the BOV model while saliency analysis is employed to ensure the high visual attention of CSR. Each CSR is regarded as an independent unit to be retrieved in the dataset. To effectively retrieve the CSRs, we design a relative saliency ordering constraint, which captures a weak saliency relative layout among interest points in the CSR. Comparison experiments with four state-of-the-art methods on the standard partial-duplicate image dataset clearly verify the effectiveness of our scheme. Further, our approach can provide a more diverse retrieval result, which facilitates the interaction of portable-device users.	bag-of-words model in computer vision;experiment;image retrieval;scale-invariant feature transform	Liang Li;Zhipeng Wu;Zheng-Jun Zha;Shuqiang Jiang;Qingming Huang	2011	2011 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2011.6011895	image texture;layout;computer vision;visual word;visualization;image retrieval;computer science;machine learning;pattern recognition;histogram;bag-of-words model in computer vision;image segmentation;automatic image annotation;information retrieval;statistics	Vision	37.601035190745144	-56.81152047485645	178405
024eb736e2278696da2de3c1499e956e89d7a4c2	a new co-saliency model via pairwise constraint graph matching	graph theory;image matching;image matching graph theory;cosaliency model multiple image saliency map single image saliency map unary constraint matching cosaliency maps pairwise constraint graph matching;feature extraction mathematical model educational institutions dynamic programming artificial intelligence signal processing communication systems	In this paper, we propose a new co-saliency model to extract co-saliency maps from a pair of images. Rather than using unary constraint matching, we use pairwise constraint graph matching to obtain more accurate co-saliency map. In our method, the co-saliency map consists of two terms, i.e., the single image saliency map and the multiple image saliency map. The single image saliency map is obtained by traditional saliency detection method. The multiple image saliency map is extracted by matching the similar regions among the images, which is casted as pairwise constraint graph matching problem. The dynamic programming method is used to solve the matching problem. We test the proposed co-saliency model on co-saliency dataset. The experimental results demonstrate the effectiveness of the proposed co-saliency model.	autostereogram;co-np;constraint graph;dynamic programming;unary operation	Fanman Meng;Hongliang Li;Guanghui Liu	2012	2012 International Symposium on Intelligent Signal Processing and Communications Systems	10.1109/ISPACS.2012.6473597	computer vision;3-dimensional matching;machine learning;pattern recognition;mathematics	Vision	36.82088716019568	-53.97701747107209	178783
a33966ad39051c6fb102f4e3b887b1337679d1eb	two-directional two-dimensional modified fisher principal component analysis: an efficient approach for thermal face verification	matrices;principal component analysis	In recent years, verification based on thermal face images has been extensively studied because of its invariance to illumination and immunity to forgery. However, most of them have not given full consideration to high-verification performance and singular withinclass scatter matrix problems. We propose a novel thermal face verification algorithm, which is named two-directional two-dimensional modified Fisher principal component analysis. First, two-dimensional principal component analysis (2-DPCA) is utilized to extract the optimal projective vector in the row direction. Then, 2-D modified Fisher linear discriminant analysis is implemented to overcome the singular withinclass scatter matrix problem of the 2-DPCA space in the column direction. Comparative experiments on the natural visible and infrared facial expression thermal face subdatabase demonstrate that the proposed approach outperforms state-of-the-art methods in terms of verification performance. © 2013 SPIE and IS&T [DOI: 10.1117/1.JEI.22.2 .023013]	algorithm;experiment;linear discriminant analysis;optimal projection equations;principal component analysis	Ning Wang;Qiong Li;Ahmed A. Abd El-Latif;Jialiang Peng;Xiamu Niu	2013	J. Electronic Imaging	10.1117/1.JEI.22.2.023013	speech recognition;computer science;pattern recognition;mathematics;matrix;statistics;principal component analysis	AI	34.04849293899611	-58.88877717723149	179477
74fd8a27dcf74cf3801c54ba4aff5bc869d375e4	automatic target recognition in sar images using quaternion wavelet transform and principal component analysis		Automatic target recognition (ATR) is the task of classifying sensed imagery from synthetic aperture radar (SAR) automatically into a canonical set of target classes. Here, a method to recognise different classes of military vehicles based on the combination of quaternionic wavelet transform (QWT) and principal component analysis (PCA) features is presented. To identify the certain region of SAR images, patches are extracted over the interest points detected from the SAR images. Then QWT features and PCA features are computed and combined for every patch. These extracted features are trained and classified using SVM. The performance of QWT is compared with two more multiresolution transforms such as ridgelet transform and log Gabor transform as well as the Scale and rotation-invariant interest point detector and descriptor, named speeded up robust features (SURF). Observations revealed that QWT outperforms the ridgelet transform, log-Gabor and SURF. The experimental evaluation is done using the MSTAR data...	automatic target recognition;principal component analysis;wavelet transform	S. Arivazhagan;R. Ahila Priyadharshini;L. Sangeetha	2017	IJCVR	10.1504/IJCVR.2017.10004040	wavelet;continuous wavelet transform;discrete wavelet transform	Vision	34.955461393954906	-58.312093848330704	179499
df27aac8f222eb0c6b73784102e37dccffa55ceb	a trainable system for object detection	people detection;computer vision;machine learning;pattern recognition;ear detection;support vector machine;face detection;haar wavelet transform;real time application;object detection;driver assistance system;principal component	This paper presents a general, trainable system for object detection in unconstrained, cluttered scenes. The system derives much of its power from a representation that describes an object class in terms of an overcomplete dictionary of local, oriented, multiscale intensity differences between adjacent regions, efficiently computable as a Haar wavelet transform. This example-based learning approach implicitly derives a model of an object class by training a support vector machine classifier using a large set of positive and negative examples. We present results on face, people, and car detection tasks using the same architecture. In addition, we quantify how the representation affects detection performance by considering several alternate representations including pixels and principal components. We also describe a real-time application of our person detection system as part of a driver assistance system.	background subtraction;brute-force search;computable function;dictionary;face detection;haar wavelet;object detection;pixel;real-time clock;real-time computing;support vector machine;wavelet transform;wheels	Constantine Papageorgiou;Tomaso A. Poggio	2000	International Journal of Computer Vision	10.1023/A:1008162616689	support vector machine;computer vision;face detection;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;principal component analysis	Vision	32.446556306378596	-56.21118290067289	179560
a2782f604ec94b2967ef9edfdedb583f1eead7aa	design and evaluation of a real-time character recognition system		Real-time character recognition systems are in huge demand due to the technological developments in the areas of humanoid robots, autonomous vehicles, voice-based assistive systems, natural language processing, image processing and many more. In this paper, design of a real-time character recognition system using Histogram of Gradient features and Support Vector Machine (SVM) classifier is proposed. An attempt is made to enhance the recognition accuracy by selecting the optimum HOG cell size and scaling factor of resized image for HOG feature extraction. The computation time required for each stage of the proposed real-time system is also analyzed and reported. In order to assess the performance of proposed system, samples from standard datasets are used for evaluation and their recognition accuracies are reported. Recognition accuracies ranging from 91.11% – 100.0% are obtained by the system based on the dataset and type of classifier employed.		S Mani Sarathy;J. Manikandan	2018	2018 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2018.8554539	support vector machine;image processing;control engineering;computer science;humanoid robot;image segmentation;feature extraction;computation;ranging;histogram;artificial intelligence;pattern recognition	Robotics	32.07338169693908	-56.77766936596029	180244
da2c462ac0bbb7f50314cbcd4d7abde05840fdb1	a novel robust algorithm for position and orientation detection based on cascaded deep neural network		Abstract Estimating position and orientation of the object by using machine vision is essential in industrial automation. The traditional canny operator and Hough transform edge detection algorithm is widely used, but its accuracy and real-time object recognition in complex backgrounds are very limited. Other algorithms such as SVM and BP network are usually inaccurate for regression issues. In this paper, the method of a cascade of convolution networks is proposed which results in high precision pose estimates. SSD is utilized to obtain the bounding box of the object to narrow down the recognition range. Convolution neural network is utilized to detect the orientation of the object. This method can extract weak features of the sample image. In generally, the proposed method possess a greatly improved accuracy and recognition rate compared with the traditional algorithm.	algorithm;artificial neural network;deep learning	Weiyang Lin;Xinyang Ren;Tiantian Zhou;Xiaojing Cheng;Mingsi Tong	2018	Neurocomputing	10.1016/j.neucom.2018.04.061	support vector machine;convolutional neural network;artificial neural network;artificial intelligence;machine vision;convolution;pattern recognition;mathematics;algorithm;minimum bounding box;hough transform;edge detection	ML	33.27139750050144	-54.89234576179698	180942
95f9913c87821cf4471f36fd47b1cbe487dacf45	astroid shaped dct feature extraction for enhanced face recognition	binary particle swarm optimization;discrete cosine transform;face recognition;feature extraction;feature selection	This paper proposes a novel Discrete Cosine Transform (DCT) spectrum based approach to enhance the performance of a face recognition (FR) system employing a unique astroid shaped feature selection from the DCT spectrum. Individual stages of the FR system are examined and an attempt is made to improve each stage. A Binary Particle Swarm Optimization (BPSO)-based feature selection algorithm is used to search the feature vector space for the optimal feature subset. Experimental results show the promising performance of astroid shaped DCT feature extraction for face recognition on ORL, UMIST, Extended Yale B and Color FERET databases.	database;discrete cosine transform;feret (facial recognition technology);facial recognition system;feature extraction;feature selection;feature vector;particle swarm optimization;return loss;selection algorithm	R. Divya;Anwesha Rath;S. Ramachandran;K. Manikantan	2012		10.1145/2381716.2381735	computer vision;machine learning;pattern recognition;mathematics;feature	AI	34.57007538112924	-59.06713658500563	181856
2def2cd9315d78bdec04a6ff08cb1c5f75ea078d	learning discriminative shape statistics distribution features for pedestrian detection	multiple channel feature;shape statistic distribution feature;feature learning	Discriminative feature plays an important role in object detection system and traditional tactics heavily depends on the hand-designed feature. Recent study shows that feature can be learned from data, and this idea opens a new way to deal with many computer vision problems. In this paper, we propose a novel feature which learns discriminative information based on data distribution and shape statistics for pedestrian detection. It makes use of data distribution which is rich of discriminative information from positive and negative samples, and also utilizes shape statistics which comes from average human template. The proposed method exploits the distribution in multiple channel image spaces, and learns an optimal hyper plane to separate pedestrians from background patches in specific area with shape statistics. It maintains the merit of simplicity in computation and also obtains powerful discriminative ability. Two versions of Shape Statistic Distribution features are proposed which are derived from Informed Haar-like feature, but more discriminative than the original one. Experimental results based on INRIA, ETH and Caltech-USA datasets show that our proposed methods can achieve state-of-art performance. Furthermore the running speed of our detector can reach at 22fps for 480×640 images.	pedestrian detection;statistical shape analysis	Jifeng Shen;Xin Zuo;Wankou Yang;Hualong Yu;Guohai Liu	2016	Neurocomputing	10.1016/j.neucom.2015.08.107	feature learning;computer vision;computer science;machine learning;pattern recognition;mathematics;feature;discriminative model	Vision	32.60936146639705	-54.37039779551712	181883
42f8a6d06a7a5ebed039aff5e94320742086650b	learning sparse kernel machines with biometric similarity functions for identity recognition	kernel support vector machines face training biological system modeling testing biomedical imaging;kernel;support vector machines biometrics access control image classification image matching;support vector machines;biometrics access control;image matching;training;fingerprint data sparse kernel machine biometric similarity function identity recognition similarity based classification biometric recognition biometric system matching algorithm kernel function cohort based method support vector machine biometric verification setting face data;biological system modeling;image classification;biomedical imaging;testing;face	We investigate the application of similarity-based classification to biometric recognition, interpreting similarity functions used in biometric systems (i.e., matching algorithms) as kernel functions. This leads us to formulate biometric recognition as a distinct two-class classification problem for each client, which can be solved even when no representation of biometric samples in a feature space of fixed dimensionality is available. We discuss the relationship of our approach with cohort-based methods, and show that using support vector machines exhibits several advantages, in terms of the automatic selection of the cohort size and elements, and of the possible update of each user model. A biometric verification setting is considered for the formulation of the approach, but experimental results with face and fingerprint data sets are reported for both verification and identification settings.	algorithm;biometrics;embedded system;feature vector;fingerprint;kernel (operating system);kernel method;sparse matrix;support vector machine	Battista Biggio;Giorgio Fumera;Fabio Roli	2012	2012 IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS)	10.1109/BTAS.2012.6374596	medical imaging;face;support vector machine;kernel method;contextual image classification;kernel;radial basis function kernel;computer science;machine learning;pattern recognition;data mining;geometry;software testing;polynomial kernel;signature recognition	Vision	32.79126080491029	-58.48204935458975	181915
714d819961043122902dc0a34cfc5ac8e44dd05b	adaptive component selection-based discriminative model for object detection in high-resolution sar imagery		This paper proposes an innovative Adaptive Component Selection-Based Discriminative Model (ACSDM) for object detection in high-resolution synthetic aperture radar (SAR) imagery. In order to explore the structural relationships between the target and the components, a multi-scale detector consisting of a root filter and several part filters is established, using Histogram of Oriented Gradient (HOG) features to describe the object from different resolutions. To make the detected components of practical significance, the size and anchor position of each component are determined through statistical methods. When training the root model and the corresponding part models, manual annotation is adopted to label the target in the training set. Besides, a penalty factor is introduced to compensate information loss in preprocessing. In the detection stage, the Small Area-Based Non-Maximum Suppression (SANMS) method is utilised for filtering out duplicate results. In the experiments, the aeroplanes in TerraSAR-X SAR images are detected by the ACSDM algorithm and different comparative methods. The results indicate that the proposed method has a lower false alarm rate and can detect the components accurately.	algorithm;constant false alarm rate;discriminative model;experiment;gradient;image resolution;object detection;preprocessor;sensitivity and specificity;sparse matrix;synthetic data;test set;zero suppression	Chu He;Mingxia Tu;Dehui Xiong;Feng Tu;Mingsheng Liao	2018	ISPRS Int. J. Geo-Information	10.3390/ijgi7020072	discriminative model;object detection;filter (signal processing);synthetic aperture radar;histogram;detector;constant false alarm rate;pattern recognition;training set;computer science;artificial intelligence	Vision	36.48732173747033	-57.028756545803226	182057
5b67891f177ff73a74ed24429029ce5fa92b63c9	visual vocabulary signature for 3d object retrieval and partial matching	3d visualization;shape retrieval;bag of words	In this paper a novel object signature is proposed for 3D obje ct r trieval and partial matching. A part-based representation is obtained by partitioning the objects into su bparts and by characterizing each segment with different geometric descriptors. Therefore, a B g of Wordsframework is introduced by clustering properly such descri ptors in order to define the so called 3D visual vocabulary. In this f a hion, the object signature is defined as a histogram of 3D visual wordoccurrences. Several examples on the Aim@Shape watertight dataset demonstrate the versatility of the proposed method in matching either 3D objects with ar iculated shape changes or partially occluded or compound objects. In particular, a comparison with the meth ods that participated to the Shape Retrieval contest 2007 (SHREC) reports satisfactory results for both object r trieval and partial matching.	autoregressive model;bag-of-words model in computer vision;cluster analysis;eurographics;fits;performance;programming paradigm;segment descriptor;vocabulary	Roberto Toldo;Umberto Castellani;Andrea Fusiello	2009		10.2312/3DOR/3DOR09/021-028	computer vision;visual word;computer science;pattern recognition;information retrieval	Vision	38.72347696280191	-57.86392896089788	182198
66b906180b6a9cd140ee8ea0e21d1adf71a834d9	relational kernel-based grasping with numerical features	robot grasping;relational kernels;numerical shape features;graph based representations;numerical feature pooling	Object grasping is a key task in robot manipulation. Performing a grasp largely depends on the object properties and grasp constraints. This paper proposes a new statistical relational learning approach to recognize graspable points in object point clouds. We characterize each point with numerical shape features and represent each cloud as a (hyper-) graph by considering qualitative spatial relations between neighboring points. Further, we use kernels on graphs to exploit extended contextual shape information and compute discriminative features which show improvement upon local shape features. Our work for robot grasping highlights the importance of moving towards integrating relational representations with low-level descriptors for robot vision. We evaluate our relational kernel-based approach on a realistic dataset with 8 objects.	attributed graph grammar;experiment;high- and low-level;kernel (operating system);numerical analysis;object point;point cloud;robot;statistical relational learning	Laura Antanas;Plinio Moreno;Luc De Raedt	2015		10.1007/978-3-319-40566-7_1	computer vision;machine learning;pattern recognition;mathematics	Robotics	36.814022538309494	-53.151668527519966	182309
ea29159df76f186054616d88d68eea04aea5964d	local feature descriptor using entropy rate	local descriptor;random walk;graph;nonsubsampled contourlet transform;entropy rate	Over the past decades, an increasing number of local feature descriptors have been proposed in the community of computer vision and pattern recognition. Although they have achieved impressive results in many applications, how to find a balance between accuracy and computational efficiency is still an open issue. To address this issue, we present a local feature descriptor using entropy rate (FDER), which is robust to a variety of image transformations. We first employ the nonsubsampled Contourlet transform to produce multiple support regions and design a graph structure to describe the sub-region. We then use the entropy rate of random walks on the designed graph to build the FDER descriptor. Extensive experiments demonstrate the superiority of proposed descriptor dealing with various image transformations in comparison with the existing state-of-the-art descriptors.	entropy rate;visual descriptor	Pu Yan;Dong Liang;Jun Tang;Ming Zhu	2016	Neurocomputing	10.1016/j.neucom.2016.01.083	computer vision;local binary patterns;gloh;machine learning;pattern recognition;mathematics;graph;random walk;entropy rate;statistics	Robotics	37.55617617001877	-57.92167316082381	182562
5af0b88a0885b02a20009c03c263a3dca16b6bb6	compressed domain video saliency detection using global and local spatiotemporal features	motion analysis;spatiotemporal feature;image understanding;partial decoding;compressed domain;image analysis;visual attention;video saliency detection	We propose a compressed domain video saliency detection algorithm.The proposed algorithm extracts features from the partially decoded data.The proposed algorithm performs the detection in real-time with good performance. A compressed domain video saliency detection algorithm, which employs global and local spatiotemporal (GLST) features, is proposed in this work. We first conduct partial decoding of a compressed video bitstream to obtain motion vectors and DCT coefficients, from which GLST features are extracted. More specifically, we extract the spatial features of rarity, compactness, and center prior from DC coefficients by investigating the global color distribution in a frame. We also extract the spatial feature of texture contrast from AC coefficients to identify regions, whose local textures are distinct from those of neighboring regions. Moreover, we use the temporal features of motion intensity and motion contrast to detect visually important motions. Then, we generate spatial and temporal saliency maps, respectively, by linearly combining the spatial features and the temporal features. Finally, we fuse the two saliency maps into a spatiotemporal saliency map adaptively by comparing the robustness of the spatial features with that of the temporal features. Experimental results demonstrate that the proposed algorithm provides excellent saliency detection performance, while requiring low complexity and thus performing the detection in real-time.		Se-Ho Lee;Jewon Kang;Chang-Su Kim	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2015.12.011	computer vision;image analysis;computer science;kadir–brady saliency detector;pattern recognition;computer graphics (images)	Vision	38.37491075134116	-53.58027980814374	183277
c09ad8fcb4143b307fd092d29ec4dce5390df9d2	palmprint recognition using a novel sparse coding technique	principal component analysis;information capacity;probabilistic neural network;sparse coding	This paper proposes a novel recognition method for palmprints using a new sparse coding (SC) algorithm proposed by us. This algorithm exploited the maximum Kurtosis as the sparse measure criterion, at one time, a fixed variance term of sparse coefficients is used to yield a fixed information capacity. Experimental results show that the feature basis vectors of palmprint images can be successfully extracted by using our SC algorithm. Using the radial basis probabilistic neural network (RBPNN), the classification task can be implemented easily. Finally, compared with methods of principal component analysis (PCA) and the classical SC, simulation results show that our algorithm is indeed efficient and effective in performing palmprint recognition task.	fingerprint;neural coding;sparse	Li Shang;Feng-Wen Cao;Zhiqiang Zhao;Jie Chen;Yu Zhang	2007		10.1007/978-3-540-72393-6_97	probabilistic neural network;speech recognition;computer science;machine learning;pattern recognition;sparse approximation;neural coding;principal component analysis	Vision	34.1402876295237	-58.903192687746454	183925
f9a03459f00c8ffbf0de73e1d1832ddc81ffaa02	realistic human action recognition with audio context	histograms;audio signal processing;motion pictures;support vector machines;audio modality;decision fusion scheme;visualization;feature extraction;bag of visual words model;action recognition;human action recognition;decision theory;signal representation;decision rules;signal classification;classification technique;decision fusion;humans;support vector machine;human action representation;visualization context videos feature extraction humans motion pictures histograms;bag of visual words;audio feature extraction;audio context;context;decision rule;hollywood human actions dataset human action recognition audio context visual modality audio modality bag of visual words model human action representation audio feature extraction support vector machine classification technique decision fusion scheme decision rules;visual modality;videos;hollywood human actions dataset;support vector machines audio signal processing decision theory feature extraction signal classification signal representation	Recognizing human actions in realistic scenes has emerged as a challenging topic due to various aspects such as dynamic backgrounds. In this paper, we present a novel approach to taking audio context into account for better action recognition performance, since audio can provide strong evidence to certain actions such as phone-ringing to answer-phone. At first, classifiers are established for visual and audio modalities, respectively. Specifically, bag of visual-words model is employed to represent human actions in visual modality, a number of audio features are extracted for audio modality, and Support Vector Machine (SVM) is employed as the classification technique. Then, a decision fusion scheme is utilized to fuse classification results from two modalities. Since audio context is not always helpful, two simple yet effective decision rules are developed for selective fusion. Experimental results on the Hollywood Human Actions (HOHA) dataset demonstrate that the proposed approach can achieve better recognition performance than that of integrating scene context. Therefor, our work provides strong confidence to further explore how audio context influences realistic human action recognition.	bag-of-words model in computer vision;hollywood;modality (human–computer interaction);ringing (signal);support vector machine	Qiuxia Wu;Zhiyong Wang;Feiqi Deng;David Dagan Feng	2010	2010 International Conference on Digital Image Computing: Techniques and Applications	10.1109/DICTA.2010.57	support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;decision rule;statistics	Vision	34.13886188921453	-54.94505897153837	183933
64c6aa4d3434fef4e5aefe0d2b5303c0cee9a482	face recognition with image sets using hierarchically extracted exemplars from appearance manifolds	appearance manifolds;representative face samples;video sequence;pattern clustering;nonlinear dimensionality reduction algorithm;image matching;pattern clustering face recognition feature extraction image matching image sequences;image sequence analysis;multiple shot face recognition;hierarchical agglomerative clustering;geodesic distance;hierarchically extracted exemplars;geodesic distances;face recognition;feature extraction;face recognition data mining clustering algorithms testing face detection video sequences voting image recognition image sequences computer science;isomap;majority voting;template matching hierarchically extracted exemplars appearance manifolds representative face samples video sequence multiple shot face recognition nonlinear dimensionality reduction algorithm isomap hierarchical agglomerative clustering geodesic distances;video database;template matching;image sequences	An unsupervised nonparametric approach is proposed to automatically extract representative face samples (exemplars) from a video sequence or an image set for multiple-shot face recognition. Motivated by a nonlinear dimensionality reduction algorithm called Isomap, we use local neighborhood information to approximate the geodesic distances between face images. A hierarchical agglomerative clustering (HAC) algorithm is then applied to group similar faces together based on the estimated geodesic distances which approximate their locations on the appearance manifold. We define the exemplars as cluster centers for template matching at the subsequent testing stage. The final recognition is the outcome of a majority voting scheme which combines the decisions from all the individual frames in the test set. Experimental results on a 40-subject video database demonstrate the effectiveness and flexibility of our proposed method	approximation algorithm;cluster analysis;computer cluster;disk image;facial recognition system;hierarchical clustering;high-availability cluster;isomap;linear discriminant analysis;nonlinear dimensionality reduction;nonlinear system;template matching;test set;unsupervised learning	Wei Fan;Dit-Yan Yeung	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.47	computer vision;machine learning;pattern recognition;mathematics	Vision	35.79207863164005	-52.93360776309484	184667
57e4c91a8c31ac7f1c417f57cdd76f789ce2c850	learning graph affinities for spectral graph-based salient object detection	salient object detection;spectral graph theory;graph affinities	Computer vision and pattern recognition techniques based on graph theory constitute a wellestablished research area due mainly to their success in efficiently representing and solving many related problems such as image segmentation [1], [2] and saliency estimation [9]. Graph construction for the related problems is traditionally performed manually. This construction involves three major steps: (1) defining the nodes of a graph, (2) defining the links/edges between the nodes according to a ABSTRACT	affinity analysis;baseline (configuration management);computer vision;experiment;graph (discrete mathematics);graph theory;image segmentation;object detection;pattern recognition	Çaglar Aytekin;Alexandros Iosifidis;Serkan Kiranyaz;Moncef Gabbouj	2017	Pattern Recognition	10.1016/j.patcog.2016.11.005	computer vision;machine learning;pattern recognition;mathematics;spectral graph theory	Vision	37.26960698015168	-55.936201290801144	184694
c0d398cab27fe3e3edcdca783e573e29c09153a3	fast 2d to 3d conversion using a clustering-based hierarchical search in a machine learning framework	compact surf descriptor clustering based hierarchical search automatic 2d to 3d conversion 3d display 3d content machine learning 3d structure query color image training database depth images analogous 3d structure depth map estimation depth map fusion;visual databases content based retrieval feature extraction image colour analysis image convertors image fusion image retrieval learning artificial intelligence pattern clustering three dimensional displays;color databases three dimensional displays computational efficiency image color analysis clustering algorithms estimation;database clustering 2d to 3d conversion fast conversion 3d inference machine learning hierarchical search surf descriptors	Automatic 2D-to-3D conversion is an important application for filling the gap between the increasing number of 3D displays and the still scant 3D content. However, existing approaches have an excessive computational cost that complicates its practical application. In this paper, a fast automatic 2D-to-3D conversion technique is proposed, which uses a machine learning framework to infer the 3D structure of a query color image from a training database with color and depth images. Assuming that photometrically similar images have analogous 3D structures, a depth map is estimated by searching the most similar color images in the database, and fusing the corresponding depth maps. Large databases are desirable to achieve better results, but the computational cost also increases. A clustering-based hierarchical search using compact SURF descriptors to characterize images is proposed to drastically reduce search times. A significant computational time improvement has been obtained regarding other state-of-the-art approaches, maintaining the quality results.	2d to 3d conversion;algorithmic efficiency;cluster analysis;color image;computation;database;depth map;machine learning;speeded up robust features;stereo display;time complexity	Jose L. Herrera;Carlos R. del-Blanco;Narciso N. García	2014	2014 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)	10.1109/3DTV.2014.6874736	correlation clustering;computer vision;computer science;machine learning;consensus clustering;pattern recognition;cluster analysis;automatic image annotation	AI	36.85533614882539	-55.3012296432262	184940
aed0753c5f40075931748db95bf904410e24aa99	an analysis-oriented roi based coding approach on surveillance video data		Driven by the growing amount of surveillance video data, intelligent video analysis has been applied to manipulate the stored videos automatically. As the most important method of video storage, the conventional coding method with high compression ratio severely degrades the video quality, which restrict the performance of intelligent analysis. In this paper, we propose an analysis-oriented region of interest (ROI) based coding approach to relieve this problem. We qualitatively analyze the effect of video compression on the performance of video analysis, such as feature similarity and object detection. Based on the analysis, we generate the ROI by the prior knowledge of interest objects rather than considering the characteristics of Human Visual System (HVS). Then, a weight-based rate control scheme is proposed to protect the quality of ROI by assigning more bits to encode it. Experimental results show that the proposed approach can reach 5.52% and 4.39% gains average over HEVC on the performance of feature similarity and object detection respectively under the same bitrate.	region of interest	Liang Liao;Ruimin Hu;Jing Xiao;Gen Zhan;Yu Chen;Jun Xiao	2016		10.1007/978-3-319-48896-7_42	computer science;artificial intelligence;computer vision;coding (social sciences);pattern recognition;region of interest;video quality;data compression;object detection;human visual system model;restrict	Vision	37.47309738886002	-52.38443076059486	184974
c82889c36a53d1c18eadcd7babb8a680d68c20fc	human face image searching system using sketches	japanese database;image databases;distance measure;image matching;visual databases face recognition image matching image representation image retrieval relevance feedback;gray scale;human face image system face recognition feret database japanese database face representation subspace linear discriminant analysis human in the loop geometrical relationship facial feature matching algorithm relevance feedback sketch to mug shot matching two phase method;feedback;face recognition;subspace linear discriminant analysis;face representation;image representation;principal component analysis;law enforcement;feret database;image search;human in the loop;sketch to mug shot matching;image searching system;facial features;humans image databases face recognition face detection feedback facial features law enforcement image retrieval content based retrieval gray scale;humans;geometrical relationship;facial feature matching algorithm;sketch image;face detection;two phase method;sketch image face recognition image searching system;relevance feedback;content based retrieval;human face image system;visual databases;image retrieval	This paper reports a human face image searching system using sketches. A two-phase method, namely, sketch-to-mug-shot matching and human face image searching using relevance feedback, is designed and developed. In the sketch-to-mug-shot matching phase, we have developed a facial feature matching algorithm using local and global features. A point distribution model is employed to represent local facial features while the global feature consists of a set of the geometrical relationship between facial features. It is found that the performance of the sketch-to-mug-shot matching is good if the sketch image looks like the mug shot image in the database. However, in some situations, it is hard to construct a sketch that looks like the photograph. To overcome this limitation, this paper makes use of the concept of ldquohuman-in-the-looprdquo and proposes a human face image searching algorithm using relevance feedback in the second phase. Positive and negative samples will be collected from the user. A feedback algorithm that employs subspace linear discriminant analysis for online learning of the optimal projection for face representation is then designed and developed. The proposed system has been evaluated using the FERET database and a Japanese database with hundreds of individuals. The results are encouraging.	feret (facial recognition technology);feret database;linear discriminant analysis;optimal projection equations;point distribution model;relevance feedback;search algorithm;two-phase locking	Pong C. Yuen;C. H. Man	2007	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2007.897588	computer vision;face detection;image retrieval;computer science;machine learning;pattern recognition;feedback;feret database;grayscale;principal component analysis	Vision	36.73238540315704	-56.00526508653795	185119
20d2b0be76a0ed725733f3b8153e502c466d3474	a supervised feature weighting method for salient object detection using particle swarm optimization		Salient object detection (SOD) is the task of localizing and segmenting the most conspicuous foreground object(s) from a scene. SOD has recently grabbed much attention in computer vision mainly because it helps find the objects that efficiently represent a scene and thus solve complex vision problems such as scene understanding. Since combining features is one of the most common ways to compute the final saliency map in SOD, considering the relative contribution of each feature is as important as feature extraction. In this paper, we develop a feature weighting method by utilizing Particle Swarm Optimization (PSO) to generate a suitable weight vector in order to combine features effectively. The performance of the new method is compared with six existing methods on three different data sets. The results suggest that by considering the importance of each feature in the combination process, the proposed method has achieved better performance than that of the competitive methods.	benchmark (computing);computer vision;deep learning;f1 score;feature extraction;information;internationalization and localization;mathematical optimization;object detection;particle swarm optimization;precision and recall;receiver operating characteristic;sensor;vector graphics;weight function	Shima Afzali;Bing Xue;Harith Al-Sahaf;Mengjie Zhang	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280948	task analysis;salient;visualization;feature extraction;object detection;data set;computer science;artificial intelligence;weighting;particle swarm optimization;pattern recognition	Vision	33.703135578048354	-54.54133243189229	185457
2a92c16dd5d944b84e6d6858e8bd564a39e07361	multi-graph multi-instance learning for object-based image and video retrieval	graph based learning;video retrieval;learning object;multi instance;multi instance learning;object based retrieval;image retrieval	Object-based image retrieval has been an active research topic in recent years, in which a user is only interested in some object in the images. As one promising approach, graph-based multi-instance learning has attracted many researchers. The existing methods often conduct learning on one graph, either in image level or in region level. While in this paper, by considering both image- and region-level information at the same time, a novel method based on multi-graph multi-instance learning is proposed. Two graphs are constructed in our method, and the relationship between each image and its segmented regions is introduced into an optimization framework. Moreover, our method is further extended to video retrieval. By exploring the relationships between video shots, representative images, and segmented regions, it can deal with the case when training labels are only assigned in shot level. Experimental results on the SIVAL image benchmark and the TRECVID video set demonstrate the effectiveness of our proposal.	benchmark (computing);image retrieval;mathematical optimization;object-based language	Fei Li;Rujie Liu	2012		10.1145/2324796.2324839	computer vision;visual word;image retrieval;computer science;machine learning;video tracking;pattern recognition;automatic image annotation	Vision	34.48155176440315	-53.88146173182572	185512
198f761acbce4926ffbb84ebb7fdcbba3b8975fa	kernel principle component analysis in pixels clustering	kernel image segmentation unsupervised learning information systems image analysis systems engineering and theory computer science educational institutions pattern recognition automation;pattern clustering;image segmentation;crm;gaussian processes;binary time series data;pattern;corel colour image kernel principle component analysis pixels clustering nonlinear kernel feature space kmeans gaussian mixture model kernel machine colour image segmentation;association rules;colour image segmentation;feature space;gaussian mixture model;kernel machine;image colour analysis;principal component analysis;principle component analysis;principal component analysis image colour analysis image segmentation pattern clustering gaussian processes;sequential mining	We propose two new methods in the nonlinear kernel feature space for pixel clustering based on the traditional KMeans and Gaussian Mixture Model (GMM). Unlike the previous work on the kernel machines, we give out a new perspective on the new developed kernel machines. That is, kernel principle component analysis (KPCA) combined with the KMeans and the GMM are kernel KMeans (KKMeans) and kernel GMM (KGMM), respectively. In this paper, we prove the new perspective on KKMeans and give out a clear statement on the KGMM as well. Based on this new perspectives, we can implement the KKMeans and the KGMM conveniently. At the end of the paper, we utilize these new algorithms on the problem of the colour image segmentation. Based on a series of experimental results on Corel Colour Images, we find that the KKMeans and KGMM can outperform the traditional KMeans and GMM consistently, respectively.	algorithm;cluster analysis;color image;feature vector;google map maker;image segmentation;k-means clustering;kernel (operating system);kernel method;kernel principal component analysis;loadable kernel module;mixture model;nonlinear system;pixel	Jing Li;Dacheng Tao;Weiming Hu;Xuelong Li	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.86	computer vision;customer relationship management;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;tree kernel;variable kernel density estimation;polynomial kernel;principal component analysis	ML	34.08267918345421	-58.27788419612239	185688
8258ba9179f5c22f9f544de0e4717b37adbaade9	face recognition using multiscale and spatially enhanced weber law descriptor	histograms face recognition face databases feature extraction robustness educational institutions;weber local descriptor;fisher score face recognition weber local descriptor feret;face recognition;fisher score;feature extraction;principal component analysis;principal component analysis face recognition feature extraction;feret;local binary pattern spatially enhanced weber law descriptor multiscale enhanced weber law descriptor mswld face recognition system wld histograms neighborhood histogram face image fisher ratio dominant bins extraction at t databases principal component analysis	The paper introduces multiscale spatial Weber local descriptor (MSWLD) for robust face recognition system. In the proposed method, WLD is calculated in different neighborhood (multiscale) and WLD histograms are obtained from blocks of an image to preserve spatial information. WLD histograms from different blocks are then concatenated to produce the final feature set of a face image. Fisher ratio is applied to extract the dominant bins from the final WLD histogram. The MSWLD is evaluated on FERET and AT&T databases. In the experiments, the proposed method outperformed two state of the art techniques, namely, principal component analysis and local binary pattern.	benchmark (computing);binary pattern (image generation);concatenation;database;eigenface;experiment;feret (facial recognition technology);facial recognition system;fisher information;local binary patterns;principal component analysis;v-optimal histograms	Muhammad Ghulam;George Bebis	2012	2012 Eighth International Conference on Signal Image Technology and Internet Based Systems	10.1109/SITIS.2012.24	facial recognition system;scoring algorithm;speech recognition;feature extraction;computer science;machine learning;pattern recognition;mathematics;feret;statistics;principal component analysis	Vision	35.27429566056664	-58.654526999753955	186110
11af2dc787989e948f199786a2b7a73f21d4e396	optimizing face recognition using pca		Principle Component Analysis PCA is a classical feature extraction and data representation technique widely used in pattern recognition. It is one of the most successful techniques in face recognition. But it has drawback of high computational especially for big size database. This paper conducts a study to optimize the time complexity of PCA (eigenfaces) that does not affects the recognition performance. The authors minimize the participated eigenvectors which consequently decreases the computational time. A comparison is done to compare the differences between the recognition time in the original algorithm and in the enhanced algorithm. The performance of the original and the enhanced proposed algorithm is tested on face94 face database. Experimental results show that the recognition time is reduced by 35% by applying our proposed enhanced algorithm. DET Curves are used to illustrate the experimental results.	algorithm;bézier curve;computation;data (computing);database;detection error tradeoff;eigenface;experiment;facial recognition system;feature extraction;matlab;optimizing compiler;pattern recognition;principal component analysis;test set;time complexity	Manal Abdullah;Majda Wazzan;Sahar Bo-saeed	2012	CoRR	10.5121/ijaia.2012.3203	computer science;artificial intelligence;machine learning;pattern recognition;eigenface	Vision	33.43072736254673	-58.521313944203165	186446
97692960a11d4316880fb229cca699293e133945	an efficient multi-resolution svm network approach for object detection in aerial images	databases;complexity theory;support vector machines;support vector machines computational complexity image classification image resolution object detection;computational cost multiresolution svm network approach aerial images descriptive feature hog object dimension descriptor vector dedicated svm computational complexity classification phase activation path urban object detection;aerial imagery network multi resolution object detection svm;feature extraction;neurons support vector machines feature extraction databases object detection computational efficiency complexity theory;neurons;computational efficiency;object detection	In this paper, we deal with the problem of object detection in aerial images. A lot of efficient approaches uses a cascade of classifiers which process vectors of descriptive features such as HOG. In order to take into account the variability in object dimension, features at different resolutions are often concatenated in a large descriptor vector. This prevents from taking into account explicitly the different resolutions but results in losing some valuable information. To overcome this problem, we propose to use a new method based on a SVM network. Each resolution is processed, regardless to the others, at the input layer level, by a dedicated SVM. The main drawback of using such a network is that the computational complexity for the classification phase drastically increases. We propose then to foster an incomplete exploration of the network by defining an activation path. This activation path determines an order to activate the network neurons, one after the other, and introduces a rejection rule which allows the process to end before crossing the whole network. Experimental results are obtained and assessed in an industrial application of urban object detection. We can observe an average gain of 17% in precision while the computational cost is divided by more than 5, with respect to a standard method.	activation function;aerial photography;algorithmic efficiency;computation;computational complexity theory;concatenation;object detection;rejection sampling;spatial variability;support vector machine	Jérôme Pasquet;Marc Chaumont;Gérard Subsol;Mustapha Derras	2015	2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2015.7324329	computer vision;computer science;viola–jones object detection framework;machine learning;pattern recognition	Vision	32.51091794200569	-55.06830469046685	186712
9b98e68e22f5398f054f6d1a473df5b2cd8a7a52	texture instance similarity via dense correspondences	animals;image recognition;robustness standards support vector machines fingerprint recognition image recognition animals;standards;support vector machines;support vector machines image classification image matching image representation image texture;fingerprint recognition;robustness;texture photos texture instance similarity dense correspondences alignment based recognition theory pixel based method robust dense correspondence estimation engine linear svm classifiers fingerprint recognition texture based animal identification pixel matching	This paper concerns the task of evaluating the similarity of textures instances: Rather than discriminating between different texture classes, our goal is to identify when two images display the same texture instance. To address this problem, we propose an approach inspired by alignment based recognition theories. We offer a pixel-based method, employing a robust, dense correspondence estimation engine, applied to an efficient, novel representation, to match the pixels of two texture photos. We describe means for quantifying the quality of these matches, considering in particular the quality of the flow established between the two images. These quality measures are effectively combined into similarity scores by using standard linear SVM classifiers. By relying on a general, alignment based approach our method can be applied to different problem domains (different texture classes) with little modification. We demonstrate this by reporting state-of-the-art results on benchmarks for fingerprint recognition and two new benchmarks for texture-based animal identification.	benchmark (computing);computation;fingerprint recognition;optical flow;pixel;problem domain;sequence alignment	Tal Hassner;Gilad Saban;Lior Wolf	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477724	image texture;support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics;texture compression;texture filtering;fingerprint recognition;robustness	Vision	36.764005217246115	-54.54606867483816	186762
64e55b0ad5b4a6886bda02d31358f351d3fe1cac	context-aware saliency detection	visual saliency;image saliency;context awareness;object recognition;context aware image saliency visual saliency;context aware;feature extraction context awareness visualization object recognition estimation image color analysis human factors;visualization;human factors;estimation;image color analysis;feature extraction;humans;context	We propose a new type of saliency&#x2014;context-aware saliency&#x2014;which aims at detecting the image regions that represent the scene. This definition differs from previous definitions whose goal is to either identify fixation points or detect the dominant object. In accordance with our saliency definition, we present a detection algorithm which is based on four principles observed in the psychological literature. The benefits of the proposed approach are evaluated in two applications where the context of the dominant objects is just as essential as the objects themselves. In image retargeting, we demonstrate that using our saliency prevents distortions in the important regions. In summarization, we show that our saliency helps to produce compact, appealing, and informative summaries.		Stas Goferman;Lihi Zelnik-Manor;Ayellet Tal	2012	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/TPAMI.2011.272	computer vision;estimation;visualization;feature extraction;computer science;human factors and ergonomics;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;statistics	Vision	38.04042461095868	-53.722801310604666	187129
dc60ce6eb9ace1dd9ec9daa26a0bb456532ce0b3	violence detection using oriented violent flows	oriented violent flows;adaboost;violence detection;svm	Nowadays, with so many surveillance cameras having been installed, the market demand for intelligent violence detection is continuously growing, while it is still a challenging topic in research area. Therefore, we attempt to make some improvements of existing violence detectors. The primary contributions of this paper are two-fold. Firstly, a novel feature extraction method named Oriented VIolent Flows (OViF), which takes full advantage of the motion magnitude change information in statistical motion orientations, is proposed for practical violence detection in videos. The comparison of OViF and baseline approaches on two public databases demonstrates the efficiency of the proposed method. Secondly, feature combination and multi-classifier combination strategies are adopted and excellent results are obtained. Experimental results show that using combined features with AdaBoost+Linear-SVM achieves improved performance over the state-of-the-art on the Violent-Flows benchmark.	baseline (configuration management);benchmark (computing);closed-circuit television;database;feature extraction;sensor;support vector machine	Yuan Gao;Hong W. Liu;Xiaohu Sun;Can Wang;Yi Liu	2016	Image Vision Comput.	10.1016/j.imavis.2016.01.006	adaboost;support vector machine;computer vision;simulation;computer science;machine learning;data mining;computer security	Vision	32.764923529395865	-55.5273814789202	187344
bd200a242978e86e6f3af6c89df38732ee3ebf08	facial expression recognition using local fisher discriminant analysis	facial expression recognition;local binary patterns;local fisher discriminant analysis	In this paper a new facial expression recognition method based on Local Fisher Discriminant Analysis (LFDA) is proposed. LFDA is used to extract the low-dimensional discriminant embedded data representations from the original high-dimensional local binary patterns (LBP) features. The K- nearest-neighbor (KNN) classifier with the Euclidean distance is adopted for facial expression classification. The experimental results on the popular JAFFE facial expression database demonstrate that the best accuracy based on LFDA is up to 85.71%, outperforming the used Principal Component Analysis (PCA), and Linear Discriminant Analysis (LDA).	linear discriminant analysis	Shiqing Zhang;Xiaoming Zhao;Bicheng Lei	2011		10.1007/978-3-642-23321-0_69	kernel fisher discriminant analysis;speech recognition;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis	Vision	33.92180578060598	-58.72308045669449	187600
d1e4eb7deb398c0e0340a378d41cd62189442cb7	face recognition from unconstrained three-dimensional face images using multitask sparse representation		We propose and evaluate a three-dimensional (3D) face recognition approach that applies the speeded up robust feature (SURF) algorithm to the depth representation of shape index map, under real-world conditions, using only a single gallery sample for each subject. First, the 3D scans are preprocessed, then SURF is applied on the shape index map to find interest points and their descriptors. Each 3D face scan is represented by keypoints descriptors, and a large dictionary is built from all the gallery descriptors. At the recognition step, descriptors of a probe face scan are sparsely represented by the dictionary. A multitask sparse representation classification is used to determine the identity of each probe face. The feasibility of the approach that uses the SURF algorithm on the shape index map for face identification/authentication is checked through an experimental investigation conducted on Bosphorus, University of Milano Bicocca, and CASIA 3D datasets. It achieves an overall rank one recognition rate of 97.75%, 80.85%, and 95.12%, respectively, on these datasets.	computer multitasking;facial recognition system;sparse approximation;sparse matrix	Samia Bentaieb;Abdelaziz Ouamri;Amine Naït-Ali;Mokhtar Keche	2018	J. Electronic Imaging	10.1117/1.JEI.27.1.013008	computer vision;artificial intelligence;computer science;pattern recognition;facial recognition system;sparse approximation	Vision	34.79490295661605	-56.29776739206516	187661
029317f260b3303c20dd58e8404a665c7c5e7339	character identification in feature-length films using global face-name matching	reconnaissance visage;character centered film browsing;analisis sociologico;social network services;graph theory;acoplamiento grafo;video browsing face identification movie analysis social network analysis;pattern clustering;reseau social;face track distance;naming;analyse amas;face affinity network;conductive films;navegacion informacion;motion pictures;movie analysis;graph method;video signal processing;global face name matching;cinematography;navigation information;image matching;graph matching method;information browsing;computational method;metodo grafo;film script;classification;graph matching;network analysis;face tracking;methode graphe;social network;couplage graphe;cluster analysis;face recognition;senal video;signal video;video signal processing cinematography face recognition feature extraction graph theory image matching pattern clustering social networking online;indexing;feature extraction;feature length film;denomination;social networking online;face name association;face identification;pattern recognition;face detection motion pictures humans social network services face recognition conductive films indexing content based retrieval laboratories pattern recognition;denominacion;video signal;social network analysis;analisis cluster;humans;computer method;state of the art method character identification feature length film global face name matching computer method video transcript film script name extraction face track clustering graph matching method face name association face affinity network name affinity network face track distance social network analysis character centered film browsing movie analysis;name affinity network;video browsing;video transcript;face detection;social analysis;analyse circuit;name extraction;analyse sociologique;content based retrieval;clasificacion;character identification;red social;analisis circuito;state of the art method;face track clustering	Identification of characters in films, although very intuitive to humans, still poses a significant challenge to computer methods. In this paper, we investigate the problem of identifying characters in feature-length films using video and film script. Different from the state-of-the-art methods on naming faces in the videos, most of which used the local matching between a visible face and one of the names extracted from the temporally local video transcript, we attempt to do a global matching between names and clustered face tracks under the circumstances that there are not enough local name cues that can be found. The contributions of our work include: 1) A graph matching method is utilized to build face-name association between a face affinity network and a name affinity network which are, respectively, derived from their own domains (video and script). 2) An effective measure of face track distance is presented for face track clustering. 3) As an application, the relationship between characters is mined using social network analysis. The proposed framework is able to create a new experience on character-centered film browsing. Experiments are conducted on ten feature-length films and give encouraging results.	authorization;cluster analysis;ieee xplore;matching (graph theory);mined;processor affinity;social network analysis	Yifan Zhang;Changsheng Xu;Hanqing Lu;Yueh-Min Huang	2009	IEEE Transactions on Multimedia	10.1109/TMM.2009.2030629	computer vision;search engine indexing;face detection;facial motion capture;social network analysis;speech recognition;network analysis;feature extraction;biological classification;computer science;graph theory;machine learning;film analysis;multimedia;cinematography;cluster analysis;world wide web;matching;social network	Vision	37.95347135712976	-57.164632693392385	187717
8a0eb9861ea9cf77ad391d63437e12567482059e	shape description based on bag of salience points	shape descriptor;bag of features;conferenceobject;image representation;salience point;visual words;image retrieval	Salient points are very important for image description because they are related to the visually most important parts of the image, leading to a compact and more discriminative representation close to human perception. Based on these promising features, in this paper we propose a new shape descriptor, namely Bag-of-Salience-Points (BoSP), using the shape salience points combined with the Bag-of-Visual-Words modeling approach. Each salience point, after extracted from the shape contour, is represented by its curvature value using a multi-scale procedure proposed in this work. Taking advantage of this representation, each salience is assigned to a visual word according to a Dictionary of Curvatures. The final shape representation is given by computing a histogram of visual words detected in the shape, combined with a spatial pooling approach that encodes the distance distribution of the visual words in relation the shape centroid. This proposed new shape description allows to analyze the dissimilarity between shapes using fast distance functions, such as the City-block distance, even if two shapes have different number of salience points. This is a powerful asset to reduce the computational complexity when retrieving images. Compared to other shape descriptors, the BoSP descriptor has the advantage of proving a powerful shape description with high recognition accuracy, a compact representation invariant to geometric transformations while demanding a low computational cost to measure the dissimilarity of shapes.	algorithmic efficiency;bag-of-words model in computer vision;computational complexity theory;data descriptor;dictionary;shape context;taxicab geometry;visual word	Glauco Vitor Pedrosa;Agma J. M. Traina;Célia A. Zorzo Barcelos	2015		10.1145/2695664.2695838	active shape model;computer vision;visual word;image retrieval;computer science;machine learning;pattern recognition;shape analysis;topological skeleton	Vision	39.05095604278144	-58.28144791666764	188071
94976d481c8d70e526ac384e2b83f2176e3f1a54	vehicle color classification using manifold learning methods from urban surveillance videos	signal image and speech processing;biometrics;pattern recognition;image processing and computer vision	Color identification of vehicles plays a significant role in crime detection. In this study, a novel scheme for the color identification of vehicles is proposed using the locating algorithm of regions of interest (ROIs) as well as the color histogram features from still images. A coarse-to-fine strategy was adopted to efficiently locate the ROIs for various vehicle types. Red patch labeling, geometrical-rule filtering, and a texture-based classifier were cascaded to locate the valid ROIs. A color space fusion together with a dimension reduction scheme was designed for color classification. Color histograms in ROIs were extracted and classified by a trained classifier. Seven different classes of color were identified in this work. Experiments were conducted to show the performance of the proposed method. The average rates of ROI location and color classification were 98.45% and 88.18%, respectively. Moreover, the classification efficiency of the proposed method was up to 18 frames per second.	algorithm;color histogram;color space;experiment;nonlinear dimensionality reduction;region of interest	Yu-Chen Wang;Chin-Chuan Han;Chen-Ta Hsieh;Kuo-Chin Fan	2014	EURASIP J. Image and Video Processing	10.1186/1687-5281-2014-48	color histogram;computer vision;color quantization;speech recognition;color normalization;color image;computer science;archaeology;pattern recognition;biometrics	Vision	33.60918466937326	-57.035130995731706	188181
690f0e424f77d8ef6bf70d3f85d33bf2b3609909	high order structure descriptors for scene images		Structure information is ubiquitous in natural scene image s and it plays an important role in scene representation. In this pap er, third order structure statistics (TOSS) and fourth order structu e statistics (FOSS) are exploited to encode higher order structure i nformation. Afterwards, based on the radial and normal slice of TOS S and FOSS, we propose the high order structure feature: third ord er structure feature (TOSF) and fourth order structure feature (FOS ). It is well known that scene images are well characterized by parti cular arrangements of their local structures, we divide the scene im age into the non-overlapping sub-regions and compute the proposed h igher order structural features among them. Then a scene classific tion is performed by using SVM classifier with these higher order str ucture features. The experimental results show that higher order s tructure statistics can deliver image structure information well an d its spatial envelope has strong discriminative ability.	encode;object-relational database;radial (radio);scene graph	Wenya Zhu;Xiankai Lu;Tao Xu;Ziyi Zhao	2014	CoRR		computer vision;machine learning;pattern recognition;mathematics	Vision	36.04178449764161	-56.93093930336264	188219
2441740d0a4b4ea508a05be54e53cb1c4296ece6	using boundary conditions for combining multiple descriptors in similarity based queries	multiple descriptor combination;cbir;similarity queries	"""Queries dealing with complex data, such as images, face semantic problems that might compromise results quality. Such problems have their source on the differences found between the semantic interpretation of the data and their low level machine code representation. The descriptors utilized in such representation translate intrinsic characteristics of the data usually color, shape and texture into qualifying attributes. Different descriptors represent different intrinsic characteristics that can get different aspects of the data while processing a similarity comparison among them. Therefore, the use of multiple descriptors tends to improve data separation and categorization, if compared to the use of a single descriptor. Another relevant fact is that some specific intrinsic characteristics are essential for identifying a subset of the data. Based on such premises, this work proposes the use of boundary conditions to identify image subsets and then use the best descriptor combination for each of these subsets aimed at decreasing the existing """"semantic gap"""". Throughout the conducted experiments, the use of the proposed technique had better results when compared to individual descriptor use employing the same boundary conditions and to various descriptors combination without the use of boundary conditions."""		Rodrigo Fernandes Barroso;Marcelo Ponciano-Silva;Agma J. M. Traina;Renato Bueno	2013		10.1007/978-3-642-41822-8_47	machine learning;pattern recognition;data mining;mathematics	NLP	37.626211023690125	-58.858361333705105	188236
38558bd53b5bab485ca4abca35a0401c0c387883	illumination invariant face recognition using quaternion-based correlation filters	correlation filters;illumination invariant face recognition;quaternions	Existing face recognition systems decrease their performance when face images are affected by lighting variations. Recently, several quaternionic representations of face image features and a quaternion-based correlation filter have been combined in order to cope with the effects of having non-properly illuminated face images. The use of this approach has the advantage of using only one training face image per person. In this paper, the original idea based on the unconstrained optimal trade-off quaternion filter (UOTQF) is extended and two additional different correlation filters in quaternionic domain are evaluate: a phase only quaternion filter (POQF) and a separable trade-off quaternion filter (STOQF). Three different quaternion-based correlation filters are designed and conjugated with four face feature extraction methods aiming at obtaining the best combination: a two-level discrete wavelet decomposition (DWT), image differentiation (DIF), discrete cosine transform (DCT) and local binary patterns (LBP). Verification and identification experiments confirms that when combining a quaternionic representation with a quaternion-based correlation filter, both with good discriminative power and illumination invariant properties, an improvement in face recognition accuracy is obtained.	belief propagation;coefficient;data integrity field;discrete cosine transform;discrete wavelet transform;experiment;facial recognition system;feature extraction;local binary patterns;multiresolution analysis;scott continuity;uncontrolled format string	Dayron Rizo-Rodriguez;Heydi Mendez Vazquez;Edel B. García Reyes	2012	Journal of Mathematical Imaging and Vision	10.1007/s10851-012-0352-0	computer vision;pattern recognition;mathematics;geometry;quaternion	Vision	34.84729103382649	-58.51059774277557	188314
8bf42f1b992b98565b94e614f19d0b1065d9574f	an innovative weighted 2dlda approach for face recognition	the australian standard research classification 210000 science general;two dimensional linear discriminant analysis weighted linear discriminant analysis face recognition;two dimensional linear discriminant analysis;linear discriminate analysis;conference paper;face recognition;feature extraction;weighted linear discriminant analysis;projective space;scattering matrix;weight function	Two Dimensional Linear Discrimination Analysis (2DLDA) is an effective feature extraction approach for face recognition, which manipulates on the two dimensional image matrices directly. However, some between-class distances in the projected space are too small and this may bring large error classification rates. In this paper we propose a new 2DLDA-based approach that can overcome such drawback in the 2DLDA. The proposed approach redefines the between-class scatter matrix by putting a weighting function based on the between-class distances, and this will balance the between-class distances in the projected space iteratively. In order to design an effective weighting function, the between-class distances are calculated and then used to iteratively change the between-class scatter matrix. Experimental results showed that the proposed approach can improve the recognition rates on the ORL database, the Yale database and the YaleB database in comparison with other 2DLDA variants.	facial recognition system	Chong Lu;Senjian An;Wanquan Liu;Xiaodong Liu	2009		10.1007/978-3-642-10467-1_9	facial recognition system;s-matrix;computer vision;projective space;weight function;speech recognition;feature extraction;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;statistics	Vision	34.23308231235635	-58.468783428328294	188611
c29ef0b9be909add1d3809b6491dbca043e6eb27	automatic removal of visual stop-words	stop words;image classification;bhattacharyya coefficient;entropy;bag of visual words;content based image retrieval	This paper presents a new methodology for the automatic estimation of the optimal amount of visual words that can be removed from a visual dictionary, such that no harm is induced in the discriminative potential of the resulting bag-of-visual-words representations. The proposed approach relies on a special definition of the entropy of each visual word when considered as a random variable, and a new definition of the overlap of class models computed with a normalized Bhattacharyya coefficient. We combined our proposed methodology with a recent approach that labels visual words as stop-words showing that this combination is beneficial to reduce the dimensionality of bag representations, while obtaining good results in terms of classification accuracy and retrieval performance.	bag-of-words model in computer vision;coefficient;dictionary;jaccard index;visual word	Edgar Roman-Rangel;Stéphane Marchand-Maillet	2014		10.1145/2647868.2655005	computer vision;entropy;contextual image classification;visual word;speech recognition;computer science;machine learning;pattern recognition;bag-of-words model in computer vision;stop words	Vision	34.67168988753596	-55.538287260527916	188878
2b712551fbb5c574ebec7283b0280add25103c31	fusing magnitude and phase features for robust face recognition	fusing magnitude;automatic face recognition system;local phase;robust face recognition;fourier phase;high accurate face recognition;phase feature;face recognition grand challenge;gabor magnitude;high dimensionality;multi-scale face model	High accurate face recognition is of great importance for realworld applications such as identity authentication, watch list screening, and human-computer interaction. Despite tremendous progress made in the last decades, fully automatic face recognition systems are still far from the goal of surpassing the human vision system, especially in uncontrolled conditions. In this paper, we propose an approach for robust face recognition by fusing two complementary features: one is the Gabor magnitude of multiple scales and orientations and the other is Fourier phase encoded by Local Phase Quantization (LPQ). To further reduce the high dimensionality of both features, patch-wise Fisher Linear Discriminant Analysis is applied respectively and further combined by score-level fusion. In addition, multi-scale face models are exploited to make use of more information and improve the robustness of the proposed approach. Experimental results show that the proposed approach achieves 96.09%, 95.64% and 95.15% verification rates (when FAR=0.1%) on ROC1/2/3 of Face Recognition Grand Challenge (FRGC) version 2 Experiment 4, impressively surpassing the best known results, i.e. 93.91%, 93.55%, and 93.12%.	authentication;face recognition grand challenge;facial recognition system;gabor filter;human–computer interaction;linear discriminant analysis;nos;speech recognition;uncontrolled format string	Yan Li;Shiguang Shan;Haihong Zhang;Shihong Lao;Xilin Chen	2012		10.1007/978-3-642-37444-9_47	computer vision;speech recognition;pattern recognition;three-dimensional face recognition	Vision	33.89975444142437	-58.72004849174456	189250
9f13a089ee72ec1c254adb97af5565ac2e9fdb20	an experimental comparison of three guiding principles for the detection salient image locations: stability, complexity, and discrimination	discrete wavelet transforms;detectors;haar wavelet;biological system modeling;layout;discrete cosine transform;computer vision;robust stability;image edge detection;discrete cosine transforms;machine vision;learning from examples;detectors image edge detection robust stability machine vision computer vision biological system modeling discrete cosine transforms discrete wavelet transforms layout autocorrelation;autocorrelation	We present an experimental comparison of the performance of representative saliency detectors from three guiding principles for the detection of salient image locations: locations of maximum stability with respect to image transformations, locations of greatest image complexity, and most discriminant locations. It is shown that discriminant saliency performs better in terms of 1) capturing relevant information for classification, 2) being more robust to image clutter, and 3) exhibiting greater stability to image transformations associated with variations of 3D object pose. We then investigate the dependence of discriminant saliency on the underlying set of candidate discriminant features, by comparing the performance achieved with three popular feature sets: the discrete cosine transform, a Gabor, and a Haar wavelet decomposition. It is show that, even though different feature sets produce equivalent results, there may be advantages in considering features explicitly learned from examples of the image classes of interest.	algorithm;clutter;discrete cosine transform;discriminant;haar wavelet;sensor	Dashan Gao;Nuno Vasconcelos	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.409	layout;computer vision;detector;feature detection;autocorrelation;machine vision;computer science;machine learning;discrete cosine transform;pattern recognition;mathematics;discrete wavelet transform	Vision	37.920372390616826	-58.932209639795296	189446
4678a2ae263e7952887df31f76ab404df74a4649	high performance human face recognition using gabor based pseudo hidden markov model		This paper introduces a novel methodology that combines the multi-resolution feature of the Gabor wavelet transformation (GWT) with the local interactions of the facial structures expressed through the Pseudo Hidden Markov model (PHMM). Unlike the traditional zigzag scanning method for feature extraction a continuous scanning method from top-left corner to right then top-down and right to left and so on until rightbottom of the image i.e. a spiral scanning technique has been proposed for better feature selection. Unlike traditional HMMs, the proposed PHMM does not perform the state conditional independence of the visible observation sequence assumption. This is achieved via the concept of local structures introduced by the PHMM used to extract facial bands and automatically select the most informative features of a face image. Thus, the long-range dependency problem inherent to traditional HMMs has been drastically reduced. Again with the use of most informative pixels rather than the whole image makes the proposed method reasonably faster for face recognition. This method has been successfully tested on frontal face images from the ORL, FRAV2D and FERET face databases where the images vary in pose, illumination, expression, and scale. The FERET data set contains 2200 frontal face images of 200 subjects, while the FRAV2D data set consists of 1100 images of 100 subjects and the full ORL database is considered. The results reported in this application are far better than the recent and most referred systems.	database;feret (facial recognition technology);facial recognition system;feature extraction;feature selection;gabor wavelet;hidden markov model;information;interaction;left corner;markov chain;pixel;return loss;right-to-left;top-down and bottom-up design;wavelet transform	Arindam Kar;Debotosh Bhattacharjee;Dipak Kumar Basu;Mita Nasipuri;Mahantapas Kundu	2013	IJAEC	10.4018/jaec.2013010105	computer vision;speech recognition;computer science;artificial intelligence;machine learning;pattern recognition	Vision	34.79144358390969	-57.70849851060115	189626
c0cfc9c9b5e49e1c3d93170ba64bc5690ae37687	clupea harengus: intraspecies distinction using curvature scale space and shapelets - classification of north-sea and thames herring using boundary contour of sagittal otoliths		We present a study comparing Curvature Scale Space (CSS) representation with Shapelet transformed data with a view to discriminating between sagittal otoliths of North-Sea and Thames Herring using otolith boundary and boundary metrics. CSS transformed boundaries combined with measures of their circularity, eccentricity and aspect-ratio are used to classify using nearest-neighbour selections with distance being computed using CSS matching methods. Shapelet transformed data are classified using a number of techniques (NearestNeighbour, Naive-Bayes, C4.5, Support Vector Machines, Random and Rotation Forest) and compared to CSS classification results. Both methods use Leave One Out Cross Validation (LOOCV). We describe the method of encoding and the matching algorithm used during CSS classification and give an overview of Shapelet transforms and the classifiers that are used on the data. It is shown that whilst CSS forms part of the MPEG-7 standard and performs better than random selection, it can be significantly out-performed by recent additions to machine-learning methods in this application. Shapelets also show that with regard to intra-species distinction, the discriminatory features of otolith boundaries may lie not in the major inflection points, but the boundary points between them.	c4.5 algorithm;cascading style sheets;contour line;cross-validation (statistics);distance (graph theory);mpeg-7;machine learning;naive bayes classifier;scale space;support vector machine;while	James Mapp;Mark Fisher;Anthony J. Bagnall;Jason Lines;Sally Warne;Joe Scutt Phillips	2013			calculus	AI	38.28855548620115	-58.665857121186455	190147
bbb1c9f7041a1a351b273b7f381e91d7ff4147a1	directional discriminant analysis for image feature extraction	high recognition rate;sd2dlda;directional filter banks;discriminant analysis;directional discriminant analysis;algorithms base;multiple directional two-dimensional linear;md2dlda;image feature extraction;popular algorithm;higher recognition rate;single directional two-dimensional linear;channel bank filters;feature extraction;image classification;novel image feature extraction;single directional two-dimensional linear discriminant analysis;multiple directional two-dimensional linear discriminant analysis	A novel subspace learning algorithm based on nearest feature line and directional derivative gradient is proposed in this paper. The proposed algorithm combines neighborhood discriminant nearest feature line analysis and directional derivative gradient to extract the local discriminant features of the samples. A discriminant power criterion based on nearest feature line is used to find the most discriminant direction in this paper. Some experiments are implemented to evaluate the proposed algorithm and the experimental results demonstrate the effectiveness of the proposed algorithm. © Springer International Publishing Switzerland 2014.	feature extraction;linear discriminant analysis	Lijun Yan;Jeng-Shyang Pan;Xiaorui Zhu	2013		10.1007/978-3-319-01796-9_37	computer vision;contextual image classification;feature extraction;computer science;machine learning;pattern recognition;feature	AI	34.733325669565964	-58.78176753842177	190281
b7aa21eb97509e6f6d12d84732d0a3159639466f	exploiting contextual spaces for image re-ranking and rank aggregation	contextual information;rank aggregation;image analysis;content based image retrieval;similarity measure;re ranking	The objective of Content-based Image Retrieval (CBIR) systems is to return the most similar images given an image query. In this scenario, accurately ranking collection images is of great relevance. In general, CBIR systems consider only pairwise image analysis, that is, compute similarity measures considering only pair of images, ignoring the rich information encoded in the relations among several images. This paper presents a novel re-ranking approach based on contextual spaces aiming to improve the effectiveness of CBIR tasks, by exploring relations among images. In our approach, information encoded in both distances among images and ranked lists computed by CBIR systems are used for analyzing contextual information. The re-ranking method can also be applied to other tasks, such as: (i) for combining ranked lists obtained by using different image descriptors (rank aggregation); and (ii) for combining post-processing methods. We conducted several experiments involving shape, color, and texture descriptors and comparisons to other post-processing methods. Experimental results demonstrate the effectiveness of our method.	content-based image retrieval;experiment;image analysis;learning to rank;relevance;video post-processing;visual descriptor	Daniel Carlos Guimarães Pedronette;Ricardo da Silva Torres	2011		10.1145/1991996.1992009	computer vision;contextual image classification;image analysis;computer science;pattern recognition;information retrieval	Vision	35.779497068063606	-55.13857309815205	190655
d613432b5934f4b50682eb71a0917946713fb8c0	find me a sky: a data-driven method for color-consistent sky search and replacement		Replacing overexposed or dull skies in outdoor photographs is a desirable photo manipulation. It is often necessary to color correct the foreground after replacement to make it consistent with the new sky. Methods have been proposed to automate the process of sky replacement and color correction. However, many times a color correction is unwanted by the artist or may produce unrealistic results. We propose a data-driven approach to sky-replacement that avoids color correction by finding a diverse set of skies that are consistent in color and natural illumination with the query image foreground. Our database consists of∼ 1200 natural images spanning many outdoor categories. Given a query image, we retrieve the most consistent images from the database according to L2 similarity in feature space and produce candidate composites. The candidates are re-ranked based on realism and diversity. We used pre-trained CNN features and a rich set of hand-crafted features that encode color statistics, structural layout, and natural illumination statistics, but observed color statistics to be the most effective for this task. We share our findings on feature selection and show qualitative results and a user-study based evaluation to show the effectiveness of the proposed method.	color;color mapping;encode;feature selection;feature vector;file spanning;unsupervised learning;usability testing;video post-processing	Saumya Rawat;Siddhartha Gairola;Rajvi Shah;P. J. Narayanan	2018		10.1007/978-3-319-73603-7_18	pattern recognition;computer science;computer vision;artificial intelligence;feature vector;data-driven;sky;color correction;feature selection	Vision	36.34223208970993	-53.923013738832495	190741
150130c842c5643742c9982c91deb6ad4e9ffcfd	covert video classification by codebook growing pattern		Recent advances in visual data acquisition and Internet technologies make it convenient and popular to collect and share videos. These activities, however, also raise the issue of privacy invasion. One potential privacy threat is the unauthorized capture and/or sharing of covert videos, which are recorded without the awareness of the subject(s) in the video. Automatic classification of such videos can provide an important basis toward addressing relevant privacy issues. The task is very challenging due to the large intra-class variation and between-class similarity, since there is no limit in the content of a covert video and it may share very similar content with a regular video. The challenge brings troubles when applying existing content-based video analysis methods to covert video classification. In this paper, we propose a novel descriptor, codebook growing pattern (CGP), which is derived from latent Dirichlet allocation (LDA) over optical flows. Given an input video V, we first represent it with a sequence of histograms of optical flow (HOF). After that, these HOFs are fed into LDA to dynamically generate the codebook for V. The CGP descriptor is then defined as the growing codebook sizes in the LDA procedure. CGP fits naturally for covert video representation since (1) optical flows can capture the camera motion that characterizes the covert video acquisition, and (2) CGP by itself is insensitive to video content. To evaluate the proposed approach, we collected a large covert video dataset, the first such dataset to our knowledge, and tested the proposed method on the dataset. The results show clearly the effectiveness of the proposed approach in comparison with other state-of-the-art video classification algorithms.	algorithm;authorization;codebook;covert channel;data acquisition;digital video;fits;ibm notes;internet;latent dirichlet allocation;optical flow;privacy;video content analysis	Liang Du;Haitao Lang;Ying-Li Tian;Chiu C. Tan;Jie Wu;Haibin Ling	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.178	computer vision;computer science;machine learning;video tracking;data mining;multimedia	Vision	34.582222982540564	-52.401802510121755	190886
ba5697bd1c49647b613aefeb57bd48f71c0bbff5	character recognition in natural scene images using rank-1 tensor decomposition	computers;text analysis;computer vision;indexes;decision support systems;pattern analysis	Many approaches to solve the problem of scene character recognition utilize local features such as histograms of oriented gradients (HoG), SIFT, Shape Contexts (SC), Geometric Blur (GB), etc. An issue associated with these methods is the ad hoc rasterization of the local features into a single vector which perturbs the global spatial correlations that carry crucial information for recognition. To address this issue, we propose a novel holistic solution by incorporating tensor decomposition to get image features and utilizing image-to-class distance metric learning (I2CDML) for classification. For each training image, we first form a 3-mode tensor by rotating it through a sequence of angles. Then we perform rank-1 decomposition on the tensor to get the descriptor for each image. Utilizing the I2CDML framework, we then learn metrics for each class that are finally used to classify test images. We report results on popular natural scene character datasets, namely Chars74K-Font, Chars74K-Image, and ICDAR2003. We achieve results better than several baseline methods based on local features (e.g. HoG) and show that leave-random-one-out-cross validation yield even better recognition performance.	baseline (configuration management);floppy-disk controller;gaussian blur;gradient;hoc (programming language);holism;optical character recognition;rasterisation;shape context	Muhammad Ali;Hassan Foroosh	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7532888	database index;computer vision;text mining;decision support system;computer science;machine learning;pattern recognition	Vision	35.37059358245229	-55.97925846051198	190961
50f8b3c1de383759d6516d0f059579ed202df1fd	pedestrian detection by using fast-hog features	linear svm;object recognition;hog;hog feature;fast feature;non pedestrian;pedestrian detection	Pedestrian detection is used in video surveillance systems and driver assistance systems. The purpose is to build automated vision systems for detecting pedestrians as shown in figure 1. We use Histograms of Oriented Gradients (HOG), which are one of the well-known features for object recognition. HOG features are calculated by taking orientation histograms of edge intensity in a local region [1]. In this paper we select the interesting point in the image by using FAST features detector and extracted HOG features around these strongest corners and use them as an input vector of linear Support Vector Machine (SVM) to classify the given input into pedestrian/non-pedestrian. By using FAST detector we reduce the number of features less than half without lowering the performance.	closed-circuit television;image gradient;outline of object recognition;pedestrian detection;sensor;support vector machine;whole earth 'lectronic link	Batoul Husain Bani Hashem;Tomoko Ozeki	2015		10.1145/2814940.2814996	computer vision;speech recognition;engineering;pattern recognition	Vision	32.750226009870154	-56.334418027595596	191379
6351add5489a03a0727c36c148c5c7e77cd4d392	web-image driven best views of 3d shapes	silhouette;saliency;web images;curvature	The rapid advance of the Internet provides available huge database of web images. In this paper, we introduce a novel approach for automatically computing the best views of 3D shapes based on their web images. Best view selection is generally an intuitive task of getting the most information of a 3D shape. The novelty of our approach is to directly explore human perception on observing 3D shapes from the relevant web images. Those images are captured from biased views of different people, thus sufficiently reflecting view choice when observing the 3D shapes. By collecting web images possibly captured from the similar views, the best view is selected as the one possessing the most web images. We experiment our method with the shapes in Princeton Shape Benchmark (PSB), as well make comparisons with traditional geometric descriptor based approaches. The results demonstrate that our method is not only robust but also able to produce more canonical views in accordance with human perception.	benchmark (computing);best practice;cloud computing;internet;pacific symposium on biocomputing;parallel computing	Hong Liu;Lei Zhang;Hua Huang	2011	The Visual Computer	10.1007/s00371-011-0638-z	computer vision;computer science;machine learning;salience;data mining;mathematics;geometry;multimedia;curvature;silhouette	Vision	36.472922322812536	-55.5883088154084	192078
a3df7bc8ecb4b25a0b655315968218d3adbad52c	high resolution satellite image indexing and retrieval using surf features and bag of visual words	associative arrays;satellite imaging;high resolution satellite images;land cover;image retrieval	In this paper, we evaluate the performance of SURF descriptor for high resolution satellite imagery (HRSI) retrieval through a BoVW model on a land-use/land-cover (LULC) dataset. Local feature approaches such as SIFT and SURF descriptors can deal with a large variation of scale, rotation and illumination of the images, providing, therefore, a better discriminative power and retrieval efficiency than global features, especially for HRSI which contain a great range of objects and spatial patterns. Moreover, we combine SURF and color features to improve the retrieval accuracy, and we propose to learn a category-specific dictionary for each image category which results in a more discriminative image representation and boosts the image retrieval performance.		Samia Bouteldja;Assia Kourgli	2016		10.1117/12.2268803	computer vision;visual word;geography;remote sensing;computer graphics (images)	Vision	37.16800928605947	-56.19901604105817	192083
610fc5a719fa16a335701c4703f2c3faa07a60d6	a comparative study of local descriptors for arabic character recognition on mobile devices	optical character recognition;image registration;mobile devices	Nowadays, the number of mobile applications based on image registration and recognition is increasing. Most interesting applications include mobile translator which can read text characters in the real world and translates it into the native language instantaneously. In this context, we aim to recognize characters in natural scenes by computing significant points so called key points or features/interest points in the image. So, it will be important to compare and evaluate features descriptors in terms of matching accuracy and processing time in a particular context of natural scene images. In this paper, we were interested on comparing the efficiency of the binary features as alternatives to the traditional SIFT and SURF in matching Arabic characters descended from natural scenes. We demonstrate that the binary descriptor ORB yields not only to similar results in terms of matching characters performance that the famous SIFT but also to faster computation suitable for mobile applications.	computation;image registration;mobile app;mobile device;optical character recognition;scale-invariant feature transform;speeded up robust features	Maroua Tounsi;Ikram Moalla;Adel M. Alimi;Franck Lebourgeois	2014		10.1117/12.2181516	computer vision;computer science;multimedia;communication	Vision	37.503581697742305	-53.33300074969145	192163
0274fa05c6d5259136ffce1bd8685e7d2a92e8b2	view-based recognition of real-world textures	vision system;selection model;local binary pattern;classification;3d texture;self organization;appearance based	A new method for recognizing 3D textured surfaces is proposed. Textures are modeled with multiple histograms of microtextons, instead of more macroscopic textons used in earlier studies. The micro-textons are extracted with the recently proposed multiresolution local binary pattern operator. Our approach has many advantages compared to the earlier approaches and provides the leading performance in the classification of Columbia-Utrecht database (CUReT) textures imaged under different viewpoints and illumination directions. It also provides very promising results in the classification of outdoor scene images. An approach for learning appearance models for view-based texture recognition using selforganization of feature distributions is also proposed. The method performs well in experiments. It can be used for quickly selecting model histograms and rejecting outliers, thus providing an efficient tool for vision system training even when the feature data has a large variability.	binary pattern (image generation);columbia (supercomputer);computer vision;experiment;feature data;spatial variability	Matti Pietikäinen;Tomi Nurmela;Topi Mäenpää;Markus Turtinen	2004	Pattern Recognition	10.1016/S0031-3203(03)00231-0	computer vision;self-organization;local binary patterns;machine vision;biological classification;computer science;machine learning;pattern recognition	Vision	36.654711153021395	-58.003357209658766	192166
8e351b3a0beaf62d8e1f8f879451f06577f9d04f	contourlet-based feature extraction with pca for face recognition	databases;facial expressions;filter bank;databases face principal component analysis face recognition transforms filter bank training;biometrics access control;multiresolution filter banks;training;principal component analysis biometrics access control face recognition feature extraction;face recognition;biometric systems;yale face database;feature extraction;principal component analysis;feret database;lighting conditions;feret database face recognition principal component analysis pca contourlet based feature extraction facial expressions lighting conditions multiresolution filter banks biometric systems contourlet transform yale face database;comparative study;transforms;contourlet transform;face;facial expression;pca;contourlet based feature extraction	Face recognition is still a challenging task because face images can vary considerably in terms of facial expressions, lighting conditions, ... etc. It is commonly known that the use of multiresolution filter banks improve the recognition accuracy of image based biometric systems. In this paper, we propose to investigate the usefulness of the multiscale and directionality properties of the contourlet transform with a view to extract more discriminant features in order to further enhance the performance of the well known principal component analysis method when applied to face recognition. The proposed method has been extensively assessed using two different databases: the YALE Face Database and the FERET Database. A series of experiments have been carried out and a comparative study suggests the efficiency of the Contourlet Transform in enhancing the classification rates of a number of known face recognition algorithms.	algorithm;biometrics;contourlet;discriminant;experiment;feret database;facial recognition system;feature extraction;filter bank;gabor filter;independent computing architecture;local-density approximation;principal component analysis;whole earth 'lectronic link	Walid Riad Boukabou;Ahmed Bouridane	2008	2008 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2008.11	facial recognition system;computer vision;speech recognition;computer science;machine learning;pattern recognition;three-dimensional face recognition;facial expression;principal component analysis	Vision	33.73744184734027	-59.071233319347655	192709
e44f59da5b123da999738d10954a54e2fb635eda	combined correlation rules to detect skin based on dynamic color clustering		Skin detection plays an important role in a wide range of image processing and computer vision applications. In short, there are three major approaches for skin detection: rule-based, machine learning and hybrid. They differ in terms of accuracy and computational efficiency. Generally, machine learning and hybrid approaches outperform the rule-based methods, but require a large and representative training dataset as well as costly classification time, which can be a deal breaker for real time applications. In this paper, we propose an improvement of a novel method on rule-based skin detection that works in the YCbCr color space. Our motivation is based on the hypothesis that: (1) the original rule can be reversed and, (2) human skin pixels do not appear isolated, i.e. neighborhood operations are taken in consideration. The method is a combination of some correlation rules based on these hypothesis. Such rules evaluate the combinations of chrominance Cb, Cr values to identify the skin pixels depending on the shape and size of dynamically generated skin color clusters. The method is very efficient in terms of computational effort as well as robust in very complex image scenes.	color space;computation;computer vision;experiment;image processing;logic programming;machine learning;multitier architecture;pixel;sensor;texture mapping	Rodrigo Augusto Dias Faria;Roberto Hirata	2018		10.5220/0006618003090316	cluster analysis;machine learning;correlation;artificial intelligence;computer science	ML	36.56955774720509	-58.3317946213251	193152
b6a3802075d460093977f8566c451f950edf7a47	facilitating and exploring planar homogeneous texture for indoor scene understanding		References Rectification aligns features to a canonical coordinate frame, mitigating in-class variance and improving recognition Features extracted upon rectification are class-discriminative and complementary to original descriptors All state-of-the-art methods on MIT Indoor 67 [2] employ machine learning for feature extraction Proposed approach is entirely handcrafted, achieving 64.54% when combining SIFT, CENTRIST & HOG descriptors	feature extraction;image rectification;machine learning;rectifier;scale-invariant feature transform	Shahzor Ahmad;Loong Fah Cheong	2016		10.1007/978-3-319-46475-6_3	computer science;artificial intelligence;computer vision;rectification;clutter;homogeneous;affine transformation;vanishing point;planar	Vision	32.14815267560143	-53.812795878015	193445
b91aba4abd11dd48e0a04da855c7b13cae4de139	image clustering based on sift-affinity propagation	visual databases data mining feature extraction image fusion internet pattern clustering transforms;affinity propagation ap image clustering data correlation scale invariant feature transform sift;affinity propagation sift affinity propagation data mining digital image information image acquisition equipment image database internet large scale image information image clustering algorithms image clustering function image feature extraction scale invariant feature transform image zooming image translation image gyration;clustering algorithms feature extraction correlation image color analysis accuracy algorithm design and analysis pattern recognition	Clustering is a hotspot issues in the field of data mining. There is abundant digital image information in the image acquisition equipment, the image database or the Internet. Facing the large scale image information with rich semantics, it is difficult to obtain accurate information as soon as possible. Therefore, it is essential for us to study efficient image clustering algorithms, in which how to measure the correlation among the samples is key issue, and will directly affect the results of clustering. In this paper, a new image clustering function based on SIFT-Affinity propagation is proposed. First, we extract image features based on Scale-invariant feature transform(SIFT), which are changeless to image zooming, translation, and gyration, and partially changeless to brightness changes and affine. Then the modelling of correlation is established and quantitatively analyzed. Finally, we utilize affinity propagation (AP) to fuse different SIFT correlations among images and obtain clustering results. Our method considers all image data as likely clustering center. The examples results prove the effectiveness and superiority of our approach.	affinity propagation;algorithm;cluster analysis;color space;data mining;digital image;experiment;internet;java hotspot virtual machine;k-means clustering;processor affinity;software propagation	Hong Zhang	2014	2014 11th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2014.6980860	image texture;correlation clustering;computer vision;feature detection;image gradient;binary image;image processing;feature extraction;computer science;machine learning;consensus clustering;digital image processing;pattern recognition;cluster analysis;automatic image annotation;top-hat transform;feature	ML	37.99351368258649	-57.076356685393044	193471
f7af90ec9dadb375fe2dce65f9a88d611834d68e	a robust hand detection method based on skin probability map		Existing hand detection methods are usually implemented with Haar-like and HOG descriptors, which lack of robustness to complicate background and the change of the lighting conditions in real applications. In this paper, a robust hand detection method is presented by exploring various feature representations based on skin probability map (SPM) instead of conventional grayscale image space. We show that, in the generated SPM image, hand region can be greatly highlighted and the background regions can be suppressed. And then, the Haar-like and HOG features are calculated from the skin mask and probability maps respectively, and named as skin-Enhanced features. In the sliding window searching stage, an efficient 3-level detection framework is introduced. In the experiment, an extensive dataset is established for the training of classifiers. And the hand detection results show that, the proposed method is much more robust than conventional hand feature descriptors.	data descriptor;feature model;grayscale;haar wavelet;map;super paper mario	Yanguo Zhao;Feng Zheng;Zhan Song	2017		10.1145/3163080.3163105	sliding window protocol;robustness (computer science);grayscale;hand region;pattern recognition;computer science;artificial intelligence	Vision	34.13518330362521	-57.59152536423817	194067
f9914594a78fb402f5fdaae5a4935984d59e4a04	multi-level dense descriptor and hierarchical feature matching for copy-move forgery detection	copy move forgery detection cmfd;invariant moment descriptor;hierarchical feature matching;multi level dense descriptor mldd;color texture descriptor	In this paper, a Multi-Level Dense Descriptor (MLDD) extraction method and a Hierarchical Feature Matching method are proposed to detect copy–move forgery in digital images. The MLDD extraction method extracts the dense feature descriptors using multiple levels, while the extracted dense descriptor consists of two parts: the Color Texture Descriptor and the Invariant Moment Descriptor. After calculating the MLDD for each pixel, the Hierarchical Feature Matching method subsequently detects forgery regions in the input image. First, the pixels that have similar color textures are grouped together into distinctive neighbor pixel sets. Next, each pixel is matched with pixels in its corresponding neighbor pixel set through its geometric invariant moments. Then, the redundant pixels from previously generated matched pixel pairs are filtered out by the proposed Adaptive Distance and Orientation Based Filtering method. Finally, some morphological operations are applied to generate the final detected forgery regions. Experimental results show that the proposed scheme can achieve much better detection results compared with the existing state-of-the-art CMFD methods, even under various challenging conditions such as geometric transforms, JPEG compression, noise addition and down-sampling. © 2016 Published by Elsevier Inc.	authentication;digital image;feature model;image moment;jpeg;mathematical morphology;pixel;sampling (signal processing)	Xiu-Li Bi;Chi-Man Pun;Xiaochen Yuan	2016	Inf. Sci.	10.1016/j.ins.2016.01.061	computer vision;machine learning;pattern recognition;mathematics	Vision	36.929833994347426	-59.100710215740484	194300
f049ea3ca734c217c380a1802d15a6d85378f55d	efficient misbehaving user detection in online video chat services	image features;online video chat;misbehaving user;contextual information;feature extraction	"""Online video chat services, such as Chatroulette, Omegle, and vChatter are becoming increasingly popular and have attracted millions of users. One critical problem encountered in such applications is the presence of misbehaving users (""""flashers"""") and obscene content. Automatically filtering out obscene content from these systems in an efficient manner poses a difficult challenge. This paper presents a novel Fine-Grained Cascaded (FGC) classification solution that significantly speeds up the compute-intensive process of classifying misbehaving users by dividing image feature extraction into multiple stages and filtering out easily classified images in earlier stages, thus saving unnecessary computation costs of feature extraction in later stages. Our work is further enhanced by integrating new webcam-related contextual information (illumination and color) into the classification process, and a 2-stage soft margin SVM algorithm for combining multiple features. Evaluation results using real-world data set obtained from Chatroulette show that the proposed FGC based classification solution significantly outperforms state-of-the-art techniques."""	algorithm;computation;feature (computer vision);feature extraction;video clip;webcam	Hanqiang Cheng;Yu-Li Liang;Xinyu Xing;Xue Liu;Richard Han;Qin Lv;Shivakant Mishra	2012		10.1145/2124295.2124301	feature extraction;computer science;machine learning;data mining;multimedia;internet privacy;world wide web;feature	Web+IR	32.24516672679207	-52.140959225388215	194531
d6e8e059bcdf22cf9f6963d25b2314e7c2267b31	semantic segmentation of rgbd images with mutex constraints	sun3d dataset semantic rgb d image segmentation mutex constraints semantic scene segmentation image region labeling method crf formulation 3d geometric structure kinect mutex constraints global object cooccurrence constraint local support relationship constraint nyu depth v2 dataset cluttered indoor scenes test generalization;semantics image segmentation three dimensional displays labeling visualization feature extraction computational modeling;image sensors image colour analysis image segmentation	In this paper, we address the problem of semantic scene segmentation of RGB-D images of indoor scenes. We propose a novel image region labeling method which augments CRF formulation with hard mutual exclusion (mutex) constraints. This way our approach can make use of rich and accurate 3D geometric structure coming from Kinect in a principled manner. The final labeling result must satisfy all mutex constraints, which allows us to eliminate configurations that violate common sense physics laws like placing a floor above a night stand. Three classes of mutex constraints are proposed: global object co-occurrence constraint, relative height relationship constraint, and local support relationship constraint. We evaluate our approach on the NYU-Depth V2 dataset, which consists of 1449 cluttered indoor scenes, and also test generalization of our model trained on NYU-Depth V2 dataset directly on a recent SUN3D dataset without any new training. The experimental results show that we significantly outperform the state-of-the-art methods in scene labeling on both datasets.	conditional random field;connected-component labeling;constraint logic programming;kinect;mutual exclusion;test set	Zhuo Deng;Sinisa Todorovic;Longin Jan Latecki	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.202	computer vision;machine learning;pattern recognition	Vision	32.916701833255246	-52.63961137234167	194875
dcaa78a966fb166ecc83b5d4ef4aa3451984a1be	satellite features for the classification of visually similar classes	image recognition;computer society;mathematics;satellites feature extraction computer vision image recognition computer science mathematics animal structures shape face recognition computer society;computer vision;face recognition;shape;local features;feature extraction;satellites;computer science;animal structures	"""We show that the discrimination between visually similar classes often depends on the detection of socalled ‘satellite features’. These are local features which are not informative by themselves, and can only be detected reliably at locations specified relative to other features. This makes satellite features difficult to extract by current classification methods. We describe a novel scheme which can extract discriminative satellite features and use them to distinguish between visually similar classes. The algorithm first searches for a set of features (""""anchor features"""") that can be found in all the similar classes. Such features can be detected because the classes are visually similar. The anchors are used to determine the locations of satellite features, which are extracted during learning and used in classification to distinguish between the similar classes. The algorithm is fully automatic, and is shown to work well for many categories of visually similar classes."""	algorithm;html element;information;statistical classification	Boris Epshtein;Shimon Ullman	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.262	facial recognition system;computer vision;feature extraction;shape;computer science;machine learning;pattern recognition;satellite	Vision	38.93419256096162	-56.32511104159811	195127
2d94dfa9c8f6708e071ef38d58f9f9bcb374cd84	a common framework for real-time emotion recognition and facial action unit detection	facial expression recognition;facial action unit detection;support vector machines;real time;support vector machines discrete cosine transforms emotion recognition face recognition image classification;training;real time emotion recognition;image classification;emotion recognition;discrete cosine transform;gold;face recognition;discrete cosine transforms;fg 2011 facial expression analysis;local appearance based face representation approach;face emotion recognition discrete cosine transforms training support vector machines face recognition gold;action unit;face;support vector machine;support vector machine classifiers;fg 2011 facial expression recognition;support vector machine classifiers real time emotion recognition facial action unit detection fg 2011 facial expression recognition fg 2011 facial expression analysis local appearance based face representation approach discrete cosine transform	In this paper, we present a common framework for realtime action unit detection and emotion recognition that we have developed for the emotion recognition and action unit detection sub-challenges of the FG 2011 Facial Expression Recognition and Analysis Challenge. For these tasks we employed a local appearance-based face representation approach using discrete cosine transform, which has been shown to be very effective and robust for face recognition. Using these features, we trained multiple one-versus-all support vector machine classifiers corresponding to the individual classes of the specific task. With this framework we achieve 24.2% and 7.6% absolute improvement over the overall baseline results on the emotion recognition and action unit detection sub-challenge, respectively.	baseline (configuration management);database;discrete cosine transform;emotion recognition;facial recognition system;job control (unix);local binary patterns;locality of reference;real-time clock;real-time computing;real-time locating system;sensor;support vector machine	Tobias Gehrig;Hazim Kemal Ekenel	2011	CVPR 2011 WORKSHOPS	10.1109/CVPRW.2011.5981817	facial recognition system;support vector machine;computer vision;speech recognition;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	31.90753195991749	-57.61678588640294	195509
43deefcf3dbe483822f508d7cbe5d68f7c943937	facial expression recognition using hessianmkl based multiclass-svm	support vector machines;gaussian processes;support vector machines computer vision emotion recognition face recognition feature extraction gaussian processes image classification image representation learning artificial intelligence polynomials;image classification;emotion recognition;polynomials;computer vision;face recognition;image representation;feature extraction;learning artificial intelligence;lbph features facial expression recognition hessianmkl based multiclass svm multikernel learning computer vision pattern recognition learning classifiers hessianmkl algorithm one against one rule kernel weight vector binary classifier simplemkl based multiclasssvm neutral expression kernel functions rbf function gaussian function polynomial function image representations hog features;kernel support vector machines vectors face recognition optimization training equations	Multikernel learning (MKL) has recently received great attention in the field of computer vision and pattern recognition. The idea behind MKL is to optimally combine and utilize multiple kernels and features instead of using a single kernel in learning classifiers. This paper presents a novel framework for MKL problem by expanding the HessianMKL algorithm into multiclass-SVM with one-against-one rule. Our framework learns one kernel weight vector for each binary classifier in the multiclass-SVM compared to the SimpleMKL based multiclass-SVM which jointly learns the same kernel weight vector for all binary classifiers. The proposed method is utilized to recognize six basic facial expressions and neutral expression by combining three kernel functions, RBF, Gaussian, and polynomial function and two image representations, HoG and LBPH features. Our experimental results show that our method performed better than SVM classifiers equipped with a single kernel and a single type of feature as well as the SimpleMKL based multiclass-SVM.	algorithm;binary classification;computer vision;kernel (operating system);math kernel library;multiclass classification;multikernel;parsing expression grammar;pattern recognition;polynomial;radial basis function;regular expression;sadness	Xiao Zhang;Mohammad H. Mahoor;Richard M. Voyles	2013	2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2013.6553807	least squares support vector machine;computer vision;kernel method;string kernel;kernel embedding of distributions;feature vector;radial basis function kernel;feature;mean-shift;computer science;machine learning;pattern recognition;graph kernel;polynomial kernel	Vision	32.171975299480074	-55.927980960246614	195640
080c9874dbd767068ef0beca47a19f3c7b1bba90	crf based text detection for natural scene images using convolutional neural network and context information		This paper presents a novel scene text detection method based on conditional random field (CRF) framework. We estimate the confidence of Maximally Stable Extremal Region (MSER) being text by leveraging convolutional neural network (CNN) to define the unary cost item. In addition, we establish the neighboring interactions for MSERs using four different features including color, shape, stroke and spatial features to define the pairwise cost item. Considering the special layout of texts appearing in natural scene images, we employ context information to recover missing text MSER candidates. Furthermore, text MSERs are grouped into candidate text lines which are verified with shape-specific classifiers by integrating gray and binary features. Experimental results on four public benchmark datasets show that the proposed method achieves the comparable performance. (C) 2018 Elsevier B.V. All rights reserved.	artificial neural network;conditional random field;convolutional neural network	Yanna Wang;Cunzhao Shi;Baihua Xiao;Chunheng Wang;Cheng-Zuo Qi	2018	Neurocomputing	10.1016/j.neucom.2017.12.058	convolutional neural network;machine learning;artificial intelligence;pairwise comparison;conditional random field;mathematics;pattern recognition;binary number;unary operation	ML	33.539743345442616	-53.92039839952914	195679
87e020d1b0fb4bd0935e72fba6ac9d94a7ea8567	unsupervised distance learning by reciprocal knn distance for image retrieval	unsupervised distance learning;content based image retrieval	This paper presents a novel unsupervised learning approach that takes into account the intrinsic dataset structure, which is represented in terms of the reciprocal neighborhood references found in different ranked lists. The proposed Reciprocal kNN Distance defines a more effective distance between two images, and is used to improve the effectiveness of image retrieval systems. Several experiments were conducted for different image retrieval tasks involving shape, color, and texture descriptors. The proposed approach is also evaluated on multimodal retrieval tasks, considering visual and textual descriptors. Experimental results demonstrate the effectiveness of proposed approach. The Reciprocal kNN Distance yields better results in terms of effectiveness than various state-of-the-art algorithms.	experiment;image retrieval;k-nearest neighbors algorithm;multimodal interaction;unsupervised learning	Daniel Carlos Guimarães Pedronette;Otávio Augusto Bizetto Penatti;Rodrigo Tripodi Calumby;Ricardo da Silva Torres	2014		10.1145/2578726.2578770	visual word;computer science;machine learning;pattern recognition;information retrieval	Vision	35.4506383548995	-54.90034848206347	195707
6e2328edc195f8d45e317a2906fb6abf075c0e31	figure-ground classification based on spectral properties of boundary image patches	visual databases computer graphics feature extraction image classification support vector machines;figure ground segregation figure ground classification spectral properties boundary image patches image understanding visual scene two dimensional sensor surfaces complex natural scenes support vector machine spectral anisotropy feature extraction image databases occlusion is;support vector machines;computer graphics;image classification;image segmentation support vector machines;feature extraction;visual databases	Image understanding requires segregation of the visual scene into perceptual objects. Due to the projection of the three-dimensional world on two-dimensional sensor surfaces, objects closer to the observer occlude those which are more distant. At any given occlusion border, it is important to decide which side is the foreground (figure) and which is the background, a decision which is influenced both by global and local image contents. In this report, we focus on local cues. We randomly select small image patches located on figure-ground borders in complex natural scenes. Spectral anisotropy features are extracted from the patches and used to train a non-linear Support Vector Machine. Using data from two large image databases (LabelMe and BSDS300), the classifier achieves an accuracy near 70% per local patch on the task of deciding which side of an occlusion is the foreground. Although in many cases global influences are important for figure-ground segregation, we suggest that the low computational cost of local computation can make it a useful strategy for figure-ground segregation.	algorithmic efficiency;computation;computational complexity theory;database;labelme;nonlinear system;radial (radio);radial basis function;randomness;stellar classification;support vector machine	Sudarshan Ramenahalli;Stefan Mihalas;Ernst Niebur	2012	2012 46th Annual Conference on Information Sciences and Systems (CISS)	10.1109/CISS.2012.6310943	support vector machine;computer vision;contextual image classification;feature detection;feature extraction;computer science;machine learning;pattern recognition;computer graphics	Vision	38.08538649024503	-56.138537831428096	196351
a44985a2aabb488394e73ec51260ed0e3a047ce6	hand segmentation with metric learning superpixels	superpixels hand segmentation metric learning;image color analysis skin measurement image segmentation adaptation models covariance matrices gesture recognition;learning artificial intelligence image colour analysis image segmentation;color space background metric learning superpixels hand area segmentation superpixels based method general color model hand pixel distribution color space distance metric learning	In this paper, we propose a novel method for hand segmentation. To improve the robustness to the wide range of hand appearances and illuminations, we segment the hand area with a superpixels based method instead of a general color model. With the exploitation of the distribution of hand pixels in color space, a distance metric learning stage is designed to promote the segmentation performance. This stage makes the points in hand areas more concentrate and pulls away from the points of background in color space. The comparisons with several widely used algorithms are made on both public available and our own datasets. The experimental results show the superior performance of our method.	algorithm;color space;pixel	Guangdong Hou;Daqing Chang;Changshui Zhang	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889284	computer vision;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Robotics	32.81053430701003	-54.461790644476146	196752
0608826f770496cb54c784659b11664ac3461280	face recognition using co-occurrence histograms of oriented gradients	magnitude gradient;face recognition hog cohog;histograms;image recognition;vocabulary face recognition feature extraction image representation;face face recognition histograms accuracy image recognition training vectors;training;vocabulary;hog;accuracy;face recognition;vectors;image representation;feature extraction;orl datasets;orl datasets face recognition co occurrence histograms histogram of oriented gradient co occurrence of oriented gradient magnitude gradient face image yale datasets;face;cohog;yale datasets;co occurrence of oriented gradient;face image;histogram of oriented gradient;co occurrence histograms	Recently, Histogram of Oriented Gradient (HOG) is applied in face recognition. In this paper, we apply Co-occurrence of Oriented Gradient (CoHOG), which is an extension of HOG, on the face recognition problem. Some weighted functions for magnitude gradient are tested. We also proposed a weighted approach for CoHOG, where a weight value is set for each subregion of face image. Numerical experiments performed on Yale and ORL datasets show that 1) CoHOG has recognition accuracy higher than HOG; 2) using gradient magnitude in CoHOG improves recognition results; and 3) weighted CoHOG approach improves accuracy recognition rate. The recognition results using CoHOG are competitive with some of the state of the art methods. This proves the effectiveness of CoHOG descriptor for face recognition.	experiment;facial recognition system;image gradient;numerical linear algebra;return loss	Thanh-Toan Do;Ewa Kijak	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288128	facial recognition system;face;computer vision;speech recognition;feature extraction;computer science;pattern recognition;histogram;mathematics;accuracy and precision;statistics	Vision	34.75528005114486	-58.367435923329886	197514
86f3d599389ebba12ee1cb2eb9c2a69f3c6018d3	histogram of dense subgraphs for image representation	image categorisation;dense subgraph histogram;dense scale invariant feature transform descriptors;image classification process;graph based substructure pattern mining algorithm;dense subgraph histogram subgraphs representation graph based substructure pattern mining algorithm dense scale invariant feature transform descriptors graph power representation image classification process bag of visual words image classification reliable representation dense regular grid structural organisation visual phrases simple pairwise features image categorisation local features spatial information image representation;image classification;dense regular grid;subgraphs representation;local features;image representation;graph power representation;reliable representation;transforms feature extraction graph theory image classification image representation;structural organisation;bag of visual words;simple pairwise features;spatial information;visual phrases	Modelling spatial information of local features is known to improve performance in image categorisation. Compared with simple pairwise features and visual phrases, graphs can capture the structural organisation of local features more adequately. Besides, a dense regular grid can guarantee a more reliable representation than the interest points and give better results for image classification. In this study, the authors introduced a bag of dense local graphs approach that combines the performance of bag of visual words expressing the image classification process with the representational power of graphs. The images were represented with dense local graphs built upon dense scale-invariant feature transform descriptors. The graph-based substructure pattern mining algorithm was applied on the local graphs to discover the frequent local subgraphs, producing a bag of subgraphs representation. The results were reported from experiments conducted on four challenging benchmarks. The findings show that the proposed subgraph histogram improves the categorisation accuracy.		Mouna Dammak;Mahmoud Mejdoub;Chokri Ben Amar	2015	IET Image Processing	10.1049/iet-ipr.2014.0189	contextual image classification;combinatorics;computer science;machine learning;pattern recognition;mathematics;spatial analysis;bag-of-words model in computer vision	Vision	37.31943413595414	-56.618788545838314	197702
8974fd1c37ab22457c3996a0dd5efe740b465f6a	visual cortex frontend: integrating lines, edges, keypoints, and disparity	object recognition;cortex visual;3d representation;multiscale method;visao computorizada;article;visual cortex	We present a 3D representation that is based on the processing in the visual cortex by simple, complex and end-stopped cells. We improved multiscale methods for line/edge and keypoint detection, including a method for obtaining vertex structure (i.e. T, L, K etc). We also describe a new disparity model. The latter allows to attribute depth to detected lines, edges and keypoints, i.e., the integration results in a 3D “wire-frame” representation suitable for object recognition.	binocular disparity;outline of object recognition	João Fabrício Mota Rodrigues;J. M. Hans du Buf	2004		10.1007/978-3-540-30125-7_82	computer vision;artificial intelligence;cognitive neuroscience of visual object recognition	ML	37.87283998259238	-55.30307998450977	197806
6e6f4323ca2712654ac1b9a3dcbd905b97fc0c1a	seabed classification using a bag-of-prototypes feature representation	prototypes image color analysis vectors lighting manuals sediments feature extraction;marine systems image classification image representation;resource exploration underwater image analysis feature representation;poly metallic nodule coverage bag of prototypes feature representation image patch feature representation concept underwater image analysis seafloor classification bop concept tile wise estimation	The increasing scientific and economic interest in the visual exploration and monitoring of marine areas is creating huge amounts of new underwater image and video data and approaches to computationally assisted analysis are desperately needed. In this paper we propose an image patch feature representation concept, the Bag of Prototypes (BoP), to cope with the individual problems in underwater image analysis. We consider the case of seafloor classification, which is relevant in many contexts such as habitat mapping or the exploration of mineral resources and show, that the BoP concept allows an efficient and accurate tile-wise estimation of poly-metallic nodule coverage in relation to two differently acquired gold standards.	computation;entity;habitat;high- and low-level;image analysis;instance (computer science);k-means clustering;object detection;pixel;scale-invariant feature transform;speeded up robust features;subpixel rendering	Timm Schoening;Thomas Kuhn;Tim W. Nattkemper	2014	2014 ICPR Workshop on Computer Vision for Analysis of Underwater Imagery	10.1109/CVAUI.2014.9	computer vision;feature detection;speech recognition;engineering;communication;feature	Vision	38.495060024144394	-55.66222662658342	197899
387164e9054d0901b77a503213dbf18d10d135c7	visual saliency model based on minimum description length	visualization;object detection entropy;redundancy;visual saliency model human fixations prediction mdl principle saliency detection performance structural redundancy operator prediction residuals entropy order adaptive predictor unpredicted image patch information minimum description length;visual systems;predictive models;entropy;adaptation models;context;visualization redundancy adaptation models entropy predictive models context visual systems	In this paper, a novel patch-wise visual saliency model based on Minimum Length Description (MDL) principle is presented. Visual saliency is measured as the unpredicted information of image patch through an order-adaptive predictor under MDL principle. Specifically, each image patch is estimated with a linear combination of several neighboring patches. The number and location of candidate patches are automatically tuned to local contexts based on MDL. Then the entropy of prediction residuals of center patch, which represents the surprise to the visual system, is used to measure the saliency. Furthermore, a structural redundancy operator is also involved to improve the saliency detection performance. Experimental results demonstrate that the predictor under MDL principle along with the structural redundancy operator can improve the accuracy of human fixations prediction. We show that the proposed model outperforms the mainstream algorithms in predicting human fixations.	algorithm;experiment;kerrison predictor;mdl (programming language);minimum description length;redundancy (engineering);seamless3d	Jing Liu;Xiaokang Yang;Guangtao Zhai;Chang Wen Chen	2016	2016 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2016.7527409	computer vision;entropy;visualization;computer science;machine learning;pattern recognition;mathematics;predictive modelling;redundancy	Vision	37.7997584876651	-53.64159294402964	198162
d761402f287c08e3ae907d67fc5e3371325eb410	pairwise rotation invariant co-occurrence local binary pattern	histograms;food;plants;pti principle pairwise rotation invariant co occurrence local binary pattern computer vision spatial co occurrence pairwise transform invariance principle pricolbp feature;texture classification;co occurrence lbps;image processing computer assisted;transforms image color analysis robustness feature extraction lighting histograms encoding;food recognition;scene recognition;rotation invariance;material recognition;transforms computer vision feature extraction;image color analysis;feature extraction;transforms;algorithms;robustness;pattern recognition automated;lighting;databases factual;encoding;flower recognition;leaf recognition	Designing effective features is a fundamental problem in computer vision. However, it is usually difficult to achieve a great tradeoff between discriminative power and robustness. Previous works shown that spatial co-occurrence can boost the discriminative power of features. However the current existing co-occurrence features are taking few considerations to the robustness and hence suffering from sensitivity to geometric and photometric variations. In this work, we study the Transform Invariance (TI) of co-occurrence features. Concretely we formally introduce a Pairwise Transform Invariance (PTI) principle, and then propose a novel Pairwise Rotation Invariant Co-occurrence Local Binary Pattern (PRICoLBP) feature, and further extend it to incorporate multi-scale, multi-orientation, and multi-channel information. Different from other LBP variants, PRICoLBP can not only capture the spatial context co-occurrence information effectively, but also possess rotation invariance. We evaluate PRICoLBP comprehensively on nine benchmark data sets from five different perspectives, e.g., encoding strategy, rotation invariance, the number of templates, speed, and discriminative power compared to other LBP variants. Furthermore we apply PRICoLBP to six different but related applications-texture, material, flower, leaf, food, and scene classification, and demonstrate that PRICoLBP is efficient, effective, and of a well-balanced tradeoff between the discriminative power and robustness.		Xianbiao Qi;Rong Xiao;Chun-Guang Li;Yu Qiao;Jun Guo;Xiaoou Tang	2014	IEEE transactions on pattern analysis and machine intelligence	10.1109/TPAMI.2014.2316826	computer vision;feature extraction;computer science;machine learning;pattern recognition;lighting;histogram;mathematics;encoding;statistics;robustness	Vision	35.49962782529208	-56.95878235444142	198641
ea9aeeb32a9cfce13e7791c8cc7b54df0094de33	spotting the difference: context retrieval and analysis for improved forgery detection and localization		As image tampering becomes ever more sophisticated and commonplace, the need for image forensics algorithms that can accurately and quickly detect forgeries grows. In this paper, we revisit the ideas of image querying and retrieval to provide clues to better localize forgeries. We propose a method to perform large-scale image forensics on the order of one million images using the help of an image search algorithm and database to gather contextual clues as to where tampering may have taken place. In this vein, we introduce five new strongly invariant image comparison methods and test their effectiveness under heavy noise, rotation, and color space changes. Lastly, we show the effectiveness of these methods compared to passive image forensics using Nimble [1], a new, state-of-the-art dataset from the National Institute of Standards and Technology (NIST).	authentication;color space;image retrieval;internationalization and localization;search algorithm	Joel Brogan;Paolo Bestagini;Aparna Bharati;Allan da Silva Pinto;Daniel M Moreira;Kevin W. Bowyer;Patrick J. Flynn;Anderson Rocha;Walter J. Scheirer	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8297049	computer vision;task analysis;artificial intelligence;information retrieval;computer science;spotting;nist;search algorithm;color space	Vision	36.071356183882656	-56.089603536436684	199057
6149229bb6c705b8e4d9fea772e7e375437947d7	gabor texture representation method for face recognition using the gamma and generalized gaussian models	linear discriminate analysis;gabor magnitude;face recognition;generalized gaussian density;generalized gaussian;subspace method;texture information;gabor phase;null space linear discriminant analysis nlda;feature level fusion	0262-8856/$ see front matter 2009 Elsevier B.V. A doi:10.1016/j.imavis.2009.05.012 * Corresponding author. Tel.: +86 13062356757; fa E-mail address: zshe@cqu.edu.cn (Z. He). A novel face recognition algorithm based on Gabor texture information is proposed in this paper. Two kinds of strategies to capture it are introduced: Gabor magnitude-based texture representation (GMTR) and Gabor phase-based texture representation (GPTR). Specifically, GMTR is characterized by using the Gamma density (C D) to model the Gabor magnitude distribution, while GPTR is characterized by using the generalized Gaussian density (GGD) to model the Gabor phase distribution. The estimated model parameters serve as texture representation. Experiments are performed on Yale, ORL and FERET databases to validate the feasibility of the proposed method. The results show that the proposed GMTRbased and GPTR-based NLDA both significantly outperform the widely used Gabor features-based NLDA and other existing subspace methods. In addition, the feature level fusion of these two kinds of texture representations performs better than them individually. 2009 Elsevier B.V. All rights reserved.	algorithm;algorithmic efficiency;converge;database;estimation theory;experiment;feret (facial recognition technology);facial recognition system;feature (computer vision);gabor filter;gabor wavelet;gamma correction;generalized timing formula;gradient descent;graphical editing framework;information theory;online algorithm;outline of object recognition;performance;return loss;time complexity;times ascent	Lei Yu;Zhongshi He;Qi Cao	2010	Image Vision Comput.	10.1016/j.imavis.2009.05.012	facial recognition system;gabor transform;computer vision;gabor–wigner transform;computer science;machine learning;pattern recognition;mathematics;gabor wavelet	Vision	35.320669435334956	-58.10877982012665	199219
b17769414db5a19c1e7113d93b9a7e9c824e8501	a sub-vector weighting scheme for image retrieval with relevance feedback	feature correlation;small samples;multi dimensional;visual features;weighted similarity metric;image retrieval with relevance feedback;relevance feedback;sub vectors;image retrieval	In this paper, a sub-vector weighting scheme is proposed for the case of small sample in image retrieval with relevance feedback. By partitioning a multi-dimensional visual feature vector to multiple sub-vectors, the singularity problem caused by small sample can be avoided by the lower dimensionality of the sub-vectors. Then the optimal weighting can be performed on these sub-vectors respectively and the similarity scores obtained are combined as the final score to rank the database images. Experimental results demonstrated that the proposed weighting scheme can significantly improve the efficacy of image retrieval with relevance feedback.	feature vector;image retrieval;relevance feedback;technological singularity	Lei Wang;Kap Luk Chan;Xuejian Xiong	2002	Int. J. Image Graphics	10.1142/S0219467802000597	visual word;image retrieval;computer science;machine learning;pattern recognition;mathematics;information retrieval	Vision	36.02013856247004	-58.65895821150724	199743
