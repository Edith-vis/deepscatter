id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
550e5d7f607951d7e33e8f23eb6b6bc97780915a	combining facial parts for learning gender, ethnicity, and emotional state based on rgb-d information		With the success of emerging RGB-D cameras such as the Kinect sensor, combining the shape (depth) and texture information to improve the quality of recognition became a trend among computer vision researchers. In this work, we address the problem of face classification in the context of RGB images and depth data. Inspired by the psychological results for human face perception, this article focuses on (i) finding out which facial parts are most effective at making the difference for some social aspects of face perception (gender, ethnicity, and emotional state), (ii) determining the optimal decision by combining the decision rendered by the individual parts, and (iii) extracting the promising features from RGB-D faces to exploit all the potential that this data provide. Experimental results on EurecomKinect Face and CurtinFaces databases show that the proposed approach improves the recognition quality in many use cases.	adaboost;artificial neural network;computer vision;convolutional neural network;database;face perception;facial recognition system;gradient boosting;kinect	Safaa Azzakhnini;Lahoucine Ballihi;Driss Aboutajdine	2018	TOMCCAP	10.1145/3152125	multimedia;optimal decision;exploit;use case;computer science;computer vision;rgb color model;facial recognition system;face perception;artificial intelligence	AI	34.10891714840424	-50.71257066289359	159714
658ca21dc28e9fee1dc7b93d20d56aa06dadf1c1	video classification via weakly supervised sequence modeling		Traditional approaches for video classification treat the entire video clip as one data instance. They extract visual features from video frames which are then quantized (e.g., K-means) and pooled (e.g., average pooling) to produce a single feature vector. Such holistic representations of videos are further used as inputs of a classifier. Despite of efficiency, global and aggregate feature representation unavoidably brings in redundant and noisy information from background and unrelated video frames that sometimes overwhelms targeted visual patterns. Besides, temporal correlations between consecutive video frames are also ignored in both training and testing, which may be the key indicator of an action or event. To this end, we propose Weakly Supervised Sequence Modeling (WSSM), a novel framework that combines multiple-instance learning (MIL) and Conditional Random Field (CRF) model seamlessly. Our model takes each entire video as a bag and one video segment as an instance . In our framework, the salient local patterns for different video categories are explored by MIL, and intrinsic temporal dependencies between instances are explicitly exploited using the powerful chain CRF model. In the training stage, we design a novel conditional likelihood formulation which only requires annotation on videos. Such likelihood can be maximized using an alternating optimization method. The training algorithm is guaranteed to converge and is very efficient. In the testing stage, videos are classified by the learned CRF model. The proposed WSSM algorithm outperforms other MIL-based approaches in both accuracy and efficiency on synthetic data and realistic videos for gesture and action classification. © 2015 Elsevier Inc. All rights reserved.	aggregate data;algorithm;conditional random field;converge;feature vector;frame (video);holism;k-means clustering;mathematical optimization;multiple-instance learning;quantization (signal processing);statistical classification;synthetic data;video clip	Jingjing Liu;Chao Chen;Yan Zhu;Wei Liu;Dimitris N. Metaxas	2016	Computer Vision and Image Understanding	10.1016/j.cviu.2015.10.012	computer vision;computer science;machine learning;video tracking;pattern recognition	Vision	32.51807129765544	-47.93464694194085	161164
431423fa561be8798a8d9ba636611df73fac82f9	gait recognition with adaptively fused gei parts		Though the general gait energy image (GEI) preserves static and dynamic information, most GEI-based gait recognition approaches do not fully exploit it, which leads to inferior performance under the conditions of appearance change, dynamic variation and viewpoint variation. Therefore, this paper proposes a novel Silhouette-based method called GEI parts (GEIs) to identify individuals. The GEIs divides GEI, as the gray-value of GEI indicates different motion of body part. Furthermore, this paper uses k-nearest neighbor as classifier and develops a feature fusion method by adding scores to the recognition results of each GEI part. The proposed method is tested on publicly available CASIA-B dataset under different conditions, by using: (1) different GEI parts individually; (2) adaptively fused GEI parts. The experimental results show that with our proposed adaptive GEIs fusion on the dynamic-static information of walking, the fused GEIs outperforms the state-of-the-art GEI.		Bei Sun;Wusheng Luo;Qin Lu;Liebo Du;Xing Zeng	2016		10.1007/978-3-319-46654-5_52	silhouette;gait;artificial intelligence;computer science;pattern recognition	Vision	35.564172185327045	-51.287927026474726	162338
0e415707ef6824dfffe9bb33b16a0a4a77485f53	human actions recognition: an approach based on stable motion boundary fields		Automatic video action recognition have been a long-standing problem in computer vision. To obtain a scalable solution for actions recognition, it is important to have efficient visual representation of motions. In this paper, we propose a new visual representation for actions based in the body motion boundaries. The first step, a set of optical flow frames highlighting the principal motions in the poses is substracted. Then, the motion boundaries are computed from the previous optical flow frames. Maximum Stable Extremal Regions are then applied to motion boundaries maps in order to obtain Motion Stable Shape (MSS) features. Local descriptors were computed based on each detected MSS to capture motion patterns. To predict the classes of the different human actions, we have represented different descriptors with a bag-of-words (BOW) model and for classification, we use a non-linear support vector machine. We have performed a set of experiments on different datasets: Weizmann, KTH, UCF sport, UCF50 and Hollywood to prove the efficiency of our developed model. The achieved results improve the state-of-the-art on the KTH and Weizmann datasets and are comparable to state-of-the-art for UCF sport and UCF50 datasets.	bag-of-words model in computer vision;deep learning;experiment;hollywood;map;nonlinear system;optical flow;performance;scalability;support vector machine;type signature	Imen Lassoued;Ezzeddine Zagrouba	2017	Multimedia Tools and Applications	10.1007/s11042-017-5477-0	computer science;artificial intelligence;support vector machine;computer vision;pattern recognition;scalability;optical flow	Vision	36.27610508420464	-50.70761958422488	162637
5e0f88ec6ec155e709b7ca47ac9109a4b33a2cbd	discriminative hierarchical modeling of spatio-temporally composable human activities	composable actions;action classification composable actions hierarchical modelling;video signal processing pattern classification pose estimation spatiotemporal phenomena;action classification;hierarchical modelling;videos dictionaries mathematical model vectors equations semantics hidden markov models;max margin framework discriminative hierarchical modeling spatio temporally composable human activities video signal processing discriminative pose dictionary model learning	This paper proposes a framework for recognizing complex human activities in videos. Our method describes human activities in a hierarchical discriminative model that operates at three semantic levels. At the lower level, body poses are encoded in a representative but discriminative pose dictionary. At the intermediate level, encoded poses span a space where simple human actions are composed. At the highest level, our model captures temporal and spatial compositions of actions into complex human activities. Our human activity classifier simultaneously models which body parts are relevant to the action of interest as well as their appearance and composition using a discriminative approach. By formulating model learning in a max-margin framework, our approach achieves powerful multi-class discrimination while providing useful annotations at the intermediate semantic level. We show how our hierarchical compositional model provides natural handling of occlusions. To evaluate the effectiveness of our proposed framework, we introduce a new dataset of composed human activities. We provide empirical evidence that our method achieves state-of-the-art activity classification performance on several benchmark datasets.	activity recognition;algorithm;benchmark (computing);dictionary;discriminative model;ground truth;microsoft research;robustness (computer science);scratch (programming language)	Ivan Lillo;Alvaro Soto;Juan Carlos Niebles	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.109	computer vision;computer science;machine learning;pattern recognition	Vision	34.85815223573941	-47.83329607938286	163358
6e6efc78480e87a48d5d0d688fd933b30a1f89c4	object detection, classification, tracking and individual recognition for sea images and videos		Manually monitoring the population displacement of fish species and the whale individuals is a painful and definitely unscalable process. Video data about fishes often require laborious visual analysis, moreover biologists often use photos of whale caudal for further analysis as it is the most discriminant pattern for distinguishing an individual whale from another. Therefore two challenges were announced in the SeaCLEF of LifeCLEF campaign, one for automatic fish categorization and enumeration, and another for automatic whale individual recognition based on visual contents. We elaborated a complex system to detect, classify and track objects (fishes) in underwater video by examining each image frame of it. We used Kalman filter to track the moving objects, and Hungarian method was used to match the pair of the objects in consecutive time periods because of many fishes. We categorized the detected fishes with C-SVC classifier, as an advanced SVM (Support Vector Machine) classifier. As further improvement we used color histograms and discriminant training method for filtering out false detections. For whale individual recognition we elaborated another system to compare the individuals by applying BoW model, during which Harris-Laplace detector and dense SIFT for creating low-level features. After that GMM based Fisher vectors were calculated and compared to each other with RBF kernel function. In addition to this we tried background segmentation as preprocessing.	categorization;complex system;discriminant;displacement mapping;fisher–yates shuffle;google map maker;harris affine region detector;high- and low-level;hungarian algorithm;kalman filter;object detection;preprocessor;radial basis function kernel;sensor;support vector machine;teaching method	Dávid Papp;Dániel Lovas;Gábor Szücs	2016			object-class detection;computer vision;object detection;artificial intelligence;pattern recognition;computer science	Vision	37.35596413898727	-49.60200848960981	163452
4209e7908dc63aaee5c901d09844209608caa1ed	online learning for human-robot interaction	eigenvalues and eigenfunctions;nonparametric discriminant analysis;human computer interaction;subspace learning;nda eigenspace representation online learning human robot interaction incremental subspace learning nonparametric discriminant analysis;human robot interaction;online learning;discriminant analysis;robots eigenvalues and eigenfunctions human computer interaction learning artificial intelligence man machine systems;incremental subspace learning;real world application;face recognition;robots;nda eigenspace representation;face recognition humans feature extraction scattering machine vision pattern recognition gaussian distribution nearest neighbor searches covariance matrix computer vision;learning artificial intelligence;man machine systems;visual processing	This paper presents a novel approach for incremental subspace learning based on an online version of the non-parametric discriminant analysis (NDA). For many real-world applications (like the study of visual processes, for instance) there is impossible to know beforehand the number of total classes or the exact number of instances per class. This motivated us to propose a new algorithm, in which new samples can be added asynchronously, at different time stamps, as soon as they become available. The proposed technique for NDA-eigenspace representation has been applied to the problem of online face recognition for human-robot interaction scenario.	algorithm;batch processing;facial recognition system;human–robot interaction;linear discriminant analysis;serial digital video out	Bogdan Raducanu;Jordi Vitrià	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383438	facial recognition system;human–robot interaction;robot;computer vision;speech recognition;computer science;machine learning;pattern recognition;linear discriminant analysis	Vision	36.65186738025828	-45.050514907874636	163935
e7a11a0b59a203e6258c297ea7639d83b3483948	modeling of human upper body for sign language recognition	sign language recognition;tracking using camshift;face humans tracking feature extraction real time systems computational modeling;qa mathematics;search algorithm;image classification;camshift human upper body modeling sign language recognition system facial features hand position forehead shoulders chest hand gestures gesture classification hub parts head size face size partial estimation;qa75 electronic computers computer science;image classification gesture recognition;scaling;sign language recognition human upper body detection scaling tracking using camshift;facial features;real time implementation;positional information;real time application;gesture recognition;human upper body detection;tk electrical engineering electronics nuclear engineering	Sign Language Recognition systems require not only the hand motion trajectory to be classified but also facial features, Human Upper Body (HUB) and hand position with respect to other HUB parts. Head, face, forehead, shoulders and chest are very crucial parts that can carry a lot of positioning information of hand gestures in gesture classification. In this paper as the main contribution, a fast and robust search algorithm for HUB parts based on head size has been introduced for real time implementations. Scaling the extracted parts during body orientation was attained using partial estimation of face size. Tracking the extracted parts for front and side view was achieved using CAMSHIFT [24]. The outcome of the system makes it applicable for real-time applications such as Sign Languages Recognition (SLR) systems.	computation;gesture recognition;image scaling;language identification;performance;real-time clock;real-time computing;requirement;sl (complexity);scalability;search algorithm;time complexity	Sara Bilal;Rini Akmeliawati;Amir Akramin Shafie;Momoh-Jimoh E. Salami	2011	The 5th International Conference on Automation, Robotics and Applications	10.1109/ICARA.2011.6144865	computer vision;contextual image classification;speech recognition;scaling;computer science;artificial intelligence;gesture recognition;search algorithm	Robotics	39.068159103082486	-49.76314973821023	164040
1b4424e06ac29b72535727b92f261f39d065e858	3d pictorial structures revisited: multiple human pose estimation	detectors;biological patents;biomedical journals;text mining;europe pubmed central;citation search;biological system modeling;citation networks;part based models human pose estimation 3d pictorial structures;computational modeling;research articles;3d pictorial structures;three dimensional displays;abstracts;part based models;open access;solid modeling;life sciences;clinical guidelines;full text;ssvm 3d pictorial structure model human 3d pose estimation 3dps model structured svm;three dimensional displays cameras solid modeling computational modeling biological system modeling detectors;rest apis;cameras;orcids;support vector machines pose estimation;europe pmc;biomedical research;human pose estimation;bioinformatics;literature search	We address the problem of 3D pose estimation of multiple humans from multiple views. The transition from single to multiple human pose estimation and from the 2D to 3D space is challenging due to a much larger state space, occlusions and across-view ambiguities when not knowing the identity of the humans in advance. To address these problems, we first create a reduced state space by triangulation of corresponding pairs of body parts obtained by part detectors for each camera view. In order to resolve ambiguities of wrong and mixed parts of multiple humans after triangulation and also those coming from false positive detections, we introduce a 3D pictorial structures (3DPS) model. Our model builds on multi-view unary potentials, while a prior model is integrated into pairwise and ternary potential functions. To balance the potentials' influence, the model parameters are learnt using a Structured SVM (SSVM). The model is generic and applicable to both single and multiple human pose estimation. To evaluate our model on single and multiple human pose estimation, we rely on four different datasets. We first analyse the contribution of the potentials and then compare our results with related work where we demonstrate superior performance.	3d pose estimation;body part;detectors;embryonic structures, nonmammalian;human-based computation;image;inference;large;numerous;obstruction;self;small round structured viruses;state space;structured support vector machine;unary operation;algorithm;triangulation	Vasileios Belagiannis;Sikandar Amin;Mykhaylo Andriluka;Bernt Schiele;Nassir Navab;Slobodan Ilic	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2015.2509986	computer vision;detector;text mining;simulation;3d pose estimation;computer science;machine learning;data mining;solid modeling;computational model;statistics	Vision	35.426823710787204	-47.36634012473676	164295
f6b0937cc379fb97ceb5d82464f2123ee48e0d6c	crowd activity change point detection in videos via graph stream mining		In recent years, there has been a growing interest in detecting anomalous behavioral patterns in video. In this work, we address this task by proposing a novel activity change point detection method to identify crowd movement anomalies for video surveillance. In our proposed novel framework, a hyperspherical clustering algorithm is utilized for the automatic identification of interesting regions, then the density of pedestrian flows between every pair of interesting regions over consecutive time intervals is monitored and represented as a sequence of adjacency matrices where the direction and density of flows are captured through a directed graph. Finally, we use graph edit distance as well as a cumulative sum test to detect change points in the graph sequence. We conduct experiments on four real-world video datasets: Dublin, New Orleans, Abbey Road and MCG Datasets. We observe that our proposed approach achieves a high F-measure, i.e., in the range [0.7, 1], for these datasets. The evaluation reveals that our proposed method can successfully detect the change points in all datasets at both global and local levels. Our results also demonstrate the efficiency and effectiveness of our proposed algorithm for change point detection and segmentation tasks.		Meng Yang;Lida Rashidi;Sutharshan Rajasegarar;Christopher Leckie;Aravinda S. Rao;Marimuthu Palaniswami	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00059	adjacency matrix;directed graph;object detection;pattern recognition;trajectory;cluster analysis;artificial intelligence;change detection;computer science;segmentation;behavioral pattern	Vision	38.27364950521029	-46.45545618666998	164472
08ff22f76a567fcbc1afec6bfbf957a560cfadc7	exploring person context and local scene context for object detection		In this paper we explore two ways of using context for object detection. The first model focusses on people and the objects they commonly interact with, such as fashion and sports accessories. The second model considers more general object detection and uses the spatial relationships between objects and between objects and scenes. Our models are able to capture precise spatial relationships between the context and the object of interest, and make effective use of the appearance of the contextual region. On the newly released COCO dataset, our models provide relative improvements of upto 5% over CNN-based state-of-the-art detectors, with the gains concentrated on hard cases such as small objects (10% relative improvement).	contextual inquiry;hard coding;object detection;sensor	Saurabh Gupta;Bharath Hariharan;Jitendra Malik	2015	CoRR		computer vision;object-based spatial database;multimedia	Vision	33.21037349696534	-51.21603078047123	165001
1fa426496ed6bcd0c0b17b8b935a14c84a7ee1c2	binary coding for partial action analysis with limited observation ratios		Traditional action recognition methods aim to recognize actions with complete observations/executions. However, it is often difficult to capture fully executed actions due to occlusions, interruptions, etc. Meanwhile, action prediction/recognition in advance based on partial observations is essential for preventing the situation from deteriorating. Besides, fast spotting human activities using partially observed data is a critical ingredient for retrieval systems. Inspired by the recent success of data binarization in efficient retrieval/recognition, we propose a novel approach, named Partial Reconstructive Binary Coding (PRBC), for action analysis based on limited frame glimpses during any period of the complete execution. Specifically, we learn discriminative compact binary codes for partial actions via a joint learning framework, which collaboratively tackles feature reconstruction as well as binary coding. We obtain the solution to PRBC based on a discrete alternating iteration algorithm. Extensive experiments on four realistic action datasets in terms of three tasks (i.e., partial action retrieval, recognition and prediction) clearly show the superiority of PRBC over the state-of-the-art methods, along with significantly reduced memory load and computational costs during the online test.	algorithm;binary code;binary number;computation;discrete optimization;experiment;iteration;iterative method;mathematical optimization	Jie Qin;Li Liu;Ling Shao;Bingbing Ni;Chen Chen;Fumin Shen;Yunhong Wang	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.712	computer vision;discriminative model;artificial intelligence;pattern recognition;binary code;support vector machine;machine learning;computer science;encoding (memory);semantics	Vision	31.946537023558417	-50.119780626754576	166171
2179779fdd977477c52703c4feef37450fe0e36e	monocular 3d pose recovery via nonconvex sparsity with theoretical analysis		For recovering 3D object poses from 2D images, a prevalent method is to pre-train an over-complete dictionary $\mathcal D=\{B_i\}_i^D$ of 3D basis poses. During testing, the detected 2D pose $Y$ is matched to dictionary by $Y \approx \sum_i M_i B_i$ where $\{M_i\}_i^D=\{c_i \Pi R_i\}$, by estimating the rotation $R_i$, projection $\Pi$ and sparse combination coefficients $c \in \mathbb R_{+}^D$. In this paper, we propose non-convex regularization $H(c)$ to learn coefficients $c$, including novel leaky capped $\ell_1$-norm regularization (LCNR), \begin{align*} H(c)=\alpha \sum_{i } \min(|c_i|,\tau)+ \beta \sum_{i } \max(| c_i|,\tau), \end{align*} where $0\leq \beta \leq \alpha$ and $0<\tau$ is a certain threshold, so the invalid components smaller than $\tau$ are composed with larger regularization and other valid components with smaller regularization. We propose a multi-stage optimizer with convex relaxation and ADMM. We prove that the estimation error $\mathcal L(l)$ decays w.r.t. the stages $l$, \begin{align*} Pr\left(\mathcal L(l)<\rho^{l-1} \mathcal L(0) + \delta \right) \geq 1- \epsilon, \end{align*} where $0<\rho<1, 0<\delta, 0<\epsilon \ll 1$. Experiments on large 3D human datasets like H36M are conducted to support our improvement upon previous approaches. To the best of our knowledge, this is the first theoretical analysis in this line of research, to understand how the recovery error is affected by fundamental factors, e.g. dictionary size, observation noises, optimization times. We characterize the trade-off between speed and accuracy towards real-time inference in applications.		Jianqiao Wangni;Dahua Lin;Ji Liu;Kostas Daniilidis;Jianbo Shi	2018	CoRR			ML	32.99696119661757	-46.25395186487485	166445
7177649ece5506b315cb73c36098baac1681b8d2	lower resolution face recognition in surveillance systems using discriminant correlation analysis		Due to large distances between surveillance cameras and subjects, the captured images usually have low resolution in addition to uncontrolled poses and illumination conditions that adversely affect the performance of face recognition algorithms. In this paper, we present a low-resolution face recognition technique based on Discriminant Correlation Analysis (DCA). DCA analyzes the correlation of the features in high-resolution and low-resolution images and aims to find projections that maximize the pair-wise correlations between the two feature sets and at the same time, separate the classes within each set. This makes it possible to project the features extracted from high-resolution and low-resolution images into a common space, in which we can apply matching. The proposed method is computationally efficient and can be applied to challenging real-time applications such as recognition of several faces appearing in a crowded frame of a surveillance video. Extensive experiments performed on low-resolution surveillance images from the SCface database as well as FRGC database demonstrated the efficacy of our proposed approach in the recognition of low-resolution face images, which outperformed other state-of-the-art techniques.	algorithm;algorithmic efficiency;closed-circuit television;computational complexity theory;discriminant;experiment;facial recognition system;image resolution;lr parser;real-time clock;real-time computing;uncontrolled format string	Mohammad Haghighat;Mohamed Abdel-Mottaleb	2017	2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)	10.1109/FG.2017.130	facial recognition system;three-dimensional face recognition;discriminant;computer vision;artificial intelligence;pattern recognition;correlation;computer science	Vision	35.073457436193024	-50.78251437434615	166446
5cc2586acac8d0effe613bd5b19c96e4905f54c4	facial expression recognition based on video		In recent years, video timing characteristics of facial expression recognition research has become a hot topic. In this article, through analyzing changes in the feature data in realtime detection of facial expression, and combining temporal and spatial features to establish models, we put forward a more rapid and more efficient method to recognize facial expressions. Firstly, the feature extraction method based on Bezier curve is adopted to extract the 2D feature points of each frame in the video stream. Then, we get the changes of temporal characteristic curve through connecting the feature points of each frame image along time slice. Finally we use nonlinear function to fit the changes of the temporal characteristics curve and we establish models for each expression to classify and recognize. The results show that the method which combines the temporal and spatial changes in facial expression recognition, has the advantages of high speed and recognition rate.	bézier curve;contour line;curve fitting;energy (psychological);expression templates;feature data;feature extraction;nonlinear system;preemption (computing);raster scan;streaming media;time series	Xin Song;Hong Bao	2016	2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)	10.1109/AIPR.2016.8010549	bézier curve;feature (computer vision);three-dimensional face recognition;facial recognition system;feature extraction;computer vision;facial expression;pattern recognition;feature data;computer science;artificial intelligence	Vision	37.831608990614946	-49.43020708483586	166816
16e2e9e4741795c004d15e95532b07943d3a3242	cps: 3d compositional part segmentation through grasping	three dimensional displays robots grasping semantics feature extraction shape training;robot vision image segmentation manipulators;3d object representation;grasping;object part segmentation;graspability;training;semantics;novel part instances segmentation cps 3d compositional part segmentation through grasping 3d algorithm compositional graph hierarchy;graspability compositional model 3d object representation object part segmentation;shape;three dimensional displays;feature extraction;robots;compositional model	"""Most objects are composed of parts which have a semantic meaning. Ahandle can have many different shapes and can be present in quite different objects, but there is only one semantic meaning to a handle, which is """"a part that is designed especially to be grasped by the hand"""". We introduce here a novel 3D algorithm named CPS for the decomposition of objects into their semantically meaningful parts. These meaningful parts are learned from experiments where robot grasps different objects. Objects are represented in compositional graph hierarchy where their parts are represented as the relationship between subparts, which are in turn represented based on the relationships between small adjacent regions. Unlike other compositional approaches, our method relies on learning semantically meaningful parts which are learned from grasping experience. This compositional part representation provides generalization for part segmentation. We evaluated our method in this respect, by training ion one dataset and evaluating it on another. We achieved on average78% part overlap accuracy for segmentation of novel part instances."""	algorithm;experiment	Safoura Rezapour Lakani;Mirela Popa;Antonio Jose Rodríguez-Sánchez;Justus H. Piater	2015	2015 12th Conference on Computer and Robot Vision	10.1109/CRV.2015.24	robot;computer vision;feature extraction;shape;computer science;artificial intelligence;machine learning;semantics	Vision	35.77613133620642	-46.1997507873861	167757
3d6ffa6bc7dac2d9ec4131bdce59506689ce2841	integrating vision and language: semantic description of traffic events from image sequences	moving object;image sequence;semantic description;virtual space;natural language processing;extraction method	We propose an event extraction method from traffic image sequences. This method extracts moving objects and their trajectories from image sequences recorded by a stationary camera. These trajectories are mapped to 3D virtual space and physical parameters such as velocity and direction are estimated. After that, traffic events are extracted from these trajectories and physical parameters based on case-frame analysis in the field of natural language processing. Our method facilitates to describe events easily and detect general traffic events and abnormal situations. The experimental results of actual intersection traffic image sequence have shown the effectiveness of the method.		Takashi Hirano;Shogo Yoneyama;Yasuhiro Okada;Yukio Kosugi	2007		10.1007/978-3-540-76856-2_45	computer vision;computer science;theoretical computer science	Vision	38.88004813953057	-47.48690284013049	167879
925d30bb14590931e945d874f8ccb1377ee213d1	on-line, incremental learning for real-time vision based movement recognition	caviar dataset;image recognition;video surveillance;real time surveillance video;legged locomotion;heuristic rule;real time;training;motion estimation;ripple down rules;knowledge acquisition incremental learning real time vision movement recognition real time surveillance video public dataset caviar dataset groundtruth labeling shopping centre movement classification heuristic rule formal knowledge maintenance ripple down rule online learning;online learning;observers;groundtruth labeling;computer vision;error analysis;real time vision ripple down rules knowledge acquisition;incremental learning;video surveillance computer vision image recognition knowledge acquisition learning artificial intelligence motion estimation pattern classification;knowledge based systems training legged locomotion humans real time systems error analysis observers;real time vision;public dataset;knowledge acquisition;pattern classification;shopping centre;ground truth;humans;learning artificial intelligence;movement recognition;knowledge based systems;ripple down rule;movement classification;formal knowledge maintenance;real time systems	In this paper we tackle the problem of recognising movement classes in real-time surveillance video. We use a popular public dataset, the CAVIAR dataset, which contains ground truth labeling of people and their activities within a shopping centre environment. The task of movement classification is often performed using simple heuristic rules, and performance can suffer when an increased number of rules are added for the task. We provide a formal knowledge maintenance technique, known as Ripple Down Rules, to provide an elegant method of representing and updating the rules. Ripple Down Rules are an on-line, incremental learning strategy, and are highly suitable for this task due to their ability to incorporate new knowledge while maintaining past knowledge.	activity recognition;closed-circuit television;gene ontology term enrichment;ground truth;heuristic;knowledge acquisition;knowledge base;machine learning;online and offline;real-time data;real-time locating system;real-time transcription;restrictive design rules;ripple;statistical parsing	Anuraag Sridhar;Arcot Sowmya;Paul Compton	2010	2010 Ninth International Conference on Machine Learning and Applications	10.1109/ICMLA.2010.75	computer vision;ground truth;computer science;artificial intelligence;knowledge-based systems;machine learning;motion estimation	Robotics	38.78204278147513	-45.156159539454784	167895
b3fbef494127a079ffa0502484475dadd792634c	3d human pose regression via robust sparse tensor subspace learning		In this paper, we present a novel algorithm called robust sparse tensor subspace learning (RSTSL) for 3D human pose regression, and it further extends the latest presented tensor learning to a sparse case. A set of interrelated sparse discriminant projection matrices for feature extraction can be obtained by introducing k-mode optimization and elastic-net algorithm into the objective function of RSTSL and each non-zero element in each discriminant projection matrix is selected from the most typical variables. Thus, the most important low-dimensional tensor feature (LDTF) that corresponds to a test image (i.e., high-order tensor) is extracted through sparse projection transformation. Moreover, we present a novel regression model called optimal-order support tensor regression (OOSTR) to build a finest mapping function between LDTF and 3D human pose configuration. Extensive simulations are conducted on two human motion databases, HumanEva and Brown databases, experimental results show that our proposed RSTSL can not only weaken the sensitivity to incoherent human motions caused by transient occlusion of cameras, sudden change in human velocity and low-frame rate but also strengthen the robustness to silhouette ambiguity, obstacle occlusion and random noise. All the results have confirmed that our tracking system achieves the most significant performance against the compared the state-of-the-art approaches, especially in the complicated human motion databases with different clustered backgrounds, human movements, clothing-style, illumination and subjects like HumanEva database.	3d projection;algorithm;database;discriminant;elastic map;feature extraction;kinesiology;loss function;mathematical optimization;multilinear subspace learning;noise (electronics);optimization problem;simulation;sparse matrix;standard test image;tracking system;velocity (software development)	Jialin Yu;Jifeng Sun	2015	Multimedia Tools and Applications	10.1007/s11042-015-3186-0	computer vision;machine learning;pattern recognition;statistics	AI	32.99451557603603	-46.7390543630753	167961
1d5b030747bd836aebf7a00ed061a2f7bdf0a84c	discriminative pose-free descriptors for face and object matching		Pose invariant matching is a very important and challenging problem with various applications like recognizing faces in uncontrolled scenarios, matching objects taken from different view points, etc. In this paper, we propose a discriminative pose-free descriptor (DPFD) which can be used to match faces/objects across pose variations. Training examples at very few representative poses are used to generate virtual intermediate pose subspaces. An image or image region is then represented by a feature set obtained by projecting it on all these subspaces and a discriminative transform is applied on this feature set to make it suitable for classification tasks. Finally, this discriminative feature set is represented by a single feature vector, termed as DPFD. The DPFD of images taken from different viewpoints can be directly compared for matching. Extensive experiments on recognizing faces across pose, pose and resolution on the Multi-PIE and Surveillance Cameras Face datasets and comparisons with state-of-the-art approaches show the effectiveness of the proposed approach. Experiments on matching general objects across viewpoints show the generalizability of the proposed approach beyond faces.	digital camera;experiment;feature vector;uncontrolled format string	Soubhik Sanyal;Sivaram Prasad Mudunuri;Soma Biswas	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1016/j.patcog.2017.02.016	computer vision;machine learning;pattern recognition;mathematics	Vision	34.60810196811094	-51.374068852085514	168127
155248dca69d0875a99896d1c028b9b6b3b2414e	video summarization by k-medoid clustering	cluster algorithm;video summarization;k medois;video indexing;clustering;indexation;video indexing and retrieval;block matching;video summary;frequency domain;full search block matching	In this paper, we propose a video summarization algorithm by multiple extractions of key frames in each shot. This algorithm is based on the k-medoid clustering algorithms to find the best representative frame for each video shot. This algorithm, which is applicable to all types of descriptors, consists of extracting key frames by similarity clustering according to the given index. In our proposal, the distance between frames is calculated using a fast full search block matching algorithm based on the frequency domain. The proposed approach is computationally tractable and robust with respect to sudden changes in mean intensity within a shot. Additionally, this approach produces different key frames even in the presence of large motion. The experiments results show that our algorithm extracts multiple representatives frames in each video shot without visual redundancy, and thus it is an effective tool for video indexing and retrieval.	block-matching algorithm;cluster analysis;cobham's thesis;experiment;k-medoids;key frame;medoid	Youssef Hadi;Fedwa Essannouni;Rachid Oulad Haj Thami	2006		10.1145/1141277.1141601	video compression picture types;reference frame;computer science;theoretical computer science;automatic summarization;machine learning;video tracking;pattern recognition;block-matching algorithm;cluster analysis;motion compensation;frequency domain;information retrieval	Vision	38.63774120417586	-51.64839401316964	168330
ee8a7df3d0d93141e9a29032dae09706c8279d67	human action classification based on sequential bag-of-words model	detectors;histograms;training data;accuracy;visualization;motion segmentation;feature extraction;feature extraction visualization motion segmentation accuracy histograms detectors training data;image segmentation image classification;rochester dataset human action classification sequential bag of words model visual words sequential bow temporal sequential structure action segmentation subaction weight subaction salience subaction discrimination evaluation training data ut interaction dataset	Recently, approaches utilizing spatial-temporal features have achieved great success in human action classification. However, they typically rely on bag-of-words (BoWs) model, and ignore the spatial and temporal structure information of visual words, bringing ambiguities among similar actions. In this paper, we present a novel approach called sequential BoWs for efficient human action classification. It captures temporal sequential structure by segmenting the entire action into sub-actions. Each sub-action has a tiny movement within a narrow range of action. Then the sequential BoWs are created, in which each sub-action is assigned with a certain weight and salience to highlight the distinguishing sections. It is noted that the weight and salience are figured out in advance according to the sub-action's discrimination evaluated by training data. Finally, those sub-actions are used for classification respectively, and voting for united result. Experiments are conducted on UT-interaction dataset and Rochester dataset. The results show its higher robustness and accuracy over most state-of-the-art classification approaches.	bag-of-words model in computer vision;experiment	Hong W. Liu;Qiaoduo Zhang;Qianru Sun	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090677	computer vision;training set;detector;visualization;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;histogram;accuracy and precision;statistics	Robotics	33.21799032439158	-51.66971599235085	168531
07625af8d73142e239b5cdccb1dd226648e4b0d4	learning scene-independent group descriptors for crowd understanding	hidden markov models visualization stability analysis robustness circuit stability psychology feature extraction;circuit stability;psychology;visualization;hidden markov models;video analysis group property analysis crowded scene understanding;feature extraction;stability analysis;robustness	Groups are the primary entities that make up a crowd. Understanding group-level dynamics and properties is thus scientifically important and practically useful in a wide range of applications, especially for crowd understanding. In this paper, we show that fundamental group-level properties, such as intra-group stability and inter-group conflict, can be systematically quantified by visual descriptors. This is made possible through learning a novel collective transition prior, which leads to a robust approach for group segregation in public spaces. From the former, we further devise a rich set of group-property visual descriptors. These descriptors are scene-independent and can be effectively applied to public scenes with a variety of crowd densities and distributions. Extensive experiments on hundreds of public scene video clips demonstrate that such property descriptors are complementary to each other, scene-independent, and they convey critical information on physical states of a crowd. The proposed group-level descriptors show promising results and potentials in multiple applications, including crowd dynamic monitoring, crowd video classification, and crowd video retrieval.	algorithm;computer vision;entity;experiment;video clip;visual descriptor	Jing Shao;Chen Change Loy;Xiaogang Wang	2017	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2016.2539878	computer vision;von neumann stability analysis;simulation;visualization;feature extraction;computer science;machine learning;hidden markov model;robustness	Vision	37.359758335557196	-47.79767849920628	168644
7dfc24b8e21fa7a397d5557574efedd218864d10	robust visual tracking via consistent low-rank sparse learning	low rank representation;temporal consistency;visual tracking;sparse representation	Object tracking is the process of determining the states of a target in consecutive video frames based on properties of motion and appearance consistency. In this paper, we propose a consistent low-rank sparse tracker (CLRST) that builds upon the particle filter framework for tracking. By exploiting temporal consistency, the proposed CLRST algorithm adaptively prunes and selects candidate particles. By using linear sparse combinations of dictionary templates, the proposed method learns the sparse representations of image regions corresponding to candidate particles jointly by exploiting the underlying low-rank constraints. In addition, the proposed CLRST algorithm is computationally attractive since temporal consistency property helps prune particles and the low-rank minimization problem for learning joint sparse representations can be efficiently solved by a sequence of closed form update operations. We evaluate the proposed CLRST algorithm against $$14$$ 14 state-of-the-art tracking methods on a set of $$25$$ 25 challenging image sequences. Experimental results show that the CLRST algorithm performs favorably against state-of-the-art tracking methods in terms of accuracy and execution time.	algorithm;dictionary;frame (video);particle filter;run time (program lifecycle phase);sparse matrix;video tracking	Tianzhu Zhang;Si Liu;Narendra Ahuja;Ming-Hsuan Yang;Bernard Ghanem	2014	International Journal of Computer Vision	10.1007/s11263-014-0738-0	computer vision;eye tracking;k-svd;computer science;machine learning;pattern recognition;sparse approximation;mathematics	Vision	33.76260406146514	-48.06392878795941	168675
998fe8ca18cc408377a04851e8b931593e074b8d	robotic grasp detection using extreme learning machine	robots feature extraction grasping object detection histograms support vector machines training;hog features robotic grasp detection extreme learning machine object grasping robot vision manipulator machine learning based method object 3d shape elm based method candidate object detection object major orientation estimation candidate object region extraction depth information sliding windows cascaded classifier training grasp identification histograms of oriented gradients;histograms of oriented gradients;robot vision feature extraction grippers image classification image segmentation learning artificial intelligence learning systems manipulators object detection;robotic grasping;machine learning;extreme learning machine;histograms of oriented gradients robotic grasping machine learning extreme learning machine	Object grasping using vision is one of the important functions of manipulators. Machine learning based methods have been proposed for grasp detection. However, due to the variety of grasps and 3D shapes of objects, how to effectively find the best grasp is still a challenging issue. Thus this paper presents an extreme learning machine (ELM) based method to cope with this issue. This proposed method consists of three successive modules, including candidate object detection, estimation of object's major orientations and grasp detection. In the first module, candidate object region is extracted based on depth information. In the second module, object's major orientations guide the directions for sliding windows. In the third module, a cascaded classifier is trained to identify the right grasp. ELM is used as the base classifier in the cascade. Histograms of oriented gradients (HOG) are used as features. Experimental results in benchmark dataset and real manipulators have shown that this proposed method outperforms other methods in terms of accuracy and computational efficiency.	3d modeling;benchmark (computing);computation;gradient;kinect;machine learning;microsoft windows;object detection;physical information;robot;simulation;statistical classification	Changliang Sun;Yuanlong Yu;Huaping Liu;Jason Jianjun Gu	2015	2015 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2015.7418921	computer vision;computer science;viola–jones object detection framework;machine learning;pattern recognition	Robotics	36.28635050515477	-47.605999900759635	168729
12408382e7f98afd22795885df14496dc7884c79	incrementally detecting moving objects in video with sparsity and connectivity	subspace learning;sparsity;connectivity;object detection	Moving object detection is crucial for cognitive vision-based robot tasks. However, due to noise, dynamic background, variations in illumination, and high frame rate, it is a challenging task to robustly and efficiently detect moving objects in video using the clue of motion. State-of-the-art batch-based methods view a sequence of images as a whole and then model the background and foreground together with the constraints of foreground sparsity and connectivity (smoothness) in a unified framework. But the efficiency of the batch-based methods is very low. State-of-the-art incremental methods model the background by a subspace whose bases are updated frame by frame. However, such incremental methods do not make full use of the foreground sparsity and connectivity. In this paper, we develop an incremental method for detecting moving objects in video. Compared to existing methods, the proposed method not only incrementally models the subspace for background reconstruction but also takes into account the sparsity and connectivity of the foreground. The optimization of the model is very efficient. Experimental results on nine public videos demonstrate that the proposed method is much efficient than the state-of-the-art batch methods and has higher F1-score than the state-of-the-art incremental methods.	algorithm;dynamic problem (algorithms);f1 score;loss function;mathematical optimization;object detection;optimization problem;sensor;sparse matrix;unified framework	Jing Pan;Xiaoli Li;Xuelong Li;Yanwei Pang	2015	Cognitive Computation	10.1007/s12559-015-9373-5	computer vision;background subtraction;computer science;connectivity;machine learning;pattern recognition;sparsity-of-effects principle;statistics	Vision	33.42852598492841	-48.30971505906572	168795
59fa5ee2fce87527c47311a4b4dd8135a6f71ed0	learning multiple experiences useful visual features for active maps localization in crowded environments	bayes filtering;visual localization;bag of words;active maps	Crowded urban environments are composed of different types of dynamic and static elements. Learning and classification of features is a major task in solving the localization problem in such environments. This work presents a gradual learning methodology to learn the useful features using multiple experiences. The usefulness of an observed element is evaluated by a scoring mechanism which uses two scores – reliability and distinctiveness. The visual features thus learned are used to partition the visual map into smaller regions. The robot is efficiently localized in such a partitioned environment using two-level localization. The concept of active map (AM) is proposed here, which is a map that represents one partition of the environment in which there is a high probability of the robot existing. High-level localization is used to track the mode of the AMs using discrete Bayes filter. Low-level localization uses a bag-of-words model to retrieve images and accurately localize the robot. The pose of the robo...	experience;map	A. H. Abdul Hafez;Manpreet Arora;K. Madhava Krishna;C. V. Jawahar	2016	Advanced Robotics	10.1080/01691864.2015.1090336	computer vision;computer science;bag-of-words model;machine learning	Robotics	33.336564869516614	-51.05897638489474	169008
213dc2741be575529af9718d0844ace667d02b34	motion flow-based video retrieval	motion analysis;moving object;video databases;image motion analysis;trajectoire;estimation mouvement;multimedia;query processing;information science;recherche image;web and internet services;search engines;helium;information retrieval;image matching;motion vectors;estimacion movimiento;corps mobile;interrogation base donnee;interrogacion base datos;video retrieval motion analysis;video retrieval;analyse mouvement;motion estimation;base donnee video;segmentation;indexing terms;motion flow based video retrieval query by example query by sketch matching strategy video database mpeg bitstreams motion vectors;video coding;compression image;trajectory;senal video;shape;signal video;image compression;motion vector;content based retrieval shape information science computer science spatial databases search engines web and internet services multimedia databases information retrieval;cuerpo movil;spatial databases;mpeg bitstreams;multimedia databases;query by sketch;video signal;moving objects;trayectoria;moving body;computer science;query by example;analisis movimiento;matching strategy;motion flow based video retrieval;video database;article;content based retrieval;models;database query;video retrieval image matching image motion analysis query processing video coding;compresion imagen;image retrieval	In this paper, we propose the use of motion vectors embedded in MPEG bitstreams to generate so-called ldquomotion flowsrdquo, which are applied to perform video retrieval. By using the motion vectors directly, we do not need to consider the shape of a moving object and its corresponding trajectory. Instead, we simply ldquolinkrdquo the local motion vectors across consecutive video frames to form motion flows, which are then recorded and stored in a video database. In the video retrieval phase, we propose a new matching strategy to execute the video retrieval task. Motions that do not belong to the mainstream motion flows are filtered out by our proposed algorithm. The retrieval process can be triggered by query-by-sketch or query-by-example. The experiment results show that our method is indeed superb in the video retrieval process.	algorithm;best, worst and average case;bitstream;embedded system;macroblock;moving picture experts group;query by example	Chih-Wen Su;Hong-Yuan Mark Liao;Hsiao-Rong Tyan;Chia-Wen Lin;Duan-Yu Chen;Kuo-Chin Fan	2007	IEEE Transactions on Multimedia	10.1109/TMM.2007.902875	video compression picture types;computer vision;index term;information science;image retrieval;shape;image compression;quarter-pixel motion;computer science;query by example;trajectory;motion interpolation;video tracking;motion estimation;block-matching algorithm;multimedia;helium;motion compensation;segmentation;world wide web;information retrieval	Vision	38.888248735443895	-51.6719847826655	169518
97acb4ee7e2296dee563c16da6c075b45114077c	understanding egocentric activities	object recognition;proceedings;inference mechanisms;object recognition gesture recognition inference mechanisms object detection;computer vision;egocentric activities;principal component analysis;post print;bag of words;object hand interactions;gesture recognition;hierarchical model;object detection;bag of words understanding egocentric activity daily activity meal preparation egocentric camera inference activity recognition methods pre trained detectors object detection hand detection consistent appearance joint modeling object hand interactions superior performance standard activity representations;activity recognition	We present a method to analyze daily activities, such as meal preparation, using video from an egocentric camera. Our method performs inference about activities, actions, hands, and objects. Daily activities are a challenging domain for activity recognition which are well-suited to an egocentric approach. In contrast to previous activity recognition methods, our approach does not require pre-trained detectors for objects and hands. Instead we demonstrate the ability to learn a hierarchical model of an activity by exploiting the consistent appearance of objects, hands, and actions that results from the egocentric context. We show that joint modeling of activities, actions, and objects leads to superior performance in comparison to the case where they are considered independently. We introduce a novel representation of actions based on object-hand interactions and experimentally demonstrate the superior performance of our representation in comparison to standard activity representations such as bag of words.	activity recognition;bag-of-words model;experiment;hierarchical database model;interaction;sensor	Alireza Fathi;Ali Farhadi;James M. Rehg	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126269	computer vision;computer science;bag-of-words model;cognitive neuroscience of visual object recognition;machine learning;gesture recognition;hierarchical database model;principal component analysis;activity recognition	Vision	34.91851307534467	-48.111571200271264	169837
007b704285e6c51896d55e4bd4a469c90b4e6dc6	extracting discriminative features using task-oriented gaze maps measured from observers for personal attribute classification		Abstract We discuss how to reveal and use the gaze locations of observers who view pedestrian images for personal attribute classification. Observers look at informative regions when attempting to classify the attributes of pedestrians in images. Thus, we hypothesize that the regions in which observers’ gaze locations are clustered will contain discriminative features for the classifiers of personal attributes. Our method acquires the distribution of gaze locations from several observers while they perform the task of manually classifying each personal attribute. We term this distribution a task-oriented gaze map. To extract discriminative features, we assign large weights to the region with a cluster of gaze locations in the task-oriented gaze map. In our experiments, observers mainly looked at different regions of body parts when classifying each personal attribute. Furthermore, our experiments show that the gaze-based feature extraction method significantly improved the performance of personal attribute classification when combined with a convolutional neural network or metric learning technique.	map	Masashi Nishiyama;Riku Matsumoto;Hiroki Yoshimura;Yoshio Iwai	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.08.001	mathematics;computer vision;artificial intelligence;convolutional neural network;discriminative model;gaze;feature extraction;pattern recognition;personal attribute	Vision	33.957192517843175	-51.72284312395439	170097
31585eb502e5964165a0e8690363ae3bef2aaf56	multi-object tracking, event modeling, and activity discovery in video sequences	and activity discovery in video sequences university of maryland;computer science multi object tracking;college park rama chellappa joo;event modeling;dissertation;seong wook	Title of dissertation: MULTI-OBJECT TRACKING, EVENT MODELING, AND ACTIVITY DISCOVERY IN VIDEO SEQUENCES Seong-Wook Joo Doctor of Philosophy, 2007 Dissertation directed by: Professor Rama Chellappa Department of Electrical and Computer Engineering, Department of Computer Science One of the main goals of computer vision is video understanding, where objects in the video are detected, tracked, and their behavior is analyzed. In this dissertation, several key problems in video understanding are addressed, focusing on video surveillance applications. Moving target detection and tracking is one of the most fundamental tasks in visual surveillance. A new moving target detection method is proposed where the temporal variance is used as a measure for characterizing object motion. Our method is experimentally shown to produce high detection rates while keeping low false positive rates. In tracking multiple objects, it is essential to correctly associate targets and measurements. We describe an efficient multi-object tracking approach that maintains multiple hypotheses over time regarding the association of targets and measurements. The data association problem is solved by a combinatorial optimization technique which finds the most likely association allowing track initiation, termination, merge, and split. Experimental results show that our method tracks through varying degrees of interactions among the targets with high success rate. Recognizing complex high-level events requires an explicit model of the structure of the events. Our approach uses attribute grammar for representing such event, which formally specifies the syntax of the symbols and the conditions on the attributes. Events are recognized using an extension of the Earley parser that handles attributes and concurrent event threads. Various examples of recognizing specific events of interest and detecting abnormal events are demonstrated using real data. Unsupervised methods for learning human activities have been largely based on clustering trajectories from a given scene. However, conventional clustering algorithms are not suitable for scenes that have many outlier trajectories. We describe a method for finding only salient groups of trajectories, using the probability of trajectories accidentally forming a group as the measure of significance of the group. The grouping algorithm finds groups that maximizes significance, while automatically determining the threshold for significance. We validate our approach on real data and analyze its performance using simulated data. MULTI-OBJECT TRACKING, EVENT MODELING, AND ACTIVITY DISCOVERY IN VIDEO SEQUENCES	algorithm;attribute grammar;closed-circuit television;cluster analysis;combinatorial optimization;computer engineering;computer vision;correspondence problem;earley parser;experiment;high- and low-level;interaction;mathematical optimization;parallel computing;sensor;unsupervised learning;xslt/muenchian grouping	Seong-Wook Joo	2007			computer vision;simulation;computer science;video tracking;data mining	Vision	38.27062146849857	-46.493292800461305	170498
2f107d18f9ed2618fc0669f320b50ff1a3fe1a01	video saliency detection with robust temporal alignment and local-global spatial contrast	saliency map;saliency detection;sparse;prior knowledge;conference paper;camera motion;low rank decomposition;sparse matrix	Video saliency detection, the task to detect attractive content in a video, has broad applications in multimedia understanding and retrieval. In this paper, we propose a new framework for spatiotemporal saliency detection. To better estimate the salient motion in temporal domain, we take advantage of robust alignment by sparse and low-rank decomposition to jointly estimate the salient foreground motion and the camera motion. Consecutive frames are transformed and aligned, and then decomposed to a low-rank matrix representing the background and a sparse matrix indicating the objects with salient motion. In the spatial domain, we address several problems of local center-surround contrast based model, and demonstrate how to utilize global information and prior knowledge to improve spatial saliency detection. Individual component evaluation demonstrates the effectiveness of our temporal and spatial methods. Final experimental results show that the combination of our spatial and temporal saliency maps achieve the best overall performance compared to several state-of-the-art methods.	map;sparse matrix	Zhixiang Ren;Liang-Tien Chia;Deepu Rajan	2012		10.1145/2324796.2324851	computer vision;sparse matrix;kadir–brady saliency detector;machine learning;pattern recognition	Vision	33.912450399392	-48.39571982344061	170650
d611f86e5992f255f0c382a24a28dd181037ebe7	low-dimensional representation of cardiac motion using barycentric subspaces: a new group-wise paradigm for estimation, analysis, and reconstruction	cardiac motion;image synthesis;low-dimensional analysis;registration	One major challenge when trying to build low-dimensional representation of the cardiac motion is its natural circular pattern during a cycle, therefore making the mean image a poor descriptor of the whole sequence. Therefore, traditional approaches for the analysis of the cardiac deformation use one specific frame of the sequence - the end-diastolic (ED) frame - as a reference to study the whole motion. Consequently, this methodology is biased by this empirical choice. Moreover, the ED image might be a poor reference when looking at large deformation for example at the end-systolic (ES) frame. In this paper, we propose a novel approach to study cardiac motion in 4D image sequences using low-dimensional subspace analysis. Instead of building subspaces relying on a mean value we use a novel type of subspaces called Barycentric Subspaces which are implicitly defined as the weighted Karcher means of k+1 reference images instead of being defined with respect to one reference image. In the first part of this article, we introduce the methodological framework and the algorithms used to manipulate images within these new subspaces: how to compute the projection of a given image on the Barycentric Subspace with its coordinates, and the opposite operation of computing an image from a set of references and coordinates. Then we show how this framework can be applied to cardiac motion problems and lead to significant improvements over the single reference method. Firstly, by computing the low-dimensional representation of two populations we show that the parameters extracted correspond to relevant cardiac motion features leading to an efficient representation and discrimination of both groups. Secondly, in motion estimation, we use the projection on this low-dimensional subspace as an additional prior on the regularization in cardiac motion tracking, efficiently reducing the error of the registration between the ED and ES by almost 30%. We also derive a symmetric and transitive formulation of the registration that can be used both for frame-to-frame and frame-to-reference registration. Finally, we look at the reconstruction of the images using our proposed low-dimensional representation and show that this multi-references method using Barycentric Subspaces performs better than traditional approaches based on a single reference.		Marc-Michel Rohé;Maxime Sermesant;Xavier Pennec	2018	Medical image analysis	10.1016/j.media.2017.12.008	pattern recognition;linear subspace;artificial intelligence;barycentric coordinate system;mathematics;deformation (mechanics);subspace topology;motion estimation;regularization (mathematics);match moving;transitive relation	Vision	36.70218559701319	-49.467001988474834	170927
969a06bc35f3d8539238901e17fc05c66d278317	multiview face retrieval in surveillance video by active training sample collection	support vector machines;face training surveillance face recognition support vector machines robustness;surveillance;training;training sample collection multiview face retrieval face recognition;face recognition;front view face image multiview face retrieval surveillance video active training sample collection;robustness;video surveillance face recognition image retrieval image sampling;face;multiview face retrieval;training sample collection	For multiview face retrieval of certain person in surveillance video, a key challenge is the lack of training samples. Generally, the law enforcement agencies usually have only one frontal view face image of the target person, however, the faces of the target person in the surveillance video could be in different orientation, and it is impossible for a classifier trained on only frontal view face to retrieve the faces under other orientation. This paper proposes an active training sample collection method for multiview face retrieval in surveillance video. First, the front view face image is used to train a classifier to retrieve the target person's front view face in videos. As the video is continuous, we can track the face and obtain side view faces of the target person. Then these selected side view faces are combined with the frontal view face to form a new training data set. The classifier is updated based on the new training data set, and can retrieve multiview faces of the target people. The experimental results prove the effectiveness of the proposed method.	closed-circuit television;multiview video coding;statistical classification;test set	Xiao-Ma Xu;Ming-Tao Pei	2014	2014 Tenth International Conference on Computational Intelligence and Security	10.1109/CIS.2014.13	facial recognition system;face;support vector machine;computer vision;face detection;object-class detection;computer science;machine learning;pattern recognition;three-dimensional face recognition;multimedia;robustness	Vision	34.85696980296666	-51.823519799954425	172089
eb8003fba09048f0106e46ce2ee6483643429681	multi-graph feature level fusion for person re-identification		Person re-identification refers to the task of matching people in non-overlapping cameras. As the concerns for public safety keep rising, the ability to accurately identify a subject in surveillance cameras is a highly demanded technique. In practice, person re-identification is challenging due to the substantial appearance shift caused by view change. Many factors, such as illumination, pose, and image quality, can affect the matching accuracy. In the past, many feature descriptors have been engineered for more robust matching in certain cases. In this paper, we propose a graph-based feature fusion scheme to effectively leverage different feature descriptors. Moreover, instead of determining the matching results by computing pairwise distance between a unknown probe and a gallery subject in the database, we learn the similarity scores between a probe and all the gallery subjects simultaneously in a graph learning framework. We use off-the-shelf features and test our method on popular benchmark datasets for person re-identification. Experimental results show that different feature descriptors can be effectively combined through this graph learning scheme and superior results are achieved as compared with the rival approaches.		Le An;Xiaojing Chen;Songfan Yang	2017	Neurocomputing	10.1016/j.neucom.2016.08.127	feature (computer vision);artificial intelligence;machine learning;fusion;image quality;pairwise comparison;pattern recognition;graph;computer science	Vision	33.35149157434926	-51.23834710016224	172270
142d4b1fd3063d3ca63938ad9c15d8a4ccda629d	spatio-temporal structure of human motion primitives and its application to motion prediction	stochastic modeling;motion primitive;motion prediction	"""This paper proposes a novel approach to structuring behavioral knowledge based on symbolization of human whole body motions, hierarchical classification of the motions, and extraction of the causality among the motions. The motion patterns are encoded into parameters of corresponding Hidden Markov Models (HMMs), where each HMM abstracts the dynamics of motion pattern, and hereafter is referred to as """"motion symbol"""". The motion symbols allow motion recognition and synthesis. The motion symbols are organized into a hierarchical tree structure representing the property of spatial similarity among the motion patterns, and this tree is referred to as """"motion symbol tree"""". Seamless motion is segmented into a sequence of motion primitives, each of which is classified as a motion symbol based on the motion symbol tree. The seamless motion results in a sequence of the motion symbols, which is stochastically represented as transitions between the motion symbols by an N-gram model. The motion symbol N-gram model is referred to as """"motion symbol graph"""". The motion symbol graph extracts the temporal causality among the human behaviors. The integration of the motion symbol tree and the motion symbol graph makes it possible to recognize motion patterns fast and predict human behavior during observation. The experiments on a motion dataset of radio calisthenics and on a large motion dataset provided by CMU motion database validate the proposed framework. This paper proposes a framework for predicting human motions based on motion patterns.The motion patterns encoded into HMMs form two structures of spatial clusters and temporal transitions.The integration of two structures was verified on the radio calisthenics motion dataset and CMU dataset."""	kinesiology	Wataru Takano;Hirotaka Imagawa;Yoshihiko Nakamura	2016	Robotics and Autonomous Systems	10.1016/j.robot.2015.09.017	computer vision;structure from motion;speech recognition;computer science;motion estimation;motion field;stochastic	Robotics	37.90613318347122	-48.77551616541027	172980
cc6e15dda8397a38d50400b0596c8cb98e1525d0	a discriminative model for object representation and detection via sparse features	object representation;quantization;learning model;training;prediction algorithms;feature extraction training image color analysis error analysis prediction algorithms computational modeling;integrating object appearance discriminative model object representation object detection sparse features boosted image patches quantization adaboost algorithm;object detection feature extraction image representation;error analysis;computational modeling;integrating object appearance;image color analysis;image representation;feature extraction;model integration;boosted image patches;sparse features;sparse features discriminative model object detection;discriminative model;object detection;adaboost algorithm	This paper proposes a discriminative model that represents an object category with a batch of boosted image patches, motivated by detecting and localizing objects with sparse features. Instead of designing features carefully and category-specifically as in previous work, we extract a massive number of local image patches from the positive object instances and quantize them as weak classifiers. Then we extend the Adaboost algorithm for learning the patch-based model integrating object appearance and structure information. With the learned model, a few features are activated to localize instances in the testing images. In the experiments, we apply the proposed method with several public datasets and achieve advancing performance.	adaboost;algorithm;discriminative model;experiment;hierarchical database model;instance (computer science);quantization (signal processing);sensor;sparse matrix	Xi Song;Ping Luo;Liang Lin;Yunde Jia	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.754	computer vision;prediction;quantization;feature extraction;computer science;machine learning;pattern recognition;computational model;discriminative model	Vision	33.69577103536812	-47.84932010960306	173510
5969d061b0b464b17ebb6d82e07fecbbe45898d5	recognizing interactions between people from video sequences		This research study proposes a new approach to group activity recognition which is fully automatic. The approach adopted is hierarchical, starting with tracking and modelling local movement leading to the segmentation of moving regions. Interactions between moving regions are modelled using Kullback-Leibler (KL) divergence. Then the statistics of such movement interactions or as relative positions of moving regions is represented using kernel density estimation (KDE). The dynamics of such movement interactions and relative locations is modelled as well in a development of the approach. Eventually, the KDE representations are subsampled and considered as inputs of a support vector machines (SVM) classifier. The proposed approach does not require any intervention by an operator.	2.5d;3d projection;activity recognition;interaction;kernel density estimation;kullback–leibler divergence;linux;radial basis function;stationary process;support vector machine	Kyle Stephens;Adrian G. Bors	2017		10.1007/978-3-319-64689-3_7	artificial intelligence;kullback–leibler divergence;operator (computer programming);divergence;support vector machine;kernel density estimation;computer science;activity recognition;pattern recognition	AI	37.58418913145856	-48.61266389088571	173792
e2c62defa3dfe877d48abfbe50a692ff8d315443	improved $${{\ell }^{1}}$$ ℓ 1 -tracker using robust pca and random projection	ell 1 l1 minimization;robust pca;particle filter;visual tracking;sparse representation;random projection	In this paper, we propose an improved $${{\ell }^{1}}$$ ℓ 1 -tracker in a particle filter framework using robust principal component analysis (robust PCA) and random projection. At first we redesign the template set and its update scheme. Three target templates and several background templates combined with the trivial templates are used to represent the candidate images sparsely. One fixed target template is generated from the image patch in the first frame. The other two are dynamic target templates, called stable target template, and fast changing one used for long time and short time, respectively. Robust PCA is used to generate and update the stable target template, and fast changing target template is initialized by the stable one at certain times. The background templates are used to strengthen the ability of distinguishing background and foreground. Then, the large set of Haar-like features are extracted and compressively sensed with a very sparse measurement matrix for the $${{\ell }^{1}}$$ ℓ 1 -tracker framework. The compressive sensing theories ensure that the sensed features preserve almost all the information of the original features. Our proposed method is more robust than the original $${{\ell }^{1}}$$ ℓ 1 -method. Experiments have been done on numerous sequences to demonstrate the better performance of our improved tracker.	bittorrent tracker;coefficient;compressed sensing;experiment;haar wavelet;mathematical optimization;neural coding;optimization problem;particle filter;random projection;robust principal component analysis;sparse matrix;usb hub	Dongjing Shan;Zhang Chao	2016	Machine Vision and Applications	10.1007/s00138-016-0750-1	speech recognition;particle filter;eye tracking;computer science;machine learning;pattern recognition;sparse approximation;mathematics	Vision	33.508813077744044	-47.08441933207182	173938
c82460c05cc800934b92ce9bba6755bdbcef5cf7	multi-period visual tracking via online deepboost learning	joint local global visual representation;online deepboost learning;visual tracking;multi period tracking framework;tracking by detection	In this paper, we propose a novel accurate and robust boosting-style tracking-by-detection method. The proposed algorithm adopts a flexible and capacity-conscious object appearance model, which combines the strengths of both local and global visual representations. We firstly propose a joint local-global visual representation, in which main local and global spatial structure information of the target is flexibly embedded in the candidate classifier set with members from multiple complexity families. In addition, to avoid over-fitting our tracker adopts an effective online DeepBoost learning method (ODB). The key capacity-conscious ability of ODB helps to avoid over-fitting and generate a more adaptive and robust tracker. Furthermore, we propose a multi-period tracking framework (MPTF) to enhance the tracker's recovery ability for tracking failures. The proposed Multi-period DeepBoost-Tracker (MPDBT) can well encode the object spatial structures and excellently handle object appearance variations, and it can also recover from tracking failures with the help of the proposed MPTF. The experimental results demonstrate that our tracker outperforms the state-of-the-art trackers. HighlightsWe propose an adaptive visual tracking algorithm via online DeepBoost learning.We propose a combining-local-global visual representation.The capacity-conscious ability of online DeepBoost learning can avoid over-fitting.We propose a multi-period tracking framework to enhance the recovery ability.Our method can achieve higher level of accuracy and robustness.The proposed tracker outperforms the state-of-the-art trackers.	video tracking	Yuehuan Wang	2016	Neurocomputing	10.1016/j.neucom.2016.03.016	computer vision;simulation;eye tracking;computer science;machine learning	Vision	33.73623623096221	-47.73686862274332	174324
1a21fe3ecb21adedb39b62e80fcf2ad9a7240dc6	"""a general framework for combining visual trackers – the """"black boxes"""" approach"""	distributed estimation;tracker combination;perforation;probability density function;kalman filter;condensation;state space;visual tracking	Over the past few years researchers have been investigating the enhancement of visual tracking performance by devising trackers that simultaneously make use of several different features. In this paper we investigate the combination of synchronous visual trackers that use different features while treating the trackers as “black boxes”. That is, instead of fusing the usage of the different types of data as has been performed in previous work, the combination here is allowed to use only the trackers' output estimates, which may be modified before their propagation to the next time step. We propose a probabilistic framework for combining multiple synchronous trackers, where each separate tracker outputs a probability density function of the tracked state, sequentially for each image. The trackers may output either an explicit probability density function, or a sample-set of it via Condensation. Unlike previous tracker combinations, the proposed framework is fairly general and allows the combination of any set of trackers of this kind, even in different state-spaces of different dimensionality, under a few reasonable assumptions. The combination may consist of different trackers that track a common object, as well as trackers that track separate, albeit related objects, thus improving the tracking performance of each object. The benefits of merely using the final estimates of the separate trackers in the combination are twofold. Firstly, the framework for the combination is fairly general and may be easily used from the software aspects. Secondly, the combination may be performed in a distributed setting, where each separate tracker runs on a different site and uses different data, while avoiding the need to share the data. The suggested framework was successfully tested using various state-spaces and datasets, demonstrating that fusing the trackers' final distribution estimates may indeed be applicable.	black box;software propagation;video tracking	Ido Leichter;Michael Lindenbaum;Ehud Rivlin	2006	International Journal of Computer Vision	10.1007/s11263-006-5568-2	kalman filter;computer vision;econometrics;probability density function;simulation;eye tracking;computer science;state space;mathematics;statistics;condensation	Vision	33.53727639126279	-46.1612273317708	174630
af14b9533f12dd1d506b5608f16c6996d2ea9501	robust online trajectory clustering without computing trajectory distances	pattern clustering;object trackers robust online trajectory clustering online pedestrian trajectory processing online vehicle trajectory processing vision based tracker real world video footage smooth vector fields bounded connected set cluster distance pairwise distances vector sets trajectory set edinburgh informatics forum pedestrian dataset public transport junction;video signal processing;set theory;vectors;video signal processing object tracking pattern clustering set theory traffic engineering computing vectors;object tracking;trajectory clustering algorithms vectors noise measurement computer vision image sequences pattern recognition;traffic engineering computing	We propose a novel trajectory clustering algorithm which is suitable for online processing of pedestrian or vehicle trajectories computed with a vision-based tracker. Our approach does not require defining distances between trajectories, and can thus process broken trajectories which are inevitable in most cases when object trackers are applied to real world video footage. Clusters are defined as smooth vector fields on a bounded connected set, and cluster distance is based on pairwise distances between vector sets. The results are illustrated on a trajectory set from the Edinburgh Informatics Forum Pedestrian Dataset, on a trajectory set from a public transport junction, and trajectories from an experimental setup in a corridor.	algorithm;cluster analysis;informatics	Michael Ulm;Norbert Brändle	2012	Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)		computer vision;simulation;computer science;machine learning;video tracking;set theory	Robotics	38.57010688700235	-46.31403598196986	175065
0055c7f32fa6d4b1ad586d5211a7afb030ca08cc	deep learning for detecting multiple space-time action tubes in videos		In this work, we propose an approach to the spatiotemporal lo c lisation (detection) and classification of multiple concurrent actions within te mporally untrimmed videos. Our framework is composed of three stages. In stage 1, appear ance and motion detection networks are employed to localise and score actions fro m c lour images and optical flow. In stage 2, the appearance network detections are boost ed by combining them with the motion detection scores, in proportion to their respect iv spatial overlap. In stage 3, sequences of detection boxes most likely to be associated wi th a single action instance, called action tubes, are constructed by solving two energy m axi isation problems via dynamic programming. While in the first pass, action paths sp anning the whole video are built by linking detection boxes over time using their cl ass-specific scores and their spatial overlap, in the second pass, temporal trimming is pe rformed by ensuring label consistency for all constituting detection boxes. We demon strate the performance of our algorithm on the challenging UCF101, J-HMDB-21 and LIRIS-H ARL datasets, achieving new state-of-the-art results across the board and signi fica tly increasing detection speed at test time.	algorithm;automotive navigation system;benchmark (computing);coherence (physics);deep learning;dynamic programming;expectation propagation;helios;mathematical optimization;optical flow;real-time computing;real-time transcription;sensor	Suman Saha;Gurkirt Singh;Michael Sapienza;Philip H. S. Torr;Fabio Cuzzolin	2016	CoRR		computer vision;simulation;machine learning;pattern recognition;mathematics	Vision	39.12099210565605	-48.35520639906627	175224
c84991fe3bf0635e326a05e34b11ccaf74d233dc	a parameter-free label propagation algorithm for person identification in stereo videos	label propagation;stereo videos;semi supervised;parameter free	Motivated by relaxing expensive and laborious person identity annotation in stereo videos, a number of research efforts have recently been dedicated to label propagation. In this work, we propose two heuristic label propagation algorithms for annotating person identities in stereo videos under the observation that the actors in two consecutive facial images in a video are more likely to be identical. In the light of this, after adjacent video frames divided into several groups, we propose our first algorithm (i.e. ZBLC4) to automatically annotate the unlabeled images with the one having the maximum summed similarity between unlabeled and labeled images in each group in the parameter-free manner. Moreover, to cope with singleton groups, an additional classifier is introduced into ZBLC4 algorithm to mitigate the suffering of unreliable prediction dependent on neighbours. We conduct experiments on three publicly-benchmarking stereo videos, demonstrating that our algorithms are superior to the state-of-the-arts.	experiment;heuristic;label propagation algorithm;software propagation	Chongsheng Zhang;Jingjun Bi;Changchang Liu;Ke Chen	2016	Neurocomputing	10.1016/j.neucom.2016.08.069	computer vision;simulation;computer science;machine learning;multimedia	Vision	32.19781361672448	-47.75781091029379	175228
7c6e61a4c0bb33c8465a7fa09bfd22e47207079e	realistic synthesis of novel human movements from a database of motion capture examples	realism;image sampling;image sampling image motion analysis biomechanics video databases image sequences statistics realistic images computer animation learning artificial intelligence;databases;video databases;human movement;image motion analysis;speech synthesis;learning;speech processing;information technology;biomechanics;motion capture data;data engineering;statistical model;realism novel human movement synthesis motion capture examples database novel motion sequences learning statistical model sampling start keyframe end keyframe keyframe animation;sampling;motion capture;start keyframe;statistical learning;hidden markov models;databases hidden markov models animation signal synthesis speech synthesis humans speech processing signal processing data engineering information technology;signal processing;animation;statistics;realistic images;humans;signal synthesis;novel human movement synthesis;novel motion sequences;learning artificial intelligence;computer animation;motion capture examples database;keyframe animation;image sequences;end keyframe;markov chain	In this paper we present a system that can synthesise novel motion sequences from a database of motion capture examples. This is achieved through learning a statistical model from the captured data which enables realistic synthesis of new movements by sampling the original captured sequences. New movements are synthesised by specifying the start and end keyframes. The statistical model identifie s segments of the original motion capture data to generate novel motion sequences between the keyframes. The advantage of this approach is that it combines the flexibility of keyframe animation with the realism of motion capture	key frame;motion capture;sampling (signal processing);statistical model	L. M. Tanco;Adrian Hilton	2000		10.1109/HUMO.2000.897383	computer vision;facial motion capture;speech recognition;computer science;motion estimation;computer graphics (images)	Graphics	38.64895576722279	-49.5262635023371	175598
ae455715feeed39522bc8d59a7869c07826c14cb	siamese neural networks for one-shot detection of railway track switches		Deep Learning methods have been extensively used to analyze video data to extract valuable information by classifying image frames and detecting objects. We describe a unique approach for using video feed from a moving Locomotive to continuously monitor the Railway Track and detect significant assets like Switches on the Track. The technique used here is called Siamese Networks – which uses 2 identical networks to learn the similarity between of 2 images. Here we will use a Siamese network to continuously compare Track images and detect any significant difference in the Track. Switch will be one of those images that will be different and we will find a mapping that clearly distinguishes the Switch from other possible Track anomalies. The same method will then be extended to detect any abnormalities on the Railway Track. Railway Transportation is unique in the sense that is has wheeled vehicles – Trains pulled by Locomotives running on guided Rails at very high speeds nearing 200 mph. Multiple Tracks on the Rail network are connected to each other using an equipment called Switch or a Turnout. Switch is either operated manually or automatically through command from a Control center and it governs the movement of Trains on different Tracks of the network. Accurate location of these Switches is very important for the railroad and getting a true picture of their state in field is important. Modern trains use high definition video cameras facing the Track that continuously record video from track. Using a Siamese network and comparing to benchmark images – we describe a method to monitor the Track and highlight anomalies.	benchmark (computing);deep learning;network switch;neural networks;sensor;shot transition detection;video	Dattaraj J. Rao;Shruti Mittal;S. Ritika	2017	CoRR		machine learning;artificial neural network;computer science;high-definition video;deep learning;track (rail transport);computer vision;train;artificial intelligence	Vision	39.094641415041814	-46.32508387722265	175834
987d59ea8c4221a03d58a1596110a3a18654f5d0	on the semantics of visual behaviour, structured events and trajectories of human action	event recognition;bayesian belief nets;gaussian mixture;temporal dynamics;human behaviour;autonomous visual events;segmentation;gaussian mixture model;pixel energy history;condensation;bayesian belief network;dynamic scene models;discontinuous motion trajectories;semantics of visual behaviour;adaptive gaussian mixture models;dynamic scenes	The problem of modelling the semantics of visual events without segmentation or computation of object-centred trajectories is addressed. Two examples are presented. The first illustrates the detection of autonomous visual events without segmentation. The second shows how high-level semantics can be extracted without spatio-temporal tracking or modelling of object trajectories. We wish to infer the semantics of human behavioural patterns for autonomous visual event recognition in dynamic scenes. This is achieved by learning to model the temporal structures of pixel-wise change energy histories using CONDENSATION. The performance of a pixel-energy-history based event model is compared to that of an adaptive Gaussian mixture based scene model. Given low-level autonomous visual events, grouping and high-level reasoning are required to both infer associations between these events and give meaning to their associations. We present an approach for modelling the semantics of interactive human behaviours for the association of a moving head and two hands under self-occlusion and intersection from a single camera view. For associating and tracking the movements of multiple intersecting body parts, we compare the effectiveness of spatio-temporal dynamics based prediction to that of reasoning about body-part associations based on modelling semantics using Bayesian belief networks. q 2002 Elsevier Science B.V. All rights reserved.	autonomous robot;autostereogram;bayesian network;behavioral pattern;computation;dynamical system;event (computing);experiment;high- and low-level;mixture model;pixel;reflections of signals on conducting lines;region growing;temporal logic;video tracking	Shaogang Gong;Jeffrey Ng Sing Kwong;Jamie Sherrah	2002	Image Vision Comput.	10.1016/S0262-8856(02)00096-3	computer vision;computer science;machine learning;bayesian network;mixture model;human behavior;segmentation;condensation	AI	38.17705529618975	-48.135980495646535	176013
6009bba115904bc3bf876224db90b232c4f0a48f	potion: pose motion representation for action recognition		Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition. We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator [4] and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by 'colorizing' each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations [6, 48]. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets.		Vasileios Choutas;Philippe Weinzaepfel;Jérôme Revaud;Cordelia Schmid	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00734	streams;optical imaging;estimator;computer vision;convolutional neural network;architecture;pattern recognition;artificial intelligence;computer science	Vision	32.71428245510309	-49.81168676208117	177062
66f87c98a95f7816d2d93b06e0d0828e20a1de88	on the design of robust classifiers for computer vision	tangent loss;scene classification;robustness computer vision boosting layout object detection image classification support vector machines support vector machine classification application software humans;probability elicitation;multiple instance learning;bayes methods;training;object tracking design robust classifiers computer vision probability elicitation robust bayes consistent loss tangent loss associated boosting algorithm tangentboost scene classification;risk management;associated boosting algorithm;computer vision;boosting;loss function;object tracking;pattern classification;robust classifiers;tracking bayes methods computer vision design object detection pattern classification;design;robustness;robust bayes consistent loss;algorithm design and analysis;tangentboost;tracking;object detection;noise	The design of robust classifiers, which can contend with the noisy and outlier ridden datasets typical of computer vision, is studied. It is argued that such robustness requires loss functions that penalize both large positive and negative margins. The probability elicitation view of classifier design is adopted, and a set of necessary conditions for the design of such losses is identified. These conditions are used to derive a novel robust Bayes-consistent loss, denoted Tangent loss, and an associated boosting algorithm, denoted TangentBoost. Experiments with data from the computer vision problems of scene classification, object tracking, and multiple instance learning show that TangentBoost consistently outperforms previous boosting algorithms.	algorithm;boosting (machine learning);computer vision;loss function;multiple instance learning;robustness (computer science)	Hamed Masnadi-Shirazi;Vijay Mahadevan;Nuno Vasconcelos	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540136	computer vision;design;risk management;computer science;noise;machine learning;pattern recognition;tracking;boosting;robustness;loss function	Vision	34.364943539392485	-45.220536232490765	177164
06dd99e77019f91e5116b9c14f5c52fb2136e0b3	tracking based multi-orientation scene text detection: a unified framework with dynamic programming	dynamic programming;scene text detection tracking based text detection multi orientation scene text dynamic programming;distortion;feature extraction;robustness;text recognition;videos feature extraction tracking text recognition robustness dynamic programming distortion;tracking;videos	There are a variety of grand challenges for multi-orientation text detection in scene videos, where the typical issues include skew distortion, low contrast, and arbitrary motion. Most conventional video text detection methods using individual frames have limited performance. In this paper, we propose a novel tracking based multi-orientation scene text detection method using multiple frames within a unified framework via dynamic programming. First, a multi-information fusion-based multi-orientation text detection method in each frame is proposed to extensively locate possible character candidates and extract text regions with multiple channels and scales. Second, an optimal tracking trajectory is learned and linked globally over consecutive frames by dynamic programming to finally refine the detection results with all detection, recognition, and prediction information. Moreover, the effectiveness of our proposed system is evaluated with the state-of-the-art performances on several public data sets of multi-orientation scene text images and videos, including MSRA-TD500, USTB-SV1K, and ICDAR 2015 Scene Videos.	algorithmic efficiency;computation;distortion;dynamic programming;elegant degradation;experiment;fifty nine;frame (physical object);gaussian blur;grand challenges;graph - visual representation;information privacy;international conference on document analysis and recognition;msra gene;matching (graph theory);numerous;obstruction;optical character recognition;performance;speaking (activity);total correlation;tree accumulation;unified framework;videocassette	Chun Yang;Xu-Cheng Yin;Wei-Yi Pei;Shu Tian;Ze-Yu Zuo;Chao Zhu;Junchi Yan	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2695104	artificial intelligence;robustness (computer science);computer vision;skew;dynamic programming;distortion;feature extraction;data set;pattern recognition;computer science;grand challenges;communication channel	Vision	37.1746193839416	-51.05188701504039	177322
c25b05e7e3cf22b121a0f02c1a88b1eb2d3fabfe	motion binary patterns for action recognition	random forest;human action recogniton	In this paper, we propose a novel feature type to recognize human actions from video data. By combining the benefit of Volume Local Binary Patterns and Optical Flow, a simple and efficient descriptor is constructed. Motion Binary Patterns (MBP) are computed in spatio-temporal domain while static object appearances as well as motion information are gathered. Histograms are used to learn a Random Forest classifier which is applied to the task of human action recognition. The proposed framework is evaluated on the well-known, publicly available KTH dataset, Weizman dataset and on the IXMAS dataset for multi-view action recognition. The results demonstrate state-of-the-art accuracies in comparison to other methods.	computation;data rate units;encode;granular computing;hollywood;local interconnect network;local binary patterns;million book project;optical flow;random forest;whole earth 'lectronic link;word lists by frequency	Florian Baumann;Jie Lao;Anna M Ehlers;Bodo Rosenhahn	2014		10.5220/0004816903850392	random forest;computer vision;computer science;machine learning;pattern recognition;data mining	Vision	35.63133276028165	-50.318163760856216	178974
02f51c928685585865fa984fe90de1ec0796e597	towards automatic identification of elephants in the wild		Identifying animals from a large group of possible individuals is very important for biodiversity monitoring and especially for collecting data on a small number of particularly interesting individuals, as these have to be identified first before this can be done. Identifying them can be a very timeconsuming task. This is especially true, if the animals look very similar and have only a small number of distinctive features, like elephants do. In most cases the animals stay at one place only for a short period of time during which the animal needs to be identified for knowing whether it is important to collect new data on it. For this reason, a system supporting the researchers in identifying elephants to speed up this process would be of great benefit. In this paper, we present such a system for identifying elephants in the face of a large number of individuals with only few training images per individual. For that purpose, we combine object part localization, off-the-shelf CNN features, and support vector machine classification to provide field researches with proposals of possible individuals given new images of an elephant. The performance of our system is demonstrated on a dataset comprising a total of 2078 images of 276 individual elephants, where we achieve 56% top-1 test accuracy and 80% top-10 accuracy. To deal with occlusion, varying viewpoints, and different poses present in the dataset, we furthermore enable the analysts to provide the system with multiple images of the same elephant to be identified and aggregate confidence values generated by the classifier. With that, our system achieves a top1 accuracy of 74% and a top-10 accuracy of 88% on the held-out test dataset.		Matthias Korschens;Bjorn Barz;Joachim Denzler	2018	CoRR			ML	32.30112322313568	-51.702624944196636	179113
5c57539e2ad96906dcd22aae405f64ef6f44df61	behavior modeling and recognition based on space-time image features	body part tracking behavior modeling behavior recognition space time image features video sequences image segmentation dynamic time warping behavior templates;image features;image segmentation;video signal processing;behavior modeling;image recognition hidden markov models biological system modeling feature extraction shape pattern recognition humans clustering methods image segmentation labeling;behavior recognition;video sequences;prior knowledge;space time;video signal processing behavioural sciences feature extraction image segmentation;behavior templates;feature extraction;body part tracking;space time image features;behavioural sciences;dynamic time warping	A novel method based on space-time image features is proposed for automatic behavior modeling and recognition. The method is composed of the following three steps: (1) video sequences are converted into a space-time image, and the image is divided into equal length segments; (2) from these segments, an unsupervised technique based on Dynamic Time Warping is used to determine the groups of different behaviors; (3) the behavior templates are built according to these groups, and are used for the recognition of behaviors. The method does not need to track the body parts and can model the behaviors without any prior knowledge. Experiments also demonstrate the effectiveness of our new method	behavior model;dynamic time warping;experiment;sparse matrix;unsupervised learning	Heping Li;Zhanyi Hu;Yihong Wu;Fuchao Wu	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.319	behavioral modeling;computer vision;feature extraction;behavioural sciences;computer science;machine learning;dynamic time warping;space time;pattern recognition;image segmentation;feature	Vision	38.238131766486106	-48.89565281429268	180092
31f1855711cf6eec28983159812cfae8425fd1e5	the measurement of cell viability based on temporal bag of words for image sequences	image sequences biology computing;feature extraction shape image motion analysis computer vision streaming media context image sequences;sift flow cell viability cell deformation temporal bag of words shape context;protoplasm streaming cell viability measurement temporal bag of words image sequences contour deformation cytoplasm streaming	The measurement of cell viability is an important and challenging topic in the cellular property researches. In this paper, we have presented a novel framework to measure the cell viability from both contour deformation and cytoplasm streaming for image sequences. The framework is distinguished by two aspects: First, we construct the appearance change field on the basis of the displacement field in order to more completely evaluate the streaming of protoplasm. Second, Temporal Bag of Words (TBoW) is introduced to capture the variation of cell viability across the temporal dimension. Experiments show that the proposed method is effective to quantize the cell viability.	bag-of-words model;displacement mapping;experiment;quantization (signal processing)	Fengqian Pang;Zhiwen Liu;Heng Li;Yonggang Shi	2015	2015 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2015.7351594	computer vision;computer science;pattern recognition	Robotics	37.55176236127964	-52.007611051335786	180467
722568894e136dcedf9f686afdb1d6b9727fd3c8	robust feature-based automated multi-view human action recognition system		Automated human action recognition has the potential to play an important role in public security, for example, in relation to the multiview surveillance videos taken in public places, such as train stations or airports. This paper compares three practical, reliable, and generic systems for multiview video-based human action recognition, namely, the nearest neighbor classifier, Gaussian mixture model classifier, and the nearest mean classifier. To describe the different actions performed in different views, view-invariant features are proposed to address multiview action recognition. These features are obtained by extracting the holistic features from different temporal scales which are modeled as points of interest which represent the global spatial-temporal distribution. Experiments and cross-data testing are conducted on the KTH, WEIZMANN, and MuHAVi datasets. The system does not need to be retrained when scenarios are changed which means the trained database can be applied in a wide variety of environments, such as view angle or background changes. The experiment results show that the proposed approach outperforms the existing methods on the KTH and WEIZMANN datasets.	database;experiment;holism;mixture model;nearest neighbour algorithm;point of interest	K. Y. Chou;Mukesh Prasad;Di Wu;Nabin Sharma;Dong-Lin Li;Yu-Feng Lin;Michael Blumenstein;Wen-Chieh Lin;Chin-Teng Lin	2018	IEEE Access	10.1109/ACCESS.2018.2809552	point of interest;mixture model;distributed computing;robustness (computer science);feature extraction;computer science;k-nearest neighbors algorithm;background subtraction;artificial intelligence;pattern recognition	Vision	35.30249564153718	-51.09859826431286	181108
99df9e95b3f3d187636918a65277baf7c98cbfc5	an efficient subsequence search for video anomaly detection and localization		This paper presents a novel framework for anomaly event detection and localization in crowded scenes. For anomaly detection, one-class support vector machine with Bayesian derivation is applied to detect unusual events. We also propose a novel event representation, called subsequence, which refers to a time series of spatial windows in proximity. Unlike recent works encoded an event with a 3D bounding box which may contain irrelevant information, e.g. background, a subsequence can concisely capture the unstructured property of an event. To efficiently locate anomalous subsequences in a video space, we propose the maximum subsequence search. The proposed search algorithm integrates local anomaly scores into a global consistent detection so that the start and end of an abnormal event can be determined under false and missing detections. Experimental results on two public datasets show that our method is robust to the illumination change and achieve at least 80% localization rate which approximately doubles the accuracy of recent works. This study concludes that anomaly localization is crucial in finding abnormal events.	anomaly detection;bayesian approaches to brain function;microsoft windows;minimum bounding box;object detection;relevance;search algorithm;sensor;sparse;support vector machine;time complexity;time series	Kai-Wen Cheng;Yie-Tarng Chen;Wen-Hsien Fang	2015	Multimedia Tools and Applications	10.1007/s11042-015-2453-4	machine learning;pattern recognition;data mining	AI	37.13883859607617	-46.6161859270918	181970
f49ef25ec6f5920f096ec733fbb903383b66a81e	effective codebooks for human action representation and classification in unconstrained videos	pattern clustering;codebook size reduction human action representation human action classification unconstrained video sequences action categorization image gradient optic flow radius based clustering deep belief networks;video signal processing;image classification;videos humans shape feature extraction video sequences detectors visualization;visual codebooks human action categorization spatio temporal local descriptors;image representation;video signal processing gesture recognition image classification image representation image sequences pattern clustering;gesture recognition;image sequences	Recognition and classification of human actions for annotation of unconstrained video sequences has proven to be challenging because of the variations in the environment, appearance of actors, modalities in which the same action is performed by different persons, speed and duration, and points of view from which the event is observed. This variability reflects in the difficulty of defining effective descriptors and deriving appropriate and effective codebooks for action categorization. In this paper, we propose a novel and effective solution to classify human actions in unconstrained videos. It improves on previous contributions through the definition of a novel local descriptor that uses image gradient and optic flow to respectively model the appearance and motion of human actions at interest point regions. In the formation of the codebook, we employ radius-based clustering with soft assignment in order to create a rich vocabulary that may account for the high variability of human actions. We show that our solution scores very good performance with no need of parameter tuning. We also show that a strong reduction of computation time can be obtained by applying codebook size reduction with Deep Belief Networks with little loss of accuracy.	bayesian network;categorization;cluster analysis;code word;codebook;computation;effective descriptive set theory;image gradient;optical flow;performance tuning;spatial variability;time complexity;vocabulary	Lamberto Ballan;Marco Bertini;Alberto Del Bimbo;Lorenzo Seidenari;Giuseppe Serra	2012	IEEE Transactions on Multimedia	10.1109/TMM.2012.2191268	computer vision;contextual image classification;computer science;machine learning;pattern recognition;gesture recognition;mathematics	Vision	36.818814813439765	-50.766957447057464	182178
415cec785cb08e2543eb537ac9b3d1a39e002e4d	inter-person occlusion handling with social interaction for online multi-pedestrian tracking		Inter-person occlusion handling is a critical issue in the field of tracking, and it has been extensively researched. Several state-ofthe-art methods have been proposed, such as focusing on the appearance of the targets or utilizing knowledge of the scene. In contrast with the approaches proposed in the literature, we propose to address this issue using a social interaction model, which allows us to explore spatio-temporal information pertaining to the targets involved in the occlusion situation. Our experimental results show promising results compared with those obtained using other methods. key words: image processing, computer vision, inter-person occlusion, online tracking	computer vision;image processing	Yuke Li;Weiming Shen	2016	IEICE Transactions		computer vision;image processing;computer science;multimedia	Vision	33.979067222499204	-50.376091410879056	182972
6b64bd1893c88aefaf80718b46912aded56abc89	bilevel visual words coding for image classification	coding stage;following coding stage;efficient coding;low discriminative power;certain localized coding rule;bilevel visual word;sparse coding;bilevel codebook generation stage;bilevel visual word;bilevel codebook;image classification	Bag-of-Words approach has played an important role in recent works for image classification. In consideration of efficiency, most methods use kmeans clustering to generate the codebook. The obtained codebooks often lose the cluster size and shape information with distortion errors and low discriminative power. Though some efforts have been made to optimize codebook in sparse coding, they usually incur higher computational cost. Moreover, they ignore the correlations between codes in the following coding stage, that leads to low discriminative power of the final representation. In this paper, we propose a bilevel visual words coding approach in consideration of representation ability, discriminative power and efficiency. In the bilevel codebook generation stage, k-means and an efficient spectral clustering are respectively run in each level by taking both class information and the shapes of each visual word cluster into account. To obtain discriminative representation in the coding stage, we design a certain localized coding rule with bilevel codebook to select local bases. To further achieve an efficient coding referring to this rule, an online method is proposed to efficiently learn a projection of local descriptor to the visual words in the codebook. After projection, coding can be efficiently completed by a low dimensional localized soft-assignment. Experimental results show that our proposed bilevel visual words coding approach outperforms the stateof-the-art approaches for image classification.	algorithmic efficiency;bag-of-words model in computer vision;cluster analysis;code;codebook;distortion;experiment;internationalization and localization;k-means clustering;neural coding;sparse matrix;spectral clustering;visual word	Jiemi Zhang;Chenxia Wu;Deng Cai;Jianke Zhu	2013			computer vision;shannon–fano coding;machine learning;pattern recognition;mathematics	AI	32.634700692840696	-49.44910334667584	183429
e65de721e68f498b6e1c63373c6df60a12533cc0	spatio-temporal laban features for dance style recognition		This work targets Dance Style Recognition in videos as an application of Human Action Recognition. We propose a novel Spatio-Temporal Laban Feature descriptor (STLF) for dance style recognition based on Laban theory. Laban Movement Analysis has become increasingly popular as a language to describe, index and record human motion. We only exploit motion features and body-pose information without encoding the appearance. The model is tested on some action recognition benchmarks and ICD, a challenging dataset of YouTube dance videos. Unlike other works, where Laban based features have been used in constrained environments, with static camera, sensors and no background noise, we employ STLF on videos in unconstrained and natural settings. It is robust to camera jitter, zoom variations and other acquisition conditions and is computationally cheap. It performs comparable or better than the state-of-the-art.	epilepsy, temporal lobe;human–computer interaction;kinesiology;silo (dataset);sensor (device);videocassette	Swati Dewan;Shubham Agarwal;Navjyoti Singh	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545251	computer vision;feature extraction;artificial intelligence;noise measurement;jitter;exploit;laban movement analysis;zoom;dance;background noise;computer science	Vision	37.826014584238656	-49.94440016751335	183917
0c93c7ec3b2e40fae06ba56c3597d9272ed65169	dynamic phase and group detection in pedestrian crowd data using multiplex visibility graphs		We study pedestrian crowd dynamics and the detection of groups in a scene. We propose a novel method to analyse pedestrian trajectories by translating them to multiplex networks, whose properties can be studied using the tools of graph theory. Our results show that simple measures on the resulting multiplex graphs accurately reflect both the global dynamics and local clustering within scenes.	closed-circuit television;cluster analysis;clustering coefficient;conditional mutual information;data (computing);degree distribution;dynamical system;graph theory;interaction;marginal model;multiplexing;numerical analysis;source data;sparse matrix;visibility graph;window function	Colin Stephen	2015		10.1016/j.procs.2015.07.318	computer vision;simulation;computer security	ML	37.68876445105767	-45.70975729769072	184652
16a61b9b213cf5d9020a3cfdf3c1575b1fac8c07	dense invariant feature based support vector ranking for person re-identification	g400 computer science;support vector machines image fusion image sensors object recognition pose estimation;feature fusion person re identification dense invariant feature support vector ranking;support vector machines;g900 others in mathematical and computing sciences;probes;bidirectional features dense invariant feature based support vector ranking person reidentification camera views pose variation viewpoint variation optimal ranking space;feature extraction;information processing;cameras feature extraction support vector machines linear programming probes conferences information processing;linear programming;cameras;conferences	Recently, support vector ranking has been adopted to address the challenging person re-identification problem. However, the ranking model based on ordinary global features cannot represent the significant variation of pose and viewpoint across camera views. Thus, a novel ranking method which fuses the dense invariant features is proposed in this paper to model the variation of images across camera views. By maximizing the margin and minimizing the error score for the fused features, an optimal space for ranking has been learned. Due to the invariance of the dense invariant features and the fusion of the bidirectional features, the proposed method significantly outperforms the original support vector ranking algorithm and is competitive with state-of-the-art techniques on two challenging datasets, showing its potential for real-world person re-identification.	algorithm;data integrity field;pose (computer vision);support vector machine	Shoubiao Tan;Feng Zheng;Ling Shao	2015	2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2015.7418284	computer vision;machine learning;pattern recognition;mathematics;ranking svm	Vision	33.67521141097503	-51.42557270461356	185947
a9f652787e5669168c7b8f632c3a343dfbaa6f4b	mining spatial and spatio-temporal rois for action recognition		Author(s): Lian, Xiaochen | Advisor(s): Yuille, Alan Loddon | Abstract: In this paper, we propose an approach to classify action sequences. We observe that in action sequences the critical features for discriminating between actions occur only within sub-regions of the image. Hence deep network approaches will address the entire image are at a disadvantage. This motivates our strategy which uses static and spatio-temporal visual cues to isolate static and spatio-temporal regions of interest (ROIs). We then use weakly supervised learning to train deep network classifiers using the ROIs as input. More specifically, we combine multiple instance learning (MIL) with convolutional neural networks (CNNs) to select discriminative action cues. This yields classifiers for static images, using the static ROIs, as well as classifiers for short image sequences (16 frames), using spatio-temporal ROIs. Extensive experiments performed on the UCF101 and HMDB51 benchmarks show that both these types of classifiers perform well individually and achieve state of the art performance when combined together. We also show qualitatively that our ROIs (selected by the algorithms) capture the most relevant parts of the image sequences.		Xiaochen Lian	2016			discriminative model;supervised learning;sensory cue;convolutional neural network;machine learning;artificial intelligence;pattern recognition;computer science	Vision	32.01242696612563	-50.123448965349084	186040
8cffe360a05085d4bcba111a3a3cd113d96c0369	learning universal multi-view age estimator using video context	video signal processing;face feature extraction estimation context face detection databases vectors;tracking sequence universal multiview age estimator learning video context face image nonfrontal face pose variation handling unlabeled web video human involved video corpus multiview face detection multiview face tracking offrontal vs profileface bundle;face recognition;internet;video signal processing face recognition image sequences internet learning artificial intelligence pose estimation;feature extraction;age estimation;ground truth;learning artificial intelligence;face detection;image sequences;pose estimation	Many existing techniques for analyzing face images assume that the faces are at nearly frontal. Generalizing to non-frontal faces is often difficult, due to a dearth of ground truth for non-frontal faces and also to the inherent challenges in handling pose variations. In this work, we investigate how to learn a universal multi-view age estimator by harnessing 1) unlabeled web videos, 2) a publicly available labeled frontal face corpus, and 3) zero or more non-frontal faces with age labels. First, a large diverse human-involved video corpus is collected from online video sharing website. Then, multi-view face detection and tracking are performed to build a large set of frontal-vs-profile face bundles, each of which is from the same tracking sequence, and thus exhibiting the same age. These unlabeled face bundles constitute the so-called video context, and the parametric multi-view age estimator is trained by 1) enforcing the face-to-age relation for the partially labeled faces, 2) imposing the consistency of the predicted ages for the non-frontal and frontal faces within each face bundle, and 3) mutually constraining the multi-view age models with the spatial correspondence priors derived from the face bundles. Our multi-view age estimator performs well on a realistic evaluation dataset that contains faces under varying poses, and whose ground truth age was manually annotated.	face detection;ground truth;supervised learning;video clip	Zheng Song;Bingbing Ni;Dong Guo;Terence Sim;Shuicheng Yan	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126248	facial recognition system;computer vision;face detection;the internet;object-class detection;pose;ground truth;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition	Vision	32.69650988877946	-48.06242739542114	186136
e4d53e7f4c2052940841abc08f9574655f3f7fb4	taichi: a fine-grained action recognition dataset		In this paper, we introduce TaiChi which is a fine-grained action dataset. It consists of unconstrained user-uploaded web videos containing camera motion and partial occlusions which pose new challenges to fine-grained action recognition compared to the existing datasets. In this dataset, 2,772 samples of 58 fine-grained action classes are manually annotated. Additionally, we provide the baseline action recognition results using the state-of-the-art Improved Dense Trajectory feature and Fisher Vector representation with an MAP (Mean Average Precision) of 51.39%.	baseline (configuration management);benchmark (computing);chi;information retrieval;sensor;video clip	Shan Sun;Feng Wang;Qi Liang;Zhe Yang	2017		10.1145/3078971.3079039	computer science;pattern recognition;computer vision;artificial intelligence	Vision	32.88950638298792	-49.92456633536183	186334
a6b2c5c527557cc86ae2ce4332b18a7850ee4e1e	exploring the spatial hierarchy of mixture models for human pose estimation	compact mixture representation;recent hierarchical model;spatial hierarchy;latent node;local deformable model;global mixture representation;hierarchical structure;well-constrained spatial model;high-order spatial relationship;new hierarchical spatial model;mixture model;pure spatial	Human pose estimation requires a versatile yet well-constrained spatial model for grouping locally ambiguous parts together to produce a globally consistent hypothesis. Previous works either use local deformable models deviating from a certain template, or use a global mixture representation in the pose space. In this paper, we propose a new hierarchical spatial model that can capture an exponential number of poses with a compact mixture representation on each part. Using latent nodes, it can represent high-order spatial relationship among parts with exact inference. Different from recent hierarchical models that associate each latent node to a mixture of appearance templates (like HoG), we use the hierarchical structure as a pure spatial prior avoiding the large and often confounding appearance space. We verify the effectiveness of this model in three ways. First, samples representing human-like poses can be drawn from our model, showing its ability to capture high-order dependencies of parts. Second, our model achieves accurate reconstruction of unseen poses compared to a nearest neighbor pose representation. Finally, our model achieves state-of-art performance on three challenging datasets, and substantially outperforms recent hierarchical models.	3d pose estimation;bayesian network;mixture model;time complexity	Yuandong Tian;C. Lawrence Zitnick;Srinivasa G. Narasimhan	2012		10.1007/978-3-642-33715-4_19	computer vision;machine learning;pattern recognition;mathematics	Vision	34.93842315091551	-45.60749726784024	187798
62e0380a86e92709fe2c64e6a71ed94d152c6643	facial emotion recognition with expression energy	video analysis;emotion recognition;computer vision;image representation	Facial emotion recognition, the inference of an emotion from apparent facial expressions, in unconstrained settings is a typical case where algorithms perform poorly. A property of the AVEC2012 data set is that individuals in testing data are not encountered in training data. In these situations, conventional approaches suffer because models developed from training data cannot properly discriminate unforeseen testing samples. Additional information beyond the feature vectors is required for successful detection of emotions. We propose two similarity metrics that address the problems of a conventional approach: neutral similarity, measuring the intensity of an expression; and temporal similarity, measuring changes in an expression over time. These similarities are taken to be the energy of facial expressions, measured with a SIFT-based warping process. Our method improves correlation by 35.5% over the baseline approach on the frame-level sub-challenge.	algorithm;baseline (configuration management);emotion recognition;facial recognition system;feature vector;neutral monism;scale-invariant feature transform	Albert C. Cruz;Bir Bhanu;Ninad Thakoor	2012		10.1145/2388676.2388777	computer vision;computer science;machine learning	Vision	33.924852199279876	-49.37716021848617	187818
7dfd8bd6907cacecd261de306ba450605d252288	evaluation of two stereo matchers on long real-world video sequences	semi global matching;performance evaluation;data measures;journal article;stereo vision;third eye method;belief propagation matching	The paper evaluates iterative semi-global matching (iSGM) and linear beliefpropagation matching (linBPM), both using a census data-cost function, which are two of the currently top-ranked stereo matchers. The evaluation is on long real-world video sequences where disparity ground-truth is not available. The paper applies two alternative (or mutually supporting) techniques for performance evaluation: the previously known third-eye method, and a few new data measures on video sequences. The main contribution of the paper is on answering the questions, how to evaluate stereo matchers on long real-world sequences if disparity ground truth is not available, and how to compare evaluation measures relatively to each other. The two stereo matchers used are illustrating the discussed evaluation measures; they could be replaced by other matchers, but evaluation results for those two matchers are also of interest on its own, by illustrating correlations in the behavior of those two basically very different matchers (defined by dynamic programming or by belief propagation optimization, respectively) on data sequences recorded in different traffic situations.		Bok-Suk Shin;Diego Caudillo;Reinhard Klette	2015	Pattern Recognition	10.1016/j.patcog.2014.04.011	computer vision;computer science;stereopsis;machine learning;pattern recognition;data mining	Vision	34.85737719877957	-50.442573241417705	188524
02efb1e4d1b3b5835ec444fff94dfda8d3542c93	sparse codes fusion for context enhancement of night video surveillance	sparse codes fusion scf;nighttime dictionary;motion extraction;video enhancement;mutual coherence learning mcl;enhanced background;daytime dictionary	Fusion-based method for video enhancement has been playing a basic but significant role, which is also proved high-efficiency. Still, there are some open questions, such as lamp-off problem, over-enhanced moving objects and night shadow. To resolve the problems, a novel method—sparse codes fusion (SCF) is proposed. With plenty of samples from daytime videos and nighttime videos of the same scene, we learn and obtain a daytime dictionary and a nighttime dictionary using the proposed mutual coherence learning (MCL) algorithm. These two dictionaries are utilized for fusion and extracting context enhanced background. Moreover, we reconstruct the nighttime dictionary to get nighttime background that would be applied in motion extraction. Then the moving objects are added into the enhanced background. Extensive experimental results show a highly comprehensive description of video frames that leads to improvements over the state of the art on many usual public video datasets.	algorithm;campus party;closed-circuit television;code;confidentiality;dictionary;display resolution;frame (video);monte carlo localization;mutual coherence (linear algebra);online and offline;sparse matrix;windows aero	Xianshu Ding;Hang Lei;Yunbo Rao	2015	Multimedia Tools and Applications	10.1007/s11042-015-2844-6	computer vision;simulation;speech recognition	Vision	35.17279336736787	-50.545508593757745	188552
0a1ef3054f15d282912dd32150fd72a89f86dd02	f-divergences driven video key frame extraction	analytical models;interpolation;cameras analytical models probability distribution computational modeling entropy measurement interpolation;measurement;jensen renyi divergence jrd key frame selection f divergences jensen shannon divergence jsd;computational modeling;jrd f divergences driven video key frame extraction shot based key frame selection technique video sequence jensen shannon divergence jensen renyi divergence jsd;video signal processing image sequences;key frame selection;probability distribution;f divergences;jensen shannon divergence jsd;entropy;jensen renyi divergence jrd;cameras	This paper proposes a shot-based key frame selection technique, aiming at generating a condensed set of frames representing the essential content of a video sequence. Inspired by the successful utilization of Jensen-Shannon Divergence (JSD) and Jensen-Rényi Divergence (JRD) for key frame selection [1] [2], we investigate several popularly accepted f-divergences for calculating the frame-by-frame distance to segment the video clip and then to obtain the key frames. Based on simulation and real test videos, the performances of the key frame selection method by using different versions of f-divergences are systematically analyzed. Extensive experimentation shows that, compared with the methods using JSD and JRD, the new technique is slightly better and computationally faster.	frame language;jackson system development;jensen's inequality;key frame;performance;rényi entropy;shannon (unit);simulation;video clip	Xiaoxiao Luo;Qing Xu;Mateu Sbert;Klaus Schöffmann	2014	2014 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2014.6890282	probability distribution;entropy;combinatorics;interpolation;pattern recognition;mathematics;computational model;measurement;statistics	Robotics	39.079571618491556	-51.603352409462545	188667
5223f3485b96bffe7dd4b3aa71e63fd2b049fcf0	is the pedestrian going to cross? answering by 2d pose estimation		Our recent work suggests that, thanks to nowadays powerful CNNs, image-based 2D pose estimation is a promising cue for determining pedestrian intentions such as crossing the road in the path of the ego-vehicle, stopping before entering the road, and starting to walk or bending towards the road. This statement is based on the results obtained on non-naturalistic sequences (Daimler dataset), i.e. in sequences choreographed specifically for performing the study. Fortunately, a new publicly available dataset (JAAD) has appeared recently to allow developing methods for detecting pedestrian intentions in naturalistic driving conditions; more specifically, for addressing the relevant question is the pedestrian going to cross? Accordingly, in this paper we use JAAD to assess the usefulness of 2D pose estimation for answering such a question. We combine CNN-based pedestrian detection, tracking and pose estimation to predict the crossing action from monocular images. Overall, the proposed pipeline provides new state-ofthe-art results.	3d pose estimation;pedestrian detection;sensor;statistical classification	Zhijie Fang;Antonio M. López	2018	2018 IEEE Intelligent Vehicles Symposium (IV)	10.1109/IVS.2018.8500413	computer science;pattern recognition;skeleton (computer programming);task analysis;artificial intelligence;feature extraction;pose;pedestrian;monocular;pedestrian detection	Vision	32.3824655976695	-50.64932379563003	188852
6b58823ce979292716bdf6b82a047759d4093da8	multi-view face pose classification by boosting with weak hypothesis fusion using visual and infrared images	image sampling;multi class adaboost;sub ensemble learning;sequential learning;sequential;learning;training;image fusion;image classification;testing;learning artificial intelligence face recognition image classification image fusion image sampling infrared imaging;visualization;boosting;visual and infrared images;face recognition;vectors;infrared imaging;feature extraction;samme multiview face pose classification weak hypothesis fusion sequential learning sensor fusion thermal infrared band visual band multiclass boosting structure multiclass adaboost classification framework thermal ir band feature descriptor visual image dataset thermal ir image dataset;visualization face boosting feature extraction testing training vectors;face;weak hypothesis fusion;learning artificial intelligence;learning multi class adaboost weak hypothesis fusion sub ensemble learning visual and infrared images sequential	This paper proposes a novel method for multi-view face pose classification through sequential learning and sensor fusion. The basic idea is to use face images observed in visual and thermal infrared (IR) bands, with the same sampling weight in a multi-class boosting structure. The main contribution of this paper is a multi-class AdaBoost classification framework where information obtained from visual and infrared bands interactively complement each other. This is achieved by learning weak hypothesis for visual and IR band independently and then fusing the optimized hypothesis sub-ensembles. In addition, an effective feature descriptor is introduced to thermal IR images. Experiments are conducted on a visual and thermal IR image dataset containing 4844 face images in 5 different poses. Results have shown significant increase in classification rate as compared with an existing multi-class AdaBoost algorithm SAMME trained on visual or infrared images alone, as well as a simple baseline classification-fusion algorithm.	adaboost;algorithm;baseline (configuration management);boosting (machine learning);interactivity;sampling (signal processing);visual descriptor;weight function	Yixiao Yun;Irene Y. H. Gu	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288287	facial recognition system;face;computer vision;contextual image classification;visualization;sequence learning;feature extraction;computer science;machine learning;pattern recognition;software testing;image fusion;boosting	Vision	36.05723021146773	-46.84729121500984	189961
952a87c68574d66a0ab1e7baf17bdf8719385518	learning cnns from weakly annotated facial images		Abstract Learning of convolutional neural networks (CNNs) to perform a face recognition task requires a large set of facial images each annotated with a label to be predicted. In this paper we propose a method for learning CNNs from weakly annotated images. The weak annotation in our setting means that a pair of an attribute label and a person identity label is assigned to a set of faces automatically detected in the image. The challenge is to link the annotation with the correct face. The weakly annotated images of this type can be collected by an automated process not requiring a human labor. We formulate learning from weakly annotated images as a maximum likelihood (ML) estimation of a parametric distribution describing the weakly annotated images. The ML problem is solved by an instance of the EM algorithm which in its inner loop learns a CNN to predict attribute label from facial images. Experiments on age and gender estimation problem show that the proposed algorithm significantly outperforms the existing heuristic approach for dealing with this type of data. A practical outcome of our paper is a new annotation of the IMDB database [26] containing 300 k faces each one annotated by biological age, gender and identity labels.		Vojtech Franc;Jan Cech	2018	Image Vision Comput.	10.1016/j.imavis.2018.06.011	artificial intelligence;convolutional neural network;pattern recognition;inner loop;mathematics;maximum likelihood;expectation–maximization algorithm;facial recognition system;heuristic;parametric statistics;annotation	Vision	32.05991498294211	-47.738058678834065	190395
703f3ce8263a274a50fe71ca6c666937910f042f	a novel approach to video transition detection based on a two-phase classification strategy	machine learning video transition detection two phase classification strategy svm imbalanced data classification video analysis;video transition detection;support vector machines;video signal processing;application software;video analysis;support vector machines support vector machine classification hidden markov models feature extraction video compression motion detection information analysis application software data mining machine learning;video compression;image classification;two phase classification strategy;imbalanced data classification;data mining;hidden markov models;video signal processing image classification learning artificial intelligence support vector machines;machine learning;feature extraction;support vector machine classification;svm;learning artificial intelligence;data classification;information analysis;motion detection	Video transition detection plays an important role in many tasks of video analysis. Aiming at the target application of commercial detection in news video, this paper tackles the problem in a unified framework and proposes an alternative classification strategy. Our method is made up of two phases. In the first stage, SVM is employed to classify the transitions into three classes: non-transition, cut, and big-transition. In the second stage, we concentrate on the discrimination of the rapid motion situation and gradual transition, which is based on another set of features. Apart from the new classification strategy proposed in this paper, imbalanced data classification issue is also taken into consideration, which has not received sufficient attention by previous work. Experimental results show that our method is effective.	collision detection;two-phase locking;unified framework;video content analysis	Shijin Li;Lin Lin;Xiaofang Li	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525426	support vector machine;computer science;machine learning;pattern recognition;data mining;one-class classification;hidden markov model	Robotics	38.76880717974842	-47.05945152789185	190883
cd6e32b00a53310f90a7138a41b855bf44129637	persons-in-places: a deep features based approach for searching a specific person in a specific location		Video retrieval is a challenging task in computer vision, especially with complex queries. In this paper, we consider a new type of complex query which simultaneously covers person and location information. The aim to search a specific person in a specific location. Bag-Of-Visual-Words (BOW) is widely known as an effective model for presenting rich-textured objects and scenes of places. Meanwhile, deep features are powerful for faces. Based on such state-of-the-art approaches, we introduce a framework to leverage BOW model and deep features for person-place video retrieval. First, we propose to use a linear kernel classifier instead of usingL2 distance to estimate the similarity of faces, given faces are represented by deep features. Second, scene tracking is employed to deal with the cases face of the query person is not detected. Third, we evaluate several strategies for fusing individual person search and location search results. Experiments were conducted on standard benchmark dataset (TRECVID Instance Search 2016) with more than 300 GB in storage and 464 hours in duration.	bag-of-words model in computer vision;benchmark (computing);bridging (networking);deep learning;experiment;gigabyte;machine learning	Vinh-Tiep Nguyen;Thanh Duc Ngo;Minh-Triet Tran;Duy-Dinh Le;Duc Anh Duong	2017	Informatica (Slovenia)		artificial intelligence;computer science;machine learning;kernel (linear algebra);classifier (linguistics);trecvid	AI	32.39967600437196	-51.7285927037995	191524
58d65e07e4798cccf632a1e6b1688f6b993069da	gradient-layer feature transform for action detection and recognition	sequence correspondence fusion;action detection;action recognition;frame difference projection;gaussian convolution	We introduce a powerful gradient based method to extract image features.We propose an efficient projection based approach for action detection.We create an effective correspondence fusion technique for action representation. Exploring action feature representation in consecutive video frames is a basic but critical issue in the area of computer vision. This paper presents a principled technique transforming gradient-based features into coherent spatial-temporal descriptors for action detection and recognition. Specifically, Gaussian convolution based technique is first applied to extract spatial features of each image frame on gradient layer, based on which the spatial features are further processed according to the forward-backward frame difference and correspondence fusion between frames for frame sequence representation. Furthermore, region of actions is labeled via thresholding the projection of difference features in horizontal-vertical direction while action types are classified via learning the fused features. We evaluate our approach on samples from KTH, Weizmann, UCF Sports dataset and ChangeDetection.NET dataset 2014, which demonstrates its applicability and effectiveness.	gradient	Liangliang Wang;Ruifeng Li;Yajun Fang	2016	J. Visual Communication and Image Representation	10.1016/j.jvcir.2016.06.023	computer vision;machine learning;pattern recognition;mathematics	Vision	36.36310344508481	-51.27960143364981	191583
2c661758d2f0e9abc4acb77c3f4fe7e10f4172be	finding periodicity in space and time	image segmentation;real world examples periodicity templates segmentation characterization spatiotemporal periodicity periodicity representation periodicity filter;image texture;optical noise legged locomotion space technology noise robustness image motion analysis object detection character recognition stochastic processes energy measurement image sequences;image segmentation image sequences image texture;image sequences	"""An algorithm for simultaneous detection, seg-mentation, and characterization of spatiotempo-ral periodicity is presented. The use of period-icity templates is proposed to localize and characterize temporal activities. The templates not only indicate the presence and location of a periodic event, but also give an accurate quantitative periodicity measure. Hence, they can be used as a new means of periodicity representation. The proposed algorithm can also be considered as a \periodicity lter,"""" a low-level model of period-icity perception. The algorithm is computation-ally simple, and shown to be more robust than optical ow based techniques in the presence of noise. A variety of real-world examples are used to demonstrate the performance of the algorithm."""	algorithm;computation;database;energy (psychological);high- and low-level;quasiperiodicity	Fang Liu;Rosalind W. Picard	1998		10.1109/ICCV.1998.710746	image texture;computer vision;computer science;machine learning;pattern recognition;mathematics;image segmentation	Vision	38.71616733528246	-48.85768386786744	191626
d8c9bad8d07ae4196027dfb8343b9d9aefb130ff	power difference template for action recognition	action recognition;power difference template;normalized projection histogram;motion kinetic velocity	This paper proposes power difference template as a new spatial-temporal representation for action recognition. Specifically, spatial power features are first extracted according to the transform of Gaussian convolution on gradients between logarithmic and exponential domain. Using the forward–backward frame power difference method, we thus present normalized projection histogram (NPH) to characterize segmented action spatial features by normalizing histogram of the 2D horizontal–vertical projections. Furthermore, from the perspective of energy conservation, motion kinetic velocity (MKV) is introduced as a supplement for representing temporal relationships of power features by supposing that the variation of power is produced by motion in the form of kinetic energy. Our power difference template fusing NPH and MKV is further integrated to a bag of word model for training and testing under a support vector machine framework. Experiments on KTH, UCF Sports, UCF101 and HMDB datasets demonstrate the effectiveness of the proposed algorithm.	algorithm;cellular automaton;convolution;gradient;human metabolome database;mit computer science and artificial intelligence laboratory;real-time transcription;support vector machine;time complexity;velocity (software development)	Liangliang Wang;Ruifeng Li;Yajun Fang	2017	Machine Vision and Applications	10.1007/s00138-017-0848-0	artificial intelligence;pattern recognition;kinetic energy;computer vision;support vector machine;mathematics;normalization (statistics);logarithm;machine learning;exponential function;gaussian;convolution;histogram	Vision	36.886917396349816	-49.94326587565258	191751
4a7b6676a11af36dfa6b871ae238eb902832040b	using mutual information to indicate facial poses in video sequences	vision system;copy detection;video filtering;vocabulary trie;indexing and retrieval;computer vision;pattern recognition;mutual information;scene understanding;video database;similarity measure;pose estimation	Estimation of the facial pose in video sequences is one of the major issues in many vision systems such as face based biometrics, scene understanding for human and others. The proposed method uses a novel pose estimation algorithm based on mutual information to extract any required facial pose from video sequences. The method extracts the poses automatically and classifies them according to view angle. Experimental results on the XM2VTS video database indicated a pose classification rate of 99.2% while it was shown that it outperforms a PCA reconstruction method which was used as a benchmark.	algorithm;benchmark (computing);biometrics;mutual information	Georgios Goudelis;Anastasios Tefas;Ioannis Pitas	2009		10.1145/1646396.1646429	computer vision;speech recognition;pose;3d pose estimation;machine vision;computer science;video tracking;pattern recognition;mutual information	Vision	37.02688081222282	-51.08805894076727	192066
41ff83ada6e250d89150d5860232b09864652648	estimation of crowd density by clustering motion cues	people counting;video surveillance;crowd monitoring;density estimation;clustering;crowd;optical flow	Understanding crowd behavior using automated video analytics is a relevant research problem in recent times due to complex challenges in monitoring large gatherings. From an automated video surveillance perspective, estimation of crowd density in particular regions of the video scene is an indispensable tool in understanding crowd behavior. Crowd density estimation provides the measure of number of people in a given region at a specified time. While most of the existing computer vision methods use supervised training to arrive at density estimates, we propose an approach to estimate crowd density using motion cues and hierarchical clustering. The proposed method incorporates optical flow for motion estimation, contour analysis for crowd silhouette detection, and clustering to derive the crowd density. The proposed approach has been tested on a dataset collected at the Melbourne Cricket Ground (MCG) and two publicly available crowd datasets—Performance Evaluation of Tracking and Surveillance (PETS) 2009 and University of California, San Diego (UCSD) Pedestrian Traffic Database—with different crowd densities (medium- to high-density crowds) and in varied environmental conditions (in the presence of partial occlusions). We show that the proposed approach results in accurate estimates of crowd density. While the maximum mean error of $$3.62$$ 3.62 was received for MCG and PETS datasets, it was $$2.66$$ 2.66 for UCSD dataset. The proposed approach delivered superior performance in $$50~\%$$ 50 % of the cases on PETS $$2009$$ 2009 dataset when compared with existing methods.	closed-circuit television;cluster analysis;computer vision;hierarchical clustering;motion estimation;optical flow;performance evaluation;supervised learning;video content analysis	Aravinda S. Rao;Jayavardhana Gubbi;Slaven Marusic;Marimuthu Palaniswami	2014	The Visual Computer	10.1007/s00371-014-1032-4	computer vision;simulation;density estimation;computer science;machine learning;optical flow;cluster analysis	Vision	38.30937679208114	-46.48940646035896	192931
4e771f7d9ff9ca15876f1767ff7c2fe8ce9aac92	moving object detection in complex scene using spatiotemporal structured-sparse rpca		Moving object detection is a fundamental step in various computer vision applications. Robust principal component analysis (RPCA)-based methods have often been employed for this task. However, the performance of these methods deteriorates in the presence of dynamic background scenes, camera jitter, camouflaged moving objects, and/or variations in illumination. It is because of an underlying assumption that the elements in the sparse component are mutually independent, and thus the spatiotemporal structure of the moving objects is lost. To address this issue, we propose a spatiotemporal structured sparse RPCA algorithm for moving objects detection, where we impose spatial and temporal regularization on the sparse component in the form of graph Laplacians. Each Laplacian corresponds to a multi-feature graph constructed over superpixels in the input matrix. We enforce the sparse component to act as eigenvectors of the spatial and temporal graph Laplacians while minimizing the RPCA objective function. These constraints incorporate a spatiotemporal subspace structure within the sparse component. Thus, we obtain a novel objective function for separating moving objects in the presence of complex backgrounds. The proposed objective function is solved using a linearized alternating direction method of multipliers based batch optimization. Moreover, we also propose an online optimization algorithm for real-time applications. We evaluated both the batch and online solutions using six publicly available data sets that included most of the aforementioned challenges. Our experiments demonstrated the superior performance of the proposed algorithms compared with the current state-of-the-art methods.	algorithm;augmented lagrangian method;computer vision;experiment;graph - visual representation;loss function;mathematical optimization;matrix regularization;numerous;object detection;online optimization;optimization problem;physical object;real-time clock;robust principal component analysis;solutions;sparse matrix	Sajid Javed;Arif Mahmood;Somaya Al-Máadeed;Thierry Bouwmans;Soon Ki Jung	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2874289	computer vision;eigenvalues and eigenvectors;object detection;sparse matrix;robust principal component analysis;artificial intelligence;matrix (mathematics);mathematics;pattern recognition;linear programming;subspace topology;laplace operator	Vision	33.06600869642634	-46.44096531483428	194000
72f4c415b5f3ecf63380b6985c95c5af2ba72632	activity recognition on a large scale in short videos - moments in time dataset		Moments capture a huge part of our lives. Accurate recognition of these moments is challenging due to the diverse and complex interpretation of the moments. Action recognition refers to the act of classifying the desired action/activity present in a given video. In this work, we perform experiments on Moments in Time dataset to recognize accurately activities occurring in 3 second clips. We use state of the art techniques for visual, auditory and spatio temporal localization and develop method to accurately classify the activity in the Moments in Time dataset. Our novel approach of using Visual Based Textual features and fusion techniques performs well providing an overall 89.23 % Top 5 accuracy on the 20 classes a significant improvement over the Baseline TRN model.	activity recognition;experiment	Ankit Shah;Harini Kesavamoorthy;Poorva Rane;Pramati Kalwad;Alexander G. Hauptmann;Florian Metze	2018	CoRR		clips;machine learning;artificial intelligence;pattern recognition;activity recognition;computer science	Vision	35.324325801910554	-49.606849780817384	195031
144cddee40fc77ee5e4773a75d0ba69dfbc854cf	rgb-d action recognition using linear coding	rgb d;linear coding;action recognition	In this paper, we investigate action recognition using an inexpensive RGB-D sensor (Microsoft Kinect). First, a depth spatial-temporal descriptor is developed to extract the interested local regions in depth image. Such descriptors are very robust to the illumination and background clutter. Then the intensity spatial-temporal descriptor and the depth spatial-temporal descriptor are combined and feeded into a linear coding framework to get an effective feature vector, which can be used for action classification. Finally, extensive experiments are conducted on a publicly available RGB-D action recognition dataset and the proposed method shows promising results.	linear code	Huaping Liu;Mingyi Yuan;Fuchun Sun	2015	Neurocomputing	10.1016/j.neucom.2013.12.061	computer vision;pattern recognition;mathematics	Vision	36.53062123587344	-50.649702286550806	195071
e237ec3e50c7681c6b88b2ab481c15e62b61f154	a real-time hand posture recognition system using deep neural networks	kinect;deep neural networks;hand tracking;posture recognition	Hand posture recognition (HPR) is quite a challenging task, due to both the difficulty in detecting and tracking hands with normal cameras and the limitations of traditional manually selected features. In this article, we propose a two-stage HPR system for Sign Language Recognition using a Kinect sensor. In the first stage, we propose an effective algorithm to implement hand detection and tracking. The algorithm incorporates both color and depth information, without specific requirements on uniform-colored or stable background. It can handle the situations in which hands are very close to other parts of the body or hands are not the nearest objects to the camera and allows for occlusion of hands caused by faces or other hands. In the second stage, we apply deep neural networks (DNNs) to automatically learn features from hand posture images that are insensitive to movement, scaling, and rotation. Experiments verify that the proposed system works quickly and accurately and achieves a recognition accuracy as high as 98.12%.	algorithm;artificial neural network;deep learning;experiment;image scaling;kinect;poor posture;proteomics;real-time transcription;requirement;sensor	Ao Tang;Ke Lu;Yufei Wang;Jie Huang;Houqiang Li	2015	ACM TIST	10.1145/2735952	computer vision;simulation;computer science;natural user interface	Vision	36.87968739145611	-46.9968684549959	195520
4844b3fc002937365e2fc07b73e182273d5772b2	multimodal framework for analyzing the affect of a group of people		With the advances in multimedia and the world wide web, users upload millions of images and videos everyone on social networking platforms on the Internet. From the perspective of automatic human behavior understanding, it is of interest to analyze and model the affects that are exhibited by groups of people who are participating in social events in these images. However, the analysis of the affect that is expressed by multiple people is challenging due to the varied indoor and outdoor settings. Recently, a few interesting works have investigated face-based group-level emotion recognition (GER). In this paper, we propose a multimodal framework for enhancing the affective analysis ability of GER in challenging environments. Specifically, for encoding a person's information in a group-level image, we first propose an information aggregation method for generating feature descriptions of face, upper body, and scene. Later, we revisit localized multiple kernel learning for fusing face, upper body, and scene information for GER against challenging environments. Intensive experiments are performed on two challenging group-level emotion databases (HAPPEI and GAFF) to investigate the roles of the face, upper body, scene information, and the multimodal framework. Experimental results demonstrate that the multimodal framework achieves promising performance for GER.	database;emotion recognition;experiment;internet;multimodal interaction;multiple kernel learning;upload;world wide web	Xiaohua Huang;Abhinav Dhall;Roland Goecke;Matti Pietik&#x00E4;inen;Guoying Zhao	2018	IEEE Transactions on Multimedia	10.1109/TMM.2018.2818015	robustness (computer science);artificial intelligence;computer science;the internet;computer vision;multiple kernel learning;feature extraction;social group;multimedia;facial recognition system;affect (psychology);social network	Vision	34.08012019389057	-50.62181689984916	196254
86988b5614cc7413a2dbb3503be7ba0cafe869fc	daily activity recognition combining gaze motion and visual features	context awareness;image recognition;bag of features;eye tracking;activity recognition	Recognition of user activities is a key issue for context-aware computing. We present a method for recognition of user daily activities using gaze motion features and image-based visual features. Gaze motion features dominate for inferring the user's egocentric context whereas image-based visual features dominate for recognition of the environments and the target objects. The experimental results show the fusion of those different type of features improves performance of user daily activity recognition.	activity recognition;context awareness	Yuki Shiga;Andreas Dengel;Takumi Toyama;Koichi Kise;Yuzuko Utsumi	2014		10.1145/2638728.2641691	computer vision;eye tracking;computer science;three-dimensional face recognition;multimedia;3d single-object recognition;activity recognition	HCI	35.0278813424297	-49.84553950773424	196362
cfec31176dde529a133586566a15bc551d052c2e	human action recognition in large-scale datasets using histogram of spatiotemporal gradients	histograms;object recognition;video surveillance;support vector machines;histogram of oriented gradients human action recognition large scale datasets histogram of spatiotemporal gradients kth dataset hollywood dataset surveillance videos virat dataset;training;loading;testing;video surveillance feature extraction object recognition;feature extraction;spatiotemporal phenomena;vehicles;histograms spatiotemporal phenomena training testing vehicles loading support vector machines	Research in human action recognition has advanced along multiple fronts in recent years to address various types of actions including simple, isolated actions in staged data (e.g., KTH dataset), complex actions (e.g., Hollywood dataset) and naturally occurring actions in surveillance videos (e.g, VIRAT dataset). Several techniques including those based on gradient, flow and interest-points have been developed for their recognition. Most perform very well in standard action recognition datasets, but fail to produce similar results in more complex, large-scale datasets. Here we analyze the reasons for this less than successful generalization by considering a state-of-the-art technique, histogram of oriented gradients in spatiotemporal volumes as an example. This analysis may prove useful in developing robust and effective techniques for action recognition.	histogram of oriented gradients;hollywood;image gradient;virat	Kishore K. Reddy;Naresh P. Cuntoor;A. G. Amitha Perera;Anthony Hoogs	2012	2012 IEEE Ninth International Conference on Advanced Video and Signal-Based Surveillance	10.1109/AVSS.2012.40	support vector machine;computer vision;feature extraction;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;histogram;software testing	Vision	37.947694357370416	-47.56294970127608	196483
7f2d497b29dd2f7be7c2100206a292f9be61ed38	spatiotemporal-boosted dct features for head and face gesture analysis	face patch;face gesture analysis;facial expression;adaboost algorithm;subject independent classification performance;dct coefficient;classification result;facial landmark position;facial landmark point;spatiotemporal dct feature;head gesture;spatiotemporal-boosted dct feature;human computer interface;support vector machine;time series	Automatic analysis of head gestures and facial expressions is a challenging research area and it has significant applications in humancomputer interfaces. In this study, facial landmark points are detected and tracked over successive video frames using a robust method based on subspace regularization, Kalman prediction and refinement. The trajectories (time series) of facial landmark positions during the course of the head gesture or facial expression are organized in a spatiotemporal matrix and discriminative features are extracted from the trajectory matrix. Alternatively, appearance based features are extracted from DCT coefficients of several face patches. Finally Adaboost algorithm is performed to learn a set of discriminating spatiotemporal DCT features for face and head gesture (FHG) classification. We report the classification results obtained by using the Support Vector Machines (SVM) on the outputs of the features learned by Adaboost. We achieve 94.04% subject independent classification performance over seven FHG. 1 2	adaboost;algorithm;coefficient;discrete cosine transform;facial recognition system;refinement (computing);support vector machine;time series	Hatice Çinar Akakin;Bülent Sankur	2010		10.1007/978-3-642-14715-9_7	computer vision;speech recognition;computer science;pattern recognition	Vision	36.59190776935314	-48.803187668298406	196589
2c23e77f9e1c8db1e5fb3093c14f6d49c337c305	domain adaptive object detection	unsupervised learning;training principal component analysis cameras vehicles object detection kernel computational modeling;kernel;training;image classification;computer vision;unsupervised learning computer vision image classification object detection sampling methods;computational modeling;unsupervised learning domain adaptive object detection domain adaptation technique transfer learning technique computer vision image classification negative image subwindow positive instance random sampling technique vehicle detection urban surveillance dataset;principal component analysis;vehicles;sampling methods;cameras;object detection	We study the use of domain adaptation and transfer learning techniques as part of a framework for adaptive object detection. Unlike recent applications of domain adaptation work in computer vision, which generally focus on image classification, we explore the problem of extreme class imbalance present when performing domain adaptation for object detection. The main difficulty caused by this imbalance is that test images contain millions or billions of negative image subwindows but just a few image subwindows containing positive instances, which makes it difficult to adapt to changes in the positive classes present new domains by simple techniques such as random sampling. We propose an initial approach to addressing this problem and apply our technique to vehicle detection in a challenging urban surveillance dataset, demonstrating the performance of our approach with various amounts of supervision, including the fully unsupervised case.	computer vision;domain adaptation;monte carlo method;object detection;sampling (signal processing)	Fatemeh Mirrashed;Vlad I. Morariu;Behjat Siddiquie;Rogério Schmidt Feris;Larry S. Davis	2013	2013 IEEE Workshop on Applications of Computer Vision (WACV)	10.1109/WACV.2013.6475036	unsupervised learning;sampling;computer vision;contextual image classification;feature detection;kernel;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;computational model;principal component analysis	Vision	36.602445158186214	-46.74523613900404	197091
a3749acf6d8576edbf4c8b88caff188c9277bc90	crowd counting method on sparse scene	video surveillance;estimation;feature extraction;statistics;sociology	In recent years, With the development of science and technology to promote the popularity of video surveillance, computer techniques have a great value on obtaining the crowd counting information of surveillance video automatically, but perspective effects, mutual occlusion between people and other factors make crowd counting difficult. This paper presents a crowd counting method on sparse scene. Firstly, analyzing the characteristics of the surveillance video to access available prior knowledge; Secondly, combining with prior knowledge to extract the characteristics of target prospects block; Finally, support vector regression machine is employed to estimate the number. Experiments show that the method improves the situation of pedestrians occlusion crowd counting estimation accuracy.	algorithm;closed-circuit television;experiment;sparse matrix;support vector machine	Huaiming Li;Fei Wang;Fangfang Song;Lianqing Wang	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7849972	computer vision;simulation;computer science;data mining	Vision	34.205510815086754	-50.413301327445836	197143
91ab27348b02d1a241f9f4778695d1bc26f55abe	augmented poselets for human body pose inference by a probabilistic graphical model	part based detection methods;probabilistic graphical models;poselets;human body pose inference	"""Human body pose estimation is a challenging task which, depending on the context of application, degree of accuracy and availability of image frames, is being faced with different state-of-the-art approaches. In this paper we propose a part-based detection approach combined with a probabilistic graphical model framework for structural constraints on monocular single images, which offers several benefits: human body joints localization inference (rather than a holistic body detection), low computational cost, and robustness against unknown poses as long as antropomorphic constraints are preserved. These outcomes make this approach feasible for applications related to portable devices or multimedia applications which need to be aware of the presence of people in real time at a low cost, and can take advantage of the knowledge about body poses. The presented approach is built by taking into account the existing """"Poselets"""" architecture and one of its foundations, the """"H3D"""" dataset. On top of this, we """"augment"""" the prior knowledge about human body structure and parts appearance in order to learn spatial probability distributions on body natural constraints, which will be used afterwards by the probabilistic graphical model."""	algorithmic efficiency;autostereogram;computation;graphical model;holism;mobile device	Pol Cirujeda;Xavier Binefa	2012		10.1145/2390821.2390827	computer vision;computer science;machine learning;data mining	Vision	33.30216381172265	-49.11004118730059	197161
e2e7a7bcfb1d5063f5b0b1519677c0095cb7242d	video partitioning and camera motion characterization for content-based video indexing	video signal processing;video cameras video signal processing visual databases indexing motion estimation image representation statistical analysis image sequences multimedia computing;motion estimation;video sequences video partitioning camera motion content based video indexing scenes moving objects shot changes detection system of view movement recognition dominant motion estimation image representation 2d affine model statistical techniques robustness efficiency real documentary video multimedia databases;multimedia computing;video indexing;camera motion;statistical analysis;indexing;video cameras;image representation;moving object detection;cameras indexing layout biomedical imaging image retrieval image sequences histograms gunshot detection systems motion detection image recognition;statistical techniques;image sequences;visual databases	This paper describes an original approach which jointly addresses the two issues of video partitioning and camera motion characterization in the context of content-based video indexing. It can cope with scenes containing moving objects. Detection of shot changes and recognition of the movements of the system-of-view are both derived from the computation, at each time instant, of the dominant motion in the image represented by a 2D affine model, and from the variation of the size of its associated support. The successive steps of the method rely on statistical techniques ensuring robustness and efficiency. Results on a real documentary video are reported and validate the proposed approach.		Patrick Bouthemy;Fabrice Ganansia	1996		10.1109/ICIP.1996.559646	computer vision;search engine indexing;quarter-pixel motion;computer science;video tracking;motion estimation;block-matching algorithm;multimedia;motion compensation;statistics;multiview video coding;computer graphics (images)	Vision	39.117211698701695	-50.99385238338412	198132
009a18d04a5e3ec23f8ffcfc940402fd8ec9488f	action recognition by weakly-supervised discriminative region localization.		Figure 1 shows our proposed weakly-supervised framework for localizing discriminative regions and recognizing actions. The first step in recognizing the action is localizing discriminative sub-regions that best describe the action. These candidates are selected using a set of D discriminative sub-region localizers. A localizer φd , learned during training, is a vector of parameters describing the probability distribution of a latent location variable. Even though localizers are not associated with any action class explicitly, using multiple localizers allows the model to select different regions in each frame to capture variations in classes. For every sub-region in each frame of the video, localizers compute the probability of that sub-region being the most discriminative in that frame as follows:	internationalization and localization	Hakan Boyraz;Syed Zain Masood;Baoyuan Liu;Marshall F. Tappen;Hassan Foroosh	2014		10.5244/C.28.111	artificial intelligence;computer science;pattern recognition;discriminative model	Vision	35.75306438600907	-49.01259772103266	198629
8c8c618967b0b39b357e0bcb9c3338d1d3534b6e	efficient human motion transition via hybrid deep neural network and reliable motion graph mining		Skeletal motion transition is of crucial importance to the simulation in interactive environments. In this paper, we propose a hybrid deep learning framework that allows for flexible and efficient human motion transition from motion capture (mocap) data, which optimally satisfies the diverse user-specified paths. We integrate a convolutional restricted Boltzmann machine with deep belief network to detect appropriate transition points. Subsequently, a quadruples-like data structure is exploited for motion graph building, which significantly benefits for the motion splitting and indexing. As a result, various motion clips can be well retrieved and transited fulfilling the user inputs, while preserving the smooth quality of the original data. The experiments show that the proposed transition approach performs favorably compared to the state-of-the-art competing approaches.	deep learning;structure mining	Bing Zhou;Xin Liu;Shu-Juan Peng;Bineng Zhong;Ji-Xiang Du	2017		10.1007/978-981-10-7299-4_60	clips;theoretical computer science;deep belief network;artificial neural network;search engine indexing;deep learning;data structure;machine learning;motion capture;restricted boltzmann machine;computer science;artificial intelligence	AI	32.78257062793546	-48.7733764326608	199250
2b376edd2f1f46dcf72d83fe081b13cac7480b03	individual recognition by kinematic-based gait analysis	image recognition;individual recognition;legged locomotion;biometrics;biological system modeling;least squares approximation;gait recognition;joints;nearest neighbor method;kinematics;feature vector;image sequence;least square;intelligent systems;gait analysis;genetic algorithm;feature selection;humans;humans legged locomotion kinematics least squares approximation biological system modeling biometrics image recognition image sequences joints intelligent systems;image sequences	Current gait recognition approaches only consider individuals walking frontopamllel t o the image plane. This makes them inapplicoble for recognizing individuals walking from different angles with respect to the image plane. In this paper, we propose a kinematic-based approach to recognize individuals by gait. The proposed approach estimates 30 human walking parameters by performing a least squares fit of the 30 kinematic model to the 2 0 silhouette eztmctedfmm a monocular image sequence. A genetic algorithm is used for feature selection from the estimated parameters, and the individuals are then recognized from the feature vectors using a nearest neighbor method. Ezperimental results show that the proposed approach achieves good performance in recognizing individuals walking fmm different angles with respect to the image plane.	feature selection;gait analysis;genetic algorithm;image plane;least squares;nearest neighbor search	Bir Bhanu;Ju Han	2002	Object recognition supported by user interaction for service robots	10.1109/ICPR.2002.1047863	computer vision;computer science;machine learning;pattern recognition;feature selection;least squares	Vision	38.643890195868	-49.92140399920596	199262
555b2f4a82048bacb7e058ba02391310455da9a0	single-frame 3d human pose recovery from multiple views	bottom up;large data sets;universiteitsbibliotheek;multiple views;parameter space;experimental evaluation	We present a system for the estimation of unconstrained 3D human upper body pose from multi-camera single-frame views. Pose recovery starts with a shape detection stage where candidate poses are generated based on hierarchical exemplar matching in the individual camera views. The hierarchy used in this stage is created using a hybrid clustering approach in order to efficiently deal with the large number of represented poses. In the following multi-view verification stage, poses are re-projected to the other camera views and ranked according to a multi-view matching score. A subsequent gradient-based local pose optimization stage bridges the gap between the used discrete pose exemplars and the underlying continuous parameter space. We demonstrate that the proposed clustering approach greatly outperforms state-of-the-art bottom-up clustering in parameter space and present a detailed experimental evaluation of the complete system on a large data set.	bottom-up parsing;cluster analysis;experiment;gradient;mathematical optimization;multiple encryption	Michael Hofmann;Dariu Gavrila	2009		10.1007/978-3-642-03798-6_8	computer vision;3d pose estimation;machine learning;top-down and bottom-up design;data mining;mathematics;parameter space;statistics	Vision	33.892567951385324	-48.02456206580433	199309
1b93206fb8dfd7fa57064b406f5b4de10269fa78	supporting fuzzy metric temporal logic based situation recognition by mean shift clustering	mean shift clustering;situation graph trees sgt;fuzzy metric temporal logic fmtl;situation recognition	This contribution aims at assisting video surveillance operators with automatic understanding of situations in videos. The situations comprise many different agents interacting in groups. To this end we extended an existing situation recognition framework based on Situation Graph Trees and Fuzzy Metric Temporal Logic. Non-parametric meanshift clustering is utilized to support the logic-based inference process for such group-based situations, namely to improve efficiency. Additionally, the underlying knowledge base was augmented to also handle multiagent queries and the situation inference was adapted to also handle inference for group-based situations. For evaluation the publicly available BEHAVE video dataset was used consisting of partially annotated real video data of persons. The results show that the proposed system is capable of correctly and efficiently understanding such group-based situations.	agent-based model;closed-circuit television;cluster analysis;interaction;knowledge base;mean shift;temporal logic	David E Muench;Eckart Michaelsen;Michael Arens	2012		10.1007/978-3-642-33347-7_21	machine learning;pattern recognition;data mining;mathematics	Vision	34.30119921908267	-46.57192880217648	199795
