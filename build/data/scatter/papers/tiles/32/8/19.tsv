id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
3c2f8a25c468edd0236928725840eb8346c8a529	assessment of the reliability of fault-tolerant software: a bayesian approach	fiabilidad;reliability;fault tolerant;bayesian approach;decision bayes;bayesian inference;software systems;prior distribution;bayes decision;ley a priori;fault tolerant system;fiabilite;sistema tolerando faltas;cost effectiveness;systeme tolerant les pannes;fiabilite logiciel;fiabilidad logicial;software design;software reliability;loi a priori	Bev Littlewood, Peter Popov, Lorenzo Strigini Centre for Software Reliability, City University, London Abstract Fault tolerant systems based on the use of software design diversity may be able to achieve high levels of reliability more cost-effectively than other approaches, such as heroic debugging. Earlier experiments have shown that the reliabilities of multi-version software systems are more reliable than the individual versions. However, it is also clear that the reliability benefits are much worse than would be suggested by naive assumptions of failure independence between the versions. It follows that it is necessary to assess the reliability actually achieved in a fault tolerant system. The difficulty here mainly lies in acquiring knowledge of the degree of dependence between the failures processes of the versions. The paper addresses the problem using Bayesian inference. In particular, it considers the problem of choosing a prior distribution to represent the beliefs of an expert assessor. It is shown that this is not easy, and some pitfalls for the unwary are identified.	bayesian approaches to brain function;bayesian network;debugging;existential quantification;experiment;fault tolerance;fault-tolerant software;formal proof;software design;software reliability testing;software system;unintended consequences	Bev Littlewood;Peter T. Popov;Lorenzo Strigini	2000		10.1007/3-540-40891-6_26	fault tolerance;engineering;artificial intelligence;machine learning;data mining;database;statistics	SE	-61.815167628055164	31.75051009784561	148811
8a05836b427f0f82a569ac8cda7c94279c4301a9	a decision support system for reliable software development	developpement logiciel;decision support systems programming testing parameter estimation software reliability probability distribution application software time measurement large scale systems uncertainty;ciclo desarrollo;probability;decision aid;life cycle;time measurement;incertidumbre;application software;uncertainty;coaccion;contrainte;testing;ayuda decision;support system;constraint;desarrollo logicial;probability distribution;probabilidad;decision support systems;software development;probabilite;cycle developpement;aide decision;incertitude;fiabilite logiciel;parameter estimation;fiabilidad logicial;software reliability;programming;large scale systems	Reliability has become a major concern for large-scale software developers. This concern has resulted in the development of a large number of models intended to describe the failure process of software. The use of these models during development has been restricted to the later stages of integration testing. A decision support system is described here which allows for the use of existing software reliability models throughout development. The system uses information from the developer in the form of constraints on the probability distributions which describe the uncertainty associated with the parameters of the various models. Model management is data driven to allow for maximum flexibility in probability assessments which can be provided by either the user or the system.	decision support system;integration testing;list of software reliability models;software developer;software development;software reliability testing	Donald E. Brown	1987	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1987.289335	probability distribution;biological life cycle;programming;long-term support;verification and validation;application software;uncertainty;decision support system;computer science;package development process;backporting;software reliability testing;software development;iterative and incremental development;software construction;probability;data mining;mathematics;software testing;constraint;estimation theory;software deployment;goal-driven software development process;software development process;software quality;statistics;time;software system;avionics software	SE	-61.95783397872082	31.728261599167283	149002
6d3f0d31196c5be0eb1298f5740fe4f23c508286	using software metrics to estimate the impact of maintenance in the performance of embedded software	software metrics;regression analysis embedded systems design space exploration software metrics maintenance;software metrics embedded systems software maintenance;maintenance;hardware software metrics predictive models algorithm design and analysis prediction algorithms embedded systems;embedded systems;design space exploration software metrics embedded software performance maintenance impact design choice nonfunctional requirements embedded systems physical metrics design metrics;regression analysis;design space exploration	This paper proposes a strategy to assist the designer in evaluating the impact of a design choice with respect to the non-functional requirements in embedded systems. We use several regression models to predict physical metrics from design metrics in order to estimate the impact on performance of software changes in the early stages of its development. This prediction can be used both during maintenance and during the first design to compare alternative module decompositions or design changes before implementation. Such an early estimation allows an efficient design space exploration with no penalty in the development time, which are crucial aspects for an embedded system.	design space exploration;embedded software;embedded system;functional requirement;non-functional requirement;software metric	Andrws Vieira;Pedro H. A. Faustini;Érika F. Cota	2014	2014 IEEE International Conference on Software Maintenance and Evolution	10.1109/ICSME.2014.86	reliability engineering;software visualization;real-time computing;computer science;systems engineering;backporting;software design;software development;software design description;software construction;software maintenance;regression analysis;software metric;avionics software	Embedded	-62.60280662781036	30.856190636372634	149291
2698b7c749c8d9e90925543500ee20192f82cccd	an empirical study of two software product line tools		In the last decades, software product lines (SPL) have proven to be an efficient software development technique in industries due its capability to increase quality and productivity and decrease cost and time-tomarket through extensive reuse of software artifacts. To achieve these benefits, tool support is fundamental to guide industries during the SPL development life-cycle. However, many different SPL tools are available nowadays and the adoption of the appropriate tool is a big challenge in industries. In order to support engineers choosing a tool that best fits their needs, this paper presents the results of a controlled empirical study to assess two Eclipse-based tools, namely FeatureIDE and pure::variants. This empirical study involved 84 students who used and evaluated both tools. The main weakness we observe in both tools are the lack adequate mechanisms for managing the variability, such as for product configuration. As a strength, we observe the automated analysis and the feature model editor.	eclipse;fits;feature model;heart rate variability;knowledge-based configuration;software development process;software product line	Kattiana Constantino;Juliana Alves Pereira;Juliana Padilha;Priscilla Vasconcelos;Eduardo Figueiredo	2016		10.5220/0005829801640171	verification and validation;software development;software construction;empirical process;computer-aided software engineering	SE	-60.88585080350485	26.81132342769664	149349
89a1beb354e5ca95be329b297f540da83f7bbc7f	testing web applications intelligently based on agent	capture-replay;intelligent agents;web application testing	Web application testing is concerned with numerous and complicated testing objects, methods and processes. In order to improve the testing efficiency, the automatic and intelligent level of the testing execution should be enhanced. So combined with the specialties of the Web applications, the necessity and feasibility of the automatic and intelligent execution of the Web application testing are analyzed firstly; Then, on the base of the related work, the executing process of the Web application testing is detailedly described and thoroughly analysed, so as to determine the steps and flows of the testing execution along with the adopted techniques and tools; next, improve the capture-replay technique and make it fit for the dynamic characters of Web applications, and adopt the intelligent Agent to realize the monitor, management and exception-handler of the whole testing execution. Thus, in this way, the process of the Web application testing can be implemented automatically and intelligently.	exception handling;intelligent agent;web application;web testing;world wide web	Lei Xu;Baowen Xu	2005			data mining;web application;computer science	SE	-56.300640625689475	31.318033148421424	149667
91455d9a91d42354a4360530009e017845104aca	system-pla: a systematic evaluation method for uml-based software product line architecture			programmable logic array;software product line;unified modeling language	Edson Alves de Oliveira Junior	2010				SE	-56.27755588184669	28.00527873537944	149721
0cf947539ce9f24e6828581c4bce408fba7d3e81	bmw-room an object-oriented method for ascet	object oriented methods;real time;code generation;object oriented;software development;software component	This paper presents an object-oriented method customized for a tool-assisted development of car software components. Tough market conditions motivate smart software development. ASCET SD is a tool to generate target code from graphic specifications, avoiding costly programming in C. But ASCET lacks guidelines on what to do, how to do it, in what order, like a fully equipped kitchen without a cooking book. Plans to employ the tool for BMW vehicle software sparked off demand for an adequate, object-oriented real-time methodology. We show how to scan the methodology market in order to adopt an already existing method for this purpose. The result of the adaptation of a chosen method to ASCET SD is a pragmatic version of ROOM, which we call BROOM. We present a modeling guidebook that includes process recommendations not only for the automotive sector, but for real-time software development in general. The method suggests to produce early prototypes that are validated and refined to completion. BROOM offers phase-independent, harmonic guidelines. Product requirements, in form of scenarios, are transformed through several activities into operational models. BROOM takes advantage of ASCET's rich experimentationand code generation features. These allow to validate emerging models on button press. The factual development of a simplified heating/cooling system at BMW serves as a running example throughout the paper. INTRODUCTION The objective of this paper is to develop an objectoriented method for the CASE-tool ASCET SD [4] which is based on the ROOM-Method [8]. German carmanufacturers hold a high market share and enjoy reputation all over the world. However, fierce competition forces top players like BMW to shake off low cost manufacturers by creating innovative value to the customer. A case in point is electronic control of elementary but complex processes, such as motor control, brake control, acceleration control, and more. To keep vehicle margins on an attractive level, but to avoid imitation, control devices are built of standard hardware components with individual software. Thus emphasis lies on the development of reliable real-time software. Computer aided software engineering (CASE-) tools allow to develop software efficiently. The focus here is on a new CASE-tool called ASCET SD [4], which generates controller code from graphically specified components. Yet ASCET SD lacks suggestions on how to develop software. So there is a need to enrich ASCET SD with a methodology that offers guidelines on what to do, how to do it, in what order, and so on. As the tool itself fosters an object-oriented (OO-) approach, so should the methodology. The remaining part of the introduction declares the notions software engineering and object-orientation. Part two shows the development of a suitable method for ASCET SD. The core of this paper is part three, where we present our method with regards to an exemplary heating/cooling system. The last part is dedicated to concluding comments. SOFTWARE ENGINEERING. To develop software is mentally challenging due to its high complexity and immaterial existence. Problems emerge of human shortcomings, e.g. insufficient precision or the rare ability to penetrate complex facts. The scientific discussion to overcome these problems is called software engineering. The traditional software engineering process model suggests to organize software development by successive phases like a waterfall [7]: In the first phase, the software requirements are analyzed. Then the fundamental structure is designed by decomposition into easy to model components. These components are implemented and tested individually before being integrated and tested as a whole. The last phase is dedicated to the maintenance of the installed software. It is an accepted fact that this model has disadvantages, above all a missing feedback to prior phases. Especially errors in analysis and design can only be corrected at high cost and threaten the whole project if discovered not before test. In the course of time, improved or new models have emerged, among them prototyping [2] and the spiral model [1]. Both foster an incremental approach. This means creating a meager but executable prototype that reveals deficiencies at an early stage. This information is used to construct an improved prototype, which can be verified again, and so on. This evolutionary prototyping cycle is repeated until the software is finished or a shortage in time or money allows to deliver at least a version with limited capabilities. These considerations form a foundation for the new method for ASCET.	code generation (compiler);comment (computer programming);component-based software engineering;computer cooling;computer-aided software engineering;executable;feedback;high- and low-level;process modeling;prototype;real-time clock;real-time computing;real-time transcription;requirement;secure digital;software developer;software development process;software prototyping;software requirements;spiral model;switch;theory;waterfall model	Max Fuchs;Dieter Nazareth;Dirk Daniel;Bernhard Rumpe	1998	CoRR	10.4271/981014	simulation;computer science;engineering;component-based software engineering;software development;software engineering;programming language;object-oriented programming;management;engineering drawing;code generation	SE	-58.929630215894605	26.7056304340457	149787
c16e75bbbfed84168e2409ddcf3ac4ec2a9ff6fc	cost-effective combinatorial test case prioritization for varying combination weights.	test case prioritization;cost effectiveness		test case	Ziyuan Wang;Baowen Xu;Lin Chen;Zhenyu Chen	2010			reliability engineering;cost-effectiveness analysis;computer science;systems engineering;data mining;simple prioritization	AI	-61.70276313767544	30.497702135900862	149920
2b15c981c8b09c53841586c27cbd81403db3e303	coverage and conformance metrics: evaluation of legacy software for software product line asset development			conformance testing;legacy system;software product line	Hyesun Lee;Kyo Chul Kang	2010			software peer review;software design description;package development process;backporting;business;software quality analyst;reliability engineering;systems engineering;software product line;social software engineering;software sizing	SE	-62.74319507797177	26.552802068911898	150332
3f2755f7195533711211cdcfdad70527061d7403	validation, verification, and testing: diversity rules	project management;system testing software testing software systems aircraft printers research and development boundary conditions reliability engineering;functionality verification validation software project management reliability inspections execution based testing quality;program verification;software project management;software quality software development management project management software reliability program testing program verification;program testing;cost effectiveness;software reliability;software quality;software development management	Many software project managers try to decide whether to enhance reliability by performing detailed inspections or by doing execution-based testing using operational profiles. The authors regard this as a false choice. Operational-profile-based testing is an important method, but it is not a simple, cost-effective panacea. Instead, they suggest a better approach: a diverse validation, verification, and testing strategy that includes inspections and execution-based testing. Such an approach addresses the more appropriate question of what selection of W and T techniques should a project employ to achieve the functionality and quality that the product requires?.		Barbara A. Kitchenham;Stephen G. Linkman	1998	IEEE Software	10.1109/52.687944	non-regression testing;test strategy;project management;reliability engineering;development testing;verification and validation;regression testing;software performance testing;system integration testing;software verification;software project management;computer science;systems engineering;engineering;acceptance testing;package development process;software reliability testing;software development;software engineering;software construction;operational acceptance testing;software testing;software quality control;software quality;software quality analyst;software peer review	SE	-62.66539144569937	30.631684171456655	150545
57d33e42d956b4ed901684e5d9f62892fa29abe0	achieving principled assuredly trustworthy composable systems and networks	satisfiability;software engineering;software architecture;untrustworthiness principled assuredly trustworthy composable systems stringent security requirements trustworthiness chats project common sense approach trustworthy systems software engineering disciplines operational practices assurance measures commercial developers;engineering drawings computer architecture software engineering software measurement;common sense;software architecture security of data software reliability;software reliability;security of data	Huge challenges exist with systems and networks that must dependably satisfy stringent requirements for security, reliability, and other attributes of trustworthiness. Drawing on what we have learned over the past decades, our CHATS project seeks to establish a coherent common-sense approach toward trustworthy systems. The approach encompasses comprehensive sets of requirements, inherently sound architectures that can be predictably composed out of well-conceived subsystems, highly principled development techniques, good software engineering disciplines, sound operational practices, and judiciously applied assurance measures. Although such an approach is likely to seem completely old-hat to some researchers and totally impractical to commercial developers, the wisdom thus embodied is seldom used consistently (if at all) in practice; if it were used wisely, much of the untrustworthiness in today’s systems would simply disappear. This paper briefly summarizes our approach and its potential benefits.	coherence (physics);composability;embodied cognition;interaction;relevance;requirement;seamless3d;software engineering;trust (emotion);trustworthy computing	Peter G. Neumann	2003		10.1109/DISCEX.2003.1194962	software security assurance;verification and validation;computer science;systems engineering;social software engineering;software development;software engineering;software construction;resource-oriented architecture;computer security;software requirements;software system	SE	-61.10234531995573	27.782441235740336	151263
dff18b5c8f00890147065eb6b8b5a0bf3b34299a	software reliability assessment and optimal version-upgrade problem for open source software	distributed development;software maintenance;neural network fusion;software performance evaluation;software development process;optimal total version upgrade time;software maintenance software reliability assessment optimal version upgrade problem open source software neural network fusion software reliability growth model software development optimal total version upgrade time;public domain software;reliability assessment;open source software development;software development environment;software development;software component;software reliability assessment;optimal version upgrade problem;software reliability growth model;critical infrastructure;software reliability public domain software software development management software maintenance software performance evaluation;software reliability;network computing;software development management;software reliability open source software programming debugging computer networks neural networks software maintenance internet software development management network servers;open source software;neural network;open source	Network technologies have made rapid progress with the dissemination of computer systems in all areas. The current software development environment has been changing into new development paradigms such as concurrent distributed development environment and the so-called open source project by using network computing technologies. Especially, OSS (Open Source Software) systems which serve as key components of critical infrastructures in the society are still ever-expanding now. We focus on OSS developed under open source project. In case of considering the effect of the debugging process on an entire system in the development of a method of reliability assessment for open source project, it is necessary to grasp the deeply-intertwined factors, such as programming path, size of each component, skill of fault reporter, and so on. In order to consider the effect of each software component on the reliability of an entire system under such open source software development, we propose a new approach to software reliability assessment by creating a fusion of neural networks and a software reliability growth model. Also, it has been necessary to manage the software development process in terms of reliability, effort, and version-upgrade time. In this paper, we find the optimal total version-upgrade time based on the total expected software maintenance effort.	artificial neural network;component-based software engineering;debugging;integrated development environment;open sound system;open-source software;population dynamics;software development process;software maintenance;software quality;software reliability testing	Yoshinobu Tamura;Shigeru Yamada	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413582	personal software process;long-term support;verification and validation;software sizing;software project management;computer science;package development process;backporting;software reliability testing;software framework;component-based software engineering;software development;software design description;software construction;critical infrastructure;development environment;software walkthrough;software maintenance;software deployment;public domain software;goal-driven software development process;software development process;artificial neural network;software quality;software metric;software system;avionics software;software peer review	SE	-62.39285750800981	31.224865932668674	151626
92961174e5a605131f5f5f1f186020dc3f9fc894	nhpp-based software reliability model with marshall-olkin failure time distribution			software quality;software reliability testing	Xiao Xiao	2015	IEICE Transactions		real-time computing;computer science;failure rate;statistics	OS	-62.47689654358027	31.79054966781877	152056
b0f49a2737590edc42b741f860a19e7b52a54b38	improving component selection and monitoring with controlled experimentation and automated measurements	component selection;computacion informatica;tool support;software measurement;multi criteria decision making;data collection;trust management;grupo de excelencia;controlled experiment;commercial off the shelf;distributed environment;ciencias basicas y experimentales;software component;digital preservation;system development;component based software development;cots;empirical evaluation;software evaluation	Context: A number of approaches have been proposed for the general problem of software component evaluation and selection. Most approaches come from the field of Component-Based Software Development (CBSD), tackle the problem of Commercial-off-the-shelf component selection and use goal-oriented requirements modelling and multi-criteria decision making techniques. Evaluation of the suitability of components is carried out largely manually and partly relies on subjective judgement. However, in dynamic, distributed environments with high demands for transparent selection processes leading to trustworthy, auditable decisions, subjective judgements and vendor claims are not considered sufficient. Furthermore, continuous monitoring and re-evaluation of components after integration is sometimes needed. Objective: This paper describes how an evidence-based approach to component evaluation can improve repeatability and reproducibility of component selection under the following conditions: (1) Functional homogeneity of candidate components and (2) High number of components and selection problem	component-based software engineering;repeatability;requirement;selection algorithm;software development;trust (emotion)	Christoph Becker;Andreas Rauber	2010	Information & Software Technology	10.1016/j.infsof.2010.02.001	reliability engineering;computer science;systems engineering;engineering;component-based software engineering;software engineering;data mining;database;programming language	SE	-59.6800808395563	27.546094798389195	152123
a1324a527a75fdc43ce1cd4a64c860af0069155f	quality models to design software architecture	analysis and design;software systems;software development process;architectural design process;software architecture;general methods;quality model;architectural style	Quality requirements, captured in general as nonfunctional requirements in the early steps of software development, influence greatly the software system’s architecture. However, also the system’s core abstractions which are functional requirements, play an important role in the definition of the initial architecture. The importance of architectural design has grown rapidly in the last few years, since the need for reliable evolutionary systems and component-based development has increased. The goals of this work are to briefly discuss several architectural design approaches and to propose a systematic way of specifying the relevant quality attributes involved in the architectural design process. The evaluation of these attributes is the base of the architectural transformation process, allowing the incremental adaptation of the initial candidate architecture. This initial candidate, selected on some key functional requirements of the system, is adapted (transformed or refined) in the design process to fulfill the established quality goals. The SQUID (Software QUality In the Development process) approach, based on the standards ISO 9126-1 and ISO 145983, is used to define the quality model corresponding to the architecture and the development process model. These models will be constructed for Bosch architectural design method, which has been selected has a case study for offering very precise guidelines on the architecture transformation process. However, our approach could be easily integrated in other development process frameworks, like for example the Rational Unified Process, customizing the architectural construction process. We feel that the application of this approach is a step forward towards the systematization and improvement of the architectural design process, with built-in quality issues. This work has been developed as a result of the European Community INCO SQUAD Project EP 962019 and the Consejo de Desarrollo Científico y Humanístico (CDCH) of the Central University of Venezuela, ARCAS project 03.13.4584.00	canonical account;component-based software engineering;evolutionary systems;expectation propagation;functional requirement;iso/iec 9126;list of system quality attributes;non-functional requirement;process modeling;rational unified process;software architecture;software development;software quality;software system	Francisca Losavio	2002	Journal of Object Technology	10.5381/jot.2002.1.4.a4	multilayered architecture;reference architecture;software architecture;personal software process;computer architecture;architecture tradeoff analysis method;verification and validation;architectural pattern;systems engineering;package development process;software design;social software engineering;software development;software design description;software engineering;software construction;software architecture description;resource-oriented architecture;software deployment;goal-driven software development process;software development process;software quality;software system;software peer review	SE	-57.48847096292485	26.745644503027602	152300
5cc4013500da319af8d5744d7bd1296b9f9880b4	automated semantic analysis of design models	design model;human interaction;model analysis;model transformation;feature interaction;large scale;formal verification;test coverage;software product lines;semantic analysis	Based on several years of experience in generating code from large SDL and UML models in the telecommunications domain, it has become apparent that model analysis must be used to augment more traditional validation and testing techniques. While model correctness is extremely important, the difficulty of use and non-scalability of most formal verification techniques when applied to large-scale design models renders them insufficient for most applications. We have also repeatedly seen that even the most complete test coverage fails to find many problems. In contrast, sophisticated model analysis techniques can be applied without human interaction to large-scale models. A discussion of the model analysis techniques and the model defects that they can detect is provided, along with some real-world examples of defects that have been caught.	correctness (computer science);fault coverage;formal verification;rendering (computer graphics);scalability;simple directmedia layer;unified modeling language	Frank Weil;Brian E. Mastenbrook;David Nelson;Paul Dietz;Aswin van den Berg	2007		10.1007/978-3-540-75209-7_12	reliability engineering;interpersonal relationship;formal verification;computer science;systems engineering;data mining;code coverage;programming language	SE	-58.15180431077817	27.848923876606346	152323
17ab8403ab14bf881a13e9f7fcb1708d97848780	reuse through inheritance: a quantitative study of c++ software	software system;class depth;inheritance hierarchy;object-oriented programming;superb tool;actual use;quantitative study;parent class;actual usefulness;object oriented programming;software systems	According to proponents of object-oriented programming, inheritance is an excellent way to organize abstraction and a superb tool for reuse. Yet, few quantitative studies of the actual use of inheritance have been conducted. Quantitative studies are necessary to evaluate the actual usefulness of structures such as inheritance. We characterize the use of inheritance in 19 existing C++ software systems containing 2,744 classes. We measure the class depth in the inheritance hierarchies, and the number of child and parent classes in the software. We find that inheritance is used far less frequently than expected.	c++;software system	James M. Bieman;Josephine Xia Zhao	1995		10.1145/211782.211794	domain analysis;real-time computing;bridge pattern;computer science;systems engineering;engineering;software development;feature-oriented domain analysis;software engineering;domain engineering;class-based programming;composition over inheritance;programming language;object-oriented programming;software system	PL	-59.80623106373434	31.539252240261696	152328
5be2ab34795a049a3da6dbb877bfaffba6a66563	towards a model-based framework for integrating usability evaluation techniques in agile software model		Various new agile software models were offered though agile manifesto as a counteraction to conventional and extensive software techniques and process design. SE followed a systematic approach of development. Whereas integrating usability in software development improved the ability of software product to be used, learned and be attractive to the users. Research showed the benefit of usability; yet, to this day agile software model continues to exhibit less importance of this quality attribute. Moreover, poor usability and inefficient design were the common reasons in software product failure. The aim of this paper was to develop a model to integrate usability evaluation methods into agile software model. This was done by proposing a unique model and evaluate the proposed model by using IEEE Std 12207-2008, ISO 9241:210.	agile software development;usability	Saad Masood Butt;Azura Onn;Moaz Masood Butt;Nadra Tabassam	2014		10.1007/978-3-319-07692-8_53	reliability engineering;component-based usability testing;usability;agile unified process;agile usability engineering;systems engineering;software engineering;usability engineering;empirical process;lean software development;heuristic evaluation	Robotics	-60.41561644274734	26.449073719654784	152424
810fda892cf754836f7de7c25d47d1a67a8bdade	finding optimal solution for satisficing non-functional requirements via 0-1 programming	analytical models;software;graph theory;optimized production technology;software optimized production technology logic gates programming profession buildings analytical models;0 1 programming problem non functional requirements quality factors design design;non functional requirements;software quality graph theory mathematical programming;logic gates;mathematical programming;programming profession;quality factors;software functions nonfunctional requirements satisfaction 0 1 programming nfr software quality design decision;design design;software quality;buildings;0 1 programming problem	On-Functional Requirements (NFRs) are vital for the success of software systems. Generally speaking, NFRs are some implicit expectations about how well the software will work, often known as software quality. For building better software, the NFRs should be considered as criteria for design decision. However, different NFRs may produce different criteria on the implementation strategies of the software functions. A trade-off analysis is needed for getting an optimal plan during design decision to satisfice NFRs as well as possible. By focusing on the NFRs that can be quantitatively specified, this paper proposes an approach to finding such an optimal solution for helping to make better decision. This approach regards the NFRs as the constraints on the implementation strategies of the software functions and models the selection of implementation strategies as a 0-1 programming problem. Then, a 0-1 programming solver can be used to find the optimal solution. An example is given to demonstrate the feasibility of this approach.	functional requirement;non-functional requirement;problem frames approach;software quality;software system;solver	Bin Yin;Zhi Jin;Wei Zhang;Haiyan Zhao;Bo Wei	2013	2013 IEEE 37th Annual Computer Software and Applications Conference	10.1109/COMPSAC.2013.69	logic gate;computer science;systems engineering;graph theory;theoretical computer science;software engineering;database;management science;programming language;non-functional requirement;software quality	SE	-59.928814895758684	28.84988466452926	152535
49ee327518883334fb08572321a482a6da326f90	quality assurance of test specifications for reactive systems	doctoralthesis	Extensive testing of modern communicating systems often involves large and complex test suites that need to be maintained throughout the life cycle of the tested systems. For this purpose, quality assurance of test suites is an inevitable task that eventually may have an impact on the quality of the system under test as well. In this thesis, we present a holistic method towards the analytical quality engineering of test specifications. We cover in detail what constitutes the quality of test specifications by adapting a quality model for software to test specifications and present how to apply targetoriented static testing to test specifications. We also introduce a dynamic testing method for test specifications, including a reverse engineering approach for test behavior models, and present a method for the consistency analysis of system responses in test suites. Based on the quality assessments made, the test suites can be improved regarding specific quality characteristics of this quality model. Finally, we validate and demonstrate the applicability of our approaches in a case study by means of a prototype implementation.	dynamic testing;holism;prototype;quality engineering;reverse engineering;static program analysis;system under test;test suite	Benjamin Immanuel Eberhardt Elmar Zeiß	2010			reliability engineering;ongoing reliability test;simulation;systems engineering;engineering;functional testing;test suite;system under test;test method;test case;test management approach	SE	-57.69278673721245	30.790774674854358	152575
45cc419d363f640cf174e4040b341b20d928c8ae	automatic generation of just-in-time online assessments from software design models	engineering;rate of change;technical occupations;computer system design;web service;software engineering;automatic generation;online systems;online assessment;software industry;just in time;evaluation;computer software;software design;database management system;statistical estimation;models;new products;automation	Computer software is pervasive in today’s society. The rate at which new versions of computer software products are released is phenomenal when compared to the release rate of new products in traditional industries such as aircraft building. This rapid rate of change can partially explain why most certifications in the software industry are generic as opposed to those in the aircraft-building industry where engineers and technicians are certified to work on a specific aircraft. For example, a software engineer may be certified on a database management system, but not on a specific implementation based on the database management system. Hence, software engineers are allowed to make critical changes to specific designs for the next release of a software product with little formal assessment of their understanding of the design. This paper presents a system that automatically generates just-in-time online assessments for judging a software engineer’s comprehension of artifacts representing software designs. The assessments thus generated are compliant with the IMS-QTI 2.1 standard. The system is based on the AXIS web-services architecture and provides a priori statistical estimates of effectiveness of each individually generated assessment.	activity diagram;arithmetical hierarchy;artifact (software development);bloom;blueprint;business process;computer-aided design;database;distributed computing;domain model;engineering design process;experiment;heuristic (computer science);high- and low-level;information management system (ims);just-in-time compilation;pervasive informatics;qti;software design;software engineer;software industry;unified modeling language	Imran A. Zualkernan;Salim Abou El-Naaj;Maria Papadopoulos;Budoor K. Al-Amoudi;Charles E. Matthews	2009	Educational Technology & Society		web service;personal software process;medical software;long-term support;verification and validation;software quality management;software engineering process group;software sizing;computer science;systems engineering;engineering;electrical engineering;package development process;backporting;software design;social software engineering;software framework;software development;software design description;evaluation;operating system;automation;software engineering;online assessment;software construction;software walkthrough;software measurement;law;software deployment;world wide web;software metric;software system;computer engineering;software peer review	SE	-59.29010264910518	27.863023274491926	152651
f88f61078f6c1a8a8852abce9f3239eee482ba2a	determination of the optimal allocation of testing resource for modular software reliability growth using lingo		It is quite natural to produce reliable software systems since the breakdown of the computer systems, which is caused by software faults, results in a tremendous loss and damage for social life. Hence, software reliability is a key factor in software development process. Testing phase of software begins with module testing whereby, modules are tested independently to remove substantial amount of faults within a specified testing resource. Therefore, in this paper four optimization problems are discussed for optimal allocation of testing resources for a modular software system. These optimization problems are formulated as nonlinear programming problems (NLPP), which incorporates log-logistic testing-effort function into inflection S-shaped software reliability growth model based on a non-homogeneous Poisson process. The NLPPs are solved using LINGO, a user-friendly software package for optimization. Finally, numerical example is given to demonstrate and to validate the performance of proposed strategies. In addition, the results of the strategies proposed in this paper are compared with [1]. It is revealed that the proposed strategies yield a gain in efficiency.	lingo (programming language);mathematical optimization;modular programming;nonlinear programming;nonlinear system;numerical analysis;population dynamics;software development process;software quality;software reliability testing;software system;usability	Nesar Ahmad;Mohammed G. M. Khan	2016	JSW	10.17706/jsw.11.7.664-676	computer science	SE	-62.227513328510916	31.595169984816675	152775
856b394a7b1fa9475816b3f3143e5c4e608ec64b	tailored responsibility within component-based systems	developpement logiciel;componente logicial;component based systems;composant logiciel;endommagement;deterioracion;desarrollo logicial;software development;software component;damaging	The concept of responsibility aims at making a computing system trustworthy for its users despite the fact that failures of IT systems cannot be completely excluded. The presented concept comprises the following issues: In case of failures, the responsible stakeholder can be identified and it is ensured that this stakeholder is willing to compensate arisen loss. This enables users to claim damages. Until now, responsibility in this sense is not considered in practical systems. We especially investigate possibilities for integration of responsibility into component-based systems whereas the interests of all involved stakeholders should be considered. The newly introduced concept of tailored responsibility enables users to pose flexible demands for responsibility.	component-based software engineering	Elke Franz;Ute Schiffel	2005		10.1007/11424529_14	computer science;component-based software engineering;software development;programming language;computer security	Embedded	-59.72695862282223	27.73729779335419	153608
e2160d5ec5bf92c442818e71b79358a26f13f8c7	analyzing time-to-market and reliability trade-offs with bayesian belief networks	developpement logiciel;belief networks;cycle time;reseau croyance;system reliability;fiabilidad;reliability;markets;red www;mercado;web engineering;reseau web;web interface;software systems;development process;satisfiability;software engineering;non functional requirement;reseau bayes;red bayes;desarrollo logicial;bayesian belief network;fiabilite;marche;software development;bayes network;web based system;world wide web;time to market;everyday life;web development	Web-based systems are software systems with a Web interface. In recent years it has successfully penetrated into our everyday life. Development of such systems has also opened a new area in software engineering: Web engineering. Many studies found that Web development often experiences much shorter cycle time than traditional software development. It conflicts with some non-functional requirements, such as system reliability. When they can not be simultaneously satisfied, trade-off decisions must be made. Such decisions are typically difficult in software development due to the complexity and uncertainty of the development process. They are usually decided in an ad hoc manner. The purpose of this paper is to propose a new methodology for conducting trade-off analysis. Bayesian Belief Networks (BBNs) is used to offer this opportunity. By capturing and modelling the cause-effect relationships between time and reliability in BBN models, it provides a systematic way for doing comprehensive trade-off analysis throughout the whole development process.	bayesian network;functional requirement;hoc (programming language);non-functional requirement;software development;software engineering;software system;web development;web engineering	Jianyun Zhou;Tor Stålhane	2005		10.1007/11531371_85	simulation;computer science;artificial intelligence;software development;operating system;software engineering;machine learning;bayesian network;data mining;database;distributed computing;systems development life cycle;web engineering;computer security;algorithm;statistics	SE	-61.06494432528506	28.85592577339382	153930
7a76e02bd2b26d62e6a0352d146de449f49cdd74	a market-based approach to software evolution	markets;software complexity;software development process;software correctness;market design;conference paper;automated verification;software evolution;profitability;mechanism design	"""Software correctness has bedeviled the field of computer science since its inception. Software complexity has increased far more quickly than our ability to control it, reaching sizes that are many orders of magnitude beyond the reach of formal or automated verification techniques.  We propose a new paradigm for evaluating """"correctness"""" based on a rich market ecosystem in which coalitions of users bid for features and fixes. Developers, testers, bug reporters, and analysts share in the rewards for responding to those bids. In fact, we suggest that the entire software development process can be driven by a disintermediated market-based mechanism driven by the desires of users and the capabilities of developers.  The abstract, unspecifiable, and unknowable notion of absolute correctness is then replaced by quantifiable notions of correctness demand (the sum of bids for bugs) and correctness potential (the sum of the available profit for fixing those bugs). We then sketch the components of a market design intended to identify bugs, elicit demand for fixing bugs, and source workers for fixing bugs. The ultimate goal is to achieve a more appropriate notion of correctness, in which market forces drive software towards a correctness equilibrium in which all bugs for which there is enough value, and with low enough cost to fix, are fixed."""	computer science;correctness (computer science);darknet market;disintermediation;ecosystem;programming complexity;programming paradigm;software bug;software development process;software evolution	David F. Bacon;Yiling Chen;David C. Parkes;Malvika Rao	2009		10.1145/1639950.1640066	mechanism design;verification and validation;real-time computing;computer science;package development process;software development;software rot;software construction;programming language;computer security	SE	-61.63839417336709	27.25627484802973	154031
5a8a845d9a0aa317feefd5e34eda33081f7d66c2	automated test case selection using feature model: an industrial case study	component family model;test case selection;product line;feature model	Automated test case selection for a new product in a product line is challenging due to several reasons. First, the variability within the product line needs to be captured in a systematic way; second, the reusable test cases from the repository are required to be identified for testing a new product. The objective of such automated process is to reduce the overall effort for selection e.g., selection time, while achieving an acceptable level of the coverage of testing functionalities. In this paper, we propose a systematic and automated methodology using a Feature Model for Testing FM_T to capture commonalities and variabilities of a product line and a Component Family Model for Testing CFM_T to capture the overall structure of test cases in the repository. With our methodology, a test engineer does not need to manually go through the repository to select a relevant set of test cases for a new product. Instead, a test engineer only needs to select a set of relevant features using FM_T at a higher level of abstraction for a product and a set of relevant test cases will be selected automatically. We applied our methodology to a product line of video conferencing systems called Saturn developed by Cisco and the results show that our methodology can reduce the selection effort significantly. Moreover, we conducted a questionnaire-based study to solicit the views of test engineers who were involved in developing FM_T and CFM_T. The results show that test engineers are positive about adapting our methodology and models FM_T and CFM_T in their current practice.	feature model;test case	Shuai Wang;Arnaud Gotlieb;Shaukat Ali;Marius Liaaen	2013		10.1007/978-3-642-41533-3_15	reliability engineering;simulation;systems engineering;engineering;software engineering;feature model	SE	-57.7490661886841	28.455906194231517	154357
1cc36ba846c07a7fe4bf4d4c39e2e250d4b9dba7	tailoring test process by using the component-based development paradigm and the xml technology	hypermedia markup languages;extensible markup language;computer aided software engineering program testing software process improvement software standards subroutines iso standards iec standards hypermedia markup languages software tools;empirical study;component based development customization technique;software process improvement;standards organizations;iso;component based development paradigm;rational objectory process;iso standards;component based development;automatic testing;software processes;international electrotechnical commission;test process meta model;iec;automatic generation;standards development;computer aided software engineering;extensible markup language software test process customization component based development paradigm xml technology iso iec software products software processes project specific standards test process meta model process tailoring scheme component based development customization technique autotp automation tool rational objectory process;iec standards;program testing;software products;process tailoring scheme;automation tool;international standards organization;xml technology;xml;software test process customization;software standards;software tools;xml standards development automatic testing component architectures iso standards iec standards software standards computer science standards organizations automation;computer science;autotp;meta model;subroutines;component architectures;project specific standards;automation	"""ISO (International Standardization Organization) and the IEC (International Electrotechnical Commission) provide numerous standards for software products and processes. Utilizing those standards specific to a project requires some tailoring to meet the development domain. This paper includes: (1) a test process meta-model, which is a test process defined in standards; (2) """"a scheme for tailoring processes"""" following a component-based development customization technique; and (3) AutoTP, an """"automation tool for tailoring"""", which is derived from XML techniques. AutoTP is a tool in which a standard test process automatically generates a tailored test process by means of a methodology and a domain. In addition, the Rational Objectory process is used as an empirical study in this paper."""		Jooyoung Seo;Byoungju Choi	2000		10.1109/APSEC.2000.896720	xml;computer science;systems engineering;software engineering;programming language;computer engineering	SE	-58.783337416667244	27.91444241103012	154437
03c1723c3cf881acfe7a28777c3cf08d7f6ac7cc	towards an integrated approach for validating qualities of self-adaptive systems	runtime analysis;integrated approach;software systems;software engineering;formal method;model checking;adaptive system;model based testing;software life cycle;programvaruteknik;software quality	Self-adaptation has been widely recognized as an effective approach to deal with the increasing complexity and dynamicity of modern software systems. One major challenge in self-adaptive systems is to provide guarantees about the required runtime qualities, such as performance and reliability. Existing research employs formal methods either to provide guarantees about the design of a self-adaptive systems, or to perform runtime analysis supporting adaptations for particular quality goals. Yet, work products of formalization are not exploited over different phases of the software life cycle. In this position paper, we argue for an integrated formally founded approach to validate the required software qualities of self-adaptive systems. This approach integrates three activities: (1) model checking of the behavior of a self-adaptive system during design, (2) model-based testing of the concrete implementation during development, and (3) runtime diagnosis after system deployment. We illustrate the approach with excerpts of an initial study and discuss for each activity research challenges ahead.	adaptive system;analysis of algorithms;formal methods;model checking;model-based testing;software deployment;software release life cycle;software system;system deployment	Danny Weyns	2012		10.1145/2338966.2336803	reliability engineering;software visualization;verification and validation;real-time computing;software sizing;system integration testing;computer science;systems engineering;package development process;software design;software reliability testing;component-based software engineering;software development;software design description;software construction;software testing;runtime verification;systems development life cycle;software deployment;software development process;software quality;software quality analyst;software system	SE	-57.72580686454768	28.22802143746613	154462
783fbd0beebf217028431749b44eee2ff6abb46a	constructing a reading guide for software product audits	software product audit;software metrics;user manuals software architecture software metrics software quality system documentation;software product architecture;software product quality evaluation;user manuals;system documentation;reading guide construction;documentation software quality iso standards iec standards software performance software architecture application software computer science project management software standards;software architecture;levels of abstraction;software product documentation;latent semantic analysis reading guide construction software product audit software product architecture software product quality evaluation software product documentation;latent semantic analysis;software quality	Architectural knowledge is reflected in various artifacts of a software product. In the case of a software product audit this architectural knowledge needs to be uncovered and its effects assessed, in order to evaluate the quality of the software product. A particular problem is to find and comprehend the architectural knowledge that resides in the software product documentation. The amount of documents, and the differences in for instance target audience and level of abstraction, make it a difficult job for the auditors to find their way through the documentation. This paper discusses how the use of a technique called latent semantic analysis can guide the auditors through the documentation to the architectural knowledge they need. Using latent semantic analysis, we effectively construct a reading guide for software product audits.	documentation;international federation for information processing;latent semantic analysis;software architecture;software engineering	Remco C. de Boer;Hans van Vliet	2007	2007 Working IEEE/IFIP Conference on Software Architecture (WICSA'07)	10.1109/WICSA.2007.20	software architecture;personal software process;medical software;verification and validation;software quality management;latent semantic analysis;computer science;systems engineering;engineering;package development process;software design;social software engineering;software development;software design description;software engineering;software construction;database;software walkthrough;software documentation;resource-oriented architecture;software measurement;software deployment;software quality control;software quality;software metric;software quality analyst;software system;software peer review	SE	-59.87227235920538	28.504164857334708	154715
5ae16a00c0f457081aac3879cc1a6162910daaa4	a study on the software test case reuse model of feature oriented	analytical models;analytical models programmable logic arrays;test case reuse model features test case reuse;programmable logic arrays;test term sand cases software test case reuse model feature oriented design methods model reuse index test data body test protocol body execution body test assets;software reusability program testing software architecture	In this paper, a test case reuse model, Test Case Reuse Model , is proposed as a solution to the normative, effectiveness and efficiency issues for the design of test cases for software. Test Case Reuse Model is a refinery, extraction and expression of design methods of test cases in test case sets with similar features of test requirements. Test Case Reuse Model consists of five components, which are the model reuse index, the basic information for model reuse, the test data body, the test protocol body and the execution body. Test Case Reuse Model is developed by testing expert based on test assets such as test term sand cases in the library. Based on Test Case Reuse Model , not only are testers able to increase the efficiency of the design for test terms and cases with the same features, but also can guarantee the normativeness and effectiveness of test design.	design for testing;requirement;software testing;test case;test data;test design	Wenwu Li;Miyi Duan	2014	2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2014.7175736	real-time computing;test suite;system under test;test method;test case;test management approach;test harness	SE	-56.61077424110576	29.012878446020576	154829
0d69c139b0b882e785debe80d8762e0f2738b0a2	web service testing tools: a comparative study		Quality of Service (QoS) has gained more importance with the increase in usage and adoption of web services. In recent years, various tools and techniques developed for measurement and evaluation of QoS of web services. There are commercial as well as open-source tools available today which are being used for monitoring and testing QoS for web services. These tools facilitate in QoS measurement and analysis and are helpful in evaluation of service performance in real-time network. In this paper, we describe three popular open-source tools and compare them in terms of features, usability, performance, and software requirements. Results of the comparison will help in adoption and usage of these tools, and also promote development and usage of open-source web service testing tools.	computer performance;information;open-source software;quality of service;real-time clock;real-time computing;requirement;response time (technology);software requirements;throughput;usability;web service	Shariq Hussain;Zhaoshun Wang;Ibrahima Kalil Toure;Abdoulaye Diop	2013	CoRR		web modeling;mobile qos;computer science;systems engineering;ws-policy;multimedia;world wide web	Metrics	-60.16191447526636	26.546587831468422	156035
658a884e00fee2084a3c85dd798cedf19f7d4aa3	exploration on integrated development environment of trusted data processing oriented software product line for web application	software;security of data object oriented programming;domain engineering;information science;software evaluation integrated development environment trusted data processing oriented software product line web application dpospl trusted software technology evidence based trustworthy software guarantee mechanism;data processing;object oriented programming;architecture description languages;trusted software technology;integrated development environment;web application;dpospl;software data processing data models production space technology information science architecture description languages;guarantee mechanism;production;space technology;integrated develop environment trustworthy software software product line domain engineering;integrated develop environment;software product line;security of data;evidence based trustworthy software;trusted data processing oriented software product line;data models;trustworthy software;software evaluation	The focus of this paper is on Data Processing-Oriented Software Product Line (DPOSPL) and trusted software technology and theory, among which evidence-based trustworthy software guarantee mechanism, and develop an integrated environment for web application of DPOSPL——DPOWSPL IDE. To ensure that this IDE be highly trustworthy, as defined in literature, it uses evidence-based software evaluation to guarantee its system trustworthiness. Finally, a concrete application of this IDE is presented.	comparison of command shells;integrated development environment;software product line;trust (emotion);web application;world wide web	Hongwen Huang;Xinyu Zhang;Li Zheng;Shulan Zhao	2010	2010 7th International Conference on Ubiquitous Intelligence & Computing and 7th International Conference on Autonomic & Trusted Computing	10.1109/UIC-ATC.2010.64	data modeling;hackystat;long-term support;verification and validation;web application;real-time computing;software sizing;data processing;information science;computer science;package development process;social software engineering;software framework;software development;domain engineering;software construction;database;space technology;software walkthrough;object-oriented programming;software deployment;software system;software peer review	SE	-56.21631871545275	30.584494126967048	156074
21ac944da2bfe87da8c73b80d90db6838236e05c	intelligent software methodologies, tools and techniques		Mobile devices changed human-computer interaction, caused the need for specialized software engineering methods and created new business opportunities. The mobile app market is highly competitive and software developers need to maintain high software quality standards for long-lasting economic success. While powerful software development kits support developers in creating mobile applications, testing them is still cumbersome, time-consuming and error-prone. Especially interaction methods depending on sensor input like device motion gestures prevent automated UI testing – developers and testers are forced to manually test all different aspects. We present an approach to integrate sensor information into user acceptance tests and use a sensor simulation engine to enable automatic test case execution for mobile applications.	acceptance testing;cognitive dimensions of notations;human–computer interaction;mobile app;simulation;software developer;software development kit;software engineering;software quality;test case;usability testing;user interface	Hamido Fujita;Guido Guizzi	2015		10.1007/978-3-319-22689-7	software construction;computer-aided software engineering	SE	-58.803817880877254	28.81564671090848	156130
6473b9f079b28e16ee8081064f960e875283d6a1	mature: a model driven based tool to automatically generate a language that supports cmmi process areas specification	informatica;software tool;user needs;domain specific modeling;automatic generation;model driven development;capability maturity model integration;assessment methods;domain specific language;process improvement	Many companies have achieved a higher quality in their processes by using CMMI. Process definition may be efficiently supported by software tools. A higher automation level will make process improvement and assessment activities easier to be adapted to customer needs. At present, automation of CMMI is based on tools that support practice definition in a textual way. These tools are often enhanced spreadsheets. In this paper, following the Model Driven Development paradigm (MDD), a tool that supports automatic generation of a language that can be used to specify process areas practices is presented. The generation is performed from a metamodel that represents CMMI. This tool, differently from others available, can be customized according to user needs. Guidelines to specify the CMMI metamodel are also provided. The paper also shows how this approach can support other assessment methods.	automatic programming;capability maturity model integration;correctness (computer science);definition;documentation;domain-specific language;embedded system;emoticon;graphical user interface;iso/iec 42010;meta-object facility;metamodeling;model-driven engineering;programming paradigm;software development;spatial variability;spreadsheet	David Musat;Víctor Castaño;José Antonio Calvo-Manzano;Juan Garbajosa	2010		10.1007/978-3-642-15666-3_5	reliability engineering;leancmmi;systems engineering;engineering;software engineering	SE	-58.90864885966152	27.98380944260663	156612
68c1a1257afdba8b7d811dff5d682dc63547518d	combining agile development and software product lines in automotive: challenges and recommendations		Software product lines (SPLs) are used throughout the automotive industry. SPLs help to manage the large number of variants and to improve quality by reuse. In order to develop high quality software faster, agile software development (ASD) practices are introduced. From both the research and the management point of view it is still not clear how these two approaches can be combined. We derive recommendations to combine ASD and SPLs based on challenges identified for an automotive-specific model. This study combines the outcome of a literature review and a qualitative interview study with 16 practitioners from the automotive domain. We evaluate the results and analyze the relationship between ASD and SPLs in the automotive domain. Furthermore, we derive recommendations to combine ASD and SPLs based on challenges identified in the automotive domain. This study identifies 86 individual challenges. Important challenges address supplier collaboration and faster software release cycles without loss of quality. The identified challenges and the derived recommendations show that the combination of ASD and SPL in the automotive industry is promising but not trivial. There is a need for an automotive-specific approach that combines ASD and SPL.	agile software development;component-based software engineering;display resolution;iso/iec 15504;point of view (computer hardware company);recommender system;reference model;requirement;spice 2;software product line;software release life cycle;spl (unix)	Philipp Hohl;Michael Stupperich;Jürgen Münch;Kurt Schneider	2018	2018 IEEE International Conference on Engineering, Technology and Innovation (ICE/ITMC)	10.1109/ICE.2018.8436277	software;reuse;systems engineering;embedded software;software release life cycle;agile software development;automotive industry;engineering	SE	-61.39072718625948	26.0133017947757	157456
9d4d77bb3687098a8595fe1bc93b6a893cf5f0be	software metrics decision support system	software metrics;multiattribute utility theory;project management;software measurement;software prototyping;prototypes;constraint equation;smdss methodology;constraint equation software metrics decision support system smdss methodology quantifiable software project goals management proof of concept tool maus multi attribute utility theory quantification method;proof of concept tool;proof of concept;software metrics decision support system;decision support system;maus;software metrics decision support systems project management software development management software prototyping prototypes utility theory decision making design methodology software design;decision support systems;software metric;software tools;software development management software metrics decision support systems software tools;multi attribute utility theory;software design;management;system management;software development management;quantifiable software project goals;software production function;quantification method;utility theory;design methodology	"""The software metrics decision support system (SMDSS) methodology was developed to address the problem of selecting the """"best"""" set of values for quantifiable software project goals commensurate with various constraints and management's desires. The methodology was prototyped in a proof-of-concept tool, called MAUS, which uses multi-attribute utility theory (MAU). The MAU-based decision-making methodology is designed to aid software and systems managers in the use of a systematic, objective, and documentable quantification method. MAUS implements some novel aspects and extensions of the MAU methodology. MAUS uses MAU to explore a """"space"""" of possible choices, and it utilizes a constraint equation to generate alternatives. The SMDSS methodology and the supportive MAUS tool promote implementation of the guidance provided in Practical Software Measurement: A Guide to Objective Program Insight."""	decision support system;software metric	Jacob W. Ulvila;James O. Chinnis;John E. Gaffney	1998		10.1109/METRIC.1998.731245	reliability engineering;decision support system;computer science;systems engineering;engineering;software engineering;data mining;management;software metric	Robotics	-60.3547044101281	27.99486849280481	158645
8b013c37e7dc23c670b3daf5f178e4792c689f23	model-driven design of object and component systems		The notion of software engineering implies that software design and production should be based on the types of theoretical foundations and practical disciplines that are established in the traditional branches of engineering. The goal is to make development of complex software systems more predictable and the systems developed more trustworthy - safe, secure and dependable. A number of theories have been well developed in the past half a century, including Abstract Data Types, Hoare Logic, Process Calculi, and I/O automata, and those alike. Based on them, techniques and tools have been developed for software specification, refinement and verification.	model-driven architecture;model-driven engineering	Zhiming Liu;Xiaohong Chen	2014		10.1007/978-3-319-29628-9_4	method;object model;idef4;object-oriented design;component	EDA	-58.119267473578276	26.1593917227025	158715
d1c97169c8dc4f469f7fd600fd874b4fcc6940c2	constructing feature model by identifying variability-aware modules		Modeling variability, known as building feature models, should be an essential step in the whole process of product line development, maintenance and testing. The work on feature model recovery serves as a foundation and further contributes to product line development and variability-aware analysis. Different from the architecture recovery process even though they somewhat share the same process, the variability is not considered in all architecture recovery techniques. In this paper, we proposed a feature model recovery technique VMS, which gives a variability-aware analysis on the program and further constructs modules for feature model mining. With our work, we bring the variability information into architecture and build the feature model directly from the source base. Our experimental results suggest that our approach performs competitively and outperforms six other representative approaches for architecture recovery.	feature model;heart rate variability;spatial variability	Yutian Tang;Hareton K. N. Leung	2017	2017 IEEE/ACM 25th International Conference on Program Comprehension (ICPC)		data mining;feature model;architecture;computer science	SE	-57.84905023114415	28.431336352977876	158737
755d943664b05d9919c533193daa9ddce46ddd1c	un panorama du test à partir de modèles formels	developpement logiciel;keyword;exigence;test automation;analyse fonctionnelle;logiciel a securite critique;metodo formal;methode formelle;automatisation;palabra clave;requirement;mot cle;automatizacion;formal method;for 0803 computer software;detection defaut;functional analysis;desarrollo logicial;testing tools;safety critical software;software development;exigencia;model based testing;information system;deteccion imperfeccion;systeme information;defect detection;sistema informacion;analisis funcional;automation	This article gives an overview of model-based testing, showing how it is an evolution of earlier approaches to automating the testing process, such as capture/replay tools, test scripting and keyword-driven testing. The advantages and disadvantages of model-based testing are discussed, as well as some of the experimental evidence which shows that model-based testing can be a cost-effective technique for detecting faults in critical software and improving the requirements of such software.; Language: French		Mark Utting	2006	Technique et Science Informatiques	10.3166/tsi.25.133-139	functional analysis;model-based testing;computer science;artificial intelligence;software development;requirement;automation;information system;algorithm	Logic	-58.84105767209316	32.31052935902244	158967
1db74b85b74d63317d9763300e14a007c9bec1d0	a study of automatic code generation for safety-critical software: preliminary report	automatic code generation;quality assurance;delivery system;application software;automated code generation;real time;analysis and design;pacemakers;development process;software engineering;deliberator devices;dependable systems;acg;software safety;model based software engineering;complex system;safety critical software;unified modeling language;mbse automatic code generation acg safety critical software combined pacemaker deliberator devices distributed patient therapy delivery systems quality assurance model based software engineering;safety critical system;mathematical model;distributed patient therapy delivery systems;program compilers safety critical software software engineering;medical treatment;program compilers;alarm systems;switches;development time;software safety mathematical model application software unified modeling language timing alarm systems switches pacemakers medical treatment quality assurance;mbse;combined pacemaker;timing	Modern safety-critical systems (e.g., combined pacemaker/deliberator devices, distributed patient therapy delivery systems) incorporate more functionality than similar devices of the past. The development of these complex systems challenges existing quality assurance techniques; results in significantly longer development times; and demands greater staffing resources to ensure quality and timely product completion. This is an interim report on a case study of the efficacy and viability of automatic code generation (ACG) techniques applied in the development of real-time, safety-critical software-dependent systems (Whalen, 1997). The research uses model-based software engineering (MBSE) practices that incorporate integrated analysis and design iterations throughout the development process. The focus of these investigations is the application of automated code generation tools that embody various methodologies, in the development of safety critical systems. There was no attempt to embark on explicit tool comparisons or evaluations.	artificial cardiac pacemaker;automatic programming;code generation (compiler);complex systems;iteration;real-time clock;software engineering	Lazar Crawford;Jared Erwin;Steafano Grimaldi;Soma Mitra;Andrew J. Kornecki;David P. Gluch	2004	Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.	10.1109/HASE.2004.1281765	reliability engineering;unified modeling language;quality assurance;application software;network switch;computer science;systems engineering;operating system;software engineering;mathematical model;programming language;computer security;software development process	SE	-61.89320481137374	29.512719365297485	160318
24005fafdc0e61ec36f1cc9f0cdeb6bc26d6d180	measuring uml models using metrics defined in ocl within the squam framework	uml metrics;ocl specification;model analysis;ocl pragmatic extensions;ocl development process	In software engineering practice, measurements may reduce development costs by improving processes and products at early stages. In model driven approaches, measurements can be conducted right from the start of a project. For UML models, a collection of metrics has been empirically validated, however, these need to be precisely defined in order to be useful. Definition of UML metrics in OCL offers a high degree of precision and portability, but due to shortcomings of this language this approach is not widespread. We propose the SQUAM framework, a tool-supported methodology to develop OCL specifications, which incorporates best practices in software development, such as libraries, testing and documentation. As a proof of concept we have developed 26 metrics for UML class diagrams in the academic context. This demonstrated the high effectiveness of our approach: quick learning, high satisfaction of developers, low imposed complexity and potential time reduction through reuse.	best practice;class diagram;documentation;library (computing);object constraint language;software development;software engineering;software portability;unified modeling language	Joanna Chimiak-Opoka	2011		10.1007/978-3-642-24485-8_5	reliability engineering;uml tool;computer science;systems engineering;applications of uml;database;object constraint language	SE	-59.16054287646041	26.53240280410702	160370
364b02a5fae339e1d6d7bbd0e8e979a0caa2da7b	an approach to analyzing the software process change impact using process slicing and simulation	project manager;simulation;quantitative analysis;change process;process model;change impact analysis;simulation model;software process;process slicing;active control	When a software process is changed, a project manager needs to perform two types of change impact analysis activities: one for identifying the affected elements of a software process which is affected by the change and the other for analyzing the quantitative impact of the change on the project performance. We propose an approach to obtain the affected elements of a software process using process slicing and developing a simulation model based on the affected elements to quantitatively analyzing the change using simulation. We suggest process slicing to obtain the elements affected by the change. Process slicing identifies the affected elements of a software process using a process dependency model. The hange impact analysis	simulation;software development process	Seunghun Park;Doo-Hwan Bae	2011	Journal of Systems and Software	10.1016/j.jss.2010.11.919	real-time computing;simulation;computer science;systems engineering;quantitative analysis;software engineering;simulation modeling;process modeling;goal-driven software development process;software development process;change impact analysis	SE	-62.6491534069249	28.716960367659123	160518
41f5164d12b808b2ec68119acc4e5324d83e342b	why reuse matters: ani's digital archive system	digital archive;extreme programming;object oriented systems;commercial off the shelf;agile methodologies;iso 9001 compliance;document management system	This practitioner report stems from a project, ANI's Digital Archive System. The report focuses on the benefits derived from a decision to ignore the popular wisdom of acquiring one of the many commercial-off-the-shelf (COTS) systems and build a document management system in-house instead. The approach, building on the existing set of object-oriented systems, provided the benefits of more specific features and total lower cost.	ani (file format);archive;requirement	Daniel Antion	2004		10.1145/1028664.1028769	simulation;extreme programming;document management system;agile software development;database	HCI	-62.04211601410829	26.089667567893223	160835
0fcbb14cf1d8a17e143dd0ba5e836b7dd09bb407	recovering architectural variability of a family of product variants	product variants;architecture variability;object oriented;product line architecture;architec ture recovery;source code;reverse engineering	A Software Product Line (SPL) aims at applying a preplanned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPL is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common functionalities and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a SPL. Nevertheless, these contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose, in this paper, an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants at the architectural level. Our work relies on Formal Concept Analysis (FCA) to analyze the variability. To validate the proposed approach, we experimented on two families of open-source product variants; Mobile Media and Health Watcher. The results show that our approach is able to identify the architectural variability and the dependencies.	business domain;formal concept analysis;heart rate variability;heuristic (computer science);hoc (programming language);mobile media;open-source software;reverse engineering;software architect;software product line;spatial variability;video game clone	Anas Shatnawi;Abdelhak-Djamel Seriai;Houari A. Sahraoui	2015		10.1007/978-3-319-14130-5_2	computer science;systems engineering;engineering;programming language;object-oriented programming;engineering drawing;reverse engineering;source code	SE	-59.13125311505581	28.977326182291982	160926
6d51d4ac301217d563860340f77cc38add0329ab	building reverse engineering tools with software components: ten lessons learned	component based tool building;formal specification;building block;component based tool building reverse engineering tool software components;reverse engineering software tools buildings software systems software prototyping prototypes application software software engineering guidelines;object oriented programming;software components;lessons learned;software component;software tools;software tools formal specification object oriented programming reverse engineering;reverse engineering tool;literature survey;reverse engineering	My dissertation explores a new approach to construct tools in the domain of reverse engineering. The approach uses already available software components as building blocks, combining and customizing them programmatically. This approach can be characterized as component-based tool-building. The goal of the dissertation is to advance the current state of component-based tool-building towards a discipline that is more predictable and formal. This is achieved with three research contributions: (1) an in-depth literature survey that identifies requirements for reverse engineering tools, (2) a number of tool case studies that utilize component-based tool-building, (3) and ten lessons learned for tool builders that have been distilled from these case studies.	component-based software engineering;experience;requirement;reverse engineering;software build	Holger M. Kienle	2007	14th Working Conference on Reverse Engineering (WCRE 2007)	10.1109/WCRE.2007.14	personal software process;verification and validation;software engineering process group;software sizing;software verification;search-based software engineering;computer science;systems engineering;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;resource-oriented architecture;software measurement;software deployment;computer-aided software engineering;software development process;software requirements;software system;computer engineering	SE	-56.01459790753241	28.766371220288477	161447
4a544a85c314781b874a578022fd87424cc87e4a	esuml-eaf: a framework to develop an energy-efficient design model for embedded software	uml;energy efficient design model;embedded software	There is a growing interest in developing embedded systems that consume low energy in such application areas as mobile communications or wireless sensor networks. To especially provide the complex and diverse functions of embedded software with limited energy consumption, many studies of low-energy software are being performed. The existing studies to analyze energy consumption of embedded software have mainly focused on source code. However, some studies recently explored model-based energy consumption analysis to fulfill the requirement of energy consumption in the early phase of software development process. This paper proposes a model-based energy consumption analysis framework to develop an energy-efficient design model of embedded software. The proposed framework can analyze energy consumption without building an additional analysis model in software development and provide the chance to fulfill the energy consumption requirements in the early phase of the software development process, which can reduce the feedback efforts.	component-based software engineering;embedded software;embedded system;enterprise architecture framework;requirement;seamless3d;software design;software development process;unified modeling language	Doohwan Kim;Jang-Eui Hong	2013	Software & Systems Modeling	10.1007/s10270-013-0337-5	unified modeling language;embedded system;verification and validation;real-time computing;software sizing;embedded software;computer science;systems engineering;engineering;package development process;software design;software framework;software development;software design description;software construction;resource-oriented architecture;software deployment;goal-driven software development process;software development process;software requirements;software metric;software system;avionics software	Embedded	-56.47626201172472	27.799253279019883	161916
3cf127be43388268df5421eb5f5440b688d267a3	moving the goalposts: coverage satisfaction is not enough	software testing;structural coverage;automated test generation	Structural coverage criteria have been proposed to measure the adequacy of testing efforts. Indeed, in some domains—e.g., critical systems areas—structural coverage criteria must be satisfied to achieve certification. The advent of powerful search-based test generation tools has given us the ability to generate test inputs to satisfy these structural coverage criteria. While tempting, recent empirical evidence indicates these tools should be used with caution, as merely achieving high structural coverage is not necessarily indicative of high fault detection ability. In this report, we review some of these findings, and offer recommendations on how the strengths of search-based test generation methods can alleviate these issues.	code coverage;fault detection and isolation	Gregory Gay;Matthew Staats;Michael W. Whalen;Mats Per Erik Heimdahl	2014		10.1145/2593833.2593837	reliability engineering;systems engineering;engineering;operations management	SE	-62.01828349409583	30.683705870231222	162127
c708a01844de4aa4b02516be34b79ec801664289	kamu kurumları tarafından hazır ticari yazılım seçiminde kullanılacak dar tabanlı bir yöntem		Public institutions generally consider only the features list and the price in tenders while acquiring commercial-of-the-shelf (COTS) software products. Yet non-functional requirements of software often dominate them in terms of cost-benefit analysis and total cost of ownership (TCO). In this paper, we emphasize the non-functional requirements that need to be included in the technical scoring/evaluation in order to select a quality product with low TCO, and how the customer can evaluate those requirements. Existing literature presents the evaluation criteria for non-functional requirements from the developer perspective. The required data for such evaluation is not usually available to the customer. We here define the criteria that the customer can really use for her	functional requirement;non-functional requirement;total cost of ownership	Emine Firuze Taytas;Mehmet Gün;Simge Bastüzel;Buse Tekin;Kivanç Dinçer	2015				SE	-61.839376192612505	26.096553976371137	162339
e9849100ad3e7c8e8120498bfe78a6e5dc774dbe	applying oo metrics to assess uml meta-models	lenguaje uml;langage modelisation unifie;feasibility;metrique orientee objet;metamodel;metamodele;metamodelo;unified modelling language;software reusability;reutilisation logiciel;practicabilidad;faisabilite	UML has been coming of age through more than seven years development, in which there are not only minor revision like from UML 1.1 to UML 1.2, but also significant improvement like final adoption of UML 2.0 submissions. However there is so far lack of an objective assessment to UML meta-models, which can be used to control and predict the evolution of the UML. In this paper we regard UML meta-models as the equivalent of Object-Oriented (OO) design models. Therefore, we can adapt OO design metrics and criteria as a method to assess UML meta-models. Our method conducts the assessment of stability and design quality to UML meta-models in versions of 1.1, 1.3, 1.4 (with Action Semantics), 1.5, and 2.0. Based on the results we analyze the evolution of the UML versions and provide the applicability suggestions to the method.	unified modeling language	Haohai Ma;Weizhong Shao;Lu Zhang;Yanbing Jiang	2004		10.1007/978-3-540-30187-5_2	metamodeling;unified modeling language;feasibility study;uml tool;computer science;applications of uml;shlaer–mellor method;programming language;object constraint language	Logic	-59.84762235398757	29.693901365654327	163095
2e27f44cd17d66c55ce6fb25308ef5bcf4ee8f84	simulating and optimising design decisions in quantitative goal models	quantitative goal models;ambulance service system;optimisation;multi objective optimisation;formal specification;tool support;technology;computer model;system analysis and design;goal oriented requirements engineering;automated techniques;search based software engineering;satisfiability;software engineering;design space;optimisation formal specification;requirements engineering;requirements;objective function;computational modeling;mathematical model computational modeling equations system analysis and design probability distribution numerical models optimization;quality requirement;science technology;numerical model;multiobjective optimisation algorithm;system design;requirement engineering;probability distribution;requirements simulation and optimisation;quantitative modelling;mathematical model;quality requirements;optimization;computer science;ambulance service system quantitative goal models requirements engineering automated techniques multiobjective optimisation algorithm;numerical models;search based software engineering requirements simulation and optimisation quality requirements goal oriented requirements engineering quantitative modelling	Making decisions among a set of alternative system designs is an essential activity of requirements engineering. It involves evaluating how well each alternative satisfies the stakeholders' goals and selecting one alternative that achieves some optimal tradeoffs between possibly conflicting goals. Quantitative goal models support such activities by describing how alternative system designs — expressed as alternative goal refinements and responsibility assignments — impact on the levels of goal satisfaction specified in terms of measurable objective functions. Analyzing large numbers of alternative designs in such models is an expensive activity for which no dedicated tool support is currently available. This paper takes a first step towards providing such support by presenting automated techniques for (i) simulating quantitative goal models so as to estimate the levels of goal satisfaction contributed by alternative system designs and (ii) optimising the system design by applying a multi-objective optimisation algorithm to search through the design space. These techniques are presented and validated using a quantitative goal model for a well-known ambulance service system.	algorithm;mathematical optimization;requirements engineering;simulation;systems design	William Heaven;Emmanuel Letier	2011	2011 IEEE 19th International Requirements Engineering Conference	10.1109/RE.2011.6051653	probability distribution;reliability engineering;computer science;systems engineering;engineering;software engineering;mathematical model;formal specification;management science;requirements engineering;satisfiability;technology	SE	-59.756454766779974	28.801998633069466	163223
e6fc62ee3d6971659e64843ce7b51ef37784bedb	a method for model based test harness generation for component testing	automatic generation;component testing;test case generation;test generation;model based testing;stubs	We present a model-based testing approach that allows the automatic generation of test artifacts for component testing. A component interacts with its clients through provided interfaces, and request services from other components through its required interfaces. Generating a test artifact that acts as though it were the client of the component under test is the easiest part, and there already exists tools to support this task. But one needs also to create substitute of the server components, which is the hardest part. Although there are also tools to help with this task, it still requires manual effort. Our approach provides a systematic way to obtain such substitute components during test generation. Results of the application of the approach in a real world component are also presented.	activity diagram;black box;client (computing);code coverage;code reuse;component-based software engineering;concurrency (computer science);exception handling;fault coverage;model-based testing;non-functional requirement;self-organized criticality;server (computing);simulation;system testing;test case;test data;test harness;test-driven development;unified modeling language;unit testing	Camila Ribeiro Rocha;Eliane Martins	2008	Journal of the Brazilian Computer Society	10.1007/BF03192549	keyword-driven testing;test data generation;model-based testing;simulation;white-box testing;manual testing;computer science;software engineering;unit testing;programming language;test management approach;test harness	SE	-55.90399953302983	31.917801368858253	163445
d1fa8cf4ab0feaa5e278accd62eddcde6b34c843	variability assessment in software product families	computacion informatica;software product family;software systems;grupo de excelencia;ciencias basicas y experimentales;assessment methods;variability;software product families;assessment;evolution	Software variability management is a key factor in the success of software systems and software product families. An important aspect of software variability management is the evolution of variability in response to changing markets, business needs, and advances in technology. To be able to determine whether, when, and how variability should evolve, we have developed the COVAMOF software variability assessment method (COSVAM). The contribution of COSVAM is that it is a novel, and industry-strength assessment process that addresses the issues that are associated to the current variability assessment practice. In this paper, we present the successful validation of COSVAM in an industrial software product family. 2008 Elsevier B.V. All rights reserved.	business requirements;heart rate variability;software system;spatial variability	Sybren Deelstra;Marco Sinnema;Jan Bosch	2009	Information & Software Technology	10.1016/j.infsof.2008.04.002	reliability engineering;verification and validation;systems engineering;engineering;software development;software engineering;evolution;software quality control;software metric;software quality analyst	SE	-62.44155273484147	27.781706565785413	163562
db9e51d21d527755879dfc5ca1e2a2dfce1bc145	structural analysis of the software architecture - a maintenance assessment case study	software architecture;structure analysis	Architectural erosion is a sign of reduced architectural quality. Quality characteristics of an architecture, such as its ability to accommodate change, are critical for an evolving product. The structure of an architecture is said to be eroded when the software within the architecture becomes resistant to change or changes become risky and time consuming. The objective of our work is to understand the signs of architectural erosion that contribute to decreased maintainability. A maintenance assessment case study is described in which we apply structural measurements to a product to determine signs of architectural erosion. It provides an understanding of a product’s quality by examining the structure of its architecture. The ability to assess architectural erosion in an evolving software product allows the quality of the architecture to be monitored to ensure its business and maintenance goals are achieved.	software architecture;structural analysis	Catherine Blake Jaktman;John Leaney	1999		10.1007/978-0-387-35563-4_26	multilayered architecture;enterprise architecture framework;functional software architecture;reliability engineering;reference architecture;software visualization;software architecture;architecture tradeoff analysis method;verification and validation;computer science;systems engineering;engineering;applications architecture;software development;software design description;software engineering;software construction;software architecture description;structural analysis;resource-oriented architecture;systems architecture;software system	SE	-61.8974697150753	27.45106752621103	163828
17206bf59236fda6723b483e448f8478090f91e1	process clustering with an algorithm based on a coupling metric	traitement;coupling;tratamiento;treatment;productivite;development;grouping;qualite;desarrollo;metric;ingenieria logiciel;couplage;software engineering;productividad;algorithme;algorithm;acoplamiento;quality;developpement;genie logiciel;metrico;agrupamiento;productivity;metrique;calidad;groupage;algoritmo	Process management has received considerable attention in recent years as a means to improve productivity and quality in the software industry. Quite often, it might be advantageous to group related processes to reduce and control the amount of effort needed to measure and manage the processes in the organization. Here we present an algorithm for grouping processes into process clusters. The article begins with the development of a coupling metric that measures the strength of the linkage between processes. Next, the algorithm is presented. Finally, we report on the results obtained while analyzing the processes in a large software development organization.	algorithm;cluster analysis;coupling (computer programming);linkage (software);software development;software industry	Tzvi Raz;Alan T. Yaung	1993	Journal of Systems and Software	10.1016/0164-1212(93)90112-B	productivity;metric;artificial intelligence;software engineering;coupling;engineering drawing;algorithm;software metric	SE	-61.70882154295829	28.589303910223528	164017
352ab7677a60ab2cc249ae2e81a2c4bdbd4a59d8	selection of business process for autonomic automation	workflow management software business data processing configuration management fault tolerant computing;process automation autonomic computing business process;runtime;process model inspection;business process automation;automation humans organizations optimization monitoring runtime;fault tolerant computing;monitoring;self configuration business process selection autonomic automation business process automation management preference process model inspection autonomic workflow;business data processing;process automation;workflow management software;optimization;self configuration;humans;process model;organizations;business process selection;configuration management;autonomic computing;autonomic automation;business process;autonomic workflow;management preference;automation	Autonomic automation is viewed as a new approach to business process automation. In this work, we propose a method to identify the best-suited business processes as candidates for an autonomic automation. Generally, this decision is made by process automation experts or inspired by management preferences [1]. Moreover, the best candidate to an autonomic automation is possibly different from a candidate to a traditional automation. To support our method we developed a tool with ability to inspect the process model and obtain specific metrics. Finally, we present the results of the proposed selection method, compared with expert choices and to existent methods for traditional automation selection.	autonomic computing;autonomic networking;build automation;business process;process modeling;selection (genetic algorithm)	Luciano D. Terres;José A. Rodrigues Nt.;Jano Moreira de Souza	2010	2010 14th IEEE International Enterprise Distributed Object Computing Conference	10.1109/EDOC.2010.30	reliability engineering;computer science;systems engineering;organization;engineering;process automation system;automation;process modeling;configuration management;process management;business process;management;autonomic computing	Robotics	-56.58784547825572	26.944759966068247	164504
2c5d17b37bf1f4e617eb7dc5dce5b0bbd400b4a3	towards validating complexity-based metrics for software product line architectures	software product line architecture;software metrics;programmable logic arrays measurement complexity theory correlation unified modeling language computer architecture software;replication;nonparametric statistics;emprical validation;metrics;program verification;software architecture;complexity rate complexity based metrics validation software product line architectures software reuse software pla pl quality return on investment roi pla variability resolution model uml modeling variability management pla complexity metrics arcade game maker agm pl nonparametric spearman correlation ranking technique comppla metric;software reusability;software reusability nonparametric statistics program verification software architecture software metrics software quality;software product line architecture correlation analysis emprical validation metrics replication;software quality;correlation analysis	Software product line (PL) is an approach that focuses on software reuse and has been successfully applied for specific domains. The PL architecture (PLA) is one of the most important assets, and it represents commonalities and variabilities of a PL. The analysis of the PLA, supported by metrics, can be used as an important indicator of the PL quality and return on investment (ROI). This paper presents the replication of a controlled experiment for validating complexity metrics for PLAs. In particular, in this replication we are focused on evaluating how subjects less-qualified than the subjects from the original experiment evaluate complexity of a PLA by means of generated specific products. It was applied a PLA variability resolution model of a given PL to a sample of subjects from at least basic knowledge on UML modeling, PL and variability management. Apart of the selection of different subjects, the same original experiment conditions were kept. The proposed PLA complexity metrics were experimentally validated based on their application to a set of 35 derived products from the Arcade Game Maker (AGM) PL. Normality tests were applied to the metrics observed values, thus, pointing out their non-normality. Therefore, the non-parametric Spearman's correlation ranking technique was used to demonstrate the correlation between the CompPLA metric and the complexity rate given by the subjects to each derived product. Such a correlation was strong and positive. The results obtained in this replication shown that even less-qualified subjects, compared to the subjects from the original experiment, are able to rate the complexity of a PLA by means of its generated products, thus corroborating the results of the original experiment and providing more evidence that the composed metric for complexity (CompPLA) can be used as a relevant indicator for measuring the complexity of PLA based on their derived products.	arcade game;belief revision;code reuse;experiment;heart rate variability;programmable logic array;region of interest;software product line;spatial variability;uml tool;unified modeling language	Anderson Marcolino;Edson Alves de Oliveira Junior;Itana Maria de Souza Gimenes;Tayana Conte	2013	2013 VII Brazilian Symposium on Software Components, Architectures and Reuse	10.1109/SBCARS.2013.18	reliability engineering;computer science;systems engineering;theoretical computer science;software construction	SE	-58.3368499249056	30.016097612726206	164615
5ded8a967eb7a6763f01a8f4cf0c5d14f11185f7	a component-based model for building reliable multi-agent systems	structural model;multiagent system;mas quality control component based model multiagent systems specification model formal specification agent oriented software engineering agent instantiation framework structural model agent flexibility adaptation capacity software design patterns xml model software agent reliability;formal specification;multi agent system;design and development;agent oriented software engineering;object oriented programming;multi agent systems;quality requirement;software reusability;design pattern;xml;software reusability multi agent systems object oriented programming formal specification software reliability software quality xml;adaptive capacity;software reliability;software quality;multiagent systems nasa software engineering conferences	In this article, we describe a specification model that seeks to couple formal specification methods and agent-oriented software engineering techniques. The objective is to allow faster formal development of flexible and reusable multiagent systems (MAS) with strict requirements of quality and reliability. The specification model is specifically tailored to support highly dynamic and evolutive characteristics of MAS. The agents are formally specified and instantiated by a framework and reuse is achieved by transforming the framework structural model into multiple agents. Agent flexibility, and adaptation capacity is ensured through the use of design patterns and properties such as: encapsulation, high-cohesion, low-coupling and through the definition of a formal XML model. The specification model represented in XML can be transformed into a code block that needs few adjustments, granting the system a high flexibility and trustworthiness. The purpose is to reduce time, effort and costs associated with MAS design and development with high quality requirements and reliability.	multi-agent system	Aluízio Haendchen Filho;Arndt von Staa;Carlos José Pereira de Lucena	2003		10.1109/SEW.2003.1270724	reliability engineering;formal methods;computer science;systems engineering;software engineering;multi-agent system;formal specification;programming language;software quality	AI	-55.7269355018471	27.23184277490201	164908
8935c6a4f5a88a9ad08421879566afc0070bffc6	does imperfect debugging affect software reliability growth?	debugging;software measurement;time measurement;software systems;testing;software engineering;error analysis;permission;error classification;fault detection;debugging software reliability fault detection software measurement permission software systems time measurement testing laboratories computer errors;software reliability growth model;software reliability;computer errors;reliability model	This paper discusses the improvement of conventional software reliability growth models by elimination of the unreasonable assumption that errors or faults in a program can be perfectly removed when they are detected. The results show that exponential-type software reliability growth models that deal with errorcounting data could be used even if the perfect debugging assumption were not held, in which case the interpretation of the model parameters should be changed. An analysis of real project data is presented.	debugging;software quality;software reliability testing;time complexity	Mitsuru Ohba;Xiao-Mei Chou	1989		10.1145/74587.74619	reliability engineering;verification and validation;regression testing;real-time computing;software sizing;software verification;computer science;engineering;software reliability testing;software engineering;software construction;software testing;debugging;software measurement;fault detection and isolation;software quality;software fault tolerance;software metric;time;software quality analyst;software system;avionics software	SE	-62.31771651666276	32.19635053850474	165171
a2ec412ec1204443ccff5b773ec9aa7ce2167023	the importance of business process modeling in software systems design	requirements specifications;modeling technique;software systems;business process modeling;language action perspective;software engineering;business process model;business process simulation;model checking;enterprise information system;failure rate;process model;petri nets;demo;petri net;business process	Despite diligent efforts made by the software engineering community, the failure of software projects keeps increasing at an alarming rate. After two decades of this problem reoccurring, one of the leading causes for the high failure rate is still poor process modeling (requirements’ specification). Therefore both researchers and practitioners recognize the importance of business process modeling in understanding and designing accurate software systems. However, lack of direct model checking (verification) feature is one of the main shortcomings in conventional process modeling methods. It is important that models provide verifiable insight into underlying business processes in order to design complex software systems such as Enterprise Information Systems (EIS). The software engineering community has been deploying the same methods that have haunted the industry with failure. In this paper, we try to remedy this issue by looking at a non-conventional framework. We introduce a business process modeling method that is amenable to automatic analysis (simulation), yet powerful enough to capture the rich reality of business systems as enacted in the behavior and interactions of users. The proposed method is based on the innovative language-action perspective. c © 2008 Elsevier B.V. All rights reserved.	augmented reality;business process;cognition;enterprise information system;entity;failure rate;formal verification;graphical user interface;interaction;language/action perspective;model checking;process modeling;quality engineering;requirement;simulation;software engineering;software system;systems design	Joseph Barjis	2008	Sci. Comput. Program.	10.1016/j.scico.2008.01.002	team software process;software engineering process group;information engineering;business requirements;computer science;knowledge management;artifact-centric business process model;business process management;software design;social software engineering;software development;software construction;process modeling;systems development life cycle;business process discovery;business process modeling;petri net;goal-driven software development process;software development process;business activity monitoring;enterprise information system	SE	-58.96838613582905	25.376512581683397	165241
73dc2ae46af311f48e8ee7c6d01b0f7cadc68ba4	a methodology for the development of special-purpose function architectures	database management;hardware architecture;development methodology	The research described in this paper concerns a generalized methodology for the development of special-purpose function architectures (SPFA). The development methodology can be used to introduce the concept of an SPFA approach to an organization.  The methodology provides an organized set of processes that can be followed to tailor the development of SPFAs to specific applications. This methodology consists of processes for identification, creation, testing, evaluation, and substitution of SPFAs. It permits a user to carefully select sets of database management functions as candidates to be moved from software into hardware, develop one or more SPFAs that perform this function, and evaluate the consequences of having the function performed as a new hardware architecture. A set of tools/components with which to carry out this methodology are included in the environment of a proposed database machine architecture development facility.	database machine;shortest path faster algorithm;software testing;system identification	Raymond A. Liuzzi;P. Bruce Berra	1982		10.1145/1500774.1500790	reliability engineering;computer science;systems engineering;computer engineering	SE	-58.71788287318384	27.545279199485478	165608
83186bd2b84cbdcc73fe8bfcac95a8972ccc6b89	software project driven analysis and development of process activities supporting web based software engineering tools	software engineering tools	The field of software engineering has seen the development of software engineering tools that allow for distributed development of software systems over the web. This paper covers analysis and process activities for a web based software design tool that served as the basis for software requirements formulation of a software process tracking tool. These software tools are an outgrowth of a software engineering project capstone. The discussion focuses on those development activities that assisted the front end of the development through software requirements formulation. This paper describes the background for the software engineering projects, software tool development processes, and the developed software tools.	capstone (cryptography);computer data storage;design tool;programming tool;requirement;scheduling (computing);software design;software development process;software engineering;software project management;software requirements;software system	Shriram Sankaran;Joseph E. Urban	2005			personal software process;verification and validation;software engineering process group;software project management;computer science;package development process;social software engineering;component-based software engineering;software development;software engineering;software construction;software walkthrough;software analytics;resource-oriented architecture;software deployment;computer-aided software engineering;software development process;software requirements;software system;software peer review	SE	-62.53306730874571	25.441486483070886	166040
eff7c2d7cd110e35422ba6130a11104cc3d4543b	a brief introduction to domain analysis	reusability;software libraries;software components;taxonomic classification;clustering;domin analysis;software component;domain analysis	The software engineering process offers opportunities to reuse work products (e.g., requirements, specifications, designs, code, tests) as well as processes descriptions (e.g., project plans, design rationales, derivations, and testing plans). In this presentation, we focus on the reuse of software components. The experience of the past two decades indicates that software reuse techniques are most successful when applied in the context of restricted application domains. Domain analysis has been identified as a major factor in the success of software reusability. We examine the role of domain analysis in software reusemwhat problems does it address and how it is done.	application domain;code reuse;component-based software engineering;domain analysis;requirement;software development process	Guillermo Arango	1994		10.1145/326619.326656	domain analysis;computer science;component-based software engineering;feature-oriented domain analysis;software engineering;data mining;programming language;world wide web	SE	-59.1730426867311	27.08016035736793	166137
f938ff5731da76840dbb7542ef9edc0754002711	matching context aware software testing design techniques to iso/iec/ieee 29119		A software system is context aware when it uses contextual information to help actors (users or other systems) to achieve their tasks. Testing this type of software can be a challenge since context and its variabilities cannot be controlled by the software tester. The ISO/IEC/IEEE 29119 intended to cover testing of any software system. It provides a common language and process for testing software systems, including a categorization of conventional testing techniques. This paper contains the initial results of our ongoing efforts to understand the testing of context aware software, Specifically, we evaluate whether the observed techniques for testing context aware software can be matched against the ISO/IEC/IEEE 29119 categories or if they represent a new breed of testing techniques. The results indicate that using conventional techniques variations to test context aware software systems does not produce evidence on their feasibility to test the context awareness features in such systems.	iso/iec 42010;software testing	Santiago Matalonga;Felyppe Rodrigues;Guilherme Horta Travassos	2015		10.1007/978-3-319-19860-6_4	iso/iec 9126;embedded system;iso/iec 42010;real-time computing;iso/iec 12207;software engineering;common management information service;iso/iec 10967	SE	-55.799527743199064	26.328716253545018	166139
af71d522f66203a525961f48e4e5fefc9f58ceb3	two-stage third-party review proposal using the enterprise architecture in software development		Abstract It is difficult to reliably prevent failures occurring after the release of software products, and efforts to do so vary from company to company. In this paper, we will propose a two-stage third-party(2S3P) review method that focuses on the design process from the perspective of Enterprise Architecture and carries out a stepwise review of design document from the perspective of natural language and quality characteristics. We will also examine the effects and consider how it affects processes after design.		Haruhiro Tsuchiya;Shuichiro Yamamoto;Yuko Murakami;Tomoyuki Yanagisawa;Naoko Kobayashi;Jiaqi Wan	2018		10.1016/j.procs.2018.08.059	data mining;enterprise architecture;natural language;engineering design process;systems engineering;software development;software design document;software;computer science	SE	-62.314372503133754	26.837255011056506	166797
04d6c537062125412daf8f300a68ebf3db370b5f	a component-based approach for embedded software development	instruments;ores component based approach embedded software development semi automated code generation cost component retrieval online repository for embedded software ontology based approach search code template software reuse;software libraries;learning curve;application software;automated code generation;component based development;component composition;software systems;object oriented programming;embedded system;embedded systems;computer aided software engineering;software reusability;ores;artificial satellites;ontologies;difference set;computer aided software engineering object oriented programming software reusability embedded systems software libraries;embedded software ores application software embedded system costs artificial satellites instruments software systems software development management ontologies;software development management;embedded software	The rapid growth in the demand of embedded systems and the increased complexity of embedded software pose an urgent need for advanced embedded software development techniques. Software technology is shifting toward semi-automated code generation and integration of systems from components. Component-based development (CBD) techniques can significantly reduce the time and cost for developing software systems. However, there are some difficult problems with the CBD approach. Component identification and retrieval as well as component composition require extensive knowledge of the components. Designers need to go through a steep learning curve in order to effectively compose a system out of available components. In this paper, we discuss an integrated mechanism for component-based development of embedded software. We develop an On-line Repository for Embedded Software (ORES) to facilitate component management and retrieval. ORES uses an ontologybased approach to facilitate repository browsing and effective search. Based on ORES, we develop the code template approach to facilitate semi-automated component composition. A code template can be instantiated by different sets of components and, thus, offers more flexibility and configurability and better reuse. Another important aspect in embedded software is the nonfunctional requirements and properties. In ORES, we capture nonfunctional properties of components and provide facilities for the analysis of overall system properties.	algorithm;automatic programming;code generation (compiler);component-based software engineering;embedded software;embedded system;glue code;non-functional requirement;population;recursion;semiconductor industry;software development process;software system;universal instantiation	I-Ling Yen;Jayabharath Goluguri;Farokh B. Bastani;Latifur Khan;John Linn	2002		10.1109/ISORC.2002.1003805	embedded system;application software;software sizing;embedded software;computer science;ontology;package development process;backporting;software framework;component-based software engineering;software development;operating system;software engineering;software construction;programming language;object-oriented programming;learning curve;computer-aided software engineering;satellite;difference set;software system;avionics software	Embedded	-55.594620927231404	27.362377623271854	166986
e513012a806d70c66b80dfd3dd5d14a2b4efafaa	collaborative product configuration: formalization and efficient algorithms for dependency analysis	dependence analysis;efficient algorithm;feature modeling;product line;indexing terms;production management;feature selection;software product line;product configuration	In the Software Product Line approach, product configuration is a key activity in which stakeholders choose features for a product. This activity is critical in the sense that careless feature selections might lead to undesirable products. Even though product configuration is seen as a team activity in which divergent interests and views are merged into a single consistent product specification, current configuration technology is essentially single-userbased. This configuration approach can be error-prone and time-consuming as it usually requires numerous interactions between the product manager and the stakeholders to resolve decision conflicts. To tackle this problem we have proposed an approach called “Collaborative Product Configuration” (CPC). In this paper, we extend the CPC approach by providing efficient dependency analysis algorithms to support the validation of workflow-based descriptions called CPC plans. In addition, we add to previous work by providing a formal description of the approach’s concepts, an augmented illustrated example, and a discussion covering several prototype tools now available.	algorithm;cartesian perceptual compression;cognitive dimensions of notations;dependence analysis;interaction;knowledge-based configuration;prototype;software product line	Marcílio Mendonça;Donald D. Cowan;William Malyk;Toacy Cavalcante de Oliveira	2008	JSW		computer science;systems engineering;operations management;product design specification;product design;engineering drawing;new product development;feature model;product engineering	SE	-55.78798239734692	25.623520063150774	167253
4029715603677d821fbd69ee13ee0361a2e54881	a clustering-based methodology for selection of fault tolerance techniques	advisory systems;clustering;fault tolerance;software development	Development of dependable applications requires selection of appropriate fault tolerance techniques that balance efficiency in fault handling and resulting consequences, such as increased development cost or performance degradation. This paper describes an advisory system that recommends fault tolerance techniques considering specified development and runtime application attributes. In the selection process, we use the K-means clustering algorithm to identify similarities between known fault tolerance techniques to select those ones that are possibly different, but simultaneously conform to developer specification. As a part of the research, we implemented a web-based system that covers definition of attributes, aggregates knowledge about fault tolerance techniques together, and implements the advisory algorithm.	fault tolerance	Pawel Lech Kaczmarek;Marcin L. Roman	2012		10.1007/978-3-642-29350-4_77	fault tolerance;real-time computing;fault coverage;computer science;software development;machine learning;data mining;cluster analysis;software fault tolerance	EDA	-58.351258776254596	30.94294744638431	168218
f9466fc246e7f87cae8a0b71657d3565a70ac229	a model transformation method in service-oriented domain modeling	domain model;analytical models;software;model transformation method;programming ontologies software engineering costs algorithm design and analysis software quality australia sun helium process design;helium;service orientation;semantics;model transformation;service oriented software development;satisfiability;software engineering;goal model model transformation process model;process design;o rgps method;goal model;process models;goal models;software reusability;software development;process control;sun;ontologies;process model;process model generation approach model transformation method service oriented domain modeling software development reusable domain assets service oriented software development o rgps method goal models process models;process model generation approach;service oriented domain modeling;programming;algorithm design and analysis;software quality;australia;reusable domain assets	As an important phase in software development, domain modeling can be used to construct reusable domain assets based on common domain requirements. In service-oriented software development, domain modeling is essential to satisfy users’ personalized requirements in a short time and at a low cost. We have proposed a method named O-RGPS for service-oriented domain modeling. In O-RGPS, goal models and process models are two kinds of important domain models that have tight relations. However, process modeling is not an easy task in domain modeling since the process models are usually very complicated, and some properties and relations have the risk of being missed. Towards this problem, this paper proposes an automatic process model generation approach by transforming the goal model. Algorithms for generating process models are designed by sufficiently analyzing decomposition and dependence relations in the goal model and transforming these relations into the process model. Furthermore, the method has been realized in a domain modeling prototype, which aims at helping domain experts in domain modeling. The experiments show that this method can help to reduce modeling time and improve the quality of models during creating process models.	algorithm;domain-specific modeling;experiment;householder transformation;model transformation;personalization;process modeling;prototype;requirement;sequal framework;service-oriented architecture;service-oriented device architecture;service-oriented software engineering;software development	Zhouxuan Sun;Jian Wang;Keqing He;Shujuan Xiang;Dunhui Yu	2010	2010 21st Australian Software Engineering Conference	10.1109/ASWEC.2010.32	domain analysis;reliability engineering;data modeling;simulation;domain;business domain;computer science;systems engineering;engineering;feature-oriented domain analysis;software engineering;domain engineering;domain model;process control;process modeling;semantics	SE	-56.20268995827342	27.78959548457656	168286
391b9ac397edcab9ffdce7f50c74e2731582d419	an integrating approach for developing distributed software systems -- combining formal methods, software reuse, and the experience base	distributed system;integrated approach;formal specification;software systems;formal description technique;formal method;personnel distributed programming software reusability formal specification specification languages software development management;experimental software engineering;specification languages;personnel;case study integrating approach distributed software systems formal methods software reuse experience base complex software systems development time to market development method distributed systems formal description technique specification and description language software reuse concepts use case driven incremental development method sdl patterns reusable artifacts reuse artifacts development team sdl pattern approach improvement methodology experimental software engineering;distributed programming;software reusability;uct;software systems electrical capacitance tomography gain measurement automata scattering documentation;time to market;experience base;use case;software reuse;software development management	The development of complex software systems is driven by many diverse and sometimes contradictory requirements such as correctness and maintainability of resulting products, development costs, and time-to-market. To alleviate these difficulties, we propose a development method for distributed systems that integrates different basic approaches. First, it combines the use of the formal description technique SDL with software reuse concepts. This results in the definition of a use-case driven, incremental development method with SDL patterns as the main reusable artifacts. Experience with this approach has shown that there are several other factors of influence, such as the quality of reuse artifacts or the experience of the development team. Therefore, we further combined our SDL pattern approach with an improvement methodology known from the area of experimental software engineering. In order to demonstrate the validity of this integrating approach, we sketch some representative outcomings of a case study.	code reuse;distributed computing;formal methods;software system	Raimund L. Feldmann;Birgit Geppert;Frank Rößler	1999		10.1109/ICECCS.1999.802850	use case;reliability engineering;verification and validation;formal methods;computer science;systems engineering;package development process;software development;software engineering;software construction;formal specification;programming language;goal-driven software development process;software development process;software system	SE	-57.868267931036605	26.960617161563725	168291
58b218754eafadf458f1917875d192b4231f8d59	a decade of modeling financial vehicles	type;real estate;dimension;role;financial instrument;domain;object oriented;financial system;software design;inheritance	While building nine object-oriented financial systems since 1989, we evolved techniques for modeling “financial vehicles” (bonds, contracts, real estate, racehorses). We specify each particular financial vehicle and vehicle type by combining objects that model the fundamental ideas behind financial vehicles. These techniques enable installed systems to accommodate new kinds of financial vehicles. Many normal software design techniques proved ineffective.	software design	William F. Frank;Anil Karunaratne	2000		10.1145/367845.367914	financial modeling;simulation;domain;software design;role;object-oriented programming;dimension;financial instrument;type;real estate	Logic	-58.658564119291746	27.16694406119295	168896
d0df689ec067c96b296ebfbaf9a2e7dcdacaaad6	system requirements and analysis issues for high assurance systems	high assurance systems;analysis issues;system requirements		requirement;system requirements	Gary Johnson;John Calvert;Kelly J. Hayhurst;John Janeri;Herbert Hecht	1998		10.1109/HASE.1998.10001	requirements analysis	SE	-62.3307904771752	25.631574291779646	169037
cd1e1e9714572d178e1ab221861b205a3ffd17c9	a family of software reliability growth models	software reliability program testing;software testing;software reliability growth models;software testing process software reliability growth models;software testing process;program testing;failure rate;software reliability software testing predictive models feedback control reliability theory computer applications application software difference equations differential equations;software reliability growth model;software reliability;feedback control	"""Software reliability growth models (SRGM's for short) are used to control the software testing process and to make it more effective. This paper describes a family of SRGM's which depends continuously on a parameter. All of the models of this family allow the prediction of lambda0, the failure rate at the beginning of the test and N0, the overall number of faults at the beginning of the test. The models in this family range from """"generally optimistic"""" to """"generally pessimistic"""". The well known and widely used basic execution time model of J. Musa belongs to this family. The new models are the result of a general theory of software reliability growth models developed by the author (to be published). Models of this family are applied to real data. It is described how this family of models can be used for feedback control of the software test process."""	failure rate;feedback;optimistic concurrency control;run time (program lifecycle phase);software quality;software reliability testing;software testing;whole earth 'lectronic link	Harald A. Stieber	2007	31st Annual International Computer Software and Applications Conference (COMPSAC 2007)	10.1109/COMPSAC.2007.20	reliability engineering;verification and validation;regression testing;real-time computing;software sizing;software performance testing;system integration testing;computer science;systems engineering;software reliability testing;software development;software engineering;failure rate;software construction;feedback;software testing;goal-driven software development process;software quality;software metric;software quality analyst;avionics software	SE	-62.57900369543398	32.018445176225185	170057
4b5d6e9515a098540f6e9c3c381006188df5a117	sq^(2)e: an approach to requirements validation with scenario question	analytical models;formal specification;scenario questions;requirements validation;query processing;software quality formal specification formal verification query processing;software system quality;scenario question query engine;software systems;satisfiability;textual requirements;engines cognition analytical models software systems production hardware radio frequency;formal verification;radio frequency;real world application;engines;requirements specification;real world application development phase software system quality textual requirements feedback based requirements validation requirements specification scenario question query engine;development phase;model based validation;cognition;production;scenario questions requirements validation feedback based validation model based validation;feedback based validation;requirement specification;software quality;feedback based requirements validation;hardware	Adequate requirements validation could prevent errors from propagating into later development phase, and eventually improve the quality of software systems. However, often validating textual requirements is difficult and error prone. We develop a feedback-based requirements validation methodology that provides an interactive and systematic way to validate a requirements model. Our approach is based on the notion of querying a model, which is built from a requirements specification, with scenario questions, in order to determine whether the model's behavior satisfies the given requirements. To investigate feasibility of our approach, we implemented a Scenario Question Query Engine (SQ2E), which uses scenario questions to query a model, and performed a preliminary case study using a real-world application. The results show that the approach we proposed was effective in detecting both expected and unexpected behaviors in a model. We believe that our approach could improve the quality of requirements and ultimately the quality of software systems.	cognitive dimensions of notations;requirement;sensor;software requirements specification;software system	Daniel Aceituna;Hyunsook Do;Seok-Won Lee	2010	2010 Asia Pacific Software Engineering Conference	10.1109/APSEC.2010.14	reliability engineering;software requirements specification;requirements management;cognition;formal verification;computer science;systems engineering;scenario;requirement;software engineering;system requirements specification;formal specification;database;non-functional testing;programming language;radio frequency;non-functional requirement;software quality;software system;satisfiability	SE	-57.96521538807731	29.427237501204935	170575
d5d63e504eabc0885d8cbbbb93efe628fa289f2d	software architecture decomposition using attributes.	decomposition;requirements;software architecture;clustering;attributes	Software architectural design has an enormous effect on downstream software artifacts. Decomposition of functions for the final system is one of the critical steps in software architectural design. The process of decomposition is typically conducted by designers based on their intuition and past experiences, which may not be robust sometimes. This paper presents a study of applying the clustering technique to support decomposition based on requirements and their attributes. The approach can support the architectural design process by grouping closely related requirements to form a subsystem or module. In this paper, we demonstrate our experiences in applying the approach to a communication protocol software system.	algorithm;cluster analysis;coefficient;communications protocol;downstream (software development);hierarchical clustering;jaccard index;new product development;rsvp-te;rational doors;requirement;requirements management;robustness (computer science);software architecture;software system;test engineer;upgma	Chung-Horng Lung;Xia Xu;Marzia Zaman	2005	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194007003410	software architecture;requirements analysis;software requirements specification;real-time computing;architectural pattern;computer science;systems engineering;software design;theoretical computer science;software development;software design description;software engineering;software construction;cluster analysis;decomposition;resource-oriented architecture;goal-driven software development process	SE	-55.7267381720531	28.85769571799122	171710
9638230dd5c41c1297786c502d2c1ebbced4b3d6	ontoqai: an ontology to support quality assurance inspections	quality assurance;inspection ontologies quality assurance unified modeling language software quality quality control;software quality capability maturity model ontologies artificial intelligence program testing quality assurance software performance evaluation;ontology quality assurance software inspection;process adherence ontoqai quality assurance inspections capability maturity model integration cmmi process and product quality assurance ppqa software inspections small and medium sized teams domain ontology quality assurance inspection formal structure software engineering agent based prototype inspection checklists noncompliance issue allocation inspection coverage;software inspection;ontology	According to Capability Maturity Model Integration (CMMI), the purpose of Process and Product Quality Assurance (PPQA) is to provide staff and management with objective insights about processes and their associated work products. Such purpose is usually achieved through the implementation of software inspections. Although software inspection be a common practice, it is time-consuming and expensive, which turns the implementation in small and medium-sized teams infeasible. To improve the software inspection, this paper proposes a domain ontology for representing the concepts of quality assurance inspection, which is independent, extensive, shareable and semantically strong. Through the ontology it is possible to provide a formal structure to support the development of software engineering solutions with quality. To support the quality assurance inspections, we developed an agent-based prototype that encapsulates the ontology model. The prototype is able to generate inspection checklists and automatically allocate noncompliance issues. We validated the approach through a case study that shows an increase of inspection coverage and adherence of process.	agent-based model;capability maturity model integration;confidentiality;map;ontology (information science);overhead (computing);process area (cmmi);prototype;response time (technology);software bug;software engineering;software factory;software inspection;xojo	João Pablo Silva da Silva;Pablo Dall Oglio;Sérgio Crespo Coelho da Silva Pinto;Ig Ibert Bittencourt;Sérgio Luis Sardi Mergen	2015	2015 29th Brazilian Symposium on Software Engineering	10.1109/SBES.2015.15	program assurance;software security assurance;reliability engineering;quality assurance;qa/qc;quality control;verification and validation;systems engineering;engineering;software engineering;software inspection;software quality control;software quality;software quality analyst;software peer review	SE	-59.282617686193866	28.115292896633314	172214
60187df590ff5a96d895d64fd307188c1a06a078	the use of software quality metrics in the materiel release process - experience report	software metrics;life cycle;ada;ada software quality software metrics;kiviat analysis software engineering life cycle support nextgen software suitability weapon systems evaluating software materiel release process source code source code metrics adastat software metrics tool ada;software engineering;experience report;lines of code;software quality documentation software safety software engineering software maintenance land vehicles road vehicles software metrics displays costs;indexation;software metric;next generation;source code;software quality	The US Army's Tank-Automotive Research Development and Engineering Center's Next Generation Software Engineering Life Cycle Support Activity (NextGen) is responsible for determining the suitability of software for release to the field. Determining the software is suitable for materiel release includes ensuring the software is safe, operationally suitable, and logistically supportable. The-Next Generation Team incorporates a thorough and well-defined process for evaluating software for materiel release that includes a detailed review of all documentation, a walk-through of a representative sample of source code, and the automated collection of several source code metrics using AdaSTATT, a commercially available software metrics tool for Ada. The metrics collected include source lines of code, cyclomatic and essential complexity, Halstead measures, and a maintainability index. Taken together, these metrics provide a valuable indication of the overall maintainability and supportability of the software. The metrics are presented using a Kiviat analysis, which provides a graphical display of the state of a module with respect to predefined limit values.	software metric;software quality	Michael Saboe	2001		10.1109/APAQS.2001.990008	reliability engineering;software visualization;personal software process;medical software;long-term support;verification and validation;software sizing;systems engineering;engineering;package development process;backporting;social software engineering;software framework;software development;software engineering;software construction;software walkthrough;software measurement;software deployment;release engineering;software quality;static program analysis;software system;software peer review	Logic	-59.49316485186053	31.838723469109986	172547
7dcb3083286171bc8ce0969e6c5eeb267880a254	quantitative evaluations of software built in with domain-specific disciplines	domain specificity;quantitative evaluation	Within software, domain-specific discipline has drawn much attention as a key concern for improving quality as well as domain-generic has. This paper presents the results from a long-term empirical study focusing on how the quality account and the reuse paradigms have been effectively utilized for defect reduction in software products.	domain-specific language	Masao J. Matsumoto	1995		10.1007/3-540-60406-5_29	reliability engineering;human–computer interaction;computer science;systems engineering	PL	-61.00403137346953	26.63834831109619	172590
6483640b15bf44815b4cc36a0c02fad8c461a4d9	extensions of uml to model aspect-oriented software systems	new technology;life cycle;separation of concern;software systems;software engineering;aspect oriented programming;unified modeling language;aspect oriented	Aspect-Oriented Programming (AOP) has arisen as a new technology to support a better SoC (Separation of Concerns), intending to contribute to the development of reusable, maintainable, and evolvable software systems. Aspects have been also explored in the early life cycle phases, aiming at contributing to a more adequate development of aspect-oriented software systems. In this perspective, in order to better represent these systems, a diversity of extensions of the UML (Unified Modeling Language) has been proposed; however, there is a lack of a complete panorama that identify all these extensions. This paper presents an overview about which are possibly all extensions of the UML to represent aspect-oriented software systems. For this, we have used a technique proposed by Evidence-Based Software Engineering (EBSE): the Systematic Mapping. As achieved results, we can observe a diversity of work; however, there is not a consensus about which are the more adequate or more used extensions. Based on this overview, interesting and important perspectives for future research can also be found.	aspect-oriented programming;aspect-oriented software development;separation of concerns;software engineering;software system;unified modeling language	Milena Guessi;Lucas Bueno Ruas de Oliveira;Elisa Yumi Nakagawa	2011	CLEI Electron. J.		reliability engineering;verification and validation;real-time computing;aspect-oriented programming;software sizing;uml tool;computer science;systems engineering;software design;social software engineering;software framework;component-based software engineering;software development;operating system;software engineering;applications of uml;software construction;database;systems development life cycle;diagramming software;programming language;software development process;use case points;software system	SE	-56.71640387236363	26.42019983479571	173339
8fccd3c36b27cfa70dfa0b39a7cfcb54c3e21077	a model-driven approach for developing a model repository: methodology and tool support	security dependability patterns;modeling artifact;repository;model driven engineering;meta model	Several development approaches have been proposed to cope with the increasing complexity of embedded system design. The most widely used approaches are those using models as the main artifacts to be constructed and maintained. The desired role of models is to ease, systematize and standardize the approach to the construction of software-based systems. To enforce reuse and interconnect the process of model specification and system development withmodels, we promote amodel-based approach coupled with a model repository. In this paper, we propose a model-driven engineering methodological approach for the development of a model repository and an operational architecture for development tools. In addition, we provide evidence of the benefits and feasibility of our approach by reporting on a preliminary prototype that provides a model-based repository of security and dependability (S&D) pattern models. Finally, we apply the proposed approach in practice to a use case from the railway domain with strong S&D requirements. © 2016 Elsevier B.V. All rights reserved.	algorithm;dependability;embedded system;model-driven architecture;model-driven engineering;programming tool;prototype;requirement;systems design	Brahim Hamid	2017	Future Generation Comp. Syst.	10.1016/j.future.2016.04.018	metamodeling;model-driven architecture;data mining;database;computer security	SE	-57.42178806056126	26.15273173409261	173350
68efc1b219c9c5acfb1ef57ac9d8b224574c4ace	softbus - an approach to software engineering for distributed real-time long-lifetime spacecraft data management systems	distributed application;real time;software systems;software engineering;software life cycle;data management system;operation and maintenance	Software Engineering can be defined as a systematic approach to the development,operation and maintenance of software systems with defined qualities, properties and reliability delivered to a pre-defined schedule and budget. This paper describes an architectural concept and design intended to contribute to software eng/neering activities in the field of distributed real-time long-lifetime spacecraft on-board data management systems. This architecture is based on the concept of a software buS hence the name SOFTBUS. The SOFIIK~ is equally applicable to centralised systems which are then capable of later distribution. Application software interfaced to the SOFTBUS is capable of being re-used and is portable. Prototyping and simulation is straightforward. The standard design and associated support tools will enhance potential reliability during the various phases of the software life cycle. The SOFfBUS is designed to ac2nmmodate changes in hardware and software technology.	centralisation;management system;on-board data handling;real-time transcription;simulation;software bus;software engineering;software maintenance;software release life cycle;software system	R. C. Allen	1987		10.1007/BFb0022113	reliability engineering;long-term support;verification and validation;software engineering process group;software sizing;software verification;software project management;systems engineering;engineering;backporting;social software engineering;component-based software engineering;software development;software engineering;software construction;systems development life cycle;software analytics;software maintenance;software deployment;software development process;software requirements;software system;software peer review	SE	-56.53859766007852	26.255592592225522	173763
625804d7a6b3f94029d2132c51a4526e99f5e16c	standardized event pair based test generation method using tss&tp	formal specification;wtp;wap;wireless application protocol;formal description;software engineering;test suite structure;general methods;protocol specification;test generation;test purpose;validation;test generation methods	In the software engineering test development takes significant resources. A general method for the creation of appropriate test suites could solve the problems of the often ad-hoc and time-consuming test generation process. The recent method uses formal specifications to support systematic derivation of complete test suites. From the formal specification using a special procedure a formalized document, the so-called Test Suite Structure (TSS) and Test Pur-poses (TP) can be created. With the help of this document developers can easily, automatically implement the test suites. The TSS & TP document also enables the persons who perform the tests to under-stand the test criteria and the steps, even if they do not actually know the protocol itself. We present a thorough picture of our test derivation method and show its efficiency on the Wireless Transaction Protocol (WTP) of the Wireless Application Protocol family (WAP). During our work in the validation phase we also found some operational flaws in the protocol specification.		Zoltán Pap;Zoltán Rétháti;Róbert Horváth;Gusztáv Adamis	2002	Acta Cybern.		wireless application protocol;computer science;test suite;data mining;database;test method;test script;test case;test management approach;test harness	EDA	-56.59946399096622	29.77027999602526	174797
e2d55453c901bc5b26ae24b80d14a9a88c35f71f	ways of applying artificial intelligence in software engineering		As Artificial Intelligence (AI) techniques become more powerful and easier to use they are increasingly deployed as key components of modern software systems. While this enables new functionality and often allows better adaptation to user needs it also creates additional problems for software engineers and exposes companies to new risks. Some work has been done to better understand the interaction between Software Engineering and AI but we lack methods to classify ways of applying AI in software systems and to analyse and understand the risks this poses. Only by doing so can we devise tools and solutions to help mitigate them. This paper presents the AI in SE Application Levels (AI-SEAL) taxonomy that categorises applications according to their point of application, the type of AI technology used and the automation level allowed. We show the usefulness of this taxonomy by classifying 15 papers from previous editions of the RAISE workshop. Results show that the taxonomy allows classification of distinct AI applications and provides insights concerning the risks associated with them. We argue that this will be important for companies in deciding how to apply AI in their software applications and to create strategies for its use.		Robert Feldt;Francisco Gomes de Oliveira Neto;Richard Torkar	2018	2018 IEEE/ACM 6th International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering (RAISE)	10.1145/3194104.3194109	software engineering;software system;automation;computer science;systems engineering;applications of artificial intelligence;software;artificial intelligence	SE	-60.00529827078229	26.05186027984987	175466
80225521f64af96d47e37a9588e2c91a6d9df869	policy rule-sets for policy based systems	protocols;formal specification;compounds;implementation;autonomic computing policy rule sets policy based system;specification;protocols compounds security artificial intelligence computer architecture industries organizations;industries;policy based systems;policy rule sets;multi agent artificial intelligence protocols;ipv6 policy rule sets policy based systems autonomic computing policy based management multi agent artificial intelligence protocols specification implementation;industrial organization;artificial intelligent;computer architecture;fault tolerant computing;policy rules;lessons learned;policy based management;ipv6;present day;protocol specification;artificial intelligence;organizations;security;autonomic computing;formal specification fault tolerant computing	Autonomic computing and policy based systems are closely related concepts that can be strong enablers for managing the next wave of information and network technologies. Lessons learned in these areas are therefore important not only for present day applications, but also for the future developments. In this paper we present some lessons learned in the deployment experiences with these technologies in the Government and the industry projects. The paper identifies the difficulties encountered in the practical development and deployment of these technologies, as well as possible approaches to overcome these difficulties.	autonomic computing;software deployment	Abdur Rahim Choudhary	2011	2011 International Conference for Internet Technology and Secured Transactions		computer science;systems engineering;software engineering;data mining	Visualization	-58.25520835389522	26.866823571653786	175731
52f1972bf94b2c8ca545057d96366325d5b3fae8	defining better test strategies with tradespace exploration techniques and pareto fronts: application in an industrial project	verification;tradespace exploration;pareto front;test;system design and analysis	Test strategies are usually defined following a point-design like method. Starting with a set of verification requirements and programmatic constraints, and usually with a generic test sequence, a baseline test approach is defined. Then, the baseline is optimized until an acceptable strategy is found. Academia has consistently shown however the benefits of using tradespace exploration techniques instead of pointbased designs. Some industrial applications seem to corroborate such findings. Yet, both academia and industry have limited the use of tradespace exploration techniques to selecting design concepts. This paper proposes that broadening the use of tradespace exploration techniques and Pareto frontiers to other activities in the field of systems engineering domain yields similar benefits. In particular, this paper presents the actual application of tradespace exploration techniques and Pareto frontiers in an industrial context to select a test strategy for a system. This paper provides thus three main contributions. First, the paper demonstrates that tradespace exploration and Pareto frontiers can be beneficial beyond concept selection. Second, the paper presents a process to use tradespace exploration techniques and Pareto frontiers for selecting test strategies. Finally, the paper showcases the application of the proposed technique and process in a real industrial setup, which yielded a number of lessons learnt. C⃝ 2016 Wiley Periodicals, Inc. Syst Eng 18: 639–658, 2015	baseline (configuration management);benchmark (computing);emergence;holism;john d. wiley;knowledge management;non-functional requirement;pareto efficiency;qualitative comparative analysis;requirement;risk assessment;system lifecycle;systems engineering;test case;test strategy;usability	Alejandro Salado	2015	Systems Engineering	10.1002/sys.21332	mathematical optimization;verification;simulation;economics;engineering;operations management;multi-objective optimization;management science;software testing	SE	-61.09334099024319	26.026272190980997	176011
c33bde165acf223c5dd4f59c46e1e6da7b89dd3c	a modeling approach to support the similarity-based reuse of configuration data	uml ocl;feature modeling;model based software engineering;internal similarities;product configuration	Product configuration in families of Integrated Control Systems (ICSs) involves resolving thousands of configurable parameters and is, therefore, time-consuming and error-prone. Typically, these systems consist of highly similar components that need to be configured similarly. For large-scale systems, a considerable portion of the configuration data can be reused, based on such similarities, during the configuration of each individual product. In this paper, we propose a model-based approach to automate the reuse of configuration data based on the similarities within an ICS product. Our approach enables configuration engineers to manipulate the reuse of configuration data, and ensures the consistency of the reused data. Evaluation of the approach, using a number of configured products from an industry partner, shows that more than 60% of configuration data can be automatically reused using our similarity-based approach, thereby reducing configuration effort.	cognitive dimensions of notations;control system;embedded software;experiment;feature model;object constraint language;software system;user space	Razieh Behjati;Tao Yue;Lionel C. Briand	2012		10.1007/978-3-642-33666-9_32	configuration management database;computer science;systems engineering;database;engineering drawing	SE	-55.94787935643498	26.080101094488583	176460
a6aa11505e462f88faddf6d5fc0fcc62c5fcb421	identifying issues and concerns in software reuse in software product lines	domain engineering;book chapter;product variants;computer software reusability domain engineering;selection of software;keywords degree of support;engineering software;network architecture;software product line;software reuse;software product lines;surveys;product development	One of the reasons for introducing software product lines (SPL) is the reduction of costs through reusing common assets for different products .   Developing assets to be reused in different products is often not easy. Increasing complexity due to the multitude of different functions and their interactions as well as a rising number of different product variants are just some of the challenges that must be faced when reusing software and other assets. In an attempt to understand the obstacles to implementing software reuse in SPL we have conducted a survey to investigate how software reuse is adopted in SPL so as to provide the necessary degree of support for engineering software product line applications and to identify some of the issues and concerns in software reuse. This survey also gathers information from SPL practitioners on what influences the selection of software to reuse within a software product line. This paper reports the results of that survey.	code reuse;software product line	Meena Jha;Liam O'Brien	2009		10.1007/978-3-642-04211-9_18	domain analysis;reliability engineering;reusability;long-term support;verification and validation;software quality management;software sizing;systems engineering;engineering;package development process;backporting;social software engineering;component-based software engineering;software development;software design description;software engineering;domain engineering;software construction;software walkthrough;resource-oriented architecture;software deployment;software requirements;software peer review	SE	-61.50519685453488	26.379506183753577	176740
9572c5cfee9b4fc847587ed97a89235b4c7a127d	comparison of software reliability assessment methods for open source software	distributed system;count data;software fault count data;analytic hierarchy process;s shaped software reliability growth model;information technology;software reliability open source software programming object oriented modeling reliability engineering information technology internet collaborative software information systems systems engineering and theory;stochastic processes distributed programming public domain software software reliability;hierarchy process;high speed data transfer network technology;open source system development;public domain software;reliability assessment;stochastic processes;software reliability assessment method;software development environment;distributed programming;assessment methods;software component;system development;software reliability growth model;software reliability;concurrent distributed system;high speed;open source system development software reliability assessment method open source software information technology high speed data transfer network technology software development environment concurrent distributed system hierarchy process s shaped software reliability growth model nonhomogeneous poisson process software component software fault count data;data transfer;nonhomogeneous poisson process;open source software;open source	IT (information technology) advanced with steady steps from 1970's is essential in our daily life. As the results of the advances in high-speed data-transfer network technology, software development environment has been changing into new development paradigm. In this paper, we propose software reliability assessment methods for concurrent distributed system development by using the analytic hierarchy process. Also, we make a comparison between the inflection S-shaped software reliability growth model and the other models based on a nonhomogeneous Poisson process applied to reliability assessment of the entire system composed of several software components. Moreover, we analyze actual software fault count data to show numerical examples of software reliability assessment for the open source project. Furthermore, we investigate an efficient software reliability assessment method for the actual open source system development	analytical hierarchy;component-based software engineering;count data;distributed computing;integrated development environment;numerical analysis;open-source software;population dynamics;programming paradigm;software development;software quality;software reliability testing	Yoshinobu Tamura;Shigeru Yamada	2005	11th International Conference on Parallel and Distributed Systems (ICPADS'05)	10.1109/ICPADS.2005.111	long-term support;verification and validation;analytic hierarchy process;software sizing;computer science;package development process;backporting;software reliability testing;component-based software engineering;software development;software design description;count data;software construction;database;distributed computing;development environment;software measurement;information technology;software deployment;public domain software;goal-driven software development process;software development process;software quality;software metric;statistics;software system;avionics software	SE	-62.250954337594244	31.345663453042597	177145
723f3b3911df5529a49a1554afa6879341387ecd	analysis of proposals to generation of system test cases from system requisites	use cases;functional requirements.;system test;test case;software systems;functional requirement;use case	System test cases allow to verify the functionality of a software system. System testing is a basic technique to guarantee quality of software systems. This work describes, analyzes and compares five proposals to generate test cases from functional requirements in a systematic way. Test cases generated will verify the adequate implementation of those functional requirements. The objective of this analysis is to determine the grade of mature of those proposals, evaluating if they can be applied in real projects and identifying which aspects needs to me improved.	functional requirement;software system;system testing;test case	Javier J. Gutiérrez;María José Escalona Cuaresma;Manuel Mejías Risoto;Jesús Torres	2005			functional specification;system under test;test management approach;systems engineering;software requirements specification;non-functional testing;test case;computer science;test data generation;reliability engineering;system testing	SE	-57.22983567297682	28.8956213069865	177329
477dd7123fd928c460def5831499af0759c37c65	building multi-view system models for requirements engineering	software change unit;institutional repositories;quality assurance;multiview system model;formal specification;fedora;software change unit multiview system model requirement engineering elicitation technique structuring elicitation session software artefact specification technique quality assurance inspection technique verification technique validation technique;system modeling;risk analysis;software maintenance;complex structure;data mining;inspection;structuring elicitation session;acceleration;vital;conflict management;software artefact;computational modeling;formal verification;verification technique;levels of abstraction;requirement engineering;solid modeling;specification technique;unified modeling language;buildings inspection acceleration risk analysis risk management quality assurance education computer languages decision making pattern matching;elicitation technique;vtls;inspection technique;validation technique;software quality;buildings;validation and verification;ils;software quality formal specification formal verification quality assurance software maintenance	Requirements engineering techniques are faced with a recurring problem of focus and structure [1]. Elicitation techniques raise the problem of focussing and structuring elicitation sessions and artefacts. Evaluation techniques raise the problem of identifying and comparing items at a common level of abstraction and granularity for risk analysis, conflict management, option selection, or prioritization.	requirement;requirements engineering	Axel van Lamsweerde	2009		10.1109/RE.2009.50	acceleration;reliability engineering;unified modeling language;quality assurance;verification and validation;systems modeling;risk analysis;inspection;formal verification;computer science;systems engineering;engineering;software engineering;requirements elicitation;formal specification;generalized complex structure;requirements engineering;solid modeling;software maintenance;computational model;software quality	SE	-56.42231116821431	27.961135043178302	177689
608db21e1f1df89bf118821db3b7d3242f437f3e	autoquest -- automated quality engineering of event-driven software	usage based testing;api autoquest automated quality engineering event driven software eds testing technique application programming interface;test automation;gui testing test automation event driven software usage based testing usability analysis;usability analysis;testing;software quality application program interfaces program testing;testing graphical user interfaces abstracts concrete monitoring usability;graphical user interfaces;program testing;monitoring;gui testing;abstracts;application program interfaces;usability;software quality;concrete;event driven software	In this paper, we present AutoQUEST, a testing platform for Event-Driven Software (EDS) that decouples the implementation of testing techniques from the concrete platform they should be applied to. AutoQUEST provides the means to define testing techniques against an abstract Application Programming Interface (API) and provides plugins to port the testing techniques to distinct platforms. The requirements on plug-in implementations for AutoQUEST are kept low to keep the porting effort low. We implemented several testing techniques on top of AutoQUEST and provide five plugins for concrete software platforms, which demonstrates the capabililities of our approach.	application programming interface;cloud computing;eclipse modeling framework;electronic document system;event-driven programming;explicit modeling;extended data services;plug-in (computing);quality engineering;requirement;software testing;system under test;usability	Steffen Herbold;Patrick Harms	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation Workshops	10.1109/ICSTW.2013.23	test strategy;black-box testing;concrete;usability;software performance testing;human–computer interaction;white-box testing;system integration testing;computer science;engineering;software reliability testing;software engineering;software construction;graphical user interface;software testing;non-functional testing;programming language;system testing;graphical user interface testing;software quality	SE	-55.73099325592996	32.27643673510724	178027
f2f2cdd693b53085b22ea1e0d71ecf58d778a64c	quantitative analysis of development defects to guide testing: a case study	software testing;software test effectiveness;quality improvement;defect analysis;software component;quantitative analysis;profitability;fault proneness;medical record systems	Many quality improvement activities can be guided by defect analysis. Development defect analysis of software components can be used to guide testing with the goal of focusing on parts of the software that were fault-prone during development. We perform a case study using defect data from a large software product (medical record system). In this study, development defect data help to identify which parts of the software might profit from being tested more and earlier because they were fault-prone during development. Several testing guidelines are proposed to make system test more effective and more efficient.	component-based software engineering;software bug;system testing	Anneliese Amschler Andrews;Catherine Stringfellow	2001	Software Quality Journal	10.1023/A:1013353520454	non-regression testing;reliability engineering;development testing;quality management;verification and validation;regression testing;software sizing;economics;system integration testing;systems engineering;quantitative analysis;engineering;acceptance testing;package development process;software reliability testing;operations management;component-based software engineering;software engineering;software construction;software testing;software metric;profitability index;software quality analyst	SE	-62.70367467583091	28.02783867336267	178336
8e4e1547bed743cac1104c512395cee1b7e90844	guaranteeing correct evolution of software product lines: setting up the problem	novel approach;assume-guarantee reasoning;research question;model check;correct evolution;core component;test assume-guarantee property;correct functioning;software product line;spl component;exploratory paper	The research question that we posed ourselves and which has led to this paper is: how can we guarantee the correct functioning of products of an SPL when core components evolve? This exploratory paper merely proposes an overview of a novel approach that, by extending and adapting assume-guarantee reasoning to evolving SPLs, guarantees the resilience against changes in the environment of products of an SPL. The idea is to selectively model check and test assume-guarantee properties on those SPL components affected by the changes.		Maurice H. ter Beek;Henry Muccini;Patrizio Pelliccione	2011		10.1007/978-3-642-24124-6_9	reliability engineering;real-time computing;engineering;operating system	SE	-56.20815627631428	28.224080032215383	178898
1c4fe9f7158743b4d35574ee882360de4da8bf35	transformation rules for platform independent testing: an empirical study	avionics;software testing;navigation unified modeling language safety testing electronic mail semantics aerospace electronics;electronic mail;functional testing;technology;computer and information science;semantics;testing;model driven development;teknik;natural sciences;navigation;large avionic applications transformation rules platform independent testing model driven development projects model level functional testing logic based test requirements logic based coverage criteria safety critical software mcdc;program testing;safety critical software;safety;unified modeling language;aerospace electronics;safety critical software avionics program testing;transformation rules;platform independent model;test requirements;empirical studies;data och informationsvetenskap;platform independent	Most Model-Driven Development projects focus on model-level functional testing. However, our recent study found an average of 67% additional logic-based test requirements from the code compared to the design model. The fact that full coverage at the design model level does not guarantee full coverage at the code level indicates that there are semantic behaviors in the model that model-based tests might miss, e.g., conditional behaviors that are not explicitly expressed as predicates and therefore not tested by logic-based coverage criteria. Avionics standards require that the structure of safety critical software is covered according to logic-based coverage criteria, including MCDC for the highest safety level. However, the standards also require that each test must be derived from the requirements. This combination makes designing tests hard, time consuming and expensive to design. This paper defines a new model that uses transformation rules to help testers define tests at the platform independent model level. The transformation rules have been applied to six large avionic applications. The results show that the new model reduced the difference between model and code with respect to the number of additional test requirements from an average of 67% to 0% in most cases and less than 1% for all applications.	avionics;functional testing;model-driven engineering;platform-independent model;requirement	Anders Eriksson;Birgitta Lindström;A. Jefferson Offutt	2013	2013 IEEE Sixth International Conference on Software Testing, Verification and Validation	10.1109/ICST.2013.28	avionics;reliability engineering;simulation;computer science;systems engineering;engineering;operating system;software engineering;semantics;software testing;programming language	SE	-57.18272160742948	29.657745926271332	179446
0e7fb6d7b11fd593157708004657af5ee0cf284d	a family of experiments to evaluate the understandability of tristar and i* for modeling teleo-reactive systems	understandability;i;tristar;requirements engineering;lenguajes y sistemas informaticos;teleo reactive	The teleo-reactive approach facilitates reactive system development without losing sight of the system goals. Objective: To introduce TRiStar as an extension of i∗ notation to specify teleo-reactive systems. To evaluate whether the notational extension is an improvement in terms of effectiveness and efficiency over the original language when it is used to specify teleo-reactive systems. Method: A family of experiments was carried out with final-year engineering students and experienced software development professionals in which the participants were asked to fill in a form designed to evaluate the efficiency and effectiveness of each of the languages. Results: Both the statistical results of the experiments, analyzed separately, and the meta-analysis of the experiments as a whole, allow us to conclude that TRiStar notation is more effective and efficient than i∗ as a requirements specification language for modeling teleo-reactive systems. Conclusion: The extensions made on i∗ have led to TRiStar definition, a more effective and efficient goaloriented notation than the original i∗ language. © 2016 Elsevier Inc. All rights reserved.	experiment;functional reactive programming;requirement;software development;software requirements specification;specification language;tristar 64	José Miguel Morales;Elena M Navarro;Pedro Sánchez-Palma;Diego Alonso	2016	Journal of Systems and Software	10.1016/j.jss.2015.12.056	reliability engineering;computer science;systems engineering;operating system;software engineering;requirements engineering;programming language;management;engineering drawing	SE	-58.75080327329159	30.31888892197364	180499
b9b31111b50013bf5837d006a407fcf00b5f43d7	a self-configuring test harness for web applications	automated testing;new technology;regression testing;web interface;self testing;model driven development;specification tests;test generation;self configuration;web technology;autonomic computing	As web applications become more complex and dependent on numerous web technologies, regression testing becomes a time-consuming and expensive endeavor. Many approaches have been proposed to automate the testing process; however few of the approaches utilize the concepts of autonomic computing to symplify the management of a test harness. In this paper, we introduce a self-configuring autonomic test harness. Our test harness monitors the client-side of a web application to detect which web technologies were used to implement the interface. It then automatically selects a testing tool capable of validating any technology-specific features, generates an automated test script, and runs it on the web application.  Test generation is performed using a model-driven approach which encodes test cases in a platform independent manner, and transforms them into platform specific tests using a model of various web technologies. In this way, our approach supports regression testing from the web interface as the application migrates to new technologies. We also present the details of a prototype used to demonstrate the approach.	autonomic computing;autonomic networking;client-side;model-driven architecture;prototype;regression testing;test automation;test case;test harness;test script;user interface;web application	Jairo Pava;Courtney Enoex;Yanelis Hernandez	2009		10.1145/1566445.1566533	web service;embedded system;web modeling;simulation;web-based simulation;web design;white-box testing;engineering;cloud testing;world wide web;graphical user interface testing;test management approach;web testing;test harness	SE	-56.020844672806064	31.514096003188868	181465
bcc0a13a4e641c28848d4900c1628fb0ecd0b7e9	"""""""the babel experiment"""": an advanced pantomime-based training in ooa&ood with uml"""	positive feedback;object oriented design;uml;software systems;pantomime;oop;hands on training;it education;software development;unified modeling language;object oriented analysis and design;ood;object oriented analysis;ooa	"""In this paper we present the original method of intensive hands-on training in Object-oriented Analysis and Design (OOA/OOD) with the Unified Modeling Language (UML). The method has been successfully used by the authors for three years.During the training, the students:go through the communication problems that are typical for large software development projects obtain the successful experience of applying UML to overcome these problems.The essence of the method is that a team of students is supposed to design a software system. They have several hours to complete the task. During this timeframe verbal and written communication is forbidden, and the UML is the only allowed language. This training is a kind of experiment for students -- they are to discover whether UML is """"a real language"""" that is suitable and beneficial for a project team.The training was successfully delivered more than ten times in both academic and corporate environments and generated positive feedback from students and customers."""	hands-on computing;positive feedback;software development;software system;unified modeling language	Vladimir L. Pavlov;Anton Yatsenko	2005		10.1145/1047344.1047426	object-oriented analysis and design;unified modeling language;simulation;uml tool;computer science;operating system;object-oriented design;software engineering;applications of uml;programming language;object constraint language	SE	-59.96010529552336	27.04616174533227	181977
8166b0daf42af6b7c8da24959ad51e58062692bd	an innovative model for object-oriented costs estimating	mots-clés : object-oriented;development effort.;estimation des coûts;cost estimation;effort de dèveloppement. key words : object oriented;object oriented	The object-oriented technological evolution involving the software industry induces changes in the managerial and organizational models for the software-houses. One of the critical issue for software producers is the cost estimation and control. The research described in this paper is aimed to verify the inadequacy of traditional software cost estimation models and to define an innovative model tailored to the specificity of object-oriented technology. The comparison to other different models allows to discuss the resulting benefits and drawbacks. MOTS-CLÉS : object-oriented, estimation des coûts, effort de dèveloppement.	performance;purchasing;sensitivity and specificity;software development effort estimation;software industry	V. Giliberti;Michele Gorgoglione;R. Vitulli	1997			cost database;software metric;reliability engineering;cost estimate;object-oriented programming;computer science;systems engineering;software sizing	SE	-62.18470709670181	27.64442047977918	182879
8ee5022026c2a810499bf7f543625fbdc8f57bfa	model driven development of mobile applications using drools knowledge-based rule		Nowadays, the significance of automatic transformation of object oriented code from design models has increased due to its benefits such as cost reduction and time efficiency. Model driven development (MDD) absolutely needs automatic approach and method to generate system from model. This paper contributes the efficient Drools rule-based transformation approach to automate the mobile application development process. In our approach, the textual model Umple considered as primary artifacts that drive the whole mobile application. The consistency of source and target model and the assessment of transformability are the critical issues in model transformation domain. In this paper, we attempt to address these issues with measuring the accuracy of consistency between source and target model and assessing the transformability using object oriented metrics. Results reveal that our approach achieved the high accuracy of consistency and sound quality of transformability.	drools;logic programming;mobile app;model transformation;model-driven engineering;sound quality;umple	Ei Ei Thu;Nwe Nwe	2017	2017 IEEE 15th International Conference on Software Engineering Research, Management and Applications (SERA)	10.1109/SERA.2017.7965726	theoretical computer science;data mining;simulation;humanoid robot;object-oriented modeling;sound quality;model transformation;unified modeling language;computer science;object-oriented programming;cost reduction;umple	SE	-58.34226243154656	28.562169907573807	182918
cae914b330e54745c53b1f99d6f11d4e668a9a08	formal versus agile: survival of the fittest	agile methods;software engineering agile methods formal methods;programming object oriented modeling erbium collaborative software software engineering hardware productivity scalability engines collaborative work;school of no longer in use;electronics and computer science;ucl;metodo formal;sobrevivencia;methode formelle;formal methods;discovery;theses;conference proceedings;testing;programmation agile;agile method;data mining;software engineering;agile programming;formal method;digital web resources;ucl discovery;open access;safety;survie;ucl library;programacion agil;book chapters;open access repository;survival;programming;article;agile method formal method;agile software engineering;ucl research	Many research have focused on new formal methods, integrating formal methods into agile ones, and assessing the agility of formal methods. This paper proves that formal methods can survive in an agile world; they are not obsolete and can be integrated into it. The potential for combining agile and formal methods holds promise. It might not always be an easy partnership, and succeeding will depend on a fruitful interchange of expertise between the two communities. Conducting a realistic trial project using a combined approach with an appropriate formal methods tool in a controlled environment will help assess the effectiveness of such an approach.	agile software development;formal methods	Sue Black;Paul Boca;Jonathan P. Bowen;Jason Gorman;Michael G. Hinchey	2009	Computer	10.1109/MC.2009.284	simulation;formal methods;agile unified process;computer science;artificial intelligence;operating system;software engineering;agile software development;management;computer security	AI	-58.954828002160866	25.85424841430591	183201
52551d5ad7fc5de5bb5a6e570a65b0ae4a6f40b9	an evolutionary methodology for optimized feature selection in software product lines		Feature modeling is the primary technology to capture and document the commonalities and variability among all of the members in a product line. Individual products are customized by selecting features according to the requirements. The work of feature selection is complex because of: 1) the complex dependencies and constraint relationship amongst features; 2) the multiple competing and conflicting non-functional requirements (NFRs); 3) the constraints to NFRs; 4) the explicit functional requirements. To select optimized feature set that conforms to the feature relations and satisfies both the functional and nonfunctional requirements and the related constraints, an evolutionary algorithm template which employs multi-objective optimization algorithms to optimally select features in SPLs, is proposed. In the experiments, two different algorithms are designed based on our template. Empirical results show the remarking performance of our algorithms on time especially when the feature models are large and complex. Keywords-Product Line Engineering;feature selection;multiobjective optimization;non-functional optimization	evolutionary algorithm;experiment;feature model;feature selection;functional requirement;heart rate variability;mathematical optimization;multi-objective optimization;non-functional requirement;software product line	Xiaoli Lian;Xiang Lin	2014			computer science;systems engineering;data mining;feature selection;software	SE	-55.66516037358969	26.476310372123336	183676
3a1553f3bdabb7f897ee0d6cce22b1d6dac64f35	formal software analysis emerging trends in software model checking	algorithm design and analysis;cost effectiveness;mathematical model;computer languages;computer science;model checking;information analysis;software analysis;embedded software;software engineering	The study of methodologies and techniques to produce correct software has been active for four decades. During this period, researchers have developed and investigated a wide variety of approaches, but techniques based on mathematical modeling of program behavior have been a particular focus since they offer the promise of both finding errors and assuring important program properties. The past fifteen years have seen a marked and accelerating shift towards algorithmic formal reasoning about program behavior - we refer to these as formal software analysis. In this paper, we define formal software analyses as having several important properties that distinguish them from other forms of software analysis. We describe three foundational formal software analyses, but focus on the adaptation of model checking to reason about software. We review emerging trends in software model checking and identify future directions that promise to significantly improve its cost-effectiveness.	abstract interpretation;mathematical model;model checking;scalability;software system;xojo	Matthew B. Dwyer;John Hatcliff;Robby;Corina S. Pasareanu;Willem Visser	2007	Future of Software Engineering (FOSE '07)		model checking;algorithm design;formal methods;cost-effectiveness analysis;embedded software;computer science;systems engineering;software design;theoretical computer science;software framework;software development;software design description;software analysis pattern;software engineering;software construction;mathematical model;formal specification;software walkthrough;data analysis;programming language;software analytics;software metric	SE	-62.825183098321304	29.73552617147505	183715
424fbd0ba470b9e76e3e6cba20b0eeab05756157	towards inverse uncertainty quantification in software development (short paper)		With the purpose of delivering more robust systems, this paper revisits the problem of Inverse Uncertainty Quantification that is related to the discrepancy between the measured data at runtime (while the system executes) and the formal specification (i.e., a mathematical model) of the system under consideration, and the value calibration of unknown parameters in the model. We foster an approach to quantify and mitigate system uncertainty during the development cycle by combining Bayesian reasoning and online Model-based testing.	analysis of algorithms;bayesian network;discrepancy function;experiment;formal specification;incremental backup;mathematical model;model checking;model-based testing;run time (program lifecycle phase);software deployment;software development process;statistical model;stochastic optimization;systems development life cycle;uncertainty quantification	Matteo Camilli;Angelo Gargantini;Patrizia Scandurra;Carlo Bellettini	2017		10.1007/978-3-319-66197-1_24	calibration;formal specification;uncertainty quantification;software development;machine learning;systems engineering;computer science;online model;inverse;artificial intelligence;bayesian inference	SE	-60.66821998688507	30.798129802738973	184212
088d278ae4635ce8c6a69d778733315e18ed918d	a knowledge management approach for industrial model-based testing	knowledge management;model based testing	This paper offers a knowledge management method for industrial model-based testing, which based on partial specifications and “attached” to the software development process that uses it. Partial specification means formal description of considerable/potentially problematic properties of a system, and is used for further automated testing. That allows reducing expenses of testing compared to developing full formal specifications. The “attached” nature of the method means that the team of testers can work independently of the basic process, without imposing on it any specific limitations connected with model-based testing. The method intends for lightweight processes where a lack of documentation and formal described requirements are absent. The paper also presents approbation of the method while testing an industrial Webapplication by means of model-based testing technology UniTesk in DataArt Inc. software company.	documentation;formal specification;iterative method;knowledge management;library (computing);model checking;model-based testing;requirement;software development process;software verification;test automation	D. V. Koznov;Vasily Malinov;Eugene Sokhransky;Marina Novikova	2009			test strategy;model-based testing;data management;computer science;knowledge management;acceptance testing;knowledge engineering;test management approach	SE	-58.47802288263085	27.78694111152429	184408
cc0c3e8915a91191cefcf86fda87ae111690aac9	evaluation of class testing complexity with object metrics.		ABSTRACT Any activities enabling one to determine the complexity of object oriented software and estimate the effort needed for testing already in the early stage of system development are very useful. Some object metrics can be used to locate parts of the design that could be error prone or complex to test. In this paper methods allowing proper calculation of class metrics for some commercial CASE tool have been described. Metrics, calculable on the basis of information kept in CASE repository and useful in the estimation of testing effort have been presented. A new class metric was also proposed. KEY WORDS: software engineering, object metrics, testing complexity. 1. INTRODUCTION Internet and multimedia software systems are often constructed using object oriented design methods and implemented in C++ or Java. Availability of a suitable and adequate measuring tool already at the early stage of a program development enables early prediction of the system complexity thus reducing the cost of making necessary changes. In recent years many researchers and practitioners have proposed metric for object oriented software [1,2,3,4]. Object metrics bibliography can be also found at //dec/bournemounth.ac.uk/ESERG/bibliography.html. Almost all research has been dedicated associating with source code a complexity number based on some rules. The complexity number can be associated either with a class [2] or with the whole project [4]. Class-level metrics can be used to identify error prone classes, to estimate testing effort and the possibility of code reuse, and to improve the quality of the class code as well. In section 2 some “class-level” metrics, which can be used to predict testing complexity, are described. The brief survey of “system-level” metrics can be found in [5]. Since the 1980s a wide range of tools to support software development have been constructed. CASE (Computer Aided Software Engineering) technology can be applied to significantly reduce software costs and development time and to increase the quality of software. The CASE workbenches may support specific design and analysis method (Select - UML) or may constitute more general diagram editing systems with the knowledge of the most common methods (Paradigm). All information about the project is kept in the repository whose structure is proprietary to the vendor and therefore its environment is usually closed. In Paradigm CASE a language [6] has been provided which enables access to almost all information kept in the repository. In Select Enterpice programs written in Visual Basics can access Select repository. Most of the metrics proposed by Chidamber- Kemerer [2] have been implemented in Paradigm script language. Few metrics, suitable for the prediction of testing effort, have been described in section 4. They can be used to reduce the complexity of the design at early stages and do not depend on the implementation language used. Paradigm enables the generation of code in many languages (C++, Java, etc) but similarly to other CASE tools (eg. Objectiff, Select, Rose) does not enable calculation of object-oriented metrics. To our best knowledge only one CASE tool, OODesigner [7], capable of evaluating useful metrics has been developed in Korea. The scripts, evaluating metrics, can be easily used in the software development supported by Paradigm or other object CASE tools providing a language to extract information from the repository. They can significantly help the development team in finding software fragments difficult to test and error-prone classes.		Ilona Bluemke	2001			software engineering;code reuse;object-oriented design;software system;computer-aided software engineering;source code;software;software development;unified modeling language;computer science	SE	-58.80678821978634	31.54930661895042	184749
af1a40207ab6bb7d086979663738d4602ad19ab0	static and completion analysis for planning knowledge base development and verification.	errors;anaylsis techniques;expert systems;software verification;rule based;scripts rule based expert systems static kb analysis syntatic errors completion;development tool;systems analysis;knowledge based systems;ai planning;knowledge base;expert system	A key obstacle hampering fielding of AI planning applications is the considerable expense of developing, verifying, updating, and maintaining the planning knowledge base (KB). Planning systems must be able to compare favorably in terms of software lifecycle costs to other means of automation such as scripts or rule-based expert systems. Consequently, in order to field real systems, planning practitioners must be able to provide: 1. tools to allow domain experts to create and debug their own planning knowledge bases; 2. tools for software verification, validation, and testing; and 3. tools to facilitate updates and maintenance of the planning knowledge base. This paper describes two types of tools for planning knowledge base development: static KB analysis techniques to detect certain classes of syntactic errors in a planning knowledge base; and completion analysis techniques, to interactively debug the planning knowledge base. We describe these knowledge development tools and describe empirical results documenting the usefulness of these tools.	automated planning and scheduling;expert system;interactivity;knowledge base;logic programming;programming tool;software development process;software documentation;verification and validation	Steve A. Chien	1996			legal expert system;knowledge base;computer science;systems engineering;knowledge management;knowledge-based systems;data mining	AI	-56.41276918818788	29.55467586880207	184771
44d565a519da65ff6321017a68ca3658ffe09604	supporting semi-automatic co-evolution of architecture and fault tree models		Abstract During the whole life-cycle of software-intensive systems in safety-critical domains, system models must consistently co-evolve with quality evaluation models like fault trees. However, performing these co-evolution steps is a cumbersome and often manual task. To understand this problem in detail, we have analyzed the evolution and mined common changes of architecture and fault tree models for a set of evolution scenarios of a part of a factory automation system called Pick and Place Unit. On the other hand, we designed a set of intra- and inter-model transformation rules which fully cover the evolution scenarios of the case study and which offer the potential to semi-automate the co-evolution process. In particular, we validated these rules with respect to completeness and evaluated them by a comparison to typical visual editor operations. Our results show a significant reduction of the amount of required user interactions in order to realize the co-evolution.	fault tree analysis;semiconductor industry	Sinem Getir;Lars Grunske;André van Hoorn;Timo Kehrer;Yannic Noller;Matthias Tichy	2018	Journal of Systems and Software	10.1016/j.jss.2018.04.001	real-time computing;completeness (statistics);architecture;automation;metamodeling;fault tree analysis;software;computer science;smt placement equipment	SE	-57.24311210076055	30.397402045123222	184828
d36b38255a1e3e0cf941317f28e73529e87de568	retrieving reusable components with variation points from software product lines	algorithm analysis;retrieval;reutilizacion;efficiency;produit logiciel;software systems;systeme recuperation;software engineering;reuse;eficacia;informatique theorique;feature extraction;variation point;feature;genie logiciel;efficacite;analyse algorithme;extraction caracteristique;reusable component;point variation;software product line;ingenieria informatica;analisis algoritmo;reutilisation;computer theory;informatica teorica;retrieval systems	Reusing software through software product lines has been recognized as useful. To improve reuse efficiency, retrieving proper systems or subsystems from software product lines for reuse is an important issue. This paper proposes a technique to retrieve reusable software systems/subsystems from software product lines. © 2006 Elsevier B.V. All rights reserved.	software product line;software system	Shih-Chien Chou;Yuan-Chien Chen	2006	Inf. Process. Lett.	10.1016/j.ipl.2006.02.015	domain analysis;verification and validation;software sizing;feature;feature extraction;computer science;package development process;backporting;software framework;component-based software engineering;software development;software design description;domain engineering;software construction;reuse;efficiency;resource-oriented architecture;software deployment;algorithm;software system	SE	-58.467613826262586	32.130880846023544	185188
3ef81254fb292222ec5af3207688d916e15910c0	a customer oriented methodology for reverse engineering software selection in the computer aided inspection scenario	software;fuzzy ahp;computer aided inspection cai;reverse engineering	Industrial metrology of mechanical components has been facing a gradual revolution recently through the application of contactless 3D scanners in computer aided inspection. Following the industrial trend and request, new modules for quality control activities on scan data have been integrated in reverse engineering software packages. The aim of this work is to propose a structured methodology for the screening and comparison of reverse engineering programs that are suitable for inspection activities. Specific features that distinguish this kind of software are described and detailed. Six different commercial software are tested using invariant scan data from a reference part that was specifically designed for inspection purposes. The selected packages are then compared by means of a multicriteria fuzzy AHP analysis that considers quantitative and qualitative criteria. The criteria were grouped into three categories related with the user, the vendor and the technical requirements. Two different scenarios are considered for the choice of the software that best suits to computer aided inspection activities. 2014 Elsevier B.V. All rights reserved.	3d scanner;commercial software;computer-aided inspection;contactless smart card;industrial robot;requirement;reverse engineering	Paolo Minetola;Luca Iuliano;Flaviana Calignano	2015	Computers in Industry	10.1016/j.compind.2014.11.002	reliability engineering;verification and validation;software engineering process group;software sizing;computer science;systems engineering;engineering;artificial intelligence;package development process;software development;software construction;software inspection;database;software walkthrough;software measurement;software deployment;computer-aided software engineering;engineering drawing;software requirements;reverse engineering;software system;mechanical engineering	SE	-62.35516390280587	29.52211266877873	185447
b8a5022156e31209deea4c815ce3655353b15ca5	backfiring: converting lines of code to function points	empirical data;software metrics;software;computer languages;high level languages;software cost estimation;interfaces;source code volume;programming language;software measurement;inquiries;function point;functional programming;function point metrics;public domain;input output;assembly;counterfeiting;protection;lines of code;function point total;internet;bidirectional equations;inputs;software projects;productivity high level languages protection counterfeiting internet equations functional programming software power generation power generation economics;lab on a chip;outputs;software development management programming languages software metrics software cost estimation human resource management;power generation;source code;logical files;productivity;direct mathematical conversion;human resource management;software development management;power generation economics;programming languages;function point total empirical data software projects function point metrics lines of code metrics backfiring direct mathematical conversion bidirectional equations source code volume programming language inputs outputs logical files inquiries interfaces;backfiring;lines of code metrics;visual databases	The availability of empirical data from projects that use both function-point and lines-of-code metrics has led to a useful technique called backfiring. Backfiring is the direct mathematical conversion of LOC data into equivalent function-point data. Because the backfiring equations are bidirectional, they also provide a powerful way of sizing, or predicting, source-code volume for any known programming language or combination of languages. The function-point metric, invented by A.J. Albrecht of IBM in the middle 1970s, is a synthetic metric derived by a weighted formula that includes five elements: inputs, outputs, logical files, inquiries, and interfaces. IBM put it into the public domain in 1979, and its use spread rapidly, particularly after the formation of the International Function Point Users Group (IFPUG) in the mid-1980s. By then, hundreds of software projects had been measured using both function points and lines of source code. Since an application's function-point total is independent of the source code, this dual analysis has led to several important discoveries. >	function point	Capers Jones	1995	IEEE Computer	10.1109/2.471193	input/output;electricity generation;productivity;public domain;the internet;empirical evidence;back-fire;lab-on-a-chip;computer science;artificial intelligence;theoretical computer science;operating system;function point;software engineering;human resource management;interface;assembly;programming language;functional programming;software measurement;source lines of code;high-level programming language;computer security;software metric;source code	Vision	-60.66024646322847	31.980471864184246	186194
a1849d0a00d9e7638de9ddefad6effeaf2d5a80b	leveraging task-based data to support functional testing of web applications	web application testing;tasks;graph algorithms	Testing is paramount in order to assure the quality of a software product. Over the last years, several techniques have been proposed to leverage the testing phase as a simple and efficient step during software development. However, the features of the web environment make application testing fairly complex. The existing approaches for web application testing are usually driven to specific scenarios or application types, and few solutions are targeted for testing the functional requirements of applications. In order to tackle this problem, we propose a task-based testing approach that provides high coverage of functional requirements. Our technique consists of reassembling classical graph algorithms in order to generate all the possible paths for the execution of a task. Performed experiments indicate that our approach is effective for supporting the functional testing of web applications.	algorithm;experiment;functional requirement;functional testing;graph theory;software development;web application;web testing	Flávio Rezende de Jesus;Leandro Guarino de Vasconcelos;Laércio Augusto Baldochi	2015		10.1145/2695664.2695917	orthogonal array testing;software performance testing;white-box testing;system integration testing;computer science;software reliability testing;theoretical computer science;operating system;machine learning;cloud testing;database;distributed computing;software testing;non-functional testing;programming language;system testing;world wide web;web testing	SE	-57.20359559758138	31.750292127752047	186644
5b7115e28211f39058167eabe4580f87a31b84f9	development of safety-critical systems and model-based risk analysis with uml	risk analysis;risk management;safety critical system	The high quality development of safety-critical systems is difficult. Many safety-critical systems are developed, deployed, and used that do not satisfy their criticality requirements, sometimes with spectacular failures. Part of the difficulty of safety-critical systems development is that correctness is often in conflict with cost. Where thorough methods of system design pose high cost through personnel training and use, they are all too often avoided. UML offers an unprecedented opportunity for high-quality safetycritical systems development that is feasible in an industrial context.	correctness (computer science);criticality matrix;display resolution;requirement;software development process;systems design;unified modeling language	Jan Jürjens;Siv Hilde Houmb	2003		10.1007/978-3-540-45214-0_28	reliability engineering;risk analysis;it risk management;risk management;systems engineering;risk analysis;system safety	SE	-58.541068901620825	26.394585776736115	186852
23dd27e90183d66e043263227c5205d01f4df477	conflict characterization and analysis of non functional requirements: an experimental approach	formal specification;software quality conflict characterization nonfunctional requirements analysis nfrs conflicts nfrs relative characteristic quantitative evidence nfrs metrics nfrs measures;measurement ontologies context security usability conferences;software quality formal specification;experiment non functional requirements conflict relative identification characterization analysis management framework;software quality;conference proceeding	Prior studies reveal that conflicts among Non Functional Requirements (NFRs) are not always absolute. They can also be relative depending on the context of the system being developed. Given that existing techniques to manage the NFRs conflicts are mainly focused on cataloguing the interrelationships among various types of NFRs, hence a technique to manage the NFRs conflicts with respect to NFRs relative characteristic is needed. This paper presents a novel framework to manage the conflicts among NFRs with respect to NFRs relative characteristic. By applying an experimental approach, the quantitative evidence of NFRs conflicts will be obtained and modeled. NFRs metrics and measures will be used in the experiments as parameters to generate the quantitative evidence. This evidence can then allow developers to identify and reason about the NFRs conflicts. We also provide an example of how this framework could be applied.	experiment;functional requirement;non-functional requirement	Dewi Mairiza;Didar Zowghi;Vincenzo Gervasi	2013	2013 IEEE 12th International Conference on Intelligent Software Methodologies, Tools and Techniques (SoMeT)	10.1109/SoMeT.2013.6645645	reliability engineering;computer science;systems engineering;software engineering;data mining;formal specification;programming language;software quality	SE	-59.85726177091648	27.446274487467573	187026
d8972af90935116eba609bb9a074931c3d62aa49	specialising in software engineering	physical problem world software engineering quasi formal computing machine non formal problem world;software engineering;software engineering design engineering power engineering and energy roads bridges physics computing chemical engineering marine vehicles problem solving protection;software engineering problem solving;problem solving	"""Developing a software-intensive system is engineering in the traditional sense: creating an artifact which transforms the physical world to meet some recognised need. The artifact is the hardware-software machine; the physical world is the system's environment; and the recognised need is the requirement. For a successful development the entailment must hold: """"machine, environment, requirement"""". By formalising the requirement and relevant environment properties, we may hope to bring the whole development within the ambit of formal development methods, improving our ability to produce a machine that will reliably ensure satisfaction of the requirement. This hope is appealing, but too simple. The nature of software-intensive systems poses rich challenges that cannot be addressed by formal reasoning alone but demand sharply focused practical disciplines. Discrete complexity, proliferation of interacting features and interoperation between disparate systems must all be reconciled with high levels of reliability, safety and security, in the context of systems that combine formal software with a non-formal environment in a tight cooperation. They challenge us to evolve rigorous and effective treatments of their non-formal worlds and of the interactions of those worlds with the formal artifacts we build. Here there is much to be learned from the established branches of engineering. The most conspicuous characteristic of the established branches of engineering is their specialised nature. For each class of artifact, the creation and gradual enrichment of a particular specialism, over many years, has allowed engineers to address both the positive and the negative requirements of the class. Positive requirements embody desired system functionality and efficiency, and evolve by invention and improved technology. Negative requirements embody the avoidance or mitigation of defects and failures that have manifested themselves in the past or can be foreseen. The non-formal world furnishes unbounded possibilities of failure. Because every formalisation is in some degree incorrect or unreliable, and its alphabet inevitably excludes phenomena that may yet prove disastrously significant, negative requirements can never be exhaustively enumerated. Dependability in addressing negative requirements is improved chiefly by the experience, recognition, and analysis of failures. To avoid repeating those failures engineers need a structure of concepts and practice that allows the lessons to be recorded, disseminated, and applied. Specialisation is the sine qua non of such a structure. For each class of artifact and system it nourishes the creation and growth of a sharply focused normal design discipline, in which the design engineer knows at the outset how the system must work at every level, what features it must have, and how its parts are to be configured. Such a standardised design explicitly satisfies the positive requirements and implicitly satisfies the currently known negative requirements. Further, practice of the design discipline directs the engineer's attention to specific concerns that are known or expected to merit special care if failure is to be avoided. Specialisation and normal design have enabled the established engineering branches to deal increasingly successfully with the non-formal physical world. Software engineering must similarly identify the dimensions of specialisation that will improve system dependability. Approaches based on erecting a specification firewall to isolate the formal software world of computer science from its non-formal environment cannot succeed. In a software- intensive system, interaction at the interface between the machine and its environment darts back and forth along the arcs of a dependency graph that embraces both machine and environment, and frustrates the desire for a total separation of concerns between the formal and the non-formal. This challenge to software engineering can be met only by sharply focused specialisms practising highly developed normal design disciplines, analogous to those that characterise the established engineering branches."""	ambit;computer science;dos;degree (graph theory);dependability;firewall (computing);formal methods;gene ontology term enrichment;interaction;interoperation;requirement;separation of concerns;software engineering	Michael Jackson	2007	Fifth IEEE International Conference on Software Engineering and Formal Methods (SEFM 2007)	10.1109/APSEC.2007.85	biological systems engineering;computing;mechatronics;software engineering process group;search-based software engineering;computer science;systems engineering;engineering;social software engineering;informatics engineering;component-based software engineering;software development;feature-oriented domain analysis;civil engineering software;software engineering;software construction;requirements engineering;engineering mathematics;software requirements;computer engineering;mechanical engineering	SE	-58.29473583954787	25.893540635739964	187059
1777aca06d29beaf15f2d8c50182c317e6f22158	evaluation of software architecture quality attribute for an internet banking system	international journal of computer applications ijca	The design phase plays a vital role than all other phases in the software development. Software Architecture has to meet both the functional and non-functional quality requirements. The Evaluation of Architecture has to be performed, so that the developers are assured that their selected Architecture will reduce the cost and effort and also enhances the various quality attributes like Availability, Reusability, Performance, Modifiability and Extendibility. The success of the system depends upon the Architecture Evaluation by the essential method to the system. The overall ranking of the candidate architecture is ascertained by assigning weight to the scenario and scenario interaction. In this paper, SAAM method is taken to evaluate the two architectures from the various available method and techniques to achieve the various quality attributes by weight metric.	list of system quality attributes;online banking;requirement;software architecture analysis method;software development	A. Meiappane;B. Chithra;V. Prasanna Venkataesan	2013	CoRR	10.5120/10189-5062	reference architecture;software architecture;space-based architecture;website architecture;database-centric architecture;computer science;operating system;resource-oriented architecture;computer security;data architecture;systems architecture	SE	-59.95731591088483	26.049742206422955	187080
17182dffa70ba9ad6eb72f0949dbb9b66520bf6c	middleware specialization for product-lines using feature-oriented reverse engineering	software;manuals;closure;design engineering;software reusability middleware product development reverse engineering;specialization;information technology;middleware reverse engineering computer science costs testing quality of service design engineering packaging information technology maintenance engineering;product line middleware specialization reverse engineering closure footprint feature oriented programming;model based approach;maintenance engineering;testing;product line;packaging;reverse engineer middleware source code middleware specialization software product line feature oriented reverse engineering software feature requirements memory footprint;software feature requirements;feature extraction;feature oriented programming;software reusability;product line engineering;middleware;source code;computer science;quality of service;feature oriented reverse engineering;memory footprint;software product line;middleware specialization;programming;footprint;qualitative evaluation;reverse engineering;reverse engineer middleware source code;product development	Supporting the varied software feature requirements of multiple variants of a software product-line while promoting reuse forces product line engineers to use general-purpose, feature-rich middleware platforms. However, each product variant now incurs memory footprint and performance overhead due to the feature-richness of the middleware along with the increased cost of its testing and maintenance. To address this tension, this paper presents FORMS (Feature-Oriented Reverse Engineering for Mmiddleware Specialization), which is a framework to automatically specialize general-purpose middleware for product-line variants. FORMS provides a novel model-based approach to map product-line variant-specific feature requirements to middleware specific features, which in turn are used to reverse engineer middleware source code and transform it to specialized forms resulting in vertical decompositions. Empirical results evaluating memory footprint reductions (40%) are presented along with qualitative evaluations of reduced maintenance efforts and an assessment of discrepancies in modularization of contemporary middleware.	general-purpose markup language;memory footprint;middleware;overhead (computing);partial template specialization;requirement;reverse engineering;software feature;software product line	Akshay Dabholkar;Aniruddha S. Gokhale	2010	2010 Seventh International Conference on Information Technology: New Generations	10.1109/ITNG.2010.217	maintenance engineering;memory footprint;programming;packaging and labeling;middleware;quality of service;feature extraction;computer science;message oriented middleware;operating system;software engineering;middleware;closure;software testing;programming language;information technology;footprint;new product development;reverse engineering;source code	SE	-56.997729029257044	27.55632737299955	187118
14c23d4b4ddcdfa3f56406fc28e0ba4ea7a2bf14	an integrated environment for software reliability modeling	software reliability modeling;reliability engineering;programming environments;data analysis environment;trademarks;ibm programming systems toronto lab smerfs tool integrated environment software reliability modeling development environment data analysis environment s plus supporting programs c awk;supporting programs;data engineering;visual programming;software reliability trademarks reliability engineering data analysis system testing software tools predictive models software quality data engineering data visualization;data visualisation;s plus;data analysis;development environment;systems analysis;integrated environment;data visualization;ibm programming systems toronto lab;smerfs tool;system testing;predictive models;software tools;awk;c;software reliability;software quality;visual programming software reliability software tools systems analysis programming environments data visualisation;modeling tool	!this paper introduces an integrated environment for software reliability modeling. The environment consiab of three major components: 1) a software reliability modeling tool SMERFS adapted to fit our development environment; 2) a general data analysis environment in S-PL US customized for software relaability modeling; and 3) some supporting programs written in C and AWK. This environment has been successfully used in the IBM Programming Systems Toronto Lab.	awk;comparison of command shells;reliability engineering;software quality;software reliability testing	Jeff Tian;Peng Lu	1993		10.1109/CMPSAC.1993.404246	reliability engineering;computer science;systems engineering;software reliability testing;software development;operating system;software engineering;database;programming language;data visualization;software quality;statistics	SE	-60.995837075960395	31.99005413484253	187297
047a23cf5176bc4d67da52aef7c0328dd94109d6	clarifying the relationship between software architecture and usability	software architecture;software systems	This paper examines in a problem posed recently concerning the relationship between software system usability and architecture. Here, we try to empirically clarify this relationship, focusing on the concept of architecture-sensitive usability mechanism. This concept represents specific usability issues that can improve software usability and that have demonstrated architectural implications. Accordingly, this paper outlines how usability needs to be decomposed to be dealt with from an architectural point of view and how the architecture-sensitive usability mechanism emerges. A list of architecture-sensitive usability mechanisms is presented and the procedure for outputting their respective architectural implications is discussed.	software architecture;software system;usability	Natalia Juristo Juzgado;Ana María Moreno;Isabel Sánchez	2004			resource-oriented architecture;systems engineering;software quality;software design description;software peer review;reference architecture;usability;architecture;computer science;social software engineering	HCI	-59.47570463381161	25.285743821845934	188240
6ca13e3d2d99b6c411991884f501fa7f71a99c18	a methodology for constructing maintainability model of object-oriented design	object-oriented design;modifiability level;modifiability model;metrics-discriminant technique;weighted-score-level technique;maintainability level;maintainability model;modifiability score;weighted sum method;constructing maintainability model;weighted-predicted-level technique;structural complexity;software reliability;discriminant analysis;software metrics;software maintenance;software design;object oriented design	It is obvious that qualities of software design heavily affects on qualities of software ultimately developed. One of claimed advantages of object-oriented paradigm is the ease of maintenance. The main goal of this work is to propose a methodology for constructing maintainability model of object-oriented software design model using three techniques. Two subcharacteristics of maintainability: understandability and modifiability are focused in this work. A controlled experiment is performed in order to construct maintainability models of object-oriented designs using the experimental data. The first maintainability model is constructed using metrics-discriminant technique. This technique analyzes the pattern of correlation between maintainability levels and structural complexity design metrics applying discriminant analysis. The second one is built using weighted-score-level technique. The technique uses a weighted sum method by combining understandability and modifiability levels which are converted from understandability and modifiability scores. The third one is created using weighted-predicted-level technique. Weighted-predicted-level uses a weighted sum method by combining predicted understandability and modifiability level, obtained from applying understandability and modifiability models. This work presents comparison of maintainability models obtained from three techniques.	linear discriminant analysis;programming paradigm;software design;weight function	Matinee Kiewkanya;Nongyao Jindasawat;Pornsiri Muenchaisri	2004	Fourth International Conference onQuality Software, 2004. QSIC 2004. Proceedings.	10.1109/QSIC.2004.1357962	reliability engineering;structural complexity;computer science;systems engineering;engineering;software design;object-oriented design;software engineering;linear discriminant analysis;software maintenance;engineering drawing;software quality;software metric	SE	-60.204908607653856	30.012121947138237	188256
037b5c87b7651d10d66f26e44e9e44993580dcd7	a selective software testing method based on priorities assigned to functional modules	resource allocation program testing formal specification;software testing;formal specification;design of testing specification selective software testing method functional modules priority assignment software systems lifetime short period development test specification test plan process properties product properties test resources;resource allocation;software systems;design of testing specification;program testing;software testing system testing embedded software software systems fault detection application software programming research and development systems engineering and theory resource management;test methods;testing priority;short period;weed management	As software systems have been introduced to many advanced applications, the size of software systems increases so much. Simultaneously, the lifetime of software systems becomes very small and thus their development is required to accomplish within relatively short period. In this paper, we propose a new selective software testing method that aims to attain the requirement of short period development. The proposed method consists of 3 steps: Assign priorities to functional modules (Step 1), Derive a test specification (Step 2), and Construct a test plan (Step 3) according to the priorities. In Step 1, for development of functional modules, we select both product and process properties to calculate priorities. Then, in Step 2, we generate detailed test items for each module according to its priority. Finally, in Step 3, we manage test resources including time and developer’s skill to attain the requirement. As a result of experimental application, we can show superiority of the proposed testing method to the conventional testing method.	functional derivative;software system;software testing;test plan	Masayuki Hirayama;Tetsuya Yamamoto;Jiro Okayasu;Osamu Mizuno;Tohru Kikuno	2001		10.1109/APAQS.2001.990028	non-regression testing;test strategy;keyword-driven testing;reliability engineering;black-box testing;software requirements specification;verification and validation;regression testing;real-time computing;software performance testing;white-box testing;manual testing;system integration testing;integration testing;computer science;systems engineering;acceptance testing;software reliability testing;software development;functional testing;software construction;software testing;system testing;test management approach	SE	-61.75388015141479	30.415725766217022	188286
b17dcca5ba89af2ec456f09232514adc4f5bdb9b	manufacturing execution systems (mes) assessment and investment decision study	manufacturing systems;decision models;comprehensive assessment;financial performance;investment decision;industrial software;anp;investment;manufacturing execution system;bocr;decision methodology;opportunity cost;critical investment decision;analytic network process;decision manufacture execution systems bocr anp assessment;investment decision making;investments manufacturing industries manufacturing automation personnel chaos computer industry software performance costs decision making virtual manufacturing;decision;manufacturing execution systems assessment;manufacturing systems decision making investment;analytic network process method;critical investment decision manufacturing execution systems assessment industrial software comprehensive assessment systematic assessment decision methodology investment decision making analytic network process method;assessment;systematic assessment;manufacture execution systems	Manufacturing execution systems (MES), a category of industrial software for the manufacturing environment, is grasping increasing attention, due to its unique ability to improve manufacturing performance, and then to have a positive impact on the financial performance of the company. For many executives, whether to implement MES to improve manufacturing and financial performance is an issue they must encounter. Since what MES brings about are not only benefits and potential opportunities, but also costs and potential risks, a comprehensive and systematic assessment is absolutely necessary before making such a critical investment decision. This paper proposes a decision methodology for MES investment decision making. In addition, a general decision model with respect to Benefits, Opportunities, Costs and Risks merits is provided. The decision methodology and decision model are validated by an undershirt manufacturer in China, by applying analytic network process (ANP) method.	manufacturing execution system	Liang Chao;Li Qing	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385148	manufacturing execution system;decision analysis;opportunity cost;investment;analytic network process	Robotics	-62.845070891406785	27.801764201592334	188478
777f139c8320808f270d616895be40f1fd637fc4	experiences in object oriented development	object oriented;software development;software life cycle	A wide variety of object-oriented (00) methodologies and tools are currently available for sotlwam development. Each methodology emphasizm various phases and activities of the software life cycle using diffixent terminologies, products, pmcesaes, and techniques for implementation. The impact of the choice of methodology on the conduct and management of a software development effort can be extensive, This paper discusses LW’S selection of an appropriate methodology and describes how the authors have adapted the chosen 00 methodology to support an Ada software development project.	ada;limewire;software development;software release life cycle	John A. Jurik;Roger S. Schemenaur	1992		10.1145/143557.143718	social software engineering;component-based software engineering;software development;object-oriented design;systems development life cycle;resource-oriented architecture;software development process	SE	-59.29539688857574	27.159596751958176	188607
1ea0c210d601aa459206cc8178cc2ac2ed6752b0	priority assessment of software requirements from multiple perspectives	multiple perspectives;systems analysis formal specification;formal specification;priority assessment;collaborative software software systems costs sun computer science software engineering customer satisfaction time to market programming explosions;complex software systems;software systems;requirements elicitation;customer satisfaction;software requirements;systems analysis;complex software systems priority assessment software requirements	The development of complex software systems involves collecting software requirements from various stakeholders. Often stakeholder perceptions conflict during the requirements elicitation phase. An effective technique to resolve such a conflict is needed. We presented a framework that prioritizes software requirements gathered from multiple stakeholders by incorporating inter-perspective relationships, which is not addressed by existing priority assessment techniques. We use a relationship matrix to analyze the impact between requirements and facilitate the integration process which assesses their priorities based on their relationships from multiple perspectives. It allows the development team to resolve conflicts effectively and concentrate their valuable time and resources on the critical few requirements from multiple perspectives that directly contribute to high customer satisfaction.	problem domain;requirement prioritization;requirements elicitation;software project management;software quality;software requirements;software system	Xiaoqing Frank Liu;Chandra Sekhar Veera;Yan Sun;Kunio Noguchi;Yuji Kyoya	2004	Proceedings of the 28th Annual International Computer Software and Applications Conference, 2004. COMPSAC 2004.	10.1109/CMPSAC.2004.1342872	reliability engineering;systems analysis;requirements analysis;personal software process;software requirements specification;verification and validation;requirement prioritization;business requirements;software verification;computer science;systems engineering;package development process;software design;social software engineering;software development;requirement;software engineering;requirements elicitation;software construction;formal specification;functional specification;software walkthrough;customer satisfaction;software deployment;software requirements;software system;software peer review	SE	-60.30985367442756	27.348597857936568	188660
534732be087598c18be38b7861e99b04bbf64f0a	requirements model for cyber-physical system		The development of cyber-physical system (CPS) is a big challenge because of its complexity and its complex requirements. Especially in Requirements Engineering (RE), there exist many redundant and conflict requirements. Eliminating conflict requirements and merged redundant/common requirements lead a challenging task at the elicitation phase in the requirements engineering process for CPS. Collecting and optimizing requirements through appropriate process reduce both development time and cost as every functional requirement gets refined and optimized at very first stage (requirements elicitation phase) of the whole development process. Existing researches have focused on requirements those have already been collected. However, none of the researches have worked on how the requirements are collected and refined. This paper provides a requirements model for CPS that gives a direction about the requirements be gathered, refined and cluster in order to developing the CPS independently. The paper also shows a case study about the application of the proposed model to transport system.	complexity;computer cluster;cyber-physical system;existential quantification;functional requirement;parallel computing;refinement (computing);requirements elicitation;requirements engineering	Md. Masudur Rahman;Naushin Nower	2017	CoRR		software requirements specification;requirements management;requirement prioritization;requirement;system requirements specification;system testing;non-functional requirement;requirements traceability	SE	-58.35158286734535	30.5313518587643	189019
604677cbe94df04c07a4d79a8039b4b6725757bf	mapping process capability models to support integrated software process assessments	process capability;software process assessment	Software process assessments have been used to verify the conformance with quality reference models or standards, usually in a context of software process improvement programs. Most of these assessments are concerned with just one specific model or standard. However, organizations could be interested in more than one model, being necessary to offer an integrated assessment that allows evaluate the conformance with different models at the same time, reducing time and costs. To do this, it is necessary to have some kind of mapping among the models to permit some level of automation. In this context, this paper presents a method to mapping process capability models and standards, including its application. A software tool is also presented to demonstrate how to use the resulting mapping in a real life situation.	conformance testing;integrated software;programming tool;real life;software development process	Marcello Thiry;Alessandra Zoucas;Leornardo Tristao	2010	CLEI Electron. J.	10.19153/cleiej.13.1.4	reliability engineering;personal software process;verification and validation;team software process;process capability;software engineering process group;process capability index;computer science;systems engineering;package development process;software construction;empirical process;goal-driven software development process	SE	-59.04243098925599	27.98623958651183	189129
47910926ca6392e5bc5eca6984d9d35a220e10e7	estimating the software reliability of smoothly degrading systems	software metrics;software testing;degradation;application software;automatic testing;software systems;smoothly degrading systems;telecommunication computing;domain based reliability measure;system degradation degree software reliability estimation smoothly degrading systems domain based reliability measure markov chains load testing algorithm industrial telecommunications systems;load testing algorithm;communication industry;telecommunication computing software reliability markov processes software metrics;industrial telecommunications systems;software reliability degradation system testing software testing software algorithms software systems automatic testing telecommunications computer science application software;software reliability estimation;software algorithms;system testing;system degradation degree;markov processes;computer science;software reliability;telecommunications;markov chains;markov chain	This paper presents the application of a domainbased reliability measure to systems that can be represented b y Markov chains. A load testing algorithm is presented, and the measure is applied to assess the reliability of these systems after they have been tested. Data are presented for three industrial telecommunications systems that had been tested using the load testing algorithm, tracking the reiiability as a function of the degree of system degradation experienced.	algorithm;elegant degradation;load testing;markov chain;smoothing;software quality;software reliability testing	Alberto Avritzer;Elaine J. Weyuker	1994		10.1109/ISSRE.1994.341370	reliability engineering;markov chain;real-time computing;load testing;computer science;systems engineering;engineering;software reliability testing;software engineering;statistics	SE	-62.17745510513197	32.101648384864504	189144
d54bad5caf4133bbbf6f4bc6ab54c3873f8f2f0d	utilizing validation experience for system validation		This paper adopts the idea of using knowledge gained by various validation sessions over time with a validation technology developed previously. The work is designed to reduce the human involvement needed to apply this technology. It introduces the reuse of test cases with the ”best solution”, discovered in previous validation sessions. This reduces the number of test cases to be solved and rated by the experts within the validation process. By reducing the workload of the involved experts, the costs of validation can be reduced. Moreover, this approach may compensate for possible shortages of expertise available for the validation process.	test case	Rainer Knauf;Avelino J. Gonzalez;Setsuo Tsuruta	2003			simulation;data validation;management science;validation rule	SE	-58.63977925065667	28.972314853819785	189356
eb86ef7d2137de3fc4a7e54fd6fb381845d262e2	iim-cbse: an integrated maturity model for cbse	component based software engineering;cmmi;cmm;software components;cbse;capability maturity model integration;capability maturity model	This study presents a newer approach for maturity analysis of component-based software processes. Conventional maturity models aim at learning from experiences of different activities so as to attain maturity. In case of component-based software, we have additional requirements of tracking maturity of software components and the composition process apart from the traditional requirements. Unfortunately, the conventional maturity models do not consider software components (and its interdependence with component-based software) in their considerations of levels and key process areas. It is therefore necessary to consider a maturity model that is based on peculiarities and importance of component-based software and hence a new model under the name integrated maturity model – component-based software engineering (IMM-CBSE) for this purpose is being proposed herein. The proposed model may use as a standard for assessment of component-based software and software components. This work starts a discussion and calls for more extensive research-oriented studies by professionals and academicians for perfection of the model.	capability maturity model;component-based software engineering;experience;feedback;interdependence;requirement;software development process	Ratneshwer Gupta;Anil Kumar Tripathi	2013	IJCAT	10.1504/IJCAT.2013.053423	standard cmmi appraisal method for process improvement;reliability engineering;personal software process;verification and validation;software engineering process group;software sizing;leancmmi;computer science;systems engineering;engineering;capability maturity model integration;social software engineering;component-based software engineering;software development;software engineering;software construction;service integration maturity model;capability maturity model;software quality;capability immaturity model	SE	-59.4807099071521	26.19058106674799	189838
aede417618b184697d4ef3a318000a1c496a2542	a taxonomy of software architecture-based reliability efforts	system reliability;fault tolerant;architecture based reliability;software systems;software architecture;quality requirement;fault tolerance;taxonomy;component model;reliability analysis;architectural style;reliable design and operational activities	Due to the complexity of the current software systems and the diversity of their architectural styles and component models, architecture-based reliability is becoming a more important quality requirement than ever before. Architecture-based reliability efforts depend on the behavior of individual components and their interactions with respect to their influences on the system reliability. Depending on different viewpoints and assumptions, a component takes various definitions and forms. As a result, numerous reliability works that involve varieties of the underlying strategies, objectives, and parameters are proposed for software architectures. Classifying these efforts is important for creating and selecting potential solutions that handle the reliability of software applications. In this paper, we provide a taxonomy of architecture-based reliability efforts. We classify these efforts according to the reliability goals, component abstraction, and level of granularity. We explain the existing techniques considering their assumptions with respect to these classification parameters and provide detailed description about the specific issues and considerations of each class.	interaction;software architecture;software system;taxonomy (general)	Atef Mohamed;Mohammad Zulkernine	2010		10.1145/1833335.1833342	reliability engineering;reference architecture;software architecture;real-time computing;systems engineering;engineering;software reliability testing;software architecture description;resource-oriented architecture	SE	-55.93878336787129	27.33495809126923	190848
79482e1a2f5bfc2998d6ff3f45b01bfa9f93a85f	a methodology for predicting the scalability of distributed production systems.	production system			Charles A. Letner;Richard L. Gimarc	2005			reliability engineering;real-time computing;systems engineering	OS	-61.597608983568534	30.293889821962175	191033
530651afbe89a747f278c0eca10b8b7ded2b32dd	how can previous component use contribute to assessing the use of cots?	object-oriented programming;software packages;cots components;component use;new usage environment;safety claim;software-based component;statistical test data	The intuitive notion exists in industry and among regulators that successful use of a commercially available software-based component over some years and within different application environments must imply some affirmative statement about the quality of the component and - in terms of a safety-case - that it should provide evidence to support a specific safety claim for usage of the component in a specific new environment. Yet, so far a method is lacking to investigate quantitatively how such evidence can inform and influence an estimate for example of the component's probability of failure per demand or per hour, and thus the evidence is not used. Currently there is no blueprint to show us what such evidence contributes to meeting a safety claim. In this paper a route is explored that may allow to make use of such prior evidence and combine it with fresh statistical test data pertaining to the new usage environment. The model proposed is an initial model but it is hoped that it can help to develop over time a framework that can be practically used by regulators and safety assessors to inform a safety case for COTS components containing a software part.	algorithm;blueprint;dependability;test data	Silke Kuball	2007	10th IEEE High Assurance Systems Engineering Symposium (HASE'07)	10.1109/HASE.2007.48		SE	-61.315375974940935	30.183140344085338	192134
e759f0c4bbf961af7d8c377350d7ab74d3b55b01	software testing research and practice	machine abstraite;software testing;maquina abstracta;program verification;abstract machine;verificacion programa;essai logiciel;fiabilite logiciel;fiabilidad logicial;verification programme;software reliability	The paper attempts to provide a comprehensive view of the field of software testing. The objective is to put all the relevant issues into a unified context, although admittedly the overview is biased towards my own research and expertise. In view of the vastness of the field, for each topic problems and approaches are only briefly tackled, with appropriate references provided to dive into them. I do not mean to give here a complete survey of software testing. Rather I intend to show how an unwieldy mix of theoretical and technical problems challenge software testers, and that a large gap exists between the state of the art and of the practice.	software testing	Antonia Bertolino	2003		10.1007/3-540-36498-6_1	verification and validation;simulation;system integration testing;software verification;computer science;artificial intelligence;package development process;software development;software design description;operating system;software construction;database;abstract machine;software testing;algorithm;software quality;software peer review	SE	-60.49868939223517	30.972187327624166	192361
d12b6c42754d6d261474b90440af8c2881532959	a statistical approach for improving the performance of a testing methodology for measurement software	statistical approach;automotive engineering;black box approach;software testing;reliability;confidence level;instruments;functional verification;software measurement;application software;software testing software measurement software performance software safety instruments automotive engineering software engineering application software software maintenance software quality;software maintenance;software engineering;automotive system;software performance;instrument software statistical approach measurement software testing methodology reliability enhancement automotive system;program testing;software safety;automatic test software;measurement software testing methodology;reliability enhancement;statistical testing;design for test;statistical techniques;correlation coefficient;test methodology black box approach instrument software reliability software engineering;software reliability;statistical testing automatic test software program testing software reliability;software quality;instrument software;test methodology	This paper describes the significant enhancements brought to an original methodology designed for testing measurement software. In a previous paper, the authors proposed a black-box seven-step procedure that allows the functional verification of complex instrument software to be performed. The main features of the procedure are concerned with the following: (1) the ability of reproducing actual correlations among the software inputs and (2) the need for a limited number of test cases. Making use of innovative statistical techniques, the methodology performance and reliability have been enhanced. Two further steps have been added with the aim of improving the correlation coefficient assessments and providing the estimations with a confidence level. Finally, a new strategy has been studied to optimize the number of test cases. The effects of the new solutions on the performance of the methodology are evaluated by applying the procedure to a complex software module employed in an automotive system. A comparison with the previous methodology version is also reported.	black box;coefficient;modular programming;psychometric software;test case	Giovanni Betta;Domenico Capriglione;Antonio Pietrosanto;Paolo Sommella	2008	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2007.915143	reliability engineering;regression testing;test data generation;systems engineering;engineering;software reliability testing;software engineering;software testing;software quality	SE	-62.55439456540266	31.42303192251067	192707
d982e952a4c057fa4e59aabeab346704857d9e61	formal specification of hypotheses for assisting computer simulation studies		The aim of computer simulation studies is to answer research questions by means of experiments. For providing reliable evidence, the procedure of the study needs to be aligned with the question and the steps of the study need to be adjusted and combined accordingly. Supporting this process is challenging as the identification, customization, and combination of adequate tools and techniques for the systematic design of simulation experiments with respect to a hypothesis is not trivial. Hence, for providing computer-aided assistance, a language for the specification of hypotheses with respect to the credible and reproducible testing of research questions in computer simulation is needed. In this paper, we propose an approach for formally specifying hypotheses that allows for automated hypothesis testing. Based on specified hypotheses, we demonstrate the assistance of simulation studies in terms of model parametrization and analysis of results with respect to the statistically sound evaluation of hypotheses.	assistive technology;automated theorem proving;computer simulation;documentation;experiment;fits;formal specification;graphical user interface;mathematical optimization;prototype	Fabian Lorig;Colja A. Becker;Ingo J. Timm	2017			personalization;statistical hypothesis testing;formal specification;parametrization;data mining;computer science	SE	-57.327412322425	29.156336112620547	192792
9f465c957276eaa2d864e1e52dcfe69f5699c1e5	an industrial case study of implementing and validating defect classification for process improvement and quality management	quality assurance;hewlett packard scheme defect classification process improvement quality management defect measurement quality assurance assessment software inspection software testing odc;software testing;industrial case study;software process improvement;odc;null;software engineering;empirical evidence;defect measurement;program testing;computer aided software engineering quality management quality assurance inspection testing embedded software embedded system automotive engineering electrostatic precipitators software engineering;hewlett packard;hewlett packard scheme;process improvement;software inspection;software quality;quality assurance assessment;software quality program testing quality assurance software process improvement;quality management;embedded software;defect classification	Defect measurement plays a crucial role when assessing quality assurance processes such as inspections and testing. To systematically combine these processes in the context of an integrated quality assurance strategy, measurement must provide empirical evidence on how effective these processes are and which types of defects are detected by which quality assurance process. Typically, defect classification schemes, such as ODC or the Hewlett-Packard scheme, are used to measure defects for this purpose. However, we found it difficult to transfer existing schemes to an embedded software context, where specific document- and defect types have to be considered. This paper presents an approach to define, introduce, and validate a customized defect classification scheme that considers the specifics of an industrial environment. The core of the approach is to combine the software engineering know-how of measurement experts and the domain know-how of developers. In addition to the approach, we present the results and experiences of using the approach in an industrial setting. The results indicate that our approach results in a defect classification scheme that allows classifying defects with good reliability, that allows identifying process improvement actions, and that can serve as a baseline for evaluating the impact of process improvements	baseline (configuration management);cellular automaton;comparison and contrast of classification schemes in linguistics and metadata;embedded software;orthogonal defect classification;software bug;software engineering;software inspection;software quality assurance	Bernd G. Freimut;Christian Denger;Markus Ketterer	2005	11th IEEE International Software Metrics Symposium (METRICS'05)	10.1109/METRICS.2005.10	reliability engineering;quality assurance;quality management;empirical evidence;embedded software;systems engineering;engineering;software engineering;software inspection;software testing;software quality	SE	-60.359303334135525	28.79441419428713	193148
777e456a733fe4ae647fc35e022365d92d5b41cb	contract-based programming for future computing with ada 2012	software;computer languages;software engineering;security;software reliability;programming	Future advanced software engineering encounters some new quality evaluation criteria and related challenges on security as well as reliability of software systems. As the next generation of the world's premier programming language for engineering safe, secure and reliable software, the latest version of programming language Ada, Ada 2012, has introduced the concept of contract-based programming into its international standard. We should utilize contract-based with Ada 2012 to solve the new criteria and challenges in future advanced software engineering. On the other hand, since Contract-Based Programming (CBP) can strictly limit and assure the correctness of programs, we usually use precondition, postcondition of the subprograms to ensure to satisfy some requirements of parameters passing and/or returned values between two components. In many cases, we also use some contracts of types to ensure the correctness of building objects, such as type invariant, static predicate, and dynamic predicate. These conditions can improve quality of software, and therefore, we can utilize CBP to solve the challenges. Although CBP is important to future advanced software engineering, there is no report about how CBP can solve the criteria and challenges in future advanced software. Therefore, this paper investigates how CBP with Ada 2012 may solve the criteria and challenges in future advanced software engineering from seven areas, such as security, continuity, reactive-ability, predictability, anticipatable-ability, self-healing-ability, and autonomous-evolution-ability.	ada;autonomous robot;correctness (computer science);next-generation network;postcondition;precondition;programming language;requirement;scott continuity;software engineering;software system;subroutine	Bo Wang;Hongbiao Gao;Jingde Cheng	2016	2016 International Conference on Advanced Cloud and Big Data (CBD)	10.1109/CBD.2016.062	software security assurance;verification and validation;computing;n-version programming;software engineering process group;software verification;search-based software engineering;computer science;systems engineering;design by contract;backporting;software design;social software engineering;software framework;component-based software engineering;software development;software engineering;software construction;software walkthrough;programming language;resource-oriented architecture;software requirements;software metric;software system	SE	-58.32121383769663	27.270110804815726	193821
46c0de6802375218373879c0e7f0ba235d9ce91e	hard facts vs soft facts	encoding java industries companies guidelines databases investments;source code analysis service purchases product purchases;migration;software maintenance purchasing;software maintenance;purchasing;roi migration;roi;source code analysis	This paper aims at describing a serious misconception commonly found in the academic world when it comes to understanding the motivations of a commercial stakeholder when he or she purchases services or products related to source code analysis and/or transformation. This misconception is related to the perceived relative importance of hard and soft facts.	purchasing;static program analysis	Darius Blasband	2008	2008 15th Working Conference on Reverse Engineering	10.1109/WCRE.2008.24	return on investment;human migration;engineering;software engineering;software maintenance	SE	-62.15136199914117	26.14167511315466	193924
ada269b6a20ba1facd0c7ffc006b134a05c4278e	development and verification of rule based systems - a survey of developers	negative affect;tool support;rule based system;verification and validation	While there is great interest in rule based systems and their development, there is little data about the tools and methods used and the issues facing the development of these systems. To address this deficiency, this paper presents the results from a survey of developers of rule based systems. The results from the survey give an overview of the methods and tools used for development and the major issues hindering the development of rule based systems. Recommendations for possible future research directions are presented. The results point to verification and validation, debugging and overall tool support as the main issues negatively affecting the development of rule based systems. Further a lack of methodologies that appropriately support developers of these systems was found.	angular defect;debugging;norm (social);recommender system;rule-based system;verification and validation	Valentin Zacharias	2008		10.1007/978-3-540-88808-6_4	rule-based system;verification and validation;simulation;computer science;artificial intelligence;data mining;affect	SE	-60.115079822665756	26.033907599194823	194235
5668eb4504811c7501dabcb29b8e1941cd2b7a0c	a testing frameworks for mobile embedded systems using mda		Embedded system can give you many benefits in putting it in your device, such as mobile phones, appliances at home, machines at the bank, lottery machine and many more, just make sure it is undergoing in embedded systems testing to have the device check. You must know that putting an embedded system in any of your device (either at home or in your business) can vary be helpful in your daily life and for the near future.One of the important phases in the life cycle of embedded software development process is the designing phase. There are different models used in this particular phase including class diagrams, state diagrams and use cases etc. To test the conformance of the software it is very essential that test cases should be derived from these specific models. Similarly regressions testing through these models are very significant for testing of modified software. There are several regression testing approaches based on these model in literature. This survey report is the analysis of the model based regression testing techniques according to the parameter identified during this study. The summary as well as the analysis of the approaches is discussed in this survey report. In the end we concluded the survey by identifying the areas of further research in the field of model based regression testing.		Haeng-Kon Kim;Roger Y. Lee	2011		10.1007/978-3-642-23202-2_6	regression testing;embedded system;software;test case;class diagram;lottery;state diagram;computer science;embedded software;use case	EDA	-57.45265366021884	28.515042681340518	194725
d9328cbfb70cca3bd038acf82db9d6808af52ec7	evaluation of goal models in reuse hierarchies with delayed decisions		Trade-off analysis through goal model evaluation has been a valuable tool for requirements elicitation and analysis. This is also true in the context of reuse. When goal models are used to describe reusable artifacts and to represent the impacts of reusable artifacts on high-level goals and qualities, they can guide the selection of reusable artifacts to build reuse hierarchies. In previous work, we introduced the use of relative contribution values for reusable goal models, while considering constraints imposed by other modeling notations. In this paper, we expand the result of goal model evaluation from the typical single satisfaction value to a range of values that are still possible based on the current task selections. In the context of reuse hierarchies, we call the remaining task selections delayed decisions because they are postponed to a higher level in the reuse hierarchy when more is known about the system under development. The extended algorithm takes into account the delayed decisions and evaluates the best and worst possible results that can be obtained with the task selections that have been made in the entire reuse hierarchy. The distinct levels in the reuse hierarchy are leveraged to manage the computational complexity of this reuse hierarchy-wide evaluation. A proof-of-concept implementation of the novel evaluation algorithm is presented in the concern-oriented software design modeling tool TouchCORE.	artifact (software development);computational complexity theory;feature selection;genetic algorithm;goal modeling;high- and low-level;real life;requirement;requirements elicitation;software design;top-down and bottom-up design	Mustafa Berk Duran;Gunter Mussbacher	2017	2017 IEEE 25th International Requirements Engineering Conference Workshops (REW)	10.1109/REW.2017.66	management science;reuse;requirements elicitation;hierarchy;software;software design;goal modeling;computational complexity theory;context model;systems engineering;computer science	SE	-56.03245323479186	25.40350239083752	195282
c61f7e296b559c2d9d7445db7b3e703c54761681	an information theory word-based metric to evaluate software maintainability and reusability	information theory		information theory	Hector M. Olague;Letha H. Etzkorn	2005			information theory;maintainability;reusability;computer science;systems engineering	SE	-62.81407874494838	26.744196158808705	195504
2b1f24970d10528bf4da4547a33326a673f15f80	empirical-bayesian availability indices of safety and time critical software systems with corrective maintenance	debugging;reliability index;probability;empirical bayesian;maintenance;software maintenance;availability;integrable system;bayes methods;probability density function;software systems;bayesian methods;prior distribution;mle;corrective maintenance;maximum likelihood estimation;bayesian availability indices;time factors;system recovery;maximum likelihood estimate;shape;software safety;safety critical software;indexation;safety;time critical;software safety software systems bayesian methods time factors availability maximum likelihood estimation debugging maintenance probability density function shape;maximum likelihood estimators;risk functions;historical data bayesian availability indices safety critical software time critical software corrective maintenance software reliability reliability index probability density function risk functions maximum likelihood estimators;software reliability;time critical software;historical data;probability safety critical software bayes methods software maintenance system recovery	If the recovery or remedial time is not incorporated in the reliability of a software module in a safety and time-critical integrated system operation, then a mere reliability index based on failure characteristics is simply not adequate and realistic. In deriving the probability density function (pdf) of the software availability, empirical Bayesian procedures will be used to employ expert engineering judgment through appropriate noninformative and informative prior distribution functions by employing various definitions of risk functions. It is emphasized that the usage of maximum likelihood estimators regardless of the extent of historical data is erroneous and misleading.	information;portable document format;software system;window of opportunity	Mehmet Sahinoglu;Edward Chow	1999		10.1109/PRDC.1999.816216	reliability engineering;computer science;data mining;maximum likelihood;statistics	SE	-61.96115034151158	31.84744535526572	195761
e131fa441ff700b1ed71d2c64af25ed125008c4a	industrial requirements to benefit from test automation tools for gui testing	software quality;gui testing;software systems	In addition to the growing complexity of software systems, test effort takes increasing amounts of time and correspondingly more money. Testing costs may be reduced without compromising on software quality by minimizing test sets through optimal selection of test cases and introducing more powerful test tools. Attaining high levels of test automation is the objective. There are problems which make the introduction of test automation in industry quite difficult. Solution providers and tool developers often do not understand the requirements in industry for test automation. Otherwise introducing test automation could become counterproductive. This paper points out essential demands on GUI test tools for industrial purpose.	graphical user interface testing;interoperability;money;requirement;software quality;software system;test automation;test case;test effort	Christof J. Budnik;Rajesh Subramanyan;Marlon Vieira	2007			reliability engineering;test management approach;software engineering;test harness;system integration testing;test case;test effort;software reliability testing;totally integrated automation;test strategy;computer science	SE	-61.80561604377987	27.504500356512462	195828
2bc074bf53f2a476549885588371803d4c71578f	functional testing of feature model analysis tools: a test suite	conjunto independiente;developpement logiciel;modelizacion;feature model analysis;mutation testing;fiabilidad;reliability;functional testing;independent set;software maintenance;software tools program testing software fault tolerance software maintenance;analyse fonctionnelle;feature modeling;product representation;mutation testing functional testing feature model analysis software product line product representation automated analysis specific testing mechanism obstacle hindering fama test suite independent test case;arquitectura logicial;feature model analysis tools;software fault tolerance;fama test suite;logical programming;analisis automatico;modelisation;software architecture;automatic analysis;ensemble independant;estimation erreur;detection defaut;program testing;functional analysis;programmation logique;obstacle hindering;error estimation;desarrollo logicial;fiabilite;software development;estimacion error;analyse automatique;estructura producto;software tools;independent test case;software product line;programacion logica;modeling;structure produit;deteccion imperfeccion;architecture logiciel;product structure;defect detection;specific testing mechanism;automated analysis;analisis funcional	A Feature Model (FM) is a compact representation of all the products of a software product line. Automated analysis of FMs is rapidly gaining importance: new operations of analysis have been proposed, new tools have been developed to support those operations and different logical paradigms and algorithms have been proposed to perform them. Implementing operations is a complex task that easily leads to errors in analysis solutions. In this context, the lack of specific testing mechanisms is becoming a major obstacle hindering the development of tools and affecting their quality and reliability. In this article, we present FaMa Test Suite, a set of implementation–independent test cases to validate the functionality of FM analysis tools. This is an efficient and handy mechanism to assist in the development of tools, detecting faults and improving their quality. In order to show the effectiveness of our proposal, we evaluated the suite using mutation testing as well as real faults and tools. Our results are promising and directly applicable in the testing of analysis solutions. We intend this work to be a first step toward the development of a widely accepted test suite to support functional testing in the community of automated analysis of feature models.	algorithm;documentation;fm broadcasting;fama im;feature model;functional testing;handy board;metamorphic testing;mutation (genetic algorithm);mutation testing;open-source software;sensor;software product line;software testing;test automation;test case;test data;test suite;white-box testing;world wide web	Sergio Segura;David Benavides;Antonio Ruiz Cortés	2011	IET Software	10.1049/iet-sen.2009.0096	functional analysis;reliability engineering;software architecture;model-based testing;systems modeling;independent set;computer science;software development;software engineering;functional testing;reliability;mutation testing;software maintenance;engineering drawing;algorithm;software fault tolerance	SE	-58.783773624805214	32.29707967109214	196184
e582c0d524429b1d45b7133a84e605bf7a741302	analysing the reliability of open source software projects	reliability;measurement;reliability management measurement documentation performance design;iso standards;measurement iso standards software reliability context usability;performance;design;usability;management;software reliability;context;documentation	Evaluation of software quality is one of the main challenges of software engineering. Several researches proposed in literature the definition of quality models for evaluating software products. However, in the context of Free/Open Source software, differences in production, distribution and support modality, have to be considered as additional quality characteristics. In particular, software reliability should be taken into account before selecting software components. In this direction, this paper evolves a quality model for Free/Open Source Software projects, called EFFORT — Evaluation Framework for Free/Open souRce projects for including reliability aspects and presents an empirical study aimed at assessing software reliability and its evolution along the software project history.	application domain;component-based software engineering;erp;futures studies;modality (human–computer interaction);open sound system;open-source software;positive feedback;software bug;software project management;software quality;sourceforge	Lerina Aversano;Maria Tortorella	2015	2015 10th International Joint Conference on Software Technologies (ICSOFT)	10.5220/0005519903480357	reliability engineering;design;personal software process;medical software;long-term support;verification and validation;software sizing;usability;performance;software project management;documentation;computer science;systems engineering;engineering;social software engineering;software reliability testing;software development;software engineering;software construction;reliability;software testing;software walkthrough;software measurement;management;software deployment;software quality control;software quality;software metric;measurement;software quality analyst;software system;software peer review	SE	-62.118596327378704	26.775168665784268	196192
77d8757c7863ebc1e3285104086b9d2bef58a540	the profes improvement methodology - enabling technologies and methodology design	developpement logiciel;informatica biomedical;biomedical data processing;learning algorithm;software process improvement;informatique biomedicale;ingenieria logiciel;algorithme apprentissage;software engineering;desarrollo logicial;software development;genie logiciel;design rationale;fiabilite logiciel;fiabilidad logicial;algoritmo aprendizaje;software reliability;qualite logiciel;software quality	Software process improvement methodologies do not typically address product issues explicitly and integration of different technologies is often weak. In the European project PROFES an integrated, product-focused software process improvement methodology has been developed. This paper gives an overview of the methodology and explains its enabling technologies. Emphasis is on how the PROFES improvement methodology was created, what was the design rationale, and how the methodology was implemented.		Janne Järvinen;Seija Komi-Sirviö;Günther Ruhe	2000		10.1007/978-3-540-45051-1_24	computer science;systems engineering;engineering;software engineering;software quality	EDA	-61.48686512604741	28.365858798506487	196508
836be599c2f0d17e96de37dd8faf1463a4a1413c	towards a cots-based development environment	programming environments;programming environments software development management software packages software reusability software quality;costs software quality software systems software engineering laboratories risk management intellectual property internet sun application software;risk reduction;development environment;software reusability;commercial off the shelf package cots based development cost reduction risk reduction software quality software reuse development software selection software integration wagehcom;software quality;software development management;software packages	"""Systems' developing using COTS components is very attractive since it promises costs and risks reduction as well as higher software quality. The process of such reuse development approach consists in examining the marketplace to identify potential COTS components, evaluate them to select the most appropriate one, adapt it to the needs and integrate it in the system. This process seems to be simple but it is not obvious in practice. In fact, the developer faces many problems like where to find potential COTS components, which is the most suitable selection/integration method to the current project and developer profile, how to choose it and how to apply it. The lack of guidance during the COTS-based development (CBD) process is conspicuous and the need of a CBD environment helping the developer in the different process steps is essential. Existing environments either don't support the specific CBD process or support only one step. In this paper, we propose an environment denoted """"WagehCom"""" that lets the developer: (i) build a customized process fitted with his/her profile and project requirements, (ii) get a strategic guidance for selection and/or integration methods choice, and (iii) get a tactic guidance to apply the advocated methods."""	component-based software engineering;requirement;software quality	Sihem Ben Sassi;Lamia Labed Jilani;Henda Hajjami Ben Ghézala	2006	Fifth International Conference on Commercial-off-the-Shelf (COTS)-Based Software Systems (ICCBSS'05)	10.1109/ICCBSS.2006.30	reliability engineering;personal software process;long-term support;verification and validation;software quality management;software sizing;software project management;systems engineering;engineering;package development process;backporting;social software engineering;software framework;software development;software engineering;software construction;software walkthrough;resource-oriented architecture;software deployment;goal-driven software development process;software development process;software quality analyst;software system;software peer review	SE	-60.96040655067185	27.07468602085234	196551
8fdfbdd3d3d5d688e1eeb448cd3b3161f5b40081	licensing reliable embedded software for safety-critical applications	computacion informatica;process quality;software systems;software certification;quality indicator;ciencias basicas y experimentales;pre developed component;safety applications;software component;product quality;grupo a;expert judgment;software reliability;software licensing;embedded software;internal standard	This article offers an overview on existing approaches for assessing the reliability of complex software with safety demands. It addresses both inherent difficulties as well as observable trends towards international standardized procedures. The contribution distinguishes between product-based and process-based quality indicators and comments on the applicability of existing techniques to evaluate them qualitatively and quantitatively. In particular, it focusses on licensing the re-use of pre-developed software components. In the light of lessons learnt from real-world accidents it proposes to adopt a procedure supporting the re-usability of component certification when licensing software systems for new safety applications.	component-based software engineering;embedded software;observable;software system;usability	Francesca Saglietti	2004	Real-Time Systems	10.1023/B:TIME.0000045318.83240.86	embedded software;computer science;component-based software engineering;software construction;internal standard;software quality;software system	SE	-61.12129437785438	29.582307436249152	196993
b924b0b12b5177d18990cc11fff02fc6f43a4dc3	lessons learned on using execution model implementation in sparx enterprise architect for verification of the topological functioning model		The execution model can improve analysis, testing and verification of software systems and their features right from the early stages of development. It helps to decrease risks and the possibility of future defects. One of the main goals and challenges for modern modeling tools is the ability to generate usable source code using the modeling approach. The system functionality can be shown as Topological Functioning Model and this functionality can be validated with the help of modeling tools. The paper presents an overview of modeling tools for the execution of models and the ways that they can aid software development. Four modeling tools are reviewed and compared based on their features and documentation – Cameo Simulation Toolkit, Enterprise Architect, Papyrus with Moka and BridgePoint. Two of them – Cameo Simulation Toolkit and Enterprise Architect, are analyzed and compared in practice. Results of the overview are the base for future work, where the tools will be applied for case studies.	documentation;enterprise architect;eyetoy;papyrus;simulation;software development;software system	Viktoria Ovchinnikova;Erika Nazaruka	2017		10.5220/0006388403550366	systems engineering;engineering;software engineering;database	SE	-57.157976546483475	25.684605314826825	197111
bc7ab2bec5abe7cf69f7ed2efdd32ea6a8acb3b4	process patterns for component-based software development	method engineering;process pattern;component based systems;component based development;software development methodologies;commercial off the shelf;situational method engineering;software development;system development;component based software development;process patterns;software product line	Component-Based Development (CBD) has been broadly used in software development, as it enhances reusability and flexibility, and reduces the costs and risks involved in systems development. It has therefore spawned many widely-used approaches, such as Commercial Off-The-Shelf (COTS) and software product lines. On the other hand, in order to gain a competitive edge, organizations need to define custom processes tailored to fit their specific development requirements. This has led to the emergence of process patterns and Method Engineering approaches. We propose a set of process patterns commonly encountered in componentbased development methodologies. Seven prominent component-based methodologies have been selected and reviewed, and a set of high-level process patterns recurring in these methodologies have been identified. A generic process framework for component-based development has been proposed based on these process patterns. The process patterns and the generic framework can be used for developing or tailoring a process for producing component-based systems.	.net framework;component-based software engineering;composer;eclipse process framework;emergence;high- and low-level;method engineering;population;process patterns;requirement;software development process;software product line	Ehsan Kouroshfar;Hamed Yaghoubi Shahir;Raman Ramsin	2009		10.1007/978-3-642-02414-6_4	reliability engineering;development testing;verification and validation;team software process;design process;software engineering process group;architectural pattern;computer science;systems engineering;engineering;package development process;backporting;software design;social software engineering;component-based software engineering;software development;requirement;software engineering;iterative and incremental development;systems development life cycle;programming language;empirical process;engineering drawing;goal-driven software development process;software development process	SE	-58.07920931150318	26.205349183097127	197795
363a842372525c50ef8274cd0e221d3794f5c148	quont: an ontology for the reuse of quality criteria	software product audits;software;ontologies software quality security throughput authentication usability computer science concrete software measurement software performance;architectural knowledge;authentication;audit quality;data mining;ontologies artificial intelligence;computer architecture;quality criteria reuse;quont;software reusability;knowledge intensive tasks;quality criteria;ontologies;software reusability ontologies artificial intelligence software quality;usability;security;software quality;architectural knowledge quality criteria reuse quont software product audits knowledge intensive tasks	Software product audits are knowledge-intensive tasks in which architectural knowledge plays a pivotal role. In the input stage of a software product audit, quality criteria are selected to which the software product should conform. These quality criteria resemble architectural tactics and can be viewed as a definition of the Soll-architecture of the product. Like tactics, the same quality criteria can be applied to different software products. However, there are currently no models that support the codification of quality criteria as reusable assets. In this work, we present an ontology that supports the reuse of quality criteria in the input stage of software product audits.		Remco C. de Boer;Hans van Vliet	2009	2009 ICSE Workshop on Sharing and Reusing Architectural Knowledge	10.1109/SHARK.2009.5069116	reliability engineering;verification and validation;software quality management;computer science;systems engineering;knowledge management;software construction;software quality control;software quality;software quality analyst	SE	-59.660660427094044	28.333965471094945	198447
653318b6b8a9cb2e90be2f9a0522928802becbaa	applying formal description techniques to software architectural design	architectural design;communicating extended finite state machine;software systems;specification and description language;software engineering;formal description technique;software architecture;extended finite state machine;workflow;software architecture design	Mature engineering disciplines have systematic quantified architectural design. Engineering advancement of the software engineering discipline will eventually require systematic and quantified software architectural design. Formal description techniques such as the specification and description language can help to achieve this goal. This paper presents a method to construct formal models of software architectures and to simulate them to predict the behavior, reliability, and performance of the software system. We use the quantified simulation results to evaluate alternative software architectural designs. Our experiments show that our approach identifies better designs.	description logic;formal methods	J. Jenny Li;Joseph Robert Horgan	2000	Computer Communications	10.1016/S0140-3664(99)00244-3	software architecture;software requirements specification;computer architecture;verification and validation;formal methods;software sizing;architectural pattern;software verification;computer science;software design;social software engineering;component-based software engineering;software development;software design description;software construction;software architecture description;resource-oriented architecture;software measurement;software deployment;software development process;software requirements;software system	SE	-57.128422051851175	25.940569533121437	198874
d6287d12541538275fd35ee140d16e30c1d7dfbc	an experimental evaluation of sec+, an enhanced search engine for component-based software development	radial charts;search engine;requirements and architecture modeling;conceptual modeling;process oriented modeling;abstractions;specification;metrics;software engineering;reuse;software builds;software architecture;bugs;methodologies;defect analysis;defect management;evolvable systems;complex systems;design;design patterns;component based software development;experimental evaluation;service discovery;open systems;modeling;method;software quality;framework;architectural style;requirements analysis	Current approaches for service discovery are inherently restricted to the exact querying. This may provide incomplete answers since queries are often overspecified and may lead to low precision and recall. To alleviate these problems, we achieved an experimental evaluation that uses of the enhanced search engine, SEC+. This engine is based on the subsumption mechanism and a function that calculates the semantic distance. Both the used rate and the non-functional features are considered to filter the selection. We show that such a solution can improve the quality of the search and can enhance both the recall and the precision.	component-based software engineering;software development;web search engine	Sofien Khemakhem;Khalil Drira;Emna Khemakhem;Mohamed Jmaiel	2008	ACM SIGSOFT Software Engineering Notes	10.1145/1360602.1360610	reliability engineering;software architecture;design;requirements analysis;complex systems;method;computer science;systems engineering;software engineering;reuse;database;service discovery;programming language;specification;metrics;software quality;search engine	SE	-57.76258184879486	31.96881454013572	199624
