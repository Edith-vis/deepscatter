id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
f37054c869be6c9230f90da0bf684633ff1949dd	farewell message	farewell message	Three years have gone since I started my term as Editor-in-Chief (EiC) of the IEEE Transactions on Information Forensics and Security (T-IFS). These have been tough but exciting years. Tough because taking care of a journal like T-IFS requires great dedication and time availability. Exciting because in these years the popularity of the journal has continued to increase and the topics falling under the wide umbrella of Information Forensics and Security (IFS) have been receiving an increasing attention by our society and now are constantly sitting on top of the agenda of governments, public and private companies, research institutions, etc. Most of all, these have been exciting years because I got in touch with a huge number of scholars, researchers, and practitioners throughout the world, belonging to a large number of different communities dealing with disciplines far away from my expertise. Serving the IFS community as EiC of T-IFS enriched me beyond any expectations I had.		Mauro Barni	2017	IEEE Trans. Information Forensics and Security	10.1109/TIFS.2017.2755979	computer science;computer security;popularity	Crypto	-62.663667821843056	-20.480772206040353	142858
2c37ee22405997dfd65a0b0c567ec5bb0c74ff58	guest editorial: special issue on the emerging applications of neural networks		It has been over 60 years since the first neural network model was developed by McCulloch and Pitts [1], and has been just over 50 years since the Mark I Peceptron model was developed by Rosenblatt and Wightman [2]. The downfall of Neural Network research was due to the limited capabilities of perceptron which was rigorously proved in the book ‘‘Perceptrons’’ by Minsky and Papert [3]. Neural network research has then gone through a period of quiet years from the mid sixties to the mid eighties. The take off of neural computing research was probably due to the announcement that Japan had resumed the research of neural networks. The US feared that they would be overtaken by the Japanese. This resulted in Ira Skurnick, a program funding manager in the US defense Science Office, decided to fund neural computing research again. To all neural computing researchers, the late eighties are well remembered because we saw how neural computing had been reinstated and repositioned. In 1986, we saw the first annual Neural Networks for Computing Conference, and we saw Rumelhart reported back his famous backpropagation learning algorithm. To date, we have seen the flourish of neural computing with its application stretched from rigorous mathematical proof to different engineering, physical science, and even business finance applications. This special issue on ‘‘The Emerging Applications of Neural Networks’’ is aimed at disseminating the latest applications of neural networks in the areas of engineering or computer science, business and management, geology and library science. It is exciting to see different researchers from different countries, and cultures come up with new thinking and applications. In the twenty-first century, We may have come to a stage that science theories and basic research are more than sufficient to our daily lives. This special issue delineates how the versatile neural networks are affecting us everyday. The papers solicited are selected from papers presented in the ICONIP 2009. Eventually, twelve papers from experts with innovation on new neural applications are invited and included in this issue. We include the paper:	algorithm;backpropagation;computer science;frank rosenblatt;itu t.50;library science;network model;neural networks;perceptron;theory;walter pitts	Tommy W. S. Chow;John Sum	2010	Neural Computing and Applications	10.1007/s00521-010-0427-z	perceptron;neural decoding;artificial neural network;mathematical proof;obstacle;streaming data;artificial intelligence;computer science	ML	-57.47400652465446	-19.937936983597606	143377
c8cca0d0b717a759b2d797176d87fdd167366f40	cgi 2017 editorial (tvcj)		Welcome to the special issue of the 34th Computer Graphics International conference (CGI’17)! Computer Graphics International is one of the oldest true international conferences in computer graphics. It is the official conference of theComputerGraphics Society (CGS), a long-standing international computer graphics organization. CGI and CGSwere initiated in Tokyo by Professor Tosiyasu L. Kunii from the University of Tokyo in 1983. Since then, the CGI conference has been held annually in many different countries across the world and gained a reputation as one of the key conferences for researchers and practitioners to share their achievements and discover the latest advances in this field. CGI this year has been organized by Faculty of Science and Technology, Keio University in Yokohama, Japan, between 27th and 30th June, 2017, with support from CGS and in cooperation with ACM/SIGGRAPH and Eurographics. This special issue of the Visual Computer is composed of the 35 best papers from CGI’17. We received 171 submissions from 33 countries and regions, making the acceptance ratio about 20%. To ensure the highest quality of publications, each paper has been reviewed by at least three experts in the field. The International Program Committee is com-	common gateway interface;computer graphics international;eurographics;siggraph	Xiaoyang Mao;Daniel Thalmann;Marina L. Gavrilova	2017	The Visual Computer	10.1007/s00371-017-1366-9		Visualization	-60.293687075566716	-17.183006458319586	144254
5c305c22b8021e0e0edaeb2061155acef9619897	'computing, we have a problem ...'	diversity;underrepresented minorities;people with disabilities;broadening participation;women	"""In May 2005, Jim Foley (Board Chair of the Computing Research Association) published an abstract that was titled with these words: """"Computing, We Have a Problem …"""". The main purpose of Dr. Foley's article was to discuss the image problem within computing, that """"the public does not fully understand, and hence does not appreciate, what computing is and why computing and computing research are important"""" [1]. He then considered the consequences of this image problem (e.g., decreased enrollments in computing degree programs) and stated what CRA planned to do to rectify the situation.  While Dr. Foley's article did not mention women, minorities, or persons with disabilities, it is clear that several groups in our society are tremendously impacted by the image problem that exists within computing. In this article, we begin by discussing the lack of participation that exists for those who are trained in computing (i.e., people who have the skills to develop computer hardware and software). We then discuss why this digital divide should be of high concern to everyone, what we can learn from other previously male-dominated fields, what you can do to help improve the current situation, and what the future might hold."""	computer hardware;credit bureau;list of code lyoko episodes	Tracy Camp	2012	Inroads	10.1145/2381083.2381097	library science;simulation;computer science;artificial intelligence	AI	-62.4885971167908	-19.909024688586225	145578
4843d0842ae715800e5f76704ad6c68cc0bcd560	towards a road map on human language technology: natural language processing	technological requirement;sketches milestone;natural language processing;road map;human language technology;nlp-related application;document summarizes contribution	This document summarizes contributions and discussions from two workshops that took place in November 2000 and July 2001. It presents some visions of NLP-related applications that may become reality within ten years from now. It investigates the technological requirements that must be met in order to make these visions realistic and sketches milestones that may help to measure our progress towards these goals.	language technology;natural language processing;requirement	Andreas Eisele;Dorothea Ziegler-Eisele	2002			natural language processing;computer science;artificial intelligence	NLP	-56.49029444228273	-18.94892431640024	146943
40f6227c7b175c189e72a46c2eac5fa2ef15975c	preface to the special issue si: satisfiability modulo theories	satisfiability modulo theories;special issue	With this special issue we celebrate the tenth anniversary of the foundation of the Satisfiability Modulo Theory (SMT) community. We invited several members of this community to submit articles dedicated to this event, and were happy to see that all of them accepted the invitation and submitted. Each paper was reviewed by at least four expert reviewers. Five out of the six submitted papers were accepted. SMT is probably the most successful academic community related to logics and verification that were built in the last decade. Indicators of this success are the annual 2-day SMT workshop, which typically attracts more participants than any other adjacent workshop, the SMT-COMP competition that steadily grows in number of submissions, the number of tools and benchmarks, the impact on related research areas, the industrial adoption, and the three text books [3, 6, 7] related to SMT that were published in the last few years. The academic impact of SMT to date can be quantified by the number of articles it produces. The term “Satisfiability modulo theories”, which was born only ten years ago, has 1730 hits in Google Scholar as of July 2012, and there are probably hundreds of other papers that are associated with SMT but do not use this term explicitly. The impact of this field on the industry can be measured by its use of SMT solvers: Microsoft uses Z3, an SMT solver developed by de Moura and Bjorner, in at least 10 different program analysis tools; Intel is using SMT solvers such as MathSat and Boolector for processor verification and software verification; other companies that are known to use SMT solvers include Galois Connection, Praxis, GrammaTech, NVIDIA, Synopsys, Mathworks, Dassault Aviation, and the list continues. SMT solvers are now standard engines in numerous industrial applications, some of which (like scheduling) go beyond the scope of deductive reasoning and formal verification. One can trace back the birth of this community to the breakthroughs in SAT solving in early 2001, and specifically to the introduction of the SAT solver Chaff by Moskewicz and others [8]. Almost immediately after its introduction several groups started investigating	altran praxis;book;boolean satisfiability problem;formal verification;google scholar;modulo operation;program analysis;satisfiability modulo theories;scheduling (computing);software verification;solver;theory;z3 (computer)	Ofer Strichman;Daniel Kroening	2013	Formal Methods in System Design	10.1007/s10703-012-0172-2	theoretical computer science;algorithm	Logic	-60.481424776255274	-18.81524075507468	146963
b5c96db65d09d474797f1d8ce2d24f5f2165181d	interval-related talks at the 2007 ieee symposium series on computational intelligence	computational intelligence	Symposium Series. On April 1–5, 2007, IEEE Computational Intelligence Society organized the first Symposium Series on Computational Intelligence in Honolulu, Hawaii. The main objective of this series is to bring together researchers and practitioners from different areas related to computational intelligence, so that they can learn new techniques and hopefully start fruitful combination of different techniques and approaches. In particular, one of the corresponding symposia—Symposium on Foundations of Computational Intelligence FOCI’07—had a special session on interval methodologies. Several other interval-related talks were presented at other session and other symposia. The interest in interval computations was emphasized by Jerry Mendel from the University of Southern California, one of the leaders in using interval techniques in computational intelligence, and a co-Chair of FOCI’07.	computation;computational intelligence	Oscar Castillo;Vladik Kreinovich	2007	Reliable Computing	10.1007/s11155-007-9040-y	computer science;artificial intelligence;mathematics;cognitive science	Theory	-59.29589224635439	-17.548407657267717	147357
d1c75f77c2389ac496c3c3fc8b260268d8a48cab	what's on your hard drive?	miscellaneous;documentation;general;languages;mass storage;software development;programming language	"""Welcome back for another installment of “What’s on Your Hard Drive?” where  you tell us about the tools you love, the tools you hate, and a category that’s  recently grown in popularity: tools that you both love and hate. And by tools  we don’t just mean pieces of software; feel free to express your love  and/or hatred for both IDEs and the programming languages we use within them. <br>  Programming language allegiances are usually much stronger than tool preferences,  so this is bound to make for some heated debate over at <a href=""""http://www.acmqueue.com"""">www.acmqueue.com</a>, where  you can both submit to WOYHD and comment on others’ submissions."""	hard disk drive;hatred;programming language	Jim Maurer	2005	ACM Queue	10.1145/1080862.1080868		OS	-61.02934088949172	-23.707759931515668	148311
2c525aba4bbda196909e02702170a3f656018016	panel on use of proprietary data	panel discussion;wider discussion;proprietary data	A panel discussion on the use of proprietary data was held at SIGIR 2012 in Portland. This report summarizes the positions put forward by the six panelists and the points that arose during the wider discussion that followed.		James P. Callan;Alistair Moffat	2012	SIGIR Forum	10.1145/2422256.2422258	data mining	Web+IR	-61.447990066079285	-17.1978639092995	148765
ad5f73e68c232bec2fdf7aef783f8e7b5c286788	learning from michael maschler and working with him		The very first class in my first year as an undergraduate student was infinitesimal calculus with Michael Maschler. Michael opened my eyes to the beauty of mathematics beyond the highschool level. In subsequent years I studied several game theory courses with Michael, and I was always amazed at the width and depth of the theory. Over the past four years Shmuel Zamir and I have written a game theory textbook with Michael. This long period of working together enabled me to see several sides of Michael’s personality: his passion for the theory, his determination, his love for the students, his clear view of the best way to explain difficult arguments to them, and the infinite amount of energy that he was willing to invest in fighting for the issues that were important to him. The textbook has just been sent to the publisher. Unfortunately Michael will not see the end product of our project, on which he spent so much time right until his last days.	first-class function;game theory	Eilon Solan	2008	Games and Economic Behavior	10.1016/j.geb.2008.09.010	neoclassical economics;welfare economics;economics	ECom	-61.16927804971807	-22.412959743142945	148816
2f88da340cbe0f838c59fb692c6ddfe3e3f82b6f	special issue: best papers of vldb 2009	special issue;best paper	This special issue of the VLDB Journal is dedicated to the best papers from the 35th International Conference on Very Large Data Bases, which took place on 24–28 August 2009 in Lyon, France. The conference received 749 submissions overall. This represents a small increase over 2008. This is particularly encouraging since many other conferences are facing a reduction, substantial in some cases, in the number of submissions. For the research tracks:	database;vldb	Serge Abiteboul;Volker Markl;Tova Milo;Jignesh Patel	2011	The VLDB Journal	10.1007/s00778-011-0222-1	database;data science;empirical research;very large database;query optimization;computer science;publication	DB	-61.005129038008384	-18.11107645707611	148944
a464b07f6ec10403aa5dbac1080f42ff03dd7af7	the back of the envelope	programming pearl	"""It was in the middle of a fascinating conversation on software engineering that Bob Martin asked me, """"How much water flows out of the Mississippi River in a day?"""" Because I had found his comments up to that point deeply insightful, I politely stifled my true response and said, """"Pardon me?"""". When he asked again I realized that I had no choice but to humor the poor fellow, who had obviously cracked under the pressures of running a large software shop within AT&T Bell Laboratories. My response went something like this. I figured that near its mouth the river was about a mile wide and maybe 20 feet deep (or about one two-hundred-andfiftieth of a mile). I guessed that the rate of flow was five miles an hour, or 120 miles per day. Multiplying 1 x 1/250 x 120 showed that the river discharged about half a cubic mile of water per day, to within an order of magnitude. 1 But so what? At that point Martin picked up a proposal on his desk for the development of a large computer-based mail system, and went through a similar sequence of calculations. Although his numbers were more precise (they were straight from the proposal), the calculations were just as simple and much more revealing. They showed that, under generous assumptions, the proposed system had a chance of working only if there were at least a hundred and twenty seconds in each minute. He had sent the design back to the drawing board the previous day. That was Bob Martin's wonderful (if eccentric) way of introducing the engineering technique of """"back-ofthe-envelope"""" calculations. The technique is standard fare in most engineering curricula, and is bread and butter for practicing engineers in many fields. Unfortunately, it is too often neglected in computing."""	cubic function;software engineering	Jon Louis Bentley	1984	Commun. ACM	10.1145/357994.381168	back-of-the-envelope calculation;classical mechanics;quantum mechanics;physics	Security	-60.2717726591843	-23.055196923144596	149367
3b592948610aa5e1bb2eca15bc208a89100a886b	elio lanzarone: a life for science		We are sincerely grateful to have been invited by the Guest Editor of this special issue, Prof. Francesca Lisi, to write an article in honor of our mentor Gaetano Aurelio Lanzarone (for students, colleagues and friends simply ‘Elio’). We were the first (Stefania) and last (Federico) of Elio’s students, and we both entertained a particularly deep relation with him. What we want to do here is to provide a memoir of his life and career, while his smile, laughter and humor cannot be described in words, but only in the language of our hearts and memories. We believe that Elio’s attitude towards life is well described by the words that Dante Alighieri in his ‘Divina Commedia’ (Divine Comedy, Inferno [Hell], Chant XXVI) attributes to Ulysses: ‘Considerate la vostra semenza: fatti non foste a viver come bruti, ma per seguir virtute e canoscenza’ (‘Call to mind from whence ye sprang: Ye were not form’d to live the life of brutes, But virtue to pursue and	divine divinity;inferno;linear algebra;ulysses iii	Stefania Costantini;Federico Gobbo	2015	J. Log. Comput.	10.1093/logcom/ext064	algorithm;applied mathematics;mathematics	NLP	-60.009231684394294	-23.403701884957137	149373
0ab4f72c04a9dec5ff5ddd8058f1dcce6e8c3bad	the 2014 “state of the journal” report		SoSyM continues to experience an increasing number of submissions. In 2012, 308 new manuscripts were submitted, of which 49 % were reviewed for regular issues, while the other 51 % were submitted for special or theme sections. The average number of days from submission to a final decision (that is, a final accept or reject) was 136 days in 2012. This is 12 days less than the average in 2011. It should be noted that this time includes the time authors spend on making major and subsequent minor revisions (typically 4 to 5 months). The number of months to online publication of an accepted paper has continued to drop as well, and is now between 3 and 4 weeks. We will continue to work on improving the review turnaround time. The impact factor published this year is 1.061. The current acceptance rate is 14 % for regular papers and 31 % for special or theme section papers. Due to the increase of submissions the current publication pipeline grew above one year. To reduce this pipeline we are pleased to announce that		Robert B. France;Geri Georg;Bernhard Rumpe;Martin Schindler	2014	Software & Systems Modeling	10.1007/s10270-014-0452-y		Metrics	-61.92458965856871	-18.511908440203	149807
486df1391624890e10e92d665082b5fb75c1b9cd	a batcher double omega network with combining		Acknowledgments I h a ve been fortunate to carry out the research for this dissertation in the stimulating and supportive environment of the Ultracomputer Research Laboratory. I w ould like to thank my advisor, Allan Gottlieb, Ultra's director, for his role in creating that environment a s w ell as for his careful reading of several drafts of my dissertation. As part of my w ork for Ultra, I have enjoyed fruitful research collaborations with Richard Kenner, my long-term partner in VLSI design, Ora Percus, my mentor in stochastic analysis and queueing theory, Y ue-sheng Liu, who helped me formalize my thinking about switch t ypes as well as simulating network behavior, Jan Edler, who was always willing to add another feature to his network simulator, and Ron Bianchini, who makes hardware work. In the computer science department as a whole, I would like to thank Alan Siegel, for reading my dissertation and making helpful comments, Ernie Davis and Richard Wallace, for serving on my committee, Elaine Weyuker, whose literature review seminar was very helpful in building both my conndence and competence to do computer science research, and Anina Karmen-Meade, who often helped me navigate NYU's bureaucratic labyrinths. Most of all, my gratitude goes to my h usband, Tom Duu, for his faith and support at all times, and to my c hildren, Rachael Evans, Keelan Evans and Timothy Duu. No matter how the research is going, my wonderful family always lls my life with joy.	computer science;elaine m. mcgraw;ken batcher;omega network;ora lassila;ork;queueing theory;simulation;tom;ultracomputer;wallace tree	Hideharu Amano;Kalidou Gaye	1991			distributed computing;parallel computing;omega network;computer science	AI	-59.020004491282236	-17.899922070932668	150238
4bebf8218912bb8b5eae32b578ddbd088c4ef6c7	special session - the future directions of dialogue-based intelligent personal assistants		"""Today is the era of intelligent personal assistants. All the major tech giants have introduced personal assistants as the front end of their services, including Apple’s Siri, Microsoft’s Cortana, Facebook’s M, and Amazon’s Alexa. Several of these companies have also released bot toolkits so that other smaller companies can join the fray. However, while the quality of conversational interactions with intelligent personal assistants is crucial for their success in both business and personal applications, fundamental problems, such as discourse processing, computational pragmatics, user modeling, and collecting and annotating adequate real data, remain unsolved. Furthermore, the intelligent personal assistants of tomorrow raise a whole set of new technical problems. The special SIGDIAL session """"The Future of Dialogue-Based Intelligent Personal Assistants"""" holds a panel discussion with notable academic and industry players, leading to insights on future directions. Time Table ● Introduction (15 minutes) ○ Overview of the session ○ Introduction of the panels ● Panel Discussion (75 minutes) ○ Short position talks ○ Discussion ○ QA and summary Organizers ● Yoichi Matsuyama, Postdoctoral Fellow, Human-Computer Interaction Institute / Language Technologies Institute, Carnegie Mellon University ● Alexandros Papangelis, Research Scientist, Toshiba Cambridge Research Laboratory Advisory Board ● Justine Cassell, Associate Dean of the School of Computer Science for Technology Strategy and Impact, Carnegie Mellon University Panelists ● Steve Young, Professor of Information Engineering, Information Engineering Division, University of Cambridge ● Jeffrey P. Bigham, Associate Professor, Human-Computer Interaction Institute / Language Technologies Institute, Carnegie Mellon University ● Thomas Schaaf, Senior Speech Scientist, Amazon ● Zhuoran Wang, CEO, trio.ai http://articulab.hcii.cs.cmu.edu/sigdial2016/"""	amnesia: the dark descent;computation;computer science;cortana (halo);human–computer interaction;information engineering;language technologies institute;language technology;list of toolkits;personal computer;siri;user modeling	Yoichi Matsuyama;Alexandros Papangelis	2016			computer science;multimedia;human–computer interaction	Web+IR	-57.297225585310805	-17.9979432700792	150947
f1d144071b42d6848594a183e2301982fa750896	motivating the church-turing thesis in the twenty-first century	computability theory;church turing thesis;markov algorithm;turing machine;simulation software;theory of computing;vector machine;register machine	Theory of Computation students frequently fail to appreciate the significance of the Church---Turing Thesis for one of two reasons. First, there is a tendency, on the part of students, to regard Church---Turing as tautologous and, consequently, devoid of important content. Second, there is a contrary impulse to view Church---Turing as unmotivated or even implausible. We describe our experience using simulation software in an effort to combat these two tendencies.	church–turing thesis;simulation software;theory of computation;turing	R. Gregory Taylor	1998		10.1145/282991.283551	support vector machine;computability theory;turing reduction;time hierarchy theorem;simulation software;nspace;computer science;turing machine;theoretical computer science;universal turing machine;turing completeness;church–turing thesis;markov algorithm;computability;programming language;non-deterministic turing machine;algorithm characterizations;algorithm;register machine;super-recursive algorithm	HCI	-57.197463459753216	-21.821767641202314	151319
75c107ca50bb630ff3317c3266c89abe35636a92	beware the engineering metaphor	software development	There are great similarities between the work of the software developer and the work of the mathematician.	software developer	Wei-Lung Wang	2002	Commun. ACM	10.1145/506218.506237	computer science;software development	SE	-55.75856754608827	-18.178812199624467	151800
036e9016231654b6d33b402644fd80488834a1e4	so what do we really mean when we say that systems biology is holistic?	simulation and modeling;systems biology;physiological cellular and medical topics;models biological;computational biology bioinformatics;terminology as topic;philosophy;system biology;algorithms;bioinformatics	An old debate has undergone a resurgence in systems biology: that of reductionism versus holism. At least 35 articles in the systems biology literature since 2003 have touched on this issue. The histories of holism and reductionism in the philosophy of biology are reviewed, and the current debate in systems biology is placed in context. Inter-theoretic reductionism in the strict sense envisaged by its creators from the 1930s to the 1960s is largely impractical in biology, and was effectively abandoned by the early 1970s in favour of a more piecemeal approach using individual reductive explanations. Classical holism was a stillborn theory of the 1920s, but the term survived in several fields as a loose umbrella designation for various kinds of anti-reductionism which often differ markedly. Several of these different anti-reductionisms are on display in the holistic rhetoric of the recent systems biology literature. This debate also coincides with a time when interesting arguments are being proposed within the philosophy of biology for a new kind of reductionism. Engaging more deeply with these issues should sharpen our ideas concerning the philosophy of systems biology and its future best methodology. As with previous decisive moments in the history of biology, only those theories that immediately suggest relatively easy experiments will be winners.	antireductionism;experiment;holism;identifier;independence day: resurgence;reductionism;systems biology;theory;explanation	Derek Gatherer	2009		10.1186/1752-0509-4-22	biology;computer science;bioinformatics;physiology;systems biology;philosophy of biology;algorithm;complex systems biology	Comp.	-57.76703566170067	-22.69570925542506	151892
529fba94fe4303a9c7379940c5af29e79317b9aa	guest editors' introduction: special section on computer arithmetic	computer applications;arithmetic	COMPUTER arithmetic is the mother of all computer research and application topics, like mathematics (as the title of a famous book by E.T. Bell) is the queen and servant of sciences and arithmetic the queen of mathematics. The etymology itself of the word computer is intrinsically related to the concept of arithmetic and mathematics. Interesting to note is that the origin of the word computer comes from the Latin word computare, which is defined as count, evaluate, calculate the result. In all cases, the connection between computers and computer arithmetic goes beyond simple etymology reasons. Computers are designed either to perform a specific calculation or to have extensive programmability for many changing tasks. In either case, at a certain level this translates into doing computations and arithmetic evaluations. Just to give two examples:	arithmetic logic unit;computation;computer	Peter Kornerup;Paolo Montuschi;Jean-Michel Muller;Eric Schwarz	2009	IEEE Trans. Computers	10.1109/TC.2008.11	arithmetic;computer science;theoretical computer science	Theory	-57.200648976219604	-22.73951881290875	152245
63319cf3bc7a8818a7fb4ea9968a95d46e23fe89	discrete mathematics for computer science majors - where are we? how do we proceed?	discrete mathematics	It has been nine years since Anthony Ralston and Mary Shaw called for a rethinking of the importance of sound mathematical training for undergraduate computer science majors [14]. In their paper they stressed the need to develop a two-year sequence in discrete mathematics for beginning computer science majors. Since that time numerous articles about such a sequence have appeared in both mathematics and computer science journals [4], [9], [12] and [13] and a number of panel sessions at professional meetings of SIGSCE and of the Mathematical Association of America (MAA) have been held. After all this time questions about the place of discrete mathematics in the undergraduate curriculum are still being debated. One question that is no longer being asked is: should discrete mathematics be part of a computer science major's undergraduate program? The questions that are being asked now and for which there are no easy answers are: at what level should discrete mathematics be taught? should there be one course, two courses or even three courses? what should the prerequisites be for these courses? and what topics should be presented in these courses? Computer scientists and mathematicians who have read the literature, listened to the debates, examined the textbooks or taught a course in discrete mathematics or discrete structures know that there appears to be little agreement as to how and what works and when it works best. This paper attempts to analyze the current situation in more detail and to offer a few suggestions to keep the dialogue alive.	cliff shaw;computer science;discrete mathematics	William A. Marion	1989		10.1145/65293.65314	computer science	Theory	-59.98657183614859	-20.490250715103947	152485
d5952d62d55ca700597408875f7700b0edc92189	the bratko-kopec test recalibrated		The B-K test (henceforth BKT) has enjoyed wide application and, as is inevitable with the tool of such long standing, has come in for a great deal of criticism as well. Critics have re that it was based on a very limited nutnber of positions (only 24). Proceeding from this fact it has been argued that, first, such a limited number could not provide a representative sample of the hugh set of all positions and, second that the selection of positions had been made in a manner ill-designed to be representative because it had concentrated, intentionally, on configurations bringing out only certain properties of the programs under test. We also note that the scoring in the BKT relies on the notion of best move; this notion may be stnbiguous in sotne positions. The present short note will argue that, in spite of its great age, the BKT still hits definite vsbdily.		Shawn Benn;Danny Kopec	1993	ICGA Journal	10.3233/ICG-1993-16305	artificial intelligence;computer science	PL	-58.83529413601928	-22.471556899223025	153196
d5b35b9562887f8dc06459d2758784955a0a31e1	let's catch the train to monte-carlo		While Monte-Carlo Tree Search (MCTS) has successfully been implemented in many games, its effectiveness appears to be greatest in the game of Go. In this thesis, Hendrik Baier even earmarks MCTS “the dominating paradigm in the challenging field of computer Go.” Having mentioned Go, there is no escaping linking another statement from this thesis to the recent astonishing accomplishment by DeepMind’s AlphaGo. It illustrates how fast computer game playing is currently improving – irrespective of whether you call that Artificial Intelligence or not. In Section 3.2.1, Baier writes: “the world’s best human Go players are still superior to the best computer programs, and writing a master strength Go program stands as a grand challenge of AI.” Four months after Hendrik Baier defended this PhD thesis, AlphaGo convincingly beat Lee Sedol 4-1. Lee is considered to be the best Go player in the world over the last decade.2 A magnificent achievement indeed, but let us not forget that it is the culmination of all the hard work over the past decades of a great many computer games researchers who have contributed to this monumental victory. One of the three pillars on which rests the success of AlphaGo is MCTS, the topic of this thesis. It would be interesting to know whether AlphaGo contains any of the MCTS enhancements that Hendrik Baier describes in this thesis or in any of his scientific papers. The oldest paper I could find (Baier and Drake, 2010), was published in the IEEE Transactions on Computational Intelligence in 2010, and describes an improvement to the Last-Good-Reply Policy in Monte Carlo Go.	alphago;artificial intelligence (video games);computation;computational intelligence;computer go;computer program;grand challenges;monte carlo method;monte carlo tree search;pc game;programming paradigm;scientific literature	Dap Hartmann	2017	ICGA Journal	10.3233/ICG-160003	simulation;monte carlo method;computer science	AI	-57.8847367985936	-21.59967158188789	153946
23dd74f75f995f2a87ed1c0365cda6d2a425559e	european aisb summer school on knowledge systems	artificial intelligent;knowledge systems	A most successful meeting was held by the Study Group on Artificial Intelligence and Simulation of Behavior at Oxford University this summer. It took the form of a number of Britain's more distinguished workers in AI, Dr. M. B. Clowes, Mr. P. Hayes, Professor H. C. Longuet-Higgins, Mr. A. Mackworth, and Professor D. Michie each giving a series of lectures and leading discussions on the problems of how human and artificial intelligence organize and use knowledge.	hayes microcomputer products;higgins;knowledge-based systems;society for the study of artificial intelligence and the simulation of behaviour	Keith Oatley	1973	SIGART Newsletter	10.1145/1056786.1056788	computer science;artificial intelligence;knowledge-based systems;operations research	AI	-57.65582526191052	-17.1618828966223	154454
a30e880e3fcbb8ad29badf1db1e5005ffcdee766	turing's taxi		From the intersection of computational science and technological speculation, with boundaries limited only by our ability to imagine what could be.Ride with an autonomous AI cab driver that might actually know too much about where it's going...	autonomous robot;computational science;turing	Brian Clegg	2017	Commun. ACM	10.1145/3107917	turing;theoretical computer science;computer science;artificial intelligence	AI	-57.04038347003368	-23.558364685501825	154475
553f26b04fb2a86594fba27a9764db163322a479	gerald edelman and giulio tononi, a universe of consciousness: how matter becomes imagination, new york: basic books, 2000, xiii+ 274 pp., $17.00 (paper), isbn 0-465-01377-5	new york;giulio tononi;gerald edelman;matter becomes imagination;basic books	new york;giulio tononi;gerald edelman;matter becomes imagination;basic books	international standard book number	David J. Cole	2003	Minds and Machines	10.1023/A:1024146021837	cognitive science;theology	NLP	-58.97786216231113	-22.797311130120114	154753
4163a92640fb85b4f4234327b149694a7fe2be69	a look at the future [guest editorial]	microprocessors;semiconductor memory;application software;special issues and sections;special issues and sections application software microprocessors semiconductor memory microcomputers computer industry;computer industry;microcomputers	If predictions about the future were as easy to come by as judgments about the past, the set of papers I organized for this 25th Anniversary issue might well have focused on the next 25 years—particularly since other papers in this issue deal with the corresponding past quartercentury. But predicting the future is always a chancey business, and trying to look 25 years into the future of our fast-moving industry is a long shot indeed. So, sacrificing symmetry for caution, we have focused most of our attention on the nearer term.		Linder Charlie Hobbs	1976	IEEE Computer	10.1109/C-M.1976.218461	application software;semiconductor memory;computer science;operating system;microcomputer	Visualization	-61.427442815024044	-21.299950598481985	154772
fc661981ca54070614873c8a0c1f4215d3652a95	the second acm sigspatial phd workshop report seattle, washington, usa: november 3, 2015		The ACM SIGSPATIAL Ph.D. Workshop is a forum where Ph.D. students present, discuss, and receive feedback on their research in a constructive atmosphere. The Workshop will be attended by professors, researchers and practitioners in the ACM SIGSPATIAL community, who will participate actively and contribute to the discussions. The workshop provided an opportunity for doctoral students to explore and develop their research interests in the broad areas addressed by the ACM SIGSPATIAL community. We invited PhD students to submit a summary of their dissertation work to share their work with students in a similar situation as well as senior researchers in the field. Submissions provided a clear problem definition, explain why it is important, survey related work, and summarize the new solutions that are pursued. Submissions also focused on describing the contribution they made in their doctoral dissertation. The strongest candidates were those who have a clear topic and research approach, and have made some progress, but who are not so far along that they can no longer make changes.		Peter Scheuermann;Mohamed Sarwat	2016	SIGSPATIAL Special	10.1145/3024087.3024093	constructive;data science;engineering management;computer science	HCI	-62.09180542029111	-17.40125096430467	154880
afd055e299d590df03d4fbd632a49845071567c0	the art and science of dynamic network visualization	relational data;network dynamics;empirical research;graph drawing;social network	"""If graph drawing is to become a methodological tool instead of an illustrative art, many concerns need to be overcome. We discuss the problems of social network visualization, and particularly, problems of dynamic network visualization. We consider issues that arise from the aggregation of continuous-time relational data (""""streaming"""" interactions) into a series of networks. We discuss our experience developing SoNIA (Social Network Image Animator, http://sonia.stanford.edu) as a prototype platform for testing and comparing layouts and techniques, and as a tool for browsing attribute-rich network data and for animating network dynamics over time. We discuss strengths and weakness of existing layout algorithms and suggest ways to adapt them to sequential layout tasks. As such, we propose a framework for visualizing social networks and their dynamics, and we present a tool that enables debate and reflection on the quality of visualizations used in empirical research. 1 This in an equally co-authored paper (names in alphabetical order). The project was generously supported by a research incentive award given to McFarland by Stanford University's Office of Technology and Licensing (grant# 2-CDZ-108). We thank the editor and anonymous JOSS reviewers for their insightful comments, and extend special thanks to Kaisa Snellman, Ben Shaw, Ozgecan Kocak, and James Moody for their contributions at various stages of this project. We also benefited from valuable feedback from participants of James G. March’s “Monday Munch” at Stanford University and participants at the satellite symposium run by Tom Snijders, “Dynamics of Networks and Behavior,” in Portoroz, Slovenia (2004). Please send all correspondence to Daniel A. McFarland, 485 Lasuen Mall, Stanford, CA 94305 (mcfarland@stanford.edu)."""	algorithm;array slicing;cliff shaw;continuation;cyber monday;data architect;data architecture;distortion;dynamic data;graph drawing;information;interaction;joss;kaisa sere;microsoft windows;mind;multi-purpose viewer;name mangling;network formation;network theory;open-source software;prototype;robbins v. lower merion school district;sampling (signal processing);social dynamics;social network;social structure;stress ball;tom;usability;word lists by frequency	Skye Bender-deMoll;Daniel A. McFarland	2006	Journal of Social Structure		data mining;cylinder;machine learning;body orifice;artificial intelligence;inlet;flow (psychology);piston;flow control valve;armature (electrical engineering);computer science;versa	HCI	-59.09067714780836	-18.341627456130336	156471
039d0c1faa85fe46d6ed4ab5f901a4f603e4ba8d	would a digital brain have a mind?	digital computers;cognitive science;mechanistic endeavors digital brain human brain digital computer mind simulation journalistic hyperbole computing professionals artificial neural networks humanistic names;280200 artificial intelligence and signal and image processing;neural nets;humans brain modeling computational modeling nervous system computer simulation silicon communication system signaling length measurement data compression retina;brain models;neuroscience;380301 philosophy of cognition;professional aspects;consciousness;artificial intelligence;320700 neurosciences;artificial intelligence neural nets brain models professional aspects digital computers;human brain	"""C ertain recent events caused me to doubt whether I know my own mind or not. Let me explain. Last week, the first of our academic year, all first-year students in our degree program underwent a supervised test in which they pull an old computer to pieces and put it back together again. We give this test to put a healthy disrespect for digital cir-cuitry—which is, at heart, only carefully polluted sand—into each stu-dent's mind as early as possible. We intend this disrespect to counter the superstition, held both by naïve students and by members of the public susceptible to media persuasion, that digital machinery has much in common with the human brain. Yesterday, I went to a lunchtime philosophy club lecture titled """" Why the Body Is the Mind. """" Because some of the discussion related to consciousness, I recalled Giorgio Buttazzo's article, """" Artificial Consciousness: Utopia or Real Possibility? """" (Computer, July 2001, pp. 24-30). The juxtaposition suggested a strange contrast between computing people, who see mental capabilities in machines because they do not appreciate how complex the human brain is, and philosophers, who see complexities in the human mind because they do not appreciate that the brain and the computer share some simple and fundamental properties. However, not being a philosopher, I find it difficult to be confident that I understand them when they discuss the mind. This uncertainty leaves at least three possibilities: • either I understand the basic nature of my mind and the philosophers don't, • vice versa, or • we share the same understanding but express it in mutually incomprehensible language. Later, I read Bob Colwell's provocative essay """" Engineering, Science, and Quantum Mechanics """" (Computer, Feb. 2002, pp. 8-10). Toward his essay's end, Colwell reported of entan-glement theory that """" the [photon's] wave function's actual point of collapse is when a conscious mind perceives the results """" and that the collapse was caused by """" the synapses of our brains, acting in concert to form our minds, at the instant we detected the photon. """" Suddenly, I felt alone, isolated, out of my depth, and fearfully vulnerable. What follows is meant to enlist your sympathy and rebuild my confidence. the principal meaning of mind is """" that which thinks, feels, and wills, exercises perception, judgment , reflection, etc., as in a human or other conscious being: the processes of the mind. """" …"""	artificial consciousness;bob colwell;computer;mind;naivety;quantum mechanics	W. Neville Holmes	2002	IEEE Computer	10.1109/MC.2002.1009512	artificial brain;computer science;artificial intelligence;consciousness;artificial neural network;cognitive science	Graphics	-56.214624137541904	-22.674905524204043	156657
22795546d96af57c3d325488f3f813243ceca4a2	from piecemeal engineering to twitter technology: toward computational societies	qu yuan;open society;social implications of technology;computational society;human factors;machine intelligence;raymond kurzweil;computational society alan mathison turing qu yuan karl popper raymond kurzweil singularity open society;social implications of technology human factors machine intelligence computational and artificial intelligence;a m turing;alan mathison turing;computational and artificial intelligence;singularity;karl popper	2 1541-1672/12/$31.00 © 2012 IEEE IEEE INTELLIGENT SYSTEMS Published by the IEEE Computer Society Letters to the Editor: Send letters, including a reference to the article in question, to bkirk@computer.org. Letters will be edited for clarity and length. Articles: If you’re interested in submitting an article for publication, see our author guidelines at www.computer.org/ intelligent/author.htm. We’d Like to Hear from You Editor: Fei-Yue Wang, State Key Laboratory of Management and Control for Complex Systems, feiyue@ieee.org Coincidentally, 23 June 2012 marked anniversaries of two significant lives: it was the centennial of Alan Mathison Turing’s birth; and it was also the	complex systems;computation;ieee intelligent systems;turing	Fei-Yue Wang	2012	IEEE Intelligent Systems	10.1109/MIS.2012.73	singularity;computer science;artificial intelligence;human factors and ergonomics	Visualization	-58.13840033795272	-18.11862305707445	156986
570a2c72813c589c82c73941c4c2242aa0c5de74	"""acm president's letter: """"decade for dialogue"""""""		"""As we reach the end of 1970, I am compelled to reflect on what this year has meant to ACM. We seem to have weathered the biennial change of officers and Council, and we have a new Executive Director. But the event of great and lasting significance was ACM 70. The report of that meeting is still in preparation, but nine key challenges from ACM 70 to the computing profession have already been extracted into the Call for Papers for ACM 71. These nine challenges are merely the first and earliest conclusions that we will derive from this year's meeting. The full report will discuss the benefits being obtained from computer uses, and it will disclose both more fundamental and more subtle demands upon us. I suspect, moreover, that the full range of the challenges and our responses to them shall require a continuing dialogue that will occupy us all for at least the coming decade. Thus, ACM 71 will provide the first picture of our ability to understand the challenges and to respond. I would like to give my views of the immediate task facing us in preparing for ACM 71. As I understand what we were told by the more thoughtful speakers and panelists in September, they want assurance that someone is doing the fundamental scientific and technical work that will lead to the answers to problems they want solved. This means to me that we have to show that R&D in computers and their applications are related to real-world problems. I conclude from this that a technical program at ACM 71 which discusses computer technology in a narrow, introspective , and self-satisfied mode will signal to the world that ACM is unable to hold up its end of the dialogue. Mel Schwartz is dedicated to keep this from happening, but he needs help from everyone who believes in our profession's ability to be responsible and responsive. Let him know now who you are and how you want to participate in the dialogue. Here are the challenges: • Inefficiency of large, complex systems. I like to think of this as the """"artichoke effect."""" We seem to have a proclivity to add features, add functions, and add interfaces-layer upon layer-onto existing systems. Each succeeding layer has less and less useful or tasty substance on it, until the outside layers merely add weight, complexity, and a prickly hindrance to reaching …"""	complex systems;complexity;computer	M. Stuart Lynn	1970	Commun. ACM	10.1145/362814.362815	computer science;operations research	Graphics	-61.029448222058996	-20.36815647782098	157734
14b27f82211b54e4994b225dc553e6a60cb5a8f3	message from the outgoing editor in chief		This issue marks a major milestone for the Machine Vision and Applications journal for several reasons: MV&A is now a bimonthly journal; it is now an official journal of the International Association for Pattern Recognition (IAPR), and it is now under the direction of a new editorial board with Professor Trivedi as the Editor-in-Chief MV&A has experienced tremendous growth since the first issue was published eight years ago. It has firmly established itself as the place to publish high-quality papers in all aspects of machine vision and its applications. The special theme issues of MV&A are recognized as excellent sources for the state of the art in application-oriented machine vision research. We are indebted to the vision and leadership provided by the founding editors-in-chief, Dr. Masakazu Ejiri, Professor Ramesh Jain, Professor Andr60osterlinck, Dr. Jorge Sanz, and Professor Jack Sklansky. During the past two years, due to the large number of good quality papers accepted for publication, we were experiencing significant delays in publishing an accepted paper. With a bimonthly publication schedule we are now able to schedule a paper within a few months of acceptance. I would like to thank the former executive editor, Mr. Gerhard Rossbach of Springer-Verlag, for allowing us to exceed our page budgets in the interim and for approving the change to a bimonthly schedule. I am very grateful to all authors and guest editors for their patience and understanding during this period. IAPR is the premier international association of societies representing researchers and practitioners with interests in pattern recognition and machine vision areas. Due to the close match between the aims and scope of MV&A and IAPR we sought to establish a formal relationship with IAPR. I am very pleased to announce that MV&A has now been approved by the IAPR Governing Board as an official publication of IAPR. Through this arrangement individuals who are members of an IAPR member society (e.g., IEEE Computer Society, which represents the USA in IAPR) may subscribe to MV&A at a special reduced rate. Please contact your IAPR representative or MV&A for more details. I am very thankful to the past executive committee of IAPR and members of its governing board for their help and support. I am particularly indebted to Professor Gunilla Borgefors, past secretary of IAPR, for her tireless efforts in making this a reality, to Professors Jake Aggarwal and Josef Kittler, the past and present presidents of IAPR, for their support (see page 1 for a message from the IAPR President), and to Mrs. Susan Duff of the IAPR secretariat and Dr. Maria Petrou, the IAPR newsletter editor, for their help in promoting this mutual cooperative arrangement. MV&A's editorial board underwent a major reorganization during the past year. Members of the editorial board are now actively involved in the editorial decisions through their participation in the selection of referees, evaluation of the referee reports, and making final recommendations on a manuscript. I am grateful for the excellent advice and support that I have received from the members of the editorial board and from Dr. Hans Woessner, the executive editor of Springer-Verlag. I am also thankful to the past and present editorial coordinators, Mrs. Donna Moore and Mrs. Mrinalini Kasturi, for their excellent secretarial support and to the production editor, Ms. May Doebl and her staff at Springer, for all their efforts to ensure timely publication of manuscripts. The reputation and utility of a journal are ultimately determined by the quality of the papers that it publishes. On behalf of the entire editorial board I would like to thank the authors for submitting high-quality manuscripts and all the referees for their voluntary service to ensure that the articles accepted for publication meet the rigorous standards of the journal. We sincerely acknowledge and appreciate the occasional feedback that we receive from our readers and encourage them to continue to send their comments, criticism, commendations, and suggestions. Finally, it is indeed my pleasure to announce that Professor Mohan Trivedi has agreed to lead MV&A as its editor-in-chief Professor Trivedi is an internationally recognized expert in the areas of machine vision and robotics. His editorial along with a brief biographical sketch follows this note.	friedrich kittler;international association for pattern recognition;international association of privacy professionals;machine vision and applications;robotics;springer (tank);the machine	Rangachar Kasturi	1995	Machine Vision and Applications	10.1007/BF01213632	computer vision;speech recognition;computer science;artificial intelligence	Visualization	-62.372069522064535	-18.230153883348695	158717
4ca1cc28fb5796854544b5962fd181ffab0aeb7e	introduction to interval analysis	interval analysis	This unique book provides an introduction to a subject whose use has steadily increased over the past 40 years. An update of Ramon Moore’s previous books on the topic, it provides broad coverage of the subject as well as the historical perspective of one of the originators of modern interval analysis. The authors provide a hands-on introduction to INTLAB, a high-quality, comprehensive MATLAB® toolbox for interval computations, making this the first interval analysis book that does with INTLAB what general numerical analysis texts do with MATLAB.	book;computation;hands-on computing;interval arithmetic;matlab;numerical analysis	Ramon E. Moore;R. Baker Kearfott;Michael J. Cloud	2009		10.1137/1.9780898717716	computer science;theoretical computer science;algorithm	Logic	-58.78532448387636	-20.586058804708824	158901
3f12bd295ac3e299f199319e21c12f64b27c22e7	introduction to artificial intelligence: an ai text	artificial intelligent	"""""""Are we intelligent enough to understand intelligence? One approach to answering this question is """"artificial intelligence,"""" the field of computer science that studies how machines can be made to act intelligently. This book is intended to be a comprehensive introduction to the field of artificial intelligence, written primarily for the student who has some knowledge of computers and mathematics (say, at the junior or senior levels of college). In general, this book is addressed to all persons who are interested in studying the nature of thought, and hopefully much of it can be read without previous, formal exposure to mathematics and computers."""	artificial intelligence;computer science	Philip C. Jackson	1974	SIGART Newsletter	10.1145/1045190.1045191	applications of artificial intelligence;natural language processing;intelligence cycle;nouvelle ai;music and artificial intelligence;marketing and artificial intelligence;progress in artificial intelligence;computer science;artificial intelligence;symbolic artificial intelligence;ai-complete;artificial psychology;artificial intelligence, situated approach	AI	-56.46492344243439	-20.851430542159633	159383
044095e21e5755dc288344ae014ef53fcc0f6500	preface and introduction		and Technology (ARIST) and the first under my editorship. Readers who are new to ARIST and interested in its purpose and evolution may wish to visit the Web site hosted on the homepage of the American Society for Information Science and Technology (ASIST), the parent organization (http://www.asis. org/Publications/ARIST/index.html). For the last twentyfive years ARIST has been expertly edited by Professor Martha Williams, to whom we owe a very considerable debt. She has set an exacting benchmark for her successors. A change in editorship is almost invariably accompanied by changes in the product, whether in terms of style, coverage, or design, or all of the foregoing. Connoisseurs of ARIST will quickly notice some of the initial changes, and not all of these, I imagine, will be greeted with unalloyed enthusiasm. Perhaps the most obvious breach with convention is the adoption of the APA (American Psychological Association) referencing style, a move welcomed rapturously, it must be said, by some contributing authors. Since the Journal of the American Society for Information Science and Technology (JASIST) uses this convention, and since ARIST and JASIST are stablemates with much in common, not least authors, readers, and topic coverage, there seems little reason for ARIST to persist with its rather baroque bibliographic practices: all the more so now that JASIST’s Preface and Introduction	annual review of information science and technology;baroque;benchmark (computing);code coverage;journal of the association for information science and technology;webserver directory index;world wide web	Blaise Cronin	2002	ARIST	10.1002/aris.1440360101	contingency table;ordered logit;conceptualization;decision matrix;mathematical economics;topic model;guttman scale;natural number;polytomous rasch model;artificial intelligence;mathematics	AI	-62.61404490978135	-19.724106806889996	159497
e2fb4ce610084ee47e6ab10691832ea09109d0e4	failure is not just one value	programming language;environmental conditions;standardisation;health problems;language design	"""An old cartoon showed a scientist, mad gleam in the eye, holding up a test-tube and saying to a colleague """"See! I have discovered a cure for which there is no disease!"""". The paper by Wong and Ooi on failure as a value [1] seemed to have a similar flavour. Though it is accepted that the motivation was different, as explained in the paper, there was still a sense of """"Here is a problem"""" (the apparent asymmetry of twos complement arithmetic) """"which needs a solution, so find a disease for which that solution is a cure"""". The argument of the present paper is that the supposed problem does not exist, and that as a proposed solution it is not a cure for the disease though the disease undoubtedly does exist."""	computation;first draft of a report on the edvac;mind;ocean observatories initiative;two's complement	Brian L. Meek	1990	SIGPLAN Notices	10.1145/87416.87470	natural language processing;computer science;programming language;programming language specification;standardization	ECom	-57.332322060780704	-22.289368803281345	159511
44708c08bb9b17ec42aded8d3cf39e23993d396d	"""errata for """"crowdsourcing algorithms for entity resolution"""" (pvldb 7(12): 1071-1082)"""		Recall #Questions dec rand node Figure 1: Corrected Figure 12(b): #Questions vs Recall for Places We discovered that there was a duplicate figure in our paper. We accidentally put Figure 13(b) for Figure 12(b). We have provided the correct Figure 12(b) above (See Figure 1). Figure 1 plots the recall of various strategies as a function of the number of questions asked for Places dataset. There was no error in the discussion in our paper (See Section 6.2.1 in our paper for more details). Obtain permission prior to any use beyond those covered by the license. Contact copyright holder by emailing info@vldb.org. Articles from this volume were invited to present their results at	box counting;crowdsourcing;rand index	Norases Vesdapunt;Kedar Bellare;Nilesh N. Dalvi	2015	PVLDB	10.14778/2735479.2735495	speech recognition;computer science;artificial intelligence;data mining	DB	-61.49007541913192	-18.514291596942495	159583
2e014de04042fd207ceaa1145e82ec08ab298450	george vladutz - obituary		Dr. George E. Vladutz, a highly respected member of the chemical information community, passed away on September 3, 1990. A native of Romania, George started his studies in chemistry at the Bucharest Polytechnical Institute and in 1952 received an M.Sc. in Chemical Engineering from the Leningrad Technological Institute. In 1956, he received a Ph.D. in Synthetic Organic Chemistry from the MendelEev Institute for Chemical Technology in Moscow. This was followed in 1967 with a DSc. in Chemical Information Science from the Institute for Organic Chemistry of the Elements, also in Moscow. As early as the fifties, during his long career (1958-1974) at VINITI, Dr. Vladutz began to provide practical uses for the computer in the storage and retrieval of information concerning chemical reactions. I n his 1963 publication “Concerning One System of Classification and Coding Chemical Reactions”, he formulated the ideas which are the basis of today’s methods for the indexing and computerized retrieval of organic chemical reactions. His was the earliest suggestion that reaction analysis could be done automatically, and he was apparently the first to describe the concept of a reaction site. He was also the first to formulate and illustrate the principals which allow a computer, using a library of rcaction types, to proceed backwards from a target compound in order to generate possible routes for its synthesis. This latter work was published 6 years prior to that of Corey and Wipke and at a time when computerized methods for searching chemical compounds-something that is taken for granted today-was in its very early stages of development. I n 1975, Dr. Vladutz emigrated from Russia and became a Visiting Lecturer at the Postgraduate School for Librarianship and Information Science at the University of Sheffield i n England. The following year, he joined the Institute for Scientific information in Philadelphia where he eventually became Manager of Basic Research. In this position, he continued his work on reaction retrieval methods and in recent years was also involved in the development of new, associative methods of information retrieval through the application of citation-based techniques of bibliographic coupling. George’s international reputation for practical insight into thc nature of the most difficult chemical information problems, combined with his ability and willingness to solve them, made him a “natural resource” for all scholars and practitioners of chemical information science. He was frequently a speaker at scientific meetings around the world and his more than 100 published papers made a solid contribution to the knowledge base of our profession. But George has left a gift even greater than the cumulative wisdom of an outstanding research career. For those of us who worked closely with him, he taught us to truly enjoy not only the pursuit of knowledge-but life itself. He approached every aspect of work and life with child-like innocence and enthusiasm. He could make a walk in the garden fun-despite a deluge of rain-because of his joy in the beauty of flowers. He could enjoy amusement parks and walks on the beach with the same intensity with which he enjoyed unraveling scientific problems. He was a teacher, he was a friend, and we loved him. Because of his work, today’s chemists reap the benefit of computerized reaction analysis and retrieval. If structures represent the language of chemistry, then reactions must be its prose, and Dr. George Vladutz is one of the greatest contributors to the widespread enjoyment and use of the chemical literature.	bibliographic coupling;cheminformatics;information retrieval;information science;knowledge base;librarian;synthetic intelligence	Bonnie Lawlor	1990	Journal of Chemical Information and Computer Sciences	10.1021/ci00068a002	combinatorics;applied mathematics;mathematics;obituary	Web+IR	-60.18410745396762	-19.43177489782145	159949
c15903ef2e7292c8382c71dcc5174a8c4c4ad41f	molecular graphics and drug design: introduction.	drug design	Effective science education for undergraduate students who are not specializing in the scientific disciplines continues to be a widespread goal in our nation's colleges and universities. At NYU there has been an unusually strong commitment to this endeavor. When the Morse Academic Plan--NYU's undergraduate core curriculum--was established in 1995, it included an innovative, three-course sequence in mathematics and science for non-majors. This sequence, called the Foundations of Scientific Inquiry (FSI), consists of three courses: Quantitative Reasoning (mathematics), Natural Science I (physical sciences) and Natural Science II (life sciences). Each of the course offerings in the FSI program has a weekly workshop or laboratory where students engage in the process of mathematical reasoning or experimental scientific investigations. A central objective of the faculty who design and teach these courses is to prepare NYU graduates to make informed personal and social decisions in a world that is becoming increasingly influenced by scientific and technological advances.	molecular graphics;stanford university centers and institutes	Teri E. Klein;Kimberle Koile	1994			computer science;drug design	AI	-58.855697790963994	-18.973329890082518	160330
d8f32b6917dbd8297d69a3e4046793c084146c53	book review: grammatical evolution: evolutionary automatic programming in an arbitrary language	automatic programming;grammatical evolution;review	This is the first book written on grammatical evolution, a new technique that is receiving increasing attention and use. Therefore, the book fulfills an important role and occupies a currently empty niche. This branch of evolutionary algorithms has been described previously only in published papers, where space limitations have reduced clarity and the understanding of the readers. Therefore, the book should be received with the utmost interest and attention by specialists. The book contains a good description of grammatical evolution, which makes it possible for people with knowledge of evolutionary algorithms to understand, use and even improve the techniques suggested. The abundance of examples and applications should convince the reader that grammatical evolution is quite general and applicable to multiple fields. This book has evolved from a doctoral thesis, and this shows in a few places where the reader would like to find further explanations, rather than having to proceed to the references to fill in the gaps. E.g., this happens in chapter 2 (a survey of previous works on the field) and in pages 45 and 106 (on the use of competent GAs). On the other hand, because it includes all the information needed to follow their work in detail, the book is clearly useful in preference to the papers previously published by its authors. Most of “Grammatical Evolution” is clear and well organized, although there are a few places (e.g. pages 45–46) where the presentation is confuse. Chapter 7 (on crossover) would be more complete with a more detailed explanation of different crossover procedures. Also some of the figures in chapter 7 are difficult to interpret because too much information has been clustered in a small space. A more complete index would also have been desirable. A good feature is the new classification proposed for the old field of genetic programming. This introduces in its stead a new field (evolutionary automatic programming), itself subdivided into two areas: tree-based genetic programming (which comprises most of the previous genetic programming field) and string-based genetic programming, which includes grammatical evolution. “Grammatical Evolution” should be useful for specialists and Ph.D. students in the fields of grammatical evolution and genetic programming, and people working in artificial intelligence and genetic algorithms in general. We would advise it as a good resource for university libraries. A reader without previous knowledge of genetic algorithms could have some difficulty in following the comparison of grammatical evolution with other methods, but even so should be capable of using the algorithms described in the book for practical applications.	artificial intelligence;automatic programming;crossover (genetic algorithm);evolutionary algorithm;genetic algorithm;genetic programming;grammatical evolution;library (computing);niche blogging	Manuel Alfonseca;Alfonso Ortega	2004	Genetic Programming and Evolvable Machines	10.1023/B:GENP.0000036057.27304.5b	natural language processing;computer science;artificial intelligence;machine learning;programming language;grammatical evolution	AI	-57.89492319910916	-22.047426006998965	160463
e017bc2187c7b35b262e05b7cb3a7bf0854f51e9	on knowledge base management systems: integrating artificial intelligence and database technologies, book resulting from the islamorada workshop 1985 (islamorada, fl, usa)		Dear readers, when you are hunting the new book collection to read this day, on knowledge base management systems integrating artificial intelligence and database technologies can be your referred book. Yeah, even many books are offered, this book can steal the reader heart so much. The content and theme of this book really will touch your heart. You can find more and more experience and knowledge how the life is undergone.	artificial intelligence;book;knowledge base;management system	Michael L. Brodie	1986				AI	-57.55673369865983	-19.318671815548523	160760
b4af6e7f989e445ba87ddd32bd9e49ad84eb9075	a mistake on my part		I first met Dov at a logic conference in Manchester, in August 1969, though we had begun a mathematical correspondence the previous year. Here are a few photos from the conference. As I recall, Dov planned to get married shortly after the conference. I found a letter in my files mentioning that I sent Dov a copy of the pictures in 1969, so this is for everybody else. Figure 1 shows Dov at Jodrell Bank, the huge radio telescope complex run by the University of Manchester.		Melvin Fitting	2005			radio telescope;mistake;performance art;history	EDA	-61.363067388974336	-19.608186208103298	161941
46c1ee48fcc6abab734af8211bdc2078562401be	up and coming sigweb supported conferences		SIGWEB supports several specialized conferences, short courses, and workshops, as well as the Annual Hypertext Conference. SIGWEB sponsored conferences focus on timely topics in applied and computational hypertext and Web disciplines and provide a place for members and the entire applied Hypermedia and Web community to exchange ideas and to meet with and expand their network of colleagues. In this article, we provide a brief overview of SIGWEB sponsored conferences, in addition to events that are in cooperation with SIGWEB.	hypermedia;hypertext	Yogesh Deshpande	2009	SIGWEB Newsletter	10.1145/1592394.1592400		DB	-62.0633396532963	-17.490013351681974	162482
b436338ccb339a2cffa8bcdb61c13d11036ed020	report on the third ifac round table discussion session (rt-15) on fuzzy decision making and applications at the seventh ifac world congress	formal presentation;ifac world congress;ifac world congress;main feature;round table discussion session;ifac round table discussion;control theory;panel member;interesting discussion;ifac report;popular subject;interesting regular session;fuzzy decision;subject matter	This report summarizes the main features of the Round Table Discussion Session (RT-15) that took place during the 7th IFAC World Congress. In spite of another parallel round table on a more popular subject (next decade of control theory and application) and some other interesting regular sessions, it was a well attended session and was attended by nearly 50 delegates (a partial list is enclosed herewith). The subject matter was discussed by six panel members who presented their work also. Following the formal presentation of their work by the panel members, some interesting discussion emerged during the session.		Madan M. Gupta	1979	Automatica	10.1016/0005-1098(79)90058-X	computer science;operations research	Crypto	-61.13715224297128	-17.53906415320898	162627
e7791d9f77ccada62316ba67a3261f3f4ac6b6f1	preface to special issue on advances in cloud computing		In 2008 the two first guest editors of this special issue, Chunming Rong and Frode Eika Sandnes, drafted the ideas for the first international conference on cloud computing during a conference in Hainan Island, China. One year later the conference became a reality and the response and interest was overwhelming. Rong and Sandnes later invited the third guest editor, Rajkumar Buyya, to join the guest editorial team. This special issue entitled Advances in Cloud Computing presents the best papers presented at the first international conference on Cloud Computing (CloudCom) that was held December 1–4 in Beijing China. It has grown to be a premiere venue on cloud computing research, and has since been hosted in Indianapolis, USA (2010), Athens, Greece (2011) and in 2012 CloudCom will be hosted in Taipei, Taiwan. Of the more than 100 papers finally accepted for presentation at CloudCom 2009 the ones that received the best review reports were invited to submit revised and extended versions of their manuscripts and these underwent several rounds of review. Of the invited papers only the handful presented herein were accepted for inclusion in this special issue.	cloud computing;cloud research;venue (sound system)	Chunming Rong;Frode Eika Sandnes;Rajkumar Buyya	2012	The Journal of Supercomputing	10.1007/s11227-012-0780-z	computer science;library science;distributed computing;china;cloud computing;beijing	HPC	-60.94615540235415	-17.57330177285852	162890
56dae94f5498a161257167340ebfe2e5d2b7b2b1	6 years of smt-comp	competition;sat modulo theories;experimental evaluation	The annual Satisfiability Modulo Theories Competition (SMT-COMP) was initiated in 2005 in order to stimulate the advance of state-of-the-art techniques and tools developed by the Satisfiability Modulo Theories (SMT) community. This paper summarizes the first six editions of the competition. We present the evolution of the competition’s organization and rules, show how the state of the art has improved over the course of the competition, and discuss the impact SMT-COMP has had on the SMT community and beyond. Additionally, we include an exhaustive list of all competitors, and present experimental results showing significant improvement in SMT solvers during these six years. Finally, we analyze to what extent the initial goals of the competition have been achieved, and sketch future directions for the competition.	knowledge spillover;modulo operation;satisfiability modulo theories;simultaneous multithreading;technical support;the 3-d battles of worldrunner;venue (sound system)	Clark W. Barrett;Morgan Deters;Leonardo Mendonça de Moura;Albert Oliveras;Aaron Stump	2012	Journal of Automated Reasoning	10.1007/s10817-012-9246-5	competition;computer science;artificial intelligence;mathematics;algorithm	NLP	-56.240294684228154	-17.581270324020892	163293
16502fecef857ae009aa7a794f3c0a585905dc2b	engineering, science, and quantum mechanics	auditory system;biology;quantum mechanics physics education electrical engineering knowledge engineering humans chemistry biology fets auditory system;physics;quantum mechanics;fets;chemistry;humans;electrical engineering;knowledge engineering	A physics grad student once asked me what I was studying. When I said electrical engineering, she was clearly disappointed. I asked why she had that reaction— while wondering why she hadn’t even bothered to hide it. She replied, “Engineers are only optimizers of what scientists have already figured out. Science provides models of how things really work, and engineers just tweak the knobs.” I knew there was something very wrong in that blithe dismissal of my field. I think I told her, Oh yeah? Well, my dad can take your dad any day. Fortunately, she didn’t demand proof. In the years since that encounter, I’ve noticed that physicists don’t single out engineering—they sometimes dismiss all the other sciences too. Theodore von Karman said it best: The scientist describes what is; the engineer creates what never was. Both science and engineering must identify and work at the limits of knowledge—meaning that both operate at the limits of human intellect. The asymmetry is that engineers know that good products don’t automatically pop out when they turn a knob, but not everyone else knows it. So it’s easy to feel underappreciated. Perhaps those who study the sciences infer some wrong ideas from the fact that university curricula generally do not expose them to engineering. Engineers, on the other hand, spend a lot of time studying science, especially chemistry and physics, and soon, I predict, biology. The motivation is a good one: Teaching budding engineers the fundamentals of quantum mechanics, for example, gives them a useful base for later studying how field-effect transistors work.	control knob;electrical engineering;gradient;intellect;optimizing compiler;quantum mechanics;transistor;turned a	Robert P. Colwell	2002	Computer	10.1109/MC.2002.989920	artificial intelligence;knowledge engineering	Theory	-59.25006214026069	-23.57017843537906	163406
9670c6fb9d28de79d87851ddd9728e267c486fa1	library as a verb: technological change and the obsolescence of place in research		In recent years, it has become almost routine to speculate on the obsolescence or perseverance of the library. These speculations usually conflate the institution of the library with its physical location. This essay presents an overview of existing opinions on whether libraries will persevere, shows how changes in technology and the research process affect the concept of the library, and ultimately argues that the library may be better viewed as a process than as a place. Originally given in presentation form at the 8th Annual Conference of Technology, Knowledge, and Society in 2012, this paper is printed in the Informing Science: the International Journal of an Emerging Transdiscipline as an invited essay.	informing science;library (computing);printing	Stewart C. Baker	2014	InformingSciJ		humanities;library science;social science;engineering	EDA	-62.016640275034646	-19.932085823135296	163680
eae0c8a51f506986aee226b7e26cc65fbeb73b95	preface: algorithms, complexity and models of computation	model of computation	This Special Issue arises from the conference Theory and Applications of Models of Computation 2007, held at Fudan University, Shanghai in May 2007. Theory and Applications of Models of Computation (TAMC) is an international conference series with an interdisciplinary character, bringing together researchers working in computer science, mathematics and the physical sciences. This interdisciplinary approach, with an emphasis on the theory of computation in a broad sense, gives the series its special appeal within China and internationally. TAMC 2007 was the 4th conference in the series. The previous meetings were held May 17–19, 2004 in Beijing, May 17– 20, 2005 in Kunming, andMay 15–20, 2006, in Beijing, China. Subsequent TAMCmeetings include the 5th Annual Conference on Theory and Applications of Models of Computation (TAMC’08), held in Xi’an, and the 6th Annual Conference on Theory and Applications of Models of Computation (TAMC’09) to be held in ChangSha, from May 26 to May 30, 2009. It is expected that 2010 will see the first TAMC meeting outside of China, provisionally planned for Prague in the Czech Republic. The enthusiasm with which TAMC 2007 has been received by the scientific community is evident in the large number of quality articles submitted to the conference. There were over 500 submissions, originating from all over the world. This presented the ProgrammeCommitteewith amajor assessment task. The ProgrammeCommittee finally selected sixty-seven papers for presentation at the conference and inclusion in the LNCS pre-proceedings volume. This results in an acceptance rate of just over 13%, making TAMC an extremely selective conference, compared with other leading international conferences. Drawing on the best of these contributions, this Special Issue contains eleven invited papers around the theme of Algorithms, Complexity and Models of Computation, all refereed to the usual high standards of TCS. This is one of three special issues for TAMC 2007, each on a different thematic area, to be published by Theoretical Computer Science, series A,Mathematical Structures in Computer Science, and the Journal of Computer Science and Technology. The TAMC conference series arose naturally in response to important scientific developments affecting howwe compute in the twenty-first century. At the same time, TAMC is already playing an important regional and international role, and promises to become a key contributor to the scientific resurgence seen throughout China and other parts of Asia. TAMC is particularly recognised as addressing the need to develop a strong theoretical base for computer scientific progress, and has become the most truly international on the emergent conference scene. The excellence of the papers to be found here are fitting signifiers of this growing international involvement. We would like to thank our fellow Programme Committee members, and the many outside referees they called on, for the hard work and expertise which they have brought to the difficult selection process consequent on the unprecedented volume of submissions to TAMC 2007: Giorgio Ausiello (Rome, Italy), Eric Bach (UWMadison), Jin-Yi Cai (Wisconsin), Nicolo Cesa-Bianchi (Milano, Italy), Jianer Chen (Texas A&M University), Yijia Chen (Shanghai Jiaotong University), Francis Chin (Hong Kong), C.T. Chong (Singapore), Kyung-Yong Chwa (KAIST, Korea), Decheng Ding (Nanjing University), Rod Downey (Wellington), Martin Dyer (Leeds), Rudolf Fleischer (FudanUniversity), Oscar Ibarra (UC Santa Barbara), Hiroshi Imai (University of Tokyo), Kazuo Iwama (Kyoto University), Tao Jiang (University of California-Riverside/Tsinghua, Beijing), Satyanarayana Lokam (Microsoft ResearchIndia), D. T. Lee (Academia Sinica, Taipei), Angsheng Li (Institute of Software, CAS), Giuseppe Longo (Paris, France), Tian Liu (Beijing University), Rudiger Reischuk (Universitat zu Lubeck), Rocco Servedio (Columbia University), Alexander Shen (Institute for Information Transmission Problems, Moscow), Yaoyun Shi (Universityof Michigan, Ann Arbor), Ted Slaman (UC Berkeley), Xiaoming Sun (Tsinghua University), Luca Trevisan (UC Berkeley), Christopher Umans (Cal Tech), Alasdair Urquhart (University of Toronto), Hanpin Wang (Beijing University), Osamu Watanabe (Tokyo Institute of Technology), Zhiwei Xu (Institute of Computing Technology, CAS), Frances Yao (City University of Hong Kong), Mingsheng Ying (Tsinghua University, Beijing). And, most importantly, we would like to thank the participants and speakers for making the event such a resounding success.We also express our appreciation to Professor Giorgio Ausiello and themembers of the Editorial Board of Theoretical Computer Science for their encouragement and advice throughout the preparation of TAMC 2007, and this Special Issue.	algorithm;columbia (supercomputer);emergence;entity–relationship model;francis;frances yao;hideki imai;independence day: resurgence;lecture notes in computer science;martin dyer;model of computation;nicolò cesa-bianchi;oscar;rod downey;ted;theoretical computer science;theory of computation;uc browser;yao graph;eric	S. Barry Cooper;Hong Zhu	2009	Theor. Comput. Sci.	10.1016/j.tcs.2008.10.026	model of computation;computational science;symbolic computation;computer science;theoretical computer science;symbolic-numeric computation;mathematics;asymptotic computational complexity;algorithm	Visualization	-59.522410324069476	-18.10065597245974	163776
0f360a4bbcc517e07977bbfcf27cc18a722b51a9	artificial consciousness: a discipline between technological and theoretical obstacles	externalism;settore m psi 01 psicologia generale;phenomenal consciousness;robotics;artificial intelligent;artificial intelligence;artificial consciousness;settore ing inf 06 bioingegneria elettronica e informatica	Artificial consciousness is still far from being an established discipline. We will try to outline some theoretical assumption that could help in dealing with phenomenal consciousness. What are the technological and theoretical obstacles that face the enthusiast scholars of artificial consciousness? After presenting an outline of the state of artificial consciousness, we will focus on the relevance of phenomenal consciousness. Artificial consciousness needs to tackle the issue of phenomenal consciousness in a physical world. Up to now, the only models that give some hope of succeeding are the various kinds of externalism.		Riccardo Manzotti;Vincenzo Tagliasco	2008	Artificial intelligence in medicine	10.1016/j.artmed.2008.07.002	artificial consciousness;computer science;artificial intelligence;externalism;robotics;cognitive science	AI	-56.00679965164445	-22.12273358311008	164217
e8500fc2e66a4a210e03a84d8ef2ae2ebf1be067	dimensions of text processing	elementary operator;text processing;data processing	Numerical data processing has dominated the computing industry from its earliest days, when computing might better have been called a craft than an industry. In those early days it was not uncommon for a mixed group of scientists and technicians to spend an entire day persuading a roomful of vacuum tubes and mechanical relays to yield up a few thousand elementary operations on numbers. The emphasis on numerical applications was a wholly natural consequence of the dominant interests of the men and women who designed, built, and operated those early computing machines.	computer;ibm ssec;level of measurement;numerical analysis;relay	Gary R. Martins	1972		10.1145/1480083.1480102	simulation;engineering;artificial intelligence	HPC	-58.99140593470477	-23.169520132902775	165110
377daa14d51694427bba6942179ad8f96bc47a3b	young researchers' views on the current and future state of hri	future;robots humans human computer interaction abstracts argon;paradigms;kuhn;human computer interaction;research paradigm;hri;human robot interaction;argon;student perspectives;research and development;abstracts;robotic platforms;robots;kuhn human robot interaction hri student perspectives paradigms history of science future workshop panel robotic platforms;humans;longitudinal real world studies young researchers views nsf workshop graduate students human robot interaction hri researchers robust platforms robot perception human perception;research and development human robot interaction;workshop panel;human perception;graduate student;history of science	"""This paper presents the results of a panel discussion titled """"The Future of HRI,"""" held during an NSF workshop for graduate students on human-robot interaction in August 2006. The panel divided the workshop into groups tasked with inventing models of the field, and then asked these groups their opinions on the future of the field. In general, the workshop participants shared the belief that HRI can and should be seen as a single scientific discipline, despite the fact that it encompasses a variety of beliefs, methods, and philosophies drawn from several """"core"""" disciplines in traditional areas of study. HRI researchers share many interrelated goals, participants felt, and enhancing the lines of communication between different areas would help speed up progress in the field. Common concerns included the unavailability of common robust platforms, the emphasis on human perception over robot perception, and the paucity of longitudinal real-world studies. The authors point to the current lack of consensus on research paradigms and platforms to argue that the field is not yet in the phase that philosopher Thomas Kuhn would call """"normal science,"""" but believe the field shows signs of approaching that phase."""	human–robot interaction;ibm notes;unavailability	Kevin Gold;Ian R. Fasel;Nathan G. Freier;Cristen Torrey	2007	2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1228716.1228764	human–robot interaction;robot;simulation;human–computer interaction;computer science;artificial intelligence;argon;perception;history of science	HCI	-55.56309932904839	-20.738254489867582	165328
109ee3d390393228fb97d7b52adef71fdc136376	george graham, philosophy of mind: an introduction	philosophy of mind	content are not, to put it mildly, exactly what Searle or the Churchlands have in mind when arguing that philosophy of mind has lost touch in a damaging way with neurobiology. Dretske and Millikan accept neither the revisionist/eliminativist approach of the Churchlands nor the apparent biological essentialism of Searle. Though I would not hesitate to recommend this volume as part of an introduction to important work in philosophy of mind, I would, for at least these reasons, recommend it as only part of such an introduction. A stronger recommendation can be given to the volume if it is being considered for the task for which it was intended, as an introduction to recent work in cognitive science and in the philosophies of mind and language. The volume certainly would make an outstanding introduction to this broad set of topics, useful to both advanced students and professionals from one of the three disciplines presented in the volume wishing to explore connections with the other two.	cognitive science;graham scan;philosophy of mind	Justin Leiber	1999	Minds and Machines	10.1023/A:1008308214908	computational theory of mind;philosophy education;philosophy of sport;philosophy of mind;western philosophy;philosophy;contemporary philosophy;epistemology;computer science;modern philosophy;american philosophy;cognitive science	AI	-58.36258020860015	-22.767294318765547	165357
2f69a22c22a0b1a13e06c30771b12215bbb367c2	some notes on computer research in eastern europe	eastern europe	"""A recent article [1] has described some of the work of the VS, zkumn~ dstav Inatematick)eh stroju in Prague and its former director, Dr. Antonin Svoboda. One aspect, of Svoboda's work neglected in the article is his contribution to practical methods of logical synthesis, which has certain distinctive features compared with American practice. In the course of intensive practical work on the Aritma T-50, S~xPO, and other projects, Dr. Svoboda developed an approach to logical synthesis which has certain distinctive features compared with American practice. As illustrated by his Darmstadt [2] and Harvard [3] papers, Dr. Svo-boda's approach is partly based upon the straight binary map, which he developed independently, rather than the Karnaugh map. ~ The first electronic digital machine eonstructed at the Institute was a small linear interpolator for milling-machine control fl'om a punched-tape program. This machine uses entirely domestic materials. It is based on a ge-diode-gated logic employing double-triode flip-flop [6]. The algorithms of the machine were developed by M. Martinek, of the second generation of Svoboda's pupils. l)ue to the great activity of the Czechoslovak-machine-tool industry, it is to be expected that this line of activity will be further pursued and expanded. Other Centers At the time the author first went to Czechoslovakia in 1948, a trade exposition was held in Prague in which electronic equipment was conspicuous by its absence. Aside from broadcast receivers and a closed-circuit tele-American practice seems to favor the latter form. """"The Veiteh technique is generally not useful for more than the four-variable case, while the Karnaugh map can be easily extended to handle six-variable problems."""" [4] (Veitch [5] proposed the straight-binary inap in 1952.) Dr. Svoboda uses six-variables for illustrative examples and homework in his courses (!), while it is the author's experience that in using his minimization method [3] twelve variables are handled without difficulty manually. At the present time Dr. Svoboda is conducting research on punched-card and computer synthesis methods. It is to be hoped that they will be published soon. vision demonstrated by the Czechoslovak Broadcasting Corporation, there was just nothing. Electronics research, as distinct from development of appliances under licenee, was just, beginning. The main supply of components was still German war surplus. A visitor to the Brno Engineering Exposition in 1958 would have been, on the contrary, very favourably impressed by Czechoslovak activities in electronics-components , instruments, research. Here we shall briefly mention some of …"""	algorithm;antonín svoboda;closed-circuit television;combinatorial chemistry;diode;flops;flip-flop (electronics);karnaugh map;punched card;second generation multiplex plus	Morton Nadler	1959	Commun. ACM	10.1145/368518.368524	computer science	HCI	-62.31201599014547	-20.60888549904622	165363
2b3579b3ce78ca283bde8f7c8cf382d3390188ec	"""report on ifip tc-2 conference: """"a technical in-depth evaluation of the ddl"""""""	technical in-depth evaluation;ifip tc-2 conference	In response to increasing interest in data management, Technical Committee 2 of IFIP has been sponsoring a series of conferences over the past year. The most recent of these was held in Wepion, near Namur, Belgium, and dealt primarily with the Schema Language defined in the CODASYL Data Description Language Journal of Development. The CODASYL Schema Language has been widely discussed in the U.S. (witness the debate at the ACM SIGMOD Conference, held last May in Ann Arbor), and the interest in Europe on this same topic is at least as great. This conference was somewhat unique, however--the focus of the entire conference was on the single topic of the CODASYL DDL, hence the topic was discussed in more detail than probably it ever has been outside of DDLC (Data Description Language Committee) itself.	codasyl;data definition language;international federation for information processing	Robert W. Taylor	1975	FDT - Bulletin of ACM SIGMOD	10.1145/984328.984329	data mining;software engineering;computer science	DB	-60.779791009237876	-17.038693575905075	166724
9492c488d0422d82958365d2cd0b981c9e77fc57	girls, boys, and computers		"""I grew up believing I should have been born a boy. In the late nineteen-fifties, all the things I wanted to do seemed to be reserved for boys. Whether it was climbing trees, being an astronaut or mathematician, or simply not having to wear pretty clothes... boys were always the ones in luck. Fortunately my parents, perhaps because they had four daughters and no sons, were supportive of my boylike interests and encouraged me to believe I could succeed at just about anything.., including all that boy-stuff. As a mathematics student in university in the seventies, when I was told by male mathematicians that there were no really good women mathematicians, I would think to myself """"that's because women were never given a chance"""". I didn't believe in genderrelated differences in ability ... just in gender-related differences in opportunity. I certainly didn't believe that boys were innately better than girls at math. Over the years since then the differences between girls' and boys' achievement in mathematics have gradually disappeared and at this point almost all educators agree they are irrelevant. In 1998, women made up 44% of the math majors at Canadian universities, a huge increase since the 1970's. Unfortunately, the reverse has happened in computer science. Today North American girls, boys, teachers, and parents frequently regard computer science and programming as something boys are better at. By 1998, women made up only 21% of the c o r n puter science majors at Canadian universities, a decrease of about 50% since the 1970's. Much of this journal issue is rightly concerned with adult females and computing. Nevertheless, many of the factors that contribute to the low participation of women in computing occur first, and perhaps most forcefully, in childhood. In the early nineties, after fifteen years of doing research in mathematics and theoretical computer science, I accidentally fell into doing research on gender-related differences in children's interactions with computers. It came out of deciding, like a number of others before me, that computer and video games might be an ideal vehicle for helping children to like and learn mathematics. After several months of unsuccessfully trying to interest mathematics educators and game designers in creating mathematical games, I decided to try to start such a project myself. The E-GEMS (Electronic Games for Education in Math and Science, see <http://taz.cs.ubc.ca/ egems>) conducted its first official research study in the summer of 1993. We had started by doing a literature search to find out what was known about children and their interactions with computer and video games. Virtually no studies had been done since 1983. Given the huge changes during the intervening decade in computers, video game systems and games, we thought it would be wise to start out with a baseline study. We incorporated gender in the study design because many computer and video games seemed boy-oriented and we wanted to be sensitive to gender issues in the design of E-GEMS games. We expected the study to reveal gender differences in game preferences and it did. What astounded me and many of the other researchers were the dramatic gender differences in the children's confidence in gaining access to computers and in playing the games. I remember thinking """"This is the nineteen-nineties! How can girls be thinking that boys are better at computers in the nineteennineties?"""" Since then, there have been many studies, including several by E-GEMS and SWIFT (our Supporting Women in Information Technology project, see <http://taz.cs.ubc.ca/swift>), that have revealed significant gender differences in computer-related interactions and perceptions of school-age children in the areas of the world where computers are common in school and home environments. Gender-related differences have been found at every age from kindergarten through secondary school. Differences have been found in almost every area related to computers: how much time children use computers, what kinds of activities they like to do with computers, how they like to do a computer activity (e.g. alone or with a friend or two), how they perceive their level of skill in using computers, and how interested they are in learning to do various things with computers. For example,"""	baseline (configuration management);generic eclipse modeling system;google summer of code;interaction;randomness;relevance;swift (programming language);theoretical computer science;women in computing	Maria M. Klawe	2002	SIGCSE Bulletin	10.1145/543812.543818	multimedia;computer science	HCI	-61.81385512184798	-23.019693257453376	166887
990100d086c389dc78879296af84f2616501d805	guest editors introduction: special section on the acm symposium on virtual reality software and technology 2015		THIS special section of IEEE Transactions on Visualization and Computer Graphics (TVCG) presents extended versions of three selected papers from the 2015 ACM Symposium on Virtual Reality Software and Technology (VRST’15). Past VRST symposia were held in Hong Kong (2010), Toronto (2012), Singapore (2013), and Edinburgh (2014). In 2015, VRST was held for the first time in the mainland of China, Beijing from November 13 to 15, 2015. VRST is an international forum for the exchange of experience and knowledge among researchers and developers concerning virtual reality software and technology. A major goal is to provide an opportunity for VR researchers to interact, share new results, show live demonstrations of their work, and discuss emerging directions for the field. This year VRST received more than 100 submissions and 18 regular papers were accepted and published in the conference proceedings. In addition, 11 short papers and 16 posters were also selected and included as a part of the conference proceedings. These papers were evaluated on the basis of their significance, novelty, and technical rigor. All the papers went through two rounds of reviews. Each submission received at least three reviews from at least one Program Committee (PC) member and two external reviewers. The discussion among the PC members and external reviewers was held online for a period of one week. Among the accepted submissions, we carefully selected three best papers and invited the respective authors to submit the extended versions to this special section of IEEE TVCG. These papers underwent the full TVCG review process. Below, we provide a brief description of each paper included in this special section. The paper “Optimal Camera Placement for Motion Capture Systems” presents a good analysis of the problems involved in placing cameras in a CAVE-like environment and shows how camera placement can be optimized by taking into account object point distributions and occlusions, while considering placement constraints, triangulation convergence angles, and camera-object distances. The paper introduces two methods for camera placement: one is based on a metric that computes target point visibility in the presence of dynamic occlusion from cameras with “good” views, and the other is based on the distribution of views of target points. The paper also proposed efficient algorithms for estimating the optimal configuration of cameras for the two metrics and a given distribution of target points. The paper on “JackIn Head: Immersive Visual Telepresence System with Omnidirectional Wearable Camera” introduces a visual telepresence system with an omnidirectional wearable camera with image motion stabilization. Spherical omnidirectional video footage taken around the head of a local user is stabilized and then broadcast to others, allowing remote users to explore the scene independently of the local user’s head direction. Also, they conducted user study to analyze the study results. Their findings show that establishing shared understanding, or common ground, in a process known as “grounding”, is important. Also, typical real-world tasks during collaboration primarily consist of three phases; object/location identification, procedural instruction, and comprehension monitoring. They confirm that those three phases cannot be avoidable and are important even in immersive remote collaboration systems. The paper on “Dynamic Projection Mapping onto Deforming Non-rigid Surface using Deformable Dot Cluster Marker” provides the description of a high-speed tracking algorithm for localizing an IR dot pattern on deformable objects. The tracking algorithm is performed in a 4-step process, utilizing two pieces of information unique to each dot in the pattern: the tracking state of the dot, and the location of the dot in the camera coordinate frame. The tracking process begins with an initialization step where the initial location of each dot is recorded. Using a high speed camera the first step of the tracking process involves updating the location of each dot within each new frame. The second step uses extrapolated data from visible dots to estimate the location of occluded/non-visible dots. Step three uses a heuristic function to identify false positives of visible dots. These three steps are optimized using a parallelization scheme to process each dot independently. The final stage of the tracking algorithm resolves duplicate locations through hashing. The algorithm is applied to a projector-based AR system in which a perspectively correct image is projected onto deformable materials marked with an invisible IR dot pattern. For information on obtaining reprints of this article, please send e-mail to: reprints@ieee.org, and reference the Digital Object Identifier below. Digital Object Identifier no. 10.1109/TVCG.2016.2643818 IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 23, NO. 3, MARCH 2017 1207	360-degree video;algorithm;ar (unix);computer graphics;email;extrapolation;head-mounted display;heuristic (computer science);identifier;motion capture;object point;omnidirectional camera;parallel computing;traffic enforcement camera;usability testing;video projector;virtual reality;wearable computer	Lili Wang;Ming C. Lin;Enhua Wu	2017	IEEE Trans. Vis. Comput. Graph.	10.1109/TVCG.2016.2643818	computer science	Visualization	-59.762182067962506	-17.966372505359274	167003
2c47d39661db3e9ba60d8b72034ed2af3975e4ee	editor's note	and systems;algorithms;design;human factors;miscellaneous;languages;three-dimensional graphics and realism;general;theory;geometric algorithms;performance	The journal continues to be in an excellent state. For the fi rst time, the entire proceedings of IEEE VR 2012 long papers became a special issue of TVCG (April 2012 issue). At the start of October 2012, TVCG had received more than 220 regular submissions, slightly fewer than last year at the same time, but more than years prior to 2011. This year, we also observed an excellent number of 95 and 437 submissions, respectively, to the IEEE VR Conference issue and the VisWeek Conference issue, which contains the Proceedings of the IEEE Visualization and Information Visualization 2011 Conferences, as well as the 10 best papers from the IEEE Conference on Visual Analytics Science and Technology (VAST). We are expecting a total of more than 800 submissions to TVCG by the end of 2012. A total of 145 articles were published in the fi rst 10 regular issues with 1,912 pages, and the VR and VisWeek special issues containing 15 and 96 conference papers, respectively. All submissions in both special issues went through a rigorous two-round journalquality review process. Practically all the 2011 papers have also been decided. From the 326 regular submissions (including 28 extended versions of Best Papers from several top venues in graphics and visualization), 70 regular papers and all 28 special section papers were eventually accepted; 93 out of 366 conference submissions were published in the VisWeek special issue. The acceptance rate is about 23 percent for the regular papers and 25 percent for the papers submitted to the VisWeek conference issue. TVCG continues to offer authors a remarkably effi cient processing of submitted manuscripts: The average time from submission to fi rst decision is less than three months and the average time from submission to publication as a preprint in the CSDL is less than seven months. With its 2011 impact factor of 2.22, TVCG remains clearly as the top journal in visualization and computer graphics overall. During 2012, the authors of TVCG regular papers were invited to give an oral presentation of their recent work at TVCG¿s partner conferences. A total of 35 TVCG papers were presented at the IEEE Virtual Reality Conference, ACM SIGGRAPH Symposium on Interactive 3D Graphics and Games, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, Pacifi c Graphics, and IEEE VisWeek 2012. Started in 2011, this new arrangement provides a unique opportunity for the audience of these conferences to keep abreast of high-quality research featured in TVCG, while encouraging more TVCG authors to attend these conferences. Both the TVCG authors and the conference attendees have been extremely positive about this new initiative and we plan to continue this conference-journal parternship in 2013.		Ming C. Lin	2011	IEEE Trans. Vis. Comput. Graph.	10.1109/TVCG.2011.94	human–computer interaction;multimedia;computer graphics (images)	Visualization	-61.324151244031945	-18.12958302493866	167598
22a7f44fe4dac02cd0b592b4d2460959d4f195fc	bioinformatics and classical literary study		This paper describes the Quantitative Criticism Lab, a collaborative initiative between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate authorial style, intertextuality, and related phenomena of literary significance. As a case study in our approach, here we review the use of sequence alignment, a common technique in genomics and computational linguistics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal similarities, which makes it ideal for identifying phonetic echoes in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages. keywords intertextuality; sequence alignment; Latin; literature; computational biology; natural language processing 0 INTRODUCTION The Quantitative Criticism Lab (QCL; www.qcrit.org), directed by the co-authors, grew out of shared interests in classical intertextuality dating back to a seminar taught in 2009. The idea for a collaborative and interdisciplinary study of intertextuality was first discussed by us in January 2014, inspired locally by Dexter’s work in computational biology and globally by the rise of the Digital Humanities and “big data” approaches to cultural study. We have since published articles on a range of topics in both humanities and science journals, and QCL has received support from the National Endowment for the Humanities, the Andrew W. Mellon Foundation, the American Council of Learned Societies, and the Neukom Institute for Computational Science. QCL maintains a physical lab space at the University of Texas at Austin, but the project team includes scholars from a number of different institutions, in addition to undergraduate research assistants and high school students and teachers. The impetus for bridging the disciplines of biology and literary study is both cultural and methodological. The cultural motive derives from the familiar analogies often drawn between literature and living organisms, whether the use of trees or woods as a metaphor for literary works and traditions, or the imaginative casting of poems and books as living embodiments or descendants of their author. Such analogies have a long pedigree and have taken many different forms: one famous	big data;bioinformatics;book;bridging (networking);computation;computational biology;computational linguistics;computational science;computer scientist;dexter (malware);digital humanities;interpretation (logic);machine learning;natural language processing;sequence alignment;text corpus	Pramit Chaudhuri;Joseph P. Dexter	2016	CoRR		natural language processing;computer science;artificial intelligence;machine learning;linguistics	NLP	-58.43137161950343	-19.87826995704416	167938
01f08fa7a95c500d8bc9264d1e6b94c69ed7088e	on zadeh's problem in probability theory		ABSTRACTWe pay tribute to Lotfi Zadeh, a long-term member of the editorial board of this journal, by drawing attention to his challenges to classical probability theory. In particular, we consider the following problem: Probably John is tall. What is the probability that John is short? What is the probability that John is very short? What is the probability that John is not very tall? We discuss possible solutions and general significance of this and similar problems.		Radim Belohlávek	2018	Int. J. General Systems	10.1080/03081079.2018.1494734	mathematical economics;discrete mathematics;fuzzy logic;probability theory;mathematics	Theory	-59.92100619841282	-21.697094730846857	168178
84b8bb27e9cfe5f117136d86faac74153d1400e1	reprint from computing reviews		As a service to our readers, SIGACT News has reached an agreement to reprint review that originally appeared in Computing Reviews of books and articles of interest to the theoretical computer science community. Computing Reviews is a monthly journal that publishes critical reviews on a broad range of computing subject, including models of computation, formal languages, computational complexity theory, analysis of algorithms, and logics and semantics of programs. ACM members can receive a subscription of Computing Reviews for $30 per year by writing to ACM headquarters.	acm sigact;algorithm;analysis of algorithms;book;computational complexity theory;formal language;model of computation;theoretical computer science	Michael C. Loui	1989	SIGACT News	10.1145/74074.74080		Theory	-60.37714724102029	-18.537254729862706	168364
e68d3f6bc053542890cc5ac573005ad981757625	a proven domain-independent scientific function-finding algorithm	real scientific data set;previous investigator;function-finding algorithm;functional form;numerical data;function-finding research-as;scientific significance;apparent functional relationship;domain-independent scientific function-finding algorithm;previous candidate;functional relationship	Programs such as Bacon, Abacus, Coper, Kepler and others are designed to find functional relationships of scientific significance in numerical data without relying on the deep domain knowledge scientists normally bring to bear in analytic work. Whether these systems actually perform as intended is an open question, however. To date, they have been supported only by anecdotal evidence-reports that a desirable answer has been found in one or more handselected and often artificial cases. In this paper, I describe a function-finding algorithm which differs radically from previous candidates in three respects. First, it concentrates rather on reliable identification of a few functional forms than on heuristic search of an infinite space of potential relations. Second, it introduces the use of distinction, significance and lack of fitthree general concepts of value in evaluating apparent functional relationships. Finally, and crucially, the algorithm has been tested prospectively on an extensive collection of real scientific data sets. Though I claim much less than previous investigators about the power of my approach, these claims may be considered-to a degree quite unfamiliar in function-finding research-as conclusively proven. Evaluating Function-Finding Systems Over the past ten years, programs like Bacon [Langley et al., 19871, Abacus [Falkenhainer, 1985; Greene, 19881, Coper [Kokar, 19861, Kepler [Wu and Wang, 19891 and others have been designed to attack a problem I call domain-independent scientific functionfinding. Each program accepts numerical data and, without relying on knowledge of the domain in which it was collected, attempts to find the underlying functional relationship which might be proposed by a scientist examining the same data. Unfortunately, while a great deal of effort has been expended in designing function-finding systems, little has been done to test them. Researchers have nearly always relied on anecdotal evidence, reporting the successes of their programs on a few hand-selected cases, most of which have consisted of artificial data generated to conform exactly to a functional relationship. Also, although performance clearly depends on the environment in which a function-finding system is deployed, researchers have omitted specification of such an environment in their reporting. What we would really like to know about a functionfinding program is not its record of successes on artificial problems chosen by the programmer, but its likelihood of success on a new problem generated in a prespecified environment and involving real scientific data. To date, function-finding research has provided no information on which an estimate of this likelihood might be based. In view of this, my recent research has concentrated on the problem of evaluating function-finding systems [Schaffer, 1989a; Schaffer, 1989b], and, in the process, I have amassed quite a large collection of real scientific data for use in testing. While the five reports cited above mention a total of only six real data sets, I have collected 352. Moreover, as I will soon describe, part of this data was collected in a systematic fashion from a specified environment, making it possible to conduct prospective trials of function-finding algorithms. Contact with real data did more than provide an acid test for existing notions, however. It led me to a fundamentally novel conception of the problem of function finding. While previous researchers have concentrated mainly on constructing one of an infinite number of possible functional forms or, equivalently, searching an infinite space of formulas, I believe it is both more accurate and more productive to view function-finding as a classification problem-one of deciding reliably between a fixed, finite set of potential relationships. This viewpoint is developed in [Schaffer, 1990b] and more fully in [Schaffer, 199Oa]. In both places, I analyze the well-known Bacon algorithm and show that, while it is surprisingly successful in the face of prospective testing, virtually all of this success is accounted for, not by the search heuristics on which published reports have concentrated, but by a mechanism for evaluating potential relationships of which the authors have said that they “hold no particular brief.” Clearly, however, if evaluation and not search is the key to successful function-finding with real data, it ought to be possible to improve performance by de828 MACHINE LEARNING From: AAAI-90 Proceedings. Copyright ©1990, AAAI (www.aaai.org). All rights reserved. veloping more sophisticated evaluation criteria. The result of my attempt to do just this is a new algorithm which it is my main purpose in this paper to present. Before I do so, though, let me take a moment to describe the data which served as both inspiration and testbed for my ideas.	algorithm;degree (graph theory);heuristic (computer science);kepler;level of measurement;machine learning;numerical analysis;programmer;prospective search;testbed;whole earth 'lectronic link	Cullen Schaffer	1990			computer science;artificial intelligence;machine learning;algorithm;statistics	AI	-61.191105680827086	-20.653334748017592	168468
914cc035b4596372a60514567853c07e79193db8	book reviews		Like other OReilly Nutshell books, this is not the first stop onthe journey of learning the topic in question. Also like the otherNutshell books, it is the definitive reference for the topic thatit covers. With over 240 classes, Cocoa is a complete classlibrary, framework, and development environment for Applesrevolutionary Mac OS X. When faced with problems such as stoppingcoding, going into the documentation, or digging around to find theclass documentation I need, there is just something magical aboutbeing able to flip through a book to find solutions.	book;documentation;operating system;system 7	Sir James	2003		10.1145/966712.966729		NLP	-61.62088057473539	-22.005704806030344	168545
918691e4a1c105b4969210dc32db96838e1580e2	reflections on biological cybernetics: past, present, prospects		In the natural sciences, scientific revolutions do happen, but they hardly ever happen as a kind of discontinuity2 in time. Quantum mechanics in the physical world of 1926–28 is stillmore or less an exception, and an exciting one. Looking back, my time as coeditor-in-chief of Biological Cybernetics from 1999 onwards and as editor-in-chief between 2006 and 2017 was both exciting and smooth, but it nevertheless prompts a few critical thoughts, all the more since the last two decades have seen important new developments in both experimental and theoretical neuroscience and in the ways of presenting them. Following the natural rhythm of time, we will focus on the past, present, and prospects of the journal as such and on what an editor-in-chief can contribute.	amiga reflections;biocybernetics;computational neuroscience;cybernetics;natural science disciplines;neuroscience discipline;quantum mechanics;thinking, function	J. Leo van Hemmen	2018	Biological Cybernetics	10.1007/s00422-018-0756-z	machine learning;applied mathematics;artificial intelligence;cybernetics;mathematics	Theory	-56.36551521493699	-22.411989834025963	168892
a28ea185673f18b138bf087b00cf4b1516e32c30	message from the editors-in-chief		Dear Reader,rnrnIt is a great pleasure and privilege to welcome you to the first volume of the SIAM Journal on Financial Mathematics. This journal, with the help of its editorial board, contributors, and readers and the SIAM staff, follows in the outstanding tradition of high-quality SIAM journals and, we hope, will become a leading journal in mathematical modeling and analysis of financial problems.rnrnThe journal began accepting submissions in October 2008. You will find the first papers accepted for this all-electronic publication. These papers provide a representative glance at the diversity of topics covered by the journal, something clearly observable from the outstanding editorial board itself. A number of papers currently in the pipeline further stress the broad vision of the journal.rnrnThe SIAM Journal on Financial Mathematics came to exist thanks to the tremendous effort of numerous people. The impetus came from the SIAM Activity Group on Financial Mathematics and Engineering (SIAG/FME) at its inaugural meeting in Boston in July 2006. While the idea of a new journal focused on computational finance had been discussed for a number of years, the enthusiasm at this meeting persuaded us, and other officers of the SIAG, to work toward this end. We received enthusiastic support from the publications committee and SIAMu0027s board and council, and the journal was approved in July 2008. We are grateful to Tim Kelley and David Marshall for their hard work and advice in establishing the journal.rnrnPreparing for the creation of the journal was arduous but rewarding. We aimed high, and at times, we wondered whether our goals were realistic. After agreeing on a charter for the journal, we made a wish list of associate editors, and it was no surprise to be asked by members of the council, “What makes you think that these distinguished scholars will accept your invitation to work for a new journal? Except for two special cases, we were delighted that all of them accepted. In hindsight, the warm reception by the applied mathematics community at large has made the experience rewarding.rnrnThe SIAM staff is a wonderful asset. In particular, Mitch Chernoff, Brian Fauth, and Heather Blythe have been a critical source of support.rnrnThe current editorial board is working very hard to guarantee timely and high-quality reviews. The whole cycle of reviewing and production has been fast on average. Please enjoy your reading, and we look forward to receiving your high-quality submissions in the future.		René Carmona;Ronnie Sircar	1996	Design Autom. for Emb. Sys.	10.1007/BF00134681	message;message switching;message broker	EDA	-61.87845236267471	-18.74337674314471	169067
ffe61db17e73177a41b2c86c9d091909c55fafe0	ten simple rules to win a nobel prize	experimental design;employment;swedes;medicinal chemistry;careers;nobel prize;scientists;chemical biology;algorithms;medicine and health sciences;physical chemistry	It is remarkable how many students, young faculty, and even senior faculty hanker after a Nobel Prize. Somehow, they think that it is possible to structure their scientific careers so that the culmination will bring this much sought-after honor. Some even think that as a Nobel laureate myself, I may have the key to success—some secrets that I can share and so greatly improve their odds of success. Unfortunately, I must begin by disappointing everyone. There is only one path that should be followed. It is summed up in Rule 1, but some of the other Rules may prove helpful—or if not helpful, then at least amusing. Preface by Philip E. Bourne, National Institutes of Health, Founding Editor-in-Chief of PLOS Computational Biology When receiving a draft of the article “Ten Simple Rules for Writing a PLOS Ten Simple Rules Article” [1], not only had we come full circle in terms of professional development, but also I knew the series was a success. Since that article was published in October 2014, two more articles have been published, and this will be the third: a total of 44 in all. Rule 2 in what I shall affectionately call the 102 article [1] suggested you need a novel topic and suggested winning a Nobel Prize was such a topic. As I hinted in my editorial comments to the 102 article, I would take up the challenge in soliciting such an article. Rich Roberts was the first person to come to mind, partly because he is a good sport, partly because we share an interest in open (to be interpreted here as candid) science, and of course because he won the Nobel Prize in Physiology or Medicine (with Phillip Sharp) in 1993 for work on gene structure. At first he was reluctant and slightly insulted, making me think I should write “Ten Simple Rules for How Not to Insult a Nobel Laureate.” The rationale is that we should not be encouraging scientists to think about science through awards but through having fun and the desire to do their best science. That should be enough. The result is exactly that—having a bit of fun and making some important points all at once. I hope you enjoy it as much as I did. 1. http://www.ploscollections.org/article/info:doi/10.1371/journal.pcbi.1003858	awards;bourne shell;computation;computational biology;curie;design rationale;editorial;first-person (video games);fred (chatterbot);mind;mixed connective tissue disease;rule (guideline);scientific publication;won	Richard J. Roberts	2015		10.1371/journal.pcbi.1004084	biology;chemical biology;physiology;design of experiments;operations research	ML	-59.797231738898354	-20.73117950757916	169237
30195c13ff5bfc9fe257a5f6942cef1f4031431d	guest editorial notes for selected papers from soca 2010	selected;editorial;bepress selected works;papers;editorial guest soca papers selected notes 2010;2010;guest;soca;notes	This special issue presents the extended and revised papers selected from the IEEE 2010 International Conference on Service-Oriented Computing and Applications (SOCA 2010), which was held in Perth, Australia. The SOCA conference is one of the major annual events sponsored by the IEEE Computer Society Technical Committee on E-Commerce (TCEC). SOCA 2010 received 111 high-quality paper submissions from 31 countries/regions. After a rigorous review process, the programme committee finally accepted 35 full papers for inclusion in the conference proceedings. After the conference, the Springer Journal of <Service Oriented Computing and Applications> invited the highly ranked papers to submit their extensions to this special issue. After two more rounds of rigorous reviews by prestigious researchers and thorough revisions by authors, only five papers have been accepted for publication. The authors of the accepted papers originated from 8 countries, and typically, 4 papers have been representative research outcome from international collaboration between academic institutions and/or industry partners. Extensive research and development in the past few years has pushed SOC technology into numerous application areas including business processes, high-performance computing, Web-based services, embedded systems, and many others. With more and more services being moved into the “cloud”, and the increasing prevalence of ubiquitous end devices that interface with the physical world, it is timely to	business process;e-commerce;embedded system;service-oriented architecture;service-oriented distributed applications;springer (tank);supercomputer	Jun Shen;Soo Dong Kim;Nalini Venkatasubramanian	2012	Service Oriented Computing and Applications	10.1007/s11761-012-0106-2	computer science;distributed computing;data science	Visualization	-61.42642668446184	-17.75716949823081	169241
ed6ceb4bc4b72e87ee18c26e3611a60a0f65c095	takashi gomi: a bridge builder in robotics		Takashi Gomi's death in September 2013 has left an enormous gap in the lives of his family and friends. Here I pay tribute to his multiple roles in bridging many gaps - so uniquely and charismatically - in the field of robotics over several decades. He was a principal player in the development of autonomous and evolutionary robotics over this period. He called his approach Non-Cartesian robotics, others used different terms such as Behavior Based Robotics or Nouvelle AI for broadly similar ideas; what united them was a critical rejection of many of the classical robotic assumptions that have been loosely called GOFAI - Good Old-Fashioned AI.	robotics	Inman Harvey	2014	Adaptive Behaviour	10.1177/1059712314541299	psychology;simulation;computer science;artificial intelligence	Robotics	-57.9368523729867	-23.114313404270607	169281
c146fcd55dbb097fd20a61b1aef3bef8dffa5aaa	producers-users-customers: towards a differentiated evaluation of research in machine translation	machine translation	The following remarks are a brief reaction to Somers’ “current research in machine translation”. Because Somers’ article is available to readers of the present volume, I shall not summarize the ideas put forward there. Instead, I shall suggest some views on the evaluation of MT research essentially claiming that Somers’ remarks are relevant and interesting, but would gain substantially by being embedded inside a somewhat wider and more differentiated perspective on the field in question. The beginnings of such a wider perspective might be seen in the two questions (1.1. and 1.2.) which, I suggest, should be put to three communities of people involved in MT (2.1.-2.3.). 1.1. How do you evaluate the history of MT to date? 1.2. Which (changes in) directions do you suggest? 2.1. Researchers 2.2. Translators 2.3. Companies, institutions, and individuals buying MT systems My argument would be that it is only through such a differentiated view that we shall be able to arrive at a realistic assessment and at strategies for MT-research which are not unduely self-interested, nor ill-informed by the exclusion of relevant commmunities.	embedded system;machine translation	Erich H. Steiner	1992	Machine Translation	10.1007/BF00398473	computer-assisted translation;natural language processing;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation	HCI	-58.716798095882375	-21.993991843636017	169449
244aeb80c023c75b0dc417200b4fd9a060a0d28e	education for computational science and engineering	computer program;computational science and engineering;mathematical programming;computers and society;manufacturing sector	Computational science and engineering (CSE) has been misunderstood to advance with the construction of enormous computers. To the contrary, the historical record demonstrates that innovations in CSE come from improvements to the mathematics embodied by computer programs. Whether scientists and engineers become inventors who make these breakthroughs depends on circumstances and the interdisciplinary extent of their educations. The USA currently has the largest CSE professorate, but the data suggest this prominence is ephemeral. Computational science and engineering (CSE) embodies the challenges in President Obama’s vision of the future, “ours to win” [1]. Wealthy nations with comparable scientific institutions can achieve parity in technical prowess. When countries have have similar populations of technical professionals, one way to excel is to make better use of expert talent through computing. Research and development (R&D) proceeds more quickly and with greater certainty by using computers to analyze information and to pursue the implications of scientific theories. The preceding “elevator speech” has been heard in Washington before.1 From “NSF Plans Help with Big Computer Problems” [2] in 1985, through “Energy Labs Urged to Boost Supercomputing Capability” [3] in 1997, to “An Endless Frontier Postponed” [4] in 2005, CSE has been used to justify the purchase of massive computers — and is being used again, in President Obama’s budget. To the contrary, the fastest computers are irrelevant to the innovations in CSE that bettered peoples’ lives and transformed how R&D is done. The breakthroughs in CSE are the mathematical programs of computing, not the machines themselves. President Obama misunderstood the “Sputnik moment” when the People’s Republic of China (PRC) built “the world’s fastest computer.” ∗6059 Castlebrook Drive; Castro Valley, CA 94552-1645 USA †jfgrcar@comcast.net, jfgrcar@gmail.com. For a brief history of CSE, search the text of articles in Nature or Science for the exact phrase “computational science.” For a more detailed timeline, search for “supercomputer.” 1 ar X iv :1 10 2. 46 51 v3 [ m at h. H O ] 9 A ug 2 01 1 Japan and the USA previously built such machines to little effect because availability of the fastest computers is severely limited by cost [5]. The cost of the absolutely fastest machines remains high even though their composition has changed. The extreme engineering that once made computers fast was overtaken by silicon miniaturization for the mass market. Today’s fastest machines are merely huge “clusters” of commodity devices working in parallel [6, 7], which many research groups can acquire in small configurations [8]. These cluster computers were predicted to be difficult to use [9, p. 458], and have proven so. At present there are two paths to achieving great speed. Either computer programs might be written to excel on clusters, or special chips might be built for one type of calculation. (Examples of specialty chips are in cell phones and game consoles.) Programming skill is also critical for the special-purpose chips because likely programs must be well characterized before building silicon to suit. The scientific literature has examples of both approaches for calculations of molecular dynamics [10, 11, 12]. The flexing motions of large molecules affect their reactivity [13], so these calculations reveal how molecules are biologically active. The importance of clever programming begins only after there is something to write a program about. The prerequisite mathematical invention often is obscured by the continuing work to keep up with computing practice. For example, at the turn of the 19th century, A. M. Legendre and C. F. Gauss [14, 15] invented what is now called regression analysis, which is used to infer parametric models in diverse fields such as econometrics and epidemiology. Gauss in 1810 [16] also invented a way to calculate regression coefficients by hand that was used for 150 years, with modifications for various types of manual computing [17, 18]. G. H. Golub in 1965 [19] invented a better way for electronic computers. Today, Golub’s method is found in libraries of computer programs, where it evolves with programming techniques [20, 21]. The example of regression analysis illustrates that the paramount breakthroughs in CSE are the mathematical inventions that live through generations of computer programs. These advances come from researchers who learn to work with mathematics across disciplinary boundaries [22]. Prominent examples are: (a) a fast method to evaluate convolutions was invented by two polymaths working in statistics and computer science [23]; (b) a method for inferring structures of proteins from x-ray diffraction patterns was invented jointly by a physicist and a mathematician [24, 25, 26]; and (c) a method for identifying the current state of a dynamical system was invented by a control engineer and refined by a programmer for the tiny computers on early space vehicles [27]. Some of these inventions enable scientific	cse html validator;coefficient;computation;computational science;computer cluster;computer program;computer science;control engineering;convolution;dynamical system;emoticon;fastest;ibm notes;library (computing);mobile phone;molecular dynamics;population;programmer;relevance;scientific literature;sputnik;supercomputer;top500;theory;timeline;uncanny valley	Joseph F. Grcar	2011	CoRR		computational science;computational science and engineering;mathematics;algebra	ML	-58.43672723661086	-21.91716567939913	170055
6c08d5842d8d01e8a7757d18db297b0ca120ed8a	discovering the real world hands-on in the classroom		"""Charles Darwin said, """"I was born a naturalist,"""" expressing his extreme fascination with nature and with the world surrounding him from his early childhood onwards. This continued fascination and excitement was an important success factor for him becoming one of the greatest scientists and discoverers of all time. Many people would agree that most-if not all-young children as well are fascinated by the world that surrounds them. Yet, when they grow up, during their teenage years other interests might replace this excitement quite rapidly. So, how could we make sure that they do not lose this precious fascination? And, at which age should we focus our efforts in order to keep the sparks of wondering burning? And, how can we give them a more or less realistic idea about what it means to be a scientist, a programmer, or an engineer working on solutions for the challenges in our society? In this article, I provide a summary of a particular Science, Technology, Engineering, Mathematics (STEM) approach we have developed that has been rolling out in schools in Belgium."""	darwin;fascination;hands-on computing;list of minor characters in the matrix series;programmer	Lieven Philips	2018	IEEE Instrumentation & Measurement Magazine	10.1109/MIM.2018.8360914	engineering;control engineering;naturalism;pedagogy	DB	-61.41829212970887	-23.12772313905317	170295
78947dba7445f06ccf723fccbc695adcbc405a49	talip perspectives, guest editorial commentary: what counts (and what ought to count)?	text analysis;light stemming;categorized corpora;arabic wikipedia;arabic natural language processing;text categorization	We agree on Publish or Perish, but do conference papers count as publications? How about articles in new journals (that aren’t yet “indexed”)? The Association for Computational Linguistics (ACL) recently started a second journal.1 One (of many) reasons for starting a second journal was a concern that some of us were hearing from our colleagues in Asia (and elsewhere) that, while conferences may count for publication in top universities in the United States, most universities have different rules. This is a serious problem for societies like the ACL: Basically, the vast majority of publications don’t count. According to the ACL Anthology Network (AAN) [Radev et al. 2009, 2013]2, the ACL has published 14k conference papers and just 750 journal articles. We need to make room for more journal articles or else researchers in our field won’t be promoted as much as researchers in other fields. Citations are also skewed strongly toward conferences, though not as much as publications. The AAN reports 82k citations in ACL conference papers and 13k citations in ACL journals. The fact that conferences account for more publications than citations	computation;computational linguistics	Kenneth Church	2014	ACM Trans. Asian Lang. Inf. Process.	10.1145/2559789	natural language processing;text mining;speech recognition;computer science;linguistics	Graphics	-58.86341671123032	-19.743838128184787	170712
329f003a55552e03cd4a93569a65d80f7a8d7dbd	message from academia europaea		After EATCS was officially founded in 1972 the Bulletin issue one was edited by Maurice Nivat in 73, issue two by Giorgio Ausiello in 76. I was elected EATCS Secretary and Bulletin editor at the 4 th ICALP (Turku, 1977). The regular appearance (three issues a year) was started by me with issue three: I am happy that some of the logo-design and structure that we introduced (particular the ideas to include technical papers) is still in use. With issue four the Bulletin crossed the 100 page limit and included photos for the first time. It continued to grow, so that mailing became a serious financial issue. It was only possible due to support of the University of Karlsruhe and later Graz University of Technology where I was also able to host the 6 th ICALP (1979). I continued as Bulletin editor till 1981 (no.14). Then Grzegorz Rozenberg took over. It is certainly he who shaped the Bulletin, expanding its scope, the number of technical contributions and reports on activities, adding DARDARA cartoons and turning each Bulletin in a veritable book, often exceeding 500 pages. Rozenberg edited an incredible 66 issues (15-80, from 1981 to 2003!). With issue 111 that appeared October 2013 the Bulletin is still going strong. The current issue is the second one edited by the new Bulletin editor Kazuo Iwama from Kyoto who invited me to write this message.	european association for theoretical computer science;icalp;logo;maurice herlihy;report;the 100	Hermann A. Maurer	2014	Bulletin of the EATCS		simulation	Visualization	-61.95651107309119	-19.20206178300422	170827
cb07fa8b15079d9e3e3cb3245a650d315ae0c52f	ai events		This new column provides information about recent and upcoming events that are relevant to the readers of AI Matters , including those supported by SIGAI. In an effort to provide concise summaries, we have abridged the descriptions provided by the organizers at the respective event web sites. We would love to hear from you if you are are organizing an event and would be interested in cooperating with SIGAI, or if you have announcements relevant to SIGAI. You can find out more about conference sponsorship and support at sigai.acm.org/activities/requesting_sponsorship.html.	organizing (structure)	Michael Rovatsos	2017	AI Matters	10.1145/3175502.3175504	world wide web;multimedia;computer science	AI	-62.369648154915495	-17.59513072172271	170833
6f26d78158dedc002f4be05c1f24ce093b9a48b9	kidrec: children & recommender systems: workshop co-located with acm conference on recommender systems (recsys 2017)		The 1st Workshop on Children and Recommender Systems (KidRec) is taking place in Como, Italy August 27th, 2017 in conjunction with the ACM RecSys 2017 conference. The goals of the workshop are threefold: (1) discuss and identify issues related to recommender systems used by children including specific challenges and limitations, (2) discuss possible solutions to the identified challenges and plan for future research, and (3) build a community to directly work on these important issues.	recommender system	Jerry Alan Fails;Maria Soledad Pera;Franca Garzotto;Mirko Gelsomini	2017		10.1145/3109859.3109956	recommender system;computer science;human–computer interaction	HCI	-60.68536499107949	-17.95993333385941	170933
1ecc5cfed21fbe3d106f1a9bda3be5fefccd4bfd	perpetual motion, evolutionary computation in industry and other chimeras		Can you apply Computational Intelligence in industry? Is there Evolutionary Computation life outside Academia? Will Benson care? I'll try to find answers to these and other questions with a few reflections from my own history.	evolutionary computation	Anna Esparcia-Alcázar	2015			engineering;artificial intelligence;operations management;operations research	Robotics	-58.80697334933648	-23.526171568428833	170965
8a191eb92992742647af16d40c2c4e0095959480	lessons from a restricted turing test	prix loebner;tecnologia electronica telecomunicaciones;computacion informatica;critical study;turing test;limitation;grupo de excelencia;metric;intelligence artificielle;etude critique;raisonnement;journal article;analyse;resolucion problema;estudio critico;interpretacion;limitacion;ciencias basicas y experimentales;razonamiento;interpretation;artificial intelligence;metrico;analysis;inteligencia artificial;test turing;reasoning;tecnologias;metrique;problem solving;resolution probleme;analisis	We report on the recent Loebner prize competition inspired by Turing's test of intelligent behavior. The presentation covers the structure of the competition and the outcome of its first instantiation in an actual event, and an analysis of the purpose, design, and appropriateness of such a competition. We argue that the competition has no clear purpose, that its design prevents any useful outcome, an d that such a competition is inappropriate given the current level of technology. We then speculate as to suitable alternatives to the Loebner prize. This report appeared in Communications of the Association for Computing Machinery , volume 37, number 6, pages 70-78, 1994. Also available as cmp-lg/9404002 and from the Center for Research in Computing Technology, Harvard University, as Technical Report TR-19-92 . The Turing Test and the Loebner Prize The English logician and mathematician Alan Turing, in an attempt to develop a working definition of intelligence free of the difficulties and philosophical pitfalls of defining exactly what constitutes the mental process of intelligent reasoning, devised a test, instead, of intelligent behavior. The idea, codified in his celebrated 1950 paper ``Computing Machinery and Intelligence'' [28], was specified as an ``imitation game'' in which a judge attempts to distinguish which of two agents is a human and which a computer imitating human responses by engaging each in a wide-ranging conversation of any topic and tenor. Turing's reasoning was that, presuming that intelligence was only practically determinable behaviorally, then any agent that was indistinguishable in behavior from an intelligent agent was, for all intents and purposes, intelligent. It is presumably uncontroversial that humans are intelligent as evidenced by their conversational behavior. Thus, any agent that can be mistaken by virtue of its conversational behavior with a human must be intelligent. As Turing himself noted, this syllogism argues that the criterion provides a sufficient, but not necessary, condition for intelligent behavior. The game has since become known as the ``Turing test'', a term that has eclipsed even his eponymous machine in Turing's terminological legacy. Turing predicted that by the year 2000, computers would be able to pass the Turing test at a reasonably sophisticated level, in particular, that the average interrogator would not be able to identify the computer correctly more than 70 per cent o f the time after a five minute conversation. On November 8, 1991, an eclectic group including academics, business people, press, and passers-by filled two floors of Boston's Computer Museum for a tournament billed as the first actual administration of the Turing test. The tournament was the first attempt on the recently constituted Loebner Prize established by New York theater equipment manufacturer Dr. Hugh Loebner and organized by Dr. Robert Epstein, President Emeritus of the Cambridge Center for Behavioral Studies, a research center specializing in behaviorist psychology. The Loebner Prize is administered by an 1 of 14 03/03/99 17:24 Lessons from a Restricted Turin g Test htt p://www.eecs.harvard.edu/shieber/ papers/loebner-rev-html/loebner-rev-html.html	communications of the acm;computer;computing machinery and intelligence;definition;eisenstein's criterion;html;intelligent agent;loebner prize;norm (social);rev;turing test;universal instantiation	Stuart M. Shieber	1994	Commun. ACM	10.1145/175208.175217	computer science;artificial intelligence;analysis;algorithm	ML	-57.822019827808134	-21.05184212165967	171679
23695311be313b47116b533e698e4075489e5943	keynote address: the hal 9000 computer and the vision of 2001: a space odyssey	space odyssey;computer hardware;precise feature film;computer technology;arthur c. clarke;keynote address;computer chess;computer science;epic film;computer speech;computer production number	"""""""I am a HAL 9000 computer production number 3. I became operational at the HAL Laboratories in Urbana, Illinois on January 12, 1997…""""--Arthur C. Clarke, 2001: A Space Odyssey (1968 novel)It's 2001: Where's HAL?2001: A Space Odyssey, Stanley Kubrick and Arthur C. Clarke's 1968 epic film about space exploration and the evolution of intelligence, was the most carefully researched and scientifically precise feature film ever made. Now, in its namesake year, we can compare the film's computer science """"visions"""" with current technological fact -- in particular those related to its central character, the HAL 9000 computer, which could speak, reason, see, play chess, plan and express emotions. In some domains reality has surpassed the vision in the film: computer chess, computer hardware, and graphics. In numerous others, reality has fallen far short: computer speech, language, vision, lipreading, planning, and common sense. The film missed some trends entirely: the film showed no laptops or PDAs and HAL as large as a school bus but in reality computers instead got small. As such, the film provides a remarkable perspective on the sweep of developments in the modern era of computer technology.This non-technical talk is profusely illustrated with clips from 2001 and current research and sheds new light on key moments of the film. You will never see the film the same way again."""	2001: a space odyssey;automated planning and scheduling;computer chess;computer hardware;computer science;computer vision;edmund m. clarke;graphics;hal;laptop;personal digital assistant;the stanley parable	David G. Stork	2001				Graphics	-56.51996148362854	-23.927866783115608	171759
79d31954e5b605175c9039acbd71fe31c373e961	technology: the inspiration exchange	community and society;research community;culture;collective intelligence;computing science	Patterns of Innovation Steven Johnson (Penguin, 2011; £9.99) Good ideas, says writer Steven Johnson, are rarely produced by lone geniuses. Innovation more often grows out of a network of minds, he argues. Universities offer the best chance for breakthroughs as they lack market pressures. Imagine sitting down at your computer each morning and choosing what to work on from a list of problems submitted by scientists from around the world. The requests cut across academic boundaries, yet have been selected with you in mind as the person best qualified to solve them. What would such a system achieve? What would your career be like as a consequence? This scenario is the vision of writer and quantum-computation expert Michael Nielsen. In 2007, he announced on his blog that he felt he could have more impact by developing new scientific tools than by pursuing his physics research. Reinventing Discovery, his thought-provoking call to arms, suggests he was right. Nielsen is convinced that, as a result of our growing ability to share information and ideas, we are living through a revolution T E C H N O L O G Y	blog;call to arms;coat of arms;computation;knowledge discovery metamodel;mind	Chris Lintott	2011	Nature	10.1038/478320a	collective intelligence;culture	AI	-61.09905435036117	-23.284650801879177	172918
9540976b1e0974444d61f55cd3df611f250420aa	turing award lecture: it's time to reconsider time	turing award lecture	"""hen I was a student, there was no such thing as comp u t e r s c i e n c e a n d m y i n t e r e s t s w e r e s t r i c t l y mathemat ics . I g r a d u a t e d f rom Car l ton College in 1958 with a BA in mathematics and went on to Pr inceton to do graduate work in mathematics . There , u n d e r the supervision of Haro ld W. Kuhn and the men to r ing of Rober t J. Aumann, I wrote a Ph.D. thesis on game t h e o r y en t i t l ed """"Three-Person C o o p e r a t i v e G a m e s witho u t Side Payments"""" [8] and graduated in the fall of 1961."""	acm turing award;haro (character)	Richard Edwin Stearns	1994	Commun. ACM	10.1145/188280.188379	computer science;artificial intelligence	Robotics	-58.14547377111893	-17.03883350227256	173258
8483d535790d212944e050006b530d0c9e00839d	an eclectic information processing system	information processing	This paper is a progress report on a computer system which is now being designed and constructed. As the title indicates, ideas that seem good have been taken from many different sources. Many features of contemporary large systems that were earlier incorporated into a plan for a large machine are now being applied to this smaller system.	computer;information processing;information processor	R. Cutts;J. Haynes;H. Huskey;J. Kaubisch;L. Laitinen;G. Tollkuhn;E. Yarwood	1972		10.1145/1479992.1480058	engineering;artificial intelligence;data science	HPC	-56.905986455569256	-20.38453145525662	173415
31624f51b52cd5367782aeb8beb8594614859a4a	yes, folks, standards are a many-splendored thing	application development	"""Would you believe that the most powerful gives his all on every festive occasion-a marvelous development productivity technique you can employ performer. is to use standards for application development? Why is the turkey so good each time and so consisStandards, or guidelines, are a written, usable formutent from year to year and place to place? Because lization of experience-successful experience. Their there are standard ways to cook turkeys. Proven use overcomes a common problem: Most project exmethods with lists of instructions for converting the perience is lost, or at best handed down by word of raw bird into a golden brown mouthwatering delight. mouth or individual behavior. Standards are descripCall them recipes if you like, but they really are stantions of what """"products"""" are needed, how they dards, and written ones at that. Every facet of the should be built, and what they should look like when fowl deed is explained in detail. But that doesn't completed. Standards can be descriptions of the best mean you can't add your own special seasoning. set of procedures to follow during the product's What it does mean is that if you follow the standard development. Written standards are a consistent, directions you will complete this culinary project in effective means of communication among the project grand style-and chances are you won't burn your team, users, and management. bird! Let me illustrate: Without standards, the target is often blurred and difficult to hit* Estimating. """"A 12-pound turkey feeds 16 people."""" I have 17 people coming for dinner. """"Would you tell me please, which way I ought to go from Let's see, that's 3/4 lb. per person; I need a 12.75 here? lb. bird. """"That depends a good deal on where you want to get to,"""" * Scheduling. """"Preheat time is 6 minutes + 15 said the cat. minutes on a side + 20 minutes per pound."""" """"I don't much care where-,"""" said Alice. """"Then it doesn't matter which way you go,"""" said the cat. Let's see, that's 12.75 x 20 = 225 minutes + 6 + 3L0 = 291 minutes. Four hours and 51 minutes LEWIS CARROLL ought to do it!"""	2.5d;schedule (project management);subroutine	Philip H. Braverman	1979	IEEE Computer	10.1109/MC.1979.1658820	computer science;rapid application development;management	HCI	-62.439685656368866	-23.775824845238713	173779
a42d300fe84df06b6cb0c87a2ca0f5ec54853409	very low bit rate video coding using 3-d models		Acknowledgments I would like to thank my supervisor, Prof. Dr.–Ing. Bernd Girod, who gave me the opportunity to join his group and to work in a stimulative and supportive atmosphere. I would also like to thank Prof. Dr. Günther Greiner for his interest in my work and for reviewing my thesis. During my doctoral studies, I had the pleasure to interact with a large number of excellent researchers and skilled engineers. I am very thankful to my colleagues and friends and Thomas Wiegand for the many stimulating discussions, joint work, and proofreading. Furthermore, I would like to thank the many others around the Telecommunications Laboratory in Erlangen who made my stay here so enjoyable. I am especially grateful to Ursula Arnold for her invaluable administrative support. Last but not least, I would like to thank my friends and my family for their valuable suggestions and support. vii Contents Notation xi Abbreviations and Acronyms xv 1 Introduction 1 1.	arnold;bernd michael rode;dr-dos;data compression;vii	Peter Eisert	2001				Vision	-58.817680201704526	-19.296963328844473	173972
9d66d8bb6bdb3b89d84d7feb163eb3d75151ebbb	"""what makes something """"ad hoc"""""""		"""We are all familiar with the phrase """"beauty is in the eye of the beholder."""" In this case we have an instance of """"the disease is in the eye of the beholder"""" which of course explains whY the cure is so elusive. The beholder rarely wants to do anything about it. To discuss this more subjectively, let's take a neutral case. Before doing so, we shall have to point out what a case can be expected to look like. A case of """"ad hocness"""" usually fits the form (or should I say the """"ad hoe"""" form)"""	eye of the beholder;fits;james hoe	Roger C. Schank	1978		10.3115/980262.980265	artificial intelligence;natural language processing;computer science	NLP	-59.32215844972367	-23.068447679105194	174185
850734115d0c47e43c02fecea103090b6775b4c0	"""review of """"systems modeling & requirements specification using ecsam: an analysis method for embedded and computer-based systems by jonah z. lavi and joseph kudish"""", dorset house publishing co. inc., 2005, isbn 0-932633-45-5"""	software phases;system modeling;program monitoring;requirement specification;software visualization	able, interesting and never dull, with loads of examples. Their text is pleasant to read. Any book has two “parents” – the author and the publisher. The author did, as stated, a first-rate job. The publisher seems to have also performed professionally, producing a well-designed text that is nice to read (nice on old and tired eyes; another attribute of endgame) and quality was an obvious concern. The book is also well illustrated.	embedded system;endgame: the calling;international standard book number;requirement;systems modeling	Garry S. Marliss	2005	ACM SIGSOFT Software Engineering Notes	10.1145/1082983.1083030	software visualization;systems modeling;computer science;software design;software engineering;programming language;operations research	SE	-62.04043639056233	-23.908766808791505	174501
798cac5e8ef8d2883e1752935eb2aa4da6651413	homage to jasis referees		The JASIS reviewers during 1978 and 1979 were : I particularly wish to acknowledge my indebtedness to this group of volunteers as a whole and to. that subset who worked so hard to help me with the transitional period when I assumed the editorship in 1977. We keep a record on all reviewers but, should I have missed anyone who has participated, I will gladly publish additional names if they are called to my attention.	journal of the association for information science and technology	Charles T. Meadow	1979	JASIS	10.1002/asi.4630300603	information retrieval;computer science	HCI	-62.491684096238416	-18.28347112793532	174520
c2e78f1c177657f71214963028bff4a6119d12af	surveyor's forum: is software science hard?	software science hard	"""data in the authors' Figure 1 would clearly have a high mean-squared error in a regression analysis; yet the authors claim that the correlation coefficient for these data shows that the results of software science are """"amazingly accurate."""" For these reasons I find """"software sci-ence"""" to be an unconvincing theory. I have serious reservations about the statement that """"the theory gives objective measures of complexity."""" I was disturbed by the seemingly uncrit-ical nature of the evaluation given """"soft-ware science"""" by Fitzsimmons and Love. When combined with the definition of """"ef-fort,"""" the definition of """"language level"""" (~) leads to an inconsistency. Let us consider the case of programs that compute functions of one argument. These can all be written as """"x-f(y)"""" in Halstead's """"potential language,"""" so that V* = 8. It follows that E = 512/(X) 2. Thus, for a fixed language, the effort of programming should be the same for all functions. (I do not think anyone believes that.) This trouble probably comes from the assumption that """"language level,"""" k, is constant over all programs for a fixed language, whereas, in fact, k may be arbitrarily small. (For programs that compute functions of one argument, this can be seen easily, since = LV* ffi 64/V, and V m a y be arbitrarily large.) Reply to Paul B. Moranda by A. Fitzsimons and T. Love Whether a complexity measure should be """"intensive"""" or """"extensive"""" is an interesting philosophical question. There are many good and valid reasons for the intuitive appeal of intensive measures. However, it was not the purpose of our paper to evaluate the intuitive appeal of one complexity measure or another. Rather, we were simply saying: """"Here's a theory of software complexity that's been applied to several large real-world databases and subjected to carefully controlled experiments, and it still holds up. We think the theory is worthy of more careful scrutiny and analysis."""" We leave to some other ambitious author the task of answering """"why"""" one theory works and another does not. It is true that there are many ways to compute the effort measure E. We used Halstead's method of counting operands and operators, as described precisely in Bu-lut, Halstead, and Bayer, """"Experimental Validation of a Structural Property of For-tran Algorithms,"""" Purdue University, CSD T R 115, April 1974. Other people have attempted various ways of computing E in order to refine the theory-just as other people have …"""	algorithm;cambridge structural database;coefficient;experiment;logical volume management;mean squared error;operand;programming complexity;theory;warez;ical	Theodore P. Baker	1978	ACM Comput. Surv.	10.1145/356744.356752	software;data mining;software engineering;computer science;surveyor	Logic	-60.766210879858825	-21.496761366228338	174710
62c4e5adaff8f5ae15540776c10c73f289873c1d	the silicon valley sentinel-observer		"""by FRANClNE FRANKLIN HEALTH CORRESPONDENT The Federal Government released today a study that shows an alarming increase in """"Specific Subject Aphasias"""", a new brain ailment that is affecting Americans age forty and over. Specific Subject Aphasias or SSA's as they are now commonly called, cause the patient to lose the ability to communicate and process information in specific subject areas. For example, one SSA victim might lose the ability to compose music after a lifetime of work in musical composition while another SSA victim might forget how to spell or to do arithmetic. The symptoms are varied, but scientists are quite sure that we are seeing the development of a new, devastating disease whose cause has yet to be determined. However, there is a growing suspicion that SSA's are somehow related to the increasing human dependence upon computer technology. The federal study showed that there were 119,450 new cases of SSA in the United States during 2027, as compared to about 99,087 new cases in the previous year, which, according to my wrist calculator, represents an increase of about 20%. SSA's are fast becoming the number one health concern among older Americans, now that Alzheimer's disease has more or less been cured by the use of drug therapies and changes in life style. Alzhe imer ' s reached its peak during the 1990s and 2000s. Late in the 1990s scientists proved that Alzheimer's had a variety of causes, some environmental and some genetic. Changes in lifestyle alone, including engagement in a lifelong program of creative activities and reduced television viewing, helped to e l iminate nearly half of all Alzheimer 's cases. Powerful drugs that stimulate the growth of neurons and atrophied dendritic connections helped to eliminate almost all of the remaining Alzheimer's cases by 2020. Ironically, it was during the year 2020 that the first cases of SSA were diagnosed. Actually, SSA's were probably around since as early as 2000, but not in sufficient numbers to raise concern. However, by 2020 the medical establishment realized that an ailment that could be potentially devastating to human cognitive and intellectual abilities was raging through the population. Unlike Alzheimer's, which affected mostly senior citizens, SSA's affect people starting around age forty, although SSA's are also of concern to the senior population. SSA's cause a patient to """"black out"""" in certain subject areas. For example, one SSA patient that we interviewed had been a composer of show tunes. However, as computers became to be used more frequently in composing show tunes and other forms of music, this patient found that his own music composition skills were diminishing. Now, this patient cannot remember or communicate even the most rudimentary rules of musical composition. Indeed, he has lost the ability to read and write musical notation. Yet, his disability is specific to musical knowledge. Outside of this specific domain his memory and his communications skills are perfectly normal. Thus, this ailment is called """"specific subject aphasia"""", aphasia being the medical term that applies when a patient can no longer speak. Some researchers have noted that these aphasias are often in areas that were once a passionate interest in a patient's life. For example, the composer that we alluded to had once been very passionate about composing music. He told us that it was a great blow to his ego when a computer system beat him in a competition to provide music for the Broadway show, O.J., that eventually became a major hit. There is growing evidence that SSA's are a direct consequence of human dependence upon computer technology. This theory was first posed by Dr. Arlene Totter at the 2024 International SSA Conference. Her theory is that SSA's are the result of a global deterioration in the microscopic physiology of the human brain that is a direct consequence of human dependence upon computer technology. Earlier this year, at the 2028 International SSA Conference that was held in Buenos Aires, researchers from Oxford University in England have come up with the first concrete evidence that the human brain is changing on a planetary scale due to the use of computer technology, as predicted by Professor Trotter's theory. This work is the research of Drs. Stephen Falcon and Isabella Wilson. Drs. Falcon and Wilson have detected a change in human brain structure in the period 2017-2027 that they attribute to the growing dependence upon computer technology. Many scientists expect Drs. Falcon and Wilson to earn the Nobel Prize in Medicine for this discovery. Their research takes us to the cutting edge of brain research. Drs. Falcon and Wilson discovered specific patterns of brain deterioration that they believe is proof of a theory that was presented in the 1980s by British biologist Rupert Sheldrake . Sheldrake postulated that learning is a species-wide phenomenon. According to Sheldrake, when one thousand people learn some new technology, the entire human race is actually learning that new technology on some level. Once a critical mass of pioneers learn that new technology, that learning transfers more easily to the rest of humanity, because all human beings arise from a common biological field that is bePlease see APHASIAS, p. 47"""	broadway (microprocessor);computer;falcon;planetary scanner;www	Richard G. Epstein	1997	SIGCAS Computers and Society	10.1145/270858.581266		HCI	-57.25736011025029	-23.727168405953044	174725
3ecd5ee57f15a5babe886aca3aa4be1b17e99575	d. ray fulkerson's contributions to operations research	operations research	Ray Fulkerson, one of the founders of mathematical programming, was a symbol of high scholarly standards to many of us. We shall miss him. Compared with the sorrow that we feel, old cliches seem cold and impersonal. A permanent tribute to his memory lies in his fundamental contributions to mathematics. We are going to review some of those which are particularly relevant to operations research and to outline their impact on the development of this field.l Our survey will be brief; for a more detailed account of Ray Fulkersonu0027s work, the reader is directed to a forthcoming article by Alan J. Hoffman in Mathematical Programming Studies. Furthermore, in the present issue of this journal, Louis J. Billera and William F. Lucas describe Ray Fulkersonu0027s life and career.	fulkerson prize;operations research	Vasek Chvátal	1976	Math. Oper. Res.	10.1287/moor.1.4.311	mathematics	Theory	-60.37247670515247	-21.100131282645517	174965
0eb460525d1580fb9f184b43b974499bce2a2ea7	realizability: a historical essay	computacion informatica;complexity theory;proof theory;wiskunde en informatica;set theory;ciencias basicas y experimentales;matematicas;classical logic;grupo a;linear logic	The purpose of this short paper is to sketch the development of a few basic topics in the history of Realizability The number of topics is quite limited and re ects very much my own personal taste biases and prejudices Realizability has over the past years developed into a subject of such dimensions that a comprehensive overview would require a fat book Maybe someone some day ought to write such a book But it will not be easy Quite apart from the huge amount of literature to cover there is the task of creating unity where there is none For Realizability has many faces each of them turned towards di erent areas of Logic Mathematics and Computer Science and this proliferation shows no signs of diminishing in our days Like a venomous carci noma Realizability stretches out its tentacles to ever more remote elds Linear Logic Complexity Theory and Rewrite Theory have already been infected The theory of Subrecursive Hierarchies too Everything connected to the calculus is heavily engaged Proof Theory is su ering Intuitionism is dead Just to name a few Did you think that at least the realm of classical logic would be safe Recently Krivine came up with a Realizability interpretation for ZF set theory Confronted with this mess I have acted like the classical impostor who walked into the hospital claiming to be a surgeon and is now wielding the knives in the operating theatre I took the nearest scalpel at hand and cut out everything that wouldn t t into either one of my two major streams meta mathematics of intuitionistic arithmetical theories and topos theoretic devel opments	computational complexity theory;computer science;linear logic;zermelo–fraenkel set theory	Jaap van Oosten	2002	Mathematical Structures in Computer Science	10.1017/S0960129502003626	linear logic;classical logic;computer science;pure mathematics;proof theory;mathematics;programming language;algorithm;set theory	Theory	-59.80055169747123	-23.342996387299436	175173
421e8585825cc678f81f6abc7832830504268cae	from start-ups to scale-ups: opportunities and open problems for static and dynamic program analysis		This paper describes some of the challenges and opportunities when deploying static and dynamic analysis at scale, drawing on the authors' experience with the Infer and Sapienz Technologies at Facebook, each of which started life as a research-led start-up that was subsequently deployed at scale, impacting billions of people worldwide. The paper identifies open problems that have yet to receive significant attention from the scientific community, yet which have potential for profound real world impact, formulating these as research questions that, we believe, are ripe for exploration and that would make excellent topics for research projects. Note: This paper accompanies the authors' joint keynote at the 18th IEEE International Working Conference on Source Code Analysis and Manipulation, September 23rd-24th, 2018 - Madrid, Spain.	dynamic program analysis;graph coloring	Mark Harman;Peter W. O'Hearn	2018	2018 IEEE 18th International Working Conference on Source Code Analysis and Manipulation (SCAM)	10.1109/SCAM.2018.00009	simulation;theoretical computer science;software;source code;static analysis;computer science;dynamic program analysis	Visualization	-60.904873812538746	-19.127497253529263	175647
6b6cf72091986c3cc2582d77cf28de60260b7253	a note on microprogramming		"""At this early stage in the work on what we term """"Microprogramming"""" it seems desirable to set forth both a projected definition of this term and a summary of our early views on the subject. Our hope is that these observations will serve to arouse interest in the problems of microprogramming and the advantages that may accrue from such a technique. Such interest might serve to stimulate further research on this subject at various computer installations. I t is felt that the difficulties involved in constructing a microprogram facility are of sufficient interest and complexity to merit wide attention. This paper initially attempts to provide a rough outline of some conditions under which microprogramming might be a useful technique. We then proceed to a discussion of the problems involved. These problems are divided into three categories: Engineering, Programming, and Evaluation. Following this, we offer a proposed definition of the verb and noun forms of the word """"microprogram."""" These definitions are presented in an effort to provide common terminology which will serve as a point of departure in future discussions. The Appendix contains a logical design for a proposed general purpose digital computer possessing modified microprogram capabilities. This design is developed in considerable detail. We point out that a practicable microprogram facility is entirely feasible within the limits of present computer techniques. I t is suggested that a thorough evaluation of the capabilities of mieroprogramming would be possible on the basis of experience gained from the construction and utilization of the proposed modified system."""	computer;microcode	Herbert T. Glantz	1956	J. ACM	10.1145/320825.320828	discrete mathematics;applied mathematics;microcode;computer science	Graphics	-56.84135409520485	-21.444895663224646	175651
4f585b3233867522738e687c7bdfcd1c6d4fc6d4	proceedings of the 22nd international symposium on high-performance parallel and distributed computing (hpdc'13) : new york, ny, usa, june 17-21, 2013		Welcome to the 22nd ACM Symposium on High-Performance Parallel and Distributed Computing (HPDCu002713). HPDCu002713 follows the tradition of previous versions of the conference by providing a high-quality, single-track forum for presenting new research results on all aspects of the design, the implementation, the evaluation, and the use of parallel and distributed systems for high-end computing. The HPDCu002713 program features six paper sessions on I/O- and Data-Intensive Computing, Networks, Communication, Checkpointing, Bugs and Errors, Multicore and GPUs, and Virtualization and Clouds, and keynotes by Garth Gibson of Carnegie Mellon University and by David Shaw of D.E. Shaw Research. This program is complemented by an interesting set of workshops on a range of timely and related infrastructure and application topics. The conference once again features the presentation of the HPDC Annual Achievement Award, which was started in 2012. The purpose of this award is to recognize individuals who have made long-lasting, influential contributions to the foundations or practice of the field of high-performance parallel and distributed computing, to raise the awareness of these contributions, especially among the younger generation of researchers, and to improve the image and the public relations of the HPDC community. The process of selecting the winner of the award was formalized this year with an open call for nominations. The recipient of the 2013 HPDC Achievement Award is Miron Livny of the University of Wisconsin, who will give an Achievement Award Talk as a keynote at the conference. The HPDCu002713 call for papers attracted 131 paper submissions. In the review process this year, we retained the two innovations that were introduced in 2012, i.e., the two-round reviewing process and author rebuttals. In the first round, all papers received three reviews, and based on these reviews, 72 papers went to the second round in which virtually all of them received another three reviews (six in total). In total, 592 reviews were provided by the 50-member Program Committee along with a number of external reviewers. For many of the 72 second-round papers, the authors submitted rebuttals. Rebuttals were carefully taken into consideration during the Program Committee deliberations as part of the selection process. In March, the Program Committee met at Rutgers University, and decided on the acceptance of 20 full papers resulting in an acceptance rate of 15.3%. The committee also accepted 14 papers for posters, of which 11 appear in the proceedings. We are very grateful to the members of the Program Committee for their hard work and for providing their reviews on time, in what was a very tight review schedule and a very rigorous review process.		M. Parashar;J. B. Weissman;D.H.J. Epema;R.J.O. Figueiredo	2013			engineering physics	Arch	-60.78050471288703	-17.748299636625486	177882
227d0bcb5122bcd125e7655b3e6857775e9aa0b6	a simple juggling robot: theory and experimentation	control algorithm;linear system	We have developed a formalism for describing and analyzing a very simple representative of a class of robotic tasks which involve repeated robot-environment interactions, among then the task of juggling. We review our empirical success to date with a new class of control algorithms for this task domain that we call “mirror algorithms.” These new nonlinear feedback algorithms were motivated strongly by experimental insights after the failure of local controllers based upon a linearized analysis. We offer here a proof that a suitable mirror algorithm is correct with respect to the local version of the specified task — the “vertical one-juggle” — but observe that the resulting ability to place poles of the local linearized system does not achieve noticeably superior transient performance in experiments. We discuss the further analysis and experimentation that should provide a theoretical basis for improving performance. For more information: Kod*Lab Disciplines Electrical and Computer Engineering | Engineering | Systems Engineering Comments Preprint version. Published in Lecture Notes in Control and Information Science, Volume 1, 1989, pages 35-73. Publisher URL: http://link.springer.com/book/10.1007/BFb0042509 NOTE: At the time of publication, author Daniel Koditschek was affiliated with Yale University. Currently, he is a faculty member in the Department of Electrical and Systems Engineering at the University of Pennsylvania. This journal article is available at ScholarlyCommons: http://repository.upenn.edu/ese_papers/669	algorithm;computer engineering;experiment;hyperlink;information science;interaction;juggling robot;nonlinear system;semantics (computer science);systems engineering	Martin Buehler;Daniel E. Koditschek;P. J. Kindlmann	1989		10.1007/BFb0042512	control engineering;computer science;artificial intelligence;control theory;linear system	ML	-59.951544009127566	-18.047832012684193	177965
20bed14b2ab9ebb3d5e28c2a89bb7646d253026c	logic, language and computation, festschrift in honor of satoru takasu		Now welcome, the most inspiring book today from a very professional writer in the world, logic language and computation festschrift in honor of satoru takasu. This is the book that many people in the world waiting for to publish. After the announced of this book, the book lovers are really curious to see how this book is actually. Are you one of them? That's very proper. You may not be regret now to seek for this book to read.	computation;regret (decision theory)	Satoru Takasu	1994				NLP	-60.326803312030194	-23.209758343578155	177999
75290e31a90d1a478fe13a1eab08aaeb64168e6a	editorial: associate editor introduction and farewell	associate editor introduction	T is my pleasure to introduce and welcome two new members of the editorial board, Audris Mockus and Neeraj Suri. Their brief biographical sketches below present their accomplishments expertise and interests. The role of the editorial board is very important and being a member of the board is a challenging and time consuming. The members of the editorial board are responsible for selecting reviewers for papers submitted to the journal and for making publication decisions. Many of them also assist in the preparation of our various special issues. The editorial board also engages in regular discussion about policy issues facing TSE. These activities are undertaken voluntarily and coexist with existing professional responsibilities. In addition, Sol Shatz has completed his term on the editorial board as an associate editor. He has served TSE and, thus, the community with both enthusiasm and energy. We have all benefitted from the work he has done. It is my great pleasure to personally thank him for his many contributions to TSE. John Knight Editor-in-Chief			2005	IEEE Trans. Software Eng.	10.1109/TSE.2005.10	medicine;management;operations research;performance art	SE	-62.37597231485746	-18.01724773711246	178033
ace5da5e739f1dc1f4caf2722b15fdcc2571c1b4	book reviews		This monumental Introduction to Neural Networks evolved from the author’s teaching notes for an undergraduate course on neural modelling. If Professor Anderson at the podium resembles Anderson the author, his students are fortunate – they have a guide who is lucid, experienced, rigorous, thorough, and possessing a dry wit. In his Introduction, Anderson places his book in “a kind of interdisciplinary neutral zone equidistant from neuroscience, cognitive science, and engineering” (p. vii). This seems to us to be exactly the right place to be, and we know of no better navigator in these deep waters. At its best, the book makes intriguing connections across the neutral zone, illuminated by the wisdom of decades at the computer and in the lab. Through his presentation of forty years’ worth of empirical evidence, Anderson recreates the history of neural modelling while at the same time assessing much of the most interesting work in cognitive science and neuroscience to date. The book’s theoretical claims are generally supported by comprehensible mathematical proofs and demonstrations. Its neural models are equally well documented and are additionally accessible as Pascal source code by anonymous ftp from MIT Press. However, the book does sometimes veer into technical details that may leave behind all but the engineers. Anderson begins his tour with the single neuron, describing its functional properties and deriving the Nernst equation (for membrane potential) and equations for time and space constants for propagating signals (Chapter 1, “Properties of Single Neurons”). After a short ferry across the synaptic cleft, the idealizing of biology begins, to culminate in a generic model neuron, a familiar item but nicely unfolded through equations and diagrams (Chapter 2, “Synaptic Integration and Neuron Models”). Vectors concisely describe model neurons acting in concert, so in Chapter 3 (“Essential Vector Operations”) Anderson tours the relevant linear algebra and Pascal coding of essential vector operations. The fun accelerates in Chapter 4 (“Lateral Inhibition and Sensory Processing”) with a working model of Limulus vision and a number of software experiments one can compare to the real neurophysiology of this famous sea-dweller. Anderson reviews decades of work on the crab and leads the reader through a number of models of lateral inhibition, illustrating phenomena such as Mach bands. (In this careful exposition, we also detect his devotion to effective teaching.) Or you can do the physiology yourself	cognitive science;diagram;dry loop;experiment;lateral thinking;linear algebra;lucid;mach;neural networks;neuron;pascal;podium;synaptic package manager;vii	J. A. Anderson	1997	Minds and Machines	10.1023/A:1017134617437		ML	-56.23441819245349	-21.49007986459769	178763
89fb79b9ab516e1c059708e242811d9f8ce56599	paper and proposal reviews: is the process flawed?	near-term future;proposal review;short paper;computing research association conference;audience commentary	At the 2008 Computing Research Association Conference at Snowbird, the authors participated in a panel addressing the issue of paper and proposal reviews. This short paper summarizes the panelists' presentations and audience commentary. It concludes with some observations and suggestions on how we might address this issue in the near-term future.		Henry F. Korth;Philip A. Bernstein;Mary F. Fernández;Le Gruenwald;Phokion G. Kolaitis;Kathryn S. McKinley;M. Tamer Özsu	2008	SIGMOD Record	10.1145/1462571.1462581	computer science;operations research	HPC	-61.59961329998113	-17.144027286894293	178778
eafc514b64656cf6959f814dd47b5725aab4b634	computing surveys' electronic symposium on the theory of computation	computing survey;electronic symposium	The 25th anniversary of the European Association for Theoretical Computer Science (EATCS) was celebrated during the 24th session of the International Colloquium on Automata, Languages and Programming (ICALP) in Bologna, Italy, on July 7-11, 1997. More than 300 scientists participated in the conference and its satellite events; there were more than 200 presentations covering all areas of theoretical computer science. There were many stimulating discussions about the role of theory in computer science during the invited talks and side events, such as those given by Robin Milner and Maurice Nivat when receiving the laurea ad honore from the University of Bologna, and a by panel on the role of European funding of basic research in computer science, with panelists from academia, industry, and the European Commission. This ACM Computing Surveys symposium on perspectives in theoretical computer science has a similar format to previous Surveys’ symposia. Our goal is not to provide a panorama of the field, but to complement such efforts by selecting short contributions with more detail on specific topics that show the interactions between theory and other areas of computer science. Our collection of papers represents a primarily European perspective on the theory of computation (the majority of ICALP participants are European). Even though this group of papers is far from a complete picture of theoretical computer science, the reader can nevertheless find useful facets that may remain hidden in a larger-grained description of the field. In our opinion, this collection bears witness to the fact that theoretical computer science is a lively area where many new activities are being born and others are flourishing. We have divided the papers of this Symposium into the following subareas:	acm computing surveys;automaton;european association for theoretical computer science;icalp;interaction;lively kernel;maurice herlihy;theory of computation	Pierpaolo Degano;Roberto Gorrieri;Alberto Marchetti-Spaccamela;Peter Wegner	1999	ACM Comput. Surv.	10.1145/333580.333017	computational science;computer science;theoretical computer science	Theory	-59.768293771413504	-18.311410255170177	178958
d635aad0aa00672b4b2cceee15a1fdff4ecae6ea	combining static and dynamic analysis to find multi-threading faults beyond data races		To the memory of Christoph Lehmann Circumstances, and a certain bias of mind, have led me to take interest in such riddles, and it may well be doubted whether human ingenuity can construct an enigma of the kind which human ingenuity may not, by proper application, resolve. Acknowledgements This thesis would not have been possible without the support of many people. First and foremost, I would like to thank my advisor, Armin Biere, for his support, his collaboration on all the papers written, and for letting me work at NASA Ames during two summers. Furthermore, much of this work, especially experiments using model checking, would not have been possible without the input and work of Viktor Schuppan. The two summer projects at Ames were an invaluable source of inspiration for me and determined the direction of my thesis. I am especially indebted to Klaus Havelund who supervised my work there, but also to Allen Goldberg and Robert Filman for sharing their ideas with me. I would also like to thank Saddek Bensalem, whom I also met at Ames, for inviting me to Verimag in 2003 and developing ideas for more future projects than could be started within the time of a single thesis. My thanks also go to Thomas Gross and Doron Peled who agreed to be my co-advisors. I am thankful for their suggestions and feedback. Students who worked for the JNuke project also contributed significantly to its success. all contributed to vital parts of JNuke. I would also like to thank Horacio Gagliano and Raphael Ackermann for work on related projects that did not find a way into this dissertation. Many thanks go to Christoph von Praun for kindly providing his benchmark applications and for quickly answering questions about the nature of the atomicity violations in them, and to Robert Stärk for sharing his profound knowledge of the intricacies of byte-code correctness and his input throughout the work involving bytecode simplification. Finally I would like to thank all my co-workers at the Computer Systems Institute, who created a friendly and stimulating environment at work, and especially the secretaries ,	ackermann function;atomicity (database systems);benchmark (computing);byte;correctness (computer science);enigma machine;experiment;foremost;goto;hardware description language;jart armin;mind;model checking;robert;symbolic computation;thread (computing)	Cyrille Artho	2005			parallel computing;multithreading;computer science	SE	-60.50824733824414	-20.51177846249817	179296
fbaa26eb1b7430855c629dfb49396a62a904ce96	taai 2011 computer-go tournaments	news item			Cheng-Wei Chou;Shi-Jim Yen;I-Chen Wu	2011	ICGA Journal	10.3233/ICG-2011-34414	computer science	Vision	-55.74151769433176	-17.433343536094412	180001
11c986fe61a5c4b56f246b24be18426165bcf5d1	from all possible worlds to small worlds: a story of how we started and where we will go doing semantics	conference paper	This is a short story of how we have evolved over the last 40 years, doing semantics. It could partially overlap with a history of PACLIC which is commemorating the 25th year or a quarter of a century of its founding. The story tells how we semanticists of natural language moved from all possible worlds to small worlds, now living in and with a tiny mobile world.	natural language;possible world	Kiyong Lee	2012			humanities;computer science;multimedia;literature	AI	-59.830828497612494	-23.072031427806472	180276
c9c7bbe9c8316739874b07bad90561cd40a4975d	message from apeser organization committee	apeser organization committee	It is our pleasure to welcome you to the First Asia-Pacific Workshop on Embedded System Education and Research (APESER 2007). This workshop is held in conjunction with the 13th International Conference on Parallel and Distributed Systems (ICPADS'07), Hsinchu, Taiwan. Embedded system design as a discipline has long been pursued in the industry in various application domains including avionics, aerospace, automobile, and industrial control. Within the Asia-Pacific region, the embedded system industry has reached a high level of success. This has resulted in a growing demand for highly-trained and skilled manpower to meet the ever growing demand for trained human resources. This has spurred a growing emphasis on embedded systems education in most of the universities in the Asia-Pacific region. This workshop was conceived for providing a forum for faculty in the Asia-Pacific region to meet and share their experiences, both in education and research on embedded systems. We hope that this gathering will result in an interesting and intellectually inspiring atmosphere for exchange of ideas.	avionics;embedded system;experience;high-level programming language;icpads;intellect;knowledge spillover;systems design	Shiao-Li Tsao;Tai-Yi Huang;John Kar-Kin Zao;Jogesh K. Muppala;Chi-Sheng Shih	2007		10.1109/ICPADS.2007.4447799	computer science;operations research	EDA	-58.58122672401213	-17.698681953199507	180791
45d8b8872023da030277db66f32dc47d321b9268	introduction to the special issue of papers from disc 2015		This special issue of Distributed Computing is based on four papers that originally appeared, in preliminary and abbreviated form, in the Proceedings of the 29th International Symposium onDistributedComputing (DISC), held onOctober 7–9, 2015 inTokyo, Japan. The paperswere chosen by the ProgramCommittee from the 42 full-length papers presented at the symposium, based on their quality and representation of the range of topics addressed in the symposium. DISC is an annual forum for presentation of high-quality and novel research on all aspects of Distributed Computing, including the theory, design, implementation and applications of distributed algorithms, systems and networks. The four selected papers offer a fascinating view of the breadth and depth of subjects presented at DISC. The first paper, by Rati Gelashvili, introduces a new technique for proving space lower bounds for the fundamental problem of consensus in shared memory systems, which results in new insights and greatly improved bounds. The second paper, by Michael Dinitz, Jeremy Fineman, Seth Gilbert and Calvin Newport, shows how the smoothed analysis technique originally introduced by Spielman and Teng for the analysis of sequential algorithms can be adapted to distributed protocols and can provide new insights into their behavior.	distributed algorithm;distributed computing;gilbert cell;shared memory;smoothed analysis;smoothing	Yoram Moses	2018	Distributed Computing	10.1007/s00446-018-0337-3	petroleum engineering;engineering;mechanical engineering	Theory	-60.51459933746358	-17.8016053332559	180913
defd439169a942908deca2b2567a3fdf9f28dd4f	"""an old discipline with a new twist: the course """"logic in action"""""""		What are the basic logical notions and skills that all beginning students should learn, and that might stay with them as a useful cultural travel kit for their lives, even when an overwhelming majority will not become professional logicians? The course “Logic in Action” http://www.logicinaction.org/ tries to convey the idea that logic is about reasoning but also much more: including information and action, both by individuals and in multi-agent settings, studied by semantic and syntactic tools, and still confirming to the standards of precision of an exact and mathematized discipline. Viewed in this way, modern logic sits at a crossroads of academic disciplines where interesting new developments occur every day. In this light introduction, I explain the main ideas behind the design of the course, which combines predicate logic with various modal logics, and I lightly discuss its current manifestations and dialects in Amsterdam, Beijing and the Bay Area, as well as its future as an EdX pilot course. 1 History of the course There is a thriving international market of new on-line logic courses today, witness the many projects presented at the successive TTL conferences 1 and the links there to earlier conferences in this series. Roughly speaking these endeavors fall into two kinds. Sometimes the new technology is used to create high-tech versions of largely standard fare in the traditional curriculum with, say, sophisticated graphics interfaces for classical natural deduction proof systems, like a Latin Mass with rock I thank the organizers of the Conference on Tools for Teaching Logic, Rennes 2015, for giving me an opportunity and a forum for reflecting on the course “Logic in Action”. I also thank the members of the core LiA development team for the course as well as the users that we know of, and finally, I am grateful to the two referees for this paper for providing very useful critical comments. 1See the website http://ttl2015.irisa.fr of these conferences. Vol. 4 No. 1 2017 IFCoLog Journal of Logic and its Applications	computer science;information system;software engineering	Johan van Benthem	2017	FLAP			Logic	-59.818198511907674	-20.25502741854664	181248
d8d12847bba4c7ade08ccba1b72cc34d3bdd1689	there’s plenty of boole at the bottom: a reversible ca against information entropy	philosophy of information;digital physics;cellular automata	“There’s Plenty of Room at the Bottom”, said the title of Richard Feynman’s 1959 seminal conference at the California Institute of Technology. Fifty years on, nanotechnologies have led computer scientists to pay close attention to the links between physical reality and information processing. Not all the physical requirements of optimal computation are captured by traditional models—one still largely missing is reversibility. The dynamic laws of physics are reversible at microphysical level, distinct initial states of a system leading to distinct final states. On the other hand, as von Neumann already conjectured, irreversible information processing is expensive: to erase a single bit of information costs ~3 × 10−21 joules at room temperature. Information entropy is a thermodynamic cost, to be paid in non-computational energy dissipation. This paper addresses the problem drawing on Edward Fredkin’s Finite Nature hypothesis: the ultimate nature of the universe is discrete and finite, satisfying the axioms of classical, atomistic mereology. The chosen model is a cellular automaton (CA) with reversible dynamics, capable of retaining memory of the information present at the beginning of the universe. Such a CA can implement the Boolean logical operations and the other building bricks of computation: it can develop and host all-purpose computers. The model is a candidate for the realization of computational systems, capable of exploiting the resources of the physical world in an efficient way, for they can host logical circuits with negligible internal energy dissipation.	algorithm;cellular automaton;computable function;computation;computer scientist;conway's game of life;entropy (information theory);halting problem;information processing;logical connective;mereology;molecular dynamics;requirement;turing	Francesco Berto;Jacopo Tagliabue;Gabriele Rossi	2016	Minds and Machines	10.1007/s11023-016-9401-6	psychology;epistemology;computer science;artificial intelligence;digital physics;algorithm;cognitive science	HPC	-56.720465816495114	-22.486371821158972	181427
8a0a297efa9552df17371d71296e856c69b4d30b	opposites attract: computational and quantitative outreach through artistic expressions	outreach;art;data science;media arts;computational science;visualization;participatory art;video;new media	Staff from the University of Tennessee's Joint institute for Computational Sciences, National Institute for Computational Sciences, and Remote Data Analysis and Visualization Center have teamed up with faculty from UT's School of Art to engage with students, the public, and the research community on a number of projects that connect the arts with the science and computing disciplines. These collaborations have led to coursework for students, videos about scientific discovery, and the production of novel, computer-mediated artwork. Both the arts and the sciences have gained from these collaborations.	computation	Amy F. Szczepanski;Christal Yost;Norman Magden;Evan Meaney;Carolyn I. Staples	2013		10.1145/2484762.2484772	library science;human–computer interaction;engineering;multimedia	ML	-58.92474109181147	-18.947361234637523	181456
5c3b59a40710ddfd6746b34f89d028a2e5119d2e	serious fun in computer science	outreach;early experience;primary school;computational thinking;usability evaluation;blow up;turing test;search algorithm;higher education;artificial intelligent;cs4fn;innovation;interactive system;widening participation in higher education;fun;human error;age groups;epsrc	"""Computer Science has been in crisis for several years. Interest in studying it has dropped dramatically. We can wring our hands, or we can do something about it. Computer Science needs to engage with pure outreach: selling the subject (for free). Our approach has been to do this by just going out and having innovative fun. This is more effective than selling specific courses or institutions. CS is after all a naturally exciting, innovative and thought-provoking subject (Oh, and by the way there are good jobs at the end too).  We see it as a Renaissance subject (so who cares what it's called or which variation the future depends on). It sits in a unique position, centrally connected to all of the sciences, arts and humanities. We are passionate about science generally, so we go out and spread that enthusiasm about it all.  What have we been doing? There is cs4fn (www.cs4fn.org) a website and magazine that we've been writing for the sheer enjoyment of it; Sodarace (www.sodarace.net) where over 130 000 registered humans and computers compete in an online Olympics; Brain Academy (www.brainacademy.qmul.ac.uk), the Compute-Ability competition with career enhancing prizes; research talks for kids on Artificial Intelligence, women in technology, disability and mutant super-hero powers (actually its about search algorithms) and so on; a Computer Science Magic show (you have to promise not to tell anyone the secrets); and exhibits at the Royal Society Summer Exhibition on mathematical illusions and their link to computer science (www.cs4fn.org/illusions). We build brains that play Snap from rope and toilet roll, and introduce a piece of paper more intelligent than anyone in the room. Kids act out Turing Tests (can you tell the human from the robot?), and we challenge them to solve puzzles with CS twists. We do real research too of course: a spin-out of our EPSRC funded research project on Human Error and Interactive Systems includes an online SpaceInvaders game-experiment (www.cs4fn.org/humanerror) the data from which helps us understand the causes of human error: can you consistently avoid making the mistake that will blow up your ship and lose all your points?.  What age group should we be targeting? Sixth formers? A major issue is that school ICT, vital as it is, gives a poor impression of how being a computer scientist is about being an innovative, creative, computational thinker. By the sixth form it is too late. It is the younger kids we have to get the message to. An early experiment has been teaching a version of a graduate level course on usability evaluation to primary school kids (www.cs4fn.org/manorside). They proved to be very innovative and """"amazing"""", """"will stay in my mind forever"""", """"I want to teach others in the school what we did"""" were some of their comments.  Our approach works: teachers, industry and the International Review of ICT have commended us & and we have seen an increase in undergraduate applications of over 130% in 2 years. Not bad when we are just having fun: serious fun."""	academy;artificial intelligence;computation;computer science;computer scientist;human error;humans;job stream;renaissance;search algorithm;sixthsense;turing test;usability	Paul Curzon	2007		10.1145/1268784.1268785	innovation;turing test;simulation;human error;computer science;multimedia;higher education;demographic profile;search algorithm	Theory	-61.5412004060701	-23.33313967681007	183023
dab1b6bdea7fcc25d3c7f76ec7f67778e524ab80	colorful patterns reveal the structures in fluids	colorful patterns reveal	Analytical methods based on color visualization are widely used today not only in the physical sciences but also in the rapidly expanding arena of life sciences. In addition to visualizing fluid flows, we have now added color imaging of dehydrated drops of biological liquids(blood serum, bile, tear, gastric juice), which are used in medical applications. The foundations of these prospective diagnostic methods are now being discussed along with some impressive examples of color visualization . This Journal contains papers from two different sources. One consists of papers extended from talks presented at the 11th Biannual Conference on Fluxes and Structures in Fluids. This conference was held at the Institute for Problems in Mechanics of the Russian Academy of Science on June 20-22, 2001. More than one hundred research scientists presented almost one hundred talks during the 3-day conference. The principal objective of the conference was discussing ways to construct closed models of environmental processes. The topic of flow imaging has come to play an important role in the journey toward achieving this goal. Recently, large sets of regular and chaotic structures in traditional and modern experimental studies can now be revealed by visualization techniques. The detailed structures of boundary layers and internal boundary currents in stratified and/or rotating fluids are now being studied under laboratory conditions. The most impressive applications of color flow visualization have been selected by the chairpersons of the various sessions for presentation in this Journal. However, it is beyond the scope of the Journal to present all the selected papers in a single issue. Therefore, papers not included in this issue will appear in subsequent issues. Parts of other presented papers are slated for publication in various Russian and International Journals. The other source of material is invited papers from researchers all over the world who wish to submit papers to the Journal of Visualization. We are grateful to all the authors for the effort to adapt their manuscripts to the Journal of Visualization’s style and standards. We are especially grateful to the chairpersons of the Conference Sessions who screened and recommended these papers out of the large number of presentations, and who have been involved in the preliminary peer review process. We’re very much looking forward to your participation at the next conference “Fluxes and Structures in Fluids”, scheduled to be held June 23-26, 2003 in St. Petersburg, Russia.	academy;projection screen;prospective search;scientific visualization	Yuli D. Chashechkin;Makoto Oki	2003	J. Visualization	10.1007/BF03180953	chemical physics;classical mechanics;physics	Visualization	-58.40928743620165	-19.405824349866595	183330
f258617c9c39f1dfc36b662e77c5defb7de63525	green technology	green;power-reduction;green technology;low-power	Green Technology computing, information, data, and communications services to many DoE projects. LBL also helps to manage the Joint Genome Institute—a key contributor to the Human Genome Project. LBL scientists have been involved in many important discoveries. Researchers have identified 16 of the elements on the periodic table, including technetium-99m, which revolutionized biomedical imaging, and americium, which is commonly used in smoke detectors. LBL researchers Louis and Walter Alvarez led a team that discovered high levels of iridium in the K-T boundary layer, a layer of clay sediments found in rocks all over the world. Their discovery led to the theory that the dinosaurs were wiped out by an iridium-rich asteroid that hit the earth 65 million years ago. Dark energy, the mysterious substance believed to make up 73% of the universe, was discovered by LBL’s Supernova Cosmology Project. The lab was also behind the discovery that cholesterol comes in both good and bad forms, which has led to improved testing and treatment for heart disease.	design of experiments;medical imaging;sensor	James Stanier	2011	ACM Crossroads	10.1145/1961678.1961691	business	Security	-56.01784785420022	-18.673472999999593	183677
35f522680766a5c48a251f9ccc2a184343dc9058	popularizing natural sciences by means of scientific fair	games;multimedia;educational	Science popularization is demanding from the financial as well as the time point of view. It is necessary to find the premises that would be easily available to general public. Another important step is to promote the event so that it would attract the audience. The preparation of scientific experiments itself also requires some financial resources. If we want to take advantage of these resources in the most useful and effective way, we have to find answers to the question: “What, where and how do we want to popularise?” In the paper, we describe one-day project aimed to popularization of scientific fields carried out by eight departments of the Faculty of Natural Sciences, Constantine the Philosopher University in Nitra. The project was named Scientific Fair – Science you can see, hear and experience. Its main goal was to present seven scientific fields Physics, Informatics, Mathematics, Geography, Ecology, Chemistry and Biology. Popularization was carried out as experimental interactive activities unveiling the undisclosed corners of science. Their aim was to inspire the audience, arouse their interest in science and motivate the participants to cognitive activities. We introduce the idea of the project in detail concentrating mainly on informatics realized by the Department of Informatics.		Martin Cápay;Jozef Kapusta;Martin Magdin;Miroslava Mesárosová;Peter Svec;Lubomíra Valovicová	2011	iJET		mathematics education;applied mathematics;computer science;science;multimedia	NLP	-58.92860094651509	-19.639233212075773	183713
3a590c3502a5ab6733e4779863663c929ae7aa43	open access - a never ending story?	case history;edition electronique;libre acceso;historique;edicion electronica;open access;electronic publishing;libre acces;estudio historico	Like all good stories, also the story of “open access” is occasionally rather confusing, and in which direction the story will develop and where and how it will end will depend on the point of view and the position of the person actually telling the story. Thereby, one has to remember that the story of “open access” is a trilogy rather than a monograph, since at least three sides with their own, independent ideas are involved: the subscribers (readers and researchers), the authors and the publishers. The present report is therefore based on our own experience as scientists serving as readers, researchers, authors, editors, referees and officers for the European Geosciences Union (egu.eu) and its open access publications, with our open access publisher Copernicus Publications (copernicus.org), and with the software house for open access publishing Copernicus Systems + Technology (copernicussystems.net). After a short retrospect on the history of the open access mission, we will discuss the advantages as well as the stumbling blocks of open access for researchers, authors and publishers and possible business models for open access publishing.	retrospect (software);software house;stumbleupon	Arne K. Richter	2008	Inf. Services and Use	10.3233/ISU-2008-0558	art;performance art;cartography	DB	-61.564714500617185	-19.635969580249565	184336
1f985a46468542eb6fb3aaab65179d2ad1f9d34a	editorial comment: the times they are a-changin'		This reminded me of the concluding remark in a paper by by Paula Kotze and Alta van der Merwe, who, in a sense, pose the same problem. They mention that their paper did not try to determine the validity of SACJ as a representative of the research landscape in Computing in South Africa. After making this statement, they then pose a number of questions. The tone of the questions seems to imply that they believe the contrary to be true. They conclude with this question:	the times	Lucas M. Venter	2010	South African Computer Journal			Theory	-59.49510417690017	-22.033435307937722	184688
af0d3ef1d089145547c7bf39684ddbbccbec1ae2	speech synthesis	speech recognition and synthesis	Any books that you read, no matter how you got the sentences that have been read from the books, surely they will give you goodness. But, we will show you one of recommendation of the book that you need to read. This speech synthesis is what we surely mean. We will show you the reasonable reasons why you need to read this book. This book is a kind of precious book written by an experienced author.	book;speech synthesis	Finn Kuusisto	2014	ACM Crossroads	10.1145/2667637	speech technology	Theory	-61.52313528910066	-23.840728380887764	184788
28b467f75ef38e673de6d0d2e0b542fb443bf01d	reducing line clutter in software engineering diagrams	cographs;list ranking;software engineering;scaling up;chip;large scale;lines of code;graphical representation;tree contraction;case tool;erew pram;tree representable graphs;parallel algorithms	"""Convent ional d i ag rammat i c aids for software are of ten of Httle use in the design and maintenance of large scale commerc ia l systems, due to t h e p h e n o m e n o n of """"line c lu t te r"""" which quickly overwhelms and c o n f u s e s t h e v i e w e r as t h e sy s t e m depic ted scales up. In this paper , we introduce a n e w g r a p h i c a l r e p r e s e n t a t i o n which is in t ended for use as par t of an interact ive CASE tool. A p ro to type imp lemen ta t i on suppor t ing the Rose t t a Chip, as we call it, has been used to successfully m a p C programs consist ing of thousands of lines of c o d e . 1 I n t r o d u c t i o n The design and maintenance of large scale commercial software systems is an onerous undertaking. Thousands, or even tens of thousands, of modules may be involved, changes to any one of which may have serious and far-reaching repercussions. The daunting task of keeping track of module interdependencies is therefore a major concern. *This research was supported, in part, by the IBM Corporation under its Shared University Research (SUR) program and by the National Science Foundation under contract CDA-8805910. First author's current address: Bell Communications Research, 3 Corporate Place, Piscataway, NJ 08854. Authors' e-mail addresses: moyer~base.bellcore.com@beiicore.com, 91in. ert @turing. cs.rpi, edu. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. © 1991 ACM 089791-382-5/91/0003/0161 $1.50 161 The past decade has witnessed the accumulation of an impressive body of evidence [1] that visual environments, in which multiple windows and graphical elements play prominent roles alongside text, may prove highly beneficial to computer users in general, and to programmers and software engineers in particular. However, the size and complexity of commercial software systems often make the use of conventional diagrammatic aids, such as directed graphs or structure charts, less valuable than initially anticipated. This is due to the phenomenon of line clutter--also known as the proverbial """"spaghetti ba l i ' -which quickly overwhelms and confuses the viewer as the system represented scales up. Locating specific modules becomes difficult, successfully navigating from one place to another close to impossible. Currently available systems [2] for computer assisted software engineering (CASE) aren' t a panacea, in that for the most part they merely transfer to t h e screen the same sorts of diagrams which software engineers used to draw on paper or on their whiteboards. Noteworthy recent efforts to address this problem have been made by Harel [3] and others. Nevertheless, we believe that the """"missing ingredient"""" in this work, where complex software systems are concerned, is a stronger bond between representation and implementat ion to allow the graphical representation to become interactive. For the past couple of years, a small team under the direction of the second author has been investigating the application of CASE to large scale systems. As part of this project, an experimental testbed called the GLASSBOARD has been implemented on Sun Microsystems color workstations. The environment is coded in C, with graphics support provided by MIT's X / l l Windows. One of its more interesting features is the socalled RC mapper, which recognizes most of the C language t and which can automatically analyze existing source code and generate a graphical representation which we have dubbed the Rosetta Chip, so named because of (a) the (deliberate) similarity to VLSI chips, and (b) the intended use, which is to help manage and uncover the long-lost secret workings of software that is inadequately documented and /or of unwieldy size."""	chart;clutter;commercial software;diagram;directed graph;email;emoticon;graphical user interface;graphics;interdependence;l-system;lu decomposition;microsoft windows;programmer;software engineer;software engineering;software system;testbed;tree accumulation;user (computing);very-large-scale integration;workstation	Todd S. Moyer;Ephraim P. Glinert	1991		10.1145/327164.327228	chip;computer science;theoretical computer science;software engineering;database;parallel algorithm;source lines of code	SE	-61.04474696489635	-19.821128231198728	184915
ff29c66b652c617840cb2e36e242b860e3966531	welcome to prof. amit sheth	selected works;bepress	It is with great pleasure and enthusiasm that I announce the creation of a new co-EIC position for the DAPD Journal. We have recently published our 21st Volume without ever missing an issue or being late, the journal is now published by Springer and we have moved completely into electronic reviewing and publishing. As we move forward, I could not have found a better partner than my old friend and college class mate, Amit Sheth. Prof. Sheth has recently moved to the Wright State University as the LexisNexis Ohio Eminent Scholar in Advanced Data Management and Analysis. There are several compelling reasons why Prof. Sheth was my only choice to help me lead the Journal going forward. He has been pioneer and a leader in the database, semantic web and workflow communities. He has consistently pursued discovery, learning and engagement more successfully than any one else I know. Of particular interest to our Journal, he co-authored 3 of the most cited papers in the journal with over 1600 combined citations: “An overview of Workflow Management System”, “OBSERVER: An Approach for Query Processing in Global Information” (3rd most cited with over 400 citations), and “Managing heterogeneous multi-system tasks to support enterprise-wide operations” (5th most cited with 198 citations). Please share with me a sincere thanks to Prof. Sheth for his willingness to work with me to take this journal to new heights.	database;earth inductor compass;management system;semantic web;springer (tank)	Ahmed K. Elmagarmid;Amit P. Sheth	2007	Distributed and Parallel Databases	10.1007/s10619-007-7017-9	computer science	DB	-61.72029679754449	-17.301176779527584	185277
2839e0dd652ce64c1cc0295e3ca6ec334f2dc121	rethinking randomness: an interview with jeff buzen, part i		For more than 40 years, Jeffrey Buzen has been a leader in performance prediction of computer systems and networks. His first major contribution was an algorithm, known now as Buzen's Algorithm, that calculated the throughput and response time of any practical network of servers in a few seconds. Prior algorithms were useless because they would have taken months or years for the same calculations. Buzen's breakthrough opened a new industry of companies providing performance evaluation services, and laid scientific foundations for designing systems that meet performance objectives. Along the way, he became troubled by the fact that the real systems he was evaluating seriously violated his model's assumptions, and yet the faulty models predicted throughput to within 5 percent of the true value and response time to within 25 percent. He began puzzling over this anomaly and invented a new framework for building computer performance models, which he called operational analysis. Operational analysis produced the same formulas, but with assumptions that hold in most systems. As he continued to understand this puzzle, he formulated a more complete theory of randomness, which he calls observational stochastics, and he wrote a book Rethinking Randomness laying out his new theory. We talked with Jeff Buzen about his work.  Peter J. DenningEditor in Chief	anomaly detection;buzen's algorithm;computer performance;operations research;performance evaluation;performance prediction;randomness;response time (technology);throughput	Peter J. Denning	2016	Ubiquity	10.1145/2986329	telecommunications;computer science;artificial intelligence;management;operations research;law;computer security;algorithm	OS	-58.52177984685796	-18.690223584131495	185338
53819c489ce95e664319b62df32ba2f6f08dfeec	"""information technology """"key to text"""" for semantic search and indexing of textual information - an essential tool for electronic publishing"""		"""Introduction The electronic editions gives essentially new features to structure and organization for searching information by the reader and the information services providers. Before the computer revolution any edition on a library shelf or under a veil of a dust on a desk, before the reader took it in his hands, meant no more than was written in its catalogue card. (Certainly, we here do not speak about the editions surrounded with light of legends). Only the electronic edition is capable to speak at the top of its voice even in the absence of the reader. The complete dictionary index of the accessible editions, which 30 years back was the dream of any visitor of the scientific library, today has become the present damnation. Let's imagine a reader who wants to find verses about love (about the real love). He will receive a vast list of references on 10, 20, 30, … ways of love, 1001 nights of love, legal, psychological, physiological features of love of sexual minorities, on love to the Fatherland and not love to certain characters. But he searched another matter! His wishes and ideas aspired to something different. He has simply formulated a search image, and the results of the search only hide his idea of love behind a detailed lexical map of the use of the word """"love"""". Fortunately, it is possible to use the skill of the electronic editions to speak (we shall recollect Ahmatova's verse: «I have learned women to speak, but God, who will force them to stop?») into a channel of intelligent, purposeful dialogue with the prospective reader. We shall in this paper discuss the technology ensuring such dialogue on the basis of the automated computed semantic search and analysis of the textual information. This dialogue is important not only for the reader, who hungers for the information he wants. It is extremely important for the author or publisher too because of the importance of the authentical prediction of the ways how to understand how the published text is understood by defferent categories of readers. N.B.! In the Appendix the results of the analysis of the text of the presented paper by the proposed methods are given: a set of key words as they are represented to the reader of the newspaper """"Times"""". The fragments of the text included into the computed summary are underlined in the text of the paper. …"""	dictionary;e-book;prospective search;semantic search;verse protocol	Mikhail Kreines	2000				NLP	-62.66428018280053	-22.400223354422728	185472
19027a1985cde04e9ff88159bbf8ab2d71814187	apache cookbook - solutions and examples for apache administrators (2. ed.)		Any books that you read, no matter how you got the sentences that have been read from the books, surely they will give you goodness. But, we will show you one of recommendation of the book that you need to read. This apache cookbook solutions and examples for apache administrators is what we surely mean. We will show you the reasonable reasons why you need to read this book. This book is a kind of precious book written by an experienced author.	book	Ken Coar;Rich Bowen	2007			mathematics education;engineering;data science;engineering physics	ML	-62.07235648040794	-23.893389495572304	186328
8f1c22d43381a71ffcac669d4a3e1e05657765f7	jim gray's fourth paradigm and the construction of the scientific record		"""i n the latter part of his career, Jim Gray led the thinking of a group of scholars who saw the emergence of what they characterized as a fourth paradigm of scientific research. In this essay, I will focus narrowly on the implications of this fourth paradigm, which I will refer to as """" data-intensive science """" [1], for the nature of scientific communication and the scientific record. Gray's paradigm joins the classic pair of opposed but mutually supporting scientific paradigms: theory and experimentation. The third paradigm—that of large-scale computational simulation— emerged through the work of John von Neumann and others in the mid-20th century. In a certain sense, Gray's fourth paradigm provides an integrating framework that allows the first three to interact and reinforce each other, much like the traditional scientific cycle in which theory offered predictions that could be experimentally tested, and these experiments identified phenomena that required theoretical explanation. The contributions of simulation to scientific progress, while enormous, fell short of their initial promise (for example, in long-term weather prediction) in part because of the extreme sensitivity of complex systems to initial conditions and chaotic behaviors [2]; this is one example in which simulation, theory, and experiment in the context of massive amounts of data must all work together. To understand the effects of data-intensive science on the"""	chaos theory;complex systems;computation;data-intensive computing;emergence;experiment;initial condition;programming paradigm;scientific communication;simulation	Clifford A. Lynch	2009			cartography;computer science	HPC	-56.389931727712245	-22.444584241755138	186452
37fa69d90b89051e99833513abd441fd448a2541	an interview with hans tutschku	hans tutschku	Computer Music Journal, 27:4, pp. 14–26, Winter 2003 2003 Massachusetts Institute of Technology. Born in 1966, Hans Tutschku began to study music at an early age. In 1982, he joined the Ensemble für Intuitive Musik Weimar, playing synthesizer and live electronics. He studied electroacoustic composition in Dresden, The Hague and Paris, and between 1989 and 1991 he accompanied Karlheinz Stockhausen on several concert tours to study sound diffusion. As a member of the Ensemble für Intuitive Musik Weimar, Mr. Tutschku has realized multimedia productions, including those involving projection of images and choreography for dance. The ensemble has given numerous concerts in Europe, Latin America, and Asia. He has composed instrumental works, works for tape, works for musicians and electronics, and music for theater, film, and ballet. During 1995–1996, Mr. Tutschku was the professor of electroacoustic composition at the Liszt Conservatory in Weimar, and in 1996 he attended the Royaumont composition workshop with Klaus Huber and Brian Ferneyhough. During 1997–2001, he taught computer music at IRCAM. He has given master classes at the Universities of São Paulo, Buenos Aires, and Singapore, the Music Academy in Budapest, as well as in Darmstadt, Stuttgart, Florence, Milan, and Porto. He has served as a jury member of the CIMESP (São Paulo) and Métamorphoses (Brussels) international competitions for electroacoustic composition. He completed a D.E.A. degree at the Parisian Sorbonne and a Ph.D. in Composition at the University of Birmingham in the UK. He has taught electroacoustic composition at the conservatory of Montbéliard since 2001. Last summer, he held a DAAD professorship at the Technical University of Berlin. He is the recipient of several international composition prizes, including Bourges, HannsEisler-Preis, CIMESP São Paulo, Prix ARS Electronica, Prix Noroit, and Prix Musica Nova. This interview was conducted on 2 April 2003, at the Ecole Nationale de Musique in Montbéliard, France.	academy;brian;computer music journal;conservatory (greenhouse);data general nova	Ketty Nez	2003	Computer Music Journal	10.1162/014892603322730479	ballet;multimedia;computer science;computer music;choreography;dance;composition (visual arts);performance art;jury		-57.60641582946049	-17.492548929872147	186565
218b75fa54c74f7d602484af3b3ef30a60ffd53d	pascal user manual and report - iso pascal standard, 4th edition		When writing can change your life, when writing can enrich you by offering much money, why don't you try it? Are you still very confused of where getting the ideas? Do you still have no idea with what you are going to write? Now, you will need reading. A good writer is a good reader at once. You can define how you write depending on what books to read. This pascal user manual and report iso pascal standard 4th edition can help you to solve the problem. It can be one of the right sources to develop your writing skill.	book;money;pascal	Kathleen Jensen;Niklaus Wirth	1991			computer science;software engineering;computer graphics (images)	PL	-62.388349714968	-23.461586549377547	186731
1cf68f617c5155771985f6c4e2950de6201ebba3	call for papers interfaces special issue: operations research in mining		Mining is the process of extracting a naturally occurring material from the earth to derive a profit. Operations research has been used in mining primarily to make decisions about when and how to perform both surface and underground extraction with respect to how to recover the material and what to do with the extracted material. Because machines are used to extract the ore, decisions about which type of machines to use, how many machines to use, and how to allocate them also arise. Mining has received increasing attention in practice, in the literature, and at conferences. The current scarcity of easily accessible, high-grade ore deposits has underscored the importance of making mining operations more efficient, especially through operations research. Furthermore, with recent improvements in hardware and software capabilities, models are becoming detailed enough to be applicable and useful to real-world operations. We solicit papers for the special issue at the strategic, tactical, and operational levels concerned with the extraction and processing of the following minerals: (1) metallic ores such as iron and copper, (2) nonmetallic minerals such as sand and gravel, and (3) fossil fuels such as coal. Mathematical methodologies appropriate for the description of extraction and processing are traditional operations research techniques in areas such as queuing, simulation, decision analysis, and optimization. Papers must belong to one or more of the following categories: • An application with a verification letter from the company or organization using the results • A novel literature review of operations research related to mining • A tutorial emphasizing the practical implementation of mining models Authors should review the Interfaces instructions on preparing a paper at http://interfaces.pubs.informs.org/ guidelines.htm. Papers must be submitted online at http://mc.manuscriptcentral.com/inte. In the Author Center of Manuscript Central, please designate the “Manuscript Type” (#1—“Type, Title, and Abstract”) as “Special Issue” in the dropdown list; in #4—“Details and Comments,” respond “Operations Research in Mining” to the question, “This paper is for which special issue?” All papers will be refereed. Articles must be received no later than October 15, 2011 for full consideration.	operations research	Andres Weintraub;Alexandra M. Newman	2011	Interfaces	10.1287/inte.1110.0564	computer science;operations research	DB	-59.421062016340606	-19.109245531146257	186978
a475a67f13962a52b7d0c49cbe21159fa032bf11	interlisp-d: further steps in the flight from time-sharing	cognitive science;time sharing;artificial intelligent	The Interslip-D project was formed to develop a personal machine inplementation of Interlisp for use as an environment for research in artificial intelligence and cognitive science [Burton et al., 80b]. This note describes the principal developments since our last report almost a year ago [Burton et al., 80a].	artificial intelligence;cognitive science;interlisp;time-sharing	Beau Sheil	1981	SIGART Newsletter	10.1145/1056743.1056745	simulation;computer science;artificial intelligence;operations research;time-sharing	AI	-56.728654164729654	-20.38581396648262	187099
c30bc3cb7f08e951376d68c2082ae5ff3ad28f16	history of sound source localization: 1850-1950		While scientists and philosophers have been interested in sound source localization since the time of the ancient Greeks, the modern study of this topic probably began in the late 19th century. Because sound has no spatial dimensions, there were many arguments at this time as to how humans localize a source based on the sound it produces. Lord Rayleigh conducted a “garden experiment” and concluded that a binaural ratio of sound level at each ear could account for his ability to identify the location of people who spoke in the garden. This type of experiment began the modern investigation of the acoustic cues used for sound source localization. In the first half of the 20th century, psychoacousticians such as Licklider, Jeffress, Mills, Newman, Rosenzweig, Stevens, von Hornbostel, Wallach, Wertheimer, and many others (documented by Boring in Sensation and Perception, 1942 and by Blauert in Spatial Hearing, 1997) added seminal papers leading to our current understanding of sound source localization. This pr...	covox speech thing	William A. Yost	2017	Proc. Meetings on Acoustics	10.1121/2.0000529	sensation;binaural recording;acoustics;acoustic source localization;engineering	HCI	-55.65828420805146	-23.391252652399377	187109
8eb17f9eff55a6a1daa350b813203c48a889d2ba	the unique advantages of the macintosh	unique advantage	"""Since 1977, the U. S. Coast Guard Academy has been associated with Dartmouth College 'through a long term contract for time sharing. Until December of 1984, the Coast Guard leased """"dumb"""" terminals and accessed the Dartmouth system for writing programs and utilizing the many applications which Dartmouth provides. Since January, 1985, the Academy has followed the lead of Dartmouth in choosing one microcomputer-the Macintosh-and providing this computer for general use of students and faculty. We maintain three Macintosh work areas in the student dorms, which are well located and fully supplied with word processing and other materials. Also a Macintosh Laboratory contains 21 Mats, three ImageWriter printers and an Apple Laser Printer. In this room we teach faculty, cadets and staff members how to use the Macintosh. There is also a """"Micro Lab"""" where other Micro's may be used."""	academy;computer terminal;laser printing;microcomputer;time-sharing	Nancy O. Broadhead;John Gregory;Chris Pelkie;Sarah Main;Richard T. Close	1985		10.1145/318741.318755	computer engineering;computer science;multimedia	HCI	-57.64804086208566	-17.26164677848059	187223
2ac1844be250339a9003ee257f48bd6053f3bafa	what's happening		The Fair 2016 A huge thank you to the Fair Convenors and the Committee of dedicated parents who make the Fair possible each and every year. We could not have asked for better weather with the sun shining all day. The Fair highlights what a great community Parkdale PS is and what an outstanding group of parents we have. Numerous people take on roles to make this event such a success and a highpoint for the year.	ftc fair information practice;ps-algol	Marisa E. Campbell	2001	Interactions	10.1145/379537.379540		HCI	-62.81715232908333	-19.088634891744015	187699
6198149420787e08c334190539213867f7ea3b9e	selected papers from the 2nd workshop on software services		This december issue of Scalable Computing: Practice and Experience is the final of two issues devoted to Cloud Computing and Applications based on Software Services.   To ensure the high quality of this issue, a reduced number of contributions that were presented at 2nd WoSS (Timisoara, June 6-9 2011) were chosen and an invitation to provide an extended version was sent. The reviewers were the same as in the Workshop so the peer-review process was exhaustive, from the abstract status prior to the Workshop until the final version that you will be reading. Additionally, the priceless feedback gathered during 2nd WoSS definitely increased the contribution quality.   As said in the previous issue, many of the authors come from countries that joined the European Union recently. This demonstrates that the European Research Family is not only growing up but also reunites around the Cloud Computing table, a bleeding edge and promising area which is expected to bring much outstanding outcome.		Dana Petcu;José Luis Vázquez-Poletti	2011	Scalable Computing: Practice and Experience	10.12694/scpe.v12i3.723	computer science;data mining;operations research	SE	-61.725326994410715	-17.720856234892818	187788
106e5eabc0db2f17f4f73f455f5bb9ba8726c5c5	a parameterized complexity tutorial	parameterized complexity tutorial;computer journal;short tutorial;parameterized complexity;fuller account;books downey-fellows;recent survey downey-thilikos	The article was prepared for the LATA 2012 conference where I will be presenting two one and half hour lecctures for a short tutorial on parameterized complexity. Much fuller accounts can be found in the books Downey-Fellows [DF98,DFta], Niedermeier [Nie06], Flum-Grohe [FG06], the two issues of the Computer Journal [DFL08] and the recent survey Downey-Thilikos [DTH11].	book;parameterized complexity;rod downey;the computer journal	Rodney G. Downey	2012		10.1007/978-3-642-28332-1_4	computer science;theoretical computer science;algorithm	EDA	-60.06316041333461	-20.12410795211008	187890
238b706b00520160304435aed5565217fb2797f8	quantification of the emotional quotient of the intelligence using the classic and fuzzy logic	fuzzy logic	"""or the past several years, the Sunday newspaper supplement Parade has featured a column called """" Ask Marilyn. """" People are invited to query Marilyn vos Savant, who at age 10 had tested at a mental level of someone about 23 years old; that gave her an intelligence quotient of 228—the highest score ever recorded. IQ tests ask you to complete verbal and visual analogies, to envision paper after it has been folded and cut, and to deduce numerical sequences, among other similar tasks. So it is a bit perplexing when vos Savant fields such queries from the average Joe (whose IQ is 100) as, What's the difference between love and infatuation? Or what is the nature of luck and coincidence? It's not obvious how the capacity to visualize objects and to figure out numerical patterns suits one to answer questions that have eluded some of the best poets and philosophers. Clearly, intelligence encompasses more than a score on a test. Just what does it mean to be smart? How much of intelligence can be specified, and how much can we learn about it from neurobiology, genetics, ethology, computer science and other fields? The defining term of intelligence in humans still seems to be the IQ score, even though IQ tests are not given as often as they used to be. The test comes primarily in two forms: the Stanford-Binet Intelligence Scale and the Wechsler Intelligence Scales (both come in adult and children's versions). Generally costing several hundred dollars, they are usually given only by psychologists, although variations of them populate bookstores and the World Wide Web. (Superhigh scores like vos Savant's are no longer possible, because scoring is now based on a statistical population distribution among age peers, rather than simply dividing the mental age by the chronological age and multiplying by 100.) Other standardized tests, such as the Scholastic Assessment Test (SAT) and the Graduate Record Exam (GRE), capture the main aspects of IQ tests. Such standardized tests may not assess all the important elements necessary to succeed in school and in life, argues Robert J. Sternberg. In his article """" How Intelligent Is Intelligence Testing? """" , Sternberg notes that traditional tests best assess analytical and verbal skills but fail to measure creativity and practical knowledge, components also critical to problem solving and life success. Moreover, IQ tests do not necessarily predict so well once populations or …"""	computer science;fuzzy logic;humans;numerical analysis;population;problem solving;vos or openvos;world wide web	Lucimar F. de Carvalho;Roberto Rabello;Rosane R. de Morais;Silvia M. Nassar;Christiane Koehler	2001			fuzzy logic;neuro-fuzzy;fuzzy cognitive map;computer science;machine learning;emotional intelligence;artificial intelligence;fuzzy control system;fuzzy classification	AI	-57.1985378829883	-23.541318102486706	188217
b612ec46b4511a72b05b9a15aad07b7f1aeb03fa	international ranking of the acm international conference on multimedia retrieval (icmr)		In 2010, there were two well-respected multimedia retrieval conferences supported by ACM: CIVR and MIR. At that time, a committee consisting of both the CIVR and MIR steering committees was formed to decide on the future. From those discussions, a merged conference was formed: the ACM International Conference on Multimedia Retrieval (http://www.acmICMR.org) which would be governed by the ICMR steering committee. Our goal for ICMR was not to create the largest conference, but the highest-quality multimedia retrieval meeting worldwide. The Chinese Computing Federation (CCF) Ranking List provides a ranking of all international peer-reviewed conferences and journals in the broad area of computer science. This list is typically consulted by academic institutions in China as a quality metric for PhD promotions and tenure track jobs. Because ICMR was a young conference, it was not on the list in 2012. So, in late 2012, a proposal was assembled by Prabha Balakrishnan, Ramesh Jain, Klara Nahrstedt and myself towards eventual placement of ICMR on the CCF Ranking. The CCF committee examined both the quality of the content and the quality of the meeting and they gave us a ranking: The ranking released in 2013 for “Multimedia and Graphics” is first split into sections A, B, and C, and the conferences are ranked numerically within each section.	acm-mm;computer science;graphics;klara nahrstedt;mir (computer);microsoft customer care framework;numerical analysis	Michael S. Lew	2013	International Journal of Multimedia Information Retrieval	10.1007/s13735-013-0047-3	multimedia;world wide web;information retrieval	DB	-62.16808017730951	-17.226211615552412	189156
a42d32759aa92f60545f3a5c02c5cc3c5d74dad3	report on the sigir 2017 workshop on ecommerce (ecom17)		The SIGIR 2017 Workshop on eCommerce (ECOM17), was a full day workshop that took place on Friday, August 11, 2017 in Tokyo, Japan. The purpose of the workshop was to serve as a platform for publication and discussion of Information Retrieval and NLP research and their applications in the domain of eCommerce. The workshop program was designed to bring together practitioners and researchers from academia and industry to discuss the challenges and approaches to product search and recommendation in the eCommerce domain. Another goal of the workshop was to examine the building of a benchmark data set to facilitate research into this topic. The workshop drew contributions from both industry as well as academia, in total the workshop received a total of twenty one submissions, and accepted thirteen papers. In addition to presentation of a subset of accepted submissions, the workshop had two keynotes by invited speakers from the industry, a poster session where all the accepted submissions were presented, a breakout session, a panel discussion, and a group discussion.	benchmark (computing);breakout box;e-commerce;information retrieval;natural language processing;session (computer science)	Jon Degenhardt;Surya Kallumadi;Yiu-Chang Lin;Maarten de Rijke;Luo Si;Andrew Trotman;Sindhuja Venkatesh;Yinghui Xu	2017	SIGIR Forum	10.1145/3190580.3190600	computer science;panel discussion;data mining	Web+IR	-60.37051047058496	-18.11120555638167	189346
6f8b7ac0d87803fcdd5b88e6e4f5bb3b749f02cd	reports of the aaai 2009 fall symposia		, by a one-day AI funding seminar. The titles of the seven symposia were as follows: An informal reception was held on Thursday, November 5. A general plenary session, in which the highlights of each symposium were presented, was held on Friday, November 6. The challenge of creating a real-life computational equivalent of the human mind requires that we better understand at a computational level how natural intelligent systems develop their cognitive and learning functions. In recent years, biologically inspired cognitive architectures (BICA) have emerged as a pow	biologically inspired cognitive architectures;cognitive architecture;computation;mind;real life	Roger Azevedo;Trevor J. M. Bench-Capon;Gautam Biswas;Ted Carmichael;Nancy Green;Mirsad Hadzikadic;Oluwasanmi O Koyejo;Unmesh Kurup;Simon Parsons;Roberto Pirrone;Henry Prakken;Alexei V. Samsonovich;Donia Scott;Richard Souvenir	2010	AI Magazine		computer science;engineering;artificial intelligence	AI	-56.59534296457434	-19.952544530217843	189620
7d7bba7af07f1a4ea75ad222ca54c92199c8b06c	can intelligence explode?	computation;rationality;evolution;singularity;intelligence;acceleration	The technological singularity refers to a hypothetical scenario in which technological advances virtually explode. The most popular scenario is the creation of super-intelligent algorithms that recursively create ever higher intelligences. It took many decades for these ideas to spread from science fiction to popular science magazines and finally to attract the attention of serious philosophers. David Chalmers’ (JCS 2010) article is the first comprehensive philosophical analysis of the singularity in a respected philosophy journal. The motivation of my article is to augment Chalmers’ and to discuss some issues not addressed by him, in particular what it could mean for intelligence to explode. In this course, I will (have to) provide a more careful treatment of what intelligence actually is, separate speed from intelligence explosion, compare what super-intelligent participants and classical human observers might experience and do, discuss immediate implications for the diversity and value of life, consider possible bounds on intelligence, and contemplate intelligences right at the singularity.	algorithm;computation;computational resource;computer;intelligence explosion;intelligent agent;interaction;langrisser schwarz;on intelligence;recursion;singularitarianism;singularity project;speculative execution;technological singularity;virtual world	Marcus Hutter	2012	CoRR			ML	-57.95222197521457	-22.66716128181654	189689
69d038ab99f87f941266e6c992224de477d4cd42	horst sachs (1927-2016)		While preparing the present issue in honour of Horst Sachs’ 90th birthday, we have been caught up by reality: Horst Sachs passed away on 25th of April 2016, having turned 89 only a few weeks before, on 27th of March. He was one of the pioneers of graph theory in Germany. Sachs graduated (1953) in Mathematics with a diploma thesis on algebraic number theory and received his Ph.D. (Dr. rer. nat.) (1958) inMathematical Physics on isoperimetric inequalities under the auspices of H. C. Grötsch. During his years at the Martin-Luther-University in Halle-Wittenberg, Sachs got interested in the young field of graph theory and habilitated (1963), again under Grötsch, on enumerative combinatorics, with a thesis on counting specific trees, forests, and circuits in graphs. Soon after he was appointed full professor at the Technical University of Ilmenau, where he taught until his retirement in 1992. Until his very last days he showed up regularly at the faculty and was a constant part of academic life. The Zentralblatt lists 105 scientific publications, 62 of them single authored, constituting Sachs’ excellent reputation as a mathematician. Many of his papers deal with algebraic properties of graphs like for example the spectrum of their adjacency matrices; in fact, the best known one is perhaps the monograph Spectra of Graphs (1980) with D. M. Cvetković and M. Doob. His textbook Einführung in die Theorie der endlichen Graphen in two volumes (1970) was one of the very few comprehensive presentations of the state of the art in graph theory those days and a valuable source of inspiration for many researchers. Apart from pure mathematics, Sachs successfully cooperated with chemists (where he worked out interesting connections to graph theory) and physicists, and was interested in the history of mathematics. Sachs joined the editorial boards of eight journals and book series, among them Combinatorica, Journal of Combinatorial Theory (B), and the present one. He always made high demands on scientific quality as well as on clarity. Whereas most of us would enjoy a beautiful proof but consider a problem to be solved if there is just any correct proof, Sachs took a step further and considered a problem to be finally settled only if it comes with a beautiful solution, in terms of transparency and precision (and other, more subjective measures). We think that Bertrand Russel’s description of mathematical beauty, taken from A History of Western Philosophy, would fit very well here: Mathematics, rightly viewed, possesses not only truth, but supreme beauty — a beauty cold and austere, like that of sculpture, without appeal to any part of our weaker nature, without the gorgeous trappings of painting or music, yet sublimely pure, and capable of a stern perfection such as only the greatest art can show. Wewould like to thank the editorial board of Discrete Mathematics for enabling and supporting the present special issue to the memory of Horst Sachs. We are particularly indebted to Daniel Král’, who gave us a lot of help, advise and feedback.	adjacency matrix;bertrand (programming language);combinatorica;daniel gajski;diploma;discrete mathematics;graph theory;horst rittel;horst sachs;isoperimetric inequality;james d. sachs;linear algebra;linked list;network address translation;rca spectra 70;scientific literature;transparency (graphic);zentralblatt math	Thomas Böhme;Jochen Harant;Matthias Kriesell;Michael Stiebitz	2017	Discrete Mathematics	10.1016/j.disc.2017.04.023		Theory	-59.18434686728888	-20.53613885646206	189904
7f117d1e33e5a9ebe4b946848255dfea0e8fee1e	the st. thomas common sense symposium: designing architectures for human-level intelligence	artificial intelligent;common sense	once a principal goal in the field of artificial intelligence. But most researchers in recent years have retreated from that ambitious aim. Instead, each developed some special technique that could deal with some class of problem well, but does poorly at almost everything else. We are convinced, however, that no one such method will ever turn out to be “best,” and that instead, the powerful AI systems of the future will use a diverse array of resources that, together, will deal with a great range of problems. To build a machine that’s resourceful enough to have humanlike common sense, we must develop ways to combine the advantages of multiple methods to represent knowledge, multiple ways to make inferences, and multiple ways to learn. We held a two-day symposium in St. Thomas, U.S. Virgin Islands, to discuss such a project—to develop new architectural schemes that can bridge between different strategies and representations. This article reports on the events and ideas developed at this meeting and subsequent thoughts by the authors on how to make progress.	artificial intelligence;tridiagonal matrix algorithm	Marvin Minsky;Push Singh;Aaron Sloman	2004	AI Magazine		computer science;engineering;artificial intelligence;operations research	AI	-56.562621156303386	-20.55134188660583	190383
4bb0847d5303ce314bcc2473bc5fcd8da735d0b5	inter-institutional co-operation in multimedia production		"""The awesome power of multimedia technology gives a properly funded and resourced production the capacity to encompass and present an entire field of knowledge in some considerable depth, rather than an inevitably shallow treatment necessitated by the essentially-restrictive conventional media of books and television. Consider, for example, three classic BBC documentary series: Jonathan Miller's 'The Body in Question"""" (transmitted in 1978), David Attenborough's """"Life on Earth (tx 19791, and Michael Andrews' 'The Birth of Europe"""" (tx 1991). When we, as multimedia producers, consider the enormous budgets which must have been expended in producing these definitive examples of the television documentary, we can enjoy a 'flight of fancy' in thinking how we might have spent the same sums had we been asked to address the same subject through a multimedia rather than television production. Granted, we could not yet hope to achieve the same audience figures, but just imagine how much deeper we could have explored the subject, and how much more imaginatively it could have been presented."""	3d television;awesome;book	Simon I. Hill	1993			multimedia;business	HCI	-62.07423886800042	-21.821062480458203	190568
13e4f4426d332901dbd329a856d69d54b369a31d	computers, communications, and the fcc		I n Greek mythology , P r o m e t h e u s stole fire f r o m h e a v e n fo r m a n ' s use. To pun ish the h u m a n race, Zeus sen t P a n d o r a to e a r t h w i th a box enc los ing all the evils t h a t flesh is he i r to. W h e n P a n d o r a opened the box, she loosened the evils on the e a r t h . Today, anyone who t akes on a t a s k t h a t is p r e g n a n t w i th u n f o r e s e e n difficulties is said to have opened P a n d o r a ' s box. Th i s m a y fit a r ecen t decision of the F e d e r a l Communica t i ons Commiss ion to open a publ ic i n q u i r y in to the g r a y a r e a be tween compu te r s and communica t ions . I n November , the FCC asked severa l ques t ions of the people who make comput e r s and communica t i ons e q u i p m e n t a n d the people who use th i s equ ipment . He re a re some of the more i m p o r t a n t ques t ions : • Should compu te r services t h a t use communica t i ons l ines be r egu l a t ed by the F C C ? • Do communica t ions companies offer services t h a t meet the needs of compu te r use r s ? • Does the c u r r e n t sys tem of c h a r g i n g fo r communica t i ons r e s t r i c t compu te r services , and a re the r a t e s equ i tab le? • How will p r o p r i e t a r y d a t a s tored in compu te r s and t r a n s m i t t e d over commun ica t ions l ines be p ro tec ted? The FCC began the i n q u i r y because of the g r o w i n g i n t e r d e p e n d e n c y be tween compute r s and communica t i ons and because two i n d u s t r i e s a re d r i f t i n g into each o the r ' s ma rke t s . In one di rect ion, comp u t e r s a re p e r m i t t i n g such services as message sw i t ch ing be tween va r ious locat ions , a n ac t iv i ty he re to fo re l imi ted to communica t i ons firms. In the o the r direct ion, common c a r r i e r s a re g r a f t i n g E D P services onto t h e i r t r a d i t i o n a l role of p r o v i d i n g communica t ions services.	emoticon;fo (complexity);s-box;thinking outside the box	James P. Titus	1967	Commun. ACM	10.1145/363018.363064	theoretical computer science;computer engineering;computer science	AI	-59.94442487690824	-23.247166196040226	190620
e2ba27ebbb52f9cb6f6cb5d56c705caa9edd63fd	proceedings of the 2017 acm conference on economics and computation		The papers in these Proceedings were presented at the Eighteenth ACM Conference on Economics and Computation (ECu002717), held between June 26 and 30, 2017, at MIT in Cambridge, Massachusetts, USA. Since 1999 the ACM Special Interest Group on Electronic Commerce (SIGecom) has sponsored EC, the leading scientific conference on advances in theory, systems, and applications at the interface of economics and computation, including applications to electronic commerce.rnrnThe program committee has selected 75 papers from among 257 submissions that were received by February 13, 2017. Paper submissions were invited in the following three non-exclusive focus areas: rnTF: Theory and FoundationsrnAI: Artificial Intelligence and Applied Game TheoryrnEEA: Experimental, Empirical, and ApplicationsrnrnrnrnThe call for papers attracted 257 distinct submissions. Each paper was reviewed by at least three program committee members and two senior program committee members on the basis of significance, scientific novelty, technical quality, readability, and relevance to the conference. Following the tradition of recent iterations of the conference, the authors were asked to align their submission with one or two of the tracks. The next table summarizes the number of submissions and the number of accepted papers for each possible combination of tracks.rnrnOut of the 75 accepted papers, 35 papers are published in these Proceedings. For the remaining 40 papers, at the authorsu0027 request, only abstracts are included along with pointers to full working papers that the authors guarantee to be reliable for at least two years. This option accommodates the practices of fields outside of computer science in which conference publishing can preclude journal publishing. We expect that many of the papers in these Proceedings will appear in a more polished and complete form in scientific journals in the future.rnrnPapers were presented in two parallel sessions, with the exception of a plenary session with talks for the following award-winners: rnSIGecom Best Paper Award: Combinatorial Cost Sharing, by Shahar Dobzinski and Shahar OvadiarnSIGecom Doctoral Dissertation Award, by Peng ShirnrnA separate SIGecom Best Paper with a Student Lead Author Award was not given as the SIGecom Best Paper Award was given to a paper that has a student lead author.rnrnTo emphasize commonalities among the problems studied at EC, and to facilitate interchange at the conference, sessions were organized by topic rather than by focus area, and no indication of a paperu0027s focus area(s) was given at the conference or appears in these proceedings.rnrnECu002717 featured the following invited plenary talks: rnGraphons: A Nonparametric Method to Model, Estimate, and Design Algorithms for Massive Networks, by Jennifer Chayes (Microsoft Research)rnFair Algorithms for Machine Learning, by Michael Kearns (University of Pennsylvania)rnACM SIGecom Test of Time Award: Truth Revelation in Approximately Efficient Combinatorial Auctions, by Daniel Lehmann, Liadan Ita Ou0027Callaghan and Yoav Shoham (ACM EC 1999, Journal of the ACM 2002)rnrnrnrnIn addition to the main technical program, ECu002717 also featured four workshops: rnThe 3rd Workshop on Algorithmic Game Theory and Data Science. Organizers: Jean Honorio, Denis Nekipelov, Renato Paes Leme, Yaron Singer, Vasilis Syrgkanis, and Elie TamerrnWorkshop on Mechanism Design for Social Good. Organizers: Rediet Abebe and Kira GoldnerrnThe 12th Workshop on the Economics of Networks, Systems and Computation (NetEcon) PC Chairs: Vincent Conitzer and Roche GuerinrnForecasting Workshop. Organizers: Rafael Frongillo, David Rothschild, and Bo Waggonerrnrnrnrnfive tutorials: rnAdvances in Game Theory for Security and Privacy, presented by Bo An, Fei Fang, and Yevgeniy VorobeychikrnAn information Theoretical View of Information Elicitation Mechanisms, presented by Yuqing Kong and Grant SchoenebeckrnBargaining, Trading, and Coordination in Networks: Theory and Challenges, presented by Benjamin GolubrnIncentivizing and Coordinating Exploration, presented by Robert Kleinberg and Aleksandrs SlivkinsrnPricing in Combinatorial Markets: Equilibria and Prophet Inequalities, presented by Michal Feldman and Brendan Lucierrnrnas well as a poster session in which 30 posters were presented.		Constantinos Daskalakis;Moshe Babaioff;Hervé Moulin	2017		10.1145/3033274	special interest group;game theory;microeconomics;readability;computer science;algorithmic game theory;mechanism design;library science;publishing;combinatorial auction;rothschild	EDA	-61.38851883747449	-17.277244609476917	191457
c04187e9660a4c4a902d5de550e25b91e91f4681	mathematical and computational modeling of tonality - theory and applications		We may not be able to make you love reading, but mathematical and computational modeling of tonality theory and applications international series in operations research management science will lead you to love reading starting from now. Book is the window to open the new world. The world that you want is in the better stage and level. World will always guide you to even the prestige stage of the life. You know, this is some of how reading will give you the kindness. In this case, more books you read more knowledge you know, but it can mean also the bore is full.	book;computation;computational model;management science;operations research	Elaine Chew	2014		10.1007/978-1-4614-9475-1		NLP	-58.91200857538064	-22.907820573035252	192158
6cde750bb6d48b33aad110a1d0059ca7eebe9b32	recent coursebooks and articles of interest: bibliography update '98		I n his Book Review Editor's Message in the December 1997 issue of Computers and Society, Paul DePalma identified a number of recent books. The annotated list of recent coursebooks included below expands on, and does not duplicate, books identified in Paul's message. Also included is a select list of recent articles published since the last bibliography update appeared in Computers and Society. For a more comprehensive list of recent articles and books (1997-1998), you can access the above URL.	book	Herman T. Tavani	1998	SIGCAS Computers and Society	10.1145/277351.606074	knowledge management;bibliography;social science;computer science	ML	-61.61976717892193	-17.755682287634553	192401
e2fd8897b61ab620321cdd48df17a1e5d891f708	ordered sets and complete lattices	ordered set;complete lattice	These notes deal with an interconnecting web of mathematical techniques all of which deserve a place in the armoury of the welleducated computer scientist. The objective is to present the ideas as a self-contained body of material, worthy of study in its own right, and at the same time to assist the learning of algebraic and coalgebraic methods, by giving prior familiarization with some of the mathematical background that arises there. Examples drawn from computer science are only hinted at: the presentation seeks to complement and not to preempt other contributions to these ACMMPC Proceedings.	computer science;computer scientist	Hilary A. Priestley	2000		10.1007/3-540-47797-7_2	complete lattice;computer science;pure mathematics;mathematics;algorithm	Theory	-59.30450484476269	-20.77477689039481	192502
04225e1138b6765bfc30017fccafa064bef3c2c0	the prospects for the utilization of informational-logical machines in chemistry (ussr)	informational-logical machines	"""The extraordinary high tempo of scientific development, characteristic of our times, has caused an unprecedented growth in the number of published articles, patents, books, etc. The sharp rise in the stream of scientific information has become especially noticeable in chemistry, a science in which the number of described individual investigated objects, i.e., the number of chemical compounds , is already close to a million while the number of known practical chemical reactions reaches many millions. Hence it is no wonder that, in spite of the presence of such effective aids as handbooks, abstract journals and their indexes, chemists are already running into considerable difficulty when searching the literature for information required by them: Presently chemists in their daily work are limited in many cases by comparatively incomplete literature searches. This happens in view of the significant loss of time necessary for a careful search and for a profound study and comparison of all knowledge pertaining to a particular problem. This leads not only to a repetition of the research already described in the literature but also to an overall decreased productivity of scientific labor owing to which the achievements of valuable practical results are hindered and their introduction into industry are delayed. In relation to this state of affairs and owing to the fact that the labor involved with the chemical literature absorbs an ever-increasing portion of the chemist's working time, the idea has arisen to apply the achievements of modern technology to handle the rapid growth of productivity of this most important form of intellectual labor. The problem of creating information machines was presented to us in 1952 by Academician A. N. Nesmeyanov, and it represented an important phase of the problem of increasing the producitivity of scientific workers. The development of information machines depends on the resolution of a whole series of engineering and theoretical problems. The first thing necessary to accomplish this is the development of a particular device capable of storing large volumes of information over long periods of time, and of retrieving this information very rapidly and in random fashion. In other words, the most important problem is creating a machine """"memory"""" which holds a large volume of mateatul for a long time. Such computing devices should be created which"""	book	L. L. Gutenmakher;G. E. Vleduts	1961	J. ACM	10.1145/321062.321071	discrete mathematics;applied mathematics;computer science	DB	-62.33152744486219	-21.926247066505766	192688
bc4624a3b0bc3b03682d953da006e5d6a3e8d5fb	editorial: a message from the outgoing editor-in-chief and associate editor-in-chief		OUR three-year term at the helm of the IEEE Transactions on Mobile Computing (TMC) ended on 31 December 2013. It has been an honor, a privilege, and a great learning experience for both of us to serve this reputed publication. Being in this position, one gets a unique view of a large and rapidly evolving fi eld of inquiry. We have been fortunate to be in this position during a time of great evolution in mobile computing and have witnessed shifts in the focus of research within the fi el d: subfi elds that were intensely active when we took over are less so now, and new “hot” topics have emerged that are starting to capture the imagination of the research community. As researchers, it is not often that one gets to obtain this perspective fi rst hand, especially for a large fi eld such as mobile computing, and we are thankful to have been given the chance both to serve the community and experience its evolution. TMC is the top journal in the fi eld of wireless and mobile computing and enjoys an excellent reputation. For this reason, it also gets a large number of submissions. In 2012 alone, TMC received more than 950 original and revised submissions. The energetic response of our authors is one factor in the continued excellence of the journal, but there are several others. We have a wonderful core of Associate Editors who maintain the high standards of the journal despite the high editorial load, and a dedicated band of nearly 2,000 reviewers whose diligent feedback guides our editorial decisions and shapes the community’s research output. The success of such a large enterprise needs a dedicated team working behind the scenes, and TMC is fortunate to have outstanding staff support, which includes Hilda Carman, Jennifer Carruth, Kathleen Henry, Kimberly Sperka, and Alicia Stickley at the Computer Society, and Tara Delaney and Rachel Murray at Allen Press. Finally, TMC’s continued position as a top journal is due in no small part to our energetic Steering Committee, led by Mani Srivastava, whose vision has driven its evolution and excellence. We owe our deepest gratitude to all of these people, without whose support our stewardship of the journal would have been immeasurably harder. Finally, it gives us both great pleasure to introduce TMC’s new Editor-in-Chief, Professor Prasant Mohapatra of the University of California, Davis. Prasant is a top-notch researcher, well known for his contributions to a wide range of topics in wireless and mobile computing, including mesh networking, wireless security, multimedia, networked sensing, routing and cross-layer designs, and many more. In addition, he brings experience and wisdom gained from serving in positions of great responsibility at his university and within the mobile computing community. Knowing Prasant personally, we have no doubt that he will excel as Editor-in-Chief, and we wish him success in his effort to guide TMC to ever greater heights of excellence!	hierarchical editing language for macromolecules;ieee transactions on mobile computing;mesh networking;routing;traffic message channel;wireless security	Ramesh Govindan;Ram Ramanathan	2014	IEEE Trans. Mob. Comput.	10.1109/TMC.2014.4	world wide web;computer science;distributed computing	Visualization	-62.2928896760508	-17.79321832836279	193162
ae217248f56c32a5bf16c0bacd05483935635264	ad hoc networks editorial for 2012		We just completed the first decade of Ad Hoc Networks journal, since its creation in July 2003. For the first two volumes we published four issues per year. For Volumes 3 and 4, we increased the number of issues to six with four regular and two special issues per year. Given the continuously increasing number of submissions, we increased the number of issues to 8 per year starting with Volume 5. We have maintained the same publication rate since then. Over the past decade, Ad Hoc Networks journal has firmly established its reputation as a high standard scholarly journal in the ad hoc networking research community. The salient features of this journal, i.e., short turnaround time, rigorous review process, and coverage of timely research topics, have together contributed to persistent submissions of very high quality papers. This forward-growth is reflected in the total number of submissions, which tripled in the last seven years. The overall acceptance rate is approximately 20%. It is with gratitude and pleasure that we extend our hearty thanks to the authors, reviewers, guest editors, members of the editorial board, and readers who have made this possible. One of the most notable successes in the decade was that our journal has been accepted in Science Citation Index Expanded and Current	citation index;display resolution;hoc (programming language)	Ian F. Akyildiz	2013	Ad Hoc Networks	10.1016/j.adhoc.2013.03.001	vehicular ad hoc network	Visualization	-61.71464166390255	-18.031066205839643	193352
f05b54b20a92c7f73b46f3933e0ed6bc102bb675	new directions in empirical translation process research exploring the critt tpr-db		New Directions in Empirical Translation Process Research (Exploring the CRITT TPR-DB), edited by Michael Carl, Srinivas Bangalore, and Moritz Chaeffer, is a book consisting of a compilation of 13 scientific articles (plus a chapter for the introduction) that share one common feature: all of them revolve around the Translation Process Research Database1 (TPR-DB) published by the Centre for Research and Innovation in Translation and Translation Technology (CRITT). This book collects a series of empirical studies of human translator behaviour based on the transparent capture of activity information during a translation session—such as eye tracking, key logging, time spent in a translation task, etc.—and how this information can be used to extract conclusions regarding a given translation task, the translation technologies used, and the behaviour of the translators participating in the task. The book is divided in three parts: a first part that describes the TPR-DB, a second part that includes different studies on translation and post-editing carried out in the framework of the tool Casmacat (Alabau et al. 2013) which produced data available in the TPR-DB, and a third part that contains a collection of works focused on modelling the behaviour of translators by using the data contained in the TPR-DB. Part I: introduction. The first part of the book only contains two chapters: Chapter 1 introduces the book, and provides a short description of the remaining thirteen chapters; Chapter 2 provides an accurate description of the CRITT Translation Process Research Database (TPR-DB), including an exhaustive description of all the metrics used in it and how are they related. As mentioned above, this database is the leitmotif of the book; it is a unique resource that contains information about translators’	compiler;decibel;eye tracking;keystroke logging;postediting;scientific literature	Miquel Esplà-Gomis	2016	Machine Translation	10.1007/s10590-016-9179-6	media studies	NLP	-59.16116628905409	-17.610455626050065	193446
90a796e1d6255066e2b96e1fdd1831cdbcf1c0a1	trust but verify	mathematics;mathematics computing;complexity theory;moore s law;theorem proving;storage area networks;physics;physics quantum computing mathematics moore s law quantum mechanics visualization storage area networks power engineering computing complexity theory;visualization;power engineering computing;quantum mechanics;computational complexity;quantum computing	believe the following mathematical statement to be true, but I can’t prove it (yet).” The old phrase is better than the one that replaced it, because it highlights an important intermediate stage in the journey from conjecture to scientific fact. The believe-but-can’t-prove-yet development stage is essential to the creation of new mathematics, but it is not the final state. Experimental mathematics, including the dramatic use of computation to explore mathematical conjectures, has made this stage much more productive than it used to be. We can look back on some rather complex paperand-pencil proofs and discover that, because of modern computational capabilities, the most intricate part of such proofs might not be necessary today. This is not because the question could be settled purely by computation, but computation can help overcome barriers. To take one example, there is a famous fact of number theory called Bertrand’s postulate. It says that for every integer n, there is always a prime number p, larger than n but less than or equal to 2n. (The “equal to” is true for the case n = 1.) The first proof was apparently due to Chebyshev, and the standard proof is due to Erdos, who supposedly created a jingle to go along with his proof:	bertrand (programming language);computation;software release life cycle	Francis Sullivan	2002	Computing in Science & Engineering	10.1109/MCISE.2002.988640	computational science;storage area network;simulation;visualization;applied mathematics;quantum complexity theory;computer science;theoretical computer science;mathematics;automated theorem proving;moore's law;quantum computer;computational complexity theory;engineering mathematics;physics;algorithm;quantum mechanics	Theory	-58.55953294298441	-21.58356207984432	193891
31e217e6859da58c64e36a36bbf3f75908fd5ce7	a subjective visit to selected topics in distributed computing	distributed computing	After presenting a personal view of distributed computing (of course, being personal, this view is partial and questionable), this invited talk will address distributed computing problems that have recently received attention in the literature. For each of them, the talk presents the problem, results from the community, results from the author (and his co-authors), and questions that remain open. The following are among the topics covered in the talk.	distributed computing	Michel Raynal	2007		10.1007/978-3-540-75142-7_3	computer science;theoretical computer science;distributed computing;world wide web	HPC	-60.975099409839814	-18.045130437046957	194284
e325281bf0b14b36cb03c357261d0f12a26c8a3a	research on artificial intelligence ethics based on the evolution of population knowledge base		Population Knowledge Base Feng Liu1,2* , Yong Shi 1,2,3,4* 1Research Center on Fictitious Economy and Data Science, the Chinese Academy of Sciences, Beijing 100190, China 2The Key Laboratory of Big Data Mining and Knowledge Management Chinese Academy of Sciences,Beijing 100190, China 3College of Information Science and Technology University of Nebraska at Omaha, Omaha, NE 68182, USA 4School of Economics and Management, University of Chinese Academy of Sciences, Beijing 100190, China e-mail: liufeng@126.com,yshi@ucas.ac.cn Abstract:The unclear developmentdirection of human society is a deep reason for that it is difficult to form a uniform ethical standard for human society and artificial intelligence. Since the 21st century, the latest advances in the Internet, brain science and artificial intelligence have brought new inspiration to the researchon the developmentdirection of human society. Through the study of the Internet brain model, AI’s IQ evaluation, and the evolution of the brain, this paper proposes that the evolution of population knowledge base is the key for judging the development direction of human society,thereby discussing the standards and norms for the construction of artificial intelligence ethics.	academy;artificial intelligence;big data;cognitive science;data mining;data science;email;information science;internet;knowledge base;knowledge management	Feng Liu;Yong Shi	2018		10.1007/978-3-030-01313-4_48	computer science;the internet;knowledge base;population;artificial intelligence;brain model	AI	-57.58411496189099	-18.616498187711006	194450
dce955b24e3569b8e527e7e371fe2655dabbe48d	what's happening: computers and fun and seventh ifip conference on human-computer interaction	human computer interaction	"""Most of the research effort in HCI is aimed at the world of work, yet leisure is also a large part of people's lives. Computing and communication equipment is now within the price range of a mass market of home users who do not find surfing the web or playing arcade games of more than passing amusement. Educational technologists want their programs to be """" edu-tainment """". Home use of computing and Internet is growing (and has interesting implications for design, as shown by Robert Kraut and colleagues in their recent Communications of the ACM article. How can HCI help in the design of fun applications? This upcoming meeting is the first in an annual series of one-day meetings that will serve to define a community of HCI researchers and practitioners who wish to contribute to developing a better understanding of computers and fun. Papers are invited relevant to this theme. As this is a rather new area these may describe work in progress or """" think pieces """" as well as completed work. Possible topics include, but are not limited to: 2 Designing for fun 2 Theories of fun 2 Measuring fun 2 Fun in education 2 Electronic communication and fun Authors should submit extended abstracts (300-500 words, on paper only please) no later than September 18th,"""	arcade game;communications of the acm;computer;human–computer interaction;international federation for information processing;internet;robert	Andrew F. Monk	1998	Interactions	10.1145/285213.285214	human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-61.229196030601116	-20.91534990614871	195255
964a90f71f2c70ac8cc0d29d083872586f1e7c58	computational neuroscience: trends in research 2005	computational neuroscience	All papers were peer reviewed prior to the meeting and the final revised versions were submitted in September last year. After meetings in Chicago, USA and Alicante, Spain, the Computational Neuroscience Meeting returned to the east coast of the USA. Abstract submissions were similar to the Chicago meeting: 240 abstracts were submitted, of which 239 were accepted for presentation at the meeting. Like Alicante, the meeting in Baltimore consisted of three days of oral presentations and evening poster sessions, followed by two days of workshops. Meeting attendance was quite high with 377 registrations. The most significant changes to the CNS meeting affected directly the proceedings volume which you are reading now. First, we stopped producing a separate book volume of the proceedings. Although these books were very nice and appreciated by authors, they were too expensive for a low-cost meeting. The 2004 and following proceedings will appear only as a special issue of the journal Neurocomputing. Second, we reviewed the proceedings papers more seriously than before. All papers in this issue were reviewed by two independent reviewers (see list below) and the authors had to resubmit a revision based on the reviewers' comments. Of the 135 papers which were submitted for publication, 18 were rejected, some after resubmission. The meeting was held at the historic Lord Baltimore hotel which is close to the nicely renovated harbor area with lots of restaurants and nightlife. The conference dinner returned to the beautiful aquarium, which was also the site of the CNS*93 dinner but has changed quite a lot since. The local organization was excellent; we especially want to thank both Asaf Keller, who invested a lot of energy in solving the many small problems typical for a large meeting, and the friendly students who were always on hand to help.	book;cns;computation;computational neuroscience;high availability	Erik De Schutter	2005	Neurocomputing	10.1016/j.neucom.2004.11.032	computational biology;computer science;machine learning	HPC	-61.729529144752966	-18.616181260034367	195263
1adb938528dc17013b0b86483a355ce90a98319f	warning: statistical benchmarking is addictive. kicking the habit in machine learning	benchmarking;performance evaluation;statistical test;machine learning;algorithm evaluation;null hypothesis tests;hypothesis test	Algorithm performance evaluation is so entrenched in the machine learning community that one could call it an addiction. Like most addictions, it is harmful and very difficult to give up. It is harmful because it has serious limitations. Yet, we have great faith in practicing it in a ritualistic manner: we follow a fixed set of rules telling us the measure, the data sets and the statistical test to use. When we read a paper, even as reviewers, we are not sufficiently critical of results that follow these rules. Here, we will debate what are the limitations and how to best address them. This article may not cure the addiction but hopefully it will be a good first step along that road.	algorithm;emoticon;experiment;machine learning;nl-complete;performance evaluation	Chris Drummond;Nathalie Japkowicz	2010	J. Exp. Theor. Artif. Intell.	10.1080/09528130903010295	statistical hypothesis testing;simulation;artificial intelligence;machine learning;statistics	ML	-60.56023678641438	-21.822868446747098	195794
2398f3db21e60416feff2207e171c369c9004a5a	inflow 2015: the third workshop on interactions of nvm/flash with operating systems and workloa: inflow '15 message from the chairs	embedded operating systems;cache partitioning;scheduling;real time systems	We would like first and foremost to thank the authors who chose to submit their papers to INFLOW ’15, who are responsible for ensuring that this workshop continues to represent some of the most cutting-edge research in storage systems. We are also grateful to the program committee for their efforts, providing three to four high-quality reviews for each paper, and to ACM SIGOPS for their support of this workshop. Finally we would like to thank the attendees at INFLOW ’15, who attended despite the concurrent SOSP History Day.	foremost;non-volatile memory;symposium on operating systems principles	Peter Desnoyers;Gokul Kandiraju	2015	Operating Systems Review	10.1145/2883591.2883596	embedded operating system;parallel computing;real-time computing;computer science;operating system;scheduling	Visualization	-60.38992026233009	-17.664396250522014	195961
3d4dfead10533777cc6161990b27fb5a16d48f05	predicate abstraction in program verification: survey and current trends	004;program verification model checking predicate abstraction refinement	Shane Legg left academia to cofound DeepMind Technologies in 2010, along with Demis Hassabis and Mustafa Suleyman. Their vision was to bring together cutting edge machine learning and systems neuroscience in order to create artificial agents with general intelligence. Following investments from a number of famous technology entrepreneurs, including Peter Thiel and Elon Musk, they assembled a team of world class researchers with backgrounds in systems neuroscience, deep learning, reinforcement learning and Bayesian statistics. In early 2014 DeepMind made international business headlines after it was acquired by Google. In this talk Shane covers some of the history behind DeepMind, his experience making the transition from academia to industry, how Google DeepMind performs research and finally some demos of the artificial agents that are under development. 1998 ACM Subject Classification I.2.11 Intelligent Agents	deep learning;formal verification;intelligent agent;machine learning;predicate abstraction;reinforcement learning;systems neuroscience	Jakub Daniel;Pavel Parizek	2014		10.4230/OASIcs.ICCSW.2014.27	computer science;theoretical computer science;database;programming language;abstraction model checking;symbolic trajectory evaluation	AI	-56.44698699722214	-19.62051799264136	197055
a8de2709a16da5f50915a4b2053ce2443dcb633a	amast'91 banquet talk	banquet talk	"""Tuesday, May 7, 1991. I sign the last grade sheet and smile at the spring sun. Finally the semester is over. A message from Teo Rus arrives. \The second conference on Algebraic Methodology and Software Technology needs a banquet speaker"""", writes Teo. I am very attered. And scared. I recall a recent banquet talk in Ann Arbor. The man went on and on. I left before he nished. On the other hand, the invitation is a challenge and an opportunity. You know, sometimes we feel like philosophers if only anybody would listen. I accept the invitation before the scare gets a hold of me. I leave my o ce and meet Kevin Compton, another member of the small computer theory group in our huge Department of Electrical Engineering and Computer Science. \How are you?"""" asks Kevin. \Well, I was ne only a few minutes ago"""", and I tell him about the invitation to give a banquet talk. \I do not envy you"""", says Kevin. Soon a message from him tells me about 5 books on public speaking in the library. I thumb the books. They have witty things on almost any subject, but do not mention algebra or software, let alone algebraic methodology and software technology. The volumes of humor are depressing. This is not it. Teo could nd a professional joker to entertain the conference. At that time in Iowa it could be a national politician. After thinking it over, I decide to take a scienti c approach and write a scholarly paper. You know, another paper never hurts your vita. The scienti c approach explains the use of \we"""" in the sequel."""	book;brownfield (software development);computer science;linear algebra;tag (game);theory of computation	Yuri Gurevich	2001			banquet;art;performance art	SE	-60.02620435754567	-23.316168154131248	197650
2e0f245c2717f47ebd668c59907ffcecd4f54af8	“… or is the question of being at once the most basic and the most concrete?” on the ambitions and responsibilities of contemporary american philosophy	everyday life	At its centennial in 2001, the American Philosophical Association bravely proclaimed: “Philosophy Matters.” But does it? It won’t unless it reaches the concreteness of everyday life. To do so was Martin Heidegger’s ambition, and one can read Saul Kripke’s books as an attempt to get mainstream American philosophy beyond its abstractions. At length, Kripke’s efforts, on one reading, failed while Heidegger’s remained incomplete. A theory of commodification can get us closer to the things that matter to us in everyday life.	book	Albert Borgmann	2009	AI & SOCIETY	10.1007/s00146-009-0238-x	social science;computer science;artificial intelligence;sociology;law	AI	-59.89574158934218	-22.508518193061583	199060
1b291b5222051d1aa93cd9e4c140feb828d891dc	new vision and goals of informatics and megachallenges of mankind. extended abstract		Currently dominating perception of computer science has its origin in a very cleverly written, and much influential, paper of Newel, Simon and Perlis, published in Science in 1967, that well captured the perception of the field at that time. The basic ideas presented in their paper were: ”Whenever there are phenomena there can be a science dealing with these phenomena. Phenomena breed sciences. Since there are computers, there is computer science. The phenomena surrounding computers are varied, complex and rich.” Since that time there have been numerous attempts to modernize such a view of computer science. Some of them centered around a slightly modified name of the field as computing science with attempts to put emphases on algorithms, programming and software instead of hardware. Some of the first along these lines was Dijkstra with his position: ”Computing science is and will always be concerned with the interplay between mechanized and human symbol manipulation usually referred to as ”computing”, and ”programming”, respectively. It is located in the direction of formal mathematics and applied logic, but ultimately, far beyond where those are now.” This was underlined by his famous, satiric but deep vision that Computer science is as much about computers as astronomy is about telescopes. In spite of all these attempts, computer-centric or computingcentric view of computer science still dominates.	algorithm;computer science;informatics;lawrence m. breed	Jozef Gruska	2013	The Computer Science Journal of Moldova		discrete mathematics;applied mathematics;informatics;artificial intelligence;computer science	Theory	-57.08496882660831	-22.2933531189741	199215
5350b74141fbcc5f579bcddb21ebc26820491511	grand challenges for constraint programming	grand challenges in constraint programming;constraint programming;big data	Every field should have its Grand Challenges. After discussing some general “why and how” issues, with brief reference to some sample challenges, we devote attention to the challenges raised by the new world of “BigData” and to some new ways of approaching the classic Grand Challenge of the Holy Grail (where one merely states the problem and the computer solves it). There can, of course, never be a definitive catalogue of Grand Challenges. The ultimate Grand Challenge is for everyone working on Constraint Programming to look up on occasion from their everyday pursuits to consider how they might contribute to a Grand Challenge, and even to try their hand at formulating their own Grand Challenges.	big data;constraint programming;grand challenges;stepping level	Eugene C. Freuder;Barry O'Sullivan	2013	Constraints	10.1007/s10601-013-9155-1	simulation;artificial intelligence;algorithm	AI	-57.93926205732441	-22.48185844525846	199270
c205bb3e04e60715d01fbadaa6e160bc71866b2e	high performance embedded architectures and compilers, 5th international conference, hipeac 2010, pisa, italy, january 25-27, 2010. proceedings		That's it, a book to wait for in this month. Even you have wanted for long time for releasing this book high performance embedded architectures and compilers 5th international conference hipeac 2010 pisa italy january 25 27 2010 proceedings lecture notes in computer science; you may not be able to get in some stress. Should you go around and seek fro the book until you really get it? Are you sure? Are you that free? This condition will force you to always end up to get a book. But now, we are coming to give you excellent solution.	compiler;embedded system;lecture notes in computer science		2010		10.1007/978-3-642-11515-8	computer science	HPC	-58.28955298830586	-23.01877894228216	199324
065e6adb04ed5e3e976bb49794d2800534ac63ea	computer stylometry of c. s. lewis's the dark tower and related texts		10 generally attributed to C. S. Lewis. The manuscript was purportedly rescued from a bonfire shortly after Lewis’s death by his literary executor Walter Hooper, but the quality of the text is hardly vintage Lewis. Using computer stylometric programs made available by Eder et al.’s (2016: Stylometry with R: A package for computational text analysis. R Journal, 8(1): 107–21) ‘stylo’ package and a word	stylometry;the r journal	Michael P. Oakes	2018	DSH	10.1093/llc/fqx043	tower;stylometry;computer science;artificial intelligence	AI	-60.488662586200746	-20.142082581184006	199732
