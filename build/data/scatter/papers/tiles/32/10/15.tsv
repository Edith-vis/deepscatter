id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4498c8f0fc0e9c5cecb1f49518801c1ef20eb362	metadata in the collaboratory for multi-scale chemical science	metadata;knowledge management;metadata extraction;domain knowledge;mathematical methods and computing;webdav;science communication;information systems chemistry metadata knowledge management collaboratory dublin core webdav;information dissemination;chemistry;collaboratory;coordinated research programs;information system;dublin core	The goal of the Collaboratory for the Multi-scale Chemical Sciences (CMCS) [1] is to develop an informatics-based approach to synthesizing multi-scale chemistry information to create knowledge in the chemical sciences. CMCS is using a portal and metadata-aware content store as a base for building a system to support inter-domain knowledge exchange in chemical science. Key aspects of the system include configurable metadata extraction and translation, a core schema for scientific pedigree, and a suite of tools for managing data and metadata and visualizing pedigree relationships between data entries. CMCS metadata is represented using Dublin Core with metadata extensions that are useful to both the chemical science community and the science community in general. CMCS is working with several chemistry groups who are using the system to collaboratively assemble and analyze existing data to derive new chemical knowledge. In this paper we discuss the project’s metadata-related requirements, the relevant software infrastructure, core metadata schema, and tools that use the metadata to enhance science.	dublin core;informatics;inter-domain;requirement	Carmen M. Pancerella;John C. Hewson;Wendy S. Koegler;David Leahy;Michael Lee;Larry A. Rahn;Christine L. Yang;James D. Myers;Brett T. Didier;Renata McCoy;Karen Schuchardt;Eric G. Stephan;Theresa L. Windus;Kaizar Amin;Sandra Bittner;Carina Lansing;Michael Minkoff;Sandeep Nijsure;Gregor von Laszewski	2003			metadata modeling;computer science;data science;database;metadata;world wide web;data element;meta data services;metadata repository	HPC	-42.24252769105364	1.7094798605394221	194044
42ddabeca3bf38540db77d94d69bb83ee0b68b93	the generation of faceted classification schemes for use in the organisation of engineering design documents	multiple perspectives;gestion informacion;gestion des connaissances;engineering design;faceted classification;classification facette;query formulation;knowledge management;clasificacion colonada;information management;gestion information;gestion conocimiento	Vast quantities of electronic information are generated and stored such that engineers may later retrieve, assimilate and ultimately utilise such information during their daily activities. Where term-based querying relies upon suitable query formulation, browsing of pre-organised information allows the information to be displayed against a descriptive structure. Traditional classification schemes generally cater for certain viewpoints, however faceted schemes concurrently describe multiple perspectives. The literature specifying the creation of such schemata is sparse, and distributed across different domains and eras. This paper reviews this literature and proposes a means by which the underlying theory may be more readily applied within engineering design.	engineering design process;faceted classification	Matt Giess;Peter J. Wild;Chris A. McMahon	2008	Int J. Information Management	10.1016/j.ijinfomgt.2007.10.001	computer science;knowledge management;artificial intelligence;data mining;information management;world wide web	DB	-44.22482365855118	0.2767861222160589	195275
b7d3fb5564bcbda3b79b7bf2ec8277635311c0e4	enhancing sciencv through semantic research profile integration with the vivo-isf ontology		SciENcv [1], the US Federal Science Experts Network Curriculum Vitae, is an online system for simplifying the creation of researcher biographical sketches or biosketches, which are required when applying for federal funding. SciENcv profiles are curated and controlled by researchers themselves they own the data, they control what data are public, and they edit and maintain the information contained within the profiles, which includes expertise, employment history, educational background, and professional accomplishments. The system leverages data from myNCBI and eRA Commons, and includes links to ORCiD [2] researcher identifiers. The structure of SciENcv biosketches is defined by an XML Schema Definition and profiles can be downloaded in XML format. The system aims to eliminate the need for researchers to repeatedly enter biosketch information and reduce the administrative burden associated with federal grant submission and reporting requirements, as well as creating a repository of researcher profile data where researchers can describe their scientific contributions in their own language.	identifier;ink serialized format;requirement;xml schema;on-line system	Marijane White;Matthew H. Brush;Shahim Essaid;Robin Champieux;Adrienne Zell;Melissa Haendel;Colin Grove;Syeda Momina Tabish;David Eichmann	2016			semantic integration;ontology-based data integration;information retrieval;ontology;data mining;computer science	DB	-43.10712759275888	1.6372239233037695	195473
fe56e8819fe12978a88ad7b68568117f2d18a1ed	nanoparticle ontology for cancer nanotechnology research	software;animals;nanoparticles;vocabulary controlled;medical informatics;bioportal;cancer;design and development;chemical composition;database management systems;semantic integration;annotation;nanotechnology;nanomaterial;basic formal ontology;npo;medical oncology;informatics;humans;ontology web language;bfo;computational biology;nanoparticle;ontology;ontology design;knowledge base	Data generated from cancer nanotechnology research are so diverse and large in volume that it is difficult to share and efficiently use them without informatics tools. In particular, ontologies that provide a unifying knowledge framework for annotating the data are required to facilitate the semantic integration, knowledge-based searching, unambiguous interpretation, mining and inferencing of the data using informatics methods. In this paper, we discuss the design and development of NanoParticle Ontology (NPO), which is developed within the framework of the Basic Formal Ontology (BFO), and implemented in the Ontology Web Language (OWL) using well-defined ontology design principles. The NPO was developed to represent knowledge underlying the preparation, chemical composition, and characterization of nanomaterials involved in cancer research. Public releases of the NPO are available through BioPortal website, maintained by the National Center for Biomedical Ontology. Mechanisms for editorial and governance processes are being developed for the maintenance, review, and growth of the NPO.		Dennis G. Thomas;Rohit V. Pappu;Nathan A. Baker	2011	Journal of biomedical informatics	10.1016/j.jbi.2010.03.001	upper ontology;health informatics;open biomedical ontologies;knowledge base;bibliographic ontology;computer science;bioinformatics;knowledge management;ontology;ontology;data mining;database;nanoparticle;ontology-based data integration;owl-s;process ontology;suggested upper merged ontology	Web+IR	-41.29743280217519	1.9879799325390377	197324
20267cdbd20806db11e3bc7e8dc58d935cec2267	metadata and semantics research		This paper presents research examining metadata capital in the context of the Viral Vector Core Laboratory at the National Institute of Environmental Health Sciences (NIEHS). Methods include collaborative workflow modeling and a metadata analysis. Models of the laboratory’s workflow and metadata activity are generated to identify potential opportunities for defining microservices that may be supported by iRODS rules. Generic iRODS rules are also shared along with images of the iRODS prototype. The discussion includes an exploration of a modified capital sigma equation to understand metadata as an asset. The work aims to raise awareness of metadata as an asset and to incentivize investment in metadata R&D.	application programming interface;bernard greenberg;digital data;download;dryad;dublin core;identifier;json;java;linked data;matching (graph theory);microservices;parsing;prototype;semiconductor industry;storage resource broker;world wide web	Sissi Closs;Rudi Studer;Emmanouel Garoufallou;Miguel-Ángel Sicilia	2014		10.1007/978-3-319-13674-5	world wide web;computer science;information retrieval;metadata;data mining;semantics;information system;modular design;originality;design methods	HPC	-41.89304000181849	3.962281868001668	198013
3730ab002688d5d98dbc95d0f595d2b3622d0ffd	a platform for research on process model collections		Business process management has received considerable attention and many companies achieved a high maturity level and hence, generated collections of process models that form a knowledge asset essential to their operations. These collections bear opportunities for innovation: Empirical research establishes methods and techniques to support and improve business process management; yet, these need to be validated with regards to process models from industry. However, due to their heterogeneity, extracting and analyzing process models from process model collections is a tedious task and time consuming. To facilitate access to process model collections, this paper presents an extensible platform for their analysis that shall support researchers and foster collaboration and reuse. This platform provides importing functionality for a set of process collections recognized in research and functionality to easily explore, transform, and extract information from process model collections. In a small showcase, we illustrate the application of the platform towards clustering a collection of process models.	beam propagation method;business process;capability maturity model;cluster analysis;ecosystem;experiment;importer (computing);openness;process modeling;reference model;repeatability;usability testing	Rami-Habib Eid-Sabbagh;Matthias Kunze;Andreas Meyer;Mathias Weske	2012		10.1007/978-3-642-33155-8_2	library science;data science;world wide web	Web+IR	-42.93202249649804	-0.09041985556300759	198404
b6cc12fc9e6817a6cb6e3a3d66d7824f6d229733	report on the second international workshop on self-managing database systems (smdb 2007)	database system	Information management systems are growing rapidly in scale and complexity, while skilled database administrators are becoming rarer and more expensive. Increasingly, the total cost of ownership of information management systems is dominated by the cost of people, rather than hardware or software costs. This economic dynamic dictates that information systems of the future be more automated and simpler to use, with most administration tasks transparent to the user. Autonomic, or self-managing, systems are a promising approach to achieving the goal of systems that are increasingly automated and easier to use. The aim of the workshop was to provide a forum for researchers from both industry and academia to present and discuss ideas related to self-managing database systems. SMDB 2007 was the first event organized by the new IEEE Computer Society Data Engineering Workgroup on Self-Managing Database Systems (http : //db.uwaterloo.ca/tcde−smdb/). The workgroup, which was founded in October 2005, is intended to foster research aimed at enabling information management systems to manage themselves seamlessly, thereby reducing the cost of deployment and administration.	autonomic computing;database;hypertext transfer protocol;information engineering;information management;information system;self-management (computer science);software deployment;total cost of ownership	Anastasia Ailamaki;Surajit Chaudhuri;Sam Lightstone;Guy M. Lohman;Patrick Martin;Kenneth Salem;Gerhard Weikum	2007	IEEE Data Eng. Bull.		computer science;database	DB	-48.17695022511788	2.857112374413116	199169
264fdcb9deb86977d9afb9cfe682e4b76f30fb48	standards for language technology - relevance and impact		While there is common concensus that IT industry at large hardly works without international standards, it is far less obvious that language technologies for practical (industrial) as well as for research purposes should also heavily rely on international standards. In addition, there are several communities and organisations that do work on standards for language industry and for computational linguistics, which has led to some degree of fragmentation and lack of cooperation. The paper reviews the state-of-the-art of global standardization efforts for language resources and language technologies and identifies most urgent work items and most pressing needs for inter-organisational and crosscommunity collaboration efforts in order to achieve a higher degree of interoperability of formats, schemata, web services, data models, etc. for the wide spectrum of language technologies, the main goal of these efforts being to achieve a much higher impact of language technology standards in research as well as in industry.	computational linguistics;data model;fragmentation (computing);interoperability;language industry;language technology;relevance;web service	Gerhard Budin	2012			computer science;systems engineering;knowledge management;language industry;management science	DB	-45.83360588108026	3.3013076721953665	199480
7dd86ff8dda5971398aa4a76b01b447d9dc38fd4	biggorilla: an open-source ecosystem for data preparation and integration		We present BIGGORILLA, an open-source resource for data scientists who need data preparation and integration tools, and the vision underlying the project. We then describe four packages that we contributed to BIGGORILLA: KOKO (an information extraction tool), FLEXMATCHER (a schema matching tool), MAGELLAN and DEEPMATCHER (two entity matching tools). We hope that as more software packages are added to BIGGORILLA, it will become a one-stop resource for both researchers and industry practitioners, and will enable our community to advance the state of the art at a faster pace.	data science;ecosystem;information extraction;open-source software	Chen Chen;Behzad Golshan;Alon Y. Halevy;Wang Chiew Tan;AnHai Doan	2018	IEEE Data Eng. Bull.		data mining;data preparation;computer science;ecosystem	DB	-43.209195051289385	-0.582660057750146	199519
efbb6163b1f64bc1ea5d60ad1ecf42df35af8432	geospatial cyberinfrastructure: past, present and future	challenges;documento electronico;outil logiciel;cyberspace;geographic information science;decision support;software tool;case history;architecture systeme;digital earth;network protocol;base de connaissances;aplicacion;realite virtuelle;realidad virtual;spatial data;geographic information;analisis datos;technology;e science;information technology;non government organization;donnee spatiale;data management;virtual reality;estrategia;journal article;sensor network;dato espacial;data model;computer network;strategy;document electronique;etat actuel;prospectiva;knowledge;data analysis;biblioteca electronica;information flow;research and development;ciberespacio;historique;prospective;sensor networks;system;state of the art;virtual organization;technologie;geospatial science;data access;cyberinfrastructure;semantic web;base conocimiento;sdi;analyse donnee;middleware;arquitectura sistema;estado actual;electronic library;virtual organizations;collaborative research;system architecture;application;spatial computing;herramienta software;citizen participation;strategie;cyberespace;geospatial data;bibliotheque electronique;electronic document;framework;cloud computing;knowledge base;tecnologia;estudio historico	A Cyberinfrastructure (CI) is a combination of data resources, network protocols, computing platforms, and computational services that brings people, information, and computational tools together to perform science or other data-rich applications in this information-driven world. Most science domains adopt intrinsic geospatial principles (such as spatial constraints in phenomena evolution) for large amounts of geospatial data processing (such as geospatial analysis, feature relationship calculations, geospatial modeling, geovisualization, and geospatial decision support). Geospatial CI (GCI) refers to CI that utilizes geospatial principles and geospatial information to transform how research, development, and education are conducted within and across science domains (such as the environmental and Earth sciences). GCI is based on recent advancements in geospatial information science, information technology, computer networks, sensor networks, Web computing, CI, and e-research/e-science. This paper reviews the research, development, education, and other efforts that have contributed to building GCI in terms of its history, objectives, architecture, supporting technologies, functions, application communities, and future research directions. Similar to how GIS transformed the procedures for geospatial sciences, GCI provides significant improvements to how the sciences that need geospatial information will advance. The evolution of GCI will produce platforms for geospatial science domains and communities to better conduct research and development and to better collect data,	communications protocol;cyberinfrastructure;decision support system;e-science;geographic information system;geospatial analysis;geospatial predictive modeling;geovisualization;google code-in;information science	Chaowei Phil Yang;Robert Raskin;Michael F. Goodchild;Mark Gahegan	2010	Computers, Environment and Urban Systems	10.1016/j.compenvurbsys.2010.04.001	wireless sensor network;geography;computer science;engineering;artificial intelligence;data science;geospatial analysis;data mining;database;world wide web;cartography;remote sensing;geographic information systems in geospatial intelligence	HPC	-44.64883857235823	-0.3161338578107053	199784
