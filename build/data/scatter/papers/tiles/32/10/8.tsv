id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
1a4a55351ce3312b53ea48412805f131382d66cd	vibrotactile feedback in steering wheel reduces navigation errors during gps-guided car driving	car navigation systems;automobiles;vibrations;gps based voice command vibrotactile feedback navigation error gps guided car driving car steering wheel driver perceptual load driver cognitive load vibration motors driving simulator driving experiment virtual environment sensory condition passenger auditory noise;gps based voice command;navigation driver circuits wheels noise haptic interfaces vibrations visualization;driving simulator;haptics;steering wheel;gps guided car driving;driving simulator vibrotactile haptics car navigation systems gps steering wheel;driver perceptual load;navigation;visualization;feedback;gps;global positioning system;vibrations automobiles driver information systems feedback global positioning system haptic interfaces steering systems;vibrotactile feedback;driver circuits;car steering wheel;navigation system;cognitive load;vibrotactile;virtual environment;haptic interfaces;vibration motors;navigation error;driving experiment;driver information systems;driver cognitive load;passenger auditory noise;noise;wheels;steering systems;haptic interface;sensory condition	We show that vibrotactile feedback displayed through the steering wheel of a car can reduce the perceptual and cognitive load of the driver, leading to less distraction and fewer navigation errors. To demonstrate the concept, two vibration motors are mounted onto the steering wheel of a driving simulator and driving experiments are performed in virtual environments under two different sensory conditions (auditory alone and auditory and vibrotactile feedback together). The results of our experiments with 12 subjects show that, if passenger auditory noise and distraction exist in the environment, the navigation errors (making a wrong turn or taking a wrong exit) are reduced when vibrotactile feedback is displayed to the users in tandem with the GPS-based voice commands.	driving simulator;experiment;feedback;global positioning system;simulation;steering wheel;virtual reality	Enes Selman Ege;Furkan Cetin;Cagatay Basdogan	2011	2011 IEEE World Haptics Conference	10.1109/WHC.2011.5945510	embedded system;simulation;global positioning system;computer science;engineering;artificial intelligence;automotive engineering;haptic technology	Robotics	-46.37078755611283	-47.23382724881358	160312
1aee66bd20099d43866109694e43742ff6a74329	investigating the effects of vibrotactile feedback on human performance in navigation tasks		Abstract Vibrotactile feedback which can provide real, comfortable and effective interaction experiences has become indispensable in human computer interaction. Many researchers working with vibrotactile feedback in navigation scenarios have proposed a variety of different methods of vibration. However, there is little work that has explored the relationship between navigation speed and vibration feedback both in intensity and modes. In order to find the answer, we implemented a prototype supplying with several vibration feedback modes and designed two exploratory studies to evaluate the human performance with different navigation speeds. We found using a hand-held vibration feedback device can reduce the workload of visual and auditory channels. The results also show that the speed of navigation has a significant influence on the rate of vibration mode identification. This paper contributes to the basic understanding of vibration feedback and offers implications for the future design of vibration feedback in navigation scenarios.	human reliability	Minghui Sun;Wenzhao Gu;Limin Wang;Liyan Dong;Qian Qian	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2018.01.017	workload;real-time computing;control engineering;navigation system;computer science;communication channel;user interface;vibration	HCI	-47.25105978294198	-47.07464360649166	160602
0a0595e19973f5a277ab22b887ce2a9edd705c98	spatial stimulus-response compatibility for hand and foot controls with vertical plane visual signals	evaluation performance;interfase usuario;performance evaluation;spatial compatibility;user interface;evaluacion prestacion;stimulus response compatibility;right handed;foot controls;hand controls;human machine interface;interface utilisateur;visual signals	Most of the previous studies on spatial S–R compatibility have been limited to the use of hand controls with visual signals on horizontal displays. This study examined the performance of 38 right-handed and right-footed participants in a four choice spatial stimulus–response (S–R) compatibility task with the use of hand and foot controls to respond to visual signals in a vertical plane. The hand and foot dominance of participants was assessed with the Lateral Preference Inventory of Coren (1993). There was a strong spatial S–R compatibility effect in the task revealed by the significant interaction of visual signal position and response key position. The spatial compatibility effect for the above/below dimension was found to be significantly stronger than for the right/left dimension, and no prevalence of right/left over above/below spatial cue was observed in this study. In addition, S–R compatibility produced larger magnitude effect for the top signals compared with bottom ones. As expected, participants responded faster with hands than with feet, and responses from the right foot were faster than those from the left foot for the right-handed/footed participants tested here. The results of this study provide vital information for designing more effective human–machine interfaces with hand and foot controls interacting with vertical plane visual signals. 2011 Elsevier B.V. All rights reserved.	human factors and ergonomics;human–machine system;interaction;lateral computing;lateral thinking;right-to-left;stimulus–response compatibility	Ken W. L. Chan;Alan H. S. Chan	2011	Displays	10.1016/j.displa.2011.02.006	human–machine interface;simulation;computer science;engineering;operating system;user interface;engineering drawing	HCI	-46.104083351722544	-47.25045578753187	161251
b4617ec38af7633056ba9d1995747e4968aab90c	design process of sonically-enhanced air gesture controls in vehicles		Touchscreen use in vehicles introduces conflict for the visual attention of drivers. This conflict increases crash risk and has been a subject of concern among driving researchers for many years. In this paper, we introduce the design process of sonically-enhanced air gesture controls to overcome drivers' visual distraction. Design applications and future research directions are discussed.	touchscreen	Seyedeh Maryam Fakhrhosseini;Jason Sterkenburg;Steven Landry;Joseph D. Ryan;Myounghoon Jeon	2017		10.1145/3131726.3131874	human–computer interaction;engineering;simulation;distraction;engineering design process;touchscreen;design process;gesture;crash	HCI	-46.86733354938625	-51.88983936437391	161769
a8507f600e13b9931afb634b56f72f743a4a731c	"""taskeye: """"a novel approach to help people interact with their surrounding through their eyes"""""""		"""In this paper, we've proposed a Human Computer Interaction System based on eye tracking which we called it as """"taskEYE"""". The """"taskEYE"""" is an Eye Ball Tracking System which is intended to assist patients that cannot perform any voluntary tasks related to daily life. Patients with spinal injuries or other with severe disabilities, who only can control their eyes can still communicate with the world using the assistive devices like one proposed. This device provides a human computer interface in order to take decisions based on their eye movement."""	assistive technology;eye tracking;human computer;human–computer interaction;tracking system	Kunal Kavale;Kiran Kokambe;Sunita Jadhav	2018	2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)	10.1109/ICALT.2018.00078	multimedia;simulation;task analysis;computer science;tracking system;eye tracking;eye movement	Robotics	-42.57694482912434	-45.66189827661238	162148
e788e3ecd0d89b0afa30bed04073a6e945695735	effect of touch screen button size and spacing on touch characteristics of users with and without disabilities	pedestrian safety;touch screen;poison control;injury prevention;safety literature;traffic safety;injury control;force;dwell time;home safety;disability;injury research;safety abstracts;human factors;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;impulse;ergonomics;suicide prevention;human computer interface;motor control	OBJECTIVE The aim of this study was to investigate the effect of button size and spacing on touch characteristics (forces, impulses, and dwell times) during a digit entry touch screen task. A secondary objective was to investigate the effect of disability on touch characteristics.   BACKGROUND Touch screens are common in public settings and workplaces. Although research has examined the effect of button size and spacing on performance, the effect on touch characteristics is unknown.   METHOD A total of 52 participants (n = 23, fine motor control disability; n = 14, gross motor control disability; n = 15, no disability) completed a digit entry task. Button sizes varied from 10 mm to 30 mm, and button spacing was 1 mm or 3 mm.   RESULTS Touch characteristics were significantly affected by button size. The exerted peak forces increased 17% between the largest and the smallest buttons, whereas impulses decreased 28%. Compared with the fine motor and nondisabled groups, the gross motor group had greater impulses (98% and 167%, respectively) and dwell times (60% and 129%, respectively). Peak forces were similar for all groups.   CONCLUSION Button size but not spacing influenced touch characteristics during a digit entry task. The gross motor group had significantly greater dwell times and impulses than did the fine motor and nondisabled groups.   APPLICATION Research on touch characteristics, in conjunction with that on user performance, can be used to guide human computer interface design strategies to improve accessibility of touch screen interfaces. Further research is needed to evaluate the effect of the exerted peak forces and impulses on user performance and fatigue.	accessibility;digit structure;fatigue;human computer;human–computer interaction;interface device component;largest;motor neuron disease;spacing;touchscreen;user-computer interface;physical hard work	Mary E. Sesto;Curtis B. Irwin;Karen B. Chen;Amrish O. Chourasia;Douglas A. Wiegmann	2012	Human factors	10.1177/0018720811433831	psychology;impulse;motor control;simulation;medicine;environmental health;human–computer interaction;engineering;suicide prevention;human factors and ergonomics;injury prevention;dwell time;forensic engineering;computer security;force;mechanical engineering	HCI	-47.27980266842108	-46.54775659753112	162160
af016f34b5e144d08bc828b4e7eaf46617fb3932	the utility of perspecta 3d volumetric display for completion of tasks	image tridimensionnelle;evaluation performance;performance evaluation;liquid crystal display;learning;display equipment;evaluacion prestacion;liquid crystal displays;affichage 3 dimensions;stereoscopy;affichage ecran plat;qualite image;aprendizaje;human subjects;lcds;apprentissage;pantalla plana;three dimensional displays;flat panel displays;image quality;3d volumetric displays;equipement affichage;stereoscopie;tridimensional image;calidad imagen;estereoscopia;equipo visualizacion;flat screen;ecran plat;affichage cristaux liquides;imagen tridimensional	This paper explores the hypothesis that the depth c ues and display quality of a 3D volumetric display provides advantages for learning simple tasks. Experimental data generated by human subjects using the Perspect a 3D Volumetric Display are compared to like data generated using a 2D flat scr een liquid crystal display (LCD). These data show that the Perspecta display provides advantages over the LCD display with respect to peak performance of sim ple tasks. Introduction The ability to understand the environment is tightl y linked to sight because humans depend heavily on their visual input to quickly acquire large amounts of information. Clearly, of the human senses, sig ht s the greatest with regard to learning and cognition. [1] Newly developed technology promises to aid learning by allowing images to be rendered in 3D sp ace.[2] This paper explores the hypothesis that th e depth cues and display quality of a 3D volumetric d isplay provides advantages for learning simple task s. Experimental data generated by human subjects using the Perspecta 3D Volumetric Display are compared to like data generated using a 2D flat screen liqui d crystal display (LCD). These data show that the Perspecta display provides advantages over the LCD display with respect to performance of simple tasks . Experiment PARTICIPANTS 106 male and female college students between ages 1 8 and 26 were invited to participate. Participants were screened for vision problems such as uncorrect ed vision or depth perception problems. Human subjects participating were randomly assigned to on e of two groups. The two groups differed in the or der in which they used the display devices.	cognition;computer performance;depth perception;display device;flat panel display;joystick;liquid-crystal display;maze solving algorithm;projection screen;randomness;signature block;ues (cipher);volumetric display	Thomas R. Tyler;Andy Novobilski;Joseph Dumas;Amye Warren	2005		10.1117/12.585423	computer vision;geography;volumetric display;engineering drawing;computer graphics (images)	HCI	-42.546569221783564	-49.52356789622151	162208
9152981755554e3c7a92e22d46511b4875cf4b96	evaluation of a tricycle-style teleoperational interface for children: a comparative experiment with a video game controller	games educational institutions mobile robots robot sensing systems wheels data gloves;remotely controlled robot design tricycle style teleoperational interface evaluation children video game controller;mobile robots;control system synthesis;telerobotics;user interfaces;user interfaces control system synthesis mobile robots telerobotics	A tricycle-style teleoperational interface for children to remotely control a robot was developed. There were two crucial requirements in its design: (1) the interface had to be intuitively controllable so that children could use it without requiring detailed instructions and (2) the control of the teleoperational system needed to be fun so that children did not get bored. In this paper, we report an experiment in which 20 children (4-8 years old) performed a range of tasks by remotely controlling a robot using two types of teleoperational interfaces: the tricycle-style interface and a standard video game controller. The results show that the children could perform the tasks better with the tricycle-style interface.	game controller;operability;requirement;robot;vii	Toshimitsu Takahashi;Masahiko Morita;Fumihide Tanaka	2012	2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2012.6343775	telerobotics;mobile robot;embedded system;simulation;computer science;artificial intelligence;user interface	Robotics	-41.96202494487614	-45.102202536446164	162753
68265fa0d8af285dfa29aaf364347894118f8b67	temporal masking characteristics of whole body vibration perception		Most of daily life's signals vary with time. Vibrations in moving vehicles or music are typical examples. Signals that are in close temporal proximity interfere with the perception of each other, which is known as temporal masking. This study evaluates the effects of temporal masking for vertical whole body vibrations (WBV). Forward vs backward masking is tested, as well as the effect of the duration of the interval between masker and target, and the duration of the target on forward masking. Temporal masking was measured for a 1000 ms masker at 40 Hz, using eight different interstimulus-intervals (ISI) ranging from 20 ms to 520 ms with two different target durations of 200 ms and 400 ms at 40 Hz. Additionally on- and off-frequency conditions have been tested. The results of forward masking measurement with 20 subjects verify that temporal masking for WBV decreases with increasing ISI as well as with increasing target duration. Off-frequency masking is not as strong as on-frequency masking. These findings are interesting in the context of comfort evaluation in the automotive industry as well as for the design of multi-modal virtual environments.	backward masking;information sciences institute;modal logic;virtual reality	Anna Schwendicke;Jing Dou;M. Ercan Altinsoy	2017	2017 IEEE World Haptics Conference (WHC)	10.1109/WHC.2017.7989948	whole body vibration;computer science;acoustics;auditory masking;perception;masking (art);ranging;backward masking	Visualization	-45.98662084387238	-50.082492185509054	162769
c55bee1ab3458c2477d64fe56ae166ab1f04d95b	body movement reduces pain intensity in virtual reality-based analgesia		ABSTRACTIn this study we investigated the relationship between body movement during virtual reality (VR) analgesia and the experience of pain. Thermal (cold) stimulation was used to inflict pain. Two measures of pain were used—pain tolerance and pain intensity. Participants were wearing head mounted displays (Oculus Rift DK2) and were playing a game created by authors of the study.Forty-six students of Wroclaw University participated in a within-subject design experiment. Each subject participated in a non-VR control condition and in two experimental conditions. Participants navigated the VR game using a computer mouse, but the mouse sensitivity (and therefore amount of physical movement necessary to navigate) was different in each experimental condition. We also measured feeling of presence in VR, game behavior, and attitudes toward the game.The amount of body movement while steering the game was related to a decrease in pain intensity, but not in pain tolerance. This was an opposite result to our previo...	virtual reality	Marcin Czub;Joanna Piskorz	2018	Int. J. Hum. Comput. Interaction	10.1080/10447318.2017.1412144	simulation;multimedia;feeling;pain tolerance;computer science;virtual reality;stimulation	HCI	-47.568883160928806	-50.241279598450184	163644
f54e3e261853ed244ed9cfbbcb5b55b0c702b785	testing the value of route directions through navigational performance		"""Route directions to reach a target point on a campus were collected from undergraduates. A """"good"""" description and a """"poor"""" one were selected, based on ratings provided by judges in terms of their value for navigational assistance. People unfamiliar with the campus were then required to navigate to the target after studying one of these descriptions. In addition, a """"skeletal"""" description, which contained the essentials needed for navigating, was constructed and used in the experiment. During navigation, we measured the frequencies of stops and of directional errors (whether these errors were self-corrected or corrected by the experimenter). Overall, the good and the skeletal descriptions resulted in better performance than the poor one. Their value as navigational aids was confirmed by measuring the navigation times. Analyzing the structure and content of the descriptions confirmed that the effectiveness of route directions depends on their ability to connect actions to landmarks, that is, to closely link ..."""		Marie-Paule Daniel;Ariane Tom;Elsa Manghi;Michel Denis	2003	Spatial Cognition & Computation	10.1207/s15427633scc0304_2	simulation;artificial intelligence;multimedia;communication	HCI	-43.913193454793415	-50.09889542307546	163910
be673f356b856246795d4ddca15eb6a3ebad7f3c	a review of simulators with haptic devices for medical training	3d simulators;training;e learning;medical training;haptic devices	Medical procedures often involve the use of the tactile sense to manipulate organs or tissues by using special tools. Doctors require extensive preparation in order to perform them successfully; for example, research shows that a minimum of 750 operations are needed to acquire sufficient experience to perform medical procedures correctly. Haptic devices have become an important training alternative and they have been considered to improve medical training because they let users interact with virtual environments by adding the sense of touch to the simulation. Previous articles in the field state that haptic devices enhance the learning of surgeons compared to current training environments used in medical schools (corpses, animals, or synthetic skin and organs). Consequently, virtual environments use haptic devices to improve realism. The goal of this paper is to provide a state of the art review of recent medical simulators that use haptic devices. In particular we focus on stitching, palpation, dental procedures, endoscopy, laparoscopy, and orthopaedics. These simulators are reviewed and compared from the viewpoint of used technology, the number of degrees of freedom, degrees of force feedback, perceived realism, immersion, and feedback provided to the user. In the conclusion, several observations per area and suggestions for future work are provided.	artificial skin;body tissue;cadaver;computer mouse;computers;dental procedures;display resolution;esthesia;hl7publishingsubsection <operations>;haptic device component;haptic technology;image stitching;immersion (virtual reality);laboratory;medical transcription;organ;orthopedics;palpation;published comment;schools, medical;sense organs;simulators;synthetic intelligence;tactile reflex epilepsy;touch sensation;touchscreen;virtual reality;keyboard	David Escobar-Castillejos;Julieta Noguez;Luis Neri;Alejandra J. Magana;Bedrich Benes	2016	Journal of Medical Systems	10.1007/s10916-016-0459-8	simulation;computer science;artificial intelligence;multimedia;haptic technology	HCI	-41.7357042025899	-47.08504029990545	164327
618f1370c6c4718e18c38c5e6a06dd45695ff115	cognitive modeling demonstrates how people use anticipated location knowledge of menu items	fitts law;cognitive models;cognitive process;visual search;cognitive model;menus	This research presents cognitive models of a person selecting anitem from a familiar, ordered, pull-down menu. Two different modelsprovide a good fit with human data and thus two different possibleexplanations for the low- level cognitive processes involved in thetask. Both models assert that people make an initial eye and handmovement to an anticipated target location without waiting for themenu to appear. The first model asserts that a person knows theexact location of the target item before the menu appears, but themodel uses nonstandard Fitts law coefficients to predict mousepointing time. The second model asserts that a person would onlyknow the approximate location of the target item, and the modeluses Fitts law coefficients better supported by the literature.This research demonstrates that people can develop considerableknowledge of locations in a visual task environment, and that morework regarding Fitts law is needed.	approximation algorithm;coefficient;cognitive model;fitts's law	Anthony J. Hornof;David E. Kieras	1999		10.1145/302979.303120	cognitive model;simulation;cognition;visual search;computer science;fitts's law	HCI	-46.36477741363205	-48.83457782357262	164873
bf4cba233af7331f90cebc5a73f4e51aceabf441	a comparative study of sonification methods to represent distance and forward-direction in pedestrian navigation	journal	This article presents a new design of using nonspeech audio (i.e., earcons, spearcons, and short pulses) to represent distance and forward-direction for pedestrian navigation in eyes-free environment. Experiment in the field is carried out with the involvement of 15 participants using within-subject design to evaluate the newly developed earcons, spearcons, and short pulses for distance and forward-direction in pedestrian navigation. Results from the experiment suggest that spearcons are efficient in tasks completion, and it conveys distance and forward-direction information to participants more accurately compared with earcons and short pulses. Overall, participants have shown their satisfaction with spearcons as an audio feedback in pedestrian navigation.	sonification	Ibrar Hussain;Ling Chen;Hamid Turab Mirza;Kong Xing;Gencai Chen	2014	Int. J. Hum. Comput. Interaction	10.1080/10447318.2014.925381	computer vision;simulation;computer science	HCI	-47.646664390940245	-48.17067739971253	165134
61c41bd4a0ef4711cfe0695bc4418c1858f03b4a	top: tactile opinion poll system for silent feedback	interactive multimedia;voting;servers ieee 802 11 standards haptic interfaces calibration flexible printed circuits vibrations real time systems;stress ball;optimal voting protocol societies democracy tactile opinion poll system silent feedback top system user feedback haptic modality smart tangible ball central software wi fi squeezometer vibrotactile clues user selection vibrotactile motor squeezometer device;stress ball voting interactive multimedia;wireless lan government data processing haptic interfaces	Voting is an integral part of modern day societies that yearn for democracy and transparency. For voting to be acceptable and wanted, it needs to be secretive, anonymus, fast, and reliable. The main aim of this paper is to propose the Tactile Opinion Poll (TOP) system for silent feedback that enables users to cast their vote and opinion in a secure and private way that is both intuitive and easy. The TOP system captures user feedback using haptic modality in real-time by squeezing a smart tangible ball (called the squeezometer) that securely communicates the user feedback to a central software via Wi-Fi. The squeezometer display vibrotactile clues to confirm user selection via a vibrotactile motor embedded in the squeezometer device. A preliminary evaluation is conducted to verify the effectiveness of the system and investigate an optimal voting protocol that the system may use.	embedded system;feedback;floor and ceiling functions;haptic technology;modality (human–computer interaction);real-time clock	Katherine Blumer;Nour AlGharibeh;Mohamad Eid	2014	2014 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE) Proceedings	10.1109/HAVE.2014.6954334	simulation;engineering;multimedia;world wide web	Embedded	-42.44172382301728	-46.14489851121163	165583
a05d843b2a6ce97ece7e57aba7b5913fc673f375	3dtouch and homer-s: intuitive manipulation techniques for one-handed handheld augmented reality	3d manipulation;3d interaction techniques;handheld augmented reality	Existing interaction techniques for mobile AR often use the multi-touch capabilities of the device's display for object selection and manipulation. To provide full 3D manipulation by touch in an integral way, existing approaches use complex multi finger and hand gestures. However, they are difficult or impossible to use in one-handed handheld AR scenarios and their usage requires prior knowledge. Furthermore, a handheld's touch screen offers only two dimensions for interaction and limits manipulation to physical screen size. To overcome these problems, we present two novel intuitive six degree-of-freedom (6DOF) manipulation techniques, 3DTouch and HOMER-S. While 3DTouch uses only simple touch gestures and decomposes the degrees of freedom, Homer-S provides full 6DOF and is decoupled from screen input to overcome physical limitations. In a comprehensive user study, we explore performance, usability and accuracy of both techniques. Therefore, we compare 3DTouch with HOMER-S in four different scenarios with varying transformation requirements. Our results reveal both techniques to be intuitive to translate and rotate objects. HOMER-S lacks accuracy compared to 3DTouch but achieves significant performance increases in terms of speed for transformations addressing all 6DOF.	ar (unix);augmented reality;display size;handheld game console;interaction technique;multi-touch;requirement;touchscreen;usability testing	Annette Mossel;Benjamin Venditti;Hannes Kaufmann	2013		10.1145/2466816.2466829	simulation;engineering;multimedia;computer graphics (images)	HCI	-44.65633557574372	-45.24238736329772	165806
ae70d6ff0e0b7954f46c27ea0848c6f7c117951d	planar hand motion guidance using fingertip skin-stretch feedback	h 1 2 models and principles user machine systems human information processing;rehabilitation;human computer interaction haptic interfaces;atmospheric measurements;particle measurements;skin;rehabilitation h 1 2 models and principles user machine systems human information processing h 5 2 information interfaces and presentation user interfaces haptic i o tactile direction cues haptics skin stretch;haptics;h 5 2 information interfaces and presentation user interfaces haptic i o;accuracy;motion segmentation;haptic interfaces medical treatment skin accuracy atmospheric measurements particle measurements motion segmentation;haptic interfaces;tactile direction cues;medical treatment;upper extremity rehabilitation planar hand motion guidance fingertip skin stretch feedback haptic device planar hand movement skin stretch feedback fingerpad user index finger angle matching test stimulus direction motion guidance test reachable workspace real time corrective feedback user performance distorted haptic perceptual response response direction bias haptic feedback human machine interaction;skin stretch	In this paper, we show that a simple haptic device can accurately guide users through planar hand movements. The device guides the user through skin stretch feedback on the fingerpad of the user's index finger. In an angle matching test evaluating two types of stimuli, users are able to discriminate between eight stimulus directions and match the motion of their hand to the stimulus direction with 10 degree accuracy. In two motion guidance tests, haptic cues effectively guide users' arm motions through the full extent of their reachable workspace. Real-time corrective feedback greatly improves user performance, keeping average user hand motions within 12 mm of the prescribed path and within 4 degree of the indicated directions. Additionally, the paper shows that participants exhibit distorted haptic perceptual responses, finding that the distortion causes a response direction bias, but that appropriate haptic feedback can correct for the effect. Such motion guidance has applications in human-machine interaction, such as upper-extremity rehabilitation.	distortion;feedback;fingertip dosing unit;game controller;global positioning system;gray platelet syndrome;haptic device component;haptic technology;human–computer interaction;hypersensitivity skin testing;ibm 7030 stretch;index finger;joystick;limb structure;longitudinal studies;matching;motion;movement;perceptual distortions;peripheral;randomized algorithm;real-time clock;real-time transcription;thinking outside the box;workspace	Sumner L. Norman;Andrew J. Doxon;Brian T. Gleeson;William R. Provancher	2014	IEEE Transactions on Haptics	10.1109/TOH.2013.2296306	computer vision;simulation;computer science;artificial intelligence;accuracy and precision;multimedia;skin;haptic technology;statistics	HCI	-44.86986112565615	-49.194906583244965	166059
ea64304c7afeb027d163f6d4e24748c096248b06	shape features of the search target modulate hand velocity, posture and pressure during haptic search in a 3d display	human movement;haptic search;salient form features;multimodal data;haptic shape exploration	We have investigated spontaneous haptic search in a scenario in which both the search target and the search field are represented by a random composition of different primitive shapes. In our experiment, blindfolded sighted individuals were asked, firstly, to learn a complex search target and, secondly, to find this search target embedded in a larger, encompassing search field. Our goal was to examine, how different shape characteristics of the complex search target influenced the overall search behaviour.#R##N##R##N#We have evaluated data of eight participants by correlating the features, representing the search behaviour, with the features, representing the target object. This approach showed that the number of vertices, the curvature and the height of the target may have a global impact on the hand posture, velocity and pressure during the haptic search.	haptic technology;poor posture;stereo display;velocity	Kathrin Krieger;Alexandra Moringen;Robert Haschke;Helge J. Ritter	2016		10.1007/978-3-319-42321-0_13	computer vision;simulation;geography;communication	HCI	-48.051642125525575	-49.35568529916775	167224
ead43517e8402c84be0e4aec844a3316cefd12f2	comprehending parametric cad models: an evaluation of two graphical user interfaces	parametric model;human performance;interfaces;cad;3d model comprehension;parametric cad;3d model;gui;parametric models;graphic user interface;error rate;experimental evaluation;usability;user satisfaction	"""In this study, we experimentally evaluated two GUI prototypes (named """"split"""" and """"integrated"""") for parametric CAD modeling. Participants in the study were asked to perform a number of 3D model comprehension tasks, using both interfaces. The tasks themselves were classified into three classes: parameterization, topological and geometrical tasks. We measured the task completion times, error rates, and user satisfaction for both interfaces. The experimental results showed that task completion times are significantly shorter when the """"split"""" interface is being used, in all cases of interest: 1) tasks taken as a whole and 2) tasks viewed by task type. There was no significant difference in error rates between the two interfaces; however, error rate was significantly higher in the case of parameterization tasks (for both interfaces). User satisfaction was significantly higher for the """"split"""" interface. The study gave us a better understanding of the human performance when perceiving and comprehending parametric CAD models, and offered insight into the usability aspects of the two studied interfaces; we also believe that the knowledge obtained could be of practical utility to implementers of parametric CAD modeling packages."""	computer user satisfaction;computer-aided design;experiment;graphical user interface;human reliability;list comprehension;usability	Sinisa Kolaric;Halil Erhan;Robert Woodbury;Bernhard E. Riecke	2010		10.1145/1868914.1869010	simulation;parametric model;human–computer interaction;computer science;graphical user interface;multimedia	HCI	-47.622539635232755	-47.92116623218993	167657
b47b633e7805fbc81194b29c5091554d304e994c	systematic movements in haptic search: spirals, zigzags, and parallel sweeps	animals;perceptual search;systematics;systematics haptic interfaces search problems visualization animals spirals planning;haptics;haptic perception systematic movement haptic search spiral strategy zigzag strategy parallel sweep strategy blindfolded sighted participant detection radius target detection;systematic;visualization;exploratory procedures;spirals;planning;search problems;haptic interfaces;exploratory procedures haptics perceptual search systematic search theory;visual perception haptic interfaces object detection;search theory	Movement strategies were investigated in a one-handed haptic search task in which blindfolded-sighted participants used either one or five fingers to find a landmark on an unstructured tactile map. Search theory predicts that systematic strategies, such as spirals, zigzags, and parallel sweeps, should be more prevalent when the searcher's detection radius is small (one finger) and less common when the detection radius is large (five fingers). As predicted, systematic strategies were more common in one-finger than five-finger search. Participants were able to exploit the larger detection radius during five-finger searches to detect targets with any of their fingers, and in one-finger search used more systematic strategies. For the most part, participants' fingers moved together during five-finger search, expanding and moving quickly when looking for search targets/distractors, and contracting and moving slowly when examining search targets/distractors. There was no evidence of fingers being used as spatial anchors or other independent finger movements in five-finger search. While targets could be found with any fingers, examination was primarily accomplished using the index and middle fingers. Overall, these results indicate that untrained sighted participants will use optimal systematic strategies during haptic search, and this behavior is appropriately modulated by detection radius.	contract agreement;finger search;fingers;fingers, unit of measurement;html element;haptic device component;haptic technology;large;modulation;movement;norm (social);tactile graphic	Valerie Morash	2016	IEEE Transactions on Haptics	10.1109/TOH.2015.2508021	search theory;planning;computer vision;simulation;visualization;computer science;artificial intelligence;systematics;haptic technology;spiral	HCI	-41.67457191324628	-49.28936657698444	167707
2c5dc78fb997d4a9e433aee4339d7b9edcb44d46	investigation of landmark-based pedestrian navigation processes with a mobile eye tracking system		Eye movements provide information on the mental processing of landmark objects while navigating. The present study investigates landmark-based navigation by pedestrians in real world environments using mobile eye tracking technology. The goal of the study is to identify whether landmarks on maps optimize the navigation procedure and the usage of a map, and imprint the cognitive map sustainably. Two independent test groups navigated through unfamiliar urban environment and were subsequently interviewed. One group had landmark visualized on a map as an additional aid, the control group did not. The results show that objects that are focused longer and more frequently transfer onto the mental map. Upon recalling objects present in the surroundings, on average 8.3 landmarks were named per interview by the landmark group, compared to 7.0 for the control group. For the control group, the usage and duration of observation of the map was thereby approximately 1.7 times greater than for the landmark group. Following the memory test, the participants in the landmark group remembered significantly more objects and located these correctly as compared to the control group. In summary, the results show that the visualization of landmarks on maps optimizes the use of maps for navigation, whereby more landmark objects transfer to long-term memory and the mental map.		Conrad Franke;Jürgen Schweikart	2016		10.1007/978-3-319-47289-8_6	computer vision;tracking system;mobile robot navigation	HCI	-44.00480898713025	-50.02067105675013	167926
d93793847e862d895acbe9bd99adbedf81121167	haptic direction indicator for visually impaired people based on pseudo-attraction force		Wayfinding is of vital importance if visually impaired pedestrians are to walk by themselves from one place to another, since they must calculate both their orientation and position. Here, a new haptic direction indicator is proposed, which will help blind pedestrians to avoid hazardous areas intuitively and safely by means of haptic navigation. A novel translational force perception method, called the “pseudo-attraction force” technique, is applied to a haptic direction indicator, which exploits the nonlinear relationship between perceived acceleration and physical acceleration to generate a force sensation. An experiment was performed to clarify the perceptual characteristics when a visually impaired person held the haptic direction indicator. The results indicate that the angular resolution of directional force under 8-direction (compass) conditions was better than that under 12-direction (clock position) conditions with the haptic direction indicator. The finding constitutes a criterion for designing smaller haptic direction indicators.	angularjs;haptic technology;heading indicator;nonlinear system	Tomohiro Amemiya	2009	e-Minds		motion detection;motion compensation;panning (camera);distortion;haptic technology;computer vision;computer science;artificial intelligence	HCI	-45.591771076921745	-50.02730827549912	169245
6ea502a4cb9cfba788a7b11de0aecf997b48c4fe	rhythmic sonic feedback for speed skating by real-time movement synchronization		A unique problem associated with the movements of a speed skating athlete inspired this practical work, looking into the question of using interactive auditory feedback to improve sporting movements and sporting movement acquisition. Presented here is a method for synchronizing the periodic movements of a subject against the movements of a model and sonifying that synchronization data in realtime. The sonic feedback is designed to convey information related to how the movements of a subject match against those of a model. A simple, inexpensive sensor system is created to capture speed skating movements and facilitate the sonification. The effectiveness of the system is demonstrated with two case studies. The first case study involves an experienced skater who had developed a significant anomaly in his technique, who uses this system to become aware of and correct his undesired movements. The second case study involves a new and inexperienced speed skating athlete who accelerates the acquisition of speed skating skills by listening and reacting to the relationship between his movements and those of a skilled speed skating athlete. While speed skating is used to demonstrate this sonic feedback technique, the algorithms can be applied to any repetitive movements.	algorithm;anomaly detection;audio feedback;experience;real-time computing;real-time transcription;sonification	Andrew Godbout;Jeffrey E. Boyd	2012	Int. J. Comp. Sci. Sport		real-time computing;rhythm;communication;synchronization;computer science	HCI	-48.08089447251901	-50.77734209952075	170235
aed0740aafe423d9468d1a248e41a5bf0fc6eca2	emergent effects in multimodal feedback from virtual buttons	multimodal feedback;performance;haptic force feedback;virtual buttons;evaluation	The continued advancement in computer interfaces to support 3D tasks requires a better understanding of how users will interact with 3D user interfaces in a virtual workspace. This article presents two studies that investigated the effect of visual, auditory, and haptic sensory feedback modalities presented by a virtual button in a 3D environment on task performance (time on task and task errors) and user rating. Although we expected task performance to improve for conditions that combined two or three feedback modalities over a single modality, we instead found a significant emergent behavior that decreased performance in the trimodal condition. We found a significant increase in the number of presses when a user released the button before closing the virtual switch, suggesting that the combined visual, auditory, and haptic feedback led participants to prematurely believe they actuated a button. This suggests that in the design of virtual buttons, considering the effect of each feedback modality independently is not sufficient to predict performance, and unexpected effects may emerge when feedback modalities are combined.	closing (morphology);emergent;feedback;haptic technology;modality (human–computer interaction);multimodal interaction;network function virtualization;user interface;workspace	Adam Faeth;Chris Harding	2014	ACM Trans. Comput.-Hum. Interact.	10.1145/2535923	simulation;performance;evaluation;multimedia	HCI	-45.82564921010308	-48.007639405309945	170328
09af367f726ac046007fa69cc44336cdf2fa4fe6	movable cameras enhance social telepresence in media spaces	telepresence;media space;motion parallax	Media space is a promising but still immature technology to connect distributed sites. We developed a simple additional function that moved a remote camera forward when a local user approached a display so that the approach was amplified by a remote person's expanding image accompanied by motion parallax. We conducted an experiment in which we observed that a movable camera enhanced social telepresence, which is the feeling of facing a remote person in the same room. Despite the camera's movement, subjects believed that the camera did not move and a zoom-in function expanded the image. Surprisingly, a zoom-in camera that expanded the image as the movable camera did, however, was ineffective probably because of a lack of motion parallax. Although we explained nothing about the camera, most subjects noticed that their walking caused the view's expansion. If a remote person initiated the camera's movement, social telepresence could not be enhanced.	media space;parallax	Hideyuki Nakanishi;Yuki Murakami;Kei Kato	2009		10.1145/1518701.1518771	parallax;computer vision;simulation;computer graphics (images)	HCI	-42.22882354529752	-50.107524986942074	170363
09b0d05fc0596573ad27188cfda846e0b0ab5ce9	mouse and touchscreen selection in the upper and lower visual fields	mice;fitts law;user study;pointing;interactive display;interactive displays;touchscreens;visual field;visual fields	Neuroanatomical evidence indicates the human eye's visual field can be functionally divided into two vertical hemifields, each specialized for specific functions. The upper visual field (UVF) is specialized to support perceptual tasks in the distance, while the lower visual field (LVF) is specialized to support visually-guided motor tasks, such as pointing. We present a user study comparing mouse- and touchscreen-based pointing for items presented in the UVF and LVF on an interactive display. Consistent with the neuroscience literature, we found that mouse and touchscreen pointing were faster and more accurate for items presented in the LVF when compared to pointing at identical targets presented in the UVF. Further analysis found previously unreported performance differences between the visual fields for touchscreen pointing that were not observed for mouse pointing. This indicates that a placement of interactive items favorable to the LVF yields superior user performance, especially for systems dependent on direct touch interactions.	interaction;six degrees of separation;touchscreen;usability testing;user interface design	Barry A. Po;Brian D. Fisher;Kellogg S. Booth	2004		10.1145/985692.985738	computer vision;simulation;human–computer interaction;computer science;fitts's law	HCI	-45.930408587569225	-47.4320207082557	170746
9fdceb03ad6a9579a20b207fde866566311a9792	evaluating subtle cueing in head-worn displays	attention;visual search;subtle visual cueing	Goal-oriented visual search in Augmented Reality (AR) can be facilitated by using visual cues to call attention to the target. However, traditional use of explicit cues may degrade visual search performance. In contrast, Subtle Cueing has been previously proposed as an alternative to explicit cueing, but little is known about how well it performs in head-tracked head worn displays (HWDs).  Using visual search research methods in simulated augmented reality environments, our user study found that Subtle Cueing improves visual search performance in HWDs, and revealed a phenomenon whereby subjects were not able to see the target, even though subjects were primed and the target was well within view.	augmented reality;usability testing	Weiquan Lu;Dan Feng;Steven K. Feiner;Qi Zhao;Henry Been-Lirn Duh	2014		10.1145/2592235.2592237	psychology;computer vision;multimedia;communication	HCI	-44.726121141109495	-47.710129649841235	171184
846229c365389028b2dd2a98c0f452135e24c355	a wearable user interface for measuring reaction time	wearable user interface;cognitive efficiency;reaction time	Reaction time (RT) tests are known as simple and sensitive tests for detecting variation in cognitive efficiency. RT tests measure the elapsed time between a stimulus and the individual's response to it. A drawback of existing RT tests is that they require the full attention of a test person which prohibits the measurement of cognitive efficiency during daily routine tasks. In this contribution we present the design and the evaluation of a wearable RT test user interface which can be operated throughout everyday life. We designed a wearable watch-like device which combines the generation of a haptic stimulus and the recognition of subject’s hand movement response. In order to show to what extent the wearable RT test is convenient to measure reaction times, we designed an experiment in which we measured the reaction times of ten subjects from two different setups. In the first half of the experiment, the participants performed a desktop-based RT test whereas in the second half of the experiment they performed the wearable RT test. In order to measure changes in the duration and variability of reaction times we induced additional cognitive load in both setups. We show that individual changes of reaction times occurred due to the cognitive load manipulation are similar for both desktop-based and wearable RT test. Additionally we investigate the subjective ratings of perceived workload. We conclude that the presented wearable RT test allows to measure changes in reaction times occurred due to additional cognitive load and hence would allow the assessment of changes in cognitive efficiency throughout everyday life.	desktop computer;haptic technology;heart rate variability;sensor;user interface;wearable computer	Burcu Cinaz;Christian Vogt;Bert Arnrich;Gerhard Tröster	2011		10.1007/978-3-642-25167-2_5	mental chronometry;real-time computing;simulation;human–computer interaction	HCI	-47.319484368262884	-47.3793369312434	172195
6615035aeeff2489c0e2e46ee563035391caa726	the effects of physical and virtual manipulatives on learning basic concepts in electronics	design;experimentation	In this study we investigated the effects of using physical manipulatives (PM) and virtual manipulatives (VM) on students' understanding of electronics. In our experiment, all participants completed two similar tasks, one with a tangible toolkit and another with a computer simulation. Both systems shared the same functionalities. Half of the participants first worked with a physical manipulative and then virtual simulation, while the rest did the opposite sequence. Our findings suggest that working with physical manipulatives might improve significantly learning gains compared to a computer simulation. Additionally, users who first worked with the physical manipulatives and then the virtual environment scored higher on the final post-test compared to participants who completed the same activities in the reverse order. This difference, however, did not reach statistical significance.	computer simulation;virtual reality	Shima Salehi;Bertrand Schneider;Paulo Blikstein	2014		10.1145/2559206.2581346	design;simulation;human–computer interaction;computer science;multimedia	HCI	-46.82527317794157	-48.716629158211816	172533
e19c43d79500a1f17c736ce0613dd231c37c042e	touch panel usability of elderly and children	fitts law;elderly;touch panels;children;usability	The purpose of this study is to test the usability for the elderly, young adult and children using four different-sized touch panels and to provide suggestions for the elderly and children when using a touch panel. We set the subjects the tasks of dragging, rotating and scaling as quickly and as accurately as possible using different-sized touch panels. In addition to compare the operating performance values for different tasks for the three age groups, this study also recorded the subjects’ hand movements. The results showed that the age and touch screen size had a significant effect on operating performance using the 4.3-in., 10.1-in., 23-in., and 42-in. touch panels. In addition, the average performance value on the touch panels using two hands was higher than the performance using one hand. Some useful and ergonomic interface design guidelines for the elderly and children were also proposed in this study. 2014 Elsevier Ltd. All rights reserved.	best, worst and average case;display size;drag and drop;field research;fitts's law;futures studies;gesture recognition;human factors and ergonomics;image scaling;input device;multi-touch;touchscreen;usability testing	Hsien-Tsung Chang;Tsai-Hsuan Tsai;Ya-Ching Chang;Yi-Ming Chang	2014	Computers in Human Behavior	10.1016/j.chb.2014.04.050	simulation;usability;human–computer interaction;computer science;multimedia;fitts's law	HCI	-47.32821759176867	-45.98379918247223	173440
2779a5233a4a9e2b7c96c47c9c5e04251a57a1a3	vibration of the white cane causing a hardness sense of an object		Previously, we conducted a psychological experiment to measure sensitivity to hardness using a white cane. The Results showed that participants had higher sensitivity to hardness when using the white cane compared to when actually tapping the target with their fingertip. This suggests that the white cane acts to provide enhanced feedback on hardness. In this study, we investigated the relationship between vibration and sense of hardness using white canes. We measured frequency of vibration of the ferrule of the cane by acceleration sensor when the cane contacted with target. And using psychological experiments, we then had participants estimate their sense of hardness for each hardness degree. It was found that there is a correlation between the hardness sense and frequency of vibration.		Kiyohiko Nunokawa;Shuichi Ino;Kouki Doi	2013		10.1007/978-3-642-39473-7_98	computer science;acceleration;human–computer interaction;composite material;cane;vibration;ferrule	Crypto	-45.76455123269143	-50.77023304337938	173563
3eda389f9d32ee72d2104afca09cf6af040f52e7	an investigation into the comprehension of map information presented in audio	audio information understanding;auditory display;spatial information;audio map system	The growth in mobile and multimodal Computing is leading to the consideration of alternative modes of information presentation, particularly in situations such as driving or walking in unfamiliar locations where the eyes are needed for primary navigation. We report the results of an experiment in which map information is presented to 10 normally sighted participants using an auditory display. Several measures of performance are reported, including the time to navigate a virtual route, keystroke errors and the ability to construct a visual representation of the route travelled based on audio instructions only. The results show significant variability in levels of performance between individuals, though most participants were able to make sense of the auditory display and produce a reasonable visual representation of the virtual route i.e. participants were able to comprehend the presented audio map.	auditory display;event (computing);heart rate variability;multimodal interaction	Feng Feng;Tony Stockman;Nick Bryan-Kinns;Dena Al-Thani	2015		10.1145/2829875.2829896	computer vision;computer science;auditory display;spatial analysis;multimedia	HCI	-46.60568846220898	-47.659430665236236	173736
2c629f5432d110c6c4bf6c3b4f38ab2d78070b01	highlighting techniques for real entities in augmented reality		One fundamental task in Augmented Reality is to highlight specific real world entities or small areas in the user‘s view. For this purpose, a total of 9 novel highlighting techniques were designed and implemented. For comparison, they are complemented by two conventional techniques often used in practice. This paper presents the techniques and evaluates them based on a user study in which 23 participants were asked questions concerning the degree of attention as well as disturbances caused by the highlighting techniques. The user study revealed that particle systems are a successful tool to effectively draw attention to a highlighted entity without causing annoying distractions.	augmented reality;color;distortion;entity;particle system;usability testing;velocity (software development)	Sebastian Fuchs;Mario Sigel;Ralf Dörner	2016		10.5220/0005674002570268	mixed reality	HCI	-44.76514129798582	-47.39119166455218	173901
db975d1c0067dbdf03c00f77ef1e71805ef9398b	toast: ten-finger eyes-free typing on touchable surfaces		Touch typing on flat surfaces (e.g. interactive tabletop) is challenging due to lack of tactile feedback and hand drifting. In this paper, we present TOAST, an eyes-free keyboard technique for enabling efficient touch typing on touch-sensitive surfaces. We first formalized the problem of keyboard parameter (e.g. location and size) estimation based on usersu0027 typing data. Through a user study, we then examined usersu0027 eyes-free touch typing behavior on an interactive tabletop with only asterisk feedback. We fitted the keyboard model to the typing data, results suggested that the model parameters (keyboard location and size) changed not only between different users, but also within the same user along with time. Based on the results, we proposed a Markov-Bayesian algorithm for input prediction, which considers the relative location between successive touch points within each hand respectively. Simulation results showed that based on the pooled data from all users, this model improved the top-1 accuracy of the classical statistical decoding algorithm from 86.2% to 92.1%. In a second user study, we further improved TOAST with dynamical model parameter adaptation, and evaluated usersu0027 text entry performance with TOAST using realistic text entry tasks. Participants reached a pick-up speed of 41.4 WPM with a character-level error rate of 0.6%. And with less than 10 minutes of practice, they reached 44.6 WPM without sacrificing accuracy. Participantsu0027 subjective feedback also indicated that TOAST offered a natural and efficient typing experience.	algorithm;dynamical system;markov chain;simulation;touch typing;touchscreen;usability testing;words per minute	Weinan Shi;Chun Yu;Xin Yi;Zhen Li;Yuanchun Shi	2018	IMWUT	10.1145/3191765	typing;word error rate;decoding methods;asterisk;computer science;pattern recognition;artificial intelligence	HCI	-46.586630439957105	-45.34220156405601	173983
4bfb86874766d3a666082da74b8e575b01782d06	demand characteristics of a questionnaire used to assess motion sickness in a virtual environment	motion sickness;motion control;testing;horses;placebo effect;virtual environment testing computer displays motion measurement horses nasa motion control chromium human factors tracking;three dimensional;virtual environment manual control three dimensional tracking placebo effect;human factors;chromium;computer displays;virtual environment;motion measurement;three dimensional tracking;nasa;manual control;tracking	The experience of motion sickness in a virtual environment may be measured through pre- and post-experiment self-reported questionnaires such as the Simulator Sickness Questionnaire (SSQ). Although research provides converging evidence that users of virtual environments can experience motion sickness, there have been no controlled studies to determine to what extent the user’s subjective response is a demand characteristic resulting from pre- and post-test measures. In this study, subjects were given either SSQ’s both pre and post virtual environment immersion, or only post immersion. This technique was used to test for contrast effects due to demand characteristics in which administration of the questionnaire itself suggests to the participant that the virtual environment may produce motion sickness. Results indicate that reports of motion sickness after immersion in a virtual environment are much greater when both pre and post questionnaires are given than when only a post test questionnaire is used. The implications for assessments of motion sickness in virtual environments are discussed.	immersion (virtual reality);virtual reality	Sean D. Young;Bernard D. Adelstein;Stephen R. Ellis	2006	IEEE Virtual Reality Conference (VR 2006)	10.1109/VR.2006.44	motion control;computer vision;chromium;simulation;human–computer interaction;computer science;human factors and ergonomics;operating system;tracking;mechanical engineering	Visualization	-43.79379781886598	-49.218029578222406	174340
268516c90bf320f4bd89baea292c948e6d4b5e53	fitts' law and the effects of input mapping and stiffness on flexible display interactions	organic user interfaces;fitts law;deformation;stiffness;deformable user interfaces;bend input;flexibility	In this paper, we report on an investigation of Fitts' law using flexible displays. Participants performed a one-dimensional targeting task as described by the ISO 9421-9 standard. In the experiment, we compared two methods of bend input: position control and rate control of a cursor. Participants performed the task with three levels of device stiffness. Results show that bend input is highly correlated with Fitts' law for both position and rate control. Position control produced significantly higher throughput values than rate control. Our experiment also revealed that, when the amount of force applied was controlled, device stiffness did not have a significant effect on performance.	cursor (databases);fitts's law;flexible display;interaction;stiffness;throughput	Jesse Burstyn;Juan Pablo Carrascal;Roel Vertegaal	2016		10.1145/2858036.2858383	simulation;human–computer interaction;computer science;fitts's law;deformation	HCI	-43.89034045181319	-48.6714206250861	174558
96cbb7b79b543747ac2c1ded27482b20c56c3878	evaluation of handsbusy vs handsfree virtual locomotion.		To navigate beyond the confines of often limited available positional tracking space, virtual reality (VR) users need to switch from natural walking input to a controller-based locomotion technique, such as teleportation or full locomotion. Overloading the hands with navigation functionality has been considered detrimental to performance given that in many VR experiences, such as games, controllers are already used for tasks, such as shooting or interacting with objects. Existing studies have only evaluated virtual locomotion techniques using a single navigation task. This paper reports on the performance, cognitive load demands, usability, presence and VR sickness occurrence of two hands-busy (full locomotion/teleportation) and two hands-free (tilt/walking-in-place) locomotion methods while participants (n=20) performed a bimanual shooting with navigation task. Though handsfree methods offer a higher presence, they don't outperform handsbusy locomotion methods in terms of performance.	chris sawyer's locomotion;controller (computing);experience;in-place algorithm;interaction;operator overloading;quantum teleportation;usability;virtual reality	Nathan Navarro Griffin;James Liu;Eelke Folmer	2018		10.1145/3242671.3242707	handsfree;control theory;simulation;usability;teleportation;virtual reality;computer science;cognitive load	HCI	-46.09128897468876	-47.67283583959411	175155
2a8250ee27b30902751a71181f5eacef332beec6	temporal awareness in teleoperation of conversational robots	operator perceived workload conversational robot teleoperation robot control high workload tasks task performance clock display operator time dependent tasks operator temporal awareness text entry tasks social robots touch typing operators;task performance;time dependent;educational robotics;time estimation situation awareness social robots teleoperation;educational robots estimation clocks timing time factors laboratories;clocks;text entry;service robots;human robot interaction;telerobotics clocks cognition human robot interaction service robots;educational robots;time factors;social robots;estimation;cognition;situation awareness;time factor;telerobotics;laboratory experiment;time estimation;teleoperation;social robot;timing	Awareness of time is particularly important for teleoperation of conversational robots, both for controlling the robot and for estimating interaction success, because people have a low tolerance for long pauses in conversation. Findings have shown that people engaged in high-workload tasks tend to underestimate the passage of time. This study confirms that this problem exists for operators controlling a conversational robot, and it investigates mechanisms for improving temporal awareness and task performance while minimizing workload. In a laboratory experiment, two approaches to helping an operator perform various information input tasks were compared: first, assisting temporal awareness by using a clock display, and second, using autonomy to assist one of the operator's time-dependent tasks. Results revealed that assisting the task itself, even without the clock, improved not only task performance but also the operator's temporal awareness. However, results regarding the effect of the clock were ambiguous: it increased workload in general and did not help temporal awareness overall, but it did improve temporal awareness for text entry tasks in particular. As text entry is an important task for teleoperation of social robots, we further investigated the problem of improving temporal awareness during text entry tasks. As the first experiment suggested the effectiveness of a clock, we further validated that the clock is specifically useful to improve temporal awareness. These results showed that the clock did not increase workload for text entry tasks; however, for touch-typing operators, the results suggested that showing a clock after the end of an interaction, rather than continuously throughout the task, could lower the operator's perceived workload.	awareness;cognition;feedback;human factors and ergonomics;marginal model;operation time;social robot;star filler;touch typing	Dylan F. Glas;Takayuki Kanda;Hiroshi Ishiguro;Norihiro Hagita	2012	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2011.2181162	human–robot interaction;real-time computing;simulation;computer science;artificial intelligence;social robot;educational robotics	HCI	-48.21526306163038	-51.655553436164496	175496
deaca27284953255cb8acc9f52cc57918d268384	modeling the difficulty for centering rectangles in one and two dimensions	touch screen;two dimensions;movement time	Centering, positioning an object within specified bounds, is a common computer task, for example making selections using a standard mouse or on a touch screen using a finger. These experiments measured times for participants ( n  = 131) to position a rectangular cursor with various widths,  p  (10 px ≤  p  ≤ 160 px), completely within rectangular targets with various widths,  w  , and tolerances,  t  =  w-p   (4 px ≤  t  ≤ 160 px) in one and two dimensions. The analysis divides the movement time into two phases, the transport time and the centering time. Centering times are modeled well by 1/ t  . All models have high correlation,  r  2 *** 0.95.		Robert Pastel	2009		10.1007/978-3-642-02577-8_96	two-dimensional space;artificial intelligence	Theory	-45.68815924480364	-47.032219532259134	175538
dce0a7ac1f6bbc333b878340d14a6eb4d6bd664b	optimum design of haptic seat for driving simulator	driving simulation;tactile seat;human perception	This work aims to design and develop an optimal vibrotactile seat to provide a high level of satisfaction to the driver. The main critical design parameters were considered and experiments were conducted to investigate the proper values of voltage, frequency, and amplitude that are specifically related to the developed haptic seat.	driving simulator;experiment;haptic technology;high-level programming language;simulation	Osama Halabi;Mariam Ba Hameish;Latefa Al-Naimi;Amna A. Al Kaabi	2014		10.1145/2671015.2671134	simulation;perception	HCI	-47.36040384722861	-47.44141385460801	175590
7a5e5d5b49cd774461e9f650bd29277551430557	augmented reality text style readability with see-through head-mounted displays in industrial context	see-through head-mounted display;text style;industrial context;completion time;different text style;empirical study;industrial environment;transparent text;ar text visualization;effective visualization;error rate;augmented reality text style;effective use	The application of augmented reality in industrial environments requires an effective visualization of text on a see-through head-mounted display (HMD). The main contribution of this work is an empirical study of text styles as viewed through a monocular optical see-through display on three real workshop backgrounds, examining four colors and four different text styles. We ran 2,520 test trials with 14 participants using a mixed design and evaluated completion time and error rates. We found that both presentation mode and background influence the readability of text, but there is no interaction effect between these two variables. Another interesting aspect is that the presentation mode differentially influences completion time and error rate. The present study allows us to draw some guidelines for an effective use of AR text visualization in industrial environments. We suggest maximum contrast when reading time is important, and the use of colors to reduce errors. We also recommend a colored billboard with transparent text where colors have a specific meaning.	ar (unix);augmented reality;bit error rate;color;internet;mathematical optimization;optical head-mounted display;programming tool;see-through display;testbed;workbench	Michele Fiorentino;Saverio Debernardis;Antonio E. Uva;Giuseppe Monno	2013	PRESENCE: Teleoperators and Virtual Environments	10.1162/PRES_a_00146	computer science;multimedia;computer graphics (images)	HCI	-43.45371916149086	-46.72539507257654	176497
4c97903b2e0a4d536d818010302f9d6545d6214c	gesture during mental rotation		Speakers gesture at high rates when explaining their solutions to spatial problems. The present work investigates one possible explanation for why speakers gesture so frequently during spatial problem solving, namely, that when speakers imagine the problem components in motion, they are particularly likely to gesture. We compared speakers’ gesture rates as they actively solved a mental rotation problem and as they described the end state of the problem. Speakers gestured at a higher rate in the rotation condition. Thus, it appears that thoughts about the problem pieces in motion in the rotation condition do lead to increased gesture rates.	gesture recognition;problem solving	Autumn B. Hostetter;Martha W. Alibali;Anne Bartholomew	2011			cognitive psychology;gesture recognition;social psychology;cognitive science;language production;mental rotation;motor imagery;psychology;perception;animation;mental image;gesture	HCI	-47.008020663199034	-50.58028805902688	177052
5f4a1f00704fd45b4e447638ec987a7395e01330	an invisible keyguard	motor disabilities;overlapkeys;accessibility;typing errors;error rate;keyboard;keyguard	Overlap errors, in which two keys are pressed down at once, are a common typing error for people with motor disabilities. Keyguards are a commonly suggested means to may reduce overlap errors. However, they are also unpopular with many users. We present an alternative to the keyguard, a software filter which targets overlap errors. Basic, keystroke timing-based, and language-based techniques for identifying and correcting overlap errors are described. Their performance is compared using a corpus of typing data recorded by keyboard users with motor disabilities. The best filter performance was obtained by keystroke timing characteristics to identify and filter out extra characters. Accuracy of error identification was dependent on the typing style of the user. The filter accurately corrected 80% of the overlap errors presented. Combining the identification and correction techniques gave a 50--75% reduction in errors for the three study participants with the highest error rates.	event (computing);overlap–add method;text corpus	Shari Trewin	2002		10.1145/638249.638275	speech recognition;computer hardware;word error rate;computer science;accessibility;world wide web	HCI	-47.67979156231687	-45.10735037260193	177247
6017f3858082999bbb73f331eb10827cbb4258f5	device- and system-independent personal touchless user interface for operating rooms		In the modern day operating room, the surgeon performs surgeries with the support of different medical systems that showcase patient information, physiological data, and medical images. It is generally accepted that numerous interactions must be performed by the surgical team to control the corresponding medical system to retrieve the desired information. Joysticks and physical keys are still present in the operating room due to the disadvantages of mouses, and surgeons often communicate instructions to the surgical team when requiring information from a specific medical system. In this paper, a novel user interface is developed that allows the surgeon to personally perform touchless interaction with the various medical systems, switch effortlessly among them, all of this without modifying the systems’ software and hardware. To achieve this, a wearable RGB-D sensor is mounted on the surgeon’s head for inside-out tracking of his/her finger with any of the medical systems’ displays. Android devices with a special application are connected to the computers on which the medical systems are running, simulating a normal USB mouse and keyboard. When the surgeon performs interaction using pointing gestures, the desired cursor position in the targeted medical system display, and gestures, are transformed into general events and then sent to the corresponding Android device. Finally, the application running on the Android devices generates the corresponding mouse or keyboard events according to the targeted medical system. To simulate an operating room setting, our unique user interface was tested by seven medical participants who performed several interactions with the visualization of CT, MRI, and fluoroscopy images at varying distances from them. Results from the system usability scale and NASA-TLX workload index indicated a strong acceptance of our proposed user interface.	android;computers;cursor (databases);distance;imagery;interaction;joystick;operating room;operative surgical procedures;simulation;system usability scale;usb;unique user;user interface device component;wearable computer;keyboard	Meng Ma;Pascal Fallavollita;Séverine Habert;Simon Weidert;Nassir Navab	2016	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-016-1375-6	simulation;human–computer interaction;multimedia	HCI	-42.80937840435427	-45.16546111002247	177475
0a4648012ee16872cf341d808858f4fc0cfff9c3	factors influencing the choice of projection textures for displaying layered surfaces	projection textures;high resolution;stereo display;visual design;statistical significance;visualization design;perception and performance;human subjects;surface perception;layered surfaces;experience design	We report on an experiment designed to examine the effect of type and size of projection textures on the ability of human subjects to correctly estimate the surface normal on each of two terrain-like layered surfaces. Normal estimation was determined by the ability to correctly orient a probe so that it is aligned with the normal local to the surface to which it is attached. All presentations were optimized for layered surface viewing, being displayed on a high resolution stereoscope and with a rocking motion to provide motion cues. Three texture types were examined, each at both coarse and fine scales. One texture consisted of random dots, containing no consistent structural pattern. The other two texture types provided structure in two orthogonal directions, with one being a regular square grid, and the other parallel hatch marks arranged in ranks to form perceptual lines orthogonal to the hatches. The experiment demonstrates that structured patterns, indicating shape in two orthogonal directions, are superior to random dots. Using a grid on the bottom layer significantly improves perception of the bottom surface. However, between grids and hatches, there is no other statistically significant evidence that using differing texture type or scale across the two surfaces enhances the ability to estimate shape.	image resolution;normal (geometry);square tiling;stereoscope;structural pattern	Alethea Bair;Donald H. House;Colin Ware	2009		10.1145/1620993.1621014	psychology;computer vision;image resolution;stereo display;experience design;computer science;statistical significance;optics;statistics;computer graphics (images)	HCI	-42.543870723664725	-51.36382766827633	177976
29bf47ab5666af6d71bd9a383a90dcec0a43c9e0	pop-up depth views for improving 3d target acquisition	3d target acquisition;fitts law;popup view;3d input device;empirical evaluation;interaction design;pointing facilitation	We present the design and experimental evaluation of pop-up depth views, a novel interaction technique for aiding in the placement or positioning of a 3D cursor or object. Previous work found that in a 3D placement task, a 2D mouse used with multiple orthographic views outperformed a 3D input device used with a perspective view with stereo. This was the case, even though the mouse required two clicks to complete the task instead of only the single click required with the 3D input device. We improve performance with 3D input devices with pop-up depth views, small inset views in a perspective display of the scene. These provide topand side-views of the immediate 3D neighborhood of the cursor, thereby allowing the user to see more easily along the depth dimension, improving the user’s effective depth acuity. In turn, positioning with the 3D input device is also improved. Furthermore, because the depth views are displayed near the 3D cursor, only tiny eye movements are required for the user to perceive the 3D cursor’s depth with respect to nearby objects. Pop-up depth views are a kind of depth view, only displayed when the user’s cursor slows down. In this manner, they do not occlude the 3D scene when the user is moving quickly. Our experimental evaluation shows that the combination of a 3D input device used with a perspective view, stereo projection, and pop-up depth views, outperforms a 2D mouse in a 3D target acquisition task, in terms of both movement time and throughput, but at the cost of a slightly higher error rate.	2d-plus-depth;computer mouse;cursor (databases);depth perception;imaging phantom;input device;interaction technique;keyboard shortcut;motion capture;orthographic projection;ray casting;stereoscopy;throughput;world-system	Guangyu Wang;Michael J. McGuffin;François Bérard;Jeremy R. Cooperstock	2011			computer vision;simulation;human–computer interaction;computer science;interaction design;fitts's law	HCI	-45.04158593444988	-46.38155441009775	178281
5e5cb918945af884c7e8e467216b8dfe3cb770fb	design and evaluation of mouse cursors in a stereoscopic desktop environment	3d scene mouse cursor evaluation mouse cursor design stereoscopic desktop environment interaction device depth conflicts 3d cursors user comfort occlusion factor depth matching factor shape factor;visual perception;visual perception ergonomics mouse controllers computers natural scenes;mouse controllers computers;three dimensional displays abstracts accuracy visualization;ergonomics;natural scenes	In a standard desktop environment a mouse has proven to be an efficient interaction device. However, when working with stereoscopic content, the shape and behavior of a standard 2D mouse cursor is not suitable, as depth conflicts between the cursor and the content can lead to confusion. In this paper, we show that specific 3D cursors can increase accuracy and user comfort. We first discuss several situations in which traditional cursors fail and explain their limitations and shortcomings. Based on these observations, we propose new mouse cursors that are designed for the use in a stereoscopic environment. In particular, we investigate the integration of the cursor in the scene in terms of occlusion, depth matching and shape and propose a new cursor that adapts to the 3D scene. We conducted an experiment to investigate accuracy, speed and comfort of the various interaction solutions.	cursor (databases);desktop computer;hidden surface determination;pointer (user interface);stereoscopy	Leïla Schemali;Elmar Eisemann	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798844	computer vision;simulation;computer science;computer graphics (images)	HCI	-44.38838154942064	-47.26197934032674	178775
a80bd3382617c6f4336bac39cce5c16fe28f7d1d	simplified standing function evaluation system for fall prevention	evaluation system;virtual light touch contact;fall prevention	This paper proposes a simplified standing-function evaluation system based on virtual light touch contact using a Wii balance board and a wearable VLTC device. In this system, a virtual partition by VLTC is first created from measurements of the subject's trunk and finger positions. Random on/off control of virtual forces from the partition enables evaluation of standing function based on the presence or absence of somatic sensory stimulation to the fingers. Evaluation and comparison experiments were conducted with 35 healthy male subjects. The results suggest that the proposed system allows to evaluate the standing function evaluation quantitavively.		Mami Sakata;Keisuke Shima;Koji Shimatani;Hiroyuki Izumi	2016		10.1007/978-3-319-43518-3_5	simulation;engineering	HCI	-45.95014234941585	-50.256368396879644	178872
62d6246e2a64b640ac7c35600b0b262bd532a789	a perceptually based spectral model for isotropic textures	user studies in visualization;three dimensions;texture synthesis;user study;three dimensional;hue saturation value;red green blue;user experience;ordinal data;perception;multivariate visualization;textures;spatial frequency	Color is routinely used to visualize interval and ordinal data, while texture is not. For color, a variety of perceptually based models is available, which can be used to convey data via straightforward mapping. The dimensionality of texture is less well understood and there are almost no perceptually based and validated models available to generate textures on demand. We present a perceptually based texture synthesis model for isotropic textures. The model uses additive synthesis of band-limited noise in the spectral domain, comparable to the RGB (red, green, blue) model for color. Via user experiments, we have derived a three-dimensional model to control the amplitudes per band in a perceptually intuitive way, comparable to the HSV (hue, saturation, value) model for color. The three dimensions used are contrast, spatial frequency, and spectral purity. Besides a mapping to the amplitudes, we have derived a mapping to a perceptually equidistant space. We show how such textures can be combined with color; applications are presented.	3d modeling;bandlimiting;color;experiment;ordinal data;pure function;texture synthesis;utility functions on indivisible goods	Danny Holten;Jarke J. van Wijk;Jean-Bernard Martens	2006	TAP	10.1145/1190036.1190039	cognitive psychology;three-dimensional space;computer vision;user experience design;computer science;optics;computer graphics (images)	Graphics	-42.21032986091008	-51.365227182787436	179342
35a02d70c64459729278eff735a7cf4d957bda81	proposals for tangible, intuitive and collaborative design of manufactured products through virtual and augmented reality environments		The aim of this research introduces general concepts for performance assessment of virtual reality environments which are relevant to the design activity of manufactured products. Moreover, it aims to present also the context of the research respect to the Thai automotive industry, because this thesis expect to build new knowledge for the benefit of Thailand economy. This research covers several contexts of Human-Computer Interaction research as well as evolution of the VR assessment in the industrial and educational sector. The state of the art with direct relevance for the current research includes: user interface, human factor issues but also usability assessment but the wish here is to go a step further towards utility assessment. This research focuses on abstract level assessment criteria including: affordance, ergonomics, intuitiveness, tangibility, and tiredness assessments. We propose and analyse an assessment method of virtual reality for given activities. We are specifically concerned by activities involved at design stage or at manufacturing preparation stage and we have a deep focus on assembly activity and motion simulation. The design of a virtual reality environment and the implementation of a computer application that supports the evaluation of these activities are presented. The main objective of such application remains the assessment of the performance of virtual reality environments to support the tasks of designers and engineers.This research consists of two main activities: The first experience to validate the evaluation and comparison of the performance of the VRES by the use of low basic sensors consisting of docking quality, task duration, and gesture instability sensor. The experimental task is a set of simple screw assemblies allowing the analysis of the basic movements such as translation, rotation, orientation, and insertion into the holes. We have implemented the experiment on four environments and invited 40 participants to our experiment protocol. The second experience was thus organized around a barrel cam mechanism simulation which is operated with a single rotation motion but which provides a complex cam 3D trajectory. Even design experts may have some difficulties to anticipate the behaviour of this system. They usually experience difficulties when tuning design parameters to ensure that the mechanism is working well. It is thus investigated virtual reality capacity to overpass these difficulties. If this capacity is demonstrated, it is also expected to evaluate and compare the performance of the virtual reality environment system by the use of high level abstract assessment technique combined with low level sensors. Thirty participants have been involved in an experience where both basic sensors and high level assessment criteria were captured. Four virtual reality environments including either stereoscopy or haptic force-feedback device were compared respect to a given task.		Channarong Trakunsaranakom	2017			affordance;haptic technology;human–computer interaction;automotive industry;usability;virtual reality;augmented reality;gesture;user interface;computer science	HCI	-45.25904241520265	-48.06991599038859	180796
5a43a91afcaf6c19da66f8a20c75ad6ee5af3ffb	game cane: an assistive 3dui for rehabilitation games	assistive interactions;rehabilitation;game cane;games three dimensional displays assistive devices training force navigation mice;mice;human computer interaction;rehabilitation exercises;user study assistive technology rehabilitation games;assistive devices;user study;patient rehabilitation;training;rehabilitative interactions;assistive 3dui;force;rehabilitation game interface design;rehabilitation game interface design game cane assistive 3dui assistive interactions rehabilitative interactions physical impairments rehabilitation exercises balance exercise assistive device training cane based 3d interface;navigation;graphical user interfaces;cane based 3d interface;three dimensional displays;games;assistive technology;balance exercise;computer games;patient rehabilitation computer games graphical user interfaces human computer interaction;physical impairments;assistive device training	The objective of this research is to effectively design a cane interface for assistive and rehabilitative interactions in games. Current 3D interfaces for games are not usable for many populations with physical impairments and are often not adaptable to rehabilitation exercises (e.g., balance exercise, assistive device training). To address this, we present the design of a novel cane-based 3D interface for rehabilitation games. We report results from a user study, which offers insight into the future design and potential effectiveness of canes as rehabilitation game interfaces.	3d user interaction;assistive technology;population;usability testing	Miguel Cantu;Eric Espinoza;Rongkai Guo;John Quarles	2014	2014 IEEE Symposium on 3D User Interfaces (3DUI)	10.1109/3DUI.2014.6798840	simulation;physical medicine and rehabilitation;human–computer interaction;engineering	HCI	-42.84574636835055	-46.219383092708824	180952
19c3a226fcdcaa93d041862d911aac527c917c15	towards visual comfort: disciplines on the scene structure design for vr contents		Inappropriate designed VR contents may cause visual discomfort (dizziness, vertigo) to users when they experience with VR head mounted displays (HMDs), This paper explored the human factors of binocular stereoscopic effects and provide some guidelines on how to achieve visual comfort in VR games.		Yanxiang Zhang;ZhongBei Wang	2018		10.1007/978-3-319-95270-3_14	stereoscopy;human–computer interaction;visual discomfort;psychology;vertigo	HCI	-44.13914934710302	-47.2768021857023	181180
6c23d6c9fecbfc1a5422882ea0e75921d8998c59	a survey on robot appearances	androids;humanoid robots principal component analysis materials usa councils educational institutions androids;cluster analysis robot appearances uncanny valley hypothesis design characteristics web based survey standardized pictures;uncanny valley;usa councils;human robot interaction;materials;robot appearances;cluster analysis;humanoid robots;principal component analysis;design;design uncanny valley human robot interaction robot appearances	Against the background of the uncanny valley hypothesis [1] and its conceptual shortcomings this study aims at identifying design characteristics which determine the evaluation of robots. We conducted a web-based survey with standardized pictures of 40 robots which were evaluated by 151 participants. A cluster analysis revealed six clusters of robots. The results are discussed with regard to implications for the uncanny valley hypothesis.	cluster analysis;robot;uncanny valley;web application	Astrid M. Rosenthal-von der Pütten;Nicole C. Krämer	2012		10.1145/2157689.2157787	human–robot interaction;design;simulation;computer science;artificial intelligence;uncanny valley;cluster analysis;principal component analysis	Robotics	-43.60442844385053	-51.29882033180593	182209
9f72ead4052b3024ad7648c780e2917fa481672a	evaluation of human-system interfaces with different information organization using an eye tracker	eye tracking;functional based task analysis;information organization	The increasing use of digitalized displays in the instrumentation u0026 control systems of nuclear power plants has brought new issues related to human-computer interaction, especially under emergency circumstances that are known to be very stressful. This paper studies how interfaces with different information organization (functional layout vs. process layout) influence human-computer interaction behaviors as emergency occurs in terms of search efficiency, difficulty of information abstraction, and workload by using the eye tracking technique on a simulated platform. The result shows that the average blink rate and average blink numbers at the two levels of information organization were different significantly. This may indicate that the functional design was superior to the process design in user workload. The results did not prove the superiority of the functional interface design to the process one in search efficiency and difficulty of information abstraction, since no significant difference was found in the number of fixations and fixations duration mean values.	eb-eye;eye tracking;knowledge organization	Kejin Chen;Zhizhong Li	2013		10.1007/978-3-642-39143-9_32	computer vision	HCI	-47.49953974281001	-47.380193474433185	182515
dd3d034d03b92664844b49c17d18bdb171fb8a99	edge sharpness perception with force and contact location information	contact location display;edge sharpness perception;contact location information;phantoms;fingers force haptic interfaces phantoms rendering computer graphics shape humans;contact location cues;virtual reality;force;computer graphic;force cues;force feedback;virtual object;virtual reality force feedback haptic interfaces;shape;virtual edge perception;fingers;haptic systems;humans;haptic interfaces;force information;rendering computer graphics;haptic systems edge sharpness perception force information contact location information virtual edge perception human edge sharpness discrimination virtual object contact location cues force cues;curvature discrimination;curvature discrimination contact location display edge sharpness perception;human edge sharpness discrimination;haptic interface	The effect of contact location information on the perception of virtual edges was investigated by comparing human edge sharpness discrimination under the force-alone and force-plus-contact-location conditions. The virtual object consisted of the 2D profile of an edge with two adjoining surfaces. Edge sharpness JNDs for both conditions increased from about 2 to 7 mm as the edge radii increased from 2.5 to 20.0 mm, and no significant difference was found between the two conditions. A follow-up experiment with the contact-location alone condition resulted in higher (worse) edge sharpness discrimination thresholds, especially at higher edge radius values. Our results suggest that contact location cues alone are capable of conveying edge sharpness information, but that force cues dominate edge sharpness perception when both types of cues are available.		Jaeyoung Park;Andrew J. Doxon;William R. Provancher;David E. Johnson;Hong Z. Tan	2011	2011 IEEE World Haptics Conference	10.1109/WHC.2011.5945539	computer vision;simulation;computer science;artificial intelligence;virtual reality;multimedia;haptic technology	Robotics	-44.120054458975275	-49.022115552803434	182842
60c6a63218ececcb1238dd4ca4a5424d17750f8b	physical versus virtual pointing	input device;human performance;analysis methods;fitts law;pointing;virtual environments;input devices;virtual environment	B u r n a b y , B C C a n a d a V 5 A 1S6 (604) 2 9 1-3 0 0 4 c h r i s t i n e _ m a c k e n z i e @ sfu.ca A B S T R A C T An experiment was conducted to investigate differences in performance between virtual pointing, where a 2-D computer image representing the hand and targets was superimposed on the workspace, and physical pointing with vision of the hand and targets painted on the work surface. A detailed examination of movement kinematics revealed no differences in the initial phase of the movement, but that the final phase of homing in on smaller targets was more difficult in the virtual condition. These differences are summarised by a two-part model of movement time which also captures the effects of scaling distances to, and sizes of, targets. The implications of this model for design, analysis, and classification of pointing devices and positioning tasks are discussed. Pointing to a location on a graphics display is an elemental gesture in many forms of human-computer interaction (HCi). Pointing movements have been studied in an attempt to understand perceptual-motor processes when we interact with real objects in the physical world. Our interest is in relating these theories and models from motor control to human performance in more abstract environments, where objects and actions represented on a graphics display are mediated by pointing devices. In particular, we wonder how limitations of current 2-D and 3-D virtual environments affect planning and control of natural movements like aiming, pointing, reaching, Permission to make digital/hard copies of all or part of this material for personal or classroom use is granted without fee provided that the copies are not made or distributed for profit or commercial advantage, the copyright notice, the title of the publication and its date appear, and notice is grasping, and manipulating objects; and how detailed analyses of movement kinematics can be used to reveal systematic effects of these constraints on human performance, in the HCI context. Woodworth [12] first proposed that human pointing movements can been understood in terms of two movement phases: an initial planned impulse which covers most of the distance, followed by a second phase of deceleration to the target under current control. According to Fitts [3], total movement time involves a tradeoff …	elemental;fitts's law;graphics;human reliability;human–computer interaction;image scaling;multiple homing;pointing device;theory;virtual reality;workspace	Evan D. Graham;Christine L. MacKenzie	1996		10.1145/238386.238532	simulation;human–computer interaction;computer science;operating system;fitts's law;input device	HCI	-45.78882432992358	-48.42036721221619	182914
cbdb544150fbba449b6952cdd1bbd0422e5eae7c	immersive virtual environment for visuo-vestibular therapy: preliminary results	visual immersion;visual vestibular interaction;virtual reality;clinical study;balance control;vestibular areflexy	"""The sense of equilibrium aggregates several interacting cues. On vestibular areflexic patients, vision plays a major role. We developed an immersive therapeutic platform, based on 3D opto-kinetic stimulation that enables to tune the difficulty of the balance task by managing the type of optic flow and its speed. The balance adjustments are recorded by a force plate, quantified by the length of the center of pressure trajectory and detection of disequilibrium corrections (leans, compensation step). Preliminary analysis shows that (i) patients report a strong immersion feeling in the motion flow, triggering intense motor response to """"fight against fall""""; (ii) the ANOVA factorial design shows a significant effect of flow speed, session number and gaze anchor impact. In conclusion, this study shows that 3D immersive stimulation removes essential limits of traditional opto-kinetic stimulators (limited 2D motions and remaining fixed background cues). Moreover, the immersive optic flow stimulation is an efficient tool to induce balance adaptive reactions in vestibular patients. Hence, such a platform appears to be a powerful therapeutic tool for training and relearning of balance control processes."""	eye;immersion (virtual reality);interaction;kinetics;motion;optic flow;optic nerve glioma, childhood;optics;opto-isolator;patients;vestibular diseases	J. D. Gascuel;H. Payno;Sébastien Schmerber;O. Martin	2012	Studies in health technology and informatics	10.3233/978-1-61499-121-2-187	simulation;engineering;multimedia;communication	Vision	-47.967346454683394	-51.059933122569134	183275
5d771d68f47bce5fa7a3f061229e1ba09c909771	the role of specular reflection in the perception of transparent surfaces - the influence on user safety	building s facade;mirror like reflections;transparency perception	"""The perception of transparency in human's build environment constitutes a significant cognitive challenge, also affecting the user's safety. It is supposed that, apart from the mid-level vision transparency cues, specular reflection is also a key feature of the perceived image taken into consideration by the visual system. In the paper, this optical phenomenon was observed and estimated based on the author's own method, here called the """"pictorial image analysis"""", which uses pairs of photographs: unmodified --- showing the virtual image on the building's transparent facade, and modified --- devoid of this image. The images were digitally processed to extract the reflection laid over the undisturbed transmitted image. The results show that evident specular reflection significantly improves the perception of transparent surfaces, but, in the case of excess or back-lit panes, it can hardly be used as perceptual cue."""		Marcin Brzezicki	2013		10.1007/978-3-642-39360-0_21	computer vision;geography;multimedia;optics	HCI	-41.72252413424514	-50.443648712186665	183344
e70dc3d36e15267366fdcf17552952063ecd30de	floor-projected guidance cues for collaborative exploration of spatial augmented reality setups		In this paper we present a floor-based user interface (UI) that allows multiple users to explore a spatial augmented reality (SAR) environment with both monoscopic and stereoscopic projections. Such environments are characterized by a low level of user instrumentation and the capability of providing a shared interaction space for multiple users. However, projector-based systems using stereoscopic display are usually single-user setups, since they can provide the correct perspective for only one tracked person. To address this problem, we developed a set of guidance cues, which are projected onto the floor in order to assist multiple users regarding (i) the interaction with the SAR system, (ii) the identification of regions of interest and ideal viewpoints, and (iii) the collaboration with each other. In a user study with 40 participants all cues were evaluated and a set of feedback elements, which are essential to guarantee an intuitive self-explaining interaction, was identified. The results of the study also indicate that the developed UI guides users to more favorable viewpoints and therefore is able to improve the experience in a multi-user SAR environment.	augmented reality;feedback;multi-user;region of interest;stereoscopy;usability testing;user interface;video projector	Susanne Schmidt;Frank Steinicke;Andrew Irlitti;Bruce H. Thomas	2018		10.1145/3279778.3279806	projector;viewpoints;stereoscopy;human–computer interaction;augmented reality;computer science;user interface	HCI	-44.65840251455489	-46.17918244466013	184116
c06b2ec0c0c9f9e055369c5031e60c1cccb91975	intrusive evaluation of ambient displays	ambient display	Ambient display is a display, which sits on the peripheral of user’s attention. Currently, the research on ambient displays is still in initial stage, so few evaluation styles are available to evaluate ambient displays. Our previous research (Shen, Eades, Hong, & Moere, 2007) proposed two evaluation styles for ambient displays: Intrusive Evaluation and Non-Intrusive Evaluation. In this journal, we focus on the first style by applying two intrusive evaluation case studies. The first case study compares the performance of three different peripheral display systems on both large and small displays. Our results indicate there is a significant difference on a primary task performance and a peripheral comprehension task between large and small displays. Furthermore, we have found that distraction may be composed by display-distraction and self-interruption, and that animation may only influence the display-distraction. In addition, a measurement of efficiency derived from cognitive science is proposed. The second case study focuses on exploring the correct disruptive order of visual cues (animation, color, area and position). Our results show that the correct disruptive order of visual cues in ambient displays is: animation, color, area and position. Furthermore, we also revealed how display-distraction influences the comprehension of ambient display. In addition, this case study further amended the measurement of efficiency, which was proposed in previous case study, to improve its accuracy. DOI: 10.4018/jaci.2009062202 IGI PUBLISHING This paper appears in the publication, International Journal of Ambient Computing and Intelligence, Volume 1, Issue 4 edited by Kevin Curran © 2009, IGI Global 701 E. Chocolate Avenue, Hershey PA 17033-1240, USA Tel: 717/533-8845; Fax 717/533-8661; URL-http://www.igi-global.com ITJ 5360	animation;area striata structure;cognitive science;digital object identifier;distraction - pain management method;interrupt;peripheral	Xiaobin Shen	2009	IJACI	10.4018/jaci.2009062202	simulation;computer science;multimedia	HCI	-47.65065621097281	-48.588864323523694	184419
8017da2828901eb021e81ede7654f20edd3caca3	the presence of field geologists in mars-like terrain	computer graphics;virtual reality;space exploration;teleoperators;computer vision;robot control;planetary geology;man machine systems;helmet mounted displays;human computer interface	The presence of human geologists is held by some to be essential to the conduct of field geology on remote planetary surfaces, so a field study was conducted to observe and characterize the nature of that presence. This study was conducted in the Mojave Desert of Southern California at the Amboy lava field, a landscape that is analogous to terrain on Mars. Two experienced planetary geologists were interviewed and observed during the conduct of surface operations. Each subject then wore a head-mounted video camera/display system, which replaced his natural vision with video vision, while attempting to conduct further surface explorations. In this study, methods of ethnographic observation and analysis have been coupled with object-oriented analysis and design concepts to begin the development of a clear path from observations in the field to the design of virtual presence systems. The existence of redundancies in field geology and presence allowed for the application of methods for understanding complex systems. As a result of this study, some of these redundancies have been characterized. Those described are all classes of continuity relations, including the continuities of continuous existence, context-constituent continuities, and state-process continuities. The discussion of each includes statements of general relationships, logical consequences of these, and hypothetical situations in which the relationships would apply. These are meant to aid in the development of a theory of presence. The discussion also includes design considerations, providing guidance for the design of virtual planetary exploration systems and other virtual presence systems. Converging evidence regarding continuity in presence is found in the nature of psychological dissociation. Specific methodological refinements should enhance ecological validity in subsequent field studies, which are in progress.	apply;complex systems;field research;planetary scanner;scott continuity;theory	Michael W. McGreevy	1992	Presence: Teleoperators & Virtual Environments	10.1162/pres.1992.1.4.375	simulation;planetary geology;computer science;artificial intelligence;space exploration;virtual reality;robot control;computer graphics	HCI	-41.76992706855967	-50.75625647508523	184538
8a095cb2e1d4a2906d13c1ed46b27619f820da75	i see your point: integrating gaze to enhance pointing gesture accuracy while driving		Mid-air pointing gestures enable drivers to interact with a wide range of vehicle functions, without requiring drivers to learn a specific set of gestures. A sufficient pointing accuracy is needed, so that targeted elements can be correctly identified. However, people make relatively large pointing errors, especially in demanding situations such as driving a car. Eye-gaze provides additional information about the drivers' focus of attention that can be used to compensate imprecise pointing. We present a practical implementation of an algorithm that integrates gaze data, in order to increase the accuracy of pointing gestures. A user experiment with 91 participants showed that our approach led to an overall increase of pointing accuracy. However, the benefits depended on the participants' initial gesture performance and on the position of the target elements. The results indicate a great potential to support gesture accuracy, but also the need for a more sophisticated fusion algorithm.	algorithm;eb-eye;pointing device	Florian Roider;Tom Gross	2018		10.1145/3239060.3239084	simulation;gaze;engineering;gesture	HCI	-45.52025393780839	-46.04971114175159	184848
b41e5d1c052947a982996d8e0791e50255da847e	a brain machine interface for command based control of a wheelchair using conditioning of oscillatory brain activity		In this research a new method of wheelchair control using a Brain Computer Interface (BCI) is proposed, in an attempt to bridge the gap between in-lab and real life applications, we believe it would provide a high level control over the BCI instead of the normal low level commands. It is anticipated to emphasis on mu rhythm to provide the control signals. The wheelchair is equipped with a mapping system, which scans the area and provides a map containing information about the user's current location and next possible destinations, then provides an optimized list of possible trajectories to reach the destination. The paradigm allows users to control the interface using motor imagery and issue commands to switch between possible trajectories and then confirm the choice. Commands trigger the motion of the wheelchair to the intended destination using a user selected path with speed up to 0.5 m/s. The interface also allows the user to interact with different robots through a common robotic system. Evaluation results indicate that this paradigm is indeed usable and could lead to promising outcomes.	brain-computer interfaces;brain–computer interface;cdisc adas-cog - commands summary score;conditioning (psychology);electroencephalography;guided imagery;high-level programming language;interface device component;meter per second;neural oscillation;programming paradigm;real life;robot (device);recurrent childhood brain stem glioma	Eyad M. Hamad;Samer I. Al-Gharabli;Munib M. Saket;Omar Jubran	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8036995	real-time computing;computer vision;brain–computer interface;artificial intelligence;wheelchair;motor imagery;control engineering;conditioning;computer science;speedup	Robotics	-46.74354051056015	-48.67358755878012	185559
3e204561c4877756984684026ca90a0861632b34	vr enhanced teleoperation	haptic display;display devices;haptic display device;virtual reality collision avoidance telerobotics;support system;force feedback;microoperation support system;modality transformation;end effector;smarttool;virtual reality testing displays visualization robots monos devices collision avoidance navigation usability hardware;collision avoidance vr enhanced teleoperation 3d stereoscopic visualization context constraint system design virtual reality selective visual attention robot tele drive distance estimation;micromanipulators;augmented reality;haptic interfaces;microspidar;end effectors	A higher sense of presence and interaction capability may be provided to a user when adopting 3D stereoscopic visualization compared to 2D viewing. Depth perception and task performance may however greatly vary for different display technologies, while available budget and application context constraint system design and approach selection. This work intends to contribute in assessing benefits of stereoscopic viewing for different virtual reality (VR) technologies. The investigation is performed under different conditions and it uses both synthetic and real images. The investigation concentrates on few aspects: comparative evaluation of VR facilities, the influence of selective visual attention on robot tele-drive, performance in collision avoidance and distance estimation. Results from a set of test trials run on 3 different VR facilities in 5 different configurations, emphasize some tendencies and represent a base for further investigations.	anomalous experiences;context (computing);depth perception;display device;stereoscopy;synthetic intelligence;systems design;television;virtual reality	Salvatore Livatino	2007	17th International Conference on Artificial Reality and Telexistence (ICAT 2007)	10.1109/ICAT.2007.51	computer vision;simulation;engineering;computer graphics (images)	Visualization	-43.36924152340328	-47.85846443336873	186623
8289398025e0ef825345c06db9bd94619bd06a96	audio-cued motor imagery-based brain-computer interface: navigation through virtual and real environments	brain computer interface bci;navigation;mental tasks;asynchronous;auditory;motor imagery mi	The aim of this work is to provide a navigation paradigm that could be used to control a wheelchair through a brain–computer interface (BCI). In such a case, it is desirable to control the system without a graphical interface so that it will be useful for people without gaze control. Thus, an audio-cued paradigm with several navigation commands is proposed. In order to reduce the probability of misclassification, the BCI operates with only two mental tasks: relaxed state versus imagination of right hand movements; the use of motor imagery for navigation control is not yet extended among the auditory BCIs. Two experiments are described: in the first one, users practice the switch from a graphical to an audio-cued interface with a virtual wheelchair; in the second one, they change from virtual to real environments. The obtained results support the use of the proposed interface to control a real wheelchair without the need of a screen to provide visual stimuli or feedback. & 2013 Elsevier B.V. All rights reserved.	brain–computer interface;experiment;graphical user interface;programming paradigm	Francisco Velasco-Álvarez;Ricardo Ron-Angevin;Leandro da Silva-Sauer;Salvador Sancha-Ros	2013	Neurocomputing	10.1016/j.neucom.2012.11.038	computer vision;navigation;simulation;computer science;asynchronous communication	Robotics	-47.993306235066335	-47.12425274309825	186691
b8a1527c38ba1c1812f366dcef5b3dd6c7fe7772	a study of the performance of steering tasks under spatial transformation of input	input device;user interface;steering task;spatial indirection;scale;three dimensional;translation;pda input devices;rotation;user interaction;virtual space	Typical direct manipulation tasks often suffer from an inherent indirection between the virtual objects that form the computer interface and the input devices through which the user interacts to manipulate these objects. This paper studies the effect of spatial indirection on the performance of interaction. For continuous input devices, spatial transformation can be decomposed into translation, rotation, and scale. Translation alone simply shifts a movement from the device space to a different position in the virtual space, preserving the direction and size of that motion. Rotation changes the direction, while scale modifies the size. This study found evidence that rotation and scale are significant factors in interaction performance. We propose a model based on these factors that can be employed to predict the time required for a task of tracing and staying inside a non-linear shape. Contrary to our initial hypothesis, moderate translation changes did not register significant variations in the required time. The results of this study are also applicable to the placement and ergonomics of physical input devices.	direct manipulation interface;human factors and ergonomics;indirection;input device;nonlinear system;virtual reality	M. Eduard Tudoreanu;Eileen Kraemer	2008		10.1145/1593105.1593196	computer vision;simulation;computer science;engineering drawing	HCI	-45.27597395199656	-47.1085948843043	187226
15c82e68f0c409388bb9ebb3c2c1fcc9bc2b87b7	a pilot study of altering depth perception with projection-based illusions		We present first approaches to manipulate perceived spatial relationships between the user and real-world objects by introducing perceptual illusions to a projection-based augmented reality (AR) environment. Therefore, we analyzed the effect of three monoscopic illusions, which are inspired by visual arts, i. e., (i) color temperature, (ii) luminance contrast and (iii) blur. The results provide positive indications that computer-generated projected illusions can influence the human depth perception in such an environment, even in the presence of additional conflicting depth cues. CCS Concepts •Human-centered computing → Mixed / augmented reality; Empirical studies in HCI;	computer-generated holography;depth perception;gaussian blur	Susanne Schmidt;Gerd Bruder;Frank Steinicke	2017		10.2312/egve.20171380	human–computer interaction;depth perception;computer science;illusion	HCI	-43.71030531820017	-47.46734275487473	187264
d398bab25577797ed5ebe2b10f83c02cdeafa7e0	virtual environment system in support of a traditional orientation and mobility rehabilitation program for people who are blind	article	BlindAid, a virtual environment system developed in part for orientation and mobility training of newly, adventitiously, and congenitally blind persons, allows interaction with different virtual structures and objects via auditory and haptic feedback. This research examined whether and how the system might help people who are blind develop orientation and mobility skills within a traditional rehabilitation program. Nine clients at The Carroll Center for the Blind (Newton, MA) explored VEs and performed virtual orientation tasks in addition to their traditional orientation and mobility training. The virtual training gave the participants additional time to learn systematic exploration and orientation strategies and their performance was evaluated using qualitative and quantitative methods. The findings supply strong evidence that practicing with the BlindAid system does enhance traditional orientation and mobility training in these areas.	accessibility;carroll morgan (computer scientist);experiment;fletcher's checksum;haptic technology;internet;linear programming relaxation;map;newton;simulation;video game rehabilitation;virtual reality;word lists by frequency	Orly Lahav;David W. Schloerb;Mandayam A. Srinivasan	2013	PRESENCE: Teleoperators and Virtual Environments	10.1162/PRES_a_00153	simulation;computer science;multimedia	HCI	-46.84699933766513	-48.22383547589228	187327
c039eb66b88e738f3fdea9bddb19e9a097ccdbc8	navigation in virtual environments using head-mounted displays: allocentric vs. egocentric behaviors		User behaviors while navigating virtual environments (VEs) using head-mounted displays (HMDs) were investigated. Particularly, spatial behaviors were observed and analyzed with respect to the virtual navigation preferences and performance. For this, two distinct navigation strategies applying allocentric and egocentric spatial perspectives were used. Participants utilized two different user interfaces (i.e., a multitouch screen and a gamepad) to employ the aforementioned strategies to perform a series of rotation, surge motion, and navigation tasks. Two allocentric and two egocentric metaphors for motion techniquesddigital map, canoe paddle, steering wheel, and wheelchairdwere established. User preferences for these motion techniques across the tasks were then observed, and their task performances on the two given interfaces were compared. Results showed that the participants preferred to apply egocentric techniques to orient and move within the environment. The results also demonstrated that the participants performed faster and were less prone to errors while using a gamepad, which manifests egocentric navigation. Results from workload measurements with the NASA-TLX and a brain-computer interface showed the gamepad to be superior to the multitouch screen. The relationships among spatial behaviors (i.e., allocentric and egocentric behaviors), gender, video gaming experience, and user interfaces in virtual navigation were also examined. It was found that female participants tended to navigate the VE allocentrically, while male participants were likely to navigate the VE egocentrically, especially while using a non-natural user interface such as the gamepad. © 2017 Elsevier Ltd. All rights reserved.	brain–computer interface;gamepad;head-mounted display;multi-touch;natural user interface;performance;steering wheel;virtual reality	Hadziq Fabroyir;Wei-Chung Teng	2018	Computers in Human Behavior	10.1016/j.chb.2017.11.033	social psychology;psychology;steering wheel;task performances;workload;human–computer interaction;paddle;user interface	HCI	-46.18212648496231	-47.525384333379534	187548
0682747c43ef86ca652459c077a80b2275865792	triangle cursor: interactions with objects above the tabletop	3d interaction;selection technique;multi touch interaction;user study;3d interaction technique;stereoscopic displays;satisfiability;tabletop displays;stereoscopic display;tabletop display	Extending the tabletop display to the third dimension using a stereoscopic projection offers the possibility to improve applications by using the volume above the table surface. The combination of multi-touch input and stereoscopic projection usually requires an indirect technique to interact with objects above the tabletop, as touches can only be detected on the surface. Triangle Cursor is a 3D interaction technique that allows specification of a 3D position and yaw rotation above the interactive tabletop. It was designed to avoid occlusions that disturb the stereoscopic perception. While Triangle Cursor uses an indirect approach, the position, the height above the surface and the yaw rotation can be controlled simultaneously, resulting in a 4 DOF manipulation technique. We have evaluated Triangle Cursor in an initial user study and compared it to a related existing technique in a formal user study. Our experiments show that users were able to perform all tasks significantly faster with our technique without loosing any precision. Most of the subjects considered the technique easy to use and satisfying.	3d interaction;cursor (databases);experiment;interaction technique;multi-touch;stereoscopy;usability testing;yaws	Sven Strothoff;Dimitar Valkov;Klaus H. Hinrichs	2011		10.1145/2076354.2076377	computer vision;simulation;computer science;computer graphics (images)	HCI	-44.00956987363532	-46.90442175244304	187598
51888b4355b9c3757da53b0f41604b3d68ae232e	placing information near to the gaze of the user	miscellaneous;h 5 m information interfaces and presentation;miscellaneous h 5 m information interfaces and presentation	Gaze tracking facilities have yet mainly been used in general for marketing or the disabled and, more specifically, in Augmented Reality, for interaction with control triggers, such as buttons. We go one step further and use the line of sight of the user to attach information. While any information may not conceal the view of the user, we displace the information by an angular degree and provide means for the user to capture the information by looking at it. With such an apporach we see a potential for faster resuming times of the original task for which a required information needs to be accessed. The demonstration shows a comparably complex primary task assisted by our gaze-mounted information and illustrates the inherent differences for information access w.r.t. conventional methods, such as listing action items at a fix position in space or on a screen.	angularjs;augmented reality;eye tracking;information access;information needs	Marcus Tönnis;Gudrun Klinker	2014		10.1109/ISMAR.2014.6948497	computer vision;human–computer interaction;computer science;multimedia	HCI	-44.62726849728181	-45.62010780627019	187664
6c6332229d2aab6fc156425cfab31df4c50ff164	the influence of age and device orientation on the performance of touch gestures		Touch interaction has become a popular and widespread interaction technique. Recent studies indicate significant potential for touch interaction with regard to the integration of older adults into the world of ICT. We carried out a study with the goal of gaining deeper insight into performance differences between young and old users as well as the influence of tablet device orientation on performance. We implemented an application for the iPad that measures various performance characteristics when performing six gestural tasks—tap, drag, pinch, pinch-pan, rotate left and rotate right—for both portrait and landscape orientations. Results showed the importance of device orientation as an influencing factor on performance and indicate that age is not the exclusive influencing factor on touch interaction performance.		Linda Wulf;Markus Garschall;Michael Klein;Manfred Tscheligi	2014		10.1007/978-3-319-08599-9_86	computer vision;multimedia;communication	HCI	-48.08604810726075	-45.26796371424074	188009
c17eacf2b07e7aac259b0065342e9f2466442c21	[poster] vergence-based ar x-ray vision	visualization three dimensional displays augmented reality x ray imaging yttrium indexes brain computer interfaces;augmented reality vergence based ar x ray vision selective visualization method opacity users gaze depth ar ghosted views;gaze contingent display;ghosted views;vergence;x ray imaging augmented reality opacity;visibility;x ray vision;augmented reality x ray vision ghosted views visibility vergence gaze contingent display;augmented reality	The ideal AR x-ray vision should enable users to clearly observe and grasp not only occludees, but also occluders. We propose a novel selective visualization method of both occludee and oc-cluder layers with dynamic opacity depending on the user's gaze depth. Using the gaze depth as a trigger to select the layers has a essential advantage over using other gestures or spoken commands in the sense of avoiding collision between user's intentional commands and unintentional actions. Our experiment by a visual paired-comparison task shows that our method has achieved a 20% higher success rate, and significantly reduced 30% of the average task completion time than a non-selective method using a constant and half transparency.	speech recognition;vergence	Yuki Kitajima;Sei Ikeda;Kosuke Sato	2015	2015 IEEE International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2015.58	computer vision;augmented reality;simulation;visibility;computer science;vergence;computer graphics (images)	Visualization	-44.189729137080946	-48.006585542834415	188420
047e7a31ab5f1d1178b378fbf3ced48d81613bc6	gamification and virtual reality for teaching mobile x-ray imaging		Mobile x-ray devices like C-arms are routinely used in the operating room to assist in surgical interventions. C-arms need to be adequately operated to avoid unnecessary radiation exposure and intraoperative delays. We present a simulation-based training system to support training of operating room personnel in a virtual environment. To make learning more efficient and enjoyable we integrated aspects of gamification. In a game-like setting the user has to generate specific radiographs of a virtual patient. Game points are awarded with regard to image accuracy time needed and overall radiation exposure. To learn basics of C-arm handling and x-ray imaging non-medical objects can be examined as a first step. A virtual reality interface is provided to allow the user to interact with the C-arm in a more realistic way. User evaluations show that this approach is widely appreciated as providing a user-friendly and sufficiently realistic training tool with a high educational value for intraoperative C-arm imaging.	coat of arms;gamification;radiography;simulation;usability;virtual reality	Matthias Süncksen;Henner Bendig;Michael Teistler;Markus Wagner;Oliver J. Bott;Klaus Dresing	2018	2018 IEEE 6th International Conference on Serious Games and Applications for Health (SeGAH)	10.1109/SeGAH.2018.8401364	task analysis;simulation;training system;virtual reality;virtual machine;virtual patient;radiation exposure;computer science	Visualization	-41.60127765966015	-47.02889320256227	188645
02e8cd1844ba3b1a91331ce01bfbfff82bfbf7eb	taptap and magstick: improving one-handed target acquisition on small touch-screens	mobile device;touch screen;one handed interaction;interaction techniques;controlled experiment;target selection;thumb interaction;touch screens;error rate;mobile devices;target acquisition;interaction technique	We present the design and evaluation of TapTap and MagStick, two thumb interaction techniques for target acquisition on mobile devices with small touch-screens. These two techniques address all the issues raised by the selection of targets with the thumb on small tactile screens: screen accessibility, visual occlusion and accuracy. A controlled experiment shows that TapTap and MagStick allow the selection of targets in all areas of the screen in a fast and accurate way. They were found to be faster than four previous techniques except Direct Touch which, although faster, is too error prone. They also provided the best error rate of all tested techniques. Finally the paper also provides a comprehensive study of various techniques for thumb based touch-screen target selection.	accessibility;cognitive dimensions of notations;context (computing);drag and drop;experiment;interaction technique;mobile device;multiplexing;touchscreen;zooming user interface	Anne Roudaut;Stéphane Huot;Eric Lecolinet	2008		10.1145/1385569.1385594	computer vision;simulation;human–computer interaction;computer science;operating system;mobile device	HCI	-45.95528568599096	-45.360471716632425	188914
6f7f4b48357b104fb3ddf8778f70a0be33a421a6	subtle cueing for visual search in head-tracked head worn displays	visual clutter subtle cueing head tracked head worn displays goal oriented visual search augmented reality scene distortion occlusion;clutter;visualization augmented reality head magnetic heads clutter erbium mobile communication;query formulation;query formulation augmented reality clutter helmet mounted displays;visual search attention subtle visual cueing;augmented reality;helmet mounted displays	Goal-oriented visual search in augmented reality can be facilitated by using visual cues to call attention to a target. However, traditional use of explicit cues can degrade visual search performance due to scene distortion, occlusion and addition of visual clutter. In contrast, Subtle Cueing has been previously proposed as an alter-native to explicit cueing, but little is known about how well it works for head-tracked head worn displays (HWDs). We investigated the effect of Subtle Cueing for head-tracked head worn displays, using visual search research methods in simulated augmented reality environments. Our user study found that Subtle Cueing improves visual search performance, and serves as a feasible cueing mechanism for AR environments using HWDs.	augmented reality;clutter;distortion;usability testing	Weiquan Lu;Dan Feng;Steven K. Feiner;Qi Zhao;Henry Been-Lirn Duh	2013	2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)	10.1109/ISMAR.2013.6671800	computer vision;augmented reality;computer science;clutter;computer graphics (images)	Visualization	-44.44176863134389	-47.71470416687626	189065
d03b4252c7ffe7abc0608bcf8445a131f8d47659	pressure-based menu selection for mobile devices	mobile device;pressure input;target selection;non visual feedback;visual feedback;mobile interaction	Despite many successes in desktop applications, little work has looked at the use of pressure input on mobile devices and the different issues associated with mobile interactions e.g. non-visual feedback. This study examined pressure input on a mobile device using a single Force Sensing Resistor (FSR) with linearised output as a means of target selection within a menu, where target menu items varied in size and location along the z-axis. Comparing visual and audio feedback, results showed that, overall, eyes-free pressure interaction reached a mean level of 74% accuracy. With visual feedback mean accuracy reached 85%. Participants could accurately distinguish up to 10 pressure levels when given adequate feedback indicating a high level of control.	apache axis;audio feedback;desktop computer;high-level programming language;interaction;mobile device	Graham A. Wilson;Craig D. Stewart;Stephen A. Brewster	2010		10.1145/1851600.1851631	embedded system;simulation;mobile interaction;human–computer interaction;computer hardware;computer science;operating system;mobile device	HCI	-46.85568784602368	-46.166880730213535	190075
7794aba6aec9b1297bf31c76d6a58daf19c27f14	rfid in robot-assisted indoor navigation for the visually impaired	potential fields algorithm robot assisted indoor navigation visually impaired radio frequency identification;oscillations;radiofrequency identification cognitive robotics dogs robot sensing systems computer science radio navigation orbital robotics testing legged locomotion passive rfid tags;navigation handicapped aids service robots radiofrequency identification indoor radio;service robots;rfid tag;potential field;navigation;handicapped aids;indoor environment;visual impairment;radio frequency identification;indoor radio;radiofrequency identification	We describe how radio frequency identification (RFID) can be used in robot-assisted indoor navigation for the visually impaired. We present a robotic guide for the visually impaired that was deployed and tested both with and without visually unpaired participants in two indoor environments. We describe how we modified the standard potential fields algorithms to achieve navigation at moderate walking speeds and to avoid oscillation in narrow spaces. The experiments illustrate that passive RFID tags deployed in the environment can act as reliable stimuli that trigger local navigation behaviors to achieve global navigation objectives.	algorithm;experiment;radio frequency;radio-frequency identification;robot	Vladimir A. Kulyukin;Chaitanya Gharpure;John Nicholson;Sachin Pavithran	2004	2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566)	10.1109/IROS.2004.1389688	radio-frequency identification;computer vision;simulation;engineering;mobile robot navigation;quantum mechanics	Robotics	-41.46399941761301	-48.735236976491166	190154
062233be91e45b055eb616da47a4e6fe718841ed	kinesthetic cues aid spatial memory	human memory;spatial memory;user study;touchscreen;gender effects;gender effect;kinesthesis;human computer interface	"""We are interestind in building and evaluating human computer interfaces that make information more memorable. Psychology research informs us that humans access memories through cues, or """"memory hooks,"""" acquired at the time we learn the information. In this paper, we show that kinesthetic cues, or the awareness of parts of our body's position with respect to itself or to the environment, are useful for recalling the positions of objects in space. We report a user study demonstrating a 19% increase in spatial memory for information controlled with a touchscreen, which provides direct kinesthetic cues, as compared to a standard mouse interface. We also report results indicating that females may benefit more than males from using the touchscreen device."""	human computer;institute for operations research and the management sciences;touchscreen;usability testing	Desney S. Tan;Randy F. Pausch;Jeanine Stefanucci;Dennis Proffitt	2002		10.1145/506443.506607	spatial memory;multimedia;proprioception;memory	HCI	-46.59607506323996	-48.00860590815593	190258
fb5c7c23dc661d35517fdb2dc72383cce4142670	automatic categorization of haptic interactions -what are the typical haptic interactions between a human and a robot?	hierarchical structure;distance function;tactile sensors haptic interfaces human computer interaction man machine systems robots;human computer interaction;communication robot;robot soft skin;human like communication automatic haptic interaction categorization human robot interactions communication robot tactile sensors robot soft skin distance function touching;human robot interactions;tactile sensor;human like communication;automatic haptic interaction categorization;haptic interfaces human robot interaction robotics and automation robot sensing systems tactile sensors intelligent robots grasping laboratories skin communication effectiveness;robots;tactile sensors;haptic interfaces;touching;man machine systems;haptic interaction	If a communication robot could have the same capability of haptic interaction as humans do, the robot would give us greater familiarity, thus shortening its communicative distance from people. To achieve such a robot, this paper proposes a method for categorizing haptic interactions between a human and a robot by using tactile sensors embedded in the soft skin covering the robot's entire body. In this method, each haptic interaction can be represented by a sequence of data points recorded from the tactile sensors. We first consider all experimental data points and map them into clusters. Next, we represent each individual haptic interaction as a sequence of those clusters. Finally, by calculating a distance function between each of these sequences of clusters, we can determine a hierarchical structure that naturally groups similar haptic interactions into categories. In the hierarchical structure, the haptic interactions are classified mainly based on the region of touching (e.g., head and arm) at the top side of the structure and the manner of touching (e.g., slap and stroke) at the bottom side of the structure. By using this hierarchical structure, the robot can categorize haptic interactions based simultaneously on the region of its body touched and on the manner of touching. The constructed hierarchical structure will also make the robot aware of the categorized regions and the categorized manners of touching that are typically observed in haptic communication between a human and a robot. Consequently, this capability will lead to more human-like communication. To confirm the validity of this method, we conducted human-robot communication experiments using 284 subjects. As a result, we clarified that the manner of touching depends on the region of touching.	categorization;haptic technology;interaction;robot	Taichi Tajika;Takahiro Miyashita;Hiroshi Ishiguro;Norihiro Hagita	2006		10.1109/ICHR.2006.321318	stereotaxy;computer vision;simulation;computer science;artificial intelligence;tactile sensor	Robotics	-41.5943678475275	-48.216936954095516	190459
8b64ddc95ba836f3bc3d3be3dd9e240c8c842818	comparing direct and remote tactile feedback on interactive surfaces	remote tactile feedback;interactive surfaces;touch input	Tactile feedback on touch surfaces has shown to greatly improve the interaction in quantitative and qualitative metrics. Recently, researchers have assessed the notion of remote tactile feedback, i.e., the spatial separation of touch input and resulting tactile output on the user’s body. This approach has the potential to simplify the use of tactile feedback with arbitrary touch devices and allows the design of novel tactile stimuli by stimulus combination. However, a formal comparison of direct and remote tactile feedback during touch input is still missing. Therefore, we conducted three consecutive laboratory studies. First, we compared the effects of both direct and remote tactile stimuli on the user’s performance during touchscreen interactions. No difference was found in the positive effects of both types of feedback. Second, we evaluated the impact of the remote tactile actuator’s position on the user’s body. For remote tactile stimuli, we found improved accuracy and interaction speed, regardless of the body location. Third we analyzed remote tactile feedback under additional cognitive load. The results support the positive effects of tactile feedback on user performance and subjective evaluation. These findings encourage us to further exploit the potential of remote tactile feedback to simplify and expand the multimodal interaction with arbitrary touch interfaces.	feedback;multimodal interaction;remote touch;touchscreen	Hendrik Richter;Sebastian Löhmann;Florian Weinhart;Andreas Butz	2012		10.1007/978-3-642-31401-8_28	computer vision;simulation;computer science;communication	HCI	-46.12568440963834	-47.162708613945895	190670
25e52a4f43e4afd8cfdc6aea95e34265d01f91d8	tap or touch?: pen-based selection accuracy for the young and old	negative affect;input device;handheld computer;tap;handheld;older adult;pocketpc;pda;accuracy;touch;pen;stylus	"""The effect of the decline in cognitive, perceptive, and motor abilities on older adults' performance with input devices has been well documented in several experiments. None of these experiments, however, have provided information on the challenges faced by older adults when using pens to interact with handheld computers. To address this need, we conducted a study to learn about the performance of older adults in simple pen-based tasks with handheld computers. The study compared the performance of twenty 18-22 year olds, twenty 50-64 year olds, and twenty 65-84 year olds. We found that for the most part, older adults were able to complete tasks accurately. An exception occurred with the low accuracy rates achieved by 65-84 year old participants when tapping on targets of the same size as the standard radio buttons, checkboxes, and icons on the PocketPC. An alternative selection technique we refer to as """"touch"""" enabled 65-84 year olds to select targets more accurately. This technique did not negatively affect the performance of the other participants. If tapping to select, making standard-sized targets 50 percent larger provided 65-84 year olds with similar advantages to switching to """"touch"""" interactions. The results suggest that """"touch"""" interactions need to be further explored to understand whether they will work in more realistic situations."""	computer;experiment;input device;interaction;mobile device;pocket pc	Juan Pablo Hourcade;Theresa R. Berkel	2006		10.1145/1125451.1125623	simulation;computer hardware;computer science;operating system;mobile device;multimedia;stylus;input device	HCI	-47.38250304326034	-45.53985335949247	191174
b3b1804698a01d00b103b1ead889adda1fd623d8	the influence of feedback on egocentric distance judgments in real and virtual environments	virtual environments;feedback;adaptation;virtual environment;space perception;head mounted display	A number of investigators have reported that distance judgments in virtual environments (VEs) are systematically smaller than distance judgments made in comparably-sized real environments. Many variables that may contribute to this difference have been investigated but none of them fully explain the distance compression. One approach to this problem that has implications for both VE applications and the study of perceptual mechanisms is to examine the influence of the feedback available to the user. Most generally, we asked whether feedback within a virtual environment would lead to more accurate estimations of distance. Next, given the prediction that some change in behavior would be observed, we asked whether specific adaptation effects would generalize to other indications of distance. Finally, we asked whether these effects would transfer from the VE to the real world. All distance judgments in the head-mounted display (HMD) became near accurate after three different forms of feedback were given within the HMD. However, not all feedback sessions within the HMD altered real world distance judgments. These results are discussed with respect to the perceptual and cognitive mechanisms that may be involved in the observed adaptation effects as well as the benefits of feedback for VE applications.	feedback;head-mounted display;virtual reality	Betty J. Mohler;Sarah H. Creem-Regehr;William B. Thompson	2006		10.1145/1140491.1140493	computer vision;simulation;computer science;virtual machine;optical head-mounted display;operating system;feedback;communication;adaptation	HCI	-44.397297378220514	-48.996282815352714	191407
3db82d8a1940c4d48bb359aed9c0dfb258e5f898	participatory design: repositioning, transferring, and personal care robots	repositioning;personal care;robotics;transferring	In our ongoing survey, we examine the feedback of robotic aids that increases independence of persons with physical disabilities. Participants are asked to evaluate the perceived pros and cons of prototypes for repositioning, transferring, and personal care by viewing 3D video simulations. Using participatory design, our goal is to develop robotics aids by actively involving prospective consumers to achieve functionality and long-term acceptability.	prospective search;robot;simulation	Kavita Krishnaswamy	2017		10.1145/3029798.3034815	simulation;computer science;artificial intelligence;multimedia;robotics	HCI	-41.28683208244861	-45.88058390081428	192230
bd4b4325d1be9018e9f7ab1d870b94162f8e9025	factors affecting mouse-based 3d selection in desktop vr systems	mouse;head tracking;mouse cursors;stereo 3d display	We present two experiments on mouse-based point selection in a desktop virtual reality system using stereo display and head-tracking. To address potential issues of using a mouse cursor with stereo display, we also evaluate the impact of using a one-eyed (mono) cursor. While a one-eyed cursor visualization eliminates depth conflicts, recent work suggests it offers worse performance than stereo cursors, possibly due to eye discomfort. Our results indicate that presenting the cursor in stereo significantly reduces performance for targets at different depths. The one-eyed cursor eliminates this effect, offering better performance than both screen-plane and geometry-sliding cursors visualized in stereo. However, it also performed slightly worse than stereo cursors in situations without depth conflicts. Our study suggests that this difference is not due exclusively to the relative transparency of such a cursor, hence eye fatigue or similar may be responsible.	cursor (databases);desktop computer;experiment;pointer (user interface);stereo display;virtual reality	Robert J. Teather;Wolfgang Stuerzlinger	2015		10.1145/2788940.2788946	computer vision;simulation;computer science;computer graphics (images)	HCI	-44.18809202776049	-47.90156551760146	192259
7c9bc10bde0c53d9ba1326f9b96abed1e1fb9967	homuncular flexibility in virtual reality	virtual reality;homuncular flexibility;avatars;tool use;body schema	Immersive virtual reality allows people to inhabit avatar bodies that differ from their own, and this can produce significant psychological and physiological effects. The concept of homuncular flexibility (Lanier, 2006) proposes that users can learn to control bodies different from their own by changing the relationship between tracked and rendered motion. We examine the effects of remapping movements in the real world onto an avatar that moves in novel ways. In Experiment 1, participants moved their legs more than their arms in conditions where leg movements were more effective for the task. In Experiment 2, participants controlling 3-armed avatars learned to hit more targets than participants in 2-armed avatars. We discuss the implications of embodiment in novel bodies.	avatar (computing);coat of arms;experiment;immersion (virtual reality);virtual reality	Andrea Stevenson Won;Jeremy N. Bailenson;Jimmy Lee;Jaron Lanier	2015	J. Computer-Mediated Communication	10.1111/jcc4.12107	psychology;simulation;computer science;virtual reality;body schema;multimedia;computer graphics (images)	HCI	-45.77524195013786	-49.104545579768974	194263
83f4ed63333abb880611ef0d9fd8e04cb8ca9e0e	study of children's hugging for interactive robot design	robot sensing systems;skin;pressure measurement;bladder;pressure sensors	We have developed a toy sized humanoid robot with soft air-filled modules on its links which sense contact and protect the robot and any interacting humans from damaging collisions. This robot, meant for robust physical interaction, is required to endure contact with children in the form of hugs and other playful interactions. It is therefore necessary to quantify the forces exerted during these interactions so that robots can be designed to both withstand these forces, as well as interact safely and intuitively in these situations. To quantify the range of forces exerted by children when performing both soft and strong hugs, we conducted a study in which 28 children (11 boys, 17 girls) between 4 and 10 years old hugged a pressure sensing doll while the pressure was recorded. We found a child's maximum expected hugging force (2.623 psi for our setup) during free play. The data gathered in this study will guide the further development of our physically interactive robot.	humanoid robot;human–computer interaction	Joohyung Kim;Alexander Alspach;Iolanda Leite;Katsu Yamane	2016	2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2016.7745173	simulation;pressure measurement;artificial intelligence;social robot;pressure sensor;skin	Robotics	-45.93081419362596	-50.33560958921906	194487
256965b0c89501a7278c8df5a328bb16b4c1225e	a design of hardware haptic interface for gastrointestinal endoscopy simulation.	conference	Gastrointestinal endoscopy simulations have been developed to train endoscopic procedures which require hundreds of practices to be competent in the skills. Even though realistic haptic feedback is important to provide realistic sensation to the user, most of previous simulations including commercialized simulation have mainly focused on providing realistic visual feedback. In this paper, we propose a novel design of portable haptic interface, which provides 2DOF force feedback, for the gastrointestinal endoscopy simulation. The haptic interface consists of translational and rotational force feedback mechanism which are completely decoupled, and gripping mechanism for controlling connection between the endoscope and the force feedback mechanism.		Yunjin Gu;Doo Yong Lee	2011	Studies in health technology and informatics	10.3233/978-1-60750-706-2-199	embedded system;simulation;human–computer interaction;computer science	HCI	-41.70310046601387	-46.8466975523115	194627
696b629845c7206157647f5d3ba8c22b03b3b4bf	"""evaluation of disambiguation mechanisms of object-based selection in virtual environment: which performances and features to support """"pick out""""?"""	3d virtual environment;human computer interaction;object selection;disambiguation mechanism	A 3D object selection technique is an essential building part of interactive virtual environment system. This ability to choose which object is the target for subsequent actions precedes any further behavior. Selection mechanism as part of manipulation process in a virtual environment is performed using different techniques. Furthermore, many problems make selection a difficult task and contribute to target ambiguity that leads users to select wrong objects by mistake. Therefore, disambiguation mechanisms have been proposed in the literature to solve target ambiguity. In this paper, we present an evaluation study of selection techniques that support disambiguation mechanisms.  We performed a comparative study of 4 selection techniques including disambiguation mechanisms: Ray-Casting, Depth Ray, SQUAD and Shadow Cone. We developed an automated test environment where we varied scene density and measured the performance of each selection technique. Our goal was to measure the velocity of each technique, its sensitivity to occlusion, how much navigation it required and how far it could select targets. We found that discrete progressive refinement used in SQUAD was the fastest solution when user acts in a scene containing few objects to select. We also found that techniques that augment the pointing metaphor by using a 3D cursor, such as Depth Ray and SQUAD, performed better than ray casting itself or cone casting.	cursor (databases);deployment environment;fastest;hidden surface determination;object-based language;performance;progressive refinement;ray casting;refinement (computing);shadow copy;test automation;velocity (software development);virtual reality;word-sense disambiguation	Zouhair Serrar;Nabil Elmarzouqi;Zahi Jarir;Jean-Christophe Lapayre	2014		10.1145/2662253.2662282	computer vision;simulation;human–computer interaction;computer science;artificial intelligence	HCI	-44.671543128045904	-47.045930374682825	194887
93b01c5ba73bbfb6196e4997a794a9e104a2dc9c	field observations of therapists conducting virtual reality exposure treatment for the fear of flying	exposure treatment;user interface;virtual reality;task analysis;field observations;exposure therapy	Recent research suggests Virtual Reality Exposure Therapy (VRET) for the treatment of fear of flying as an important reliable technique for this phobia. This paper focuses on the role of the therapist during an exposure session. Six therapists were observed in 14 sessions with 11 different patients. Results show that in 93% of the observed sessions, therapists started with a similar flight pattern. Furthermore, a total of 20 errors were observed where therapists initiated inappropriate sound recordings such as pilot or purser announcements. Findings suggest that the system might be improved by providing the therapist with automatic flying scenarios.	virtual reality therapy	Willem-Paul Brinkman;Guntur Sandino;Charles van der Mast	2009			simulation;computer science;task analysis;virtual reality;multimedia;user interface	Visualization	-48.16988828977359	-51.38803555697575	194913
e02a8d2a848b063b73559d541bc6118afdc5ecd1	modifying keyboard layout to reduce finger-travel distance	keyboards;layout;simulated annealing;thumb;human factors;mobile handsets;mathematical model	Ample evidence shows that the Qwerty layout is inefficient for one-finger typing on virtual keyboards: letters in common digrams are placed on opposite sides of the keyboard, resulting in an unnecessarily long finger-travel distance. In this paper, we report on using simulated annealing for modifying keyboard layout to reduce finger-travel distance for entering letters of the English alphabet. Our investigation led to improved 3 X 10, 4 X 7, and 5 X 6 layouts, all of which reduced the weighted sum of finger-travel distances for all digrams (denoted by d) by about 40% over Qwerty. The most improved layout is the 5 X 6 layout, which reduces d from 3.31 key widths (the value of d for Qwerty) to 1.78 key widths. These layouts also outperform the best layout reported in the literature.	iterative method;plover;simulated annealing;virtual keyboard;weight function	Nan Yang;Amol Dattatraya Mali	2016	2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2016.0034	layout;embedded system;mathematical optimization;simulation;simulated annealing;computer science;human factors and ergonomics;mathematical model;statistics	Visualization	-46.45782599117239	-45.23278991057793	194973
6757e93b9e8b7da08f2d52e0fbb7fa4656e8ce17		zooming user interfaces;spatial memory;text entry;emoji;soft keyboard;interaction technique;mobile input	Current soft keyboards for emoji entry all present emoji in the same way: in long lists, spread over several categories. While categories limit the number of emoji in each individual list, the overall number is still so large, that emoji entry is a challenging task. The task takes particularly long if users pick the wrong category when searching for an emoji. Instead, we propose a new zooming keyboard for emoji entry. Here, users can see all emoji at once, aiding in building spatial memory where related emoji are to be found. We compare our zooming emoji keyboard against the Google keyboard and find that our keyboard allows for 18% faster emoji entry, reducing the required time for one emoji from 15.6 s to 12.7 s. A preliminary longitudinal evaluation with three participants showed that emoji entry time over the duration of the study improved at up to 60 % to a final average of 7.5 s.	emoji;image resolution;input method;smartphone	Henning Pohl;Dennis Stanke;Michael Rohs	2016		10.1145/2935334.2935382	spatial memory;simulation;human–computer interaction;computer science;multimedia;world wide web;interaction technique	HCI	-46.9754123619131	-45.423862380146176	195065
5eb41f788d1bddc83606a919b6761f94a12c4413	touchscreens vs. traditional controllers in handheld gaming	mobile device;learning curve;game design;touchscreen;tactile feedback;game input;handheld gaming;haptic feedback;hardware design;fingertip interaction;mobile interaction	We present a study which compares touchscreen-based controls and physical controls for game input using Ubisoft's Assassin's Creed: Altair's Chronicles. Our study used the Apple iPhone as a representative touchscreen-based controller and the Nintendo DS for its physical control pad. Participants completed a game level four times on each platform. Level completion time and number of player deaths were recorded. Results indicate that physical buttons allowed significantly better performance than virtual buttons. Specifically, the number of character deaths on the iPhone was 150% higher than on the DS, while level completion time on the DS was 42% faster. The learning curve for the touchscreen version of the game was also steeper. Participants strongly preferred the physical buttons of the Nintendo DS. We conclude that either game designers should consider alternative input methods for touchscreen devices, or hardware designers should consider the inclusion of physical controls.	assassin's creed;gamepad;handheld game console;input method;touchscreen	Loutfouz Zaman;Daniel Natapov;Robert J. Teather	2010		10.1145/1920778.1920804	embedded system;simulation;engineering;multimedia	HCI	-47.13091236750155	-45.460559748954054	195382
8e93048d4cf0a6e0c7cdc3af29bbdef1eac9a53f	one-dimensional handwriting: inputting letters and words on smart glasses	text entry;unistroke gestures;one dimensional input	We present 1D Handwriting, a unistroke gesture technique enabling text entry on a one-dimensional interface. The challenge is to map two-dimensional handwriting to a reduced one-dimensional space, while achieving a balance between memorability and performance efficiency. After an iterative design, we finally derive a set of ambiguous two-length unistroke gestures, each mapping to 1-4 letters. To input words, we design a Bayesian algorithm that takes into account the probability of gestures and the language model. To input letters, we design a pause gesture allowing users to switch into letter selection mode seamlessly. Users studies show that 1D Handwriting significantly outperforms a selection-based technique (a variation of 1Line Keyboard) for both letter input (4.67 WPM vs. 4.20 WPM) and word input (9.72 WPM vs. 8.10 WPM). With extensive training, text entry rate can reach 19.6 WPM. Users' subjective feedback indicates 1D Handwriting is easy to learn and efficient to use. Moreover, it has several potential applications for other one-dimensional constrained interfaces.	algorithm;iterative design;iterative method;language model;palm os;smartglasses;words per minute	Chun Yu;Ke Sun;Mingyuan Zhong;Xincheng Li;Peijun Zhao;Yuanchun Shi	2016		10.1145/2858036.2858542	natural language processing;speech recognition;computer science	HCI	-46.826389691198386	-45.15455637583608	195498
9e601265b6a5dd0251242685ddb3414ac175c404	haptic glove for finger rehabilitation	exergames;rehabilitation;motion pictures;virtual reality;haptic interfaces visualization motion pictures games force virtual reality actuators;actuators;force;exergames haptic rehabilitation virtual reality;visualization;raising a cup game haptic glove finger rehabilitation fingers impairment virtual environment haptic feedback fingers strength patient rehabilitation program exergames squeezing a ball game;games;virtual reality computer games data gloves patient rehabilitation;haptic interfaces;haptic	The impairment of the fingers is one of the main problems that prevents patients performing their daily activities. Researchers have used virtual environment combined with haptic feedback to improve the strength of the fingers. In this study, we design and implement a cost-effective, light, and easy to use haptic glove that helps patients in their rehabilitation program. The virtual environment are consisted of two exergames namely, squeezing a ball game, and raising a cup game. The system is tested on 20 healthy subjects for four days. Since the preliminary results have shown a general acceptance of the proposed system by the users, we can assume that there is a clear potential for haptic feedback to improve the overall performance of the users.	haptic technology;power glove;virtual reality;wired glove	Mohamad Hoda;Basim Hafidh;Abdulmotaleb El-Saddik	2015	2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2015.7169803	games;simulation;visualization;human–computer interaction;computer science;virtual reality;multimedia;haptic technology;force;actuator	Robotics	-42.4297030091509	-46.29657591820448	196735
02736c545ad0bc497c8cf6662d8080807961bb3c	peltier haptic interface (phi) for improved sensation of touch in virtual environments	virtual reality;virtual environment;haptic interface	In this report we describe an advanced virtual reality glove that we are developing, called the Peltier Haptic Interface (PHI), which will provide improved sensation of touch in virtual environments. PHI will provide force/pressure feedback that can be varied independently on each finger, as well as temperature sensation that can be varied non-uniformly over the whole hand. The combination of these sensations will provide a more realistic sense of touch and significantly increase the realism of virtual environments. PHI will find extensive applications in biomedical simulations, teaching, industrial line training, and many other areas.	haptic technology;simulation;virtual reality	P. Sines;B. Das	1999	Virtual Reality	10.1007/BF01421809	simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;virtual reality;multimedia;haptic technology	Visualization	-42.01340303989275	-47.2887097315351	196740
58e3ac7d617c038a6611fc16f84981b94f089f13	perceived instability of virtual haptic texture. i. experimental studies	surface rendering;detection threshold;force feedback;virtual environment;frequency domain;high frequency;haptic interface	This paper presents a quantitative characterization of the instability that a human user often experiences while interacting with a virtual textured surface rendered with a force-reflecting haptic interface. First, we quantified the degree of stability/ instability during haptic texture rendering through psychophysical experiments. The stiffness of the virtual textured surface upon detection of instability was measured under a variety of experimental conditions using two texture rendering methods, two exploration modes, and various texture model parameters. We found that the range of stiffness values for stable texture rendering was quite limited. Second, we investigated the attributes of the proximal stimuli experienced by a human hand while exploring the virtual textured surface in an attempt to identify the sources of perceived instability. Position, force, and acceleration were measured and then analyzed in the frequency domain. The results were characterized by sensation levels in terms of spectral intensity in dB relative to the human detection threshold at the same frequency. We found that the spectral bands responsible for texture and instability perception were well separated in frequency such that they excited different mechanoreceptors and were, therefore, perceptually distinctive. Furthermore, we identified the high-frequency dynamics of the device to be a likely source of perceived instability. Our work has implications for displaying textured surfaces through a force feedback device in a virtual environment.	algorithm;control theory;encoder;experiment;frequency band;haptic technology;imaging phantom;instability;interaction;quantization (signal processing);rendering (computer graphics);resonance;stylus (computing);velocity (software development);virtual reality	Seungmoon Choi;Hong Z. Tan	2004	Presence: Teleoperators & Virtual Environments	10.1162/1054746041944867	computer vision;simulation;computer science;artificial intelligence;haptic technology	Visualization	-43.68452711227823	-50.15043377777846	197568
c595050390451c790b3916b8ac383aba6fc1f679	ar based environment for exposure therapy to mottephobia	augmented reality;exposure therapy;mottephobia	Mottephobia is an anxiety disorder revolving around an extreme, persistent and irrational fear of moths and butterflies leading sufferers to panic attacks. This study presents an ARET (Augmented Reality Exposure Therapy) environment aimed to reduce mottephobia symptoms by progressive desensitization. The architecture described is designed to provide a greater and deeper level of interaction between the sufferer and the object of its fears. To this aim the system exploits an inertial ultrasonic-based tracking system to capture the user’s head and wrists positions/orientations within the virtual therapy room, while a couple of instrumented gloves capture fingers’ motion. A parametric moth behavioral engine allows the expert monitoring the therapy session to control many aspects of the virtual insects augmenting the real scene as well as their interaction with the sufferer.	augmented reality;desensitization (telecommunications);head-mounted display;progressive scan;tracking system;virtual reality therapy;wired glove	Andrea F. Abate;Michele Nappi;Stefano Ricciardi	2011		10.1007/978-3-642-22021-0_1	simulation;engineering;forensic engineering;communication	HCI	-41.99407455705825	-45.51080516621376	197848
b5aa77cde8ac09bd821cfd34d0a0d1c12688779b	human-computer interaction. towards intelligent and implicit interaction		People with severe speech or language problems rely on augmentative and alternative communication (AAC) to supplement existing speech or replace speech that is not functional. However, many people with severely motor disabilities are limited to use AAC, because most of AAC use the mechanical input devices. In this paper, to solve the limitations and offer a practical solution to disabled person, a virtual keyboard system using a biosignal interface is developed. The developed system consists of bio-signal interface, training and feedback program, connecting module and virtual keyboard. In addition, we evaluate how well do subjects control the system. From results of preliminary usability test, the usefulness of the system is verified.	advanced audio coding;british informatics olympiad;human–computer interaction;input device;usability testing;virtual keyboard	Masaaki Kurosu	2013		10.1007/978-3-642-39342-6	human–computer interaction;computer science	HCI	-41.558809869831535	-45.40606809094665	197933
89b290c58f17858d90f3163f4a62b1a953c6df9c	developing and evaluating two gestural-based virtual environment navigation methods for large displays	gestural interaction;navigation in virtual environments;3duis	In this paper we present two methods to navigate in virtual environments displayed in a large display using gestures detected by a depth sensor. We describe the rationale behind the development of these methods and a user study to compare their usability performed with the collaboration of 17 participants. The results suggest the users have a better performance and prefer one of them, while considering both as suitable and natural navigation methods.	design rationale;range imaging;usability testing;virtual reality	Paulo Dias;João Parracho;João Cardoso;Beatriz Quintino Ferreira;Carlos Ferreira;Beatriz Sousa Santos	2015		10.1007/978-3-319-20804-6_13	simulation;human–computer interaction;engineering;multimedia	HCI	-45.41390700558982	-46.87957292478034	198005
87f5c5e492ef7471952660310720051a7c80cb5f	virtual restraint of hand position and posture for cooperative virtual object manipulation with ungrounded force display device	virtual restraint virtual coupling cooperative manipulation;virtual reality;virtual reality cooperative systems force feedback;force feedback;task performance virtual restraint hand position hand posture cooperative virtual object manipulation cooperative object manipulation shared virtual environments ungrounded force display devices holding object remote users virtual force virtual coupling object manipulation assistance method virtual hands restraint recognition restraint force evaluation experiments;cooperative systems;force thumb vectors haptic interfaces delays	In cooperative object manipulation in shared virtual environments with ungrounded force display devices, a holding object is involuntarily released when the remote users move apart. That is caused by excessive virtual force generated by an uninhibited increase in distance between the users and virtual coupling. This study proposes an object manipulation assistance method that virtually restrains remote users' hand positions and postures. Hand positions and postures are adjusted to partially match the virtual hands. To assist the recognition of restraint between users' hands, a method of enhancing the restraint force is also proposed. The results of the evaluation experiments indicated the feasibility for improving task performance in cooperative object manipulation.	algorithm;display device;experiment;poor posture;virtual reality	Kinya Fujita;Yukinobu Takehana;Katsuhiro Kamata	2013	2013 World Haptics Conference (WHC)	10.1109/WHC.2013.6548411	computer vision;simulation;computer science;engineering;artificial intelligence;virtual reality;haptic technology	Visualization	-45.08670660068986	-48.16508296920777	198846
aef8d37c493901ed94acf4abcc0ed561d82b394d	game system of coordination skills training for elderly people		In this paper, we propose upper-limb-grasp motion as coordinated movement of five fingers and upper limbs. This coordination skill is important of daily living and we use unconscious. We divide upper-limb-grasp motion into 6 motion elements game system analysis and visualize with upper-limb-grasp motion measurement controller and game contents. We compared elderly and younger people and consider the result. As the result, we research exercise menu and game contents, technique of visualization.		Nobumitsu Shikine;Yuki Hayashi;Takeshi Akiba;Mami Tanasaki;Junichi Hoshino	2016		10.1007/978-3-319-46100-7_3	simulation;physical medicine and rehabilitation;multimedia;skills management	Vision	-47.130783128676406	-49.69947711517191	199813
