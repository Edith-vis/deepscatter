id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
37e5949c043f865901e90c2809f8b0c3825a9382	project scriabin v.3	sonification;touch screen;synaesthesia	"""Project Scriabin is an interactive implementation of Alexander Scriabin's experimentation with """"opposite mapping direction"""", that is, mapping from hue (colour) to pitch (sound). Main colour to sound coding was implemented by Scriabin's colour scale."""	pitch (music)	Chang-Min Han	2007		10.1145/1279740.1279828	computer vision;sonification;human–computer interaction;computer science;multimedia;computer graphics (images)	Graphics	-47.95617376190299	-33.5143281795995	174956
71ea81415532efa4cd4aaea55cd578a903907fe3	the 'detection, perception and object-presence' framework: a unified structure for investigating illusory representations of reality	virtual reality;molecular visualization;virtual environment;augmented reality;usability	An  illusory representation of reality  is percieved to actually  be  the object or environment that it depicts. For example, Augmented Reality creates the illusion that a computer-generated object is a real object that exists in a person's environment. Conversely, Virtual Reality aims to 'immerse' the user and make them perceive, to some degree, to be in the virtual environment. Furthermore, the object that a representation depicts may actually be another form of representation. For example, a computer-generated image could create the illusion of being a photograph. This paper proposes a unified framework for assessing the success of illusory representations of reality; namely the  'Detection, Perception, Object-Presence framework' .		Emily Bennett;Brett Stevens	2006		10.1145/1140491.1140536	computer vision;augmented reality;computer-mediated reality;artificial reality;simulation;usability;reality–virtuality continuum;computer science;virtual machine;operating system;virtual reality;mixed reality;multimedia;immersion	Robotics	-43.25463195267532	-37.9597005777207	177392
57aa26666a2de1f57c1617a1aa5eac0fc70cc121	musical audio source separation based on user-selected f0 track	graphical user interface;user guided audio source separation;non negative matrix factorization	A system for user-guided audio source separation is presented in this article. Following previous works on time-frequency music representations, the proposed User Interface allows the user to select the desired audio source, by means of the assumed fundamental frequency (F0) track of that source. The system then automatically refines the selected F0 tracks, estimates and separates the corresponding source from the mixture. The interface was tested and the separation results compare positively to the results of a fully automatic system, showing that the F0 track selection improves the separation performance.	source separation;user interface	Jean-Louis Durrieu;Jean-Philippe Thiran	2012		10.1007/978-3-642-28551-6_54	speech recognition;computer science;multimedia;audio signal flow;communication	HCI	-45.18065630083309	-33.15647998506056	178491
9d89e2c0b87f5a76fc7a004d97c864e0fab2571f	interactive light and sound installation using artificial intelligence	audience experience;installation;g700 artificial intelligence;light;multi sensory environment;sound;artificial intelligence;aesthetic interactions;computer mediated art	This work aims at investigating the relationship between creation and fruition of art in interactive installations that exhibit intelligent responsiveness to participant input. A practice-based research led to the development of an interactive environment, in which the audience members are partners in constructing meanings and producing contents. The presented piece Which is your brass voice? employs artificial intelligence techniques to create unique multi-sensory experience for the audience. The artificial intelligence behind the artwork analyses the individual nuances of people's voices and creates a new music composition in real time on the base of its vast musical knowledge. User input is transformed into sound and coloured lights for further exploration of computer mediated aesthetic experiences. A brief description of the technical system is also included.	artificial intelligence	Gloria Ronchi;Claudio Benghi	2014	IJART	10.1504/IJART.2014.066456	psychology;visual arts;art;aesthetics;music and artificial intelligence;human–computer interaction;installation;computer science;artificial intelligence;operating system;light;multimedia;communication;sound	AI	-48.24762060500762	-35.16921522139669	178516
d2bcedde64cefbb0c349fce5c4d11dd4aa9d12c1	an implementation of eye movement-driven biometrics in virtual reality		As eye tracking can reduce the computational burden of virtual reality devices through a technique known as foveated rendering, we believe not only that eye tracking will be implemented in all virtual reality devices, but that eye tracking biometrics will become the standard method of authentication in virtual reality. Thus, we have created a real-time eye movement-driven authentication system for virtual reality devices. In this work, we describe the architecture of the system and provide a specific implementation that is done using the FOVE head-mounted display. We end with an exploration into future topics of research to spur thought and discussion.		Dillon J. Lohr;Samuel-Hunter Berndt;Oleg V. Komogortsev	2018		10.1145/3204493.3208333	rendering (computer graphics);architecture;computer vision;virtual reality;eye tracking;biometrics;authentication;eye movement;artificial intelligence;computer science	Visualization	-42.06268032682251	-37.47837210927463	178550
398de251003649bb6c42bd341b084750b29eaaa6	facsvatar: an open source modular framework for real-time facs based facial animation		Embodied Conversational Agents often employ advanced multimodal analysis of human users for affective inference, however, its facial expressions in response are often pre-made or coded animations. Generated data-driven facial animations have the advantage that they can be more natural and do not require a database at run-time. The open source modular framework FACSvatar is presented, which processes and animates FACS based data in real-time. Tools that create 3D human models are supported and facial data can be visualized in popular tools in the gaming industry. All functionality is split-up into modules set-up in a publisher-subscriber pattern to provide easy integration with other platforms. A deep learning module for real-time generation of AU data for data-driven animation is implemented. A user evaluation of their expressions being animated through our framework was done. Their ratings were slightly positive, but more improvements have to be made in terms of data quality and individual fine-tuning. Also, the modules' latency and performance have been measured. On average, FACS data is visualized in 28.55 ms.	artificial neural network;bcs-facs;data quality;deep learning;embodied agent;fear, uncertainty and doubt;machine learning;map;multimodal interaction;open-source software;real-time locating system;real-time web;smoothing	Stef van der Struijk;Hung-Hsuan Huang;Maryam Sadat Mirzaei;Toyoaki Nishida	2018		10.1145/3267851.3267918	multimedia;virtual actor;deep learning;computer facial animation;animation;human–computer interaction;facial expression;modular design;expression (mathematics);computer science;artificial intelligence;data quality	Graphics	-41.78163351522847	-33.92660176515503	179008
dae3a543f0faf5e124f4ccfa1736f2096e06ae24	digital umwelt: towards a didactic use of natural interfaces	human computer interaction;umwelt;natural user interfaces;user;didactics	"""The spread of Natural Interfaces, based on devices which allow the retrieval to the Human Computer Interaction of natural paradigms of human interaction sound, voice, touch, movement, limiting graphic interfaces: the interaction doesn't occur """"through the mirror"""" Carroll, 2012 of the screen, but it takes place through movement, in the natural space of the user, in relation to an augmented digital umwelt that inter-acts continuously with the user's whole body. The aim of this work is to present natural interfaces as the tool that constitutes the effective place of convergence between body and movement, manipulation of spatial reference systems and man-machine interaction, and inquire the possible didactic declinations."""		Pio Alfredo Di Tore;Nadia Carlomagno;Stefano Di Tore;Maurizio Sibilio	2013	IJDLDC	10.4018/jdldc.2013010104	psychology;user;human–computer interaction;computer science;umwelt;multimedia;post-wimp;communication;user interface	HCI	-47.262173845306066	-36.39935984479574	180081
4c4dfab8bbe648b016b7f5c6e9c906fc68c6e8cb	msf format for the representation of speech synchronized moving image		This paper describes the structure of a new multimedia file format. Also the procedures for implementing its encoder and the player are described. Multimedia Sound File(MSF) format reduced the size of the file. The display software is improved in the points that it requires only small sized image database compared to that of current similar programs require hugh amount of image database. This software tool can effectively display animated facial images and speech s ounds together in synchronized form even at PC level. Implemented tool can be used as a plugin or an independent form. Encoder software is implemented to facilitate the production of the msf file. Files from the segmentation of speech signal into ph nemic units are used as an input to the encoder.	encoder;microsoft solutions framework;plug-in (computing);programming tool	Cheol-Woo Jo	1998			speech recognition;computer science	SE	-44.021523506181445	-32.36781467459622	180411
972101ca596f024d8fe31a5a22c5184e4aba987b	drum on: interactive personal instrument learning system	gpu computing;motion estimation;learning system;hitting time;mpeg 4;animal imaging	Drum On is a prototype system to enhance personal instrument practice that may conventionally create boredom and limitation. The MIDI signal generating from the electrical drum is handled by a computer and each animating image interacts with this signal. The images displayed on the drum by a projector are animated to directly give rhythm cues for the drum player. The animations subsequently react with proper hit timing.	drum memory;midi;prototype;video projector	Jaehyuck Bae;Byungjoo Lee;Sungmin Cho;Yunsil Heo;Hyunwoo Bang	2012		10.1145/2342896.2342981	computer vision;real-time computing;simulation;computer science;operating system;motion estimation;hitting time;mpeg-4;general-purpose computing on graphics processing units;statistics;computer graphics (images)	HCI	-41.676027734220604	-37.09256713665363	180807
0de9f5954077000c5f8fa3f6d7b1e0aecc188179	“table-centric interactive spaces for real-time collaboration: solutions, evaluation, and application scenarios”	tabletop interaction;groupware;real time;interactive spaces;real time collaboration;collaborative environment;interactive space;situation assessment	"""Tables have historically played a key role in many real-time collaborative environments, often referred to as """"war rooms"""". Today, these environments have been transformed by computational technology into spaces with large vertical displays surrounded by numerous desktop computers. However, despite significant research activity in the area of tabletop computing, very little is known about how to best integrate a digital tabletop into these multi-surface environments. In this paper, we identify various design requirements for the implementation of a system intended to support such an environment. We then present a set of designs that demonstrate how an interactive tabletop can be used in a real-time operations center to facilitate collaborative situation-assessment and decision-making."""	computation;desktop computer;real-time locating system;real-time transcription;requirement;spaces	Daniel J. Wigdor;Chia Shen;Clifton Forlines;Ravin Balakrishnan	2006		10.1145/1133265.1133286	simulation;human–computer interaction;computer science;multimedia;situation analysis;collaborative software	HCI	-44.14064847007066	-36.710001753005194	180961
bc67eb08d23cfeeefc4eeaa2c92c0254a8a1b151	holobody galleries: blending augmented and virtual reality displays of human anatomy	virtual gallery;wearable technology;game engine;virtual reality;human anatomy;augmented reality;mixed reality	In this paper, we propose a novel way to illustrate human anatomy by combining virtual and augmented reality display technologies. Our intention is to create an anatomy gallery, which blends real world anatomy specimens, in the form of 3D printed models, with virtual reality scenes, and use augmented reality technology to seamlessly move between these different modalities. Hand gesture interfaces are used to navigate and interact with the elements displayed in VR and in real. The project introduced here serves as a first step towards a multi-user, distributed exploration environment to experience, teach and learn about human anatomy, physiology and kinesiology.	alpha compositing;augmented reality;display device;kinesiology;multi-user;printing;virtual reality	Markus Santoso;Christian Jacob	2016		10.1145/2927929.2927944	augmented reality;computer-mediated reality;artificial reality;simulation;engineering;metaverse;mixed reality;multimedia;immersion;computer graphics (images)	Visualization	-43.19271833583132	-37.64052377739774	181212
6de1e797af2ebb2339848ef0d93ac9eb759f9c05	pixels to parks: new animation techniques for fantasyland	new animation technique	We present the innovative techniques that brought to life an audio-animatronic Lumiere for Disney World’s newest attraction-Enchanted Tales With Belle. Walt Disney Imagineering and Walt Disney Animation Studios collaborated to create a pipeline that was flexible and intuitive for feature film animators. First, motion was digitally choreographed on the computer with a virtual rig. Next, the software detected constraints and motion limitations that would exist on the physical audio-animatronic. This allowed the artist to resolve them and animate physically plausible motions. Finally, the performance was transferred to the audio-animatronic, faithfully recreating the artist-produced motion using motors and machinery.	animatronics;belle (chess machine);pipeline (computing);pixel	Akhil Madhani;Justin Walker;Gene S. Lee;Aaron Adams;Alexis Wieland;Evan Goldberg	2013		10.1145/2504459.2504466	computer vision;simulation;computer graphics (images)	Graphics	-43.59838154069125	-35.23711518833538	181500
984e9df38eb93781aabca41c91499317e9351ef6	combining virtual reality enabled simulation with 3d scanning technologies towards smart manufacturing		Recent introduction of low-cost 3D sensing and affordable immersive virtual reality have lowered the barriers for creating and maintaining 3D virtual worlds. In this paper, we propose a way to combine these technologies with discrete-event simulation to improve the use of simulation in decision making in manufacturing. This work will describe how feedback is possible from real world systems directly into a simulation model to guide smart behaviors. Technologies included in the research include feedback from RGBD images of shop floor motion and human interaction within full immersive virtual reality that includes the latest headset technologies.	3d scanner;headset (audio);immersion (virtual reality);simulation;virtual reality;virtual world;world-system	Windo Hutabarat;John Oyekan;Christopher Turner;Ashutosh Tiwari;Neha Prajapat;Xiao-Peng Gan;Anthony Waller	2016	2016 Winter Simulation Conference (WSC)		simulation;computer science;engineering;resist;virtual reality;multimedia;solid modeling;manufacturing;data visualization;computer graphics (images)	Visualization	-42.72772826524278	-36.698714786915836	182228
cb3cc4370fcb3cd88caacc8cb990c1de19ae68ce	plink blink: collaborative music production via blinking eyes	sound installation;computer vision;interactive art;blink detection;interactive installation	Plink Blink, an interactive art installation, allows three participants to make collaborative music by blinking their eyes. Human beings can blink voluntarily and involuntarily but generally they do not think about blinking. Blinking goes unnoticed because it is a silent action. In contrast to its quiet nature, blinking is rhythmic. As a result, blinking is a resourceful input for sound generation. We wanted to link two sensory organs, eyes and ears, creating a playful and surprising environment.	blink;interactive art	Özge Samanci;Blacki Li Rudi Migliozzi;Daniel Sabio	2014		10.1145/2663806.2663809	psychology;computer vision;multimedia;communication	HCI	-47.907464521297946	-35.881755808933164	182323
32b5a315cf4dbb03b09342a9ce67f30d2e278b0d	a framework for efficient and rapid development of cross-platform audio applications	prototipificacion rapida;anotacion;traitement signal;audio systems;audio signal processing;multimedia;metadata;concepcion sistema;systeme audio;annotation;acoustic signal processing;data type;software frameworks;audio processing rapid prototyping;input output;algorithme;prototipo;algorithm;audio processing;synthese signal;rapid prototyping;media processing;traitement signal audio;system design;feature extraction;signal processing;design pattern;multimedia communication;metamodels;metadonnee;software framework;sintesis senal;design patterns;traitement signal acoustique;metadatos;signal synthesis;extraction caracteristique;analisis sonido;sound analysis;communication multimedia;audio analysis;procesamiento senal;prototype;conception systeme;software frameworks multimedia;analyse son;prototypage rapide;algoritmo	CLAM is a C++framework that offers a complete development and research platform for the audio and music domain. Apart from offering an abstract model for audio systems, it also includes a repository of processing algorithms and data types as well as a number of tools such as audio or MIDI input/output. All these features can be exploited to build cross-platform applications or to build rapid prototypes to test signal and media processing algorithms and systems. The framework also includes a number of stand-alone applications that can be used for tasks such as audio analysis/synthesis, plug-in development or metadata annotation.In this article we give a brief overview of CLAM's features and applications.	algorithm;expect;input/output;midi;plug-in (computing)	Xavier Amatriain;Pau Arumí;David García	2006	Multimedia Systems	10.1007/s00530-007-0109-6	embedded system;real-time computing;speech recognition;telecommunications;audio signal processing;computer science;software framework;operating system;signal processing	ML	-45.00565624769758	-32.16850936387257	182717
1ab30f03fafd5337f7f01935977af050e086fa96	freqtric drums: a musical instrument that uses skin contact as an interface	interpersonal communication;human computer interaction;skin contact;musical instruments;computer networks and communications;touch;musical instrument;artificial intelligence;interaction design	Freqtric Drums is a new musical, corporal electronic instrument that allows us not only to recover face-to-face communication, but also makes possible body-to-body communication so that a self image based on the sense of being a separate body can be significant altered through an openness to and even a sense of becoming part of another body. Freqtric Drums is a device that turns audiences surrounding a performer into drums so that the performer, as a drummer, can communicate with audience members as if they were a set of drums. We describe our concept and the implementation and process of evolution of Freqtric Drums.	drum memory;openness	Tetsuaki Baba;Taketoshi Ushiama;Kiyoshi Tomimatsu	2007		10.1145/1279740.1279827	human–computer interaction;computer science;interaction design;multimedia;interpersonal communication	HCI	-48.25289750352507	-36.526432508837225	183063
c3ed8ca165678d1371e5f9954220563cefc03bfc	x3d-based web 3d resources integration and reediting	3d visualization;animal behavior;interactive animation;content delivery;software development;interactive environment;open standard	X3D is an open standard for 3D shape content delivery, which combines both 3D shapes and animation behaviors into a single file. IntelligentBox is a prototyping software development system which represents any 3D object as a Box, i.e., a functional 3D visual object. X3D can provide more interactivity if it is empowered by IntelligentBox to add more reedit-ability. IntelligentBox provides an interactive environment for X3D shapes. This paper shows how X3D file shape information can be reused in IntelligentBox, and how IntelligentBox enables users to interactively add interactive animation functions to the original X3D models.	x3d	Zhoufan Zhou;Hisao Utsumi;Yuzuru Tanaka	2009		10.1007/978-3-642-04875-3_47	visualization;open standard;computer science;software development;operating system;multimedia;world wide web;computer graphics (images)	DB	-41.74171112542062	-32.12128439501592	183118
19bedc0e95aa62f842c79c87fcba302b676935b0	the prosthetic conga: towards an actively controlled hybrid musical instrument		The prosthetic conga is a hybrid instrument that is played and heard in the regular way, but with a resonant behaviour that can be manipulated in the virtual domain. This realised via sound reinforcement with a loudspeaker inside the conga, and sensing the membrane vibration. While sounding the instrument in this way is relatively simple, achieving precise control over the resonance properties of the hybrid instrument via active control poses a number of technical challenges. This paper discusses these challenges, as well as some of the implications for performance and composition.	automatic sounding;loudspeaker;neuroprosthetics;resonance;sound reinforcement system	Maarten van Walstijn;Pedro Rebelo	2005			visual arts;acoustics;engineering;communication	HCI	-45.64834983824023	-35.11066217484491	183952
e902fdfbeed7da56f5bdaaf18c1270ee2fd7ca0f	remote move-related operation awareness in real-time internet-based collaborative graphics design systems	prediction.;collaborative graphics design;operation awareness;real time;graphic design	Supporting awareness of other cooperators is an idea that holds promise for improving the usability of real-time distributed collaborative graphics design systems. However, network jitter may cause serious problems in providing real-time awareness information such as monitoring the remote motion of objects or the control points of objects in such Internet-based designing system. Halting and jumping presence of remote move-related operations may occur. In this paper, we present novel algorithm and scheme that can improve the move-related operation display performance of accuracy and smoothness. Experiments were carried out to test the effectiveness of the scheme and the algorithm. The results show that by applying effective remote motion prediction and introducing jitter buffer, the usability of the system can be greatly enhanced.	graphics;real-time transcription	Bo Jiang;Jianxv Yang	2006			graphic design;the internet;multimedia;environmental graphic design;graphics;human–computer interaction;distributed collaboration;computer science	Graphics	-42.45310636073399	-35.655771155702155	184021
2978ed092d95e4cf75602ee5896c39ec610d2002	development of 3d tawaf simulation for hajj training application using virtual environment	3d application;visual informatics;crowd;interactive training method	3D application has become very popular among people due to its capability to imitate real world environment and its interactivity to entertain users. This paper proposed an interactive training method for Hajj education in Malaysia by providing user a scene of Tawaf (one of Hajj pilgrimage rituals). Participants are provided with a flexible user control which is connected wirelessly to the system. The Tawaf simulation is created based on crowd technology and in this case we use a new method to control the movements of human character around Kaaba. Based on the user testing feedbacks, users are thrilled with the system, particularly because it is user-friendly and flexible.	interactivity;simulation;teaching method;usability testing;user interface;virtual reality	Mohd Shafry Mohd Rahim;Ahmad Zakwan Azizul Fata;Ahmad Hoirul Basori;Arief Salleh Rosman;Tamar Jaya Nizar;Farah Wahida Mohd Yusof	2011		10.1007/978-3-642-25191-7_8	simulation;human–computer interaction;engineering;multimedia	HCI	-44.26512348521892	-37.681701514894165	184108
12f255626a97e6085f932e8ce7f122560a46a1d3	step: a scripting language for embodied agents		In this paper we propose a scripting language, called STEP, for embodied agents, in particular for their communicative acts like gestures and postures. Based on the formal semantics of dynamic logics, STEP has a solid semantic foundation, in spite of a rich number of variants of the compositional operators and interaction facilities on the worlds. STEP has been implemented in the distributed logic programming language DLP, a tool for the implementation of 3D web agents. In this paper, we also discuss principles of scripting language design for embodied agents and several aspects of the application of STEP.	agent architecture;digital light processing;embodied agent;humanoid animation;iso 10303;lotus improv;perlin noise;programming language;scripting language;semantics (computer science);vrml;virtual human markup language;virtual world;web application;world wide web;xml	Zhisheng Huang;Anton Eliëns;Cees T. Visser	2004			embodied agent;computer science;theoretical computer science;programming language	PL	-42.235520544092175	-33.07019113832413	184897
8a3e2df6ba43a4c62706aed6e5512900a3c018b9	experimenting with noise in markerless motion capture	glitch;animation;noise	Visual culture has embraced the visual glitch as just one of many aesthetics associated with digital media. A glitch is often associated with noise in a technological system. Some motion capture systems experience noise and glitches as they process human movement. Under normal conditions, a glitch is undesirable because it decreases the usability of the capture. This short paper and demonstration introduce our research into the non-traditional use of motion capture data for the generation of artistic animated works.	digital media;experiment;glitch;image noise;motion capture;usability	A. Bill Miller	2015		10.1145/2790994.2791019	computer vision;simulation;engineering;multimedia	HCI	-41.35972049928459	-36.86820867012204	184968
5a2db3a72435af80f4abb1fd6a403abc17fd7398	design and implementation of an object-oriented, interactive animation system	object oriented		interactivity	Eric Peeters	1993			computer animation;computer facial animation;interactive media;animation;multimedia;computer graphics (images);object-oriented programming;interactive skeleton-driven simulation;computer science	Graphics	-42.03890995789466	-32.69062266065756	185208
05f90801c9c45886c2155ffcae3f4eb6d4138be3	mixed reality participants in smart meeting rooms and smart home environments	human computer interaction;smart home;mobile robot;ambient intelligence;virtual reality;user profile;embodied agents;translational research;remote participation;multimodal interaction;smart object;mixed reality;smart environments;smart environment;embodied agent	Human–computer interaction requires modeling of the user. A user profile typically contains preferences, interests, characteristics, and interaction behavior. However, in its multimodal interaction with a smart environment the user displays characteristics that show how the user, not necessarily consciously, verbally and nonverbally provides the smart environment with useful input and feedback. Especially in ambient intelligence environments we encounter situations where the environment supports interaction between the environment, smart objects (e.g., mobile robots, smart furniture) and human participants in the environment. Therefore it is useful for the profile to contain a physical representation of the user obtained by multi-modal capturing techniques. We discuss the modeling and simulation of interacting participants in a virtual meeting room, we discuss how remote meeting participants can take part in meeting activities and they have some observations on translating research results to smart home environments.	ambient intelligence;consciousness;home automation;human–computer interaction;mixed reality;mobile robot;modal logic;multimodal interaction;online and offline;sensor;simulation;smart environment;smart objects;user profile;virtual reality	Anton Nijholt;Job Zwiers;Jan Peciva	2007	Personal and Ubiquitous Computing	10.1007/s00779-007-0168-x	simulation;human–computer interaction;computer science;virtual reality;multimedia;smart environment;internet of things	HCI	-47.17149018064089	-37.62755043480133	185413
50e25597580800f82f4578525c5f7b671568c47f	adjutant: a framework for flexible human-machine collaborative systems	industrial robot adjutant software human machine collaborative systems flexible interaction technology flexible instruction technology small to medium scale manufacturing in home assistance robotic surgery collaborative robotic system user interaction modality human robot collaborative operation user interfaces user interaction paradigm tool affordance perceptual grounding templates tool movement primitive collaborative manufacturing tasks;production engineering computing control engineering computing human robot interaction industrial robots;robots collaboration kernel trajectory surgery geometry user interfaces	Flexible interaction and instruction is a key enabling technology for expanding robotics into small to medium scale manufacturing, in-home assistance for physically disabled individuals, and robotic surgery. In these cases, performing a task manually is neither practical nor scalable, yet complete automation is cost-prohibitive or impossible. Thus, our interest is in collaborative systems that can be easily trained to work with an operator. This collaborative robotic system should be instructable in a generalizable way for a wide range of tasks, and should generalize to new tasks gracefully with minimal retraining. At the same time, for a given task, the system should take advantage of user interaction modalities needed to accomplish the task, subject to the constraints of the available interfaces. These ideas motivate the Adjutant framework. Adjutant supports human-robot collaborative operations for ranges of user roles and robot capability. Adjutant models human-robot systems via sets of robot capabilities, composable high-level functions that can be specialized to specific tasks, and collaborative behaviors which relate these capabilities to specific user interfaces or interaction paradigms. Adjutant also incorporates several methods encapsulating reusable task information into capabilities, thus specializing them, including tool affordances, perceptual grounding templates, and tool movement primitives. We have implemented Adjutant as a software framework in ROS and, in this paper, explore the utility of Adjutant for performing several real-world collaborative manufacturing tasks on an industrial robot test-bed.	booting;consistency model;experiment;high- and low-level;immersion (virtual reality);industrial robot;look and feel;programming paradigm;remote manipulator;requirement;robot operating system;robotics;scalability;software framework;television;testbed;usability testing;user interface;virtual reality	Kelleher Guerin;Sebastian D. Riedel;Jonathan Bohren;Gregory D. Hager	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6942739	computer vision;simulation;interactive systems engineering;human–computer interaction;engineering	Robotics	-45.28612506683493	-37.23741592096761	186066
1a96d0ecdc39498302f15bd3179366059fcaa8ea	the babbletunes system: talk to your ipod!	dialogue system;natural interaction;dynamic data;multimodal interaction;multimodal dialogue systems;named entity	This paper presents a full-fledged multimodal dialogue system for accessing multimedia content in home environments from both portable media players and online sources. We will mainly focus on two aspects of the system that provide the basis for a natural interaction: (i) the automatic processing of named entities which permits the incorporation of dynamic data into the dialogue (e.g., song or album titles, artist names, etc.) and (ii) general multimodal interaction patterns that are bound to ease the access to large sets of data.	dialog system;dynamic data;multimodal interaction;named entity	Jan Schehl;Alexander Pfalzgraf;Norbert Pfleger;Jochen Steigner	2008		10.1145/1452392.1452408	natural language processing;dynamic data;speech recognition;computer science;multimodal interaction;multimedia	NLP	-45.389013072213466	-32.429798502141544	186766
c82a2e871024882b5b0258297173ec5358738fdf	user-centered design of gpu-based shader programs	user interface;gpu;user centered design;shader	In the context of game engines with graphical user interfaces, shader programs for GPUs (graphics processing units) are an asset for game development that is often used by artists and game developers without knowledge of shader programming. Thus, it is important that non-programmers are enabled to explore and exploit the full potential of shader programs. To this end, we develop principles and guidelines for the design of usercentered graphical interfaces for shaders. With the help of several examples, we show how the requirements of a user-centered interface design influence the choice of widgets as well as the choice of the underlying shader	computer graphics;experiment;game engine;graphical user interface;graphics processing unit;high- and low-level;preprocessor;programmer;requirement;shader;user experience;user-centered design;video game development	Martin Kraus	2012			computer architecture;user-centered design;shader;computer hardware;computer science;fixed-function;operating system;unified shader model;user interface;hlsl2glsl;computer graphics (images)	HCI	-41.77069548815263	-31.71517275688303	187526
e3fd7b441a9d4c218bf2e4a3aee7634b91b8f2e0	synchronization of video-embedded 3d contents for tiled display system		Synchronization among screens is very important for correctly working of tiled display systems, which is regarded as one of fundamental technologies for providing a very large and high resolution display. This paper proposes a novel approach to synchronization of animation and video-embedded 3D content, so that complicated 3d virtual scenarios can be playbacked in a tiled fashion over multiple devices. Based on a grid configuration, a display controller collects the information of rendering threads and sends synchronization beacons to all tiled windows in a multicast way and each tiled window will adapt its render progress accordingly. A ready bar-based synchronization method for video is used to overcome the influence of packet losses and the variance of buffering and networking delay on video frames. The experiment results show that all kinds of 3D contents, including 3D models, textures, animations, virtual cursors and videos embedded can be rendered in a consistent way even under poor networking conditions.	3d modeling;data buffer;embedded system;mathematical optimization;microsoft windows;multi-user;multicast;network packet;region of interest;texture mapping;tiling window manager;video display controller;virtual reality	Longjiang Li;Shawn Xiong;Jianjun Yang;Yuming Mao	2018	2018 27th International Conference on Computer Communication and Networks (ICCCN)	10.1109/ICCCN.2018.8487457	rendering (computer graphics);control theory;multicast;animation;synchronization;network packet;computer science;thread (computing);solid modeling;distributed computing	Visualization	-42.27783097772674	-35.14051089382068	187645
f8fe04327d28468c9152e3283dc320b31b39e620	raat - the reverie avatar authoring tool	formal specification;character mesh raat reverie avatar authoring tool javascript library online 3d character creator software avatar embodiment world wide web networked virtual environments socialization purposes entertainment purposes avatar authoring tools online character creation applications virtual environment specifications web based real time solution photorealistic integration user physical appearance;software libraries;authoring languages;authoring systems;browser software 3d avatar avatar customization virtual world;internet;html acceleration servers visualization navigation context information services;avatars;software libraries authoring languages authoring systems avatars formal specification internet	Avatar embodiment within the World Wide Web has gained a lot of popularity in recent years thanks to the introduction of networked virtual environments created for socialization and entertainment purposes. As each of these virtual worlds generates a unique set of user requirements concerning representation preferences based on the environment's context, it becomes clear that every attempt at creating such virtual worlds should encourage the development of the appropriate avatar authoring tools, being based on a thorough study of avatar desirable features. The Reverie Avatar Authoring Tool (RAAT) introduced in this paper helps developers address these ever-emerging avatar feature requirements, allowing them to easily set up and design online character creation applications, tailored to the virtual environment specifications. Summarizing the design process to a simple task of documenting the application interface within a single script, RAAT encapsulates the demanding tasks of character creation within simple function calls, while also offering a web-based real-time solution for photorealistic integration of user physical appearance onto the character mesh.	3d reconstruction;cascading style sheets;component-based software engineering;disk mirroring;graphical user interface;hypertext transfer protocol;kinect;lifting scheme;real-time locating system;real-time transcription;requirement;socialization;software documentation;usability;user requirements document;virtual reality;virtual world;web application;web developer;world wide web	Konstantinos C. Apostolakis;Petros Daras	2013	2013 18th International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2013.6622788	human–computer interaction;computer science;multimedia;world wide web	Visualization	-41.509871364147536	-33.413962390214515	187723
49adbd54cb75ef65d350ac6394868852a707f6d4	virtual tools that carry attributes for interactively specifying intermediate manufacturing processes	human computer interaction;virtual reality;interactive system;human machine interface;augmented reality	This paper describes an interactive system for specifying robotic tasks using virtual tools that allow an operator to reach into a live video scene and direct robots to use corresponding real tools in complex scenarios that involve integrating a variety of otherwise autonomous technologies. The attribute rich virtual tools concept provides a human-machine interface that is robust to unanticipated developments and tunable to the specific requirements of a particular task. This Interactive Specification concept is applied to intermediate manufacturing tasks.	autonomous robot;interaction;interactivity;requirement;specification language;user interface	Thenkurussi Kesavadas;David J. Cannon	1995	Virtual Reality	10.1007/BF02009723	human–machine interface;augmented reality;simulation;human–computer interaction;computer science;artificial intelligence;operating system;virtual reality;multimedia	Visualization	-43.10948730005794	-34.786276695321	187743
6435c6c4ed66ebaf75025d0208c40410248f5d72	designing time-based interactions with multimedia	time based media;time design;multimedia frameworks;real time;multimedia systems;software framework	The current model of time in multimedia frameworks poses particular problems when designing multimedia systems with time-based interaction. We propose to expand and extend an existing distinction between semantic time and real time from music and film theory to multimedia systems design. The semantic time concept also forms the foundation of a new software framework for multimedia systems that we are building, that, unlike most existing frameworks, includes mechanisms for time-based effects such as time-stretching.	interaction;multimedia framework;software framework;systems design	Eric Lee	2005		10.1145/1101149.1101363	real-time computing;simulation;computer science;software framework;multimedia	PL	-46.634274278122064	-33.616322721241545	187926
a79037220149bfde9654511e840de99fa809cfaf	towards a wearable interface for immersive telepresence in robotics		In this paper we present an architecture for the study of telepresence, immersion and human-robot interaction. The architecture is built around a wearable interface that provides the human user with visual, audio and tactile feedback from a remote location. We have chosen to interface the system with the iCub humanoid robot, as it mimics many human sensory modalities, including vision (with gaze control) and tactile feedback, which offers a richly immersive experience for the human user. Our wearable interface allows human participants to observe and explore a remote location, while also being able to communicate verbally with others located in the remote environment. Our approach has been tested from a variety of distances, including university and business premises, and using wired, wireless and Internet based connections, using data compression to maintain the quality of the experience for the user. Initial testing has shown the wearable interface to be a robust system of immersive teleoperation, with a myriad of potential applications, particularly in social networking, gaming and entertainment.		Uriel Martinez-Hernandez;Michael Szollosy;Luke W. Boorman;Hamideh Kerdegari;Tony J. Prescott	2016		10.1007/978-3-319-55834-9_8	immersive technology;computer vision;human–computer interaction;multimedia	HCI	-46.64987805004944	-37.88292568065582	188047
69aa06cb5e2d73c80f2092ece9d6a54aab56cdd1	easy living in the virtual world: a noble approach to integrate real world activities to virtual worlds	context awareness;virtual living;mobile computer;event detection;virtual agent virtual living acoustic event detection activity recognition sound signal processing second life;acoustic testing;signal processing;graphical representation;agent based system;intelligent agent;avatars;acoustic signal detection;system testing;second life;common sense;humans;acoustic event detection;acoustic sensors;point of view;human activity;sound signal processing;humans intelligent agent avatars acoustic sensors signal processing acoustic signal detection event detection system testing context awareness acoustic testing;virtual agent;virtual worlds;activity recognition	Virtual worlds like “Second Life” are popular graphical representations of real (and fictitious) places, which are inhabited by real people in the form of personal avatars. The existence of people in these worlds is either (1) as avatars manipulated by users (to make them walk, fly, chat, etc), or (2) as pre-scripted agents, called “bots”, which are programmed to display some predefined behavior in the virtual world. Research that aims to bridge real life and these virtual worlds to simulate virtual living, while challenging and promising, is currently rare. Only very recently the mapping of real-world activities to virtual worlds has been attempted by processing multiple sensors data along with inference logic for real-world activities. Detecting or inferring human activity using such simple sensor data is often inaccurate and insufficient. Hence, this paper explains to infer human activity from environmental sound cues and common sense knowledge, which is an inexpensive alternative to other sensors (e.g., accelerometers). We discuss the challenges to implement such a system from the signal processing and agent based system point of view. To the best of our knowledge, this system pioneers the use of environmental sound based activity recognition in mobile computing to reflect one’s real-world activity in virtual worlds.	activity recognition;agent-based model;avatar (computing);commonsense knowledge (artificial intelligence);graphical user interface;mobile computing;real life;second life;sensor;signal processing;simulation;virtual reality;virtual world	Shaikh Mostafa Al Masum;Helmut Prendinger;Keikichi Hirose;Mitsuru Ishizuka	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.196	computer vision;simulation;computer science;artificial intelligence;signal processing;system testing;intelligent agent;activity recognition	AI	-45.243831124397566	-36.18722420834708	189550
284e029b554c91bc40ba51044338525341e56eaf	student-sensitive multimodal explanation generation for 3d learning environments	learning environment;multimedia systems;natural language;knowledge integration;3d graphics;knowledge base	"""Intelligent multimedia systems hold great promise for knowledge-based learning environments. Because of recent advances in our understanding of how to dynamically generate multimodal explanations and the rapid growth in the performance of 3D graphics technologies, it is becoming feasible to create multimodal explanation generators that operate in realtime. Perhaps most compelling about these developments is the prospect of enabling generators to create explanations that are customized to the ongoing \dialogue"""" in which they occur. To address these issues, we have developed a student-sensitive multimodal explanation generation framework that exploits a discourse history to automatically create explanations whose content, cinematography, and accompanying natural language utterances are customized to the dialogue context. By these means, they create integrative explanations that actively promote knowledge integration. This framework has been implemented in CineSpeak, a studentsensitive multimodal explanation generator."""	3d computer graphics;knowledge integration;multimodal interaction;natural language	Brent H. Daniel;William H. Bares;Charles B. Callaway;James C. Lester	1999			natural language processing;knowledge base;knowledge integration;computer science;knowledge management;artificial intelligence;machine learning;multimedia;natural language	AI	-45.93935212315382	-32.26446418196216	190567
5cf27c19d278b9fc9738aa70c5bbfc3b69ccc104	resonant bits: harmonic interaction with virtual pendulums	resonance;human computer interaction;slow technology;tangible user interface;ideomotor control	This paper presents the concept of Resonant Bits, an interaction technique for encouraging engaging, slow and skilful interaction with tangible, mobile and ubiquitous devices. The technique is based on the resonant excitation of harmonic oscillators and allows the exploration of a number of novel types of tangible interaction including: ideomotor control, where subliminal micro-movements accumulate over time to produce a visible outcome; indirect tangible interaction, where a number of devices can be controlled simultaneously through an intermediary object such as a table; and slow interaction, with meditative and repetitive gestures being used for control. The Resonant Bits concept is tested as an interaction method in a study where participants resonate with virtual pendulums on a mobile device. The Harmonic Tuner, a resonance-based music player, is presented as a simple example of using resonant bits. Overall, our ambition in proposing the Resonant Bits concept is to promote skilful, engaging and ultimately rewarding forms of interaction with tangible devices that takes time and patience to learn and master.	interaction technique;mobile device;resonance;tv tuner card;tangible user interface	Peter D. Bennett;Stuart Nolan;Ved Uttamchandani;Michael Pages;Kirsten Cater;Mike Fraser	2015		10.1145/2677199.2680569	simulation;human–computer interaction;engineering;communication	HCI	-48.1354581370385	-36.64486404447732	191059
fe1b322e9d6e8e0da819a731f24a453d6aecc3c7	trips: the rochester interactive planning system	transportation logistics domain;rochester interactive planning system;graphical display;conversational planning assistant;trips collaborates;quicktime movie;rudimentary guidance	This demonstration showcases TRIPS, The Rochester Interactive Planning System, an intelligent, collaborative, conversational planning assistant. TRIPS collaborates with the user using both spoken dialogue and graphical displays to solve problems in a transportation logistics domain. In our demonstrations, users are encouraged to sit down and try the system, with only rudimentary guidance from us. For further information, including QuickTime movies of the system in action, please visit our website at the URL listed above.	graphical user interface;infographic;logistics;quicktime	George Ferguson;James F. Allen	1999			simulation;human–computer interaction;artificial intelligence;multimedia	HCI	-47.53042713739649	-37.0127659604419	191309
c8ca6dbd7fcf248a7f0fed001d5746837a336dba	live coding youtube		Music listening has changed greatly with the emergence of music streaming services, such as Spotify or YouTube. However, did it inspire us to make new experimental music? Live Coding YouTube is a response to the anticipation of novel performance practices using streaming media. A live coder uses any available video from YouTube, a video streaming service, as source material to perform an improvised audiovisual piece. The challenge is to manipulate the emerging media that are streamed from a networked service given the limited functionality of the API provided. The piece finds parallels in early experimental music that manipulates magnetic tape and vinyl records. On the contrary, the audiovisual space that a musician can explore on the fly is practically infinite. The performance system is built entirely on a web browser and publicly available in the following address: https://livecodingyoutube.github.io/	emergence;live coding;on the fly;parallels desktop for mac;performance;streaming media;visual effects	Sang Won Lee;Jungho Bang;Georg Essl	2018		10.1145/3170427.3177759	multimedia;on the fly;magnetic tape;anticipation;active listening;bricolage;computer science;live coding	Metrics	-44.720299738228455	-31.97246867050876	191756
2c705705126eeb13d0aa0deae2f834f22152be4c	celestia: a vocal interaction music game	music visualization;vocal interaction;game design;pitch detection	Voice is one of the most natural means of expression and the vocal interaction is gaining popularity in game development field. In this paper, we present Celestia, a vocal interaction music game that detects different pitches to trigger specific visual events, and explain the design and development phases of it.	video game development	Yang Shi;Cheng Yang	2013		10.1145/2468356.2479485	game design;speech recognition;computer science;pitch detection algorithm;music visualization	HCI	-48.04421987002462	-35.542249504209266	192595
da1819f0f737ca2b4a7c104ae9fc03b26b5109cd	evolution of gori.node garden	expressive imagery;non realistic modeling	At the first level, the subject of gardening is one p rson, ‘a gardener’ so called here. The garden visualizes his or her private communication data collected by PDA phone and its a ddress book. The gardener plants a seed and waters on his or er private social network garden which could be understood as one big node. Technically one control board is controlling all plants which are connected to a server computer.	gardening (cryptanalysis);personal digital assistant;server (computing);social network	Jee Hyun Oh	2006		10.1145/1179849.1180028	computer vision;simulation;computer science	Theory	-48.21179139017463	-37.9650375517146	192976
08eb8df581aeea93e757a037cc635b350c92ffbe	a paradigm for physical interaction with sound in 3-d audio space		Immersive virtual environments offer the possibility of natural interaction within a virtual scene that is familiar to users because it is based on everyday activity. The use of such environments for the representation and control of interactive musical systems remains largely unexplored. We propose a paradigm for working with sound and music in a physical context, and develop a framework that allows for the creation of spatialized audio scenes. The framework uses structures called soundNodes, soundConnections, and DSP graphs to organize audio scene content, and offers greater control compared to other representations. 3-D simulation with physical modelling is used to define how audio is processed, and offers a high degree of expressive interaction with sound, particularly when the rules of sound propagation are bent. Sound sources and sinks are modelled within the scene along with the user/listener/performer, creating a navigable 3-D sonic space for sound-engineering, musical creation, listening, and		Michael Wozniewski;Zack Settel;Jeremy R. Cooperstock	2006			speech recognition;acoustics;communication	HCI	-46.13565017445354	-35.02337052581415	193361
3608a10215ed81b66f23a2cffceed444774b1342	exploring programmable light spaces using actively deformable mirrors	architectural space;programmable space;programmable light;video displays;programmable architecture;ubiquitous displays;interaction design	In this study, we propose a new approach for displaying images and controlling light spaces using actively deformable mirrors. This approach enables programming the mirror-reflected light from sunlight or other parallel/point light sources to create arbitrary light spaces in various scenarios.		Munehiko Sato;Mehdy Chaillou;Tomohiro Tanikawa;Michitaka Hirose	2013		10.1145/2468356.2468653	computer vision;human–computer interaction;computer science;interaction design;computer graphics (images)	Vision	-42.700918212377765	-37.72723365072539	194046
27ae84f52dbf564c15bde8a6464a84a28974d370	sound intensity gradients in an ambient intelligence audio display	responsive environments;play;ambient intelligence;real time;sound design;game;auditory display;responsive environment	This paper describes the prototype of a real-time responsive audio display for an ambient intelligent game named socio-ec(h)o. The audio display relies on a gradient response to represent and anticipate player action. We describe the audio display schema, and discuss results of our current experimentation in guiding player actions through types of audio feedback, for creating sound recognition, perceptions of change and sound intensity.	ambient intelligence;audio feedback;gradient;prototype;real-time transcription	Milena Droumeva;Ron Wakkary	2006		10.1145/1125451.1125597	sound design;games;simulation;human–computer interaction;computer science;auditory display;multimedia	HCI	-47.08271905152168	-37.43102412125156	194625
bd712dc8cda3485317d050abeca49f5e9985fb91	the immersadesk™ semi-immersive virtual reality workstation	immersive virtual reality	"""come the standard for small-scale, projectionbased immersive VR systems. The original ImmersaDeskTM, currently man¬ ufactured and distributed by Pyramid Systems, Inc. as the ImmersaDeskTM Classic (Fig. 1), was intended to be not only compact, but reason¬ ably portable; a """"roll-around"""" VR tool, if you will. However, it became apparent after a few months that a more portable solution was re¬ quired as users were transporting ImmersaDesksTM to all manner of locations, including trade shows, exhibitions, and conferences. This requirement to go """"on the road"""" led to the de-"""	immersion (virtual reality);semiconductor industry;virtual reality;workstation	Mark Couling	1998	Cyberpsy., Behavior, and Soc. Networking	10.1089/cpb.1998.1.391	psychology;immersive technology;computer-mediated reality;cave automatic virtual environment;medicine;computer science;metaverse;mixed reality;immersion	HPC	-48.19044489278876	-31.016267347870123	195378
08c6e504dd497c1ac4d022eb34fd8f78e3b4b160	shared interactive video for teleconferencing	panoramic video;interactive video;distance learning;pan tilt zoom;camera control;collaborative device control;video enabled device control;video conferencing;gesture based device control;system architecture;video communication	"""We present a system that allows remote and local participants to control devices in a meeting environment using mouse or pen based gestures """"through"""" video windows. Unlike state-of-the-art device control interfaces that require interaction with text commands, buttons, or other artificial symbols, our approach allows users to interact with devices through live video of the environment. This naturally extends our video supported pan/tilt/zoom (PTZ) camera control system, by allowing gestures in video windows to control not only PTZ cameras, but also other devices visible in video images. For example, an authorized meeting participant can show a presentation on a screen by dragging the file on a personal laptop and dropping it on the video image of the presentation screen. This paper presents the system architecture, implementation tradeoffs, and various meeting control scenarios."""	authorization;computer mouse;control system;drag and drop;laptop;microsoft windows;pan–tilt–zoom camera;systems architecture;virtual camera system	Chunyuan Liao;Qiong Liu;Don Kimber;Patrick Chiu;Jonathan Foote;Lynn Wilcox	2003		10.1145/957013.957129	distance education;directx video acceleration;computer vision;video;video production;uncompressed video;telecommunications;computer science;video capture;video tracking;professional video camera;multimedia;video processing;smacker video;videoconferencing;systems architecture;computer graphics (images);non-linear editing system	HCI	-43.05393729593406	-36.22333600034381	195757
cce8d653a0e2e0a7231d42fc8ad9a1d9787101b9	signal processing supports a new wave of audio research: spatial and immersive audio mimics real-world sound environments [special reports]		In an era of ubiquitous video, audio is often relegated to a secondary role. Yet electronic audio in all of its various forms is now staging a strong comeback as listeners, increasingly dissatisfied with the output of highly compressed audio files and streams, seek higher sound quality on all types of fixed and mobile platforms. Signal processing is largely responsible for taking audio in all of its forms to new levels of quality. On the cutting edge of audio research is spatial audio, or ambisonics, a technique that takes a dimensional approach to sound, mimicking the way people hear in real life. By channeling the characteristics of sound as it travels through space and time, ambisonics envelops listeners inside a threedimensional (3-D) audio sphere that makes recorded music sound startlingly natural in whatever setting it’s used.	audio signal processing;sound card	John Edwards	2018	IEEE Signal Process. Mag.	10.1109/MSP.2017.2784881	computer vision;computer science;artificial intelligence;headphones;streams;loudspeaker;immersion (virtual reality);acoustics;signal processing;sound quality;solid modeling;ambisonics	Visualization	-46.30825296893111	-34.02230225043724	195994
beb438853617e03a568fefe696fe53ebe402442a	a graphical user interface for midi signal generation and sound synthesis				Youichi Horry	1994			i-cubex;speech recognition;acoustics;human–computer interaction	EDA	-47.103440447906	-34.2544118017744	196373
05bd73f2f271b38b7f38f7f5bc4f8a4ee25d04cd	musicfabrik: a playable, portable speaker		In this work-in-progress we introduce our prototype, a portable speaker augmented to be used as an electronic music sketchpad which allows musicians to play, record and compose multi-track music on the fly. We developed the MusicFabrik cover for the portable speaker, a playable textile cover which, although in functional sketch stage now, shows promise of becoming a useful tool for impromptu loop creation and collaborative music playing. We present our initial user encounters of the sketch. See video: https://youtu.be/pe2QaYv1Imo.	impromptu;on the fly;prototype;sketchpad	Vanessa Julia Carpenter;Baldur Kampmann;Antonio Stella;Martin Maunsbach;Martin Minovski;Nikolaj Ville-France;Dan Overholt	2018		10.1145/3240167.3240242	human–computer interaction;multimedia;electronic music;on the fly;sketch;impromptu;computer science;new interfaces for musical expression	AI	-46.82629778175731	-35.33853610045669	196726
415326798ce4f69631af00e0e790cd8cb8c8c969	the interaction engine: tools for prototyping connected devices	single board linux computer;connected devices;instructions;physical computing;interactive devices	In this workshop, we will familiarize participants with the Interaction Engine, a system for prototyping connected, interactive devices using low cost, single-board Linux computers and Arduino microcontrollers. Our main objective is to introduce participants to the basic architecture of connected devices and provide hands-on experience creating networked, physical hardware. The Interaction Engine is a generic framework, not a specialized toolkit. We employ widely available, community-supported tools that can enable web-connected hardware capable of merging tangible interfaces with audio/visual web interfaces. We view low-cost single-board computers as an enabling technology, representing the next step for tangible, embedded, and embodied designs enabling deep interaction between physical and digital worlds. This workshop will be a starting point for participants to begin exploring connected device development and will provide a basic set of tools and skills that participants can use in their own applications.	arduino;embedded system;hands-on computing;linux;microcontroller;prototype;single-board computer;user interface	Nikolas Martelaro;Michael U Shiloh;Wendy Ju	2016		10.1145/2839462.2854117	embedded system;human–computer interaction;computer hardware;computer science	HCI	-46.24117167627605	-37.597187131954875	197496
648b2486c16c3291b6894d3d55cd543638ebfdd2	takashi's seasons	input device;motion pictures;real time;hybrid;digital and analogue;shadow puppet;seasonality	"""Takashi's Seasons is a sequential live shadow puppet/video performance in which a number of interpretations of the four seasons are performed by an artist. Controlled with fishing line and wooden dowels, the puppets cast shadows on the screen. At the same time, the puppeteer controls the content being projected, and triggers sound effects using a custom input device. Working in precarious unison, the shadows of the puppets are synchronized with the animation, creating a unique live action performance. Animation and sound are composited with shadows in real time; rather than relying on a series of pre-rendered animation sequences, the artist produces """"motion pictures"""" via a combination of seasonal sounds, live shadow puppet manipulation, and the projection of shadow-like animation sequences."""		Takashi Kawashima;Togo Kida;Yoshimasa Niwa	2006		10.1145/1180639.1180873	computer vision;simulation;hybrid;computer science;operating system;multimedia;input device;seasonality;statistics;computer graphics (images)	Graphics	-41.219218885608	-36.08570237941193	197617
ad7b3d57177427b6e71a88171619d0ec03cdd558	design and use of pseudo-electronic whiteboard for low-vision education	legibility;interfase usuario;user interface;classroom;user assistance;low vision;assistance utilisateur;asistencia usuario;aula clase;interface utilisateur;legibilidad;enseignement;lisibilite;educacion;salle cours;teaching;ensenanza	We have built a pseudo-electronic whiteboard system (prototype) for low-vision education incorporating a commercial capturing system which mimic drawing on a plain whiteboard onto a monitor window (server-window). Moreover the capturing is such that the server window turns into a super pseudo-electronic whiteboard (projeting the captured whiteboard onto the whiteboard is essential). We implemented the distribution of the server-window to the class for low-vision education. We have added processing for increasing readability. Additionally the class shares separate window-area for questions and answers. The pseudo-electronic whiteboard system we made makes teacher and low-vision students share classroom education, which had been completely ignored so far.		Yasuyuki Murai;Hisayuki Tatsumi;Masahiro Miyakawa;Shinji Tokumasu	2004		10.1007/978-3-540-27817-7_89	education;simulation;computer science;operating system;multimedia;user interface;computer graphics (images)	HCI	-44.56203245764941	-31.36819963373456	197770
4a34d433a481c65da9e4f10ac132d03c35b8ef1a	personal universal controllers: controlling complex appliances with guis and speech	speech interfaces;remote control;personal universal controller;handheld computer;automatic generation;communication channels	We envision a future where each person will carry with them a personal universal controller (PUC), a portable computerized device that allows the user to control any ap-pliance within their environment. The PUC has a two-way communication channel with each appliance. It downloads a specification of the appliance's features and then automati-cally generates an interface for controlling that appliance (graphical, speech, or both). In this demonstration we pre-sent a working PUC system that automatically generates graphical and speech interfaces, and controls real appli-ances, including a shelf stereo and a Sony camcorder.	channel (communications);graphical user interface;personal and ubiquitous computing	Jeffrey Nichols;Brad A. Myers;Michael Higgins;Joseph Hughes;Thomas K. Harris;Ronald Rosenfeld;Kevin Litwack	2003		10.1145/765891.765896	embedded system;simulation;human–computer interaction;computer science;multimedia;world wide web;remote control;channel	HCI	-46.339959892813134	-37.86214749117185	198747
3ad58e5758d8c0d42a8270b848cc041333541839	a flock of words: real-time animation and video controlled by algorithmic music analysis	music analysis;real time animation		algorithmic composition;flock;real-time cmix	Robert Rowe;Doris Vila;Eric Singer	1996		10.1145/253607.253766	speech recognition;computer facial animation;computer science;music visualization;computer animation;multimedia;computer graphics (images)	Graphics	-46.375649771239885	-32.80589720235762	198905
d7f0fcb9836271ba6b67a71a03051322de630c2a	a cross-media music retrieval system by converting color changes into tonal changes	music image retrieval image sequences mobile computing;user interface media retrieval mood analysis multimedia database;web based music resources cross media music retrieval system color changes image sequence continuous tonal changes delta functions distance values heterogeneous image music criteria visual impressions time oriented invisible impression visual query construction method;measurement image color analysis media mood visualization user interfaces engines;user interface;media retrieval;mood analysis;multimedia database	We propose a cross-media music retrieval system that provides its users with a toolkit for formulating their own query by describing their emotional requirements for changes in mood of music as a sequence of images. To interpret the query, the system uses a metric space to convert the color changes in images into continuous tonal changes in music, and vice versa. The system provides delta functions for music and images to compute the changes as distance values in the metric space. Our system calculates the sentiment-oriented relevance score of the query and music by comparing their computed distance values. The advantage of the system is that it provides a bridge between heterogeneous image and music criteria by converting the visual impressions of images into a time-oriented invisible impression of music. This bridging operation is the foundation of our visual query construction method. This method allows users to search for music by subtly manipulating the form of a query in a trial-and-error manner by detecting the moods of music as color changes in images. This method is suitable for searching for unknown music that satisfies the preferences of users in web-based music resources that store many types of music, but without any publishing information related to the music.	bridging (networking);color;computation;image;mobile device;personalization;relevance;requirement;sensor;touchscreen;web application	Yoshiyuki Kato;Shuichi Kurabayashi	2014	2014 IEEE 15th International Conference on Mobile Data Management	10.1109/MDM.2014.51	computer science;operating system;multimedia;user interface;world wide web	Web+IR	-44.61070562340423	-33.45218475790126	198987
30f5364a1a926f91607b28c0b412b82b4d369678	application of directdraw in real-time data acquisition system	software;gdi function;directdraw;real time data acquisition system;video signal processing computer games data acquisition graphical user interfaces operating systems computers;video signal processing;graphical interface;real time;windows operating system;working conditions;data acquisition system;real time data;game development;graphic interface data acquisition real time display directdraw video page;directdraw method;sleep;page jitter phenomenon;graphical user interfaces;operating system;streaming media;high priority;sleep physiology parameter acquisition analysis system;video page;graphic interface;graphic interface real time data acquisition system windows operating system real time smooth display gdi function page jitter phenomenon game development tools directdraw method software video page switching technology sleep physiology parameter acquisition analysis system computer image display;software video page switching technology;real time display;computer games;real time smooth display;data acquisition;operating systems computers;real time systems graphics hardware streaming media software data acquisition sleep;graphics;computer image display;game development tools;hardware;real time systems	In many data acquisition systems which based on the Windows operating system often require to display interface to achieve real-time smooth display the current curve data, in no affecting high-priority-level working conditions such as data acquisition, storage and so on. Using the traditional GDI function under Windows will cause the page jitter phenomenon. This paper introduces the brief principle of DirectDraw that is the game development tools, use the DirectDraw method and software “video page” switching technology, and achieve a smooth real-time displaying the current curve data. This method has been successfully applied in the sleep physiology parameter acquisition analysis system, displaying computer images have advantages of fast processing and no flickering.	data acquisition;directdraw;flicker (screen);graphics device interface;microsoft windows;operating system;programming tool;real-time clock;real-time data;real-time transcription;smoothing;timer;video game development	Guo-Zhuang Liang;Chang-Jie Zhou;Wei Meng;Shi-Qiang Hu	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580634	real-time computing;computer hardware;computer science;graphical user interface;data acquisition;computer graphics (images)	Robotics	-42.5000160599722	-34.64962925158201	199210
a80a69e7687787aaa9605855e540dee836642038	agora - the multi-user environment for co-authoring documents	multi user		agora;multi-user	Waldemar Wieczerzycki	1999			world wide web	NLP	-48.226074620872005	-31.852235131900162	199256
29d6a7cff6a504ccf707d87a150f63175acef256	aspects of visualizing information for a real-time hypermedia musical environment.	real time			Michael Kieslinger;Tamas Ungvary	1996			human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-47.81417741139486	-33.0476977806144	199574
