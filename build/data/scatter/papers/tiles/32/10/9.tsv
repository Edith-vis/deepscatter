id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
8905b66c00be26f90c9fd04120444ac98e10f81d	snap and translate using windows phone	user interface;augmented reality camera based ocr mobile translation user interface text detection;language translation;user intention guided text extraction snap and translate windows phone mobile app english menu english sign chinese translation intelligent text extraction machine translation client plus cloud architecture chinese ocr chinese to english translation ui design;optical character recognition software;user interfaces cloud computing document image processing language translation mobile computing natural language processing;image edge detection;image color analysis;dictionaries;mobile communication;document image processing;robustness;text detection;mobile translation;augmented reality;mobile computing;user interfaces;optical character recognition software image edge detection dictionaries cameras mobile communication robustness image color analysis;natural language processing;cameras;camera based ocr;cloud computing	"""We have developed a prototype of a mobile app called """"Snap and Translate"""" on """"Windows Phone 7"""". A person who is reading an English menu/sign and wants a Chinese translation of an English word or phrase or paragraph can use a Windows Phone to snap an image of the text, tap the word or swipe the phrase or circle the paragraph with a finger, and get a Chinese translation displayed on the screen of the phone. This is enabled by seamless integration of three Microsoft technologies: intelligent text extraction, OCR, and machine translation based on a client-plus-cloud architecture. The current prototype also supports Chinese OCR plus Chinese-to-English translation. In this paper, we highlight the UI design of the system and the corresponding user-intention guided text extraction approach to achieving a compelling user experience."""	book;cloud computing;distortion;machine translation;microsoft windows;mobile app;optical character recognition;printing;prototype;seamless3d;signage;user experience;user interface design;windows phone	Jun Du;Qiang Huo;Lei Sun;Jian Sun	2011	2011 International Conference on Document Analysis and Recognition	10.1109/ICDAR.2011.166	augmented reality;speech recognition;computer science;multimedia;user interface;mobile computing;world wide web	NLP	-43.80533346650783	-43.77024074464325	160025
0f7d635212a60aa0c43f84b1eb11035a91a6f30c	multi-user interaction on media facades through live video on mobile devices	input device;mobile device;media facades;urban space;interface distribution;interaction techniques;multi user;multi user interaction;augmented reality;interaction technique	The increasing number of media facades in urban spaces offers great potential for new forms of interaction especially for collaborative multi-user scenarios. In this paper, we present a way to directly interact with them through live video on mobile devices. We extend the Touch Projector interface to accommodate multiple users by showing individual content on the mobile display that would otherwise clutter the facade's canvas or distract other users. To demonstrate our concept, we built two collaborative multi-user applications: (1) painting on the facade and (2) solving a 15-puzzle. We gathered informal feedback during the ARS Electronica Festival in Linz, Austria and found that our interaction technique is (1) considered easy-to-learn, but (2) may leave users unaware of the actions of others.	15 puzzle;clutter;interaction technique;mobile device;mobile phone;multi-user;projector;scenario (computing)	Sebastian Boring;Sven Gehring;Alexander Wiethoff;Anna Magdalena Blöckner;Johannes Schöning;Andreas Butz	2011		10.1145/1978942.1979342	augmented reality;simulation;human–computer interaction;computer science;operating system;mobile device;multimedia;interaction technique;input device	HCI	-46.76345054805187	-39.28451831958994	160056
320149bd992b1f0f1e269ea0d90130166728538f	anamorphicons: an extended display with a cylindrical mirror	anamorphosis;tangible;phicon;interactive system;anamorphicon;flat panel display	We developed an interactive system which takes the technique of Anamorphosis, with a 2D display and a cylindrical mirror. In this system, a distorted image is shown on a flat panel display or tabletop surface, and the original image will appear on the cylindrical mirror when a user puts it on the display. By detecting the position and rotation of the cylinder, the system provides interaction between the user and the image on the cylinder. In our current prototype, an iPad screen and its multi-touch display is used to detect the cylinder.	cylinder seal;flat panel display;interactivity;multi-touch;prototype;sensor;ipad	Chihiro Suga;Itiro Siio	2011		10.1145/2076354.2076396	computer vision;computer hardware;engineering;computer graphics (images)	HCI	-42.529956944522596	-39.82995529898827	160106
5824bd532ea47fc54124c79c8b4b1b1657e3b9ea	real-world interaction with camera phones	distributed system;systeme reparti;image processing;telephone portable;pervasive computing;procesamiento imagen;angulo rotacion;traitement image;2 dimensional;mobile phone;informatica difusa;captador medida;large scale;sistema coordenadas;measurement sensor;telefono movil;sistema repartido;capteur mesure;informatique diffuse;camera phone;rotation angle;angle rotation;systeme coordonnee;coordinate system	With the integration of cameras, mobile phones have evolved into networked personal image capture devices. Camera-phones can perform image processing tasks on the device itself and use the result as an additional means of user input and a source of context data. In this paper we present a system that turns such phones into mobile sensors for 2-dimensional visual codes. The proposed system induces a code coordinate system and visually detects phone movements. It also provides the rotation angle and the amount of tilting of the camera as additional input parameters. These features enable applications such as item selection and interaction with large-scale displays. With the code coordinate system, each point in the viewed image – and therefore arbitrarily shaped areas – can be linked to specific operations. A single image point can even be associated with multiple information aspects by taking different rotation and tilting angles into account.	autostereogram;code;computer vision;download;image plane;image processing;interaction technique;mobile phone;optical mouse;printing;sensor;sourceforge;transit map;web content;wire-frame model	Michael Rohs	2004		10.1007/11526858_7	embedded system;computer vision;two-dimensional space;simulation;image processing;computer science;operating system;coordinate system;camera phone;ubiquitous computing	HCI	-43.94222234635979	-40.78064610012196	160683
941305f324d9a84339d83b3eda3012334696f28c	a joystick interface for tongue operation with adjustable reaction force feedback	pins;mice;force;life supporting equipment joystick interface tongue operation adjustable reaction force feedback severely disabled persons tactile sensations;interactive devices force feedback handicapped aids haptic interfaces;robots;tongue;force tongue pins electromyography wheelchairs robots mice;electromyography;wheelchairs	This paper proposes a novel tongue-operated joy- stick device for severely disabled persons. The human tongue has superior movement and tactile sensations. By providing force information during the device's operation, user can skillfully control life supporting equipment, such as electric powered wheelchairs, robotic manipulators for meals through interaction with this equipment. Therefore, we designed a joystick mechanism that enables not only the generation of control commands for the equipment but also the adjustment of the reaction force during its operation. The adjustment of the reaction force enables the user to know if there is any interference between the equipment and the environment and to improve task performance. In our proposed mechanism, the elastic pins surround the tip of the joystick and restrict its movement by this contact. The reaction force is controlled easily by adjusting the support length of the elastic pins. This paper describes the detailed mechanism and its basic performance through several experiments.	555 timer ic;experiment;haptic technology;interference (communication);joystick;prototype;robot;robotic arm;theory	Shinya Kajikawa;Kyohei Takahashi;Akihide Mihara	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353903	robot;control engineering;embedded system;simulation;computer science;engineering;artificial intelligence;force	Robotics	-41.46778742354812	-44.35480098371075	161348
4a4da833e999cc555213ef81460dca8c4826e815	tangemon: a new tui to capture precious moments	ubiquitous photography;table top;interaction metaphors;tangible interface;nfc	Humans cherish everyday precious moments and love to capture these through photographs. But one of the main constraints for capturing such instances is the use of direct devices like mobile phones or portable cameras which impedes the instantaneous moment in order to capture it. In this paper, we introduce, Tangemon, a Tangible User Interface (TUI) to capture these moments seamlessly through euphoric human gestures like toasting, act of saying cheers, bashing cans, etc. with ellipsoidal tangible inter-communicating cocktail pieces. Our system has mimicked natural cocktail garnishing citrus fruits like lemons, oranges, limes, clementine, etc. taking lemons as particular use-case example. These tangible bits act as the trigger for the omnipresent camera when they are in close proximity. A new interaction of narrative photo sharing is also explored. The project outcome is a high-fidelity working prototype.	clementine;humans;mobile phone;prototype;tangible user interface	Akash Harlalka;PJ Pradeep;Kushagra Sinha;Neha Singh	2014		10.1145/2676702.2676718	simulation;engineering;communication;computer graphics (images)	HCI	-46.58493468251276	-39.70154148114931	161763
291eb6c0340f4a3915c69f89e25cd714b2329c8e	an animated emoji feedback system for eating rate guidance		In this demonstration, we introduce an animated emoji feedback system that can help users eating at an appropriate rate. The system measures the user's eating rate in real time and sends appropriate feedback to the user in accordance with the measured eating rate via Samsung's animated AR Emoji.	emoji	Joohee Kim;Byung-Chull Bae	2018		10.1145/3267305.3267577	human–computer interaction;multimedia;emoji;computer science	HCI	-45.4662473697894	-43.78918108629097	162251
b809dfc831516f6992b12fd7c35b95bb53dfeb6e	smarttiles: mobility and wireless programmability in children's construction and crafts	tiles personal digital assistants prototypes light emitting diodes biological materials automata computer displays mobile computing programming profession computer interfaces;wireless programming computers and education smarttiles mobile computing computationally enhanced construction kits;cellular radio;smarttiles;mobile computer;computers and education;computer science education;educational aids;educational computing smarttiles mobility wireless programmability children construction children crafts construction kit elements touch sensitive tile user customizable cellular automaton programs pda interface;computationally enhanced construction kits;notebook computers;wireless programming;mobile computing;programming cellular radio mobile computing educational aids computer science education notebook computers;programming;cellular automaton	This paper presents a working prototype of a mobile, programmable set of construction kit elements for children. SmartTiles are small, lightweight, independently programmable tile objects that can be combined to cover various sorts of planar surfaces; each touch-sensitive tile contains its own computer and LED, and communicates with its neighboring tiles when placed on an appropriate background material. Collectively, the tiles enact user-customizable cellular automaton programs and thus display complex and fascinating dynamical patterns of light. In this paper, we discuss the implementation of SmartTiles and explore their potential use as an instance of mobile computation for children. We also discuss the way in which the tiles can be programmed wirelessly via a PDA interface, and discuss the implications of this sort of programming for educational computing more generally.	cellular automaton;computation;personal digital assistant;prototype;touchscreen	Nwanua Elumeze;Michael Eisenberg	2005	IEEE International Workshop on Wireless and Mobile Technologies in Education (WMTE'05)	10.1109/WMTE.2005.58	simulation;computer science;multimedia;computer engineering	Robotics	-46.54761996865976	-39.76146285673896	162590
fcedd2a9367a72f31cb93c2078609e2e4362378f	supporting workspace awareness in groupware (video program).	workspace awareness	Real-time groupware systems often let each participant control their own view into a shared workspace. However, when collaborators do not share the same view they lose their awareness about where and how others are interacting with the workspace artifacts. We have designed a number of add-on awareness windows that help people regain this awareness. Two general strategies and several variations are illustrated in this video that extend work done in a few other groupware systems. First, radar overviews shrink the entire workspace to fit within a single window. Awareness is indicated by overlaying the overview with boxes representing others' viewports, by telepointers that show where they are working, and by seeing changes to objects in the workspace as they are made. The workspace can be represented within the radar overview as a scaled miniature, by stylized objects, or by its semantic structure. Second, two types of detailed views show some or all of what another person can see, providing awareness of fine-grained details of others' actions.		Carl Gutwin;Saul Greenberg;Mark Roseman	1996		10.1145/240080.318418	simulation;human–computer interaction;computer science;multimedia;world wide web;engineering support	HCI	-45.868189715477676	-38.806348146674466	163086
3dd4e3b2240e48af9795fd1e8757a9de4113b27b	audio or tactile feedback: which modality when?	mobile device;crossmodal interaction;context of use;mobile touchscreen interaction;qa75 electronic computers computer science;tactile feedback;auditory feedback;information interfaces and presentation;mobile interaction	When designing interfaces for mobile devices it is import-ant to take into account the variety of contexts of use. We present a study that examines how changing noise and dis-turbance in the environment affects user performance in a touchscreen typing task with the interface being presented through visual only, visual and tactile, or visual and audio feedback. The aim of the study is to show at what exact environmental levels audio or tactile feedback become inef-fective. The results show significant decreases in perform-ance for audio feedback at levels of 94dB and above as well as decreases in performance for tactile feedback at vibration levels of 9.18g/s. These results suggest that at these levels, feedback should be presented by a different modality. These findings will allow designers to take advantage of sensor enabled mobile devices to adapt the provided feed-back to the user's current context.	audio feedback;limbo;mobile device;modality (human–computer interaction);touchscreen	Eve E. Hoggan;Andrew Crossan;Stephen A. Brewster;Topi Kaaresoja	2009		10.1145/1518701.1519045	mobile interaction;human–computer interaction;computer science;operating system;mobile device;multimedia	HCI	-46.89003809311621	-44.461054150611325	163216
6d3de4813430a154be8478512ce1688eac32fece	building a toolkit for fabricating interactive objects		Despite the recent proliferation of easy-to-use personal fabrication devices, designing custom objects that are useful remains challenging. RFID technology can allow designers to easily embed rich and robust interaction in custom creations at low cost.	radio-frequency identification	Andrew Spielberg;Alanson P. Sample;Scott E. Hudson;Jennifer Mankoff;James McCann	2016	ACM Crossroads	10.1145/2889427	simulation;multimedia	HCI	-45.33940638272076	-38.539384041962556	163372
e6fdc3cd184e94126b8f1207be325822dc0f54b9	gazehorizon: enabling passers-by to interact with public displays by gaze	gaze interface;walk up and use;public display;eye tracking;spontaneous interaction;field study	Public displays can be made interactive by adding gaze control. However, gaze interfaces do not offer any physical affordance, and require users to move into a tracking range. We present GazeHorizon, a system that provides interactive assistance to enable passers-by to walk up to a display and to navigate content using their eyes only. The system was developed through field studies culminating in a four-day deployment in a public environment. Our results show that novice users can be facilitated to successfully use gaze control by making them aware of the interface at first glance and guiding them interactively into the tracking range.	interactivity;software deployment	Yanxia Zhang;Jörg Müller;Ming Ki Chong;Andreas Bulling;Hans-Werner Gellersen	2014		10.1145/2632048.2636071	computer vision;eye tracking;multimedia;field research	HCI	-45.9524077406964	-43.10559971665246	164729
0e16f1944c4bf4a2c08e2efa7f5555716a797c7d	tagurit: a proximity-based game of tag using lumalive e-textile displays	social computing;organic user interfaces;user interface;tangible computing;e textiles;ubiquitous computing;philips lumalive;wearable computer;wearable computing;virtual worlds	We present an electronic game of tag that uses proximity sensing and Lumalive displays on garments. In our game of tag, each player physically represents a location-tagged Universal Resource Indicator (URI). The URIs, one chaser and two target players, wear touch-sensitive Lumalive display shirts. The goal of the game is for the chaser to capture a token displayed on one of the Lumalive shirts, by pressing a touch sensor located on the shirt. When the chaser is in close proximity to the token player, the token jumps to the shirt of the second closest player, making this children's game more challenging for adult players. Our system demonstrates the use of interactive e-textile displays to remove the technological barrier between contact and proximity in the real world, and the seamless representation of gaming information from the virtual world in that real world.	electronic game;seamless3d;tag (game);touchscreen;uniform resource identifier;virtual world	Sylvia H. Cheng;Kibum Kim;Roel Vertegaal	2011		10.1145/1979742.1979707	simulation;wearable computer;human–computer interaction;computer science;operating system;multimedia;world wide web;ubiquitous computing;social computing	HCI	-46.710431268915954	-39.720918371479954	164993
8d60d9e7e1408ea719c6f4700b92436b9189bb4c	circuit stickers: peel-and-stick construction of interactive electronic prototypes	conductive inkjet;rapid prototyping;silver ink;physical computing;solderless electronics;tangible interfaces	We present a novel approach to the construction of electronic prototypes which can support a variety of interactive devices. Our technique, which we call circuit stickers, involves adhering physical interface elements such as LEDs, sounders, buttons and sensors onto a cheap and easy-to-make substrate which provides electrical connectivity. This assembly may include control electronics and a battery for standalone operation, or it can be interfaced to a microcontroller or PC. In this paper we illustrate different points in the design space and demonstrate the technical feasibility of our approach. We have found circuit stickers to be versatile and low-cost, supporting quick and easy construction of physically flexible interactive prototypes. Building extra copies of a device is straightforward. We believe this technology has potential for design exploration, research proto-typing, education and for hobbyist projects.	microcontroller;personal computer;sensor	Steve Hodges;Nicolas Villar;Nicholas Chen;Tushar Chugh;Jie Qi;Diana Nowacka;Yoshihiro Kawahara	2014		10.1145/2556288.2557150	embedded system;simulation;human–computer interaction;computer science;operating system;physical computing	HCI	-45.37831718779494	-38.64145383419076	165566
3452877c3081f6c2aa22c8db0c537e26a705a758	constructive exploration of spatial information by blind users	building block;computational method;blind users;force feedback;object tracking;augmented reality;orientation aids;spatial information;force feedback devices	When blind people wish to walk through an area not fully known to them, they have to prepare themselves even more thoroughly than sighted pedestrians. We propose a new approach to support this preparation with the help of an interactive computer method, called constructive exploration. Using this method, the user is guided in physically constructing the spatial arrangement to be learned using building blocks. We describe two implementations of the concept, one with a graspable interface with object tracking and the other employing a force feedback device. We report on first tests of the implementations.	geographic information system;haptic technology	Jochen Schneider;Thomas Strothotte	2000		10.1145/354324.354375	computer vision;augmented reality;simulation;computer science;video tracking;spatial analysis;multimedia;haptic technology	HCI	-45.374968131573304	-40.91856987589501	165780
56724f06d2cf5a3f7269fc7477b5a99ad7fa0d72	possession techniques for interaction in real-time strategy augmented reality games	limiting factor;user interface;interaction;gaming;real time strategy;virtual environment;augmented reality;interaction technique	There have been a number of interactive games created for Augmented Reality (AR) environments. In this paper, interaction techniques to support Real-Time Strategy (RTS) games in AR environments are investigated. One limiting factor is that the user's position within the virtual environment must correspond to their position in the physical world. If it is obvious to the user there is no correspondence between the two, the illusion of a consistent environment will be broken. The primary problem with adapting an RTS game for an AR environment is that the player will need to manage a large force of life-sized military units, which cannot be done effectively if the user is confined to what they can see and how fast they can move in the physical world. In this paper, we introduce the use of AR-VR transitions and a new technique that is called possession, which attempts to address these problems. Possession essentially gives the player the ability to move inside the head of any of their units. This allows the user to see everything that is visible to that unit, and manage their forces with the usual interface even though they are detached from their own body. The possession technique allows control of units over large ranges, makes micromanagement of distant groups possible, and implements realistic views of the world that match what a user would expect in the physical world. Our user interface supports a more realistic interface than is possible in traditional desktop games. Our new techniques were implemented in an operational AR-RTS game that we have named ARBattleCommander.	augmented reality;desktop computer;interaction technique;real-time locating system;real-time transcription;user interface;virtual reality	Keith Phillips;Wayne Piekarski	2005		10.1145/1178477.1178584	simulation;human–computer interaction;engineering;multimedia	HCI	-43.204846295191665	-38.15458127331441	165956
39c3b1b503d91d03fbfa380510c2acef338db518	improving accessibility of mobile devices with easywrite	assistive technologies;cerebral palsy;motor disabilities;touch based entry method;mobile accessibility	This paper describes the development process of EasyWrite, a text-entry method for mobile devices that allows people with hand coordination problems to use small computer devices such as smartphones, tablet PCs, or other touchscreen machines. This text-entry method aims at improving typing accuracy and reducing frustration of people affected by this motor disability when using small devices. EasyWrite was developed following an iterative and user-centred process. Starting from requirements elicited from observing potential users with mild and moderate motor disabilities and information provided by a literature review, a low-fidelity prototype was built and evaluated. This early prototype was refined throughout several design and evaluation iterations. Its current state is a functional prototype that works on Android phones. The functional prototype usability was evaluated through user tests. The result of this process is a small virtual keyboard for mobile devices that has less and bigger keys as compared...	accessibility;mobile device;mount rainier (packet writing)	Rui Godinho;Paulo A. Condado;Marielba Zacarias;Fernando G. Lobo	2015	Behaviour & IT	10.1080/0144929X.2014.981584	simulation;human–computer interaction;engineering;multimedia	HCI	-47.101191979749146	-44.83330787860827	166168
b3d68d968c90cd4fab3c2886e6388d4c1e5c9d83	tangible braille plot: tangibly exploring geo-temporal data in virtual reality		Despite the resurgence of virtual reality (VR), the primary method of interacting with the environment is using generic controllers. Given the often-purpose-built nature of applications within VR, this is surprising, as despite the effort put into the design of the application itself, the same attention is not paid to the input control. This is despite the advantages that tangible interfaces have for user understanding, especially in the context of visualization, where user understanding is paramount. This paper presents the adaptation of a previous 2D temporal-geospatial visualization into VR, and more importantly, describes the development of a novel 8DOF Tangible User Interface developed to support the exploration of that data. For our application, this centers around the exploration of geospatial data to explore colocation and divergence of entities, but could easily be extended to other domains. We present our novel controller as an example of the benefits of the utilization of purpose built physical controllers as a first-tier method of enabling immersive analytics. We describe the immersive system and controller, followed by an example use case and other applications encouraging further development of novel tangibles as a key component of immersive data analytics.	colocation centre;entity;independence day: resurgence;interaction;multitier architecture;tangible user interface;vergence;virtual reality	James Walsh;Andrew Cunningham;Ross Smith;Bruce Thomas	2018	2018 International Symposium on Big Data Visual and Immersive Analytics (BDVA)	10.1109/BDVA.2018.8534024	data visualization;input device;visualization;human–computer interaction;immersion (virtual reality);tangible user interface;virtual reality;data analysis;computer science;analytics	Visualization	-45.873853025748716	-39.93002883531671	166196
108e3f79357895b0285f67eed634c0d7615960c6	bci framework based on games to teach people with cognitive and motor limitations		Abstract   Nowadays, Human-Computer Interaction (HCI) systems e.g. keyboard, mouse and touch screen are frequently used by anyone. However, those ways to interact with computers may be not suitable for disabled persons. Brain-Computer Interface (BCI) is an HCI system which can be used as an alternative for these persons. This approach allows operating a computer using only the brain signals, for instance, imagine a situation where a participant only needs to think about an action in order to make it happen: to move, to select an object, to shift gaze, to control the movements of their (virtual) body, or to design the environment itself, by “thought” alone.  In this paper we propose a BCI framework to process brain signals resulting from imagination processes that is used together with serious games to teach or improve autonomy of physically handicapped people. Two simple scenarios based on grasp and eye-gaze imagination games were developed to test the framework. Games consist on choosing the right object to put on the recycling bin or to choose a piece of a puzzle to fit the other one in a white board. Through the proposed approach we are able to discriminate the direction intended by the player. Our results show that we achieve about 80% of accuracy when a block of 30 seconds of imagination is considered.	brain–computer interface	José Cecílio;João Andrade;Pedro Martins;Miguel Castelo-Branco;Pedro Nuno San-Bento Furtado	2016		10.1016/j.procs.2016.04.101	simulation;artificial intelligence;machine learning	HCI	-45.27450387510701	-44.23264297582242	166246
ed6fe12970a7b15e85d9104eee9b4ea55ec7177c	android app of location awareness using li-fi		Li-Fi is a technology that uses visible light from a Light Emitter Diode (LED) to transmit high speed data to a photo detector, which is connected to a smartphone or tablet. Location awareness refers to devices that can passively or actively determine their location with respect to a well-known location wireless communications device. In this paper we describe a Li-Fi based application, in which a smart phone or tablet with a light sensor and within the range of a Li-Fi lamplight, immediately trigger events like displaying a picture, accessing a Web page, or playing a video.	diode;light field;location awareness;smartphone;tablet computer;web page	Sergio Sandoval Reyes;Victor Hugo Herver Segura	2016	Research in Computing Science		location awareness;li-fi;android (operating system);internet privacy;computer science	HCI	-44.433949653532935	-41.650182708875604	166481
f6ad193ff7e13351163963dfc7f5f6eec19be5cd	study of the smartphone operation support system for visually impaired people		In this paper, we propose the smartphone operation support system for visually impaired. It is difficult for visually impaired people to grasp complicated screen configuration of smartphone. Visually impaired people have a problem that the people don't know the operation position of touch panel. Therefore, we created the touch screen application for visually impaired people to get the information easily. As a result, when comparing conventional application and proposed application in three environments “screen display”, “screen non-display”, and “blindfold”, we confirmed a drastic reduction in operation time of proposed method.	operation time;smartphone;touchscreen	Taiki Torigoe;Yoshihisa Nakatoh	2018	2018 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2018.8326296	computer vision;artificial intelligence;task analysis;computer science;operations support system;grasp	HCI	-44.929649084915795	-43.15593859449841	166679
d0bb14033d11613c50958379e6825859c31c15ee	techniques in swept frequency capacitive sensing: an open source approach		This paper introduces a new technique for creating Swept Frequency Capacitive Sensing with open source technology for use in creating richer and more complex musical gestures. This new style of capacitive touch sensing is extremely robust compared to older versions and will allow greater implementation of gesture recognition and touch control in the development of NIMEs. Inspired by the Touché project, this paper discusses how to implement this technique using the community standard hardware Arduino instead of custom designed electronics. This sensing technique requires only passive components and can be used to enhance the touch sensitivity of many conductive materials, even biological materials such as plants. This paper also introduces a new Arduino Library, SweepingCapSense, which simplifies the coding required to implement this technique. Lastly, this paper will discuss its use in the project known as Cultivating Frequencies.	arduino;capacitive sensing;gesture recognition;open-source software	Colin Honigman;Jordan Hochenbaum;Ajay Kapur	2014			capacitive sensing;gesture recognition;coding (social sciences);electronics;human–computer interaction;computer science;arduino;electronic component;gesture	HCI	-45.47324538000371	-38.244347564332756	166883
ca27050a5fd25016657ff4a1046f38043b4b7a56	mobipicture: browsing pictures on mobile devices	new technology;mobile device;adaptive content delivery;form factor;image adaptation;image browsing;mobile communication;attention model	Pictures have become increasingly common and popular in mobile communication. However, due to the limitation of mobile devices, there is a need to develop new technologies to facilitate the browsing of pictures on the small screen. MobiPicture is a prototype system which includes a set of novel features to aid or automate a set of common image browsing tasks such as the thumbnail view, set-as-background, zooming and scrolling.	image viewer;mobile device;prototype;scrolling;television;thumbnail	Ming-Yu Wang;Xing Xie;Wei-Ying Ma;HongJiang Zhang	2003		10.1145/957013.957037	computer vision;mobile search;mobile telephony;form factor;computer science;operating system;mobile device;multimedia;world wide web	HCI	-48.236211601381186	-41.22110641467132	166918
098a08304a41808e8fd4145717afe556a276f852	investigating drone motion as pedestrian guidance		Flying drones have the potential to act as navigation guides for pedestrians, providing more direct guidance than the use of handheld devices. Rather than equipping a drone with a display or indicators, we explore the potential for the drone's movements to communicate the route to the walker. For example, should the drone maintain a constant distance a few meters in front of the pedestrian, or should it position itself further along the navigation route, acting as a beacon to walk towards? We created a set of flying drone gestures and evaluated them in an online survey (n = 100) and an in-the-wild user test (n = 10) where participants were guided on a walking route by a flying drone. As a result, we propose an initial set of drone gestures for pedestrian navigation and provide further design recommendations.	amiga walker;course (navigation);field research;handheld game console;human–computer interaction;mobile device;smartphone;unmanned aerial vehicle	Ashley Colley;Lasse Virtanen;Pascal Knierim;Jonna Häkkilä	2017		10.1145/3152832.3152837	human–computer interaction;pedestrian;drone;computer science;human–robot interaction;mobile device;gesture	HCI	-46.454652363825204	-43.43583987221152	166999
6666374a639e41883ac20f412910ef67e06b36b8	designing a gesture-based interaction with an id tag	silicon;gesture based;information systems;id tag;interaction;probabilistic algorithm;rfid tag;id tag gesture based interaction accelerometer;radiofrequency identification accelerometers gesture recognition information systems;accuracy;probabilistic algorithm gesture based interaction personal id tag ambient information system vibrator embedded accelerometer sensor rfid tag;silicon probabilistic logic batteries pattern recognition accuracy;interaction pattern;batteries;pattern recognition;accelerometer;information system;probabilistic logic;accelerometers;gesture recognition;radiofrequency identification	We designed a gesture-based interface with a personal ID tag that interacts with an ambient information system. An ID tag is equipped with a vibrator, an embedded accelerometer sensor, and an RFID tag. In the suggested system, users can not only retrieve and store information, but customize interaction patterns with the ID tag that can be best suited to personal preferences. Patterns of gesture-based interaction are classified with a probabilistic algorithm developed in the study that use signal data from five different axes of the accelerometer sensor.	embedded system;gesture recognition;information system;randomized algorithm;vibrator (electronic)	Seongil Lee;Kyohyun Song;Jiho Choi;Yeeun Choi;Minsun Jang	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5641961	embedded system;speech recognition;computer science;gesture recognition;accelerometer;information system;statistics	Robotics	-43.57325928317463	-43.656620876857104	167023
fa09ca36126895972df9b2e5beafde4c461a8204	image-driven haptic rendering		Haptic interaction requires the content creators to make haptic models of the virtual objects while it is not always possible or feasible, especially when it comes to using real images or videos as elements of interaction. We, therefore, propose tangible images and image-driven haptic rendering where a displayed image is used as a source of the force-feedback calculations at any pixel touched by the haptic device. We introduce the main idea and describe how it is implemented as a core algorithm for image-driven haptic rendering, as well as for a few particular cases of haptic rendering emphasizing colors, contours and textures of the objects displayed in the images. Implementations of the proposed method to desktop tangible image application and haptic video communication on the web are presented as a	3d reconstruction;algorithm;color;desktop computer;haptic technology;image histogram;image segmentation;interactive media;motor theory of speech perception;object detection;pixel	Shahzad Rasool;Alexei Sourin	2014	Trans. Computational Science	10.1007/978-3-662-43790-2_4	stereotaxy;computer vision;computer science;multimedia;computer graphics (images)	Graphics	-42.08838390598376	-38.13614926668472	167123
c5448018ce672d883e8924dedc148785b1d91861	laplacian vision: augmenting motion prediction via optical see-through head-mounted displays and projectors	musical instrument;vibrotactile;haptic		head-mounted display;movie projector	Yuta Itoh;Yuichi Hiroi;Jiu Otsuka;Maki Sugimoto;Jason Orlosky;Kiyoshi Kiyokawa;Gudrun Klinker	2016		10.1145/2929464.2949028	computer vision;computer science;artificial intelligence;multimedia;haptic technology	Vision	-41.56593562092489	-38.90194967630641	167139
200ae0d256f0cfd04ba89ea0e18fc574c063d505	traxion: a tactile interaction device with virtual force sensation	light transport;light fields;image reconstruction;image sensor	"""Several systems have been proposed for incorporating haptic feedback into Human-Computer Interactions. There are two types of haptic feedback. In one type, which we call """"force-feedback,"""" a real-world physical force is created. In the second type, called """"tactile-display,"""" physical sensations, such as vibrations, are created as an additional feedback method."""		Jun Rekimoto	2014		10.1145/2614066.2614079	iterative reconstruction;computer vision;image sensor	HCI	-41.54534816804855	-39.84636756371924	167359
0d8c2b82066de2ba0a8c8dce5d6429971c45e108	fingertip guiding manipulator: haptic graphic display for mental image creation	aide handicape;handicapped aid;ayuda minusvalido;interfase usuario;representation graphique;affichage graphique;user interface;representation fonction;manipulateur;blind;graphic display;user assistance;assistance utilisateur;manipulador;sensibilidad tactil;user experience;function representation;asistencia usuario;representacion funcion;grafo curva;visualizacion grafica;interface utilisateur;manipulator;sensibilite tactile;ciego;graphics;tactile sensitivity;aveugle	A fingertip guiding manipulator was developed as a haptic graphic display for blind people: it helps them create mental images of raised graphics and reliefs with stroke order. Through fingertip kinesthetic sense, while foreseeing the direction of the upcoming stroke by perceiving the knob's direction, users experience the uplifted and fallen strokes by perceiving their fingertips being pulled in the horizontal and vertical directions. Especially, the up-lifting function expands a representation capability from “single stroke graphics” to “multiple stroke graphics	haptic technology	Yoshihiko Nomura;Yuki Yagi;Tokuhiro Sugiura;Hirokazu Matsui;Norihiko Kato	2006		10.1007/11788713_164	computer vision;user experience design;simulation;computer science;graphics;artificial intelligence;manipulator;function representation;user interface;computer graphics (images)	HCI	-46.47699353099548	-42.15153650573665	168048
7120d6651a01a7841e990c0f909162b6a2c03de8	precise and rapid interaction through scaled manipulation in immersive virtual environments	scaled manipulation;human computer interaction;haptic interfaces virtual reality human computer interaction solid modelling tracking;virtual objects;tracked hand held wand;direct manipulation;user study;prism;virtual reality;control display ratio;scaling up;3d space;precise manipulation immersive virtual environments virtual objects tracked hand held wand stylus object grabbing object positioning force feedback 3d space user interaction environment complexity prism scaled manipulation user behavior control display ratio controlled virtual object motion user hand movement direct manipulation;force feedback;long distance;controlled virtual object motion;object positioning;object grabbing;geometric modeling;immersive virtual environments;virtual environment arm computer graphics force feedback virtual reality haptic interfaces fingers motion control chromium computational modeling;user behavior;virtual environment;haptic interfaces;precise manipulation;user interaction;environment complexity;stylus;tracking;interaction technique;solid modelling;user hand movement;immersive virtual environment	"""A significant benefit of an immersive virtual environment is that it provides users the ability to interact with objects in a very natural, direct way; often realized by using a tracked, hand-held wand or stylus to """"grab"""" and position objects. In the absence of force feedback or props, it is difficult and frustrating for users to move their arms, hands, or fingers to precise positions in 3D space, and more difficult to hold them at a constant position, or to move them in a uniform direction over time. The imprecision of user interaction in virtual environments is a fundamental problem that limits the complexity of the environment the user can interact with directly. We present PRISM (precise and rapid interaction through scaled manipulation), a novel interaction technique which acts on the user's behavior in the environment to determine whether they have precise or imprecise goals in mind. When precision is desired, PRISM dynamically adjusts the """"control/ display"""" ratio which determines the relationship between physical hand movements and the motion of the controlled virtual object, making it less sensitive to the user's hand movement. In contrast to techniques like Go-Go, which scale up hand movement to allow """"long distance"""" manipulation; PRISM scales the hand movement down to increase precision. We present the results of a user study which shows that PRISM significantly out-performs the more traditional direct manipulation approach."""	bottleneck (engineering);coat of arms;cursor (databases);direct manipulation interface;direct mode;haptic technology;image scaling;interaction technique;mobile device;near field communication;prism (surveillance program);stylus (computing);tracking system;usability testing;velocity (software development);virtual reality;virtual world;wap identity module;wavelet	Scott Frees;G. Drew Kessler	2005	IEEE Proceedings. VR 2005. Virtual Reality, 2005.	10.1109/VR.2005.60	computer vision;simulation;human–computer interaction;computer science;virtual machine;artificial intelligence;geometric modeling;prism;virtual reality;tracking;haptic technology;stylus;interaction technique	Visualization	-41.060994024023614	-38.379232114642896	168081
351541bc06725659cb03431d31cbb5e05dd64161	vr management tools: beyond spatial presence	virtual reality;interactive environment;spatial presence;virtual reality environment;management tool	We have created three types of user-eontmlled management tool for use in virtual reality environments: the 3DMenu, the M-Cube, and the SuperCube. 3DMenus are equivalent to the menu systemsfound in two-dimensional interactive environments,but have the necessaryspatial preseneefer the immersive 3D world of a VR application. M-Cubes are directly equivalent to 3DMenus but by using all six available surfaces to present selection alternatives, occupy significantly less space. SuperCubes, in contrast to both these approaches, reflect a move beyond spatial presence by the attachment of meaning to fheii manipulation; the dimensions of space are also dimensions of information. KEVWORDS: Virtual Reality, VR, interface management tools, 3D, information dimensio@ity, menus. INTRODUCTION Most of today’s user interfaces for standard 3D applications do not make much use of the thii dimension [1]. The result is that users interact with variations of standard, 2D interface objects such as menus. No attempt is nwie to capitalize on the possibilities for enhanwd HCI that the extra dimension provides, partly due to the absence of appropriate design models. Work to develop convincing VR has focused chiefly on achieving a nmlistic sense of a 3D world, and of manipulable 3D objects with which the user can interact within that virtual world [4]. Relatively little attention has been dirwted to how the user interacts with those aspects of the application that do not form part of the simulated 3D reality. We refer to techniques that support such interactions as VR munugemen? tools, and we report three approaches to their design. THREE VR MANAGEMENT TOOLS Any tools existing in a VR environment must possess full thmedimensional presenee. The most obvious approach is to adapt standard interaction techniques, such as menus, for 3D worlds. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee andlor specific permission. CH194 Companion-4/94 Boston, Massachusetts USA 41994 ACM 0-89781 -651 -4/94 /0319 . ..$3.50 iuis(ii?iss.nus.sg 3DMenus ‘Ilw ‘virtual menus’ WWl~y standard described by Jacoby et al. [3] are menu systems rendered in three dimensio& for use in VR environments. Gloved hand gestures are used for menu invocation, to highlight an item, to select an item, and to move the menu. Chir focus is on more accurate interactive tools for prezise 3D work such as surgery [5], and our 3DMenus were designed with this type of application in mind.	3d computer graphics;attachments;cube;cubes;human–computer interaction;interaction technique;mobilehci;samsung gear vr;user interface;virtual reality;virtual world	John A. Waterworth;Luis Serra	1994		10.1145/259963.260384	computer-mediated reality;computer science;artificial intelligence;virtual reality;mixed reality	HCI	-45.058580724864534	-38.434265087855486	169277
a7d03bd5c1ba6320c8bdbfffe0d55391ef36bd2e	tablaction: collaborative brainstorming system with stylus-fingertip interactions on tablet pcs	interaction;interaction style;tablet pc;brainstorming meeting;interaction design;stylus;fingertip	In this paper, we introduce the Tablaction that supports collaborative brainstorming process by unifying various interactions utilize stylus and multi-touch finger input. Additionally, we demonstrate the specially designed stylus for it, and the paper prototype test. In a brainstorming meeting, there is a problem that everyone cannot equally participate in the brainstorming. There were several approaches to this problem; however, their interaction styles were limited to the devices. Tablet devices with multi-touch capability are commercialized fast in these days. In addition, the researches on pressure recognition to replace the touch panel are actively on progress. With the technologies, the tablets would be capable to recognize a stylus, and distinguishing it with a fingertip. We expect that the interaction design with the capability will accelerate idea expressions in a brainstorming meeting.	interaction design;multi-touch;paper prototyping;prototype;stylus (computing);tablet computer;touchscreen	Hyunyoung Kim;Kuenhwan Kwak;Jongwoo Jung;Insik Myung;Minsoo Hahn	2010		10.1145/1900179.1900195	interaction;simulation;human–computer interaction;computer science;interaction design;multimedia;stylus	HCI	-47.12857344993312	-41.907923825497946	169474
bc7c43a0dff97f1129768b5bca8f693ee3a8f2c8	cadlens: haptic feedback for navigating in 3d environments		CADLens is a tangible input device with haptic feedback for navigating three dimensional Computer-Aided Design (CAD) environments. CADLens is intended to provide more intuitive controls with a low barrier of entry for first time or less experienced CAD users. Navigation in the CAD environment is possible through manipulating the physical controller with guidance provided through haptic force feedback. In CAD environments, a common viewport used by designers is the perspective viewport camera - CADLens represents the virtual camera position through its synchronized physical form which the user can move, zoom, and rotate. We conducted a study with 10 users in which participants were asked to complete navigation tasks using a model of a building. Users were able to successfully complete the navigation tasks and through contextual interviews provided feedback for system improvements.	computer-aided design;contextual inquiry;haptic technology;input device;midi controller;viewport;virtual camera system	Nikolaj Haulrik;Rasmus M. Petersen;Timothy Merritt	2017		10.1145/3064857.3079132	multimedia;engineering;viewport;simulation;control theory;input device;zoom;haptic technology;3d modeling;computer vision;artificial intelligence	HCI	-44.361095623959	-39.17432759272522	169495
0021f48e898ae8d8ae7a8371c0a450577ac97d4e	advanced graphics technology	displays;image reconstruction;object recognition;emerging technologies;rubber;laser radar;machine learning;three dimensional;computer graphics;robotics;artificial intelligent;photoelasticity;gesture recognition;prototypes;augmented reality;computer graphic;emerging technology;graphics;image recognition;visualization;robot kinematics;lidar;mobile computing;human computer interaction;merging	As computing becomes more mobile, there's an increased need to develop more ad vanced input tools and methods. Screens are smaller, cameras are more ubiquitous, and touch technology is everywhere. Yet entering text, choosing graphics entities, performing drag-and drop, and so on are still difficult. This month's column describes new input modalities that add more options to researchers' arsenals. Much of the technology I describe contains approaches first tried in desktop configurations. Mobility has ac celerated the pace of adoption and adds even more demands in terms of responsiveness, flexibility, and adaptability to an individuals.	choose (action);computation (action);desktop computer;entity;graphics;responsiveness	David J. Kasik	2010	IEEE Computer Graphics and Applications	10.1109/MCG.2010.47	lidar;computer vision;augmented reality;computer science;artificial intelligence;operating system;gesture recognition;multimedia;robotics;emerging technologies;mobile computing;computer graphics (images)	Visualization	-45.749977952512744	-41.188716868154934	169908
e75173681aa78ef7af82b0b47b1d800e58f63b12	a projected augmented reality system for remote collaboration	hit;technology;lab;kinect fusion;conference contribution paper in published proceedings;projection remote collaboration kinect fusion augmented reality;hitlab;projection;interface;human;nz;kinectfusion;augmented reality;remote collaboration	This paper describes an AR system for remote collaboration using a captured 3D model of the local user’s scene. In the system a remote user can manipulate the scene independently of the view of the local user and add AR annotations that appear projected into the real world. Results from a pilot study and the design of a further full study are presented.	ar (unix);augmented reality;bmc remedy action request system;ccir system a;java annotation	Matthew Tait;Tony Tsai;Nobuchika Sakata;Mark Billinghurst;Elina Vartiainen	2013		10.1109/ISMAR.2013.6671838	augmented reality;simulation;projection;computer science;interface;multimedia;computer graphics (images);technology	HCI	-43.1043709692978	-38.49169157557694	170789
129be707c4422e11f7abf8d0481ca13839c35a63	sensing foot gestures from the pocket	hands free interaction;mobile device;visually impaired users;foot based gestures;mobile phone;machine learning;user experience;eyes free interaction;mobile devices	Visually demanding interfaces on a mobile phone can diminish the user experience by monopolizing the user's attention when they are focusing on another task and impede accessibility for visually impaired users. Because mobile devices are often located in pockets when users are mobile, explicit foot movements can be defined as eyes-and-hands-free input gestures for interacting with the device. In this work, we study the human capability associated with performing foot-based interactions which involve lifting and rotation of the foot when pivoting on the toe and heel. Building upon these results, we then developed a system to learn and recognize foot gestures using a single commodity mobile phone placed in the user's pocket or in a holster on their hip. Our system uses acceleration data recorded by a built-in accelerometer on the mobile device and a machine learning approach to recognizing gestures. Through a lab study, we demonstrate that our system can classify ten different foot gestures at approximately 86% accuracy.	accessibility;interaction;lambda lifting;machine learning;mobile device;mobile phone;user experience	Jeremy Scott;David Dearman;Koji Yatani;Khai N. Truong	2010		10.1145/1866029.1866063	embedded system;user experience design;simulation;computer science;operating system;mobile device	HCI	-45.67728000019562	-43.75758658121668	170829
37a406af077ad9a71ed44089d475d53f5cce8e17	making textile sensors from scratch	craft;diy;sensors;handmade;electronic textiles;e textiles;soft circuitry;conductive materials	This workshop will explore the use of low-cost materials and tools to build textile-based interfaces. We will introduce a range of methods for handcrafting textile sensors and circuitry. Participants will learn techniques developed by the workshop leaders and will also be encouraged to use our material library to design their own custom sensors. The goal of the workshop is to familiarize participants with available electronic textile materials and introduce them to a variety of sensor and circuitry construction techniques.	e-textiles;electronic circuit;sensor	Hannah Perner-Wilson;Leah Buechley	2010		10.1145/1709886.1709972	engineering;nanotechnology;mechanical engineering	HCI	-45.55437540147826	-38.264530104238766	171236
0e45d570ff566ba289519d4696e317dcec879759	friidom: an intelligent driver for automatic configuration in modular display systems	display driver;modular display;languages and literatures;intelligent system;free form display;automatic configuration;asic	FrIIDoM provides an intelligent solution for automatic configuration in modular display systems. Modular displays are displays that consist of small, identical modules. There are a number of ways they can be used: removing the limitation in multiplexability in passive matrix PM displays, increasing brightness in PMOLED displays, reducing power consumption in bistable displays, creating large-scale displays or displays with variable size and shape. For the modules to work together as one display, some initial configuration is required. FrIIdoM consists of four different drivers, with rising levels of complexity, each with their own way of automating the configuration. The simplest systems just make sure that every module is capable of receiving specific image data, irrespective of the shape of the display. The more complex systems are capable of detecting the presence and location of each module, thus detecting the shape of the display. In the created GUI, the user sees a real-time view of the display configuration, even when modules are added or removed while the display is running. While FrIIDoM is equipped with the drivers to drive a PM LED display, they can be easily changed to incorporate other types of display technologies. Several test modules were created, and we were able to build and successfully drive random display creations. Copyright © 2013 John Wiley & Sons, Ltd.		Pieter Bauwens;Jan Doutreloigne	2014	I. J. Circuit Theory and Applications	10.1002/cta.1918	embedded system;computer hardware;computer science;engineering;application-specific integrated circuit;engineering drawing;display device	EDA	-42.03172606653304	-41.41443111486838	171521
5db48dad154a0c50d83a33e692cba37d4dea1712	smart audio/video playback control based on presence detection and user localization in home environment	automatic control;microphones;audio visual systems;time of flight;smart home;sensors;user interface;ambient intelligence;information retrieval;audio video;home appliances smart audio video playback control presence detection user localization home environment software based home control platform 3d camera microphone array pir sensors upnp dlna rendering devices pc tv audio system xml documents user interface mobile phones;three dimensional;kinect;3d camera;mobile phone;arrays;user localization;design and implementation;microphone array;three dimensional displays;multimedia communication;xml;cameras sensors three dimensional displays microphones multimedia communication arrays;mobile handsets;user localization 3d camera ambient intelligence microphone array passive infrared sensor smart home kinect time of flight upnp control point;xml document;xml audio visual systems cameras home automation microphone arrays mobile computing mobile handsets optical sensors user interfaces;upnp control point;optical sensors;microphone arrays;infrared;mobile computing;user interfaces;cameras;home automation;passive infrared sensor	This paper presents the design and implementation of a simple software-based home control platform used for the automatic control of audio/video devices. The system facilitates integration of various residential sensors, with an accent on users' localization and presence detection. For the presence detection and localization we utilize three, the most frequently used technologies: visual (3D camera), audio (Microphone array) and passive infrared (PIR sensors). The home controller interprets information about user's position as a command issued to a list of UPnP/DLNA rendering devices (PC, TV or Audio system). Current distance and user's position can activate, abort or change video presentation, pause and continue playback, amplify sound or silence it automatically with regard to information retrieved from sensors and actions described in a so-called ambient behavior patterns. The way the system automatically responds to detected positions is controllable and changeable, and it is defined by executing XML documents which represent the behavior patterns - scripts. By using the easily accessible user interface, users are able to choose one of the available scripts or to prepare a new one, and to set up different audio/video modes in a room, similarly to switching profiles on mobile phones. The system is not limited to presence detection and audio/video control. Performing simple modifications of behavior scripts, the controller can interpret data from different kinds of sensors in an unobtrusive way of controlling various home appliances.	algorithm;automatic control;home automation;internationalization and localization;microphone;mobile phone;personal computer;sensor;sound card;universal plug and play;unobtrusive javascript;user interface;xml	Bojan Mrazovac;Milan Z. Bjelica;Istvan Papp;Nikola Teslic	2011	2011 Second Eastern European Regional Conference on the Engineering of Computer Based Systems	10.1109/ECBS-EERC.2011.16	embedded system;home automation;xml;computer science;engineering;operating system;automatic control;multimedia;user interface;mobile computing;world wide web	HCI	-43.83551296930744	-41.857337899804406	171898
d633ccf65e947ab462b0923b15c89ad15ddaa5ca	stompboxes: kicking the habit		Sensor-based gesture recognition is investigated as a possible solution to the problem of managing an overwhelming number of audio effects in live guitar performances. A realtime gesture recognition system, which automatically toggles digital audio effects according to gestural information captured by an accelerometer attached to the body of a guitar, is presented. To supplement the several predefined gestures provided by the recognition system, personalized gestures may be trained by the user. Upon successful recognition of a gesture, the corresponding audio effects are applied to the guitar signal and visual feedback is provided to the user. An evaluation of the system yielded 86% accuracy for user-independent recognition and 99% accuracy for user-dependent recognition, on average.	audio signal processing;gesture recognition;performance;personalization	Gregory Burlet;Ichiro Fujinaga	2013			human–computer interaction;multimedia;computer science;habit	HCI	-45.508974829083215	-43.845464878319646	172504
4f3300e8078f91c0e2b6197d1573ec89da20d0c7	a mobility aid for the support to walking and object transportation of people with motor impairments	person tracking motor impaired persons mobility aid walking object transportation motorised rollator force enhancement collision avoidance follow me mode bilateral grips strain gage force sensors motor speed control trilateration system ultrasonic sensing technology infrared sensing technology us sensing technology ir sensing technology user worn transponder;infrared sensing technology;input device;walking;velocity control;motorised rollator;force sensors;legged locomotion;prototypes;strain control;mobile robots;bilateral grips;motor impaired persons;us sensing technology;medical robotics handicapped aids collision avoidance strain gauges force sensors transponders materials handling mobile robots;medical robotics;force enhancement;handicapped aids;materials handling;transportation;trilateration system;collision avoidance;infrared;mobility aid;transponders;strain gauges;person tracking;follow me mode;speed control;user worn transponder;ir sensing technology;strain gage force sensors;legged locomotion transportation force sensors force control prototypes collision avoidance strain control capacitive sensors velocity control transponders;object transportation;force sensor;capacitive sensors;ultrasonic sensing technology;motor speed control;force control	In this paper we describe a motorised rollator which aims at supporting people with motor impairments while walking and transporting objects. The prototype presented here incorporates a few additional functions, including the force enhancement to the user in the direct control mode, and the ability to follow the user at a distance, with or without collision avoidance, in the follow-me mode. The user input device for the direct control mode is composed of bilateral grips, equipped with strain gage force sensors, whose output signals are used for motor speed control. The follow-me mode is built around a trilateration system based on ultrasonic and infrared sensing technologies; the system activates a user-worn transponder for person tracking. We discuss both the motivations behind the control modes of interest and the technical solutions we have devised for their implementation. The results of some preliminary experiments are also presented and discussed, to show the feasibility of the approach.		Angelo M. Sabatini;Vincenzo Genovese;Elena Pacchierotti	2002		10.1109/IRDS.2002.1043942	control engineering;mobile robot;embedded system;transport;simulation;infrared;strain gauge;computer science;engineering;artificial intelligence;transponder;electronic speed control;prototype;capacitive sensing;input device;force-sensing resistor	HCI	-41.21582199980276	-44.16451365121935	172603
6303a5c722db1349c238c2778d4d2225c6b5f4f1	magic broker: a middleware toolkit for interactive public displays	personal mobile devices;protocols;middleware large screen displays protocols java pervasive computing prototypes advertising interactive systems web services graphical user interfaces;middleware interactive application large screen display mobile interaction;mobile device;design and development;pervasive computing;prototypes;restful web services protocol magic broker middleware toolkit interactive public displays large screen displays personal mobile devices;middleware interactive devices large screen displays;web service;restful web services protocol;large screen display;graphical user interfaces;interactive application;middleware toolkit;interactive public displays;web services;public display;middleware;large screen displays;interactive systems;mobile interaction;interactive devices;magic broker;java;advertising;qa76 computer software	Large screen displays are being increasingly deployed in public areas for advertising, entertainment, and information display. Recently we have witnessed increasing interest in supporting interaction with such displays using personal mobile devices. To enable the rapid development of public large screen interactive applications, we have designed and developed the MAGIC Broker. The MAGIC Broker provides a set of common abstractions and a RESTful Web services protocol to easily program interactive public large screen display applications with a focus on mobile device interactions. We have carried out a preliminary evaluation of the MAGIC Broker via the development of a number of prototypes and believe our toolkit is a valid first step in developing a generic support infrastructure to empower developers of interactive large screen display applications.	display device;interaction;middleware;mobile device;representational state transfer;web service	Aiman Erbad;Michael Blackstock;Adrian Friday;Rodger Lea;Jalal Al-Muhtadi	2008	2008 Sixth Annual IEEE International Conference on Pervasive Computing and Communications (PerCom)	10.1109/PERCOM.2008.109	web service;computer science;operating system;multimedia;internet privacy;world wide web;ubiquitous computing	HCI	-48.25537329754386	-38.73096451664805	172621
f316e549aeef715baf1437e5d2d847df23f5b061	multimodal kinect-supported interaction for visually impaired users	eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems	"""This paper discusses Kreader, a proof-of-concept for a new interface for blind or visually impaired users to have text read to them. We use the Kinect device to track the users body. All feedback is presented with auditory cues, while a minimal visual interface can be turned on optionally. Interface elements are organized in a list manner and placed ego-centric, in relation to the user's body. Moving around in the room does not change the element's location. Hence visually impaired users can utilize their """"body-sense"""" to find elements. Two test sessions were used to evaluate Kreader. We think the results are encouraging and provide a solid foundation for future research into such an interface, that can be navigated by sighted and visually impaired users."""	kinect;multimodal interaction	Richard Gross;Ulrich Bockholt;Ernst W. Biersack;Arjan Kuijper	2013		10.1007/978-3-642-39188-0_54	simulation;human–computer interaction;engineering;multimedia	HCI	-47.95892616741739	-39.592398673773424	172631
4c0964813c631891be333dd08e278777906ed4f6	humantenna: using the body as an antenna for real-time whole-body interaction	whole body;body as antenna;real time;whole body gestures;interactive system;human body;mobile application;gesture recognition;electrical noise	Computer vision and inertial measurement have made it possible for people to interact with computers using whole-body gestures. Although there has been rapid growth in the uses and applications of these systems, their ubiquity has been limited by the high cost of heavily instrumenting either the environment or the user. In this paper, we use the human body as an antenna for sensing whole-body gestures. Such an approach requires no instrumentation to the environment, and only minimal instrumentation to the user, and thus enables truly mobile applications. We show robust gesture recognition with an average accuracy of 93% across 12 whole-body gestures, and promising results for robust location classification within a building. In addition, we demonstrate a real-time interactive system which allows a user to interact with a computer using whole-body gestures	astah*;computer vision;cylinder-head-sector;gesture recognition;instrumentation (computer programming);interactivity;mobile app;mobile device;numerical aperture;real-time clock;real-time computing;real-time locating system;user interface	Gabe Cohn;Dan Morris;Shwetak N. Patel;Desney S. Tan	2012		10.1145/2207676.2208330	computer vision;human body;simulation;computer science;gesture recognition;noise	HCI	-43.132910084066275	-42.390465510588804	173165
e9147406ec282df92bd738a4e3c3d8ae8e815e1a	huggy pajama: a mobile parent and child hugging communication system	programmable environments;communication system;learning;user centered design;tangicons;system design;iterative design;evaluation;tangible media;physical interaction	Huggy Pajama is a novel wearable system aimed at promoting physical interaction in remote communication between parent and child. This system enables parents and children to hug one another through a novel hugging interface device and a wearable, hug reproducing pajama connected through the Internet. The hugging device is a small, mobile doll with an embedded pressure sensing circuit that is able to accurately sense varying levels of the range of human force produced from natural touch. This device sends hug signals to a haptic jacket that simulates the feeling of being hugged to the wearer. It features air pockets actuating to reproduce hug, heating elements to produce warmth that accompanies hug, and color changing pattern and accessory to indicate distance of separation and communicate expressions. In this paper, we present the system design of Huggy Pajama.	embedded system;haptic technology;human–computer interaction;internet;systems design;user (computing);wearable computer	James Keng Soon Teh;Adrian David Cheok;Roshan Lalintha Peiris;Yongsoon Choi;Vuong Thuong;Sha Lai	2008		10.1145/1463689.1463763	simulation;engineering;multimedia;communication	HCI	-46.89133679135647	-39.34191337165204	173515
a989d1688c8c76b447edd92d9781a6658a36dcf8	ambient intelligence	people detection;ambient intelligence;smart environments.;machine learning	In the last five years we have seen significant advances in three promising technology areas: virtual environments, in which 3D displays and interaction devices immerse the user in a synthesized world, mobile communication and sensors, in which increasingly small and inexpensive terminals and wireless networking allow users to roam the real world without being tethered to stationary machines. The merging of these areas allows the emergence of a new vision: the Ambient Intelligence (AmI), a pervasive and unobtrusive intelligence in the surrounding environment supporting the activities and interactions of the users. The most ambitious expression of AmI is the Mobile Mixed Reality: the enhancement of information of a mobile user about a real scene through the embedding of one or more objects (3D, images, videos, text, computer graphics, sound, etc) within his/her sensorial field. Within this vision, the concept of presence evolves, too. The sense of “being there” covers only the simulation and immersive sides of the sense of presence. To be “present” in the augmented context offered by the AmI, the user has to be aware of its meaning and accept its hybridity. Only “making sense there”, the user experiences a full sense of presence.	ambient intelligence;anomalous experiences;computer graphics;emergence;experience;interaction;mixed reality;pervasive informatics;sensor;simulation;stationary process;virtual reality	Pierpaolo Loreti;Francesco Vatalaro;Fabrizio Davide	2017		10.1007/978-3-319-56997-0		HCI	-47.04089835489118	-39.86844459945447	174119
8505bb8b305bdace83868af1718ee4d04e551456	sparsh: passing data using the body as a medium	data sharing;interactive method;touch based interaction;intuitive interfaces;copy and paste	SPARSH explores a novel interaction method to seamlessly transfer data among multiple users and devices in a fun and intuitive way. The user touches a data item they wish to copy from a device, conceptually saving in the user's body. Next, the user touches the other device they want to paste/pass the saved content. SPARSH uses touch-based interactions as indications for what to copy and where to pass it. Technically, the actual transfer of media happens via the information cloud.	data item;interaction;multi-user;paste;sparsh (software)	Pranav Mistry;Suranga Nanayakkara;Pattie Maes	2011		10.1145/1958824.1958946	computer science;multimedia;internet privacy;world wide web	HCI	-46.72011401406361	-39.79293235578887	174220
3e754e505db570d6460e9a43e3e0cb5554b2699f	interfacing to the foot: apparatus and applications	body suit;degree of freedom;real time;wireless sensing;motion capture;base station;sensor shoes	We describe a system that we have developed to capture detailed, multimodal gesture expressed at the foot. It is embodied in a pair of shoes, each of which measures 16 degrees of freedom (tactile, inertial, positional). No tethers or wires are attached to the shoes; data is directly telemetered wirelessly off each foot to a remote base station and host computer, yielding full state updates at 50 Hz. This system, having evolved over 3 years, has been used for real-time expressive performance by a diverse set of artists, including gymnasts, jugglers, and dancers. Ongoing work is exploring the extraction of high-level podiatric gesture.	gesture recognition;high- and low-level;host (network);multimodal interaction;real-time transcription;shoes	Joseph A. Paradiso;Kai-yuh Hsiao;Ari Y. Benbasat	2000		10.1145/633292.633389	embedded system;motion capture;simulation;computer science;base station;degrees of freedom	HCI	-42.31546835549801	-42.579742928285846	174287
e4b9d793cce7416600c677098478ef857f5b436b	managing smartwatch notifications through filtering and ambient illumination	notification filtering;smartwatch;notifications;led	The ongoing development of smart, wearable devices opens up a new range of possibilities with respect to human-computer interaction. Recent research has confirmed that smartwatches are primarily used to visualize notifications. However, the limited screen size is at odds with the ever-growing amount of information. Often, explicit interaction is needed to get an overview on the currently available information. We provide an aggregation/filtering approach as well as several displaying concepts based on a self-built, power-efficient smartwatch prototype with twelve full-color LEDs around a low-resolution display. In a user study with twelve participants, we evaluated our concepts, and we conclude with guidelines that could easily be applied to today's smartwatches to provide more expressive notification systems.	ambient intelligence;android wear;computer form factor;display size;human–computer interaction;prototype;response time (technology);smartwatch;usability testing;wearable technology	Frederic Kerber;Christoph Hirtz;Sven Gehring;Markus Löchtefeld;Antonio Krüger	2016		10.1145/2957265.2962657	simulation;computer science;operating system;smartwatch;multimedia;world wide web;light-emitting diode	HCI	-47.67521732504398	-41.28878148965154	174290
b6775db86a96bf95da1fb241d1cfa9ec38c8729c	navigating a maze with balance board and wiimote	human computer interface	Input from the lower body in human-computer interfaces can be beneficial, enjoyable and even entertaining when users are expected to perform tasks simultaneously. Users can navigate a virtual (game) world or even an (empirical) dataset while having their hands free to issue commands. We compared the Wii Balance Board to a hand-held Wiimote for navigating a maze and found that users completed this task slower with the Balance Board. However, the Balance Board was considered more intuitive, easy to learn and ‘much fun’.	mobile device;wii	Wim Fikkert;Niek Hoeijmakers;Paul E. van der Vet;Anton Nijholt	2009		10.1007/978-3-642-02315-6_18	simulation;human–computer interaction;computer science;multimedia	HCI	-45.6350068490298	-43.873231598531284	174957
4a27d6794661825a09c06460bbcbc344ec67f27b	flexcam: using thin-film flexible oled color prints as a camera array	thin film;organic user interfaces;user interface;tangible user interface;flexible displays;organic light emitting diode;field of view;tangible user interfaces;flexible camera arrays	FlexCam is a novel compound camera platform that explores interactions with color photographic prints using thinfilm flexible color displays. FlexCam augments a thinfilm color Flexible Organic Light Emitting Diode (FOLED) photographic viewfinder display with an array of lenses at the back. Our prototype allows for the photograph to act as a camera, exploiting flexibility of the viewfinder as a means to dynamically re-configure images captured by the photograph. FlexCam's flexible camera array has altered optical characteristics when flexed, allowing users to dynamically expand and contract the camera's field of view (FOV). Integrated bend sensors measure the amount of flexion in the display. The degree of flexion is used as input to software, which dynamically stitches images from the camera array and adjusts viewfinder size to reflect the virtual camera's FOV. Our prototype envisions the use of photographs as cameras in one aggregate flexible, thin-film device.	aggregate data;field of view in video games;flexible organic light-emitting diode;interaction;oled;prototype;sensor;virtual camera system	Connor Dickie;Nicholas Fellion;Roel Vertegaal	2012		10.1145/2212776.2212383	smart camera;computer vision;field of view;oled;flexible display;user interface;thin film;computer graphics (images)	HCI	-43.904183826565024	-39.77122877207422	175128
8a04373c82b692acfa43c895a1c27a44f9bdbb67	personal sensory vr interface utilizing wearable technology		Current wearable technology centered on medical and healthcare fields has grown, and various types of controllers are being developed along with the development of virtual reality interfaces. Nevertheless, commercialized controllers cannot yet deliver a variety of sensory information to users in virtual reality (VR) environments as they are still providing haptic vibration level information. To convey a high sense of immersion through the presence of users in the virtual environment, a personal sensory VR system focusing on the user's touch is proposed in this study. Emogle is a personal haptic VR interface that is worn on the user's hand and was developed to deliver various types of sensory information through vibration, thermal emotion, wind, and electrical stimulation in a VR environment.	experiment;functional electrical stimulation;haptic technology;immersion (virtual reality);interaction design;interactivity;multi-touch;virtual reality;wearable computer;wearable technology	Jiyoung Kang;Junhee Lee;Seungyeon Jin	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539405	human–computer interaction;wearable technology;immersion (virtual reality);visualization;haptic technology;virtual machine;virtual reality;sensory system;computer science;stimulation	Visualization	-46.4760808908224	-41.208966226212986	175154
2cbe0d922ab6f157069d434ad04b94d4cd7648e9	tangible sketching of interactive haptic materials	design tool;human computer interaction;manniska datorinteraktion interaktionsdesign;haptics;tangible interface;tactile perception;on the fly;design;interaction design;sketching;tangible interfaces;haptic interaction;material properties	The activity of sketching can be highly beneficial when applied to the design of haptic material interaction. To illustrate this approach we created a design tool with a tangible hardware interface to facilitate the act of haptic material sketching and used this tool to design an anatomy exploration application. We found this approach particularly efficient in designing non-visual properties of haptic materials. The design tool enabled instant tactile perception of changes in material properties combined with the ability to make on-the-fly adjustments, thus creating a sense of pliability.	design tool;haptic technology;hardware interface design;tactile imaging	Jonas Forsslund;Ioanna Ioannou	2012		10.1145/2148131.2148156	human–computer interaction;engineering;multimedia;computer graphics (images)	HCI	-44.94072988252463	-39.36283306846837	175409
b0c19e24db2eaa659bd38fbeadca0aeb2e141445	development and evaluation of the cabin immersive multiscreen display	virtual reality	This paper describes the design, implementation, and evaluation of the CABIN immersive multiscreen display. CABIN consists of five screens that surround users, and one of the five, the floor screen, is made of tempered glass able to support the usersu0027 weight. In CABIN, computer graphics images and video images can be projected. By using a five-screen configuration, the user is able to look at three-dimensional objects from various directions, as well as look around a wide area of the virtual world. A quantitative evaluation showed that multiple screens are indispensable to support a large area of viewpoint change for the purpose of displaying three-dimensional objects. In addition, distortion of the generated virtual space was evaluated. From the experimental results, the user is able to perceive space accurately with little distortion when standing near the center of the display space. © 1999 Scripta Technica, Syst Comp Jpn, 30(1): 13–22, 1999	multi-screen video	Michitaka Hirose;Tetsuro Ogi;Shohei Ishiwata;Toshio Yamada	1999	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199901)30:1%3C13::AID-SCJ2%3E3.0.CO;2-L	simulation;computer science;artificial intelligence;virtual reality;multimedia;computer graphics (images)	NLP	-41.76375613756725	-38.80691827388976	176563
6675616bd2afe22245ebd6ce721ccc32a8a0404d	a simple standalone sign based recognition translator without using super computer processing	sign language;recognition system;sd card;pic microcontroller	In this paper, the design and development of a simple standalone sign based recognition translator without the use of supercomputer processing is presented. The main components for digital signal processing (DSP) and data storage in this translator system are peripheral interface controller (PIC) microcontroller and secure digital (SD) card respectively. The PIC 18F4620 is chosen as system's DSP because it is able to interface with the SD card. The input of sign gestures is being recognized and translated into text message such that people can understand its meanings. A flex sensor is attached to each finger in a data glove which has the capability to measure the finger sign. The five fingers per hand can create many different signs that can be translated into different text messages. The attached PIC 18F4620 microcontroller in the data glove receives the data associated with a given sign. These data are then diagnosed and translated by matching with the predefined data in the SD card data storage device by using deterministic matching model. When the data is matched, the matched predefined data will be sent to the microcontroller to display its text message on an on-board LCD module. The developed standalone translating system is flexible and adaptive as it can be reprogrammable according to the users that have special disabilities without any aid from a supercomputer.		Sandy Siu Ying Ko;Wei Lun Ng;Ng Chee Kyun;Nor Kamariah Noordin	2013		10.1007/978-3-319-02958-0_14	embedded system;speech recognition;computer hardware;computer science	Vision	-41.44434417502035	-43.61774925158017	176564
b132b00a6c687273bf4797f4cf4aa26b6c767777	"""embodied proactive human interface """"pico-2"""""""	human gesture;legged locomotion;tangible telecommunication device;monocular video camera;human interface;humanoid robots;video cameras;human friendly active interface;pico 2;natural gesture reproduction;humanoid type two legged robot;tracking technique;gesture recognition;embodied proactive human interface;legged robot;robot technology	"""We are conducting research on """"embodied proactive human interface"""". The aim of this research is to develop a new human-friendly active interface based on two key technologies, an estimation mechanism of human intention for supporting natural communication named """"proactive interface"""", and a tangible device using robot technology This paper introduces the humanoid-type two-legged robot named """"PICO-2 """", which was developed as a tangible telecommunication device for the proactive human interface. In order to achieve the embodied telecommunication with PICO-2, we propose new tracking technique of human gestures using a monocular video camera mounted on PICO-2, and natural gesture reproduction by PICO-2 which absorbs the difference of body structure between the user and the robot"""	computer;nsa product types;robot;robotics;usability;user interface	Ryo Kurazume;Hiroaki Omasa;Seiichi Uchida;Rin-ichiro Taniguchi;Tsutomu Hasegawa	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.488	computer vision;simulation;computer science;humanoid robot;gesture recognition;human interface device	Robotics	-42.38103418133112	-42.63537904310011	176967
2335688740303824e9a881fc4e42f459dec48a0c	deus em machina: on-touch contextual functionality for smart iot appliances	context sensing;smart home;iot;object sensing;internet of things;recognition;emi;smart appliances	"""Homes, offices and many other environments will be increasingly saturated with connected, computational appliances, forming the """"Internet of Things"""" (IoT). At present, most of these devices rely on mechanical inputs, webpages, or smartphone apps for control. However, as IoT devices proliferate, these existing interaction methods will become increasingly cumbersome. Will future smart-home owners have to scroll though pages of apps to select and dim their lights? We propose an approach where users simply tap a smartphone to an appliance to discover and rapidly utilize contextual functionality. To achieve this, our prototype smartphone recognizes physical contact with uninstrumented appliances, and summons appliance-specific interfaces. Our user study suggests high accuracy 98.8% recognition accuracy among 17 appliances. Finally, to underscore the immediate feasibility and utility of our system, we built twelve example applications, including six fully functional end-to-end demonstrations."""	end-to-end principle;home automation;internet of things;machina (company);mobile app;prototype;smartphone;usability testing	Robert Xiao;Gierad Laput;Yonghui Zhang;Chris Harrison	2017		10.1145/3025453.3025828	embedded system;human–computer interaction;computer science;operating system;internet privacy;world wide web;computer security;internet of things	HCI	-47.67553262029364	-41.333811836494476	177328
07aa5666b7ba4105ad0904870b603738e88c4fba	multi-touch everywhere!	musical interfaces;interactive music;multi touch interaction;computer vision	We present a portable device that enables users to turn any flat surface into a multi-touch controller for music and other media applications. The device is playable either with the hands, mallets or sticks. We also present a software editor for configuring the surface and creating control interfaces using a library of control elements, such as buttons, sliders, and pads.	mobile device;multi-touch	Alain Crevoisier;Greg Kellum	2009		10.1145/1597990.1598032	computer vision;human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-43.37998316863483	-39.851454141745464	177548
7c7e1e5098e3acd70515bef03bd92ff5fb5d84f7	the continuous interaction space: interaction techniques unifying touch and gesture on and above a digital surface	3d interaction;portable devices;interactive tabletops;touch;gestures;tangibles;surfaces;continuous interaction space	The rising popularity of digital table surfaces has spawned considerable interest in new interaction techniques. Most interactions fall into one of two modalities: 1) direct touch and multi-touch (by hand and by tangibles) directly on the surface, and 2) hand gestures above the surface. The limitation is that these two modalities ignore the rich interaction space between them. To move beyond this limitation, we first contribute a unification of these discrete interaction modalities called the continuous interaction space. The idea is that many interaction techniques can be developed that go beyond these two modalities, where they can leverage the space between them. That is, we believe that the underlying system should treat the space on and above the surface as a continuum, where a person can use touch, gestures, and tangibles anywhere in the space and naturally move between them. Our second contribution illustrates this, where we introduce a variety of interaction categories that exploit the space between these modalities. For example, with our Extended Continuous Gestures category, a person can start an interaction with a direct touch and drag, then naturally lift off the surface and continue their drag with a hand gesture over the surface. For each interaction category, we implement an example (or use prior work) that illustrates how that technique can be applied. In summary, our primary contribution is to broaden the design space of interaction techniques for digital surfaces, where we populate the continuous interaction space both with concepts and examples that emerge from considering this space as a continuum.	apache continuum;categorization;digital geometry;digital recording;interaction technique;iterative method;iterative refinement;multi-touch;population;refinement (computing);triune continuum paradigm;unification (computer science)	Nicolai Marquardt;Ricardo Jota;Saul Greenberg;Joaquim A. Jorge	2011		10.1007/978-3-642-23765-2_32	computer vision;simulation;human–computer interaction;computer science;artificial intelligence;natural user interface;surface;gesture	HCI	-46.221567025913664	-40.04177320303584	177581
66de0748fedd94a59117584bd35060603b775ada	replicating interactive surfaces using distortion techniques	mobile;interactive tabletop;tuio;poster;focus context	In this paper we present a distortion technique that provides a focus & context view of an interactive surface’s screen on a mobile device. Simply showing a reduced version of the surface’s screen on the mobile device would not have been sufficient as UI elements could be too small to be manipulated. Users modify the region of interest (ROI) of the focus & context distortion via gestural input on their device. We employ this technique in our system that transmits interaction on the mobile device via TUIO to the interactive surface. Thus, users may interact remotely with any TUIO-based application on the surface without additional implementation effort while real-time constraints are still met.	distortion;mobile device;real-time clock;region of interest;tangible user interface	Jooyoung Lee;Ralf Dörner;Johannes Luderschmidt;HyungSeok Kim;Jee-In Kim	2012			human–computer interaction;computer science;multimedia;computer graphics (images)	HCI	-44.02153599661515	-40.53194583029335	178116
986ce0dfb7a225832d27da29cedc7727910d58a3	a mounting foot-type force-sensing device for a desk with haptic sensing capability		This study deals with the development of a force-sensing device for converting a desk into an interface by mounting the device on the supports or legs of these objects. The system is composed of four force sensors with clamps, and a PC. Contact force and its position on a regular desk can be estimated from the response of the four force sensors fixed on the legs of the desk. Some application programs using this interface are introduced.	haptic technology	Toshiaki Tsuji;Tatsuki Seki;Sho Sakaino	2014		10.1007/978-4-431-55690-9_37	embedded system;human–computer interaction;computer hardware	HCI	-42.54325273410011	-41.69427287068082	178264
c2c686cdc5bc40ae83d9e338e04a2a86fe8514bd	stage- vs. channel-strip metaphor - comparing performance when adjusting volume and panning of a single channel in a stereo mix	stage metaphor;usability testing;channel strip;music production;interface;evaluation;multitouch;mixing;user interfaces	This study compares the stage metaphor and the channel strip metaphor in terms of performance. Traditionally, music mixing consoles employ a channels strip control metaphor for adjusting parameters such as volume and panning of each track. An alternative control metaphor, the so-called stage metaphor lets the user adjust volume and panning by positioning tracks relative to a virtual listening position. In this study test participants are given the task to adjust volume and panning of one channel (in mixes consisting of three channels) in order to replicate a series of simple pre-rendered mixes. They do this using (1) a small physical mixing controller and (2) using an iPad app, which implements a simple stage metaphor interface. We measure how accurately they are able to replicate mixes in terms of volume and panning and how fast they are at doing so. Results reveal that performance is surprisingly similar and thus we are not able to detect any significant difference in performance between the two interfaces. Qualitative data however, suggests that the stage metaphor is largely favoured for its intuitive interaction confirming earlier studies.	pre-rendering;self-replicating machine;sound card;ipad	Steven Gelineck;Dannie Korsgaard;Morten Büchert	2015			visual arts;simulation;human–computer interaction;computer science;artificial intelligence;evaluation;interface;multimedia;mixing;programming language;user interface	HCI	-44.6080305205402	-40.32788700295552	178656
90fd756eb8d6947f0139f66d0e50e98c5840959b	half-qwerty: a one-handed keyboard facilitating skill transfer from qwerty	input device;disabled users;human performance;one handed keyboard;input tasks;portable computers;qwerty;input devices;skill transfer	Half-QWERTY is a new one-handed typing technique, designed to facilitate the transfer of two-handed typing skill to the one-handed condition. It is performed on a standard keyboard, or a special half keyboard (with full-sized keys). In an experiment using touch typists, hunt-and-peck typing speeds were surpassed after 3-4 hours of practice. Subjects reached 50% of their two-handed typing speed after about 8 hours. After 10 hours, all subjects typed between 41 % and 73% of their two-handed speed, ranging from 23.8 to 42.8 wpm. These results are important in providing access to disabled users, and for the design of compact computers. They also bring into question p;evious re;earch cl~ming finger actions if one ‘hand map to the other via spatial congruence rather than mirror image. K E Y WO R D S: Input devices, input tasks, human performance, one-handed keyboard, QWERTY, portable computers, disabled users, skill transfer. INTRODUCTION The idea of a one-handed keyboard is not new. As early as 1968, Engelbart and English [2] used a one-handed chord keybomd in conjunction with a newly developed input device — the mouse. The user entered text with one hand, while using the mouse to enter spatial information with the other. However, unlike the mouse, acceptance of onehanded keyboards has been limited to very specific Permission to oopy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and tha title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ~ 1993 ACM 0-89791 -575-51931000410088 . ..S 1.50 applications, such as keyboards for the disabled. There are several reasons for this, but chief among them is the need to learn a new typing technique. For most people, the benefit of touch typing with one hand is not worth the cost of learning to do it. This paper describes a new approach to one-handed text entry which exploits the skills already developed in twohanded typing. It is called, “Half-QWERTY~ because it uses only half of the QWERTY keyboard. The technique can be used on an unmodified standard QWERTY keyboard (using only half of the available keys, Figure 1), or with a special half keyboard (Figures 2 & 3). The former provides wide access to the technique, The latter provides a compact keyboard with full-sized keys supporting touch typing on portable computers, for example. The present study examines the degree to which skill transfers from QWERTY to Half-QWERTY keyboards. THE HALF-QWERTY CONCEPT1 Most one-handed keyboards are chord keyboards. HalfQWERTY is not. The design builds on two principles: 1. A user’s abflity to touch type on a standard QWERTY keyboard. 2. The fact that the human hands are symmetrical — one hand is a mirror image of the other. A Half-QWERTY keyboard is comprised of all the keys typed by one hand, with the keys of the other hand unused lPatents pending. International Application # PCT/CA90/00274 published March 21, 1991, under International Publication # W091/03782.	chorded keyboard;computer keyboard;congruence of squares;human reliability;input device;portable computer;touch typing;words per minute	Edgar Matias;I. Scott MacKenzie;William Buxton	1993		10.1145/169059.169097	embedded system;simulation;computer hardware;computer science;operating system;input device	HCI	-48.113649404288886	-44.1314120349213	178755
1c7dd6777d4255ee7c9ad0a76a75bc404cbdb957	resistopalatography as an assistive technology for users with spinal cord injuries	sensors tongue computers force mice wheelchairs;wheelchairs force sensors handicapped aids injuries medical control systems neurophysiology pressure measurement pressure sensors user interfaces;tongue wheelchair interface assistive technology resistopalatography tongue computer interface;wheelchair joystick equivalent resistopalatography spinal cord injuries wheelchair control cursor movement force sensitive sensors dental retainer plate tongue pressure measurement assistive technology tongue computer interface tongue wheelchair interface mouse cursor equivalent	Translating a desired user's input using conventional methods such as a keyboard and mouse for the computer, or a joystick for a wheelchair is a major challenge faced by users whom have no limb control. This paper describes an iteration of Resistopalatography, a method using the tongue as a pointing device to emulate the use of the hand for cursor movement and wheelchair control. The system employs force sensitive sensors located within the mouth on a dental retainer plate to measure the tongue's pressure against the hard palate. The position and force of tongue against the sensors can be translated into mouse cursor or wheelchair joystick equivalents.	assistive technology;computer keyboard;computer mouse;cursor (databases);equivalent weight;game controller;hard palate;iteration;joystick device component;photographic plate;pointer (user interface);pointing device;retainer device component;spinal cord injuries;sensor (device)	Robert Horne;Steve Kelly;Paul Sharp	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319362	embedded system;engineering;biological engineering;audiology	Robotics	-41.54408023644312	-44.269053676145376	178841
c42f98eb6fb7b7af0408597a4e126e69f44fa314	an evaluation of input devices for use in the isee human-synthesizer interface			input device	Roed Vetegaal;Barry Eaglestone	1994			input device;electronic engineering;simulation;computer science	HCI	-43.035012412955254	-41.30167370370003	179251
cc9740e40d5e4821205b7605e66f5824c62e8777	birds on paper: an alternative interface to compose music by utilizing sketch drawing and mobile device	pencil drawing;tangible device;music composition;human computer interaction;mobile device;personal digital assistant;ease of use;signal processing;user experience;electric conductivity;interaction design	"""In this paper we describes a new concept of utilizing a mobile device or Personal Digital Assistant (PDA) for musical composition. We design a new interface that combines the ease of use of a pencil and the portability and customizability of mobile device. Our proposed kit involves the affordances provided by paper computing in order to provide user experiences to novice users. By effectively using the principle of electrical conductivity and signal processing, we have developed a functional prototype (""""Birds on Paper"""") that enables users to compose their own music. Our proposed kit consists of 4 main elements, i.e.: pencil, birds-shaped sensor, hub connector, and mobile device or PDA. Pencil can be applied on a piece of paper as the main medium to visualize the musical composition. Touching the graphite surface of the drawing will trigger an audio feedback in the form of musical notes. Musical notes will be generated based on the thickness and the length of the pencil drawings, thus enables users to intuitively compose the music according to their preference. In addition to the description of the kit, we also discuss the concept behind the design and possible user scenarios."""	audio feedback;bespoke;graphite;mobile device;personal digital assistant;prototype;scenario (computing);sensor hub;signal processing;thickness (graph theory);usb hub;usability	Chen-Wei Chiang;Shu-Chuan Chiu;Anak Agung Gede Dharma;Kiyoshi Tomimatsu	2012		10.1145/2148131.2148175	simulation;human–computer interaction;engineering;multimedia	HCI	-47.38073145340244	-39.34453325035948	179459
70650e70288fac03eb010239c8ac0807350a1d06	injecting life into toys	side channel information;smart grid;power monitoring;smartphone applications;crowdsourcing	"""This paper envisions a future in which smartphones can be inserted into toys, such as a teddy bear, to make them interactive to children. Our idea is to leverage the smartphones' sensors to sense children's gestures, cues, and reactions, and interact back through acoustics, vibration, and when possible, the smartphone display. This paper is an attempt to explore this vision, ponder on applications, and take the first steps towards addressing some of the challenges. Our limited measurements from actual kids indicate that each child is quite unique in his/her """"gesture vocabulary"""", motivating the need for personalized models. To learn these models, we employ signal processing-based approaches that first identify the presence of a gesture in a phone's sensor stream, and then learn its patterns for reliable classification. Our approach does not require manual supervision (i.e., the child is not asked to make any specific gesture); the phone detects and learns through observation and feedback. Our prototype, while far from a complete system, exhibits promise -- we now believe that an unsupervised sensing approach can enable new kinds of child-toy interactions."""	cloud computing;ecosystem;gesture recognition;interaction;mobile computing;personalization;prototype;real-time computing;sensor;signal processing;smartphone;toys;vocabulary	Songchun Fan;Hyojeong Shin;Romit Roy Choudhury	2014		10.1145/2565585.2565606	simulation;engineering;multimedia;communication	HCI	-47.42491933547281	-40.448623740678315	179890
887befc65c840a25db499868e5774be49ea1f6c0	object selection in virtual environments using an improved virtual pointer metaphor	selection metaphors;virtual reality;interaction techniques	In this paper we describe object selection techniques and metaphors for virtual environments (VEs). By combining and extending known techniques, we introduce an improved virtual pointer metaphor which enhances interactive object selection. The extension of the virtual pointer metaphor is based on a bendable ray, which is visualized by a quadratic beziér curve and points to the selectable object closest to the additionally visualized direction ray of the pointer. Strategies for determining this object are discussed and compared.	bézier curve;input device;pointer (computer programming);rollable display;usability testing;virtual reality	Frank Steinicke;Timo Ropinski;Klaus H. Hinrichs	2004		10.1007/1-4020-4179-9_46	computer vision;simulation;computer science;engineering drawing	Visualization	-42.55303123958012	-39.727876986881036	179928
c3b2adfb9e1180a6bffe48a637af57253b5d14f2	a generic direct-manipulation 3d-auditory environment for hierarchical navigation in non-visual interaction	hierarchical structure;auditory interface;auditory interfaces;direct manipulation;visual interaction;non visual interaction;re usability;interactive method;blind users;3d audio;toolkits	Auditory presentation methods may significantly enhance the interaction quality during user-computer dialogue. The impact of auditory interaction methods is important in the context of non-visual interaction, where audio is the primary direct pcrccption output modality. In a few cases, 3D-audio output techniques have been employed for providing interaction for blind users. Unfortunately, such developments have been too specialized and do not support re-usability of the implemented approaches and techniques in different contexts, where non-visual interaction needs to be realized. A generic re-usable environment has been implemented, based on 3D audio, 3D pointing, hand gestures and voice input, which is applicable in all cases that interactive hierarchically structured selections from sets of alternatives must be handled. This environment has been used to implement the hierarchical navigation dialogue in a multimedia non-visual toolkit currently under development. It is composed of a set of modules implementing re-usable functionality with which interaction for non-visual hierarchical navigation can be realized within any non-visual interaction toolkit.	direct manipulation interface;interaction design;modality (human–computer interaction);usability	Anthony Savidis;Constantine Stephanidis;Andreas Korte;Kai Crispien;Klaus Fellbaum	1996		10.1145/228347.228366	computer vision;human–computer interaction;computer science;multimedia	HCI	-48.247896181861805	-39.92622481262099	180164
43f820f1bdde4c2cacb283ed6f5977ab7c6a663a	smart phone interaction with registered displays	smart phones computer displays large screen displays switches smart cameras digital integrated circuits image registration registers mice wireless communication;sensors;user study;smart phone;data mining;markerless display registration scheme;three dimensional displays;mobile radio;pixel;lenses;smart phone interaction;optical sensors;smart phone interaction display registration;mobile computing;markerless display registration scheme smart phone;display registration;cameras;mobile radio mobile computing	This article describes the theory and practice of display registration with smart phones, findings in initial user studies, and opportunities for developing markerless display registration schemes.	smartphone	Nick Pears;Daniel Jackson;Patrick Olivier	2009	IEEE Pervasive Computing	10.1109/MPRV.2009.35	embedded system;computer vision;computer science;sensor;operating system;lens;multimedia;mobile computing;pixel	Visualization	-43.932372993917575	-41.43376937891628	180239
bdc5bc4ae9ea4b5482bd20cfc449cdbc734423ad	advanced graphics technology		As computing becomes more mobile, thereu0027s an increased need to develop more ad vanced input tools and methods. Screens are smaller, cameras are more ubiquitous, and touch technology is everywhere. Yet entering text, choosing graphics entities, performing drag-and drop, and so on are still difficult. This monthu0027s column describes new input modalities that add more options to researchersu0027 arsenals. Much of the technology I describe contains approaches first tried in desktop configurations. Mobility has ac celerated the pace of adoption and adds even more demands in terms of responsiveness, flexibility, and adaptability to an individuals.	computer graphics	David J. Kasik	2010	IEEE Computer Graphics and Applications	10.1109/MCG.2011.35	multimedia;computer vision;adaptability;artificial intelligence;gesture recognition;drag and drop;graphics;mobile computing;computer science;augmented reality;computer graphics;sixthsense	Visualization	-45.78257889365728	-41.21102822608945	180298
fc5d3744b15c5ecad03be4007435f5109b9472f1	hand waving in command spaces: a framework for operating home appliances		ABSTRACTIn this paper, hand waving in a ‘command space’ is proposed as a new framework for operating home appliances. A command space is associated with the operation of a home appliance; an operation is retrieved by waving a hand in the command space. A home appliance operation system with the function of multiplexing command space modules and integrating multiple commands is constructed. With the proposed framework and system, home appliance operation is possible only by hand waving, an intuitive gesture for a human. Experiments were conducted that verified that complex operations, such as scheduling the recording of a TV program, can be performed with the system. The usability of the constructed system is evaluated using Brookeu0027s System Usability Scale (SUS). The average SUS score was 66.8, which indicates that the subjects had relatively positive impression on the system in spite of long operation time.		Petri Tuohimäki;Takuya Kawamura;Hidetsugu Asano;Takeshi Nagayasu;Kazunori Umeda	2018	Advanced Robotics	10.1080/01691864.2018.1515661	gesture recognition;engineering;control engineering;impression;scheduling (computing);human–computer interaction;system usability scale;usability;human interface device;gesture;multiplexing	Robotics	-42.265645558978186	-43.86219975734152	180384
161b16c3d889c2d2265267ca0b26f933b894cb0a	interaction methods for smart glasses		Since the launch of Google Glass in 2014, smart glasses havemainly been designed to supportmicro-interactions. The ultimate goal for them to become an augmented reality interface has not yet been attained due to an encumbrance of controls. Augmented reality involves superimposing interactive computer graphics images onto physical objects in the real world. This survey reviews current research issues in the area of humancomputer interaction for smart glasses. The survey first studies the smart glasses available in the market and afterwards investigates the interaction methods proposed in the wide body of literature. The interaction methods can be classified into hand-held, touch, and touchless input. This paper mainly focuses on the touch and touchless input. Touch input can be further divided into on-device and on-body, while touchless input can be classified into hands-free and freehand. Next, we summarize the existing research efforts and trends, in which touch and touchless input are evaluated by a total of eight interaction goals. Finally, we discuss several key design challenges and the possibility of multi-modal input for smart glasses.	adobe freehand;augmented reality;categorization;computer graphics;glass;human–computer interaction;mobile device;modal logic;modality (human–computer interaction);smartglasses;touchscreen;user interface	Lik-Hang Lee;Pan Hui	2017	CoRR		computer science;human–computer interaction;distributed computing;augmented reality;wearable computer;encumbrance	HCI	-47.91821339051445	-41.487107256071326	180547
1502cc8acd0ee8207ba2c049981dcddb562be158	inkdraw: physical ink-based interface for capturing and manipulating drawings on digital display	interactive whiteboard;tangible user interface;visual thinking;physical ink;frustrated total internal reflection	This paper introduces a new drawing interface named iNkDraw for capturing ink marks on a digital display drawn using physical ink such as dry erase markers. By using ink-based FTIR, the interface directly detects the ink location on the drawing surface, thus any ink-oriented interactions are intrinsically supported -- e.g. controlling the thickness and faintness of lines by varying pen/brush movement, erasing the drawings by wiping the ink off with a cloth or even with a finger. Furthermore, iNkDraw can distinguish between drawing and touch, which allows users to digitally manipulate the captured drawings using pointers and menus. Several applications using the interface have been implemented. Limitations on using physical ink as an interfacing medium are also discussed.	display device;interaction;multi-touch;prototype;syncope (medicine);thickness (graph theory);usability	Kayato Sekiya;Shinpei Chihara	2010		10.1145/1935701.1935711	visual arts;engineering;multimedia;computer graphics (images)	HCI	-43.58855498887676	-39.97359879944418	180894
0aa74ca062dac811ca7774503973e2313e859c60	grabrics: a foldable two-dimensional textile input controller	public records;wearable;websearch;ubiquitous interface;fabric;electronic textile;foldable user interface;rwth publications;input controller	Textile interfaces can be ubiquitously integrated into the fabrics that already surround us. So far, existing interfaces transfer concepts, such as buttons and sliders, to the textile domain without leveraging the affordances and qualities of fabric. This paper presents Grabrics, a two-dimensional textile sensor that is manipulated by grabbing a fold and moving it between your fingers. Grabrics can be integrated invisibly into everyday clothing or into textile objects, like a living room sofa, while minimizing accidental activation. We describe the construction and the fold-based interaction technique of our Grabrics sensor. A preliminary study shows that Grabrics can be folded and manipulated from any arbitrary position, and it can detect 2D stroke gestures.	angularjs;apache axis;displacement mapping;error-tolerant design;image resolution;interaction technique;item unique identification;microcontroller;resultant;rollable display;touchscreen	Nur Al-huda Hamdan;Florian Heller;Chat Wacharamanotham;Jan Thar;Jan O. Borchers	2016		10.1145/2851581.2892529	public records;simulation;wearable computer;computer science	HCI	-42.9968299205881	-41.63804854035867	181722
d76c96a21b9ba80a535446ced5caf3e1776c2bd4	display navigation by an expert programmer: a preliminary model of memory	preliminary model;display navigation;expert programmer	Skilled programmers, working on natural tasks, navigate large information displays with apparent ease. We present a computational cognitive model suggesting how this navigation may be achieved. We trace the model on two related episodes of behavior. In the first, the user acquires information from the display. In the second, she recalls something about the first display and scrolls back to it. The episodes are separated by time and by intervening displays, suggesting that her navigation is mediated by long-term memory, as well as working memory and the display. In the first episode, the model automatically learns to recognize what it sees on the display. In the second episode, a chain of recollections, cued initially by the new display, leads the model to imagine what it might have seen earlier. The knowledge from the first episode recognizes this image, leading the model to scroll in search of the real thing. This model is a step in developing a psychology of skilled programmers working on their own tasks.	cognitive model;display device;programmer	Erik M. Altmann;Jill H. Larkin;Bonnie E. John	1995		10.1145/223904.223905	human–computer interaction;computer hardware;computer science;computer graphics (images)	HCI	-46.53716825366938	-41.62145556940543	181785
a63c772cf39beb696a609e2d6adae3e61c62c7d4	using heart rate to control an interactive game	heart rate interaction;real time;user study;interactive method;human interface;heart rate;unconventional computer human interfaces;physically interactive computer game;physical exercise;physical interaction;computer game	This paper presents a novel way of using real-time heart rate information to control a physically interactive biathlon (skiing and shooting) computer game. Instead of interfacing the game to an exercise bike or other equipment with speed output, the skiing speed is directly proportional to heart rate. You can freely choose the form of physical exercise, which makes it easier for people with different skill levels and backgrounds to play together. The system can be used with any exercise machine or form. To make playing meaningful instead of simply exercising as hard as you can, a high heart rate impedes the shooting part of the game by making the sight less steady. This balancing mechanism lets the player try out different tactics, varying from very slow skiing and sharp shooting to fast skiing and random shooting. The game has been evaluated in a user study with eight participants. The results show that heart rate interaction is fun and usable interaction method.	pc game;real-time transcription;usability testing	Ville Nenonen;Aleksi Lindblad;Ville Häkkinen;Toni Laitinen;Mikko Jouhtio;Perttu Hämäläinen	2007		10.1145/1240624.1240752	simulation;physical exercise;human–computer interaction;computer science;operating system;multimedia;human interface device	HCI	-45.32756588074867	-42.84846435937446	181833
f6f86bbfbf1ed66598a64400bc6e65eff62462d1	game systems by using a brain computer interface and mixed reality		We developed game systems combining an EEG (β/α ratio)-based brain-computer interface and mixed reality technology. We reported development and experimental evaluation of a shooting game in which users try to hit targets placed in the mixed reality space with the bullets operated by hand gesture and concentration. We also reported development of a fighting game in which the beta/alpha ratios decide offensive power in the game. In this study, we discussed the results of the experimental evaluation for the shooting game from the view point of cognitive science, and then we report experimental evaluation of the fighting game.		Ryota Horie;Koujiro Goto;Yuya Ootsuka	2018	2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2018.8574845	brain–computer interface;simulation;mixed reality;computer science;gesture	Visualization	-45.4443605979562	-44.11630416550733	181926
52cfad2ff7e3972e5b6c525b4e0e02aeba63cccb	scroll, tilt or move it: using mobile phones to continuously control pointers on large public displays	fatigue;user study;cursor control;mobile phone;large scale;public display;error rate;optical flow;input techniques mappings;accelerometers;target acquisition;interaction technique	Large and public displays mostly provide little interactivity due to technical constraints, making it difficult for people to capture interesting information or to influence the screen's content. Through the combination of largescale visual output and the mobile phone as an input device, bidirectional interaction with large public displays can be enabled. In this paper, we propose and compare three different interaction techniques (Scroll, Tilt and Move) for continuous control of a pointer located on a remote display using a mobile phone. Since each of these techniques seemed to have arguments for and against them, we conducted a comparative evaluation and discovered their specific strengths and weaknesses. We report the implementation of the techniques, their design and results of our user study. The experiment revealed that while Move and Tilt can be faster, they also introduce higher error rates for selection tasks.	input device;interaction technique;interactivity;mobile phone;pointer (computer programming);usability testing	Sebastian Boring;Marko Jurmu;Andreas Butz	2009		10.1145/1738826.1738853	embedded system;simulation;human–computer interaction;word error rate;computer science;operating system;optical flow;multimedia;accelerometer;interaction technique	HCI	-46.5945266082668	-44.36907825483424	182004
1f449cdb438c608cd612439faf2771a97629a19e	wearable capacitive-based wrist-worn gesture sensing system		Gesture control plays an increasingly significant role in modern human-machine interactions. This paper presents an innovative method of gesture recognition using flexible capacitive pressure sensor attached on user’s wrist towards computer vision and connecting senses on fingers. The method is based on the pressure variations around the wrist when the gesture changes. Flexible and ultrathin capacitive pressure sensors are deployed to capture the pressure variations. The embedding of sensors on a flexible substrate and obtain the relevant capacitance require a reliable approach based on a microcontroller to measure a small change of capacitive sensor. This paper is addressing these challenges, collect and process the measured capacitance values through a developed programming on LabVIEW to reconstruct the gesture on computer. Compared to the conventional approaches, the wrist-worn sensing method offerings a low-cost, lightweight and wearable prototype on the user’s body. The experimental result shows that the potentiality and benefits of this approach and confirms that accuracy and number of recognisable gestures can be improved by increasing number of sensor.	capacitive sensing;computer vision;gesture recognition;interaction;labview;microcontroller;polyethylene terephthalate;prototype;sensor;wearable computer	Xiangpeng Liang;Hadi Heidari;Ravinder Dahiya	2017	2017 New Generation of CAS (NGCAS)	10.1109/NGCAS.2017.80	gesture recognition;capacitive sensing;microcontroller;wearable computer;pressure sensor;gesture;capacitance;embedded system;engineering	HCI	-42.34467978443155	-41.729315435163365	182023
61f2f6c8a6eb918546a65eaabb4b3a6eec84b262	hairlytop interface: a basic tool for active interfacing	hairlytop interface;shape memory alloy;surface display;haptic;soft actuator;smart hair;smart material interface	The Hairlytop Interface is a high scalability interface composed of hair-like units called smart hairs. The original version of the smart hair comprised a shape-memory alloy, drive circuits, and a light sensor. Simply placing the smart hair above a light display device enabled each smart hair to be bent and controlled by modulating the intensity of light from the display. Various prototypes of the Hairlytop Interface have been created to show its high flexibility in configuration. This flexibility should help users to develop their own moving interfaces.	display device;modulation;scalability	Shuhei Umezu;Masaru Ohkubo;Yoshiharu Ooide;Takuya Nojima	2014		10.1145/2658779.2658793	embedded system;computer hardware;computer science;artificial intelligence;shape-memory alloy;haptic technology	HCI	-42.45466584243898	-41.22490817512832	183119
618ff528c8f0d7d5be1769a6ed1ca09c47a7d821	interaction with hand gesture for a back-projection wall	extra wiring;back-projection wall environment;hand gesture;interaction capability;pinching state;immersive display;hand gesture interaction systemfor;despitethe superior image quality;interaction device;back-projection wall;black light;white printing paper;satisfiability;geometric modeling;computer vision;computer graphics;virtual reality;machine vision;virtual environment;image quality;layout;gesture recognition	(Semi)immersive displays like back projection walls have become widely used in various visual domains. Despite the superior image quality of `through-the-windows VR' users are still not satisfied with the interaction capabilities - mostly due to the extra wiring for the interaction devices. This paper represents a hand gesture interaction system for a back-projection wall environment, supporting object manipulation (translation, rotation, and uniform scale) and selection tasks through the vision-tracking technique. We propose to use thimble-shaped fingertip markers made of white printing paper with a `black light' source. Gesture parameters are calculated by 3D positions of the marked fingertips and their pinching states	algorithm;computer vision;epipolar geometry;feature detection (computer vision);feature detection (web development);image quality;kalman filter;microsoft windows;motion estimation;printing;thresholding (image processing);tracking system;wiring	Hyosun Kim;Dieter W. Fellner	2004	Proceedings Computer Graphics International, 2004.	10.1109/CGI.2004.1309240	image quality;layout;computer vision;simulation;machine vision;computer science;virtual machine;geometric modeling;operating system;gesture recognition;virtual reality;computer graphics;object-oriented programming;gesture;satisfiability;computer graphics (images)	Visualization	-41.936918573712674	-39.57304301370504	183387
d4a64fc03a157d8a7a528773f198975a13e0f69e	valentine: an environment for home office worker providing informal communication and personal space	continuous media;awareness space;audio video;informal communication;virtual office;field of view;personal space;presence;concentration	In this paper, we propose the virtual ofice environment which integrates the natural communication and the secure private space. The features of this system are described below. 1) This system has virtual shared room which is based on the idea of 9hared Room Metaphor”. 30 graphics on SGI workstation is used-for this system. It uses Ethemet(TCP/IP) for signal connection and FDDI(UDP/IP) for continuous media(i.e., realtimd audio/video streams). 2) This system realizes the field of view of human being to support natural communication between members by using OUT UAround View” technique. 3) Yound Effect” are also used to help users feel the presence of other members. For instance, members hear the opening-sound of the door when someone logs in OUT system and the sound of footsteps when someone is wall&g around OUT viutual room. 4) Sometimes this system avoids unlimited flow of awareness. A person concentrating his/her work may not want to perceive ezcessive awareness of others. To support such situation, we define uAwareness Space” which restn’cts the field where other member5 awareness is transmitted. Awareness Space changes in size with the degree of concentration which is measured through two factors; the movement of a chair and the frequency of keyboard typing. 5) Weadphone Metaphor”; a picture of headphone is attached above his/her image and changed it’s color depending on the degree of concentration. The view of this headphone enables other members to recognize his/her state and it can be a criterion he is available to communicate or not.	color;desktop metaphor;graphics;headphones;phone connector (audio);streaming media;workstation	Shinkuro Honda;Hironari Tomioka;Takaaki Kimura;Takaharu Oosawa;Ken-ichi Okada;Yutaka Matsushita	1997		10.1145/266838.267356	simulation;field of view;computer science;personal space;multimedia;communication;concentration	HCI	-47.23456504294756	-38.73417190465634	183401
b7b89290d946ad0ad5c87f655537779c04534a7c	multi-modal natural dialogue	graphical user interface builders;dialogue sequencing;user interface management systems	Eye To analyze the user’s looking behavior wc usc a head mounted, corncal reflection eye tracker. The user looks through a half-silvered mirror; an infrared LED light shines frori above and lights up the eye. An infrared-sensitive camera picks up the reflection from the cyc off the mirror and sends the resulting TV signal to an AT 286 computer for image processing. The resulting eye data is analyzed intojixations, saccudes, and blinks,	cyc;eye tracking;image processing	Kristinn R. Thórisson;David B. Koons;Richard A. Bolt	1992		10.1145/142750.150714	user interface design;interface metaphor;human–computer interaction;natural language user interface;computer science;knowledge management;multimedia;natural user interface;user interface;multiple document interface	Graphics	-43.762067861846084	-40.958128354961666	183532
a132709a1d1b0bfd4435eb0860a527b83ba918ee	sammi: a spatially-aware multi-mobile interface for analytic map navigation tasks	spatial interaction;around device interaction;ubiquitous analytic interfaces;peephole interaction	Motivated by a rise in the variety and number of mobile devices that users carry, we investigate scenarios when operating these devices in a spatially interlinked manner can lead to interfaces that generate new advantages. Our exploration is focused on the design of SAMMI, a spatially-aware multi-device interface to assist with analytic map navigation tasks, where, in addition to browsing the workspace, the user has to make a decision based on the content embedded in the map. We focus primarily on the design space for spatially interlinking a smartphone with a smartwatch. As both smart devices are spatially tracked, the user can browse information by moving either device in the workspace. We identify several design factors for SAMMI and through a first study we explore how best to combine these for efficient map navigation. In a second study we compare SAMMI to the common Flick-&-Pinch gestures for an analytic map navigation task. Our results reveal that SAMMI is an efficient spatial navigation interface, and by means of an additional spatially tracked display, can facilitate quick information retrieval and comparisons. We finally demonstrate other potential use cases for SAMMI that extend beyond map navigation to facilitate interaction with spatial workspaces.	browsing;embedded system;information retrieval;mobile device;smart device;smartphone;smartwatch;spatial navigation;workspace	Khalad Hasan;David Ahlström;Pourang Irani	2015		10.1145/2785830.2785850	computer vision;simulation;human–computer interaction;computer science	HCI	-46.45658894053926	-42.74723124891374	183684
ed0cdb6a44db3bccee2d3f6db225931b82ad1a7b	gazelaser: a hands-free highlighting technique for presentations	remote pointing;gaze tracking;presentation aid	Presentation aids, such as the laser pointer, are commonly used in lectures and public speeches. Their effect on the audience has not been properly studied. We present an experiment that compares several pointer alternatives. One of them is GazeLaser, a new solution that does not need a manually operated pointer, but is based on the lecturers' gaze. It fares well in comparison, but comes second to the pointing tool available in PowerPoint. The experiment brings up issues that need to be taken into account when developing GazeLaser further.	pointer (computer programming)	Oleg Spakov;Harri Siirtola;Howell O. Istance;Kari-Jouko Räihä	2016		10.1145/2851581.2892504	simulation;computer science;multimedia	HCI	-47.970559887464795	-43.269810111180334	183906
e53fc46d7c19c731584234735a8e03df193fdf92	koto learning support method considering articulations		Playing the Koto requires various skills such as reading Koto scores and understanding string positions instantly. In addition, there are many articulations, and some of them have no information about the timing for switching fingers or pushing a string down in Koto scores. Therefore, learning to play the Koto is difficult. In this paper, we propose a method to support beginners in practicing the Koto considering articulations. The method directly presents information for effective Koto performance, such as the string positions color-coded by fingering, timing of picking, picking directions, and articulations to the strings and soundboard. An experimental system presents this information by using projection mapping. We evaluated its effectiveness for beginners and an experienced person by comparative experiments through three user studies. As a result of these studies, we found that beginners were able to learn the Koto more effectively than by the traditional method, but our system is not useful for experienced people.		Mayuka Doi;Homei Miyashita	2017		10.1007/978-3-319-76270-8_26	human–computer interaction;multimedia;computer science;projection mapping	SE	-47.74725076179181	-44.323253629007006	184301
4bf4e8b0435e5a6df994650fc77aaa7afe773337	extendedhand on wheelchair	wheelchair;virtual hand;communication;tablet	In this paper, we present a novel welfare system which utilizes a spatial augmented reality technique. Hand is a crucial component in human-human communication. For example, we can intuitively indicate an object or place by reaching and pointing it to nearby partners. Unfortunately, for wheelchair users, such communication is often limited because their reaching ranges are narrow, and moving their bodies to the target is tiresome. To solve this issue, we propose a novel wheelchair system on which a battery-powered mobile projector is mounted. A user manipulates the projected virtual hand as an extension of the real one using a touch panel equipped on an armrest of the wheelchair. We implement our proposed system and demonstrate the effectiveness.	armrest;augmented reality;reaching definition;touchscreen;video projector	Yuki Asai;Yuta Ueda;Ryuichi Enomoto;Daisuke Iwai;Kosuke Sato	2016		10.1145/2984751.2985738	embedded system;simulation	HCI	-44.13847596779849	-42.94984031874177	184344
44b6968f05863eb8b8c4ede91f91dc3f02e4ac8f	human-machine interface to control a robot with the nintendo wii remote	human machine interface	This paper intends to demonstrate the development of and easy-to-use local human-machine interface that would allow any user to control all kinds of service robots intuitively. This interface is based on the Nintedo Wii remote controller and consists of three operating modes: a steering wheel, an infra-red monitor and a movement identifier. These modes were tested on a cleaning robot and they led to very satisfactory results, proving that the Wiimote is an inexpensive and interesting way of making this kind of interfaces.	identifier;plasma cleaning;remote control;robot;steering wheel;user interface;wii remote plus	Daniel Coutinho;Armando Sousa;Luís Paulo Reis	2009			human–machine interface;embedded system;simulation;computer science;computer graphics (images)	Robotics	-42.75828903726338	-42.85568501065837	184863
53e5b37a47e020c30a82377a4509b7de0880ee49	haptic music exercises	haptic i o;music control;physical interaction design;computer music;input output devices;haptic feedback;interactive systems;force sensor;physical interaction	Pluck, ring, rub, bang, strike, and squeeze are all simple gestures used in controlling music. A single motor/encoder plus a force-sensor has proved to be a useful platform for experimenting with haptic feedback in controlling computer music. The surprise is that the “best” haptics (precise, stable) may not be the most “musical”. Author	bang file;encoder;experiment;haptic technology;pluck	Bill Verplank	2005			input/output;simulation;computer science;artificial intelligence;operating system;multimedia;computer music;haptic technology;programming language;force-sensing resistor	HCI	-41.976335307847805	-43.859717559846004	184992
120300da77b97c7192c8b8f6337b9251c8d926d2	a lightweight approach for augmented reality on camera phones using 2d images to simulate 3d	3d;three dimensions;mobile computer;interior design;motion tracking;computer vision;mobile phone;optical tracking;camera phone;augmented reality;mobile computing	We describe a lightweight augmented reality approach enabling arbitrarily complex virtual models to be operated even on low end camera phones. For optimal performance we apply simple motion tracking with manual initialization, and sequences of 2D images to give illusion of three dimensions. Our approach enables also augmenting of real objects, not just virtual ones. A prototype implementation of the solution exists for Series 60/Symbian OS mobile phones. Augmented interior design is presented as the main application example.	augmented reality;camera phone;mobile phone;prototype;simulation	Petri Honkamaa;Jani Jäppinen;Charles Woodward	2007		10.1145/1329469.1329490	three-dimensional space;computer vision;augmented reality;interior design;simulation;computer science;operating system;camera phone;mobile computing;computer graphics (images)	Visualization	-43.842953329823345	-40.86730312345227	185270
4f477c0d4caf12b1d2186563e52139168c5ff73d	augmented reality card game based on user-specific information control	entertainment computing;game design;card game;user specific information control;augmented reality	In this paper, we describe new way to use augmented reality (AR) for entertainment. AR is a technology that overlays virtual objects on real objects in real world as a natural and intuitive interface. We think AR also can provide multiple users with different sight in real world and control information displayed for each user. We use these AR features for entertainment to present another enjoyment for users. As an example of our proposal, we implemented a card game system and evaluated it to examine usefulness of our concept for entertainment.	augmented reality;multi-user;utility	Seiko Myojin;Arata Sato;Nobutaka Shimada	2012		10.1145/2393347.2396416	game design;augmented reality;simulation;human–computer interaction;computer science;artificial intelligence;mixed reality;multimedia	HCI	-45.55631527804535	-40.01685223981532	185578
29eaeaf6bf450aa52e7b6cef5fc660b1ac3dc034	tangicube: introducing bi-directional interactivity with a virtual agent in a tangible mixed reality environment	physical output modality;user study;tangible mixed reality;bi directional interaction;user experience;haptic feedback;mixed reality;virtual agent	TangiCube introduces bi-directional interactivity with a virtual agent in a tangible mixed reality environment. While the virtual agent interacts with the user, the user is able to react to the agent with the help of physical output modality. Likewise, the user can interact with the virtual agent and the agent behaves and reacts as if it has a physical embodiment. This two-way interaction process is mediated by a tangible hardware cube embedded with vibrator to generate haptic feedback and Bluetooth module to communicate with PC wirelessly. Results of a preliminary user study show that TangiCube improves user experience within the mixed reality environment.	bluetooth;embedded system;haptic technology;interactivity;mixed reality;modality (human–computer interaction);usability testing;user experience;vibrator (electronic)	Steven Zhiying Zhou;Derek S Tan;Dong Wei;Yuta Nakayama	2009		10.1145/1690388.1690477	computer-mediated reality;simulation;human–computer interaction;engineering;multimedia	HCI	-46.92686831843093	-39.441315782309864	186139
88d361d65656441808f7699918fb96677301fc50	multi-sensory browser and editor model	multimedia;human interface;computer accessibility;computer human interface;speech enabled application model;multi sensory user environments	This paper describes early results from an ongoing research project. Its purpose is to study the feasibility of full incorporation of bidirectional sound into a standard computer user5 application interface environment. The authors attempt to create highly independent, navigable, active objects that provide functions, via sound l/O, comparable to those found with a conventional visual-tactile interthce. Users should be able to seamlessly mix and match commands without regard to the I/O mechanism chosen. A prototype named the Voice Enabled Reading Assistant (VERA), which was written using this Aural-Oral User Interface (A-OUI) is described. VERA is used both to test the user interface and as a tool for generating ideas for improvements that will be incorporated in subsequent &eriments.	input/output;organic user interface;prototype	James W. Ryder;Kanad Ghose	1999		10.1145/298151.298418	human–computer interaction;computer science;operating system;multimedia;world wide web;human interface device	HCI	-47.799072682121846	-39.39230367263367	186247
362af0b16312adbdb5c056012eb7435968bc9af7	an object expression system using depth-maps	vanishing point;projection;mapping;difference image;augmented reality;depth map	The current of Augmented Reality are data gloves or markers used for interactions between object and background. But, this results in the inconvenience in use and reduced immersiveness. To reinforce immersiveness in AR, added input devices should be removed. To this end, spatial coordinates should be accurately perceived even when a marker has been attached. In this paper, an object expression system was proposed that uses depth-maps for interactions without any additional input devices in order to improve immersiveness. Immersiveness was improved by projecting obtained images on 2D spaces, extracting vanishing lines, calculating the virtual spatial coordinates of the projected images, and varying the sizes of the inserted objects in accordance with the sizes of the areas of virtual coordinates, based on the images projected on the 2D coordinates. By using this system, the use of 3D modelers could be excluded when 3D objects were created; thus, the efficiency of object creation could be improved.	augmented reality;input device;interaction;map;object lifetime	Jong-Chan Kim;Kyeong-Jin Ban;DaeHeon Park;YangSun Lee	2011	Multimedia Tools and Applications	10.1007/s11042-011-0955-2	computer vision;augmented reality;vanishing point;projection;depth map;computer graphics (images)	Visualization	-41.470789619940675	-39.460292025879276	186872
2ea6502a96f431911d59ab5901bf0791399329f2	the effect of focus cues on separation of information layers	3d;multilayer;depth cues;display;s3d	Our eyes use multiple cues to perceive depth. Current 3D displays do not support all depth cues humans can perceive. While they support binocular disparity and convergence, no commercially available 3D display supports focus cues. To use them requires accommodation, i.e. stretching the eye lens when focusing on an individual distance. Previous work proposed multilayer and light field displays that require the eye to accommodate. Such displays enable the user to focus on different depths and blur out content that is out of focus. Thereby, they might ease the separation of content displayed on different depth layers. In this paper we investigate the effect of focus cues by comparing 3D shutter glasses with a multilayer display. We show that recognizing content displayed on a multilayer display takes less time and results in fewer errors compared to shutter glasses. We further show that separating overlapping content on multilayer displays again takes less time, results in fewer errors, and is less demanding. Hence, we argue that multilayer displays are superior to standard 3D displays if layered 3D content is displayed, and they have the potential to extend the design space of standard GUI.	active shutter 3d system;binocular disparity;binocular vision;depth perception;digital media;gaussian blur;graphical user interface;graphics software;image editing;light field;map;microsoft windows;movie projector;parallax;stereo display;timeline	Patrick Bader;Niels Henze;Nora Broy;Katrin Wolf	2016		10.1145/2858036.2858312	computer vision;depth perception;computer science;3d computer graphics	HCI	-41.85213175939789	-39.14930920207026	187411
60e47b9ec8fcf3aee6d51dd7747ba2b8d6be7c7c	guiding visual search tasks using gaze-contingent auditory feedback	sonification;gaze contingent;visual search;guidance;auditory feedback;eye tracking	In many applications it is necessary to guide humans' visual attention towards certain points in the environment. This can be to highlight certain attractions in a touristic application for smart glasses, to signal important events to the driver of a car or to draw the attention of a user of a desktop system to an important message of the user interface. The question we are addressing here is: How can we guide visual attention if we are not able to do it visually? In the presented approach we use gaze-contingent auditory feedback (sonification) to guide visual attention and show that people are able to make use of this guidance to speed up visual search tasks significantly.	contingency (philosophy);desktop computer;smartglasses;sonification;user interface	Viktor Losing;Thies Pfeiffer;Lukas Rottkamp;Michael Zeunert	2014		10.1145/2638728.2641687	computer vision;sonification;visual search;human–computer interaction;eye tracking;computer science;multimedia	HCI	-47.430524482749036	-43.4166248512577	187426
ea1acbd1fc437c543a1012cec1e9a7eea0c72a93	z-touch: an infrastructure for 3d gesture interaction in the proximity of tabletop surfaces	multi touch table;interactive surface;gesture interaction;three dimensional;3d hand gesture interaction;multi touch;infrared;depth map;high speed;bezier curve	Sensing the depth (distance from the surface) of fingers/hands near a tabletop is very important. It allows us to use three-dimensional (3D) gesture interaction in multi-touch applications as we do in the real world. We introduce Z-touch, a multi-touch table that can sense the approximate postures of fingers or hands in the proximity of the tabletop's surface. Z-touch uses a vision-based posture sensing system. Multilayered infrared (IR) laser planes are synchronized with shutter signals from a high-speed camera, which captures each layer of the laser images. A depth map is obtained by using the captured image. Our prototype works at ~30 fps. Z-touch not only uses with the finger/hand contact points but also the angle of the hovering fingers. The interaction with the finger angles is unique and allows users to control multiple parameters by using a single finger. In this study, we introduce the principle of the method of finger detection and its applications (e.g., drawing, map zooming viewer, Bezier curve control).	3d computer graphics;approximation algorithm;bézier curve;depth map;movie projector;multi-touch;poor posture;prototype;traffic enforcement camera	Yoshiki Takeoka;Takashi Miyaki;Jun Rekimoto	2010		10.1145/1936652.1936668	computer vision;simulation;engineering;computer graphics (images)	HCI	-42.3856172368572	-40.6413690041294	187592
09a753d46159801c208d2434fd44ecf3490659cc	level-ups: motorized stilts that simulate stair steps in virtual reality	real walking;virtual reality;head mounted display	"""We present """"Level-Ups"""", computer-controlled stilts that allow virtual reality users to experience walking up and down steps. Each Level-Up unit is a self-contained device worn like a boot. Its main functional element is a vertical actuation mechanism mounted to the bottom of the boot that extends vertically. Unlike traditional solutions that are integrated with locomotion devices, Level-Ups allow users to walk around freely (""""real-walking""""). We present Level-Ups in a demo environment based on a head-mounted display, optical motion capture, and integrated with two different game engines. In a user study, participants rated the realism of stepping onto objects 6.0 out of 7.0 when wearing Level-Ups compared to 3.5 without."""	virtual reality	Dominik Schmidt;Robert Kovacs;Vikram Mehta;Udayan Umapathi;Sven Köhler;Lung-Pan Cheng;Patrick Baudisch	2015		10.1145/2702123.2702253	simulation;computer science;optical head-mounted display;virtual reality;multimedia;computer graphics (images)	Visualization	-42.918488888275206	-38.97771260046454	188754
d6052b92b1be86cdb8cdac88fe5a1762f7f12e29	ez ballot with multimodal inputs and outputs	multimodal input;multimodal output;gestural input;accessible voting	Current accessible voting machines require many voters with visual, cognitive and dexterity limitations to vote with assistance, if they can vote at all. To address accessibility problems, we developed the EZ Ballot. The linear layout of the EZ ballot structure fundamentally re-conceptualizes ballot design to provide the same simple and intuitive voting experience for all voters, regardless of ability or input/output (I/O) device used. Further, multimodal I/O interfaces were seamlessly integrated with the ballot structure to provide flexibility in accommodating voters with different abilities.	accessibility;input/output;multimodal interaction	Seunghyun Tina Lee;Xiao Xiong;Elaine Yilin Liu;Jon A. Sanford	2012		10.1145/2384916.2384960	simulation;computer security	HCI	-45.590170244931755	-39.47558313261338	188917
2e2609b93383e79618bda0fdbb1f0650be032583	"""development of a force and torque hybrid display """"gyrocubestick"""""""	torque;control systems;touch physiological;grounding;multimodal interface;reactive force grounding;virtual reality;wires;earthing;force feedback;navigation;gyrocubestick display;tactile sensation;human body;displays;next generation;tactile sensors;torque hybrid display device;arm;humans;haptic interfaces;resistance force;multimedia interface	Multimodal interfaces with the five senses attract a great deal of attention as for the next generation multimedia interface. Especially, the interfaces with additional tactile sensation and force feedback enable us to perform pointing and manipulation operations and navigation operations easily rather than the conventional interfaces without them. The display device using pin array and manipulation arms is the most popular to display tactile sensation and force feedback. The arms and wires restrict user's movement and reachable area around the device base connected to the arms. These devices are relative larger and are not convenient for mobile uses. Therefore, in this paper, a torque and force feedback device of non-grounding and no reaction base on the human body has been developed, which displays the torque of virtual objects and normal and tangential force on the surfaces as the resistance force and tactile sensation.	coat of arms;display device;haptic technology;multi media interface;multimodal interaction;next-generation network	Norio Nakamura;Yukio Fukui	2005	First Joint Eurohaptics Conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems. World Haptics Conference	10.1109/WHC.2005.37	ground;control engineering;computer vision;simulation;computer science;engineering;artificial intelligence;virtual reality;physics	HCI	-41.47620054753328	-43.468082108498905	189058
d85c573c09e39dc78b6f009af9a22ecb67d05c54	the attentive hearing aid: eye selection of auditory sources for hearing impaired users	input device;remote control;user needs;hearing impaired;hearing impairment;multi modal input;attentive user interface;assistive technology;input devices;eye tracking;hearing aid	An often-heard complaint about hearing aids is that their amplification of environmental noise makes it difficult for users to focus on one particular speaker. In this paper, we present a new prototype Attentive Hearing Aid (AHA) based on ViewPointer, a wearable calibration-free eye tracker. With AHA, users need only look at the person they are listening to, to amplify that voice in their hearing aid. We present a preliminary evaluation of the use of eye input by hearing impaired users for switching between simultaneous speakers. We compared eye input with manual source selection through pointing and remote control buttons. Results show eye input was 73% faster than selection by pointing and 58% faster than button selection. In terms of recall of the material presented, eye input performed 80% better than traditional hearing aids, 54% better than buttons, and 37% better than pointing. Participants rated eye input as highest in the “easiest”, “most natural”, and “best overall” categories.	amplifier;eye tracking;prototype;remote control;wearable computer	Jamie Hart;Dumitru Onceanu;Changuk Sohn;Doug Wightman;Roel Vertegaal	2009		10.1007/978-3-642-03655-2_4	speech recognition;computer science;operating system;input device	HCI	-43.483505561910015	-44.845926337612944	189100
67c108e8342c8a8d90ec9487724884a60dc4c492	free virtual navigation using motor imagery through an asynchronous braincomputer interface	brain computer interface;motor imagery;virtual environment	In this paper, an asynchronous braincomputer interface is presented that enables the control of a wheelchair in virtual environments using only one motor imagery task. The control is achieved through a graphical intentional control interface with three navigation commands (move forward, turn right, and turn left) which are displayed surrounding a circle. A bar is rotating in the center of the circle, so it points successively to the three possible commands. The user can, by motor imagery, extend this bar length to select the command at which the bar is pointing. Once a command is selected, the virtual wheelchair moves in a continuous way, so the user controls the length of the advance or the amplitude of the turns. Users can voluntarily switch from this interface to a noncontrol interface (and vice versa) when they do not want to generate any command. After performing a cue-based feedback training, three subjects carried out an experiment in which they had to navigate through the same fixed path to reach an objective. The results obtained support the viability of the system.	graphical user interface;the circle (file system);virtual reality	Francisco Velasco-Álvarez;Ricardo Ron-Angevin;Maria José Blanca-Mena	2010	PRESENCE: Teleoperators and Virtual Environments	10.1162/pres.19.1.71	brain–computer interface;computer vision;simulation;computer science;virtual machine;motor imagery	Visualization	-45.07886113995604	-44.43837258870417	189105
851fdbfa02827ff0e23ff4d0f68279a051be43f1	nested immersion: describing and classifying augmented virtual reality	notebook computers augmented reality automotive engineering cad;virtual reality;virtual environments;taxonomy context visualization virtual environments solid modeling augmented reality;visualization;virtual reality taxonomy nested immersion augmented virtual reality classification automotive design review use cases tracked tablet cave immersive virtual environment augmented reality taxonomy mixed reality taxonomy;solid modeling;taxonomy;h 5 3 information interfaces and presentation group and organization interfaces computer supported cooperative work h 5 1 information interfaces and presentation multimedia information systems artificial augmented and virtual realities h 5 2 information interfaces and presentation user interfaces input devices and strategies;augmented reality;context	We present a system, intended for automotive design review use cases, that incorporates a tracked tablet in a CAVE, where both the tablet and the CAVE provide different views and interaction possibilities within the same virtual scene. At its core, this idea is not novel. However, the literature reveals few examples of this paradigm in which virtual information is presented on a second physical device to augment an immersive virtual environment. Similarly, it is unclear where the system should be positioned within existing augmented/mixed/virtual reality taxonomies. We argue that interactions occur within a nesting of virtual and physical contexts, and that formalizing these relationships is important when attempting to understand perceptual issues. The goal of this paper is, thus, to describe the new system by proposing a scheme to formally identify sources of bias and then adapting an existing taxonomy to classify such systems.		William E. Marsh;Frédéric Mérienne	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223466	augmented reality;computer-mediated reality;artificial reality;simulation;visualization;human–computer interaction;computer science;artificial intelligence;instructional simulation;metaverse;virtual reality;mixed reality;multimedia;solid modeling;taxonomy;mechanical engineering	Visualization	-44.26711118746404	-38.28191254306906	190142
000cdd833772d47218dae8c7a7342db5c096be5e	non-obscuring binocular eye tracking for wide field-of-view head-mounted-displays	prototypes;lenses calibration rendering computer graphics estimation prototypes;estimation;computer graphics i 3 7 three dimensional graphics and realism virtual reality computer graphics c 3;lenses;special purpose and application based systems real time and embedded systems;vr application nonobscuring binocular eye tracking wide field of view head mounted display lens based head mounted display calibration pupil tracking realtime eye tracking gaze estimation relative gaze direction virtual reality applications 3d printed prototype low cost hmd;rendering computer graphics;calibration;special purpose and application based systems real time and embedded systems computer graphics i 3 7 three dimensional graphics and realism virtual reality computer graphics c 3;virtual reality calibration gaze tracking helmet mounted displays lenses	We present a complete hardware and software solution for integrating binocular eye tracking into current state-of-the-art lens-based Head-mounted Displays (HMDs) without affecting the user's wide field-of-view off the display. The system uses robust and efficient new algorithms for calibration and pupil tracking and allows realtime eye tracking and gaze estimation. Estimating the relative gaze direction of the user opens the door to a much wider spectrum of virtual reality applications and games when using HMDs. We show a 3d-printed prototype of a low-cost HMD with eye tracking that is simple to fabricate and discuss a variety of VR applications utilizing gaze estimation.	algorithm;binocular vision;eye tracking;head-mounted display;printing;prototype;virtual reality	Michael Stengel;Steve Grogorick;Martin Eisemann;Elmar Eisemann;Marcus A. Magnor	2015	2015 IEEE Virtual Reality (VR)	10.1109/VR.2015.7223443	computer vision;estimation;calibration;simulation;computer science;real-time computer graphics;lens;prototype;statistics;3d computer graphics;computer graphics (images)	Visualization	-41.981199160664595	-39.6746140544531	190204
ac2ed58ab4f5170559a2fce5d70d20ddc723d738	exploring the design of a wearable device to turn everyday objects into playful experiences	play;multi sensory interfaces;interactive system;children;ubiquitous computing;wearable device;intuitive interfaces;smart environments;physical interaction	In this paper we present a wearable device in the form of a bracelet that turns everyday objects into interactive physical gameplay. We combine physical exploration and interactive entertainment by providing real-time audio and light feedback without the need to be in front of a screen. In contrast with today's computer, video and smartphone games, our system has the potential to enhance children's physical, social and outdoor play. We designed a set of playful applications that seamlessly integrate technology with outdoor game play, music, sports and social interactions.	interaction;real-time clock;smartphone;video;wearable technology	Judith Amores;Xavier Benavides;Roger Boldu;Pattie Maes	2015		10.1145/2702613.2732885	games;simulation;human–computer interaction;computer science;multimedia;ubiquitous computing	HCI	-46.76178253705046	-39.90076038761266	190800
37649ce9965db970aec792dc2952c17ff35070e3	real-time image based lighting for 360-degree panoramic video	text;image based lighting;mr;ar;ibl;augmented reality;mixed reality;360 video	The application of the newly popular content medium of 360 degree panoramic video to the widely used offline lighting technique of image based lighting is explored, and a system solution for real-time image based lighting of virtual objects using only the provided 360 degree video for lighting is developed. The system solution is suitable for use on live streaming video input, and is shown to run on consumer grade graphics hardware at the high resolutions and framerates necessary for comfortable viewing on head mounted displays, rendering at over 60 frames per second for stereo output at 1182x1464 per eye on a mid-range graphics card. Its use in several real-world applications is also studied, and extension to consider real-time shadowing and reflection is explored.		Thomas Iorns;Taehyun Rhee	2015		10.1007/978-3-319-30285-0_12	computer vision;augmented reality;computer science;mixed reality;multimedia;image-based lighting;computer graphics (images)	Graphics	-41.715961160150385	-38.59945153158032	190862
16595d321b257dd3c28bff95bdd3e42d6254aeca	human sensing using visible light communication	sensing;skeleton reconstruction;visible light communication	We present LiSense, the first-of-its-kind system that enables both data communication and fine-grained, real-time human skeleton reconstruction using Visible Light Communication (VLC). LiSense uses shadows created by the human body from blocked light and reconstructs 3D human skeleton postures in real time. We overcome two key challenges to realize shadow-based human sensing. First, multiple lights on the ceiling lead to diminished and complex shadow patterns on the floor. We design light beacons enabled by VLC to separate light rays from different light sources and recover the shadow pattern cast by each individual light. Second, we design an efficient inference algorithm to reconstruct user postures using 2D shadow information with a limited resolution collected by photodiodes embedded in the floor. We build a 3 m x 3 m LiSense testbed using off-the-shelf LEDs and photodiodes. Experiments show that LiSense reconstructs the 3D user skeleton at 60 Hz in real time with 10 degrees mean angular error for five body joints.	algorithm;angularjs;embedded system;ray (optics);real-time clock;testbed;vlc media player	Tianxing Li;Chuankai An;Zhao Tian;Andrew T. Campbell;Xia Zhou	2015		10.1145/2789168.2790110	telecommunications;computer science;visible light communication	Mobile	-42.36916498447992	-42.37099468043918	190940
fb021eddf8fe64dd09346673115a2685d22d4992	robotic border crosser tng - creating an interactive mixed reality		In this paper is described an interactive mixed reality which is pre- sented by a mobile robot. It explains the structure and functionality of the mixed reality and illustrated, how the combination works with the robot. In ad- dition some evaluation results of the interactive screen are presented. Usage scenario for the interactive mixed reality is the Industriemuseum Chemnitz. This kind of exhibition is suitable for viewing the inner functions of an exhibit to see how this technology works. The view into a technical device can occurs with the help of a public screen which is projected on the exhibits surface. Via an interactive layer it's possible for users to interact with the indicated contents. This interactive projection system is mobile thereby the robot can transport the public screen through the museum and from exhibit to exhibit. This interactive screen contains videos, animations and pictures of the functionality of exhibits. With the assistance of this mobile system the visitor can learn more about the exhibits in general and their specific functionality.		Anke Tallig	2014		10.1007/978-3-319-07230-2_66	simulation;human–computer interaction;multimedia;computer graphics (images)	HCI	-44.84204302973322	-39.15245747688664	191231
c896cf44dbcfa81afc7e2decb1cbc7a5b2c7ccdb	haptic in-vehicle gesture controls		Recent efforts have proposed the use of hand gestures to control in-vehicle infotainment systems (IVISs) in an attempt to reduce visual demand and therefore reduce crash risk. These efforts however lack tactile feedback resulting in a decreased sense of control over the user's intended actions. Here, we describe a demo that uses commercially available novel technologies which mitigate this problem by using focused ultrasound to accurately deliver a radiation force onto the user's operating hand. This new paradigm of mid-air haptic feedback presents new opportunities and challenges towards designing effective interaction languages for IVISs. In this paper, we describe our demo prototype and how it addresses some of these challenges.	haptic technology;programming paradigm;prototype	Orestis Georgiou;Valerio Biscione;Adam Harwood;Daniel Griffiths;Marcello Giordano;Benjamin Long;Tom Carter	2017		10.1145/3131726.3132045	human–computer interaction;simulation;engineering;haptic technology;radiation;gesture;crash	HCI	-46.728153274299856	-43.83111818367644	191575
524dc446d11715bf8140ab2bb5725ccedb7ce5f5	reconfiguring and fabricating special-purpose tangible controls	end user fabrication;deformable devices;actuator mechanisms;tangible interfaces	Unlike regular interfaces on touch screens or desktop computers, tangible user interfaces allow for more physically rich interactions that better uses the capacity of our motor system. On the flipside, the physicality of tangibles comes with rigidity. This makes it hard to (1) use tangibles on systems that require a variety of controls and interaction styles, and (2) make changes to physical interfaces once manufactured. In my research, I explore techniques that allow users to reconfigure and fabricate tangible interfaces in order to mitigate these issues.	desktop computer;interaction;tangible user interface;touchscreen	Raf Ramakers	2015		10.1145/2815585.2815587	embedded system;simulation;human–computer interaction	HCI	-45.091855603795736	-39.34737434715442	192177
ff3ae28edd409466746779ebed07d914146277e2	selection-based mid-air text entry on large displays		Most text entry methods require users to have physical devices within reach. In many contexts of use, such as around large displays where users need to move freely, device-dependent methods are ill suited. We explore how selection-based text entry methods may be adapted for use in mid-air. Initially, we analyze the design space for text entry in mid-air, focusing on singlecharacter input with one hand. We propose three text entry methods: H4 MidAir (an adaptation of a game controller-based method by MacKenzie et al. [21]), MultiTap (a mid-air variant of a mobile phone text entry method), and Projected QWERTY (a mid-air variant of the QWERTY keyboard). After six sessions, participants reached an average of 13.2 words per minute (WPM) with the most successful method, Projected QWERTY. Users rated this method highest on satisfaction and it resulted in the least physical movement.	game controller;mobile phone;multitap;words per minute	Anders Markussen;Mikkel Rønne Jakobsen;Kasper Hornbæk	2013		10.1007/978-3-642-40483-2_28	huffman coding;control theory;multimedia;mobile phone;computer hardware;computer science;words per minute	HCI	-46.98175955460206	-44.94476546859824	192282
14e1f236383bd61ae5a8908c2dffe358a91b7d86	using magnetic forces to convey state information: an exploration of a haptic technology	two state;user study;interactive computer systems haptics;state information;remote sites;haptics;simulating flows;conference paper;magnetic force;touch interaction;keywords haptic technology;user interaction	Using magnetic forces to provide haptic information is an area that remains largely unexplored and provides exciting opportunities for interaction that may not otherwise be possible. We present several example applications that illustrate how this technology is well suited to remote site collaboration techniques, conveying state information and simulating flows and turbulence. Furthermore, we present the results of our preliminary user study which has indicated that magnetic forces can convey two states of information to the user with 100% accuracy, and can convey three states with 89% accuracy.	haptic technology;simulation;turbulence;usability testing	Jessica Tsimeris;Tamás D. Gedeon;Michael Broughton	2012		10.1145/2414536.2414630	simulation;magnetic field;human–computer interaction;computer science;multimedia;haptic technology	HCI	-46.06439289180249	-43.13148794730435	192631
7109bc1ef956ca7db19ed7c9b7420f2551fadb03	unraveling natural interfaces for co-located groupware: lessons learned in an experiment with music	human computer interaction;time sharing;multi user;indexing terms;musical instruments;computer vision;collaborative environment;lessons learned;co located collaboration;gesture recognition;human computer interface;co located groupware	The computer is an ubiquitous element of modern society, nonetheless, human computer interaction is still rather inflexible. Particularly in local collaborative environments, like office meetings, the property that the mouse and keyboard exhibit of being a gateway for the individual to act upon a workspace, has barred the way to the production of co-located collaboration technologies, because the users have to time-share their actions upon the workspace. Despite recent developments in touch sensitive multi-user tabletops, we still believe the portability, low cost and potentially large input area of vision sensors presents the most promising approach to unravel natural human computer interfaces. We present in this paper our design of a computationally inexpensive vision based interface that allows multiple users to interact simultaneously with a single computer by performing hand gestures, which are filmed by a static video camera. This interface attempts to continuously recognize predefined postures and movements using a view-dependent method. We also present A.C.O, a co-located groupware application that receives input from the vision-based interface and allows users around a table to collaborate playing synthesized music instruments by moving their hands. This prototype gave us important hints on the more immediate obstacles the technology must overcome.	collaborative software;human computer;human–computer interaction;image sensor;multi-user;prototype;software portability;touchscreen;workspace	João Carreira;Paulo Peixoto	2006	Journal of Multimedia	10.4304/jmm.1.5.18-24	computer vision;simulation;index term;human–computer interaction;computer science;operating system;gesture recognition;multimedia;world wide web;time-sharing	HCI	-45.64968310709443	-39.69976167230838	193018
0a5fedf1c1cbb26eefa5c8a0edc12af1f1edda60	elastic connections: separating and observation methods for complex virtual objects	3d user interface vision based ui high speed camera	Today's technology enables users to manipulate complex, multi-part 3D virtual objects such as industrial products, structures designed by CAD, and models of the human body in large 3D space. In general modeling software, parts of such a complex 3D virtual objects are grouped and manipulated together, but not individually, for efficient operation. Therefore, a separating operation is necessary when the user wants to observe or manipulate only a part of a complex object. We have developed a system that realizes gesture-based separation and observation of a group of parts within complex virtual objects in 3D space. One of the practical applications of our system is in training, such as learning the structure of the human body or industrial products. In our system, users can separate a group of parts freely by pulling and cutting a virtual rubber band. The widths of these virtual rubber bands are relative to the connection strength between each part. This allows the user to easily understand relationships between parts. Additionally, this feedback provides a comfortable operational familiarity.	computer-aided design	Mai Otsuki;Tsutomu Oshita;Asako Kimura;Fumihisa Shibata;Hideyuki Tamura	2012		10.1145/2407336.2407374	computer vision;simulation;computer science;kernel virtual address space	Graphics	-42.99606471204389	-39.54318473393414	193502
7a12aa3cc20257e5bcd41b860d537cb025283556	optical free-form surfaces in off-axis head-worn display design	optical design techniques;mirrors;optical glass;optics;free form;form factor;g 1 10 numerical analysis applications;input output;opto mechanical packaging optical free form surface lightweight off axis head worn display design mobile augmented reality single element eyeglass display design dual element eyeglass display design free form optics fabrication technology fabricated mirror prototype;numerical analysis;radial basis function;optical glass augmented reality helmet mounted displays mirrors optical design techniques optical fabrication;b 4 2 hardware input output devices;field of view;optical fabrication;optical system design;g 1 10 numerical analysis applications optics optical system design free form radial basis functions b 4 2 hardware input output devices;augmented reality;mobile augmented reality;helmet mounted displays;radial basis functions	Head-worn displays are becoming a viable visual display option for mobile augmented reality. In this paper, we highlight the challenges and some recent progress towards compact and lightweight head-worn displays tending towards the eyeglass form factor. Single and dual-element eyeglass display designs that leverage the advances in free-form optics fabrication technology will be presented. There are three new contributions in this paper: first, we present a single-element eyeglass display design and the fabricated mirror prototype; second, we present our second generation dual-element eyeglass display design with the revised opto-mechanical packaging; third, we present our initial studies on the field of view limit with an 8 mm pupil for the dual-element design.	augmented reality;human–computer interaction;ibm notes;internet information services;optic axis of a crystal;prototype;radial (radio);radial basis function;second generation multiplex plus	Ozan Cakmakci;Sophie Vo;Simon Vogl;Rupert Spindelbalker;Alois Ferscha;Jannick P. Rolland	2008	2008 7th IEEE/ACM International Symposium on Mixed and Augmented Reality	10.1109/ISMAR.2008.4637320	augmented reality;radial basis function;simulation;computer science	HCI	-44.895634645503385	-38.19915040827287	193709
9af14ba3a11660bd128a9ef56a4905a6ad11314e	a multiple sensor-based shoe-mounted user interface designed for navigation systems for the visually impaired	computerised navigation;handicapped aids;human computer interaction;infrared detectors;optical sensors;user interfaces;infrared-based proximity detection;multiple sensor-based shoe-mounted user interface;navigation systems;shoe-mounted asssitive device;visually impaired;wireless data communication setup;infrared-based proximity detection;navigation system for visually impaired;shoe-mounted asssitive device	Assistive technology and assistive devices have been researched and developed to promote an independent daily life for visually impaired individuals. Navigation for the visually impaired may be one of the most challenging tasks due to high expectations on both the functionality and the mechanical specifications of the devices. This research aims at developing a multiple sensor-based hand-free device as a navigation aid to detect obstacles. The developed device incorporates a sensor unit and a feedback unit with either a wired or a wireless data communication setup. The working principle and the design criteria are identified with several working scenarios for potential applications. The implementation of the design into a prototype device is discussed in details. The prototype has been tested, and recommendations of possible improvements and future works are proposed.	assistive technology;prototype;sensor;user interface	J. Zhang;Chon Wai Lip;Soh-Khim Ong;Andrew Y. C. Nee	2010	2010 The 5th Annual ICST Wireless Internet Conference (WICON)		user interface design;computer vision;navigation;simulation;infrared;human–computer interaction;computer science;information security;cognitive neuroscience of visual object recognition;user interface	HCI	-43.1718019135812	-43.7892698108457	193820
b9255e0585a5a1d140a75881b214666c6fdaa295	tactile displays: overview and recent advances	onde surface;interfaz grafica;new technology;affichage graphique;articulo sintesis;ultrason;mems;affichage tactile;electric stimulation;graphical interface;user interface;article synthese;ultrasound;implementation;visualizacion tactil;manufacturing process;tangible interface;onde acoustique;graphic display;interface ordinateur;computer graphic;etat actuel;microactuators;surface acoustic wave;tactation;atmosfera controlada;surface wave;general population;ultrasonido;microactionneur;microelectromechanical device;procedimiento fabricacion;tactile communication;tactile display;state of the art;dispositif microelectromecanique;acoustic wave;visualizacion grafica;estado actual;onda superficie;procede fabrication;implementacion;computer interfaces;review;dispositivo microelectromecanico;interface graphique;tactile displays;controlled atmosphere;atmosphere controlee;focused ultrasound;onda acustica	Tactation is the sensation perceived by the sense of touch, and is based on the skin’s receptors. Touch is a common medium used by the general population and the sensory impaired. Tactile substitution can be used by the blind or deaf in order to: (a) enhance access to computer graphical user interfaces and (b) enhance mobility in controlled environments. The skin nerves can be stimulated through six types of receptors by mechanical, electrical, or thermal stimuli. Modalities, such as vibration and pressure, can stimulate these receptors. Advances in tactile communication using implementations of the actuating devices have been developed via several new technologies. These technologies include static or vibrating pins, focused ultrasound, electrical stimulation, surface acoustic waves, and other. This paper is a review of the state-of-the-art in the physiological and technological principles, considerations and characteristics, as well as latest implementations of microactuator-based tactile graphic displays. We also review fabrication technologies, in order to demonstrate the potential and limitations in tactile applications. 2007 Elsevier B.V. All rights reserved.	acoustic cryptanalysis;emoticon;functional electrical stimulation;graphical user interface;population;tactile graphic	Vasilios G. Chouvardas;Amalia N. Miliou;Miltiadis K. Hatalis	2008	Displays	10.1016/j.displa.2007.07.003	acoustic wave;electronic engineering;surface wave;computer science;engineering;electrical engineering;surface acoustic wave;controlled atmosphere;ultrasound;graphical user interface;microelectromechanical systems;user interface;implementation;tactile sensor;physics	HCI	-41.08569235168043	-41.94999670946274	194009
217e647ec1dcd7cc57b7abf43cc0121934aa4f28	milkey: multi illuminated indicator for keypad	notch filter;keywords display;keypad;input mode	An indicator with switchable faces that suits the cellular phone keypad is proposed and implemented. The prototype, which can show 3 patterns, consists of notch filters (each of which yields a different pattern) and light sources. Each pattern, which emits in a different color, can be activated simply by switching the same color of the illuminating light. Since the patterns do not disturb each other, large characters can be covered on the whole keytop. This indicator is also suitable for handheld/portable devices, such as PDA or notebook PC.	color;handheld game console;laptop;mobile phone;norm (social);personal digital assistant;prototype	Hiroyuki Manabe;Masaaki Fukumoto	2007		10.1145/1377999.1378026	embedded system;computer hardware;computer science;band-stop filter;computer graphics (images)	Vision	-41.08332009701115	-42.484355820981534	194356
de520e0b49541b522c9395bf81ac2536a5b5a920	analysis of the user experience in a 3d gesture-based supported mobile vr game		The work presented in this paper, explored the enhancement of User Experience (UX) by introducing a novel gesture-based controller in a mobile multiplayer Virtual Reality (VR) game. Using only the smartphone's RGB camera, the image input was used for both gesture analysis, capable of understanding user actions, as well as segmenting the real hand that was illustrated in the Virtual Environment (VE). Users were also able to share the VR space by cooperating in a survival-strategy scenario. The results from the user studies indicated that both the bare hand controller and the addition of another player in the VR scene, affected the experience for the participants. Users had a stronger feeling of presence in the VE when participated with an other user, and the visual representation of their hand in the VR world made the interactions seem more natural. Even though, there is still a number of limitations, this project nodes this approach capable of offering a natural and engaging solution of VR interaction, capable of rich UX while maintaining a low entry level for the end users.	a/ux;interaction;smartphone;usability testing;user experience;virtual reality	Abraham Georgiadis;Shahrouz Yousefi	2017		10.1145/3139131.3141224	end user;simulation;multimedia;virtual reality;human–computer interaction;user experience design;computer science;feeling;control theory;virtual machine;rgb color model;gesture	HCI	-45.50134814072826	-42.10690928573906	194719
6a6a16867af318a375d4ad89b15de2a8321ec5d7	a japanese input method for mobile terminals using surface emg signals	input method;text input;surface electromyogram;human interface;new generation interfaces;electromyogram;mobile terminal	The common use of mobile terminals is for text input. However, mobile terminals cannot be equipped with sufficient amount of keys because of the physical restrictions. To solve this problem we developed an input method using surface electromyogram (sEMG), treating arm muscle movements as input signals. This method involves no physical keys and can be used to input Japanese texts. In our experiments, the system was capable of inputting Japanese characters with a finger motion recognition rate of approximately 80%.	dictionary;electromyography;experiment;input method;japanese input methods;mouse button;signal processing	Akira Hatano;Kenji Araki;Masafumi Matsuhara	2008		10.1007/978-3-642-00609-8_2	simulation;speech recognition;engineering;communication	HCI	-43.51144479769043	-43.73846287008418	195037
4d718f7c94d3be1f81e9d4368aa53b23d9e898ef	a study on pseudo-haptics by cursor moving with motion blur	image motion analysis;image restoration;motion blur pseudo haptics;motion blur;pseudo haptics;mouse controllers computers;haptic interfaces;mouse controllers computers haptic interfaces image motion analysis image restoration;mouse pseudo haptics pointing device cursor moving motion blur illusion haptics sensation;resistance haptic interfaces electrical resistance measurement mathematical model equations immune system gravity	Pseudo haptics is illusion haptics sensation by modification of motion of cursor with pointing device such as mouse manipulated by hand. We inform the measurements which are pseudo haptics effect of pointing device cursor moving with motion blur.	cursor (databases);gaussian blur;haptic technology;pointing device	Masahiro Fukushima;Tsuyoshi Takemoto;Makoto Fujimura	2013	2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2013.135	computer vision;computer science;multimedia;computer graphics (images)	Robotics	-41.34135388087543	-40.80116705764001	195110
67f35b16000810314ca3b2851dbb58a90d59bcc5	designing to capture and share life experiences for persons with aphasia	aphasia;swinburne;capturing;life logging;prototyping	In this paper we present the design of an image capturing device for persons with aphasia. The early concepts were validated with one speech therapist and the usability of the camera was tested with one aphasic person. Our semi-autonomous hand-held camera, tapered to meet specific interaction and ergonomic needs of expressive aphasics. This camera is able to capture daily experiences by creating photographs that receives significant, automatically added tags. We present the design case study with early evaluation results by proxy users.	autonomous robot;human factors and ergonomics;mobile device;proxy server;semiconductor industry;usability	Abdullah Al Mahmud;Jeffrey L Braun;Jean-Bernard Martens	2010		10.1145/1851600.1851680	simulation;computer science;prototype;multimedia;lifelog	HCI	-45.40420890672117	-42.57934518478435	195137
22e48c9c53deafefe2872b398a630d4ce4a59079	virtual projection: exploring optical projection as a metaphor for multi-device interaction	mobile device;real estate;perspective projection;user study;feature tracking;handheld projection;handheld device;large displays;use case;interaction technique	Handheld optical projectors provide a simple way to overcome the limited screen real-estate on mobile devices. We present virtual projection (VP), an interaction metaphor inspired by how we intuitively control the position, size, and orientation of a handheld optical projector's image. VP is based on tracking a handheld device without an optical projector and allows selecting a target display on which to position, scale, and orient an item in a single gesture. By relaxing the optical projection metaphor, we can deviate from modeling perspective projection, for example, to constrain scale or orientation, create multiple copies, or offset the image. VP also supports dynamic filtering based on the projection frustum, creating overview and detail applications, and selecting portions of a larger display for zooming and panning. We show exemplary use cases implemented using our optical feature-tracking framework and present the results of a user study demonstrating the effectiveness of VP in complex interactions with large displays.	3d projection;frustum;handheld game console;interaction;map projection;mobile device;modeling perspective;motion estimation;movie projector;usability testing;video projector	Dominikus Baur;Sebastian Boring;Steven K. Feiner	2012		10.1145/2207676.2208297	computer vision;simulation;human–computer interaction;computer science;operating system;mobile device;computer graphics (images)	HCI	-44.19556894731598	-40.31418926530555	195312
777efbeeb0f109b14eab2ceaa863b29c7beef492	coin size wireless sensor interface for interaction with remote displays	wireless sensor;input device;human computer interaction;interaction with large screen display;user interface;large screen display;non verbal communication	Human gestures are typical examples of non-verbal communication, and help people communicate smoothly [1]. However, using camera to recognizing gesture needs high processing power and suffer from delays in recognition [2]. Sometimes distance between large screen and user is a problem as for example in pen based interaction user must be attached to screen. So our main motivation is how we should design a user interface that use cookie wireless sensor [3] as an input device. In this paper we describe the interface setting, method of extracting motion and direction from 3D accelometer, using the tilting gesture. Then we proposed a method that allows users to define their own tilting positions and refer it to certain directions. Then we describe a menu selection interface that is based on pie menu for interaction with remote displays. An evaluation of the proposed interface in terms of accuracy, time and attached objects has been conducted.	3d film;experiment;flight simulator;gesture recognition;input device;pie menu;sensor;smoothing;user interface	Ayman Atia;Shin Takahashi;Jiro Tanaka	2007		10.1007/978-3-540-73107-8_81	nonverbal communication;10-foot user interface;human–computer interaction;computer science;operating system;multimedia;natural user interface;user interface;interaction technique;input device	HCI	-44.85403872851898	-43.14047767425126	195581
34a0d450a226f7ee6dbc43c658a68e5219317515	using smart eyeglasses as a wearable game controller	mobile sensing;eyewear;smart glasses;activity recognition	We investigated how regular eyeglasses fitted with sensing and processing capabilities can be used as a wearable game controller. For this purpose we built a demo based on our smart eyeglasses prototype. WISEglass allows the wearer to control the game Pac-Man through head motions. The control signals are extracted from a gyroscope mounted in the temple of WISEglass. Furthermore we integrated heart rate information into the game.	game controller;gyroscope;prototype;wearable computer	Florian Wahl;Martin Freund;Oliver Amft	2015		10.1145/2800835.2800914	embedded system;smartglasses;computer vision;simulation;computer science;activity recognition	Mobile	-43.7296090260005	-42.04663154927579	195848
fef6fda535d6b86fa81245fcc64e3843420266d8	an interaction and product design of gesture based tv remote control	remote control;interaction;gesture;tv remote control;product design	INTRODUCTION Our target device, AIR-Remote, is a rod-shape device with 3gyroscopes and 3 accelerometers so that it can realize the size, shape, speed, and acceleration of a user hand movements in three dimensional space. Like other multimodal approaches have been done, ours also focused on natural and intuitive interaction design, as well as fun-touse interaction due to the characteristics of TV remote. We created gestures through free brainstorming sessions, and at the same time conducted a series of user research in order to find out the context of use of TV remote control. Finally, we decided user friendly gesture set and applied it on working prototype designed innovatively.	interaction design;multimodal interaction;prototype;remote control;usability;user research	Sang Hwan Kim;Joonho Ok;Hyun Joo Kang;Min-Chul Kim;Mijeong Kim	2004		10.1145/985921.986124	computer vision;interaction;computer science;multimedia;product design;gesture;remote control	HCI	-45.17037002525472	-42.06476111085406	197068
058a918a4f49e78a60c2925a3c66ade8cc6d83d0	wavelet menus: a stacking metaphor for adapting marking menus to mobile devices	input device;menu techniques;marking menu;mobile device;marking menus;multimedia data;mobile devices;wave menus	Exploration and navigation in multimedia data hierarchies (e.g., photos, music) are frequent tasks on mobile devices. However, visualization and interaction are impoverished due to the limited size of the screen and the lack of precise input devices. As a result, menus on mobile devices do not provide efficient navigation as compared to many innovative menu techniques proposed for Desktop platforms. In this paper, we present Wavelet, the adaptation of the Wave menu for the navigation in multimedia data on iPhone. Its layout, based on an inverted representation of the hierarchy, is particularly well adapted to mobile devices. Indeed, it guarantees that submenus are always displayed on the screen and it supports efficient navigation by providing previsualization of the submenus.	input device;item unique identification;mobile device;previsualization;stacking;wavelet	Jérémie Francone;Gilles Bailly;Laurence Nigay;Eric Lecolinet	2009		10.1145/1613858.1613919	embedded system;computer science;operating system;mobile device;multimedia;menu bar;world wide web	HCI	-46.10140431203326	-42.158685392097844	197424
6a15ac1f0842838a906fa7770cfe79d7b8771a24	situated information spaces and spatially aware palmtop computers	tecnologia electronica telecomunicaciones;computacion informatica;palmtop computers;grupo de excelencia;information space;information access;3d control and display;ciencias basicas y experimentales;tecnologias	article in this issue) will further these abilities and cause the generation of short-range and global electronic information spaces to appear lhroughout our everyday environments. How will this information be organized, and how will we interact with it? Wherever possible, we should look for ways of associating electronic information with physical objects in our environment. This raeans that our information spaces will be 3D. The SemNet system [4] is an example of a tool that offers users access to large, complicated 3D information spaces. Our goal is to go a step further by grounding and situating the information in a physical context to provide additional understanding of the organization of the space and to improve user orientation. As an example of ubiquitous computing and situated information spaces, consider a fax machine. The electronic data associated with a fax machine should be collecl:ed, associated , and colocated with [he physical device (see Figure 1). This means that your personal electronic phone book, a log of your incoming and outgoing calls, and fax messages could be accessible by browsing a situated 3D electronic information space surrounding the fax machine. The information would be organized by the layout of the physical device. Incoming calls would be located near 1:he earpiece of the hand receiver while outgoing calls would be situated near the mouthpiece. The phone, book could be found near the keypad. A log of the outgoing fax messages would be found near the fax paper feeder while a log of the incoming faxes would be located at the paper dispenser tray. These logical information hot spots on the physical device can be moved and customized by users according to their personal organizations. The key idea is that the physical object anchors the information, provides a logical means of partitioning and organizing the associated information space, and serves as a retrieval cue for users. A major design requirement of situated information spaces is the ability for users to visualize, browse, and manipulate the 3D space using a ,.RoE.ALL-portable, palmtop computer. That is, instead of a large fixed display on a desk, we want a small, mobile display to act as a window onto the information space. Since the information spaces will consist of multimedia data, the display of the palmtop should be able to handle all forms of data including text, graphics, video, and audio. Moreover, the desire to merge the physical and …	browsing;colocation centre;fax;graphics;handheld pc;mobile phone;organizing (structure);palmtop pc;peripheral;situated;spaces;ubiquitous computing	George W. Fitzmaurice	1993	Commun. ACM	10.1145/159544.159566	embedded system;simulation;human–computer interaction;computer science	HCI	-47.46266277437392	-39.07077250413601	197749
d1c4d7875d72d8446d1034b91779345c91b4c873	griplaunch: a novel sensor-based mobile user interface with touch sensing housing	capacitive touch sensing;recognition;grip pattern recognition;accelerometer;motion sensing;mobile terminal;mobile terminal recognition	This paper describes a novel way of applying capacitive sensing technology to a mobile user interface. The key idea is to use grip-pattern, which is naturally produced when a user tries to use the mobile device, as a clue to determine an application to be launched. To this end, a capacitive touch sensing system is carefully designed and installed underneath the housing of the mobile device to capture the information of the user's grip-pattern. The captured data is then recognized by dedicated recognition algorithms. The feasibility of the proposed user interface system is thoroughly evaluated with various recognition tests.	user interface	Wook Chang;Joonah Park;Hyunjeong Lee;Joonkee Cho;Byung Seok Soh;Junghyun Shim;Gyunghye Yang;Sung-Jung Cho	2006	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2006.6.4.304	embedded system;computer vision;simulation;engineering	AI	-43.3898931077044	-42.759343684158075	198387
c534c39f4db21c2b540fc912b8c2a3403af6a705	immersive haptic interaction with media	image tridimensionnelle;4230;0130c;tridimensional image;haptic technology;video;structure and motion;3d video;imagen tridimensional;haptic interaction	New 3D video representations enable new modalities of interaction, such as haptic interaction, with 2D and 3D video for truly immersive media applications. Haptic interaction with video includes haptic structure and haptic motion for new immersive experiences. It is possible to compute haptic structure signals from 3D scene geometry or depth information. This paper introduces the concept of haptic motion, as well as new methods to compute haptic structure and motion signals for 2D video-plus-depth representation. The resulting haptic signals can be rendered using a haptic cursor attached to a 2D or 3D video display. Experimental results and a demo system are available.	cursor (databases);display device;game demo;haptic technology;video	Nuray Dindar;A. Murat Tekalp;Cagatay Basdogan	2010		10.1117/12.863387	stereotaxy;computer vision;video;computer science;multimedia;haptic technology;computer graphics (images)	Visualization	-41.700801742284376	-38.33846890142927	198498
f4ff5dcdaa52237905112a5c37b8ff20b9ef509c	towards a realistic haptic-based dental simulation		Recently there has been a remarkable increase in the use of technology in medical and dental education. Haptics technologies allow the operator to interact with the simulation environment using the sense of touch. In this paper we investigate three facets for realistic simulation with periodontists: (1) a custom grip is designed to attach dental instruments to the haptic interface in order to enhance the grip, (2) two haptic interfaces are utilized to simulate haptic feedback with both the dental instrument and the mirror instrument, and (3) a finger rest mechanism based on parallel manipulation is used for the intraoral fulcrum during probing. A haptic-based simulation system, named the Haptodont, is developed to evaluate the three facets of realism. A subjective evaluation is conducted with five dental experts to collect more information about perceptions, insights, and experiences to shape the second generation requirements and design for the Haptodont system. Future work will focus on further development and quantitative usability testing (with both dental experts and students) with the goal to improve the educational experience, outcomes and skills of clinicians/students.	cognitive walkthrough;graphical user interface;haptic technology;optic axis of a crystal;requirement;second generation multiplex plus;simulation;usability testing;user experience	Georgios Karafotias;Georgios Korres;Dianne Sefo;Peter Boomer;Mohamad A. Eid	2017	2017 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE)	10.1109/HAVE.2017.8240351	dental instruments;human–computer interaction;usability;perception;haptic technology;solid modeling;computer science	Robotics	-43.71977318905781	-38.901453398826575	198554
b802a7806d2444441296dbb6c6422adc0a9840e5	realization of multilayer occlusion between real and virtual scenes in augmented reality	moving object;graph theory;emo node;groupware;human computer interaction;collaborative work;visual perception augmented reality graph theory groupware human computer interaction;real time;computer supported cooperative work;collaboration;virtual reality;layout;collaborative tools;alpha channel;augmented reality system;virtual scenes;nonhomogeneous media layout augmented reality collaborative work virtual reality collaboration displays computer interfaces tree graphs collaborative tools;near field;multilayer occlusion;tree graphs;nonhomogeneous media;emo nodes;displays;real scenes;visual perception;scene graph tree augmented reality multilayer occlusion alpha channel emo node;augmented reality;computer interfaces;scene graph tree;indoor field occluded objects;emo nodes multilayer occlusion real scenes virtual scenes augmented reality system computer supported cooperative work indoor field occluded objects scene graph tree	Augmented reality system is well suited for computer supported cooperative work. For achieving visual realism, correctly handling and representing occlusion between virtual and real objects in augmented reality scene is essential. In this paper we present an approach for realizing multilayer occlusion. Differing qualitatively from previous work in AR occlusion, our algorithm realizes multilayer occlusion, and its application domain involves indoor-field occluded objects, which are several meters distant from the viewer. Previous related work has focused on monolayer occlusion, and near-field occluded objects, which are within or just beyond arm's reach. We designed a special scene graph tree comprised of some special nodes, namely EMO nodes. According to the location of real moving object, different EMO node is activated in real-time, consequently realizing the multilayer occlusion. Experimental results are provided to demonstrate the multilayer indoor-field occlusion	algorithm;application domain;augmented reality;computer-supported cooperative work;real-time clock;scene graph	Yan Feng;Wenjie Du;Xiaoqi Guan;Fei Gao;Yimin Chen	2006	2006 10th International Conference on Computer Supported Cooperative Work in Design	10.1109/CSCWD.2006.253124	alpha compositing;layout;computer vision;augmented reality;visual perception;near and far field;computer science;graph theory;computer-supported cooperative work;virtual reality;multimedia;management;tree;computer graphics (images);collaboration	Vision	-42.27862767874253	-38.80060759048489	198940
10e37692f1a69a253fa269ed4bfa443d8f647c89	10 inventions on modular keyboards: a triz based analysis	bepress selected works;patent;triz computer keyboard computer hardware hardware inventions computer patents inventive problem solving keyboard design portable keyboards flexible keyboard folding keyboard collapsible keyboard modular keyboards;triz;computer keyboard;keyboard;invention;modular keyboard	A keyboard is the most important input device for a standard computer. But today's keyboard is not merely a keyboard rather performs a various different activities which were never thought at the time when a keyboard was first invented. As a standard keyboard is quite spacious many inventions try to use the space of keyboard to use for various activities. The objective of the inventions is to make a keyboard modular, so that specific parts of the keyboard can be detached or attached as per the need. The advantages of a modular keyboard are as follows: Easy to attach and detach external components. Break the keyboard and remove the part that is not required, thus giving an advantage of space. Expanding the parts of the keyboard thereby giving the advantage of size. Make the keyboard folding thereby making it easy to carry Attach mouse, telephone, speakers etc. to the keyboard. Background problem The modern keyboards are having more number of special purpose keys. The 83 keys of the early days' keyboards have been increased to 101 keys to include additional control and function keys. But this enhancement of the keyboard has certain drawbacks. (i) There are two sets of navigation keys out of which most users use only either of the sets. (ii) Besides the user has to use one or more external pointing devices each having a separate cable to connect to the rear side of the computer making the table messy. (iii) Moreover each device needs additional desk space. Solution provided by the invention Ganthier et al. found the solutions to all the above by disclosing a modular keyboard (US patent 5865546, Assigned to Compaq, Issued in Feb 99). The modular keyboard has several openings in which any of the input modules can be inserted. The user can insert a module and replace a module independently. The disclosed modular keyboard has two additional slots other than the basic keyboard module. The user can use any two extra devices depending on need, either a numeric pad and a trackball, or a trackball and joystick or any two other devices depending on need. The user can plug in and plug out any device as required. A controller in the keyboard assembly will determine which types of input device modules are coupled with the keyboard assembly. This has the advantage of no extra cables, quick reconfiguration and saving of space. TRIZ …	assembly language;computer keyboard;input device;joystick;trackball	Umakant Mishra	2013	CoRR	10.2139/ssrn.932269	embedded system;keyboard computer;computer hardware;invention	HCI	-44.610176578746874	-42.93814186087626	199586
79d6db3fd97bd57f9422be8f6197015853e88b27	development of a 3d pointing voice interface using a three-axis microphone array	computers;microphones;pointing device;input device;three axis microphone array;data mining;user mouth 3d position;arrays;handicapped aids;cursor controls;microphone array;three dimensional displays;3 dimensional voice pointing device;speech recognition;3 dimensional;correlation;haptic interfaces;3d pointing voice interface;noise	A mouse or keyboard is widely used as an input device for operating computers. However, because these devices are hand controlled, handicapped people who can hardly move their hands are unable to operate computers. In this study, we propose a novel interface that controls the cursor on a screen with a 3-dimensional voice pointing device. This pointing device can estimate the 3D position of the user's mouth while the user makes an utterance. The user can control the cursor by moving the mouth and making an utterance near a vertical surface. In addition, we implement a zoom function of the screen so that users can adjust the cursor position when they want the cursor to point at a small button or icon. Users can control the zoom function by moving the head forwards or backwards. We confirmed the effectiveness of the zoom function experimentally.	apache axis;computer mouse;cursor (databases);experiment;input device;microphone;pointing device	Yasuharu Hashimoto;Akira Sasou	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326290	footmouse;three-dimensional space;pointer;speech recognition;computer hardware;computer science;noise;correlation;input device;pointing device	HCI	-44.80011419594447	-44.227214103180735	199692
a46b6ae9ea3b2804e2ac5c62a6cd1204cf62ce7a	single-handed interaction for mobile and wearable computing	mobile;wearable;single handed interaction;encumbered	Screen sizes on devices are becoming smaller, to allow them to fit in more places (e.g., wrists, sports bands and small music players). At the same time, screen sizes can be seen to become larger to accommodate new experiences (e.g., phablets, tablets, eReaders). Each of these trends can make devices difficult to use with only one hand (e.g., fat-finger or reachability). However, there are many occasions when the user's other hand is occupied (encumbered) or not available. The aim of this research is to explore, create and study novel interaction techniques that support effective single-hand usage on mobile and wearable devices.	interaction technique;reachability;tablet computer;wearable computer;wearable technology	Hui-Shyong Yeo	2016		10.1145/2957265.2963110	simulation;wearable computer;human–computer interaction;computer science;operating system;mobile technology;multimedia	HCI	-46.940759352840885	-42.771514211511246	199962
874edf678187887cc0d158f188a956b4f3100a59	geltouch: localized tactile feedback through thin, programmable gel	thermoresponsive hydrogel;tactile feedback	We present GelTouch, a gel-based layer that can selectively transition between soft and stiff to provide tactile multi-touch feedback. It is flexible, transparent when not activated, and contains no mechanical, electromagnetic, or hydraulic components, resulting in a compact form factor (a 2mm thin touchscreen layer for our prototype). The activated areas can be morphed freely and continuously, without being limited to fixed, predefined shapes. GelTouch consists of a poly(N-isopropylacrylamide) gel layer which alters its viscoelasticity when activated by applying heat (>32 C). We present three different activation techniques: 1) Indium Tin Oxide (ITO) as a heating element that enables tactile feedback through individually addressable taxels; 2) predefined tactile areas of engraved ITO, that can be layered and combined; 3) complex arrangements of resistance wire that create thin tactile edges. We present a tablet with 6x4 tactile areas, enabling a tactile numpad, slider, and thumbstick. We show that the gel is up to 25 times stiffer when activated and that users detect tactile features reliably (94.8%).	analog stick;indium tin oxide;multi-touch;prototype;tablet computer;touchscreen	Viktor Miruchna;Robert Walter;David Lindlbauer;Maren Lehmann;Regine von Klitzing;Jörg Müller	2015		10.1145/2807442.2807487	computer science	HCI	-42.595625185605385	-41.358978011274964	199966
