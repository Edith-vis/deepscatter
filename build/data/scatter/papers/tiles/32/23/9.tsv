id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
fe6dc966a4431a4585d689e5c5a6bf315d3041eb	a visual landmark framework for indoor mobile robot navigation	robot localization;object recognition;image segmentation mobile robots path planning object detection object recognition;sensor phenomena and characterization;electronic mail;image segmentation;indoor mobile robot navigation;mobile robot;path planning;landmark model;interest points;reference frame;visual landmark framework;mobile robots;vision functions;landmark detection;partial hausdorff distance;visual landmarks;navigation;landmark based navigation;landmark recognition visual landmark framework indoor mobile robot navigation vision functions landmark based navigation buildings global reference frame partial hausdorff distance landmark model landmark detection;image edge detection;mobile robots navigation image segmentation image edge detection robot localization laser modes sensor phenomena and characterization architecture electronic mail robot vision systems;mobile robot navigation;hausdorff distance;architecture;landmark recognition;robot vision systems;laser modes;global reference frame;buildings;object detection	This article presents vision functions needed on a mobile robot to deal with landmark-based navigation in buildings. Landmarks are planar, quadrangular surfaces, which must be distinguished from the background, typically a poster on a wall or a door-plate. In a first step, these landmarks are detected and their positions with respect to a global reference frame are learned; this learning step is supervised so that only the best landmarks are memorized, with an invariant representation based on a set of interest points. Then, when the robot looks for visible landmarks, the recognition procedure takes advantage of the partial Hausdorff distance to compare the landmark model and the detected quadrangles. The paper presents the landmark detection and recognition procedures, and discusses their performances.	hausdorff dimension;mobile robot;performance;reference frame (video);supervised learning	Jean-Bernard Hayet;Frédéric Lerasle;Michel Devy	2002		10.1109/ROBOT.2002.1014344	mobile robot;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation	Robotics	50.70822577652955	-38.63814070190185	152925
28f6817cb923c626c6f255e1a82f5c67aa84d688	probabilistic tracking and recognition of non-rigid human motion			kinesiology	Fei Huang	2006				Vision	51.406675798152804	-43.12802939703228	153508
9c8d91f8a5740db1f4810775a32d274a3e1a7ad3	a design and research of eye gaze tracking system based on stereovision	pupil;ccd camera;tracking system;eye gaze tracking;support vector regression;gaze tracking;stereovision;purkinje image;gaze direction;eye gaze;ellipse fitting	"""A new design for an eye gaze tracking system based on stereovision technique is presented. The system consists of two CCD cameras and two novel light sources for stereovision. The way of getting pupil position is to do subtraction of two images, the """"bright pupil"""" and the """"dark pupil"""", which are gained by illuminating user's eyes alternately. The pupil center is located by ellipse fitting when the Purkinje image is gained in the """"dark pupil"""", so the local gaze direction can be obtained. We also use support vector regression to figure out the mapping relationship from eye parameters to gaze point, and the interference from head motion may be eliminated by using 3D eyeball data. The experimental results show that the system can achieve an average accuracy of 1.8 degree and be robust in gaze tracking under large head movements."""		Pengyi Zhang;Zhiliang Wang;Siyi Zheng;Xuejing Gu	2009		10.1007/978-3-642-04070-2_32	support vector machine;computer vision;tracking system;eye tracking;computer science;stereopsis;artificial intelligence;machine learning;charge-coupled device;computer graphics (images)	HCI	47.39818901613347	-44.37458717308042	154207
c7c280e2631db1d1de33a126f9b04b5c2cd4c2f1	object recognition on humanoids with poveated vision	object representation;object recognition;kernel function;layout;multiple views;computer vision;object recognition humanoid vision foveated vision;eyes;humanoid robots;object recognition robot vision systems cameras humanoid robots eyes robotics and automation computer vision lenses face detection layout;visual search;eye movement;field of view;lenses;support vector machine;face detection;robot vision systems;robotics and automation;cameras	Object recognition requires a robot to perform a number of nontrivial tasks such as finding objects of interest, directing its eyes towards the objects, pursuing them, and identifying the objects once they appear in the robot¿s central vision. In this paper we describe a system that make use of foveated vision to solve the problem of object recognition on a humanoid robot. The system employs a biologically motivated object representation scheme based on Gabor kernel functions to represent multiple views of objects. We demonstrate how to utilize support vector machines to identify known objects in foveal images using this representation. A mechanism for visual search is integrated into the system to find a salient region and to place an object of interest in the field of view of foveal cameras. The framework also includes a control scheme for eye movements, which are directed using the results of attentive processing in peripheral images.	computer vision;humanoid robot;outline of object recognition;peripheral;support vector machine	Ales Ude;G. Gheng	2004	4th IEEE/RAS International Conference on Humanoid Robots, 2004.	10.1109/ICHR.2004.1442692	kernel;layout;support vector machine;computer vision;face detection;simulation;deep-sky object;field of view;visual search;form perception;computer science;humanoid robot;object-oriented design;cognitive neuroscience of visual object recognition;lens;3d single-object recognition;eye movement	Robotics	48.015661386960815	-39.26413641098826	154574
4daa262e27c41cb9664c94790de477f055d3c52c	a comparison of three methods for measure of time to contact	vision system;robot vision affine transforms collision avoidance edge detection mobile robots motion control;relative position;image features;obstacle detection;image feature;motion control;active contour;image segmentation;rate of change;edge detection;comunicacion de congreso;image flow;mobile robots;active contours;scene reconstruction;scale invariant ridge segment;active contour affine scale;computer vision;reactive control;laplace equations;time measurement motion detection motion control biological control systems layout image reconstruction motion estimation active contours image analysis image segmentation;robot vision;obstacle avoidance;estimation;time to contact;human visual system;affine transformation time to contact obstacle detection reactive control motion control image feature active contour affine scale scale invariant ridge segment image brightness derivatives image flow;affine transformation;pixel;affine transforms;collision avoidance;camera calibration;depth estimation;image brightness derivatives;3d reconstruction;scale invariance;cameras;noise;conference lecture	Time to contact (TTC) is a biologically inspired method for obstacle detection and reactive control of motion that does not require scene reconstruction or 3D depth estimation. Estimating TTC is difficult because it requires a stable and reliable estimate of the rate of change of distance between image features. In this paper we propose a new method to measure time to contact, active contour affine scale (ACAS). We experimentally and analytically compare ACAS with two other recently proposed methods: scale invariant ridge segments (SIRS), and image brightness derivatives (IBD). Our results show that ACAS provides a more accurate estimation of TTC when the image flow may be approximated by an affine transformation, while SIRS provides an estimate that is generally valid, but may not always be as accurate as ACAS, and IBD systematically over-estimate time to contact.	active contour model;airborne collision avoidance system;approximation algorithm;experiment;ion beam deposition	Guillem Alenyà;Amaury Nègre;James L. Crowley	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354024	3d reconstruction;motion control;mobile robot;computer vision;estimation;camera resectioning;simulation;edge detection;machine vision;computer science;noise;scale invariance;active contour model;control theory;affine transformation;obstacle avoidance;image segmentation;human visual system model;feature;pixel	Robotics	52.08735840142506	-41.03792595968522	154724
5ecf7d41665012d2204ee4f01df085d6884d311d	visual robotic object grasping through combining rgb-d data and 3d meshes		In this paper, we present a novel framework to drive automatic robotic grasp by matching camera captured RGB-D data with 3D meshes, on which prior knowledge for grasp is pre-defined for each object type. The proposed framework consists of two modules, namely, pre-defining grasping knowledge for each type of object shape on 3D meshes, and automatic robotic grasping by matching RGB-D data with pre-defined 3D meshes. In the first module, we scan 3D meshes for typical object shapes and pre-define grasping regions for each 3D shape surface, which will be considered as the prior knowledge for guiding automatic robotic grasp. In the second module, for each RGB-D image captured by a depth camera, we recognize 2D shape of the object in it by an SVM classifier, and then segment it from background using depth data. Next, we propose a new algorithm to match the segmented RGB-D shape with predefined 3D meshes to guide robotic self-location and grasp by an automatic way. Our experimental results show that the proposed framework is particularly useful to guide camera based robotic grasp.	3d printing	Yiyang Zhou;Wenhai Wang;Wenjie Guan;Yirui Wu;Heng Lai;Tong Lu;Min Cai	2017		10.1007/978-3-319-51811-4_33	computer vision;machine learning	Robotics	48.912453298822754	-40.40075704364984	155286
307d322d6a296305c6a0896c5566217a0d448d21	posenet: a convolutional network for real-time 6-dof camera relocalization	pose feature posenet real time 6 dof camera relocalization monocular six degree of freedom relocalization system single rgb image convolutional neural network large scale outdoor scenes image plane regression problems transfer learning large scale classification data motion blur camera intrinsics point based sift registration;regression analysis convolution image motion analysis neural nets real time systems;cameras training neural networks robot vision systems simultaneous localization and mapping real time systems quaternions	We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3 degrees accuracy for large scale outdoor scenes and 0.5m and 5 degrees accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.	algorithm;artificial neural network;audio engineer;convolutional neural network;end-to-end principle;gaussian blur;high-level programming language;image plane;intrinsic function;mathematical optimization;real-time clock;real-time computing	Alex Kendall;Matthew Koichi Grimes;Roberto Cipolla	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.336	computer vision;simulation;computer graphics (images)	Vision	50.950605574009835	-44.808314583010585	155674
041d49767d51464636ec8d92c9bd77c41966f67f	six degree-of-freedom hand/eye visual tracking with uncertain parameters	moving object;image motion;manipulators;robotic visual servoing adaptive control image motion calibration;robot vision systems cameras target tracking error correction manipulators motion detection parameter estimation calibration adaptive control delay;pistage;position measurement robot vision adaptive control intelligent control tracking;image processing;cross correlation;robotic visual servoing;bepress selected works;degree of freedom;correction erreur;adaptive control algorithm;multi robotic system;six degree of freedom hand eye visual tracking;adaptive control;manipulateur;rastreo;multi robot system;motion estimation;robotics;intelligent control;puma 560 manipulators;robot manipulator;algorithme;3d robotic visual tracking;camera motion;model error;robot vision;computational delays;control adaptativo;moving targets;stereo image processing robot vision target tracking tracking motion estimation adaptive control error compensation;error correction;modeling errors;stereo image processing;error compensation;position measurement;commande adaptative;robot vision systems cameras target tracking error correction manipulators parameter estimation calibration adaptive control delay image processing;puma 560 manipulators six degree of freedom hand eye visual tracking uncertain parameters 3d robotic visual tracking moving targets image plane camera motion adaptive control algorithm modeling errors computational delays image processing multi robotic system;algorithms;robot vision hand eye visual tracking uncertain parameters 3d robotic visual tracking moving targets image plane motion detection adaptive control modeling error compensation;image plane;robotique;uncertain parameters;parameter estimation;target tracking;sum of squared difference;visual tracking;visual servoing;robot vision systems;calibration;cameras;tracking	Algorithms for 3D robotic visual tracking of moving targets whose motion is 3D and consists of translational and rotational components are presented. The objective of the system is to track selected features on moving objects and to place their projections on the image plane at desired positions by appropriate camera motion. The most important characteristics of the proposed algorithms are the use of a single camera mounted on the end-effector of a robotic manipulator (eye-inhand configuration), and the fact that these algorithms do not require accurate knowledge of the relative distance of the target object from the camera frame. This fact makes these algorithms particularly useful in environments that are difficult to calibrate. The camera model used introduces a number of parameters that are estimated on-line, further reducing the algorithms’ reliance on precise calibration of the system. An adaptive control algorithm compensates for modeling errors, tracking errors, and unavoidable computational delays which result from time-consuming image processing. Experimental results are presented to verify the efficacy of the proposed algorithms. These experiments were performed using a multirobotic system consisting of Puma 560 manipulators.	algorithm;experiment;image plane;image processing;online and offline;programmable universal machine for assembly;puma (microarchitecture);robot end effector;video tracking	Nikolaos Papanikolopoulos;Bradley J. Nelson;Pradeep K. Khosla	1995	IEEE Trans. Robotics and Automation	10.1109/70.466612	control engineering;computer vision;camera auto-calibration;calibration;error detection and correction;adaptive control;eye tracking;image processing;computer science;cross-correlation;motion estimation;errors-in-variables models;control theory;tracking;image plane;degrees of freedom;robotics;estimation theory;visual servoing;statistics;intelligent control	Robotics	53.71854267046751	-39.842757748096446	156546
5b2607087d33744edddfa596bf31dbb9ba4ed84e	real-time object detection, localization and verification for fast robotic depalletizing	three dimensional displays cameras robot vision systems service robots;service robots;real time object detection automotive multiresolution surfel model manipulation robot robotic depalletizing object verification object localization;robotic assembly automotive components manipulators motion control object detection;three dimensional displays;robot vision systems;cameras	Depalletizing is a challenging task for manipulation robots. Key to successful application are not only robustness of the approach, but also achievable cycle times in order to keep up with the rest of the process. In this paper, we propose a system for depalletizing and a complete pipeline for detecting and localizing objects as well as verifying that the found object does not deviate from the known object model, e.g., if it is not the object to pick. In order to achieve high robustness (e.g., with respect to different lighting conditions) and generality with respect to the objects to pick, our approach is based on multi-resolution surfel models. All components (both software and hardware) allow operation at high frame rates and, thus, allow for low cycle times. In experiments, we demonstrate depalletizing of automotive and other prefabricated parts with both high reliability (w.r.t. success rates) and efficiency (w.r.t. low cycle times).	experiment;object detection;real-time clock;real-time transcription;robot;robustification;sensor;surfel;verification and validation	Dirk Holz;Angeliki Topalidou-Kyniazopoulou;Jörg Stückler;Sven Behnke	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353560	embedded system;computer vision;simulation;engineering	Robotics	50.54984550267224	-41.57096931244533	156845
1f2f712253a68cd9f8172de19297e35cec7919dd	vision system of facial robot shfr- iii for human-robot interaction		The improvement of human-robot interaction is an inevitable trend for the development of robots. Vision is an important way for a robot to get the information from outside. Binocular vision model is set up on the facial expression robot SHFRIII, this paper develops a visual system for human-robot interaction, including face detection, face location, gender recognition, facial expression recognition and reproduction. The experimental results show that the vision system can conduct accurate and stable interaction, and the robot can carry out human-robot interaction.	artificial intelligence;binocular vision;experiment;face detection;facial recognition system;human–robot interaction;robot	Xianxin Ke;Yujiao Zhu;Yang Yang;Jizhong Xin;Zhitong Luo	2016		10.5220/0005994804720478	computer vision	Robotics	46.589722624556	-41.8889445794038	157223
ac6b8d779b3e794ecebe7b21833204627b36c06a	shape recognition of laser beam trace for human-robot interface	image processing;robot navigation;shape recognition;laser pointer;optical flow	A robot navigation system using the pattern recognition of figures drawn by a laser pointer has been proposed. Typical figures are registered and assigned to individual robot commands. Each figure is identified based on the feature of its edges. This system detects trace of laser beam and calculates its optical flow vectors. Each figure has its own characteristic distribution pattern of vector inclinations. By evaluating the optical flow pattern of displayed laser spot, the system distinguishes the shape of a laser beam trace and provides the command to a robot corresponding to the drawn figure. The proposed system has been applied to the mobile robot, and shows its effectiveness by steering successfully.	algorithm;experiment;feature extraction;image processing;mobile robot;noise reduction;optical flow	Takeshi Tsujimura;Yoshihiro Minato;Kiyotaka Izumi	2013	Pattern Recognition Letters	10.1016/j.patrec.2013.03.023	computer vision;image processing;computer science;optical flow	Vision	48.856173769986015	-41.243599570174645	157739
843d7b5236cb200a37ce0beb2e48cfdb8d62c125	depth compensation model for gaze estimation in sport analysis	training;geometry;gaze tracking;binocular eye tracking setups depth compensation model sport analysis parallax error head mounted eye trackers eye tracker parameters ocular biometric parameters gaze estimation method epipolar geometry;estimation;sport gaze tracking image processing;transmission line matrix methods;calibration;cameras;cameras gaze tracking calibration training estimation geometry transmission line matrix methods	A depth compensation model is presented as a novel approach to reduce the effects of parallax error for head-mounted eye trackers. The method can reduce the parallax error when the distance between the user and the target is prior known. The model is geometrically presented and its performance is tested in a totally controlled environment with aim to check the influences of eye tracker parameters and ocular biometric parameters on its behavior. We also present a gaze estimation method based on epipolar geometry for binocular eye tracking setups. The depth compensation model has shown very promising to the field of eye tracking. It can reduce 10 times less the influence of parallax error in multiple depth planes.	binocular vision;biometrics;epipolar geometry;eye tracking;parallax	Fabricio Batista Narcizo;Dan Witzner Hansen	2015	2015 IEEE International Conference on Computer Vision Workshop (ICCVW)	10.1109/ICCVW.2015.107	computer vision;estimation;calibration;mathematics;geometry;statistics;computer graphics (images)	Vision	48.10901074805698	-44.6487901367585	158303
fabefb9df39d79870364d512484a5ca7d9450e44	real-time slam with a high-speed cmos camera	ccd camera;image motion analysis;image processing;sensors;mobile robot;real time;robotic arm;real time processing;self localization;simultaneous localization and mapping cmos image sensors cameras robot vision systems robot sensing systems charge coupled image sensors mobile robots augmented reality robustness jitter;mobile robots;robust control;slam robots cameras image motion analysis jitter mobile robots robot vision robust control sensors;robot arm;motion blur;robot vision;sensor;low localization uncertainty;simultaneous localization and mapping;fast imaging;robustness;augmented reality;jitter;high speed cmos camera;real time slam;slam robots;high speed;cameras;sensor real time slam high speed cmos camera mobile robot self localization simultaneous localization and mapping robustness motion blur jitter image processing low localization uncertainty robotic arm	A typical task in mobile robotics or augmented reality applications is self-localization in an unknown environment. In the robotics community the localization and mapping of the unknown environment is called SLAM (simultaneous localization and mapping). Important constraints in SLAM using visual input are real-time processing and robustness against motion blur or jitter. The contribution of this paper is in enhancing the performance of a well known SLAM method using a high-speed CMOS camera. Benefits of this camera are that it allows fast image processing, little motion blur and low localization uncertainty. SLAM performance with a high-speed camera is demonstrated on a robotic arm. The result of the experiments is that for SLAM applications in robotics, where the motion is not smooth, the high-speed CMOS camera is a more suitable sensor than a standard CCD camera.	augmented reality;cmos;charge-coupled device;experiment;extended kalman filter;gaussian blur;ieee 1394;image processing;image quality;image sensor;microsoft windows;mobile robot;real-time clock;real-time transcription;robotic arm;robotics;sampling (signal processing);simultaneous localization and mapping;traffic enforcement camera;velocity (software development)	Peter Gemeiner;Wolfgang Ponweiser;Peter Einramhof;Markus Vincze	2007	14th International Conference on Image Analysis and Processing (ICIAP 2007)	10.1109/ICIAP.2007.108	mobile robot;embedded system;computer vision;camera auto-calibration;augmented reality;simulation;robotic arm;image processing;computer science;sensor	Robotics	51.376065517007724	-41.50122960563996	158627
f64def3e5b13cd26f0b6290033a9ae018e8bde99	3d articulated human motion analysis system using a single low-cost rgb-d sensor		In this paper, an cost-effective, highly accurate 3D articulated human motion analysis system is proposed. In the proposed system, a single low-cost RGB-D sensor captures a color image and depth one of human motion. Then, a deep learning-based 2D motion analysis process accurately estimates intermediate 2D articulated human motion from the color image. Finally, a color-to-depth warping-based 3D motion transform process effectively compute final 3D articulated human motion from the depth image. The proposed system is cost-effective but highly accurate. Experimental results show that the proposed system outperforms the commercial system using the same RGB-D sensor when compared in terms of accuracy.	color image;deep learning;kinesiology;sensor	Jong-Sung Kim;Myung-Gyu Kim	2018	2018 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2018.8539680	motion analysis;image sensor;image warping;color image;deep learning;rgb color model;computer vision;artificial intelligence;computer science	Robotics	50.72536551337683	-44.807022800390676	158700
cf9ba448c727ec6b6b6fe9c0a1d942a07022b89c	face tracking active vision system with saccadic and smooth pursuit	people detection;surveillance;face tracking active vision system;dc motor face tracking active vision system people detection people tracking human robot interaction robotic vision rapid eye movements color histogram based matching linear prediction;visual servo face tracking active vision hybrid control;hybrid control;color histogram;human robot interaction;dc motor;linear predictive;face tracking;color histogram based matching;face recognition;robot vision;smooth pursuit;target tracking face recognition man machine systems robot vision surveillance;eye movement;robotic vision;visual servo;people tracking;linear prediction;target tracking;face detection;visual servoing;rapid eye movements;face detection machine vision tracking robot vision systems cameras robustness surveillance head servomechanisms layout;rapid eye movement;man machine systems;active vision	- People detection and tracking is very important and fundamental functionality in surveillance and human-robot interaction. This paper presents a robotic vision module with rapid eye movements for fast face tracking. Face tracking is initiated by robust face detection followed with color histogram-based matching and linear prediction of face position. Rapid and smooth eye movements (i.e. panning and tilting) are embodied by linear and hybrid (i.e. velocity-based FF + position based FB) control scheme as well as powerful yet widely used DC motor. Experimental results demonstrated ultra-fast and extremely smooth pursuit of moving face, at 6 deg/40 ms (approximately 10 m/sec at the distance of 3 m), with abrupt change in motion direction (e.g., jumping), with sustained robustness to the change of pose (e.g., from frontal face to nearly back of head).	active vision;color histogram;face detection;facial motion capture;feedback;human–robot interaction;pose (computer vision);robot;robustness (computer science);velocity (software development)	Masakazu Matsugu;Kan Torii;Yoshinori Ito;Tadashi Hayashi;Tsutomu Osaka	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340120	facial recognition system;human–robot interaction;color histogram;computer vision;face detection;facial motion capture;simulation;active vision;linear prediction;computer science;engineering;dc motor;smooth pursuit;visual servoing;eye movement	Robotics	47.80842477109003	-41.70868256200394	158821
fcf0876175bdc80cc2e3b9a0635703efdb2f1c29	spad dcnn: localization with small imaging lidar and dcnn		Small 3D LIDAR and a multimodal-based localization are fundamentally important for autonomous robots. This paper describes presentation and demonstration of a sensor and a method for LIDAR-image based localization. Our small LIDAR, named SPAD LIDAR, uses a single-photon avalanche diode (SPAD). The SPAD LIDAR incorporates laser receiver and environmental light receiver in a single chip. Therefore, the sensor simultaneously outputs range data and monocular image data. By virtue of this structure, the sensor requires no external calibration between range data and monocular image data. Based on this sensor, we introduce a localization method using a deep convolutional neural network (SPAD DCNN), which fuses SPAD LIDAR outputs: range data, monocular image data, and peak intensity data. Our method regresses LIDAR's position in an environment. We also introduce improved SPAD DCNN, designated as Fast SPAD DCNN. To reduce the computational demands of SPAD DCNN, Fast SPAD DCNN integrates range data and peak intensity data. The integrated data reduces runtime without greatly increasing localization error compared to the conventional method. We evaluate our SPAD DCNN and Fast SPAD DCNN localization method in indoor environments and compare its performance. Results show that SPAD DCNN and Fast SPAD DCNN improve localization in terms of accuracy and runtime.	artificial neural network;autonomous robot;convolutional neural network;diode;internationalization and localization;motion capture;multimodal interaction;supervised learning	Seigo Ito;Shigeyoshi Hiratsuka;Mitsuhiko Ohta;Hiroyuki Matsubara;Masaru Ogawa	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206167	convolutional neural network;computer vision;chip;computer science;artificial intelligence;lidar	Robotics	46.63865082027374	-39.6808803239265	159267
df8759a05bc8a49ff19e81ba7ac712ef955937b1	estimating 3d hand position and orientation using stereo		We present an approach for estimating the 3D absolute position and orientation of the hand using a planar model of the back of the hand. We use stereo cameras to first build a sparse disparity map of the estimated area of the back of the hand. Then, the best fitting plane to the disparity points is computed using robust estimation. Finally, the 3D hand plane is calculated based on the disparity plane and the six parameters of position and orientation of the back of the hand are estimated . Experimental results demonstrate the accuracy of this technique.	binocular disparity;image plane;numerical analysis;sparse matrix;stereo camera;stereo cameras;virtual reality	Afshin Sepehri;Yaser Yacoob;Larry S. Davis	2004			computer vision;artificial intelligence;pattern recognition;computer science;stereo cameras;planar	Vision	52.71049184079137	-41.77078632376808	159806
b7444d8c0ae0840f2359e65fe512797978bac846	active multi-view object search on a humanoid head	human centered environment;humanoid robot;random access memory;foveal stereo camera system;active multiview object detection;robot vision humanoid robot active multiview object detection visual search human centered environment fes feature ego sphere foveal stereo camera system;fes;layout;visualization;robot vision;humanoid robots;visual perception cameras humanoid robots object detection robot vision;machine vision;visual search;robots;layout humans cameras humanoid robots robot vision systems robustness visual system head machine vision robotics and automation;feature ego sphere;robustness;visual perception;humans;search problems;head;visual system;robot vision systems;human activity;robotics and automation;cameras;object detection	Visual search is a common daily human activity and a prerequisite to the interaction with objects encountered in cluttered environments. Humanoid robots that are supposed to take part in human daily life should possess similar capabilities in terms of representing, attending to and recalling objects of interest in order to ensure robust perception in human-centered environments. In this paper, we present necessary processes, memories and representations which allow to identify and store locations of objects, encountered from different angles of view, in a visual search task. In particular, we introduce the so-called Feature Ego-Sphere (FES) as the scene memory for a humanoid robot. Experiments comprising different visual search tasks have been carried out on an active humanoid head equipped with perspective and foveal stereo camera systems. The scene is analyzed actively using both camera systems in order to find instances of searched objects in a consistent and persistent manner.	asch conformity experiments;humanoid robot;persistent data structure;shin megami tensei: persona 3;stereo camera	Kai Welke;Tamim Asfour;Rüdiger Dillmann	2009	2009 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2009.5152503	computer vision;simulation;machine vision;computer science;humanoid robot;artificial intelligence	Robotics	48.75723005041028	-38.98335333684343	159929
6bf41197aeed8a0b9a7c30c49e73ddde0df86a55	a bank of unscented kalman filters for multimodal human perception with mobile service robots	bayesian estimation;mobile robot;real time;h670 robotics and cybernetics;service robotics;human tracking;qa75 electronic computers computer science;robot perception;mobile service;face recognition;service robot;bayesian estimator;human tracking and recognition;face detection;unscented kalman filter;human perception	A new generation of mobile service robots could be ready soon to operate in human environments if they can robustly estimate position and identity of surrounding people. Researchers in this field face a number of challenging problems, among which sensor uncertainties and realtime constraints. In this paper, we propose a novel and efficient solution for simultaneous tracking and recognition of people within the observation range of a mobile robot. Multisensor techniques for legs and face detection are fused in a robust probabilistic framework to height, clothes and face recognition algorithms. The system is based on an efficient bank of Unscented Kalman Filters that keeps a multihypothesis estimate of the person being tracked, including the case where the latter is unknown to the robot. Several experiments with real mobile robots are presented to validate the proposed approach. They show that our solutions can improve the robot’s perception and recognition of humans, providing a useful contribution for the future application of service robotics.	algorithm;experiment;face detection;facial recognition system;kalman filter;mobile robot;multimodal interaction;real-time locating system;robotics;scalability	Nicola Bellotto;Huosheng Hu	2010	I. J. Social Robotics	10.1007/s12369-010-0047-x	facial recognition system;kalman filter;mobile robot;computer vision;face detection;simulation;bayes estimator;computer science;artificial intelligence;perception	Robotics	48.21568327132594	-40.763940410291916	161012
1656dfb725af96448089f5508fbf399c91f3d5ac	error modeling in stereo navtgation	navegacion;robot movil;erreur;errores;measurement error;image processing;mobile robot;vision estereoscopica;camera television;navigation motion estimation mobile robots robot vision systems cameras measurement errors gaussian distribution error correction robot motion computational modeling;vision stereoscopique;locomotion;motion;three dimensional;robot industriel;navigation;robots motion planning;robot mobile;triangulacion;television camera;robots;stereo vision;robot industrial;position estimation;motion planning;camara de television;triangulation;vision artificielle;error;robots motion planning robots locomotion;stereopsis;artificial vision;gaussian distribution;robots locomotion;moving robot;industrial robot;vision artificial	on scalar models of measurement error in triangulation. Using t h m-dimensional (3D) Gaussian distributions to model triangulation error is shown lo lead lo much better performance. How to compute the error model from image correspondences. estimate robot motion between frames, and update the global positions of the robot and the hndm8rks over time are discussed. Simulations show that, compared to scalar error models, the 3D Gaussian reduces the variance in robot position estimates and better distinguishes rotational from translational motion. A short indoor run with real images supported these conclusions and computed the final robot position to within two percent of distance and one degree of orientation. These results illustrate the importance of error modeling in stereo vision for this and other applications.	computer simulation;errors-in-variables models;robot;stereopsis	Larry H. Matthies;Steven A. Shafer	1986	IEEE J. Robotics and Automation	10.1109/JRA.1987.1087097	computer vision;simulation;image processing;computer science;stereopsis;artificial intelligence;computer graphics (images)	Robotics	53.64382879982549	-39.920400566995745	161029
083e639456f48d179bdbf02285215b954cfe8622	real-time human body tracking based on data fusion from multiple rgb-d sensors		In this work we present a human pose estimation method based on the skeleton fusion and tracking using multiple RGB-D sensors. The proposed method considers the skeletons provided by each RGB-D device and constructs an improved skeleton, taking into account the quality measures provided by the sensors at two different levels: the whole skeleton and each joint individually. Then, each joint is tracked by a Kalman filter, resulting in a smooth tracking performance. We have also developed a new dataset consisting of six subjects performing seven different gestures, recorded with four Kinect devices simultaneously. Experimental results performed on this dataset show that the system obtains better smoothness results than the most representative methods found in the literature. The proposed system operates at a processing rate of 25 frames per second (including the whole algorithm loop, i.e., data acquisition and processing) without the explicit use of the multithreading capabilities of the system.	algorithm;data acquisition;kalman filter;kinect;multithreading (computer architecture);real-time clock;sensor;thread (computing)	Juan C. Núñez;Raúl Cabido;Antonio S. Montemayor;Juan José Pantrigo	2016	Multimedia Tools and Applications	10.1007/s11042-016-3759-6	computer vision;simulation	Robotics	47.7758702469676	-40.30073656131858	162057
b48831b8c4313da09622cf885ec2ccaceb25f8ba	a principle of minimum translation search approach for object pose refinement	detected poses;object pose refinement;geometrically consistent objects configuration;uncertainty;depth limited search minimum translation search approach object pose refinement object pose estimation approaches detected poses inaccurate noisy poses overlapping objects cluttered environments rigid body assumption minimum translation search promts interpenetration free configuration shipping containers geometrically consistent objects configuration pose estimation accuracy a star search;computer and information science;rigid body assumption;search problems three dimensional displays solid modeling shape containers uncertainty;search problems pose estimation robot vision;pose estimation accuracy;natural sciences;overlapping objects;promts;cluttered environments;depth limited search;robot vision;shape;inaccurate noisy poses;three dimensional displays;minimum translation search approach;solid modeling;datavetenskap datalogi;datavetenskap;minimum translation search;search problems;computer science;shipping containers;a star search;object pose estimation approaches;interpenetration free configuration;containers;pose estimation	The state-of-the-art object pose estimation approaches represent the set of detected poses together with corresponding uncertainty. The inaccurate noisy poses may result in a configuration of overlapping objects especially in cluttered environments. Under a rigid body assumption the inter-penetrations between pairs of objects are geometrically inconsistent. In this paper, we propose the principle of minimum translation search, PROMTS, to find an inter-penetration-free configuration of the initially detected objects. The target application is to automate the task of unloading shipping containers, where a geometrically consistent configuration of objects is required for high level reasoning and manipulation. We find that the proposed approach to resolve geometrical inconsistencies improves the overall pose estimation accuracy. We examine the utility of two selected search methods: A-star and Depth-Limited search. The performance of the search algorithms are tested on data sets generated in simulation and from real-world scenarios. The results show overall improvement of the estimated poses and suggest that depth-limited search presents the best overall performance.	3d modeling;3d pose estimation;a* search algorithm;apache axis;approximation algorithm;computation;computational resource;graph traversal;heuristic;high-level programming language;iterative deepening depth-first search;programming paradigm;refinement (computing);roblog;simulation;state space	Rasoul Mojtahedzadeh;Achim J. Lilienthal	2015	2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2015.7353776	computer vision;simulation;pose;uncertainty;shape;computer science;machine learning;mathematics;depth-limited search;solid modeling;statistics	Robotics	53.64271961022251	-40.66284065923011	162659
cb4d7d19710506e05587542c118ae505db171028	deepvo: a deep learning approach for monocular visual odometry		Deep Learning based techniques have been adopted with precision to solve a lot of standard computer vision problems, some of which are image classification, object detection and segmentation. Despite the widespread success of these approaches, they have not yet been exploited largely for solving the standard perception related problems encountered in autonomous navigation such as Visual Odometry (VO), Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM). This paper analyzes the problem of Monocular Visual Odometry using a Deep Learningbased framework, instead of the regular ’feature detection and tracking’ pipeline approaches. Several experiments were performed to understand the influence of a known/unknown environment, a conventional trackable feature and pre-trained activations tuned for object classification on the network’s ability to accurately estimate the motion trajectory of the camera (or the vehicle). Based on these observations, we propose a Convolutional Neural Network architecture, best suited for estimating the object’s pose under known environment conditions, and displays promising results when it comes to inferring the actual scale using just a single camera in real-time.	autonomous robot;computer vision;convolutional neural network;deep learning;experiment;feature detection (computer vision);network architecture;object detection;real-time clock;simultaneous localization and mapping;structure from motion;visual odometry	Vikram Mohanty;Shubh Agrawal;Shaswat Datta;Arna Ghosh;Vishnu Dutt Sharma;Debashish Chakravarty	2016	CoRR		computer vision;simulation;visual odometry;machine learning;odometry	Robotics	46.77237627590747	-40.57547657779523	163031
eb4ca701b213d1b657dba75f839c88f6eb0eb848	mobile robot map building from time-of-flight camera	mobile robot;occupancy grid;mapping;time of flight tof camera	A map building algorithm for mobile robots is introduced in this paper. The perceived environment is represented in a map containing in each cell a probability of presence of an object or part of an object. The environment is represented as a collection of modular occupancy grids which are added to the map as far as the mobile robot finds objects outside the existing grids. In this approach a time-of-flight (ToF) camera is exploited as a range sensor for mapping. Indeed, one of the areas where ToF sensors are adequate is in obstacle avoidance, because the detection region is not only horizontal but also vertical, allowing to detect obstacles with complex shapes. The main steps of the map building algorithm are extensively described in the paper. The results of testing the algorithm are considered in two different indoor environments. 2012 Elsevier Ltd. All rights reserved.	algorithm;mobile robot;obstacle avoidance;sensor;time-of-flight camera	Sergio Almansa-Valverde;José Carlos Castillo;Antonio Fernández-Caballero	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.02.006	mobile robot;computer vision;simulation;computer science;artificial intelligence;machine learning;occupancy grid mapping;computer graphics (images)	Robotics	51.10898406550592	-38.71078269359672	163589
ca803486c01afe6fffcfa6f063f053ea194d9563	pedestrian detection and localization using 3d range data	ions;biological system modeling;two dimensional displays;laser radar;three dimensional displays;laser modes	This paper presents an approach to exploit three-dimensional laser range finder to detect and locate pedestrian over time. The sensor is employed on a vehicle. The approach uses geometric and motion features to detect human signatures. According to the different characteristics in different height of the human body to establish a model of human body. We establish a number of sample models to reflect the diversity of the human body, to improve the probability of a variety of pedestrians detection. Moving objects should be matched with the human body model. Experimental carry out with 3D data illustrate the efficiency and accuracy of our approach.	pedestrian detection;type signature	Bin Li;Jiadong Shi;Minghe Cao;Rongkai Zhang;Jianzhong Wang	2016	2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)	10.1109/ROBIO.2016.7866473	lidar;computer vision;simulation;engineering;optics;ion	Robotics	51.011481798945894	-39.58884509282417	164097
0a9b390648f0e6e6b7f99b0ae26fa9a5e12ef6bb	semantic interpretation of mobile laser scanner point clouds in indoor scenes using trajectories				Shayan Nikoohemat;Michael Peter;Sander Oude Elberink;George Vosselman	2018	Remote Sensing	10.3390/rs10111754		HCI	52.17648843113273	-42.71763234721798	164717
4638761485f98cc104e23f229ac295b2957d802c	robust orientation estimate via inertial guided visual sample consensus		This paper presents a novel orientation estimate approach named inertial guided visual sample consensus (IGVSAC). This method is intentionally designed for capturing the orientation of human body joints in free-living environments. Unlike the traditional visual-based orientation estimation methods, where outliers among putative image-pair correspondences are removed based on hypothesize-and-verify models such as the computationally costly RANSAC, our approach novelly exploits prior motion information (i.e., rotation and translation) deduced from the quick-response inertial measurement unit (IMU) as the initial body pose to assist camera in removing hidden outliers. In addition, our IGVSAC algorithm is able to ensure estimation accuracy even in the presence of a large quantity of outliers, thanks to its capability of rejecting apparent mismatches. The estimated orientation from the visual sensor is, in turn, able to correct long-term IMU drifts. We conducted extensive experiments to verify the effectiveness and robustness of our IGVSAC algorithm. Comparisons with highly accurate VICON and OptiTrack Motion Tracking Systems prove that our orientation estimate system is quite suitable for capturing human body joints.	algorithm;experiment;random sample consensus;tracking system	Yinlong Zhang;Wei Liang;Yang Li;Haibo An;Jindong Tan	2017	Personal and Ubiquitous Computing	10.1007/s00779-017-1040-2	robustness (computer science);ransac;computer science;outlier;inertial frame of reference;body joints;inertial measurement unit;computer vision;artificial intelligence;match moving	Vision	53.637524875943896	-41.517554487676904	164935
0d7dfcc9667137f8b8bb3effc9d04de5c5208aae	target localisation based on dynamic pyroelectric infrared sensor	localisation model;detecting angle;detecting line;pyroelectric infrared sensor;dynamic	Pyroelectric infrared sensor is highly precise in direction-finding, but not in distance measurement because of the influence of the complex environment. In the case of only considering the target direction, detecting angle and detecting line are defined, and a target localisation model based on detecting line sequence was developed. The model's solution is analysed in detail. When at least four detecting lines can be obtained, among which two existing parallel detecting lines are not parallel with at least two of the others, the target's moving trajectory could be determined uniquely. In the linear and curvilinear motion experiment, the maximum error is 0.35 m and 0.45 m, respectively. The findings demonstrated the effectiveness of the method.		Ze-Bing Wang;Wei Yang;Li Qin	2014	IJWMC	10.1504/IJWMC.2014.062038	computer vision	Vision	49.693285511212935	-41.43402289075461	165068
5898d86bfaa0e6ef61cad05d82992ea2baa7e1d0	learning to detect visual grasp affordance	grasping;training;pose estimation affordance autonomous agent grasping machine learning object detection;training data;willow garage pr2 robot visual grasp affordance estimation local texture like measures object category measures continuous pose estimates grasp point locations max margin optimization category level continuous pose regression autonomous object detection grasping system;estimation;pipelines;robots;object detection;estimation robots pipelines grasping training training data object detection;robot vision image texture object detection pose estimation regression analysis	Appearance-based estimation of grasp affordances is desirable when 3-D scans become unreliable due to clutter or material properties. We develop a general framework for estimating grasp affordances from 2-D sources, including local texture-like measures as well as object-category measures that capture previously learned grasp strategies. Local approaches to estimating grasp positions have been shown to be effective in real-world scenarios, but are unable to impart object-level biases and can be prone to false positives. We describe how global cues can be used to compute continuous pose estimates and corresponding grasp point locations, using a max-margin optimization for category-level continuous pose regression. We provide a novel dataset to evaluate visual grasp affordance estimation; on this dataset we show that a fused method outperforms either local or global methods alone, and that continuous pose estimation improves over discrete output models. Finally, we demonstrate our autonomous object detection and grasping system on the Willow Garage PR2 robot.	2.5d;3d pose estimation;autonomous robot;clutter;grasp;grasp (object-oriented design);mathematical optimization;object detection;scope (computer science);social affordance;willow	Hyun Oh Song;Mario Fritz;Daniel Göhring;Trevor Darrell	2016	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2015.2396014	robot;computer vision;training set;estimation;simulation;3d pose estimation;computer science;artificial intelligence;machine learning;pipeline transport;statistics	Robotics	47.934411498800564	-39.49790320255131	165590
2799d53ca80d67f104bef207a667fa12b4c59d62	multiple-person tracking for a mobile robot using stereo		This paper describes a method of detecting and tracking two or more persons from images captured with a moving camera. We propose a method of person tracking using distance information calculated with a stereo camera. Each person’s position is estimated by using Kalman filter in consideration of movement of a mobile robot and persons. Additionally, we report an experimental result on control of mobile robot following a specific person.	kalman filter;mobile robot;sensor;stereo camera	Junji Satake;Jun Miura	2009			computer vision;simulation;tracking system	Robotics	48.78234794440196	-41.074335727782206	166062
68184265e5bf8dc2ebe7edbd234e562b42b6e908	hands and face tracking for vr applications	tracking system;corresponding author;real time;virtual reality;perceptual user interfaces;data association;face tracking;motion capture;skin color;color segmentation;visual tracking;3d reconstruction	In this paper, we present a robust real-time 3D tracking system of human hands and face. This system can be used as a perceptual interface for virtual reality activities in a workbench environment. The main advantage of our system is that the human, that is in front of the virtual reality device, do not needs any type of marker or special suite. The system includes a real time color segmentation module to detect in real-time the skin-color pixels present in the images. The results of this skin-color segmentation are skin-color blobs that are the inputs of a data association module that labels the blobs pixels using a set of object state hypothesis from previous frames. The 2D tracking results are used for the 3D reconstruction of hands and face in order to obtain the 3D positions of these limbs. Finally, we present several results using the H-ANIM standard to show the system output performance.	3d reconstruction;anim;algorithm;computer graphics;correspondence problem;heuristic;human computer;humanoid animation;human–computer interaction;image segmentation;pixel;real-time clock;real-time computing;tracking system;virtual reality;workbench	Xavier Varona;Jose Maria Buades Rubio;Francisco J. Perales López	2005	Computers & Graphics	10.1016/j.cag.2004.12.002	3d reconstruction;computer vision;facial motion capture;motion capture;simulation;tracking system;eye tracking;computer science;artificial intelligence;virtual reality;computer graphics (images)	Vision	47.111222672787235	-42.86387650667488	166086
1b17a1350a4dddf238dee2a1f8f2c6fd4504ed61	vision-based global localization for mobile robots with hybrid maps of objects and spatial layouts	object recognition;mobile robot;laser range finder;visual cues;scan matching;indoor environment;stereo vision;global localization;visual features;spatial relationships;point cloud;depth map;hybrid map;invariant feature;pose estimation	0020-0255/$ see front matter 2009 Published b doi:10.1016/j.ins.2009.06.030 * Corresponding author. Fax: +82 2 958 5629. E-mail address: skee@kist.re.kr (S.-K. Park). This paper presents a novel vision-based global localization that uses hybrid maps of objects and spatial layouts. We model indoor environments with a stereo camera using the following visual cues: local invariant features for object recognition and their 3D positions for object pose estimation. We also use the depth information at the horizontal centerline of image where the optical axis passes through, which is similar to the data from a 2D laser range finder. This allows us to build our topological node that is composed of a horizontal depth map and an object location map. The horizontal depth map describes the explicit spatial layout of each local space and provides metric information to compute the spatial relationships between adjacent spaces, while the object location map contains the pose information of objects found in each local space and the visual features for object recognition. Based on this map representation, we suggest a coarse-to-fine strategy for global localization. The coarse pose is estimated by means of object recognition and SVDbased point cloud fitting, and then is refined by stochastic scan matching. Experimental results show that our approaches can be used for an effective vision-based map representation as well as for global localization methods. 2009 Published by Elsevier Inc.	apache axis;depth map;fax;mobile robot;outline of object recognition;point cloud;stereo camera	Soonyong Park;Soohwan Kim;Mignon Park;Sung-Kee Park	2009	Inf. Sci.	10.1016/j.ins.2009.06.030	spatial relation;mobile robot;computer vision;simulation;pose;sensory cue;computer science;stereopsis;cognitive neuroscience of visual object recognition;point cloud;3d single-object recognition;depth map	Robotics	51.29784935202752	-38.641858071314225	166294
4b5e2e9c73a54000d067378c495fd1c62e94734f	high-speed object tracking across multiple networked cameras	cameras delays synchronization target tracking protocols machine vision vehicles;computer vision;object tracking;ptp high speed object tracking multiple networked cameras networked vision system multiple camera views frame acquisition personal computer ethernet link tracked target position timestamp precision time protocol;object tracking computer vision local area networks;local area networks	This paper presents a networked vision system for tracking objects moving across multiple camera views at high speed. Frame acquisition in each camera is triggered by a personal computer (PC), which also computes the target position and sends data to another PC via an Ethernet link. The data includes the tracked target position and the timestamp when the frame acquisition was triggered. The timescale on which the timestamps are based is precisely synchronized between the PCs by using the precision time protocol (PTP). Therefore, locally acquired data in each PC and data received via Ethernet can be precisely aligned in time. A 5 mm-diameter object moving across two cameras at 4000 mm/s at a distance of 300 mm from the cameras was successfully tracked by the system with an average frame rate of more than 720 frames-per-second.	experimental system;network switch;order by;personal computer;precision time protocol;weatherstar	Akihito Noda;Yuji Yamakawa;Masatoshi Ishikawa	2013	Proceedings of the 2013 IEEE/SICE International Symposium on System Integration	10.1109/SII.2013.6776743	embedded system;computer vision;real-time computing;computer science	Robotics	49.01142975099912	-43.15515596019702	167202
c24276f0945598b92ee2e25f4d3fed2601cb0485	control of a ptz camera in a hybrid vision system	cameras geometry calibration machine vision robot vision systems mathematical model;hybrid vision system fisheye camera ptz target detection	In this paper, we propose a new approach to steer a PTZ camera in the direction of a detected object visible from another fixed camera equipped with a fisheye lens. This heterogeneous association of two cameras having different characteristics is called a hybrid stereo-vision system. The presented method employs epipolar geometry in a smart way in order to reduce the range of search of the desired region of interest. Furthermore, we proposed a target recognition method designed to cope with the illumination problems, the distortion of the omnidirectional image and the inherent dissimilarity of resolution and color responses between both cameras. Experimental results with synthetic and real images show the robustness of the proposed method.	closed-circuit television;distortion;domain generation algorithm;epipolar geometry;fisheye;image rectification;image resolution;lattice problem;molecular descriptor;omnidirectional camera;pan–tilt–zoom camera;region of interest;robotics;stereopsis;synthetic intelligence;template matching	François Rameau;Cédric Demonceaux;Desire Sidibé;David Fofi	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004734703970405	stereo cameras;computer vision;machine vision;computer graphics (images)	Vision	51.21616036024694	-39.98229330790243	167365
2f467e0d8c8ddfd36e7367b02e493a3bdd84748e	development of a motion analysis system and human-machine interaction through digital image processing and virtual reality	motion analysis;rehabilitation;human computer interaction;digital image processing;image segmentation;pixel tracking cameras image segmentation virtual environment;patient rehabilitation;rehabilitation routines motion analysis system human machine interaction digital image processing virtual reality artificial vision biomechanical analysis;virtual reality;motion estimation;rehabilitation routines;virtual reality human computer interaction image segmentation motion estimation patient rehabilitation;motion capture;human motion;human body;pixel;virtual reality tracking human computer interaction rehabilitation;human motion analysis;quantitative analysis;virtual environment;biomechanical analysis;artificial vision;cameras;human machine interaction;tracking;motion analysis system	Human motion analysis using artificial vision is an interesting topic in the biomechanical analysis of the human body for rehabilitation routines. The people with motor disorders can achieve better and faster results if a quantitative analysis of patient progress is carried out by precision. Nowadays, there are many systems capable to capture human body movements, however these tools can be invasive, uncomfortable and expensive, reducing the use and application of this technology in rehabilitation. This paper proposes a system of motion capture and analysis through digital image processing techniques, using markers to detect and track points. Moreover, the kinematics of the human body and virtual environments are proposed to obtain valuable information about the human motion during the rehabilitation.	computer vision;digital image processing;human–computer interaction;kinesiology;motion capture;virtual reality	Rubén Posada-Gómez;Oscar Osvaldo Sandoval-Gonzalez;Erika Sanchez-Muniz	2011	CONIELECOMP 2011, 21st International Conference on Electrical Communications and Computers	10.1109/CONIELECOMP.2011.5749375	computer vision;motion capture;human body;simulation;computer science;quantitative analysis;engineering;virtual machine;digital image processing;motion estimation;virtual reality;tracking;image segmentation;pixel;computer graphics (images)	Robotics	47.82723518040629	-43.91718417295311	167671
2e61b6e73bdc2ddfc7c9dd947ee9b1b540dc492a	active rough shape estimation of unknown objects	shape estimation;optimisation;grasping;uncertainty;non linear optimization;multiple views;medical robotics;handicapped aids;assistant robot;one click grasping tool;robot vision;shape;estimation;cameras grasping estimation shape three dimensional displays uncertainty jacobian matrices;three dimensional displays;nonlinear optimization rough shape estimation one click grasping tool assistant robot;next best view;robot vision handicapped aids medical robotics object detection optimisation;rough shape estimation;nonlinear optimization;jacobian matrices;cameras;everyday life;object detection;active vision	This paper presents a method to determine the rough shape of an object. This is a step in the development of a ldquoone click grasping toolrdquo, a grasping tool of everyday-life objects for an assistant robot dedicated to elderly or disabled. The goal is to determine the quadric that approximates at best the shape of an unknown object using multi-view measurements. Non-linear optimization techniques are considered to achieve this goal. Since multiple views are necessary, an active vision process is considered in order to minimize the uncertainty on the estimated parameters and determine the next best view. Finally, results that show the validity of the approach are presented.	active vision;free viewpoint television;linear programming;mathematical optimization;rough set	Claire Dune;Éric Marchand;Christophe Collewet;Christophe Leroux	2008	2008 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2008.4651005	active shape model;computer vision;estimation;simulation;active vision;uncertainty;nonlinear programming;shape;computer science;artificial intelligence;statistics	Robotics	47.815680940908905	-43.594724628734724	167741
59467f78c574bbc3625a78eb92a698c6fcb5cd4f	vision based mobile node localization using a landmark	mobile nodes;mobile communication robot kinematics equations vectors automation mobile robots;webcam;localization;geometry;projective geometry;mobile robots;distance measurement;robot vision;mobile robot vision based mobile node localization object shape changes object size changes trigonometric concepts projective geometry concepts position coordinates localisation using landmarks;mobile node;localization mobile nodes webcam;robot vision distance measurement geometry mobile robots	Mobile nodes localization scheme using a landmark is proposed. It is based on the shape/size changes of object with its perspective view and trigonometric concepts. It incorporates vision based localization and projective geometry concepts and elaborates that how shape/ size changes of an object by varying distances/ view angle with changed position provides distinct position coordinates. The problem has been modeled to confirm authenticity of proposed solution and various features have been critically brought to lime light.	internationalization and localization;lime	Shahid Ahmed Bangash;Abdul Ghafoor	2011	The 5th International Conference on Automation, Robotics and Applications	10.1109/ICARA.2011.6144891	mobile robot;computer vision;projective geometry;simulation;internationalization and localization;computer science;mathematics;geometry	Robotics	50.89786374885119	-39.40618834070316	171252
ddd35c55a0f31f7efddf0917c79c8ad87bafde72	object recognition and formation of hand shape in human grasping movements	object recognition		outline of object recognition	Hiroshi Fukuda;Naohiro Fukumura;Masazumi Katayama;Yoji Uno	1998			machine learning;artificial intelligence;artificial neural network;computer science;cognitive neuroscience of visual object recognition	Robotics	48.447972633323246	-38.651123768418834	171841
5adc441a24c52ee16e14118fd309cfabe3cdb644	person following robot behavior using deep learning.		Human-robot interaction (HRI) is a field with growing impact as robot applications are entering into homes, supermarkets and general human environments. Person following is an interesting capability in HRI. This paper presents a new system for a robust person following behavior inside a robot. Its perception module addresses the person detection on images using a pretrained TensorFlow SSD Convolutional Neural Network which provides robustness even on tough lighting conditions. It also includes a face detector and a FaceNet CNN to reidentify the target person. Care has been put to allow real-time operation. The control module implements two PID controllers for a reactive smooth response, moving the robot towards the target person without distracting with other people around. The entire system has been experimentally validated on a real TurtleBot2 robot, with an Asus Xtion RGBD camera.		Ignacio Condés;José María Cañas	2018		10.1007/978-3-319-99885-5_11	robot;pid controller;convolutional neural network;deep learning;perception;robustness (computer science);artificial neural network;behavior-based robotics;computer vision;artificial intelligence;computer science	Robotics	46.67229075045105	-40.63292292851978	171842
aa5698b17174afc56150e538555cbbe1a293bf53	dynamic aspects in active vision	active vision	Abstract   The term  active  stresses the role of the motion or, generally speaking, the dynamic interaction of the observer with the environment. This concept emphasizes the relevance of determining scene properties from the temporal evolution of image features. Within the active vision approach, two different aspects are considered: the advantages of space-variant vision for dynamic visual processing and the qualitative analysis of optical flow. A space-variant sampling of the image plane has many good properties in relation to active vision. In particular, two examples are presented in which a log-polar representation is used for active vergence control and to estimate the time-to-impact during tracking egomotion. These are just two modules which could fit into a complete active vision system, but already highlight the advantages of space-variant sensing within the active vision paradigm. In the second part of the paper the extraction of qualitative properties from the optical flow is discussed. A new methodology is proposed in which optical flows are analyzed in terms of anomalies as unexpected velocity patterns or inconsistencies with respect to some predicted dynamic feature. Two kinds of knowledge are necessary: about the dynamic of the scene (for example, the approximate motion of the camera) and about the task to be accomplished, which is analogous to (qualitatively) knowing the kind of scene I would expect to see. The first simply implies the measurement of some motion parameters (for example, with inertial sensors) directly on the camera, or to put some constraints in the egomotion. The second requirement implies that the visual process is task-driven. Some examples are presented in which the method is successfully applied to robotic tasks.	active vision	Massimo Tistarelli;Giulio Sandini	1992	CVGIP: Image Understanding	10.1016/1049-9660(92)90089-L	computer vision;structure from motion;simulation;active vision;computer science;artificial intelligence	Vision	51.46297392666941	-40.57261083739277	172027
50f4486ddd09e45dfb767a7ee82db487a1ecd413	accuracy of free-space detection: monocular versus binocular vision	automobiles;estimation;roads;stereo vision;transforms;algorithm design and analysis	Autonomous on-road vehicles or vision-based driver assistance benefit from free-space analysis. This paper evaluates the accuracy of free-space detection in stereo and monocular vision on KITTI benchmark data. Such an evaluation of low-level computer vision algorithms is, for example, also necessary as free-space analysis is recently becoming an important module for designing vehicle test beds. The novelty of this paper is defined by comparing a designed monocular algorithm with a selected binocular algorithm on long sequences of images, taken for different road profiles and lighting conditions. The results demonstrate potentials of monocular vision as applicable, for example, if using mobile devices only. Furthermore, this paper extends the detection of a lower envelop in a v-disparity image, usually done by estimating a straight line or B-spline, by using polynomial curve fitting.	algorithm;b-spline;benchmark (computing);binocular disparity;binocular vision;camera resectioning;computer vision;curve fitting;high- and low-level;mobile device;polynomial;requirement;stereopsis;testbed	Noor Haitham Saleem;Reinhard Klette	2016	2016 International Conference on Image and Vision Computing New Zealand (IVCNZ)	10.1109/IVCNZ.2016.7804422	computer stereo vision;algorithm design;computer vision;estimation;simulation;computer science;stereopsis;statistics;computer graphics (images)	Vision	52.71839146989808	-41.70973677311417	173230
d43b354d7e438e2b6c7527467848a4057a6e1a2c	self-calibration and control of a ptz camera based on a spherical mirror	cameras mirrors layout visualization intrusion detection equations video surveillance application software focusing signal analysis;self calibration process;mirrors;computer supervised intrusion detection self calibration process ptz camera spherical mirror video surveillance;video surveillance;tracking ptz camera spherical mirror self calibration;computer supervised intrusion detection;intrusion detection;ptz camera;spherical mirror;visualization;trajectory;mathematical model;video surveillance cameras security of data;self calibration;calibration;security of data;cameras;tracking	In video surveillance applications, PTZ cameras can focus and analyze in details specific zones of the scene. In a computer supervised intrusion detection, a single PTZ camera is unable to visualize the entire scene at once. This article proposes an original solution to this problem, by using an additional spherical mirror. Besides the equations needed to control the PTZ camera, this article presents also a self calibration processes of the camera with the mirror.	closed-circuit television;intrusion detection system;pan–tilt–zoom camera;time-of-flight camera	Lionel Robinault;Ionel Pop;Serge Miguet	2009	2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance	10.1109/AVSS.2009.37	intrusion detection system;computer vision;camera auto-calibration;calibration;camera resectioning;simulation;visualization;computer science;trajectory;mathematical model;tracking;curved mirror;computer graphics (images)	Vision	49.43077276511937	-41.733422368132835	173571
4f4dbfe507209aef3c73da6fb3193621017dfc82	real-time upper body 3d pose estimation from a single uncalibrated camera	3d virtual environment;hierarchical structure;human movement;human computer interaction;real time;3d pose estimation;motion capture data;motion capture;human body	This paper outlines a method of estimating the 3D pose of the upper human body from a single uncalibrated camera. The objective application lies in 3D Human Computer Interaction where hand depth information offers extended functionality when interacting with a 3D virtual environment, but it is equally suitable to animation and motion capture. A database of 3D body configurations is built from a variety of human movements using motion capture data. A hierarchical structure consisting of three subsidiary databases, namely the frontal-view Hand Position (top-level), Silhouette and Edge Map Databases, are pre-extracted from the 3D body configuration database. Using this hierarchy, subsets of the subsidiary databases are then matched to the subject in real-time. The examples of the subsidiary databases that yield the highest matching score are used to extract the corresponding 3D configuration from the motion capture data, thereby estimating the upper body 3D pose.	3d pose estimation;database;human computer;human–computer interaction;motion capture;polygonal modeling;real-time clock;real-time transcription;virtual reality	Antonio S. Micilotta;Eng-Jon Ong;Richard Bowden	2005		10.2312/egs.20051019	computer vision;simulation;3d pose estimation;articulated body pose estimation;computer graphics (images)	Vision	47.14532812979008	-42.72477783422648	173908
a8d9ed0ed4b3ee4e4655cbd8f98999882b05613d	a 3d classifier trained without field samples	solid modelling cad computational geometry image classification image segmentation;image segmentation;cad;computational geometry;image classification;velodyne sensor alignment based 3d classifier 3d geometric model computer aided design cad global 3d template matching segmentation technique sparse 3d data;iterative closest point algorithm buildings accuracy training solid modeling shape principal component analysis;solid modelling	This paper presents a 3D classifier that is shown to maintain performance whether trained with real sensor data from the field or purely trained with 3D geometric (Computer Aided Design, CAD, like) models (downloaded from the Internet for instance). The proposed classifier is a global 3D template matching technique which exploits the location of the ground surface for more accurate alignment. The segmentation and position of the ground is given by the segmentation technique in [7] (which does not assumed the ground to be flat). The proposed classifier outperforms Spin Image and Fast Point Feature Histogram (FPFH) based classifiers by up to 30% (the latter being tested at different scales), in the case of sparse 3D data acquired with a Velodyne sensor. In addition, the experimental results suggest that field samples may not be required in the training set of alignment-based 3D classifiers. This finding implies that the laborious task of gathering hand labelled field data for training may be avoidable for this type of classifier.	computer-aided design;sparse matrix;statistical classification;template matching;test set	Bertrand Douillard;Alastair James Quadros;Peter Morton;James Patrick Underwood;Mark De Deuge	2012	2012 12th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2012.6485261	computer vision;contextual image classification;computational geometry;computer science;machine learning;pattern recognition;cad;image segmentation;scale-space segmentation	Robotics	53.24958156912042	-43.721511542380114	174763
392ab3e4cd629d907f5272c1ae38dd3ce25b5e78	self-maintaining camera calibration over time	vision system;calibration maintaining;stereovision systems self maintaining camera calibration intelligent robotic system performance vision system vibrations thermal expansion;thermal expansion;pose determination;temporal data;computer vision;stereo image processing;dynamic vision;3d vision;camera calibration;cameras calibration robot vision systems intelligent robots focusing machine vision hip intelligent systems robot kinematics thermal expansion;stereo image processing calibration computer vision;calibration	The success of an intelligent robotic system depends on the performance of its vision-system which in turn depends to a great extend upon the quality of its calibration. During the execution of a task the vision-system is subject to external influences such as vibrations, thermal expansion etc. which affect and possibly render invalid the initial calibration. Moreover, it is possible that the parameters of the vision-system like e.g. the zoom or the focus are altered intentionally in order to perform specific vision-tasks. This paper describes a technique for automatically maintaining calibration of stereovision systems over time without using again any particular calibration apparatus. It uses all available information, i.e. both spatial and temporal data. Uncertainty is systematically manipulated and maintained. Synthetical and real data are used to validate the proposed technique, and the results compare very favourably with those given by classical calibration methods.	3d projection;camera resectioning;robot;stereopsis;synthetic data	Zhengyou Zhang;Veit Schenk	1997		10.1109/CVPR.1997.609325	thermal expansion;smart camera;computer vision;calibration;camera resectioning;simulation;machine vision;computer science;temporal database;robot calibration;computer graphics (images)	Robotics	52.73037613706472	-40.69835537073603	175225
71f24f5f02251a33fd2a79b00d997534558360e0	motion from focus	triangulation method;object recognition;image motion analysis;motionless monocular camera triangulation method 3d object motion recognition stereo camera;3d object motion recognition;stereo camera;cameras focusing lenses robot vision systems competitive intelligence layout humans image sensors laboratories stereo vision;motionless monocular camera;object recognition cameras image motion analysis;cameras	Based on the triangulation method, the 3D motion of an object can be completely recognized by a stereo camera. However, the question whether or not the 3D motion of an object can be completely recognized by a motionless/fixed monocular camera is the yet-unanswered question. In this paper we propose a method using a motionless monocular camera of which the focus is changed in cycle to recognize the absolute 3D motion of an object	nonuniform sampling;sampling (signal processing);stereo camera	Huynh Quang Huy Viet;Makoto Sato;Hiromi T. Tanaka	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.815	triangulation;stereo camera;computer vision;camera auto-calibration;structure from motion;camera resectioning;deep-sky object;computer science;cognitive neuroscience of visual object recognition;3d single-object recognition;motion field;computer graphics (images)	Vision	51.475557390930774	-41.47399768150961	175226
19c8759bc3190a75c3e97f39bd6979b10b0bd483	video segmentation and stabilization for ballcam	ballcam;video stabilization;activity segmentation	We present a video stabilization algorithm for ball camera systems that undergo extreme egomotion during sports play. In particular, we focus on the BallCam system which is an American football embedded with an action camera at the tip of the ball. We propose an activity-aware video stabilization algorithm which is able to understand the current activity of the BallCam, which uses estimated activity labels to inform a robust video stabilization algorithm. Activity recognition is performed with a deep convolutional neural network, which uses optical flow.	activity recognition;algorithm;artificial neural network;convolutional neural network;embedded system;optical flow;visual odometry	Ryohei Funakoshi;Vishnu Naresh Boddeti;Kris Makoto Kitani;Hideki Koike	2017		10.1145/3041164.3041210	computer vision;simulation;geography;video tracking;computer graphics (images)	Vision	50.68869438115164	-43.138169173951454	175381
77d3f881014d8aa40404b34034bfee1ceade65ac	sensor fusion and play strategy programming for micro soccer robots	micro soccer robot;vision system;lens distortion;microrobots;image distortion;yuv threshold;image segmentation;image processing;sensor fusion robot programming robot sensing systems robot kinematics machine vision robot vision systems image processing cameras lenses calibration;robot vision system;robot vision;play strategy programming;camera calibration sensor fusion play strategy programming micro soccer robot robot vision system yuv threshold object position correction image distortion image calibration image processing collision avoidance color classification;image colour analysis;multi robot systems;sport collision avoidance image colour analysis image segmentation microrobots multi robot systems robot programming robot vision sensor fusion;image calibration;object position correction;collision avoidance;robot soccer;sport;sensor fusion;camera calibration;color classification;camera calibration robot soccer color classification;robot programming;light adaptation	Our research focused on the programming of the vision system and strategy for a soccer robot system. For the vision system, the varying lighting conditions of the robot soccer field attributed to problems in identify colors of different objects on the field. We developed a light adaptable vision system by applying the YUV threshold method. We also proposed an object position correction approach to deal with the lens distortion in the image. This approach avoided the time-consuming image calibration process, by adjusting the object positions after image processing. Experimental results showed that our vision system had a high recognition rate and accurate position computation even under changing lighting conditions. The development of a strategy program to avoid collision is also covered in this paper	color;computation;distortion;image processing	Fanny F. L. Tong;Zoe Y. M. Chong;Max Q.-H. Meng	2004	2004 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2004.1521892	computer vision;simulation;image processing;computer science;engineering;sport;sensor fusion;image segmentation;computer graphics (images)	Robotics	49.59816082050847	-40.58104163424385	176076
2c44093ace4d91085c36f6f47850f5921d7e9b4a	redundancy reduction in 3d facial motion capture data for animation	motion capture data;motion capture;low latency;3d mesh simplification;real time animation;feature analysis;visual attention	Research on the perception of dynamic faces often requires real-time animations with low latency. With an adaptation of principal feature analysis [Cohen et al. 2002], we can reduce the number of facial motion capture markers by 50%, while retaining the overall animation quality.	facial motion capture;real-time clock	Daniela I. Wellein;Cristóbal Curio;Heinrich H. Bülthoff	2007		10.1145/1272582.1272613	pattern recognition;computer vision;facial motion capture;motion capture;simulation;computer science;operating system;computer graphics (images);low latency	Graphics	48.618621571867884	-43.37629947409195	176370
98a834fc4bf8d2d17d8fdacf0d2c094d39a1c762	a sensor-fusing system for spatial circle and trunk parameter estimation	sensor fusing;object parameter estimation;inertial sensor	This paper proposes a fusing system consisting of a camera, a line laser, and an inertial measurement unit (IMU) for spatial circle and trunk parameter estimation. The line laser provides a stripe on the object surface that is captured by the camera. When using the system to scan an object, the 3D coordinate points on the surface of the scanned object are calculated with respect to a world coordinate frame by fusing the system movement and laser point coordinates, where the system movement is estimated from the IMU data by solving a smoothing optimization problem. Based on the information from the scanning process, algorithms are derived for detecting the spatial circle’s diameter and the trunk’s parameters. We also performed experiments to verify the feasibility of the system in real applications.	estimation theory	Quoc Khanh Dang;Young Soo Suh	2016	Journal of Circuits, Systems, and Computers	10.1142/S0218126616501176	computer vision;simulation	EDA	49.675124292382726	-41.80503184498431	176859
24ab7531a3fb058c8dbb11ef37dd31b297ff1bcf	combining 2d and 3d features to improve road detection based on stereo cameras			stereo camera;stereo cameras	Guorong Cai;Songzhi Su;Wenli He;Yun-Dong Wu;Shaozi Li	2018	IET Computer Vision	10.1049/iet-cvi.2017.0266	computer vision;artificial intelligence;stereo cameras;mathematics	Vision	51.92262056325284	-42.74871235919838	178582
428bd37ab8f60df54919aacd0474a4d2905368a7	3d based landmark tracker using superpixels based segmentation for neuroscience and biomechanics studies		Examining locomotion has improved our basic understanding of motor control and aided in treating motor impairment. Mice and rats are premier models of human disease and increasingly the model systems of choice for basic neuroscience. High frame rates (250 Hz) are needed to quantify the kinematics of these running rodents. Manual tracking, especially for multiple markers, becomes timeconsuming and impossible for large sample sizes. Therefore, the need for automatic segmentation of these markers has grown in recent years. Here, we address this need by presenting a method to segment the markers using the SLIC superpixel method. The 2D coordinates on the image plane are projected to a 3D domain using direct linear transform (DLT) and a 3D Kalman filter has been used to predict the position of markers based on the speed and position of markers from the previous frames. Finally, a probabilistic function is used to find the best match among superpixels. The method is evaluated for different difficulties for tracking of the markers and it achieves 95% correct labeling of markers.	3d modeling;coefficient;dlt;image plane;kalman filter;thresholding (image processing)	Omid Haji Maghsoudi;Andrew Spence	2017	CoRR		neuroscience;pattern recognition;frame rate;computer science;probabilistic logic;kalman filter;image plane;landmark;artificial intelligence;direct linear transformation;kinematics;computer vision;segmentation	Vision	49.88101722619312	-44.72620837123215	178680
fd774fd5bfef33717c4fa1f01b1b0e9d5ecd8a0e	optic flow-based vision system for autonomous 3d localization and control of small aerial vehicles	vision system;uav;control algorithm;visual slam;small aerial vehicle control;robust estimator;information extraction;aerial real time implementation;real time;unmanned aerial vehicle;low resolution;kalman filter;inertial measurement unit;autonomous 3d localization;autonomous localization;optic flow based vision;flight guidance and control;nonlinear control system;optical flow;guidance and control;3d vision;depth map;structure from motion;optic flow;uav 3d optic flow;structure from motion sfm	The problem considered in this paper involves the design of a vision-based autopilot for small and micro Unmanned Aerial Vehicles (UAVs). The proposed autopilot is based on an optic flow-based vision system for autonomous localization and scene mapping, and a nonlinear control system for flight control and guidance. This paper focusses on the development of a real-time 3D vision algorithm for estimating optic flow, aircraft self-motion and depth map, using a low-resolution onboard camera and a low-cost Inertial Measurement Unit (IMU). Our implementation is based on 3 Nested Kalman Filters (3NKF) and results in an efficient and robust estimation process. The vision and control algorithms have been implemented on a quadrotor UAV, and demonstrated in real-time flight tests. Experimental results show that the proposed vision-based autopilot enabled a small rotorcraft to achieve fully-autonomous flight using information extracted from optic flow. © 2009 Elsevier B.V. All rights reserved.	aerial photography;algorithm;autonomous robot;autopilot;binary prefix;computation;control system;depth map;experiment;global positioning system;image quality;kalman filter;lateral thinking;modulo operation;nonlinear system;nvidia 3d vision;optical flow;real-time clock;reflection mapping;unmanned aerial vehicle	Farid Kendoul;Isabelle Fantoni;Kenzo Nonami	2009	Robotics and Autonomous Systems	10.1016/j.robot.2009.02.001	computer vision;simulation;computer science;optical flow;information extraction	Robotics	53.234072120281866	-38.03072630973364	178943
199b1d3f54f548864924eeadae4935b469c0d8c6	an onboard monocular vision system for autonomous takeoff, hovering and landing of a micro aerial vehicle	monocular vision;tecnologia industrial tecnologia mecanica;tecnologia electronica telecomunicaciones;ellipse;projective geometry;micro aerial vehicle;landing;tecnologias;grupo a;onboard vision	In this paper, we present an onboard monocular vision system for autonomous takeoff, hovering and landing of a Micro Aerial Vehicle (MAV). Since pose information with metric scale is critical for autonomous flight of a MAV, we present a novel solution to six degrees of freedom (DOF) pose estimation. It is based on a single image of a typical landing pad which consists of the letter “H” surrounded by a circle. A vision algorithm for robust and real-time landing pad recognition is implemented. Then the 5 DOF pose is estimated from the elliptic projection of the circle by using projective geometry. The remaining geometric ambiguity is resolved by incorporating the gravity vector estimated by the inertial measurement unit (IMU). The last degree of freedom pose, yaw angle of the MAV, is estimated from the ellipse fitted from the letter “H”. The efficiency of the presented vision system is demonstrated comprehensively by comparing it to ground truth data provided by a tracking system and by using its pose estimates as control inputs to autonomous flights of a quadrotor.	aerial photography;algorithm;autonomous robot;autostereogram;ground truth;real-time clock;six degrees of separation;the circle (file system);tracking system;yaws	Shaowu Yang;Sebastian Scherer;Andreas Zell	2013	Journal of Intelligent and Robotic Systems	10.1007/s10846-012-9749-7	computer vision;projective geometry;simulation;monocular vision;aeronautics;ellipse	Robotics	53.10833121651464	-41.348305823468365	179808
12b5b233ec844929d8cbb45c9e63a26a5854f0fa	efficient gpu-based construction of occupancy girds using several laser range-finders	paper;image processing;intelligent sensors laser fusion grid computing sensor phenomena and characterization sensor systems shape measurement intelligent robots laser modes vehicles sensor fusion;laser ranging calculus computer graphics grid computing image texture;computer graphics;texture mapping;occupancy grid;graphics processor unit;laser ranging;computer graphic;image texture;laser range finder;nvidia geforce fx 5650;calculus;field of view;nvidia;graphical processor unit gpu occupancy grids laser range finders fusion occupancy information different embedded sensors texture mapping computer graphics calculus architecture;algorithms;opengl;grid computing;coordinate system	Building occupancy grids (OGs) in order to model the surrounding environment of a vehicle implies to fusion occupancy information provided by the different embedded sensors in the same grid. The principal difficulty comes from the fact that each can have a different resolution, but also that the resolution of some sensors varies with the location in the field of view. In this article we present a new exact approach to this issue and we explain why the problem of switching coordinate systems is an instance of the texture mapping problem in computer graphics. Therefore we introduce a calculus architecture to build occupancy grids with a graphical processor unit (GPU). Thus we present computational time results that can allow to compute occupancy grids for 50 sensors at frame rate even for a very fine grid. To validate our method, the results with GPU are compared to results obtained through the exact approach	bayesian network;central processing unit;computer graphics;embedded system;facebook platform;graphics processing unit;high- and low-level;image resolution;performance;ray tracing (graphics);real-time clock;robot;robotics;sensor;texture mapping;time complexity	Manuel Yguel;Olivier Aycard;Christian Laugier	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.281817	image texture;texture mapping;embedded system;computer vision;field of view;image processing;computer science;coordinate system;occupancy grid mapping;computer graphics;grid computing;computer graphics (images)	Robotics	51.59408803225353	-39.43079882665325	179832
01662b8ba97cb9aa2426840479b6f9ed00d76a88	vision-based absolute indoor point positioning in the hallway without image database	root mean square vision based absolute indoor point positioning hallway single camera vision based absolute point positioning system;computer vision;navigation;navigation cameras computer vision;navigation image databases cameras transforms root mean square standards handheld computers;cameras	This paper proposes a method for the vision-based absolute point positioning in the hallway using a single camera without image database. Our vision-based absolute point positioning system has less than 1 m error with respect to the root mean square and 0.5 standard deviation.	mean squared error;positioning system	Hyunho Lee;Jae-Hun Kim;Seok Lee;Sanghoon Lee;Taikjin Lee	2013	2013 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2013.6486896	computer vision;navigation;simulation;computer science;computer graphics (images)	Robotics	52.56841652297489	-40.32558988261927	180187
eeb2e3a96a9ec27332bb2c514ea26d97aa52e6b6	sail-map: loop-closure detection using saliency-based features	detectors;feature extraction visualization detectors robots robustness vectors image color analysis;viewpoint change sail map method loop closure detection saliency based features robotic localization robotic navigation topological simultaneous localization and mapping slam camera image matching saliency based feature feature detector feature descriptor image perturbation;visualization;vectors;image color analysis;feature extraction;robots;robustness;slam robots feature extraction mobile robots object detection path planning robot vision	Loop-closure detection, which is the ability to recognize a previously visited place, is of primary importance for robotic localization and navigation problems. We here introduce SAIL-MAP, a method for loop-closure detection based on vision only, applied to topological simultaneous localization and mapping (SLAM). Our method allows the matching of camera images using a novel saliency-based feature detector and descriptor. These features have been designed to benefit from the robustness to viewpoint change and image perturbations of bio-inspired saliency algorithms. Additionally, the same algorithm is used for the detector and descriptor. The results obtained on different large-scale data sets demonstrate the efficiency of the proposed solution for localization problems.	apl;british informatics olympiad;computation;field-programmable gate array;for loop;internationalization and localization;matlab;map;parallel algorithm;robot;simulation;simultaneous localization and mapping;sparse matrix;the c++ programming language	Merwan Birem;Jean-Charles Quinton;François Berry;Youcef Mezouar	2014	2014 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2014.6943206	robot;computer vision;detector;feature detection;visualization;feature extraction;computer science;artificial intelligence;kadir–brady saliency detector;machine learning;pattern recognition;mathematics;feature;robustness	Robotics	51.33684554294115	-38.25131363714516	180478
896fafb65ad2d8f9c184250e1a0c55e8b8ea8cf1	qualitative representation of outdoor environment along route	dynamic programming;color cues;image segmentation;structure map;image sequences robots image segmentation layout pattern matching buildings urban areas color;color;disparity;side view;mobile robots;dynamic program;layout;outdoor environment;dynamic programming based method;urban areas;robot vision;image colour analysis;pattern matching;robots;color cues qualitative representation outdoor environment landmarks side view panoramic representation structure map relative spatial management dynamic programming based method disparity;qualitative representation;relative spatial management;panoramic representation;landmarks;buildings;image sequences;intermediate representation	Thzs paper proposes a n idea of representing qualitatively a route in an. ou tdoor env ironment in t e r m s of landmarks . A robot cont inuous ly scans the sideview and yields a n in termedia te representation called panoramic representation. T h e objects in the scene are segmented based u p o n range and color i n f o r m a t i o n in the panoramic representation, and they are t h e n arranged in to a structure map which prov ides qualitative i n f o r m a t i o n o n t h e e n v i r o n m e n t structure. T h e objects which h,ave distinctive fea tures along the route are selected as the landmarks . S ince their relative spatial arrangement is invar iant w i t h the camera speed, the memorized l a n d m a r k s c a n be matched w i t h the objects in t h e n e w structure m a p by a d y n a m i c programmingbased method. T h e correctness of matching is f u r t h e r verified by using t h e disparity and color cues. Resul t s of exper iments in a real ou tdoor world s h o w t h e effect i veness and robustness of th i s method.	binocular disparity;correctness (computer science);emoticon;env;integrated development environment;landmark point	Shigang Li;Saburo Tsuji;Akira Hayashi	1996		10.1109/ICPR.1996.546014	robot;layout;mobile robot;computer vision;computer science;dynamic programming;pattern matching;image segmentation;intermediate language;computer graphics (images)	Vision	49.760005633555274	-39.22133393899704	180765
040db0da288b5210f43ff6d192f91474ac9143ce	uav, come to me: end-to-end, multi-scale situated hri with an uninstrumented human and a distant uav	detectors;visualization;robots;face;ieee 802 11 standard;unmanned aerial vehicles;cameras	We present the first demonstration of end-to-end far-to-near situated interaction between an uninstrumented human user and an initially distant outdoor autonomous Unmanned Aerial Vehicle (UAV). The user uses an arm-waving gesture as a signal to attract the UAV's attention from a distance. Once this signal is detected, the UAV approaches the user using appearance-based tracking until it is close enough to detect the human's face. Once in this close-range interaction setting, the user is able to use hand gestures to communicate its commands to the UAV. Throughout the interaction, the UAV uses colored-light-based feedback to communicate its intent to the user. We developed this system to work reliably with a low-cost consumer UAV, with only computation off-board. We describe each component of this interaction system, giving details of the depth estimation strategy and the cascade predictive flight controller for approaching the user. We also present experimental results on the performance of the complete system and its individual components.	aerial photography;autonomous robot;computation;computer vision;control system;end system;end-to-end principle;experiment;human–robot interaction;requirement;servo;situated;unmanned aerial vehicle;usability;velocity (software development);video tracking;visual servoing	Valiallah Monajjemi;Sepehr Mohaimenianpour;Richard T. Vaughan	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759649	robot;face;computer vision;detector;simulation;visualization;computer science;engineering;artificial intelligence	Robotics	47.017409298218155	-40.54145129683307	180973
3363639e22da7201fe13f1ec3e426f13bf6e7488	night-time indoor relocalization using depth image with convolutional neural networks	neural networks;color;computer architecture;image color analysis;feature extraction;robot vision systems;cameras	In this work, we present a Convolutional Neural Network(CNN) with depth images as its inputs to solve the relocalization problem of a moving platform in night-time indoor environment. The developed algorithm can estimate the camera pose in an end-to-end manner with 0.40m and 7.49° errors in real time during night. It does not require any geometric computation as it directly uses a CNN for 6 DOFs pose regression. The architecture and its encoding methods of depth images are discussed. The proposed method is also evaluated on benchmark datasets collected from a motion capture system in our lab.	algorithm;artificial neural network;benchmark (computing);computation;computational geometry;convolutional neural network;end-to-end principle;motion capture;neural networks;preprocessor	Ruihao Li;Qiang Liu;Jianjun Gui;Dongbing Gu;Huosheng Hu	2016	2016 22nd International Conference on Automation and Computing (ICAC)	10.1109/IConAC.2016.7604929	computer vision;simulation;feature extraction;computer science;machine learning;artificial neural network;computer graphics (images)	Robotics	51.029765257666526	-44.71682887247239	181027
0b023e4bff678b3fe43e8494fca1dcd1932b1de2	probabilistic 3d mapping based on gnss snr measurements	bayesian estimation probabilistic 3d mapping gnss measurement snr measurement global navigation satellite system physically motivated sensor model satellite signal blockage signal to noise ratio posterior distribution sparse factor graph loopy belief propagation;topography earth bayes methods cartography data visualisation estimation theory geophysical signal processing geophysical techniques inference mechanisms satellite navigation surface topography measurement;global positioning system satellites signal to noise ratio receivers buildings three dimensional displays probabilistic logic	A probabilistic approach to 3-dimensional mapping is proposed that only uses data gathered by GNSS (Global Navigation Satellite System) devices. To accomplish this, the environment is gridded and a physically motivated sensor model is developed that assigns likelihoods of blockage to satellite signals based on their measured SNR (signal-to-noise ratio). It is then shown that the posterior distribution of the map represents a sparse factor graph on which a low complexity implementation of Loopy Belief Propagation can be used for efficient Bayesian estimation. Experimental results are presented which demonstrate our algorithm's ability to coarsely map in three dimensions a corner of a university campus.	android;belief propagation;casio loopy;experiment;factor graph;galileo (satellite navigation);particle filter;randomized algorithm;satellite navigation;scalability;signal-to-noise ratio;simultaneous localization and mapping;sparse matrix	Andrew T. Irish;Jason T. Isaacs;François Quitin;João Pedro Hespanha;Upamanyu Madhow	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6854028	computer vision;gnss augmentation;machine learning	Robotics	53.534779774839954	-42.60569642901438	181258
7bd90fa3b04cbdc64a2be4a7512540d8bdf726b7	robust and efficient mobile robot self-localization using laser scanner and geometrical maps	robot localization;mobile robot;robustness mobile robots object detection costs robust control navigation fuses filters covariance matrix intelligent robots;efficient algorithm;path planning;pose estimation kalman filters mobile robots monte carlo methods object detection path planning;monte carlo simulation methods geometrical maps laser scanner mobile robot self localization object detection pose estimation methods dead reckoning sensors;kalman filters;laser scanner;mobile robots;pose estimation methods;geometrical maps;mobile robot self localization;monte carlo simulation methods;algebraic method;dead reckoning sensors;sum of squares;dead reckoning;monte carlo simulation;monte carlo methods;object detection;pose estimation	In this paper, we present a set of robust and efficient algorithms with O(N) cost for object detection and pose estimation with a laser ranger based on regular geometrical maps. Firstly, a multiple line fitting method is described, related with walls at the environment, which minimizes the sum of squared distances for contiguous lines and constitutes a global pattern with regular constrained angles. Secondly, beacons, related with columns at the environment, are estimated with the circle algebraic method. Two pose estimation methods are presented, based on detected beacons and lines. These methods have proved to be robust and faster than other methods. In addition to this, a redundant kinematic model is used for dead-reckoning sensors, which may improve estimations. Finally, the robot localization is performed with an EKF during normal navigation and a global localization method, based on Monte Carlo simulation methods, at initialization	3d pose estimation;algorithm;automation;column (database);dead reckoning;extended kalman filter;line fitting;linear algebra;map;mobile robot;monte carlo method;monte carlo methods for option pricing;nonlinear system;object detection;particle filter;robotic mapping;robotics;robustness (computer science);sensor;simulation;simultaneous localization and mapping;the circle (file system);ranger	Leopoldo Armesto;Josep Tornero	2006	2006 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2006.282325	mobile robot;computer vision;simulation;computer science;artificial intelligence;machine learning;statistics;monte carlo method	Robotics	53.124270811753156	-38.55355378913249	181835
5043082024760fcfc74cca9059f6775aa7a67170	building recognition based on indirect location of planar landmark in flir image sequences	indirect location;flir image sequences;building recognition;error analysis;planar landmark;perspective transformation	A novel method is proposed to deal with the problem of building recognition in forward-looking infrared (FLIR) image sequences. Two computational models are deduced in this paper: one is the perspective transformation model, through which an image is transformed perspectively from downward-looking state to forward-looking state; the other is the indirect location model, through which the position of a building is computed in an FLIR image. In addition, in order to illustrate the application scope of our method, the error analysis is presented. The proposed approach is validated by extensive experiments, with images taken in different weather conditions, seasons and at different times. Superior recognition results were obtained on three FLIR image sequences we collected.		Dengwei Wang;Tianxu Zhang;Meijun Wan;Wenjun Shi;Longsheng Wei;Xiaoyu Yang	2011	IJPRAI	10.1142/S0218001411008695	computer vision	Vision	51.263380632850385	-41.61434690519685	182944
4a3f1f61b70d914b2089b8b3cc328248de464fc3	an orientation factor for object-oriented slam		Current approaches to object-oriented SLAM lack the ability to incorporate prior knowledge of the scene geometry, such as the expected global orientation of objects. We overcome this limitation by proposing a geometric factor that constrains the global orientation of objects in the map, depending on the objects’ semantics. This new geometric factor is a first example of how semantics can inform and improve geometry in objectoriented SLAM. We implement the geometric factor for the recently proposed QuadricSLAM that represents landmarks as dual quadrics. The factor probabilistically models the quadrics’ major axes to be either perpendicular to or aligned with the direction of gravity, depending on their semantic class. Our experiments on simulated and real-world datasets show that using the proposed factors to incorporate prior knowledge improves both the trajectory and landmark quality.	apache axis;experiment;map;robot;simulation;simultaneous localization and mapping;synthetic intelligence	Natalie Jablonsky;Michael Milford;Niko Sünderhauf	2018	CoRR		control engineering;computer vision;engineering;perpendicular;quadric;semantics;trajectory;object-oriented programming;artificial intelligence	Robotics	50.6609467742965	-41.71123889116185	183097
bca2a4512871ddb2d1a85cb25e7549fc22c3322f	detection of vertical pole-like objects in a road environment using vehicle-based laser scanning data	mobile mapping;traffic sign;tree trunk;feature extraction;vehicle based laser scanning;laser scanning;utility pole	Accurate road environment information is needed in applications such as road maintenance and virtual 3D city modelling. Vehicle-based laser scanning (VLS) can produce dense point clouds from large areas efficiently from which the road and its environment can be modelled in detail. Pole-like objects such as traffic signs, lamp posts and tree trunks are an important part of road environments. An automatic method was developed for the extraction of pole-like objects from VLS data. The method was able to find 77.7% of the poles which were found by a manual investigation of the data. Correctness of the detection was 81.0%.	algorithm;computation;correctness (computer science);location-based service;oblique projection;overfitting;point cloud;primitive recursive function;reference implementation;superheterodyne receiver;von luschan's chromatic scale	Matti Lehtomäki;Anttoni Jaakkola;Juha Hyyppä;Antero Kukko;Harri Kaartinen	2010	Remote Sensing	10.3390/rs2030641	laser scanning;computer vision;simulation;feature extraction;machine learning;optics;remote sensing	HCI	53.35960384132928	-43.50625591512404	183279
b5a681f6514fdbfabd166ea61785d1f3d51150fa	object shape characterisation using a haptic probe		One of the fundamental challenges in robotics is object detection in unstructured environments where objects’ properties are not known a priori. In these environments sensing is susceptible to error. Conventional sensors, for example sonar sensors, video cameras and infrared sensors, have limitations that make them inadequate for successful completion of outdoor tasks where vision is partially or totally impaired. For instance, in harsh atmospheric conditions, the poor image quality that video cameras suffer and the low spatial resolution sonar sensors exhibit make them unsuitable for object detection. Therefore, we propose an alternative approach for object characterisation in unstructured environments using active touch. A simulated anthropomorphic haptic probe system with 3 DOF and a force sensor was used in this approach. The system measures and analyses forces at contact points, and data useful for object characterisation were realised. Two shapes, a sphere and a cuboid, were modelled and used in the object detection simulation and satisfactory results were obtained.	haptic technology	O. P. Odiase;Robert C. Richardson	2005		10.1007/3-540-26415-9_99	cuboid;object detection;sonar;computer vision;image quality;artificial intelligence;haptic technology;computer science;robotics;image resolution	Robotics	52.16526131293913	-40.15800183745502	183551
6648d2d1655ec4dbe286a5d7a53198cec8c12504	mobile robot localization by geometric hashing and model-based scene matching	image features;geometric invariants;weighted distance voting criterion;mobile robot;uncertainty;robot centered coordinates;image matching;path planning;geometric hashing;model based scene matching;matching efficiency mobile robot localization geometric hashing model based scene matching indoor mobile robot geometric invariants robot centered coordinates weighted distance voting criterion intensity image;mobile robots;matching efficiency;layout;navigation;robot vision;indoor mobile robot;voting;hash table;feature extraction;solid modeling;mobile robot localization;mobile robots solid modeling layout robot kinematics dead reckoning uncertainty cameras voting navigation wheels;dead reckoning;sensor fusion;cameras;robot kinematics;wheels;intensity image	This paper describes a new technique for determining the initial location and orientation of an indoor mobile robot by geometric hashing and model-based scene matching. The indoor scenes are first modeled off-line in hashing tables using geometric invariants of some special model edge points. Model-based scene matching is completed in three stages:(l) extraction of scene image features and derivation of the the geometric invariants from these features in robot-centered coordinates, (2) finding the best matched candidate and the corresponding transformation using geometric hashing and a weighted distance voting criterion, and (3) the ver$cation of the match by backprojection of model features onto the intensity image of the scene. Matching eflciency is increased substantially by restricting the number of bases through the ordering of image ,fi?atures und the concurrence o f ,features during geometric hashing. Experimental results show that the proposed method is quite efficient and reliable.	geometric hashing;hash function;line level;mobile robot;online and offline;robotic mapping;scene graph	Yi-Bing Yang;Hung-Tat Tsui	1996		10.1109/ICPR.1996.546015	mobile robot;computer vision;simulation;computer science;machine learning;mathematics;locality preserving hashing	Robotics	52.110415121990606	-39.033082351433116	183901
942ce02033e102a23afa42b5ecf5a6008b034457	fast view-based pose estimation of industrial objects in point clouds using a particle filter with an icp-based motion model		The registration of an observed point set to a known model to estimate its 3D pose is a common task for the autonomous manipulation of objects. Especially in industrial environments, robotic systems need to accurately estimate the pose of objects in order to successfully perform picking, placing or assembly tasks. However, the characteristics of industrial objects often cause difficulties for classical pose estimation algorithms, especially when using IR depth sensors. In this work, we propose to solve ambiguities of the pose estimate by representing the it as a virtual view on a reference model within an adapted particle filter system. Therefore, a simple but fast method to cast views from the reference model is presented, making a training phase obsolete while increasing the accuracy of the estimate. The view-based approach increases the robustness of the registration process and reformulates the pose estimation as a problem of determining the most likely view using a particle filter. By incorporating a local optimizer (ICP) into the dynamics model of the particle filter, the proposed method directs the search in the 6-dimensional pose space, thereby reducing the amount of needed particles to about 50 while decreasing the convergence time to a minimum and therefore making it viable for real-time pose estimation. In contrast to other pose estimation methods, this approach explores the possibilities of sequential pose estimation by only using plain point clouds without additional features.	3d pose estimation;algorithm;autonomous robot;computation;experiment;feature model;mathematical optimization;particle filter;point cloud;real-time clock;reference model;sensor;state space	Bjarne Großmann;Volker Krüger	2017	2017 IEEE 15th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2017.8104794	robustness (computer science);engineering;point cloud;reference model;video tracking;articulated body pose estimation;3d pose estimation;particle filter;pose;computer vision;artificial intelligence	Robotics	53.66974681438457	-40.77991216214037	184616
68c2912bf84f31e6764f81f0a7bf23ee17f39654	bare-fingers touch detection by the button's distortion in a projector–camera system	finger shadows;keyboards;missed detection rate;error reduction;finger height;human computer interaction;edge detection;virtual keyboard;cameras image edge detection thumb transmission line matrix methods mobile handsets equations;touch detection rate;image sensors;ips;touch detection;bare finger touch interaction;button distortion detection;thumb;graphical user interfaces;image edge detection;region of interest extraction;projector camera system;computational complexity;touch action judgment;homography mapping;feature extraction;graphical user interface button;mobile handsets;gui;triangulation;transmission line matrix methods;power point viewing;mobile computing;mobile devices bare finger touch detection button distortion detection projector camera system interactive projection system ips bare finger touch interaction regular planar surfaces graphical user interface button gui finger height region of interest extraction homography mapping computational complexity reduction edge detection algorithm error reduction finger shadows touch action judgment virtual keyboard power point viewing missed detection rate false detection rate touch detection rate human computer interaction;computational complexity reduction;regular planar surfaces;edge detection projector camera system human computer interaction touch detection triangulation;interactive systems;edge detection algorithm;mobile devices;interactive projection system;false detection rate;cameras;object detection;object detection computational complexity edge detection feature extraction graphical user interfaces human computer interaction image sensors interactive systems keyboards mobile computing;bare finger touch detection	In this paper, we propose a novel interactive projection system (IPS), which enables bare-finger touch interaction on regular planar surfaces (e.g., walls, tables), with only one standard camera and one projector. The challenge of bare-finger touch detection is recovering the touching information just from the 2-D image captured by the camera. In our method, the graphical user interface (GUI) button is projected on the surface and is distorted by the finger when clicking it, and there is a significant positive correlation between the button's distortion and the finger's height to the surface. Therefore, we propose a novel, fast, and robust algorithm, which takes advantage of the button's distortion to detect the touch action. The proposed touch detection algorithm is performed in three stages: 1) region of interest extraction through a homography mapping, by which the computational complexity of the following processing is reduced; 2) the button's distortion detection using a special edge detection algorithm, which greatly reduces the errors due to the influence of the finger's shadows and edges; and 3) touch action judgment by the button's distortion. Several applications (e.g., virtual keyboard, power point viewing), which use the proposed touch detection method based on the buttons, are shown in this paper. An evaluation is performed on the virtual keyboard and the results demonstrate that the proposed approach can detect bare-finger touch in real time with the missed detection rate of 1.00%, false detection rate of 2.08%, and touch detection rate of 96.92% at the typical projected distance.	algorithm;approximation;computation;computational complexity theory;distortion;edge detection;experiment;graphical user interface;homography (computer vision);programmed data processor;prototype;region of interest;robustness (computer science);touchscreen;video projector;virtual keyboard	Jun Hu;Guolin Li;Xiang Xie;Zhong Lv;Zhihua Wang	2014	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2013.2280088	computer vision;simulation;edge detection;computer science;graphical user interface;mobile computing;computer graphics (images)	Visualization	47.78446429357913	-42.66857553722234	185081
7cdf10af427ad1afaf36e47ff622d08a902b86ff	3d scene reconstruction from cylindrical panoramic images	omnidirectional vision;mobile robot;head direction;universiteitsbibliotheek;epipolar geometry;stereo vision;3d scene reconstruction;panoramic image;depth estimation	In this paper we address the problem of recovering 3-D scene structure from omnidirectional cylindrical panoramic images. In our application, the images are obtained from a single omni-directional camera mounted on top of a mobile robot. Depth can be estimated from a single pair of images by a stereo vision technique. However, depth cannot be estimated reliably in the robot’s heading direction. Moreover, depth estimates obtained from a single pair of images tend to be noisy. To overcome these problems we have developed a multi-baseline stereo vision method which uses more than two images obtained from multiple non co-linear viewpoints. Depth data obtained from different pairs of images is fused in a probabilistic framework. We present our method and experimental results obtained using image data acquired by our robot. The results indicate that robust and reliable depth estimates can be obtained using the proposed method.	algorithm;baseline (configuration management);course (navigation);depth map;microsoft windows;mobile robot;similarity measure;stereopsis	Roland Bunschoten;Ben J. A. Kröse	2002	Robotics and Autonomous Systems	10.1016/S0921-8890(02)00257-9	computer stereo vision;mobile robot;computer vision;computer science;stereopsis;epipolar geometry;computer graphics (images)	Robotics	52.49452508172663	-44.53212849497266	185250
36e4743a1522dfc8efab199e4e036b440f7df341	towards an assembly plan from observation: part ii: correction of motion parameters based on fact contact constraints	moving object;human performance;singular value decomposition;data mining;robot arm;robotic assembly assembly systems humans face detection data mining machine vision error correction robot programming equations computer science;assembly planning;machine vision;error correction;assembly systems;robotic assembly;humans;computer science;face detection;robot programming;observations methods	We have been developing a novel method to program a robot, an APO (assembly-plan- from-observation) method. A human performs assem- bly operations in front of the APO system's TV caimera. The APO system recognizes such assembly operations and generates an assembly plan to repeat the :assembly operations using its robot arm. We have developped a APO system which extracts two kinds of information from observed object config- urations: 1) face-contact relation and 2) motion pa- rameters necessary to move objects around. Under the presence of error, due to error-contaminated motion pa- rameters, the system may fail to perform an assembly operation, although a face-contact relation and thus an assembly operation is correctly recovered. This paper proposes a method to correct eirr~lneous motion parameters based on a face contact re)(ation. We assume that a given face contact relation reflects the actual face contact correctly. Motion paramleters are determined by solving face contact constraint equaitions, given by face contact relations, simultaneously )using the singular value decomposition method. We implement this method in the APO system, apply the method to several assembly examples, and verify the effectiveness of tlhe method. are the most promising. Yet, these methods are often in- convepient and impractical.		Takashi Suehiro;Katsushi Ikeuchi	1992		10.1109/IROS.1992.601948	human performance technology;computer vision;face detection;simulation;error detection and correction;robotic arm;machine vision;computer science;artificial intelligence;singular value decomposition	Robotics	50.76063714896727	-41.08084215557749	185363
1f01e1c1895af4a4f4c6c7c789346658c9f71e40	development of an omnidirectional vision system for environment perception	stereo image processing environmental factors face recognition humanoid robots image reconstruction robot vision;cameras face humanoid robots robot vision systems three dimensional displays face detection;humanoid robots;three dimensional displays;humanoid robot portable omnidirectional vision system for environment perception ovsep 5 degrees of freedom 5dof control architecture visual processing part stereovision face detection algorithm 3d reconstruction performance evaluation human face detection human face tracking;face;face detection;humanoid robot omnidirectional vision stereo vision visual servo control;robot vision systems;cameras	In this paper, we describe a portable Omnidirectional Vision System for Environment Perception (OVSEP) with 5 Degrees of Freedom (DOFs). OVSEP has a modular design and it is equipped with three main components: hardware, control architecture and visual processing part (omnidirectional vision and stereovision). A face detection algorithm and a method for 3D reconstruction have been used to evaluate the performance of OVSEP. Experimental results show that OVSEP was capable of detecting and tracking human faces in real time, correctly positioning stereo vision, and performing 3D reconstruction. Thus OVSEP can provide detailed information of surrounding environment in real time for the study of humanoid robot falling.	3d reconstruction;algorithm;face detection;humanoid robot;modular design;sensor;stereopsis	Yongbo Song;Qing Shi;Qiang Huang;Toshio Fukuda	2014	2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)	10.1109/ROBIO.2014.7090412	face;computer vision;face detection;simulation;object-class detection;computer science;humanoid robot;artificial intelligence;robot control;computer graphics (images)	Robotics	49.24872358704949	-40.19372713443428	185886
b86ecec31fc2bfebeceead42ecc258a8b242c2de	range-video fusion and comparison of inverse perspective algorithms in static images	zero bank algorithm range video fusion inverse perspective algorithms static images autonomous land vehicle road following task 3 d space location 2 d image epipolar plane approach search spaces flat earth algorithm hill and dale algorithm;inverse perspective;range data;data gathering;autonomous system;search space;estudio comparativo;correspondence problem;mobile robots;robotics;static image;three dimensional;sistema autonomo;computer vision;algorithme;etude comparative;algorithm;exact algorithm;systeme autonome;comparative study;autonomous land vehicle;robotica;robotique;constant width;vehiculo todo terreno;crosscountry vehicle;roads image segmentation cameras navigation land vehicles charge coupled image sensors laboratories two dimensional displays computational efficiency remotely operated vehicles;mobile robots computer vision;laser range scanner;heuristic algorithm;algoritmo;vehicule tout terrain	Creating three-dimensional (3-D) descriptions of road boundaries from single 2-D images of segmented road is a central problem for the autonomous land vehicle (ALV) road-following task. It is shown how heuristic simplifying assumptions can be avoided by using actively scanned laser range data to determine the 3-D space location of features in a 2-D image. An epipolar-plane approach that is commonly used to restrict search spaces in stereo correspondence problems is used to combine data gathered from the ALV's video camera and an ERIM laser range scanner for accurate 3-D descriptions of roads. In addition, various heuristic inverse perspective algorithms are compared with each other and with the video/range scanner fusion approach in terms of accuracy, failure situations, and estimated computational speed. The simplest of these is the flat-earth algorithm where the assumption is made that the vehicle is at all times resting in a level attitude on an infinite flat plane; image points were projected onto this plane. A second approach, the hill-and-dale algorithm, permits the plane's elevation to vary such that image pairs of left and right road edge points define a road of constant width. The zero-bank algorithm imposes a constant-road-width constraint on heuristically extracted image pairs of road edge points and the algorithm seeks image pairs of edge points that correspond to true 'crossword' road edges on curving roads. Graphical output describing the performance of the various algorithms on real-world scenes is presented and discussed. >	algorithm	David G. Morgenthaler;Steven Hennessy;Daniel DeMenthon	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.61202	heuristic;mobile robot;three-dimensional space;computer vision;mathematical optimization;simulation;computer science;autonomous system;artificial intelligence;machine learning;comparative research;control theory;robotics;correspondence problem;reverse perspective;data collection	Robotics	52.48803255154227	-38.18386736967058	185976
604d23bc2a17ef9a54d8cba4bb1c954aee54e643	estimating 3-0 movement of a rigid object: experimental results				Jia-Qi Fang;Thomas S. Huang	1983			artificial intelligence;machine learning;computer science;computer vision	Robotics	51.66722598214974	-42.99098009499221	186151
a2c16d4ac85ef322c527e99474f6a90650391c9a	localization of rgb-d camera networks by skeleton-based viewpoint invariance transformation	rgb d camera networks localization human activity recognition human upper torso detection data fusion surveillance area context aware service human behavior analysis kinects neighboring cameras orientation neighboring cameras relative location svit automatic localization technique skeleton based viewpoint invariant transformation;image motion analysis;sensor fusion cameras image motion analysis object detection;rgb d cameras;cameras joints vectors torso shoulder three dimensional displays;human skeleton rgb d cameras network localization viewpoint invariance;viewpoint invariance;sensor fusion;human skeleton;cameras;network localization;object detection	Combining depth information and color image, RGB-D cameras provide ready detection of humans and the associated 3D skeleton joints data, facilitating if not revolutionizing conventional image centric researches in, among others, computer vision, surveillance, and human activity analysis. Applicability of a D-RBG camera, however, is restricted by its limited range of frustum of depth in the range of 0.8 to 4 meters. Although a RGB-D camera network, constructed by deployment of several RGB-D cameras at various locations, could extend the range of coverage, it requires precise localization of the camera network: relative location and orientation of neighboring cameras. By introducing a skeleton-based viewpoint invariant transformation (SVIT) that derives relative location and orientation of a detected human's upper torso to a RGB-D camera, this paper presents a reliable automatic localization technique without the need for additional instrument or human intervention. By respectively applying SVIT to two neighboring RGB-D cameras on a commonly observed skeleton, the respective relative position and orientation of the detected human's skeleton to these two respective cameras can be obtained before being combined to yield the relative position and orientation of these two cameras, thus solving the localization problem. Experiments have been conducted where two Kinects are situated with bearing differences of about 45 degrees and 90 degrees when coverage extended by up to 70% with the installment of an additional Kinect. The same localization technique can be applied repeatedly to a larger number of RGB-D cameras, thus extending the applicability of RGB-D cameras to camera networks in making human behavior analysis and context-aware service in a lager surveillance area.	color image;computer vision;frustum;kinect;situated;software deployment;viewpoint	Yun Han;Sheng-Luen Chung;Jeng-Sheng Yeh;Qi-Jun Chen	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.263	computer vision;simulation;computer science;sensor fusion	Robotics	50.74843721986795	-39.46387490501099	186245
6ee02bfb911c1bd0ec7dfb57cca38d1d8a559848	relative pose estimation of surgical tools in assisted minimally invasive surgery	relative position;motor skills;minimal invasive surgery;real time;computer vision;exterior orientation;pose estimation	Minimally Invasive Surgery (MIS) is one of these applications where usually only 2D information is available to perform a 3D task. It requires a high degree of sensory-motor skills to overcome the disengagement between action and perception caused by the physical separation of the surgeon with the operative site. The integration of body movements with visual information serves to assist the surgeon providing a sense of position. Our purpose in this paper is to present a solution to the exterior orientation problem based on computer vision, as a tool in assisted interventions, locating the instruments with respect to the surgeon. Having knowledge of the 3D transformations applied to the instrument and its projections in the image plane, we show it is possible to estimate its orientation with only two different rotations and also its relative position if scale information is supplied. Experimental results show some advantages of this new algorithm such as simplicity and real-time performance.	3d pose estimation	Agustin A. Navarro;Edgar Villarraga;Joan Aranda	2007		10.1007/978-3-540-72849-8_54	computer vision;simulation;pose;motor skill;computer science	Robotics	48.90721932735186	-40.71157496484042	186443
21d1c79a44188cc7809920ad7281967a634cabe2	geometry-based optic disk tracking in retinal fundus videos		Fundus video cameras enable the acquisition of image sequences to analyze fast temporal changes on the human retina in a noninvasive manner. In this work, we propose a tracking-by-detection scheme for the optic disk to capture the human eye motion on-line during examination. Our approach exploits the elliptical shape of the optic disk. Therefore, we employ the fast radial symmetry transform for an efficient estimation of the disk center point in successive frames. Large eye movements due to saccades, motion of the head or blinking are detected automatically by a correlation analysis to guide the tracking procedure. In our experiments on real video data acquired by a low-cost video camera, the proposed method yields a hit rate of 98% with a normalized median accuracy of 4% of the disk diameter. The achieved frame rate of 28 frames per second enables a real-time application of our approach.	experiment;online and offline;radial (radio);real-time computing;real-time transcription	Anja Kürten;Thomas Köhler;Attila Budai;Ralf-Peter Tornow;Georg Michelson;Joachim Hornegger	2014		10.1007/978-3-642-54111-7_26	computer vision;computer science;optics;computer graphics (images)	Vision	47.082948278903416	-44.7735443141391	186688
56805fbccbc69c5426ed38ed6eb5464a8af00bce	oriented radial distribution on depth data: application to the detection of end-effectors	motion analysis;human computer interaction motion capture motion analysis machine vision classification;image motion analysis;human computer interaction;probability;probability cameras end effectors image motion analysis object detection pose estimation;foot;classification;motion capture;head foot humans cameras real time systems proposals;local features;machine vision;human motion;human body;end effector candidate blobs oriented radial distribution depth data end effector detection 3d body human extremity detection human motion capture domain system depth cameras body pose estimation problem head detection hand detection feet detection temporal dimension probabilistic descriptors;humans;head;proposals;cameras;object detection;end effectors;conference lecture;real time systems;pose estimation	End-effectors are considered to be the main topological extremities of a given 3D body. Even if the nature of such body is not restricted, this paper focuses on the human body case. Detection of human extremities is a key issue in the human motion capture domain, being needed to initialize and update the tracker. Therefore, the effectiveness of human motion capture systems usually depends on the reliability of the obtained end-effectors. The increasing accuracy, low cost and easy installation of depth cameras has opened the door to new strategies to overcome the body pose estimation problem. With the objective of detecting the head, hands and feet of a human body, we propose a new local feature computed from depth data, which gives an idea of its curvature and prominence. Such feature is weighted depending on recent detections, providing also a temporal dimension. Based on this feature, some end-effector candidate blobs are obtained and classified into head, hands and feet according to three probabilistic descriptors.	kinesiology;motion capture;probabilistic database;radial (radio);robot end effector;sensor	Xavier Suau;Javier Ruiz Hidalgo;Josep R. Casas	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288002	computer vision;robot end effector;motion capture;human body;simulation;pose;machine vision;biological classification;computer science;probability;head;foot;statistics;computer graphics (images)	Robotics	50.1683583132258	-40.638602123463016	186757
5b1521bbb1a5ab06a6256f48628dbdbb1919302b	detecting and identifying people in mobile videos	people detection;tracking system;smart phone;multimedia application;target localization;visual features;feature fusion;location awareness;camera calibration;close range;orientation estimation;mobile video	In this paper, we propose a system capable of detecting and identifying people in videos captured by smart phones. We discuss the challenges to extend existing location aware multimedia applications from annotating static landmarks in distance to annotating dynamic people in close range with significant pose variations. We propose to use a hybrid video and RF tracking system to enable accurate observer and target localization, and extract part models comprised of Maximally Stable Color Regions for each target. The model can efficiently detect possible positions of targets in the video, which are then used as dynamic landmarks to calibrate the camera orientation. Finally, positions of all targets in the video are jointly estimated using both visual features and spatial constraints. Experiments show that our approach can locate identified targets in video with significantly higher accuracy than back-projection using camera orientation estimations from accelerometers and magnetometers.	camera phone;camera resectioning;experiment;location awareness;radio frequency;sensor;smartphone;tracking system	Xunyi Yu;Aura Ganz	2011		10.1145/2072298.2071927	computer vision;camera resectioning;simulation;tracking system;computer science;artificial intelligence;video tracking;multimedia	HCI	47.12681520719909	-43.484648909212225	187343
aa35297aa62cb0edb055c178d21129f5b6846925	positioning an underwater vehicle through image mosaicking	visio per ordinador;imatges segmentacio;image motion analysis;reconeixement optic de formes;image processing;underwater vehicles;imaging segmentation;apparent motion;underwater vehicles cameras motion detection navigation oceans floors underwater tracking lighting optical scattering feature extraction;mobile robots;info eu repo semantics article;garbi underwater vehicle underwater vehicle positioning image mosaicking visual maps undersea exploration undersea navigation feature selection feature matching point detection homography computation mosaic construction discriminative properties 3d metric information;computer vision;feature extraction underwater vehicles mobile robots position measurement image motion analysis;imatges processament;feature extraction;optical pattern recognition;discriminacio visual;position measurement;visual discrimination;feature selection;ultrasonic sensor	Mosaics have been commonly used as visual maps for undersea exploration and navigation. The position and orientation of an underwater vehicle can be calculated by integrating the apparent motion of the images which form the mosaic. A feature-based mosaicking method is proposed in this paper. The creation of the mosaic is accomplished in four stages: feature selection and matching, detection of points describing the dominant motion, homography computation and mosaic construction. In this work we demonstrate that the use of color and textures as discriminative properties of the image can improve, to a large extent, the accuracy of the constructed mosaic. The system is able to provide 3D metric information concerning the vehicle motion using the knowledge of the intrinsic parameters of the camera while integrating the measurements of an ultrasonic sensor. The experimental results on real images have been tested on the GARBI underwater vehicle.	algorithm;camera resectioning;color;computation;experiment;feature selection;homography (computer vision);image stitching;map;motion estimation;real-time clock;sonar (symantec);throughput	Rafael García;Joan Batlle;Xavier Cufí;Josep Amat	2001		10.1109/ROBOT.2001.933043	mobile robot;computer vision;simulation;image processing;feature extraction;computer science;ultrasonic sensor;feature selection;remote sensing	Robotics	51.09092064634195	-39.517474944225306	187762
bc251393d7195496af8c3601739c81b43269ab82	understanding spatial configuration of robot's environment from stereo motion images	robot sensing systems;stereo motion images;mobile robots artificial intelligence computer vision computerised pattern recognition;image segmentation;intelligent robots;mobile robot;mobile robots;computerised pattern recognition;computer vision;3d model;robot vision;binocular vision;image edge detection;image reconstruction;pattern recognition;image segmentation mobile robots intelligent robots image edge detection robot vision systems cameras robot kinematics image reconstruction optical sensors robot sensing systems;artificial intelligence;optical sensors;3d model spatial configuration understanding robot vision pattern recognition stereo motion images mobile robot binocular vision;spatial configuration understanding;spatial configuration;robot vision systems;cameras;robot kinematics	When a robot moves around, it obtains a large number of input images of its environment taken at different viewpoints. The mobile robot considered has binocular vision and it obtains a sequence of stereo images from the environment. This paper proposes a method to build the 3D model of the robot's environment and to understand its spatial configuration from these input images, which is one of the central problems for the study of an intelligent mobile robot. >	robot	Masahiko Yachida;Akihiro Tsudo	1991		10.1109/IROS.1991.174597	mobile robot;monte carlo localization;computer vision;simulation;computer science;artificial intelligence;mobile robot navigation;computer graphics (images)	Robotics	50.20355426378922	-39.06003619859339	188115
44538273e7f5fb6e2e8aca8b0815958873d2b1f8	high speed/accuracy visual servoing based on virtual visual servoing with stereo cameras	least mean squares methods;operation speed virtual visual servoing stereo camera pose estimation accuracy industrial position controller position based visual servoing pbvs appearance model based vvs approach hybrid stereo trigonometry method position estimation accuracy weighted least squares method orientation estimation graphic processing unit gpu online trajectory generator variable image processing cycle fixed common robot controller cycle nonreal time image processing opengl rendered image;virtual reality;visual servoing cameras graphics processing units industrial control industrial robots least mean squares methods pose estimation robot vision stereo image processing trajectory control virtual reality;robot vision;graphics processing units;industrial robots;industrial control;stereo image processing;cameras estimation image processing visual servoing accuracy trajectory;visual servoing;trajectory control;cameras;pose estimation	This paper presents high speed and high accuracy visual servoing system. The algorithm has three major improvements, which can be implemented in practical applications; Firstly high-accuracy pose estimation by using stereo cameras, secondly real time implementation issues with non-real-time image processing platform and thirdly a consideration for industrial position controller. To resolve the issues, position-based visual servoing (PBVS) is adopted and appearance model based virtual visual servoing (VVS) is applied for pose estimation. VVS approach does not compute the stereo matching but directly compares the OpenGL rendered image and camera image for each camera; estimate the position/orientation using VVS independently for each camera; and provides a theoretically optimal compromise among those estimates. To enhance estimation accuracy, a hybrid method of stereo trigonometry for position estimation and weighted least squares for orientation estimation is proposed to combine the information from the stereo cameras. Operation speed is increased by using graphic processing unit (GPU) acceleration and an on-line trajectory generator which can accommodate the variable cycle of the image processing and the fixed cycle of a common robot controller. Finally, some experimental results illustrate the effectiveness of the proposed framework.	3d pose estimation;algorithm;computer stereo vision;graphics processing unit;image processing;least squares;online and offline;opengl;real-time locating system;robot;stereo camera;stereo cameras;visual servoing	Takashi Nammoto;Koichi Hashimoto;Shingo Kagami;Kazuhiro Kosuge	2013	2013 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2013.6696330	stereo camera;computer vision;simulation;pose;computer science;artificial intelligence;virtual reality;visual servoing;computer graphics (images)	Robotics	53.38546894146807	-41.28918349321303	188567
1dfc94473a57cb81ccb985c43d2a4e5d6dad7710	relief patterned-tile classification for automatic tessella assembly	vision system;robot manipulator;industrial application	This paper presents the detection and classification part of an industrial machine for automated assembly of decorative tessellae over patterned tiles that have significant reliefs. The machine consists of two vision systems for detecting the tiles and the tessellae, and a robotic manipulator to make the assembly. One of the vision systems detects the tessellae and the other one detects and classifies the tiles, finding the positions and orientations on the tiles where tessellae have to be mounted. Lateral illumination is used to enhance shadows and characterize the relief pattern. The shadow pattern is used as the main feature for classifying a tile to a model. The method works with a variety of models, is quite robust, and achieves very good classification results, as required for an industrial application.		José Miguel Sanchiz Martí;Jorge Badenas;Francisco José Forcada	2010		10.1007/978-3-642-13022-9_37	computer vision;simulation;machine vision;computer science;artificial intelligence	Vision	49.71443443407192	-40.34094759284482	188883
a1a629149967c4aed93fabebd14f8beefbbe4f5f	region classification for robust floor detection in indoor environments	obstacle detection;incremental learning;stereo imaging;indoor environment;homography;ground truth;data validation;floor anomaly detection	A novel framework based on stereo homography is proposed for robust floor/obstacle detection, capable of producing dense results. Floor surfaces and floor anomalies are identified at the pixel level using the symmetric transfer distance from the ground homography. Pixel-wise results are used as seed measurements for higher lever classification, where image regions with similar visual properties are processed and classified together. Without requiring any prior training, the method incrementally learns appearance models for the floor surfaces and obstacles in the environment, and uses the models to disambiguate regions where the homography-based classifier cannot provide a confident response. Several experiments on an indoor database of stereo images with ground truth data validate the robustness of our proposed technique.	experiment;ground truth;homography (computer vision);pixel	Ehsan Fazl Ersi;John K. Tsotsos	2009		10.1007/978-3-642-02611-9_71	homography;computer vision;simulation;homography;ground truth;computer science;data validation	Vision	47.460143192778496	-39.77350887988103	190391
4ee7c4862cf6dbb3551ab02e00ce9d966b23d801	a robotic vision system to measure tree traits		The autonomous measurement of tree traits, such as branching structure, branch diameters, branch lengths, and branch angles, is required for tasks such as robotic pruning of trees as well as structural phenotyping. We propose a robotic vision system called the Robotic System for Tree Shape Estimation (RoTSE) to determine tree traits in field settings. The process is composed of the following stages: image acquisition with a mobile robot unit, segmentation, reconstruction, curve skeletonization, conversion to a graph representation, and then computation of traits. Quantitative and qualitative results on apple trees are shown in terms of accuracy, computation time, and robustness. Compared to ground truth measurements, the RoTSE produced the following estimates: branch diameter (mean-squared error 0.99 mm), branch length (mean-squared error 45.64 mm), and branch angle (mean-squared error 10.36 degrees). The average run time was 8.47 minutes when the voxel resolution was 3 mm3.	autonomous robot;computation;graph (abstract data type);ground truth;larry laffer;lawrence m. breed;mean squared error;mobile robot;online and offline;real life;robustness (computer science);run time (program lifecycle phase);supercomputer;time complexity;trellis quantization;voxel	Amy Tabb;Henry Medeiros	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206497	robustness (computer science);artificial intelligence;computer vision;graph (abstract data type);voxel;simulation;branching (version control);computer science;ground truth;mobile robot;machine vision	Robotics	53.392112197935	-38.515335326032215	190426
9872c81f7e8d01e8358f21f50c7ae861928da7cf	estimating rigid transformation between two range maps using expectation maximization algorithm		We address the problem of estimating a rigid transformation between two point sets, which is a key module for target tracking system using Light Detection And Ranging (LiDAR). A fast implementation of Expectation-maximization (EM) algorithm is presented w hose complexity isO(N) with N the number of scan points.	expectation–maximization algorithm;tracking system	Shuqing Zeng	2012	CoRR		computer vision;mathematical optimization;combinatorics;mathematics	Robotics	53.312155273079476	-42.648690134263326	190743
9e2db6c8ade9880149d75d5404ec9d90e6fad931	improving vision-based self-localization	monte carlo localization;feature detection;robot soccer	After removing the walls around the field, vision-based localization has become an even more interesting approach for robotic soccer. The paper discusses how removal of the wall affects the localization task in RoboCup, both for vision-based and non-visual approaches, and argues that vision-based Monte Carlo localization based on landmark features seems to cope well with the changed field setup. An innovative approach for landmark feature detection for vision-based Monte Carlo Localization is presented. Experimental results indicate that the approach is robust and reliable.	algorithmic efficiency;artificial intelligence;feature detection (computer vision);feature detection (web development);image noise;monte carlo localization;monte carlo method;robot;robustness (computer science);sensor	Hans Utz;Alexander Neubeck;Gerd Mayer;Gerhard K. Kraetzschmar	2002		10.1007/978-3-540-45135-8_3	monte carlo localization;computer vision;simulation;computer science;artificial intelligence;feature detection	Robotics	52.81520795029103	-39.37347818846294	190754
4b43e61ba12ea61be3471178cce806a152efc7d5	method for presenting virtual objects to multiple fingers on two-hands using multiple single-point haptic devices	software;multiple single point haptic devices;object recognition;grasping;human computer interaction;electronic mail;virtual objects;phantoms;object recognition haptic interfaces human computer interaction;haptic device;shape recognition;teleoperators;indexes;imaging phantoms;servers;shape;multiple fingers;fingers;fingers haptic interfaces shape imaging phantoms character recognition grasping virtual environment teleoperators cities and towns electronic mail;cities and towns;virtual environment;haptic interfaces;character recognition;shape recognition virtual objects multiple fingers multiple single point haptic devices	In this paper, we discuss a method for presenting virtual objects to multiple fingers on both hands using multiple single-point haptic devices operating over a network. We evaluated the effect of the number of fingers used on ability to recognize shapes using this system. The experimental results suggest that the number of contact points do not sufficiently improve the ability to recognize shapes, and the method used for recognizing object shape is rather different between using one finger and using multiple fingers.	experiment;haptic technology;phantom reference;technical support	Takuya Handa;Tadahiro Sakai;Toshiya Morita	2009	World Haptics 2009 - Third Joint EuroHaptics conference and Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/WHC.2009.4810810	database index;computer vision;simulation;shape;computer science;virtual machine;operating system;cognitive neuroscience of visual object recognition;haptic technology;server	Robotics	47.660040671635905	-42.76754870882343	190942
540b927722d9e072c56c41728cce9fe7a87850df	real-time stereo tracking for head pose and gaze estimation	motion analysis;ergonomic assessment real time stereo tracking head pose estimation gaze estimation computer systems motion analysis head tracking real time stereo vision gaze direction human computer interaction consumer research;electrical capacitance tomography;human computer interaction;humans electrical capacitance tomography stereo vision application software ergonomics computer vision face detection cameras australia uniform resource locators;real time stereo vision;application software;real time;head tracking;computer systems;head pose estimation;computer vision;conference paper;human subjects;face recognition;face recognition tracking stereo image processing real time systems gesture recognition;stereo image processing;stereo vision;humans;face detection;gaze direction;uniform resource locators;consumer research;ergonomics;real time stereo tracking;gesture recognition;gaze estimation;cameras;tracking;ergonomic assessment;australia;real time systems	Computer systems which analyse human face/head motion have attracted significant attention recently as there are a number of interesting and useful applications. Not least among these is the goal of tracking the head in real time. A useful extension of this problem is to estimate the subject's gaze point in addition to his/her head pose. This paper describes a real-time stereo vision system which determines the head pose and gaze direction of a human subject. Its accuracy makes it useful for a number of applications including human/computer interaction, consumer research and ergonomic assessment.	real-time transcription	Rhys Newman;Yoshio Matsumoto;Sebastien Rougeaux;Alexander Zelinsky	2000		10.1109/AFGR.2000.840622	facial recognition system;computer vision;face detection;application software;simulation;computer science;stereopsis;gesture recognition;articulated body pose estimation;tracking;computer graphics (images)	Vision	47.70790002370886	-43.737069372778606	191287
956ea367d9c0216555e1dabdc6dc1a7702f3f6c7	target object identification and localization in mobile manipulations	experimental tests;tfnvc algorithm target object identification target object localization mobile manipulation multisensor fusion laser range finder image processing ambient light stability color information information processing camera image lrf pitching scan 3d image reconstruction depth of field information system calibration color feature extraction color image image shape 3d depth of field image triangular facet normal vector clustering;object recognition;pattern clustering;manipulators;image processing;3d imaging;feature extraction three dimensional displays cameras mobile communication image color analysis manipulators object recognition;depth of field;shape recognition;mobile robots;laser ranging;three dimensional;stability;robot vision;laser range finder;target tracking cameras feature extraction image colour analysis image reconstruction laser ranging manipulators mobile robots object tracking pattern clustering robot vision sensor fusion shape recognition stability;three dimensional displays;image color analysis;image colour analysis;feature extraction;image reconstruction;object tracking;information processing;mobile communication;sensor fusion;target tracking;mobile manipulator;object identification;cameras;color image	How to make mobile manipulator autonomously identify and locate target object in unknown environment, this is a very challenging question. In this paper, a multi-sensor fusion method based on camera and laser range finder (LRF) for mobile manipulations is proposed. Although the camera can acquire rich perceptual information, the image processing is very complex and easily influenced from the change in ambient light. Moreover, it can not directly provide the depth information of the environment. Since the LRF has the ability to directly measure 3D coordinates and the stability against the ambient light influence, meanwhile, the camera has the ability to acquire color information, the combination of the two sensors by making use of their advantages is utilized to obtain more accurate measurements as well as to simplify information processing. To overlay the camera image with the measurement points of the LRF pitching scan and to reconstruct the 3D image which includes the depth-of-field information, the model and the calibration of the system are built. Based on the combination of the color features extracted from the color image and the shape, size features extracted from the 3D depth-of-field image, the target object identification and localization are implemented autonomously. In order to extract the shape and size features, a triangular facet normal vector clustering (TFNVC) algorithm is introduced. The effectiveness of the proposed method and algorithm are validated by some experimental testing and analysis carried out on the mobile manipulator platform.	algorithm;autonomous robot;cluster analysis;color image;image processing;information processing;mobile manipulator;motion planning;normal (geometry);obstacle avoidance;sensor;stereoscopy	Yong Jiang;Ning Xi;Qin Zhang;Yunyi Jia	2011	2011 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2011.6181276	iterative reconstruction;mobile robot;stereoscopy;three-dimensional space;computer vision;simulation;mobile telephony;color image;stability;information processing;image processing;feature extraction;computer science;cognitive neuroscience of visual object recognition;video tracking;mobile manipulator;depth of field;sensor fusion;computer graphics (images)	Robotics	50.77659920128011	-39.36770673177753	191422
1b405440d7ca90ed3f1c65697194fee9aab4cecf	model based estimation of camera position in 3d scene		  The aim of the presented work was the development of a tracking algorithm for a single camera in a 3D scene with known objects.  The input of the algorithm is a sequence of images, and the main assumption was the apriori knowledge of relative location  of model objects - colourful boxes. The scene model was used to obtain a precise estimation of the camera’s position. The  algorithm consists of two steps: matching of model image feature points and then fitting of the model image to the actual  scene image. The results of the presented study were used to estimate the motion of a stereo camera setup in the indoor scene,  allowing for verification of the model-free navigation algorithm.    		Pawel Pelczynski	2011		10.1007/978-3-642-23154-4_21	camera auto-calibration;camera matrix;camera resectioning;pinhole camera model	Vision	53.13703744473051	-41.54934686982216	191475
4d24583aaee50368833b293a7aa0b1e65b77e076	comparative analysis of methods for the log boundaries isolation		The scrutiny of boundaries isolation methods is presented in this paper. The newly developed algorithms, based on regression analysis and integral projection are compared with Hough transform in order to analyze their effectiveness for the specific problem of moving logs control. The comparative analysis of the methods was carried out on the database of images obtained from video sequence of real industrial process by the criteria of accuracy and operation speed. Results of the test show that the line-by-line scanning method with posterior LOWESS regression analysis has the best accuracy. However, the best appropriate for the implementation in the real-time control systems based on machine vision technology is consecutive line selection method due to its reasonable accuracy and impressive performance.	algorithm;automation;control system;database;hough transform;image file formats;image processing;image scanner;informatics;machine vision;pixel;qualitative comparative analysis;real-time clock;real-time computer graphics;robotics;run time (program lifecycle phase)	Artem V. Kruglov;Yuriy V. Chiryshev	2015	2015 12th International Conference on Informatics in Control, Automation and Robotics (ICINCO)		hough transform;computer vision;image processing;computer science;machine learning;regression analysis;computer graphics (images)	Robotics	46.945958413274795	-44.68582147805749	191747
ef2d699769e39727e6eb26ab3ce77126a2b811d7	remote point-of-gaze estimation with single-point personal calibration based on the pupil boundary and corneal reflections	cameras reflection light sources calibration cornea visualization estimation;visual angle;iris recognition;eye images;prototype system;corneal reflection;head movements;point of gaze;corneal reflections;visualization;pupil boundary;single point personal calibration;light source;remote noncontact point of gaze estimation;estimation;virtual images;video cameras;feature extraction;video cameras feature extraction iris recognition light reflection light sources mean square error methods;rms pog estimation errors;remote gaze estimation;mean square error methods;remote gaze estimation point of gaze pupil boundary corneal reflections single point personal calibration;light reflection;estimation error;calibration;cornea;reflection;cameras;eye images remote noncontact point of gaze estimation single point personal calibration pupil boundary corneal reflection head movements virtual images light source visual angle rms pog estimation errors prototype system video cameras;light sources	This paper describes a new method for remote, non-contact point-of-gaze (PoG) estimation that tolerates head movements and requires a simple personal calibration procedure in which the subject has to fixate a single calibration point. This method uses the pupil boundary and at least two corneal reflections (virtual images of light sources) that are extracted from eye images captured by at least two video cameras. Experimental results obtained with a prototype system exhibited RMS PoG estimation errors of approx. 0.4–0.6° of visual angle. Such accuracy is comparable to that of the best commercially available systems, which use multiple-point personal calibration procedures, and significantly better than that of any other systems that use a single-point personal calibration procedure and have been previously described in the literature.	approximation;prototype;reflection (computer graphics)	Elias Daniel Guestrin;Moshe Eizenman	2011	2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)	10.1109/CCECE.2011.6030604	visual angle;computer vision;estimation;calibration;virtual image;reflection;visualization;feature extraction;computer science;iris recognition;statistics;computer graphics (images)	Vision	47.85515410085318	-44.45818588679352	191815
99783dbc279f6bbfff1d402f7eac652554d3b2b1	proposal of eye-gaze recognize method for input interface without infra-red ray equipment	opencv;image processing;opencv assistive technology eye gaze web camera image processing welfare support equipment;welfare support equipment;rectangular approximation eye gaze recognize method inexpensive eye gaze input interface disabled people noncontact input interface infra red rays ir motion template opencv library iris binary image;web camera;assistive technology;iris recognition visualization calibration estimation accuracy cameras approximation methods;iris recognition approximation theory gaze tracking handicapped aids;eye gaze	The main purpose of this study is to develop the inexpensive eye-gaze input interface for disabled people. The eye-gaze inputting can suit many situations, and it has little load for user because it is non-contact input interface. Many projects about the eye-gaze have been studied recently. Most of the productions of eye-gaze inputting use infra-red rays (IR) to detect the iris in the eyes. However the harm of IR for human eyes has been pointed out. In addition, if the interface requires the camera which has IR, the users must purchase the specific devices. Therefore we adopted a PC and the camera doesn't have the IR function. We propose the system by using the motion-template that is one of the functions of OpenCV library for tracking motion. However this function can't recognize the point where the user watches on monitor. It can recognize only the motion. That's why we made calibrate function to relate the eye-gaze with the monitor. We propose two methods to recognize eye-gaze. Both methods require the iris binary image. This image shows the iris shape and we expect that user's visual point can be calculated from this iris image. One method uses gravity point to calculate the point. Other method uses the rectangular approximation to calculate it. We did experiments for some subjects by both methods and compared the results to validate which method is proper and how much the calibrate function is accurate. In the experiment, the function randomly spotted a blue target on the monitor. The target position changes on a regular basis. In this experiment, the user stares at the target and we check the accuracy. If this function or methods are proper, the function correctly recognizes the user stare at target or near. For more accuracy, we will consider about how to detect the iris correctly in the future.	approximation;binary image;eb-eye;experiment;input device;opencv;randomness	Kazuki Fukushima;Naruki Shirahama	2014	15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2014.6888742	computer vision;simulation;eye tracking;image processing;computer science;computer graphics (images)	HCI	47.54165973551898	-42.91779002683942	192399
46aac39e58d894b5affa4cbe98ba1137622cab80	knowledge supervised perceptual grouping based qualitative building detection from monocular aerial images	perceptual grouping;aerial image;dynamic matching;distance map;reinvestigation;hypothesis generation and verification;knowledge supervised perceptual grouping;qualitative building detection;building detection	This paper addresses an important and practical problem in computer vision and pattern recognition — qualitative target detection from aerial images. In particular, it discusses the problem of qualitative building detection based on a monocular aerial image. The approach proposed, due to its independence of site models or camera calibration information, complements the model based approaches developed in the rest of the research community of building detection from aerial images. Specifically, a knowledge supervised perceptual grouping (KSPG) system based on reinvestigation, and hypothesis generation and verification, is presented, and is shown to be reasonably robust in experiments using real data.	aerial photography	Zhongfei Zhang;Rohini K. Srihari	2003	International Journal on Artificial Intelligence Tools	10.1142/S0218213003001113	computer vision;computer science;machine learning;pattern recognition;distance transform	Robotics	50.28061935486121	-44.07356867802867	192752
e4f4f130bfa1aace4999198989fe63cc61714d36	orientation of image sequences in a point-based environment model	engineering;electrical electronic;optical scanners;technology;spatial resection;prior knowledge;motion;image sensors;imaging sensor;science technology;spatial resection point based environment model intensity image sequence camera calibration long range 3d sensors terrestrial laser scanners imaging sensor;long range 3d sensors;intensity image sequence;image sequence;terrestrial laser scanners;optical scanners cameras image sensors image sequences;artificial intelligence;imaging science photographic technology;long range;image sequences layout cameras image sensors image reconstruction sensor systems computer vision clouds application software tracking;computer science;camera calibration;point based environment model;terrestrial laser scanner;pose;calibration;image sensor;cameras;image sequences	In this paper we propose a method to determine the exterior orientation of each frame of an intensity image sequence using prior knowledge of the scene stored in a point-based environment model (PEM). The orientation is performed by tracking landmarks across the image sequence acquired with a calibrated camera. The landmarks are intensity features, which are automatically extracted from the PEM. The PEM can easily be acquired by long range 3D sensors, such as terrestrial laser scanners. The orientation procedure of the imaging sensor is solely based on spatial resection.	digital imaging;image sensor;interrupt;mobile agent;point cloud;scale-invariant feature transform;terrestrial television	Jan Böhm	2007	Sixth International Conference on 3-D Digital Imaging and Modeling (3DIM 2007)	10.1109/3DIM.2007.38	computer vision;simulation;computer science;image sensor	Robotics	50.69252016519249	-39.98851721336215	192773
9b01cce83eeb5fb5058e6e375db61ffb1b56184a	mobile care robot localization and guidance using image structure map	image edge detection cameras robot vision systems robot kinematics;image motion analysis;video signal processing;ultrasonic imaging;mobile robots;medical robotics;closed circuit television;feedback;robot vision;collision avoidance;mobile care robot localization feedback obstacle detection ultrasound movement process motion detection mobile robot positioning cctv system video images computer vision guidance method image structure map;video signal processing closed circuit television collision avoidance feedback health care image motion analysis medical robotics mobile robots robot vision ultrasonic imaging;image structure map computer vision mobile robot localization guidance;health care	This paper presents a localization and guidance method for mobile robots based on computer vision. The purpose of this study was to use CCTV-system video images to position and guide mobile robots. This method enables direct observation of the robot and is not influenced by the characteristics of a specific region or area, changes in the environment at a certain time are used to guide the robot's movement. In this study, an image structure map was designed to guide the movement of the robot, and motion detection was integrated to position the robot. Throughout the movement process, ultrasound was used to detect obstacles and send feedback to the image structure map, which then immediately labeled the map and changed the course of the robot. This method may be directly employed for care robots and can be extended to other applications in the future.	closed-circuit television;computer vision;feature recognition;internationalization and localization;mobile robot;robotic mapping	Chuan-Pin Lu;Ta-Hua Yeh;Wei Huang	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.648	mobile robot;robot learning;embedded system;computer vision;simulation;machine vision;computer science;artificial intelligence;feedback;robot control;mobile robot navigation;health care	Robotics	49.861719896711826	-38.198202846051295	192824
abf96dacac7853cbcf958a61d003f5996107db4f	soft synchronization: synchronization for network-connected machine vision systems	linux image processing soft synchronization network connected machine vision systems computer vision image acquisition external trigger events hardware triggering software triggering time tagging stochastic model;video streaming;soft synchronization;image processing;time tagging;linux image processing;network connected machine vision systems;manufacturing automation;hardware triggering;computer vision;synchronisation;software triggering;operating system;stochastic processes;image acquisition;network connectivity;manufacturing automation frame synchronization machine vision;synchronization;machine vision;manufacturing;factory automation;synchronisation computer vision stochastic processes;external trigger events;stochastic model;machine vision hardware image processing computer vision application software tagging streaming media stochastic processes uncertainty calibration;automation;frame synchronization	Many computer vision applications require synchronization between image acquisition and external trigger events. Hardware and software triggering are widely used but have several limitations. Soft synchronization is investigated, which operates by time tagging both trigger events and images in a video stream and selecting the image corresponding to each trigger event. A stochastic model is developed for soft synchronization; and, based on the model, the uncertainty interval and confidence for correct image selection are determined, and an efficient calibration method is derived. Soft synchronization is experimentally demonstrated on a Linux image processing computer with a camera connected by the IEEE-1394 serial bus. To minimize timing variability, time tags are associated with images and events in the corresponding interrupt service routines within the operating system of the image processing computer. The model-based analysis applied to the experimental hardware shows that image/event synchronization within inter-frame interval can be achieved with 99% confidence. Experiments confirm this result.	computer vision;experiment;ieee 1394;image processing;linux;machine vision;operating system;serial communication;spatial variability;streaming media;synchronization (computer science)	Brian S. R. Armstrong;Sandhya K. Puthan Veettil	2007	IEEE Transactions on Industrial Informatics	10.1109/TII.2007.909422	embedded system;synchronization;computer vision;real-time computing;machine vision;telecommunications;image processing;computer science;operating system;automation;data synchronization;synchronization;frame synchronization;statistics	Embedded	48.89100836535579	-42.77492760964241	193811
6edb7367a66b20376a38de71edcf1e9a5372a715	integrating deep semantic segmentation into 3-d point cloud registration		Point cloud registration is the task of aligning 3D scans of the same environment captured from different poses. When semantic information is available for the points, it can be used as a prior in the search for correspondences to improve registration. Semantic-assisted Normal Distributions Transform (SE-NDT) is a new registration algorithm that reduces the complexity of the problem by using the semantic information to partition the point cloud into a set of normal distributions, which are then registered separately. In this letter we extend the NDT registration pipeline by using PointNet, a deep neural network for segmentation and classification of point clouds, to learn and predict per-point semantic labels. We also present the Iterative Closest Point  (ICP) equivalent of the algorithm, a special case of Multichannel Generalized ICP. We evaluate the performance of SE-NDT against the state of the art in point cloud registration on the publicly available classification data set Semantic3d.net. We also test the trained classifier and algorithms on dynamic scenes, using a sequence from the public dataset KITTI. The experiments demonstrate the improvement of the registration in terms of robustness, precision and speed, across a range of initial registration errors, thanks to the inclusion of semantic information.	algorithm;artificial neural network;complexity;deep learning;experiment;iterative closest point;iterative method;java platform, standard edition;point cloud;point set registration	Anestis Zaganidis;Li Sun;Tom Duckett;Grzegorz Cielniak	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2848308	robustness (computer science);point cloud;control theory;artificial neural network;engineering;segmentation;special case;iterative closest point;normal distribution;artificial intelligence;pattern recognition	Vision	53.27005602307964	-43.68031460188386	193962
bdc9b3d8dd615232b21c4d1c0f1a83c979f0ec45	successive human tracking and posture estimation with multiple omnidirectional cameras		We propose a successive method for human tracking and posture estimation by using multiple omnidirectional cameras appropriate for Machine Learning method. A stable estimation for foot and head position is executed by the combination analysis with particle filter processing. Moreover, a classification method is accomplished by using the constraint of the connected line between head and foot position. The combination both this constraint and relative height from head to foot is possible to distinguish typical four postures for human activities in an indoor scene. We believe that this continuity of each data helps smooth convergence to the time-sequential learning for the discrimination between normal and abnormal behavior.		Shunsuke Akama;Akihiro Matsufuji;Eri Sato-Shimokawara;Shoji Yamamoto;Toru Yamaguchi	2018	2018 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2018.00019	computer vision;omnidirectional antenna;particle filter;trajectory;convergence (routing);computer science;artificial intelligence	Robotics	47.56935124205629	-41.040310417402644	194297
14419de75c7424e6c48c72493de879314bac1802	detekce a vizualizace specifických rysů v mračnu bodů ; detection and vizualization of features in a point cloud			point cloud	Jiří Kratochvíl	2018				Vision	51.94387290367599	-43.00432962068357	194523
6dad4f7c5771ed4623b63c79c2664b79e3788f4f	simulation of the visual self-localization of mobile robots based on image similarity metrics	measurement;accuracy;visualization;simulation estimation theory global positioning system image texture mobile robots robot vision;image distortions simulation visual self localization mobile robots image similarity metrics estimation gps device image textures;robot vision systems;cameras;measurement robot kinematics cameras robot vision systems visualization accuracy;robot kinematics	In the paper the idea of the visual self-localization of mobile robots based on the estimation of image similarity is discussed. It is assumed that a rough position of the mobile robot is known e.g. from the built-in GPS device. Assuming known orientation of the robot, the main advantage of the application of image analysis algorithms is the increase of the self-localization accuracy. The basic idea of our approach is related to the use of image similarity metrics for the estimation of the distance to the known positions associated with the images stored in the database. In order to verify the the validity of the proposed approach for the images containing textures characteristic for urban areas, some experiments have been conducted in Java based Simbad environment. As the result of applying various image similarity metrics for this purpose, the noticeable increase of the localization accuracy has been obtained for the artificial model of the urban area developed in Simbad environment. Nevertheless, achieved results are encouraging for further experiments utilizing natural images captured by real cameras also in the presence of various image distortions.	algorithm;distortion;experiment;gps navigation device;global positioning system;image analysis;java;mobile robot;simbad;simulation	Mateusz Teclaw;Piotr Lech;Krzysztof Okarma	2015	2015 20th International Conference on Methods and Models in Automation and Robotics (MMAR)	10.1109/MMAR.2015.7283999	computer vision;simulation;computer science;computer graphics (images)	Robotics	52.442569775393984	-40.07066325494024	194624
c1ebb45f16ae37f38ddbc4b68c0405ae30560d6f	image based exploration for indoor environments using local features	robot exploration;local features;indoor environment;image based navigation;topological mapping	This paper presents an approach to explore an unknown indoor environment using vision as the sensing modality, thereby building a topological map of images. The contribution of this paper is in a new approach that identifies the next best place to move from a node in the topological graph. This decision is taken locally at a node by choosing the next best direction, when there are open spaces before the robot, and globally by choosing the next best node to branch off a new exploration, when there are no open spaces before the robot. We propose a method to assign weights to nodes for this purpose. Weight is defined as a function of the depth of local descriptors of images, and the number of times they were seen across different nodes. The efficacy of the approach to explore office like environments is verified through several experiments on a P3DX robot.	experiment;modality (human–computer interaction);robot;topological graph	Aravindhan K. Krishnan;K. Madhava Krishna;Supreeth Achar	2010		10.1145/1838206.1838450	computer vision;simulation	Robotics	51.02948094440892	-38.42434762769856	194811
fbfb023170b77ec593dfed1c640e9e080598232a	overlapping and non-overlapping camera layouts for robot pose estimation		We study the use of overlapping and non-overlapping camera layouts in estimating the ego-motion of a moving robot. To estimate the location and orientation of the robot, we investigate using four cameras as non-overlapping individuals, and as two stereo pairs. The pros and cons of the two approaches are elucidated. The cameras work independently and can have larger field of view in the non-overlapping layout. However, a scale factor ambiguity should be dealt with. On the other hand, stereo systems provide more accuracy but require establishing feature correspondence with more computational demand. For both approaches, the extended Kalman filter is used as a real-time recursive estimator. The approaches studied are verified with synthetic and real experiments alike.	3d pose estimation;branching factor;experiment;extended kalman filter;real-time clock;recursion;robot;synthetic intelligence	Mohammad Ehab Ragab	2015	CoRR		computer vision;simulation;computer graphics (images)	Robotics	52.601792696661406	-41.876584931861856	195089
fc696742be0ec9dcde21ac663e2cfc0ee22b3145	localization based on the gradient information for dem matching		This paper proposes the localization algorithm that estimates a ground position by comparing the recovered elevations estimated from aerial images with digital elevation model (DEM). The proposed algorithm consists of two stages: recovering the sampled elevations from multiple aerial images and matching them with DEM. While conventional algorithms estimate the elevation field, that is, recovered elevation map (REM) over a whole image, the proposed algorithm recovers the elevations only at finite number of sample points from a multiple image sequence and does not require rotation of REM. So, the proposed algorithm can estimate the ground position accurately by using a wide recovered area and can estimate the position much faster than conventional ones. Additionally, the proposed algorithm makes use of the gradient information of terrain at multiple sample points of multiple aerial images for considering global characteristics. Computer simulations with various images show the effectiveness of the proposed algorithm. Figure 1: Area recovered by multiple images	aerial photography;algorithm;computational complexity theory;computer simulation;digital elevation model;gradient descent;machine vision;real-time clock;real-time computing;simulation	Dong-Gyu Sim;Rae-Hong Park	1998			elevation;computer vision;terrain;mathematics;pattern recognition;local algorithm;aerial image;artificial intelligence;digital elevation model	Vision	53.31134860908266	-43.32644206784123	195118
078a1882ced158bc009cd36a5a1c99f3aadcc32b	silhouette lookup for automatic pose tracking	application software;video sequences;humans cameras video sequences application software particle tracking smoothing methods data mining infrared heating computer vision computer science;data mining;computer vision;smoothing methods;infrared heating;humans;computer science;particle tracking;cameras	Computers should be able to detect and track the articulated 3-D pose of a human being moving through a video sequence. Current tracking methods often prove slow and unreliable, and many must be initialized by a human operator before they can track a sequence. This paper introduces a simple yet effective algorithm for tracking articulated pose, based upon looking up observed silhouettes in a collection of known poses. The new algorithm runs quickly, can initialize itself without human intervention, and can automatically recover from critical tracking errors made while tracking previous frames in a video sequence.	algorithm;lookup table	Nicholas R. Howe	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.438	computer vision;application software;simulation;tracking system;computer science;video tracking;articulated body pose estimation;infrared heater;computer graphics (images)	Vision	48.67021038014864	-43.955090024650275	196525
a94e8766fa6c402aba1644e9db2f6624fda57668	approximated environment features with application to trajectory annotation	geometry;navigation;trajectory;indexing;buildings;mobile radio mobility management	Indoor location-based services is an ever emerging field of research and application. In particular indoor navigation, i.e. positioning and route planning in freely walkable space, gets much attention. The philosophy of this paper is to incorporate isovists as a numerical representation of the local environment to enable new types of indoor LBS. This paper has got two main contributions. First, we define discrete isovists as an approximation of exact isovists using a simple ray casting approach. Second, we define, create, and use isovist feature cubes for semantic evaluations of floor plans as well as trajectory annotation.	approximation algorithm;directional statistics;information retrieval;location-based service;numerical analysis;olap cube;ray casting	Sebastian Feld;Martin Werner;Claudia Linnhoff-Popien	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7849993	computer vision;simulation;geography;multimedia	Visualization	51.50719930948568	-43.39719784566776	197577
c5d485ddd4436668fb793717ac15eba6ad0c002c	generalizing epipolar-plane image analysis on the spatiotemporal surface	spatiotemporal surface;temporal structure 3d spatiotemporal laplacian 3d connectivity information geometry computer vision picture processing pattern recognition epipolar plane image analysis spatiotemporal surface autonomous navigation coherent spatial descriptions scene feature estimates;scene feature estimates;sensor phenomena and characterization;image motion analysis;epipolar plane image analysis;geometry;picture processing;3d spatiotemporal laplacian;motion estimation;layout;picture processing geometry pattern recognition;coherent spatial descriptions;three dimensional;scene reconstruction;image analysis spatiotemporal phenomena image motion analysis geometry cameras navigation information analysis layout motion estimation sensor phenomena and characterization;computer vision;navigation;temporal structure;3d connectivity information;spatiotemporal phenomena;pattern recognition;image analysis;autonomous navigation;epipolar plane image;information analysis;cameras	The previous implementations of our Epipolar-Plane Image Analysis mapping technique demonstrated the feasibility and benefits of the approach, but were carried out for restricted camera geometries. The question of more general geometries made the technique's utility for autonomous navigation uncertain. We have developed a generalization of our analysis that (a) enables varying view direction, including variation over time (b) provides three-dimensional connectivity information for building coherent spatial descriptions of observed objects; and (c) operates sequentially, allowing initiation and refinement of scene feature estimates while the sensor is in motion. To implement this generalization it was necessary to develop an explicit description of the evolution of images over time. We have achieved this by building a process that creates a set of two-dimensional manifolds defined at the zeros of a three-dimensional spatiotemporal Laplacian. These manifolds represent explicitly both the spatial and temporal structure of the temporally evolving imagery, and we term them spatiotemporal surfaces. The surfaces are constructed incrementally, as the images are acquired. We describe a tracking mechanism that operates locally on these evolving surfaces in carrying out three-dimensional scene reconstruction.	autonomous robot;coherence (physics);computation;emoticon;epipolar geometry;image analysis;laplacian matrix;mimd;motion estimation;real-time clock;refinement (computing);simd;viewing cone;voxel	H. Harlyn Baker;Robert C. Bolles	1989	International Journal of Computer Vision	10.1007/BF00054837	layout;three-dimensional space;computer vision;navigation;image analysis;computer science;theoretical computer science;machine learning;motion estimation;mathematics;data analysis	Vision	49.87131810921715	-42.68351112062827	197919
66ac09c38282b0504c9eb015710fb2783dfca1cd	an active head tracking system for distance education and videoconferencing applications	histograms;distance education;teleconferencing;tracking system;magnetic heads;distance learning;pan tilt zoom;head tracking;video compression;large scale;shape;machine vision;distance learning teleconferencing cameras magnetic heads target tracking histograms video compression shape machine vision large scale systems;target tracking;cameras;large scale systems;active control	We present a system for automatic head tracking with a single pan-tilt-zoom (PTZ) camera. In distance education the PTZ tracking system developed can be used to follow a teacher actively when s/he moves in the classroom. In other videoconferencing applications the system can be utilized to provide a close-up view of the person all the time. Since the color features used in tracking are selected and updated online, the system can adapt to changes rapidly. The information received from the tracking module is used to actively control the PTZ camera in order to keep the person in the camera view. In addition, the system implemented is able to recover from erroneous situations. Preliminary experiments indicate that the PTZ system can perform well under different lighting conditions and large scale changes.	color;experiment;motion capture;pan–tilt–zoom camera;tracking system	Sami Huttunen;Janne Heikkilä	2006	2006 IEEE International Conference on Video and Signal Based Surveillance	10.1109/AVSS.2006.19	distance education;computer vision;simulation;machine vision;computer science;statistics;computer graphics (images)	Robotics	46.91151099590508	-44.8632865713021	198098
5d07f4ebee31a6f1076860337407d37d65bb55f0	developing robotic systems with multiple sensors	object recognition;vision ordenador;range information;range data;robots computer vision position measurement;robotics;inspection;three dimensional;informacion;computer vision;optimization problem;intelligent robot;manipulacion;information integration;coordinate transformation;industrial robots;robots;information processing;industrial robot computer vision position measurement multiple sensors vision range proximity touch sensory data orientation model based object recognition module image to world coordinate transformation module reprocessing segmentation 3 d primitive extraction multisensory information integration optimization minimum error solution;position measurement;robust method;pattern recognition;robotica;vision ordinateur;manipulation;robotique;reconnaissance forme;robot inteligente;reconocimiento patron;inspeccion;robot sensing systems sensor systems robot kinematics service robots orbital robotics robustness image analysis information analysis object recognition image recognition;information analysis;capteur multiple;information;robot intelligent	A general approach is presented for the integration of vision, range, proximity, and touch sensory data to derive a better estimate of the position and orientation (pose) of an object appearing in the work space. Efficient and robust methods for analyzing vision and range data to derive an interpretation of input images are discussed. Vision information analysis includes a model-based object recognition module and an image-to-world coordinate transformation module to identify the three-dimensional (3-D) coordinates of the recognized objects. The range information processing includes modules for reprocessing, segmentation, and 3-D primitive extraction. The multisensory information integration approach represents sensory information in a sensor-independent form and formulates an optimization problem to find a minimum-error solution to the problem. The capabilities of a multisensor robotic system are demonstrated by performing a number of experiments using an industrial robot equipped with several sensors of differing types. >	robot;sensor	Mohan Manubhai Trivedi;Mongi A. Abidi;Richard O. Eason;Rafael C. González	1990	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.61201	robot;optimization problem;three-dimensional space;computer vision;simulation;pose;information;inspection;information processing;computer science;artificial intelligence;information integration;coordinate system;cognitive neuroscience of visual object recognition;robotics;data analysis	Robotics	50.5969355445865	-38.44696643506944	198586
065163823d827c1b3384a894e9d042eb5eac7261	indoor map generation from multiple lidar point clouds		"""This paper presents a new algorithm for building an indoor map by integrating point clouds of 2D light detection and ranging (LIDAR) scanners in indoor environments. Iterative closest point (ICP) algorithm is one of the well-known methods for such purpose and often used for mobile robot SLAM. However, the algorithm is designed based on """"dense"""" (or """"continuous"""") measurement of the same space with known relative positions of measurement points and angles, and it does not often work efficiently if the measurement is sparse and/or LIDAR locations are unknown. Such situations are seen when some installed LIDARs in a room are used to build a background indoor map for object tracking, or mobile LIDARs are used by technicians to build a digital indoor map, where each space is captured only at a few locations with different angles. To tackle this issue, our method extracts line segments and edge points as features from given LIDAR point clouds and finds shape coincidences commonly contained in a pair of given point clouds to identify positional relationships between LIDARs. By this information, these point clouds can be integrated into common 2D coordinates. Indoor map generation is realized by sequentially applying this integration procedure to every pair of point clouds. Also, the experiments on real data show that our method can identify the relative positions of LIDARs with 10cm-order errors in average, and by sequentially applying the point-cloud integration, the generated maps have only 3% errors."""	algorithm;edge detection;experiment;ground truth;iterative closest point;iterative method;map;mobile robot;point cloud;simultaneous localization and mapping;sparse matrix;transformation matrix;whole earth 'lectronic link	Hikaru Yoshisada;Yuma Yamada;Akihito Hiromori;Hirozumi Yamaguchi;Teruo Higashino	2018	2018 IEEE International Conference on Smart Computing (SMARTCOMP)	10.1109/SMARTCOMP.2018.00076	point cloud;computer vision;video tracking;feature extraction;lidar;iterative closest point;ranging;line segment;mobile robot;artificial intelligence;computer science	Robotics	53.10821808193179	-42.515280776405916	199195
746d7772a9b3c69462088ced2b7ae95d624a84ec	a fast and accurate 3-d rangefinder using the biris technology: the trid sensor	biris technology;autonomous vehicle;application software;3 d rangefinder;layout;computer industry;image sensors;computer vision distance measurement image sensors;computer vision;robot manipulator;3 d sensing device;distance measurement;industrial application;robustness;lateral resolution;trid sensor;robot vision systems robustness computer vision application software robot kinematics layout cameras computer industry costs geometrical optics;robot vision systems;lateral resolution 3 d rangefinder biris technology trid sensor 3 d sensing device;cameras;robot kinematics;geometrical optics	In many industrial applications such as autonomous vehicle guidance and robotic manipulatitons, reliable 3 0 informations must be extracted from the scene in order to provide a robust description of the environment in which the system evolves. This paper presents the TRID sensor, a simple, fast, robust and accurate 3-0 sensing device based on the Biris technology. TRID is limited to a lateral resolution of one point. Experimental resultJ: show that an accuracy better than 0.15% can be obtuined for the depth component (Z} of 3-Dpoints at a distance of 1.3 meterfrom a wooden black painted beam surjace with an ucquisition rate of 8.5 pointsh.	autonomous robot;lateral thinking	Frédéric Loranger;Denis Laurendeau;Régis Houde	1997		10.1109/IM.1997.603848	layout;geometrical optics;computer vision;application software;simulation;computer science;image sensor;robot kinematics;robustness	Robotics	51.25996607835301	-40.214558714942946	199416
ca78bc4f24d1ae61b85aabf3df3bbcd95f8939ff	landmark-based homing navigation using omnidirectional depth information	arrangement order matching;distance sensor;distance-estimated landmark vector;homing navigation;landmark navigation;landmark vector	A number of landmark-based navigation algorithms have been studied using feature extraction over the visual information. In this paper, we apply the distance information of the surrounding environment in a landmark navigation model. We mount a depth sensor on a mobile robot, in order to obtain omnidirectional distance information. The surrounding environment is represented as a circular form of landmark vectors, which forms a snapshot. The depth snapshots at the current position and the target position are compared to determine the homing direction, inspired by the snapshot model. Here, we suggest a holistic view of panoramic depth information for homing navigation where each sample point is taken as a landmark. The results are shown in a vector map of homing vectors. The performance of the suggested method is evaluated based on the angular errors and the homing success rate. Omnidirectional depth information about the surrounding environment can be a promising source of landmark homing navigation. We demonstrate the results that a holistic approach with omnidirectional depth information shows effective homing navigation.	algorithm;angularjs;distance;estimated;feature extraction;greater;holism;holomovement;image resolution;inspiration function;missile guidance;mobile robot;multiple homing;navigation;obstruction;physical object;plasma cleaning;rechargeable battery;robot (device);snapshot (computer storage);structured-light 3d scanner;vector map;waypoint;weight;biologic segmentation	Changmin Lee;Seung-Eun Yu;Daeeun Kim	2017		10.3390/s17081928	snapshot (computer storage);engineering;mobile robot;feature extraction;landmark;mount;omnidirectional antenna;vector map;computer vision;artificial intelligence;homing (biology)	Robotics	51.335947908411484	-38.9912175779921	199517
bc13cb3cc84e8ec8b6baff2aa3e79ed327279976	multi-robot cooperative edge detection using kalman filtering	kalman filtering;image processing;map building;edge detection;sensor data fusion;multiple robots;cooperative sensing;article	This paper presents a design and implementation of cooperative edge detection of multiple mobile robots in an indoor environment. We propose a method to fuse sensory data from two mobile robots at different locations based on Kalman filtering techniques. To demonstrate the proposed method, we constructed two mobile robots for experimental purpose. A USB Web camera was put at the front of each robot for environment recognition. Image acquisition and processing are performed onboard the robot exploiting a Linux-based embedded platform. Processed scenic range data from robots are transferred through wireless Ethernet to a robot-home server, where a global representation of the environment is maintained. The constructed map can be accessed at a remote site for tele-operation of the robots through Internet. Each robot can update its knowledge of the world by downloading the map from the server. Experimental results of two-robot cooperative sensing in a test environment are presented to demonstrate the effectiveness of the proposed method.	deployment environment;download;edge detection;embedded system;home server;internet;kalman filter;linux;mobile robot;server (computing);television;usb;webcam	Kai-Tai Song;Hung-Ting Chen	2004	Intelligent Automation & Soft Computing	10.1080/10798587.2004.10642884	kalman filter;embedded system;computer vision;simulation;edge detection;image processing;computer science	Robotics	48.785975466200185	-39.89081893556303	199648
223679bdc3c2fa7fd591493160db9f5bfbde5f0f	vision slam in the measurement subspace	representation;range data;constraints vision slam representation features symmetries;vision slam;robotics;indexing terms;symmetries;features;engineering and technology;teknik och teknologier;simultaneous localization and mapping sensor phenomena and characterization cameras robot sensing systems robot kinematics mobile robots position measurement size measurement computational complexity;simultaneous localization and mapping;robotteknik och automation;constraints	In this paper we describe an approach to feature representation for simultaneous localization and mapping, SLAM. It is a general representation for features that addresses symmetries and constraints in the feature coordinates. Furthermore, the representation allows for the features to be added to the map with partial initialization. This is an important property when using oriented vision features where angle information can be used before their full pose is known. The number of the dimensions for a feature can grow with time as more information is acquired. At the same time as the special properties of each type of feature are accounted for, the commonalities of all map features are also exploited to allow SLAM algorithms to be interchanged as well as choice of sensors and features. In other words the SLAM implementation need not be changed at all when changing sensors and features and vice versa. Experimental results both with vision and range data and combinations thereof are presented.	algorithm;euler;general frame;line level;motion planning;robotic mapping;sensor;simultaneous localization and mapping	John Folkesson;Patric Jensfelt;Henrik I. Christensen	2005	Proceedings of the 2005 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2005.1570092	computer vision;simulation;index term;computer science;artificial intelligence;machine learning;robotics;representation;simultaneous localization and mapping	Robotics	51.51888465621031	-39.05251024383183	199981
