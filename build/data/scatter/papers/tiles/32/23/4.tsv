id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
b4e0e3499c602428490998dd541d1c55b825a650	mapping techniques for aligning sulci across multiple brains	conformal map;geometric feature;mean curvature	Visualization and mapping of function on the cortical surface is difficult because of its sulcal and gyral convolutions. Methods to unfold and flatten the cortical surface for visualization and measurement have been described in the literature. This makes visualization and measurement possible, but comparison across multiple subjects is still difficult because of the lack of a standard mapping technique. In this paper, we describe two methods that map each hemisphere of the cortex to a portion of a sphere in a standard way. To quantify how accurately the geometric features of the cortex -- i.e., sulci and gyri -- are mapped into the same location, sulcal alignment across multiple brains is analyzed, and probabilistic maps for different sulcal regions are generated to be used in automatic labelling of segmented sulcal regions.	alignment;brain;convolution;imagery;map	Duygu Tosun;Maryam E. Rettmann;Jerry L. Prince	2003	Medical image analysis	10.1007/978-3-540-39903-2_105	conformal map;combinatorics;topology;mean curvature;mathematics;geometry	Visualization	46.60387629575037	-75.85856871565859	177534
2067e8fd972ab49eebecf415440bdb8441d05cd5	morphology-based three-dimensional interpolation	medical diagnostic imaging morphology based three dimensional interpolation two dimensional images three dimensional reconstruction gaps between available image slices morphology based algorithm missing data interpolation object centroids dilation operator digital images transformation erosion operator distance maps multiple objects holes synthesized cases general object interpolation;morphologie;image tridimensionnelle;mathematical morphology;metodo matematico;interpolation;mathematical method;algorithm performance;image processing;dato que falta;interpolation surface morphology computational complexity medical services biomedical equipment two dimensional displays image reconstruction data visualization data mining digital images;algorithms humans image processing computer assisted imaging three dimensional;implementation;objet test;analisis forma;interpolacion;procesamiento imagen;traitement image;three dimensional;donnee manquante;ejecucion;morphology;resultado algoritmo;image reconstruction;medical image processing;interpolation method;performance algorithme;methode mathematique;medical image processing interpolation mathematical morphology image reconstruction;tridimensional image;medical application;pattern analysis;missing data;morfologia;objeto prueba;test object;analyse forme;imagen tridimensional	In many medical applications, the number of available two-dimensional (2-D) images is always insufficient. Therefore, the three-dimensional (3-D) reconstruction must be accomplished by appropriate interpolation methods to fill gaps between available image slices. Here, the authors propose a morphology-based algorithm to interpolate the missing data. The proposed algorithm consists of several steps. First, the object or hole contours are extracted using conventional image-processing techniques. Second, the object or hole matching issue is evaluated. Prior to interpolation, the centroids of the objects are aligned. Next, the authors employ a dilation operator to transform digital images into distance maps and they correct the distance maps if required. Finally, the authors utilize an erosion operator to accomplish the interpolation. Furthermore, if multiple objects or holes are interpolated, they blend them together to complete the algorithm. The authors experimentally evaluate the proposed method against various synthesized cases reported in the literature. Experimental results show that the proposed method is able to handle general object interpolation effectively.	algorithm;alignment;digital image;dilation (morphology);erosion (morphology);experiment;extraction;galaxy morphological classification;image processing;interpolation imputation technique;matching;map;mathematical morphology;missing data;pathological dilatation;physical object	Tong-Yee Lee;Wen-Hsiu Wang	2000	IEEE Transactions on Medical Imaging	10.1109/42.875193	computer vision;bilinear interpolation;morphology;image processing;interpolation;computer science;stairstep interpolation;mathematics;geometry;nearest-neighbor interpolation;algorithm;statistics;trilinear interpolation	Visualization	46.64778015488154	-79.73339795057002	177907
6cf96d06bb03301282769baaa09f8a557a1bf222	comprehensive maximum likelihood estimation of diffusion compartment models towards reliable mapping of brain microstructure	diffusion mri;maximul likelihood estimation;brain;diffusion compartment models;variable projection;mixture models	Diffusion MRI is a key in-vivo non invasive imaging capability that can probe the microstructure of the brain. However, its limited resolution requires complex voxelwise generative models of the diffusion. Diffusion Compartment (DC) models divide the voxel into smaller compartments in which diffusion is homogeneous. We present a comprehensive framework for maximum likelihood estimation (MLE) of such models that jointly features ML estimators of (i) the baseline MR signal, (ii) the noise variance, (iii) compartment proportions, and (iv) diffusion-related parameters. ML estimators are key to providing reliable mapping of brain microstructure as they are asymptotically unbiased and of minimal variance. We compare our algorithm (which efficiently exploits analytical properties of MLE) to alternative implementations and a state-of-theart strategy. Simulation results show that our approach offers the best reduction in computational burden while guaranteeing convergence of numerical estimators to the MLE. In-vivo results also reveal remarkably reliable microstructure mapping in areas as complex as the centrum semiovale. Our ML framework accommodates any DC model and is available freely for multi-tensor models as part of the ANIMA software.		Aymeric Stamm;Olivier Commowick;Simon K. Warfield;Simone Vantini	2016		10.1007/978-3-319-46726-9_72	diffusion mri;econometrics;mathematical optimization;radiology;computer science;machine learning;mixture model;statistics	ML	49.17007450942082	-79.28318125493647	180756
92863135459800748303554ec09e1c319e209d6f	structural adaptive segmentation for statistical parametric mapping	statistical parametric map;multiple testing;functional mri;signal detection;structure adaptive smoothing;multiscale testing;image enhancement;multiple comparisons;noise reduction;functional magnetic resonance images;adaptive smoothing;signal to noise ratio	Functional Magnetic Resonance Imaging inherently involves noisy measurements and a severe multiple test problem. Smoothing is usually used to reduce the effective number of multiple comparisons and to locally integrate the signal and hence increase the signal-to-noise ratio. Here, we provide a new structural adaptive segmentation algorithm (AS) that naturally combines the signal detection with noise reduction in one procedure. Moreover, the new method is closely related to a recently proposed structural adaptive smoothing algorithm and preserves shape and spatial extent of activation areas without blurring their borders.	acclimatization;adaptive filter;algorithm;algorithmic efficiency;blurred vision;column (database);detection theory;iterative method;kernel;magnetic resonance imaging;miller columns;noise reduction;normal statistical distribution;quantum field theory;request for tender;signal detection (psychology);signal-to-noise ratio;silo (dataset);simulation;smoothing (statistical technique);stripes;biologic segmentation;fmri;significance level	Jörg Polzehl;Henning U. Voss;Karsten Tabelow	2010	NeuroImage	10.1016/j.neuroimage.2010.04.241	computer vision;mathematical optimization;mathematics;scale-space segmentation;multiple comparisons problem;statistics;smoothing	ML	51.48583543848491	-76.525210126161	181428
21e3e1dcf6debe65cf59fb3301187c5e78a809d0	"""corrigendum to """"nonparametric joint shape learning for customized shape modeling"""" [comput. med. imaging graph. 34(2010) 298-307]"""				Gözde B. Ünal	2012	Comp. Med. Imag. and Graph.	10.1016/j.compmedimag.2012.01.001	mathematical optimization;machine learning;mathematics	Vision	48.400797231215556	-77.7631632648098	184024
0602482e55889e59ea6fe2b40c575146cbeb6bfc	an approximation approach to measurement design in the reconstruction of functional mri sequences	bayesian compressed sensing;functional mri;kalman filter;mutual information	The reconstruction quality of a functional MRI sequence is not only determined by the reconstruction algorithms but also by the information obtained from measurements. This paper addresses the measurement design problem of selecting k feasible measurements such that the mutual information between the unknown image and measurements is maximized, where k is a given budget. To calculate the mutual information, we utilize correlations of adjacent functional MR images via modelling an fMRI sequence as a linear dynamical system with an identity transition matrix. Our model is based on the key observation that variations of functional MR images are sparse over time in the wavelet domain. In cases where this sparsity constraint obtains, the measurement design problem is intractable. We therefore propose an approximation approach to resolve this issue. The experimental results demonstrate that the proposed approach successes in reconstructing functional MR images with greater accuracy than by random sampling.	approximation	Shulin Yan;Lei Nie;Chao Wu;Yike Guo	2013		10.1007/978-3-319-02753-1_12	computer vision;mathematical optimization;machine learning;mathematics	Vision	49.90933428773891	-78.95701406248692	184228
d8a4ee2db0353309756408e782c220148ac06300	regularization of diffusion tensor maps using a non-gaussian markov random field approach	gaussian markov random field;largest eigenvalue;density functional;eigenvectors;diffusion tensor	In this paper we propose a novel non-Gaussian MRF for regularization of tensor fields for fiber tract enhancement. Two entities are considered in the model, namely, the linear component of the tensor, i.e., how much line-like the tensor is, and the angle of the eigenvector associated to the largest eigenvalue. A novel, to the best of the author’s knowledge, angular density function has been proposed. Closed form expressions of the posterior densities are obtained. Some experiments are also presented for which color-coded images are visually meaningful. Finally, a quantitative measure of regularization is also calculated to validate the achieved results based on an averaged measure of entropy.	angularjs;bayesian network;entity;experiment;map;markov chain;markov random field;matrix regularization;planar (computer graphics);progressive enhancement;tract (literature)	Marcos Martín-Fernández;Carlos Alberola-López;Juan Ruiz-Alzola;Carl-Fredrik Westin	2003		10.1007/978-3-540-39903-2_12	diffusion mri;symmetric tensor;mathematical optimization;combinatorics;mathematical analysis;tensor field;tensor;radiology;eigenvalues and eigenvectors;cartesian tensor;tensor;tensor contraction;cauchy stress tensor;mathematics;tensor density	Vision	51.291306040166965	-73.74125294477221	184334
f65ff40f81b9d710246392ee1a77bd6b5a8f0804	projective filtering of time warped ecg beats	time warp;time series;projective filtering;ecg processing;signal processing;principal component analysis;state space;dynamic time warping	The paper proposes a modification of the nonlinear state-space projections (NSSP) method. Our approach, when applied to ECG signal processing, considerably improves the method's performance. One of the crucial operations in NSSP is the search for neighborhoods of the state-space trajectory points. The modification proposed is based on imposing a few restrictions on the time location of the neighborhood points. Dynamic time warping, a technique which allows for nonlinear alignment of time series or sequences of vectors, is applied as a straightforward solution to the task of neighborhoods determination with the restrictions imposed. The influence of nonlinear alignment on the distributions of the determined neighborhoods is presented, and the resulting method of ECG enhancement is investigated.	acclimatization;alignment;dimensions;display resolution;dynamic time warping;euclidean distance;hl7publishingsubsection <operations>;lss gene;noise reduction;nonlinear system;normal statistical distribution;projections and predictions;signal processing;signal-to-noise ratio;spatial variability;state space;time series	Marian Kotas	2008	Computers in biology and medicine	10.1016/j.compbiomed.2007.08.002	computer vision;mathematical optimization;computer science;state space;machine learning;dynamic time warping;signal processing;time series;mathematics;statistics;principal component analysis	ML	50.115096935267715	-75.20879773847982	185325
d929b5ac06c3e8a5e9a4918c78f1dbf6a7b3f89e	a spin glass based framework to untangle fiber crossing in mr diffusion based tracking	spin glass;white matter;diffusion weighted	We propose a general approach to the reconstruction of brain white matter geometry from diffusion-weighted data. This approach is based on an inverse problem framework. The optimal geometry corresponds to the lowest energy configuration of a spin glass. These spins represent pieces of fascicles that orient themselves according to diffusion data and interact in order to create low curvature fascicles. Simulated diffusion-weighted datasets corresponding to the crossing of two fascicle bundles are used to validate the method.	graph theory;information;map	Yann Cointepas;Cyril Poupon;Denis Le Bihan;Jean-Francois Mangin	2002		10.1007/3-540-45786-0_59	spin glass;geometry	Vision	47.2837729049721	-77.67122854741913	185409
8991e7c99f1f7e3894be67f20185cd8f4ff4e22f	a breast density-dependent power-law model for digital mammography	parenchymal texture;anatomic noise;power spectra;cascaded systems model	In mammograms, the parenchymal patterns have been described by a Wiener power spectrum that has an inverse power-law shape at low spatial frequencies. Its frequency content can then be described by the parameter, β. Previously, we have shown that there is some dependence of β on the relative amount of fibroglandular tissue or volumetric breast density (VBD). Here, we develop a mathematical model that simulates the distributions of β as a function of VBD. This model will be useful for clinically-relevant task-based image analyses that incorporate realistic tissue backgrounds.	algorithm;mathematical model;simulation;spectral density;tomosynthesis	James G. Mainprize;Martin J. Yaffe	2012		10.1007/978-3-642-31271-7_98	computer vision;speech recognition;pathology;engineering	Graphics	49.72409312743672	-76.27758905747498	185635
95965018ed9435f3f6902df6d80d1f575193821a	an analytical model of divisive normalization in disparity-tuned complex cells	probability density function;disparity estimation;computer simulation;complex cell;analytical model	Based on the energy model for disparity-tuned neurons, we calculate probability density functions of complex cell activity for random-dot stimuli. We investigate the effects of normalization and give analytical expressions for the disparity tuning curve and its variance. We show that while normalized and non-normalized complex cells have similar tuning curves, the variance is significantly lower for normalized complex cells, which makes disparity estimation more reliable. The results of the analytical calculations are compared to computer simulations.	binocular disparity;computer simulation;performance tuning	Wolfgang Stürzl;Hanspeter A. Mallot;Alois Knoll	2007		10.1007/978-3-540-74690-4_79	computer simulation;econometrics;probability density function;computer science;machine learning;mathematics;statistics	ML	49.73414951301005	-76.2502802434403	186089
8c7adaa206803e727c4f074f4f2ba30771f7da79	line integral convolution for visualization of fiber tract maps from dti	diffusion tensor images;rat brain;spinal cord;computer graphic;data visualization;line integral convolution;vector field;diffusion tensor	Diffusion tensor imaging (DTI) can provide the fundamental information required for viewing structural connectivity. However, robust and accurate acquisition and processing algorithms are needed to accurately map the nerve connectivity. In this paper, we present a novel algorithm for extracting and visualizing the fiber tracts in the CNS specifically, the spinal cord. The automatic fiber tract mapping problem will be solved in two phases, namely a data smoothing phase and a fiber tract mapping phase. In the former, smoothing is achieved via a weighted TV-norm minimization which strives to smooth while retaining all relevant detail. For the fiber tract mapping, a smooth 3D vector field indicating the dominant anisotropic direction at each spatial location is computed from the smoothed data. Visualization of the fiber tracts is achieved by adapting a known Computer Graphics technique called the line integral convolution, which has the advantage of being able to cope with singularities in the vector field and is a resolution independent way of visualizing the 3D vector field corresponding to the dominant eigen vectors of the diffusion tensor field. Examples are presented to depict the performance of the visualization scheme on three DT-MR data sets, one from a normal and another from an injured rat spinal cord and a third from a rat brain.	line integral convolution;tract (literature)	Tim McGraw;Baba C. Vemuri;Zhizhou Wang;Yunmei Chen;Murali Rao;Thomas H. Mareci	2002		10.1007/3-540-45787-9_77	diffusion mri;computer vision;vector field;line integral convolution;computer science;theoretical computer science;machine learning;mathematics;data visualization	Visualization	46.88986825581846	-75.62520507582198	186704
6233ed71841ffb8d00f50bb55e321d7476be7ebf	snakes and splines for tracking non-rigid heart motion	second order;time varying;two dimensions;dynamic programming algorithm;dynamic program;gradient descent;displacement vector field;vector field;coordinate system	MRI is unique in its ability to non-invasively and selectively alter tissue magnetization, and create tagged patterns within a deforming body such as the heart muscle. The resulting patterns (radial or SPAMM patterns) define a time-varying curvilinear coordinate system on the tissue, which we track with B-snakes and coupled B-snake grids. The B-snakes are optimized by a dynamic programming algorithm operating on B-spline control points in discrete pixel space. Coupled B-snake optimization based on an extension of dynamic programming to two dimensions, and gradient descent are proposed. Novel spline warps are also proposed which can warp an area in the plane such that two embedded snake grids obtained from two SPAMM frames are brought into registration, interpolating a dense displacement vector field. The reconstructed vector field adheres to the known displacement information at the intersections, forces corresponding snakes to be warped into one another, and for all other points in the plane, where no information is available, a second order continuous vector field is interpolated.	spline (mathematics)	Amir A. Amini;Rupert W. Curwen;John C. Gore	1996		10.1007/3-540-61123-1_144	gradient descent;position;mathematical optimization;two-dimensional space;vector field;euclidean vector;coordinate system;dynamic programming;control theory;mathematics;geometry;second-order logic	Vision	47.453414849894884	-75.53420569035195	187009
94dcba3e0826dcf1421c7c26f1539b250f850aea	poisson statistic and half-quadratic regularization for emission tomography reconstruction algorithm	convergence of numerical methods emission tomography image reconstruction medical image processing smoothing methods stochastic processes noise statistical analysis;electrical capacitance tomography;moise regularized algorithm;convergence;gaussian likelihood;probability;emission computerized tomography poisson statistic half quadratic regularization emission tomography reconstruction algorithm edge preserving smoothing nonlinear regularisation gaussian likelihood artur algorithm moise regularized algorithm statistical noise map em algorithm maximum a posteriori expectation maximization one step late technique numerical simulation data real data convergence;image processing;computed tomography;half quadratic;iterative algorithms;convergence of numerical methods;reconstruction algorithms;testing;artur algorithm;statistics reconstruction algorithms computed tomography smoothing methods electrical capacitance tomography iterative algorithms image reconstruction probability testing numerical simulation;real data;poisson statistic;smoothing methods;map em algorithm;statistical analysis;stochastic processes;emission tomography reconstruction algorithm;image reconstruction;medical image processing;emission tomography;edge preserving smoothing;computerized tomography;numerical simulation data;expectation maximization algorithm;statistics;statistical noise;half quadratic regularization;reconstruction algorithm;nonlinear regularisation;maximum a posteriori expectation maximization;emission computerized tomography;noise;one step late technique;numerical simulation	In emission computerized tomography, the use of realistic constraints such as edge-preserving smoothing lead to nonlinear regularisation. Charbonnier et al. (see IEEE Trans. on Image Processing, 1994) used the half-quadratic regularization in order to solve this problem. Applied together with a Gaussian likelihood, it formed the ARTUR algorithm. We propose a new regularized algorithm called MOISE which takes into account the Poisson nature of the statistical noise and uses this half-quadratic regularization. For that reason, MOISE differ from the MAP EM (maximum a posteriori expectation maximization) algorithm developed by P.J. Green (1990) which uses the one step late technique. We tested MOISE and compared it with ARTUR, on numerical simulation and real data. The results show that, despite the slowness of convergence, the half-quadratic regularization can be applied in the case of a Poisson statistic.	algorithm;tomography	Pierre Malick Koulibaly;Pierre Charbonnier;Laure Blanc-Féraud;Ivan Laurette;Jacques Darcourt;Michel Barlaud	1996		10.1109/ICIP.1996.560997	iterative reconstruction;edge-preserving smoothing;econometrics;mathematical optimization;convergence;expectation–maximization algorithm;image processing;computer science;noise;probability;mathematics;statistical noise;software testing;statistics	Vision	53.335949972845825	-75.10743148989486	188729
15bb596f2d190f754ee314fd377f967a192f4905	a 3d image smoothing method for dynamic functional imaging	image processing;3d imaging;smoothing method;statistical properties;functional imaging;smoothing;noise reduction;medical imaging;time domain;kinetics;domain specificity;spatial resolution	Dynamic functional imaging has been limited by the finite spatial resolution and high noise. Various smoothing methods have been proposed to reduce noise from functional image. However, these smoothing methods are usually based on the spatial domain and local statistical properties. Smoothing algorithms specifically designed for dynamic functional image data have not previously been investigated in detail. We present a new 3D smoothing method that aim to diminish the noise and improve the quality of the dynamic images. By taking advantage of domain specific physiological 3D kinetic feature related to functional images and their physiological activity information in time domain, this technique can provide much smoothed dynamic functional images with high noise reduction while preserving edges and subtle details.	functional imaging;smoothing	Tom Weidong Cai;David Dagan Feng;Roger R. Fulton	2000			computer vision;mathematical optimization;computer science;machine learning;smoothing	Vision	50.83493939121109	-77.27242932481145	189992
ff73088fd04eb4e913289e16201973258e81b4b1	image restoration in computed tomography: restoration of experimental ct images	image restoration computed tomography image storage pixel optimization methods adaptive filters power measurement noise measurement gradient methods spatial resolution;biophysics;iterative method;femur;computerized axial tomography;tomodensitometria;radiodiagnostic;optimisation;biophysique;osteopathie;restauration image;computed tomography;numerical solution;resolution spatiale;optimizacion;resolucion espacial;etude theorique;genie biomedical;osteoporose;etude experimentale;objet test;bone disease;rayon x;exploracion;image restoration;conjugate gradient method;diseases of the osteoarticular system;power spectrum;rayos x;test objet;metodo iterativo;local adaptation;restauracion imagen;radiodiagnostico;tomodensitometrie;biomedical engineering;methode iterative;point spread function;medical image processing;cadaver femur experimental ct images restoration blurred images restoration storage requirements optimization spatially variant point spread function computational speed quadrant symmetry approach cartesian coordinates locally adaptive restoration method noise prewhitening filter noise power spectrum conjugate gradient method numerical solution resolution phantom;estudio teorico;computerised tomography;osteoporosis;exploration;sistema osteoarticular patologia;ingenieria biomedica;optimization;medical image processing computerised tomography medical diagnostic computing;radiodiagnosis;theoretical study;x ray;osteopatia;medical diagnostic computing;objeto prueba;estudio experimental;systeme osteoarticulaire pathologie;measurement noise;biofisica;spatial resolution	It is pointed out that to restore experimental computed tomography (CT) images which have been blurred by a spatially variant point spread function (PSF), a quadrant symmetry method which simultaneously optimizes storage requirements for the estimated PSFs and computational speed is used. The quadrant symmetry approach required less than 0.579 Megawords of storage for 9x9 pixel spatially variant PSFs in a 256x256 pixel image, allowing image restoration in Cartesian coordinates. A locally adaptive restoration method formulated using a noise prewhitening filter derived from the measured noise power spectrum, the conjugate gradient method was used to obtain a numerical solution. Restored images for a resolution phantom and for a cadaver femur are presented.		Satyapal Rathee;Zoly J. Koles;Thomas R. Overton	1992	IEEE transactions on medical imaging	10.1109/42.192690	image restoration;computer vision;image resolution;exploration;point spread function;mathematics;iterative method;conjugate gradient method;optics;computed tomography;spectral density	Vision	52.37251506197124	-79.27843060298348	190879
1160301668210b635e80bb52ba27abce23f123c1	morphology-preserving smoothing on polygonized isosurfaces of inhomogeneous binary volumes	binary volume;morphology preservation;local spatial quadrics;mesh smoothing;polygonized isosurface	Polygonized isosurfaces of anatomical structures commonly suffer from severe artifacts (e.g., noise and staircases), due to inhomogeneous binary volumes. Most state-of-the-art techniques can reduce these artifacts but inevitably ruining anatomical structures’ morphology. Given an initial polygonization of an isosurface, we first eliminate these apparent staircases based on a context-aware Laplace filter, and then solve the morphology-preserving problem of anatomical structures as an optimization of the local spatial quadrics (LSQ) of fitted Bézier surfaces duringmesh evolution. This results in a conceptually simple approach that provides a unified framework for not only handling artifacts, but also for enabling the morphology preservation of anatomical structures. © 2014 Elsevier Ltd. All rights reserved.	algorithm;artifact (software development);bézier curve;case preservation;discrete laplace operator;galaxy morphological classification;isosurface;mathematical morphology;mathematical optimization;smoothing;unified framework;visual comparison	Mingqiang Wei;Lei Zhu;Jinze Yu;Wai-Man Pang;Jianhuang Wu;Jing Qin;Pheng-Ann Heng	2015	Computer-Aided Design	10.1016/j.cad.2014.08.015	computer science;geometry;engineering drawing;computer graphics (images)	Vision	49.45848919645535	-74.14510571257308	191098
0c7707878d87d80b70c383520df0b62798cc3384	nmr object boundaries: b-spline modeling and estimator performance	medical image processing biomedical nmr maximum likelihood estimation error analysis image reconstruction splines mathematics smoothing methods gaussian noise positron emission tomography single photon emission computed tomography;gaussian noise;nuclear magnetic resonance spline maximum likelihood estimation estimation error smoothing methods gaussian noise image reconstruction positron emission tomography;maximum likelihood;optimal estimation;biomedical nmr;cramer rao lower bound;mean square;maximum likelihood estimation;splines mathematics;positron emission tomography;error analysis;smoothing methods;penalized maximum likelihood;image reconstruction;medical image processing;single photon emission computed tomography;estimation error;side information;article;pet system nmr object boundaries b spline modeling estimator performance estimation error bounds optimal estimators closed boundary curves nmr image cramer rao lower bound mean square estimate error system smoothing gaussian noise performance penalized maximum likelihood estimators maximum likelihood estimators anatomical side information functional tomographic image reconstruction spect system	B-SPLINE MODELING AND ESTIMATOR PERFORMANCE Stephen R. Titus, Alfred O. Hero III, Je rey A. Fessler Department of EECS University of Michigan Ann Arbor, MI 48109 stitus@engin.umich.edu ABSTRACT We give estimation error bounds and specify optimal estimators for continuous, closed boundary curves in an NMR image. The boundary is parameterized using periodic B-Splines. A Cramer-Rao lower bound on mean-square-estimate error in the presence of system smoothing and Gaussian noise is derived, and the performance of maximum likelihood and penalized maximum likelihood estimators is compared to this bound. Finally, we comment on the usefulness of estimates of the boundary for providing anatomical side information in the reconstruction of functional tomographic images like those of a PET or SPECT system.	b-spline;computer science;gaussian blur;smoothing;tomography	Stephen R. Titus;Alfred O. Hero;Jeffrey A. Fessler	1995		10.1109/ICASSP.1995.479982	econometrics;mathematical optimization;mathematics;maximum likelihood;statistics	Vision	53.243472305497185	-75.07824379737256	191728
41d06fd405925a5f22add3a50185b24dedbbd5e5	particle-based sampling and meshing of surfaces in multimaterial volumes	visualizations;visualizations index terms 8212 sampling meshing;mathematics computing;particle based sampling;interaction;visual queries;geometry;computational geometry;index terms 8212;indexing terms;data mining;nonmanifold geometry;sampling methods solid modeling biological materials geometry biomedical materials magnetic resonance imaging data visualization data mining geophysics computing scientific computing;journal article;sampling;data visualisation;navigation;geophysics computing;multimaterial volumes;index terms sampling;visual exploration;labeled volume data;magnetic resonance imaging;solid modeling;meshing;delaunay based meshing algorithms;data visualization;particle system;scientific computing;material intersections;biological materials;quality index;labeled volume data particle based sampling multimaterial volumes nonmanifold geometry material intersections delaunay based meshing algorithms;sampling methods;visual analytics;mesh generation;multivariate data;sampling methods computational geometry data visualisation mathematics computing mesh generation;biomedical materials;volume data	Methods that faithfully and robustly capture the geometry of complex material interfaces in labeled volume data are important for generating realistic and accurate visualizations and simulations of real-world objects. The generation of such multimaterial models from measured data poses two unique challenges: first, the surfaces must be well-sampled with regular, efficient tessellations that are consistent across material boundaries; and second, the resulting meshes must respect the nonmanifold geometry of the multimaterial interfaces. This paper proposes a strategy for sampling and meshing multimaterial volumes using dynamic particle systems, including a novel, differentiable representation of the material junctions that allows the particle system to explicitly sample corners, edges, and surfaces of material intersections. The distributions of particles are controlled by fundamental sampling constraints, allowing Delaunay-based meshing algorithms to reliably extract watertight meshes of consistently high-quality.	algorithm;approximation;cns;computation;computational geometry;delaunay triangulation;gibbs sampling;gradient;ibm notes;inspiration function;interpolation;national center for research resources;particle system;physical object;preprocessor;sampling (signal processing);sampling - surgical action;scheme;silo (dataset);simulation;time complexity;tracer;trunk structure;united states national institutes of health;volume mesh;biomedical engineering field	Miriah D. Meyer;Ross T. Whitaker;Robert Michael Kirby;Christian Ledergerber;Hanspeter Pfister	2008	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2008.154	sampling;computer science;theoretical computer science;mathematics;geometry;data visualization;statistics;computer graphics (images)	Visualization	48.88021442449219	-77.11716075497907	193463
2549833a4324bde21add895f9d9ebbcc76f8e160	image statistics and anisotropic diffusion	eigenvalues and eigenfunctions;noise statistics;image processing;juser websearch publications database;juser;edge detection;publikationsdatenbank;image processing applications;diffusion approaches;diffusion processes;natural images;image statistics;anisotropic diffusion;websearch;eigenvalues;computer vision;learning systems;corrupted image data;local structure;statistics anisotropic magnetoresistance training data filters acoustic noise noise reduction statistical distributions inference algorithms layout probability;spatial statistics image statistics anisotropic diffusion sensing techniques image processing applications noisy image data corrupted image data image denoising diffusion approaches edge stopping function natural images eigenvalues local structure tensor image reconstruction noise statistics;machine vision;image reconstruction;noisy image data;statistics;learning artificial intelligence image denoising image reconstruction computer vision edge detection eigenvalues and eigenfunctions statistics diffusion;image denoising;learning artificial intelligence;local structure tensor;diffusion;spatial statistics;edge stopping function;sensing techniques;image edge analysis	Many sensing techniques and image processing applications are characterized by noisy, or corrupted, image data. Anisotropic diffusion is a popular, and theoretically well understood, technique for denoising such images. Diffusion approaches however require the selection of an “edge stopping” function, the definition of which is typically ad hoc. We exploit and extend recent work on the statistics of natural images to define principled edge stopping functions for different types of imagery. We consider a variety of anisotropic diffusion schemes and note that they compute spatial derivatives at fixed scales from which we estimate the appropriate algorithm-specific image statistics. Going beyond traditional work on image statistics, we also model the statistics of the eigenvalues of the local structure tensor. Novel edge-stopping functions are derived from these image statistics giving a principled way of formulating anisotropic diffusion problems in which all edge-stopping parameters are learned from training data.	algorithm;anisotropic diffusion;b-spline;cobham's thesis;column (database);gradient descent;hoc (programming language);image processing;iterative reconstruction;mathematical optimization;microsoft outlook for mac;mixture model;noise reduction;scene statistics;seagate st1;statistical model;structure tensor	Hanno Scharr;Michael J. Black;Horst W. Haussecker	2003		10.1109/ICCV.2003.1238435	iterative reconstruction;computer vision;edge detection;machine vision;image processing;eigenvalues and eigenvectors;computer science;machine learning;pattern recognition;mathematics;spatial analysis;diffusion;anisotropic diffusion	Vision	51.478806329491526	-74.26128628978869	195081
e39a0834122e08ba28e7b411db896d0fdbbad9ba	maximum likelihood estimation of depth maps using photometric stereo	gaussian noise;depth map estimation;lambertian model;maximum likelihood;gaussian processes;estimation method;maximum likelihood estimation mathematical model equations gaussian noise covariance matrix;linear regression;light microscopy;maximum likelihood estimation;linear system;image noise;extended yale face database b;finite difference method;finite difference methods photometric stereo depth map maximum likelihood estimation nonlinear regression;nonlinear regression estimates;maximum likelihood estimate;photometry;photometric stereo;image sequence;stereo image processing;mathematical model;stereo image processing gaussian processes maximum likelihood estimation photometry regression analysis;regression analysis;zero mean gaussian model;extended yale face database b maximum likelihood estimation photometric stereo depth map estimation lambertian model zero mean gaussian model image noise nonlinear regression estimates;depth map;finite difference methods;covariance matrix;nonlinear regression	Photometric stereo and depth-map estimation provide a way to construct a depth map from images of an object under one viewpoint but with varying illumination directions. While estimating surface normals using the Lambertian model of reflectance is well established, depth-map estimation is an ongoing field of research and dealing with image noise is an active topic. Using the zero-mean Gaussian model of image noise, this paper introduces a method for maximum likelihood depth-map estimation that accounts for the propagation of noise through all steps of the estimation process. Solving for maximum likelihood depth-map estimates involves an independent sequence of nonlinear regression estimates, one for each pixel, followed by a single large and sparse linear regression estimate. The linear system employs anisotropic weights, which arise naturally and differ in value to related work. The new depth-map estimation method remains efficient and fast, making it practical for realistic image sizes. Experiments using synthetic images demonstrate the method's ability to robustly estimate depth maps under the noise model. Practical benefits of the method on challenging imaging scenarios are illustrated by experiments using the Extended Yale Face Database B and an extensive data set of 500 reflected light microscopy image sequences.	algorithmic efficiency;anisotropic diffusion;approximation;arabic numeral 0;depth map;estimated;experiment;generalized least squares;gradient;image noise;lambertian reflectance;light microscopy;linear iga bullous dermatosis;linear regression body surface area formula for infants and children;linear system;nonlinear system;normal (geometry);normal statistical distribution;photometric stereo;photometry;pixel;signal-to-noise ratio;software propagation;sparse matrix;synthetic intelligence;time complexity;usability;weight;benefit	Adam P. Harrison;Dileepan Joseph	2012	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2011.249	econometrics;microscopy;pattern recognition;mathematics;maximum likelihood;maximum likelihood sequence estimation;statistics	Vision	52.640774492955934	-76.11885006437032	195352
2596797606638dba29a856e7a20770eb03c79160	globally optimal coupled surfaces for semi-automatic segmentation of medical images		Manual delineations are of paramount importance in medical imaging, for instance to train supervised methods and evaluate automatic segmentation algorithms. In volumetric images, manually tracing regions of interest is an excruciating process in which much time is wasted labeling neighboring 2D slices that are similar to each other. Here we present a method to compute a set of discrete minimal surfaces whose boundaries are specified by user-provided segmentations on one or more planes. Using this method, the user can for example manually delineate one slice every n and let the algorithm complete the segmentation for the slices in between. Using a discrete framework, this method globally minimizes a cost function that combines a regularizer with a data term based on image intensities, while ensuring that the surfaces do not intersect each other or leave holes in between. While the resulting optimization problem is an integer program and thus NP-hard, we show that the equality constraint matrix is totally unimodular, which enables us to solve the linear program (LP) relaxation instead. We can then capitalize on the existence of efficient LP solvers to compute a globally optimal solution in practical times. Experiments on two different datasets illustrate the superiority of the proposed method over the use of independent, label-wise optimal surfaces ((sim )5% mean increase in Dice when one every six slices is labeled, with some structures improving up to (sim )10% in Dice).	algorithm;experiment;integer programming;lp-type problem;linear programming relaxation;loss function;mathematical optimization;maxima and minima;medical imaging;np-hardness;optimization problem;region of interest;supervised learning;unimodular polynomial matrix	Juan Eugenio Iglesias	2017		10.1007/978-3-319-59050-9_48	linear programming relaxation;scale-space segmentation;algorithm;linear programming;tracing;computer science;segmentation;optimization problem;unimodular matrix;integer	Vision	48.10498870499527	-73.71402599391607	195811
9dfad5f51cd0fd58507d01feb890df4f9205f16e	two-dimensional phase unwrapping using robust derivative estimation and adaptive integration	interferometrie optique;light interferometry;metodo cuadrado menor;mrmcg;nuclear magnetic resonance imaging;metodo adaptativo;partial derivatives;methode moindre carre;least squares approximations;two dimensional phase unwrapping;magnetic fields;imagineria rmn;restauration image;image processing;least squares method;magnetic resonance images;phase images unwrapping;radar abertura sintetica;procesamiento imagen;two dimensional displays;image restoration;methode adaptative;least square method;indexing terms;conjugate gradient method;integration;noise robustness;traitement image;magnetic resonance image;fiber optic;2d phase unwrapping;algorithme;mask definition;algorithm;restauracion imagen;robust derivative estimation;conjugate gradient;mr imaging;optical imaging;nonrectangular objects;adaptive signal processing;noise level;adi method;magnetic resonance;modelo 2 dimensiones;metodo gradiente conjugado;integracion;phase estimation;deroulement phase;magnetic resonance imaging;noise levels;adaptive method;least square;phase estimation noise robustness magnetic resonance magnetic resonance imaging optical interferometry two dimensional displays noise level optical imaging magnetic fields blood flow;mrmcg two dimensional phase unwrapping 2d phase unwrapping robust derivative estimation partial derivatives noise robust adaptive integration adi method phase images unwrapping noise levels input images least squares methods conjugate gradient mrm method synthetic image shear magnetic resonance images fiber optic interferometry images mask definition block size computational requirements nonrectangular objects;estimacion parametro;modele 2 dimensions;conjugate gradient methods image processing adaptive signal processing noise least squares approximations magnetic resonance imaging light interferometry;imagerie rmn;input images;fiber optic interferometry images;optical interferometry;blood flow;parameter estimation;methode gradient conjugue;estimation parametre	The adaptive integration (ADI) method for two-dimensional (2-D) phase unwrapping is presented. The method uses an algorithm for noise robust estimation of partial derivatives, followed by a noise robust adaptive integration process. The ADI method can easily unwrap phase images with moderate noise levels, and the resulting images are congruent modulo 2pi with the observed, wrapped, input images. In a quantitative evaluation, both the ADI and the BLS methods (Strand et al.) were better than the least-squares methods of Ghiglia and Romero (GR), and of Marroquin and Rivera (MRM). In a qualitative evaluation, the ADI, the BLS, and a conjugate gradient version of the MRM method (MRMCG), were all compared using a synthetic image with shear, using 115 magnetic resonance images, and using 22 fiber-optic interferometry images. For the synthetic image and the interferometry images, the ADI method gave consistently visually better results than the other methods. For the MR images, the MRMCG method was best, and the ADI method second best. The ADI method was less sensitive to the mask definition and the block size than the BLS method, and successfully unwrapped images with shears that were not marked in the masks. The computational requirements of the ADI method for images of nonrectangular objects were comparable to only two iterations of many least-squares-based methods (e.g., GR). We believe the ADI method provides a powerful addition to the ensemble of tools available for 2-D phase unwrapping.	adaptive quadrature;algorithm;alternating direction implicit method;block size (cryptography);computation;conjugate gradient method;dna integration;display resolution;immunostimulating conjugate (antigen);instantaneous phase;integration host factors;interferometry;iteration;least squares;masks;modulo operation;newton's method;optical fiber;optics;physical object;register machine;requirement;resonance;strand (programming language);synthetic data;synthetic intelligence;tissue fiber	Jarle Strand;Torfinn Taxt	2002	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/TIP.2002.804567	computer vision;computer science;magnetic resonance imaging;mathematics;conjugate gradient method;least squares	Vision	53.26870697147241	-78.67389632198133	196114
80b11e5d1156da5c10803d5e663aebe3755ea5d1	an evolutionary algorithm for ct image reconstruction	ct image;boltzmann mutation;median mutation;genetic algorithms		ct scan;evolutionary algorithm;iterative reconstruction	Fathelalem F. Ali;Kazunori Matsuo;Zensho Nakao;Yen-Wei Chen	2000	JRM	10.20965/jrm.2000.p0466	mathematical optimization;bioinformatics;machine learning	Vision	52.756274116636845	-77.53883794790625	197353
90284b2c97c45ee0a3a6e5f538d4af096fe49cc3	fast and accurate modelling of longitudinal and repeated measures neuroimaging data	sensitivity and specificity;female;brain;data interpretation statistical;middle aged;qa mathematics;male;universiteitsbibliotheek;qp physiology;image enhancement;image interpretation computer assisted;neuroimaging;reproducibility of results;models statistical;algorithms;longitudinal studies;humans;mild cognitive impairment;alzheimer disease;computer simulation;aged;aged 80 and over	"""Despite the growing importance of longitudinal data in neuroimaging, the standard analysis methods make restrictive or unrealistic assumptions (e.g., assumption of Compound Symmetry--the state of all equal variances and equal correlations--or spatially homogeneous longitudinal correlations). While some new methods have been proposed to more accurately account for such data, these methods are based on iterative algorithms that are slow and failure-prone. In this article, we propose the use of the Sandwich Estimator method which first estimates the parameters of interest with a simple Ordinary Least Square model and second estimates variances/covariances with the """"so-called"""" Sandwich Estimator (SwE) which accounts for the within-subject correlation existing in longitudinal data. Here, we introduce the SwE method in its classic form, and we review and propose several adjustments to improve its behaviour, specifically in small samples. We use intensive Monte Carlo simulations to compare all considered adjustments and isolate the best combination for neuroimaging data. We also compare the SwE method to other popular methods and demonstrate its strengths and weaknesses. Finally, we analyse a highly unbalanced longitudinal dataset from the Alzheimer's Disease Neuroimaging Initiative and demonstrate the flexibility of the SwE method to fit within- and between-subject effects in a single model. Software implementing this SwE method has been made freely available at http://warwick.ac.uk/tenichols/SwE."""	algorithm;alzheimer's disease neuroimaging initiative;estimated;iterative method;kaplan–meier estimator;least-squares analysis;leucaena pulverulenta;monte carlo method;ordinary least squares;silo (dataset);simulation;unbalanced circuit;weakness	Bryan Guillaume;Xue Hua;Paul M. Thompson;Lourens J. Waldorp;Thomas E. Nichols	2014		10.1016/j.neuroimage.2014.03.029	psychology;computer simulation;alzheimer's disease;econometrics;neuroscience;medicine;computer science;artificial intelligence;statistics;neuroimaging	ML	48.80601108567961	-79.38669177779892	197805
c1c3324ee5253394b2602345fe6b388baaf7ec04	computationally efficient mutual information estimation for non-rigid image registration	pattern based video coding;histograms;estimation theory;kernel;complexity theory;cauchy schwartz quadratic mi;high dimensionality;pattern similarity metric;pattern motion;content sensitive thresholds;mutual information image registration entropy computational complexity histograms biomedical imaging image analysis magnetic analysis magnetic resonance imaging optimized production technology;video sequences;joints;indexing terms;magnetic resonance image;rate distortion optimization function;optimization problem;non rigid image registration;pattern mode;estimation;non rigid registration;computational complexity;motion vector;magnetic resonance imaging;image registration;image quality;h 264;mutual information;magnetic resonance imaging mutual information non rigid registration renyi s mi cauchy schwartz quadratic mi;entropy;threshold free pattern;motion estimation times;renyi s mi;lagrangian multipliers;arbitrary shape segmentation;macroblock;image quality threshold free pattern pattern based video coding h 264 arbitrary shape segmentation macroblock pattern mode content sensitive thresholds rate distortion optimization function motion estimation times video sequences computational complexity pattern motion motion vector pattern similarity metric lagrangian multipliers;image registration computational complexity estimation theory	The accuracy and computational complexity of mutual information (MI) estimation are critical factors in multi-modality non-rigid image registration. This paper discusses the accuracy and complexity of MI estimation approaches based on non-rigid registration functions. General formulations have been derived for Shannon's and Renyi's definitions of MI, as well as Cauchy- Schwartz quadratic MI. The results obtained indicate that a fuzzy histogram binning estimation approach is significantly faster and more accurate than the conventional non-parametric Parzen window estimation approach. The analytical formulations obtained for various MI definitions are continuously differentiable and are shown to be computationally efficient for high-dimensional optimization problems particularly for non-rigid image registration.	algorithmic efficiency;computational complexity theory;image registration;kernel density estimation;mathematical optimization;mutual information;product binning;shannon (unit);window function	Ali Gholipour;Nasser Kehtarnavaz	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4712124	image quality;optimization problem;computer vision;entropy;mathematical optimization;estimation;kernel;index term;image registration;magnetic resonance imaging;pattern recognition;histogram;mathematics;mutual information;lagrange multiplier;estimation theory;macroblock;computational complexity theory;statistics	Vision	48.4777004413482	-75.595027356131	198524
db01b6be21e40aaf060c69bb19a641e69e804202	finding deformable shapes by correspondence-free instantiation and registration of statistical shape models		This paper addresses the problem of finding a deformable shape by instantiation and registration of a statistical shape model (SSM) to the observation. A correspondence-free approach based on expectation conditional maximization (ECM) framework is proposed, and a robust and efficient implementation is presented. Preliminary experiments conducted on SSM of both femur and pelvis resulted in an average mean reconstruction error of 2.7mm (50 sparse observation points) and of 1.1mm/1.0mm (100 sparse points, with/without noise) for femur, as well as a mean reconstruction error of 4.36mm for pelvis, which demonstrated the efficacy of the proposed approach.	universal instantiation	Weiguo Xie;Steffen Schumann;Jochen Franke;Paul Alfred Grützner;Lutz-Peter Nolte;Guoyan Zheng	2012		10.1007/978-3-642-35428-1_32	computer vision;pattern recognition;mathematics;engineering drawing	Vision	46.928193109256654	-78.05460368048205	198855
