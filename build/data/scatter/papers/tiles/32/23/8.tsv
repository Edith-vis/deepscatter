id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4f85505e6bc85393fbffdd4a0ee45ecd55de5409	pop-up slam: semantic monocular plane slam for low-texture environments	layout;state estimation;image edge detection;three dimensional displays;solid modeling;simultaneous localization and mapping;cameras	Existing simultaneous localization and mapping (SLAM) algorithms are not robust in challenging low-texture environments because there are only few salient features. The resulting sparse or semi-dense map also conveys little information for motion planning. Though some work utilize plane or scene layout for dense map regularization, they require decent state estimation from other sources. In this paper, we propose real-time monocular plane SLAM to demonstrate that scene understanding could improve both state estimation and dense mapping especially in low-texture environments. The plane measurements come from a pop-up 3D plane model applied to each single image. We also combine planes with point based SLAM to improve robustness. On a public TUM dataset, our algorithm generates a dense semantic 3D model with pixel depth error of 6.2 cm while existing SLAM algorithms fail. On a 60 m long dataset with loops, our method creates a much better 3D model with state estimation error of 0.67%.	3d modeling;algorithm;autostereogram;color depth;matrix regularization;motion planning;pixel;real-time clock;semiconductor industry;simultaneous localization and mapping;sparse matrix	Shichao Yang;Yu Song;Michael Kaess;Sebastian Scherer	2016	2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2016.7759204	layout;computer vision;simulation;computer science;artificial intelligence;solid modeling;computer graphics (images);simultaneous localization and mapping	Robotics	52.25253531506851	-46.21903688863575	152801
79882f6bcca94a0308afdefe77531edab63d9dc1	cramér-rao bound for line constrained trajectory tracking		In this paper, target tracking constrained to short-term linear trajectories is explored. The problem is viewed as an extension of the matrix decomposition problem into low-rank and sparse components by incorporating an additional line constraint. The Cramér-Rao Bound (CRB) for the trajectory estimation is derived; numerical results show that an alternating algorithm which estimates the various components of the trajectory image is near optimal due to proximity to the computed CRB. In addition to the theoretical contribution of incorporating an additional constraint in the estimation problem, the alternating algorithm is applied to real video data and shown to be effective in estimating the trajectory despite it not being exactly linear.	algorithm;background subtraction;numerical analysis;sparse matrix;the matrix	Amr Elnakeeb;Urbashi Mitra	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462512	mathematical optimization;sparse matrix;computer science;cramér–rao bound;trajectory;matrix decomposition	Robotics	51.708810283563984	-48.5236006148386	152836
a6c46ed45ee3f252f2c2b64d48f8af8b8118aef4	optical flow and deformable objects	eigenvalues and eigenfunctions;motion field;plane;optical device;simulated deformations optical flow deformable objects planar linear vector field optical device 2d motion field eigenvalues projective invariant normal rototranslation angular velocity;planar linear vector field;projective invariant;eigenvalues;image motion analysis optical devices vectors deformable models motion analysis virtual colonoscopy eigenvalues and eigenfunctions analytical models image analysis information analysis;eigenvalues and eigenfunctions image sequences angular velocity;deformable objects;deformation;simulated deformations;angular velocity;optical flow;2d motion field;normal rototranslation;vector field;image sequences;singular point	When a plane undergoes a deformation that can be represented by a planar linear vector eld, the projected vector eld on the image plane of an optical device is at most quadratic. This 2D motion eld has one singular point, with eigenvalues identical to those of the singular point describing the deformation. As a consequence, the nature of the singular point of the deformation is a projective invariant. When the plane moves and experiences a linear deformation at the same time, the associated 2D motion eld is still quadratic with at most 3 singular points. In the case of a normal rototranslation, i.e. when the angular velocity is normal to the plane, and of a linear deformation , the 2D motion eld has at most one singular point and substantial information on the rigid motion and on the deformation can be recovered from it. Experiments with simulated deformations and real de-formable objects show that the proposed analysis can provide accurate results and information on more general 3D deformations.	angularjs;experiment;image plane;optical flow;simulated annealing;simulation;velocity (software development)	Andrea Giachetti;Vincent Torre	1995		10.1109/ICCV.1995.466869	computer vision;singular point of a curve;vector field;topology;eigenvalues and eigenvectors;angular velocity;computer science;plane;optical flow;mathematics;geometry;motion field;deformation	Vision	53.688385171907974	-50.75163862001327	154408
2a9187c1d9e9c52efd5e8408422c0ecbe82e91b0	human tracking by structured body parts	pattern clustering;optimal estimation;structured body parts human tracking;joints;human tracking;shape;image color analysis;feature extraction;clustering method;human body;object tracking;pattern clustering feature extraction object tracking;structured body parts;humans;global similarity human tracking structured body part nonrigid object tracking shape variation patches clustering method structured constraint method;target tracking;humans target tracking image color analysis joints conferences shape;conferences	Tracking non-rigid objects with significant shape variation in complex scenario is a difficult problem. Human tracking is a special case of this problem since human body has good local rigid properties. In this paper, we propose a novel human tracking method which explores the local rigid properties while keeping the global structure very well. This method consists of three stages. First, the human body is represented by structured rigid parts extracted using patches clustering method. Then, the rigid parts are tracked through a structured constraint method. Finally, the optimal estimated state of the object is obtained through global similarity. Experimental results show that the proposed method has good performance for human tracking with big posture and shape change.	cluster analysis;human-based computation;poor posture;self-similarity	Yingkun Xu;Lei Qin;Shuqiang Jiang;Qingming Huang	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116101	optimal estimation;computer vision;human body;simulation;feature extraction;shape;computer science;video tracking;pattern recognition	Robotics	48.404053636644704	-49.308159175204075	154460
38137e0276cfe494582c3684ba3de7ff70102572	articulated body motion capture by annealed particle filtering	electrical capacitance tomography;general tracking;filtering;articulated body motion tracking;annealing;legged locomotion;application software;degree of freedom;annealed particle filtering;search algorithm;biological system modeling;motion trajectories;biomechanics;motion estimation;simulated annealing;annealing humans animation biological system modeling electrical capacitance tomography filtering application software legged locomotion stochastic processes computational complexity;motion tracking;configuration space;motion capture;view restrictions;stochastic processes;optical tracking;computational complexity;particle filter;modified particle filter;animation;high dimensional configuration spaces;filtering theory motion estimation optical tracking search problems biomechanics simulated annealing;colour coding;humans;search problems;full articulated body motion recovery;full articulated body motion recovery articulated body motion capture annealed particle filtering articulated body motion tracking search algorithms exponential computational complexity colour coding prior assumptions motion trajectories view restrictions general tracking modified particle filter high dimensional configuration spaces continuation principle annealing narrow peaks fitness function;articulated body motion capture;exponential computational complexity;prior assumptions;filtering theory;narrow peaks;search algorithms;continuation principle;fitness function	The main challenge in articulated body motion tracking is the large number of degrees of freedom (around 30) to be recovered. Search algorithms, either deterministic o r stochastic, that search such a space without constraint, fa ll foul of exponential computational complexity. One approac h is to introduce constraints — either labelling using marker s or colour coding, prior assumptions about motion trajectories or view restrictions. Another is to relax constraint s arising from articulation, and track limbs as if their motions were independent. In contrast, here we aim for general tracking without special preparation of subjects or restri ctive assumptions. The principal contribution of this paper is the development of a modified particle filter for search in high dimensional configuration spaces. It uses a continuation principle, based on annealing, to introduce the influence of narrow peaks in the fitness function, gradually. The new algorithm, termed annealed particle filtering, is shown to be capable of recovering full articulated body motion efficiently.	biconnected component;computational complexity theory;condensation algorithm;configuration interaction;constraint (mathematics);continuation;decade (log scale);fitness function;jd - java decompiler;kinesiology;motion capture;particle filter;real-time transcription;simulated annealing;stochastic gradient descent;time complexity	Jonathan Deutscher;Andrew Blake;Ian D. Reid	2000		10.1109/CVPR.2000.854758	computer vision;mathematical optimization;simulation;computer science;biomechanics;machine learning;mathematics;statistics;search algorithm	Vision	47.575019640824486	-48.923090485215596	154469
eeef2c4f1cc8e54152e03725daba4fd03ebba8c2	feature-based real-time video stabilization for vehicle video recorder system	video stabilization;moving camera;optical flow;feature point	This paper presents a fast and effective method based on features to obtain real-time video stabilization for vehicle video recorder system. The corresponding feature points are first obtained from two consecutive frames and then optical flows are calculated based on these points. Next, the obtained optical flows are mapped to polar coordinates to obtain clusters and remove incorrect optical flows. These obtained clusters are used to evaluate the global motion and rotation angle. Finally, the obtained global motion and rotation angle are smoothed and then compensated to obtain the stabilized video. Experimental results show that the proposed method has good performance for video stabilization.	computation;digital video recorder;effective method;experiment;optical flow;real-time clock;real-time operating system;sensor;smoothing	Wu-Chih Hu;Chao-Ho Chen;Yi-Jen Su;Tzu-Hsing Chang	2017	Multimedia Tools and Applications	10.1007/s11042-017-4369-7	computer science;artificial intelligence;computer vision;motion compensation;videocassette recorder;block-matching algorithm;image stabilization;polar coordinate system;effective method;optical flow	Robotics	48.745165523409945	-46.27375115664573	155293
286fb7d654d9e5e6fc54a51374117e1eafe63cb3	human pose extraction from monocular videos using constrained non-rigid factorization.	image features;non linear programming;statistical model;motion capture	We focus on the problem of automatically extracting the 3D configuration of human poses from 2D image features tracked over a finite interval of time . This problem is highly non-linear in nature and confounds standard regression techniques. Our approach effectively marries a non-rigid factorization algorithm with prior learned statistical models from archival motion capture database. We show that a stand alone non-rigid factorization algorithm is highly unsuitable for this problem. However, when coupled with the learned statistical model in the form of a constrained nonlinear programming method, it yields a substantially better solution.	algorithm;archive;emoticon;iterative method;iterative refinement;kinesiology;lagrange multiplier;mathematical optimization;motion capture;nonlinear programming;nonlinear system;optimization problem;refinement (computing);semiconductor industry;stationary process;statistical model;window function	Appu Shaji;Behjat Siddiquie;Sharat Chandran;David Suter	2007		10.5244/C.21.92	statistical model;computer vision;motion capture;nonlinear programming;computer science;machine learning;pattern recognition;feature;statistics	Vision	51.12262327357703	-50.08122792695359	155375
965c8c4b0b3fbed1f5503f5c6c3329fa6236899d	fast and efficient depth map estimation from light fields		The paper presents an algorithm for depth map estimation from the light field images in relatively small amount of time, using only single thread on CPU. The proposed method improves existing principle of line fitting in 4- dimensional light field space. Line fitting is based on color values comparison using kernel density estimation. Our method utilizes result of Semi-Global Matching (SGM) with Census transform-based matching cost as a border initialization for line fitting. It provides a significant reduction of computations needed to find the best depth match. With the suggested evaluation metric we show that proposed method is applicable for efficient depth map estimation while preserving low computational time compared to others.	algorithm;central processing unit;computation;depth map;kernel density estimation;light field;line fitting;second generation multiplex;time complexity	Yuriy Anisimov;Didier Stricker	2017	2017 International Conference on 3D Vision (3DV)	10.1109/3DV.2017.00046	3d reconstruction;pattern recognition;computer vision;computer science;light field;initialization;kernel density estimation;line fitting;thread (computing);artificial intelligence;depth map	Robotics	50.26585421328049	-51.522199063504175	156932
1dafc61f2206b4778909d8dda7e0709053d0dd63	element-free elastic models for volume fitting and capture	moving object;distance measure;surface fitting;deformable models;surface reconstruction;noise robustness;iterative methods;surface fitting shape deformable models potential energy energy measurement volume measurement cameras noise robustness surface reconstruction iterative methods;shape;energy measurement;visual hull;volume measurement;point cloud;potential energy;cameras	We present a new method of fitting an element-free volumetric model to a sequence of deforming surfaces of a moving object. Given a sequence of visual hulls, we iteratively fit an element-free elastic model to the visual hull in order to extract the optimal pose of the captured volume. The fitting of the volumetric model is acheived by minimizing a combination of elastic potential energy, a surface distance measure, and a self-intersection penalty for each frame. A unique aspect of our work is that the model is mesh free - since the model is represented as a point cloud, it is easy to construct, manipulate and update the model as needed. Additionally, linear elasicity with rotation compensation makes it possible to handle local deformations and large rotations of body parts much more efficiently than other volume fitting approaches. Our experimental results for volume fitting and capture in a multi-view camera setting demonstrate the robustness of element-free elastic models against noise and self-occlusions.	algorithmic efficiency;color;computation;computer vision;curve fitting;elasticity (data store);iterative method;motion capture;motion compensation;online and offline;point cloud;real-time clock;refinement (computing);simulation;texture mapping;visual hull;volume mesh	Jaeil Choi;Andrzej Szymczak;Greg Turk;Irfan A. Essa	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.110	computer vision;mathematical optimization;surface reconstruction;shape;potential energy;point cloud;mathematics;geometry;iterative method	Vision	52.43140310177372	-50.901227573761986	156938
6304819129f5042df7aaab3e59c619a8778dd170	algebraic methods for image processing and computer vision	computer aided design;vision ordenador;design automation;image numerique;estimation mouvement;image processing;image processing computer vision polynomials equations motion estimation image sequences computational geometry design automation robot kinematics collision avoidance;algebraic methods;inverse position problems;cad;inverse problems computer vision motion estimation polynomials computational geometry cad kinematics robot vision mobile robots;grobner basis;computational geometry;procesamiento imagen;motion estimation;mobile robots;robotics;base grobner;grobner bases;kinematics;simultaneous polynomial equations;traitement image;uniqueness of solution;three dimensional;polynomials;computer vision;equation polynomiale;polynomial equation;algebraic geometry;nombre bezout;robot vision;ecuacion polinomial;algebraic method;imagen numerica;homotopy methods;surface intersection;3d motion structure;geometria algebraica;homotopy methods algebraic methods image processing computer vision simultaneous polynomial equations uniqueness algebraic geometry 3d motion estimation 3d motion structure surface intersection computer aided design inverse position problems kinematics robotics bezout numbers grobner bases;vision ordinateur;collision avoidance;digital image;3d motion estimation;uniqueness;inverse problems;robot kinematics;geometrie algebrique;image sequences;bezout numbers	Many important problems in image processing and computer vision can be formulated as the solution of a system of simultaneous polynomial equations. Crucial issues include the uniqueness of solution and the number of solutions (if not unique), and how to find numerically all the solutions. The goal of this paper is to introduce to engineers and scientists some mathematical tools from algebraic geometry which are very useful in resolving these issues. Three-dimensional motion/structure estimation is used as the context. However, these tools should also be helpful in other areas including surface intersection in computer-aided design, and inverse position problems in kinematics/robotics. The tools to be described are Bezout numbers, Grobner bases, homotopy methods, and a powerful theorem which states that under rather general conditions one can draw general conclusions on the number of solutions of a polynomial system from a single numerical example.		Robert J. Holt;Thomas S. Huang;Arun N. Netravali	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.503913	mobile robot;three-dimensional space;computer vision;kinematics;combinatorics;discrete mathematics;image processing;algebraic geometry;computational geometry;computer science;inverse problem;gröbner basis;motion estimation;cad;mathematics;geometry;robotics;digital image;robot kinematics;algorithm;polynomial	Vision	53.20595200416235	-51.382650297950086	157487
2afbd5f0e809a0f3ef128ddf968ff8fe3ccb01f3	learning image alignment without local minima for face detection and tracking	minimisation;fitness cost;subspace learning;cost function;image alignment;nickel;surface fitting;optimization learning image alignment local minima face detection tracking active appearance model surface fitting cost function;active appearance model;face alignment;surface fitting cost function;computational modeling;tracking face recognition learning artificial intelligence minimisation object detection surface fitting;face recognition;shape;face detection cost function active appearance model principal component analysis training data surface reconstruction optimization methods shape testing robots;principal component analysis;pixel;learning image alignment;face;optimization;learning artificial intelligence;face detection;local minima;tracking;object detection	Active appearance models (AAMs) have been extensively used for face alignment during the last 20 years. While AAMs have numerous advantages relative to alternate approaches, they suffer from two major drawbacks: (i) AAMs are especially prone to local minima in the fitting process; (ii) few if any of the local minima of the cost function correspond to acceptable solutions. To minimize these problems, this paper proposes a method to learn the fitting cost function that explicitly optimizes that the local minima occur at and only at the places corresponding to the correct fitting parameters. The paper explores two methods to parameterize the cost function: pixel weighting and subspace learning. Experiments on synthetic and real data show the effectiveness of our approach for face alignment.	active appearance model;experiment;face detection;information;loss function;maxima and minima;pixel;quadratic function;synthetic intelligence;template matching	Minh Hoai Nguyen;Fernando De la Torre	2008	2008 8th IEEE International Conference on Automatic Face & Gesture Recognition	10.1109/AFGR.2008.4813455	computer vision;computer science;machine learning;pattern recognition	Vision	47.86651403214878	-51.37523815528059	157546
73966f821e3304504eac9fe7049dc66335c9b08e	lidar-based object tracking and shape estimation using polylines and free-space information		Reliable object perception is a vital requirement for automated driving. Despite the availability of precise contour measurements, most state-of-the-art tracking systems still represent object geometry as bounding boxes. However, there are objects operating in public traffic for which the box assumption is highly inappropriate. We therefore propose to represent object contours using 2D polylines. Taking into account the mutual dependence of object poses and shape, our tracking framework targets at a simultaneous estimation of both states. Moreover, we propose to augment scan segments with free-space information at their boundaries and show how this knowledge can be incorporated into the tracking framework and beyond. Evaluation with real scan data shows that our method produces accurate dynamic estimates and consistent shape reconstructions.		Stefan Kraemer;Christoph Stiller;Mohamed Essayed Bouzouraa	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8593385	computer vision;radar tracker;artificial intelligence;bounding overwatch;video tracking;tracking system;computer science;lidar	Robotics	51.52266872437198	-45.61972809829832	158038
8ef8ff3f4d9e17fde5adea7247f9d43e2aea0604	depth video based human model reconstruction resolving self-occlusion	time of flight;monocular time of flight camera depth video human model reconstruction self occlusion computer vision animation 3dtv 3d human model image sequence;depth image;video signal processing;self occlusion;human modeling;time of flight camera;three dimensional television;self occlusion problem;computer vision;3d human model;computational modeling;time of flight camera depth image human modeling self occlusion problem;3d model;three dimensional displays;human model reconstruction;image reconstruction;human body;pixel;solid modeling;image sequence;animation;monocular time of flight camera;depth video;three dimensional displays humans cameras noise pixel solid modeling computational modeling;3dtv;humans;computer animation;cameras;video signal processing computer animation computer vision image reconstruction image sequences solid modelling three dimensional television;solid modelling;noise;image sequences	Generation of a 3D model has been actively researched in computer vision area. Especially, modeling of a realistic human is a topic of great interest due to its use in many application areas such as animation and 3DTV. In this paper, a novel method is presented to generate a 3D human model using a sequence of depth images from a monocular TOF (time-of-flight) camera. Mainly, we solve a 'self-occlusion' problem which occurs in the depth data where a human model is partially occluded by other parts of a human body. This becomes very critical when we want to develop a human model for various applications. We demonstrate the capability of our method by showing experimental results that resolves a multiple occlusion problem.	3d reconstruction;3d television;algorithm;computer vision;data acquisition;depth map;encode;hidden surface determination;object-based language;stereopsis;stereoscopy	In Yeop Jang;Kwan H. Lee	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5606349	iterative reconstruction;anime;computer vision;time of flight;human body;simulation;computer science;noise;computer animation;solid modeling;computational model;pixel;computer graphics (images)	Vision	52.24486241009682	-51.14563836440144	158220
32e523ee43d4dc4fd4ecd9b0563ca484f11a99ea	human pose estimation from silhouettes - a consistent approach using distance level sets	human tracking;level set methods;fast marching methods;constrained optimization;model-based estimation;three dimensional;fast marching method;pose estimation;level set method;level set	We present a novel similarity measure (likelihood) for estimating three-dimensional human pose from image silhouettes in model-based vision applications. One of the challenges in such approaches is the construction of a model-to-image likelihood that truly reflects the good configurations of the problem. This is hard, commonly due to the violation of consistency principle resulting in the introduction of spurious, unrelated peaks/minima that make the search for model localization difficult. We introduce an entirely continuous formulation which enforces model estimation consistency by means of an attraction/explanation silhouette-based term pair. We subsequently show how the proposed method provides significant consolidation and improved attraction zone around the desired likelihood configurations and elimination of some of the spurious ones. Finally, we present a skeleton-based smoothing method for the image silhouettes that stabilizes and accelerates the search process.	3d pose estimation;fast marching method;mathematical optimization;maxima and minima;normal (geometry);semiconductor consolidation;similarity measure;smoothing;straight skeleton	Cristian Sminchisescu;Alexandru Telea	2002				Vision	50.216226736389125	-52.00500725858085	158566
29dd33e48327ab3ba6619f37e1b56eb96c18e752	robust and fast teat detection and tracking in low-resolution videos for automatic milking devices		We present a system for detection and tracking of cow teats, as part of the construction of automatic milking devices (AMDs) in the dairy industry. We detail algorithmic solutions for the robust detection and tracking of teat tips in low-resolution video streams produced by embedded time-of-flight cameras, using a combination of depth images and point-cloud data. We present a visual analysis tool for the validation and optimization of the proposed techniques. Compared to existing state-of-the-art solutions, our method can robustly handle occlusions, variable poses, and geometries of the tracked shape, and yields a correct tracking rate for over 90% for tests involving real-world images obtained from an industrial AMD robot.	algorithm;automatic milking;cloud computing;embedded system;end system;end-to-end principle;image resolution;mathematical optimization;point cloud;refinement (computing);rendering (computer graphics);robot;sensor;sion's minimax theorem;streaming media;template matching;visual analytics	Matthew van der Zwan;Alexandru Telea	2015		10.5220/0005299205200530	computer vision;simulation	Robotics	53.75193354296926	-46.102069412739326	158656
51075bc0ca88a32cec60625cfd24043f1b70e7df	linear-time joint probabilistic data association for multiple extended object tracking		The Joint Probabilistic Data Association (JPDA) filter for multiple object tracking is based on the assumption that at most one measurement originates from a target object. However, with the development of high-resolution sensors, it is often the case that multiple spatially distributed detections are obtained from a single object. To tackle this emerging data association challenge, this paper presents a JPDA method based on the Poisson spatial measurement model for extended objects. As the constraint that one target gets at most one measurement is relaxed, the marginal association probabilities can be obtained with linear complexity in the number of measurements and targets. The proposed method is compared to a partition-based multiple extended object tracking algorithm.	algorithm;correspondence problem;image resolution;java platform debugger architecture;marginal model;sensor;time complexity	Shishan Yang;Kolja Thormann;Marcus Baum	2018	2018 IEEE 10th Sensor Array and Multichannel Signal Processing Workshop (SAM)	10.1109/SAM.2018.8448430	mathematical optimization;time complexity;computer science;probabilistic logic;video tracking;partition (number theory);clutter;poisson distribution	Robotics	46.871021411607025	-46.525975331671624	158905
405438ec7ce36a4e6ec476224ed1540d650d52f1	deformable and articulated 3d reconstruction from monocular video sequences	thesis;computer science	This thesis addresses the problem of deformable and articul tedstructure from motionfrom monocular uncalibrated video sequences. Structure from mo tion is defined as the problem of recovering information about the 3D structure of scenes ima ged by a camera in a video sequence. Our study aims at the challenging problem of non-rigid shape s (e.g. a beating heart or a smiling face). Non-rigid structures appear constantly in our every day life, think of a bicep curling, a torso twisting or a smiling face. Our research seeks a genera l method to perform 3D shape recovery purely from data, without having to rely on a pre-co mputed model or training data. Open problems in the field are the difficulty of the non-linear stimation, the lack of a real-time system, large amounts of missing data in real-world video se quences, measurement noise and strong deformations. Solving these problems would take us f ar beyond the current state of the art in non-rigid structure from motion. This dissertation p resents our contributions in the field of non-rigid structure from motion, detailing a novel algor ithm that enforces the exact metric structure of the problem at each step of the minimisation by p rojecting the motion matrices onto the correct deformable or articulated metric motion manifoldsrespectively. An important advantage of this new algorithm is its ability to handle miss ing data which becomes crucial when dealing with real video sequences. We present a generic bilinear estimation framework, which improves convergence and makes use of the manifold con strai ts. Finally, we demonstrate a sequential, frame-by-frame estimation algorithm, which provides a 3D model and camera parameters for each video frame, while simultaneously buil ding a model of object deformations. Submitted for the degree of Doctor of Philosophy Queen Mary, University of London	3d reconstruction;algorithm;bilinear filtering;bilinear transform;genera;graph edit distance;missing data;naruto shippuden: clash of ninja revolution 3;nonlinear system;real-time computing;real-time transcription;structure from motion	Marco Paladini	2012			computer vision;multimedia;computer graphics (images)	Vision	51.65454737636808	-49.23173995836444	159261
5a25d2ef7a8bd2234f061ccaf5b311595e316914	stable pose estimation with a motion model in real-time application	estimation cameras kalman filters real time systems computational modeling transmission line matrix methods motion measurement;kalman filters;geometry;pose ambiguity;computer vision;pose stabilization;computational modeling;estimation;real time systems computer vision geometry kalman filters pose estimation;transmission line matrix methods;motion measurement;pose stabilization pose estimation pose ambiguity;kalman filter stable pose estimation real time application object pose computer vision calibrated camera real time pose estimation algorithms target object geometric illusions error function pose jumping pose jittering pose ambiguity problem;cameras;real time systems;pose estimation	Estimation of a object pose from camera is a well-developing topic in computer vision. In theory, the pose from a calibrated camera can be uniquely determined. But in practice, most of the real-time pose estimation algorithms suffer from pose ambiguity due to low accuracy of the target object. We think that pose ambiguity¡Xtwo distinct local minima of the according error function¡Xexist because of the phenomenon of geometric illusions. Both of the ambiguous poses are plausible. After obtaining the solution of two minima (pose candidates), we develop a real-time algorithm for stable pose estimation of a target objects with a motion model. In the experimental results, the proposed algorithm diminish the significance of pose jumping and pose jittering effectively. To the best of our knowledge, this is the first work to solve the pose ambiguity problem with motion model in real-time application.	3d pose estimation;algorithm;computer vision;maxima and minima;real-time clock;real-time computing;real-time locating system	Po-Chen Wu;Jui-Hsin Lai;Ja-Ling Wu;Shao-Yi Chien	2012	2012 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2012.176	kalman filter;computer vision;estimation;simulation;pose;3d pose estimation;computer science;control theory;articulated body pose estimation;computational model	Robotics	50.845896711864995	-46.84685890743975	159609
1350ef778ccb5ead83963daf0b88c046756a2003	divide and conquer: a hierarchical approach to large-scale structure-from-motion		In this paper we present a novel pipeline for large-scale SfM. We first organise the images into a hierarchical tree built using agglomerative clustering. The SfM problem is then solved by reconstructing smaller image sets and merging them into a common frame of reference as we move up the tree in a bottom-up fashion. Such an approach drastically reduces the computational load for matching image pairs without sacrificing accuracy. It also makes the resulting sequence of bundle adjustment problems well-conditioned at all stages of reconstruction. We use motion averaging followed by global bundle adjustment for reconstruction of each individual cluster. Our 3D registration or alignment of partial reconstructions based on epipolar relationships is both robust and reliable and works well even when the available camera-point relationships are poorly conditioned. The overall result is a robust, accurate and efficient pipeline for large-scale SfM. We present extensive results that demonstrate these attributes of our pipeline on a number of large-scale, real-world datasets and compare with the state-of-the-art.	batch processing;bottom-up parsing;bundle adjustment;cluster analysis;computation;condition number;emoticon;epipolar geometry;speedup;top-down and bottom-up design;workstation	Brojeshwar Bhowmick;Suvam Patra;Avishek Chatterjee;Venu Madhav Govindu;Subhashis Banerjee	2017	Computer Vision and Image Understanding	10.1016/j.cviu.2017.02.006	computer vision;mathematical optimization;machine learning;mathematics	Vision	51.9779087705555	-47.90387817869464	159886
45dfc2055a4363b1b2644e317b84b94f1e5ef497	direct solution to the minimal generalized pose	cameras estimation polynomials vectors computational modeling;polynomials;computational modeling;vectors;estimation;minimal problems in computer vision absolute pose generalized camera models homography matrix;minimal based method minimal generalized pose estimation imaging systems augmented reality robotics minimal pose problem generalized camera models planar homography geometric elements image pixel three dimensional world coordinate system;cameras;pose estimation	Pose estimation is a relevant problem for imaging systems whose applications range from augmented reality to robotics. In this paper we propose a novel solution for the minimal pose problem, within the framework of generalized camera models and using a planar homography. Within this framework and considering only the geometric elements of the generalized camera models, an imaging system can be modeled by a set of mappings associating image pixels to 3-D straight lines. This mapping is defined in a 3-D world coordinate system. Pose estimation performs the computation of the rigid transformation between the original 3-D world coordinate system and the one in which the camera was calibrated. Using synthetic data, we compare the proposed minimal-based method with the state-of-the-art methods in terms of numerical errors, number of solutions and processing time. From the experiments, we conclude that the proposed method performs better, especially because there is a smaller variation in numerical errors, while results are similar in terms of number of solutions and computation time. To further evaluate the proposed approach we tested our method with real data. One of the relevant contributions of this paper is theoretical. When compared to the state-of-the-art approaches, we propose a completely new parametrization of the problem that can be solved in four simple steps. In addition, our approach does not require any predefined transformation of the dataset, which yields a simpler solution for the problem.	3d pose estimation;algebraic equation;algorithm;augmented reality;computation;computation (action);computer vision;eigen (c++ library);estimated;experiment;homography (computer vision);imaging system;intersection of set of elements;kernel (linear algebra);muscle rigidity;null value;numerical analysis;numerical error;pixel;plant roots;polynomial;pose (computer vision);qr decomposition;radiation;random sample consensus;robotics;silo (dataset);small;solutions;synthetic data;time complexity	Pedro Miraldo;Helder Araújo	2015	IEEE Transactions on Cybernetics	10.1109/TCYB.2014.2326970	computer vision;mathematical optimization;estimation;pose;3d pose estimation;articulated body pose estimation;mathematics;geometry;computational model;statistics;polynomial	Vision	53.5215189210649	-48.81922058084147	160107
3ebec79c7daf041a7d317b5bd5341386b7195e7d	an empirical analysis of errors in structure from motion	motion analysis;feature detection;image motion analysis;empirical analysis;video sequences;data mining;system performance;computer vision;error analysis;image registration;error analysis motion analysis video sequences image motion analysis computer vision motion detection tracking image registration data mining cameras;missing data;estimation error;structure from motion;motion detection;cameras;tracking	The process of registering airborne video to some reference imagery described by Cooke et al. [2] involves feature detection and tracking, structure from motion with missing data, georeferencing, dense matching and 2D image registration. This paper considers two aspects of this process. Firstly, it compares the errors produced by a range of methods for extracting the camera parameters and the scene structure from an incomplete set of feature measurements. Secondly, it examines the effect of this error on the overall system performance by describing a framework in which the error in the final registration may be automatically estimated at any point in a video sequence. The quality of the estimated errors in some of the intermediate results is then checked numerically using an airborne video sequence of an airport control tower.	airborne ranger;algorithm;error analysis (mathematics);feature detection (computer vision);feature detection (web development);geolocation;ground truth;image registration;missing data;monoid factorisation;monte carlo method;numerical analysis;structure from motion;tomasi–kanade factorization	Tristrom Cooke	2007	9th Biennial Conference of the Australian Pattern Recognition Society on Digital Image Computing Techniques and Applications (DICTA 2007)	10.1109/DICTA.2007.4426790	computer vision;structure from motion;missing data;computer science;image registration;pattern recognition;feature detection;computer performance;tracking;computer graphics (images)	Vision	52.63705108244279	-49.6573202270018	160290
c96065b3d3dbde4d34a31fc1fc11e6d588d01140	performance evaluation of stereo algorithms for automotive applications	vertical science platform;performance evaluation;real time;research paper;patents;research platform;journals;researchers network	The accuracy of stereo algorithms is commonly assessed by comparing the results against the Middlebury database. However, no equivalent data for automotive or robotics applications exist and these are difficult to obtain. We introduce a performance evaluation scheme and metrics for stereo algorithms at three different levels. This evaluation can be reproduced with comparably low effort and has very few prerequisites. First, the disparity images are evaluated on a pixel level. The second level evaluates the disparity data roughly column by column, and the third level performs an evaluation on an object level. We compare three real-time capable stereo algorithms with these methods and the results show that a global stereo method, semi-global matching, yields the best performance using our metrics that incorporate both accuracy and robustness.	algorithm;performance evaluation	Pascal Steingrube;Stefan K. Gehrig;Uwe Franke	2009		10.1007/978-3-642-04667-4_29	computer vision;simulation;computer science;multimedia	Vision	51.55044614636316	-45.06833270117628	160319
d842cd450aa717c090af16585d06e1de5816f40b	illumination change compensation and extraction of corner feature orientation for upward-looking camera-based slam	slam upward looking camera feature extraction;feature extraction simultaneous localization and mapping lighting cameras robustness robot vision systems;slam;slam robots cameras feature extraction kalman filters mobile robots nonlinear filters robot vision;ekf based slam illumination change compensation corner feature orientation extraction upward looking camera based slam performance improvement retinex algorithm input image extended kalman filter;upward looking camera;feature extraction	In this paper, the improved upward-looking camera based SLAM is proposed. With the corner features that are highly robust to the illumination change and contain their orientation information, SLAM performance can improve effectively. The illumination properties from the image can be rejected using the retinex algorithm to make the input image less sensitive to the illumination change. In addition, to deal with the correspondence problem between indistinguishable adjacent corner features, the orientation of the corner feature is additionally taken into account. These robust features are used as a landmark for the extended Kalman filter EKF-based SLAM to evaluate the robustness. A series of experiments show the improved results using the proposed scheme.	algorithm;correspondence problem;experiment;extended kalman filter;simultaneous localization and mapping	Chansoo Park;Jae-Bok Song	2015	2015 12th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2015.7358941	computer vision;simulation;feature extraction;computer science;machine learning	Robotics	46.76131431491921	-45.16083167603511	160520
3fcf0576ec83fa6345272b77505f61310605318a	probabilistic event logic for interval-based event recognition	holistic reasoning;event recognition;video signal processing image representation image segmentation knowledge based systems maximum likelihood detection object detection probabilistic logic temporal logic;image segmentation;interval based event recognition;video signal processing;event segmentation;temporal logic;event detection;spanning interval data structure probabilistic event logic interval based event recognition event detection event segmentation video representation motion blur occlusions holistic reasoning pel knowledge base confidence weighted formulas temporal event logic map inference algorithm;interval data;temporal event logic;temporal constraints;motion blur;image representation;probabilistic event logic;video representation;maximum likelihood detection;spanning interval data structure;pel;map inference algorithm;probabilistic logic;confidence weighted formulas;videos silicon probabilistic logic detectors inference algorithms spatiotemporal phenomena target tracking;occlusions;knowledge based systems;object detection;knowledge base	This paper is about detecting and segmenting interrelated events which occur in challenging videos with motion blur, occlusions, dynamic backgrounds, and missing observations. We argue that holistic reasoning about time intervals of events, and their temporal constraints is critical in such domains to overcome the noise inherent to low-level video representations. For this purpose, our first contribution is the formulation of probabilistic event logic (PEL) for representing temporal constraints among events. A PEL knowledge base consists of confidence-weighted formulas from a temporal event logic, and specifies a joint distribution over the occurrence time intervals of all events. Our second contribution is a MAP inference algorithm for PEL that addresses the scalability issue of reasoning about an enormous number of time intervals and their constraints in a typical video. Specifically, our algorithm leverages the spanning-interval data structure for compactly representing and manipulating entire sets of time intervals without enumerating them. Our experiments on interpreting basketball videos show that PEL inference is able to jointly detect events and identify their time intervals, based on noisy input from primitive-event detectors.	algorithm;benchmark (computing);discus;data structure;experiment;file spanning;gaussian blur;high- and low-level;holism;ibm notes;internationalization and localization;internet information services;knowledge base;map;scalability;sensor	William Brendel;Alan Fern;Sinisa Todorovic	2011	CVPR 2011	10.1109/CVPR.2011.5995491	computer vision;knowledge base;temporal logic;computer science;machine learning;pattern recognition;mathematics;permissible exposure limit;image segmentation;probabilistic logic	Vision	49.44768444440964	-49.51554362298436	160568
4d5a9fb76cfbfcc10add10a5ad0866ed918021f6	study on epipolar geometry restoration of parallel binocular vision	cross correlation;accurate matching;image matching;computational geometry;cross correlation sparse matching algorithm epipolar geometry restoration parallel binocular vision image corner pair matching;image corner pair matching;computer vision;image restoration robot kinematics parallel robots robot vision systems computational geometry robotics and automation machine vision laboratories layout cameras;epipolar geometry;connotative constraint;binocular vision;cross correlation sparse matching algorithm;gray level cross correlation binocular vision epipolar geometry accurate matching connotative constraint;stereo image processing computational geometry computer vision image matching;stereo image processing;gray level cross correlation;parallel binocular vision;epipolar geometry restoration	The epipolar geometry restoration offline and online for the parallel binocular vision system were discussed in this paper. For the former, a method that uses a common calibration board to realize automatic and accurate matching of the image corner pairs was developed; for the latter, a connotative constraint was proved by geometry theory and the corresponding cross-correlation sparse matching algorithm was introduced. The constraint was that for any space point projected into two images, the horizontal coordinate in the left image of this point is bigger than that in the right image. Standard and real images experiments were made for each kind of problem and the results show that the algorithms for each problem were valid.	algorithm;binocular vision;circuit restoration;cross-correlation;epipolar geometry;experiment;online and offline;sparse matrix	Hongwei Gao;Chengdong Wu;Bin Li	2006	2006 IEEE International Conference on Robotics and Biomimetics	10.1109/ROBIO.2006.340086	binocular vision;computer vision;computational geometry;computer science;cross-correlation;mathematics;geometry;fundamental matrix;epipolar geometry;computer graphics (images)	Robotics	53.552472736836336	-50.38776529996275	160783
c87815f0d8279eae9560810dba3b7ce503c495f8	stereoscopic depth estimation for online vision systems	dreidimensionales maschinelles sehen;stereokamera;echtzeitbildverarbeitung;fahrerassistenzsystem;mobiler roboter;tiefenerkennung	A lot of work has been done in the area of machine stereo vision, but a severe drawback of todayu0027s algorithms is that they either achieve high accuracy and robustness by sacrificing real-time speed or they are real-time capable but with major deficiencies in quality. In order to tackle this problem this thesis presents two new methods which exhibit a very good balance between computational effort and depth accuracy.rnFirst, the summed normalized cross-correlation is proposed which constitutes a new cost function for block-matching stereo processing. In contrast to most standard cost functions it hardly suffers from the fattening effect while being computationally very efficient. Second, the direct surface fitting, a new algorithm for fitting parametric surface models to stereo images, is introduced. This algorithm is inspired by the homography-constrained gradient descent methods but in contrast to these allows also for the estimation of non-planar surfaces. Experimental evaluations demonstrate that both newly introduced algorithms are competitive to state-of-the-art in terms of accuracy while having a much lower computational time.	stereoscopy	Nils Einecke	2012			computer vision	Robotics	51.85554623400246	-47.494049671067366	161202
98ff9dec350149427dcac62bcef4b0ce21a6069b	video surveillance and multimedia forensics: an application to trajectory analysis	video surveillance;trajectories analysis;people surveillance;circular statistics	This paper reports an example of application (i.e., trajectory analysis) in which forensics and video surveillance techniques are jointly employed for providing a new tool of multimedia forensics. Advanced video surveillance techniques are used to extract from a multi-camera system the trajectories of the moving people which are then modelled by either their positions (projected on the ground plane) or their directions of movement. These two representations can be useful for querying large video repositories, by searching for similar trajectories in terms of either sequences of positions or trajectory shape (encoded as sequence of angles, where positions do not care). Preliminary examples of the possible use of this approach are shown.	closed-circuit television;speedup	Simone Calderara;Andrea Prati;Rita Cucchiara	2009		10.1145/1631081.1631085	computer vision;simulation;geography;video tracking;multimedia	Vision	49.45811933240234	-45.93758099783023	161352
2d7d7a0a334c8e0ea763e74d4326c3c0fd54885b	w-pnp method: optimal solution for the weak-perspective n-point problem and its application to structure from motion		Camera calibration is a key problem in 3D computer vision since the late 80’s. Most of the calibration methods deal with the (perspective) pinhole camera model. This is not a simple goal: the problem is nonlinear due to the perspectivity. The strategy of these methods is to estimate the intrinsic camera parameters first; then the extrinsic ones are computed by the so-called PnP method. Finally, the accurate camera parameters are obtained by slow numerical optimization. In this paper, we show that the weak-perspective camera model can be optimally calibrated without numerical optimization if the L2 norm is used. The solution is given by a closed-form formula, thus the estimation is very fast. We call this method as the Weak-Perspective n-Point (W-PnP) algorithm. Its advantage is that it simultaneously estimates the two intrinsic weak-perspective camera parameters and the extrinsic ones. We show that the proposed calibration method can be utilized as the solution for a subproblem of 3D reconstruction with missing data. An alternating least squares method is also defined that optimizes the camera motion using the proposed optimal calibration method.	3d projection;3d reconstruction;algorithm;bundle adjustment;camera resectioning;computation;computer stereo vision;computer vision;iteration;java;linear least squares (mathematics);linear programming;mathematical optimization;missing data;nonlinear system;numerical linear algebra;orthographic projection;pinhole camera model;point cloud;reprojection error;structure from motion;virtual reality headset	Levente Hajder	2017		10.5220/0006158902650276	computer vision;computer science;artificial intelligence;structure from motion	Vision	52.47362060396975	-48.514139094503534	161431
770870e8f5f8351b9757f8c40e82b39d30d43465	linear and quadratic subsets for template-based tracking	image motion analysis;image matching;set theory image matching image motion analysis;set theory;motion parameters;template based matching;subset selection;convergence optimization methods computer vision linearity testing computer science laboratories explosions cost function computational efficiency;quadratic subsets;template based tracking;motion parameters linear subsets quadratic subsets template based tracking template based matching;linear subsets	We propose a method that dramatically improves the performance of template-based matching in terms of size of convergence region and computation time. This is done by selecting a subset of the template that verifies the assumption (made during optimization) of linearity or quadraticity with respect to the motion parameters. We call these subsets linear or quadratic subsets. While subset selection approaches have already been proposed, they generally do not attempt to provide linear or quadratic subsets and rely on heuristics such as textured-ness. Because a naive search for the optimal subset would result in a combinatorial explosion for large templates, we propose a simple algorithm that does not aim for the optimal subset but provides a very good linear or quadratic subset at low cost, even for large templates. Simulation results and experiments with real sequences show the superiority of the proposed method compared to existing subset selection approaches.	algorithm;computation;earthbound;experiment;heuristic (computer science);mathematical optimization;online and offline;pixel;simulation;synthetic data;time complexity	Selim Benhimane;Alexander Ladikos;Vincent Lepetit;Nassir Navab	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383179	mathematical optimization;combinatorics;discrete mathematics;template matching;mathematics;set theory	Vision	50.47684887652871	-50.69082540609895	161631
1de62c96f042ca0f5f7eb22c2f3cec4a6881b548	on the two-view geometry of unsynchronized cameras		We present new methods of simultaneously estimating camera geometry and time shift from video sequences from multiple unsynchronized cameras. Algorithms for simultaneous computation of a fundamental matrix or a homography with unknown time shift between images are developed. Our methods use minimal correspondence sets (eight for fundamental matrix and four and a half for homography) and therefore are suitable for robust estimation using RANSAC. Furthermore, we present an iterative algorithm that extends the applicability on sequences which are significantly unsynchronized, finding the correct time shift up to several seconds. We evaluated the methods on synthetic and wide range of real world datasets and the results show a broad applicability to the problem of camera synchronization.	algorithm;computation;fundamental matrix (computer vision);homography (computer vision);iterative method;random sample consensus;synchronization (computer science);synthetic intelligence;timeshift	Cenek Albl;Zuzana Kukelova;Andrew W. Fitzgibbon;Jan Heller;Matej Smíd;Tomás Pajdla	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.593	homography;computer vision;mathematical optimization;mathematics;geometry	Vision	52.016207747115395	-48.817494599748706	161745
9a603051a941569273a65bdf8f6b4025af890b98	combining preference analysis with local constraints for rapid hypothesis generation	acceleration;estimation computational modeling computer vision acceleration pattern analysis robustness;computer vision;computational modeling;estimation;fundamental matrix estimation rapid hypothesis generation preference analysis local constraints hypothesis generation robust model fitting method;robustness;pattern analysis;set theory computational geometry curve fitting probability	Hypothesis generation is crucial to many robust model fitting methods. In this paper, we propose an effective hypothesis generation method by adopting conditional sampling with local constraints. We choose data to generate hypotheses according to sampling weights, which are computed according to ordered residual indices. To sample a minimal subset, we randomly choose a seed datum, compute sampling weights of all data with regard to the seed datum, search the neighborhood set of the seed datum by using the sampling weights, and then sample the remaining data of the minimal subset from the neighborhood set. It has two advantages to consider the neighboring information in guided sampling: It raises the probability of generating all-inlier minimal subsets and it reduces the computational loads in hypotheses generation. The proposed method shows good performance in fundamental matrix estimation using real image pairs.	computation;curve fitting;fundamental matrix (computer vision);geodetic datum;randomness;sampling (signal processing)	Taotao Lai;Da-Han Wang;Guobao Xiao;Hanzi Wang	2014	2014 13th International Conference on Control Automation Robotics & Vision (ICARCV)	10.1109/ICARCV.2014.7064452	acceleration;computer vision;econometrics;estimation;computer science;machine learning;mathematics;computational model;statistics;robustness	Robotics	48.457773317072665	-47.95703562883471	162386
418f3976fc89a42fb9fb712fb10462d6d29d6aa8	motion adjustment for extrinsic calibration of cameras with non-overlapping views	visual odometry;non overlapping views;video surveillance;nonoverlapping view;motion adjustment;multi camera calibration;multi camera calibration motion adjustment non overlapping views handeye calibration;motion estimation;hand eye calibration;translational velocity;distance measurement;video surveillance calibration distance measurement motion estimation traffic engineering computing video cameras;vectors;estimation;mobile platform;video cameras;cameras calibration vehicles vectors estimation robot vision systems equations;environment perception;field of view;handeye calibration;traffic engineering computing;vehicles;parameter estimation;camera calibration;vehicle;nonoverlapping camera configuration;vehicle motion adjustment camera calibration nonoverlapping view mobile platform environment perception nonoverlapping camera configuration hand eye calibration visual odometry motion estimation translational velocity parameter estimation;robot vision systems;calibration;cameras	This paper adresses the issue of calibrating multiple cameras on a mobile platform. Due to decreasing sensor prices and increasing processing performance, the use of multiple cameras in vehicles becomes an attractive possibility for environment perception. To avoid restrictions relating to the camera arrangement, we focus on non-overlapping camera configurations. Hence, we resign the usage of corresponding features between the cameras. The hand-eye calibration technique based on visual odometry is basically able to solve this problem by exploiting the cameras' motions. However, this technique suffers from inaccuracies in motion estimation. Especially the absolute magnitudes of the translational velocities of each camera are essential for a successful calibration. This contribution presents a novel approach to solve the hand eye calibration problem for two cameras on a mobile platform with non-overlapping fields of view. The so-called motion adjustment simultaneously estimates the extrinsic parameters up to scale as well as the reltive motion magnitudes. Results with simulated and real data are presented.	bundle adjustment;camera resectioning;computation;embedded system;kalman filter;mobile operating system;motion compensation;motion estimation;visual odometry;whole earth 'lectronic link	Frank Pagel	2012	2012 Ninth Conference on Computer and Robot Vision	10.1109/CRV.2012.20	computer vision;estimation;calibration;camera resectioning;simulation;field of view;computer science;visual odometry;motion estimation;estimation theory;computer graphics (images)	Vision	50.779138267889046	-46.76819483617825	162923
141a3ddc164a981320700cf969048255e89ff6db	hybrid focal stereo networks for pattern analysis in homogeneous scenes		In this paper we address the problem of multiple camera calibration in the presence of a homogeneous scene, and without the possibility of employing calibration object based methods. The proposed solution exploits salient features present in a larger field of view, but instead of employing active vision we replace the cameras with stereo rigs featuring a long focal analysis camera, as well as a short focal registration camera. Thus, we are able to propose an accurate solution which does not require intrinsic variation models as in the case of zooming cameras. Moreover, the availability of the two views simultaneously in each rig allows for pose re-estimation between rigs as often as necessary. The algorithm has been successfully validated in an indoor setting, as well as on a difficult scene featuring a highly dense pilgrim crowd in Makkah.	active vision;algorithm;autonomous robot;focal (programming language);ground truth;image stitching;logistics;object-based language;pattern recognition;preprocessor;pyramid (image processing);r language;robotic mapping	Emanuel Aldea;Khurom H. Kiyani	2014		10.1007/978-3-319-16634-6_50	computer vision;simulation;computer graphics (images)	Vision	50.335393521823086	-45.14579333096675	164174
5ce09c6d963d4871e3108f1d54c7e272cf3c3102	tracking moving objects as spatio-temporal boundary detection	minimisation;moving object;image motion analysis;bayes methods;partial differential equations target tracking image motion analysis image sequences bayes methods minimisation;space time;energy function;partial differential equations;object detection motion estimation tracking equations image motion analysis image sequences level set cameras motion analysis motion measurement;target tracking;boundary detection;camera motion moving object tracking image sequence motion boundary detection space time domain spatio temporal surface bayesian image partitioning energy functional minimization smooth closed surfaces euler lagrange partial differential equations level set evolution equations multiple non simultaneous independent motions;image sequences	 3 J E C ¡  ¢,£¤b¥¦ , £§ ̈&©1¥¦ a£« ¥­¬ ®C ̄ £§¥±°O2G£ £Er2g3μ ́&¥­¬O° 9¢]¶ &®·¥­¬b° ̧g1Bo¡μ3 ̄£§ ]¥­¬»21⁄4 ¡R1⁄2¡ ¡¬J3μw9¢_¥­¶ 2·°3⁄4 ̄ ¿1¡©À ̈3⁄4 ̄£μ3 ̄£EÁ ¥­¬O°]£¤3Â  ¤¢ ̄2g3μ1°O¡¬J¡r2G£μ ̈_1¡©¶ÃG£E¥BG¬S1μ& ¬J ̈g2&¥B *¥­¬1⁄4£¤3 E323⁄43μ ̄Á£E¥­¶ # ̈3⁄4G¶Ã2G¥­¬JÄÆÅ, £E¥­¶ 2&£§¥B&¬w ¢ £¤b¥¦ E32&£§¥B&Á£«¡¶Ç3&r2GÈ  ¤¢ ̄2g3μÇ¥¦ 8¢ ̄&¶* ÈÉ2G£R ̈2C Ç2 ÊÇ2&©g ¥B2G¬¿¥­¶Ã2¡°O$ 2G£E¥­£E¥BG¬ ¥­¬b° JRg1¡ÈË ̄¶ÃÄÍÌv¥­¬ ¥­¶¥ËÎW2G£E¥BG¬Ï9¢¿£¤3vR  ÈÐ£E¥­¬O°  ̄¬  ̄§°g©Ã¢μ ¬J3¡Á £E¥BG¬J2&È| ¡μR ́G .2! ¡&ÈÐ £§¥B&¬1¡¥B2C ¡μ ̈»£GÑ 2&r ̈» μ¶ÃWG£¤Ò3¡ÈËC ¡μ ̈  ¤¢ ̄2g3μ ̄ ÂÑ7b¥B3μÀ3μG¥­¬J3¡¥B ̈gÃÑ$¥­£­1⁄4¶ÃG£E¥BG¬ ̧1μG ¬J ̈3⁄42G¥B ̄ μÓ  2G®G ¶ 2&È­È32Grμ2CÓ72&¬J ̈, 2G£E¥­£E¥BG¬Ã£¤3Æ¥­¶Ã2¡°O ¥­¬ £« r«°g¥BG¬ Æ9¢Æ3μG¬ Á £Er2C £E¥­¬O°S¶ÃG£E¥BG¬62g3¡£E¥­®·¥­£E©gÄÔ 3¿Åa ÈË¡Á§Õn2·°&R2G¬O°O* 2G£E¥B2GÈ ̈G¥ Ö1 ̄r¡¬ £E¥B2GÈ8μ1⁄2 ̄ 2&£§¥B&¬3  ¢ ¶¥­¬ ¥­¶¥ËÎW2G£E¥BG¬1⁄42GRr× ̄JR μ ¡μ ̈2G ÈË¡®C¡Èb ¡ ̄£n¡®GGÈÐ £E¥BG¬_μ1⁄2¡ 2G£E¥BG¬ ̄Ä 3¢ ̄&¶* ÈÉ2G£E¥BG¬_ ̈3⁄4· ̄ ,¬ G£ rμ1⁄2¡ ¥­r  £E¥­¶ 2&£§¥B&¬.9¢*£­ ¥­¶ 2·°3⁄4 ¶ÃG£E¥BG¬ÃØa ̄ÈË ̈v2&¬J ̈w ̈gW ̄ ¬J&£2G R  ¶Ã<2S ́G¬ GÑ$¬1R23⁄43R ́·°grG ¬ ̈gÄÏÙ£ 2GÈ­ÈË&Ñ 1⁄4¶ ÈÐ£§¥  ÈË ¬J&¬ Á« ¥­¶ ÈÐ£«2&¬JμGb ¥­¬ ̈g3¡¬J ̈3⁄4 ̄¬ £¶ &£§¥B&¬3 ¿£«S·3μ3¡ ¿2&¬J ̈CÓ  ¬J ̈3⁄4 ̄1 ¡G¶ÃÂ2G R  ¶ÇJ£E¥BG¬ RÓ 3μ2G¬S2g3μ3μG ¬ £ ¢ ̄G|3μ2G¶Ã ̄r2Ã¶ &Á £E¥BG¬1⁄4Ñ$¥­£¤3& £8 ¥BG* ̄ £§¥­¶Ã2G£E¥BG¬k9¢#£¤b¥¦ |¶ÃG£E¥BG¬ Ä		Amar Mitiche;Rosario El-Feghali;Abdol-Reza Mansouri	2002		10.1109/IAI.2002.999899	computer vision;minimisation;mathematical optimization;match moving;structure from motion;space time;motion estimation;mathematics;geometry;motion field;partial differential equation;statistics;linear motion	Robotics	50.48875174385025	-50.00656853529869	164718
875939d67412e169ea1dbe77995f69b79d80cc22	multi-camera head pose estimation	support vector machines;multiple views;datavetenskap datalogi;datavetenskap;computer and systems science;people tracking;computer science	Estimating people’s head pose is an important problem, for which many solutions have been proposed. Most existing solutions are based on the use of a single camera and assume that the head is confined in a relatively small region of space. If we need to estimate unintrusively the head pose of persons in a large environment, however, we need to use several cameras to cover the monitored area. In this work, we propose a novel solution to the multi-camera head pose estimation problem that exploits the additional amount of information that provides multi-camera configurations. Our approach uses the probability estimates produced by multi-class support vector machines to calculate the probability distribution of the head pose. The distributions produced by the cameras are fused, resulting in a more precise estimate than the one provided individually. We report experimental results that confirm that the fused distribution provides higher accuracy than the individual classifiers and a high robustness against errors.	3d pose estimation;experiment;fuzzy logic;support vector machine;vector graphics	Rafael Muñoz-Salinas;Enrique Yeguas-Bolivar;Alessandro Saffiotti;Rafael Medina Carnicer	2012	Machine Vision and Applications	10.1007/s00138-012-0410-z	support vector machine;computer vision;simulation;3d pose estimation;computer science;machine learning;data mining;articulated body pose estimation	Vision	49.36136118449363	-46.71705230011265	165635
a2500105aef622a2f7d2aa92a2ae0d461fc8c98f	analysis of colour space transforms for person independent aams	unit selection;active appearance model;statistical model;face tracking;deformable objects;active appearance models;audiovisual text to speech synthesis;facial expression;close range	Statistical models of non-rigid deformable objects such as Active Appearance Models (AAM) are a popular means of both registration, tracking and synthesis of faces. Due to rapid fitting and good accuracy they are used extensively for facial expression tracking and analysis. A problem facing AAM based face tracking, is their inability to generalise well to unseen faces especially from unseen databases. One way to overcome this problem is through the combined use of various available databases, as some of them capture lighting variations, others pose or expression. In addition, this allows us to capture more variation in ethnicity, gender and age. Use of multiple databases gives us a better opportunity to create person, expression, and pose independent models. A problem arises because of the heterogeneity of the available databases due to use of different lenses, exposure times, external lightning or shadows etc. We describe an approach that leads to improved convergence of AAM fitting at close range when training a model on two different databases. In addition, this approach offers a substantial improvement when fitting images from unseen databases.	active appearance model;automatic acoustic management;color space;database;facial motion capture;lightning;statistical model	Tadas Baltrusaitis;Peter Robinson	2010		10.1145/1924035.1924048	computer vision;speech recognition;geography;communication	Vision	47.320133942815595	-51.099740034502744	165654
fbd23608673a2772852264e16eac01ff071b6057	accurate, fast and robust realtime face pose estimation using kinect camera	minimisation;user modelling cameras computer graphics computer vision face recognition gesture recognition iterative methods minimisation pose estimation real time systems;user modelling;face pose estimation;computer graphics;computer vision;iterative methods;face recognition;depth images computer vision face pose estimation;depth images;gesture recognition;cameras;face estimation fitting cameras solid modeling adaptation models;real time systems;pose estimation;real time face pose estimation kinect camera microsoft kinect depth sensor real time gesture recognition depth image data user behavior analysis man machine interaction modality user specific model iterative closest point algorithm point vertices surface normals fitting procedure normal vectors point cloud head rotation facial expression partial face occlusion computer vision	Since its release in late 2010 the Microsoft Kinect depth sensor has boosted real time gesture recognition and new man-machine interaction endeavors in the computer vision community. Based on depth image data, in this paper we propose an accurate, fast and robust face pose estimation approach, which for example can be of interest for user behavior analysis, or be of use as a means of man machine interaction modality. In our method we apply the depth sensor to create a user specific model which is fitted with an Iterative Closest Point algorithm. This model consists of point vertices and surface normals. In the fitting procedure we employ the normal vectors for the minimization of distances between the model and the measured point cloud. As the experimental results show, our method is precise, fast and robust in case of strong head rotation, even during facial expression and partial face occlusion.	3d pose estimation;algorithm;boyce–codd normal form;computer vision;gesture recognition;iterative closest point;iterative method;kinect;modality (human–computer interaction);normal (geometry);point cloud;range imaging;real-time clock;vertex (graph theory)	Robert Niese;Philipp Werner;Ayoub Al-Hamadi	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.89	computer vision;minimisation;simulation;pose;computer science;gesture recognition;iterative method;computer graphics;computer graphics (images)	Vision	49.59008140571724	-47.731752799141795	166332
e4075f168bf5fa596ce07cd174cc553bd6fac627	the statistics of driving sequences -- and what we can learn from them	q measurement;temporal egomotion parameter sequence driving sequences statistics driving car motion egomotion statistics feature matching feature tracking egomotion estimation kitti odometry benchmark sequences coordinated turn motion model uncertainty reduction outlier detection;uncertainty;covariance matrices;cameras covariance matrices vehicles q measurement correlation adaptive optics uncertainty;statistical analysis feature extraction image matching image sequences motion estimation object detection object tracking;vehicles;correlation;cameras;adaptive optics	The motion of a driving car is highly constrained and we claim that powerful predictors can be built that 'learn' the typical egomotion statistics, and support the typical tasks of feature matching, tracking, and egomotion estimation. We analyze the statistics of the 'ground truth' data given in the KITTI odometry benchmark sequences and confirm that a coordinated turn motion model, overlaid by moderate vibrations, is a very realistic model. We develop a predictor that is able to significantly reduce the uncertainty about the relative motion when a new image frame comes in. Such predictors can be used to steer the matching process from frame n to frame n + 1. We show that they can also be employed to detect outliers in the temporal sequence of egomotion parameters.	benchmark (computing);database;ground truth;kerrison predictor;motion estimation	Henry Bradler;Birthe Anne Wiegand;Rudolf Mester	2015	2015 IEEE International Conference on Computer Vision Workshop (ICCVW)	10.1109/ICCVW.2015.24	computer vision;simulation;uncertainty;machine learning;mathematics;geometry;adaptive optics;correlation;statistics	Vision	50.44607455330684	-46.68961359113339	166614
1e13368c48c664f2da0eb87d91ac87de259af227	3d inference and modelling for video retrieval	surface fitting information retrieval cameras maximum likelihood estimation content based retrieval image reconstruction image retrieval databases internet surface reconstruction;plane;3d modelling;random sampling;computational geometry;iterative random sample consensus plane fitting;video retrieval;ransac;euclidean distance;maximum likelihood estimation;fitting;conference paper;iterative methods;computational modeling;maximum likelihood estimate;ransac inference modelling plane fitting;estimation;three dimensional displays;feature extraction;solid modeling;image sequence;video retrieval computational geometry image sequences iterative methods maximum likelihood estimation solid modelling;3d inference;depth map;3d modelling video retrieval iterative random sample consensus plane fitting image sequences maximum likelihood estimation euclidean distance 3d inference;cameras;inference;solid modelling;image sequences	A new scheme is proposed for extracting planar surfaces from 2D image sequences. We firstly perform feature correspondence over two neighboring frames, followed by the estimation of disparity and depth maps, provided a calibrated camera. We then apply iterative random sample consensus (RANSAC) plane fitting to the generated 3D points to find a dominant plane in a maximum likelihood estimation style. Object points on or off this dominant plane are determined by measuring their Euclidean distance to the plane. Experimental work shows that the proposed scheme leads to better plane fitting results than the classical RANSAC method.	algorithm;binocular disparity;camera resectioning;digital video;euclidean distance;experiment;iterative method;map;online and offline;random sample consensus	Huiyu Zhou;Abdul H. Sadka;Richard M. Jiang	2008	2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services	10.1109/WIAMIS.2008.37	computer vision;computational geometry;machine learning;pattern recognition;mathematics;maximum likelihood;statistics	Vision	51.163168302836525	-49.528499101368645	167015
4cb87e15afbe5dcd9d664e94b954e05a29202e9f	real-time estimation of human body posture from monocular thermal images	human posture estimation;moment of inertia;frames per second;image segmentation;image matching;real time;motion estimation;thermal image;thermal imaging;infrared imaging;human body;center of gravity;real time systems motion estimation infrared imaging image segmentation image matching;genetic algorithm;infrared;distance transform;humans image motion analysis image sequence analysis biological system modeling thermal engineering head robustness motion analysis image sequences elbow;robustness human body posture monocular thermal images real time method thermal images infrared camera genetic algorithm learning;contour analysis;real time systems	This paper introduces a new real-time method t o estimate the posture of a human from thermal images acquired b y an infrared camera regardless of the background and lighting conditions. Distance transformation is performed for the human body area extracted from the thresholded thermal image for the calculation of the center of gravity. After the orientation of the upper half of the body as obtained b y calculating the moment of inertia, significant points such as the top of the head, the tips of the hands and foo t are heuristically located. In addition, the elbow and knee positions are estimated from the detected (significant) points using a genetic algorithm based learning procedure. The experimental results demonstrate the robustness of the proposed algorithm and real-time (faster than 20 frames per second) performance.	foobar;genetic algorithm;heuristic;poor posture;real-time clock;real-time computing	Shoichiro Iwasawa;Kazuyuki Ebihara;Jun Ohya;Shigeo Morishima	1997		10.1109/CVPR.1997.609290	computer vision;human body;simulation;genetic algorithm;infrared;computer science;motion estimation;moment of inertia;image segmentation;distance transform;center of gravity;frame rate;computer graphics (images)	Robotics	48.03996040159745	-45.358213915965976	167297
a0bd8cc8ad455204dfef47f2c02ddb3e3d6bbe7d	global optimization of object pose and motion from a single rolling shutter image with automatic 2d-3d matching	rolling shutter;motion estimation;matching	Low cost CMOS cameras can have an acquisition mode called rolling shutter which sequentially exposes the scan-lines. When a single object moves with respect to the camera, this creates image distortions. Assuming 2D-3D correspondences known, previous work showed that the object pose and kinematics can be estimated from a single rolling shutter image. This was achieved using a suboptimal initialization followed by local iterative optimization. We propose a polynomial projection model for rolling shutter cameras and a constrained global optimization of its parameters. This is done by means of a semidefinite programming problem obtained from the generalized problem of moments method. Contrarily to previous work, our optimization does not require an initialization and ensures that the global minimum is achieved. This allows us to build automatically robust 2D-3D correspondences using a template to provide an initial set of correspondences. Experiments show that our method slightly improves previous work on both simulated and real data. This is due to local minima into which previous methods get trapped. We also successfully experimented building 2D-3D correspondences automatically with both simulated and real data.	approximation;autostereogram;cmos;coherence (physics);distortion;experiment;global optimization;iterative method;loss function;mathematical optimization;maxima and minima;moment problem;motion detector;movie projector;optimization problem;polynomial;scan line;semidefinite programming;signal-to-noise ratio	Ludovic Magerand;Adrien Bartoli;Omar Ait-Aider;Daniel Pizarro-Perez	2012		10.1007/978-3-642-33718-5_33	matching;computer vision;mathematical optimization;simulation;computer science;motion estimation;mathematics	Vision	53.18579994392207	-48.88789932917747	167303
3c26a29113540c324342b23124619ec558f22880	three-dimensional object recognition and registration for robotic grasping systems using a modified viewpoint feature histogram	vision guided robot;viewpoint feature histogram descriptor;iterative closest point;kinect sensor	This paper presents a novel 3D feature descriptor for object recognition and to identify poses when there are six-degrees-of-freedom for mobile manipulation and grasping applications. Firstly, a Microsoft Kinect sensor is used to capture 3D point cloud data. A viewpoint feature histogram (VFH) descriptor for the 3D point cloud data then encodes the geometry and viewpoint, so an object can be simultaneously recognized and registered in a stable pose and the information is stored in a database. The VFH is robust to a large degree of surface noise and missing depth information so it is reliable for stereo data. However, the pose estimation for an object fails when the object is placed symmetrically to the viewpoint. To overcome this problem, this study proposes a modified viewpoint feature histogram (MVFH) descriptor that consists of two parts: a surface shape component that comprises an extended fast point feature histogram and an extended viewpoint direction component. The MVFH descriptor characterizes an object's pose and enhances the system's ability to identify objects with mirrored poses. Finally, the refined pose is further estimated using an iterative closest point when the object has been recognized and the pose roughly estimated by the MVFH descriptor and it has been registered on a database. The estimation results demonstrate that the MVFH feature descriptor allows more accurate pose estimation. The experiments also show that the proposed method can be applied in vision-guided robotic grasping systems.	3d pose estimation;3d single-object recognition;algorithmic efficiency;bmp6 wt allele;binary file descriptor library;data descriptor;experiment;feature vector;iterative closest point;iterative method;kinect;mobile manipulator;outline of object recognition;physical object;point cloud;robot;robotics;vector field histogram;viewpoint;vision guided robotic systems;visual descriptor;grasp;registration - actclass	Chin-Sheng Chen;Po-Chun Chen;Chih-Ming Hsu	2016		10.3390/s16111969	computer vision;local binary patterns;pose;3d pose estimation;computer science;machine learning;pattern recognition;iterative closest point	Robotics	48.303527522597115	-48.12419049430255	167624
5c22c218e9dc1514fe4b805f7f2e3ed348a74ac0	adaptive multi-resolution fitting and its application to realistic head modeling	object recognition;image resolution;model adaptation;surface fitting;laser scanner;adaptive signal processing;computational complexity;image sequence;optimization adaptive multiresolution fitting realistic head modeling object modeling high quality range points laser scanners noise point cloud image sequences adaptive subdivision scheme control model local complexity 3d data complexity compatibility mesh model local details representation;subdivision scheme;point cloud;head clouds surface fitting surface emitting lasers laser modes laser noise image sequences cameras programmable control adaptive control;multi resolution;adaptive signal processing image sequences object recognition surface fitting computational complexity image resolution;object model;image sequences	The general approach for object modeling is to construct the surface from the high-quality range points obtained from laser scanners. In this paper, we face the noise point cloud obtained from image sequences by a common camera and develop a novel algorithm of adaptive multi-resolution fitting (AMRF) for object modeling. This algorithm combines the adaptive subdivision scheme with multi-resolution fitting so that the control model is subdivided locally and adaptively according to the local complexity of the point cloud and approximates the 3D data level by level. The proposed method can conquer the holes and outliers efficiently and create full compatibility between the complexity of the mesh model and the representation of the local details. We apply the proposed method to the complete head modeling with the real data, and the results seem very promising.	algorithm;curve fitting;experiment;global optimization;mathematical optimization;nonlinear programming;nonlinear system;point cloud;subdivision surface	Chenghua Xu;Long Quan;Yunhong Wang;Tieniu Tan;Maxime Lhuillier	2004	Geometric Modeling and Processing, 2004. Proceedings	10.1109/GMAP.2004.1290057	laser scanning;adaptive filter;computer vision;image resolution;object model;computer science;theoretical computer science;cognitive neuroscience of visual object recognition;point cloud;computational complexity theory;computer graphics (images)	Robotics	51.745082454342814	-50.01144726446481	167922
056164de33d4149cead8f7576b4d528388a40748	camera-to-camera geometry estimation requiring no overlap in their visual fields	monocular normal flow camera to camera geometry estimation multicamera system image motion data capturing motion correspondence based calibration method optical flow estimation image streams;image motion analysis;geometry;data capture;indexing terms;geometry cameras calibration image motion analysis geometrical optics motion estimation layout streaming media focusing automation;video cameras calibration geometry image motion analysis image sequences;video cameras;extrinsic camera parameters;active vision camera calibration extrinsic camera parameters;optical flow;camera calibration;visual field;calibration;active vision;image sequences	Calibrating the relative geometry between cameras which would move against one another from time to time is an important problem in multi-camera system. Most of the existing calibration technologies are based on the cross-camera feature correspondences. This paper presents a new solution method. The method demands image data captured under a rigid motion of the camera pair, but unlike the existing motion correspondence-based calibration methods, it does not estimate optical flows nor motion correspondences explicitly. Instead it estimates the inter-camera geometry from the observations that are directly available from the two image streams -the monocular normal flows. Experimental results on real image data are shown to illustrate the feasibility of the solution.	binocular vision;optical flow;overlap–add method;regular language description for xml;stereo camera	Ding Yuan;Ronald Chung	2007	2007 IEEE International Conference on Image Processing	10.1109/ICIP.2007.4378933	computer vision;calibration;camera resectioning;index term;active vision;computer science;optical flow;automatic identification and data capture;mathematics;motion field;computer graphics (images)	Robotics	53.26630273080086	-50.59900188841405	168016
6b43a88ef1f06a312632c6f88b465e302dfd22ef	models and algorithms for efficient multiresolution topology estimation of measured 3-d range data	topology;range data;topology clustering algorithms computer graphics laboratories clouds computer errors computational geometry solid modeling signal processing algorithms cognition;computational geometry;triangular mesh;indexing terms;estimation algorithm;three dimensional;computer vision;solid modelling computational complexity topology computer vision computational geometry mesh generation;model error;computational complexity;polygonal meshes;mesh generation;multiresolution topology estimation algorithm multiresolution polygonal mesh topology estimation problem cognition compactness regularity voronoi patches k means clustering algorithm triangular mesh model dynamic mesh model equiangularity constraint tolerable modeling error topology estimation polygonal mesh;k means clustering;equilibrium state;solid modelling	In this paper, we propose a new efficient topology estimation algorithm to construct a multiresolution polygonal mesh from measured three-dimensional (3-D) range data. The topology estimation problem is defined under the constraints of cognition, compactness, and regularity, and the algorithm is designed to be applied to either a cloud of points or a dense mesh. The proposed algorithm initially segments the range data into a finite number of Voronoi patches using the K-means clustering algorithm. Each patch is then approximated by an appropriate polygonal and eventually a triangular mesh model. In order to improve the equiangularity of the mesh, we employ a dynamic mesh model, in which the mesh finds its equilibrium state adaptively, according to the equiangularity constraint. Experimental results demonstrate that satisfactory equiangular triangular mesh models can be constructed rapidly at various resolutions, while yielding tolerable modeling error.	anatomy, regional;approximation algorithm;cluster analysis;cognition;k-means clustering;polygon mesh;statistical cluster	In Kyu Park;Kyoung Mu Lee;Sang Uk Lee	2003	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/TSMCB.2003.814301	three-dimensional space;mesh generation;mathematical optimization;combinatorics;index term;thermodynamic equilibrium;computational geometry;computer science;theoretical computer science;triangle mesh;machine learning;errors-in-variables models;mathematics;laplacian smoothing;computational complexity theory;t-vertices;k-means clustering	Visualization	51.61725301673837	-50.05193572454785	168380
204a8441f5dea770edebe3a77913ad7b3d996f03	3d video stabilization for a humanoid robot using point feature trajectory smoothing	humanoid robot;point feature trajectory 3d video stabilization humanoid robot;video signal processing humanoid robots motion estimation robot vision smoothing methods solid modelling video cameras;video signal processing;3d video stabilization;reference frame;motion estimation;stabilized method;camera motion;smoothing methods;robot vision;humanoid robots;video cameras;virtual stabilized camera 3d video stabilization method humanoid robot camera motion point feature trajectory smoothing camera motion estimation 2d video stabilization 2d camera motion model real 3d space error accumulation global motion estimation 3d camera motion real unstable camera;cameras three dimensional displays trajectory robot vision systems humanoid robots motion estimation image sequences;3d video;structure from motion;global motion estimation;point feature trajectory;solid modelling	This paper proposes a new 3D video stabilization method for a humanoid robot without explicit global camera motion estimation with respect to a reference frame. 2D video stabilization that uses a 2D camera motion model works well if scenes are far from the camera of the robot or can be modeled as a plane. However, because a humanoid robot operates in a real 3D space, these 2D video stabilization approaches may fail to stabilize unstable videos. Furthermore, straightforward 3D video stabilization methods that depend on structure-from-motion to estimate global camera motion might fail in the long run because error accumulation in the global motion estimation step is inevitable with the elapse of time. Instead, our method uses only the relative 3D camera motion between the real unstable camera and the virtual stabilized camera through point feature trajectory smoothing. In order to evaluate our method, we use a real humanoid robot that is able to walk and run. The experimental results show that the proposed 3D video stabilization system is a potential candidate for application in humanoid robots.	control theory;humanoid robot;motion estimation;reference frame (video);smoothing;structure from motion;tree accumulation	Yeon Geol Ryu;Hyun Chul Roh;Myung Jin Chung;Jung Woo Heo;Jun Ho Oh	2011	2011 11th IEEE-RAS International Conference on Humanoid Robots	10.1109/Humanoids.2011.6100822	computer vision;camera auto-calibration;simulation;computer science;humanoid robot;artificial intelligence;computer graphics (images)	Robotics	50.929265477271684	-46.703141980709724	169297
637fae92b5186aab4eecc719db644e2c8d650d68	dense visual slam with probabilistic surfel map		"""Visual SLAM is one of the key technologies to align the virtual and real world together in Augmented Reality applications. RGBD dense Visual SLAM approaches have shown their advantages in robustness and accuracy in recent years. However, there are still several challenges such as the inconsistencies in RGBD measurements across multiple frames that could jeopardize the accuracy of both camera trajectory and scene reconstruction. In this paper, we propose a novel map representation called Probabilistic Surfel Map (PSM) for dense visual SLAM. The main idea is to maintain a globally consistent map with both photometric and geometric uncertainties encoded in order to address the inconsistency issue. The key of our PSM is proper modeling and updating of sensor measurement uncertainties, as well as the strategies to apply them for improving both the front-end pose estimation and the back-end optimization. Experimental results on publicly available datasets demonstrate major improvements with our approach over the state-of-the-art methods. Specifically, comparing with <inline-formula><tex-math notation=""""LaTeX"""">$\sigma$</tex-math><alternatives><inline-graphic xlink:href=""""23tvcg11-yan-2734458-ieq-1-source.gif""""/></alternatives></inline-formula>-DVO, we achieve a 40% reduction in absolute trajectory error and an 18% reduction in relative pose error in visual odometry, as well as an 8.5% reduction in absolute trajectory error in complete SLAM. Moreover, our PSM enables generation of a high quality dense point cloud with comparable accuracy as the state-of-the-art approach."""	activities involving roller skating (inline) and skateboarding;align (company);augmented reality;display resolution;frame (physical object);mathematical optimization;photometry;point cloud;slamf1 gene;simultaneous localization and mapping;surfel;visual odometry;xlink	Zhixin Yan;Mao Ye;Liu Ren	2017	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2017.2734458	point cloud;artificial intelligence;3d reconstruction;computer vision;robustness (computer science);computer science;augmented reality;surfel;pose;simultaneous localization and mapping;visual odometry	Visualization	52.294790987370355	-46.033340158693356	169602
55426bdd285aa790eec32db54b92a32e91a1404b	dynamic object localization using hand-held cameras	approximation algorithms;camera motion effects dynamic object localization hand held cameras camera settings exposure settings complex object motions images capture image over segmentation;optical imaging;cameras dynamics heuristic algorithms clustering algorithms approximation algorithms lighting optical imaging;dynamics;heuristic algorithms;object tracking image capture image motion analysis image segmentation;clustering algorithms;lighting;cameras	We consider the problem of separating static and dynamic regions of a scene when the camera also undergoes motion while capturing still images. We assume that we do not have any other information about the scene and the camera settings. Given two images, we would like to estimate the static and dynamic regions corresponding to one image with respect to the other image. The proposed solution involves over-segmentation of the image and dense correspondence between the two images. We show that the proposed approach works well even when there are changes in illumination and exposure settings while capturing the two images. We evaluate the performance of the proposed approach by demonstrating the results on different scenes with complex object motions.	add-ons for firefox;adobe photoshop;gimp;graphics software;image editing;mobile device	Sai Chowdary Gullapally;Sri Raghu Malireddi;Shanmuganathan Raman	2015	2015 Twenty First National Conference on Communications (NCC)	10.1109/NCC.2015.7084827	computer vision;simulation;computer science;image-based lighting;computer graphics (images)	Vision	49.850947225839796	-48.76477544643053	169641
f5c44e23784c4158106c06802186e4de2cc869ac	multi-view 3d data acquisition using a single uncoded light pattern		This research concerns the acquisition of 3-dimensional data from images for the purpose of modeling a person’s head. This paper proposes an approach for acquiring the 3-dimensional reconstruction using a multiple stereo camera vision platform and a combination of passive and active lighting techniques. The proposed oneshot active lighting method projects a single, binary dot pattern, hence ensuring the suitability of the method to reconstruct dynamic scenes. Contrary to the conventional spatial neighborhood coding techniques, this approach matches corresponding spots between image pairs by exploiting solely the redundant data available in the multiple camera images. This produces an initial, sparse reconstruction, which is then used to guide a passive lighting technique to obtain a dense 3-dimensional representation of the object of interest. The results obtained reveal the robustness of the projected pattern and the spot matching algorithm, and a decrease in the number of false matches in the 3-dimensional dense reconstructions, particularly in smooth and textureless regions on the human face.	algorithm;data acquisition;data compression;sparse matrix;stereo camera	Stefania Cristina;Kenneth P. Camilleri;Thomas Galea	2011			simulation;science, technology and society;data acquisition;artificial intelligence;computer science;robotics	Vision	49.97908077688874	-46.029691458833234	169829
b22dda161e5512ab05a3744193d7ec898916dea4	large-scale multiview 3d hand pose dataset		Accurate hand pose estimation at joint level has several uses on human-robot interaction, user interfacing and virtual reality applications. Yet, it currently is not a solved problem. The novel deep learning techniques could make a great improvement on this matter but they need a huge amount of annotated data. The hand pose datasets released so far present some issues that make them impossible to use on deep learning methods such as the few number of samples, high-level abstraction annotations or samples consisting in depth maps. In this work, we introduce a multiview hand pose dataset in which we provide color images of hands and different kind of annotations for each, i.e the bounding box and the 2D and 3D location on the joints in the hand. Besides, we introduce a simple yet accurate deep learning architecture for real-time robust 2D hand pose estimation.	3d pose estimation;baseline (configuration management);deep learning;glossary of computer graphics;high- and low-level;human–robot interaction;image plane;map;minimum bounding box;olami–feder–christensen model;point of view (computer hardware company);real-time clock;real-time locating system;virtual reality	Francisco Gomez-Donoso;Sergio Orts;Miguel Cazorla	2017	CoRR		pattern recognition;computer vision;hand color;open problem;mathematics;deep learning;rgb color model;pose;augmented reality;minimum bounding box;artificial intelligence;user interface	Vision	53.586526133392866	-45.43786067336647	170228
1b8a8e27508f2928f0b7119c8c7548cc31c6cebd	adaptive background generation for automatic detection of initial object region in multiple color-filter aperture camera-based surveillance system	video surveillance cameras filtering theory image colour analysis image registration image resolution object detection object tracking;3d based cameras;video surveillance;realtime depth estimation;image resolution;intelligent transport system;surveillance system;optical filters;advanced safety vehicles;video analysis adaptive background generation method initial object region automatic detection multiple color filter aperture camera simultaneous object detection depth estimation mca camera color shifting vector estimation simplified elastic registration algorithm pyramid based multiresolution framework object tracking system consumer video surveillance systems;interference reduction;image sensors;elastic registration algorithm;stereo image processing filtering theory image colour analysis image sensors object detection;vectors;general methods;estimation;automatic detection;image color analysis;image colour analysis;cameras image color analysis optical filters estimation apertures vectors optical imaging;er algorithm;image registration;object tracking;stereo image processing;stereo vision;realtime depth tracking;object depth estimation;prespecified distance transformation function;object region automatic detection;color based background generation method;depth estimation;distance transform;multi resolution;efficient estimation;object detection estimation;mca camera based object detection system;intelligent transport systems;advanced safety vehicles object region automatic detection multiple color filter aperture camera based surveillance system adaptive background generation method color based background generation method interference reduction elastic registration algorithm er algorithm realtime depth estimation realtime depth tracking prespecified distance transformation function stereo vision object detection estimation object depth estimation mca camera based object detection system intelligent transport systems 3d based cameras	In this paper, we present an adaptive background generation method for automatic selection of initial object regions, which realizes simultaneous object detection and depth estimation using multiple color-filter aperture (MCA) camera. Since the conventional background generation method does not fit the depth estimation using the MCA camera, we propose a novel color-based background generation method which can reduce interference in the object region for stable depth estimation. For efficient estimation of color shifting vectors in the extracted object region, a simplified elastic registration (ER) algorithm is used. The proposed simplified method is essential factor to realize realtime depth estimation and tracking, which is the primary condition for consumer applications. Finally, the object distance is determined by using the relationship between the pre-specified distance transformation function and the estimated shifting vectors of the corresponding object region. Although traditional depth estimation methods generally use dual cameras for stereo vision, the proposed method uses only a single camera for both object detection and depth estimation. Experimental results show that the proposed MCA camera-based object detection system can be used in a variety of consumer surveillance systems such as intelligent transport systems, 3D-based cameras and advanced safety vehicles.	algorithm;elastic matching;erdős–rényi model;interference (communication);object detection;stereopsis	Seungwon Lee;Jung-Hyun Lee;Monson H. Hayes;Joonki Paik	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6170061	aperture;computer vision;estimation;intelligent transportation system;image resolution;computer science;stereopsis;image registration;video tracking;image sensor;optical filter;distance transform;computer graphics (images)	Vision	49.77444383316616	-49.167124655684376	170597
d4f34bc1dbaa06bcaef01f182b78640afc15c058	nid-slam: robust monocular slam using normalised information distance		We propose a direct monocular SLAM algorithm based on the Normalised Information Distance (NID) metric. In contrast to current state-of-the-art direct methods based on photometric error minimisation, our information-theoretic NID metric provides robustness to appearance variation due to lighting, weather and structural changes in the scene. We demonstrate successful localisation and mapping across changes in lighting with a synthetic indoor scene, and across changes in weather (direct sun, rain, snow) using real-world data collected from a vehicle-mounted camera. Our approach runs in real-time on a consumer GPU using OpenGL, and provides comparable localisation accuracy to state-of-the-art photometric methods but significantly outperforms both direct and feature-based methods in robustness to appearance changes.	algorithm;depth map;graphics processing unit;information theory;key frame;network interface device;opengl;real-time clock;refinement (computing);robotics;simultaneous localization and mapping;synthetic intelligence	Geoffrey Pascoe;Will Maddern;Michael Tanner;Pedro Pinies;Paul Newman	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.158	computer vision;robustness (computer science);opengl;artificial intelligence;simultaneous localization and mapping;pattern recognition;information distance;direct methods;monocular;computer science	Vision	52.35223511091904	-45.67296561886648	170743
3dd0b2cf19629e32b5d392bba13233ac2127afac	optimal correspondences from pairwise constraints	graph theory;object recognition;image alignment;image registration combinatorial mathematics graph theory image reconstruction;heuristic method;datorseende och robotik autonoma system;geometry;correspondence problem;3d registration;computer vision;optimal;combinatorial methods pairwise constraints computer vision object recognition partial 3d reconstructions image alignment geometric consistency ransac algorithms em like algorithms graph methods 3d 3d registration 2d 3d registration;pairwise constraints;image reconstruction;image registration;computer vision image reconstruction cameras application software object recognition merging computer errors explosions stereo image processing stereo vision;matematik;combinatorial mathematics	Correspondence problems are of great importance in computer vision. They appear as subtasks in many applications such as object recognition, merging partial 3D reconstructions and image alignment. Automatically matching features from appearance only is difficult and errors are frequent. Thus, it is necessary to use geometric consistency to remove incorrect correspondences. Typically heuristic methods like RANSAC or EM-like algorithms are used, but they risk getting trapped in local optima and are in no way guaranteed to find the best solution. This paper illustrates how pairwise constraints in combination with graph methods can be used to efficiently find optimal correspondences. These ideas are implemented on two basic geometric problems, 3D-3D registration and 2D-3D registration. The developed scheme can handle large rates of outliers and cope with multiple hypotheses. Despite the combinatorial explosion, the resulting algorithm which has been extensively evaluated on real data, yields competitive running times compared to state of the art.	algorithm;angularjs;approximation algorithm;branch and bound;c11 (c standard revision);computer vision;constraint algorithm;heuristic;linear algebra;local optimum;np-completeness;outline of object recognition;polynomial;quadratic equation;quartic function;random sample consensus;semantics (computer science);speedup;vertex cover	Olof Enqvist;Klas Josephson;Fredrik Kahl	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459319	iterative reconstruction;computer vision;computer science;image registration;graph theory;theoretical computer science;cognitive neuroscience of visual object recognition;machine learning;mathematics;correspondence problem	Vision	50.583832255560516	-50.67037028067601	170887
1125760c14ea6182b85a09bf3f5bad1bdad43ef5	a probabilistic approach to linear subspace fitting for computer vision problems	object recognition;computer vision principal component analysis inference algorithms lighting photometry fitting bayesian methods cameras pixel object recognition;variational bayes;bayesian inference;bayesian methods;probabilistic approach;computer vision;fitting;multi dimensional;expectation maximization;photometry;principal component analysis;pixel;expectation maximization algorithm;inference algorithms;lighting;structure from motion;cameras	Several computer vision problems, such as some of photometric problems and the problem of affine structure from motion, are formulated as fitting linear subspace(s) to point data in a multi-dimensional space. In ideal cases the linear subspaces can easily be computed by PCA/SVD algorithms. Unfortunately this will not apply to real cases, since there are outliers and missing components in real data. Furthermore it is sometimes necessary to fit multiple different subspaces to a set of point data in a situation where each point belongs to one of the subspaces but it is unknown which subspace each point belongs to. One straightforward solution to these advanced cases is to adopt the expectation maximization framework based on Bayesian inference. However, this solution does not seem to have been well considered in computer vision community, as far as the above problems of linear subspace fitting are concerned. This paper presents expectation maximization algorithms and its extension, variational Bayes-based algorithm, for several cases of linear subspace fitting and applies them to computer vision problems.	computer vision;curve fitting;expectation–maximization algorithm;principal component analysis;singular value decomposition;structure from motion;variational principle	Takayuki Okatani	2004	2004 Conference on Computer Vision and Pattern Recognition Workshop	10.1109/CVPR.2004.286	computer vision;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	52.84016756333953	-50.24662805383364	170922
593252a9d836e6f2ef5515ccc5effc2d2e090646	least squares estimation of 3d shape and motion of rigid objects from their orthographic projections	least squares approximations;image motion analysis;least squares approximation shape data mining application software computer vision biomedical engineering video coding guidelines motion pictures mpeg 4 standard;motion picture experts group;orthography;indexing terms;three dimensional;mpeg 7 least squares estimation 3d shape estimation motion estimation rigid objects orthographic projections 2d projections computer vision biomedical engineering video coding mining mpeg 4;computer vision;video coding;biomedical engineering;least squares approximations image motion analysis;theoretical analysis;least squares estimate;shape parameter;3d structure;structure from motion;3d motion	ÐThe extraction of motion and shape information of three-dimensional objects from their two-dimensional projections is a task that emerges in various applications such as computer vision, biomedical engineering, and video coding and mining especially after the recent guidelines of the Motion Pictures Expert Group regarding MPEG-4 and MPEG-7 standards. Present work establishes a novel approach for extracting the motion and shape parameters of a rigid threedimensional object on the basis of its orthographic projections and the associated motion field. Experimental results have been included to verify the theoretical analysis. Index TermsÐ3D motion, 3D structure, structure from motion, orthography.	algorithm;computation;computer vision;data compression;isometric projection;least squares;mpeg-7;motion field;orthographic projection;signal-to-noise ratio;simulation;smoothing;structure from motion;theory;wire-frame model	Yiannis Xirouhakis;Anastasios Delopoulos	2000	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.845382	three-dimensional space;computer vision;structure from motion;index term;orthography;computer science;theoretical computer science;motion estimation;mathematics;motion field;shape parameter;computer graphics (images)	Vision	49.11858005108263	-50.469705490539006	172534
11d4e0d84f60ba4bec07e4bbd20ae36867705c07	fusion of auxiliary imaging information for robust, scalable and fast 3d reconstruction		One of the potentially effective means for 3D reconstruction is to reconstruct the scene in a global manner, rather than incrementally, by fully exploiting available auxiliary information on imaging condition, such as camera location by GPS, orientation by IMU(or Compass), focal length from EXIF etc. However these auxiliary information, though informative and valuable, is usually too noisy to be directly usable. In this paper, we present a global method by taking advantage of such noisy auxiliary information to improve SfM solving. More specifically, we introduce two effective iterative optimization algorithms directly initiated with such noisy auxiliary information. One is a robust iterative rotation estimation algorithm to deal with contaminated EG(epipolar graph), the other is a robust iterative scene reconstruction algorithm to deal with noisy GPS data for camera centers initialization. We found that by exclusively focusing on the inliers estimated at the current iteration, called potential inliers in this work, the optimization process initialized by such noisy auxiliary information could converge well and efficiently. Our proposed method is evaluated on real images captured by UAV(unmanned aerial vehicle), StreetView car and conventional digital cameras. Extensive experimental results show that our method performs similarly or better than many of the state-of-art reconstruction approaches, in terms of reconstruction accuracy and scene completeness, but more efficient and scalable for large-scale image datasets.	3d reconstruction;aerial photography;algorithm;converge;cross-validation (statistics);digital camera;exif;focal (programming language);global positioning system;google street view;information;iteration;mathematical optimization;scalability;unmanned aerial vehicle	Hainan Cui;Shuhan Shen;Wei Gao;Zhanyi Hu	2014		10.1007/978-3-319-16865-4_15	computer vision	Vision	52.83956896884821	-47.248120974772895	172668
87dab1c3a9807a620946d2dc7efab8c2510d6305	structure and motion from scene registration	atmospheric measurements;particle measurements;motion estimation;arrays;motion estimation image registration;optical imaging;estimation;image registration;3d scene flow scene registration 3d structure dense 3d motion dynamic nonrigid 3d scene dense multicamera array dense 3d volumetric representation intensity value confidence measure 3d motion estimation nonrigid registration dense 3d scalar volumes;robustness;cameras arrays estimation optical imaging robustness atmospheric measurements particle measurements;cameras	We propose a method for estimating the 3D structure and the dense 3D motion (scene flow) of a dynamic nonrigid 3D scene, using a camera array. The core idea is to use a dense multi-camera array to construct a novel, dense 3D volumetric representation of the 3D space where each voxel holds an estimated intensity value and a confidence measure of this value. The problem of 3D structure and 3D motion estimation of a scene is thus reduced to a nonrigid registration of two volumes - hence the term ”Scene Registration”. Registering two dense 3D scalar volumes does not require recovering the 3D structure of the scene as a preprocessing step, nor does it require explicit reasoning about occlusions. From this nonrigid registration we accurately extract the 3D scene flow and the 3D structure of the scene, and successfully recover the sharp discontinuities in both time and space. We demonstrate the advantages of our method on a number of challenging synthetic and real data sets.	algorithm;computation;experiment;light field;motion estimation;point set registration;preprocessor;scalability;scene graph;synthetic data;voxel	Tali Basha;Shai Avidan;Alexander Sorkine-Hornung;Wojciech Matusik	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247830	computer vision;estimation;computer science;image registration;motion estimation;optical imaging;mathematics;robustness;computer graphics (images)	Vision	52.266671709177224	-49.57465426738927	173739
8f2fb01e05cac0fd5d68c752e634d60eef779b06	continuous-time visual-inertial odometry for event cameras		Event cameras are bioinspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. However, due to the fundamentally different structure of the sensor's output, new algorithms that exploit the high temporal resolution and the asynchronous nature of the sensor are required. Recent work has shown that a continuous-time representation of the event camera pose can deal with the high temporal resolution and asynchronous nature of this sensor in a principled way. In this paper, we leverage such a continuous-time representation to perform visual-inertial odometry with an event camera. This representation allows direct integration of the asynchronous events with microsecond accuracy and the inertial measurements at high frequency. The event camera trajectory is approximated by a smooth curve in the space of rigid-body motions using cubic splines. This formulation significantly reduces the number of variables in trajectory estimation problems. We evaluate our method on real data from several scenes and compare the results against ground truth from a motion-capture system. We show that our method provides improved accuracy over the result of a state-of-the-art visual odometry method for event cameras. We also show that both the map orientation and scale can be recovered accurately by fusing events and inertial data. To the best of our knowledge, this is the first work on visual-inertial fusion with event cameras using a continuous-time framework.		Elias Mueggler;Guillermo Gallego;Henri Rebecq;Davide Scaramuzza	2018	IEEE Transactions on Robotics	10.1109/TRO.2018.2858287	motion blur;odometry;high dynamic range;control theory;smart camera;visual odometry;temporal resolution;pose;mathematics;sensor fusion;computer vision;artificial intelligence	Vision	52.68334643639275	-45.27332805063207	175772
821977886c2d7a3b1bbb04023ecf1e85c5e1ac11	model-based nonrigid motion recovery from sequences of range images without point correspondences	motion analysis;finite element methods;nonrigid motion analysis;image motion analysis;range data;nonlinear finite element models;motion analysis finite element methods humans shape computer vision tracking computer science data engineering image sequences biomedical engineering;force directions;data engineering;shape deformation;force recovery;computer vision;motion sequence;biomedical engineering;shape;finite element analysis image sequences image motion analysis;range image;range image sequences;force recovery model based nonrigid motion recovery range image sequences nonrigid motion analysis nonlinear finite element models motion sequence shape deformation force directions;humans;finite element analysis;model based nonrigid motion recovery;computer science;tracking;image sequences	1 In this paper we propose a new method for accurate nonrigid motion analysis when point correspondence data is not available. We construct nonlinear nite element models by integrating range data and prior knowledge about an object's properties. We attempt to recover the motion sequence given an initial alignment of the model with the rst frame of the sequence. The main idea of the method is to nd the forces that are responsible for the motion or shape deformation of the given object. The task is broken into subtasks of nding the forces for each frame. Both absolute values and directions of these forces are taken into consideration and iteratively varied not only for each frame, but also between the frames. Our work demonstrates the possibility of accurate nonrigid motion analysis and force recovery from range image sequences containing nonrigid objects and large motion without interframe point correspondences.	correspondence problem;nonlinear system;range imaging	Leonid V. Tsap;Dmitry B. Goldgof;Sudeep Sarkar	1998		10.1109/ICIP.1998.723351	computer vision;information engineering;computer science;finite element method;mathematics;geometry;computer graphics (images)	Vision	52.370682670742156	-50.82981434533359	176012
4d7679117ee2bc811f7c5b1d7d0cb79e1b0b3f97	visual servoing from robust direct color image registration	image features;resorting image feature;arbitrary illumination changes;parametric model;illumination changes images;visual servoing scheme;color;robust color image registration;unknown imaging conditions robust color image registration illumination changes images directly registering color images exploit pixel intensities resorting image feature robust visual servoing scheme obtained optimal parameters arbitrary illumination changes gray level images;optimal method;exploit pixel intensities;visual servoing gray codes image colour analysis optimal control robust control;robust control;prior knowledge;unknown imaging conditions;gray level images;optimal control;robust;directly registering color images;obtained optimal parameters;image color analysis;image colour analysis;image registration;robustness;lighting;visual servoing;visual servoing robustness color cameras lighting pixel gray scale image registration intelligent robots usa councils;cameras;direct method;color image;gray codes	To date, there exist only few works on the use of color images for visual servoing. Perhaps, this is due to the difficulties usually found to cope with illumination changes in these images. This paper presents new parametric models and optimization methods for robustly and directly registering color images. Direct methods refer to those that exploit the pixel intensities, without resorting to image features. We then show how a robust and generic visual servoing scheme can be constructed using the obtained optimal parameters. The proposed models ensure robustness to arbitrary illumination changes in color images, do not require prior knowledge (including the spectral ones) of the object, illuminants or camera, and naturally encompass gray-level images. Furthermore, the exploitation of all information within the images, even from areas where no features exist, allow the algorithm to achieve high levels of accuracy. Various results are reported to show that visual servoing can indeed be highly accurate and robust despite unknown objects and unknown imaging conditions.	algorithm;color depth;color image;existential quantification;image registration;mathematical optimization;pixel;visual servoing	Geraldo F. Silveira;Ezio Malis	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354423	direct method;robust control;gray code;computer vision;parametric model;color image;optimal control;computer science;image registration;lighting;optics;visual servoing;feature;robustness;computer graphics (images)	Robotics	51.128959458294126	-49.195064484855536	176103
e367e29b4515b86daf2e6c1fe007ca3c5767ed23	k-means clustering for efficient and robust registration of multi-view point sets		Generally, there are three main factors that determine the practical usability of registration, i.e., accuracy, robustness, and efficiency. In real-time applications, efficiency and robustness are more important. To promote these two abilities, we cast the multi-view registration into a clustering task. All the centroids are uniformly sampled from the initially aligned point sets involved in the multi-view registration, which makes it rather efficient and effective for the clustering. Then, each point is assigned to a single cluster and each cluster centroid is updated accordingly. Subsequently, the shape comprised by all cluster centroids is used to sequentially estimate the rigid transformation for each point set. For accuracy and stability, clustering and transformation estimation are alternately and iteratively applied to all point sets. We tested our proposed approach on several benchmark datasets and compared it with state-of-the-art approaches. Experimental results validate its efficiency and robustness for the registration of multi-view point sets. ∗Corresponding author: Jihua Zhu Email address: zhujh@xjtu.edu.cn (Jihua Zhu) Preprint submitted to Pattern Recognition May 1, 2018 ar X iv :1 71 0. 05 19 3v 4 [ cs .C V ] 3 0 A pr 2 01 8	3d scanner;benchmark (computing);cluster analysis;computer cluster;email;k-means clustering;pattern recognition;real-time clock;real-time computing;usability	Zutao Jiang;Jihua Zhu;Shanmin Pang;Yaochen Li	2017	CoRR			Vision	50.39461508626497	-48.12367175568501	177040
6a5762bd3f423f60f3f57dad94bd4072ad29a392	a noniterative greedy algorithm for multiframe point correspondence	acoplamiento grafo;surveillance noniterative greedy algorithm multiframe point correspondence monocular image sequences np hard problems polynomial time algorithm optimization real time systems occlusion multiple heuristics point tracking algorithm;optimisation;graphe biparti;occlusion;algorithme glouton;grafo bipartido;trajectoire point;path cover of directed graph index terms point correspondence target tracking motion occlusion point trajectory data association bipartite graph matching;correspondance point;oclusion;greedy algorithms;point trajectory;indexing terms;motion;graph matching;data association;polynomials;point correspondence;couplage graphe;index terms point correspondence;computational complexity;directed graph;graphe oriente;poursuite cible;greedy algorithm;algoritmo gloton;grafo orientado;greedy algorithms image sequences layout motion analysis particle tracking humans polynomials real time systems surveillance target tracking;path cover of directed graph;target tracking;bipartite graph;tracking greedy algorithms image sequences real time systems computational complexity optimisation polynomials;tracking;bipartite graph matching;algorithms artificial intelligence computer graphics computer simulation image enhancement image interpretation computer assisted information storage and retrieval models biological models statistical movement numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted subtraction technique user computer interface video recording;real time systems;image sequences;association donnee	This work presents a framework for finding point correspondences in monocular image sequences over multiple frames. The general problem of multiframe point correspondence is NP-hard for three or more frames. A polynomial time algorithm for a restriction of this problem is presented and is used as the basis of the proposed greedy algorithm for the general problem. The greedy nature of the proposed algorithm allows it to be used in real-time systems for tracking and surveillance, etc. In addition, the proposed algorithm deals with the problems of occlusion, missed detections, and false positives by using a single noniterative greedy optimization scheme and, hence, reduces the complexity of the overall algorithm as compared to most existing approaches where multiple heuristics are used for the same purpose. While most greedy algorithms for point tracking do not allow the entry and exit of the points from the scene, this is not a limitation for the proposed algorithm. Experiments with real and synthetic data over a wide range of scenarios and system parameters are presented to validate the claims about the performance of the proposed algorithm.	correspondence problem;experiment;frame (physical object);greedy algorithm;heuristics;loss function;mathematical optimization;np-hardness;p (complexity);real-time clock;real-time computing;sensor;synthetic data	Khurram Shafique;Mubarak Shah	2005	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2005.1	computer vision;greedy randomized adaptive search procedure;mathematical optimization;combinatorics;greedy algorithm;ramer–douglas–peucker algorithm;computer science;machine learning;mathematics	Vision	50.17054321193615	-49.68112107977691	177102
cafa10eafec7ad8c51088b6caf81d302e578cf8a	efficient and robust vehicle localization	image segmentation image motion analysis video signal processing video cameras road traffic automobiles feature extraction image sequences;automatic vehicle location;image motion analysis;closed form solution;image segmentation;automobiles;image processing;video signal processing;road traffic;3d wireframe model robust vehicle localization efficient vehicle localization robust pose determination efficient pose determination traffic scenes monocular intensity images calibrated cameras 3d space 3d motions translation rotation point to line segment distance rotation parameters geometric relationships imaginary planes closed form solutions sub problems vertex neighborhood constraint occlusion clutter robust algorithm image sequences;3d model;video cameras;feature extraction;signal processing;robustness vehicles layout closed form solution image recognition computer vision image segmentation sun laboratories pattern recognition;image sequences	In this paper, we proposc a novel algorithm for efficient and robust pose detemiination of vehicles in traffic scenes from single monocular intensity images using calibrated cameras. We consider the pose determination process as a scries of evolutions from initial pose to correct pose in 3D space, which can be decomposed into two independent 3D motions: translation and rotation. The translation parameters are obtained based on Point-to-Line-Segment Distance (PLS Distance), while the rotation parameters are determined by geometric relationships among a set of specially constructed but imaginary planes. Closed-form solutions to both sub-problems are obtained, thus avoiding the usual shortcoming of rclatively high computational cost of traditional 3D-model based approaches. In addition, Vertex Neighborhood Constraint (VNC) is introduced to improve the robustness of the method. Experimental results show that the algorithm works well even under severe occlusion and clutter.	3d modeling;algorithm;algorithmic efficiency;clutter;computation;drug vehicle;imaginary time;iterative method;motion;papillon-lefevre disease;real-time clock;real-time computing;solutions;vertex;weight function;wire-frame model	Hno YLlng;Jianguang Lou;Hongzan Sun;Weiming Hu;Tieniu Tan	2001		10.1109/ICIP.2001.958501	computer vision;closed-form expression;simulation;image processing;feature extraction;computer science;automatic vehicle location;signal processing;mathematics;image segmentation;computer graphics (images)	Vision	48.60225914081445	-49.03548641579972	177232
8e572a0dd076987c2fb30aefa32682c71f0466b7	device-tagged feature-based localization and mapping of wide areas with a ptz camera	focal length variations feature based localization wide area mapping ptz camera pan tilt zoom camera camera pose keypoints database coarse localization camera odometry visual landmarks matching;pan tilt zoom camera;pose estimation cameras image matching image representation;feature based localization;long period;sensors;surveillance;image matching;pan tilt zoom;visual landmarks matching;focal length variations;motion estimation;maintenance engineering;ptz camera;layout;state estimation;runtime;visual landmarks;camera pose;estimation;three dimensional displays;image representation;simultaneous localization and mapping;camera odometry;cameras layout target tracking robustness simultaneous localization and mapping motion estimation state estimation runtime object detection surveillance;wide area mapping;robustness;coarse localization;keypoints database;experimental evaluation;target tracking;cameras;object detection;pose estimation	This paper proposes a new method for estimating and maintaining over time the pose of a single Pan-Tilt-Zoom camera (PTZ). This is achieved firstly by building offline a keypoints database of the scene; then, in the online step, a coarse localization is obtained from camera odometry and finally refined by visual landmarks matching. A maintenance step is also performed at runtime to keep updated the geometry and appearance of the map.	camera resectioning;match moving;odometry;online and offline;pan–tilt–zoom camera;real-time locating system;recovery procedure;run time (program lifecycle phase)	Alberto Del Bimbo;Giuseppe Lisanti;Iacopo Masi;Federico Pernici	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops	10.1109/CVPRW.2010.5543172	maintenance engineering;layout;computer vision;camera auto-calibration;estimation;pose;computer science;sensor;motion estimation;robustness;computer graphics (images);simultaneous localization and mapping	Vision	50.16200694865765	-46.51187269909579	178968
6adba0935bd6e9f053a647ac8859205782a5a333	a comparison of weighted ls methods with ls methods in 3-d motion estimation from stereo image sequences	gaussian noise;least squares approximations;gaussian noise motion estimation stereo image processing least squares approximations image sequences;motion estimation;three dimensional;independent and identically distributed;covariance matrices;approximate solution;image sequence;stereo image processing;ordinary least square;motion estimation least squares approximation gaussian noise parameter estimation covariance matrix least squares methods navigation statistics image sequences sun;errors in variables;closed form approximate solution motion parameter estimation i i d gaussian noise weighted ls methods 3 d motion estimation stereo image sequences point correspondences minimum errors in variables formulation ordinary least squares formulation closed form matrix weighted solution;image sequences	Estimating 3-D motion parameters from three-dimensional point correspondences is considered. A minimum errors-in-variables formulation of the motion estimation problem is developed to obtain stable and accurate solutions and is connected with the ordinary least squares formulation of the problem. When covariance matrices of 3-D points are known, a closed-form matrix-weighted solution is derived for estimating the motion parameters in the presence of independent and identically distributed (i.i.d.) Gaussian noise. In a general case, a closed-form approximate solution is presented. The experimental results demonstrate that the authors' solutions are accurate and reliable in the presence of noise with different deviations. >	least squares;motion estimation	Xuening Sun;Minas E. Spetsakis	1993		10.1109/CVPR.1993.340988	independent and identically distributed random variables;generalized least squares;gaussian noise;three-dimensional space;computer vision;econometrics;mathematical optimization;ordinary least squares;computer science;motion estimation;errors-in-variables models;mathematics;non-linear least squares;least squares;statistics	Vision	51.86920532844931	-51.43380056595027	179199
884500ae8d81a9ef69ac37fd2b4f7303d7e07a04	online appearance-based face and facial feature tracking	face recognition;feature extraction;geometry;image motion analysis;image registration;image sequences;maximum likelihood estimation;search problems;video signal processing;3d face tracking;deformable 3d face model;directed search;facial animations;facial feature tracking;geometrical parameters;head motion tracking;image registration;local exhaustive search;observation likelihood;online appearance models;parameter space method;steepest ascent method;video sequences	We propose a simple framework that utilizes online appearance models for 3D face and facial feature tracking with a deformable model. Adapting the geometrical parameters for each frame adopts a steepest ascent method in the observation likelihood using a local exhaustive and directed search in the parameter space. The observation likelihood is based on the current appearance and the registered images. The developed framework is straightforward and has the following advantages. First, it does not require any a priori statistical facial texture. Second, it does not require any a priori transition model for the 3D motion. Video sequences featuring large head motions, large facial animations, and external illumination variations are successfully tracked, which demonstrate the efficiency of the developed framework.	gradient descent;motion estimation;times ascent	Fadi Dornaika;Franck Davoine	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1334653	facial recognition system;computer vision;speech recognition;computer facial animation;feature extraction;computer science;image registration;pattern recognition;maximum likelihood;parameter space;statistics	Vision	47.45406121852229	-49.032955754282824	179604
c03a2776c25e8d8b999d0c9607e1873f9b47918a	articulated point pattern matching in optical motion capture systems	optical tracking image matching image motion analysis computer vision;image motion analysis;image matching;feature tracking;conference contribution;computer vision;motion capture;optical tracking;human motion;pattern matching biomedical optical imaging skeleton computer vision image reconstruction computer science object recognition humans tracking motion analysis;model fitting;point pattern matching;human motion capture data articulated object tracking articulated object identification pattern matching optical motion capture system computer vision articulated movement near rigid segment moving dots sequence segment based articulated model fitting algorithm self initializing identification point feature tracking system pose estimation motion sequence synthetic pose;pose estimation	Tracking and identifying articulated objects have received growing attention in computer vision in the past decade. In market-based optical motion capture (MoCap) systems, an articulated movement of near-rigid segments is represented via a sequence of moving dots of known 3D coordinates, corresponding to the captured marker positions. We propose a segment-based articulated model-fitting algorithm to address the problem of self-initializing identification and pose estimation utilizing one frame of data in such point-feature tracking systems. It is ultimately crucial for recovering the complete motion sequence. Experimental results, based on synthetic pose and real-world human motion capture data, demonstrate the performance of the algorithm.	motion capture;pattern matching	Baihua Li;Horst Holstein;Qinggang Meng	2002		10.1109/ICARCV.2002.1234837	computer vision;match moving;structure from motion;motion capture;simulation;pose;computer science;motion estimation;articulated body pose estimation;motion field;computer graphics (images)	Vision	49.42440054038207	-47.196697405335016	179852
a81ccb51897201636d96c7590de0b50f9b7d4f54	three-dimensional object matching in mobile laser scanning point clouds	object recognition;large volume mls point clouds 3d object matching mobile laser scanning point clouds information extraction 3d point clouds point cloud scene locally affine invariant geometric constraint affine transformations 3d correspondence computation 3d object detection point cloud object classification riegl vmx 450 system;affine transforms geophysical image processing image classification image matching remote sensing by laser beam;3d object matching mobile laser scanning point clouds information extraction 3d point clouds point cloud scene locally affine invariant geometric constraint affine transformations 3d correspondence computation 3d object detection point cloud object classification riegl vmx 450 system large volume mls point clouds;mathematical transformations;laser applications;mobile laser scanning mls object classification point cloud 3 d object detection 3 d object matching;scanning;article;remote sensing by laser beam affine transforms geophysical image processing image classification image matching;3 d object matching mobile laser scanning mls object classification point cloud 3 d object detection;three dimensional displays mobile communication remote sensing lasers object detection roads linear programming;surface analysis	This letter presents a 3-D object matching framework to support information extraction directly from 3-D point clouds. The problem of 3-D object matching is to match a template, represented by a group of 3-D points, to a point cloud scene containing an instance of that object. A locally affine-invariant geometric constraint is proposed to effectively handle affine transformations, occlusions, incompleteness, and scales in 3-D point clouds. The 3-D object matching framework is integrated into 3-D correspondence computation, 3-D object detection, and point cloud object classification in mobile laser scanning (MLS) point clouds. Experimental results obtained using the 3-D point clouds acquired by a RIEGL VMX-450 system showed that completeness, correctness, and quality of over 0.96, 0.94, and 0.91 are achieved, respectively, with the proposed framework in 3-D object detection. Comparative studies demonstrate that the proposed method outperforms the two existing methods for detecting 3-D objects directly from large-volume MLS point clouds.	computation;correctness (computer science);information extraction;object detection;point cloud;sensor	Yongtao Yu;Jonathan Li;Haiyan Guan;Fukai Jia;Cheng Wang	2015	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2014.2347347	transformation;computer vision;theoretical computer science;cognitive neuroscience of visual object recognition;surface weather analysis;mathematics;remote sensing	Vision	50.23437801797513	-48.87268963590235	180241
39259816d9aa6a6b0ae275da7e6419d84ffa5415	robust factorization for the affine camera: analysis and comparison	image features;affine projection;information technology;geometry;video sequences;layout;computer vision;outlier detection;robustness cameras video sequences layout computer errors computer vision geometry shape information technology australia;shape;robustness;projective structure;computer errors;cameras;australia	Based on our previous work on the use of subspace distances for the outlier detection problem in video sequences under affine projection, this paper reports our further analysis of the problem and presents two algorithms for computing the reprojection errors of image features in the outlier detection process. Extensive experiments on real video sequences have been conducted to verify the performance of the algorithms. The key contributions of the paper are presentation of the relationship between subspace distances and reprojection errors and demonstration that reprojection errors can be estimated without explicitly computing the projective structure.	algorithm;anomaly detection;experiment;map projection;sensor;virtual reality headset	Du Q. Huynh;Anders Heyden	2002		10.1109/ICARCV.2002.1234887	layout;reprojection error;computer vision;shape;computer science;theoretical computer science;mathematics;information technology;feature;robustness;computer graphics (images)	Vision	53.3555763080699	-50.159002443196435	180772
498bbca1be29d289c1a187c18d1d6854283d9b30	dense versus sparse approaches for estimating the fundamental matrix	performance evaluation;fundamental matrix;optical flow;3d reconstruction	There are two main strategies for solving correspondence problems in computer vision: sparse local feature based approaches and dense global energy based methods. While sparse feature based methods are often used for estimating the fundamental matrix by matching a small set of sophistically optimised interest points, dense energy based methods mark the state of the art in optical flow computation. The goal of our paper is to show that this separation into different application domains is unnecessary and can be bridged in a natural way. As a first contribution we present a new application of dense optical flow for estimating the fundamental matrix. Comparing our results with those obtained by feature based techniques we identify cases in which dense methods have advantages over sparse approaches. Motivated by these promising results we propose, as a second contribution, a new variational model that recovers the fundamental matrix and the optical flow simultaneously as the minimisers of a single energy functional. In experiments we show that our coupled approach is able to further improve the estimates of both the fundamental matrix and the optical flow. Our results prove that dense variational methods can be a serious alternative even in classical application domains of sparse feature based approaches.	calculus of variations;computation;computer vision;experiment;fundamental matrix (computer vision);optical flow;sparse matrix	Levi Valgaerts;Andrés Bruhn;Markus Mainberger;Joachim Weickert	2011	International Journal of Computer Vision	10.1007/s11263-011-0466-7	3d reconstruction;computer vision;econometrics;mathematical optimization;computer science;machine learning;optical flow;mathematics;fundamental matrix	Vision	50.5172615090356	-51.97840228098135	180848
a365ebc4a02bbdb0f552961328712e83d0c88cd2	efficient distance estimation for fitting implicit quadric surfaces	estimation theory;computational geometry;surface fitting;surface fitting computational geometry computer vision estimation theory;computer vision;surface treatment;geometric distance estimation shortest euclidean distance estimation implicit quadric fitting surface orthogonal orientation estimation computational complexity surface fitting framework minimization;estimation;three dimensional displays;principal component analysis;approximation methods;surface fitting euclidean distance computer vision nonlinear equations computational complexity state estimation application software reverse engineering data visualization object recognition;noise	This paper presents a novel approach for estimating the shortest Euclidean distance from a given point to the corresponding implicit quadric fitting surface. It first estimates the orthogonal orientation to the surface from the given point; then the shortest distance is directly estimated by intersecting the implicit surface with a line passing through the given point according to the estimated orthogonal orientation. The proposed orthogonal distance estimation is easily obtained without increasing computational complexity; hence it can be used in error minimization surface fitting frameworks. Comparisons of the proposed metric with previous approaches are provided to show both improvements in CPU time as well as in the accuracy of the obtained results. Surfaces fitted by using the proposed geometric distance estimation and state of the art metrics are presented to show the viability of the proposed approach.	central processing unit;computational complexity theory;curve fitting;euclidean distance;geometric median;implicit surface;multivariate interpolation	Angel Domingo Sappa;Mohammad Rouhani	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414072	mathematical optimization;estimation;combinatorics;computational geometry;noise;mathematics;geometry;estimation theory;distance;statistics;principal component analysis	Vision	51.54100508290025	-51.670022202641796	180925
02b9e3290bec2fc66b0555ce40c31b9e350dbab4	toward a stratification of helmholtz stereopsis	computer society;metric reconstruction;reflectivity;image matching;photometric matching constraint;helmholtz stereopsis;surface reconstruction;surface reflectance;affine reconstruction;light source;shape;multiple camera;photometry;image reconstruction;stereo image processing;stereo vision;metric reconstruction stratification helmholtz stereopsis surface reconstruction surface reflectance multiple camera light source photometric matching constraint affine reconstruction;photometric light sources;stratification;lighting;cameras photometry lighting stereo vision light sources image reconstruction surface reconstruction reflectivity shape computer society;cameras image reconstruction photometric light sources image matching stereo image processing solid modelling;cameras;light sources;solid modelling	Helmholtz stereopsis has been previously introduced as a surface reconstruction technique that does not assume a model of surface reflectance. This technique relies on the use of multiple cameras and light sources, and it has been shown to be effective when the camera and source positions are known. Here, we take a stratified look at uncalibrated Helmholtz stereopsis. We derive a new photometric matching constraint that can be used to establish correspondence without any knowledge of the cameras and sources (except that they are co-located), and we determine conditions under which we can obtain affine and metric reconstructions. An implementation and experimental results are presented.	metric;matrix multiplication;normal (geometry);stereopsis;the matrix	Todd E. Zickler;Peter N. Belhumeur;David J. Kriegman	2003		10.1109/CVPR.2003.1211402	iterative reconstruction;stratification;computer vision;surface reconstruction;photometry;shape;stereopsis;lighting;mathematics;geometry;reflectivity	Vision	53.67334656194141	-50.99701322888558	181191
3c3602eaa31995e054e3ede23bdac9cd66de8d54	from rendering to tracking point-based 3d models	real time tracking;point based rendering;real time;apparent motion;laser scanner;triangular mesh;computer vision;gpgpu;3d model;efficient implementation;point based model;graphic processing unit;shape priors;visual tracking;surface splatting	This paper adds to the abundant visual tracking literature with two main contributions. First, we illustrate the interest of using Graphic Processing Units (GPU) to support efficient implementations of computer vision algorithms, and secondly, we introduce the use of point-based 3D models as a shape prior for real-time 3D tracking with a monocular camera. The joint use of point-based 3D models together with GPU allows to adapt and simplify an existing tracking algorithm originally designed for triangular meshes. Point-based models are of particular interest in this context, because they are the direct output of most laser scanners. We show that state-of-the-art techniques developed for point-based rendering can be used to compute in real-time intermediate values required for visual tracking. In particular, apparent motion predictors at each pixel are computed in parallel, and novel views of the tracked object are generated online to help wide-baseline matching. Both computations derive from the same general surface splatting technique which we implement, along with other low-level vision tasks, on the GPU, leading to a real-time tracking algorithm.	3d modeling	Christophe Dehais;Géraldine Morin;Vincent Charvillat	2010	Image Vision Comput.	10.1016/j.imavis.2010.03.001	laser scanning;computer vision;simulation;eye tracking;computer science;triangle mesh;general-purpose computing on graphics processing units;computer graphics (images)	Vision	53.42182504863183	-46.435712024307755	181613
f664d7a4328b7caaa37662e9dab523da22950978	a markerless motion capture system with automatic subject-specific body model acquisition and robust pose tracking from 3d data	image features;image motion analysis;search problems image motion analysis image sequences object tracking pose estimation;scene flow;complex multiplication;computer model;search method;3d scene flow motion information markerless motion capture system automatic subject specific body model acquisition robust pose tracking 3d full body motion multiple image sequences automatic body model acquisition pose recovery voxel body model particle based stochastic search algorithm robust metric;model acquistion;joints;three dimensional;motion capture;pose tracking;computational modeling;three dimensional displays tracking computational modeling robustness solid modeling cameras joints;three dimensional displays;solid modeling;object tracking;image sequence;volumetric reconstruction;robustness;search problems;scene flow model acquistion pose tracking volumetric reconstruction;cameras;tracking;stochastic search;image sequences;pose estimation	We present a markerless system for recovering 3D full-body motion from multiple image sequences. Our approach supports robust pose tracking from 3D data as well as automatic body model acquisition. For initialization, a subject-specific voxel body model that fits well to the shape of the subject being tracked is automatically created from the beginning volume. Then for pose recovery, the voxel body model is matched to the image features via a hierarchical pose search method. We use a new particle based stochastic search algorithm and introduce a robust metric, which is incorporated with joint limits, physical constraints and fuses multiple 3D cues involving volume spatial and 3D scene flow motion information. Results on several complex multiple sequences show the robustness and effectiveness of our approach.	fits;motion capture;search algorithm;stochastic optimization;voxel	Zheng Zhang;Seah Hock Soon;Chee Kwang Quah;Jixiang Sun	2011	2011 18th IEEE International Conference on Image Processing	10.1109/ICIP.2011.6116397	computer simulation;three-dimensional space;computer vision;motion capture;simulation;pose;complex multiplication;computer science;video tracking;articulated body pose estimation;tracking;solid modeling;computational model;feature;robustness;computer graphics (images)	Vision	49.517094924520855	-48.79298095441153	182262
09a704d2a77e95c0f1af072451d92902e35431d5	recovering the geometry of single axis motions by conic fitting	3d geometry recovery;image sequence 3d geometry recovery uncalibrated rotation angles unknown rotation angles single axis motions conic fitting images space point;image motion analysis;tensile stress;computer graphics;image sequences stereo image processing image motion analysis;computational geometry;computer vision;physics;unknown rotation angles;image reconstruction;image sequence;stereo image processing;uncalibrated rotation angles;tensile stress cameras robustness image reconstruction computer science physics computational geometry image sequences computer vision computer graphics;robustness;computer science;images;cameras;conic fitting;image sequences;single axis motions;space point	In this paper, we propose a new approach for recovering 3D geometry from uncalibrated and unknown rotation angles of single axis motions. Unlike previous approaches, the computation of the trifocal tensor is not required. The new approach is based on fitting a conic to the corresponding points in different images of the same space point. It is then shown that the essential geometry of single axis motion is encoded by a conic. Since we are determining only 5 parameters from N views instead of 18 parameters from a subsequence of 3 views, our approach is much more simple and robust. The experiments on a real image sequence demonstrate the accuracy and robustness of the new algorithm.	curve fitting;optic axis of a crystal	Guang Jiang;Hung-Tat Tsui;Long Quan;Shang-Qian Liu	2001		10.1109/CVPR.2001.990489	iterative reconstruction;computer vision;computational geometry;computer science;mathematics;geometry;stress;computer graphics;robustness;computer graphics (images)	Vision	53.336925850623835	-50.449271457007065	182363
a391b5dc4ad36f9125702d744613b0686df77e43	illumination invariant multi-pose face tracking	probability;multipose face tracking;manifolds;tensor based illumination reduction;pose nonlinearity;face tracking;lighting face detection target tracking head computer graphics laboratories speech processing information processing computer science tensile stress;face recognition;estimation;object specific incremental face model;heuristic algorithms;face probability multipose face tracking face appearance variation illumination invariant pose manifold pose nonlinearity object specific incremental face model patch alignment algorithm tensor based illumination reduction;probability face recognition lighting pose estimation;face modeling;patch alignment algorithm;face;lighting;probabilistic logic;illumination invariant pose manifold;illumination invariance;tracking;face appearance variation;face probability;pose estimation	A novel face tracking algorithm is proposed which is less insensitive to large face appearance variation caused by lighting and viewpoints. The proposed dynamic face appearance model includes an illumination invariant pose manifold to represent the pose nonlinearity and an object-specific incremental multi-pose face model. The pose manifold is built with the patch alignment algorithm, where the tensor based illumination reduction is used to define the local patch. Particularly, the probability of the face belonging to the pose manifold can be used for choosing the object-specific face model. The experimental results show the face tracking model can successfully track faces under unseen poses and changing illuminations.	3d pose estimation;algorithm;facial motion capture;nonlinear system	Wei Wei;Yanning Zhang;Zenggang Lin	2009	2009 Fifth International Conference on Image and Graphics	10.1109/ICIG.2009.112	facial recognition system;face;computer vision;estimation;facial motion capture;pose;manifold;computer science;machine learning;pattern recognition;probability;lighting;mathematics;tracking;probabilistic logic;statistics	Vision	46.81622311527211	-51.36578202541777	182388
3c7420dee03c0f667c955ebc70345df48adb58f2	real-time arm motion imitation for human-robot tangible interface	time complexity;3d point cloud;real time;tangible interface;motion imitation;motion capture;human motion capture;human motion;stereo vision;property a;local minima;markerless method	In this paper we deal with a remote meeting system with tangible interface, in which a robot is used as tangible avatar instead of a remote meeting partner. For realizing such system, it is a critical issue how the robot imitates human motions with natural and exact. So, we suggested a new method that human arm motion is captured with a stereo vision system and transferred to the robotic avatar with real-time. For capturing 3D arm motions based on markerless method, we proposed a new metaball-based method which was designed in order to have some robust and efficient properties: a modified iso-surface equation of metaball for overcoming local minima and a downsizing method of 3D point cloud for improving time complexity. With our meeting system, we have implemented our new algorithm and run at approximately 12–16 Hz. Also, its accuracy in motion capturing could be acceptable for robot motion generation.		Yukyung Choi;Syungkwon Ra;Soohwan Kim;Sung-Kee Park	2009	Intelligent Service Robotics	10.1007/s11370-009-0037-8	time complexity;computer vision;motion capture;simulation;computer science;stereopsis;maxima and minima;computer graphics (images)	Robotics	48.35755002314224	-45.07089950579814	183506
11b3e7a59f4e031cc6579304fd1bf0a4eee0cd17	efficient approximation of the mahalanobis distance for tracking with the kalman filter	kalman filter;mahalanobis distance	In this paper, we address the problem of tracking feature points along image sequences efficiently. Thus, to estimate the undergoing movement we use an approach based on Kalman filtering, which performs the prediction and correction of the features’ movement in every image frame. Measured data is incorporated by optimizing the global association set built on efficient approximations of the Mahalanobis distance (MD). We analyze the difference between the usage in the tracking results of the original MD formulation and its more efficient approximation, as well as the related computational costs. Experimental results which validate our approach are presented. (Extended paper from the International Symposium CompIMAGE – Coimbra, Portugal, 20-21 October 2006.)	algorithmic efficiency;approximation;extended euclidean algorithm;international symposium on fundamentals of computation theory;kalman filter;molecular dynamics	Raquel Ramos Pinho;João Manuel R. S. Tavares;Miguel V. Correia	2006			kalman filter;predictor–corrector method;image analysis;computer science;artificial intelligence;mahalanobis distance;tracking;statistics	Robotics	48.176832077712696	-47.27312327680476	183584
f807e7ff2d5d6517009de53b56db3f0f996add51	real-time tracking and pose-estimation of walking human from silhouette images	top view camera;model based human motion tracking scheme;optimisation;real time optimization processing model based human motion tracking scheme silhouette image sequence top view camera calibration scheme point feature extract pose estimation;image motion analysis;legged locomotion humans cameras tracking calibration image sequences feature extraction laboratories noise robustness noise shaping;real time tracking;real time;reliability modeling;tracking calibration cameras feature extraction image motion analysis image sequences optimisation pose estimation real time systems;foot;real time optimization processing;real time optimization;three dimensional displays;feature extraction;human motion;image sequence;point feature extract;humans;calibration;calibration scheme;silhouette image sequence;cameras;tracking;real time systems;image sequences;pose estimation	A reliable model-based human motion tracking scheme is presented. In this approach, silhouette image sequences from a top-view camera and a side-view camera work together to track human motion in 3D space in real time. The adoption of one top view camera introduces many attractive characteristics. A convenient calibration scheme is presented by decoupling different camera parameters to the largest extent. Point features that are easy to extract and robust to noise are used to track human motion and resolve ambiguity reliably for challenging motions such as an orientation-free walking. In addition, the framework is able to perform quick and satisfactory pose estimation with a real-time optimization processing.	3d pose estimation;coupling (computer programming);kinesiology;mathematical optimization;real-time clock;real-time computing;real-time transcription	Yisong Chen;Guoping Wang	2009	2009 IEEE Workshop on Computational Intelligence for Visual Intelligence	10.1109/CIVI.2009.4938978	computer vision;camera auto-calibration;simulation;geography;motion field;computer graphics (images)	Vision	50.139945323072574	-46.735357104196126	184009
8990eb9392b336355908cd56e9968ecb76bea542	on the quantitative analysis of craniofacial asymmetry in 3d	minimisation;face estimation correlation cost function least squares approximations measurement linear programming;least squares approximations;craniofacial asymmetry;midline based approaches;measurement;cost function;3d facial scans;landmark based approaches;computer vision;face recognition;estimation;linear programming;surface based approaches;face;correlation;minimisation computer vision face recognition least squares approximations;least squares minimization;3d facial scans craniofacial asymmetry least squares minimization surface based approaches midline based approaches landmark based approaches computer vision	We address a systematic evaluation of facial asymmetry from a population of 100 high-quality laser scans, which are first symmetrized and then manipulated to introduce 25 synthetic patterns with a variety of asymmetries. A quantitative evaluation is performed by comparing these known asymmetries with those estimated by different automatic algorithms. Estimation of the actual asymmetries present in the original surface was also addressed. We find that widely used methods based on least-squares minimization not only fail to produce accurate estimates but, in some cases, recover asymmetry patterns that are radically different from the actual asymmetry of the input surfaces, with low or even negative correlation coefficients. A number of alternative algorithms are tested, including landmark-, midline- and surface-based approaches. Among these, we find that the best performance is obtained by a hybrid approach combining surface and midline points, framed within a least median of squares algorithm with weights that decay exponentially with the distance from the midline and an additional term to ensure that the recovered pattern of asymmetry is itself symmetric.	algorithm;algorithmic trading;coefficient;facial symmetry;least squares;synthetic intelligence	Federico Sukno;Mario A. Rojas;John L Waddington;Paul F. Whelan	2015	2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)	10.1109/FG.2015.7163143	computer vision;mathematical optimization;machine learning;mathematics	Vision	52.1223927849509	-51.99531138446878	184322
e818649c5052aeaeb13c77d2e87064c11fbc535f	3d shape reconstruction from incomplete silhouettes in multiple frames	motion estimation;visualization;shape;three dimensional displays;feature extraction;image reconstruction;pixel;intelligent method 3d shape reconstruction incomplete silhouettes multiple frames multiple cameras volume intersection method time sequences rigid motion motion estimation 3d feature points silhouette extraction;image reconstruction cameras motion estimation feature extraction shape measurement surface reconstruction costs;3d shape reconstruction;cameras;motion estimation feature extraction image reconstruction	3D shapes are reconstructed from silhouettes obtained by multiple cameras with the volume intersection method. In recent work, methods of integrating silhouettes in time sequences have been proposed. The number of silhouettes can be increased by integrating silhouettes in multiple frames. The silhouettes of a rigid object in multiple frames are integrated with its rigid motion. This motion is often estimated with 3D feature points extracted from silhouettes. When the estimated motion has large error, shapes are reconstructed with missing parts. This error is given by the incomplete extraction of 3D feature points, which is caused by additional and missing regions of extracted silhouettes. We cannot prevent silhouettes from being extracted with the additional and missing regions in real environments. Here, we propose an intelligent method of integrating incomplete silhouettes where outcrop points, which are 3D feature points for estimating motion, play an important role. The reconstructed shape can be evaluated referring to how many outcrop points have been included in the reconstructed shape of another frame. Although the evaluation does not represent the accuracy of estimated motion directly, it does guarantee that outstanding parts will be preserved in the reconstructed shape. Silhouettes in multiple frames can be integrated with fewer missing and additional parts based on this evaluation.	evaluation function;visual hull	Masahiro Toyoura;Masaaki Iiyama;Takuya Funatomi;Koh Kakusho;Michihiko Minoh	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761591	iterative reconstruction;computer vision;visualization;feature extraction;shape;computer science;pattern recognition;motion estimation;mathematics;geometry;pixel	Vision	51.97303145166269	-50.76383515519694	184532
9ff2917bd0bc22f52286dfb870ef854ce1bcf0a7	precise isotropic scaling iterative closest point algorithm based on corner points for shape registration		The traditional iterative closest point (ICP) algorithm could register two points sets well, but it is easily affected by local dissimilar. To deal with this problem, this paper proposes an isotropic scaling ICP algorithm with corner point constraint. First, an objective function is proposed under the guidance of the corner points, as the corner points can preserve the similar of the whole shapes. Secondly, a new ICP algorithm is used to complete the isotropic scaling registration. At each step of this new algorithm, the correspondence is built based on the closest point searching, and then a closed-form solution of the transformation is computed. The experimental results demonstrate that our algorithm can prevent the influence of the local dissimilar and improve the registration precision compared with the traditional ICP algorithm.	3d reconstruction;algorithm;genetic algorithm;image scaling;iterative closest point;iterative method;loss function;optimization problem	Shaoyi Du;Wenting Cui;Xuetao Zhang;Liyang Wu;Lei Xiong	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122879	scaling;computer science;iterative closest point;algorithm;algorithm design;linear programming;convergence (routing)	Robotics	50.55308026597266	-51.54966842731612	185256
1d0128b9f96f4c11c034d41581f23eb4b4dd7780	automatic construction of robust spherical harmonic subspaces	dense 3d facial shape recovery robust spherical harmonic subspaces in the wild facial images photometric stereo low rank matrix decomposition 3d model sparse facial feature localisation techniques;stereo image processing feature extraction image reconstruction matrix decomposition solid modelling;shape lighting three dimensional displays harmonic analysis robustness matrix decomposition solid modeling	In this paper we propose a method to automatically recover a class specific low dimensional spherical harmonic basis from a set of in-the-wild facial images. We combine existing techniques for uncalibrated photometric stereo and low rank matrix decompositions in order to robustly recover a combined model of shape and identity. We build this basis without aid from a 3D model and show how it can be combined with recent efficient sparse facial feature localisation techniques to recover dense 3D facial shape. Unlike previous works in the area, our method is very efficient and is an order of magnitude faster to train, taking only a few minutes to build a model with over 2000 images. Furthermore, it can be used for real-time recovery of facial shape.	feature recognition;photometric stereo;real-time clock;real-time recovery;sparse matrix	Patrick Snape;Yannis Panagakis;Stefanos P. Zafeiriou	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298604	computer vision;machine learning;pattern recognition;mathematics	Vision	48.801445085822145	-49.82672018153979	186023
f44010da6e3fffbf5d4f5ea5979bea2c5f9c474e	intelligent surveillance system with see-through technology	translucent blending intelligent surveillance system see through technology parking lot management underground environment feature based image stitching image blending technique video stream multiple cameras feature correspondence detection matrix estimation outlier filtering scheme epipolar geometry ransac homography matrices image fusion;image processing;surveillance;video surveillance computational geometry image fusion image segmentation matrix algebra traffic engineering computing;cameras feature extraction surveillance streaming media estimation artificial intelligence;underground parking garages;video cameras;artificial intelligence	This paper proposes a new intelligent surveillance system for parking lot management in underground environments. The feature based image stitching and image blending techniques are used to let video streams captured by multiple cameras be fused into single output whose blind area could be easily seen through. Such system allows that anyone can immediately understand what is going on in the whole monitored area with a simple glance and without prior geometrical knowledge of the place because the blind area is translucent. The image stitching processes consists of four main parts:(1) feature correspondence detection and fundamental matrix estimation, (2) outlier filtering scheme based on the epipolar geometry and RANSAC, (3) computing the homography matrices between each pair of images, (4) projectively warping the images with their corresponding homography matrices, and conducting image fusion with the non-overlapping parts of the warped images. Finally, the process of translucent blending is applied to eliminate the blind areas inside the stitched image, and then the corresponding cameras behind the occluding pillar provide pixels for translucentizing. We have implemented the preliminary system with six surveillance cameras at underground parking lot environments, and experiment results of real world video sequences have been performed to verify the proposed design.	3d modeling;alpha compositing;autostereogram;binocular disparity;closed-circuit television;computer vision;edge detection;epipolar geometry;fundamental matrix (computer vision);image fusion;image stitching;multi-storey car park;parallax;pixel;random sample consensus;streaming media	Yu-Chen Lin;Che-Tsung Lin;Cheng-Chuan Chang;Long-Tai Chen	2014	17th International IEEE Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2014.6958002	homography;computer vision;simulation;geography;image stitching;computer graphics (images)	Vision	50.58218644091233	-47.91240763970219	186819
89bf8f9b83bf52c5a28dd6bfa9010a8eec3f6408	towards semantic slam using a monocular camera	object recognition;information extraction;real time;kalman filters;semantics;three dimensional;robot vision;hand held camera semantic slam algorithm monocular slam system monocular camera simultaneous localisation and mapping geometric map monocular image sequence extended kalman filter object recognition thread robot;three dimensional displays;slam robots cameras image sequences kalman filters object recognition robot vision;solid modeling;image sequence;simultaneous localization and mapping;simultaneous localization and mapping cameras three dimensional displays object recognition semantics solid modeling;slam robots;cameras;object model;image sequences	Monocular SLAM systems have been mainly focused on producing geometric maps just composed of points or edges; but without any associated meaning or semantic content. In this paper, we propose a semantic SLAM algorithm that merges in the estimated map traditional meaningless points with known objects. The non-annotated map is built using only the information extracted from a monocular image sequence. The known object models are automatically computed from a sparse set of images gathered by cameras that may be different from the SLAM camera. The models include both visual appearance and tridimensional information. The semantic or annotated part of the map -the objects- are estimated using the information in the image sequence and the precomputed object models.	algorithm;map;precomputation;simultaneous localization and mapping;sparse language;sparse matrix	Javier Civera;Dorian Gálvez-López;Luis Riazuelo;Juan D. Tardós;J. M. M. Montiel	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6094648	kalman filter;three-dimensional space;computer vision;object model;computer science;cognitive neuroscience of visual object recognition;semantics;solid modeling;information extraction;computer graphics (images);simultaneous localization and mapping	Robotics	51.9759707014198	-46.702960683410666	187379
c6162223661d9a927eb973f688d53e7dfe6836a2	model evolution: an incremental approach to non-rigid structure from motion	image motion analysis;image representation computer graphics image motion analysis;large nonlinear deformation;computer graphics;compressive shape representation;input image set represention;deformable models;severe occlusion;nonrigid structure from motion;shape representation;computational modeling;3d model;shape;consistent 3d shapes;three dimensional displays;image representation;image reconstruction;solid modeling;perspective camera projection;synthetic data;deformable models shape image reconstruction cameras computational efficiency image coding computer vision large scale systems face robustness;multiple evolutionary paths;input image set represention model evolution nonrigid structure from motion severe occlusion perspective camera projection large nonlinear deformation consistent 3d shapes 3d model multiple evolutionary paths compressive shape representation;structure from motion;cameras;model evolution	In this paper, we present a new framework for non-rigid structure from motion (NRSFM) that simultaneously addresses three significant challenges: severe occlusion, perspective camera projection, and large non-linear deformation. We introduce a concept called a model graph, which greatly reduces the computational cost of discovering groups of input images that depict consistent 3D shapes. A 3D model is constructed for each input image by traversing the model graph along multiple evolutionary paths. A compressive shape representation is constructed, which (1) consolidates the multiple 3D models for each image reconstructed during model evolution and (2) reduces the number of models needed to represent the input image set. Assuming feature correspondences are known, we demonstrate our algorithm on both real and synthetic data sets that exemplify all three aforementioned challenges.	3d modeling;algorithm;computation;computational complexity theory;exemplification;nonlinear system;structure from motion;synthetic data	Shengqi Zhu;Li Zhang;Brandon M. Smith	2010	2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2010.5540085	iterative reconstruction;computer vision;structure from motion;shape;computer science;mathematics;geometry;solid modeling;computer graphics;computational model;synthetic data;computer graphics (images)	Vision	52.17887107613138	-50.288134862191406	188520
ff1c2e057b654ca92558fc52fc8db6497f6c04ea	learning to look at humans - what are the parts of a moving body?	a priori knowledge;number of clusters	We present a system that can segment articulated, non-rigid motion without a priori knowledge of the number of clusters present in the analyzed scenario. We combine existing algorithms for tracking and extend clustering techniques by a self-tuning heuristic. Application to video sequences of humans shows good segmentation into limbs.	humans	Thomas Walther;Rolf P. Würtz	2008		10.1007/978-3-540-70517-8_3	computer vision;a priori and a posteriori;computer science;artificial intelligence;machine learning	Robotics	47.802141338861894	-48.86969087503305	188566
1237ad7250fa33e378bacabb5ed5c0d1e90043ca	merging overlapping depth maps into a nonredundant point cloud		Combining long sequences of overlapping depth maps without simplification results in a huge number of redundant points, which slows down further processing. In this paper, a novel method is presented for incrementally creating a nonredundant point cloud with varying levels of detail without limiting the captured volume or requiring any parameters from the user. Overlapping measurements are used to refine point estimates by reducing their directional variance. The algorithm was evaluated with plane and cube fitting residuals, which were improved considerably over redundant point clouds.	algorithm;algorithmic efficiency;depth map;level of detail;point cloud	Tomi Kyöstilä;C. Herrera DanielHerrera;Juho Kannala;Janne Heikkilä	2013		10.1007/978-3-642-38886-6_53	computer vision;point cloud;artificial intelligence;pattern recognition;merge (version control);cube;computer science;point estimation;limiting	Graphics	52.46624813905386	-47.1187507561274	188681
b498990a90a2b60802f9532fdf88f23caed7e864	multiple person tracking using omnidirectional cameras	signal processing;object tracking;lenses;optimization;markov processes;conferences cameras signal processing optimization lenses markov processes object tracking;cameras;conferences	Person tracking in videos is crucial in different areas such as security applications. In this work we present a method that first finds human presence probabilities on discrete locations via variational Bayesian inference using images obtained from omnidirectional cameras and then uses that information to solve the tracking problem as a flow optimization problem. In our experiments on the BOMNI dataset, we have increased tracking performance (MOTA) to %86.39, which was reported as %68.18 using the baseline method.	baseline (configuration management);experiment;mathematical optimization;omnidirectional camera;optimization problem;variational principle	Baris Evrim Demiröz;Albert Ali Salah;Lale Akarun	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830458	computer vision;simulation;tracking system;computer science;machine learning;signal processing;video tracking;lens;markov process;statistics	Vision	46.668723139715645	-47.3675520235282	188851
8674b58ff33d3fd0241e543cd1fdb0c5a21bde8c	a fully automatic approach to facial feature tracking based on image registration	facial feature tracking;real time;emotion recognition;facial expression tracking automatic facial feature detection automatic facial feature tracking image registration candide 3d wireframe model model shape position initialization method head pose tracking;tracking emotion recognition face recognition feature extraction image registration pose estimation;computational modeling;face recognition;shape;model shape;three dimensional displays;facial expression tracking;feature extraction;image registration;solid modeling;automatic facial feature tracking;automatic facial feature detection;facial feature detection;facial features;face;candide 3d wireframe model;head;position initialization method;head pose tracking;facial features image registration face detection shape head robustness deformable models computer vision cameras active appearance model;tracking;pose estimation	This paper presents a real time, fully automatic facial feature detection and tracking approach. The head pose and facial action is tracked by a modified Candide 3D wireframe model based on an improved image registration technique. An effective model shape and position initialization method is also proposed. Experimental results demonstrate that our system is accurate, robust and fast enough for common applications, even when there are great pose and expression variations.	feature detection (computer vision);feature detection (web development);image registration;motion estimation;wire-frame model	Xuetao Feng;Yangsheng Wang;Bin Ding;Xiaoyan Wang	2008	2008 Ninth International Workshop on Image Analysis for Multimedia Interactive Services	10.1109/WIAMIS.2008.6	facial recognition system;face;computer vision;speech recognition;pose;feature extraction;shape;computer science;image registration;pattern recognition;tracking;solid modeling;head;computational model	Vision	47.4513879532512	-49.19121002393377	189026
18a2080598c73d0fb2cf10812889485a3056828e	random motion for camera calibration	image motion analysis;image registration;statistical analysis;video cameras;camera calibration;comotion statistics random motion;image matching;view registration	In the paper we show that by using co-motion statistics random motion can be used for the registration of views and calibration of cameras. The introduced algorithm finds point correspondences in two views without searching for any structures and without the need for tracking continuous motion.	algorithm;camera resectioning	Zoltán Szlávik;Tamás Szirányi;László Havasi;Csaba Benedek	2005	2005 13th European Signal Processing Conference		computer vision;computer science;pattern recognition;motion estimation;motion field;computer graphics (images)	Vision	51.08969102027681	-49.22149639431112	189218
8ebce5f95fa1954f36fb6038bf9aa40b1ced2556	structured global registration of rgb-d scans in indoor environments		RGB-D scanning of indoor environments (offices, homes, museums, etc.) is important for a variety of applications, including on-line real estate, virtual tourism, and virtual reality. To support these applications, we must register the RGB-D images acquired with an untracked, hand-held camera into a globally consistent and accurate 3D model. Current methods work effectively for small environments with trackable features, but often fail to reproduce large-scale structures (e.g., straight walls along corridors) or long-range relationships (e.g., parallel opposing walls in an office). In this paper, we investigate the idea of integrating a structural model into the global registration process. We introduce a fine-to-coarse algorithm that detects planar structures spanning multiple RGB-D frames and establishes geometric constraints between them as they become aligned. Detection and enforcement of these structural constraints in the inner loop of a global registration algorithm guides the solution towards more accurate global registrations, even without detecting loop closures. During experiments with a newly created benchmark for the SUN3D dataset, we find that this approach produces registration results with greater accuracy and better robustness than previous alternatives.	algorithm;benchmark (computing);bilateral filter;cluster analysis;connected component (graph theory);coplanar waveguide;experiment;file spanning;hierarchical clustering;high- and low-level;inner loop;iteration;iterative method;mathematical optimization;maxima and minima;microsoft windows;mobile device;multi-objective optimization;online and offline;p (complexity);pixel;proxy server;ps (unix);random sample consensus;refinement (computing);sensor;shape analysis (digital geometry);sid meier's alpha centauri;virtual reality;virtual tour;virtual world	Maciej Halber;Thomas A. Funkhouser	2016	CoRR		computer vision;simulation	Vision	52.684779394599055	-46.29654076909196	189310
befb590853be85b0f0a189b16fccaa54becf7b01	statistical methods in video processing		This paper describes a method for accurate dense reconstruction of a complex scene from a small set of high-resolution unorganized still images taken by a hand-held digital camera. A fully automatic data processing pipeline is proposed. Highly discriminative features are first detected in all images. Correspondences are then found in all image pairs by wide-baseline stereo matching and used in a scene structure and camera reconstruction step that can cope with occlusion and outliers. Image pairs suitable for dense matching are automatically selected, rectified and used in dense binocular matching. The dense point cloud obtained as the union of all pairwise reconstructions is fused by local approximation using oriented geometric primitives. For texturing, every primitive is mapped on the image with the best resolution. The global structure reconstruction in the first step allows us to work with an unorganized set of images and to avoid error accumulation. By using object-centered geometric primitives we are able to preserve the flexibility of the method to describe complex free-form structures, preserve the possibility to build the dense model in an incremental way, and to retain the possibility to refine the cameras and the dense model by bundle adjustment. Results are demonstrated on partial models of a circular church and a Henri de Miller’s sculpture. We observed spatial resolution in the range of centimeters on objects of about 20 m in size.	additive white gaussian noise;anytime algorithm;append;approximation;approximation algorithm;baseline (configuration management);binocular vision;bundle adjustment;caret;clutter;cognitive dimensions of notations;computer stereo vision;computer vision;dlt;data dependency;digital camera;dynamic programming;errors-in-variables models;fundamental matrix (computer vision);homography (computer vision);image rectification;image resolution;interpolation;iterative method;lecture notes in computer science;mathematical optimization;matrix multiplication;maxima and minima;mean squared error;mobile device;nonlinear system;point cloud;residual (numerical analysis);return statement;singular value decomposition;springer (tank);stacking;synthetic intelligence;texture mapping;total least squares;transport layer security;tree accumulation;utility functions on indivisible goods;video processing;visual hull	Gerhard Goos;Juris Hartmanis;Jan van Leeuwen;David Hutchison;Dorin Comaniciu;Kenichi Kanatani;Rudolf Mester;David Suter	2004		10.1007/b104157	video processing	Vision	52.858350515593976	-47.48386525192559	189331
f2e968eecf9804a1fd48c02dd20722a16116a0c7	contours, optic flow, and prior knowledge: cues for capturing 3d human motion in videos		Human 3D motion tracking from video is an emerging research field with many applications demanding highly detailed results. This chapter surveys a high quality generative method, which employs the person’s silhouette extracted from one or multiple camera views for fitting an a-priori given 3D body surface model. A coupling between pose estimation and contour extraction allows for reliable tracking in cluttered scenes without the need of a static background. The optic flow computed between two successive frames is used for pose prediction. It improves the quality of tracking in case of fast motion and/or low frame rates. In order to cope with unreliable or insufficient data, the framework is further extended by the use of prior knowledge on static joint angle configurations.	algorithm;b-spline;binary image;coat of arms;control point (mathematics);display resolution;epipolar geometry;fundamental matrix (computer vision);human-based computation;interpolation;kernel density estimation;kinesiology;optical flow;pixel;polygon mesh;reference frame (video);rough set;significant figures;spline (mathematics);topological skeleton;tree accumulation;video tracking;visual computing	Thomas Brox;Bodo Rosenhahn;Daniel Cremers	2006		10.1007/978-1-4020-6693-1_11	computer vision;multimedia;communication	Vision	50.164534428694026	-46.93764922848038	189869
49f7bbd45949af8f025e987eb42051001a8da070	an integrated linear technique for pose estimation from different geometric features	vision ordenador;perspective projection;forma geometrica;geometric feature;computer vision;calibration camera;erreur estimation;geometrical shape;estimacion parametro;error estimacion;projection perspective;exterior camera;vision ordinateur;forme geometrique;estimation error;parameter estimation;estimation parametre;camera calibration;exterior camera parameter estimation;proyeccion perspectiva;pose estimation	Existing linear solutions for the pose estimation (or exterior orientation) problem suffer from a lack of robustness and accuracy partially due to the fact that the majority of the methods utilize only one type of geometric entity and their frameworks do not allow simultaneous use of different types of features. Furthermore, the orthonormality constraints are weakly enforced or not enforced at all. We have developed a new analytic linear least-squares framework for determining pose from multiple types of geometric features. The technique utilizes correspondences between points, between lines and between ellipse-circle pairs. The redundancy provided by different geometric features improves the robustness and accuracy of the least-squares solution. A novel way of approximately imposing orthonormality constraints on the sought rotation matrix within the linear framework is presented. Results from experimental evaluation of the new technique using both synthetic data and real images reveal its improved robustness and accuracy over existing direct methods.	3d pose estimation;linear least squares (mathematics);synthetic data	Qiang Ji;Mauro S. Costa;Robert M. Haralick;Linda G. Shapiro	1999	IJPRAI	10.1142/S0218001499000410	computer vision;mathematical optimization;perspective;camera resectioning;pose;computer science;mathematics;geometry;estimation theory	Vision	53.46720703684354	-49.903087484148585	190375
696aecf98ea53db282d44997228e574da0c1c168	motion from three weak perspective images using image rotation	correspondence;3d object weak perspective images image rotation linear algorithm motion estimation rotation matrices correspondence object pose orthographic projection;motion estimation;weak perspective;matrix algebra;linear system;equations linear systems graphics biomedical engineering image analysis;object pose;matrix algebra image sequences motion estimation;image rotation;rigid object motion;image sequences	In this paper, it is shown that by using image rotation, one can develop a linear algorithm to find motion using three weak perspective images. By using the correspondence of four points over a pair of these images, a function can be developed which allows one to perform the necessary rotation. With the (correct image rotation, one need only add a third image to have an overdetermined linear system with which to solve for the unknown elements of the rotation matrices relating these three images.	3d projection;algorithm;isometric projection;linear system	John Ostuni;Stanley M. Dunn	1996	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.476013	computer vision;topology;plane of rotation;computer science;rotation formalisms in three dimensions;motion estimation;mathematics;geometry;euler's rotation theorem;linear system;motion field	Vision	53.67224632664285	-51.09789886285009	190830
a59ae39cfeb74ee9fde00803cbcb97e2e2740c51	graphtracker: a topology projection invariant optical tracker	categories and subject descriptors according to acm ccs i 4 8 scene analysis image processing and computer vision tracking;i 3 6 computer graphics methodology and techniques interaction techniques	In this paper, we describe a new optical tracking algorithm for pose estimation of interaction devices in virtual and augmented reality. Given a 3D model of the interaction device and a number of camera images, the primary difficulty in pose reconstruction is to find the correspondence between 2D image points and 3D model points. Most previous methods solved this problem by the use of stereo correspondence. Once the correspondence problem has been solved, the pose can be estimated by determining the transformation between the 3D point cloud and the model.  Our approach is based on the projective invariant topology of graph structures. The topology of a graph structure does not change under projection: in this way we solve the point correspondence problem by a subgraph matching algorithm between the detected 2D image graph and the model graph.  There are four advantages to our method. First, the correspondence problem is solved entirely in 2D and therefore no stereo correspondence is needed. Consequently, we can use any number of cameras, including a single camera. Secondly, as opposed to stereo methods, we do not need to detect the same model point in two different cameras, and therefore our method is much more robust against occlusion. Thirdly, the subgraph matching algorithm can still detect a match even when parts of the graph are occluded, for example by the users hands. This also provides more robustness against occlusion. Finally, the error made in the pose estimation is significantly reduced as the amount of cameras is increased.	3d modeling;3d pose estimation;algorithm;augmented reality;correspondence problem;graph (discrete mathematics);hidden surface determination;network topology;point cloud;polygonal modeling;subgraph isomorphism problem	Ferdi A. Smit;Arjen van Rhijn;Robert van Liere	2006		10.2312/EGVE/EGVE06/063-070	computer vision;machine learning;mathematics;computer graphics (images)	Vision	50.92702947988478	-48.12211910396683	191103
b07d625d9a11a7fe2d6818c83c86061a3f83a821	visual hand gesture segmentation using signer model for real-time human-computer interaction application	hand segmentation;motion analysis;biological system modeling tracking motion analysis history performance evaluation testing robustness error analysis computational complexity real time systems;model track;human computer interaction;history;performance evaluation;real time;biological system modeling;testing;human computer interaction hausdorff distance motion analysis model track hand segmentation;error analysis;computational complexity;error rate;hausdorff distance;robustness;tracking;object model;real time systems	The task of automatic gesture segmentation is highly challenging due to the computational burden, the presence of unpredictable body motion and ambiguous nongesture hand motion. In this paper, a new approach is developed using Hausdorff based model tracking technique for the application of real-time human-computer interaction. This paper proposed a Three Phases Model Tracking approach, which consists of two main stages; one is motion history analysis, which classifies dynamic gesture into preparation, retraction and nucleus state based on temporal relationship. The other is model tracking, which tracks signer model and object model with different constraint based on the classified state. Finally, gesture model is extracted based on matching object model and signer model and the hand gesture region is segmented from the gesture model. Experiments are performed to test the robustness of gesture segmentation under various hand scale and complex background. The segmentation error rate and computational complexity are also analyzed to demonstrate that the proposed Three Phases Model Tracking approach can be applicable to real-time human-computer interaction system.	computation;computational complexity theory;digital history;gesture recognition;hausdorff dimension;human–computer interaction;real-time clock;real-time locating system;real-time transcription	Tsung-Han Tsai;Chung-Yuan Lin	2007	2007 IEEE Workshop on Signal Processing Systems	10.1109/SIPS.2007.4387611	hausdorff distance;computer vision;simulation;object model;word error rate;computer science;tracking;software testing;computational complexity theory;robustness	Vision	47.23466414727972	-48.94865298255506	191782
1765c55ccdf703b4eaf4547c5b985e8354346deb	an efficient object tracking algorithm with adaptive prediction of initial searching point	fiabilidad;reliability;pistage;coordinate transform;filtro kalman;image processing;adaptive kalman filter;algoritmo adaptativo;filtre kalman;rastreo;procesamiento imagen;kalman filter;traitement image;adaptive algorithm;algorithme adaptatif;fiabilite;coordinate transformation;object tracking;poursuite cible;filtro adaptable;filtre adaptatif;initial searching point;target tracking;adaptive filter;tracking	In object tracking, complex background frequently forms local maxima that tend to distract tracking algorithms from the real target. In order to reduce such risks, we utilize an adaptive Kalman filter to predict the initial searching point in the space of coordinate transform parameters so that both tracking reliability and computational simplicity is significantly improved. Our method tracks the changing rate of the transform parameters and makes prediction on future values of the transform parameters to determine the initial searching point. More importantly, noises in the Kalman filter are effectively estimated in our approach without any artificial assumption, which makes our method able to adapt to various target motions and searching step sizes without any manual intervention. Simulation results demonstrate the effectiveness of our algorithm.	computational complexity theory;kalman filter;maxima and minima;search algorithm;simulation	Jiyan Pan;Bo Hu;Jian Qiu Zhang	2006		10.1007/11949534_112	adaptive filter;kalman filter;computer vision;mathematical optimization;image processing;computer science;coordinate system;video tracking;reliability;control theory;mathematics;tracking	Vision	47.48047738140663	-47.06926657407298	192105
485984ca90234f4aa6e235d57d1545252df02886	a fast snake-based method to track football player	active contour;active contour model;real time	We present in this paper a new method to track moving objects. It is based on snakes or active contour models. We are concerned with football game analysis and so the moving objects tracked are representing football 'players. The camera is moving too. Our active contour algorithm does not need any preprocessing step contrary to most of the snake-based methods. It is based on classical energies used in active contour algorithms but also on a balloon energy in order to reduce the contour to fit the tracked object. The tracking step does not include any position prediction and is based on a snake initialisation followed by snake deformation. The method implemented is fast enough to consider a real-time framework and has been successfully tested on football game image sequences.	active contour model;algorithm;color image;feasible region;mathematical optimization;multiprocessing;preprocessor;real-time clock;workstation	Sébastien Lefèvre;Cyril Fluck;Benjamin Maillard;Nicole Vincent	2000			computer vision;mathematics;pattern recognition;football;artificial intelligence;active contour model	Vision	48.11945019338713	-45.33066043658798	192359
38eacaecdfe74f92a80f81d89364b003647dfa47	model-based 3d hand tracking with on-line shape adaptation		One of the shortcomings of the existing model-based 3D hand tracking methods is the fact that they consider a fixed hand model, i.e. one with fixed shape parameters. In this work we propose an online model-based method that tackles jointly the hand pose tracking and the hand shape estimation problems. The hand pose is estimated using a hierarchical particle filter. The hand shape is estimated by fitting the shape model parameters over the observations in a frame history. The candidate shapes required by the fitting framework are obtained by optimizing the shape parameters independently in each frame. Extensive experiments demonstrate that the proposed method tracks the pose of the hand and estimates its shape parameters accurately, even under heavy noise and inaccurate shape initialization.	online and offline	Alexandros Makris;Antonis A. Argyros	2015		10.5244/C.29.77	computer vision;pattern recognition;computer science;artificial intelligence;particle filter;online model;initialization	Vision	47.8644229878705	-47.716208312738345	193011
a0edfd00d0db7099d779b399174e7bf38d6415f1	real-time whole-body human motion tracking based on unlabeled markers	kalman filters;biological system modeling;kinematics;covariance matrices;tracking;real time systems	In this paper, we present a novel online approach for tracking whole-body human motion based on unlabeled measurements of markers attached to the body. For that purpose, we employ a given kinematic model of the human body including the locations of the attached markers. Based on the model, we apply a combination of constrained sample-based Kalman filtering and multi-target tracking techniques: 1) joint constraints imposed by the human body are satisfied by introducing a parameter transformation based on periodic functions, 2) a global nearest neighbor (GNN) algorithm computes the most likely one-to-one association between markers and measurements, and 3) multiple hypotheses tracking (MHT) allows for a robust initialization that only requires an upright standing user. Evaluations clearly demonstrate that the proposed tracking provides highly accurate pose estimates in realtime, even for fast and complex motions. In addition, it provides robustness to partial occlusion of markers and also handles unavoidable clutter measurements.	algorithm;clutter;experiment;hungarian algorithm;joint constraints;kalman filter;kinesiology;mhtml;motion capture;one-to-one (data model);real-time computing;real-time transcription;reference model	Jannik Steinbring;Christian Mandery;Florian Pfaff;Florian Faion;Tamim Asfour;Uwe D. Hanebeck	2016	2016 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2016.7849550	computer vision;simulation;control theory;mathematics	Vision	48.21366666098734	-46.987910912194145	193377
f1e85e9f790710323ada826a6c45c907e7c5dfe3	real-time monocular 6-dof head pose estimation from salient 2d points		We propose a real-time and robust approach to estimate the full 3D head pose from extreme head poses using a monocular system. To this end, we first model the head using a simple geometric shape initialized using facial landmarks, i.e., eye corners, extracted from the face. Next, 2D salient points are detected within the region defined by the projection of the visible surface of the geometric head model onto the image, and projected back to the head model to generate the corresponding 3D features. Optical flow is used to find the respective 2D correspondences in the next video frame. Assuming that the monocular system is calibrated, it is then possible to solve the Perspective-n-Point (PnP) problem of estimating the head pose given a set of 3D features on the geometric model surface and their corresponding 2D correspondences from optical flow in the next frame. The experimental evaluation shows that the performance of the proposed approach achieves, and in some cases improves the state-of-the-art performance with a major advantage of not requiring facial landmarks (except for initialization). As a result, our method also applies to real scenarios in which facial landmarks-based methods fail due to self-occlusions.	3d pose estimation;geometric modeling;optical flow;perspective-n-point;real-time clock;real-time computing;real-time transcription	Jilliam María Díaz Barros;Frederic Garcia;Bruno Mirbach;Didier Stricker	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296255	computer vision;feature extraction;initialization;robustness (computer science);artificial intelligence;geometric modeling;pose;computer science;pattern recognition;geometric shape;monocular;optical flow	Robotics	50.917058431592835	-46.17260284762207	193602
3c72cfdd65bedd465ad9e075e2ab9990025d82d6	multi-target tracking through opportunistic camera control in a resource constrained multimodal sensor network	cameras robot sensing systems switches kalman filters tracking target tracking distance measurement;robot sensing systems;video surveillance;wide area video surveillance;data collection;pan tilt zoom;kalman filters;audio video;homography estimation;sparse camera networks multi target tracking opportunistic camera control resource constrained multimodal sensor network wide area video surveillance;camera control;image sensors;graph matching;sensor network;distributed sensors;dynamic environment;distance measurement;opportunistic camera control;region of interest;video surveillance distributed sensors image sensors target tracking;multi target tracking;audio video tracking;graph matching audio video tracking camera control homography estimation;target tracking;scene understanding;switches;camera network;cameras;tracking;resource constrained multimodal sensor network;sparse camera networks	While wide-area video surveillance is an important application, it is often not practical, from a technical and social perspective, to have video cameras that completely cover the entire region of interest. For obtaining good surveillance results in a sparse camera networks requires that they be complemented by additional sensors with different modalities, their intelligent assignment in a dynamic environment, and scene understanding using these multimodal inputs. In this paper, we propose a probabilistic scheme for opportunistically deploying cameras to the most interesting parts of a scene dynamically given data from a set of video and audio sensors. The audio data is continuously processed to identify interesting events, e.g., entry/exit of people, merging or splitting of groups, and so on. This is used to indicate the time instants to turn on the cameras. Thereafter, analysis of the video determines how long the cameras stay on and whether their pan/tilt/zoom parameters change. Events are tracked continuously by combining the audio and video data. Correspondences between the audio and video sensor observations are obtained through a learned homography between the image plane and ground plane. The method leads to efficient usage of the camera resources by focusing on the most important parts of the scene, saves power, bandwidth and cost, and reduces concerns of privacy. We show detailed experimental results on real data collected in multimodal networks.	closed-circuit television;homography (computer vision);image plane;multimodal interaction;privacy;region of interest;sparse matrix;video sensor technology	Jayanth Nayak;Luis Gonzalez-Argueta;Bi Song;Amit K. Roy-Chowdhury;Ertem Tuncel	2008	2008 Second ACM/IEEE International Conference on Distributed Smart Cameras	10.1109/ICDSC.2008.4635682	kalman filter;computer vision;simulation;wireless sensor network;network switch;computer science;video tracking;image sensor;tracking;statistics;matching;data collection;computer graphics (images);region of interest	Vision	46.96596093001244	-45.517271746995114	193991
5c5a487cfcac8630ebb47b89ccd8684008687559	new efficient solution to the absolute pose problem for camera with unknown focal length and radial distortion	radial distortion	In this paper we present a new efficient solution to the absolute pose problem for a camera with unknown focal length and radial distortion from four 2D-to-3D point correspondences. We propose to solve the problem separately for non-planar and for planar scenes. By decomposing the problem into these two situations we obtain simpler and more efficient solver than the previously known general solver. We demonstrate in synthetic and real experiments significant speedup as our new solvers are about 40× (non-planar) and 160× (planar) faster than the general solver. Moreover, we show that our two solvers can be joined into a new general solver, which gives comparable or better results than the existing general solver for of most planar as well as non-planar scenes .	code;correspondence problem;distortion;experiment;focal (programming language);matlab;radial (radio);solver;speedup;synthetic intelligence	Martin Bujnak;Zuzana Kukelova;Tomás Pajdla	2010		10.1007/978-3-642-19315-6_2	distortion;computer vision;mathematical optimization;computer science;mathematics;geometry	Vision	53.350687387975896	-48.64628001283696	194990
1ce2afc11655cb5439afbfcb7ec937593a576c94	scene segmentation from depth and color data driven by surface fitting	color;depth;segmentation;nurbs;kinect;nurbs model scene segmentation scheme color data surface fitting color information consumer depth cameras depth data 3d surface estimation scheme multidimensional vectors geometry information normalized cuts spectral clustering;surface fitting cameras image colour analysis image segmentation pattern clustering;nurbs segmentation depth color kinect;image segmentation image color analysis three dimensional displays splines mathematics joints surface topography	Scene segmentation is a very challenging problem for which color information alone is often not sufficient. Recently the introduction of consumer depth cameras has opened the way to novel approaches exploiting depth data. This paper proposes a novel segmentation scheme that exploits the joint usage of color and depth data together with a 3D surface estimation scheme. Firstly a set of multi-dimensional vectors is built from color and geometry information and normalized cuts spectral clustering is applied to them in order to coarsely segment the scene. Then a NURBS model is fitted on each of the computed segments. The accuracy of the fitting is used as a measure of the plausibility that the segment represents a single surface or object. Segments that do not represent a single surface are split again into smaller regions and the process is iterated until the optimal segmentation is obtained. Experimental results show how the proposed method allows to obtain an accurate and reliable scene segmentation.	cluster analysis;iteration;memory segmentation;multivariate interpolation;non-uniform rational b-spline;plausibility structure;scheme;spectral clustering	Giampaolo Pagnutti;Pietro Zanuttigh	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025894	computer vision;range segmentation;non-uniform rational b-spline;computer science;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;scale-space segmentation;segmentation;computer graphics (images)	Robotics	48.949754750317744	-51.827752544063195	195015
cc635f590d55049cffed409d979d4b5edcd19959	smart particle filtering for 3d hand tracking	filtering particle tracking particle filters stochastic processes sampling methods robustness cameras skin joints computer vision;optimisation;image motion analysis;high dimensionality;bayes methods;optimal method;psi_visics;bayes methods filtering theory image motion analysis optimisation;stochastic meta descent;gradient descent;particle filter;hand tracking;filtering theory;bayesian distribution articulated structure smart particle filtering 3d hand tracking stochastic metadescent method	Solving the tracking of an articulated structure in a reasonable time is a complex task mainly due to the high dimensionality of the problem. A new optimization method, called stochastic meta-descent (SMD), based on gradient descent with adaptive and parameter specific step sizes was introduced previously [M. Bray et al., 2004] to solve this challenging problem. While the local optimization works very well, reaching the global optimum is not guaranteed. We therefore propose a novel algorithm which combines the SMD optimization with a particle filter to form 'smart particles'. After propagating the particles, SMD is performed and the resulting new particle set is included such that the original Bayesian distribution is not altered. The resulting 'smart particle filter' (SPF) tracks high dimensional articulated structures with far fewer samples than previous methods. Additionally, it can handle multiple hypotheses, clutter and occlusion which pure optimization approaches have problems. The performance of the SMD particle filter is illustrated in challenging 3D hand tracking sequences demonstrating a better robustness and accuracy than those of a single SMD optimization or an annealed particle filter.	algorithm;bittorrent tracker;clutter;gesture recognition;global optimization;hidden surface determination;loss function;mathematical optimization;overhead (computing);parallel computing;particle filter;service mapping description;stochastic gradient descent;surface-mount technology	Matthieu Bray;Esther Koller-Meier;Luc Van Gool	2004	Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.	10.1109/AFGR.2004.1301612	gradient descent;computer vision;mathematical optimization;particle filter;auxiliary particle filter;computer science;machine learning	Vision	47.11770057389077	-48.77447123003615	195304
354d83f2da6997df81951bc7a0aec188704df14d	optimal combination of multiple sensors including stereo vision	stereo vision	The statistical combination of information from multiple sources is considered. The particular needs of the target application, stereo vision, require that the formulation be adequate to deal with highly correlated errors and constraints, and that it deal naturally with geometrical data.	sensor;stereopsis	John Porrill;Stephen Pollard;John E. W. Mayhew	1987	Image Vision Comput.	10.1016/0262-8856(87)90046-1	computer stereo vision;stereo cameras;computer vision;computer science;stereopsis;mathematics	Vision	48.19070954518084	-47.241192193511914	195664
052a7ec4cb6e54b31f2e718d38aa3c5f38d0002e	a geometric calibration methodology for single-head cone-beam x-ray systems	minimisation;minimization;extrinsic parameter;x ray inspection;robotics;methode geometrique;inspection;geometrical method;radiography;computer vision;cone beam;geometric calibration;robotica;x ray radiography;vision ordinateur;etalonnage;robotique;metodo geometrico;parameter estimation;estimation parametre;camera calibration;radiographie rx;computational efficiency;calibration;object model;extrinsic parameters;x rays	During X-ray based quality inspection, accurate reconstruction of a 3D object model from a set of its 2D X-ray projections requires efficient geometric calibration, i.e., accurate estimation of the geometric parameters of the setup. We present a calibration methodology for the estimation of the geometric parameters of single-head cone-beam X-ray radiography systems. Our method is related to known approaches regarding camera calibration and geometric calibration of tomography/radiography systems, but performs better in terms of computational efficiency.		Kostas J. Kyriakopoulos;P. Yiannakos;V. Kallipolites;K. Domales	1999	Journal of Intelligent and Robotic Systems	10.1023/A:1008024611699	computer vision;minimisation;mathematical optimization;calibration;camera resectioning;radiography;object model;inspection;computer science;robotics;estimation theory	Robotics	52.03426734612434	-51.7884860516214	196000
f02d2182bd07dccb6d45b435788250c832056ecf	3d deformable face model for pose determination and face synthesis	face synthesis;estimation theory;wire frame model;estimation theory face recognition edge detection image colour analysis feature extraction image matching;edge detection;face shape;image matching;pose determination;deformable models;color model;wire;image synthesis;3d model;image generation;face recognition;3d deformable face model;shape;complex deformation 3d deformable face model pose determination face synthesis edge model color model wire frame model face shape facial features image analysis rotational parameters 2d face templates face image matching pose estimation;image edge detection;image color analysis;image colour analysis;feature extraction;rotational parameters;face modeling;facial features;image analysis;deformable models facial features wire shape image color analysis head face recognition face detection image edge detection image generation;head;face detection;complex deformation;edge model;2d face templates;pose estimation;face image matching	7KLV SDSHU GHVFULEHV D IDFH UHFRJQLWLRQ V\VWHP EDVHG RQ D UHFRJQLWLRQ E\ V\QWKHVLV DSSURDFK *LYHQ DQ LPDJH RI DQ XQNQRZQ IDFH WKH SRVH RI WKH IDFH LV ILUVW HVWLPDWHG E\ PDWFKLQJ LW WR D ' GHIRUPDEOH IDFH PRGHO ZKLFK FDQ HQFRGH VKDSH DV ZHOO DV WH[WXUH 7KLV PRGHO LV D FRPSRVLWH RI WKUHH VXE PRGHOV HGJH PRGHO FRORU PRGHO DQG D ZLUH IUDPH PRGHO ZKLFK MRLQWO\ GHVFULEH WKH VKDSH RI WKH IDFH DQG YDULRXV IDFLDO IHDWXUHV $IWHU HVWLPDWLQJ WKH SRVH RI WKH XQNQRZQ IDFH WKH V\VWHP V\QWKHVL]HV IDFH LPDJHV RI NQRZQ VXEMHFWV LQ WKH VDPH SRVH DV WKH XQNQRZQ IDFH $ OHDVW VTXDUHV SURFHGXUH LV XVHG IRU IDFH V\QWKHVLV WR EHVW DSSUR[LPDWH WKH OLJKWLQJ LQ WKH XQNQRZQ IDFH LPDJH LQ WHUPV RI WKH WUDLQLQJ IDFH LPDJHV 7KH XQNQRZQ IDFH LV ILQDOO\ FODVVLILHG DV WKH VXEMHFW ZKRVH V\QWKHVL]HG LPDJH LV PRVW VLPLODU 7KH QRYHOW\ RI WKLV PHWKRG OLHV LQ WKH XVH RI WKH FRPSRVLWH ' GHIRUPDEOH IDFH PRGHO IRU IDFH DQDO\VLV WR \LHOG WKH ' VKDSH DQG WH[WXUH IDFH UHSUHVHQWDWLRQ DQG WKHUHE\ IDFLOLWDWH IDFH V\QWKHVLV DQG SRVH LQYDULDQW IDFH UHFRJQLWLRQ ([SHULPHQWDO UHVXOWV VKRZ WKDW WKH PHWKRG LV FDSDEOH RI GHWHUPLQLQJ SRVH DQG UHFRJQL]LQJ IDFHV DFFXUDWHO\ RYHU D ZLGH UDQJH RI SRVHV DQG ZLWK YDU\LQJ OLJKWLQJ FRQGLWLRQV 5HFRJQLWLRQ UDWHV RI KDYH EHHQ DFKLHYHG ZKLFK LV KLJKHU WKDQ WKDW RI WKH 3&$ PHWKRG LQ D FRPSDUDWLYH HYDOXDWLRQ	data quality;letter-quality printer;limewire;logical volume management;rs-232	Mun Wai Lee;Surendra Ranganath	1999		10.1109/ICIAP.1999.797605	computer vision;face detection;color model;image analysis;pose;edge detection;wire-frame model;feature extraction;shape;computer science;pattern recognition;estimation theory;head;computer graphics (images)	DB	49.05970980594327	-50.37690426501534	196151
0bf6d70bc85a2ad4170658e7beba7c6fc924dc69	line-based monocular graph slam		This paper presents a new line based 6-DOF monocular algorithm that uses the iSAM2, a point-based Graph SLAM approach. We extend iSAM2 to minimize the reprojection error of the line features to solve the line-based SLAM problem. A specific line representation is exploited that combines the Plücker Coordinates and the Cayley representation. The Plücker Coordinates are used for the 3D line projection function, and the Cayley representation helps to update the lines parameters during the non-linear optimization process. An undelayed initialization method with inverse depth parameters is also adopted. Both simulation and real experiments are carried out showing that the approach achieves high accuracy and consistency, and outperforms EKF-based SLAM method.	algorithm;experiment;extended kalman filter;geographic coordinate system;linear programming;magma;mathematical optimization;nonlinear programming;nonlinear system;real-time clock;reprojection error;requirement;simulation;simultaneous localization and mapping;virtual reality headset	Dong Ruifang;Vincent Frémont;Simon Lacroix;Isabelle Fantoni;Changan Liu	2017	2017 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI)	10.1109/MFI.2017.8170369	visualization;projection (set theory);artificial intelligence;computer vision;simultaneous localization and mapping;computer science;plücker coordinates;initialization;extended kalman filter;monocular;image segmentation	Robotics	52.711200461999084	-48.3065465416995	197998
05d138a9e8080d3ac6204500d0014545a4b65a0b	robust and precise registration of oblique images based on scale-invariant feature transformation algorithm	least squares approximations;image matching;weighted least square;oblique images;accuracy;vectors;estimation;scale invariant feature transform;image registration;mathematical model;least square matching lsm;least squares matching;scale invariant feature transformation sift affine invariant image registration least square matching lsm oblique images;robustness;affine invariant;accuracy vectors mathematical model estimation robustness equations;scale invariant feature transformation sift;least squares approximations image matching image registration;coarse to fine multistage strategy oblique image registration scale invariant feature transformation algorithm automatic registration weighted least square matching method 2 d projective transformation normalized cross correlation metric image matching;normalized cross correlation	The automatic registration of oblique images taken at different viewpoints remains a challenge until today. Based on scale-invariant feature transformation (SIFT) algorithm, a robust and accurate weighted least square matching (LSM) (SIFT/LSM) method modeled using 2-D projective transformation is proposed for highly accurate registration of oblique images. Normalized cross correlation (NCC) metric modified by an adaptive scale and orientation of SIFT features (SIFT/NCC) is proposed to obtain a good initial estimation for the SIFT/LSM. For practical use, image matching is implemented using a coarse-to-fine multistage strategy by sequentially incorporating the standard SIFT algorithm, SIFT/NCC, and SIFT/LSM. Experiments conducted on oblique images of real-world scenes demonstrate the feasibility of the proposed approach.	algorithm;oblique projection	Huachao Yang;Shubi Zhang;Yongbou Wang	2012	IEEE Geosci. Remote Sensing Lett.	10.1109/LGRS.2011.2181485	computer vision;mathematical optimization;estimation;image registration;cross-correlation;pattern recognition;mathematical model;scale-invariant feature transform;mathematics;accuracy and precision;statistics;robustness	Vision	50.3415267026945	-51.753963198569146	198145
cff22dd0da3f8fa660949f46ac52544152c8146e	fast and robust absolute camera pose estimation with known focal length		Some 3D computer vision techniques such as structure from motion (SFM) and augmented reality (AR) depend on a specific perspective-n-point (PnP) algorithm to estimate the absolute camera pose. However, existing PnP algorithms are difficult to achieve a good balance between accuracy and efficiency, and most of them do not make full use of the internal camera information such as focal length. In order to attack these drawbacks, we propose a fast and robust PnP (FRPnP) method to calculate the absolute camera pose for 3D compute vision. In the proposed FRPnP method, we firstly formulate the PnP problem as the optimization problem in the null space that can avoid the effects of the depth of each 3D point. Secondly, we can easily get the solution by the direct manner using singular value decomposition. Finally, the accurate information of camera pose can be obtained by optimization strategy. We explore four ways to evaluate the proposed FRPnP algorithm with synthetic dataset, real images, and apply it in the AR and SFM system. Experimental results show that the proposed FRPnP method can obtain the best balance between computational cost and precision, and clearly outperforms the state-of-the-art PnP methods.	3d pose estimation;aerial photography;algorithm;algorithmic efficiency;archive;augmented reality;computation;computer stereo vision;computer vision;displacement mapping;focal (programming language);fastest;iterative method;kernel (linear algebra);mathematical optimization;optimization problem;p3p;perspective-n-point;point cloud;simultaneous localization and mapping;singular value decomposition;structure from motion;synthetic intelligence;unmanned aerial vehicle	Mingwei Cao;Wei Jia;Yang Zhao;Shujie Li;Xiaoping Liu	2017	Neural Computing and Applications	10.1007/s00521-017-3032-6	3d reconstruction;focal length;mathematical optimization;mathematics;real image;structure from motion;computer vision;singular value decomposition;pose;artificial intelligence;augmented reality;optimization problem	Vision	52.899028584524075	-48.38531291187029	198590
35be6e0b529cac00078441683ee3f3af21f3d47d	fusion4d: real-time performance capture of challenging scenes	4d reconstruction;real time;info eu repo semantics article;nonrigid;multi view	We contribute a new pipeline for live multi-view performance capture, generating temporally coherent high-quality reconstructions in real-time. Our algorithm supports both incremental reconstruction, improving the surface estimation over time, as well as parameterizing the nonrigid scene motion. Our approach is highly robust to both large frame-to-frame motion and topology changes, allowing us to reconstruct extremely challenging scenes. We demonstrate advantages over related real-time techniques that either deform an online generated template or continually fuse depth data nonrigidly into a single reference model. Finally, we show geometric reconstruction results on par with offline methods which require orders of magnitude more processing time and many more RGBD cameras.	algorithm;coherence (physics);motion capture;online and offline;real-time clock;real-time computing;real-time locating system;reference model	Mingsong Dou;Sameh Khamis;Yury Degtyarev;Philip L. Davidson;Sean Ryan Fanello;Adarsh Kowdle;Sergio Orts;Christoph Rhemann;David Kim;Jonathan Taylor;Pushmeet Kohli;Vladimir Tankovich;Shahram Izadi	2016	ACM Trans. Graph.	10.1145/2897824.2925969	computer vision;real-time computing;simulation;computer science;computer graphics (images)	Graphics	52.95940796568519	-46.40613583520543	198859
af4b755e1686d2dc81eb69e92307115597e42fe5	registration and integration of multiple depth images using signed distance function	iterative closest point icp;depth camera;3d sensing;virtual reality;3d scanning;registration;implicit moving least squares imls;signed distance registration;depth integration;3d reconstruction;cameras;computer vision technology	Depth camera is a new technology with the potential of radically changing the way to record and interact with 3D virtual environments. With depth cameras, one has access to depth information up to 30 frames per second, which is much faster than previous 3D scanners. This speed enables new applications where objects are no longer required to be static for 3D sensing. There is, however, a trade-off between the speed and the quality of the results. Depth images acquired with current depth cameras exhibit a high level of noise and have low resolution, which pose a real obstacle when incorporating the 3D information into computer vision techniques. To overcome these limitations, the speed of depth camera could be leveraged by combining data from multiple depth frames together. Thus, good registration and integration methods are needed that are specifically designed for such low quality data. Towards this goal, this paper presents a novel method for the registration and integration of multiple depth frames over time onto a global model represented by an implicit moving least square surface. Experimental results demonstrate promising high performance compared to existing methods.	3d scanner;algorithm;approximation;computation;computer vision;high-level programming language;image resolution;smoothing;virtual reality	Daniel B. Kubacki;Huy Quang Bui;S. Derin Babacan;Minh N. Do	2012		10.1117/12.924275	3d reconstruction;computer vision;simulation;computer science;virtual reality;computer graphics (images)	Vision	53.64179427916666	-46.050900168351255	199419
be66fe9e33e30b19174bfc23bf0248fb80f9939a	towards using virtual forces for image registration	geophysical image processing;uav;map building;video cameras autonomous aerial vehicles computer vision geophysical image processing image registration;unmanned aerial vehicle;map building virtual forces image registration uav;computer vision;aerial image;video cameras;image registration;virtual forces;autonomous aerial vehicles;map building virtual forces aerial image registration disaster unmanned aerial vehicles cameras flight attitude quasi orthographic images overlapping images uav;force feature extraction image registration vectors optimization cameras robots;no reference	This paper introduces a novel method for registering a large amount of aerial images when camera parameters are almost unknown and no reference images are available. The envisioned application is the creation of an overview map of a disaster area from images made by unmanned aerial vehicles (UAVs) equipped with cameras. The camera systems only have uncertain information about flight attitude. With traditional methods, the relatively small perspective errors per image sum up over time resulting in perspective errors which prevent building a consistent map from successively arriving images. By using virtual forces between images, the image parameters are continuously adapted to the current map. The small projective errors of quasi orthographic images are distributed among overlapping images. Having UAVs which deliver quasi orthographic images, our approach can build a contemporary overview without the need to have all images in advance.	aerial photography;algorithm;cluster analysis;computation;distortion;image registration;iteration;k-means clustering;orthographic projection;overview map;time complexity;unmanned aerial vehicle	Claudius Stern;Christoph Rasche;Lisa Kleinjohann;Bernd Kleinjohann	2011	The 5th International Conference on Automation, Robotics and Applications	10.1109/ICARA.2011.6144925	computer vision;computer science;image registration;computer graphics (images)	Robotics	51.29335997332798	-47.69595584255425	199741
