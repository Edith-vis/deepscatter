id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
933b86a16c53e9779fcf14401b158e5522c5698e	gm/lm based error concealment for mpeg-4 video transmission over high lossy and noisy networks	bandwidth limited networks;propagation losses;error concealment;motion compensation;noisy networks;gm lm based error concealment;wireless network;temporal spatial information;video quality;motion estimation;video sequences;redundant information;transform coding;noise measurement;mpeg 4 video transmission;motion compensated;video coding;streaming media transform coding video sequences pixel noise measurement propagation losses forward error correction;mpeg 4 advanced simple profile coded video;forward error correction;streaming media;video coding image reconstruction image sequences motion estimation;error prone channels;macro block;image reconstruction;pixel;video transmission;global motion local motion;high lossy networks;global motion;motion compensation gm lm based error concealment mpeg 4 video transmission high lossy networks noisy networks video quality bandwidth limited networks error prone channels global motion local motion mpeg 4 advanced simple profile coded video temporal spatial information redundant information;spatial information;image sequences	Error concealment (EC) techniques are often adopted at the decoder side to improve the reconstructed video quality, in case of some information is lost during transmission on a bandwidth limited wired networks or wireless networks with error-prone channels. In this paper, a global motion/local motion (GM/LM) based error concealment technique is proposed for transporting MPEG-4 advanced simple profile (ASP) coded video over high lossy and noisy networks. Different from the conversional error concealment methods, which make full use of the temporal spatial information to recover the error macro-blocks, the proposed GM/LM based method can efficiently improve the reconstructed video quality by using the redundant information which sent at encoder side. The redundant information consists of the global motion parameters and global motion compensation/local motion compensation (GMC/LMC) map information for each macro-block (MB) of P-frames. Comparison results with different error concealment methods on different video sequences show the effectiveness of the proposed GM/LM based error concealment method.	advanced audio coding;cognitive dimensions of notations;encoder;error concealment;global motion compensation;little man computer;lossy compression	Xueming Qian;Guizhong Liu;Huan Wang	2008	2008 International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2008.268	iterative reconstruction;computer vision;transform coding;telecommunications;computer science;noise measurement;video quality;wireless network;motion estimation;spatial analysis;forward error correction;motion compensation;pixel;statistics	Robotics	48.38505019743474	-16.321764216269173	188624
f8a621b4eba8f19aeaca14b4040426e622e6f43a	joint texture-shape optimization for mpeg-4 multiple video objects	teletrafic;dynamic programming;theorie vitesse distorsion;video object;evaluation performance;texture;rate distortion;programacion dinamica;mpeg 4 standard bit rate shape rate distortion dynamic programming layout decoding motion compensation arithmetic encoding;performance evaluation;decoding;flow rate regulation;evaluacion prestacion;gestion trafic;simulation;rate distortion r d;compresion senal;traffic control;simulacion;joint texture shape optimization;dynamic program;recherche developpement;verification model joint texture shape optimization mpeg 4 multiple video objects optimal multiple video object bit allocation rate distortion models dynamic programming technique decoded picture quality;traffic management;forma geometrica;qualite image;compression signal;image texture;algorithme;rate distortion theory;asignacion bit;allocation bit;algorithm;rate control;video coding;shape optimization;teletrafico;research and development;rate distortion r d bit allocation joint texture shape optimization mpeg 4 multiple video object rate control;regulation debit;decoding image texture dynamic programming rate distortion theory video coding;investigacion desarrollo;arbitrary shape body;image quality;signal compression;geometrical shape;textura;programmation dynamique;teletraffic;gestion trafico;mpeg 4;forme geometrique;calidad imagen;corps forme arbitraire;bit allocation;regulation trafic;cuerpo forma arbitraria;multiple video object;regulacion trafico;regulacion caudal;verification model;algoritmo	In this paper, we present a uniform framework for optimal multiple video object bit allocation in MPEG-4. We combine the rate-distortion (R-D) models for the texture and shape information of arbitrarily shaped video objects to develop the joint texture-shape R-D models. The dynamic programming technique is applied to optimize the bit allocation for the multiple video objects. The simulation results demonstrate that the proposed joint texture-shape optimization algorithm outperforms the MPEG-4 verification model on the decoded picture quality.	algorithm;blum axioms;computational complexity theory;distortion;dynamic programming;image quality;mad;mathematical optimization;random effects model;shape optimization;simulation	Zhenzhong Chen;King Ngi Ngan	2005	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2005.852621	image quality;image texture;computer vision;active traffic management;simulation;rateâ€“distortion theory;computer science;shape optimization;dynamic programming;video tracking;texture;mpeg-4;algorithm	Vision	47.174185038155464	-14.81502968631045	188863
5904ae87741a663a194992c05292d7545f3b399f	statistical optimization of pr-qmf banks and wavelets	high pass;performance measure;discrete wavelet transforms;phase change materials;quadrature mirror filters;quantization;minimum reconstruction error;subband signals coding;pulse code modulation;filter bank;statistical optimization;band pass filters;cross correlation;prototypes;kl transform;multiresolution signal decomposition;correlation methods;coding gain;statistical analysis signal resolution encoding filtering theory circuit optimisation wavelet transforms band pass filters pulse code modulation correlation methods quadrature mirror filters;wavelet transforms;pr qmf banks;statistical analysis;filter bank phase change materials prototypes signal resolution image reconstruction signal processing signal synthesis discrete wavelet transforms quantization decorrelation;image reconstruction;signal processing;optimal power complementary filters;signal resolution;decorrelation;filter banks;signal synthesis;circuit optimisation;pcm;encoding;subband signals coding pr qmf banks optimal power complementary filters statistical optimization multiresolution signal decomposition performance measure filter banks cross correlation delays minimum reconstruction error coding gain pcm kl transform decorrelation;filtering theory;delays	The design of optimal power complementary filters for multiresolution signal decomposition is considered. As a new performance measure in statistical optimization of filter banks the norm of cross-correlation (all subbands including all delays) will be used for optimization. The obtained filters will be compared to filters having minimum reconstruction error when the highpass branches are dropped. In the paraunitary case, which is considered here, this reconstruction error is closely related to the coding gain of PCM in subbands over direct PCM. While the KL-transform combines both minimum reconstruction error and perfect decorrelation, filter banks do not. It is clear that the norm of cross-correlation becomes very important when better techniques than PCM are considered for the coding of subband signals.	coding gain;cross-correlation;decorrelation;filter bank;mathematical optimization;principal component analysis;quadrature mirror filter;signal-to-noise ratio;wavelet	Alfred Mertins	1994		10.1109/ICASSP.1994.390077	pulse-code modulation;computer vision;speech recognition;telecommunications;computer science;signal processing;control theory;mathematics;statistics	Vision	47.67216533610625	-11.718283405106197	191761
0bbcc1a59d282349f1f96dc8c142d2247dfd3f95	trade off between robustness and r-d performance in zero padding multiple description	multiple description;discrete fourier transforms video coding image sequences;video coding;multiple description coding;robustness covariance matrix image sampling performance analysis karhunen loeve transforms quantization additive noise image coding communication standards degradation;r d performance zero padding multiple description coding video communications dft domain sampling factor;video communication;discrete fourier transforms;zero padding;image sequences	tion coding (MDC) by sequence underimplemented by preand post-processing, ciated to most standards for image/video . Unfortunately, this method suffers from ion when only one description is received. rcome this limitation, over sampling of ro padding in the DFT domain has been prove correlation between subsequent hieving better estimation of the lost price to be paid is a degradation of the case both descriptions are received. In this t analytical results of zero padded MDC, in the selection of the best trade-off ance and over sampling factor. INTRODUCTION ich enable the creation of multiple images or video sequences, by means of ocessing to standard co-decoders are very t, they allow to exploit a possible spatial hieve transmission robustness, without ification of wide spread standards. A method to achieve this goal it to split the to be encoded into two sub sequences, d on to the standard encoder (e.g. JPEG in ges). At the receiver side, the received sub mbined to achieve the full quality image. scription is corrupted or lost, it can be he received one, exploiting the correlation o. Therefore, the quality of the decoded dependent on the correlation between the er sampling of the original sequence prior ion generation can help improving the acteristics, and has been proposed in [1], gh, the amount of zero padding should be lled, as an excessive sequence expansion em performance in case both descriptions are received. In transmitted bit characteristics a analytical resul padded MDC (Z best trade off bet en supported by CERCOM Center for Multimedia ons 2 ZP-MDC Let us consider N random process , whe assume that this Z =P-N zeros i normalized DFT sequence can be N X I R 2 0 Ïƒ =	elegant degradation;encoder;i/o controller hub;jpeg;pâ€“n junction;sampling (signal processing);stochastic process;video post-processing	Gabriella Olmo;Tammam Tillo	2003		10.1109/ISSPA.2003.1224648	telecommunications;computer science;theoretical computer science;multiple description coding	ML	49.51555959709522	-13.31454391234518	192173
36b0a512536ef651e59ec36951aa0bd369fc8092	a post-processing algorithm for performance enhancement of remote video-based monitoring systems	wireless networks;video surveillance;awgn channel;image coding;rail transportation;video signal processing;perceptive quality;surveillance;awgn channel real time post processing algorithm performance enhancement remote video based monitoring systems jpeg based video surveillance noisy channels noise altered blocks spatio temporal redundancy jpeg transmission simulations abandoned objects detection perceptive quality;real time post processing algorithm;real time;awgn channels video signal processing surveillance real time systems computerised monitoring;noise altered blocks;transform coding;remote video based monitoring systems;digital cameras;monitoring system;awgn channels;jpeg transmission simulations;bandwidth;spatio temporal redundancy;remote monitoring;performance enhancement;abandoned objects detection;computerised monitoring;noisy channels;side information;object detection;remote monitoring object detection image coding digital cameras bandwidth transform coding rail transportation wireless networks surveillance image sequences;jpeg based video surveillance;real time systems;image sequences	~ This work presents a real-time post-processing algorithm developed for enhancing the performances of remote JPEG-based video-surveillance applications, seriously degraded by transmission over noisy channels. The aim of the algorithm is to distinguish between blocks changes due to variations in the observed scene and noisealtered blocks, which contain errors due to ehanncl noise and can be corrected exploiting the strong spatio-temporal redundancy of the encoded digital source without any side information. Experimental results, obtained through JPEG transmission simulations performed in the context of B remote video-surveillance system devoted to detection of abandoned objects, show a good improvement both in terms of perceptive quality and of the performance of the overall video-surveillance system.	algorithm;jpeg;performance;real-time clock;simulation;video post-processing	Claudio Sacchi;Fabrizio Granelli;Carlo S. Regazzoni	1999		10.1109/MMSP.1999.793861	embedded system;computer vision;real-time computing;transform coding;computer science;wireless network;bandwidth;rmon	EDA	49.382064477025025	-16.02648505235834	192387
229661899e1757f340ef487fd4c43f678c3533ca	perceptually-based robust image transmission over wireless channels	radio networks;channel coding;erasures;variable length codes;wireless channels;error correction codes;image coding;error protection;visual quality level;neural networks;decoding;source coding system;neural nets;neural network perceptually based robust image transmission wireless channels error protection perceptually based image coder wavelet transformation uniform quantization variable length entropy encoding coded data spatially scalable stream transmission errors tolerance unequal error protection system turbo codes neural networks decoder error prediction erasures outer turbo code concatenation reed solomon codes code rate redundancy minimisation throughput performance visual quality level high visual quality images low signal to noise ratio channel coding source coding system;image communication;outer turbo code concatenation;decoder error prediction;visual communication;reed solomon codes;turbo codes;high visual quality images;transform coding;low signal to noise ratio;wavelet transforms;code rate;robustness image communication image coding turbo codes redundancy protection quantization entropy streaming media image segmentation;wavelet transform;image transmission;uniform quantization;unequal error protection system;entropy codes;unequal error protection;perceptually based robust image transmission;coded data;transmission errors tolerance;redundancy minimisation;perceptually based image coder;radio networks visual communication image coding error correction codes reed solomon codes variable length codes transform coding wavelet transforms entropy codes turbo codes channel coding decoding source coding neural nets;throughput performance;variable length entropy encoding;spatially scalable stream;wavelet transformation;turbo code;neural network;source coding;spatial scalability	A robust image transmission system incorporating perceptually-based coding and error protection is propose d for wireless channels. The perceptually-based image coder consists of a wavelet transformation, uniform quantizatio n, and variable-length entropy encoding. The coded data is placed in a spatially scalable stream divided into segments of varying perceptual importance and tolerance to transmis sion errors. A novel unequal error protection system is designed using turbo codes for the wireless channel, neural networks to predict decoder error and calculate erasures, and outer turbo-code concatenation of Reed-Solomon codes at different rates. A model of error tolerance is developed to minimize the added redundancy required to meet the desired throughput performance and visual quality level. The proposed system provides excellent reliability allowing t he transmission of high visual quality images at a small additional redundancy and low signal-to-noise ratio (4.5 dB).	artificial neural network;concatenated error correction code;concatenation;entropy encoding;error-tolerant design;reedâ€“solomon error correction;scalability;signal-to-noise ratio;sion's minimax theorem;throughput;turbo code;wavelet transform	Michael Eoin Buckley;Marcia G. Ramos;Sheila S. Hemami;Stephen B. Wicker	2000		10.1109/ICIP.2000.899244	turbo code;speech recognition;telecommunications;computer science;theoretical computer science;machine learning;mathematics;artificial neural network;statistics;wavelet transform	Mobile	48.201137221009205	-13.44738875951153	192606
487c178458e547d428c925f170509b3b9cbdfe97	universal coding of function spaces as a model signal compression	entropy data compression extraterrestrial measurements encoding signal representations computer science mathematical model signal processing speech decoding;data compression;decoding;epsilon entropy;signal representations;universal coding;speech;transform coding;function space;signal processing;kolmogorov entropy;entropy codes;signal compression;mathematical model;kolmogorov entropy signal compression mathematical model universal coding data compression epsilon entropy;entropy;computer science;entropy codes data compression transform coding;extraterrestrial measurements;encoding	This paper addresses the problem of signal compression, basing on the mathematical model, in which a set of all possible signals is considered as a function space with a metric /spl rho/. The main attention is focused on the minimization of the size of compressed representation, when function characteristics are not known precisely.	mathematical model;signal compression;universal code (data compression)	Boris Ryabko;Jaakko Astola	2004	Data Compression Conference, 2004. Proceedings. DCC 2004	10.1109/DCC.2004.1281483	data compression;entropy;transform coding;speech recognition;function space;computer science;speech;theoretical computer science;signal processing;pattern recognition;mathematical model;mathematics;encoding;statistics	EDA	48.44069133405179	-10.882266942588922	193270
c9e51e39ea19fd82fd531a2aa3a964eca090b06e	shallow sparse autoencoders versus sparse coding algorithms for image compression	matching pursuit algorithms;quantisation signal data compression image coding;rate distortion;image coding;training;complexity;image compression;image reconstruction;dictionaries;sparse autoencoders;quantization noise image compression sparse coding algorithm shallow sparse autoencoder t sparse autoencoder t sparse ae winner take all autoencoder wta ae rate distortion trade off;shallow architectures;sparse matrices;complexity image compression sparse autoencoders shallow architectures;image coding dictionaries training matching pursuit algorithms sparse matrices rate distortion image reconstruction	This paper considers the problem of image compression with shallow sparse autoencoders. We use both a T-sparse autoencoder (T-sparse AE) and a winner-take-all autoencoder (WTA AE). A performance analysis in terms of rate-distortion trade-off and complexity is conducted, comparing with LARS-Lasso, Coordinate Descent (CoD) and Orthogonal Matching Pursuit (OMP). We show that, WTA AE achieves the best rate-distortion trade-off, it is robust to quantization noise and it is less complex than LARS-Lasso, CoD and OMP.	algorithm;autoencoder;coordinate descent;distortion;image compression;lasso;matching pursuit;neural coding;openmp;quantization (signal processing);sparse matrix;weapon target assignment problem	Thierry Dumas;Aline Roumy;Christine Guillemot	2016	2016 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2016.7574708	iterative reconstruction;complexity;sparse matrix;image compression;computer science;theoretical computer science;machine learning;pattern recognition;sparse approximation;algorithm;autoencoder	Robotics	49.09250036261193	-11.914865865945133	193636
82e9322d723720f00ce4c3ed1d7a27efbef1724c	dynamic rate allocation algorithm using adaptive lms end-to-end distortion estimation for video transmission over error prone network	lms algorithm;h 264 avc;bit rate allocation;end to end distortion		algorithm;distortion	Angelo R. dela Cruz;Ryan Rhay P. Vicerra;Argel A. Bandala;Elmer P. Dadios	2016	JACIII	10.20965/jaciii.2016.p0106	mathematical optimization;real-time computing;least mean squares filter;computer science	Networks	48.51717599478971	-15.87413833443757	193718
d6e7ce8d7624502684db3dd59e565bfa18e6f31c	a linear source model and a unified rate control algorithm for dct video coding	coding bit rate;theorie vitesse distorsion;quantization;evaluation performance;rate distortion;image content;picture complexity;image coding;linear source model;unified rate control algorithm;performance evaluation;data compression;video signal processing;videoconference;flow rate regulation;helium;quantized transform coefficients;evaluacion prestacion;telecommunication control;simulation;modele lineaire;compresion senal;video coding systems;simulacion;modelo lineal;transform coding;indexing terms;bit rate;compression signal;model parameter;algorithme;rate distortion theory;quantisation signal;algorithm;rate control;video coding;codage video;regulation debit;rate distortion analysis;discrete cosine transforms;mpeg 2;signal compression;linear model;dct video coding;discrete cosine transforms video coding image coding bit rate videoconference quantization communication system control entropy transform coding;mpeg 4;traitement signal video;shannon s source coding theorem;source code;entropy;rate distortion theory discrete cosine transforms video coding data compression source coding quantisation signal adaptive estimation transform coding telecommunication control;rate distortion analysis linear source model dct video coding transform coding systems unified rate control algorithm coding bit rate quantized transform coefficients shannon s source coding theorem model parameter image content picture complexity adaptive estimation video coding systems mpeg 2 h 263 mpeg 4 simulation results;communication system control;h 263;estimacion adaptativa;simulation results;adaptive estimation;regulacion caudal;estimation adaptative;source coding;algoritmo;transform coding systems	We show that, in any typical transform coding systems, there is always a linear relationship between the coding bit rate and the percentage of zeros among the quantized transform coefficients, denoted by . Based on Shannonâ€™s source coding theorem, a theoretical justification is provided for this linear source model. The physical meaning of the model parameter is also discussed. We show that it is directly related to the image content and is a measure of picture complexity. In video coding, we propose an adaptive estimation scheme to estimate this model parameter. Based on the linear source model and the adaptive estimation scheme, a unified rate control algorithm is proposed for various standard video coding systems, such as MPEG-2, H.263, and MPEG-4. Our extensive simulation results show that the proposed rate control outperforms other algorithms reported in the literature by providing much more accurate and robust rate control.	algorithm;coefficient;data compression;discrete cosine transform;ising model;line source;mpeg-2;open-source software;scheme;shannon (unit);shannon's source coding theorem;simulation;transform coding	Zhihai He;Sanjit K. Mitra	2002	IEEE Trans. Circuits Syst. Video Techn.	10.1109/TCSVT.2002.805511	linear network coding;speech recognition;shannonâ€“fano coding;telecommunications;harmonic vector excitation coding;variable-length code;computer science;theoretical computer science;context-adaptive variable-length coding;coding tree unit;mathematics;context-adaptive binary arithmetic coding;algorithm;statistics;source code	Metrics	48.39472810541516	-14.796730064222439	194005
cf304d3267a1fbb9573dfa30043a36c12d7c2adf	adaptive quantization without side information	image coding;adaptive decoding;data compression;building block;image coding adaptive lossless compression adaptive quantization algorithm arithmetic coder input statistics estimation model estimation quantizer design source statistics adaptive scalar quantization encoder decoder quantized information;quantization algorithm design and analysis arithmetic statistics decoding huffman coding entropy coding dynamic range statistical distributions scholarships;lossless compression;adaptive codes;quantisation signal;design technique;scalar quantization;arithmetic codes;adaptive signal processing;statistical analysis;data compression adaptive codes adaptive signal processing adaptive estimation quantisation signal arithmetic codes statistical analysis adaptive decoding image coding;side information;adaptive estimation	We propose to extend some of the ideas of adaptive lossless compression to design an adaptive quantization algorithm. Noting that the performance of an arithmetic coder is as good as its estimation of the statistics of the input, we split our quantizer into two building blocks: model estimation and quantizer design. The main idea is that , as long as the model estimation manages to track down the changes in source statistics, a standard quantizer design technique which assumes that the source follows the estimated model can be used. As an example of this type of design we study an adaptive scalar quantization scheme where we impose the restriction that no side information can be sent, i.e. encoder and decoder must perform their adaptation based on the quantized information.	algorithm;arithmetic coding;encoder;lossless compression;quantization (signal processing)	Antonio Ortega;Martin Vetterli	1994		10.1109/ICIP.1994.413725	data compression;adaptive filter;discrete mathematics;computer science;theoretical computer science;mathematics;lossless compression;statistics	EDA	49.464030186343166	-12.299463187936253	194567
5aaf79493cd0edab1b49d3ffc168d502cc51c244	applying informed coding and embedding to design a robust high-capacity watermark	filigranage;sensitivity and specificity;watermarking;watermarking noise robustness pixel additive noise low pass filters filtering image coding convolutional codes iterative methods noise shaping;image coding;image processing;perceptual shaping watermarking system informed coding informed embedding trellis code iterative method dirty paper code;data compression;low pass filters watermarking trellis codes iterative methods image coding data compression;filigrana;lossy compression;computer graphics;additive noise;procesamiento imagen;low pass filter;traitement image;patents as topic;algorithme;computer security;iterative methods;algorithm;image interpretation;codificacion;informed embedding;information embedding;dirty paper coding;signal processing;eurecom ecole d ingenieur telecommunication centre de recherche graduate school research center communication systems;robustesse;coding;informed coding;reproducibility of results;product labeling;pattern recognition;dirty paper code;high capacity;algorithms;robustness;low pass filters;algorithms computer graphics computer security data compression image interpretation computer assisted patents as topic pattern recognition automated product labeling reproducibility of results sensitivity and specificity signal processing computer assisted;trellis codes;computer assisted;perceptual shaping;automated;iteration method;side information;watermarking informed coding informed embedding perceptual shaping high capacity robustness dirty paper code;codage;robustez;algoritmo	We describe a new watermarking system based on the principles of informed coding and informed embedding. This system is capable of embedding 1380 bits of information in images with dimensions 240/spl times/368 pixels. Experiments on 2000 images indicate the watermarks are robust to significant valumetric distortions, including additive noise, low-pass filtering, changes in contrast, and lossy compression. Our system encodes watermark messages with a modified trellis code in which a given message may be represented by a variety of different signals, with the embedded signal selected according to the cover image. The signal is embedded by an iterative method that seeks to ensure the message will not be confused with other messages, even after addition of noise. Fidelity is improved by the incorporation of perceptual shaping into the embedding process. We show that each of these three components improves performance substantially.	additive white gaussian noise;algorithm;blocking (computing);checking (action);confusion;consistency model;convolutional code;digital watermarking;dimensions;discrete cosine transform;distortion;embedding;exhibits as topic;experiment;gabor filter;iterative method;joyce;lossy compression;low-pass filter;mathematics;noise shaping;performance;pixel;robustness (computer science);trellis quantization;utility functions on indivisible goods;watermark (data file);message	Matthew L. Miller;GwenaÃ«l J. DoÃ«rr;Ingemar J. Cox	2004	IEEE Transactions on Image Processing	10.1109/TIP.2003.821551	computer vision;speech recognition;low-pass filter;image processing;computer science;theoretical computer science;signal processing;iterative method;algorithm	Graphics	46.59231043223194	-12.960418586142826	194793
4978c55118c7380660baba3603e1c8b3b84a7926	multiple description video coding technique based on data hiding in the tree structured haar transform domain	transformation ondelette;protection information;data hiding;evaluation performance;transformation haar;multiple description;video techniques;4230;performance evaluation;image processing;0705p;structure arborescente;0130c;integer wavelet transform;imagerie;technique video;traitement image;wavelet transforms;video coding;imagery;signal transmission;redundancy;signal video;transmision senal;codage video;proteccion informacion;transmission signal;estructura arborescente;information protection;tree structure;video transmission;multiple description coding;imagineria;video signals;video;haar transforms;4230v;encoding;redondance;codage	ABSTRACT In this contribution a Multiple Description Coding scheme for video transmission over unreliable channel ispresented. The method is based on an integer wavelet transform and on a data hiding scheme for exploitingthe spatial redundancy and for reducing the scheme overhead. Experimental results show the eectiveness ofthe proposed scheme.Keywords: Multiple Description Coding, Tree Structured Haar domain, Data Hiding 1. INTRODUCTION One of the objectives of communication systems designers is the delivering of multimedia data in a fast, reliable,and low cost way. Unfortunately, network congestions, channel variability, attacks, software errors, hardwarefaults, and other factors, are increasing the probability that the transmitted stream will be received partiallyimpaired. Strongest channel coding techniques and retransmission based transport protocols, can improve theexisting performances. When we are dealing with real time communication, such as video conferences or livestreaming, the data becomes useless if it is received with delay. This is especially true when many independentusers, equipped with low power consumption terminals (as new generation of cellular phones equipped alsowith WIFI), send real time MJPEG2000 video to an access point (AP) in case, for example, of emergencysituations as Â”ooding or trac congestion. The peculiarities of this scenario, low power terminals, error pronetransmission channel, and shared communication system, are perfectly tuned to the application of an advancedvideo coding technique.One promising technique for improving streaming media quality is the multiple description coding (MDC).Firstly developed for speech communication over the telephone network, it is now applied for delivering in-formation over noisy and unreliable channels. The basic idea is to split and to encode the source messageinto two (or more) complementary descriptions, which are independently transmitted to the receiver by usingseparate channels or paths. The decoder can reconstruct the original message also from a single descriptorbut with lower quality, such as the baseline quality video. The more descriptors are received, the higher thereconstruction quality. To this aim, a certain amount of redundancy is added to each descriptor, increasing theoverall bit rate. Figure 1 shows the block diagram of MDC when two descriptions of the source are adopted.MDC works dierently than conventional (SDC) Single Description Coders (MPEG, H.261, H.264, ...)which produce a single data stream. As already stated, the cost of this resiliency is in the increased amountof data to be delivered and in increased computational complexity. The more redundancy in each descriptor,the higher the quality when a subset of delivered descriptors is received. The trade-o between complexity andeciency is strictly depending on the application scenario.Starting from the early works on speech communications, many researcher have approached the MDC frame-work. The theoretical aspects were Â“rstly formalized back in 1979	data compression;haar wavelet	Michela Cancellaro;Marco Carli;Alessandro Neri	2010		10.1117/12.840678	transmission;real-time computing;video;telecommunications;image processing;computer science;theoretical computer science;multiple description coding;tree structure;redundancy;information hiding;information protection policy;encoding;wavelet transform	Vision	48.15531269035174	-14.44194966690923	196653
64a10bf86d3e2532d2112ffb0504667481f96168	joint source-channel rate control for pixel-domain distributed video coding	adaptive quantization joint source channel rate control pixel domain distributed video coding noisy transmission environments source coding channel coding video stream decoding failure probability end to end distortion;channel coding;video streaming;decoding;joint source channel rate control;noisy transmission environments;joints;joint source channel;rate control;adaptive quantization;video coding;joint source channel coding;forward error correction;rate allocation;error correction;video streaming channel coding decoding source coding video coding;random variable;error resilience;distributed video coding;pixel domain distributed video coding;decoding failure probability;robustness;source code;decode forward;video stream;failure probability;end to end distortion;source coding;error resilience distributed video coding joint source channel coding;decoding forward error correction source coding video coding channel coding robustness joints	We study the scenario of pixel-domain distributed video coding for noisy transmission environments and propose a method to allocate the available rate between source coding and channel coding to generate a robust video stream. Having observed in experiments the uncertainty of the source and the channel coding rate, we model them as random variables via offline training, estimate the decoding failure probability and calculate the mean end-to-end distortion. Adaptive quantization is performed for each slice to minimize its mean end-to-end distortion. With this joint source-channel rate allocation, we compare the robustness of two coding prototypes, namely distributed video coding and distributed video coding with forward error correction. According to our experimental results, under same total bit budget, the distributed video coding only scheme proves more robust than the latter one and the gain is up to 1 dB in PSNR.	data compression;decibel;distortion;end-to-end principle;error detection and correction;experiment;forward error correction;online and offline;peak signal-to-noise ratio;pixel;quantization (signal processing);streaming media	Hu Chen;Eckehard G. Steinbach;Chang Wen Chen	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946786	scalable video coding;linear network coding;real-time computing;telecommunications;harmonic vector excitation coding;computer science;theoretical computer science;context-adaptive variable-length coding;coding gain;coding tree unit;mathematics;context-adaptive binary arithmetic coding;statistics;source code	Robotics	48.51612208663925	-16.22468663716185	196778
8c76db4ba2965ee05ccd196a1c9488cdf06fa7e1	adaptive multiple description video coding and transmission for scene change	signal image and speech processing;information systems applications incl internet;temporal sampling;video coding;multiple description coding;communications engineering networks	In view of perfect compatibility with the standard source and channel codec, temporal sampling-based multiple description coding (MDC) has become a better choice for practical applications. However, for the frames change from one scene to another temporal correlation may be destroyed by temporal sampling extremely, which results in the false estimation when the related frames are lost at the side decoder. Therefore, in this article the frames containing scene change are detected and duplicated before temporal sampling, which maintains better temporal correlation in each description. Furthermore, for better rate distortion performance temporal sampling is employed adaptively, that is, frame skipping or up-sampling according to the motion characteristics in original video. The experimental results exhibit better performance of the proposed scheme than other schemes whether in the onâ€“off MDC environment or packet lossy network, especially about 15 dB improvements for the frames with scene change. Therefore, it may be a promising choice for video transmission over error-prone channels, especially over wireless networks.		Mengmeng Zhang;Huihui Bai	2012	EURASIP J. Wireless Comm. and Networking	10.1186/1687-1499-2012-265	computer vision;telecommunications;computer science;multiple description coding;multimedia;computer network	Vision	48.816038577585324	-16.186792937552074	198941
6ab6b9ed54517160f5f6c7e317a9d5dfcab86827	scalar-vector quantization of medical images	canal con ruido;medical images compression;radiology;quantization;image storage;image reconstruction medical images scalar vector quantization image coding fixed rate encoder rate distortion performance optimal entropy constrained scalar quantizers memoryless sources fixed rate quantizer image transmission noisy channels magnetic resonance images medical images compression picture archiving and communication systems pacs all digital radiology environment hospitals image storage;rate distortion;medical imagery;image numerique;transformation cosinus;entropia;pacs;image coding;quantization biomedical imaging image coding picture archiving and communication systems image storage rate distortion magnetic noise magnetic resonance bit rate monitoring;image processing;medical images;data compression;biomedical nmr vector quantisation medical image processing rate distortion theory;magnetic resonance images;fixed rate quantizer;radiologia;biomedical nmr;variable length code;noisy channel;hospitals;procesamiento imagen;radiologie;fixed rate encoder;biomedical imaging;source sans memoire;canal avec bruit;memoryless source;bit rate;picture archiving and communication system;traitement image;rate distortion theory;codificacion;mr imaging;cuantificacion vectorial;scalar quantization;vector quantization;medical image;monitoring;image transmission;error propagation;memoryless sources;magnetic resonance;image reconstruction;medical image processing;entropie;transformacion coseno;imagen numerica;coding;rate distortion performance;imagerie medicale;vector quantizer;entropy;magnetic noise;imageneria medical;compresion dato;digital image;optimal entropy constrained scalar quantizers;cosine transform;fuente sin memoria;vector quantisation;scalar vector quantization;noisy channels;compression donnee;codage;picture archiving and communication systems;all digital radiology environment;quantification vectorielle	A new coding scheme based on the scalar-vector quantizer (SVQ) is developed for compression of medical images. The SVQ is a fixed rate encoder and its rate-distortion performance is close to that of optimal entropy-constrained scalar quantizers (ECSQs) for memoryless sources. The use of a fixed-rate quantizer is expected to eliminate some of the complexity of using variable-length scalar quantizers. When transmission of images over noisy channels is considered, our coding scheme does not suffer from error propagation that is typical of coding schemes using variable-length codes. For a set of magnetic resonance (MR) images, coding results obtained from SVQ and ECSQ at low bit rates are indistinguishable. Furthermore, our encoded images are perceptually indistinguishable from the original when displayed on a monitor. This makes our SVQ-based coder an attractive compression scheme for picture archiving and communication systems (PACS). PACS are currently under study for use in an all-digital radiology environment in hospitals, where reliable transmission, storage, and high fidelity reconstruction of images are desired.		Nader Mohsenian;Homayoun Shahri;Nasser M. Nasrabadi	1996	IEEE transactions on image processing : a publication of the IEEE Signal Processing Society	10.1109/83.480776	computer vision;entropy;image processing;computer science;theoretical computer science;mathematics;picture archiving and communication system	Vision	47.40434614144149	-13.13572729227199	199626
aeccd028b062947a658fc620fcf9242011a87813	lossless coding of still images using minimum-rate predictors	variable length codes;coding efficiency;prediction error;image coding;medical simulation;data compression;least mean squares methods;cost function;image coding cost function design methodology mean square error methods predictive models optimization methods medical simulation digital images biomedical imaging remote sensing;biomedical imaging;mmse predictors;still image coding;prediction theory;remote sensing;prediction errors;minimum rate predictors;mean square error methods;lossless image coding;variable length codes still image coding minimum rate predictors linear predictor design lossless image coding minimum mean square error mmse predictors optimized predictor mse criterion coding efficiency cost function prediction errors simulation results linear quantizer;linear predictor design;predictive models;minimum mean square error;least mean squares methods image coding data compression prediction theory;linear quantizer;simulation results;digital images;mse criterion;optimized predictor;optimization methods;design methodology	In this paper we propose a novel method for designing linear predictors and apply it to a lossless image coding scheme. In general, the Minimum Mean Square Error (MMSE) is an important concept in image coding. However, predictors which are optimized on the basis of the MSE criterion are not necessarily optimum from a viewpoint of coding efficiency. Thereupon our method optimizes a predictor so that a cost function which represents the amount of information on prediction errors can have a minimum. Simulation results indicate that the proposed coding scheme is superior to the coding scheme which utilizes the conventional MMSE predictors in terms of coding efficiency.	algorithmic efficiency;kerrison predictor;loss function;lossless compression;simulation	Ichiro Matsuda;Hirofumi Mori;Susumu Itoh	2000		10.1109/ICIP.2000.900912	data compression;medical simulation;minimum mean square error;mathematical optimization;shannonâ€“fano coding;design methods;computer science;entropy encoding;context-adaptive variable-length coding;machine learning;mean squared prediction error;coding tree unit;mathematics;predictive modelling;algorithmic efficiency;context-adaptive binary arithmetic coding;digital image;statistics	AI	47.86648670378599	-13.094210446901888	199774
