id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
5994eb9bed19aac42583bcd154b811c40ab85263	a non-contact device for tracking gaze in a human computer interface	gaze;analisis imagen;pupil;cabeza;interfase usuario;pistage;image processing;real time tracking;user interface;eye detection;pupille;real time;localization;rastreo;procesamiento imagen;hci;gaze tracking;detection oeil sans contact;localizacion;mirada;traitement image;estimation regard;regard;camera motion;localisation;interface utilisateur;image analysis;head;eye tracking;tete;video oculographie;analyse image;pupila;gaze estimation;tracking;human computer interface;pistage oeil	This paper presents a novel design for a non-contact eye detection and gaze tracking device. It uses two cameras to maintain real-time tracking of a person s eye in the presence of head motion. Image analysis techniques are used to obtain accurate locations of the pupil and corneal reflections. All the computations are performed in software and the device only requires simple, compact optics and electronics attached to the user s computer. Three methods of estimating the user s point of gaze on a computer monitor are evaluated. The camera motion system is capable of tracking the user s eye in real-time (9 fps) in the presence of natural head movements as fast as 100 /s horizontally and 77 /s vertically. Experiments using synthetic images have shown its ability to track the location of the eye in an image to within 0.758 pixels horizontally and 0.492 pixels vertically. The system has also been tested with users with different eye colors and shapes, different ambient lighting conditions and the use of eyeglasses. A gaze accuracy of 2.9 was observed. 2004 Elsevier Inc. All rights reserved.	amiga reflections;color;computation;computer monitor;human computer;human–computer interaction;image analysis;motion system;pixel;real-time clock;real-time computing;shading;synthetic intelligence;tracking system	Borna Noureddin;Peter D. Lawrence;C. F. Man	2005	Computer Vision and Image Understanding	10.1016/j.cviu.2004.07.005	computer vision;image analysis;simulation;internationalization and localization;eye tracking;image processing;computer science;eye tracking on the iss;tracking;user interface;head;computer graphics (images)	Vision	48.55894504800134	-56.1466273903837	158116
fa6a06505a63894fa522de8a217beccc6668cf74	fast extraction of ellipses	optimisation;optimum approximation;image resolution;information science;geometry;picture processing;feature space;shape;machine vision;projection;optimum approximation ellipses fast extraction heuristic function pyramid structure feature space projection;solid modeling;ellipses;pattern recognition;image analysis;picture processing optimisation pattern recognition;parameter estimation;fast extraction;switches;heuristic function;processing speed;pyramid structure;image resolution switches machine vision geometry shape solid modeling image analysis parameter estimation information science	A fast extraction of ellipses from an image is introduced. The process is divided into two steps: first, it looks for the areas where there might be ellipses by using heuristic function defined and pyramid structure; then finds the parameters of the ellipses in each area searched by a combination method that consists of transforming feature space, projecting and optimum approximation approaches. The experiments are satisfied: the ellipses are extracted from an outdoor Image. The results show that the heuristic function is eminent for finding areas where there might be ellipses, and the fitting method is ingenious so that both fast processing speed and suitable parameters of the ellipse are obtained.	approximation;experiment;feature vector;heuristic (computer science)	Runsheng Wang;Allen R. Hanson;Edward M. Riseman	1988		10.1109/ICPR.1988.28279	computer vision;image analysis;image resolution;feature vector;machine vision;projection;information science;network switch;shape;computer science;machine learning;pattern recognition;mathematics;geometry;solid modeling;estimation theory;heuristic function;ellipse	Robotics	48.85640634008498	-52.72203672640978	158153
8c90dd6aa74f57de912dc01d43edb9b1d4be6d36	line segment based man-made object recognition using invariance	plane homography man made object recognition line segment features hypothesise and test;geometric invariants;object recognition;man made object recognition;image segmentation;object recognition testing layout image edge detection transmission line matrix methods shape robotic assembly artificial intelligence computer science nonlinear equations;local plane homography calculation;computational geometry;testing;layout;matrix algebra;coordinate transformation matrix;data mining;three dimensional;line segment features;hypothesise and test;computational modeling;shape;image edge detection;three dimensional displays;coordinate transformation;transforms computational geometry matrix algebra nonlinear equations object recognition;transforms;line segment based man made object recognition;coplanar line segment features;mathematical model;artificial intelligence;robotic assembly;nonlinear equations;transmission line matrix methods;computer science;linear equations;coplanar line segment features line segment based man made object recognition coordinate transformation matrix nonlinear equations local plane homography calculation geometric invariants;plane homography	The traditional three-dimensional object recognition method based on hypothesise and test need to solve the coordinate transformation matrix from scene to model through a group of non-linear equations. Therefore, it has a very high complexity. This paper presents a man-made object recognition method based on the geometry feature of line segments characteristics, and disperses the overall coordinate transformation calculation in every local plane homography calculation, reduces the complexity of the solution. First we prematch the feature points using geometric invariants, then assume and solve the plane homography matrix between scenes to model. After that we match the line segments on the homography plane, and by this we verify the assumption. Experiments proved that this method can rapidly and accurately identify man-made objects which contain coplanar line segment features.	3d projection;experiment;fast fourier transform;homography (computer vision);linear equation;nonlinear system;outline of object recognition;transformation matrix	ZhenYu Qiu;Hui Wei	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.149	homography;layout;three-dimensional space;transformation matrix;computer vision;computational geometry;shape;computer science;coordinate system;cognitive neuroscience of visual object recognition;machine learning;mathematical model;mathematics;geometry;software testing;linear equation;image segmentation;computational model	Robotics	50.22632074803846	-52.51934705224456	159316
089afafccf01b319bbd4d558df6f4b6637889ab0	registering two overlapping range images	image scanners;optical scanners;image matching;interest points;computational geometry;computational geometry image registration optical scanners image scanners image matching;range image;image registration;image matching overlapping range image registration intensity data triangulation 3d range points 2d interest points 3d triangulations 3d data pruning compatibility tests matching triangles range image pairs;iterative closest point algorithm image registration information technology councils testing solid modeling error correction iterative algorithms image converters robustness;point of interest	This paper describes a method of automatically performing the registration of two range images that have signi cant overlap. We rst nd points of interest in the intensity data that comes with each range image. Then we perform a triangulation of the 3D range points associated with these 2D interest points. All possible pairs of triangles between the two 3D triangulations are then matched. The fact that we have 3D data available makes it possible to e ciently prune matches. We do this pruning by using a simple and e ective set of compatibility tests between potentially matching triangles and vertices. The best match is the one that aligns the largest number of interest points between the two range images. The algorithms are demonstrated experimentally on a number of di erent	3d scanner;algorithm;brute-force search;computation;delaunay triangulation;experiment;interest point detection;point of interest;prune and search;range imaging;requirement;triangulation (geometry)	Gerhard Roth	1999		10.1109/IM.1999.805349	computer vision;mathematical optimization;feature detection;range segmentation;point of interest;template matching;image processing;computational geometry;computer science;image registration;mathematics;geometry	Vision	49.89913365121315	-53.09617967238429	161059
309ef98cb3553d82f27886664dcb89047f6396f2	a non-iterative procedure for rapid and precise camera calibration	estimacion;calibrage;erreur;position;image processing;procedimiento;direccion;calibracion;procesamiento imagen;posicion;movie camera;traitement image;criterio;camara;criterion;estimation;direction;critere;pattern recognition;reconnaissance forme;error;camera calibration;reconocimiento patron;calipering;camera;procedure	A simple, rapid and precise camera calibration method is described by which the position and direction of a camera are calculated without using conventional iterative processes. Error parameters are defined as differences in the values of calculated intrinsic parameters from their fixed ones. Camera parameters are adjusted non-iteratively so that the error parameters become sufficiently small. The rotation matrix orthogonality constraints are completely preserved. Experimental error estimation using an actual mark image shows that the position error is less than 0.1 mm and that the direction error is less than 5 × 10−4 rad, which correspond to relative errors of 5 × 10−5 and 1 × 10−4, respectively. These calibration procedures from the image acquisition to the result output are completed automatically within 6s using a VAX11/780 (1 MIPS)	camera resectioning;iterative method	Minoru Ito;Akira Ishii	1994	Pattern Recognition	10.1016/0031-3203(94)90061-2	procedure;computer vision;estimation;camera resectioning;simulation;image processing;computer science;position	Vision	50.11474975485129	-57.47108951840242	162651
b4835d963c44bd9c29ba52303b78875232b607f8	structural matching in computer vision using probabilistic relaxation	computer vision stereo vision object recognition feature extraction roads simulated annealing image recognition image segmentation motion estimation image fusion;graph theory;probabilistic relaxation;image recognition;object recognition;vision ordenador;probability;image segmentation;road network;vision estereoscopica;edge detection;image matching;heuristic programming;heuristic method;extraction forme;image fusion;2d images;vision stereoscopique;model transformation;metodo heuristico;motion estimation;feature matching;simulated annealing;edge matching;geometric feature;stereo pair;relajacion;computer vision;deteccion contorno;binary relations;probabilistic model;detection contour;extraccion forma;straight line segment;structural matching;roads;feature extraction;matching;heuristic formulae;stereo vision;attributed relational graph;modele probabiliste;pattern recognition;binary relation;relaxation;graph theory computer vision image matching feature extraction relaxation theory probability heuristic programming;vision ordinateur;methode heuristique;reconnaissance forme;reconocimiento patron;stereopsis;relaxation theory;real time application;pattern extraction;parallel processing;binary relations structural matching computer vision probabilistic relaxation feature matching 2d images heuristic formulae aerial road network images edge matching stereo pair attributed relational graph straight line segment;measurement noise;probabilistic reasoning;modelo probabilista;aerial road network images	In this paper, we develop the theory of probabilistic relaxation for matching features extracted from 2D images, derive as limiting cases the various heuristic formulae used by researchers in matching problems, and state the conditions under which they apply. We successfully apply our theory to the problem of matching and recognizing aerial road network images based on road network models and to the problem of edge matching in a stereo pair. For this purpose, each line network is represented by an attributed relational graph where each node is a straight line segment characterized by certain attributes and related with every other node via a set of binary relations.	aerial photography;computer vision;heuristic;linear programming relaxation;silk road	William J. Christmas;Josef Kittler;Maria Petrou	1995	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.400565	parallel processing;computer vision;computer science;stereopsis;graph theory;3-dimensional matching;machine learning;pattern recognition;binary relation;mathematics	Vision	47.15822570121231	-55.27117480843878	162697
bc2f68dbdd23f3bc7187169e5e3b66793f5a89a3	weakly supervised scale-invariant learning of models for visual recognition	modelizacion;alignement;scale model;object recognition;structural model;vision ordenador;clutter;learning algorithm;image processing;modele reduit;occlusion;automovil;supervised learning;scaling law;localization;occultation;procesamiento imagen;oclusion;reconnaissance objet;localizacion;algorithme apprentissage;probabilistic approach;traitement image;semi supervised learning;animal;computer vision;modelisation;modelo reducido;fouillis echo;localisation;automobile;ley escala;constellation model;enfoque probabilista;approche probabiliste;confusion eco;motor car;learning object;alineamiento;pattern recognition;invariante;vision ordinateur;reconnaissance forme;apprentissage supervise;parts and structure model;loi echelle;reconocimiento patron;ocultacion;aprendizaje supervisado;algoritmo aprendizaje;modeling;alignment;invariant;scale invariance;probability of detection	We investigate a method for learning object categories in a weakly supervised manner. Given a set of images known to contain the target category from a similar viewpoint, learning is translation and scale-invariant; does not require alignment or correspondence between the training images, and is robust to clutter and occlusion. Category models are probabilistic constellations of parts, and their parameters are estimated by maximizing the likelihood of the training data. The appearance of the parts, as well as their mutual position, relative scale and probability of detection are explicitly described in the model. Recognition takes place in two stages. First, a feature-finder identifies promising locations for the model”s parts. Second, the category model is used to compare the likelihood that the observed features are generated by the category model, or are generated by background clutter. The flexible nature of the model is demonstrated by results over six diverse object categories including geometrically constrained categories (e.g. faces, cars) and flexible objects (such as animals).	clutter;face (geometry);supervised learning	Rob Fergus;Pietro Perona;Andrew Zisserman	2006	International Journal of Computer Vision	10.1007/s11263-006-8707-x	computer vision;systems modeling;internationalization and localization;occultation;image processing;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;invariant;scale invariance;statistical power;clutter;supervised learning;scale model	Vision	47.118314668271985	-58.49274165491433	162984
f3bc557869f984eb12512745cd6e8d89de0d09ff	generic-model based human-body modeling	modelizacion;scale model;interfase usuario;estimation mouvement;piel;modele reduit;generic model;peau;user interface;skin;estimacion movimiento;divertissement;hombre;motion estimation;motion capture data;cuerpo humano;anatomia;corps humain;anatomically based modeling;human body model;modelisation;modelo reducido;vie artificielle;human body;systeme osteoarticulaire;sistema osteoarticular;human;osteoarticular system;interface utilisateur;anatomie;modeling;anatomy;entertainment;artificial life;homme;human body modeling	This paper presents a generic-model based human-body modeling method which take the anatomical structure of the human body into account. The generic model contains anatomical structure of bones and muscles of the human body. For a given target skin mesh, the generic model can be scaled according to the skin, and then morphed to be fitted to the shape of the target skin mesh. After an anchoring process, the layered model can be animated via key-framing or motion capture data. The advantage of this approach is its convenience and efficiency comparing to existing anatomically-based modeling methods. Experimental results demonstrate the success of the proposed human-bodymodeling method.	algorithm;approximation;cyberware;framing (world wide web);image scaling;matlab;microsoft research;morphing;motion capture;semiconductor industry	Xiaomao Wu;Lizhuang Ma;Ke-Sen Huang;Yan Gao;Zhihua Chen	2005		10.1007/11558651_20	entertainment;human body;simulation;systems modeling;computer science;artificial intelligence;motion estimation;skin;user interface;scale model;artificial life	Vision	49.655026644060285	-55.55222613760515	163131
6b0ec36d4d8a09e64a016fd67e65416f7f5f8ee8	occluding contour detection using affine invariants and purposive viewpoint control	image motion analysis machine vision computational geometry;computational geometry motion estimation computer vision;computational geometry;motion estimation;computer vision;visible rim occluding contour detection affine invariants viewpoint control	We present an approach for identifying the occluding contour and determining its sidedness using an active (i.e., moving) observer. It is based on the non-stationarity property of the visible rim: When the observer’s viewpoint is changed, the visible rim is a collection of curves that “slide,” rigidly or non-rigidly, over the surface. We show that the observer can deterministically choose three views on the tangent plane of selected surface points to distinguish such curves from stationary surface curves (i.e., surface markings). Our approach demonstrates that the occluding contour can be identified directly, i.e., without first computing surface shape (distance and curvature).	deterministic algorithm;invariant (computer science);stationary process	Kiriakos N. Kutulakos;Charles R. Dyer	1994		10.1109/CVPR.1994.323847	computer vision;topology;computational geometry;computer science;motion estimation;mathematics;geometry	Vision	51.36375436660444	-54.389496813027634	163225
87e1442571ed8e13c6e940372876f66c052a73fd	phase-sensitive periodical correlation of local beam descriptors for image registration	local descriptor;image registration;期刊论文;point set matching;outlier rejection	A new descriptor referred to as local beam descriptor is proposed to figure out the directionally spatial arrangement of the key points in a local region. As a byproduct, a phase value to maximize the periodical correlation of the short time series represented descriptors can be obtained. Consequently, most erroneous matches disagreeing with each other in terms of phase value can be filtered out to reduce the computational load of subsequent matching considerably. Then, the fine matching can be focused on robustness based on the similar triangles formed by every triple of matched point pairs. As confirmed experimentally, the proposed solution augments scale-invariant feature transform (SIFT) and random sample consensus (RANSAC) in terms of both reduced time cost and the ability to match image pairs missed by such classical solution. An ensemble solution incorporating the proposed solution with SIFT based solution outperforms using either individual solution alone since they represent different views of image contents complementary to each other.	image registration	Su Yang;Jiulong Zhang;Weishan Zhang	2016	Neurocomputing	10.1016/j.neucom.2015.09.042	computer vision;computer science;image registration;machine learning;pattern recognition;mathematics	Vision	48.763349985225254	-54.009248400343864	163254
081b75eb898f6a772ab0a3073bc1dee5e0f3ac92	compact - a 3d shape representation scheme for polyhedral scenes		COMPACT is a high level surface representation particularly suitable for description of polyhedral objects and scenes and for recognition by matching to geometrical models. It builds on a line segment representation as developed in INRIA and converts a set of line segments into a set of surfaces (planes etc.). These are further converted into object / scene faces whose boundaries, internal features and mutual relationships are made explicit. The aim is to make matching to models of objects and scenes both faster and more constrained.	high-level programming language;polyhedral	Pavel Grossmann	1987		10.5244/C.1.33	discrete mathematics;topology;geometry	Vision	50.77850869955953	-53.78128552675636	163306
0aa88f552b8f40f47d25a87c06570d85aa761aba	modelling nonrigid object from video sequence under perspective projection	optimisation sous contrainte;modelizacion;constrained optimization;image tridimensionnelle;interfase usuario;pistage;mise a jour;user interface;perspective projection;technology;computer science artificial intelligence;rastreo;man machine system;actualizacion;optimizacion con restriccion;modelisation;reconstruction image;science technology;shape;reconstruccion imagen;factorization method;image reconstruction;image sequence;cognition;motion recovery;projection perspective;cognicion;tridimensional image;sistema hombre maquina;interface utilisateur;secuencia imagen;computer science;proyeccion perspectiva;modeling;computer science theory methods;3d structure;updating;tracking;sequence image;imagen tridimensional;systeme homme machine	The paper is focused on the problem of estimating 3D structure and motion of nonrigid object from a monocular video sequence. Many previous methods on this problem utilize the extension technique of factorization based on rank constraint to the tracking matrix, where the 3D shape of nonrigid object is expressed as weighted combination of a set of shape bases. All these solutions are based on the assumption of affine camera model. This assumption will become invalid and cause large reconstruction errors when the object is close to the camera. The main contribution of this paper is that we extend these methods to the general perspective camera model. The proposed algorithm iteratively updates the shape and motion from weak perspective projection to fully perspective projection by refining the scalars corresponding to the projective depths. Extensive experiments on real sequences validate the effectiveness and improvements of the proposed method.	3d projection;algorithm;experiment;feature recognition;recursion (computer science)	Guanghui Wang;Yantao Tian;Guoqiang Sun	2005		10.1007/11573548_9	computer vision;constrained optimization;cognition;shape;computer science;artificial intelligence;mathematics;geometry;tracking;user interface;technology	Vision	50.376681296820486	-56.245559757300015	163582
af8e6ea3940d38a77dd61e02ca691181f5233342	visual measurement estimation for autonomous vehicle navigation	grain size;metodo monte carlo;restauration image;modele mathematique;image processing;autonomous vehicle;forme onde;sensors;mobile robot;ultrasound;real time;simulacion numerica;erreur quadratique moyenne;procesamiento imagen;methode monte carlo;image restoration;mobile robots;modelo matematico;traitement image;desconvolucion;experimental result;information gathering;restauracion imagen;forma onda;obstacle avoidance;imagen borrosa;grosor grano;mean square error;motion vector;blurred image;monte carlo method;simulation numerique;image sequence;resultado experimental;mathematical model;deconvolution;autonomous navigation;ultrasonography;waveform;image floue;error medio cuadratico;mobile agent;resultat experimental;cameras;numerical simulation;pose estimation;grosseur grain	The autonomous navigation of a mobile vehicle can be described as the task it undertakes to move itself in the environment through a series of positions based on information gathered by its sensors. In order to accomplish this task, the vehicle has to cope with two main subtasks namely obstacle avoidance and self localization. The later implies in the ability to determine its position and orientation with respect to the environment. This work describes a simple but eecient method that performs pose estimation for a mobile vehicle based on visual information from artiicial landmarks using a sequence of frames from an uncalibrated camera. The landmark is segmented from image sequences and the vehicle's localization is computed using landmark geometric properties and vehicle's motion vector. This methodology can be easily extended to be used by diierent types of mobile agents. One of the key advantages is that it is computationally eecient making it suitable for real time navigation. Experiments conducted with a Nomad 200 mobile robot equipped with a color camera system have shown the method to be repeatable and very robust to noise. Visual measurements were compared with readings from other onboard sensors such as ultrasound with excellent consistency.	autonomous robot;mobile agent;mobile robot;nomad 200;obstacle avoidance;sensor	Mario F. M. Campos;Luiz Chaimowicz	1999		10.1117/12.354713	computer vision;simulation;geography;cartography	Robotics	49.36210735358756	-57.07614069178867	163880
9b0a8e32c787c369764dc6fa9f9d51ae73a67744	a fast object orientation estimation and recognition technique for underwater acoustic imaging	object oriented;orientation estimation;underwater acoustics	This paper describes a technique for automatic fast orientation estimation and recognition of man-made objects present in underwater acoustic images. A method able (a) to estimate the orientation of the object principal axis, and (b) to recognize the object itself by using information about boundary segments and their angular relations, is presented. This method is based on a simple voting approach applied directly to the edge discontinuities of an image. Often, in underwater acoustic images some undesired effects are present, such as halos around objects, speckle that creates discontinuities, and distortion. Voting approach is very robust with respect to these effects, so allowing good results also when images have a very poor quality. A sequence of real acoustic images is shown for testing the proposed technique validity.	acoustic cryptanalysis	Alessandra Tesei;Andrea Trucco;Daniele Zambonini	1995		10.1007/3-540-60298-4_337	computer vision;underwater acoustics;speech recognition;computer science;object-oriented programming	Robotics	52.8209112096936	-58.1825452451186	163916
78f5951f70f9be649d741506cf63c2e75cca37ad	agglomerative grouping of observations by bounding entropy variation	modelizacion;mise a jour;filtro kalman;metodo entropia maxima;grouping;filtre kalman;principio minimo;kalman filter;similitude;actualizacion;modelisation;proximite;proximidad;minimum principle;proximity;similarity;inferencia;pattern recognition;agrupamiento;reconnaissance forme;similitud;theorie information;reconocimiento patron;methode entropie maximum;modeling;information theoretic;method of maximum entropy;maximum entropy;inference;information theory;updating;groupage;principe minimum;teoria informacion	An information theoretic framework for grouping observations is proposed. The entropy change incurred by new observations is analyzed using the Kalman filter update equations. It is found, that the entropy variation is caused by a positive similarity term and a negative proximity term. Bounding the similarity term in the spirit of the minimum description length principle and the proximity term in the spirit of maximum entropy inference a robust and efficient grouping procedure is devised. Some of its properties are demonstrated for the exemplary task of edgel grouping.	computer stereo vision;edge detection;greedy algorithm;information theory;kalman filter;minimum description length;principle of maximum entropy	Christian Beder	2005		10.1007/11550518_13	kalman filter;systems modeling;similarity;information theory;maximum entropy probability distribution;principle of maximum entropy;similitude;calculus;mathematics;distance;algorithm;statistics	Vision	49.90026116564328	-59.03762358014024	164305
cadc46111b440a7341030a3257178fc0b33fe6c3	evaluating human motion complexity based on un-correlation and non-smoothness	motion analysis;non smoothness;motion capture data;motion complexity;motion capture;human motion;human body;human motion analysis;uncorrelation	Determining the complexity of a human motion is useful for human motion analysis with potential applications such as biomechanics, sport training, entertainment. There has not been much research effort in finding a complexity measure to evaluate the whole human body motion automatically. In this paper, we present a novel approach to evaluate the complexity of human motion based on motion captured data. Our proposed complexity measure considers the un-correlation among active joint dimensions and the nonsmoothness of each joint dimension in the temporal direction. It is logical to expect that a motion is more complex if the joint dimensions are less correlated and the temporal movement is less smooth. The experimental results show that our proposed complexity measure is able to cluster the same type of motions and differentiate motions with different observed complexities.		Yang Yang;Howard Leung;Lihua Yue;Liqun Deng	2010		10.1007/978-3-642-15696-0_50	computer vision;structure from motion;motion capture;human body;simulation;computer science;motion estimation;motion field	Robotics	47.90386091196098	-54.701746891185984	164426
ecf8351236d9e48dfe63d164bcf0c7f1e959ef06	feature-based detection and correction of occlusions and split of video objects	video object;sudden variation;entidad solida;vision ordenador;pistage;video surveillance;video objects;occlusion;complexite calcul;validacion;surveillance;real time;occultation;split;rastreo;oclusion;variacion brusca;segmentation;computer vision;solid feature;separation;separacion;complejidad computacion;vigilancia;entite geometrique;feature based;senal video;signal video;monitoring;computational complexity;temps reel;object tracking;poursuite cible;video signal;tiempo real;validation;vision ordinateur;variation brusque;monitorage;video surveillance tracking;target tracking;ocultacion;monitoreo;split feature based;segmentacion;tracking;temporal change	This paper proposes a novel algorithm for the real-time detection and correction of occlusion and split in object tracking for surveillance applications. The paper assumes a feature-based model for tracking and is based on the identification of sudden variations of spatio-temporal features of objects to detect occlusions and splits. The detection is followed by a validation stage that uses past tracking information to prevent false detection of occlusion or split. Special care is taken in case of heavy occlusion, when there is a large superposition of objects. For the detection of splits, in addition to the analysis of spatio-temporal changes in objects’ features, our algorithm analyzes the temporal behavior of split objects to discriminate between errors in segmentation and real separation of objects, such as in a deposit event. Both objective and subjective experimental results show the ability of the proposed algorithm to detect and correct, both, split and occlusion of objects. The proposed algorithm is suitable in video surveillance applications due to its good performance in multiple, heavy, and total occlusions, its ability to differentiate between real object separation and faulty object split, its handling of simultaneous occlusion and split events, and its low computational complexity. The algorithm This work was partially supported by the National Science and Engineering Research Council of Canada (NSERC). C. Vázquez (B) Advanced Video Systems, Communications Research Centre, Ottawa, ON, Canada e-mail: carlos.vazquez@crc.ca M. Ghazal · A. Amer Electrical and Computer Engineering, Concordia University, Montréal, QC, Canada e-mail: moha_mo@encs.concordia.ca A. Amer e-mail: amer@encs.concordia.ca was integrated into an on-line video surveillance system and tested under several conditions with promising results.	algorithm;closed-circuit television;computational complexity theory;computer engineering;email;hidden surface determination;online and offline;real-time clock	Carlos Vázquez;Mohammed Ghazal;Aishy Amer	2009	Signal, Image and Video Processing	10.1007/s11760-008-0055-6	computer vision;simulation;occultation;computer science;video tracking;tracking;computational complexity theory;split;segmentation;computer graphics (images)	Vision	48.04735955461388	-56.6995572057103	165419
531abf1bec9f844cbd3f0b86ec6044f92027797a	a sensor fusion framework using multiple particle filters for video-based navigation	auxiliary variable;navegacion;filtre particule;reseau capteur;vision ordenador;multiple condensation trackers;multisensor;analisis escena;pistage;analyse scene;sensor fusion framework;video based navigation geographical information system gis global positioning system gps multiple condensation trackers sensor fusion;systeme information geographique;geographic information system;sensor fusion computer vision computerised navigation geographic information systems global positioning system particle filtering numerical methods;video based navigation;road traffic;echantillonnage;systeme gps;rastreo;global position system;road signalling;fusion capteur;prior knowledge;variable auxiliar;data fusion;filtro particulas;senalizacion trafico;layout;gps system;signalisation routiere;fusion de sensores;sensor fusion particle filters navigation roads global positioning system layout computer vision particle tracking sampling methods machine vision;global positioning system gps;computer vision;multiple view;sampling;geographical information systems;navigation;red sensores;trafic routier;senal video;signal video;partition sampling;roads;global positioning system;geographical information systems sensor fusion framework multiple particle filters video based navigation computer vision module partition sampling global positioning systems;particle filter;geographic information systems;machine vision;fusion donnee;sensor array;computer vision module;geographical information system gis;video signal;vue multiple;trafico carretera;vision ordinateur;navigation system;particle tracking;sensor fusion;particle filters;sampling methods;muestreo;fusion datos;global positioning systems;radiolocalizacion;multiple particle filters;capteur multiple;radiolocalisation;sistema informacion geografica;tracking;vista multiple;particle filtering numerical methods;variable auxiliaire;sistema gps;scene analysis;computerised navigation	This paper presents a sensor-fusion framework for video-based navigation. Video-based navigation offers the advantages over existing approaches. With this type of navigation, road signs are directly superimposed onto the video of the road scene, as opposed to those superimposed onto a 2-D map, as is the case with conventional navigation systems. Drivers can then follow the virtual signs in the video to travel to the destination. The challenges of video-based navigation require the use of multiple sensors. The sensor-fusion framework that we propose has two major components: (1) a computer vision module for accurately detecting and tracking the road by using partition sampling and auxiliary variables and (2) a sensor-fusion module using multiple particle filters to integrate vision, Global Positioning Systems (GPSs), and Geographical Information Systems (GISs). GPS and GIS provide prior knowledge about the road for the vision module, and the vision module, in turn, corrects GPS errors.	computer vision;geographic information system;global positioning system;particle filter;sampling (signal processing);sensor	Li Bai;Yan Wang	2010	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2010.2043431	turn-by-turn navigation;computer vision;simulation;global positioning system;computer science;geographic information system;mobile robot navigation;statistics	Vision	48.853502742774914	-56.65661357772379	165961
303ee8891fd9462eb64093f6ba2d34d4291657c1	an improved coordinate system for point correspondences of 2d articulated shapes	eccentricity transform;discrete shape;coordinate system	To find corresponding points in different poses of the same articulated shape, a non rigid coordinate system is used. Each pixel of each shape is identified by a pair of distinct coordinates. The coordinates are used to address corresponding points. This paper proposes a solution to a discretization problem identified in a previous approach. The polar like coordinate system is computed in a space where the problem cannot occur, followed by mapping the computed coordinates to pixels.	connected component (graph theory);contour line;correspondence problem;discretization;pixel	Adrian Ion;Yll Haxhimusa;Walter G. Kropatsch	2009		10.1007/978-3-642-04397-0_9	curvilinear coordinates;computer vision;cylindrical coordinate system;bispherical coordinates;topology;paraboloidal coordinates;coordinate space;spherical coordinate system;elliptic cylindrical coordinates;ellipsoidal coordinates;coordinate system;elliptic coordinate system;toroidal coordinates;bipolar coordinates;mathematics;geometry;barycentric coordinate system;log-polar coordinates;clip coordinates;bipolar cylindrical coordinates;parabolic coordinates;orthogonal coordinates;skew coordinates	Vision	49.59827083974232	-54.205502305375326	166157
36fdb7f057ffd6769243c90ab59072e2fb32dcaf	inferring segmented dense motion layers using 5d tensor voting	motion analysis;moving object;image tridimensionnelle;vision ordenador;streaming;pistage;image motion analysis;one step method;trajectoire;estimation mouvement;video streaming;tensile stress voting spatiotemporal phenomena motion segmentation computer vision image sequences image segmentation image converters image reconstruction streaming media;image segmentation;tensile stress;image processing;5d tensor voting;dense motion layers;3d spatiotemporal volume;image converters;metodo un paso;tensor voting;analisis forma;base donnee temporelle;estimacion movimiento;corps mobile;dense temporal trajectories;rastreo;procesamiento imagen;motion estimation;intelligence artificielle;segmentation;fiber bundle representation;traitement image;camera motion dense motion layers 5d tensor voting local spatiotemporal approach motion segmentation dense temporal trajectories image sequence image representation 3d spatiotemporal volume mathematical formalism spatiotemporal smoothness constraint fiber bundle representation video stream;spatial database;local spatiotemporal approach;computer vision;algorithms artificial intelligence image enhancement image interpretation computer assisted imaging three dimensional motion pattern recognition automated reproducibility of results sensitivity and specificity;camera motion;reconstruction image;mosaicking motion analysis tensor voting optical flow segmentation;motion segmentation;transmission en continu;spatiotemporal smoothness constraint;trajectory;senal video;signal video;reconstruccion imagen;streaming media;voting;image representation;image reconstruction;cuerpo movil;image sequence;segmentation image;methode un pas;mathematical formalism;spatiotemporal phenomena;video signal;tridimensional image;artificial intelligence;spatial data structures;vision ordinateur;secuencia imagen;trayectoria;temporal databases;optical flow;base dato especial;pattern analysis;voto;moving body;inteligencia artificial;transmision fluyente	We present a novel local spatiotemporal approach to produce motion segmentation and dense temporal trajectories from an image sequence. A common representation of image sequences is a 3D spatiotemporal volume, (x,y,t), and its corresponding mathematical formalism is the fiber bundle. However, directly enforcing the spatiotemporal smoothness constraint is difficult in the fiber bundle representation. Thus, we convert the representation into a new 5D space (x,y,t,vx,vy) with an additional velocity domain, where each moving object produces a separate 3D smooth layer. The smoothness constraint is now enforced by extracting 3D layers using the tensor voting framework in a single step that solves both correspondence and segmentation simultaneously. Motion segmentation is achieved by identifying those layers, and the dense temporal trajectories are obtained by converting the layers back into the fiber bundle representation. We proceed to address three applications (tracking, mosaic, and 3D reconstruction) that are hard to solve from the video stream directly because of the segmentation and dense matching steps, but become straightforward with our framework. The approach does not make restrictive assumptions about the observed scene or camera motion and is therefore generally applicable. We present results on a number of data sets.	3d reconstruction;algorithm;cross-correlation;graphics processing unit;mathematics;mosaic - computer software;ncsa mosaic;pixel;semantics (computer science);smoothing (statistical technique);stage level 1;streaming media;tissue fiber;velocity (software development);vyos;anatomical layer;biologic segmentation	Changki Min;Gérard G. Medioni	2008	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.70802	3d reconstruction;iterative reconstruction;computer vision;simulation;tensor;voting;formalism;image processing;computer science;trajectory;motion estimation;optical flow;mathematics;tracking;temporal database;image segmentation;stress;segmentation;spatial database;electoral-vote.com;computer graphics (images)	Vision	50.9477491127312	-56.51715411474689	166201
0b69047bfb449aa31754c5c407dced9ff47b340d	3-d object recognition using passively sensed range data	object recognition;range data;stable models;camera motion;model matching;active sensing;image modeling;object model	Model-based object recognition is typically addressed by first deriving structure from images, and then matching that structure with stored objects. While recognition should be facilitated through the derivar tion of as much structure as possible, most researchers have found that a compromise is necessary, as the processes for deriving that structure are not sufficiently robust. We present a technique for the extraction, and subsequent recognition, of 3-D object models from passively sensed images. Model extraction is performed using a depth from camera motion technique, followed by simple interpolation between the determined depth values. The resultant models are recognised using a new technique, implicit model matching, which was originally developed for use with models derived from actively sensed range data [1]. The technique performs object recognition using secondary representations of the 3-D models, hence overcoming the problems frequently associated with deriving stable model primitives. This paper, then, describes a technique for deriving 3-D structure from passively sensed images, introduces a new approach to object recognition, tests the approach robustness of the approach, and hence demonstrates the potential for object recognition using 3-D structure derived from passively sensed data.	outline of object recognition	Kenneth M. Dawson;David Vernon	1992		10.1007/3-540-55426-2_92	computer vision;object model;computer science;cognitive neuroscience of visual object recognition;pattern recognition;3d single-object recognition	Robotics	46.732516173545044	-55.288677842933275	166327
5167d863a4a073d3cddfd185c155e42eac9fc32a	direct matrix factorization and alignment refinement: application to defect detection	minimization;matrix factorization;transmission line matrix methods optimization matrix decomposition accuracy robustness sparse matrices minimization;image alignment;matrix factorization defect detection industrial inspection template matching image alignment;production engineering computing inspection matrix algebra minimisation object detection;accuracy;industrial inspection;matrix decomposition;template guided approach direct matrix factorization alignment refinement defect detection approach template differencing template image parameter tuning false alarm removal pixel wise accuracy robust rank minimization alignment method template guided image matrix alignment refined defect free images explicit error component;robustness;optimization;transmission line matrix methods;template matching;sparse matrices;defect detection	Defect detection approaches based on template differencing require precise alignment of the input and template image, however, such alignment is easily affected by the presence of defects. Often, non-trivial pre/post-processing steps and/or manual parameter tuning are needed to remove false alarms, complicating the system and hampering automation. In this work, we explicitly address alignment and defect extraction jointly, and provide a general iterative algorithm to improve both their performance to pixel-wise accuracy. We achieve this by utilizing and extending the robust rank minimization and alignment method of [12]. We propose an effective and efficient optimization algorithm to decompose a template-guided image matrix into a low-rank part relating to alignment-refined defect-free images and an explicit error component containing the defects of interest. Our algorithm is fully automatic, training-free, only needs trivial pre/post-processing procedures, and has few parameters. The rank minimization formulation only requires a linearly correlated template image, and a template-guided approach relieves the common assumption of small defects, making our system very general. We demonstrate the performance of our novel approach qualitatively and quantitatively on a real-world data-set with defects of varying appearance.	algorithm;angular defect;autoregressive integrated moving average;data structure alignment;iterative method;mathematical optimization;pixel;refinement (computing);robustness (computer science);scalability;software bug;video post-processing	Zhen Qin;Peter van Beek;Xu Chen	2014	2014 Canadian Conference on Computer and Robot Vision	10.1109/CRV.2014.26	computer vision;theoretical computer science;machine learning;mathematics;matrix decomposition	Vision	52.01587719322212	-53.1416020362122	167496
bbe2174c31d22c0785e0cbfba87a509b3d2466ca	rigid and non-rigid face motion tracking by aligning texture maps and stereo 3d models	reconnaissance visage;mimica;traitement signal;on line systems;adaptive appeareance models;metodo adaptativo;evaluation performance;texture;vision ordenador;performance evaluation;image processing;modelo 3 dimensiones;modele 3 dimensions;biometrie;mimique;evaluacion prestacion;orientado aspecto;texture mapping;biometrics;biometria;procesamiento imagen;three dimensional model;methode adaptative;carta de datos;traitement image;3d registration;motion tracking;face tracking;computer vision;feasibility;registro imagen;detection mouvement;automatic recognition;3d model;face recognition;recalage image;systeme en ligne;mappage;signal processing;image registration;facial action tracking;adaptive method;poursuite cible;textura;pattern recognition;deteccion movimiento;robust 3d registration;vision ordinateur;aspect oriented;mapping;reconnaissance forme;facial expression;target tracking;reconocimiento patron;procesamiento senal;motion detection;practicabilidad;faisabilite;oriente aspect;reconocimiento automatico;reconnaissance automatique	Accurate rigid and non-rigid tracking of faces is a challenging task in computer vision. Recently, appearance-based 3D face tracking methods have been proposed. These methods can successfully tackle the image variability and drift problems. However, they may fail to provide accurate out-of-plane face motions since they are not very sensitive to out-of-plane motion variations. In this paper, we present a framework for fast and accurate 3D face and facial action tracking. Our proposed framework retains the strengths of both appearance and 3D data-based trackers. We combine an adaptive appearance model with an online stereo-based 3D model. We provide experiments and performance evaluation which show the feasibility and usefulness of the proposed approach. 2007 Elsevier B.V. All rights reserved.	3d modeling;algorithm;algorithmic efficiency;central processing unit;computation;computer vision;experiment;facial motion capture;heart rate variability;map;performance evaluation;real-time clock;texture mapping	Fadi Dornaika;Angel Domingo Sappa	2007	Pattern Recognition Letters	10.1016/j.patrec.2007.06.011	texture mapping;computer vision;facial motion capture;aspect-oriented programming;image processing;computer science;image registration;signal processing;texture;facial expression;biometrics;computer graphics (images)	Vision	46.97188080207553	-57.89203161948882	168070
5a7639bc2cba5a057a4e8289d0c76c66d6c1d9f2	feature correspondence and motion recovery in vehicle planar navigation	navegacion;robot movil;pistage;estimation mouvement;image processing;autonomous vehicle;autonomous system;estimacion movimiento;extraction forme;rastreo;procesamiento imagen;motion estimation;robotics;transformacion hough;traitement image;sistema autonomo;navigation;robot mobile;extraccion forma;pattern matching;systeme autonome;pattern recognition;robotica;hough transformation;hough transform;transformation hough;robotique;concordance forme;pattern extraction;moving robot;tracking	"""This paper describes a strategy to feature point correspondence and motion recovery in vehicle navigation. A transformation of the image plane is proposed that keeps the motion of the vehicle on a plane parallel to the transformed image plane. This permits to de""""ne linear tracking """"lters to estimate the real-world positions of the features, and allows us to select the matches that accomplish the rigidity of the scene by a Hough transform. Candidate correspondences are selected by similarity, taking into account the smoothness of motion. Further processing brings out the """"nal matching. The methods have been tested in a real application. ( 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	artificial neural network;autonomous robot;cluster analysis;feature extraction;hough transform;image plane;matching (graph theory);pattern recognition	José Miguel Sanchiz Martí;Filiberto Pla	1999	Pattern Recognition	10.1016/S0031-3203(98)00139-3	hough transform;computer vision;image processing;computer science;robotics;computer graphics (images)	Vision	48.89657045243234	-57.874068963442	168125
b688500fc356cdce1afa29cccf1f3b9b4c28f1fd	robust principal component analysis for computer vision	computer vision;least squares approximations;principal component analysis;computer vision;least squares estimation;outliers;principal component analysis;robust m-estimation algorithm	Principal Component Analysis (PCA) has been widely used for the representation of shape, appearance, and motion. One drawback of typical PCA methods is that they are least squares estimation techniques and hence fail to account for “outliers” which are common in realistic training sets. In computer vision applications, outliers typically occur within a sample (image) due to pixels that are corrupted by noise, alignment errors, or occlusion. We review previous approaches for making PCA robust to outliers and present a new method that uses anintra-sampleoutlier process to account for pixel outliers. We develop the theory of Robust Principal Component Analysis (RPCA) and describe a robust M-estimation algorithm for learning linear multivariate representations of high dimensional data such as images. Quantitative comparisons with traditional PCA and previous robust algorithms illustrate the benefits of RPCA when outliers are present. Details of the algorithm are described and a software implementation is being made publically available.	algorithm;computer vision;least squares;pixel;robust principal component analysis	Fernando De la Torre;Michael J. Black	2001		10.1109/ICCV.2001.10084	computer vision;ransac;non-linear iterative partial least squares;least trimmed squares;computer science;machine learning;pattern recognition;statistics;principal component analysis;clustering high-dimensional data	Vision	50.71402258265043	-54.720357552749064	168622
e9b80e0facab2966bdbc8831c765d92ba39d3027	highly automated image recomposition : the picture you wish you had taken	reconnaissance visage;mimica;interfaz grafica;interfaces;image processing;graphical interface;man machine dialogue;biometrie;mimique;localization;biometrics;4230s;biometria;procesamiento imagen;automatisation;localizacion;automatizacion;traitement image;ease of use;algorithme;algorithm;automatic recognition;face recognition;localisation;pattern recognition;graphic user interface;dialogo hombre maquina;algorithms;facial features;reconnaissance forme;facial expression;facial recognition systems;video;reconocimiento patron;face detection;4230v;interface graphique;reconocimiento automatico;reconnaissance automatique;dialogue homme machine;algoritmo;automation	Have you ever lamented, I wish I had taken a different picture of that 'Kodak Moment' when everyone was smiling and no one blinked? With image recomposition, we strive to deliver the one-click fix experience to customers so they can easily create the perfect pictures that they never actually took. To accomplish this, a graphic user interface was created that integrates existing and new algorithms, including face detection, facial feature location, face recognition, expression recognition, face appearance and pose matching, and seamless blending. Advanced modes include face relighting. This system is capable of performing image recomposition from a mixture of videos and still photos, with ease of use and a high degree of automation.		Jiebo Luo;Phoury Lei	2007		10.1117/12.706926	computer vision;image processing;computer science;artificial intelligence;graphical user interface;algorithm;computer graphics (images)	Vision	47.51821267081105	-58.35450842012261	168652
2f9363ef688c00ecd864faa0c97cd37992be366e	can early stage vision detect topology	human vision;apparent motion;experience base	The apparent motion reveals what in an image that human vision detects first. Chen's assumption that early stage vision can percept global topology is proved incorrect in theory and experiments. Based upon psychological studies on human vision, a new theory, the blurred matching, was introduced into visual computation which well fits the results of all experiments about apparent motion, Chen's included. An algorithm was constructed accordingly which predicts apparent motion by comparison of distances.	algorithm;computation;entity–relationship model;experiment;fits	Lifu Liu;Nanyuan Zhao;Bian Zhaoqi	1989			computer vision;structure from motion;simulation;artificial intelligence;mathematics	Vision	53.66235317409471	-52.45520358721067	168827
cdc4d96331c9cef4e57ed6bac55d0b3887b8b2b7	3-d structure from visual motion: modeling, representation and observability	observability;vitesse;scale model;velocity;vision ordenador;estructura 3 dimensiones;estimation mouvement;filtro kalman;image processing;modele reduit;observabilidad;estimacion movimiento;emplazamiento arqueologico;filtre kalman;procesamiento imagen;kalman filter;site archeologique;motion estimation;observabilite;velocidad;traitement image;time varying system;structure 3 dimensions;computer vision;modelo reducido;3 d computer vision;systeme parametre variable;systeme non lineaire;nonlinear system observability;vision ordinateur;sistema parametro variable;three dimensional structure;sistema no lineal;archaeological sites;non linear system	"""The problem of \Structure From Motion"""" concerns the reconstruction of the three-dimensional structure of a scene from its projection onto a moving two-dimensional surface. Such a problem is solved eeectively by the human visual system, judging from the ease with which we perform delicate control tasks involving vision as a sensor such as reaching for objects in the environment or driving a car. In this paper we study \Structure From Motion"""" from the point of view of dynamical systems: we rst formalize the problem of three-dimensional structure and motion reconstruction as the estimation of the state of certain nonlinear dynamical models. Then we study the feasibility of \Structure From Motion"""" by analyzing the observability of such models. The models which deene the visual motion estimation problem for feature points in the Euclidean 3-D space are not locally observable; however, the non-observable manifold can be easily isolated by imposing metric constraints on the state-space. One of the peculiarities of vision as a sensor is its richness, which can be a disadvantage when we are interested only in few of the unknown parameters. For instance, if we want to control the direction of heading of our car by measuring brightness values on our retina, we have to overcome the eeects that the shape of the environment, its reeectance properties, illumination and other quantities have on our measurements. Invariance to undesired parameters can be achieved by appropriate modeling or by choice of representation of the parameter space. We propose and analyze models for three-dimensional structure that are independent of three-dimensional motion and vice-versa. Estimating unknown parameters from such models amounts to the identiication of non-linear and implicit systems with parameters on diierentiable manifolds, such as a sphere or the so-called essential manifold."""	course (navigation);dynamical system;motion estimation;nonlinear system;observable;state space;statistical manifold	Stefano Soatto	1997	Automatica	10.1016/S0005-1098(97)00048-4	kalman filter;computer vision;structure from motion;observability;simulation;image processing;motion estimation;control theory;mathematics;velocity;motion field;scale model	Vision	51.08409987313371	-57.176716663691906	169312
810ea175ba8d5f42919f24a507e7d80af4a809a8	a probabilistic spatial data model	modelizacion;error medida;pistage;forma;measurement error;spatial data;loi probabilite;composition;ley probabilidad;composicion;localizacion objeto;object location;rastreo;probabilistic approach;data model;erreur mesure;modelisation;shape;object oriented;enfoque probabilista;approche probabiliste;probability distribution;oriente objet;visual tracking;forme;modeling;orientado objeto;localisation objet;autonomous robot;spatial information;tracking	Spatial information in autonomous robot tasks is uncertain due to measurement errors, the dynamic nature of the world, and an incompletely known environment. We present a probabilistic spatial data model capable of describing relevant spatial data, such as object location, shape, composition, and other parameters, in the presence of uncertainty. Uncertain spatial information is modeled through continuous probability distributions on values of attributes. The data model is designed to support our visual tracking and navigation prototype.	data model	Yoram Kornatzky;Solomon Eyal Shimony	1996	Inf. Sci.	10.1016/0020-0255(95)00241-3	computer vision;computer science;probabilistic database;artificial intelligence;spatial analysis;statistics	DB	47.005462530338114	-55.58650019819584	170527
39441388e1aef39d4a183685673fc5b9fa867b52	a grid-computing based multi-camera tracking system for vehicle plate recognition	image recognition;reconocimiento imagen;pistage;haute performance;tracking system;numerical method;distributed computing;multigrille;rastreo;68g35;vehicle plate recognition;grid;recognition system;autoroute;metodo numerico;route;rejilla;autopista;68u10;vehicle identification;imaging;multigrid;poursuite cible;identificacion vehiculo;reconnaissance image;multigrilla;alto rendimiento;grille;carretera;calculo repartido;formation image;highway;formacion imagen;target tracking;grid computing;high performance;freeway;calcul reparti;methode numerique;tracking;identification vehicule	There are several ways that can be implemented in a vehicle tracking system such as recognizing a vehicle color, a shape or a vehicle plate itself. In this paper, we will concentrate ourselves on recognizing a vehicle on a highway through vehicle plate recognition. Generally, recognizing a vehicle plate for a toll-gate system or parking system is easier than recognizing a car plate for the highway system. There are many cameras installed on the highway to capture images and every camera has different angles of images. As a result, the images are captured under varied imaging conditions and not focusing on the vehicle itself. Therefore, we need a system that is able to recognize the object first. However, such a system consumes a large amount of time to complete the whole process. To overcome this drawback, we installed this process with grid computing as a solution. At the end of this paper, we will discuss our obtained result from an experiment.	automatic number plate recognition;grid computing;match moving;vehicle tracking system	Zalili Binti Musa;Junzo Watada	2006	Kybernetika		medical imaging;route;computer vision;simulation;tracking system;numerical analysis;tracking;grid;multigrid method;grid computing	Robotics	48.24735627220299	-57.51206104701766	171166
092b8b86456234b72d944e76dcad3c9506663382	metric learning for image alignment	modelizacion;alignement;ajustamiento modelo;vision ordenador;theorie locale;local theory;image processing;cost function;image alignment;learning model;image matching;lucas kanade;procesamiento imagen;metric;active appearance model;traitement image;metric learning;computer vision;registro imagen;ajustement modele;modelisation;reconocimiento de patrones en imagenes;recalage image;pattern matching;image registration;model matching;alineamiento;estimacion parametro;metrico;active appearance models;vision ordinateur;teoria local;concordance forme;parameter estimation;estimation parametre;local minima;modeling;template matching;alignment;metrique;appariement image	Image alignment has been a long standing problem in computer vision. Parameterized Appearance Models (PAMs) such as the Lucas-Kanade method, Eigentracking, and Active Appearance Models are commonly used to align images with respect to a template or to a previously learned model. While PAMs have numerous advantages relative to alternate approaches, they have at least two drawbacks. First, they are especially prone to local minima in the registration process. Second, often few, if any, of the local minima of the cost function correspond to acceptable solutions. To overcome these problems, this paper proposes a method to learn a metric for PAMs that explicitly optimizes that local minima occur at and only at the places corresponding to the correct fitting parameters. To the best of our knowledge, this is the first paper to address the problem of learning a metric to explicitly model local properties of the PAMs’ error surface. Synthetic and real examples show improvement in alignment performance in comparison with traditional approaches. In addition, we show how the proposed criteria for a good metric can be used to select good features to track.	active appearance model;align (company);computer vision;loss function;lucas–kanade method;maxima and minima	Minh Hoai Nguyen;Fernando De la Torre	2009	International Journal of Computer Vision	10.1007/s11263-009-0299-9	computer vision;active appearance model;image processing;computer science;artificial intelligence;mathematics;geometry	Vision	49.91998077931888	-56.376203387733696	171190
59fcfc55e198da79bdc5e458641ec21553102d77	efficient and low-cost 3d structured light system based on a modified number-theoretic approach	signal image and speech processing;image tridimensionnelle;three dimensional display systems;gray code;estrategia optima;diminution cout;visualitzacio tridimensional informatica;defasaje;implementation;three dimensional shape;biomechanics;phase shift;dephasage;biomecanique;info eu repo semantics article;forma tridimensional;optimal strategy;number theory;quantum information technology spintronics;forme tridimensionnelle;three dimensional imaging in biomechanics;code gray;tridimensional image;theorie nombre;imatge tecniques d;biomecanica;teoria numeros;implementacion;reduccion costes;codigo gray;3d structure;strategie optimale;cost lowering;imaging systems;imagen tridimensional;imatges tridimensionals en biomecanica	3D scanning based on structured light (SL) has been proven to be a powerful tool to measure the three-dimensional shape of surfaces, especially in biomechanics. We define a set of conditions that an optimal SL strategy should fulfill in the case of static scenes and then we present an efficient solution based on improving the number-theoretic approach (NTA). The proposal is compared to the well-known Gray code (GC) plus phase shift (PS) technique and the original NTA, all satisfying the same set of conditions but obtaining significant improvements with our implementation. The technique is validated in biomechanical applications such as the scanning of a footprint left on a “foam box” typically made for that purpose, where one of the ultimate goals could be the production of a shoe insole.	image processing;image scanner;instantaneous phase;multiplexing;programming paradigm;sl (complexity);structured light;structured-light 3d scanner;theory;video projector;whole earth 'lectronic link	Tomislav Pribanic;Hrvoje Dzapo;Joaquim Salvi	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/474389	gray code;number theory;computer science;artificial intelligence;biomechanics;mathematics;phase;implementation;algorithm	Graphics	50.256245832171636	-56.069884480452785	171443
2582dd37d3011cf50dac8c2c400eda06a651900e	first sight: a human body outline labeling system	motion analysis;vision system;moving image;analisis imagen;vision ordenador;image motion analysis;human body outline labeling system;image segmentation;moving human body;edge detection;outline;labeled two dimensional human body stick figure;self occlusions;biological system modeling;biomechanics;hombre;motion estimation;coincidence edge;motion;data mining;imagen movil;corps humain;image mobile;computer vision;deteccion contorno;human body model;information gathering;detection contour;posture;self occlusions first sight human body outline labeling system moving human body image sequence labeled two dimensional human body stick figure shape posture;first sight;shape;humans labeling image sequences biological system modeling machine vision image motion analysis information analysis motion analysis data mining shape;machine vision;human body;indexation;image segmentation image sequences biomechanics motion estimation edge detection;image sequence;human;model;image analysis;vision ordinateur;secuencia imagen;humans;difference picture;analyse image;information analysis;pose;labeling;sequence image;stick figure;ribbon;homme;contour coincidence;image sequences	First Sight, a vision system in labeling the outline of a moving human body, is proposed in this paper. The emphasis of First Sight is on the analysis of motion information gathered solely from the outline of a moving human object. Two main processes are implemented in First Sight. The first process uses a novel technique to extract the outline of a moving human body from an image sequence. The second process, which employs a new human body model, interprets the outline and produces a labeled two-dimensional human body stick figure for each frame of the image sequence. Extensive knowledge of the structure, shape, and posture of the human body is used in the model. The experimental results of applying the technique on unedited image sequences with self-occlusions and missing boundary lines are encouraging. Index Items-Coincidence edge, difference picture, human body, human body model, labeling, model, motion, outline, pose, posture, ribbon, stick figure.	poor posture	Maylor K. H. Leung;Yee-Hong Yang	1995	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.385981	computer vision;labeling theory;human body;pose;edge detection;machine vision;shape;computer science;motion;motion estimation;image segmentation;data analysis;computer graphics (images)	Vision	49.53353392628403	-57.84834669187789	173643
5e0ce7719dcb315145284fea50fd7c96df3599ab	model-based recognition in robot vision	object representation;object recognition;vision ordenador;modelo 3 dimensiones;learning;robots industrial vision systems;modele 3 dimensions;estudio comparativo;image understanding;robotics vision systems;three dimensional model;robotics;imagen nivel gris;classification;computer vision;aprendizaje;etude comparative;reconstruction image;apprentissage;robot vision;application industrielle;feature extraction;image reconstruction;image niveau gris;comparative study;pattern recognition;robotica;industrial application;vision ordinateur;reconstruccion de imagen;robotique;reconnaissance forme;extraction caracteristique;reconocimiento patron;grey level image;article;clasificacion;matching model;aplicacion industrial;computer programming algorithms	"""This paper presents a comparative study and survey of model-based object-recognition algorithms for robot vision. The goal of these algorithms is to recognize the identity, position, and orientation of randomly oriented industrial parts. In one form this is commonly referred to as the """"bin-picking"""" problem, in which the parts to be recognized are presented in a jumbled bin. The paper is organized according to 2-D, 2½-D, and 3-D object representations, which are used as the basis for the recognition algorithms. Three central issues common to each category, namely, feature extraction, modeling, and matching, are examined in detail. An evaluation and comparison of existing industrial part-recognition systems and algorithms is given, providing insights for progress toward future robot vision systems."""	2.5d;algorithm;feature extraction;outline of object recognition;randomness	Roland T. Chin;Charles R. Dyer	1986	ACM Comput. Surv.	10.1145/6462.6464	iterative reconstruction;computer vision;feature extraction;biological classification;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;comparative research;robotics	Robotics	47.267918718673464	-59.078724430283636	173708
49fc91e1e6c9690140ab063192a36ec8a492b381	measuring object surface shape and reflectance properties	analisis imagen;realite virtuelle;modelo 3 dimensiones;realidad virtual;computer graphics;modele 3 dimensions;analisis forma;reconstitution forme;virtual reality;three dimensional model;surface reconstruction;computer graphic;reconstruction surface;reconstitucion forma;image analysis;pattern analysis;reconstruccion superficie;pattern recovery;grafico computadora;analyse image;infographie;analyse forme;object model	An object model for computer graphics applications should contain two aspects of information: shape and reflectance properties of the object. A number of techniques have been developed for modeling object shapes by observing real objects. In contrast, attempts to model reflectance properties of real objects have been rather limited. In most cases, modeled reflectance properties are too simple or too complicated to be used for synthesizing realistic images of the object. In this paper, we propose a new method for modeling object reflectance properties, as well as object shapes, by observing real objects. First, an object surface shape is reconstructed by merging multiple range images of the object. By using the reconstructed object shape and a sequence of color images of the object, parameters of a reflection model are estimated in a robust manner. The recovered object shape and reflectance properties are then used for synthesizing object images with realistic shading effects under arbitrary illumination conditions.		Yoichi Sato;Mark D. Wheeler;Katsushi Ikeuchi	1998		10.1007/3-540-63931-4_236	active shape model;computer vision;image analysis;surface reconstruction;object model;computer science;virtual reality;programming language;computer graphics;computer graphics (images)	Vision	51.69070398733602	-57.067750573146355	173935
de0a3e73fce319b608abbbbcced42237f2911267	joint optical flow estimation, segmentation, and 3d interpretation with level sets	level sets;image tridimensionnelle;vision ordenador;estructura 3 dimensiones;estimation mouvement;image segmentation;image processing;flux optique;curve evolution;estimacion movimiento;corps mobile;level set;procesamiento imagen;linear least square;optical flow estimation;motion estimation;segmentation;traitement image;structure 3 dimensions;computer vision;motion segmentation;flujo optico;cuerpo movil;image sequence;segmentation image;tridimensional image;vision ordinateur;secuencia imagen;optical flow;moving body;variational method;three dimensional structure;3d structure;3d motion;variational methods;sequence image;imagen tridimensional;methode variationnelle	This paper describes a variational method with active curve evolution and level sets for the estimation, segmentation, and 3D interpretation of optical flow generated by independently moving rigid objects in space. Estimation, segmentation, and 3D interpretation are performed jointly. Segmentation is based on an estimate of optical flow consistent with a single rigid motion in each segmentation region. The method, which allows both viewing system and viewed objects to move, results in three steps iterated until convergence: (a) evolution of closed curves via level sets and, in each region of the segmentation, (b) linear least squares computation of the essential parameters of rigid motion, (c) estimation of optical flow consistent with a single rigid motion. The translational and rotational components of rigid motion and regularized relative depth are recovered analytically for each region of the segmentation from the estimated essential parameters and optical flow. Several examples with real image sequences are provided which verify the validity of the method. 2005 Elsevier Inc. All rights reserved.	calculus of variations;computation;emoticon;experiment;iteration;linear least squares (mathematics);optical flow	Hicham Sekkati;Amar Mitiche	2006	Computer Vision and Image Understanding	10.1016/j.cviu.2005.11.002	computer vision;image processing;computer science;level set;segmentation-based object categorization;mathematics;geometry;image segmentation;scale-space segmentation	Vision	50.77012565016172	-56.903341536276905	173959
6a1286c70ea6d9a768bc6b443a0f486f9943c11a	metric reconstruction of planes utilizing off-the-plane features	contraste;parallelisme;orthogonality;vision ordenador;vanishing points;metric;parameterization;parametrizacion;conico;computer vision;projective plane;registro imagen;plano proyectivo;parallelism;paralelismo;recalage image;vanishing point;video cameras;human visual system;image registration;plan projectif;camera video;projective space;aparato visual;invariante;metrico;appareil visuel;vision ordinateur;etalonnage;camera calibration;camara de video;visual system;conics;range of conics;metric invariants;conique;calibration;parametrisation;invariant;orthogonalite;metrique;image warping;conic dual to the circular points;ortogonalidad	Metric reconstruction of a projected plane is equivalent to estimating metric invariants of the plane in projective space. Working with an unknown planar scene taken with an uncalibrated camera, however, estimating the metric invariants may not be possible only with features on the plane, because the human visual system is not good at detecting sufficient information. Although the conventional algorithms use only features on the plane to recover the plane metric, we show that features not on the plane can be utilized. Study about the range of the dual conics and its self-polar triangle verifies that the metric invariant, the conic dual to the circular points of a plane is constrained with the orthogonal vanishing points under the assumption of the same camera. Using the constraint, we can get the orthogonal vanishing points from the given metric invariants, or inversely, the metric invariants from the orthogonal vanishing points. We also show that an image warping based on parallelism and orthogonality gives a physically meaningful parameterization of the metric invariant. With this new parameterization and the self-polar constraint, it is possible to recover the metric of a plane from information that the human visual system can easily detect without explicit camera calibration. 2010 Elsevier Inc. All rights reserved.	algorithm;camera resectioning;experiment;geo warping;image warping;invariant (computer science);parallel computing;sensor;synthetic data;vanishing point	Jun-Sik Kim;In-So Kweon	2011	Computer Vision and Image Understanding	10.1016/j.cviu.2010.09.005	metric signature;convex metric space;computer vision;injective metric space;topology;vanishing point;fubini–study metric;metric differential;metric;fisher information metric;word metric;metric connection;intrinsic metric;chebyshev distance;mathematics;geometry;image plane;equivalence of metrics;bk tree	Vision	50.313980114815806	-56.95178192691696	174498
f37809d5dce96a82662b0d648fe675310365bf36	interactive learning of a multiple-attribute hash table classifier for fast object recognition	vision system;hachage;modelizacion;automatic;image recognition;object recognition;reconocimiento imagen;vision ordenador;decision tree;uncertainty modeling;learning;clasificador;automatico;arbol decision;computer vision;systeme conversationnel;aprendizaje;modelisation;apprentissage;hashing;classifier;hybrid method;interactive system;hash table;estructura datos;reconnaissance image;sistema conversacional;pattern recognition;classificateur;automatique;interactive learning;dihedral angle;system development;vision ordinateur;structure donnee;reconnaissance forme;reconocimiento patron;modeling;data structure;arbre decision	Multiple-attribute hashing is now considered to be a powerful approach for the recognition and localization of 3D objects on the basis of their invariant properties. In the systems developed to date, the structure of the hash table is fixed and must be created by the system developer?an onerous task especially when the number of attributes is large, as it must in systems that use both geometric and nongeometric attributes. Another deficiency of previous systems is that uncertainty is treated as a fixed value and not modeled. In this paper, we will present a system, named MULTI-HASH, which uses the tools of decision trees and uncertainty modeling for the automatic construction of hash tables. The decision-tree framework in MULTI-HASH is based on a hybrid method that uses both qualitative attributes, such as the shape of a surface, and quantitative attributes such as color, dihedral angles, etc. The human trainer shows objects to the vision system and, in an interactive mode, tells the system the model identities of the various segmented regions, etc. Subsequently, the decision-tree-based framework learns the structure of the hash table.	hash table;outline of object recognition	Lynne L. Grewe;Avinash C. Kak	1995	Computer Vision and Image Understanding	10.1006/cviu.1995.1030	feature hashing;computer vision;hash table;hash function;systems modeling;dynamic perfect hashing;data structure;classifier;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;decision tree;universal hashing;dihedral angle;coalesced hashing;automatic transmission;hash tree	Vision	47.43741890416364	-58.989224693736006	174589
057e9e5e5777d948e56b38687f1dfca93176fdd1	learning to estimate scenes from images	motion analysis;low resolution;markov network;high resolution imager;synthetic data	We seek the scene interpretation that best explains image data. For example, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image). From synthetic data , we model the relationship between image and scene patches , and between a scene patch and neighboring scene patches. Given' a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene. This yields an efficient method to form low-level scene interpretations. We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones.	digital image processing;high- and low-level;image resolution;markov chain;markov random field;optical flow;synthetic data	William T. Freeman;Egon C. Pasztor	1998			computer vision;image resolution;scene statistics;mathematics;statistics;synthetic data;computer graphics (images)	ML	53.39580026975322	-54.055096109194714	176520
32749e496a0aa979aa0d1261936b0a3d686ae83e	on geometric variational models for inpainting surface holes	equation derivee partielle;modelizacion;calculo de variaciones;partial differential equation;vision ordenador;ecuacion derivada parcial;interpolation;euler lagrange equation;modele geometrique;ecuacion euler lagrange;partial dierential equation;laplace equation;equation euler lagrange;equation ordre 2;remplissage;laplacian;ecuacion laplace;surface holes;courbure;variational formulations;second order equation;filling;variational formulation;computer vision;equation elliptique;geometric approach;elliptic equation;modelisation;reconstruction image;laplacien;calcul variationnel;inpainting;laplaciano;reconstruccion imagen;image reconstruction;courbe niveau;distancia;curvatura;curvature;vision ordinateur;ecuacion orden 2;curva nivel;vector field;mean curvature;variational models;ecuacion eliptica;imagen color;scalar and vector fields;modeling;image couleur;variational calculus;equation laplace;distance;contour line;color image;absolute minimizing lipschitz extension;geometrical model;relleno;modelo geometrico	Geometric approaches for filling-in surface holes are introduced and studied in this paper. The basic idea is to represent the surface of interest in implicit form, and fill-in the holes with a scalar, or systems of, geometric partial differential equations, often derived from optimization principles. These equations include a system for the joint interpolation of scalar and vector fields, a Laplacian-based minimization, a mean curvature diffusion flow, and an absolutely minimizing Lipschitz extension. The theoretical and computational framework, as well as examples with synthetic and real data, are presented in this paper.	calculus of variations;computation;computational complexity theory;gradient descent;graph cuts in computer vision;heuristic;inpainting;interpolation;mathematical optimization;synthetic intelligence;variational principle	Vicent Caselles;Gloria Haro;Guillermo Sapiro;Joan Verdera	2008	Computer Vision and Image Understanding	10.1016/j.cviu.2008.01.002	iterative reconstruction;computer vision;laplace operator;mathematical analysis;vector field;systems modeling;topology;color image;interpolation;mean curvature;mathematics;geometry;curvature;elliptic curve;distance;partial differential equation;contour line;calculus of variations;laplace's equation;inpainting	Vision	51.815413618806005	-57.89584069314373	176671
396c30d37029745b98b15f1efcaf5ab845b8a453	robot location determination in a complex environment by multiple marks	robot movil;location problem;probleme localisation;image processing;espacio 3 dimensiones;procesamiento imagen;intelligence artificielle;robotics;traitement image;robot mobile;espace 3 dimensions;three dimensional space;pattern recognition;robotica;artificial intelligence;robotique;problema localizacion;inteligencia artificial;reconnaissance forme;reconocimiento patron;moving robot	Abstract   A new approach of using multiple marks for determining three-dimensional robot location in a complex environment, which may be a composed space or a space with rough terrain, is proposed. One principal idea is to pre-locate a large enough number of marks with different labels at different planar places that at least one mark can be seen by the camera, wherever the robot is in the working space. The other concept is that by integrating the pattern recognition technique with the three-dimensional geometrical transformation method, the system can determine the robot location without the need of fitting any prerequisite alignment. This novel property enables the robot to work in a space with rough terrain. Experimental results from using three marks in a space composed of two warehouses have shown that the location error is less than 3% on average.	robot	King-Chü Hung;Chia-Nian Shyi;Jau-Yien Lee;Tsong-Chiang Lee	1988	Pattern Recognition	10.1016/0031-3203(88)90030-1	three-dimensional space;computer vision;image processing;computer science;artificial intelligence;robotics	Vision	48.25052666145746	-58.925489401738375	177219
4fdb34e5a0e9d873d9a39faee42cf995b45e283d	stereo analysis of archaelogical scenes using monogenic signal representation		This paper presents the results of an experimental study regarding the application of recent stereo analysis theories in the frequency domain, particularly the phase congruency and monogenic filtering methods. The initial approach to the stereo matching problem employed feature based correlation methods. How- ever, the requirement for more dense depth-map output led us to the development of disparity map estimation methods, minimizing a matching cost function be- tween image regions or pixels. The cost function consists of a newly proposed similarity measure function, based on the geometrical properties of the mono- genic signal. Our goal was to examine the performance of these methods in a stereo matching problem setting, on photos of complicated scenes. Two objects were used for this purpose: (i) a scene from an ancient Greek temple of Acropolis and (ii) the outside scene of the gate of an ancient theatre. Due to the complex structure of the photographed objects, classic techniques used for stereo match- ing give poor results. On the contrary, the three-dimensional models and disparity map of the scene computed when applying the proposed method, are much more detailed and consistent.		Manthos Alifragis;Costas S. Tzafestas	2009		10.1007/978-3-642-11840-1_10	computer stereo vision;computer vision;simulation;computer graphics (images)	NLP	52.94092734805543	-54.314113571648356	178655
440b8655dfa7fc724ded505f573d3612a1938689	full-body performance animation with sequential inverse kinematics	modelizacion;animacion por computador;iterative method;manipulators;mouvement corporel;estimation mouvement;articulated figures;real time;estimacion movimiento;cinematica;hombre;motion estimation;motion reconstruction;metodo secuencial;motion;problema inverso;sequential method;kinematics;metodo iterativo;modelisation;inverse problem;ball joint;methode iterative;human motion;temps reel;cinematique;human;methode sequentielle;tiempo real;inverse kinematics;arm;articulacion de rotulas;computer animation;movimiento corporal;modeling;probleme inverse;human animation;body movement;liaison rotule;homme;animation par ordinateur	In this paper, we present an analytic-iterative Inverse Kinematics (IK) method, called Sequential IK (SIK), that reconstructs 3D human full-body movements in real time. The input data for the reconstruction is the least possible (i.e., the positions of wrists, ankles, head and pelvis) in order to be usable within a low-cost human motion capture system that would track only these six features. The performance of our approach is compared to other well-known IK methods in reconstruction quality and computation time obtaining satisfactory results for both. The paper first describes how we handle the spine and the clavicles before offering a simple joint limit model for ball-and-socket joints and a method to avoid self-collisions induced by the elbow. The second part focuses on the algorithms comparison	abductive reasoning;algorithm;artificial intelligence;charge-coupled device;coat of arms;computation;computer vision;coordinate descent;dls format;database;digital puppetry;international karate +;inverse kinematics;iterative method;iterative reconstruction;jacobian matrix and determinant;kinesiology;least squares;levenberg–marquardt algorithm;moore–penrose pseudoinverse;motion capture;poor posture;singular value decomposition;time complexity;zenon kulpa	Luis Unzueta;Manuel Peinado;Ronan Boulic;Angel Suescun	2008	Graphical Models	10.1016/j.gmod.2008.03.002	computer vision;kinematics;simulation;systems modeling;computer science;inverse problem;motion;inverse kinematics;motion estimation;computer animation;iterative method;arm architecture;ball joint;computer graphics (images)	AI	49.57758790432022	-56.12377315842268	179230
565c7b3d875becc31340c403358904f6d3c55c74	texture boundary detection based on the long correlation model	texture;vision ordenador;analisis escena;analyse scene;high resolution;image processing;least squares method;maximum likelihood;maximum vraisemblance;procesamiento imagen;picture processing;prior knowledge;intelligence artificielle;least square method;picture processing parameter estimation pattern recognition;traitement image;computer vision;algorithme;texture boundary detection;algorithm;image segmentation image edge detection layout image analysis humans face detection image resolution frequency estimation least squares methods frequency domain analysis;maximum likelihood method texture boundary detection picture processing pattern recognition long correlation model least squares method frequency domain;textura;pattern recognition;artificial intelligence;vision ordinateur;inteligencia artificial;parameter estimation;maximo semejanza;frequency domain;boundary detection;maximum likelihood method;large classes;long correlation model;scene analysis;algoritmo	The problem of detecting texture boundaries without assuming any knowledge on the number of regions or the types of textures is considered. Texture boundaries are often regarded as better features than intensity edges, because a large class of images can be considered a composite of several different texture regions. An algorithm is developed that detects texture boundaries at reasonably high resolution without assuming any prior knowledge on the texture composition of the image. The algorithm utilizes the long correlation texture model with a small number of parameters to characterize textures. The parameters of the model are estimated by a least-squares method in the frequency domain. The existence and the location of texture boundary is estimated by the maximum-likelihood method. The algorithm is applied to several different images, and its performance is shown by examples. Experimental results show that the algorithm successfully detects texture boundaries without knowing the number of types of textures in the image. >		Rangasami L. Kashyap;Kie B. Eom	1989	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.23113	bidirectional texture function;image texture;computer vision;image processing;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;least squares;texture filtering;statistics	Vision	47.124075623805126	-58.302965564532826	179297
04140671b0871fde940778e0919f03440ac9b2cf	landmark recognition using invariant features	robot navigation;random noise;affine moment invariants;mobile robot navigation;landmark recognition;invariant feature;moment invariant	The paper deals with a view-invariant recognition of circular landmarks used for mobile robot navigation. The recognition model based on the ane moment invariants (AMIs) is introduced. The recognition ability of the AMIs regarding this particular landmark shape is investigated in the presence of additive random noise and/or in the case of various viewing angles. The results of the experiments in real situations, which proved the discriminability and the stability of the recognition model, are shown. Ó 1999 Elsevier Science B.V. All rights reserved.	experiment;invariant (computer science);mobile robot;noise (electronics);robotic mapping;utility functions on indivisible goods	Barbara Zitová;Jan Flusser	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00031-8	computer vision;speech recognition;computer science;mobile robot navigation	Robotics	47.176264618099104	-53.61572497403186	179312
8c48ba2822790ec30b463c601d3fc411e07cc91d	structure and motion of nonrigid object under perspective projection	traitement signal;image tridimensionnelle;methode recursive;recursive estimation;evaluation performance;linear estimation;optimisation;decomposition valeur singuliere;mise a jour;estimation mouvement;estimation recursive;performance evaluation;image processing;optimizacion;geometric optimization;perspective projection;weak perspective projection;nonrigid object;probleme non lineaire;evaluacion prestacion;estimacion movimiento;singular value decomposition;estimacion lineal;metodo recursivo;procesamiento imagen;recursive method;motion estimation;geometrical optimization;nonlinear problems;traitement image;algorithme;actualizacion;algorithm;reconstruction image;factorization;estimation lineaire;factorizacion;reconstruccion imagen;image reconstruction;signal processing;image sequence;poursuite cible;factorisation;projection perspective;tridimensional image;secuencia imagen;optimization;decomposicion valor singular;target tracking;structure and motion;proyeccion perspectiva;nonlinear optimization;procesamiento senal;structure from motion;updating;sequence image;imagen tridimensional;algoritmo	The paper focuses on the problem of structure and motion of nonrigid object from image sequence under perspective projection. Many previous methods on this problem utilize the extension technique of SVD factorization based on rank constraint to the tracking matrix, where the 3D shape of nonrigid object is expressed as a weighted combination of a set of shape bases. All these solutions are based on the assumption of Affine camera model. This assumption will become invalid and cause large reconstruction errors when the object is close to the camera. In this paper, we propose two algorithms, namely the linear recursive estimation and the nonlinear optimization, to extend these methods to general perspective camera model. Both algorithms are based on the shape and motion of weak perspective projection. The former one updates the solutions from weak perspective to perspective projection by refining the scalars corresponding to the projective depths recursively. The latter one is based on nonlinear optimization by minimizing the perspective reprojection residuals. Extensive experiments on simulated data and real image sequences are performed to validate the effectiveness of our new algorithms and noticeable improvements over the previous solutions are observed. 2006 Elsevier B.V. All rights reserved.	3d projection;algorithm;experiment;linear algebra;map projection;mathematical optimization;nonlinear programming;nonlinear system;recursion;singular value decomposition;synthetic data;test data;virtual reality headset	Guanghui Wang;Hung-Tat Tsui;Zhanyi Hu	2007	Pattern Recognition Letters	10.1016/j.patrec.2006.09.006	computer vision;mathematical optimization;combinatorics;image processing;computer science;signal processing;mathematics;geometry;factorization	Vision	50.430163511057145	-56.33149669947224	179468
d5078b392b2e86d396095deb2e58878fabb3ae85	a preprocessing method for automatic break lines detection	break lines;remote sensing by laser beam;interpolation;canny edge detection algorithm;image edge detection laser radar filters infrared detectors clouds infrared imaging buildings interpolation image resolution computer science;image resolution;edge detection;automatic break line detection;image fusion;filters;laser radar;infrared images;lidar cloud points;data fusion;line detection;data mining;remote sensing by laser beam edge detection geophysical signal processing geophysical techniques image fusion image registration infrared imaging optical radar radiometry;radiometry;optical radar;image edge detection;infrared imaging;geophysical signal processing;three dimensional displays;clouds;image registration;pixel;3d filter;registration;3d filter preprocessing method automatic break line detection nonbreak line edge elimination ir images lidar cloud points data fusion canny edge detection algorithm image registration;preprocessing method;edge elimination;computer science;edge elimination lidar break lines infrared images edge detection registration;ir images;meteorology;infrared detectors;buildings;cloud point;nonbreak line edge elimination;geophysical techniques;lidar	We present a preprocessing method for automatic break line detection. Our method is given a set of edges (break lines and non-break lines) we eliminate the non-break line edges leaving only those with higher probability of being break lines. Our method is based on fusing IR images and LiDAR cloud points. In the first step, we apply the Canny edge detection algorithm to the IR images (producing a superset of break lines). Then we project the LiDAR points onto a 2-D plane, ignoring the set of points that are greater than a selected threshold (different elevation thresholds have been selected), which allows the footprints of some elevated structures to appear clearly in the set of projected points. Those structures that appear in both LiDAR points and IR images are used as references for registration of LiDAR cloud points with the IR images. After registration, we eliminate all the edges that appear in flat areas, which is achieved by applying a 3D filter to the LiDAR points.	algorithm;canny edge detector;edge detection;preprocessor	Yassine Belkhouche;Bill P. Buckles;Xiaohui Yuan;Laura Steinberg	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4778975	lidar;computer vision;computer science;optics;remote sensing	Vision	52.83925263804325	-55.755858132469875	179711
4f1e3c7f27301c25784b329cf84f486087f0e3be	image matching with scale adjustment	rotation invariant descriptors;high resolution;robust estimator;image matching;interest points;low resolution;rotation invariance;scale space;points of interest;matching;matching constraints;geometric model;high resolution imager;point of interest;matching method	In this report we address the problem of matching two images with two di erent resolutions: a high-resolution image and a low-resolution one. The di erence in resolution between the two images is not known and without loss of generality one of the images is assumed to be the high-resolution one. On the premise that changes in resolution act as a smoothing equivalent to changes in scale, a scale-space representation of the highresolution image is produced. Hence the one-to-one classical image matching paradigm becomes one-to-many because the low-resolution image is compared with all the scale-space representations of the high-resolution one. Key to the success of such a process is the proper representation of the features to be matched in scale-space. We show how to represent and extract interest points at variable scales and we devise a method allowing the comparison of two images at two di erent resolutions. The method comprises the use of photometricand rotation-invariant descriptors, a geometric model mapping the high-resolution image onto a low-resolution image region, and an image matching strategy based on local constraints and on the robust estimation of this geometric model. Extensive experiments show that our matching method can be used for scale changes up to a factor of 6. Key-words: matching, scale-space, points of interest This work has been partially supported by Société Aérospatiale, 1998 2001. Appariement d'Images avec Ajustement d'Echelle Résumé : Dans ce rapport on s'intéresse au problème d'appariement de deux images ayant des résolutions di érentes : une image haute résolution et une image basse résolution. La di érence de résolution entre les deux images n'est pas connue et, sans perte de généralité, une des deux images est supposée être l'image haute résolution. Sur la base qu'un changement de résolution est équivalent à un e et de lissage dû à un changement d'échelle, une représentation dans l'espace d'échelle de l'image haute résolution est produite. Par conséquent, le paradigme classique d'appariement d'images une à une devient un problème d'appariement une à plusieurs parce que toutes les images dans l'espace d'échelle d'une image sont comparées à l'autre image. Une condition pour qu'une telle stratégie de comparaison marche bien est la représentation des caractéristiques d'image à toutes les échelles. On illustre comment on peut représenter et extraire des points d'intérêt à toutes les échelles et on construit une méthode pour comparer deux images ayant deux résolutions di érentes. La méthode englobe l'utilisation de descripteurs photométriques et géométriques invariants, une transformation géométrique entre les deux images ainsi qu'une stratégie d'appariement basée sur des contraintes locales et sur l'estimation robuste de cette transformation. Un grand nombre d'expérimentations montre que notre méthode est e ective pour un facteur de changement de résolution allant jusq'à 6. Mots-clés : appariement, espace d'échelle, points d'intérêt Image matching with scale adjustment 3 Figure 1: An example of an image pair with di erent resolutions: low-resolution (left) and high-resolution (right).	espace;estdomains;experiment;geometric modeling;image registration;image resolution;linear algebra;lo que tú quieras oír;one-to-many (data model);one-to-one (data model);point of interest;programming paradigm;sans institute;scale space;smoothing	Yves Dufournaud;Cordelia Schmid;Radu Horaud	2004	Computer Vision and Image Understanding	10.1016/j.cviu.2003.07.003	computer vision;mathematical optimization;point of interest;scale space;template matching;image resolution;computer science;optimal matching;mathematics;geometry	Vision	50.175511213564874	-59.069311673293264	180053
1c68aa69032258f09726d5c720f786effbe5411e	silhouette segmentation in multiple views	silicon;image segmentation;monocular color information;foreground region extraction silhouette segmentation multiple scene views monocular color information multiview spatial consistency constraint multiview image background subtraction methods;silhouette segmentation;pixel image color analysis image segmentation three dimensional displays shape cameras silicon;satisfiability;foreground region extraction;background region;three dimensional;multiple views;background subtraction methods;a priori knowledge;shape;multiview spatial consistency constraint;silhouette segmentation background region foreground region multiview silhouette consistency;three dimensional displays;image color analysis;image colour analysis;feature extraction;pixel;background subtraction;foreground region;multiview image;multiple scene views;multiview silhouette consistency;image segmentation feature extraction image colour analysis;user interaction;article;cameras;tracking	In this paper, we present a method for extracting consistent foreground regions when multiple views of a scene are available. We propose a framework that automatically identifies such regions in images under the assumption that, in each image, background and foreground regions present different color properties. To achieve this task, monocular color information is not sufficient and we exploit the spatial consistency constraint that several image projections of the same space region must satisfy. Combining the monocular color consistency constraint with multiview spatial constraints allows us to automatically and simultaneously segment the foreground and background regions in multiview images. In contrast to standard background subtraction methods, the proposed approach does not require a priori knowledge of the background nor user interaction. Experimental results under realistic scenarios demonstrate the effectiveness of the method for multiple camera set ups.	3d modeling;background subtraction;image segmentation;malignant fibrous histiocytoma;physical object;projection defense mechanism;published comment;view model;biologic segmentation	Wonwoo Lee;Woontack Woo;Edmond Boyer	2011	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2010.196	three-dimensional space;computer vision;a priori and a posteriori;background subtraction;feature extraction;shape;computer science;pattern recognition;tracking;image segmentation;silicon;pixel;satisfiability;computer graphics (images)	Vision	53.26579587334065	-53.48215865454145	180249
51d79231b0d981ead82b6e09ef7cf30df81fd425	combining pca and lfa for surface reconstruction from a sparse set of control points	object representation;perceptual quality;interpolation;3d control points;principal component analysis image reconstruction image representation interpolation;euclidean distance;surface reconstruction;interpolation scheme;euclidean distance surface reconstruction 3d control points human heads object representation principal component analysis interpolation scheme;image representation;image reconstruction;interpolation method;principal component analysis;principal component analysis surface reconstruction least squares approximation interpolation function approximation noise cancellation shape control approximation methods surface fitting humans;approximation scheme;ground truth;human heads	This paper presents a novel method for 3D surface reconstruction based on a sparse set of 3D control points. For object classes such as human heads, prior information about the class is used in order to constrain the results. A common strategy to represent object classes for a reconstruction application is to build holistic models, such as PCA models. Using holistic models involves a trade-off between reconstruction of the measured points and plausibility of the result. We introduce a novel object representation that provides local adaptation of the surface, able to fit 3D control points exactly without affecting areas of the surface distant from the control points. The method is based on an interpolation scheme, opposed to approximation schemes generally used for surface reconstruction. Our interpolation method reduces the Euclidean distance between a reconstruction and its ground truth while preserving its smoothness and increasing its perceptual quality	approximation;control point (mathematics);euclidean distance;farkas' lemma;ground truth;holism;interpolation;java platform, micro edition;pascal;plausibility structure;principal component analysis;sparse language;sparse matrix	Reinhard Knothe;Sami Romdhani;Thomas Vetter	2006	7th International Conference on Automatic Face and Gesture Recognition (FGR06)	10.1109/FGR.2006.31	computer vision;mathematical optimization;mathematics;geometry	Vision	52.76980524705089	-52.72387933021028	180436
8821e9f62e0e62036e06b080ffee8bf8440fb3d5	joint detection for potsherds of broken earthenware	image segmentation;shape image reconstruction assembly image segmentation humans information science;image reconstruction;two dimensional images joint detection potsherds broken earthenware jigsaw puzzle assembling shape assumptions partial verification contour segmentation process segment description process segment verification process candidate extraction process candidate verification process;image reconstruction image segmentation	In this paper, we propose a new strategy f o r detecting the joint among two potsherds. Joint detect ion problems were studied in jigsaw puzzle assembling. However, the shape assumptions of a piece used in the past researches cannot be applied to joint detection to reconstruct broken earthenware. To detect the joint , the most similar section among two contours must be detected by partial verification. In our strategy, each contour is segmented f o r partial verification without making any assumption about the shape of a potsherd, unlike previous jigsaw puzzle assembling methods. Our strategy consists of f ive processes: the contour segmentation process, the segment description process, the segment verification process, the candidate extraction process, and the candidate verification process. W e also present experimental results of our strategy with two-dimensional images of potsherds.	memory segmentation;sensor	Kenta Hori;Masakazu Imai;Tsukasa Ogasawara	1999		10.1109/CVPR.1999.784718	iterative reconstruction;computer vision;computer science;image segmentation	Vision	48.06329616698981	-53.52530052849556	180830
2b1436d686a5d9a612972918dee0683883167457	temporal segmentation tool for high-quality real-time video editing software	motion analysis;software;image motion analysis;image segmentation;image resolution;video signal processing;video sequences temporal segmentation tool high quality real time video editing software video content novel real time high quality shot detection strategy very fast pixel based analysis gradual transitions efficient edge based analysis false detections candidate transitions;motion analysis video editing software real time high qualitytemporal segmentation abrupt transition detection gradualtransition detection;motion segmentation;vectors;video signal processing image motion analysis image resolution image segmentation image sequences real time systems;image edge detection;vectors streaming media cameras motion segmentation image edge detection real time systems software;streaming media;gradualtransition detection;abrupt transition detection;video editing software;real time high qualitytemporal segmentation;cameras;real time systems;image sequences	The increasing use of video editing software requires faster and more efficient editing tools. As a first step, these tools perform a temporal segmentation in shots that allows a later building of indexes describing the video content. Here, we propose a novel real-time high-quality shot detection strategy, suitable for the last generation of video editing software requiring both low computational cost and high quality results. While abrupt transitions are detected through a very fast pixel-based analysis, gradual transitions are obtained from an efficient edge-based analysis. Both analyses are reinforced with a motion analysis that helps to detect and discard false detections. This motion analysis is carried out exclusively over a reduced set of candidate transitions, thus maintaining the computational requirements demanded by new applications to fulfill user needs.	algorithmic efficiency;computation;digital video;display resolution;global illumination;pixel;real-time clock;real-time transcription;requirement;sensor;shot transition detection	Carlos Cuevas;Narciso N. García	2012	IEEE Transactions on Consumer Electronics	10.1109/TCE.2012.6311337	computer vision;image resolution;computer science;multimedia;image segmentation;computer graphics (images)	Visualization	48.127163479567066	-53.76546220468384	180945
eb29effd05c1dddc63d08ac819b1d56e36175784	contour extrapolation using probabilistic cue combination	shape constraints;interpolation;human vision;turning;boundary conditions;computer model;extrapolation;psychology;optimal interpolation;shape;image edge detection;calculus;spirals;calculus of variation;humans;extrapolation shape interpolation humans psychology image edge detection calculus spirals boundary conditions turning;cue combination	A common approach to the problem of contour interpolation is based on the calculus of variations. The optimal interpolating contour is taken to be one that minimizes a given smoothness functional. Two important such functionals are total curvature (or bending energy) and variation in curvature. We analyzed contours extrapolated by human observers given arcs of Euler spirals that disappeared behind an occluding surface. Irrespective of whether the Euler spirals had increasing or decreasing curvature as they approached the occluding edge, visually-extrapolated contours were found to be characterized by decaying curvature. This curvature decay is modeled in terms of a Bayesian interaction between probabilistically-expressed constraints to minimize curvature and minimize variation in curvature. The analysis suggests that using fixed smoothness functionals is not appropriate for modeling human vision. Rather, the relative weights assigned to different probabilistic shape constraints may vary as a function of distance from the point(s) of occlusion. Implications are discussed for computational models of shape completion.	calculus of variations;computational model;contour line;euler;experiment;extrapolation;interpolation;modal logic;spatial variability	Manish Singh;Jacqueline M. Fulvio	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.61	computer simulation;mathematical optimization;topology;principal curvature;interpolation;shape;boundary value problem;mean curvature;mathematics;geometry;torsion of a curve;curvature;extrapolation;calculus of variations;radius of curvature;spiral	Vision	53.00217417122684	-54.527366436399085	181051
d1cbd116c1a353be956acfcc79138b1c24987069	scene parsing using region-based generative models	modelizacion;graph theory;estimation theory;semantic object detector;vision ordenador;belief;analisis escena;analyse scene;scene classification;critical region;probability;multimedia;scene probability estimation;outdoor scene;generic model;pavement;factor graph;probability belief maintenance computer vision estimation theory graph theory image classification object detection;scene parsing;follaje;region based scene configuration model;semantics;image classification;region critica;probabilistic approach;semantica;semantique;computer vision;chaussee;modelisation;semantic scene classification;analyse syntaxique;croyance;analisis sintaxico;object oriented;region based generative model;enfoque probabilista;approche probabiliste;syntactic analysis;feuillage;oriente objet;calzada;vision ordinateur;factor graph semantic scene classification computer vision scene parsing semantic object detector region based scene configuration model region based generative model outdoor scene pairwise relationship scene probability estimation loopy belief propagation;belief maintenance;layout detectors object detection face detection image edge detection computer science computer vision fault detection character generation belief propagation;spatial relationships;generative models;creencia;semantic features;modeling;semantic features factor graph generative models scene classification;orientado objeto;loopy belief propagation;object detection;region critique;foliage;scene analysis;pairwise relationship	"""Semantic scene classification is a challenging problem in computer vision. In contrast to the common approach of using low-level features computed from the whole scene, we propose """"scene parsing"""" utilizing semantic object detectors (e.g., sky, foliage, and pavement) and region-based scene-configuration models. Because semantic detectors are faulty in practice, it is critical to develop a region-based generative model of outdoor scenes based on characteristic objects in the scene and spatial relationships between them. Since a fully connected scene configuration model is intractable, we chose to model pairwise relationships between regions and estimate scene probabilities using loopy belief propagation on a factor graph. We demonstrate the promise of this approach on a set of over 2000 outdoor photographs, comparing it with existing discriminative approaches and those using low-level features"""	belief propagation;casio loopy;computer vision;factor graph;generative model;high- and low-level;parsing;scene graph;sensor;software propagation	Matthew R. Boutell;Jiebo Luo;Christopher M. Brown	2007	IEEE Transactions on Multimedia	10.1109/TMM.2006.886372	spatial relation;computer vision;statistical hypothesis testing;contextual image classification;systems modeling;scene statistics;computer science;graph theory;belief;machine learning;parsing;factor graph;pattern recognition;probability;semantics;estimation theory;object-oriented programming;statistics;belief propagation	Vision	46.52546225633309	-56.60955410132644	182413
19f71082e3abb91a2cdbe6b96fe40b81a60f197e	a ground truth for motion-based video-object segmentation	ground truth design procedure motion based video object segmentation critical factor classification video scripts chroma studio pixel level quality segmentation masks motion based algorithms;image motion analysis;complexity theory;image segmentation;video signal processing;image classification;motion estimation;video segmentation;motion based video object segmentation;segmentation ground truth;indexing terms;object segmentation cameras algorithm design and analysis humans licenses shape motion segmentation computer vision disk recording video recording;computer vision;accuracy;object segmentation;motion segmentation;research purpose;video object segmentation;pixel level quality segmentation masks;critical factor classification;motion based algorithms;video corpus segmentation ground truth video segmentation object segmentation;chroma studio;ground truth design procedure;video scripts;video corpus;ground truth;video signal processing image classification image motion analysis image segmentation;algorithm design and analysis;cameras	This paper describes the design procedure followed to generate a ground truth for the evaluation of motion-based algorithms for video-object segmentation. A thorough review and classification of the critical factors that affect the behavior of segmentation algorithms results in a set of video scripts which have then been filmed. Foreground objects have been recorded in a chroma studio, in order to automatically obtain pixel-level high quality segmentation masks for each generated sequence. The resulting corpus (segmentation ground-.truth plus filmed sequences mounted over different backgrounds) is available for research purposes under a license agreement.	algorithm;display resolution;ground truth;irene greif;pixel;text corpus	Fabricio Tiburzi;Marcos Escudero-Viñolo;Jesús Bescós;José María Martínez Sanchez	2008	2008 15th IEEE International Conference on Image Processing	10.1109/ICIP.2008.4711680	algorithm design;computer vision;contextual image classification;index term;ground truth;computer science;segmentation-based object categorization;motion estimation;accuracy and precision;multimedia;image segmentation;scale-space segmentation;computer graphics (images)	Robotics	47.88600250327125	-54.08032774753031	182441
944e6e779e032eda6931b3a34bb04a9e6d4ef6b1	a corner model and optimal filter	detectors;feature detection;interest point detection;edge detection;optimal filtering;interest points;optimal filter;patten recognition;artificial scene;snr;straight line edge;signal noise ratio;gauss filter;natural scene;filters image edge detection detectors image reconstruction gaussian processes noise robustness object detection motion detection computer science software engineering;image edge detection;feature extraction;object tracking;photogrammetric task;gauss filter corner model optimal filter patten recognition photogrammetric task feature detection natural scene artificial scene camera calibration object tracking pattern reconstruction interest point detection noise unit step edge function straight line edge;robustness;filtering theory edge detection feature extraction;camera calibration;noise unit step edge function;filtering theory;corner model;noise;field flow fractionation;pattern reconstruction;snr corner model optimal filter interest point detection straight line edge signal noise ratio	Patten recognition and photogrammetric tasks require models capable of detecting and measuring complex features normally found in all kind of natural and artificial scenes. Corners are the special features in images, and are of great use in computing camera calibration, object tracking and pattern reconstruction. Basically, a corner is defined as the junction points of two or more straight-line edges or curbs. In this paper, a novel optimal filter of interest point detection is presented based on a noise unit step edge function that is defined by the straight line edges. This filter is compared to other filters such as Gauss filter etc. The experimental results show that this optimal filter is robust.	camera resectioning;interest point detection;photogrammetry;robustness (computer science);sensor	Youfu Wu;Hongmei Liu;Guang Zhou	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.255	adaptive filter;computer vision;speech recognition;kernel adaptive filter;computer science;pattern recognition;filter design;signal-to-noise ratio;m-derived filter	Vision	48.165484828156906	-52.53234250644464	183175
0294bb961a093fc500c2bad2c18993168abb72a7	surface dependent representations for illumination insensitive image comparison	multiscale oriented filter;illumination insensitive image;real image;image gradient direction;surface dependent representations;material property;new mixed strategy;gabor jet;different lighting condition;general object;image comparison method;mixed strategy;illumination;material properties	We consider the problem of matching images to tell whether they come from the same scene viewed under different lighting conditions. We show that the surface characteristics determine the type of image comparison method that should be used. Previous work has shown the effectiveness of comparing the image gradient direction for surfaces with material properties that change rapidly in one direction. We show analytically that two other widely used methods, normalized correlation of small windows and comparison of multiscale oriented filters, essentially compute the same thing. Then, we show that for surfaces whose properties change more slowly, comparison of the output of whitening filters is most effective. This suggests that a combination of these strategies should be employed to compare general objects. We discuss indications that Gabor jets use such a mixed strategy effectively, and we propose a new mixed strategy. We validate our results on synthetic and real images	amoxicillin;appendix;ar (unix);arabic numeral 0;axis vertebra;blob detection;coefficient;computation (action);cylinder seal;decorrelation;dental whitening;detection theory;difference of gaussians;digital image;diphtheria-tetanus-(whole-cell) pertussis vaccine;emoticon;equivalent weight;erwinia phage vb_eamm-y2;gabor filter;hl7publishingsubsection <operations>;hantavirus sin nombre ab.igg:acnc:pt:ser:qn;hantavirus sin nombre ab.igg:titr:pt:ser:qn;histogram equalization;image gradient;image registration;matching;microsoft windows;nizatidine 75 mg oral tablet [axid ar];normal statistical distribution;optic axis of a crystal;particle filter;rough set;synthetic intelligence;tablespoon dosing unit;tellurium;antibiotic om 704a	Margarita Osadchy;David W. Jacobs;Michael Lindenbaum	2007	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2007.250602	material properties;computer vision;computer science;lighting;mathematics;geometry;statistics	Vision	53.68266720626765	-56.51109974092297	183329
cfa845546b77f6ff45a81cef4a76470ffcd72707	3-d head pose estimation for monocular image	reconnaissance visage;modelizacion;image tridimensionnelle;monocular vision;cabeza;vision ordenador;vision monoculaire;image processing;facies;auxiliary information;procesamiento imagen;traitement image;head pose estimation;computer vision;modelisation;posture;face recognition;robustesse;postura;pattern recognition;vision monocular;tridimensional image;robustness;vision ordinateur;head;reconnaissance forme;tete;reconocimiento patron;modeling;imagen tridimensional;robustez;pose estimation	3-D head pose estimation plays an important role in many applications, such as face recognition, 3-D reconstruction and so on. But it is very difficult to estimate 3-D head pose only from a single monocular image directly without other auxiliary information. This paper proposes a new human face pose estimation algorithm using a single image based on pinhole imaging theory and 3-D head rotation model. The key of this algorithm is to obtain 3-D head pose information based on the relations of projections and the positions changing of seven facial points. Experiments show the proposed method has good performance in both accuracy and robustness for human face pose estimation using only a single monocular image.	3d pose estimation	Yingjie Pan;Hong Zhu;Ruirui Ji	2005		10.1007/11540007_35	computer vision;simulation;systems modeling;pose;3d pose estimation;facies;image processing;computer science;monocular vision;articulated body pose estimation;head;robustness;computer graphics (images)	Vision	47.41484318521545	-57.7006634162364	183741
78aa54e1c3f210f85605e8612a682872d52bbe1c	from shading to local shape	eigenvalues and eigenfunctions;image representation image reconstruction;local shape descriptors;shape from shading;surface reconstruction;statistical models;shape;image reconstruction;captured photographs shape information small image patch mid level scene descriptor local shape distributions quadratic representation surface reconstruction synthetic images;lighting;transmission line matrix methods;shape lighting eigenvalues and eigenfunctions transmission line matrix methods image reconstruction surface reconstruction noise;3d reconstruction;noise	We develop a framework for extracting a concise representation of the shape information available from diffuse shading in a small image patch. This produces a mid-level scene descriptor, comprised of local shape distributions that are inferred separately at every image patch across multiple scales. The framework is based on a quadratic representation of local shape that, in the absence of noise, has guarantees on recovering accurate local shape and lighting. And when noise is present, the inferred local shape distributions provide useful shape information without over-committing to any particular image explanation. These local shape distributions naturally encode the fact that some smooth diffuse regions are more informative than others, and they enable efficient and robust reconstruction of object-scale shape. Experimental results show that this approach to surface reconstruction compares well against the state-of-art on both synthetic images and captured photographs.	diffuse reflection;encode;inference;information;quadratic function;shading;synthetic intelligence;photograph	Ying Xiong;Ayan Chakrabarti;Ronen Basri;Steven J. Gortler;David W. Jacobs;Todd E. Zickler	2015	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2014.2343211	3d reconstruction;iterative reconstruction;active shape model;statistical model;computer vision;photometric stereo;surface reconstruction;shape;computer science;noise;heat kernel signature;shape analysis;lighting;mathematics;geometry;topological skeleton;statistics	Vision	53.76066607096904	-53.4220480993798	183787
f2438a354b10d8cb78843fde331ba366d384f8d7	recognizing walking people	object recognition;analisis numerico;learning algorithm;estimation mouvement;algorithm performance;estimacion movimiento;point location;motion estimation;reconnaissance objet;algorithme apprentissage;satisfiability;three dimensional;analyse numerique;numerical analysis;resultado algoritmo;image sequence;performance algorithme;pattern recognition;reconnaissance forme;reconocimiento patron;algoritmo aprendizaje;structure from motion	We present a method for the recognition of walking people in monocular image sequences based on the extraction of coordinates of specific point locations on the body. The method works by a comparison of sequences of recorded coordinates with a library of sequences from different individuals. The comparison is based on the evaluation of view invariant and calibration independent view consistency constraints. These constraints are functions of corresponding image coordinates in two views and are satisfied whenever the two views are projected from the same three-dimensional (3D) object. By evaluating the view consistency constraints for each pair of frames in a sequence of a walking person and a stored sequence, we obtain a matrix of consistency values that ideally are zero whenever the pair of images depict the same 3D posture. The method is virtually parameter free and computes a consistency residual between a pair of sequences that can be used as a distance for clustering and classification. Using interactively extracted data we present experimental results that are superior to those of previously published algorithms both in terms of performance and generality. KEY WORDS—structure from motion, calibration, object recognition	algorithm;cluster analysis;interactivity;outline of object recognition;poor posture;statistical classification;structure from motion	Stefan Carlsson	2000		10.1007/3-540-45054-8_31	three-dimensional space;computer vision;structure from motion;numerical analysis;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;point location;motion estimation;satisfiability	Vision	49.858692170553	-56.52536655726607	185415
161b699eb05e9f46ef6d314ac6fd02e5d54f0a5a	mesh simplification with hierarchical shape analysis and iterative edge contraction	hierarchical structure;hierarchical partitioning;optimisation;iterative methods solid modelling computational geometry mesh generation optimisation;mesh simplification;shape approximation;computational geometry;shape analysis;iterative methods;shape partitioning algorithms iterative algorithms surface reconstruction algorithm design and analysis data visualization merging application software sampling methods solids;level of detail;object hierarchies;shape approximation mesh simplification hierarchical shape analysis iterative edge contraction half edge contraction optimization memoryless quadric metric error;algorithms computer graphics computer simulation image enhancement image interpretation computer assisted imaging three dimensional information storage and retrieval numerical analysis computer assisted pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted user computer interface video recording;mesh generation;journal magazine article;solid modelling	We present a novel mesh simplification algorithm. It decouples the simplification process into two phases: shape analysis and edge contraction. In the analysis phase, it imposes a hierarchical structure on a surface mesh by uniform hierarchical partitioning, marks the importance of each vertex in the hierarchical structure, and determines the affected regions of each vertex at the hierarchical levels. In the contraction phase, it also divides the simplification procedure into two steps: half-edge contraction and optimization. In the first step, memoryless quadric metric error and the importance of vertices in the hierarchical structure are combined to determine one operation of half-edge contraction. In the second step, it repositions the vertices in the half-edge simplified mesh by minimizing the multilevel synthesized quadric error on the corresponding affected regions from the immediately local to the more global. The experiments illustrate the competitive results.	algorithm;contract agreement;edge contraction;experiment;iterative method;level of detail;mathematical optimization;polygon mesh;shape analysis (digital geometry);text simplification;vertex (geometry)	Jingqi Yan;Pengfei Shi;David Zhang	2004	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2004.1260766	mesh generation;computer vision;mathematical optimization;computational geometry;computer science;theoretical computer science;machine learning;level of detail;shape analysis;mathematics;geometry;iterative method	Visualization	53.733288298206205	-58.51896446501031	185548
78d13ed612e31764efe677c617adb27c93c90677	camera calibration and geo-location estimation from two shadow trajectories	coordonnee geographique;contraste;altura;coordenada geografica;vision ordenador;ombre;metrologia;measurement;ombroscopie;localizacion objeto;location estimation;object location;metric;metrologie;orientation;metrology;shadowscopy;conico;rectification;computer vision;sombrascopia;hauteur;sombra;restauracion propiedad;geographical coordinate;shadow;geographical location;medida;shadow trajectory;orientacion;metric rectification;analemmatic sundial;restauration propriete;metrico;vision ordinateur;etalonnage;mesure;rectificacion;camera calibration;localisation objet;conics;conique;calibration;metrique;height;recovery properties	1077-3142/$ see front matter 2010 Elsevier Inc. A doi:10.1016/j.cviu.2010.04.003 * Corresponding author. E-mail address: jolinwulin1986@gmail.com (L. Wu The position of a world point’s solar shadow depends on its geographical location, the geometrical relationship between the orientation of the sunshine and the ground plane where the shadow casts. This paper investigates the property of solar shadow trajectories on a planar surface and shows that camera parameters, latitude, longitude and shadow casting objects’ relative height ratios can be estimated from two observed shadow trajectories. One contribution is to recover the horizon line and metric rectification matrix from four observations of two shadow tips. The other contribution is that we use the design of an analemmatic sundial to get the shadow conic and furthermore recover the camera’s geographical location. The proposed method does not require the shadow casting objects or a vertical object to be visible in the recovery of camera calibration. This approach is thoroughly validated on both synthetic and real data, and tested against various sources of errors including noise, number of observations, objects locations, and camera orientations. We also present applications to image-based metrology. 2010 Elsevier Inc. All rights reserved.	camera resectioning;closed-circuit television;geolocation;image rectification;location (geography);shadow copy;shadow volume;stationary process;synthetic data;synthetic intelligence	Lin Wu;Xiaochun Cao;Hassan Foroosh	2010	Computer Vision and Image Understanding	10.1016/j.cviu.2010.04.003	computer vision;camera auto-calibration;shadow;height;calibration;camera resectioning;metric;mathematics;geometry;conic section;orientation;location;metrology;measurement;rectification	Vision	51.026123601029596	-57.01085632425429	185750
f87f5dc610c3c0996db05dde5d89d8a93c52e0e9	a practical algorithm for structure and motion recovery from long sequence of images	analisis imagen;vision ordenador;affine projection;estimation mouvement;reconstitution forme;estimacion movimiento;algoritmo recursivo;visual navigation;motion estimation;computer vision;camera motion;algorithme recursif;reconstitucion forma;image sequence;image analysis;vision ordinateur;recursive algorithm;pattern recovery;structure and motion;analyse image;real time application	In this paper we present an algorithm for structure and motion (SM) recovery under affine projection from video sequences that is suitable for real time applications. The algorithm tracks the motion of a single structure, be it an object or the entire scene itself, allowing for any type of camera motion. This could be used for example to track the motion of a vehicle in a warehouse (single object, static camera) or for visual navigation from a moving platform (track scene from moving camera). The algorithm requires a set of features to be detected in each frame, and that at least four features are correctly matched between each three consecutive frames. Compared to previous algorithms, this novel algorithm has a lower computational cost, dynamically detects outliers and allows for previously lost features to reappear in the sequence. The algorithm has been tested on real image sequences, and compared to other algorithms we have found that our algorithm has both a smaller error and a lower computational time.	algorithm	Miroslav Trajkovic;Mark Hedley	1997		10.1007/3-540-63507-6_234	computer vision;match moving;structure from motion;image analysis;simulation;computer science;motion estimation;motion field;recursion;computer graphics (images)	Vision	48.996451885604706	-56.810446099267566	185768
ad39eba2aa92a8d81dc6f9fa5bfe2122adc159b8	efficient combination of histograms for real-time tracking using mean-shift and trust-region optimization	moving object;optimisation;pistage;optimizacion;occlusion;intervalo confianza;real time tracking;real time;occultation;corps mobile;rastreo;oclusion;mean shift;confidence interval;histogram;trust region;histogramme;object oriented;intervalle confiance;temps reel;cuerpo movil;object tracking;poursuite cible;pattern recognition;tiempo real;oriente objet;optimization;temps retard;moving body;reconnaissance forme;delay time;target tracking;reconocimiento patron;ocultacion;histograma;orientado objeto;tiempo retardo;tracking	Histogram based real-time object tracking methods, like th e MeanShift tracker of Comaniciu/Meer or the Trust-Region tracke r of Liu/Chen, have been presented recently. The main advantage is that a suited histogram allows for very fast and accurate tracking of a moving object even in the case of partial occlusions and for a moving camera. The problem is which hist ogram shall be used in which situation. In this paper we extend the framewor k of histogram based tracking. As a consequence we are able to formulate a tracker that uses a weighted combination of histograms of different features. We compar e our approach with two already proposed histogram based trackers for differen t historgrams on large test sequences availabe to the public. The algorithms run in real-time on standard PC hardware.	adaptive histogram equalization;algorithm;entity–relationship model;experiment;feature model;lecture notes in computer science;mathematical optimization;mean shift;real-time clock;shadow volume;springer (tank);trust region	Ferid Bajramovic;Christoph Gräßl;Joachim Denzler	2005		10.1007/11550518_32	computer vision;simulation;confidence interval;mean-shift;occultation;computer science;histogram matching;video tracking;balanced histogram thresholding;histogram;tracking;trust region;adaptive histogram equalization;object-oriented programming;statistics;image histogram	Vision	47.902114670737355	-56.87306632123292	185977
8c533fed7ce3307571f15327900401512a49feb0	object tracking using background subtraction and motion estimation in mpeg videos	subtraction;moving object;vision ordenador;pistage;estimation mouvement;image processing;plane motion;mpeg video;transformation cosinus discrete;estimacion movimiento;corps mobile;sustraccion;rastreo;procesamiento imagen;soustraction;motion estimation;traitement image;computer vision;histogram;compression image;senal video;histogramme;signal video;image compression;discrete cosine transforms;motion vector;cuerpo movil;object tracking;background subtraction;poursuite cible;robust method;mouvement plan;video signal;vision ordinateur;moving body;target tracking;histograma;movimiento plano;tracking;compresion imagen	We present a fast and robust method for moving object tracking directly in the compressed domain using features available in MPEG videos. DCT domain background subtraction in Y plane is used to locate candidate objects in subsequent I-frames after a user has marked an object of interest in the given frame. DCT domain histogram matching using Cb and Cr planes and motion vectors are used to select the target object from the set of candidate objects. The target object position is finally interpolated in the predicted frames to obtain a smooth tracking across GOPs.	algorithmic efficiency;background subtraction;computation;data compression;discrete cosine transform;histogram matching;interpolation;motion compensation;motion estimation;moving picture experts group	Ashwani Aggarwal;Susmit Biswas;Sandeep Singh;Shamik Sural;Arun K. Majumdar	2006		10.1007/11612704_13	computer vision;background subtraction;subtraction;image processing;image compression;computer science;video tracking;motion estimation;histogram;mathematics;tracking;multimedia;computer graphics (images)	Vision	48.920787021840994	-56.7982706482473	187188
888d61b84e82a0a0f043d102e768ab35e76aaaae	object tracking using multiple camera video streams	systeme temps reel;moving object;transmission longue distance;video streaming;image processing;0705p;multi view technology;surveillance;0130c;localization;angle observation;viewing angle;imagerie;localizacion;blanco movil;traitement image;motion tracking;transmision larga distancia;wireless sensor network;algorithme;detection objet;accuracy;detection mouvement;imagery;imaging system;precision;localisation;signal video;long distance;sensor networks;video cameras;robustesse;object tracking;poursuite cible;camera video;deteccion movimiento;traffic analysis;cible mobile;algorithms;robustness;angulo observacion;imagineria;video signals;video;target tracking;4230v;modeling;long distance transmission;motion detection;technologie multi vues;cameras;moving target;imaging systems;sequence image;object detection;object model;robustez;real time systems;image sequences	Two synchronized cameras are utilized to obtain independent video streams to detect moving objects from two different viewing angles. The video frames are directly correlated in time. Moving objects in image frames from the two cameras are identified and tagged for tracking. One advantage of such a system involves overcoming effects of occlusions that could result in an object in partial or full view in one camera, when the same object is fully visible in another camera. Object registration is achieved by determining the location of common features in the moving object across simultaneous frames. Perspective differences are adjusted. Combining information from images from multiple cameras increases robustness of the tracking process. Motion tracking is achieved by determining anomalies caused by the objects' movement across frames in time in each and the combined video information. The path of each object is determined heuristically. Accuracy of detection is dependent on the speed of the object as well as variations in direction of motion. Fast cameras increase accuracy but limit the speed and complexity of the algorithm. Such an imaging system has applications in traffic analysis, surveillance and security, as well as object modeling from multi-view images. The system can easily be expanded by increasing the number of cameras such that there is an overlap between the scenes from at least two cameras in proximity. An object can then be tracked long distances or across multiple cameras continuously, applicable, for example, in wireless sensor networks for surveillance or navigation.	streaming media	Mehrübe Mehrübeoglu;Diego A. Rojas;Lifford McLauchlan	2010		10.1117/12.854091	computer vision;simulation;wireless sensor network;image processing;video tracking;accuracy and precision;three-ccd camera;computer graphics (images)	Vision	49.109648682589125	-56.78453502553662	187572
844908fc98daa3c22bd079892a14a30a5491c067	a bayesian approach to model matching with geometric hashing	tratamiento paralelo;hachage;modelizacion;algoritmo paralelo;image features;hypercube;complejidad espacio;image recognition;object recognition;reconocimiento imagen;parallel algorithm;quad tree;modele mathematique;image processing;traitement parallele;bayesian approach;multiprocessor;time complexity;maximum likelihood;complexite calcul;binary image;geometrie algorithmique;quad arbol;parallel manipulator;maximum vraisemblance;computational geometry;procesamiento imagen;modelo matematico;interseccion;image bruitee;traitement image;algorithme parallele;modelisation;imagen sonora;complejidad computacion;complexite temps;hashing;degeneration;algorithme pram;indexing;computational complexity;hash table;feature extraction;noisy image;model matching;indexation;image binaire;reconnaissance image;indizacion;space complexity;mathematical model;pattern recognition;approche bayes;imagen binaria;quad arbre;geometria computacional;hash function;reconnaissance forme;weight function;complexite espace;reconocimiento patron;multiprocesador;intersection;complejidad tiempo;pram algorithm;modeling;article;maxima verosimilitud;parallel processing;multiprocesseur;hipercubo	Geometric hashing methods provide an efficient approach to indexing from image features into a database of models. The hash functions that have typically been used involve quantization of the values, which can result in nongraceful degradation of the performance of the system in the presence of noise. Intuitively, it is desirable to replace the quantization of hash values and the resulting binning of hash entries by a method that gives increasingly less weight to a hash table entry as a hashed feature becomes more distant from the hash entry position. In this paper, we show how these intuitive notions can be translated into a well-founded Bayesian approach to object recognition and give precise formulas for the optimal weight functions that should be used in hash space. These extensions allow the geometric hashing method to be viewed as a Bayesian maximum-likelihood framework. We demonstrate the validity of the approach by performing similarity-invariant object recognition using models obtained from drawings of military aircraft and automobiles and test images from real-world grayscale images of the same aircraft and automobile types. Our experimental results represent a complete object recognition system, since the feature extraction process is automated. Our system is scalable and works rapidly and very efficiently on an 8K-processor CM - 2, and the quality of results using similarity-invariant model matching is excellent.	geometric hashing	Isidore Rigoutsos;Robert A. Hummel	1995	Computer Vision and Image Understanding	10.1006/cviu.1995.1038	feature hashing;parallel processing;computer vision;hash table;double hashing;hash function;linear hashing;perfect hash function;dynamic perfect hashing;primary clustering;image processing;computational geometry;computer science;artificial intelligence;consistent hashing;theoretical computer science;machine learning;universal hashing;mathematics;k-independent hashing;rolling hash;suha;2-choice hashing;algorithm;hash tree;hash filter	Vision	47.46238807555149	-58.9669078698244	187725
17d2db1e271ebf82147ecb3c6fc101d0c46d8314	differential algorithm for the determination of shape from shading using a point light source	point light source;estimacion;ombre;shape from shading;restauration image;analyse surface;surface plan;image restoration;algorithme;algorithm;restauracion imagen;reconstruction image;sombra;estimation;reconstruccion imagen;shadow;image reconstruction;analisis superficie;pattern recognition;surface lambertienne;superficie;surface;reconnaissance forme;local shading analysis;reconocimiento patron;surface analysis;algoritmo	This paper presents a linear algorithm for recovering shape information of Lambertian surfaces from the .xhading information inherent in a single 20 image. The local orientation of a planar Lambertian surface patch is Jetermined from a single video image when both camera ,rnd a light source are near to the surface. A simple linear relationship is derived between the ratios of the measured image intensities and their first partial derivatives and the components of the surface normal vector. .4n advantage of the formulation is that the resulting quations are independent of the intensity strength of the rlluminating source as well as the surface reflection coefficient (albedo). Results for noisy synthetic images as well as real images are presented.	algorithm;approximation;gaussian blur;glossary of computer graphics;gradient;illumination (image);lambertian reflectance;normal (geometry);numerical analysis;photometric stereo;pixel;randomness;reflection coefficient;shading;synthetic intelligence	H. U. Rashid;Peter Burger	1992	Image Vision Comput.	10.1016/0262-8856(92)90006-O	iterative reconstruction;image restoration;computer vision;estimation;shadow;photometric stereo;computer science;surface weather analysis;surface;computer graphics (images)	Vision	51.67507564372535	-56.97472221687014	188289
57d33c0f8d6998d665a7ec6672a56cf8e7729c14	detection of facial characteristics based on edge information	low resolution;facial feature detection;ellipse fitting	In this paper, a novel method for eye and mouth detection and eye center and mouth corner localization, based on geometrical information is presented. First, a face detector is applied to detect the facial region, and the edge map of this region is extracted. A vector pointing to the closest edge pixel is then assigned to every pixel. x andy components of these vectors are used to detect the eyes and mouth. For eye center localization, intensity information is used, after removing unwanted effects, such as light reflections. For the detection of the mouth corners, the hue channel of the lip area is used. The proposed method can work efficiently on low-resolution images and has been tested on the XM2VTS database with very good results.	computation;computational semantics;emoticon;failure;feature detection (computer vision);feature detection (web development);pixel;reflection (computer graphics)	Stylianos Asteriadis;Nikos Nikolaidis;Ioannis Pitas;Montse Pardàs	2007			computer vision;image resolution;computer science;pattern recognition	Vision	52.675842925164844	-55.75566824735329	188574
169623e01f55d22dc5a8c3775336c4da34036b91	image texture prediction using colour photometric stereo	prediccion;eficacia sistema;image recognition;reconocimiento imagen;image resolution;texture image;performance systeme;surface texture;system performance;image texture;photometry;photometric stereo;reconnaissance image;photometrie;imagen color;prediction;resolution image;fotometria;image couleur;color image	The purpose of this work is to analyse what happens to the surface information when the image resolution is modi ed. We deduce how the surface texture appears if seen from di erent distances. Using Colour Photometric Stereo a method for predicting how surface texture looks like when changing the distance of the camera is presented. We use this technique on the recognition of textures seen from di erent distances. Real sets of images have been used in order to evaluate the performance of the recognition system.	computer vision;image resolution;image texture;lambertian reflectance;photometric stereo;test data	Xavier Lladó;Joan Martí;Maria Petrou	2002		10.1007/3-540-36079-4_31	image texture;surface finish;computer vision;photometric stereo;image resolution;color image;prediction;photometry;computer science;computer performance;computer graphics (images)	Vision	52.93221236796937	-56.785694694369	188690
9fa223327a558ab6efdaa5a8d0b53225397e9634	heteroscedastic projection based m-estimators	parameter estimation noise robustness computer vision maximum likelihood estimation vectors computer errors computer society pattern recognition;computer society;projection pursuit;maximum likelihood estimation;noise robustness;computer vision;vectors;pattern recognition;robust regression;parameter estimation;computer errors	Robust regression methods, such as RANSAC, suffer from a sensitivity to the scale parameter used for generating the inlier-outlier dichotomy. Projection based M-estimators (pbM) offer a solution to this by reframing the regression problem in a projection pursuit framework. In this paper we modify the pbM formulation to obtain an improved pbM algorithm. Furthermore, the modified algorithm is easily generalized to handle heteroscedastic data . The superior performance of heteroscedastic pbM, as compared to simple pbM, is experimentally verified.	algorithm;coefficient;conjugate gradient method;curve fitting;epipolar geometry;estimation theory;experiment;fundamental matrix (computer vision);iteration;netpbm format;polynomial;random sample consensus;time complexity	Raghav Subbarao;Peter Meer	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05) - Workshops	10.1109/CVPR.2005.467	projection pursuit;econometrics;computer science;machine learning;mathematics;maximum likelihood;estimation theory;robust regression;statistics	Vision	51.354725510159355	-52.39779637774265	189752
4e2cedb1e5d7b146ac50a0a59f95d7e708ea7e0f	invariant feature set in convex hull for fast image registration	hausdorff distance invariant feature set convex hull fast image registration extractable features convex diagonal convex quadrilateral geometric invariance;set theory feature extraction image registration;extractable features;set theory;convex diagonal;feature extraction;image registration;fast imaging;hausdorff distance;image registration pattern matching computer vision shape measurement data mining image segmentation euclidean distance feature extraction layout noise robustness;fast image registration;invariant feature set;convex quadrilateral;convex hull;similarity measure;real time computing;invariant feature;geometric invariance	In this paper, a novel feature set in images for registration is identified. Unique, geometrically invariant and easily extractable features in images called convex diagonal, convex quadrilateral are used for accurate image registration. Convex diagonals, convex quadrilaterals have attractive properties like easy extraction, geometric invariance and frequent occurrence. Coordinates, length and orientation information of corresponding convex diagonals in different images is used for initial transformation estimate. Corresponding convex hulls of scene objects are matched using Hausdorff distance as similarity measure operator. Coarse level estimate facilitates efficient, real time computation for final registration process. Initial transformation estimate based on convex diagonals, extracted from convex hull of scene objects, is refined using fine level image details to minimize errors originating from quantization and same convex hull information for different object shapes. The behavior of reference quadrilateral is robust against noise, outliers and broken edges.	algorithm;computation;convex hull;estimation theory;geographic coordinate system;hausdorff dimension;image fusion;image registration;parameter (computer programming);robustness (computer science);similarity measure	Rashid Minhas;Q. M. Jonathan Wu	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414078	support function;hausdorff distance;mathematical optimization;conic optimization;convex optimization;convex polytope;topology;convex combination;orthogonal convex hull;convex body;feature extraction;convex conjugate;computer science;image registration;convex hull;absolutely convex set;mathematics;geometry;convex set;convex curve;effective domain;proper convex function;set theory;choquet theory	Vision	50.15955697856255	-53.773442698342635	191258
e513cdeb8eac4833e99f8b80a982508517ac38c3	kona: a multi-junction detector using minimum description length principle	object recognition;image understanding;minimum description length principle	"""Corners, T-, Y-, X-junctions give vital depth cues which is a critical aspect of image \understanding"""": junctions form an important class of features invaluable in most vision systems. The three main issues in a junction (or any feature) detector are: scale, location, and, the junction (feature) parameters. The junction parameters are (1) the radius, or size, of the junction, (2) the kind of junction: lines, corners, 3-junctions such as T or Y, or, 4-junction such as X-junction, etcetra, (3) angles of the wedges, and, (4) intensity in each of the wedges. Our main contribution in this paper is a modeling of the junction (using the minimum description length principle), which is complex enough to handle all the three issues and simple enough to admit an eeective dynamic programming solution. Kona is an implementation of this model. Similar approach can be used to model other features like thick edges, blobs and end-points."""	dynamic programming;minimum description length	Laxmi Parida;Davi Geiger;Robert A. Hummel	1997		10.1007/3-540-62909-2_72	cognitive neuroscience of visual object recognition;pattern recognition;mathematics;geometry	Vision	47.00486700938522	-54.88882635357465	191866
0a5d5f359614a5cb9f42f5b9e2ee6409975703e2	multi-view face segmentation using fusion of statistical shape and appearance models	machine abstraite;modelizacion;ajustamiento modelo;vision ordenador;image segmentation;analisis estadistico;image processing;automovil;facies;shape model;estimation method;segmentacion de imagenes;point to point;image matching;asm;multi view segmentation;maquina abstracta;database;procesamiento imagen;base dato;angulo rotacion;probabilistic approach;aam;traitement image;statistical model;abstract machine;computer vision;multiple view;ajustement modele;modelisation;model fusion;statistical analysis;reconocimiento de patrones en imagenes;automobile;enfoque probabilista;approche probabiliste;motor car;model matching;image sequence;rotation angle;analyse statistique;segmentation image;base de donnees;vue multiple;vision ordinateur;angle rotation;secuencia imagen;frontal;shape modeling;modeling;face segmentation;appariement image;sequence image;vista multiple	This paper demonstrates how a weighted fusion of multiple Active Shape (ASM) or Active Appearance (AAM) models can be utilized to perform multi-view facial segmentation with only a limited number of views available for training the models. The idea is to construct models only from frontal and profile views and subsequently fuse these models with adequate weights to segment any facial view. This reduces the problem of multi-view facial segmentation to that of weight estimation, the algorithm for which is proposed as well. The evaluation is performed on a set of 280 landmarked static face images corresponding to seven different rotation angles and on several video sequences of the AV@CAR database. The evaluation demonstrates that the estimation of the weights does not have to be very accurate in the case of ASM, while in the case of AAM the influence of correct weight estimation is more critical. The segmentation with the proposed weight estimation method produced accurate segmentations in 91% of 280 testing images with the median point-to-point error varying from two to eight pixels (1.8– 7.2% of average inter-eye distance). 2009 Elsevier Inc. All rights reserved.	active appearance model;algorithm;human body weight;interpolation;lateral thinking;maxima and minima;memory segmentation;pixel;point-to-point protocol;test set	Constantine Butakoff;Alejandro F. Frangi	2010	Computer Vision and Image Understanding	10.1016/j.cviu.2009.11.001	statistical model;computer vision;systems modeling;facies;image processing;point-to-point;computer science;artificial intelligence;abstract machine;image segmentation	Vision	47.529851814915524	-57.70444793898436	191993
288867d402c28886a44b56d1e692bb491058c4d5	automatic detection and tracking of human heads using an active stereo vision system	vision system;cabeza;vision ordenador;pistage;tracking system;illumination;systeme vision;maximum likelihood;traitement image stereoscopique;methode echelle multiple;hierarchized structure;head tracking;maximum vraisemblance;rastreo;structure hierarchisee;metodo escala multiple;canal vertical;computer vision;human face;automatic detection;detecteur tete humaime;heart tracking;pattern matching;stereo image processing;stereo vision;detection automatique;vision ordinateur;multiscale method;head;vision active;concordance forme;vertical channel;tete;face detection;multiscale;visage humain;eclairement;template matching;article;detection face;estructura jerarquizada;maxima verosimilitud;tracking;maximum likelihood head detector;alumbrado;active vision	A new head tracking algorithm for automatically detecting and tracking human heads in complex backgrounds is proposed. By using an elliptical model for the human head, our Maximum Likelihood (ML) head detector can reliably locate human heads in images having complex backgrounds and is relatively insensitive to illumination and rotation of the human heads. Our head detector consists of two channels: the horizontal and the vertical channels. Each channel is implemented by multiscale template matching. Using a hierarchical structure in implementing our head detector, the execution time for detecting the human heads in a 512×512 image is about 0.02 second in a Sparc 20 workstation (not including the time for image acquisition). Based on the ellipse-based ML head detector, we have developed a head tracking method that can monitor the entrance of a person, detect and track the person's head, and then control the stereo cameras to focus their gaze on this person's head. In this method, the ML head detector and the mutually-supported constraint are used to extract the corresponding ellipses in a stereo image pair. To implement a practical and reliable face detection and tracking system, further verification using facial features, such as eyes, mouth and nostrils, may be essential. The 3D position computed from the centers of the two corresponding ellipses is then used for fixation. An active stereo head has been used to perform the experiments and has demonstrated that the proposed approach is feasible and promising for practical uses.	stereopsis	C.-Y. Tang;Z. Chen;Y.-P. Hung	2000	IJPRAI	10.1142/S0218001400000118	computer vision;face detection;simulation;template matching;active vision;machine vision;tracking system;computer science;stereopsis;pattern matching;tracking;maximum likelihood;head;computer graphics (images)	Vision	48.48788157744308	-57.51191120084599	192483
4eb13c81e94bddde78fa143e8544fd9aa27db858	a study on the dual vanishing point property	image tridimensionnelle;dual vanishing line;vision ordenador;image processing;perspective projection;procesamiento imagen;3d coplanar lines;interseccion;traitement image;parallel image lines;ligne coplanaire;computer vision;parallel imaging;dual vanishing point;binocular vision;vanishing point;projection perspective;tridimensional image;vision ordinateur;linea coplanaria;intersection;proyeccion perspectiva;article;true or false intersection;imagen tridimensional;coplanar line	"""Vanishing points and vanishing lines are useful information in computer vision. In this study, an interesting dual property of vanishing point is """"rst introduced. Next, we point out that there also exists a dual property of vanishing line. With the dual vanishing point and vanishing line properties, some 3D intersection inference can be made based on their image lines. Two applications are given to illustrate the usage of the new results. The """"rst one is to derive the 3D pose determination of a circle using two parallel image lines. The second one uses six specially designed 3D lines to adjust the cameras with respect to a """"xture in a binocular vision system such that the resultant camera coordinate axes become parallel. ( 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	binocular vision;computer vision;image plane;line–line intersection;pattern recognition;resultant;vanishing point	Jen-Bin Huang;Zen Chen;Jenn-Yih Lin	1999	Pattern Recognition	10.1016/S0031-3203(99)00002-3	binocular vision;computer vision;perspective;vanishing point;image processing;intersection;mathematics;geometry	Vision	50.35712084904261	-57.755594580568065	192651
fec1ccb35103a9ce5d252ad3d2a02ac8aeb8333c	creating panoramas on mobile phones	presentation ligne appelante;software;moving object;mosaicism;mobile device;imagen fija;mosaicisme;telephone portable;implementation;presentation de la ligne apellante;0130c;cell phones;identificacion de llamada entrante;blanco movil;radiocommunication service mobile;mobile phone;fixed image;signal video;mobile radio;video recording;mobile handsets;on the fly;cible mobile;image fixe;video signals;video;enregistrement video;4230v;calling line identification presentation;mobile devices;matlab;moving target;sequence image;image mosaicing;image sequences	Image stitching is used to combine several images into one wide-angled mosaic image. Traditionally mosaic images have been constructed from a few separate photographs, but nowadays that video recording has become commonplace even on mobile phones, it is possible to consider also video sequences as a source for mosaic images. However, most stitching methods require vast amounts of computational resources that make them unusable on mobile devices. We present a novel panorama stitching method that is designed to create high-quality image mosaics from both video clips and separate images even on low-resource devices. The software is able to create both 360 degree panoramas and perspective-corrected mosaics. Features of the software include among others: detection of moving objects, inter-frame color balancing and rotation correction. The application selects only the frames of highest quality for the final mosaic image. Low-quality frames are dropped on the fly while recording the frames for the mosaic. The complete software is implemented on Matlab, but also a mobile phone version exists. We present a complete solution from frame acquisition to panorama output with different resource profiles that suit various platforms.	algorithm;color balance;computational resource;image stitching;matlab;mobile device;mobile phone;on the fly;symbian;usability;video clip	Jani Boutellier;Miguel Bordallo López;Olli Silvén;Marius Tico;Markku Vehviläinen	2007		10.1117/12.703527	computer vision;telecommunications;image stitching;mobile device;multimedia;computer graphics (images)	Vision	48.55656903516907	-56.09801723147612	192929
e21a8c34d58c1a7dedaa9dd83c8c1cab11a233fa	bilateral filtering-based optical flow estimation with occlusion detection	bilateral filtering;modelizacion;television;calculo de variaciones;filtering;ajustamiento modelo;discontinuity;vision ordenador;filtrage;discontinuite;balance energetico;mise a jour;filtro optico;detecteur image;proceso difusion;image processing;bilan energetique;occlusion;flux optique;spatial coherence;occultation;filtrado;adaptive control;processus diffusion;procesamiento imagen;diffusion anisotrope;optical flow estimation;oclusion;anisotropic diffusion;traitement image;energy function;computer vision;blurring image;actualizacion;ajustement modele;modelisation;calcul variationnel;control proceso;motion blur;variational approach;senal video;signal video;flujo optico;difusion anisotropica;flow field;optical filter;model matching;energy balance;process control;filtro adaptable;energy loss;video signal;vision ordinateur;optical flow;detector imagen;diffusion process;discontinuidad;anisotropic scattering;filtre adaptatif;perdida aparente de definicion;ocultacion;modeling;adaptive filter;commande processus;image sensor;variational calculus;filtre optique;updating;floutage	Using the variational approaches to estimate optical flow between two frames, the flow discontinuities between different motion fields are usually not distinguished even when an anisotropic diffusion operator is applied. In this paper, we propose a multi-cue driven adaptive bilateral filter to regularize the flow computation, which is able to achieve the smoothly varied optical flow field with highly desirable motion discontinuities. First, we separate the traditional one-step variational updating model into a two-step filtering-based updating model. Then, employing our occlusion detector, we reformulate the energy functional of optical flow estimation by explicitly introducing an occlusion term to balance the energy loss due to the occlusion or mismatches. Furthermore, based on the twostep updating framework, a novel multi-cue driven bilateral filter is proposed to substitute the original anisotropic diffusion process, and it is able to adaptively control the diffusion process according to the occlusion detection, image intensity dissimilarity, and motion dissimilarity. After applying our approach on various video sources (movie and TV) in the presence of occlusion, motion blurring, non-rigid deformation, and weak textureness, we generate a spatial-coherent flow field between each pair of input frames and detect more accurate flow discontinuities along the motion boundaries.	anisotropic diffusion;bilateral filter;calculus of variations;coherence (physics);computation;experiment;hidden surface determination;optical flow;smoothing;variational principle	Jiangjian Xiao;Hui Cheng;Harpreet S. Sawhney;Cen Rao;Michael A. Isnardi	2006		10.1007/11744023_17	filter;adaptive filter;computer vision;systems modeling;adaptive control;occultation;image processing;computer science;discontinuity;diffusion process;process control;image sensor;optical flow;optical filter;bilateral filter;television;anisotropic diffusion;calculus of variations	Vision	51.4761021815595	-57.18693014943192	193332
a0d13b8ff3d2e4d21fef2360b502e774ee4b2441	reconstruction of a symmetrical object from its perspective image	analisis imagen;formation image tridimensionnelle;image processing;3d imaging;perspective projection;extraction forme;vanishing points;procesamiento imagen;traitement image;reconstruction image;extraccion forma;reconstruccion imagen;image reconstruction;projection perspective;image analysis;formacion imagen tridimensional;proyeccion perspectiva;analyse image;pattern extraction	Abstract   This paper introduces an approach for reconstructing a symmetrical 3D object from a single perspective image, by determining the eye point of the perspective image and the symmetrical plane of the object presented in the image, then each time choosing a pair of symmetrical points on the object, and calculating their 3D coordinates with the position relations to the eye point and the symmetrical plane. All of the operations are performed in a 3D coordinate system that is constructed by the analysis on the position distribution of vanishing points in the perspective image.		Xiuwei Zhao;Sun Jiaguang	1994	Computers & Graphics	10.1016/0097-8493(94)90058-2	iterative reconstruction;stereoscopy;computer vision;perspective;image analysis;vanishing point;image processing;computer science;mathematics;geometry	Vision	50.44769368756152	-57.80642705170064	193874
4fc00498433e2bf17c91897e2a096f93ea5c2d9e	simple and robust hard cut detection using interframe differences	modelizacion;background modeling;estimation mouvement;gestalt theory;image processing;estimacion movimiento;procesamiento imagen;teoria gestalt;motion estimation;grande deviation;probabilistic approach;theorie gestalt;traitement image;modelisation;gran desviacion;enfoque probabilista;approche probabiliste;pattern recognition;reconnaissance forme;reconocimiento patron;modeling;large deviation	In this paper we introduce a simple method for the detection of hard cuts using only interframe differences. The method is inspired in the computational gestalt theory. The key idea in this theory is to define a meaningful event as large deviation from the expected background process. That is, an event that has little probability to occur given a probabilistic background model. In our case we will define a hard cut when the interframe differences have little probability to be produced by a given model of interframe differences of non-cut frames. Since we only use interframe differences, there is no need to perform motion estimation, or other type of processing, and the method turns to be very simple with low computational cost. The proposed method outperforms similar methods proposed in the literature.	algorithmic efficiency;background process;computation;formal methods;gestalt psychology;heuristic (computer science);motion estimation;normalization (image processing);online and offline;shot transition detection;thresholding (image processing)	Alvaro Pardo	2005		10.1007/11578079_43	computer vision;systems modeling;image processing;computer science;artificial intelligence;motion estimation;mathematics;algorithm	Vision	47.118132130357246	-58.044960849892654	193950
fe97430ed36be54b037b6b96d41c80415125a273	an evolutionary algorithm for the registration of 3-d surface representations	representation;image tridimensionnelle;eficacia sistema;optimisation;sistema experto;image processing;optimizacion;surface representation;etude experimentale;logique floue;performance systeme;procesamiento imagen;logica difusa;intelligence artificielle;algoritmo genetico;system performance;traitement image;fuzzy logic;optimization problem;neuro fuzzy;neuro fuzzy system;estimacion parametro;algorithme genetique;pattern recognition;superficie;tridimensional image;artificial intelligence;algorithme evolutionniste;genetic algorithm;surface;algoritmo evolucionista;optimization;inteligencia artificial;reconnaissance forme;systeme expert;evolutionary algorithm;parameter estimation;estimation parametre;reseau neuronal;reconocimiento patron;estudio experimental;red neuronal;imagen tridimensional;fitness function;representacion;neural network;expert system	The registration of 3-D surface representations is an important task for the recognition of objects and for the fusion of different views (reconstruction). Finding the transformation parameters that optimally align two non-calibrated segmented scene descriptions is a difficult, complex optimization problem. In this paper an Evolutionary Algorithm (EA) is presented which offers a solution to the registration problem. The fitness function which estimates the quality of the transformation parameters is based on single surface comparisons achieved by a neuro-fuzzy system. We demonstrate some registration experiments with synthetic and real scene descriptions, showing the robustness of the registration with respect to segmentation noise and partial visibility. ( 1999 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.	align (company);autonomous system (internet);boundary representation;care-of address;evolutionary algorithm;experiment;fitness function;fuzzy control system;global optimization;gradient descent;mathematical optimization;multistage interconnection networks;neuro-fuzzy;nonlinear programming;nonlinear system;optimization problem;pattern recognition;synthetic intelligence	Daniel Fischer;Peter Kohlhepp;Frank Bulling	1999	Pattern Recognition	10.1016/S0031-3203(98)00090-9	fuzzy logic;optimization problem;genetic algorithm;image processing;computer science;artificial intelligence;neuro-fuzzy;machine learning;evolutionary algorithm;estimation theory;surface;representation;fitness function;expert system;algorithm	Vision	49.991108089799646	-58.73639547730763	194337
321d5964ba334b8ef042d6d9b85bfb8cd5d77ce3	application of the total least squares procedure to linear view interpolation	view interpolation;least square method;laboratory tests;total least square;independent and identically distributed;image reconstruction;least square	It is shown that, in comparison to the results obtained from a conventional least squares approach, a total least squares solution leads to significant improvements in the geometry and appearance of images synthesised in a linear combination of views procedure. Use of the total least squares criterion is appropriate when errors on the control points are independently and identically distributed between the basis images and the target image being synthesised. When this is not be the case it is pointed out that the generalised total least squares procedure should be used. A synthetic object is used to evaluate the improvement in geometric accuracy obtained by use of the total least squares solution in comparison to a classical least squares method. Simulated and real images of laboratory test objects are similarly used to illustrate the improvement in appearance of images reconstructed by means of the total least squares procedure.	interpolation;synthetic data;total least squares	D. M. Kennedy;A. F. Buxton;J. H. Gilby	1999		10.5244/C.13.31	generalized least squares;total least squares;econometrics;mathematical optimization;mathematics;non-linear least squares;least squares;statistics;recursive least squares filter	Vision	51.73869323643242	-52.095228432061106	194457
a37ba11a0ad8d915a1e0f42a76f38674fa09a04a	joint segmentation and image interpretation	comprension imagen;vision ordenador;algorithm performance;image processing;modelo markov;etude experimentale;procesamiento imagen;segmentation;traitement image;markov random field;computer vision;algorithme;algorithm;image interpretation;markov model;campo aleatorio;interpretacion imagen;resultado algoritmo;performance algorithme;comprehension image;vision ordinateur;interpretation image;modele markov;scene understanding;image comprehension;estudio experimental;segmentacion;champ aleatoire;scene analysis;algoritmo;random field	Abstract   The problem of image interpretation is formulated in the framework of modular integration and multiresolution. The formulation essentially involves the concept of reductionism and multiresolution, where the image interpretation task is broken down into simpler subtasks of segmentation and interpretation. Moreover, instead of solving the vision task at the finest resolution Ω, we solve the synergetically coupled vision subtasks at coarser resolutions   Ω  −ξ   for   Ω  ⩾ξ>0   and use the results obtained at resolution   (  Ω  −ξ)   to solve the vision task at resolution   (  Ω  −ξ+1)  . We present a solution to the joint segmentation and interpretation problem in the proposed framework. For the interpretation part we exploit the Markov random field (MRF) based image interpretation scheme developed by Modestino and Zhang [ IEEE Trans Pattern Anal. Mach. Intell . pp. 606–615 (1992)]. Experimental results on both indoor and outdoor images are presented to validate the proposed framework.		K. Sunil Kumar;Uday B. Desai	1999	Pattern Recognition	10.1016/S0031-3203(98)00006-5	computer vision;random field;image processing;computer science;artificial intelligence;mathematics;markov model;segmentation;algorithm;statistics	Vision	47.94556772922717	-58.18337635743099	194920
6d1f49dad5c692d3fbc576675ff07cb1655c1d74	recognition and localization of generic objects for indoor navigation using functionality	modelizacion;navegacion;image tridimensionnelle;architecture systeme;image processing;complexite calcul;procesamiento imagen;robot navigation;robotics;traitement image;repesentation fonctionnelle;modelisation;navigation;complejidad computacion;functional representation;computational complexity;function representation;pattern recognition;spatial representation;superficie;robotica;tridimensional image;arquitectura sistema;surface;geometric model;robotique;reconnaissance forme;reconocimiento patron;system architecture;modeling;imagen tridimensional	Abstract   Autonomous indoor navigation requires a robot to have abilities to recognize landmarks and to avoid obstacles. Our goal is to provide these abilities in a generic way so that the robot need not have an accurate and complete geometric model of all the objects in its environment. Rather, we wish to provide abilities to recognize common objects such as desks and doors which can be used as landmarks. We use a functionality-based representation for objects. Objects consist of functional parts, which are characterized by their significant surfaces, and by the accompanying functional evidence for them. Our system works with planar surfaces only and assumes that the objects are in a ‘standard’ pose. The localization and orientation of an object are represented with the most significant surface in an ‘s-map’. Our system has been tested on a number of real scenes where it performs robustly and efficiently; some results are shown in the paper.	internationalization and localization	Dongsung Kim;Ramakant Nevatia	1998	Image Vision Comput.	10.1016/S0262-8856(98)00067-5	computer vision;navigation;systems modeling;image processing;computer science;geometric modeling;function representation;robotics;computational complexity theory;surface	Vision	47.91844791893593	-58.769408098399225	195222
578fc331cc01891710c7662e0a04b4e7af268314	structural rectification of non-planar document images: application to graphics recognition	image recognition;reconocimiento imagen;traitement image document;representation graphique;high resolution;document analysis;image processing;algorithm analysis;perspective correction;representacion grafica;digital camera;document graphique;joint triangulation;analyse documentaire;documento grafico;view synthesis;triangulacion;reconocimiento grafico;traitement document;reconnaissance image;document image processing;analisis documental;analyse algorithme;triangulation;document pre processing;reconnaissance graphique;document processing;graphics recognition;graphic document;analisis algoritmo;graphics;graphical recognition;tratamiento documento	Document analysis and graphics recognition algorithms are normally applied to the processing of images of 2D documents scanned when flattened against a planar surface. Technological advancements in recent years have led to a situation in which digital cameras with high resolution are widely available. Consequently, traditional graphics recognition tasks may be updated to accommodate document images captured through a camera in an uncontrolled environment. In this paper the problem of document image rectification is discussed. The rectification targets the correction of perspective and geometric distortions of document images taken from uncalibrated cameras, by synthesizing new views which are better suited for existing graphics recognition and document analysis techniques. The proposed approach targets cases in which the document is not necessarily flat, without relaying on specific modeling assumptions, and by utilizing one or more overlapping views of the document. Document image rectification results are provided for several cases.	graphics;image rectification	Gady Agam;Changhua Wu	2001		10.1007/3-540-45868-9_25	computer vision;document processing;image resolution;image processing;triangulation;computer science;graphics;document layout analysis;multimedia;computer graphics (images)	Vision	50.30676262782728	-57.752428429624864	195301
216facfbc4e584db6dfaadb9d9b6c225b1b24984	multimodal estimation of discontinuous optical flow using markov random fields	motion analysis;synthetic image sequences;analisis imagen;vision ordenador;bayesian estimation;theoretical framework;global statistical models;bayes methods;image motion analysis markov random fields optical sensors motion estimation layout optical computing image sequences motion analysis rendering computer graphics bayesian methods;champ aleatoire markov;markov random fields;optical flow estimation;analyse mouvement;synthetic image sequences multimodal estimation discontinuous optical flow markov random fields dense velocity fields ill posed problem motion boundaries occlusion regions motion estimation visual motion flow estimation bayesian estimation global statistical models gradient based motion constraint equations feature based motion constraint equations deterministic relaxation algorithms real world image sequences;motion estimation;motion boundaries;discontinuous optical flow;indexing terms;analyse multiresolution;statistical model;markov random field;velocity field;visual motion;relajacion;computer vision;flow estimation;traitement occlusion;gradient based motion constraint equations;deterministic relaxation algorithms;multiple constraints;campo aleatorio;real world image sequences;dense velocity fields;contrainte multiple;ill posed problem;image sequence;occlusion processing;statistics;map estimation;bayesian estimator;relaxation;image analysis;vision ordinateur;optical flow;markov processes;feature based motion constraint equations;occlusion regions;multimodal estimation;multiresolution analysis;analyse image;statistics bayes methods image sequences markov processes motion estimation;champ aleatoire;image sequences;multiple constraint;flot optique;random field	The estimation of dense velocity fields from image sequences is basically an ill-posed problem, primarily because the data only partially constrain the solution. It is rendered especially difficult by the presence of motion boundaries and occlusion regions which are not taken into account by standard regularization approaches. In this paper, we present a multimodal approach to the problem of motion estimation in which the computation of visual motion is based on several complementary constraints. It is shown that multiple constraints can provide more accurate flow estimation in a wide range of circumstances. The theoretical framework relies on bayesian estimation associated with global statistical models, namely, Markov Random Fields. The constraints introduced here aim to address the following issues: optical flow estimation while preserving motion boundaries, processing of occlusion regions, fusion between gradient and feature-based motion constraint equations. Deterministic relaxation algorithms are used to merge information and to provide a solution to the max imum a posteriori estimation of the unknown dense motion field. The algorithm is well suited to a multiresolution implementation which brings an appreciable speed-up as well as a significant improvement of estimation when large displacements are present in the scene. Experiments on synthetic and real world image sequences are reported.	algorithm;computation;experiment;gradient;linear programming relaxation;markov chain;markov random field;matrix regularization;motion estimation;motion field;multimodal interaction;multiresolution analysis;optical flow;statistical model;synthetic intelligence;velocity (software development);well-posed problem;world file	Fabrice Heitz;Patrick Bouthemy	1993	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.250841	multiresolution analysis;statistical model;computer vision;mathematical optimization;random field;vector field;image analysis;index term;bayes estimator;computer science;relaxation;motion estimation;optical flow;mathematics;markov process;motion field;statistics	Vision	52.206835000860565	-56.05816686610967	195401
2d82e45e1602e4068e604869cd9233535ebc0c2c	multi-view hair capture using orientation fields	multiple depth maps;hair styles multiview hair capture orientation fields realistic 3d hair geometry reconstruction omnipresent occlusions complex discontinuities specular appearance multiview hair reconstruction algorithm structure aware aggregation hair color appearance local hair orientation mrf matching energy multiple depth maps mrf optimization template refinement procedure color based methods;mrf matching energy;standards;complex discontinuities;image matching;color based methods;specular appearance;geometry;surface reconstruction;hair styles;realistic 3d hair geometry reconstruction;multiview hair reconstruction algorithm;image colour analysis;image reconstruction;stereo vision;multiview hair capture;hair geometry image reconstruction cameras surface reconstruction stereo vision standards;structure aware aggregation;omnipresent occlusions;local hair orientation;template refinement procedure;hair color appearance;image reconstruction geometry image colour analysis image matching;cameras;mrf optimization;orientation fields;hair	Reconstructing realistic 3D hair geometry is challenging due to omnipresent occlusions, complex discontinuities and specular appearance. To address these challenges, we propose a multi-view hair reconstruction algorithm based on orientation fields with structure-aware aggregation. Our key insight is that while hair's color appearance is view-dependent, the response to oriented filters that captures the local hair orientation is more stable. We apply the structure-aware aggregation to the MRF matching energy to enforce the structural continuities implied from the local hair orientations. Multiple depth maps from the MRF optimization are then fused into a globally consistent hair geometry with a template refinement procedure. Compared to the state-of-the-art color-based methods, our method faithfully reconstructs detailed hair structures. We demonstrate the results for a number of hair styles, ranging from straight to curly, and show that our framework is suitable for capturing hair in motion.	3d computer graphics;algorithm;baseline (configuration management);color;depth map;epipolar geometry;ground truth;ibm notes;markov random field;mathematical optimization;refinement (computing);robot;skolem normal form;synthetic data	Linjie Luo;Hao Li;Sylvain Paris;Thibaut Weise;Mark Pauly;Szymon Rusinkiewicz	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247838	iterative reconstruction;computer vision;surface reconstruction;computer science;stereopsis;mathematics;geometry	Vision	53.73809549910449	-53.08877458976631	195895
85035b76254cb1101fad6b75d2249ec624fc73fb	exploiting transitivity of correlation for fast template matching	autocorrelacion;sensitivity and specificity;metodo correlacion;algorithm performance;coeficiente correlacion;imaging three dimensional;cross correlation;video signal processing;correlation method;image matching;efficient algorithm;correlation croisee;localization;information technology;natural images;testing;localizacion;correlation based match measures;fast template matching;algorithme;signal processing computer assisted;brightness;algorithm;image enhancement;video signal processing image matching;localisation;entire solution;current measurement;image interpretation computer assisted;data dependence;computational complexity;resultado algoritmo;pattern matching;statistics as topic;engineering management;poursuite cible;reproducibility of results;performance algorithme;pattern recognition;algorithms;robustness;pattern recognition automated;normalized cross correlation auto correlation correlation coefficient cross correlation fast template matching;concordance forme;reconnaissance forme;intrareference local autocorrelation;computer science;subtraction technique;target tracking;reconocimiento patron;algorithms image enhancement image interpretation computer assisted imaging three dimensional pattern recognition automated reproducibility of results sensitivity and specificity signal processing computer assisted statistics as topic subtraction technique;correlation coefficient;computational efficiency;auto correlation;template matching;coefficient correlation;elimination algorithm fast template matching elimination algorithms exhaustive search correlation based match measures intrareference local autocorrelation natural images;autocorrelation computational efficiency current measurement testing information technology computer science engineering management computational complexity brightness robustness;elimination algorithms;methode correlation;elimination algorithm;autocorrelation;exhaustive search;correlacion cruzada;normalized cross correlation;algoritmo	Elimination Algorithms are often used in template matching to provide a significant speed-up by skipping portions of the computation while guaranteeing the same best-match location as exhaustive search. In this work, we develop elimination algorithms for correlation-based match measures by exploiting the transitivity of correlation. We show that transitive bounds can result in a high computational speed-up if strong autocorrelation is present in the dataset. Generally strong intrareference local autocorrelation is found in natural images, strong inter-reference autocorrelation is found if objects are to be tracked across consecutive video frames and strong intertemplate autocorrelation is found if consecutive video frames are to be matched with a reference image. For each of these cases, the transitive bounds can be adapted to result in an efficient elimination algorithm. The proposed elimination algorithms are exact, that is, they guarantee to yield the same peak location as exhaustive search over the entire solution space. While the speed-up obtained is data dependent, we show empirical results of up to an order of magnitude faster computation as compared to the currently used efficient algorithms on a variety of datasets.	algorithm;appendix;autocorrelation;brute-force search;clinical use template;coefficient;computation (action);euclidean distance;excretory function;feasible region;frame (physical object);physical object;silo (dataset);social inequality;template matching;vertex-transitive graph	Arif Mahmood;Sohaib Khan	2010	IEEE Transactions on Image Processing	10.1109/TIP.2010.2046809	autocorrelation;computer science;cross-correlation;machine learning;pattern recognition;mathematics;information technology;algorithm;statistics	ML	49.65119046376883	-53.73056992833342	196073
633093afc5f3ac64d1c2e74f45a9f5d096e80948	determination of vehicle density from traffic images at day and nighttime	teletrafic;systeme temps reel;nonlinear optics;image processing;binary image;0130c;vehicle detection;intensidad elevada;taillights;traitement image;headlamps;algorithme;telecommunication traffic;differentiation;internet;differenciation;roads;pixel;field of view;descarga;image binaire;intensite elevee;optique non lineaire;imagen binaria;algorithms;downloading;diferenciacion;high intensity;4230v;matlab;cars;cameras;telechargement;real time systems	In this paper we extend our previous work to address vehicle differentiation in traffic density computations 1 . The main goal of this work is to create vehicle density history for given roads under different weather or light conditions and at different times of the day. Vehicle differentiation is important to account for connected or otherwise long vehicles, such as trucks or tankers, which lead to over-counting with the original algorithm. Average vehicle size in pixels, given the magnification within the field of view for a particular camera, is used to separate regular cars and long vehicles. A separate algorithm and procedure have been developed to determine traffic density after dark when the vehicle headlights are turned on. Nighttime vehicle recognition utilizes blob analysis based on head/taillight images. The high intensity of vehicle lights are identified in binary images for nighttime vehicle detection. The stationary traffic image frames are downloaded from the internet as they are updated. The procedures are implemented in MATLAB. The results of both nighttime traffic density and daytime long vehicle identification algorithms are described in this paper. The determination of nighttime traffic density, and identification of long vehicles at daytime are improvements over the original work 1 .		Mehrübe Mehrübeoglu;Lifford McLauchlan	2007		10.1117/12.700982	nonlinear optics;the internet;simulation;upload;field of view;binary image;telecommunications;image processing;differentiation;optics;physics;pixel	Vision	48.590338548132785	-56.73110588730448	196361
59c24ecd8ef12a0dc96976223b454ac1cb19820b	research on computer vision-based for uav autonomous landing on a ship	imageria termica;uav;evaluation performance;vision ordenador;transformation affine;radiacion infrarroja;performance evaluation;autonomous system;real time;evaluacion prestacion;unmanned aerial vehicle;arriere plan;rayonnement ir;journal;sistema autonomo;computer vision;engin volant autonome;background;infrared radiation;affine moment invariants;thermal imaging;maquina autonoma voleando;affine transformation;systeme autonome;autonomous landing;imagerie thermique;vision ordinateur;basic research;transformacion afin;moment invariant	In this paper, a novel approach to UAV’s automatic landing on the ship’s deck is proposed. We present the design of the cooperative object, and then begin our basic research on UAV autonomous landing on a ship by using computer vision and affine moment invariants. We analyze the infrared radiation images in our experiments by extracting the target from the background and then recognizing it. Also, we calculate the angle of yaw. We study the basic research concerning automatic UAV navigation and landing on the deck. Based on our experiments, the average recognition time is 17.2 ms which is obtained through the use of affine moment invariants. This type of speed is expected to improve the reliability and real-time performance of autonomous UAV landing. 2009 Elsevier B.V. All rights reserved.	autonomous robot;computer vision;experiment;real-time transcription;unmanned aerial vehicle;yaws	Guili Xu;Yong Zhang;Shengyu Ji;Yuehua Cheng;Yupeng Tian	2009	Pattern Recognition Letters	10.1016/j.patrec.2008.12.011	computer vision;simulation;infrared;computer science;autonomous system;affine transformation;mathematics;geometry	AI	48.48053762082254	-57.50053749637837	197260
8c3a8e61615345617725110142f2de22e6b1467e	vectorial computation of the optical flow in color image sequences	flow tensor;vectorial optical flow;optical flow	Actually in most applications, optical flow is computed from one luminance Y-plane and only a few methods refer to color optical flow. In fact, a brief analysis shows that these methods are either marginal approaches, whether dramatically time consuming techniques. Here, we propose a vectorial approach based upon a joint analysis of a structure tensor and a so called flow tensor, both computed from image derivatives	color image;computation;optical flow	Bertrand Augereau;Benoit Tremblais;Christine Fernandez-Maloigne	2005			computer vision;computer science;optical flow;mathematics;geometry;optics	Vision	53.18608608886367	-53.93661464814764	198161
a6fe24a99873e31e35a19f63d33063fd2baa8d14	parameter adjustment for a dynamic programming track-before-detect-based target detection algorithm	signal image and speech processing;dynamic programming;imageria termica;ajustamiento modelo;programacion dinamica;deteccion blanco;dynamic program;blanco movil;detection cible;algorithme;ajustement modele;algorithm;quantum information technology spintronics;thermal imaging;model matching;image sequence;poursuite cible;programmation dynamique;track before detect;cible mobile;imagerie thermique;secuencia imagen;rapport signal bruit;relacion senal ruido;target tracking;signal to noise ratio;target detection;moving target;sequence image;algoritmo	This paper addresses the problem of tracking a dim moving point target in a sequence of IR images. The proposed tracking system, based on the track-before-detect (TBD) approach, is designed to track and detect dim maneuvering targets from an image sequence under low SNR conditions; a dynamic programming algorithm (DPA) is used to process the frames in the sequence. This paper deals with the practical issues of setting the parameters of our algorithm to maximize tracking capability. In our flexible approach, we are able to accommodate different levels of noise and different target velocities and mobilities. Our algorithm is tested on real data to determine its efficacy.	algorithm;dynamic programming;pixel;preprocessor;sampling (signal processing);signal-to-noise ratio;sparse matrix;track-before-detect;tracking system;tree accumulation;velocity (software development)	Ofir Nichtern;Stanley R. Rotman	2008	EURASIP J. Adv. Sig. Proc.	10.1155/2008/146925	track-before-detect;simulation;computer science;artificial intelligence;dynamic programming;signal-to-noise ratio;algorithm	Vision	48.782941187229156	-56.93487918530702	198775
d0f209db1528945bd88903379c2359f2677ff6ce	multi-objective genetic programming for object detection	detectors;genetic program;training;false alarm rate;alternative object detection programs;genetic programming;decision maker;coin photographs;multiple objectives;coin photographs multiobjective genetic programming object classification large image object localisation decision maker alternative object detection programs;shape;object detection training pixel shape detectors faa genetic programming;large image;pixel;detection rate;multiobjective genetic programming;genetic algorithms;faa;object classification;object localisation;object detection genetic algorithms;object detection	In object detection, the goals of successfully discriminating between different kinds of objects (object classification) and accurately identifying the positions of all objects of interest in a large image (object localisation) are potentially in conflict. We propose a Multi-Objective Genetic Programming (MOGP) approach to the task of providing a decision-maker with a diverse set of alternative object detection programs that balance between high detection rate and low false-alarm rate. Experiments on two datasets, simple shapes and photographs of coins, show that it is difficult for a Single-Objective GP (SOGP) system (which weights the multiple objectives a priori) to evolve effective object detectors, but that an MOGP system is able to evolve a range of effective object detectors more efficiently.	central processing unit;experiment;genetic programming;object detection;sensor;standard of good practice;two-phase commit protocol	Thomas Liddle;Mark Johnston;Mengjie Zhang	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586072	genetic programming;computer vision;decision-making;detector;method;object-class detection;genetic algorithm;shape;computer science;artificial intelligence;viola–jones object detection framework;object-oriented design;machine learning;pattern recognition;constant false alarm rate;pixel	Vision	46.58931395048603	-54.23867465473083	199145
698483d95cbb65e7ac9e72966b195564c0f4623b	key frame extraction from unstructured consumer video clips	estensibilidad;0705k;evaluation performance;personal communication networks;performance evaluation;0705;telecommunication sans fil;zoom lenses;clip video;confidence measure;digital camera;segmentation;representation sous forme image;radiocommunication service mobile;digital cameras;camera motion;data analysis;reseau communication personnel;telecomunicacion sin hilo;mobile radio;analyse donnee;ground truth;video clips;frame based representation;extensibilite;scalability;video;analisis semantico;analyse semantique;segmentacion;cameras;extraction method;temporal change;semantic analysis;wireless telecommunication	We present a key frame extraction method dedicated to summarize unstructured consumer video clips acquired from digital cameras. Analysis of spatio-temporal changes over time provides meaningful information about the scene and the cameraman's general intents. First, camera and object motion are estimated and used to derive motion descriptors. A video is segmented into homogeneous segments based on major types of camera motion (e.g., pan, zoom, pause, steady). Dedicated rules are used to extract candidate key frames from each segment. Confidence measures are computed for the candidates to enable ranking in semantic relevance. This method is scalable so that we can produce any desired number of key frames from the candidates. We demonstrated the effectiveness of our method by comparing results with the ground truth agreed by multiple judges.	key frame;video clip	Christophe Papin;Jiebo Luo	2007		10.1117/12.704373	computer vision;scalability;video;telecommunications;ground truth;computer science;multimedia;data analysis;segmentation;computer graphics (images)	NLP	48.29596200431853	-55.776929599839384	199154
74d866c0d442c0d5c1e2ea699c8304cafe86f74d	hierarchical partial matching and segmentation of interacting cells	separate cell boundary;interacting cell;individual cell boundary;multiple cell interaction;multiple touching cell;hierarchical partial matching;shortest path algorithm;individual boundary;elastic partial matching problem;computed shortest path tree;partial-curve matching problem;shortest path problem	We propose a method that automatically tracks and segments living cells in phase-contrast image sequences, especially for cells that deform and interact with each other or clutter. We formulate the problem as a many-to-one elastic partial matching problem between closed curves. We introduce Double Cyclic Dynamic Time Warping for the scenario where a collision event yields a single boundary that encloses multiple touching cells and that needs to be cut into separate cell boundaries. The resulting individual boundaries may consist of segments to be connected to produce closed curves that match well with the individual cell boundaries before the collision event. We show how to convert this partial-curve matching problem into a shortest path problem that we then solve efficiently by reusing the computed shortest path tree. We also use our shortest path algorithm to fill the gaps between the segments of the target curves. Quantitative results demonstrate the benefit of our method by showing maintained accurate recognition of individual cell boundaries across 8068 images containing multiple cell interactions.	amazon elastic compute cloud (ec2);amiga walker;clutter;data collection;diane;dynamic time warping;experiment;ibm notes;in-phase and quadrature components;interaction;matching;norm (social);one-to-many (data model);short;shortest path problem;track (course);walkers;collision	Zheng Wu;Danna Gurari;Joyce Y. Wong;Margrit Betke	2012	Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention	10.1007/978-3-642-33415-3_48	mathematical optimization;combinatorics;discrete mathematics;mathematics	Vision	49.26574415407178	-54.08544698694638	199207
137fd30d7da0a429ebba88d646de8315470329d5	robust region grouping via internal patch statistics	optimisation;image segmentation;statistical analysis;image representation;feature extraction;image segmentation semantics vectors feature extraction image color analysis noise optimization;statistical analysis feature extraction image representation image segmentation natural scenes optimisation;segmentation accuracy robust region grouping internal patch statistics multiscale low rank representation image segmentation input image partitioning optimal super pixel pair affinity matrix low level super pixel features image noises low rank refined affinity matrix natural images local small size image patterns semantic region internal image statistics real image databases cross scale consistency constraint optimization procedure;natural scenes	In this work, we present an efficient multi-scale low-rank representation for image segmentation. Our method begins with partitioning the input images into a set of super pixels, followed by seeking the optimal super pixel-pair affinity matrix, both of which are performed at multiple scales of the input images. Since low-level super pixel features are usually corrupted by image noises, we propose to infer the low-rank refined affinity matrix. The inference is guided by two observations on natural images. First, looking into a single image, local small-size image patterns tend to recur frequently within the same semantic region, but may not appear in semantically different regions. We call this internal image statistics as replication prior, and quantitatively justify it on real image databases. Second, the affinity matrices at different scales should be consistently solved, which leads to the cross-scale consistency constraint. We formulate these two purposes with one unified formulation and develop an efficient optimization procedure. Our experiments demonstrate the presented method can substantially improve segmentation accuracy.	affinity analysis;autostereogram;constraint satisfaction problem;copper–tungsten;database;experiment;high- and low-level;image segmentation;mathematical optimization;pixel;processor affinity;scene statistics	Xiaobai Liu;Liang Lin;Alan L. Yuille	2013	2013 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2013.252	image texture;computer vision;feature detection;range segmentation;binary image;image processing;feature extraction;computer science;morphological gradient;machine learning;segmentation-based object categorization;free boundary condition;pattern recognition;image moment;mathematics;region growing;image segmentation;minimum spanning tree-based segmentation;scale-space segmentation	Vision	51.86960241990219	-53.788813706437125	199722
2f5a39efa1868919d44fc07a5d74cec7f62f2009	automatic mars rover detection from multi-temporal hirise imagery	mars;azimuth;hirise phase correlation change detection illumination variation;distortion;remote sensing;robustness;image registration astronomical image processing;lighting correlation robustness azimuth mars distortion remote sensing;lighting;pc based pixel wise image co registration automatic mars rover detection multitemporal hirise imagery phase correlation based change detection algorithm temporal hirise images illumination variation camera motion;correlation	In this paper, we present a Phase Correlation (PC) based change detection algorithm which is able to automatically detect very small target such as the Mars rover in multi-temporal HiRISE images under illumination variation and camera motion. The robustness of PC to illumination variation was proved by our previous work while the geometric distortion caused by camera motion can be eliminated by PC-based pixel-wise image co-registration. Experiments using three HiRISE images taken under different illumination and view angle demonstrate that our algorithm can robustly detect the Mars rover as small as 6×9 pixels in conditions of varying illumination and camera 3D status.	algorithm;distortion;phase correlation;pixel;rover (the prisoner)	Xue Wan;Jian Guo Liu;Gareth Morgan;Hongshi Yan	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326456	computer vision;mars exploration program;distortion;computer science;lighting;azimuth;optics;correlation;physics;robustness;remote sensing	Vision	53.71055301370226	-56.045692489069566	199796
