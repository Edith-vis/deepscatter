id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
5ef7ec6faeed4591f5d3e8baadbceb0c262fc3d5	evaluation of bottlenecks in an educational cloud environment	virtual machine;ubuntu enterprise cloud uec cloud educational environment;computer aided instruction;virtual machining;cloud;educational environment;servers;ubuntu enterprise cloud uec;monitoring;servers cloud computing virtual machining educational institutions monitoring buildings;educational institutions cloud computing computer aided instruction;high performance;universities bottlenecks educational cloud environment;buildings;cloud computing	Many companies and universities make use of a Cloud-based educational environment. Such environments, however, usually require a high-performance server, making them rather expensive. In this study, we use low-cost PCs to construct a Cloud-based educational environment. But compared with a high-performance server, performance drops as the number of instances increases. In this paper, we investigate the bottlenecks in running instances.	bonnie++;cloud computing;cron;environment variable;graphical user interface;hypervisor;iscsi;input/output;server (computing);solid-state drive;upload	Shinichiro Kibe;Minoru Uehara;Motoi Yamagiwa	2011	2011 Third International Conference on Intelligent Networking and Collaborative Systems	10.1109/INCoS.2011.87	cloud computing security;simulation;cloud computing;computer science;operating system;cloud testing;world wide web	HPC	-32.58435503879	55.518101242766235	178665
b3bb03418ed2f8471e23434c6b9ec4a6cbd7fb62	mobile cloud computing and other mobile technologies: survey	different technology;digital device;technology mobile cloud computing;mobile user;desktop computing;resulting technology;cloud computing;mobile computing;ubiquitous computing;mobile technology;mobile cloud computing	Cloud computing is a resulting technology from many fields of computing. The concept core of cloud computing is to get services and processing capacity over the Internet. This technology reduces cost, increase storage, automate systems, and introduce flexibility and mobility of information. Many technologies have been emerged from the cloud computing such as mobile cloud computing. Mobile cloud computing is a combination between mobile computing and cloud computing, aims at providing optimal services for mobile users. Because mobile computing includes using computers during the movement from place to place to provide users with their maximum need, they have the ability to access other computer, other digital and portable devices around them. The emergence of, nearly similar, technology that deal with this issue is called ubiquitous computing. Ubiquitous computing implies making the digital devices available while they are effectively invisible to users. Its aim is to break away from the desktop computing to provide computational services to a user when and where required. These technologies are limited and encountered several challenges. Mobile cloud computing will support these technologies and solve most of these challenges. In this paper, we survey the resulted technology mobile cloud computing, and we explore different technologies that preceded it such as cloud computing, mobile computing and ubiquitous computing.	mobile cloud computing	Amal Abunaser;Sawsan Alshattnawi	2013	J. Mobile Multimedia		computer simulation;cloud computing security;mobile search;context-aware pervasive systems;simulation;cloud computing;computer science;mobile technology;end-user computing;traffic flow;traffic conflict;cloud testing;distributed computing;utility computing;services computing;mobile computing;world wide web;provisioning;grid computing;computer network;autonomic computing	HCI	-32.07701903853963	60.36743672405343	179287
1991b7e12bb9a59ef60dba46cf6c01514a81740a	jovaku: globally distributed caching for cloud database services using dns	dns;distributed caching;dynamodb cloud database services distributed caching dns;dynamodb;databases servers mobile communication relays libraries protocols availability;cloud database services	Cloud database services are a convenient building block for emerging mobile cloud applications. A central database can simplify application architectures by serving both as a reliable point of contact and as a repository for critical state. Meanwhile, the issues of availability and scalability can be delegated to the cloud service provider. The convenience of this approach is balanced by associated costs, both in terms of latency and financial expenses. Hence, an attractive middle ground is to employ caching of data in a layer between applications and the cloud, to reduce the load imposed on the cloud database service. This paper presents Jovaku, a generic caching layer for cloud database services that can induce significant performance improvements and cost savings. Jovaku demonstrates the viability of a truly global caching infrastructure by building on the existing DNS system. Database operations are relayed through the DNS protocol, allowing results to be cached in DNS servers close to client devices. This greatly simplifies deployment, and offers supreme availability, allowing devices anywhere to benefit from database caching. Our evaluation shows that the latency to access Amazon DynamoDB is significantly reduced for requests that hit the cache, and that applications can benefit from caching with hit rates as low as 5%.	amazon dynamodb;cache (computing);cloud database;database caching;distributed cache;mobile cloud computing;scalability;software deployment	Robert Pettersen;Steffen Viken Valvåg;Åge Kvalnes;Dag Johansen	2014	2014 2nd IEEE International Conference on Mobile Cloud Computing, Services, and Engineering	10.1109/MobileCloud.2014.20	computer science;operating system;database;world wide web;domain name system;computer network	DB	-27.77238660813832	58.295632884112386	179526
60fa4f32755fecd3fdcdf2365e5845ade820baec	special section: grid and pervasive computing 2009	mashups;pervasive computing;distributed computing;workflow;security;grid computing;sensing web;cloud computing	This Special Section of Future Generation Computer Systems contains selected high-quality papers from the 4th International Conference on Grid and Pervasive Computing (GPC 2009), which was held in May 2009 in Geneva, Switzerland, and its related workshops. Research problems in these papers have been analyzed systematically, and for specific approaches or models a detailed evaluation was performed to demonstrate their feasibility and advantages. The papers were selected on this basis and also peer reviewed thoroughly. © 2010 Elsevier B.V. All rights reserved.	computer;switzerland;ubiquitous computing	Massimo Cafaro;Henning Müller;Nabil Abdennadher	2011	Future Generation Comp. Syst.	10.1016/j.future.2010.11.019	workflow;cloud computing;computer science;information security;operating system;end-user computing;distributed computing;utility computing;world wide web;ubiquitous computing;grid computing;mashup	HPC	-32.50917430150748	53.77747068500394	180402
d315db3b72a779102a6685941756425d7618e0d7	from devops to bizops: economic sustainability for scalable cloud applications		Virtualization of resources in cloud computing has enabled developers to commission and recommission resources at will and on demand. This virtualization is a coin with two sides. On one hand, the flexibility in managing virtual resources has enabled developers to efficiently manage their costs; they can easily remove unnecessary resources or add resources temporarily when the demand increases. On the other hand, the volatility of such environment and the velocity with which changes can occur may have a greater impact on the economic position of a stakeholder and the business balance of the overall ecosystem. In this work, we recognise the business ecosystem of cloud computing as an economy of scale and explore the effect of this fact on decisions concerning scaling the infrastructure of web applications to account for fluctuations in demand. The goal is to reveal and formalize opportunities for economically optimal scaling that takes into account not only the cost of infrastructure but also the revenue from service delivery and eventually the profit of the service provider. The end product is a scaling mechanism that makes decisions based on both performance and economic criteria and takes adaptive actions to optimize both performance and profitability for the system.	business ecosystem;cloud computing;devops;elasticity (data store);experiment;heart rate variability;itil;image scaling;programming paradigm;scalability;software as a service;software system;traffic exchange;velocity (software development);volatility;web application	Marios Fokaefs;Cornel Barna;Marin Litoiu	2017	TAAS	10.1145/3139290	distributed computing;virtualization;service delivery framework;service provider;management science;profitability index;scalability;computer science;devops;environmental economics;cloud computing;business ecosystem	Web+IR	-28.929112220263836	59.21782803023438	181454
7fa8ee14ae01b0350ed1772ace27a4622ae0452b	nebulas: using distributed voluntary resources to build clouds	possible solution;voluntary resource;current cloud model;current cloud service;viable cloud paradigm;data movement cost;voluntary cloud;cloud service;end-user host;less-managed cloud	Current cloud services are deployed on wellprovisioned and centrally controlled infrastructures. However, there are several classes of services for which the current cloud model may not fit well: some do not need strong performance guarantees, the pricing may be too expensive for some, and some may be constrained by the data movement costs to the cloud. To satisfy the requirements of such services, we propose the idea of using distributed voluntary resources—those donated by end-user hosts—to form nebulas: more dispersed, lessmanaged clouds. We first discuss the requirements of cloud services and the challenges in meeting these requirements in such voluntary clouds. We then present some possible solutions to these challenges and also discuss opportunities for further improvements to make nebulas a viable cloud paradigm.	cloud computing;programming paradigm;requirement	Abhishek Chandra;Jon B. Weissman	2009			simulation;management science	DB	-29.542619582329824	59.19920263779949	182306
5e7381ebecb6ece31f418ec55ecb8e55786fd89b	information logistics and fog computing: the ditas* approach		Data-intensive applications are usually developed based on Cloud resources whose service delivery model helps towards building reliable and scalable solutions. However, especially in the context of Internet of Things-based applications, Cloud Computing comes with some limitations as data, generated at the edge of the network, are processed at the core of the network producing security, privacy, and latency issues. On the other side, Fog Computing is emerging as an extension of Cloud Computing, where resources located at the edge of the network are used in combination with cloud services. The goal of this paper is to present the approach adopted in the recently started DITAS project: the design of a Cloud platform is proposed to optimize the development of data-intensive applications providing information logistics tools that are able to deliver information and computation resources at the right time, right place, with the right quality. Applications that will be developed with DITAS tools live in a Fog Computing environment, where data move from the cloud to the edge and vice versa to provide secure, reliable, and scalable solutions with excellent performance.	cloud computing;data-intensive computing;fog computing;itil;industry 4.0;information and computation;information logistics;internet of things;scalability;tag cloud	Pierluigi Plebani;David García-Pérez;Maya Anderson;David Bermbach;Cinzia Cappiello;Ronen I. Kat;Frank Pallas;Barbara Pernici;Stefan Tai;Monica Vitali	2017				HPC	-30.72422367714934	59.770574646800185	182993
d9df993e52851544ab782d33c8a1611883405082	a novel service-oriented platform for the internet of things		As Internet of Things (IoT) has received substantial attention in industry and academia recently, many IoT devices and IoT platforms have been proposed and being developed. In this paper we propose a novel IoT platform, called SoPIoT, that is different from existent IoT platforms in several aspects. Since a device is abstracted with a set of services it provides, any computing resources can be easily integrated into the platform. SoPIoT allows a user to define a composite service dynamically at run-time by a script language program. To SoPIoT, the IoT system looks like a distributed system that consists of many computing resources, running multiple applications currently where an application corresponds to a composite service. The central middleware maps and schedules the services to the computing resources. The scalability of SoPIoT is achieved by forming the hierarchy of middlewares. The viability of the proposed IoT platform is confirmed by building a smart office test-bed. Experimental results show that a central middleware can support more than 1,000 devices.	computer programming;distributed computing;internet of things;map;middleware;network packet;requirement;scalability;scheduling (computing);scripting language;service-oriented device architecture;service-oriented modeling;testbed	Hyunjae Lee;Eunjin Jeong;Donghyun Kang;Jinmyeong Kim;Soonhoi Ha	2017		10.1145/3131542.3131549	computer network;computer science;scalability;service-oriented architecture;schedule;internet of things;hierarchy;scripting language;middleware	Mobile	-30.611442522998072	54.746470170823315	183353
04b2550aeae86fcbd51223b532451876aaec623a	~okeanos: large-scale cloud service using ceph	tecnologia electronica telecomunicaciones;tecnologias	~okeanos is a large-scale public cloud service powered by Synnefo and run by GRNET. Ceph is a distributed storage solution that targets scalability over commodity hardware. This article focuses on how we use Ceph to back the storage of ~okeanos. More specifically, we will describe what we aimed for in our storage system, the challenges we had to overcome, certain tips for setting up Ceph, and experiences from our current production cluster.		Filippos Giannakos;Evangelos Koukis;Constantinos Venetsanopoulos;Nectarios Koziris	2014	;login:		embedded system;real-time computing;engineering;operating system	OS	-28.114780731869935	57.989838897664775	183385
4aeba5bb7099e84469d59fe35dbc82053031c92e	using osmotic services composition for dynamic load balancing of smart city applications		Edge computing takes computation away from the Cloud closer to the physical world. Therefore, it reduces the cost of communication bandwidth between IoT devices and the Cloud. However, Edge computing imposes certain limitations in computation power because due to poor hardware capacity of the devices. This restriction may significantly affect the performance of the deployed applications, especially Smart City applications. This limitations also could be aggravated by unpredictable human behaviors wich will easily make the Edge computation node overloaded. Osmotic computing is a new IoT application programming paradigm that provides an opportunity to balance the workload between Edge and Cloud therefore to overcome the load imbalance problem of Smart City applications. To this end, we propose an Osmotic Execution Framework that leverages state-of-the-art microservices techniques to deploy and execute a Smart City application in a distributed environment including Edge and Cloud. Finally, we evaluate load balancing through latency time analysis of our framework with a real-world smart parking application.		Arthur Souza;Zhenyu Wen;Nélio Cacho;Alexander Romanovsky;Philip James;Rajiv Ranjan	2018	2018 IEEE 11th Conference on Service-Oriented Computing and Applications (SOCA)	10.1109/SOCA.2018.00029	computer science;smart city;distributed computing;microservices;scalability;programming paradigm;cloud computing;distributed computing environment;load balancing (computing);edge computing	HPC	-27.533504091343872	59.319068231719825	184226
2e6144dec5255a19b48e480d092a464b673972b7	ontology-based personalized telehealth scheme in cloud computing		Cyber Physical Social Systems (CPSS) has been rapidly developing in recent years because of the dramatic growth of mobile techniques and Internet-based technologies. As an emerging novel technical paradigm, cloud computing is becoming an efficient mechanism that has been broadly implemented in multiple fields with using Internet-enabled devices. Telehealth system is one of the crucial domains in applications of Cyber Physical Systems (CPS), which is also considered an important component in smart city. However, Personalized Cloud-based Telehealth (PCTH) is still facing a great challenge due to restrictions deriving from various perspectives, such as cloud resource allocations and real-time information transfers. This paper concentrates on the issue of optimizing the performance of the resource allocation for achieving smart tele-health with obtaining and analyzing real-time data within the dynamic application environment in cloud computing. The dynamic data environment is mainly associated with social connections generated by physicians or health organizations. Along with this focus, we propose a novel approach, entitled Smart Cloud-based Telehealth Cyber Physical Social Systems (SCT-CPSS), to enable mobile CPSS to offer users a real-time health information service based on the social networking behaviors and inputs. Main algorithms used in this proposed mechanism include Real-Time Matching with Dynamic Programming Algorithm (RTM-DPA) and Monte Carlo-based Real-Time Analysis Algorithm (MC-RTAA). Our experiment examination has evaluated the performance of the proposed paradigm.	cloud computing	Keke Gai;Lei Zou;Liehuang Zhu	2018		10.1007/978-3-319-94289-6_4	data mining;computer science;smart city;monte carlo method;cloud computing;cyber-physical system;telehealth;dynamic data;social network;resource allocation;distributed computing	HPC	-33.61505172603796	58.130269913995704	184483
dea702c94f34870fa700aded8daef68afb7a791a	rc3e: reconfigurable accelerators in data centres and their provision by adapted service models	acceleration;computer architecture;field programmable gate arrays;adaptation models;program processors;cloud computing;hardware	Computing performance and scalability are essential ingredients in modern data centres offering cloud services. Field Programmable Gate Arrays (FPGAs) provide a promising opportunity to improve performance, security and energy efficiency because their hardware architecture can be adapted directly to the application. In this paper we present the development of our FPGA cloud architecture, beginning with realistic use cases and adapted service models for the use of reconfigurable hardware accelerators in a cloud context. Our architecture supports both applications where FPGAs are tightly coupled to host processors and applications using the FPGA for a secured cloud access in an overall homogeneous system. In contrast to other approaches, we model the system as a whole with a more flexible FPGA provision. The application service provider has the opportunity to offer a service with an individual FPGA design or customized secure interfaces to the cloud. For an abstraction from the real hardware and to achieve high device utilization, the FPGAs and the interfaces are fully virtualized. Because of the uncommon reconfigurable hardware integration and our adapted service models, we developed a special resource management system (RC3E) which serves as a hypervisor for the virtualized hardware. The demonstration of an intelligent load balancing as well as the system's performance and flexibility conclude our approach of FPGA services in a cloud.	central processing unit;cloud computing;elasticity (cloud computing);field-programmable gate array;fragmentation (computing);graphics processing unit;hardware acceleration;hypervisor;job scheduler;job stream;load balancing (computing);mathematical optimization;microsoft outlook for mac;scalability;scheduling (computing);system of linear equations	Oliver Knodel;Patrick Lehmann;Rainer G. Spallek	2016	2016 IEEE 9th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2016.0013	acceleration;embedded system;parallel computing;real-time computing;cloud computing;reconfigurable computing;computer science;operating system;field-programmable gate array	HPC	-29.361116580185016	56.89315190029359	186178
82a3bb5602a307eb3bd5f122a8255a0d294866a4	research on meteorological data sharing technology based on service oriented architecture	meteorological data;web server;service oriented architecture;sharing technology	The numerical weather prediction acts according to the atmosphere actual situation, giving certain starting value and edge value, through value computation, solution description weather successional variation process hydromechanics and thermodynamics system of equations, forecast future weather method. This paper shows how to migrate a traditional distributed scientific computing to a grid computing environment.	computation;computational science;grid computing;numerical analysis;numerical weather prediction;service-oriented architecture	Jianhua Du;Jiwu Xin;Shenghong Wu	2013		10.1145/2556871.2556910	simulation;computer science;service-oriented architecture;data mining;world wide web;web server	HPC	-31.29051384229867	53.534344138224306	186333
829ccf0e305e435880cde6ec66288fbbfd4ca126	multi-scale real-time grid monitoring with job stream mining	online monitoring;system monitoring data mining fault tolerant computing grid computing learning artificial intelligence middleware natural sciences computing query processing statistical analysis;e science grid;autonomic computing grid monitoring online clustering;glite reporting service;query processing;online clustering;large computational system complexity;probability density function;data stream;monitoring grid computing middleware distributed computing testing computational modeling physics computing real time systems statistical learning embedded computing;anomaly detection;real time;offline monitoring module;distributed computing;e science grid multiscale real time grid monitoring job stream mining large computational system complexity management tool autonomic grid computing grid middleware computational query statistical learning egee grid gstrap system strap data streaming algorithm glite reporting service online monitoring module anomaly detection offline monitoring module visual inspection;grid monitoring;system monitoring;grid middleware;testing;autonomic grid computing;websearch;data mining;physics computing;computational query;accuracy;distance measurement;multiscale real time grid monitoring;fault tolerant computing;computational modeling;statistical learning;statistical analysis;monitoring;visual inspection;strap data streaming algorithm;long term trend;cern document server;egee grid;gstrap system;middleware;management tool;learning artificial intelligence;natural sciences computing;grid computing;autonomic computing;online monitoring module;embedded computing;job stream mining;real time systems	The ever increasing scale and complexity of large computational systems ask for sophisticated management tools, paving the way toward Autonomic Computing. A first step toward Autonomic Grids is presented in this paper; the interactions between the grid middleware and the stream of computational queries are modeled using statistical learning. The approach is implemented and validated in the context of the EGEE grid. The GStrAP system, embedding the StrAP Data Streaming algorithm, provides manageable and understandable views of the computational workload based on gLite reporting services. An online monitoring module shows the instant distribution of the jobs in real-time and its dynamics, enabling anomaly detection. An offline monitoring module provides the administratorwith a consolidated view of the workload, enabling the visual inspection of its long-term trends.	aggregate data;anomaly detection;autonomic computing;circa;computation;egi;image histogram;interaction;job stream;machine learning;middleware;online and offline;real-time clock;real-time transcription;scalability;streaming algorithm;system administrator;tracing (software);usability;user interface;virtual organization (grid computing);visual inspection;glite	Xiangliang Zhang;Michèle Sebag;Cécile Germain	2009	2009 9th IEEE/ACM International Symposium on Cluster Computing and the Grid	10.1109/CCGRID.2009.20	system monitoring;probability density function;anomaly detection;computer science;operating system;middleware;data mining;database;distributed computing;accuracy and precision;software testing;computational model;grid computing;statistics;autonomic computing;visual inspection	HPC	-32.05825256488817	56.436430602573814	187323
f0855ea2c2ae7197e9e7ca1f0d76107bceb81a48	papyrus: a system for data mining over local and wide area clusters and super-clusters	data mining predictive models data analysis workstations network servers temperature next generation networking statistical analysis explosions ip networks		computer cluster;data mining;papyrus	Stuart Bailey;Robert L. Grossman;Harimath Sivakumar;Andrei L. Turinsky	1999		10.1109/SC.1999.10043	computer science;data science;data mining;world wide web	ML	-26.668845739247452	54.57305064209866	188149
bc606422be9223af3ec8fd6b4a3145f3068f01e7	system and analysis for low latency video processing using microservices		Author(s): VASUKI BALASUBRAMANIAM, KARTHIKEYAN | Advisor(s): Porter, George M | Abstract: The evolution of big data processing and analysis has led to data-parallel frameworks such as Hadoop, MapReduce, Spark, and Hive, which are capable of analyzing large streams of data such as server logs, web transactions, and user reviews. Videos are one of the biggest sources of data and dominate the Internet traffic. Video processing on a large scale is critical and challenging as videos possess spatial and temporal features, which are not taken into account by the existing data-parallel frameworks. There are a broad range of users who want to apply sophisticated video processing pipelines such as transcoding, feature extraction, classification, scene cut detection and digital compositing to video contentParallel video processing poses several significant research challenges to the existing data processing frameworks. Current systems are capable of processing videos but with higher resource startup times, a small degree of parallelism, low average resource utilization, coarse-grained billing, and higher latency. This research proposes a low latency software run-time for processing a single video efficiently by orchestrating cloud-based microservices. The system leverages lightweight microservices provided by Amazon Web Services Lambda framework.		Karthikeyan Balasubramaniam	2017			microservices;streams;real-time computing;feature extraction;video processing;big data;cloud computing;latency (engineering);web service;computer science	OS	-28.214058539464396	59.647540589967306	188160
0a2708457272f89cbfe1e7d050bc46b8a8e028fc	a dynamic aware-feedback system for service replica management	distributed computing service network replica management;distributed processing;virtualisation cloud computing distributed processing;service oriented distributed software dynamic aware feedback system service replica management cloud service distributed computing virtualization technology data file block data network software service instruction set;virtualisation;cloud computing;delays optimization computational modeling software systems cloud computing distributed databases	Cloud service provides a new model of distributed computing. The virtualization technology makes it easy to manage the computing unit, the service, as a data file block. So the optimization to a data network, such as P2P, CDN, etc. can be reused in service network. However, since software service is not a static data file but dynamic running instruction set, there will be a lot more factors to be taken into account than pure data circumstance. This paper discusses the technology of service network optimization, and proposes a new method and system of service replica management to improve the service oriented distributed software performance.	algorithm;content delivery network;distributed computing;mathematical optimization;peer-to-peer;pure data;simulation;software as a service;software performance testing;software system;x86 virtualization	Hui Zhang;Ke Xu	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664449	network intelligence;real-time computing;service catalog;cloud computing;computer science;value-added network;operating system;cloud testing;software as a service;distributed computing;utility computing;distributed design patterns;service virtualization;data as a service;replication	HPC	-26.83045434170742	57.42414796397162	189471
f886559bd4a60f7e9a4e384aadb35db311b33e52	virtualization for hpc	virtualization;high performance computing;hardware virtual machining cloud computing operating systems;virtualisation parallel processing;virtualization high performance computing;high performance computing hpc virtualization commercial enterprise environments;parallel processing;virtualisation	While virtualization is widely used in commercial enterprise environments, to date it has not played a significant role in High Performance Computing (HPC). This has begun to change as organizations realize that virtualization can help address a number of longstanding problems found in HPC environments while also enabling new capabilities not previously available.		Joshua E. Simons	2012	2012 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2012.6402913	parallel processing;full virtualization;parallel computing;virtualization;application virtualization;cloud computing;computer science;virtual machine;operating system;hardware virtualization;distributed computing;storage virtualization	HPC	-28.179324904805743	56.021761753112365	191166
a0a843d5b94a6d1bfeb636b25ece5e92c0d933e7	fitscale: scalability of legacy applications through migration to cloud		One of the key benefits of Cloud computing is elasticity, the ability of the system infrastructure to adapt to the workload changes by automatically adjusting the resources on-demand. Horizontal scaling refers to the method of adding or removing resources from the resource pool. As such it is appealing to enterprises who seek to migrate their legacy systems as it requires no application rewrite or refactoring. Vertical scaling approach offers a mechanism to maintain continuous performance while reducing resource cost through reconfiguration of the resource. The challenge is, however, in being able to automatically identify the right size of the target resource such as a VM or a container. Moreover, choice of scalability policies is not intuitive due to application complexity, topology and variability in system performance parameters that need to be considered.	scalability	Jinho Hwang;Maja Vukovic;Nikos Anerousis	2016		10.1007/978-3-319-46295-0_8	workload;real-time computing;control reconfiguration;scalability;scaling;cloud computing;computer science;legacy system;elasticity (economics);code refactoring	HPC	-28.744322072826346	59.132533786966135	191189
0a8f22957415165cd93202148a814f483289f6f1	native runtime environment for internet of things		Over the past few years, it has become obvious that Internet of Things, in which all embedded devices are interconnected, will soon turn into reality. These smart devices have limited processing and energy resources and there is a need to provide security guarantees for applications running on them. In order to meet these demands, in this paper we present a POSIX-compliant native runtime environment which runs over a L4-based microkernel, a thin operating system, on top of which applications for embedded devices can run and thus be a part of the Internet of Things.	internet of things;runtime system	Valentina Manea;Mihai Carabas;Lucian Mogosanu;Laura Gheorghe	2015		10.1007/978-3-319-17996-4_34	web of things;computer science;operating system;distributed computing;world wide web	EDA	-29.412072715060027	57.12480208203024	191715
bdf4ed1be9ee5a3ed1e6d5273da5b82c4c6d7d0a	using the cloud to replace traditional physical networking laboratories (abstract only)	academic computing;virtual servers;virtual labs;cloud computing	The cloud has become prevalent today and is used both in academia as well as industry. Many services, such as Amazon Web Services, provide an inexpensive way to provision servers in many different platforms. These cloud providers also have prebuilt instances which have many different popular configurations, such as LAMP and many popular databases. This bird of a feather session will discuss experiences with utilizing the cloud both as a temporary fix for times when hardware is unavailable for laboratories (such as during renovations) as well as using the cloud as a permanent replacement for physical labs	amazon web services;cloud computing;database;lamp (software bundle);web service	Magdy Ellabidy;John P. Russo	2014		10.1145/2538862.2544243	simulation;cloud computing;computer science;operating system;software engineering;multimedia;world wide web	DB	-32.12366573435652	55.69497008779958	192380
50aecc1134bcddf8d8195f1dd1c2557f5c905c9d	a mapreduce cluster deployment optimization framework with geo-distributed data	data processing;geo distributed data;big data;cluster deployment;distributed databases;clustering algorithms;bandwidth;optimization;mapreduce;hadoop;programming	Big Data processing has become the common business needs in government and enterprise applications, e.g., Analysis or detection of climate change, economic development, or online customer behavior. Hadoop is the most mature open source big data processing framework, which implements the MapReduce programming paradigm. The mass source data are stored in HDFS supported by Hadoop and processed parallelly in computing nodes of a cluster. However, in many cases, the source data is simultaneously distributed across multiple data centers(Geo-distributed). Existing deployment research, merely focusing on moving all data to one data center to process, is often limited by the size of input data and the network transmission capacity between data centers, resulting in a lethal impact on the performance of big data processing. In this paper, we deal with Geo-distributed data sets, analyze possible cluster deployment way and then select the optimal one with the proposed cluster deployment optimization framework. We introduce decision making algorithm that the optimization framework relies on to determine an optimized cluster deployment way. In addition, we prove the benefit of our optimization framework by final experiment in Amazon EC2 over the common deployment for Geo-distributed data. The results show that the decision making algorithm is accurate and the optimization framework can significantly improve the Geo-distributed data processing performance by giving the optimized cluster deployment way.	algorithm;amazon elastic compute cloud (ec2);apache hadoop;big data;business requirements;data center;distributed computing;enterprise software;mapreduce;mathematical optimization;mega man network transmission;open-source software;programming paradigm;run time (program lifecycle phase);software deployment;source data	Shanshan Li;Qinghua Lu;Weishan Zhang;Liming Zhu	2015	2015 IEEE 12th Intl Conf on Ubiquitous Intelligence and Computing and 2015 IEEE 12th Intl Conf on Autonomic and Trusted Computing and 2015 IEEE 15th Intl Conf on Scalable Computing and Communications and Its Associated Workshops (UIC-ATC-ScalCom)	10.1109/UIC-ATC-ScalCom-CBDCom-IoP.2015.179	programming;big data;data processing;computer science;operating system;data mining;database;cluster analysis;world wide web;bandwidth	DB	-27.18691215158249	58.23120385901761	192617
5bcef19bfe60f34e06aecd87b9fc09383f0fb188	clouds-of-clouds for dependability and security: geo-replication meets the cloud		The complexity of large cloud offerings makes it extremely hard to guarantee their dependability and security. This paper extracts lessons from some years of research on the notion of using several clouds – instead of a single one– with the objective of achieving high dependability and security. We show that using such clouds-of-clouds it is possible for services to continue to operate correctly despite dependability and security issues in a subset of the clouds. We show this approach with three cases: clouds-of-clouds for storage with the DepSky system; cloudof-clouds for data processing with MapReduce; the execution of arbitrary services in clouds-of-clouds with the EBAWA algorithm.	algorithm;cloud computing;dependability;hypertext transfer protocol;interrupt latency;mapreduce	Miguel Correia	2013		10.1007/978-3-642-54420-0_10	cloud computing security;real-time computing;computer science;operating system;dependability;database;distributed computing;computer security	Security	-29.837492589348365	58.03998048755621	193564
e88ff7cd061ec07c1a1bfd0fcce3595a0154a0f0	visual effects in the age of the cloud		The Visual Effects industry is presently grappling with how to best take advantage of cloud computing, a technology which has transformed the practice of software in many industries. The ability to treat the provisioning and configuration of render farm hardware with the flexibility of software is highly attractive, but the learning curve can be challenging to juggle with busy production schedules. Fully managed web services have also taken hold in some parts of the production pipeline, with more likely to come. Software vendors creating web services need to enable studios with the right combination of security, backwards compatibility, ease of use, and programmability, so they may adopt these technologies without interrupting their Visual Effects production.  In this panel, we will discuss current usage of cloud computing in Visual Effects, how it is trending, and how it interacts with other factors like the growth of VFX-oriented open source software. Studios range in their use of render farms from full on-premises setups through hybrid setups blending their premises with the cloud to all-in cloud rendering. We will explore how fast internet connections and efficient streaming desktop technology are enabling full end-to-end production to move to the cloud with Zero Client workstations. Our panel consists of a diverse group of technologists, representing both Visual Effects studios and the creators of software for the industry.	alpha compositing;backward compatibility;cloud computing;desktop computer;end-to-end encryption;interrupt;on-premises software;open-source software;provisioning;render farm;thin client;usability;visual effects;web service;workstation	Mark Wiebe;Jason Fotter;Dan Wexler;Panos Zompolas;Phil Peterson	2018		10.1145/3209621.3214896	backward compatibility;computer engineering;computer graphics (images);rendering (computer graphics);web service;software;cloud computing;render farm;provisioning;usability;computer science	HCI	-29.104126021495507	55.53220963846386	193637
f6e1da051d438a262181e64aeac7dc2eb5a3e52e	a virtualization-based saas enabling architecture for cloud computing	libraries;software;computer architecture cloud computing web and internet services application software displays large scale systems software design software systems protocols web server;protocols;legacy software;back end resource pool;virtualization;application software;web and internet services;client operation system;virtualization based vsaas enabling architecture;virtual machining;ivic platform;cloud;software systems;ivic saas virtualization cloud;os level virtualization;software architecture grid computing internet;computer architecture;large scale;software architecture;saas applications virtualization based vsaas enabling architecture cloud computing internet ivic platform virtual computing environment client operation system device capability os level virtualization remote display technologies legacy software back end resource pool;servers;virtual computing environment;saas applications;internet;operating system;displays;software as a service;remote display technologies;web server;software design;grid computing;device capability;ivic;large scale systems;cloud computing;saas	With the increasing prevalence of large scale cloud computing environment, researchers has draw more attention about how to provide software as a service through the internet. In this paper, a novel approach named vSaaS is proposed in iVIC platform, which is the virtual computing environment for HaaS and SaaS applications. The aim of this approach is to provide the software as a service from a cloud computing environment over the Internet; users are able to access different software transparently with no limitation on the client operation system or device capability. OS-level virtualization and remote display technologies are employed in the vSaaS system. Benefit from above technologies, massive exist legacy software could be easily adopted without any recompilation and redevelopment work. Software can be dynamic streaming deployed in the back-end resource pool in a dynamic streaming way. Finally, comprehensive experiments are conducted to demonstrate the feasibility and performance of the vSaaS implementation.	cloud computing;display device;experiment;legacy system;operating system;operating-system-level virtualization;software as a service	Liang Zhong;Tianyu Wo;Jianxin Li;Bo Li	2010	2010 Sixth International Conference on Autonomic and Autonomous Systems	10.1109/ICAS.2010.28	embedded system;cloud computing;computer science;operating system;software as a service;world wide web	HPC	-29.228657810083146	55.27728310351998	194114
70f502e01285cda355e1a7ba82b911c75eb1d63b	storage active service: model, method and practice	model design;storage allocation;high performance computing memory hardware computer architecture psychology cache storage databases computer interfaces embedded computing application specific integrated circuits;storage system;fundamental unit;data management;object oriented programming;psychology;service model;computer architecture;data storage;forecasting theory;data access;storage allocation forecasting theory object oriented programming;storage object;psychology storage active service storage object forecast mechanism;storage active service;forecast mechanism	For storage system, data service only passively responds to data consumer's requests. With the increasing requirements of mass data storage and the development of the hardware technology of computer architecture, storage active service becomes a crucial research topic. How to utilize the computation power on the future storage device to provide high performance computation and data access has been a hot and challenging task. Storage object is a fundamental unit that comprises not only data but also attributes. Storage object has been proposed in the past, but its definition should be extended. The goals of the storage active service project are to combine and distill previous work into a simple and usable model, design a storage active service mechanism as an understandable extension to object, and most importantly build a working system - a system that augments conventional structured and unstructured data management with data, attributes and trigger as an integral part of the storage object. This paper provides numerous motivating practices for storage active service and lays out preliminary plans for the forecast mechanism based on psychology	computation;computer architecture;computer data storage;data access;data-intensive computing;distributed computing;embedded system;floor and ceiling functions;holism;object-based language;requirement;server (computing)	Lingfang Zeng;Dan Feng;Fang Wang;Lei Tian;Zhan Shi	2006	2006 Japan-China Joint Workshop on Frontier of Computer Science and Technology	10.1109/FCST.2006.31	data access;embedded system;real-time computing;simulation;converged storage;data management;computer science;operating system;computer data storage;database;data efficiency;information repository;fundamental unit;computer security	HPC	-27.35244447509315	55.27278347776361	194218
191a766b54514405bfd0d4b1e1c592390c84010f	a survey on cloud computing elasticity	elasticity;elasticity cloud computing virtual machining monitoring servers resource management measurement;survey cloud computing elasticity;academic solution cloud computing elasticity computing paradigm elasticity mechanism classification commercial solution;pattern classification cloud computing;pattern classification;survey;cloud computing	Elasticity is a key feature in the cloud computing context, and perhaps what distinguishes this computing paradigm of the other ones, such as cluster and grid computing. Considering the importance of elasticity in cloud computing context, the objective of this paper is to present a comprehensive study about the elasticity mechanisms available today. Initially, we propose a classification for elasticity mechanisms, based on the main features found in the analysed commercial and academic solutions. In a second moment, diverse related works are reviewed in order to define the state of the art of elasticity in clouds. We also discuss some of the challenges and open issues associated with the use of elasticity features in cloud computing.	cloud computing;computer cluster;elasticity (cloud computing);elasticity (data store);grid computing;programming paradigm	Guilherme Galante;Luis Carlos Erpen De Bona	2012	2012 IEEE Fifth International Conference on Utility and Cloud Computing	10.1109/UCC.2012.30	simulation;cloud computing;computer science;operating system;data mining;utility computing;elasticity;world wide web	HPC	-30.65613169572109	58.80531842858123	195851
debe76907d018c2c350cc22e03af5d4c42f04289	job scheduling strategies for parallel processing		Nearly all existing HPC systems are operated by resource management systems based on the queuing approach. With the increasing acceptance of grid middleware like Globus, new requirements for the underlying local resource management systems arise. Features like advanced reservation or quality of service are needed to implement high level functions like co-allocation. However it is difficult to realize these features with a resource management system based on the queuing concept since it considers only the present resource usage. In this paper we present an approach which closes this gap. By assigning start times to each resource request, a complete schedule is planned. Advanced reservations are now easily possible. Based on this planning approach functions like diffuse requests, automatic duration extension, or service level agreements are described. We think they are useful to increase the usability, acceptance and performance of HPC machines. In the second part of this paper we present a planning based resource management system which already covers some of the mentioned features.	high-level programming language;job shop scheduling;middleware;quality of service;requirement;scheduling (computing);service-level agreement;usability	Gerhard Goos;Jan van Leeuwen	2003		10.1007/10968987		HPC	-29.031980885779976	53.84665509405429	196771
19f6dc32959a998f2579716cd4a5a8798b58269a	scimitar: scalable stream-processing for sensor information brokering	detectors;information management cloud computing;subscriptions throughput storms scalability cloud computing prototypes fasteners;information exchange;information management;brokering prototype scimitar scalable stream processing sensor information brokering current sensor collection information management services information exchange information producers information consumers surveillance platforms cloud computing large scale distributed computation cloud based information broker;distributed data processing;information brokering sensor cloud computing stream computing publish subscribe;cloud computing	Current sensor collection capabilities produce an incredible amount of data that needs to be processed, analyzed, and distributed in a timely and efficient manner. Information Management (IM) services supporting a publish-subscribe and query paradigm can be a powerful general purpose approach to enabling this information exchange between decoupled and dynamic information producers and consumers. These IM services will only be of value, however, if they can support operations in a manner that is responsive to the sheer quantity and frequency of data produced by surveillance platforms. Cloud computing is the technology of choice for providing the resources and services needed to enable and mange large-scale distributed computation. To date, there has been little work to develop highly scalable, dynamic IM processing and dissemination services in a cloud computing environment. In this paper we discuss our design, implementation and evaluation of a prototype cloud-based information broker which is a critical component of a highly scalable, distributed IM System. The brokering prototype is designed using a distributed stream processing framework and is shown to scale nearly linearly with the number of computing nodes as information load and subscription quantity increases.		Kurt Rohloff;Jeffrey Cleveland;Joseph P. Loyall;Timothy Blocher	2013	MILCOM 2013 - 2013 IEEE Military Communications Conference	10.1109/MILCOM.2013.313	cloud computing security;cloud computing;computer science;database;distributed computing;utility computing;world wide web	HPC	-31.349614279373213	57.85524655632509	196860
16c7eb0edf1628cdb4fba12d569d9fa9e21d6bd5	virtualized in-cloud security services for mobile devices	mobile device;software complexity;antivirus;mobile agent;network services;behavior analysis;malware detection;mobile devices	Modern mobile devices continue to approach the capabilities and extensibility of standard desktop PCs. Unfortunately, these devices are also beginning to face many of the same security threats as desktops. Currently, mobile security solutions mirror the traditional desktop model in which they run detection services on the device. This approach is complex and resource intensive in both computation and power. This paper proposes a new model whereby mobile antivirus functionality is moved to an off-device network service employing multiple virtualized malware detection engines. Our argument is that it is possible to spend bandwidth resources to significantly reduce on-device CPU, memory, and power resources. We demonstrate how our in-cloud model enhances mobile security and reduces on-device software complexity, while allowing for new services such as platform-specific behavioral analysis engines. Our benchmarks on Nokia's N800 and N95 mobile devices show that our mobile agent consumes an order of magnitude less CPU and memory while also consuming less power in common scenarios compared to existing on-device antivirus software.	antivirus software;central processing unit;cloud computing security;computation;desktop computer;extensibility;malware;mobile agent;mobile device;mobile security;nokia n800 internet tablet;platform-specific model;programming complexity	Jon Oberheide;Kaushik Veeraraghavan;Evan Cooke;Jason Flinn;Farnam Jahanian	2008		10.1145/1622103.1629656	embedded system;mobile search;mobile web;computer science;operating system;mobile technology;mobile device;mobile computing;computer security	Security	-29.071311227866932	56.64013460183016	197673
472f991de9d13e64df6a0f3801ef3416029c83ca	emulation at very large scale with distem	peer to peer computing linux encapsulation scalability ip networks virtualization protocols;clustershell exascale systems large scale cloud infrastructures distem emulator very large scale emulated platforms vxlan overlay network virtual nodes physical nodes parallel command runners taktuk;virtualization large scale emulation vxlan;virtualisation local area networks overlay networks	Prospective exascale systems and large-scale cloud infrastructures are composed of dozens of thousands of nodes. Evaluating applications that target such environments is extremely difficult. In this paper, we present an extension of the Distem emulator to allow experimenting on very large scale emulated platforms thanks to the use of a VXLAN overlay network. We demonstrate that Distem is capable of emulating 40,000 virtual nodes on 168 physical nodes, and use the resulting emulated environment to compare two efficient parallel command runners: TakTuk and Cluster Shell.	cloud computing;emulator;experiment;overlay network;prospective search	Tomasz Buchert;Emmanuel Jeanvoine;Lucas Nussbaum	2014	2014 14th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2014.64	parallel computing;computer science;operating system;distributed computing;computer network	HPC	-28.97706657745975	54.420605814484084	198723
099ea1bd50f6e77f4dcbeac7ed9cba5b1331b760	imms: iot management and monitoring system		The Internet of Things already comprehends billions of connected smart devices. The related issues are vast as well, ranging from the lack of standardized communication to big data management. This paper presents IoT Management and Monitoring System (IMMS), a solution focused on the management of big environments such as smart cities. The The proposed platform brings a novel database implementation to handle large volumes of data and provide quick responses to queries. Its design allows scalable and distributed deployments, while accepting diverse data types. The system was evaluated both in a real-world testbed, in a university campus, and a simulated environment. Results showed low CPU costs while consuming 1.1GB to store 21.5 million measurements. Besides, its scalability behaved as expected, maintaining its performance adequate as data volume increased.	big data;central processing unit;gigabyte;internet of things;scalability;simulation;smart city;smart device;testbed;virtual reality	Joao Victor Lucena do Monte;Vinicius Matos da Silveira Fraga;Andrea Maria Nogueira Cavalcanti Ribeiro;Djamel Fawzi Hadj Sadok;Judith Kelner	2018	2018 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2018.8538755	data type;task analysis;data visualization;scalability;big data;distributed computing;logic gate;computer science;ranging;testbed	DB	-27.498472865084512	53.78309566809515	199148
53a127ed7187791e7a52dacb7af2805dbbf281e1	ezilla fast deployment cloud toolkit with network infrastructure service	network operating systems;cloud;virtualization technology;virtual machines;webos;middleware;sr iov;cloud computing;virtual cluster	Deployment of server virtualization almost becomes a universal basic concept of making full use of the data center resources. With the evolution trend for powerful multi-core servers, more memory capacity, and greater bandwidth network channel, it is necessary to think about the network I/O. To contribute this issue, Ezilla has been developed by Pervasive Computing Team at National Center for High-performance Computing (NCHC). Ezilla integrates the Cloud middleware, virtualization technology, and Web-based Operating System (WebOS) to build virtual machines in the distributed computing environment. The main character of Ezilla is simplifying a lot complexity of utilizing Clouds. Via the Ezilla with SR-IOV feature, Cloud users can create virtual clusters that are customized to meet the specific needs from the users. Our goal is to make scientists or users painlessly run their works on Clouds, and greatly improve the network I/O performance.	algorithmic efficiency;asynchronous i/o;cloud computing;data center;distributed computing environment;giove;middleware;multi-core processor;national center for high-performance computing;operating system;single-root input/output virtualization;software deployment;throughput;ubiquitous computing;usability;virtual machine;virtual private server;webos;x86 virtualization	Hui-Shan Chen;Yi-Lun Pan;Chang-Hsing Wu;Hsi-En Yu;Kuo-Yang Cheng;Weicheng Huang	2012	2012 Third International Conference on Networking and Computing	10.1109/ICNC.2012.49	full virtualization;virtualization;cloud computing;computer science;virtual machine;operating system;distributed computing;world wide web;computer network	HPC	-29.256007578003487	54.594023281026374	199307
