id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
04f1dfb0db0d5e8c52b1bcc15069567cbdfbfc17	automatic rule generation for constraint enforcement in active databases	constraint enforcement;active databases;automatic rule generation	In conventional database systems, consistency is preserved either by forbidding operations that violate integrity constraints or by rolling back transactions that produce inconsistent database states. A third way to integrity maintenance is that of post processing faulty transactions, by introducing system-generated compensating actions that lead to a database state which is consistent and as close as possible to that intended by the user. Active databases provide the appropriate technology to implement this paradigm.		Piero Fraternali;Stefano Paraboschi;Letizia Tanca	1992			data mining;database	AI	-26.528904045522875	13.547344706641592	166330
c66279490d0ef8175a105727481b6ef570980a00	a conceptual language for querying object oriented data	qa75 electronic computers computer science;object oriented;object oriented database systems;qa76 computer software	A variety of languages have been proposed for object oriented database systems in order to provide facilities for ad hoc querying. However, in order to model at the conceptual level, an object oriented schema de nition language must itself provide facilities for describing the behaviour of data. This paper demonstrates that with only modest extensions, such a schema de nition language may serve as a query notation. These extensions are concerned solely with supporting the interactive nature of ad hoc querying, providing facilities for naming and displaying query operations and their results.	database;hoc (programming language)	Peter J. Barclay;Jessie B. Kennedy	1994		10.1007/3-540-58235-5_43	computer science;conceptual schema;data mining;database;programming language;object-oriented programming;object definition language	DB	-30.639975478992096	12.088970396646767	167066
b0c8166da0f746944be00b5098f580620a3009bc	a computer-aided methodology for conceptual data-base design		Abstract   In this paper we describe an integrated system to aid data-base conceptual design. In this system, incremental enrichments of the conceptual description of user requirements both with a top-down and a bottom-up discipline are allowed.  Such enrichments involve both data and transactions. Data enrichments can be expressed by a disciplined set of design primitives. For every primitive any inconsistencies arising are described and a set of scenarios to eliminate them is suggested.  Transaction enrichments are expressed in terms of declarations which define these transactions at various conceptual refinement levels. Such declarations can be compared to the partial conceptual schema obtained by data enrichments and possible incompleteness and inconsistency situations arising are described.  Future developments of the research program are also outlined.	database	Carlo Batini;Maurizio Lenzerini;Gaetano Santucci	1982	Inf. Syst.	10.1016/0306-4379(82)90018-7	computer science;data mining;database;algorithm	DB	-31.428006833791347	12.614038137191976	167211
03256a06a5cfe495246a5757a8ee2123bb9019de	a typed calculus for querying distributed xml documents	parallel composition;xml schema;process calculi;formal model;programming language;code optimization;building block;computer model;heterogeneous databases;formal semantics;web service;programming model;process calculus;pattern matching;xml document;service oriented architecture;document type definition;type system	We study the problems related to querying large, distributed XML documents. Our proposal takes the form of a new process calculus in which XML data are processes that can be queried by means of concurrent pattern-matching expressions. What we achieve is a functional, strongly-typed programming model based on three main ingredients: an asynchronous process calculus that draws features from π-calculus and concurrent-ML; a model where both documents and expressions are represented as processes, and where evaluation is represented as a parallel composition of the two; a static type system based on regular expression types.	concurrent ml;pattern matching;process calculus;programming model;regular expression;strong and weak typing;type system;xml	Lucia Acciai;Michele Boreale;Silvano Dal-Zilio	2006		10.1007/978-3-540-75336-0_11	computer simulation;xml validation;xml encryption;process calculus;streaming xml;computer science;theoretical computer science;document structure description;xml database;database;xml signature;programming language;xml schema editor;efficient xml interchange	PL	-29.409025981679083	14.248118503889385	169384
670875b0e8b910db266922d4c699aaea065fd8e1	a method for database requirements collection	interfase usuario;database view modeling;base donnee;sistema experto;architecture systeme;end user computing;form definition systems;north america;america del norte;amerique du nord;amerique;user interface;selection;database;base dato;conception;etats unis;estados unidos;database requirements;diseno;systeme definition forme;design;arquitectura sistema;interface utilisateur;systeme expert;seleccion;america;system architecture;expert system	Abstract:We describe an approach for increasing user involvement in the collection of database requirements. The approach is based on a form definition system that makes inferences from examples of form instances. The system permits the user to state requirements directly or to provide examples of requirements. Inferences from examples are made from the bottom up by considering one example at a time using a collection of rules and heuristics. To control the burden of providing lengthy examples, the system generates some of its own examples, provides access to example histories, and offers informative feedback to help the novice user become an expert user. The inference strategies support the collection of application-specific requirements, such as the hierarchical structure of a form, and invariant properties, such as functional dependencies among form fields.		Veronica P. Tseng;Michael V. Mannino	1989	J. of Management Information Systems	10.1080/07421222.1989.11517857	selection;design;simulation;computer science;artificial intelligence;data mining;database;user interface;expert system	DB	-28.194087989244828	12.45425665867034	170786
fcab88145e16cda6b6dbbecd0ab020a382571b5a	object-oriented database as a dynamic system with implicit state	algebraic specification;base donnee;mise a jour;maintenance;database;object database;base dato;dynamic system;dynamical system;state;systeme dynamique;object oriented;etat;mantenimiento;oriente objet;puesta al dia;object oriented database;sistema dinamico;orientado objeto;updating;object model	A formalization of object-oriented database concepts in the context of algebraic specifications with implicit state is proposed. An object database schema is represented as a dynamic system and an object database instance as a state algebra. The paper also provides a formalization of binding modes and a rigorous treatment of null value.	dynamical system	S. Kazem Lellahi;Alexandre V. Zamulin	2001		10.1007/3-540-44803-9_19	semi-structured model;computer science;theoretical computer science;dynamical system;database;programming language;view;database schema;algorithm;database design	DB	-29.854486768226938	13.392332604256058	170856
2c42606ecfa2889ec3929a2fca6944542da7813a	h-pcte-a high performance object management system for system development environments	software tools object oriented databases programming environments;programming environments;management system;external view distributed development environment h pcte object management system integrated system development environments tools fine grained data data modeling main memory oriented implementation recovery;integrable system;data management;data model;development environment;environmental management data models object oriented modeling access control very large scale integration hardware software tools transaction databases navigation tail;system development;software tools;object oriented databases;high performance	H-PCTE is an object management system (OMS) for distributed, open, and integrated system development environments. H-PCTE performs up to several thousand simple operations per second. H-PCTE is intended to be a basis of environments in which tools operate directly on fine-grained data stored in the object base. Fine-grained data modelling has several important implications for the architecture of environments, for tool design, and for the necessary functionality of the OMS. H-PCTE’s performance is due to mainmemory-oriented implementation techniques and due to a careful adaptation of the OMS services to the data management needs of tools; as a result, tools need not pay for OMS services which they do not really need. The latter applies in particular to recovery and the external view facilities.	data modeling;flops;order management system	Udo Kelter	1992		10.1109/CMPSAC.1992.217605	integrable system;real-time computing;data model;data management;computer science;systems engineering;operating system;management system;database;development environment	DB	-32.709937402292184	12.99224946664446	171413
1a8719e45ea0a4d4706f60bb362f1127eb9a09aa	llo: an object-oriented deductive language with methods and method inheritance	object oriented data model;deductive object oriented database;data model;object oriented;polymorphism;type classes	Integrating value-oriented and object-oriented data models is one of the active research directions in database field. Current Horn clause languages do not have the concept of methods and there is no polymorphism. In this paper, we consider Horn clause programs from an object-oriented perspective. Our observation is that Horn clause languages can be extended and object-oriented features can be integrated with the support of a proper data model. Based on this observation, we present a deductive object-oriented database language, LLO, together with its supporting data model, which uses meta variables as an abstract mechanism to build type/class hierarchies. Methods are defined by rules and method inheritance is achieved through typing and unification mechanisms. Procedural semantics is also presented and several related issues are addressed.	class hierarchy;data model;deductive language;horn clause;query language;unification (computer science)	Yanjun Lou;Z. Meral Özsoyoglu	1991		10.1145/115790.115819	natural language processing;polymorphism;data model;computer science;object;database;programming language;object-oriented programming;object definition language	DB	-30.189289067433958	11.348998207216168	171476
6dcc6d01bf2b20e9ae50f7c89320b985a3d12f92	semiorder database for complex activity recognition in multi-sensory environments	query language;query processing computerised monitoring sensor fusion temporal databases visual databases data models;query processing;real time;data model;pattern language;temporal databases;event detection semiorder database multi sensory monitoring environments complex activity recognition spatio temporal compositions atomic semantic units temporal activity composition temporal uncertainty partial orders intransitive concurrency event storage semiorder data model query language semiorder pattern language real time environment transducer subsystem;sensor fusion;computerised monitoring;data models;uncertainty concurrent computing database languages spatial databases event detection object detection visual databases prototypes data models data engineering;visual databases;activity recognition;partial order	A prototype semiorder database used for activity recognition in multi-sensory monitoring environments is described. Activities are spatio-temporal compositions of events, which are a type of atomic semantic units for such compositions. The focus is on the temporal composition of activities from events in the presence of bounded duration of temporal uncertainty in an event occurrence. Such temporal uncertainty forces the concurrency between event occurrences to be intransitive. Under certain assumptions, a subclass of partial orders, known as semiorders, models such intransitive concurrency appropriately. The semiorder database stores events and their semiorder temporal order of occurrences. A semiorder data model and the corresponding query language that embeds a semiorder pattern language are the main constituents of the semiorder database. We demonstrate this database and queries for activity recognition in a real time environment. The demonstration also includes a transducer subsystem for detection of events.	activity recognition;concurrency (computer science);data model;norm (social);pattern language;prototype;query language;transducer	Shailendra K. Bhonsle;Amarnath Gupta;Simone Santini;Ramesh C. Jain	2000		10.1109/ICDE.2000.839496	partially ordered set;data modeling;data model;computer science;data mining;pattern language;database;sensor fusion;temporal database;information retrieval;query language;activity recognition	DB	-28.85860580931981	13.787431699620594	172589
0c6b385a7887b700ee47364d33897e8d9988851d	incorporating constraints in probabilistic xml	modelizacion;dependencia dato;data dependency;aggregation function;computacion informatica;data integrity;integrite donnee;metodo minimax;xml language;echantillonnage;securite informatique;statistical databases;interrogation base donnee;minimax method;vector space;database;xml database;interrogacion base datos;base dato;probabilistic approach;satisfiability;probabilistic databases;probabilistic xml;interpretacion probabilista;sampling;probabilistic interpretation;computer security;modelisation;base donnee statistique;probabilistic database;interpretation probabiliste;ciencias basicas y experimentales;query evaluation;enfoque probabilista;approche probabiliste;seguridad informatica;methode minimax;base de donnees;dependance donnee;algorithms;espace vectoriel;sampling probabilistic data;muestreo;grupo a;modeling;espacio vectorial;database query;langage xml;constraints;lenguaje xml	Constraints are important, not only for maintaining data integrity, but also because they capture natural probabilistic dependencies among data items. A probabilistic XML database (PXDB) is the probability subspace comprising the instances of a p-document that satisfy a set of constraints. In contrast to existing models that can express probabilistic dependencies, it is shown that query evaluation is tractable in PXDBs. The problems of sampling and determining well-definedness (i.e., whether the aforesaid subspace is nonempty) are also tractable. Furthermore, queries and constraints can include the aggregate functions count, max, min, and ratio. Finally, this approach can be easily extended to allow a probabilistic interpretation of constraints.	acm transactions on database systems;aggregate data;aggregate function;approximation algorithm;automata theory;bayesian network;bottom-up parsing;cobham's thesis;constraint (mathematics);constraint satisfaction;data integrity;elementary function;fuzzy logic;hartmut neven;interaction technique;maxima and minima;michael luby;np-completeness;numerical analysis;polynomial;probabilistic turing machine;sampling (signal processing);scalability;statistical model;statistical relational learning;tree automaton;twig;warren abstract machine;xml database;xml schema	Sara Cohen;Benny Kimelfeld;Yehoshua Sagiv	2009	ACM Trans. Database Syst.	10.1145/1567274.1567280	sampling;xml;systems modeling;vector space;computer science;probabilistic database;data integrity;data mining;xml database;database;algorithm;satisfiability	DB	-27.62708168060679	11.449741716132959	172996
eff4ce86b694568db1b6e8061ba2842fcbc7210f	a design specification and a server implementation of the inverse referential integrity constraints	declarative constraint specification;inclusion dependencies;inverse referential integrity constraint	The inverse referential integrity constraints (IRICs) are specialization of non-key-based inclusion dependencies (INDs). Keybased INDs (referential integrity constraints) may be fully enforced by most current relational database management systems (RDBMSs). On the contrary, non-key-based INDs are completely disregarded by actual RDBMSs, obliging the users to manage them via custom procedures and/or triggers. In this paper we present an approach to the automated implementation of IRICs integrated in the SQL Generator tool that we developed as a part of the IIS*Studio development environment. In the paper the algorithms for insertion, modification and deletion control are presented, alongside with parameterized patterns for their implementation for DBMSs MS SQL Server 2008 and Oracle 10g. It is also given an example of generated procedures/triggers.	algorithm;data integrity;database trigger;microsoft sql server;partial template specialization;referential integrity;relational database management system;server (computing)	Slavica Aleksic;Sonja Ristic;Ivan Lukovic;Milan Celikovic	2013	Comput. Sci. Inf. Syst.	10.2298/CSIS111102003A	referential integrity;computer science;theoretical computer science;database;programming language	DB	-29.191959867882957	15.428549903753927	176976
8c72b0bb61851166dbe9d3fe50c5fa18fcf4303d	distributed access system for uniform and scalbale data and service access	information retrieval geographic information systems distributed databases;layered architecture;kernel;collision mitigation;information systems;application software;information retrieval;computer model;large scale earth science research;scalable data;service access;uniform data;large scale;geoscience;scientific modeling;computational modeling;design and implementation;distributed environment;geographic information systems;collision mitigation computational modeling kernel information systems computer science application software large scale systems geoscience biomedical computing nasa;distributed databases;computational modeling systems;computer science;nasa;large scale earth science research distributed access system scalable data uniform data service access computational modeling systems scientific modeling;biomedical computing;distributed access system;large scale systems	"""Computational modeling systems (CMS) are designed to resolve many of the shortcomings associated with systems currently employed in providing support for a wide range of scientific modeling applications. We identify the requirements of a """"reasonable"""" CMS and identify the requirements of Amazonia, a CMS intended to support modeling in large-scale earth science research. Amazonia has been implemented as an open and layered architecture. In this paper we discuss the design and implementation of the distributed access system, a key component of the Amazonia Kernel that supports the organization of and access to data and services in a distributed environment."""		Anuradha Mahadevan Sastri;Divyakant Agrawal;Amr El Abbadi;Terence R. Smith	1995		10.1109/MASS.1995.528238	application software;kernel;scientific modelling;computer science;theoretical computer science;multitier architecture;data mining;database;computational model;distributed database;information system;distributed computing environment	HPC	-33.42775294943568	12.811177507884592	177311
4d9e53436f4a4260e43a5c1ba1d13d1a02c9133e	remote attribute updating for language-based editors	satisfiability;attribute grammar	A major drawback to the use of attribute grammars in language-based editors has been that attributes can only depend on neighboring attributes in a program's syntax tree. This paper concerns new attribute-grammar-based methods that, for a suitable class of grammars, overcome this fundamental limitation. The techniques presented allow the updating algorithm to skip over arbitrarily large sections of the tree that more straightforward updating methods visit node by node. These techniques are then extended to deal with aggregate values, so that the attribute updating procedure need only follow dependencies due to a changed component of an aggregate value. Although our methods work only for a restricted class of attribute grammars, satisfying the necessary restrictions should not place an undue burden on the writer of the grammar.	aggregate data;algorithm;attribute grammar;parse tree	Thomas W. Reps;Carla Marceau;Tim Teitelbaum	1986		10.1145/512644.512645	l-attributed grammar;attribute domain;computer science;theoretical computer science;s-attributed grammar;data mining;programming language;attribute grammar;algorithm;satisfiability	PL	-26.616757668562226	17.68402110001951	178381
6bf6a7a34520027d52336e032659ee621721e87a	conceptual modelling patterns for roles	modelizacion;aspect dynamique;lenguaje programacion;object oriented language;programming language;web semantique;conceptual analysis;semantics;intelligence artificielle;semantica;semantique;analisis conceptual;modelisation;conceptual schema;conceptual modelling;dynamic aspect;object oriented;web semantica;semantic web;langage programmation;oriente objet;role models;artificial intelligence;aspecto dinamico;inteligencia artificial;analyse conceptuelle;modeling;orientado objeto	Roles are meant to capture dynamic and temporal aspects of realworld objects. The role concept has been used with many semantic meanings: dynamic class, aspect, perspective, interface or mode. This paper identifies common semantics of different role models found in the literature. Moreover, it presents a set of conceptual modelling patterns for the role concept that include both the static and dynamic aspects of roles. In particular, we propose the Role as Entity Types conceptual modelling pattern to deal with the full role semantics. A conceptual modelling pattern is aimed at representing a specific structure of knowledge that appears in different domains. The use of these patterns eases the definition of roles in conceptual schemas. In addition, we describe the design of schemas defined by using the patterns in order to implement them in any object-oriented language.	conceptual schema	Jordi Cabot;Ruth Raventós	2006		10.1007/11617808_6	natural language processing;conceptual model;computer science;artificial intelligence;semantics;programming language;object-oriented programming	AI	-30.451270282686625	13.695596471432156	178782
c93ee5d793fac8baf815498a8f8ec9b3aa1d5f5e	design data management in a distributed hardware environment	circuit cad;distributed databases;cad database system;ic design;design data;distributed data management;distributed hardware environment;environmental schema;logical organization;database system;data management	In this paper we present a database system for IC design that combines an optimal logical organization of the design data with the facilities that are offered by a distributed hardware environment which consists of processors, storage provisions and network facilities. We model the physical, logical and distributed aspects of the hardware environment, yielding an environmental schema. The logical structure of the database system is mapped on this environment, resulting in a distributed data management concept. This concept is illustrated by the architecture of a prototype CAD database system.	central processing unit;computer-aided design;database;integrated circuit design;prototype	G. W. Sloof;Peter Bingley;Patrick Dewilde;T. G. R. van Leuken;Pieter van der Wolf	1990			process design;workstation;electronic design automation;data management;computer science;database model;prototype;distributed design patterns;database schema;distributed database;physical data model;database design;integrated circuit design	DB	-32.900544108329775	12.722757359467506	178896
5e6278afdd3975ad354ff59c8eed445c76343409	synthesizing transformations on hierarchically structured data	data transformations;programming by example;program synthesis	This paper presents a new approach for synthesizing transformations on tree-structured data, such as Unix directories and XML documents. We consider a general abstraction for such data, called hierarchical data trees (HDTs) and present a novel example-driven synthesis algorithm for HDT transformations. Our central insight is to reduce the problem of synthesizing tree transformers to the synthesis of list transformations that are applied to the paths of the tree. The synthesis problem over lists is solved using a new algorithm that combines SMT solving and decision tree learning. We have implemented our technique in a system called HADES and show that HADES can automatically synthesize a variety of interesting transformations collected from online forums.	algorithm;data model;decision tree learning;hierarchical database model;transformers;unix;xml	Navid Yaghmazadeh;Christian Klinger;Isil Dillig;Swarat Chaudhuri	2016		10.1145/2908080.2908088	computer science;theoretical computer science;programming language;data transformation;algorithm	PL	-28.628018199754656	12.845117506200266	179299
ed55e5859eae705fe8d68461ec85a505be876e82	problems for knowledge discovery in databases and their treatment in the statistics interpreter explora	verification;lenguaje programacion;lenguaje natural;base donnee;programming language;sistema informatico;langage naturel;database;base dato;base connaissance;computer system;intelligence artificielle;systeme adaptatif;object oriented;natural language;decouverte connaissance;adaptive system;langage programmation;sistema adaptativo;oriente objet;artificial intelligence;base conocimiento;systeme informatique;knowledge discovery in database;inteligencia artificial;verificacion;orientado objeto;knowledge base	Abstract#R##N##R##N#In this article we describe some goals and problems of KDD. Approaches are presented which have been implemented in the Statistics Interpreter Explora, a prototype assistant system for discovering interesting findings in recurrent datasets. We introduce patterns to identify what is interesting in data and give some examples of patterns for difference-, change-, and trend-detection. Then we summarize what must be specified to define a pattern. Besides some descriptive parts, this includes a procedural verification method. Object-oriented programming techniques can simplify the specializations of general patterns. We identify search as a constituent principle of discovery and introduce object structures as a basis to induce a graph structure on the search space. We mention several strategies for graph search and describe approaches for dealing with the aggregation, redundancy, and overlapping problems. Then we address the presentation of findings in natural language and graphical form, focusing on the methods to design good graphical presentations by knowledge-based techniques. Finally, we discuss the paradigm of an adaptive discovery assistant, including the problem of how to reuse the discovered knowledge for further discovery. © 1992 John Wiley & Sons, Inc.	data mining;database	Willi Klösgen	1992	Int. J. Intell. Syst.	10.1002/int.4550070707	knowledge base;verification;computer science;artificial intelligence;adaptive system;machine learning;data mining;natural language;object-oriented programming;algorithm	ML	-30.066382049982412	14.556341552462033	179404
6b1b88083d5e64dfcacad6c8385d8d4533929d3f	concepts of a data base simulation language	usage distribution;volatility;activity;physical design;cost analysis;storage cost;retrieval cost;level of detail;reorganization cost;deterioration;acquisition cost;file organization model;data structure;database analysis;statistical distribution;database performance;performance modelling	Performance modelling of data base systems requires taking into consideration the complex interactions between the different physical design parameters and the system workload parameters. In order to facilitate a data base designer in evaluating various implementation strategies, a simulation language is presented which has three distinct components (1) data definition (2) query definition and (3) mapping to storage definition. A number of features characterize this type of descriptive mechanism. First, the system workload parameters (1 and 2) must be described in terms of statistical distributions, which implies that the storage structure is subject to the same stochastic variability. Secondly, the level of detail required to describe mappings to storage for simulation purposes is lower than that required by standard data definition or mapping languages. The process of embedding a structure in storage is decomposed into a number of steps, each introducing additional implementation oriented details.	data definition language;database;interaction;level of detail;physical design (electronics);simulation language;spatial variability	Peter Scheuermann	1977		10.1145/509404.509427	probability distribution;physical design;volatility;data structure;database tuning;computer science;cost–benefit analysis;theoretical computer science;level of detail;data mining;database;programming language	DB	-28.696751922218308	12.159790872968152	179680
711e4f746b4698dc7f0f7fc40ce9ae5a83d1008a	a module for improving data access and management in an integrated cad environment	query language;integrated cad environment;pascal source code;design data;modular system;careful database schema generation;complex design object;high level query language;data access;schema analyzer;application program;interactive level performance;data center;design automation;design methodology;project management;navigation;relational databases;environmental management;database languages;source code;application software	A modular system is presented for handling design data, centered around the relational DBMS Ingres, taking advantage of a very careful database schema generation and of the use of software modules to comply with the specific requirements of design data. This work is mainly concerned with one of these modules, called LIPS, which enables the system to handle design data local to an application program with a high level query language, interactive level performances and the ability to support complex design objects. LIPS is composed of two programs: a schema analyzer and a precompiler, and its query language (Loquel) must be embedded in Pascal source code.	computer-aided design;data access;database schema;embedded system;high-level programming language;ingres;object-relational database;pascal;performance;preprocessor;query language;relational database management system;requirement	G. P. Barabino;G. S. Barabino;G. Bisio;Michele Marchesi	1985	22nd ACM/IEEE Design Automation Conference	10.1145/317825.317948	project management;embedded system;electronic design automation;computer science;operating system;data mining;database;programming language;query language	EDA	-31.280400258916533	12.231257517770688	180492
1f5c68ad32ad7f866668c6a9c7372aa2cc0e7968	the extraction of feature points from dem geographic data in cloud computing environment	mpi feature points extraction dem geographic data cloud computing parallel processing technology big data calculation network storage digital elevation model large scale regular grid windows azure message passing interface;feature points cloud computing windows azure;data mining;feature extraction;parallel processing cloud computing digital elevation models geographic information systems message passing;program processors;feature extraction data mining program processors	As the technology of obtaining geographic data continues to update, we can get geographic data more conveniently and efficiently. However, handling a large number of geographic data becomes the bottleneck of geography. As a new parallel processing technology, Cloud Computing shows an excellent performance in the big data calculation and network storage. Digital Elevation Model (DEM) shows the fluctuation of the terrain feature and includes the structure information of landform, for example the valley points and the peak points. The points play a very important position when we reconstruct the surface and reappear the surface under the multi-scale. In order to solve the problem of extracting the feature points' inefficiency in a large-scale regular grid, the paper proposed one thinking which is that we create one virtual net on the Windows azure and use the Message Passing Interface (MPI) to construct a parallel environment to extract the feature points in the regular grid which is incised with one design. By comparing the time parallel computing and serial computing consume, we can get that the parallel computing can improve the efficiency. This thinking can help us apply the Cloud Computing on the analysis of large-scale geography data.	big data;cloud computing;digital elevation model;geographic information system;message passing interface;microsoft azure;parallel computing;quantum fluctuation;regular grid	Shengming Wang;Yumin Chen;Yongfeng Liu;Qianjiao Wu;Hang Chen;Xiaoxiao Zhu	2015	2015 23rd International Conference on Geoinformatics	10.1109/GEOINFORMATICS.2015.7378594	computer science;theoretical computer science;data-intensive computing;data mining;database	HPC	-31.590205326621277	17.942370268766933	180641
44b033728cf14462e9ec02b0448c1c073cda9bee	relating diagrams to logic	graphics system;conceptual graph;petri net;state transition diagram;entity relationship;knowledge interchange format	Although logic is general enough to describe anything that can be implemented on a digital computer, the unreadability of predicate calculus makes it unpopular as a design language. Instead, many graphic notations have been developed, each for a narrow range of purposes. Conceptual graphs are a graphic system of logic that is as general as predicate calculus, but they are as readable as the specialpurpose diagrams. In fact, many popular diagrams can be viewed as special cases of conceptual graphs: type hierarchies, entity-relationship diagrams, parse trees, dataflow diagrams, flow charts, state-transition diagrams, and Petri nets. This paper shows how such diagrams can be translated to conceptual graphs and thence into other systems of logic, such as the Knowledge Interchange Format (KIF). 1 Representing Logic in Graphs	chart;computer;conceptual graph;dataflow;entity–relationship model;first-order logic;flowchart;formal system;knowledge interchange format;parse tree;parsing;petri net;state diagram;state transition table	John F. Sowa	1993		10.1007/3-540-56979-0_1	conceptual graph;knowledge interchange format;state diagram;entity–relationship model;computer science;artificial intelligence;theoretical computer science;database;petri net	Logic	-26.742858163850656	17.999055966393986	181073
5fe203371c00ec9de43bd2990d9c382afa8809d7	relationship abstractions for an effective hypertext design: augmentation and globalization		Data abstractions, i.e., aggregation and generalization, are useful for representing complex objects e ectively. They provide high level semantic constraints as well as extend the capabilities of entity description in the E-R model. However, corresponding concepts of relationship abstraction are not directly available, particularly in hypertext systems. We propose two types of relationship abstractions, augmentation and globalization, aiming at the improvement of relationship design phases. The former is an abstraction which turns information held in relationships into that of attributes for existing entities. The latter is an abstraction which generates global-to-local relationship hierarchies. We show the advantages of these abstractions.	entity;entity–relationship model;high-level programming language;hypertext;information	Yoshinori Hara;Arthur M. Keller;Gio Wiederhold	1991			simulation;human–computer interaction;computer science;multimedia	DB	-30.71630217251456	11.644106150727952	181182
72325f1b581514cc18c922fb1d261596600f872d	temporal aggregation in active database rules	active database system;management system;ucla;active database;temporal data;object database;time series;garbage collection;expressive power;temporal aggregation;partitions;cyclic garbage;temporal reasoning;knowledge discovery	An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any. In this paper, we introduce the language TREPL, which addresses this problem. The TREPL prototype, under development at UCLA, offers primitives for temporal aggregation that exceed the capabilities of state-of-the-art composite event languages, and are comparable to those of TSMS languages. TREPL also demonstrates a rigorous and general approach to the definition of composite event language semantics. The meaning of a TREPL rule is formally defined by mapping it into a set of Datalog1S rules, whose logic-based semantics characterizes the behavior of the original rule. This approach handles naturally temporal aggregates, including user-defined ones, and is also applicable to other composite event languages, such as ODE, Snoop and SAMOS.	active database;expressive power (computer science);logic programming;management system;open dynamics engine;prototype;temporal database;time series;snoop	Iakovos Motakis;Carlo Zaniolo	1997		10.1145/253260.253359	computer science;time series;data mining;management system;database;temporal database;knowledge extraction;garbage collection;programming language;expressive power;algorithm	DB	-28.417783054818305	12.969882752900203	181729
901224e8228ebf3e5168ca23d395c831b306b6a6	aspects of the classification dependency in the integration of structural karlstad enterprise modelling schemata	systemvetenskap informationssystem och informatik;information systems;it is management and development methodologies	In this paper, we address the classification dependency used to illustrate that one concept is an instance-of another concept. In doing so, we present four important aspects of the classification dependency in the integration of implementation-neutral Karlstad Enterprise Modelling schemata. First and second, the classification dependency can be used not only to recognise and resolve power types but also to recognise and resolve homonyms. Third, in inference rules, the classification dependency can be used to deduce both new concepts and dependencies from already existing ones. Fourth, the classification dependency can be used to counter the occurrence of semantic loss and maintain the vocabulary used in the source schemata. The classification dependency can even be used to semantically enrich the integrated conceptual schema. The four aspects of the classification dependency should also be viewed as important aspects to take into account during the development of a semi-automatic method for the integration of implementation-neutral structural EM schemata. Finally, by applying the classification dependency, several problems, such as power types and homonyms, might be recognised that otherwise could pass unnoticed in the integration process.	conceptual schema;enterprise modelling;semiconductor industry;vocabulary	Peter Bellström	2012			computer science;artificial intelligence;data mining;database;information system	Web+IR	-32.40385543448475	13.881334890548642	182386
616207e611b43649ee526eb37515ae27704b18f2	query processing for complex objects	complex objects;query language;optimisation;architecture systeme;prima;optimizacion;query processing;implementation;sistema informatico;interrogation base donnee;interrogacion base datos;query optimization;computer system;lenguaje interrogacion;data model;mad;ejecucion;object oriented;komplexes objekt;compiler optimization;relational model;oriente objet;arquitectura sistema;optimization;systeme informatique;modele donnee;langage interrogation;query translation;systeme gestion base donnee;information system;system architecture;nonstandard datenbanksystem;sistema gestion base datos;orientado objeto;database management system;article;database query;systeme information;relationale datenbank;sistema informacion	Over the last few years several new data models together with their languages have been developed to meet the increasing requirements of engineering or office applications. A major characteristic of these data models is their ability to process and manage complex objects which the relational model does not provide adequate support for. Whereas the problem of query translation for relational languages has provoked broad research activities during the last fifteen years, the analogous problem of translating nonprocedural queries on complex objects into lower level programs for efficient execution has received only little attention. This paper tries to reveal the new aspects of query translation and execution on complex objects as compared to similar activities when processing flat relations. For this purpose, we investigate the essential concepts necessary to perform compilation, optimization, and execution of queries on complex objects.	compiler;data model;database;mathematical optimization;relational model;requirement	Theo Härder;Bernhard Mitschang;Harald Schöning	1991	Data Knowl. Eng.	10.1016/0169-023X(92)90037-C	query optimization;relational model;data model;computer science;artificial intelligence;optimizing compiler;database;object-oriented programming;implementation;information system;algorithm;query language;systems architecture	DB	-28.544226814483974	11.771024402626947	182858
b501a31aaa171b035383f3181c83b978fbc9e453	object-oriented heterogeneous database for materials science	computerized databases;materials science;available computerized databases;object-oriented database;materials scientist;relevant external data source;heterogeneous external data source;object-oriented heterogeneous database;materials science data;new data;diverse databases;physical property data;object oriented	As a part of the scientific database research underway at the Oregon Graduate Institute, we are collaborating with materials scientists in the research and development of an extensible modeling and computation environment for materials science. Materials scientists are prolific users of computers for scientific research. Modeling techniques and algorithms are well known and refined, and computerized databases of chemical and physical property data abound. However, applications are typically developed in isolation, using information models specifically tailored for the needs of each application. Furthermore, available computerized databases in the form of CDs and on-line information services are still accessed manually by the scientist in an off-line fashion. Thus researchers are repeatedly constructing and populating new custom databases for each application. The goal of our research is to bridge this gulf between applications and sources of data. We believe that object-oriented technology in general and data-bases in particular, provide powerful tools for transparently bridging the gap between programs and data. An object-oriented database that not only manages data generated by user applications, but also provides access to relevant external data sources can be used to bridge this gap. An object-oriented database for materials science data is described that brings together data from heterogeneous non-object-oriented sources and formats, and presents the user with a single, uniform object-oriented schema that transparently integrates these diverse databases. A unique multilevel architecture is presented that provides a mechanism for efficiently accessing both heterogeneous external data sources and new data stored within the database.	heterogeneous database system	David M. Hansen;David Maier;James Stanley;Jonathan Walpole	1992	Scientific Programming		data modeling;database theory;computer science;data administration;operating system;data mining;database;object-oriented programming;world wide web;computer security;database testing;database design	HPC	-33.513804237855865	12.872845826423815	182903
bce007c7257c7e86d3c38ba245189598fcdb52eb	active database support for step/express models	active database;dynamic behaviour;modelling language;object oriented database;expert system	The ISO STEP speci®cations aim to provide an eective means through which product information can be shared and exchanged between applications and enterprises. EXPRESS is a modelling language within the STEP speci®cations, which is used to describe product data. An EXPRESS model can be implemented on a database repository, queried upon and manipulated. However, EXPRESS models capture only the static aspects of a system. In this paper, proposed extensions to the EXPRESS language are made in order to model the dynamic behaviour of products and processes. A translator is implemented to transform such models to an active database, which has been developed on an object-oriented database, ObjectStore. The active component of the system is based on CLIPS, an expert system. Throughout the paper, a case based on a work ̄ow process is used to demonstrate how a model can be mapped onto an active database.	action algebra;active database;boolean expression;clips;data model;declaration (computer programming);event condition action;expert system;function model;iso 10303;inference engine;modeling language;objectstore;parameter (computer programming)	Yue Dong;C. F. Y. Chee;Yang He;A. Goh	1997	J. Intelligent Manufacturing	10.1023/A:1018581426556	database tuning;computer science;systems engineering;artificial intelligence;database model;data mining;database;database schema;expert system;database testing;database design	DB	-32.95202899443296	11.618919880848207	183156
f3f505967d0e9047260749900bfad5644f7ce823	problem-solving methods		State Machines [Gurevich, 1994]. Basically, MLPM and MCL extend MLCM with new elementary state transition types which cover the grainsize of state transitions in knowledge-based reasoning. As a consequence, we get an approach that integrates existing proposals, overcomes several of their shortcomings and ad-hoc solutions, and provides an axiomatization which enables the use of mechanized proof support. The structure of this chapter is as follows. First, we introduce the knowledge specification languages (ML) 2 and KARL focusing on their dynamics. We use the experience with these languages to derive requirements for an appropriate semantic framework for the specification of the dynamics of the reasoning of knowledge-based systems. Then we introduce the logics MLPM and MCL and provide their syntax and semantics. We use MCL to formalize the inference and control constructs of the KADS languages and Abstract State Machines and provide a comparison with work that uses different solutions. 4.1 Specification Languages for Knowledge-Based Systems In this subsection we introduce the two languages KARL and (ML) 2 , focusing on their formal means for specifying the reasoning process of knowledge-based systems. Both use variants of the CommonKADS model of expertise as conceptual framework (i.e., system architecture) for specifying a knowledge-based system. CommonKADS [Schreiber et al., 1994] uses task and inference layers for specifying the reasoning process. The task layer introduces the goal that is to be achieved by the system and it decomposes the overall task into subtasks and defines control over them. It combines a functional specification with the specification of the dynamic reasoning process that realizes the functionality. The inference layer defines the elementary inference steps, the relations between them, and the role of the domain knowledge for the reasoning process. A simple example will be used to illustrate the modeling concepts of both languages (see Fig. 21). The task of the knowledge-based system consists of finding the diagnosis with the highest preference for a given set of symptoms. Our example consists of two inference actions: • generate , which creates possible hypotheses based on the given findings and the causal relationships at the domain layer, and • select , which assigns a preference to hypotheses and selects the diagnosis with the highest preference. The knowledge role finding provides input to the inference action generate , the knowledge role hypothesis delivers the results of the reasoning of generate to select , and the knowledge role diagnosis provides the results of select as output. The two knowledge roles causality and preference provide knowledge necessary for the inference process. It is mapped from the domain layer. A simple control flow at the task layer is defined by first executing generate and then applying select to its output. 1) COLD, Common Object-oriented Language for Design, was developed at Phillips Research Eindhoven in several ESPRIT-projects (cf. [Feijs & Jonkers, 1992]). 4 Logics for Knowledge-Based Systems: MLPM and MCL 63	abstract state machines;axiomatic system;business logic;causality;control flow;finite-state machine;functional specification;hoc (programming language);knowledge acquisition and documentation structuring;knowledge-based systems;macintosh common lisp;requirement;state transition table;systems architecture	Dieter Fensel	2000		10.1007/3-540-44936-1	computer science	AI	-27.697896109669422	17.261976632123503	184004
a3520a8a16f76b6148ff89e2ee39d24aa7784353	versions and change notification in an object-oriented database system	cad/cam;artificial intelligence;database management systems;office automation;(artificial-intelligence);ai;cad/cam;orion;change notification;computer-aided-design/computer-aided-manufacturing;dynamic schema evolution;object-oriented data model;object-oriented database system;office-information-system	At MCC we have built a prototype object-oriented database system, called ORION to support applications from the CAD/CAM, AI, and OIS domains. Advanced functions supported in ORION include versions, change notification, composite objects, dynamic schema evolution, and multimedia data. The versions and change notification features were based on the model we developed earlier in [CHOU86]. In this paper, we show how we have integrated our model of versions and change notification into the ORION object-oriented data model, and also provide an insight into system overhead that versions and change notification incur.	computer-aided design;data model;database;overhead (computing);prototype;schema evolution	Hong-Tai Chou;Won Young Kim	1988			intelligent database;computer science;systems engineering;data mining;database;software testing;combinational logic;database schema;algorithm;database design;computer-aided technologies	DB	-32.61253775348683	11.726732481973158	184364
512f669b8979e0fa70388ffb56fee931838c9c5e	эволюционирующие онтологии в аспекте управления темпоральными или изменяющимися фактами (evolving ontologies in the aspect of handling temporal or changeable artifacts)		We propose an algebraic approach to building ontologies which capable of evolution under the influence of new facts and which have some internal mechanisms of validation. For this purpose we build a formal model of the interactions of objects, and find out the limitations on transactions with objects imposed by this model. Then, in the context of the formal model, we define basic entities of the model of knowledge representation: concepts, samples, properties, and relationships. In this case the formal limitations are induced into the model of knowledge representation in a natural way.	entity;interaction;knowledge representation and reasoning;mathematical model;ontology (information science)	Aleksey Demidov	2015			natural language processing;ontology (information science);artificial intelligence;computer science	ML	-30.99886925641841	13.528737632182603	184893
f28b6760f391e82849eceb2a6e4b025fadd45f87	spin! data mining system based on component architecture	extraction information;distributed system;interfase usuario;systeme reparti;algorithm analysis;analisis datos;information extraction;componente logicial;user interface;composant logiciel;program verification;data mining;dominio trabajo;data analysis;software architecture;verificacion programa;sistema repartido;fouille donnee;domaine travail;decouverte connaissance;data visualization;software component;descubrimiento conocimiento;analyse donnee;interface utilisateur;visualisation donnee;analyse algorithme;workspace;component architecture;verification programme;busca dato;extraccion informacion;analisis algoritmo;architecture logiciel;knowledge discovery	The SPIN! data mining system has a component-based architecture, where each component encapsulates some specific functionality, e.g., it can be a data source, an analysis algorithm or visualization module. Individual components can be visually linked within one workspace for solving different data mining tasks. The SPIN! convenient user interface and flexible underlying component architecture provide a powerful integrated environment for executing main tasks constituting a typical data mining cycle: data preparation, analysis, and visualization. 1 Component Architecture SPIN! data mining system has a component architecture. This means that it provides only an infrastructure and environment while all the system functionality comes from separate software modules called components. Components can be easily plugged-in the system thus allowing for an expansion of its capabilities. In this sense it is very similar to such general purpose environments as Eclipse. Each component is developed as an independent module for solving one or a limited number of tasks. For example, there may be components for data access, analysis or visualization. In order to solve complex problems components need to communicate and use each other. All components are implemented on the basis of CoCon Common Connectivity Framework, which is a set of generic interfaces and objects in Java and allows components to communicate within one workspace.. The idea is that components can be connected by means of different types of connections. Currently there exist three connections: visual, hierarchical and user defined. Visual connections are used to link a component with its view (similar to Model-View-Controller architecture). Hierarchical connections are used to compose parent-child relationships among components within one workspace, e.g., between folder and its elements. The third and the main type is the user connection, which is used to arbitrary link components in the workspace according to the task to be solved (like in Clementine). It is important that components explicitly declare connectivity capabilities, i.e., how they can be connected and with what other components they can work. 2 Workspace Management The SPIN! system is configured to include some set of components and then it automatically updates its main menu, tool bar and other functions (Fig. 1). An importance of such extensibility for data mining has been stressed in [5]. Workspace is a set of components and connections among them. It can be stored in or retrieved from a persistent storage. Workspace appears in two views: tree view and graph view. In tree view the hierarchical structure of the workspace is visualized where components are individual tree nodes, which can be expanded or collapsed. In graph view components are visualized as nodes of the graph while user connections are graph edges. Components can be added to the workspace by choosing them either in menu or in tool bar. After a component has been added it should be connected with other relevant components. An easy and friendly way to do this consists in drawing an arrow from the source component to the target one. While adding connections between components the environment uses information about their connectivity so that only components, which are able to cooperate, can be really connected. Fig. 1. The SPIN! Data mining system client interface: workspace (upper left window), rule base (upper right window), database connection (lower left window), database query and algorithm (lower right windows). Each component has an appropriate view, which is also a connectable component. Each component can be opened in a separate window so that the user can use its functions. When a workspace component is opened the system automatically creates a view, connects it with the model and then displays it within window. 3 Running Data Mining Algorithms The typical data mining tasks include data preprocessing, analysis and visualization. For data access the SPIN! system includes Database Connection and Database Query components. The Database Connection is intended for storing information about the database where the data is stored. To use this database this component need to be connected with some other component, e.g., in graph view. The Database Query component describes one query, i.e., how its result set is generated from tables in the database. Essentially this component is a SQL query design tool, which allows for describing a result set by choosing tables, columns, restrictions, functions etc. Notice also that both Database Connection and Database Query components do not work by themselves and it is some other components that makes use of them. Such encapsulation of functionality and use of user connections to compose various aggregates has been one of the main design goals of the SPIN! component architecture. Any knowledge discovery task includes data analysis step where the dataset obtained from preprocessing step is processed by some data mining algorithm. The SPIN! system currently includes several data mining algorithm components, e.g., subgroup discovery [1], rule induction based on empty intervals in data [4], spatial association rules [2], spatial cluster analysis, Bayesian analysis. To use some algorithm, say, Optimist rule induction [4], we need to add this component in the workspace and connect it with Database Query where the data is loaded from and Rule Base component where the result is stored. The algorithm can be started by pressing the Start button in its view. After that it runs in a separate thread either on the client or within Enterprise Java Bean container on the server [3]. The rules generated by the algorithm are stored in Rule Base component connected to the algorithm. The rules can be visualized and studied by opening this component in a separate view.	algorithm;association rule learning;clementine;cluster analysis;column (database);comparison of command shells;component-based software engineering;data access;data mining;data pre-processing;database;design tool;eclipse;encapsulation (networking);enterprise javabeans;existential quantification;extensibility;java platform, enterprise edition;microsoft windows;model–view–controller;modular programming;persistence (computer science);plug-in (computing);preprocessor;query language;result set;rule induction;rule-based system;spin;sql;select (sql);server (computing);user interface;workspace	Alexandr A. Savinov	2004		10.1007/978-3-540-30116-5_64	software architecture;simulation;common component architecture;computer science;component-based software engineering;data mining;database;knowledge extraction;data stream mining;data analysis;user interface;information extraction;data visualization;workspace;data architecture	ML	-31.266831156094483	13.294006995002148	185967
a3d069cba4e95b307070ec642e013347acefa891	generating example data for dataflow programs	example data;indexation;biased sampling;dataflow programming	While developing data-centric programs, users often run (portions of) their programs over real data, to see how they behave and what the output looks like. Doing so makes it easier to formulate, understand and compose programs correctly, compared with examination of program logic alone. For large input data sets, these experimental runs can be time-consuming and inefficient. Unfortunately, sampling the input data does not always work well, because selective operations such as filter and join can lead to empty results over sampled inputs, and unless certain indexes are present there is no way to generate biased samples efficiently. Consequently new methods are needed for generating example input data for data-centric programs.  We focus on an important category of data-centric programs, dataflow programs, which are best illustrated by displaying the series of intermediate data tables that occur between each pair of operations. We introduce and study the problem of generating example intermediate data for dataflow programs, in a manner that illustrates the semantics of the operators while keeping the example data small. We identify two major obstacles that impede naive approaches, namely (1) highly selective operators and (2) noninvertible operators, and offer techniques for dealing with these obstacles. Our techniques perform well on real dataflow programs used at Yahoo! for web analytics.	dataflow programming;sampling (signal processing);web analytics	Christopher Olston;Shubham Chopra;Utkarsh Srivastava	2009		10.1145/1559845.1559873	real-time computing;sampling bias;computer science;theoretical computer science;database;programming language	DB	-28.112904373779894	16.697604083119103	186975
095ff084a38c490326206d11d314f269412b5b12	the goql language and its formal specifications	graphical query languages;formal specifications;query language;oodbm;object query language;formal specification	The Graphical Object Query Language (GOQL) is a graphical query language that complies with the ODMG standard and runs on top of the o2 DBMS. The language provides users with the User’s View (UV) and the Folders Window (FW), which serve as the foundation upon which end-users can pose ad-hoc queries. The UV is a graphical representation of any underlying ODMG scheme. Among its advantages is that it hides from end-users most of the perplexing details of the object-oriented database model, such as methods, hierarchies and relationships. To achieve this, the UV does not distinguish between methods, attributes and relationships, it encapsulates is -a hierarchies and it utilises a number of desktop metaphors whose semantics can be easily understood by end-users. The FW is a condensed version of the UV and provides the starting point for constructing queries. In this paper, we demonstrate, using an example, the UV and the FW and the way they support the construction of graphical queries. We then present the formal specifications of the language. We first give a formal definition of an object-oriented database schema in the GOQL model. The UV is then formally defined as a mapping from a GOQL object-oriented database schema. The formal definition of the UV allows us to formally define the graphical constructs of GOQL and the syntax analysis of the language.	database model;database schema;desktop computer;desktop metaphor;formal language;formal specification;graphical user interface;hoc (programming language);object data management group;object query language;parsing;window function	Euclid Keramopoulos;Philippos Pouyioutas;Tasos Ptohos	2008	IJCSA		object definition language;data control language;database;programming language;object language;data definition language;query by example;database schema;rdf query language;computer science;sargable	PL	-30.81137136396003	12.465267616287996	187141
6ba1aabbd8d3195a0c6dde4743ce0ce9c5e94bee	object-oriented data modelling for spatial databases	object oriented data model;geographic information system;object oriented design;spatial database;united kingdom;relational model;object oriented approach;data modelling;object oriented modelling;database design;multimedia database;entity relationship;graphic information system	Data modelling is a critical stage of database design. Recent research has focused upon object-oriented data models, which appear more appropriate for certain applications than either the traditional relational model or the entity-relationship approach. The object-oriented approach has proved to be especially fruitful in application areas, such as the design of geographical information systems which have a richly structured knowledge domain and are associated with multimedia databases. This article discusses the key concepts in object-oriented modelling and demonstrates the applicability of an object-oriented design methodology to the design of geographical information systems. In order to show more clearly how this methodology may be applied, the paper considers the specific object-oriented data model IFO. Standard cartographic primitives are represented using IFO, which are then used in the modelling of some standard administrative units in the United Kingdom. The paper concludes by discussing current r...	data modeling;spatial database	Michael F. Worboys;Hilary M. Hearnshaw;David J. Maguire	1990	International Journal of Geographical Information Science	10.1080/02693799008941553	data modeling;relational model;entity–relationship model;data model;computer science;data science;object-oriented design;database model;data mining;database;geographic information system;spatial database;database design	DB	-31.633586734160506	12.750593955829943	187983
93e7d0199198e9c5bc5f4ebabc4d0e230fce9d8a	a multi-representation spatial data model	multiple representation;control dimensional;base donnee repartie;base donnee;dimensional control;distributed database;representacion espacial;red www;resolution spatiale;resolucion espacial;spatial data;implementation;reseau web;database;base repartida dato;base dato;metric;analyse multiresolution;spatial database;controle dimensionnel;multi dimensional;internet;level of detail;base donnee spatiale;multiple database;spatial representation;world wide web;metrico;representation spatiale;modele donnee;multi resolution;implementacion;multiresolution analysis;multibase;metrique;analisis multiresolucion;data models;spatial resolution	Geo-referenced information is characterised by the fact that it may be represented on maps at different levels of detail or generalisation. Ideally a spatial database will provide access to spatial data across a continuous range of resolution and multiple levels of generalisation. Existing work on multiresolution databases has treated generalisation control as one-dimensional. Here we extend the concept of multi-resolution spatial databases to provide support for multiple representations with variable resolution. Therefore the controls on generalisation become multi-dimensional with spatial resolution as one dimension and various types of generalisation style metrics as the other dimensions. We present a multi-representation spatial data model based on this approach and illustrate the implementation of multi-representation geometry in association with an online web demonstration.	cartographic generalization;class hierarchy;data model;futures studies;level of detail;map;multiresolution analysis;object type (object-oriented programming);sdb (debugger);software incompatibility;source data;spatial database;storage efficiency	Sheng Zhou;Christopher B. Jones	2003		10.1007/978-3-540-45072-6_23	multiresolution analysis;data modeling;computer vision;the internet;image resolution;metric;computer science;level of detail;data mining;database;spatial analysis;implementation;distributed database;spatial database	DB	-30.403200087682173	12.959074532767511	188772
f495751862c81551b29fc3e423d5a4253462fea8	using the f2 oodbms to support incremental knowledge acquisition	knowledge based system;ripple down rules;knowledge acquisition object oriented databases spatial databases database systems knowledge based systems data engineering knowledge engineering;knowledge based systems knowledge acquisition object oriented databases;object oriented database management system;knowledge acquisition;update mechanisms incremental knowledge acquisition ripple down rules knowledge base exceptions nested ripple down rules domain concepts database schema evolution f2 object oriented database management system multiple instantiation object migration multiobjects feature;object oriented databases;schema evolution;knowledge based systems;knowledge base	Ripple down rules (RDR) is an incremental knowledge acquisition (KA) methodology, where a knowledge base (KB) is constructed as a collection of rules with exceptions. Nested ripple down rules (NRDR) is an extension of this methodology which allows the expert to enter her/his own domain concepts and later refine these concepts hierarchically. In this paper we show similarities between incremental knowledge acquisition and database schema evolution, and propose to use the F2 object-oriented database management system (OODBMS) to implement an NRDR knowledge based system. We use the existing non-standard features of F2 and show how multiple instantiation and object migration (known as multiobjects feature in F2), and schema evolution capabilities in F2 easily accommodate all the update mechanisms required to incrementally build an NRDR KB. We illustrate our approach with a KA session.	knowledge acquisition	Lina Al-Jadir;Ghassan Beydoun	2002		10.1109/IDEAS.2002.1029679	knowledge base;computer science;knowledge management;knowledge-based systems;open knowledge base connectivity;data mining;database;knowledge extraction	DB	-32.41521810490786	11.292666881685275	188782
b05f806fe4f5f07822736a509ff00bbe00691593	a new version of gtxl	working group;graph transformation	GTXL (Graph Transformation Exchange Language) is designed to support and stimulate developers to provide their graph transformation-based tools with an exchange functionality regarding the integration with other tools. For this exchange XML was chosen as underlying technology. The exchange of graphs is facilitated by the exchange format GXL which is also XML-based. GTXL uses GXL to describe the graph parts of a graph transformation system. A first version of GTXL arose from the format discussion within the EU Working Group APPLIGRAPH. Trying to restimulate the discussion on a common exchange format for graph transformation systems, this paper presents a new version of GTXL. Three important changes have been made. At first, an integrated presentation of rules is introduced, secondly the expression of more general conditions is supported and finally the storage of the underlying semantics of a graph transformation system by means of special attributes is proposed.	gxl;graph rewriting;xml	Leen Lambers	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.12.030	wait-for graph;working group;computer science;theoretical computer science;database;graph;algorithm;graph rewriting	DB	-31.346547525489427	15.4415803836199	188796
82de298573bb12ff80a615d291514add5c48274b	consistent document engineering		When a group of authors collaboratively edits interrelated documents, consistency problems occur almost immediately. Current document management systems (DMSs) provide useful mechanisms such as document locking and version control, but often lack consistency management facilities. At best, consistency is “defined” via informal guidelines, which do not support automatic consistency checks. In this thesis, we complement traditional DMSs by consistency management. We propose to use formal consistency rules that capture semantic consistency requirements. Rules are formalized in a variant of temporal logic. A static type system supports rule formalization, where types also define (formal) document models. In implementing a tolerant view of consistency, we do not expect that the documents satisfy consistency rules. Instead, our novel semantics precisely pinpoints inconsistent document parts and indicates when, where, and why documents are inconsistent. Speed is a key issue in consistency management. Therefore, we develop efficient techniques for consistency checking while retaining our tolerant semantics. Just pinpointing inconsistencies is, however, insufficient for flexible consistency management. We extend our consistency checking approach towards suggesting repairs, which resolve inconsistencies. The critical issues are to suggest only some of the best (i.e., least costly) repairs and to generate repairs efficiently. Therefore, we develop a new two-step approach. First, we employ directed acyclic graphs (DAGs) to carry repairs. These graphs are called suggestion DAGs (short: S-DAGs). In contrast to the enumeration of all possible repairs, S-DAGs provide a suitable means to generate repairs efficiently and to limit the search space for good repairs. Second, from S-DAGs, we derive one repair collection for all consistency rules. Due to the separation of repair derivation from S-DAG generation, the repository is locked during the computationally cheap S-DAG generation only. We have implemented a prototype of a consistency management tool. Our case study in the field of software engineering shows that our contributions can significantly improve consistency management in document engineering and scale to a practically relevant problem size.	analysis of algorithms;directed acyclic graph;document engineering;eventual consistency;lock (computer science);processor consistency;prototype;requirement;software engineering;temporal logic;type system;version control	Jan Scheffczyk	2004			systems engineering;computer science;document engineering	DB	-26.67989509386686	13.69198060043107	188939
58485fac158c3443424ba608e77d12f1bf7d2ca5	dbfw: a simple database framework for the evaluation and maintenance of automated theorem prover data	databases;libraries;database operations;dbfw;electrical capacitance tomography;chaos;setheo proof data;automated theorem prover;automatic testing;statistical databases;truth maintenance theorem proving interactive systems deductive databases statistical databases;interactive proof systems;data mining;text files;theorem proving;theorem prover;feedback;automatic object extraction;integrated logical functions;text based interface;integrated logical functions dbfw database implementation framework automated theorem prover data evaluation data maintenance medium sized datasets automatic object extraction text files text based interface database operations document generation setheo proof data statistics usage feedback ilf interactive proof system;database implementation framework;medium sized datasets;data maintenance;statistics;ilf interactive proof system;databases data mining electrical capacitance tomography libraries automatic testing read only memory statistics feedback chaos;data evaluation;interactive systems;document generation;read only memory;usage feedback;truth maintenance;deductive databases	This paper describes a simple yet generic database implementation framework for medium sized datasets, as they occur during tests and applications of automated theorem provers. The implementation covers automatic extraction of database objects from a set of text les, a text-based interface for simple database operations, and a tool for document, report and Webpage generation. This paper refers to a database of SETHEO proof data (Setheodb) as an example. It concludes with the description of DBFW as a part of the interactive proof system ILF 2].	automated theorem proving;database;document;interactive proof system;performance tuning;postgresql;proof calculus;software testing;text-based (computing);text-based user interface;transbase;web page;world wide web	Peter Jakobi;Andreas Wolf	1998		10.1109/CSMR.1998.665799	database theory;intelligent database;computer science;theoretical computer science;database model;data mining;database;automated theorem proving;programming language;view;database schema;physical data model;database testing;database design	DB	-29.903468873944927	16.358868666404742	189319
f07e36a6bfc1b9e8c2c3465b3d073f3ea749c9d7	from core ocl invariants to nested graph constraints		Meta-modeling including the use of the Object Constraint Language (OCL) forms a well-established approach to design domainspecific modeling languages. This approach is purely declarative in the sense that instance construction is not needed and not considered. In contrast, graph grammars allow the stepwise construction of instances by the application of transformation rules. In this paper, we consider meta-models with Core OCL invariants and translate them to nested graph constraints for typed attributed graphs. Models and meta-models are translated to instance and type graphs. We show that a model satisfies a Core OCL invariant iff its corresponding instance graph satisfies the corresponding nested graph constraint. The aim of this work is to establish a first formal relation between meta-modeling and the theory of graph transformation including constraints to come up with an integrated approach for defining modeling languages in an optimal way in the future.	declarative programming;graph rewriting;metamodeling;modeling language;object constraint language;stepwise regression	Thorsten Arendt;Annegret Habel;Hendrik Radke;Gabriele Taentzer	2014		10.1007/978-3-319-09108-2_7	lattice graph;combinatorics;discrete mathematics;null model;directed graph;null graph;graph property;constraint graph;clique-width;forbidden graph characterization;comparability graph;graph automorphism;mathematics;voltage graph;graph;programming language;complement graph;algorithm;graph rewriting	PL	-29.148241674706465	16.18815125662907	189673
0b313b4a2e532f99ea77f7112ed95fa75ea76bce	object-oriented data models: a perspective and comparative review	modelizacion;complex objects;knowledge base management system;engineering design;object oriented data model;articulo sintesis;aplicacion;article synthese;modelisation;conceptual schema;object oriented database management system;object oriented;oriente objet;object oriented database;systeme gestion base donnee;information system;application;review;modeling;sistema gestion base datos;orientado objeto;database management system;object oriented paradigm	This paper reviews the research efforts in the area of object-oriented database modelling. The object-oriented paradigm and abstraction mechanisms are briefly discussed. The conceptual schema and data operations in an object-oriented data model are described. Ongoing efforts in creating object-oriented DBMS are classified into three categories; (1) those that are directly based on the object-oriented paradigm; (2) extensions to relational systems; (3) experimental toolkits (or storage systems). A review of the salient features of some sample prototype efforts is presented. This is followed by a comparison of these prototype models on the basis of criteria essential to database modelling	data model	Neelam Bhalla	1991	J. Information Science	10.1177/016555159101700302	systems modeling;computer science;conceptual schema;artificial intelligence;data mining;database;object-oriented programming;information system;engineering design process	NLP	-31.781868916248573	13.02229514108983	190144
6ba90225e4345b32896046ec0c83cde9b1fb734e	reducing inference control to access control for normalized database schemas	databases;safety security in digital systems;base relacional dato;controle acces;confidencialidad;procesamiento informacion;algorithm analysis;securite;controlled query evaluation;efficiency;coaccion;interrogation base donnee;contrainte;database;safety systems;interrogacion base datos;base dato;68p15;pregunta documental;relational database;systeme numerique;confidentiality;confidentialite;eficacia;digital system;constraint;inference control;informatique theorique;query evaluation;digital systems;safety;information processing;base de donnees;decision;query;efficacite;systeme securite;base de donnees relationnelle;sistema numerico;preservation;evaluation;analyse algorithme;access control;evaluacion;traitement information;seguridad;preservacion;database query;analisis algoritmo;requete;computer theory;informatica teorica	Considering relational databases, controlled query evaluation preserves confidentiality even under inferences but at the expense of efficiency. Access control, however, enables efficiently computable access decisions but cannot automatically assure confidentiality because of missing inference control. In this paper we investigate constraints sufficient to eliminate (nontrivial) inferences in relational databases with the objective of replacing controlled query evaluation by access control mechanisms under preservation of confidentiality.	access control;database schema	Joachim Biskup;David W. Embley;Jan-Hendrik Lochner	2008	Inf. Process. Lett.	10.1016/j.ipl.2007.09.007	confidentiality;information processing;relational database;computer science;access control;evaluation;data mining;database;efficiency;constraint;system safety;preservation;computer security	DB	-26.877592294840717	12.212564866459479	190215
1d80d615f3ba7a78a778cc465acb75aab56f5672	lk: a language for capturing real world meanings of the stored data	generalization operators;drugs;control systems;database system;knowledge base component;drugs database systems knowledge representation data models computer science data engineering knowledge engineering joining processes control systems information retrieval;capturing real world meanings;database management systems;knowledge representation language l k;information retrieval;specialization;query languages database management systems inference mechanisms knowledge representation;inference mechanisms;inexact reasoning;associability;data engineering;inference query;query languages;expressive power;data semantics;inference operations;stored data;granularity;database systems;inference query language capturing real world meanings stored data knowledge representation language l sub k knowledge base component database system granularity specialization generalization operators inexact reasoning heuristically controlled environment associability inference operations;joining processes;computer science;language;knowledge representation;tight coupling;heuristically controlled environment;data models;knowledge base;knowledge engineering	A knowledge representation language L/sub k/ is introduced that is tailored for expressing the real-world meanings of stored data. L/sub k/ is developed to achieve a tight coupling between a knowledge base component and the database system. L/sub k/ offers (1) a flexible descriptive power which facilitates concepts to be expressed at different levels of granularity; (2) a versatile association mechanism which is capable of linking partially related concepts; and (3) a set of specialization and generalization operators that enable inexact reasoning in a heuristically controlled environment. Examples are provided to illustrate the language's expressive power, its associability, and the inference operations. An example of processing an inference query is given to show the application of various utilities of L/sub k/. >		D. G. Shin	1991		10.1109/ICDE.1991.131523	natural language processing;data modeling;knowledge base;granularity;coupling;computer science;knowledge engineering;data mining;database;language;programming language;expressive power;query language	AI	-29.590440954755906	11.436600890935363	191399
93211894a632fd492bc9d2f941c7dbf0194ebb6b	fsml: fusion simulation markup language for interoperability of data and analysis tools	analytical models;plasma simulation;xml schema;three dimensions;data visualisation xml open systems data analysis electronic data interchange software tools plasma simulation meta data application program interfaces physics computing;collaboration;data exchange;plasma physics;practical reasoning;analytical models markup languages data analysis plasma simulation xml lan interconnection collaboration internetworking physics data visualization;data format;physics computing;lan interconnection;physics;data visualisation;semantic metadata;data analysis;data exchange fsml fusion simulation markup language interoperability data sharing data analysis tools data formats fusion simulation plasma simulation xml syntactic metadata semantic metadata api visualization tool avs express modules m3d nimrod;application program interfaces;markup languages;data visualization;xml;internetworking;meta data;software tools;open systems;markup language;electronic data interchange	As the fusion community becomes more interconnected and problems become more complex, very close collaborative efforts are expected. This requires internetworking various codes, comparing solutions from multiple solvers, and sharing of data and data analysis tools. However, the data formats and data analysis tools used in fusion and plasma simulations are highly heterogeneous. Imposing one standard data format and one type of tools is unrealistic due to historical and practical reasons. In this paper, we propose to create the Fusion Simulation Markup Language or FSML - an XML based system for describing and accessing fusion and plasma physics simulation data of various formats used in the community. The system consists of syntactic and semantic metadata organized in specialized XML schemas and APIs written for accessing data from major data analysis and visualization tools. We present the preliminary results in formulation the FSML schema and APIs in AVS/Express modules, and demonstrate their application for two large three-dimension fusion simulation codes M3D and NIMROD. The results show that FSML schema and the set of tools developed provide a strong initial momentum and technology for the community effort to enhance data exchange and interoperability of analysis tools.	code;dynamical simulation;internetworking;interoperability;markup language;plasma active;xml	Svetlana G. Shasharina;Chuang Li	2005	CLADE 2005. Proceedings Challenges of Large Applications in Distributed Environments, 2005.	10.1109/CLADE.2005.1520913	data exchange;computer science;operating system;data mining;database;distributed computing;markup language;programming language;world wide web;data visualization	HPC	-32.35312825557302	16.800582810240826	191686
cfb5167fa75594d9a6f47d9651b49a3623449225	looking for the objects in object-relational dbmss (panel)	complex objects;query language;relation algebra;time change;query optimization;data type;feature matching;complex data;relational model;smalltalk;normal form;database design;data structure;object relational;object model;new products	"""The Relational Model first came into vogue in the early 1980's. It was based on the simplifying idea that all data could be modeled as mathematical relations (tables in """"normal form""""). Permissible operations on this table data structure were specified by the relation algebra and calculus. The Relational Model led to years of research in areas such as query languages, query optimization, transaction models, and database design methodologies. This research has dominated DBMS conferences for the last fifteen years and has also lead to major products offerings in wide-spread use in the computer industry today.While the Relational revolution was happening in the DBMS community there was a minority opinion emerging from the object community. This minority opinion surfaced in heated debates and panels at DBMS conferences where it was often pitted against """"relational purists."""" The object proponents wanted to discuss storing and retrieving complex objects and relationships in databases while the relational purists insisted on maintaining """"mathematical purity and simplicity"""". These panels were often some of the most acrimonious (and entertaining) at these conferences and many thought there was no way to bridge the chasm between the two schools of thought.However times changes and so do the realities of the commercial world. Object languages such as C++, Smalltalk, and Java have become de facto standards. The Internet and the PC have increased the demand for complex data types. And the relational model is evolving to accommodate these realities.All of the major RDBMS vendors have announced plans for, or are already shipping, Object-Relational DBMS (ORDBMS) products. A natural question for the object community is how well these new products will address the well-known """"impedance mismatch"""" between a pure object model and the relational model. For example, how does one make Java, Smalltalk, or C++ objects persist using an ORDBMS? Can one search for these objects in the database using their methods?The vendors for ORDBMS are claiming well-known OO features in their implementations, such as extensible data types, inheritance, object identity, and object language bindings. This panel will allow the major vendors to explain how those features match up with the kinds of object models that OO developers are accustomed to. The panel will consist of representatives from three of the major ORDBMS vendors as well as a representative of """"pure object think"""". The vendor representatives will each present a brief overview of the object-related features of their products and will explain why OO programmers are going to have an easier time with these ORDBMSs. This will be followed by counterpoint discussion from the pure object thinker."""	c++;characteristic impedance;data structure;database design;database normalization;flaming (internet);internet;java;mathematical optimization;object language;object-relational database;programmer;pure function;query language;query optimization;relational database management system;relational model;sql/olb;smalltalk;whole earth 'lectronic link	Lougie Anderson;Michael J. Carey;Kenneth R. Jacobs;Erin Kinikin;David Maier	1997		10.1145/263698.263724	query optimization;relational model;object-based language;object model;data structure;daylight saving time;data type;computer science;theoretical computer science;data mining;relation algebra;database;programming language;database design;query language;complex data type	DB	-30.630806901655987	11.233441022110924	192086
f3b389e4d589ab70330ad728fad1b704d16ae71d	automating logical file design	relational data;data restructuring;design tool;logic design;data translation;data base;network structure;data structure;data definition	Data base design is currently a costly and time consuming activity. Part of this overall design is concerned with the logic of the underlying network structure, and this part is commonly called logical design. Logical design involves a tedium of calculations which can be automated on a program and used as a design tool. The basic approach is applicable to a wide variety of data base handlers, such as IMS, the DBTC proposal, CIS, and others. The approach has been prototyped and a version suitable for IMS is now being used (DBDA) as a program product.  This paper describes the basic concepts and how they can be applied to IMS, DBTG or relational implementations. The data structure needed to support a particular application program is called the local view, and input to the design tool is the collection of all local views. Local views are constructedusing certain primitives which the integration of the local views.  The diagnostics of the design tool program will partially depend on the data base handler. Each handler, (IMS, DBTG., etc.) has different network restrictions which limit the local views which can be generated from the network. Different network restrictions result in different diagnostics. There is no relational data base handler to evaluate for network restrictions.	data base task group;data structure;database;design tool;information management system (ims);software design	George U. Hubbard;Norman Raver	1975		10.1145/1282480.1282498	data definition language;logic synthesis;data structure;relational database;computer science;theoretical computer science;data mining;database;programming language	SE	-29.17786098937661	12.55532013680081	193968
d9ec184c15abbb200a49c46b958a8000808da897	compiling er specifications into declarative programs	declarative programming;programming language;software engineering;declarative languages;entity relationship	This paper proposes an environment to support high-level database programming in a declarative programming language. In order to ensure safe database updates, all access and update operations related to the database are generated from high-level descriptions in the entityrelationship (ER) model. We propose a representation of ER diagrams in the declarative language Curry so that they can be constructed by various tools and then translated into this representation. Furthermore, we have implemented a compiler from this representation into a Curry program that provides access and update operations based on a highlevel API for database programming.	abstraction layer;application programming interface;compiler;complexity;conceptual schema;curry;data integrity;data model;database;declarative programming;diagram;entity;entity–relationship model;erdős–rényi model;executable;high- and low-level;high-level programming language;human-readable medium;programmer;requirement;sql;seamless3d;type safety	Bernd Brassel;Michael Hanus;Marion Müller	2007	CoRR		natural language processing;first-generation programming language;declarative programming;very high-level programming language;data manipulation language;programming domain;entity–relationship model;computer science;functional logic programming;database;programming paradigm;inductive programming;datalog;fifth-generation programming language;programming language	DB	-29.5723034238805	11.575243135133462	194983
0cec00ff14c0830b334b6f2e017774bf43c46b9d	view updatability based on the models of a formal specification	ajustamiento modelo;base donnee;formal specification;productivite;database;base dato;ingenieria logiciel;software engineering;productividad;data model;formal method;specification formelle;ajustement modele;especificacion formal;category theory;conceptual modelling;theorie categorie;model matching;department of health;genie logiciel;formal specication;information system;productivity;teoria categoria;systeme information;sistema informacion	Information system software productivity can be increased by improving the maintainability and modifiability of the software produced. This latter in turn can be achieved by the provision of comprehensive support for views, since view support allows application programs to continue to operate unchanged when the underlying information system is modified. But, supporting views depends upon a solution to the view update problem, and proposed solutions to date have only had limited, rather than comprehensive, applicability. This paper presents a new treatment of view updates for formally specified information systems. The formal specification technique we use is based on category theory and has been the basis of a number of successful major information system consultancies. We define view updates by a universal property in a subcategory of models of the formal specification, and explain why this indeed gives a comprehensive treatment of view updatability, including a solution to the view update problem. However, a definition of updatability which is based on models causes some inconvenience in applications, so we prove that in a variety of circumstances updatability is guaranteed independently of the current model. The paper is predominantly theoretical, as it develops the theoretical basis of a formal methods technique, but the methods described here are currently being used in a large consultancy for a government Department of Health. Because the application area, information systems, is rarely treated by formal methods, we include some detail about the formal methods used. In fact they are extensions of the usual category theoretic specification techniques, and the solution to the view update problem can be seen as requiring the existence of an initial model for a specification.	algorithm;category theory;computation;data model;data visualization;formal methods;formal specification;graphical user interface;information system;interoperability;rapid prototyping;semiconductor industry;software repository	Michael Johnson;Robert D. Rosebrugh	2001		10.1007/3-540-45251-6_31	productivity;formal methods;data model;computer science;artificial intelligence;data mining;formal specification;database;mathematics;programming language;computer security;information system;algorithm;category theory	DB	-31.8667970518036	12.911845607896218	195042
e5501c95c5ff927183f7bbc2fe593bbaade3d1f7	a survey of schema evolution in object-oriented databases	database system;database versioning;software prototyping;object oriented databases cad cam software prototyping;database design schema evolution object oriented databases database population database schema cad cam case database systems prototyping;cad cam;object oriented databases;object oriented database;database design;schema evolution;object oriented databases database systems transaction databases computer aided manufacturing computer aided software engineering design automation cadcam prototypes management information systems technology management	Changes in the real world may require both the database population and the database schema to evolve. Particularly, this is the case in CAD/CAM and CASE database systems, in which the design objects constantly evolve in every aspect. On the other hand, the prototyping of a database design may also involve changes to both the structure and behavior of a schema. Unfortunately, most of the current systems offer little support for schema evolution. In this paper, we survey the recent development of the research in the schema evolution of objectoriented databases. The main issues in the current research are identified.	computer-aided design;computer-aided software engineering;database design;database schema;schema evolution	Xue Li	1999		10.1109/TOOLS.1999.796507	schema migration;database theory;information schema;database server;intelligent database;computer science;systems engineering;three schema approach;conceptual schema;data administration;database model;data mining;database;view;database schema;database testing;database design;spatiotemporal database	DB	-32.617967663242034	11.830721289266323	195749
7e42912d3918c9fcf3978ae6e98eacfd7dc396a8	on metamodel superstructures employing uml generalization features		We employ UML generalization features in order to describe multi-level metamodels and their connections. The basic idea is to represent several metamodel levels in one UML and OCL model and to connect the metamodels with (what we call) a superstructure. The advantage of having various levels in one model lies in the uniform handling of all levels and in the availability of OCL for constraining models and navigating between them. We establish the connection between the metamodel levels by typing links that represent the instance-of relationship. These typing links are derived from associations that are defined on an abstraction of the metamodel classes and that are restricted by redefines and union constraints in order to achieve level-conformant typing. The abstraction of the metamodel classes together with the connecting associations and generalizations constitutes the superstructure.	class diagram;database model;definition;metamodeling;object constraint language;object diagram;relational database;relational model;schedule (computer science);unified modeling language	Martin Gogolla;Matthias Sedlmeier;Lars Hamann;Frank Hilken	2014			superstructure;metamodeling;typing;theoretical computer science;abstraction;generalization;unified modeling language;computer science	AI	-30.42397173642492	12.62039813427872	196021
8c799b9587e68f19137a8268eeef7ac7939cef7e	a conceptual model for a knowledge base homogeneously stored in a database environment	heterogeneous environment;conceptual model;software engineering;artificial intelligent;object oriented;state transition diagram;entity relationship;knowledge base	This paper proposes a conceptual approach as a way for storing Knowledge Bases in a Database Environment. The evolution of the model happened with the use of Entity-Relationship (ER) diagrams, Venn diagrams, Object-Oriented (OO) models and State Transition diagrams. With that model we are working in the construction of a Knowledge Server homogeneously stored in a Database environment and speculate about the possibility of that database to be shared in an heterogeneous environment. This research is an integration effort between the areas of Software Engineering (SE), Artificial Intelligence (AI) and Databases (DB) at Instituto Militar de Engenharia (IME).	knowledge base	Emmanuel Passos;Alberto Sade;Cícero Garcez;Asterio K. Tanaka	1995		10.1007/BFb0034822	knowledge base;state diagram;entity–relationship model;computer science;knowledge management;conceptual schema;artificial intelligence;conceptual model;domain model;data mining;object-oriented programming	DB	-33.44294089414901	11.90262206939346	197558
7fe8658156340b98ecbb191c53f0f60db081876f	optimal evaluation of path predicates in object-oriented queries	optimisation;optimizacion;information retrieval;interrogation base donnee;interrogacion base datos;predicate calculus;search strategy;algoritmo genetico;camino optimo;calcul predicat;chemin optimal;optimal path;recherche information;object oriented;calculo predicado;strategie recherche;algorithme genetique;oriente objet;genetic algorithm;optimization;recuperacion informacion;orientado objeto;database query;estrategia investigacion	Query optimization in object-oriented databases requires new techniques for supporting features such as methods, path expressions, and so on. In this paper we address the optimization of path predicates in object-oriented queries. We apply the genetic search strategies to our optimization problem, and show that our formulation is well-suited to genetic algorithms.		Sang-Koo Seo;Yoon-Joon Lee	1994	Microprocessing and Microprogramming	10.1016/0165-6074(94)90033-7	genetic algorithm;computer science;artificial intelligence;first-order logic;database;programming language;object-oriented programming;algorithm	PL	-26.843179283495786	11.547887948810104	198396
4341c6fe0bcfed07a604418154a81654a935dd81	a bottom-up interpreter for a database language with updates and transactions	bottom up	Deductive databases with updates in rule bodies do not allow bottom-up execution model. This is due to the introduction of control in rule bodies. However, bottom-up execution models are very important due to the set oriented query-answering process of database systems. In 4] we have proposed a rule language to avoid the above drawback and to provide transaction optimization through transaction transformation. In this paper we describe a prototype that provide a bottom-up meta interpreter for the database rule language and will allow to check the validity of future extensions theoretical conjecture about transaction optimization and integrity constraints. The experience in the use of KBMS1 as a tool to develop a run time support for the rule language is reported together with an overview of the system architecture.	bottom-up parsing;data integrity;deductive database;mathematical optimization;prototype;query language;run time (program lifecycle phase);systems architecture;top-down and bottom-up design	Elisa Bertino;Barbara Catania;Giovanna Guerrini;Maurizio Martelli;Danilo Montesi	1994			programming language;database;top-down and bottom-up design;interpreter;computer science;database transaction	DB	-28.916792481007963	11.528555724848905	199252
d4b238bb32db2d0a49a46448160960bf5af3857b	learning from highly structured data by decomposition	systeme intelligent;procesamiento informacion;systeme apprentissage;sistema inteligente;logical programming;higher order;learning systems;learning system;programmation logique;information processing;intelligent system;property a;information system;traitement information;programacion logica;systeme information;structured data;sistema informacion	This paper addresses the problem of learning from highly structured data. Speciically, it describes a procedure, called decomposition , that allows a learner to access automatically the subparts of examples represented as closed terms in a higher-order language. This procedure maintains a clear distinction between the structure of an individual and its properties. A learning system based on decomposition is also presented and several examples of its use are described.	data model	René MacKinney-Romero;Christophe G. Giraud-Carrier	1999		10.1007/978-3-540-48247-5_55	higher-order logic;information processing;data model;computer science;artificial intelligence;machine learning;programming language;information system;algorithm	ML	-28.298767385583513	12.045802804934151	199375
