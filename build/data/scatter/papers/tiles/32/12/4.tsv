id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c2526d43ed11a6cf7a38d2b3bc4445a42541a4c4	source and translation classification using most frequent words		Recently, translation scholars have made some general claims about translation properties. Some of these are source language independent while others are not. Koppel and Ordan (2011) performed empirical studies to validate both types of properties using English source texts and other texts translated into English. Obviously, corpora of this sort, which focus on a single language, are not adequate for claiming universality of translation properties. In this paper, we are validating both types of translation properties using original and translated texts from six European languages.	interference (communication);structural similarity;text corpus;the 100;universality probability;verification and validation	Zahurul Islam;Armin Hoenen	2013			dynamic and formal equivalence;natural language processing;speech recognition;example-based machine translation;computer science;machine translation;rule-based machine translation;programming language;machine translation software usability	NLP	-27.452291528926995	-74.66045008946101	131029
916625b47bb7b5ff558858e19a5393648eb86af2	processing of quantitative expressions with measurement units in the nominative, genitive, and accusative cases for belarusian and russian		This paper outlines an approach to the stage-by-stage solution of the computer-linguistic problem of the processing of quantitative expressions with measurement units by means of the linguistic processor NooJ. The focus is put on the nominative, genitive, and accusative cases for Belarusian and Russian. The paper gives a general analysis of the problem providing examples not only for Belarusian and Russian, but also for English.	algorithm;automaton;display resolution;finite-state machine;nooj	Yury Hetsevich;Alena Skopinava	2014		10.1007/978-3-319-10816-2_13	units of measurement;computer science;natural language processing;artificial intelligence;nominative case;expression (mathematics);text processing;genitive case	Logic	-29.100843640125184	-77.3704063157274	131280
8fec3dc6db66f14a670d30572cfcd89c680be865	análisis ascendente bidireccional de tag dirigido por el núcleo tig	traitement automatique des langues naturelles;syntactic parsing;tree insertion grammar;linguistique appliquee;grammaire d arbres adjoints;analyse syntaxique automatique;algorithme;algorithm;tree adjoining grammar;grammaire formelle;grammaire d insertion d arbres;formal grammar;computational linguistics;linguistique informatique;natural language processing;applied linguistics;algoritmo	We define a bidireccional bottom-up parser for Tree Adjoining Grammars (TAG). This parser is a mixture of a defined bidireccional bottom-up parser for TAG and a new parser for Tree Insertion Grammars (TIG) that we present here. We show that the new mixed parser for TAG presents a significative reduction of the theoretical complexity.	bottom-up parsing;insertion sort;power-on reset;tree-adjoining grammar	Vicente Carrillo Montero;Víctor J. Díaz Madrigal;Miguel A. Alonso	2003	Procesamiento del Lenguaje Natural		natural language processing;tree-adjoining grammar;computer science;computational linguistics;applied linguistics;linguistics;formal grammar;algorithm	NLP	-26.745215333701495	-78.69492437842003	131358
82fd9f3c3cfffe287fe9d275f68aa33766274178	identifying signs of syntactic complexity for rule-based sentence simplification				Richard Evans;Constantin Orasan	2019	Natural Language Engineering	10.1017/S1351324918000384	natural language processing;artificial intelligence;computer science;rule-based system;parsing;syntax;text simplification;sentence	NLP	-29.43862014811191	-79.0607999181392	131481
65e9f0bb7e155716693f126730da389fd723e784	anticipating false implicatures: cooperative responses in question-answer systems	question answering system			Julia Hirschberg	1984			natural language processing;mathematics;communication;algorithm	NLP	-32.45048148189371	-79.2772226977653	131584
0d88c5367ace4ace29098eaa322bf8fd71d72ec5	a distributed platform for sanskrit processing		Sanskrit, the classical language of India, presents specific challenges for computational linguistics: exact phonetic transcription in writing that obscures word boundaries, rich morphology and an enormous corpus, among others. Recent international cooperation has developed innovative solutions to these problems and significant resources for linguistic research. Solutions include efficient segmenting and tagging algorithms and dependency parsers based on constraint programming. The integration of lexical resources, text archives and linguistic software is achieved by distributed interoperable Web services. Resources include a morphological tagger and tagged corpus.	algorithm;anaphora (linguistics);brill tagger;computational linguistics;constraint programming;digital library;galaxy morphological classification;grams;interaction;interoperability;library (computing);n-gram;natural language processing;parsing;part-of-speech tagging;semiconductor industry;text corpus;transcription (software);user interface	Pawan Goyal;Gérard P. Huet;Amba P. Kulkarni;Peter M. Scharf;Ralph Bunker	2012			classical language;artificial intelligence;constraint programming;web service;morphology (linguistics);computer science;computational linguistics;natural language processing;software;parsing;sanskrit	NLP	-31.188216399094525	-75.60692546636773	131769
26cfe1ff6f17d568a8974b550f0dc4964e18ead5	cognitively inspired nlp-based knowledge representations: further explorations of latent semantic analysis	semantic similarity;knowledge representation;latent semantic analysis	Natural-language based knowledge representations borrow their expressiveness from the semantics of language. One such knowledge representation technique is Latent semantic analysis (LSA), a statistical, corpus-based method for representing knowledge. It has been successfully used in a variety of applications including intelligent tutoring systems, essay grading and coherence metrics. The advantage of LSA is that it is efficient in representing world knowledge without the need for manual coding of relations and that it has in fact been considered to simulate aspects of human knowledge representation. An overview of LSA applications will be given, followed by some further explorations of the use of LSA. These explorations focus on the idea that the power of LSA can be amplified by considering semantic fields of text units instead of pairs of text units. Examples are given for semantic networks, category membership, typicality, spatiality and temporality, showing new evidence for LSA as a mechanism for knowledge representation. The results of such tests show that while the mechanism behind LSA is unique, it is flexible enough to replicate results in different corpora and languages.	commonsense knowledge (artificial intelligence);knowledge representation and reasoning;knowledge-based systems;latent semantic analysis;natural language;self-replicating machine;semantic network;simulation;text corpus	Max M. Louwerse;Zhiqiang Cai;Xiangen Hu;Matthew Ventura;Patrick Jeuniaux	2006	International Journal on Artificial Intelligence Tools	10.1142/S0218213006003090	natural language processing;knowledge representation and reasoning;semantic similarity;latent semantic analysis;computer science;artificial intelligence;machine learning;probabilistic latent semantic analysis	AI	-33.039116409970475	-80.018825854297	131884
e20ae4ad0fbeeb4eae1d6a44b43edd8fc4f978ca	computing the structure of language for neuropsychiatric evaluation				Guillermo A. Cecchi;Viatcheslav Gurev;Steve Heisig;Raquel Norel;Irina Rish;Samantha R. Schrecke	2017	IBM Journal of Research and Development		natural language processing	NLP	-31.42743147731307	-78.60170807124213	132007
970a876f8ab64efe4ad95c8f1e23e5c491763415	anaphora and coreference resolution: a review		Entity resolution aims at resolving repeated references to an entity in a document and forms a core component of natural language processing (NLP) research. This field possesses immense potential to improve the performance of other NLP fields like machine translation, sentiment analysis, paraphrase detection, summarization, etc. The area of entity resolution in NLP has seen proliferation of research in two separate sub-areas namely: anaphora resolution and coreference resolution. Through this review article, we aim at clarifying the scope of these two tasks in entity resolution. We also carry out a detailed analysis of the datasets, evaluation metrics and research methods that have been adopted to tackle this NLP problem. This survey is motivated with the aim of providing the reader with a clear understanding of what constitutes this NLP problem and the issues that require attention.	anaphora (linguistics);automatic summarization;machine translation;natural language processing;sentiment analysis	Rhea Sukthanker;Soujanya Poria;Erik Cambria;Ramkumar Thirunavukarasu	2018	CoRR		automatic summarization;artificial intelligence;machine translation;natural language processing;sentiment analysis;name resolution;review article;coreference;computer science;paraphrase	NLP	-28.88131360735278	-73.54776743149333	132236
679e43ff18e74a4e70a90a56358757181cf7de31	construcción y minimización eficiente de transductores de letras a partir de diccionarios con paradigmas	paradigms;computacion informatica;filologias;transductores de estados finitos;info eu repo semantics article;informacion documentacion;linguistica;ciencias basicas y experimentales;dictionaries;diccionarios;lexical proceesing;procesamiento lexico;grupo a;finite state transducers;ciencias sociales;grupo b;paradigmas	This paper introduces a model of dictionary management to build lexical processors based on paradigms. First, examples are given to show the expressivity of this model and that it can be applied to a wide variety of languages. Next, a method is explained that allows for an efficient construction of letter transducers extracted from dictionaries by taking advantage of the use of paradigms. Finally, the result that has been obtained with the implemented system is presented.	automata theory;central processing unit;compiler;computation;computational linguistics;dictionary;directed acyclic graph;expressive power (computer science);finite-state machine;linear algebra;machine translation;naruto shippuden: clash of ninja revolution 3;springer (tank);transducer;unique name assumption;watson (computer)	Sergio Ortiz-Rojas;Mikel L. Forcada;Gema Ramírez-Sánchez	2005	Procesamiento del Lenguaje Natural		natural language processing;computer science;linguistics;rule-based machine translation;algorithm	Logic	-30.06715484389488	-79.15208178179033	132833
2ba312e6bf3cc46997613fa111df205cf8acdef7	emulation of human sentence processing using an automatic dependency shift-reduce parser		The methods of NLP and Cognitive Science can complement each other for the design of better models of the human sentence processing mechanism, on the one hand, and the development of better natural language parsers, on the other. In this paper, we show the performance of an automatic parser consistent with the architecture of the human parser of [2] on various human sentence processing experimental materials. Moreover, we use a linking hypothesis based on the concept of surprisal [9] to explain human reaction time patterns. Although our results are generally not consistent with the human performance, our emulations contribute to understanding the architecture of the human parser and its disambiguation strategies better. We also suggest that these strategies may possibly be used for improving the performance of automatic parsers.	automatic taxonomy construction;cognitive science;emulator;human reliability;natural language processing;parsing;self-information;shift-reduce parser;word-sense disambiguation	Atanas Chanev	2008			natural language processing;linguistics;programming language;simple lr parser	NLP	-32.928061566114664	-79.89157431113536	133040
01dc69c77cc4ec407e44c2f9e7ff2fe32af59153	improving automated alignment in multilingual corpora	dynamic program;conference paper;eastern europe;of research and development;european union;parallel corpora;natural language processing;language engineering	We report on methods of improving multilingual text alignments that have been produced in a simple dynamic-programming scheme, by automated detection of possible misalignments. Details of methods involving cognates, speciallyidentified words, and propositional contents of sentences are given, together with notable features of their performance on parallel corpora in a number of different types of European languages.	dynamic programming;parallel text;text corpus	John A. Campbell;Niladri Chatterjee;Mauro Manela;Alex Chengyu Fang	1996			natural language processing;computer science;artificial intelligence	ML	-29.58982184678621	-76.32414270340588	133260
14bfe14e1a87005fbd92c9851614367e9290326a	dkie: open source information extraction for danish	information extraction	Danish is a major Scandinavian language spoken daily by around six million people. However, it lacks a unified, open set of NLP tools. This demonstration will introduce DKIE, an extensible open-source toolkit for processing Danish text. We implement an information extraction architecture for Danish within GATE, including integrated third-party tools. This implementation includes the creation of a substantial set of corpus annotations for dataintensive named entity recognition. The final application and dataset is made are openly available, and the part-of-speech tagger and NER model also operate independently or with the Stanford NLP toolkit.	brill tagger;gate;information extraction;interoperability;ner model;named-entity recognition;natural language processing;open-source intelligence;open-source software;part-of-speech tagging;treebank	Leon Derczynski;Camilla Vilhelmsen Field;Kenneth S. Bøgh	2014			natural language processing;computer science;data mining;database;information extraction	NLP	-32.427369392049876	-74.07169366342886	133299
e18dd3e716569f08c4e1713d963d7876497a3ecf	g2p conversion of names: what can we do (better)?	names;dutch;index terms: g2p conversion;machine based learning;indexing terms;proper names;p2p;index terms	In this contribution it is shown that a good approach for the grapheme-to-phoneme conversion of proper names (e.g. person names, toponyms, etc), is to use a cascade of a general purpose grapheme-to-phoneme (G2P) converter and a special purpose phoneme-to-phoneme (P2P) converter. The G2P produces an initial transcription that is then transformed by the P2P. The latter is automatically trained on reference transcriptions of names belonging to the envisaged name category (e.g. toponyms). The P2P learning process is conceived in such a way that it can take account of high order determinants of pronunciation, such as specific syllables, name prefixes and name suffixes. The proposed methodology was successfully tested on person names and toponyms, but we believe that it will also offer substantial reductions of the cost for building pronunciation lexicons of other name categories.	lexicon;transcription (software)	Henk van den Heuvel;Jean-Pierre Martens;Nanneke Konings	2007				Web+IR	-27.41229066765364	-73.75547073326844	133462
11a6ceec4babc502ece0241e3f56b750c7ed0067	depending on collaborations: dependencies as contextual associations				James E. Rumbaugh	1998	JOOP		natural language processing;contextual associations;programming language;computer science;artificial intelligence	Vision	-32.180826696587	-79.23296365541378	133608
5e8aa14dba952d93a54b7d309dc37f15e51ee9b4	from tale to speech: ontology-based emotion and dialogue annotation of fairy tales with a tts output	fairy tale;semantic network;storytelling;text to speech;ontology;natural language processing	In this demo and poster paper, we describe the concept and implementation of an ontology-based storyteller for fairy tales. Its main functions are (i) annotating the tales by extracting timeline information, characters and dialogues with corresponding emotions expressed in the utterances, (ii) populating an existing ontology for fairy tales with the previously extracted information and (iii) using this ontology to generate a spoken version of the tales. Common natural language processing technologies and resources, such as part-of-speech tagging, chunking and semantic networks have been successfully used for the implementation of the three tasks mentioned just above, including the integration of an open source text-to-speech system. The code of the system is publicly available.	natural language processing;netware file system;open-source software;part-of-speech tagging;population;semantic network;shallow parsing;speech synthesis;timeline	Christian Eisenreich;Jana Ott;Tonio Süßdorf;Christian Willms;Thierry Declerck	2014			natural language processing;upper ontology;epistemology;computer science;ontology;ontology;database;linguistics;semantic network;world wide web	NLP	-31.22216362946966	-76.95886438489826	133936
4343d3edd00a85171db0351baca54e8678feffdd	mining naturally-occurring corrections and paraphrases from wikipedia's revision history		Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedia’s revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.	belief revision;biological anthropology;categorization;error detection and correction;rewriting;wikipedia	Aurélien Max;Guillaume Wisniewski	2010			natural language processing;artificial intelligence;error detection and correction;information retrieval;paraphrase;computer science;spelling;typology;text processing;rewriting;annotation	NLP	-30.96013755359339	-74.49065313894509	134236
2630e54ab9216bc7f29328bc8d8f21304f18056a	cross-lingual link discovery by using link probability and bilingual dictionary			bilingual dictionary	Sin-Jae Kang	2011			natural language processing;speech recognition;artificial intelligence;computer science;bilingual dictionary	Vision	-29.688617837025372	-77.2194038143691	134436
6b31d9b6c76040b2c198eb70d1cfaa8f57d125df	automated information retrieval: theory and methods, by valery i. frants, jacob shapiro, and vladimir g. voiskunskii	information retrieval		information retrieval	Geoffrey Z. Liu	1998	JASIS	10.1002/(SICI)1097-4571(199808)49:10%3C953::AID-ASI11%3E3.0.CO;2-D		NLP	-31.79738088747322	-77.0797361865583	134606
545209287bae1d941cf4ee548f087863c938380e	spoken language parsing strategies in a conversational system			parsing	T. L. Soto;Jose F. Quesada	1998			natural language processing;machine learning;computer science;parsing;artificial intelligence;spoken language	NLP	-29.570647105903276	-79.91288504263949	135375
d156fcd29ea1287852d9df6b541e67ac6f6bda4a	the penn chinese treebank: phrase structure annotation of a large corpus	filologias;linguistica;grupo a	With growing interest in Chinese Language Processing, numerous NLP tools (e.g., word segmenters, part-of-speech taggers, and parsers) for Chinese have been developed all over the world. However, since no large-scale bracketed corpora are available to the public, these tools are trained on corpora with di erent segmentation criteria, part-of-speech tagsets and bracketing guidelines, and therefore, comparisons are diÆcult. As a rst step towards addressing this issue, we have been preparing a large bracketed corpus since late 1998. The rst two installments of the corpus, 250 thousand words of data, fully segmented, POS-tagged and syntactically bracketed, have been released to the public via LDC (www.ldc.upenn.edu). In this paper, we discuss several Chinese linguistic issues and their implications for our treebanking e orts and how we address these issues when developing our annotation guidelines. We also describe our engineering strategies to improve speed while ensuring annotation quality.	andi gutmans;brill tagger;coding tree unit;cognitive science;data pre-processing;entity–relationship model;galaxy morphological classification;hilbert–huang transform;inter-rater reliability;linguistic data consortium;microsoft word for mac;natural language processing;open road tolling;parsing;part-of-speech tagging;phrase structure rules;preprocessor;relevance;systran;syntactic predicate;text corpus;text segmentation;timeline;treebank;yang	Naiwen Xue;Fei Xia;Fu-Dong Chiou;Martha Palmer	2005	Natural Language Engineering	10.1017/S135132490400364X	natural language processing;speech recognition;computer science;linguistics	NLP	-30.49824459726626	-76.74195345216127	135449
3f47c15f960ab250b8396eaed3ebf6827c4e8e19	enhancing text spotting with a language model and visual context information			language model	Ahmed Sabir;Francesc Moreno-Noguer;Lluís Padró	2018		10.3233/978-1-61499-918-8-271	natural language processing;language model;spotting;artificial intelligence;computer science	HCI	-29.366799879625756	-78.48175769159025	135614
b69c134f9a716d758593f2a0dbf870c43107d7c6	diple, modular methodology and tools for heterogeneous tei corpora		"""École nationale des chartes The École nationale des chartes publishes a variety of electronic corpora, 1 focused on historical sources (medieval, but also, modern and contemporary). A dictionary, 2 a collection of acts, 3 or a manuscript 4 are very different types of documents, each requiring different structures and interfaces. A narrative manuscript needs a table of contents, a dictionary, fast access to headwords, and acts, the ability to sort by dates. Each editorial project should allow customization, but efficient development requires that the tools and corpora are as normalized as possible. New needs emerge, such as natural language processing research, requiring large corpora with normalized metadata sets and word tagging. For several months we have been working on a platform to address these needs: Diple is a collection of tools to organize modular production, publication, and searching of electronic corpora. The TEI guidelines (500 XML elements, 1500 pages of documentation) allow endless variations in encoding, even for identical objects. For example, italic in our corpora has been encoded with different combinations of <(hi|emph) rend=""""(italique|italic|ital.|i| itlaic|...)"""">. After several years of development, with different encoders, each electronic edition becomes an independent software, with its own encoding, mistakes, workarounds, with also different technologies for publication or fulltext searching. Diple starts with housekeeping. First, for all our tagged texts, we wrote a precise document type definition (in Relax-NG syntax) in order to define three main and shared schemas :-file metadatas (<teiHeader>)-general text (blocks and inlines)-structure for a specific type of text (ex: acts and charters, dictionaries) The normalization of the corpora is a more sustainable investment than new software. These shared schemas are extremely helpful for normalizing and validating XML instances, and therefore allow us to take advantage of earlier TEI editions. Of course, the Diple TEI schema is modular, allowing customization for each editorial project. 5 An editor can then focus on the specificities of each edition. Are named entities sufficiently tagged to generate automatic indexes? Are the sentences chunked, the words lemmatized? Moreover, this work of normalization of our XML corpora is a small price to pay to factorize our code, for instance to create a standard XSLT engine: the screen transformation of a new corpus conforming to those schemas is done by this engine, increasing our publication productivity. In the end, the XSLT of a specific edition is short, focusing on the very specific aspects of the corpus (its custom …"""	dictionary;document;documentation;encoder;headword;lemmatisation;named entity;natural language processing;relax ng;text encoding initiative;text corpus;workaround;xml;xslt;école nationale des chartes	Frédéric Glorieux;Olivier Canteaut;Vincent Jolivet	2010			natural language processing;modular design;artificial intelligence;computer science	Web+IR	-32.55648859415996	-75.09783014622245	136031
774a8e36962a050337231e333c56ab7494d46c5d	ww1 and ww2 on a specialist e-forum. applying corpus tools to the study of evaluative language				Malgorzata Sokól	2010			computer science	NLP	-32.17616686250386	-78.18592374067919	136196
dbc8494bfd391344370047b04d51240d63c69c53	linguistico-statistical approach and logics applied in documentary system	statistical approach;logics;connectors;computational linguistic;statistics	Tk project deafs with automatic text processing and computer aided it$wmation search in dynanic system. The Documentation It@matso “ n System (DIS) is based on a linguistic model fix tk recognition qf a written text to deternu”ne tk reference function (Noun Phrase). Many ambiguities in tats are due to tk use of cfassical methods of computing. The idea of tk work here is to atroct tk “read” ~ tk coordination (cor@nctiorrz). Tk aim of the model is to limit production of multiple solutions generated by the connectors . We propose to ‘optimize” i~ormatiorr search through text processing and query formulation in natwal language using the contribution of logics and statistics. Ta mal O1gOriIhS is contribnfed to detectwn and correction tk signs qf pstnctuatiart . Tk CERSI is designed a model for tk processing of information stack documents based on a linguiztic~ statistical qoprooch and corqruter-huvtan dialogue.	documentation	Omar Larouk	1993		10.1145/162754.167176	natural language processing;artificial intelligence;mathematics	NLP	-30.185132025750203	-78.04187689130588	136290
04127798ed5f4156f978edd286871a67ac4be9de	recognition algorithm for korean postpositions by detecting prosody boundaries	grammar;sufijo;lenguaje natural;coreano;lenguaje documental;language use;pistage;suffix;langage naturel;rastreo;intelligence artificielle;voice;voz;korean;langage documentaire;coreen;grammaire;natural language;prosodie;information language;artificial intelligence;inteligencia artificial;suffixe;prosody;gramatica;tracking;prosodia;accentual phrase;voix	In this paper we proposes the algorithm of recognizing postpositions and suffixes in Korean spoken language, using prosodic information. At first, we detect grammatical boundaries automatically by using prosodic information of the accentual phrase, and then we recognize grammatical function words by backward tracking from the boundaries. The experiment employs 300 sentential speech data of 10 men's and 5 women's voice spoken in standard Korean, in which 1080 APs and eleven postpositions and suffixes are included. The result shows the recognition rate of postpositions in two cases. In one case that includes just correctly detected boundaries, the recognition rate is 97.5%, and in the other case that includes all detected boundaries, the recognition rate is 74.8%.	algorithm;semantic prosody;sensor	Ki-Young Lee;Jong Kuk Kim;Myung Jin Bae	2005		10.1007/11558590_55	natural language processing;speech recognition;computer science;grammar;tracking;prosody;natural language;voice;korean	NLP	-26.858645092087183	-79.01201558516895	136405
11842fe733b483e1f9bea67e9250536e04c88308	an account of opinion implicatures		While previous sentiment analysis research has concentrated on the interpretation of explicitly stated opinions and attitudes, this work initiates the computational study of a type of opinion implicature (i.e., opinion-oriented inference) in text. This paper describes a rule-based framework for representing and analyzing opinion implicatures which we hope will contribute to deeper automatic interpretation of subjective language. In the course of understanding implicatures, the system recognizes implicit sentiments (and beliefs) toward various events and entities in the sentence, often attributed to different sources (holders) and of mixed polarities; thus, it produces a richer interpretation than is typical in opinion analysis.	entity;logic programming;sentiment analysis	Janyce Wiebe;Lingjia Deng	2014	CoRR		natural language processing	NLP	-33.11421919137733	-80.07181641079664	136670
6ba88febe7004c90fcdcde15928ad3ab13a0f2ee	converting the tüba-d/z treebank of german to universal dependencies		This paper describes the conversion of TüBa-D/Z, one of the major German constituency treebanks, to Universal Dependencies. Besides the automatic conversion process, we describe manual annotation of a small part of the treebank based on the UD annotation scheme for the purposes of evaluating the automatic conversion. The automatic conversion shows fairly high agreement with the manual annotations.	attachments;brown corpus;documentation;heuristic (computer science);microsoft outlook for mac;one-to-one (data model);treebank;urban dictionary	Çagri Çöltekin;Ben Campbell;Erhard W. Hinrichs;Heike Telljohann	2017			treebank;natural language processing;linguistics;computer science;artificial intelligence;german	NLP	-29.24273940915173	-76.35700027010837	137006
f5e4aaf3da0b537e006eceea87079bb8b0eeb973	corpora of slovene spoken language for multi-lingual applications		$EVWUDFW 7KH GRPDLQ RI VSRNHQ ODQJXDJH WHFKQRORJLHV UDQJHV IURP VSHHFK LQSXW DQG RXWSXW V\VWHPV WR FRPSOH[ XQGHUVWDQGLQJ DQG JHQHUDWLRQ V\VWHPV LQFOXGLQJ PXOWL PRGDO V\VWHPV RI ZLGHO\ GLIIHULQJ FRPSOH[LW\ VXFK DV DXWRPDWLF GLFWDWLRQ PDFKLQHV DQG PXOWLOLQJXDO V\VWHPV IRU H[DPSOH DXWRPDWLF GLDORJXH DQG WUDQVODWLRQ V\VWHPV 7KH GHILQLWLRQ RI VWDQGDUGV DQG HYDOXDWLRQ PHWKRGRORJLHV IRU VXFK V\VWHPV LQYROYHV WKH VSHFLILFDWLRQ DQG GHYHORSPHQW RI KLJKO\ VSHFLILF VSRNHQ ODQJXDJH FRUSXV DQG OH[LFRQ UHVRXUFHV DQG PHDVXUHPHQW DQG HYDOXDWLRQ WRROV ($*/(6 +DQGERRN 7KLV SDSHU SUHVHQWV WKH 0REL/X] VSRNHQ UHVRXUFHV RI WKH 6ORYHQH ODQJXDJH ZKLFK ZLOO EH PDGH IUHHO\ DYDLODEOH IRU UHVHDUFK SXUSRVHV LQ VSHHFK WHFKQRORJ\ DQG OLQJXLVWLFV	exponential hierarchy;inertial reference unit;letter-quality printer;rs-232;text corpus	Jerneja Zganec-Gros;France Mihelic;Simon Dobrisek;Tomaz Erjavec;Mario Zganec	2000			speech recognition;natural language processing;artificial intelligence;computer science;spoken language	DB	-29.688777269486042	-77.93322975562597	137069
a1b3cdef7e439039f39594cf9909f0f18c36865e	initialism disambiguation: man versus machine	information and reference skills;estudio comparativo;etude comparative;disambiguation;analyse correlation;comparative study;analisis correlacion;correlation analysis	Disambiguation of ambiguous initialisms and acronyms is critical to the proper understanding of various types of texts. A model that attempts to solve this has previously been presented. This model contained various baseline features, including contextual relationship features, statistical features, and language-specific features. The domain of Jewish law documents written in Hebrew and Aramaic is known to be rich in ambiguous abbreviations and therefore this model was implemented and applied over 2 separate corpuses within this domain. Several common machine-learning (ML) methods were tested with the intent of finding a successful integration of the baseline feature variants. When the features were evaluated individually, the best averaged results were achieved by a library for support vector machines (LIBSVM); 98.07% of the ambiguous abbreviations, which were researched in the domain, were disambiguated correctly. When all the features were evaluated together, the J48 ML method achieved the best result, with 96.95% accuracy. In this paper, we examine the system's degree of success and the degree of its professionalism by conducting a comparison between this system's results and the results achieved by 39 participants, highly fluent in the research domain. Despite the fact that all the participants had backgrounds in religious scriptures and continue to study these texts, the system's accuracy rate, 98.07%, was significantly higher than the average accuracy result of the participants, 91.65%. Further analysis of the results for each corpus implies that participants over-complicate the required task, as well as exclude vital information needed to properly examine the context of a given initialism.	word-sense disambiguation	Yaakov HaCohen-Kerner;Ariel Kass;Ariel Peretz	2013	JASIST	10.1002/asi.22909	social science;computer science;artificial intelligence;comparative research;data mining;world wide web;algorithm	NLP	-27.400738602177224	-74.34602636992975	138175
c61562f35edd555752fd4ac0129f25756ef2d240	shallow discourse parsing with maximum entropy model		In recent years, more research has been devoted to studying the subtask of the complete shallow discourse parsing, such as indentifying discourse connective and arguments of connective. There is a need to design a full discourse parser to pull these subtasks together. So we develop a discourse parser turning the free text into discourse relations. The parser includes connective identifier, arguments identifier, sense classifier and non-explicit identifier, which connects with each other in pipeline. Each component applies the maximum entropy model with abundant lexical and syntax features extracted from the Penn Discourse Tree-bank. The head-based representation of the PDTB is adopted in the arguments identifier, which turns the problem of indentifying the arguments of discourse connective into finding the head and end of the arguments. In the non-explicit identifier, the contextual type features like words which have high frequency and can reflect the discourse relation are introduced to improve the performance of non-explicit identifier. Compared with other methods, experimental results achieve the considerable performance.	daisy digital talking book;discourse relation;domain of discourse;identifier;logical connective;parsing;principle of maximum entropy	Jingjing Xu	2017	CoRR		discourse relation;principle of maximum entropy;parsing;syntax;identifier;natural language processing;classifier (linguistics);artificial intelligence;mathematics	NLP	-29.01571495733431	-76.64038004194339	138511
c1c1fd3aab35bb4fbef9b7becd62014a9c9f4e09	two case studies on translating pronouns in a deep syntax framework		We focus on improving the translation of the English pronoun it and English reflexive pronouns in an English-Czech syntaxbased machine translation framework. Our evaluation both from intrinsic and extrinsic perspective shows that adding specialized syntactic and coreference-related features leads to an improvement in translation quality.	machine translation	Michal Novák;Zdenek Zabokrtský;Anna Nedoluzhko	2013			natural language processing;personal pronoun;reflexive pronoun;example-based machine translation;computer science;linguistics;subject pronoun;rule-based machine translation	NLP	-28.606281947948812	-76.38517420240173	138786
d97d46e96721e84604d379139fac8a63fc0463b8	sublanguages in machine translation	target langauge equivalent;linguistic engineering;weighting mechanism;theoretical concept;sublanguage notion;real mt application;machine translation;various attempt	There have been various attempts at using the sublanguage notion for disambi-guation and the selection of target language equivalents in machine translation. In this paper a theoretical concept and its implementation in a real MT application are presented. Above this, means of linguistic engineering like weighting mechanisms are proposed.	compiler;machine translation;sublanguage;theoretical definition	Heinz-Dirk Luckhardt	1991			dynamic and formal equivalence;natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-28.73912629033555	-77.24073928919053	138809
6f2448dcaeb90d03b27c3cc2b1e6d54dc1dcb768	a framework for the automatic generation of indian sign language				Tirthankar Dasgupta;Anupam Basu;Plaban Kumar Bhowmick;Pabitra Mitra	2010	J. Intelligent Systems	10.1515/JISYS.2010.19.2.125	natural language processing;speech recognition;linguistics	NLP	-29.728877731875656	-79.40469872588514	139562
164dab5afb4688a664be6d2dd3f43bee8983ae8a	elements of a computational model for multi-party discourse: the turn-taking behavior of supreme court justices	discourse analysis;computational linguistics;law;conditional random field;second order;data segmentation;discourse marker;computer model	This paper explores computational models of multi-party discourse, using transcripts from U.S. Supreme Court oral arguments. The turn-taking behavior of participants is treated as a supervised sequence labeling problem and modeled using firstand secondorder Conditional Random Fields. We specifically explore the hypothesis that discourse markers and personal references provide important features in such models. Results from a sequence prediction experiment demonstrate that incorporating these two types of features yields significant improvements in performance. This work is couched in the broader context of developing tools to support legal scholarship, although we see other NLP applications as well. Publication Date: January 14, 2008	computational model;conditional random field;http 303;natural language processing;sequence labeling	Timothy Hawes;Jimmy J. Lin;Philip Resnik	2009	JASIST	10.1002/asi.21087	natural language processing;discourse marker;social science;computer science;artificial intelligence;discourse analysis;computational linguistics;linguistics;natural language;data segment;conditional random field;second-order logic	NLP	-27.437824268068297	-74.82449640009146	140399
0617dd6924df7a3491c299772b70e90507b195dc	the automatic content extraction (ace) program - tasks, data, and evaluation		The objective of the ACE program is to develop technology to automatically infer from human language data the entities being mentioned, the relations among these entities that are directly expressed, and the events in which these entities participate. Data sources include audio and image data in addition to pure text, and Arabic and Chinese in addition to English. The effort involves defining the research tasks in detail, collecting and annotating data needed for training, development, and evaluation, and supporting the research with evaluation tools and research workshops. This program began with a pilot study in 1999. The next evaluation is scheduled for September 2004. Introduction and Background Today’s global web of electronic information, including most notably the www, provides a resource of unbounded information-bearing potential. But to fully exploit this potential requires the ability to extract content from human language automatically. That is the objective of the ACE program – to develop the capability to extract meaning from multimedia sources. These sources include text, audio and image data. The ACE program is a “technocentric” research effort, meaning that the emphasis is on developing core enabling technologies rather than solving the application needs that motivate the research. The program began in 1999 with a study intended to identify those key content extraction tasks to serve as the research targets for the remainder of the program. These tasks were identified in general as the extraction of the entities, relations and events being discussed in the language. In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it (NIST 1999). The ACE program, however, attempts to take the task “off the page” in the sense that the research objectives are defined in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called “named entity” task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an answer. In a real sense, the task is to detect things that “aren’t there”. Reference resolution thus becomes an integral and critical part of solving the problem. During the period 2000-2001, the ACE effort was devoted solely to entity detection and tracking. During the period 2002-2003, relations were explored and added. 1 While the ACE program is directed toward extraction of information from audio and image sources in addition to pure text, the research effort is restricted to information extraction from text. The actual transduction of audio and image data into text is not part of the ACE research effort, although the processing of ASR and OCR output from such transducers is. Now, starting in 2004, events are being explored and added as the third of the three original tasks.	ace;information extraction;message understanding conference;named entity;transducer;transduction (machine learning);world wide web	George R. Doddington;Alexis Mitchell;Mark A. Przybocki;Lance A. Ramshaw;Stephanie Strassel;Ralph M. Weischedel	2004				NLP	-31.35835049332132	-73.97345861972187	140941
53ea39bb16473da6feadd017e9a86f8e00a7741e	priberam compressive summarization corpus: a new multi-document summarization corpus for european portuguese		In this paper, we introduce the Priberam Compressive Summarization Corpus, a new multi-document summarization corpus for European Portuguese. The corpus follows the format of the summarization corpora for English in recent DUC and TAC conferences. It contains 80 manually chosen topics referring to events occurred between 2010 and 2013. Each topic contains 10 news stories from major Portuguese newspapers, radio and TV stations, along with two human generated summaries up to 100 words. Apart from the language, one important difference from the DUC/TAC setup is that the human summaries in our corpus are compressive: the annotators performed only sentence and word deletion operations, as opposed to generating summaries from scratch. We use this corpus to train and evaluate learning-based extractive and compressive summarization systems, providing an empirical comparison between these two approaches. The corpus is made freely available in order to facilitate research on automatic summarization.	automatic summarization;multi-document summarization;television channel;text corpus;upsampling	Miguel B. Almeida;Mariana S. C. Almeida;André F. T. Martins;Helena Figueira;Pedro Mendes;Cláudia Pinto	2014			artificial intelligence;natural language processing;automatic summarization;newspaper;european portuguese;scratch;computer science;multi-document summarization;portuguese;sentence	NLP	-32.64152497264461	-74.98586192739353	141450
5dccc9de07574306763bb37bd83e041d1bee6cd5	left-right trees, a new tree structure for machine translation	tree structure;machine translation		machine translation;tree structure	Kouichi Kurokawa;Takumi Kasai	1998	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199805)29:5%3C84::AID-SCJ9%3E3.0.CO;2-J	natural language processing;tree rotation;computer science;machine learning;pattern recognition;machine translation;tree structure;search tree	NLP	-27.393737299575214	-79.55971313990928	141539
73e28cc07e7af4e893b91cbf710acc2f16f7e2a4	knowledge extraction with nooj using a syntactico-semantic approach for the arabic utterances understanding.		Regarding the amelioration of NLP field, knowledge extraction has become an interesting research topic. Indeed, the need to an improvement through the NLP techniques has become also necessary and advantageous. Hence, in a general context of the construction of an Arabic touristic corpus equivalent to those of European projects MEDIA and LUNA, and due to the lack of Arabic electronic resources, we had the opportunity to expand the EL-DicAr of [11] by knowledge hinging on Touristic Information and Hotel Reservations (TIHR). Thus, in the same manner of [11], we have developed local grammars for the recognition of essential knowledge in our field of study. This task facilitates greatly the subsequent work of understanding user utterances interacting with a dialogue system.		Chahira Lhioui;Anis Zouaghi;Mounir Zrigui	2016		10.1007/978-3-319-75487-1_42	artificial intelligence;arabic;knowledge extraction;computer science;natural language processing;information retrieval;compound;named entity;rule-based machine translation	NLP	-30.414170953312777	-75.82199525496034	141936
855e31d46cf6a48b1618f47d46781d196bd3c11c	improving the communicability of spreadsheet desgins: annotating with descriptive tags			spreadsheet	David G. Hendry;Thomas R. G. Green;David J. Gilmore;Simon P. Davies	1992				HCI	-32.35073034705082	-77.15979017844502	142765
63bd507fbdf7d8e6dbe17081544853d2027b77cd	multilingual generation in data base nl-interface			nl (format)	Michael V. Boldasov	2003	Prague Bull. Math. Linguistics		natural language processing;artificial intelligence;computer science	NLP	-31.0097419292099	-77.60981829696681	143473
56bc8cde00522b16eeca6a415990ea7330642e98	ontology-based interoperation of linguistic tools for an improved lemma annotation in spanish	informatica;telecomunicaciones	In this paper, we present an ontology-based methodology and architecture for the comparison, assessment, combination (and, to some extent, also contrastive evaluation) of the results of different linguistic tools. More specifically, we describe an experiment aiming at the improvement of the correctness of lemma tagging for Spanish. This improvement was achieved by means of the standardisation and combination of the results of three different linguistic annotation tools (Bitext’s DataLexica, Connexor’s FDG Parser and LACELL’s POS tagger), using (1) ontologies, (2) a set of lemma tagging correction rules, determined empirically during the experiment, and (3) W3C standard languages, such as XML, RDF(S) and OWL. As we show in the results of the experiment, the interoperation of these tools by means of ontologies and the correction rules applied in the experiment improved significantly the quality of the resulting lemma tagging (when compared to the separate lemma tagging performed by each of the tools that we made interoperate).	brill tagger;correctness (computer science);functional discourse grammar;interoperability;interoperation;ontology (information science);parallel text;parser;part-of-speech tagging;web ontology language;xml	Antonio Pareja-Lora;Guadalupe Aguado de Cea	2010			natural language processing;computer science;data mining;database;information retrieval	NLP	-28.50094768318798	-74.56627200827697	143502
8556c5d28477099808a6b9d7f720d03c2604dce1	defining and classifying space builders for information extraction	classifying space;information extraction	The paper addresses the question of Information Extraction aimed at multilingual text generation, or text re-writing. This method provides an alternative to traditional Machine Translation, but is also related to text summarization. Given a source text, a re-writing system selects and structures the textual information in order to generate a “content report”. The present approach is inspired by recent IE-research, classical speech act theory, and Cognitive Semantics, especially the Theory of Mental Spaces and employed in an experimental system for understanding of news reports. The authors focus on the problem of identification and interpretation of ‘space builders’, i.e. linguistic signals for establishing mental spaces.	automatic summarization;experimental system;information extraction;machine translation;natural language generation;spaces	Barbara Gawronska;Björn Erlendsson;Niklas Torstensson	2004			information extraction;data mining;computer science;classifying space	NLP	-33.600671470550765	-78.16182519358347	143664
e9c1f510bcf5933d3cf8ec8108a04a9ba601a843	a fully statistical approach to natural language interfaces	statistical approach;natural language interface system;end-to-end system;maps input utterance;trained statistical model;statistical process;semantic interpretation;representation frame;statistical model;natural language interface	We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.	anaphora (linguistics);end system;end-to-end principle;map;natural language user interface;parsing;semantic interpretation;statistical model	Scott Miller;David Stallard;Robert J. Bobrow;Richard M. Schwartz	1996			natural language processing;language identification;statistical model;semantic interpretation;semantic computing;speech recognition;natural language user interface;computer science;linguistics	NLP	-28.49497084154322	-80.01405566806282	143742
3d9db1146acd2da5ad7b85d81c737f9260576c37	periods, capitalized words, etc.	nom propre;sentence boundary;computacion informatica;abreviation;normalisation;information retrieval;text processing;analyse textuelle;filologias;grupo de excelencia;text analysis;majuscule;linguistique appliquee;methode;classification;proper noun;linguistica;etiquetage automatique;elephants;ciencias basicas y experimentales;proper names;disambiguation;frontiere de phrase;normalization;part of speech;error rate;capital letter;computational linguistics;abbreviation;desambiguisation;grupo a;linguistique informatique;method;natural language processing;tagging;applied linguistics	In this article we present an approach for tackling three important aspects of text normalization: sentence boundary disambiguation, disambiguation of capitalized words in positions where capitalization is expected, and identification of abbreviations. As opposed to the two dominant techniques of computing statistics or writing specialized grammars, our document-centered approach works by considering suggestive local contexts and repetitions of individual words within a document. This approach proved to be robust to domain shifts and new lexica and produced performance on the level with the highest reported results. When incorporated into a part-of-speech tagger, it helped reduce the error rate significantly on capitalized words and sentence boundaries. We also investigated the portability to other languages and obtained encouraging results.	brill tagger;lexicon;part-of-speech tagging;sentence boundary disambiguation;software portability;text normalization;word-sense disambiguation	Andrei Mikheev	2002	Computational Linguistics	10.1162/089120102760275992	natural language processing;speech recognition;computer science;computational linguistics;applied linguistics;proper noun;linguistics	NLP	-26.961194161009253	-76.69246059810534	143870
88feca21ad8dc99b2d328b832f39da3d4a171ede	automatic labelling of multi-sensor speech database: issues and perspectives				Nathalie Parlangeau;Régine André-Obrecht;Alain Marchal	1995			speech recognition;natural language processing;labelling;data mining;artificial intelligence;computer science	NLP	-31.06991313316035	-77.3159014079115	144400
83290b120cf23de7510e5fca96f633c35ea7db84	restricciones sobre orden de constituyentes basadas en corpus y destinadas a la desambiguación sintáctica		Spanish is a quite flexible constituent order language. However, the order in which functions appear is not arbitrary but is due to several factors, some of them formally detectable. This paper presents a proposal of description of the order of clause-level constituents in Spanish, based in corpus and aimed at the desambiguation of syntactic trees.	linear algebra;syntactic predicate;text corpus	M. Pilar Valverde Ibañez	2006	Procesamiento del Lenguaje Natural		artificial intelligence;linguistics;natural language processing;computer science	NLP	-27.995859838182792	-77.29542009372032	144581
323f98a8d19c5de86f8c0757ef8de13762b9429f	nederlab: towards a single portal and research environment for diachronic dutch text corpora		The Nederlab project aims to bring together all digitized texts relevant to the Dutch national heritage, the history of the Dutch language and culture (circa 800 – present) in one user friendly and tool enriched open access web interface. This paper describes Nederlab halfway through the project period and discusses the collections incorporated, back-office processes, system back-end as well as the Nederlab Research Portal end-user web application.	circa;text corpus;usability;user interface;web application	Hennie Brugman;Martin Reynaert;Nicoline van der Sijs;René van Stipriaan;Erik F. Tjong Kim Sang;Antal van den Bosch	2016			world wide web;artificial intelligence;web application;natural language processing;user friendly;text corpus;computer science;user interface	NLP	-33.65546870647311	-75.73301329077337	144743
21702c1c486b18998905a681b3d86b466481abe8	responsa: a full-text retrieval system with linguistic processing for a 65-million word corpus of jewish heritage in hebrew	text retrieval		document retrieval	Yaacov Choueka	1989	IEEE Data Eng. Bull.		natural language processing;speech recognition;computer science	DB	-31.279077247243187	-76.97480101188303	144855
4ae65d869b476c2b493793256d8eb2ba19888fff	lexical resources for noun compounds in czech, english and zulu.	noun;production process;semantic relations;structural properties	In this paper we discuss noun compounding, a highly generative, productive process, in three distinct languages: Czech, English and Zulu. Derivational morphology presents a large grey area between regular, compositional and idiosyncratic, non-compositional word forms. The structural properties of compounds in each of the languages are reviewed and contrasted. Whereas English compounds are head-final and thus left-branching, Czech and Zulu compounds usually consist of a leftmost governing head and a rightmost dependent element. Semantic properties of compounds are discussed with special reference to semantic relations between compound members which cross-linguistically show universal patterns, but idiosyncratic, language specific compounds are also identified. The integration of compounds into lexical resources, and WordNets in particular, remains a challenge that needs to be considered in terms of the compounds’ syntactic idiosyncrasy and semantic compositionality. Experiments with processing compounds in Czech, English and Zulu are reported and partly evaluated. The obtained partial lists of the Czech, English and Zulu compounds are also described.	gcu grey area;galaxy morphological classification;wordnet	Karel Pala;Christiane Fellbaum;Sonja E. Bosch	2010			natural language processing;noun;nominalization;noun phrase;proper noun;scheduling;linguistics	NLP	-28.17581157023025	-77.56451034856357	145534
4abc9bf510a93b8f7b34ecb0ea9380181d19af45	detección automática de chilenismos verbales a partir de reglas morfosintácticas. resultados preliminares	morphosyntactic rules;computacion informatica;filologias;chilenismo verbal;info eu repo semantics article;informacion documentacion;linguistica;automatic detection;ciencias basicas y experimentales;deteccion automatica;smorph;grupo a;ciencias sociales;verbal chilenismo;mps;grupo b;reglas morfosintacticas	In this paper, the tasks made for obtaining an automatic extractor for verbal chilenismos using natural language rules are described. With this objective, a formalization of lexical, morphological and syntactic features was made, for a subsequent computational implementation. Firstly, verbal chilenismos were classified in four kinds, according to the use registered in the dictionaries and syntactic features: pure, pure-clitic, of sense, and of senseclitic. Secondly, syntactic rules were established for the automatic recognition. Smorph and Post Smorph Module were used in the computational work, both use natural language rules. The method was tested in a corpus composed by 5194 tweets produced in Chile, obtaining 85.54% of precision, 96.16% of coverage, and 90.53% of F-measure. The results show that this method is able for this kind of work, all the same, some limitations and mistakes were detected and more specific and new rules are necessary for the recognition task and for filtering wrong tagged.	computation;dictionary;lexicon;natural language;randomness extractor;text corpus	Walter Koza;Pedro Alfaro;Ricardo Martínez	2015	Procesamiento del Lenguaje Natural		metre per second	NLP	-27.26051194415388	-77.82839949752007	145940
0b01f22be8b39ab0c13ed92e2b9bb0638c95a584	structural metadata annotation: moving beyond english	katedra kybernetiky;kybernetika;informacni a řidici systemy;automaticke řizeni;metadata extraction;mandarin chinese;uměla inteligence;speech to text;publications structural metadata annotation moving beyond english;publikace structural metadata annotation moving beyond english	The goal of metadata extraction (MDE) is to enable technology that can take raw speech-to-text output and refine it into forms that are more useful to humans and to downstream automatic processes. Starting in 2003, a structural metadata annotation task was defined for English as part of the DARPA EARS Program. A significant new challenge for MDE is the addition of new languages. This paper reports on work undertaken to apply MDE annotation to data from three very different languages: Mandarin Chinese, Levantine Arabic, and conversational Czech. Details of annotation task modifications are provided for each language; along with a general overview of data and annotation tools for non-English MDE.	downstream (software development);model-driven engineering;speech recognition;spontaneous order;super robot monkey team hyperforce go!	Stephanie Strassel;Jáchym Kolár;Zhiyi Song;Leila Barclay;Meghan Lammie Glenn	2005			natural language processing;speech recognition;mandarin chinese;image retrieval;computer science;linguistics;metadata;world wide web;information retrieval	NLP	-33.17945438860319	-76.28629299115875	146529
a991ce7177a97a0cdc368350c6d0699e0b2ba98a	dansk morfologi til automatisk analyse (danish morphology for automatic analysis) [in danish]			mathematical morphology	Hanne Ruus	1983			genealogy;danish;history	NLP	-31.935170969476278	-78.30037994085193	146795
5c131aef8bff9769af435e34beaf0c382d39a941	signbank: software to support web based dictionaries of sign language		Signbank is a web application that was originally built to support the Auslan Signbank on-line web dictionary, it was an Open Source re-implementation of an earlier version of that site. The application provides a framework for the development of a rich lexical database of sign language augmented with video samples of signs. As an Open Source project, the original Signbank has formed the basis of a number of new sign language dictionaries and corpora including those for British Sign Language, Sign Language of the Netherlands and Finnish Sign Language. Versions are under development for American Sign Language and Flemish Sign Language. This paper describes the overall architecture of the Signbank system and its representation of lexical entries and associated entities.	dictionary;entity;lexical database;online and offline;text corpus;web application	Steve Cassidy;Onno Crasborn;Henri Nieminen;Wessel Stoop;Micha Hulsbosch;Susan Even;Erwin Komen;Trevor Johnson	2018			natural language processing;speech recognition;artificial intelligence;sign language;web application;lexical database;computer science;software	NLP	-33.09090872086827	-74.84618369099906	147070
f48a72ddf25ac606775e91419018481e08f025b4	bias and agreement in syntactic annotations		We present a study on two key characteristics of human syntactic annotations: anchoring and agreement. Anchoring is a well known cognitive bias in human decision making , where judgments are drawn towards pre-existing values. We study the influence of anchoring on a standard approach to creation of syntactic resources where syntactic annotations are obtained via human editing of tagger and parser output. Our experiments demonstrate a clear anchoring effect and reveal un-wanted consequences, including overestima-tion of parsing performance and lower quality of annotations in comparison with human-based annotations. Using sentences from the Penn Treebank WSJ, we also report the first systematically obtained inter-annotator agreement estimates for English syntactic parsing. Our agreement results control for anchoring bias, and are consequential in that they are on par with state of the art parsing performance for English. We discuss the impact of our findings on strategies for future annotation efforts and parser evaluations 1 .	brill tagger;experiment;inter-rater reliability;parsing;the wall street journal;treebank	Yevgeni Berzak;Yan Huang;Andrei Barbu;Anna Korhonen;Boris Katz	2016	CoRR		natural language processing;computer science;linguistics	NLP	-27.45660939599935	-75.36726147338443	147483
a1707b3e32e63da3662c0b1cc393e8f34217a118	automatic detection of modality with itgetaruns	settore inf 01 informatica;settore l lin 01 glottologia e linguistica	In this paper we present a system for modality detection which is then used for Subjectivity and Factuality evaluation. The system has been tested lately on a task for Subjectivity and Irony detection in Italian tweets, where the performance was 10 and 4, respectively, over 27 participants overall. We will focus our paper on an internal evaluation where we considered three national newspapers Il Corriere, Repubblica, Libero. This task was prompted by a project on the evaluation of press stylistic features in political discourse. The project used newspaper articles from the same sources over a period of three months, thus including latest political 2013 governmental crisis. We intended to produce a similar experiment and evaluate results in comparison with previous 2011 crisis. In this evaluation, we focused on Subjectivity, Polarity and Factuality which include Modality evaluation. Final graphs at the end of the paper will show results confirming our previous findings about differences in style, with Il Corriere emerging as the most atypical.	brill tagger;experiment;modality (human–computer interaction);parsing;propositional calculus	Rodolfo Delmonte	2015		10.1007/978-3-319-19581-0_38	speech recognition;computer science;algorithm	NLP	-28.56345155528323	-75.60106747662961	147831
3692c1674b8ec1790984f5173c58595c571c46d0	easyenglish: a tool for improving document quality	authoring tool	"""We describe the authoring tool, EasyEnglish, which is part of IBM's internal SGML editing environment, Information Development Workbench. EasyEnglish helps writers produce clearer and simpler English by pointing out ambiguity and complexity as well as performing some standard grammar checking. Where appropriate, EasyEnglish makes suggestions for rephrasings that may be substituted directly into the text by using the editor interface. EasyEnglish is based on a full parse by English Slot Grammar; this makes it possible to produce a higher degree of accuracy in error messages as well as handle a large variety of texts. 1 I n t r o d u c t i o n Like most other big corporations today, IBM is interested in cost-effective, yet high-quality information dissemination. Every year, many pages of online and printed documentation are produced. No matter what part of the world the documentation is written in, it is normally first written in English, and then translated into all the other supported languages. IBM has developed a number of tools to help writers cope with this task of information development. In this paper, we describe EasyEnglish, a tool that helps writers produce clearer and simpler English by pointing out ambiguity and complexity. Where appropriate, EasyEnglish makes suggestions for rephrasings. The EasyEnglish system can be viewed as a """"grammar checker++"""", in that standard grammar checking facilities such as spell-checking, word count (sentence length), and detection of passive constructions are available in addition to the checks for ambiguity. Furthermore, facilities for user-defined controlled vocabulary are available. Totally, there are currently about forty checks. EasyEnglish is part of IBM's internal Information Development Workbench (IDWB), an SGML-based document creation and document management system. ArborText's Adept editor is used with IDWB 1 EasyEnglish summarizes the problems encountered in a given document by giving an overall rating, the Clarity Indez (CI). The CI has to be in a certain range before the document can be accepted for publication. EasyEnglish combines features from both standard grammar checkers and Controlled Language (CL) compliance checkers with checks for structural ambiguity in a way that we believe is general enough to be useful for any writer.., not just tec~hnical writers. It has been claimed that the restrictions found in CLs mostly reflect the inadequacies of the MT systems used in conjunction with CLs (Cl~mencin 1996; van der Eijk et al. 1996; Hayes et al. 1996). It is certainly the case that preprocessing a document with the same parser that is used for source analysis improves the MT results. EasyEnglish uses the same parser as LMT (McCord 1989a, 1989b). This offers an obvious advantage for MT results. Other MT systems, including the KANT system (Mitamura and Nyberg 1995; Nyberg and Mitamura 1996), see the advantage of this. However, we claim that a document that has been """"EasyEnglished"""" is also easier to understand for native speakers as well as nonnative speakers of English. A similar point has been made for Caterpillar Technical English (Hayes ct al. 1996). We think, however, that our approach is more general because our use of a broad-coverage, geaeral English grammar 2 allows us to go beyond the concept of CL to look for more general types of ambiguities. I EasyEnglish also works with the XEDIT editor on VM and the EPM editor on OS/2. An earlier version of EasyEnglish was written in Prolog; however, the current version is written in pure ANSI C, and hence the question of platform is mainly a matter of supplying an appropriate editor interface. ~English Slot Grammar (McCord 1980, 1990, 1993)"""	ansi c;controlled natural language;controlled vocabulary;documentation;electronic counter-countermeasure;error message;grammar checker;hayes microcomputer products;kant;kaisa nyberg;logistic model tree;os/2;parsing;preprocessor;printing;prolog;spell checker;standard generalized markup language;word lists by frequency;workbench;xedit	Arendse Bernth	1997		10.3115/974557.974581	natural language processing;computer science;multimedia;world wide web	NLP	-33.492952106581875	-74.77040079632987	148284
f43a36dadaa986a1fcaead435d89ce0c76891dda	assessing the efficiency of suffix stripping approaches for portuguese stemming		Stemming is the process of reducing inflected words to their root form, the stem. Search engines use stemming algorithms to conflate words in the same stem, reducing index size and improving recall. Suffix stripping is a strategy used by stemming algorithms to reduce words to stems by processing suffix rules suitable to address the constraints of each language. For Portuguese stemming, the RSLP was the first suffix stripping algorithm proposed in literature, and it is still widely used in commercial and open source search engines. Typically, the RSLP algorithm uses a list-based approach to process rules for suffix stripping. In this article, we introduce two suffix stripping approaches for Portuguese stemming. Particularly, we propose the hash-based and the automata-based approach, and we assess their efficiency by contrasting them with the state-of-the-art list-based approach. Complexity analysis shows that the automata-based approach is more efficient in time. In addition, experiments on two datasets attest the efficiency of our approaches. In particular, the hash-based and the automata-based approaches outperform the list-based approach, with reduction of up to 65.28% and 86.48% in stemming time, respectively.	stemming	Wadson Gomes Ferreira;Willian Antônio dos Santos;Breno Macena Pereira de Souza;Tiago Matta Machado Zaidan;Wladmir Cardoso Brandão	2015		10.1007/978-3-319-23826-5_21	arithmetic;data mining;mathematics;stemming;algorithm	Logic	-28.465047302537652	-74.0957762539156	148357
3e9148d8c6724ad5d66d247b14fa3b0ad19c3b34	annotating wall street journal texts using a hand-crafted deep linguistic grammar	rich linguistic annotation;semi-automated annotation process;multi-functional linguistic resource;wide-coverage grammar;on-going effort;penn treebank;various stage;annotating wall street journal;wall street journal section;hand-crafted deep linguistic grammar	References ● Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank: motivation and preliminary applications. In Proceedings of COLING 2002. Taipei, Taiwan. ● Dan Flickinger. 2002. On building a more efficient grammar by exploiting types. In Collaborative Language Engineering, pages 1–17. CSLI Publications. ● Yi Zhang and Valia Kordoni. 2008. Robust Parsing with a Large HPSG Grammar. In Proceedings of LREC 2008, Marrakesh, Morocco. ● Ulrich Callmeier. 2001. Efficient parsing with large-scale unification grammars. Master’s thesis, Saarland University, Saarbruecken, Germany. ● Stephan Oepen and Ulrich Callmeier. 2000. Measure for Measure: Parser Cross-Fertilization.Towards Increased Component Comparability and Exchange. In Proceedings of the 6th International Workshop on Parsing Technology, Trento, Italy. ● Peter Adolphs, Stephan Oepen, Ulrich Callmeier, Berthold Crysmann, Daniel Flickinger, Bernd Kiefer. 2008. Some Fine Points of Hybrid Natural Language Parsing. In Proceedings of LREC 2008, Marrakesh, Morocco. Annotation Cycle HPSG Parsing	exploit (computer security);head-driven phrase structure grammar;lingo (programming language);parser;parsing;the wall street journal;treebank;unification (computer science)	Valia Kordoni;Yi Zhang	2009			natural language processing;speech recognition;computer science;linguistics	NLP	-32.07844264157434	-75.92630196405412	148900
bfbf95848d3b46bb78e63312e7a93e1ef323d851	making sense of collocations	noun;raw materials;semantic information;machine learning;nearest neighbor;computational linguistics;off the shelf;linguistique informatique	Lexico-semantic collocations (LSCs) are a prominent type of multiword expressions. Over the last decade, the automatic compilation of LSCs from text corpora has been addressed in a significant number of works. However, very often, the output of an LSC-extraction program is a plain list of LSCs. Being useful as raw material for dictionary construction, plain lists of LSCs are of a rather limited use in NLP-applications. For NLP, LSCs must be assigned syntactic and, especially, semantic information. Our goal is to develop an ‘‘off-the-shelf’’ LSC-acquisition program that annotates each LSC identified in the corpus with its syntax and semantics. In this article, we address the annotation task as a classification task,viewing it as a machine learning problem. The LSC-typology we use are the lexical functions from the Explanatory Combinatorial Lexicology; as lexico-semantic resource, EuroWordnet has been used. The applied machine learning technique is a variant of the nearest neighbor-family, which is defined over lexico-semantic features of the elements of LSCs. The technique has been tested on Spanish verb–noun bigrams. 2005 Elsevier Ltd. All rights reserved.	algorithm;bigram;biological anthropology;bootstrapping (compilers);collocation;compiler;dictionary;display resolution;eurowordnet;experiment;hoare logic;human-readable medium;lexical function;lexico;machine learning;microsoft word for mac;natural language processing;parsing;semi-continuity;text corpus;word-sense disambiguation;wordnet	Leo Wanner;Bernd Bohnet;Mark Giereth	2006	Computer Speech & Language	10.1016/j.csl.2005.10.002	natural language processing;noun;speech recognition;computer science;computational linguistics;raw material;machine learning;linguistics;k-nearest neighbors algorithm	NLP	-28.911605468893413	-76.04848767758511	149147
e2a6cedd1b6c4aee070e0d22205a72fa211ba1dc	a character-based indexing and word-based ranking method for japanese text retrieval		7KLV SDSHU GHVFULEHV D -DSDQHVH WH[W UHWULHYDO V\VWHP WKDW ZH DSSOLHG WR WKH -DSDQHVH DG KRF ,5 WDVN LQ WKH 17&,5 :RUNVKRS $ FKDUDFWHU EDVHG LQGH[LQJ DQG ZRUG EDVHG UDQNLQJ PHWKRG ZDV LPSOHPHQWHG RQ WKLV V\VWHP 7KH V\VWHP JHQHUDWHV DQ LQGH[ IRU WLWOH! DEVWUDFW! DQG NH\ZRUG! SDUWV LQ GRFXPHQWV ,W SDUVHV RQO\ GHVFULSWLRQ! SDUWV LQ VHDUFK WRSLFV DV TXHULHV ,WV UDQNLQJ VWUDWHJ\ LV YHU\ VLPSOH ,W XVHV WKH YHFWRU VSDFH EDVHG RQ VKRUW XQLWV RI -DSDQHVH ZRUGV ,W GHOHWHV VWRS ZRUGV LQ D TXHU\ DQG FDOFXODWHV WKH 7) ,') VFRUH IRU HDFK GRFXPHQW ,WV DYHUDJH SUHFLVLRQ VFRUH IRU WKH WUDLQLQJ VHW RI VHDUFK WRSLFV LV ([SHULPHQWDO UHVXOWV VKRZ WKH HIIHFWLYHQHVV RI XVLQJ WKH VKRUW XQLWV RI ZRUGV DQG GHOHWLQJ VWRS ZRUGV LQ D TXHU\ .H\ZRUGV	data quality;discontinuous galerkin method;document retrieval;inertial reference unit;letter-quality printer;logical volume management;rs-232	Toshikazu Fukushima;Susumu Akamine	1999			visual word;concept search;information retrieval;document retrieval;search engine indexing;ranking;artificial intelligence;tf–idf;pattern recognition;computer science	DB	-30.321722817973505	-77.54074196678332	149397
be669841d0dfd853b33b215016c4ea988b5e6711	xmg: extensible metagrammar		In this article, we introduce eXtensible MetaGrammar (XMG), a framework for specifying tree-based grammars such as Feature-Based Lexicalized Tree-Adjoining Grammars (FB-LTAG) and Interaction Grammars (IG). We argue that XMG displays three features that facilitate both grammar writing and a fast prototyping of tree-based grammars. Firstly, XMG is fully declarative. For instance, it permits a declarative treatment of diathesis that markedly departs from the procedural lexical rules often used to specify tree-based grammars. Secondly, the XMG language has a high notational expressivity in that it supports multiple linguistic dimensions, inheritance, and a sophisticated treatment of identifiers. Thirdly, XMG is extensible in that its computational architecture facilitates the extension to other linguistic formalisms. We explain how this architecture naturally supports the design of three linguistic formalisms, namely, FB-LTAG, IG, and Multi-Component Tree-Adjoining Grammar (MC-TAG). We further show how it permits a straightforward integration of additional mechanisms such as linguistic and formal principles. To further illustrate the declarativity, notational expressivity, and extensibility of XMG, we describe the methodology used to specify an FB-LTAG for French augmented with a unification-based compositional semantics. This illustrates both how XMG facilitates the modeling of the tree fragment hierarchies required to specify tree-based grammars and of a syntax/semantics interface between semantic representations and syntactic trees. Finally, we briefly report on several grammars for French, English, and German that were implemented using XMG and compare XMG with other existing grammar specification frameworks for tree-based grammars.	context-free grammar;debugger;dependency grammar;dialog system;extensibility;formal system;fully buffered dimm;identifier;incremental compiler;integrated development environment;lexical functional grammar;mathematical morphology;natural language generation;online and offline;open-source software;programming language;redundancy (engineering);semantics (computer science);syntactic predicate;tree-adjoining grammar;unification (computer science)	Benoît Crabbé;Denys Duchier;Claire Gardent;Joseph Le Roux;Yannick Parmentier	2013	Computational Linguistics	10.1162/COLI_a_00144	natural language processing;tree-adjoining grammar;l-attributed grammar;computer science;linguistics;programming language;algorithm	NLP	-29.081726826757905	-80.01074272991013	149932
6182565c9d2a6f6ca1eae61634974b5b12f9ef86	resource creation for training and testing of normalisation systems for konkani-english code-mixed social media text		Code-Mixing is the mixing of two or more languages or language varieties in speech. Apart from the inherent linguistic complexity, the analysis of code-mixed content poses complex challenges owing to the presence of spelling variations and non-adherence to a formal grammar. However, for any downstream Natural Language Processing task, tools that are able to process and analyze code-mixed social media data are required. Currently there is a lack of publicly available resources for code-mixed Konkani-English social media data, while the amount of such text is increasing everyday. The lack of a standard dataset to evaluate these systems makes it difficult to make any meaningful comparisons of their relative accuracies.	social media	Akshata Phadte	2018		10.1007/978-3-319-91947-8_26	natural language processing;data mining;code-mixing;konkani;spelling;social media;linguistic sequence complexity;computer science;artificial intelligence;formal grammar	NLP	-29.22774620168766	-73.66060566487653	149961
8aaf7cbe18ea89dfbdb287ab168ec35311bc590b	a natural language approach for the design of batch operating procedures.	natural language		natural language	Andreas A. Linninger	1998	Informatica (Slovenia)		natural language processing;computer science;natural language;programming language	PL	-29.49191430374925	-80.1252397469864	150250
f8dc980a9575796068b2529bc68ca725f6b9555c	harmony in diversity: the language codes in english-chinese poetry translation			language code	Xiaxing Pan;Xinying Chen;Haitao Liu	2018	DSH	10.1093/llc/fqx001		NLP	-31.433321838717294	-78.50687415262378	150630
f95414d7d496056e5155e69b17a760f9cffbcaf6	computational theory of short distance reflexive anaphoric devices in urdu discourse for effective machine translation	computability theory;rule based;anaphora resolution;anaphoric devices;target language;natural language;source language;discourse;antecedent;machine translation	Computational treatment of natural languages is a very challenging task. For effective machine translation of multi-sentential discourse units from a source language into some target language, resolution of anaphoric expressions is of crucial importance. This paper presents the computational treatment of the rarely studied form of pronominal reference called reflexives. A knowledge-poor, rule-based algorithm has been developed to identify and resolve the reflexives in Urdu discourses. The proposed algorithm resolves the reflexives in Urdu discourses with more than 85% accuracy.	algorithm;anaphora (linguistics);compiler;computation;logic programming;machine translation;natural language	Mohammad Naveed Ali;Mohammad Aamir Khan;Mohammad Abid Khan	2009		10.1145/1838002.1838023	natural language processing;computer science;linguistics;communication	NLP	-28.341274865040486	-76.888613434845	150678
787c11ccfeeef35af5e9b7d6ad9d1af2a2e6e08b	postagging and semantic dictionary creation for hittite cuneiform		On our poster we want to present ongoing work to create an automatic natural language processing tool for Hittite cuneiform. Hittite cuneiform texts are to this day manually transcribed by the respective experts and then published in a transliteration format (commonly ATF). Pictures of the original cuneiform tablet may be provided and more rarely cuneiform representations in Unicode are present. Due to recent advancements in the field (such as Cuneify) an automatic translation of many Hittite cuneiform transliterations to their respective cuneiform representation is possible.	cuneiform;dictionary;machine translation;natural language processing;tablet computer;unicode	Timo Homburg	2017			hittite cuneiform;geography;ancient history	NLP	-32.88012967544154	-75.36115905308993	150813
04d4610301b9be538a7ee0a7759cb4503c18f53d	introduction: special issue on anaphora resolution in machine translation and multilingual nlp	anaphora resolution;machine translation		anaphora (linguistics);machine translation;natural language processing	Ruslan Mitkov	1999	Machine Translation	10.1023/A:1011132522992	natural language processing;computer science;linguistics;machine translation	NLP	-30.70446879964248	-77.58956936001972	151079
e9e92eba5a2029faf05a040a5cfb19dbcc4acb7e	hfst-swener ― a new ner resource for swedish		Named entity recognition (NER) is a knowledge-intens ive information extraction task that is used for re cognizing textual mentions of entities that belong to a predefined set of categor i s, such as locations, organizations and time expr essions. NER is a challenging, difficult, yet essential preprocessing technology f or many natural language processing applications, a d particularly crucial for language understanding. NER has been actively explo red in academia and in industry especially during t he last years due to the advent of social media data. This paper describes the conv ersion, modeling and adaptation of a Swedish NER sys tem from a hybrid environment, with integrated functionality from var ious processing components, to the Helsinki FiniteState Transducer Technology (HFST) platform. This new HFST-based NER (HFST-SweNE R) is a full-fledged open source implementation that supports a variety of generic named entity types and consists of multiple , reusable resource layers, e.g., various n-gram-ba sed named entity lists	expr;information extraction;n-gram;named entity;natural language processing;natural language understanding;open-source software;preprocessor;sed;social media;transducer	Dimitrios Kokkinakis;Jyrki Niemi;Sam Hardwick;Krister Lindén;Lars Borin	2014				NLP	-32.30288415402228	-73.90086633162635	151164
1aa9b1e2acd7dfc4ad2ec039ce7a7a879d042a58	propagation in bipartite graphs for topic extraction in stream of textual data			software propagation;text corpus	Thiago de Paulo Faleiros	2016				NLP	-30.541994033238367	-76.9665156006376	151423
4470878e7dc9bc6e7429b8d66b88996ada0e4fb1	a computational algorithm for metrical classification of verse		The science of versification and analysis of verse in Sanskrit is governed by rules of metre or chandas. Such metre-wise classification of verses has numerous uses for scholars and researchers alike, such as in the study of poets and their style of Sanskrit poetical works. This paper presents a comprehensive computational scheme and set of algorithms to identify the metre of verses given as Sanskrit (Unicode) or English E-text (Latin Unicode). The paper also demonstrates the use of euphonic conjunction rules to correct verses in which these conjunctions, which are compulsory in verse, have erroneously not been implemented.	algorithm;computation;computational problem;database schema;parsing;shamash;unicode;verse protocol	N. Rama;Meenakshi Lakshmanan	2010	CoRR		artificial intelligence;algorithm	NLP	-31.562099029314574	-74.84441189513844	151751
f542fff8f365504d3488493ecce5550226269dc2	extraction from relative and embedded interrogative clauses in danish	article	In Danish relative clauses and embedded interrogative clauses are not extraction islands. However, there is an asymmetry between the two clauses. In Danish it is possible to extract the subject out of an embedded interrogative clause. Extraction of the subject out of a relative clause, on the other hand, is not allowed. In this paper we present a formal HPSG analysis of extraction in Danish which treats the extraction out of relative and embedded interrogative clauses in a uniform manner, and the asymmetry between the clauses will be shown to follow from a more general constraint on adjuncts.	embedded system;head-driven phrase structure grammar;np (complexity)	Anne Bjerre	2011			natural language processing;dependent clause;computer science;linguistics;algorithm	EDA	-27.370019997021732	-76.81327428320314	151874
d01d434d560def2f0f3ab4952359e4394f0c2f62	a study in urdu corpus construction	british broadcasting company;natural language processing community;raw urdu text;urdu newspaper;urdu corpus construction;natural language processing;urdu text;urdu web site;test nlp tool;written text;available urdu corpus	We are interested in contributing a small, publicly available Urdu corpus of written text to the natural language processing community. The Urdu text is stored in the Unicode character set, in its native Arabic script, and marked up according to the Corpus Encoding Standard (CES) XML Document Type Definition (DTD). All the tags and metadata are in English. To date, the corpus is made entirely of data from British Broadcasting Company’s (BBC) Urdu Web site, although we plan to add data from other Urdu newspapers. Upon completion, the corpus will consist mostly of raw Urdu text marked up only to the paragraph level so it can be used as input for natural language processing (NLP) tasks. In addition, it will be hand-tagged for parts of speech so the data can be used to train and test NLP tools.	character encoding;natural language processing;text corpus;unicode;xml	Dara Becker;Kashif Riaz	2002			natural language processing;speech recognition;computer science;corpus linguistics;linguistics	NLP	-32.73731202071272	-75.04979792874497	152660
cddb1d4b9abe93428800ab68851619597d3d17b2	ontolingannot’s ontologies: facilitating interoperable linguistic annotations (up to the pragmatic level)		This paper presents the OntoLingAnnot annotation framework, already developed for the annotation of morphological, syntactic, semantic and discourse phenomena, and its extension to cover the annotation of pragmatic phenomena. This extension was considered the ideal test bed for the interoperability of the linguistic annotations performed by means of the platform, since (i) pragmatics itself deals with a real mix of different linguistic topics, such as speech acts, pragmatic coherence relations, deixis, presuppositions and implicatures; and (ii) it clearly interacts with the rest of levels, since (potentially) every linguistic unit at any level can have a pragmatic projection. In particular, it introduces the different pragmatic units that can be used to annotate texts and dialogues using the framework. These pragmatic units are included in the set of ontologies associated to OntoLingAnnot, whose design requirements and development process are also described here. Besides, this paper shows as well the main principles and properties of the OntoLingAnnot annotation framework that help its different annotations interoperate.		Antonio Pareja-Lora	2012		10.1007/978-3-642-28249-2_12	natural language processing;computer science;linguistics;communication	NLP	-30.555675070660257	-75.55263846364993	152794
f55593285274c36bb090746d76b5ba4739a65a5c	generative transformation via abstract change script				Yangrui Yang	2018	KES Journal	10.3233/KES-180391	computer science;generative grammar;machine learning;artificial intelligence	Logic	-30.020199938025893	-80.09288573120428	153203
4df1f1390695e0ea27ce6ff70f5df1a248531db7	peeking through the language barrier: the development of a free/open-source gisting system for basque to english based on apertium.org	lengua vasca;computacion informatica;traduccion automatica;basque;lengua inglesa;filologias;assimilation;info eu repo semantics article;informacion documentacion;codigo abierto;linguistica;ciencias basicas y experimentales;asimilacion;software libre;evaluation;english;evaluacion;gisting;grupo a;ciencias sociales;grupo b;machine translation;free software;open source	The article describes the development of a machine translation system from Basque to English designed for assimilation (gisting) built on the free/opensource rule-based machine translation platform Apertium, and evaluates it preliminarly using a new method based in Cloze tests in which readers are asked to fill out gaps in a reference translation. The results indicate that the availability of the raw translations by a system with a dictionary of about 10,000 entries and about 300 translation rules increase significantly the ability of readers to complete the tests successfully.	apertium;data assimilation;dictionary;logic programming;open-source software;rule-based machine translation	Jim O'Regan;Mikel L. Forcada	2013	Procesamiento del Lenguaje Natural		computer-assisted translation;speech recognition;assimilation;example-based machine translation;computer science;evaluation;english;linguistics;machine translation;rule-based machine translation;world wide web	NLP	-27.858874279747376	-77.95761335260748	153362
36cfd35e55c4b89d9175ba7585af625a5df2b5c2	a heuristic approach to english-into-japanese machine translation	data representation;point of view;machine translation	Practical machine translation must be considered from a heuristic point of view rather than from a purely rigid analytical l inguistic method. An English-into-Japanese translation system named ATHENE based on a Heuristic Parsing Model (HPM) has been developed. The experiment shows some advantageous points such as simplification of transforming and generating phase, semilocalization of multiple meaning resolution, and extendability for future grammatical refinement. HPM-base parsing process, parsed tree, grammatical data representation, and translation results are also described.	data (computing);extensibility;heuristic;machine translation;parsing;refinement (computing);text simplification	Yoshihiko Nitta;Atsushi Okajima;Fumiyuki Yamano;Koichiro Ishihara	1982		10.3115/991813.991858	dynamic and formal equivalence;natural language processing;synchronous context-free grammar;transfer-based machine translation;example-based machine translation;computer science;theoretical computer science;external data representation;linguistics;machine translation;rule-based machine translation;algorithm	NLP	-28.39881988645032	-80.09846092439942	153525
2ad931eded21ab5aef585dd377a4fafcc6acf9ad	authorship verification with entity coherence and other rich linguistic features notebook for pan at clef 2013.		We adopt Koppel et al.’s unmasking approach [5] as the major framework of our authorship verification system. We enrich Koppel et al.’s original word frequency features with a novel set of coherence features, derived from our earlier work [2], together with a full set of stylometric features. For texts written in languages other than English, some stylometric features are unavailable due to the lack of appropriate NLP tools, and their coherence features are derived from their translations produced by Google Translate service. Evaluated on the training corpus, we achieve an overall accuracy of 65.7%: 100.0% for both English and Spanish texts, while only 40% for Greek texts; evaluated on the test corpus, we achieve an overall accuracy of 68.2%, and roughly the same performance across three languages.	google translate;natural language processing;stylometry;text corpus;word lists by frequency	Vanessa Wei Feng;Graeme Hirst	2013			natural language processing;computer science;communication;information retrieval	NLP	-28.81696669758663	-74.25269342894853	153700
97ef1f6e8ba6e81b9bef89a7b5e9bfe9e7d5198b	building a standard dataset for arabie sentiment analysis: identifying potential annotation pitfalls		Sentiment Analysis (SA) is one of the hottest research fields nowadays. It is concerned with identifying the sentiment conveyed in a piece of text. The current efforts in SA require the existence of standard datasets for training/testing purposes. Such datasets already exist for some languages such as English. Unfortunately, the same cannot be said about other languages such as Arabic. Currently existing Arabic SA datasets are restricted (in their domain, size, dialects covered, etc.) and/or have limited availability. Moreover, the annotation process did not receive the proper attention it deserves. Some of the existing datasets relied on the author's point of view for annotation, while others employed annotators, but did not take into account the personal variations between the annotators and how would that affect their agreement. This study presents our efforts to build a standard Arabic dataset with the above concerns in mind. The constructed dataset is intended for generic use as it contains reviews from different domains written in Modern Standard Arabic (MSA) as well as several dialects. As for the annotation process, it is given high attention by studying the inter-annotator agreements and investigating the potential factors affecting them.	limited availability;mind;sentiment analysis;text corpus	Mohammed Al-Kabi;Areej A. Al-Qwaqenah;Amal H. Gigieh;Kholoud Alsmearat;Mahmoud Al-Ayyoub;Izzat Alsmadi	2016	2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2016.7945822	modern standard arabic;sentiment analysis;data mining;user-generated content;arabic;computer science;annotation	NLP	-29.4475053249396	-73.32961700394038	153733
4b670f7cf0d3db76f64156b8b4a0e87c18713cf9	coreference resolution in dialogues in english and portuguese	probabilistic model;direct observation;antecedent-likelihood theory;statistical procedure;coreference resolution;decision tree;automatic annotation;coreference case;dialogue corpus;aggregate combination;four-attribute annotation;model building	This paper introduces a methodology to analyse and resolve cases of coreference in dialogues in English and Portuguese. A four-attribute annotation to analyse cases of anaphora was used to analyse a sample of around three thousand cases in each language collected in dialogue corpora. The information thus gathered was analysed by means of exploratory and model-building statistical procedures. A probabilistic model was then built on the basis of aggregate combinations of categories across the four attributes. This model, in combination with direct observation of cases, was used to build an antecedentqikelihood theory, which is at present being organised as a decision tree for the purpose of testing with a view for automatic annotation and subsequent resolution of coreference cases in dialogues in both languages. It is thought that the findings could be extended to Spanish, Italian and possibly French.	aggregate data;anaphora (linguistics);decision tree;statistical model;text corpus	Marco Rocha	1999			natural language processing;speech recognition;computer science;data mining	NLP	-28.147455524692372	-73.44065777644437	154484
b19f86e1fa7a46fa01674bebc6e6e17e7d056887	using sgml as a basis for data-intensive nlp	corpus annotation;traitement automatique des langues naturelles;computacion informatica;logiciel;humanidades;psicologia y educacion;filologias;tei;database;structure de documents;humanidades generalidades;sgml;linguistique de corpus;annotation de corpus;etude comparative;didacticas aplicadas;ciencias basicas y experimentales;comparative study;base de donnees;corpus linguistics;filologias generalidades;computational linguistics;linguistique informatique;grupo b;natural language processing	This paper describes the lt nsl system (McKelvie et al, 1996), an architecture for writing corpus processing tools. This system is then compared with two other systems which address similar issues, the GATE system (Cunningham et al, 1995) and the IMS Corpus Workbench (Christ, 1994). In particular we address the advantages and disadvantages of an sgml approach compared with a non-sgml database approach.	gate;natural language processing;standard generalized markup language;workbench	David McKelvie;Chris Brew;Henry S. Thompson	1997	Computers and the Humanities	10.1023/A:1001053128638	natural language processing;speech recognition;computer science;computational linguistics;comparative research;corpus linguistics;linguistics;sgml	NLP	-30.915075240948035	-78.02923170386451	154668
1a62d25325ff528b929a988cde2efe5a0e6f213b	abstracting a dialog act tagset for meeting processing		This paper analyses three existing tagsets for dialogue acts, i.e., the function of utterances in dialogue. Then, a new tagset is proposed, named MALTUS, designed for the annotation of meeting recording transcripts. Several criteria for tagset definition are discussed, along with the possible theoretical inspiration for dialogue act tagsets. The DAMSL, SWBD-DAMSL, and ICSI-MR tagsets are analyzed with respect to the previous considerations. The definition of MALTUS is followed by quantitative data from the conversion and validation of ICSI-MR data, and then by perspectives on automatic tagging using MALTUS, and on further user-based studies of its relevance. POPESCU-BELIS, Andréi. Abstracting a Dialogue Act Tagset forMeeting Processing. In: Maria Teresa Lino, Maria Francisca Xavier, Fátima Ferreira, Rute Costa and Raquel Silva. LREC 2004 (Fourth International Conference on Language Resources and Evaluation). ELRA European Language Ressources Association, 2004. p. 1415-1418	switzerland;dialog	Andrei Popescu-Belis	2004			natural language processing;artificial intelligence;dialog act;information retrieval;computer science	NLP	-32.855315324639015	-76.41497558341423	154726
5d6e79d99fbe9832e2b33505a92bf1f2ce99e39a	adapting verbnet to french using existing resources		VerbNet is an English lexical resource for verbs that has proven useful for English NLP due to its high coverage and coherent classification. Such a resource doesn’t exist for other languages, despite some (mostly automatic and unsupervised) attempts. We show how to semi-automatically adapt VerbNet using existing resources designed for different purposes. This study focuses on French and uses two French resources: a semantic lexicon (Les Verbes Français) and a syntactic lexicon (Lexique-Grammaire).	coherence (physics);lexicon;natural language processing;semiconductor industry;unsupervised learning;verbnet	Quentin Pradet;Laurence Danlos;Gaël de Chalendar	2014			semantic lexicon;syntax;artificial intelligence;verbnet;natural language processing;french;computer science;lexicon	NLP	-27.90950909571812	-76.26665824939104	155142
7e0444866e49b7b360b0c5ac2783535052e27a43	linguistically motivated evaluation of english-latvian statistical machine translation			statistical machine translation	Inguna Skadina;Kristine Levane-Petrova;Guna Rabante	2012		10.3233/978-1-61499-133-5-221	machine translation;machine translation software usability;natural language processing;speech recognition;latvian;example-based machine translation;artificial intelligence;computer science	NLP	-27.794458428441306	-79.08397943420017	155681
729da2e6ba1493b965de3cecdaf579eee0a1874b	simple negation scope resolution through deep parsing: a semantic solution to a semantic problem		In this work, we revisit Shared Task 1 from the 2012 *SEM Conference: the automated analysis of negation. Unlike the vast majority of participating systems in 2012, our approach works over explicit and formal representations of propositional semantics, i.e. derives the notion of negation scope assumed in this task from the structure of logical-form meaning representations. We relate the task-specific interpretation of (negation) scope to the concept of (quantifier and operator) scope in mainstream underspecified semantics. With reference to an explicit encoding of semantic predicate-argument structure, we can operationalize the annotation decisions made for the 2012 *SEM task, and demonstrate how a comparatively simple system for negation scope resolution can be built from an off-the-shelf deep parsing system. In a system combination setting, our approach improves over the best published results on this task to date.	align (company);complementarity theory;content adaptation;denotational semantics;downstream (software development);embedded system;emoticon;error analysis (mathematics);general-purpose markup language;microsoft outlook for mac;minimal recursion semantics;natural language understanding;parse tree;parsing;quantifier (logic);scope resolution operator;shallow parsing;treebank;web crawler	Woodley Packard;Emily M. Bender;Jonathon Read;Stephan Oepen;Rebecca Dridan	2014			natural language processing;computer science;negation as failure;theoretical computer science;linguistics;algorithm	NLP	-29.427132495045466	-76.03820793752355	156246
2d113bbeb6f2f393a09a82bc05fdff61b391d05d	a rule based approach to discourse parsing	universiteitsbibliotheek	In this paper we present an overview of recent developments in discourse theory and parsing under the Linguistic Discourse Model (LDM) framework, a semantic theory of discourse structure. We give a novel approach to the problem of discourse segmentation based on discourse semantics and sketch a limited but robust approach to symbolic discourse parsing based on syntactic, semantic and lexical rules. To demonstrate the utility of the system in a real application, we briefly describe the architecture of the PALSUMM system, a symbolic summarization system being developed at FX Palo Alto Laboratory that uses discourse structures constructed using the theory outlined to summarize written English prose texts. 1	diagram;knuth–morris–pratt algorithm;palo;parsing;random-access memory	Livia Polanyi;Chris Culy;Martin van den Berg;Gian Lorenzo Thione;David D. Ahn	2004			natural language processing;parser combinator;bottom-up parsing;s-attributed grammar;linguistics;top-down parsing	NLP	-31.996592554413144	-76.30004909688337	156294
8f3562f72df06edc53476ab82e3b09bb3536b023	italian clitic patterns in pregroup grammar: state of the art		Pregroup calculus and pregroup grammars are introduced by Lambek as an algebraic tool for the grammatical analysis of natural languages and the computation of strings of words and sentences. Some interesting aspects of natural languages have been profitably handled by means of pregroups. In the present paper we focus on a chosen aspect of Italian grammar - clitic pronouns - and show how to tackle it by means of different types of pregroup grammars. We start with classical pregroup grammars, proceed to product pregroup grammars and then introduce tupled pregroup grammars. Advantages and disadvantages of the different approaches are discussed and compared.	pregroup grammar	Claudia Casadio;Aleksandra Kislak-Malinowska	2014		10.1007/978-3-642-54789-8_9	natural language processing;linguistics	NLP	-28.936433064739877	-77.38104360249385	156731
c0d97f507dcfa718bba2a0a11216043f4442f500	anaphora resolution for biomedical literature by exploiting multiple resources	lenguaje natural;linguistique;genie biomedical;lenguaje uml;speech processing;langage naturel;tratamiento palabra;anaphora resolution;traitement parole;semantics;langage modelisation unifie;algoritmo genetico;semantica;semantique;analyse syntaxique;linguistica;biomedical engineering;analisis sintaxico;anafora;syntactic analysis;natural language;unified modelling language;proceedings paper;algorithme genetique;genetic algorithm;feature selection;ingenieria biomedica;anaphor;anaphore;article;semantic association;linguistics	In this paper, a resolution system is presented to tackle nominal and pronominal anaphora in biomedical literature by using rich set of syntactic and semantic features. Unlike previous researches, the verification of semantic association between anaphors and their antecedents is facilitated by exploiting more outer resources, including UMLS, WordNet, GENIA Corpus 3.02p and PubMed. Moreover, the resolution is implemented with a genetic algorithm on its feature selection. Experimental results on different biomedical corpora showed that such approach could achieve promising results on resolving the two common types of anaphora.	anaphora (linguistics);feature selection;genetic algorithm;pubmed;text corpus;thesaurus;wordnet	Tyne Liang;Yu-Hsiang Lin	2005		10.1007/11562214_65	natural language processing;unified modeling language;speech recognition;genetic algorithm;computer science;parsing;speech processing;semantics;linguistics;natural language;feature selection	NLP	-27.358429931838874	-77.50863047625366	156751
291002a7e317732641c96b3459fc8da0ded35550	zombilingo: eating heads to perform dependency syntax annotation (zombilingo : manger des têtes pour annoter en syntaxe de dépendances) [in french]		This paper presents ZOMBILINGO, a Game With A Purpose (GWAP) that allows for the dependency syntax annotation of French corpora. The created resource is freely available on the game Web site. Mots-clés : jeux ayant un but, complexité, annotation, syntaxe en dépendances.	human-based computation game;meaning–text theory;text corpus	Karën Fort;Bruno Guillaume;Valentin Stern	2014				NLP	-31.225847126690866	-77.83950212913076	156763
ce216a6b027285ed65ea15d2e6f03f4a27c11fb5	initial readability assessment of clinical trial eligibility criteria		Various search engines are available to clinical trial seekers. However, it remains unknown how comprehensible clinical trial eligibility criteria used for recruitment are to a lay audience. This study initially investigated this problem. Readability of eligibility criteria was assessed according to (i) shallow and lexical characteristics through the use of an established, generic readability metric; (ii) syntactic characteristics through natural language processing techniques; and (iii) health terminological characteristics through an automated comparison to technical and lay health texts. We further stratified clinical trials according to various study characteristics (e.g., source country or study type) to understand potential factors influencing readability. Mainly caused by frequent use of technical jargons, a college reading level was found to be necessary to understand eligibility criteria text, a level much higher than the average literacy level of the general American population. The use of technical jargons should be minimized to simplify eligibility criteria text.	auditory recruitment;classification;document completion status - documented;eligibility determination;in the beginning... was the command line;knowledge acquisition;natural language processing;nomenclature;population;text corpus;web search engine	Tian Kang;Noémie Elhadad;Chunhua Weng	2015	AMIA ... Annual Symposium proceedings. AMIA Symposium		medical education;readability;study type;eligibility determination;clinical trial;syntax;data science;literacy;population;comprehension;computer science	SE	-32.28723107284596	-73.72253945222859	157054
9ea948de6d7c2e0a1e0f34cda0a31e82b941dae0	evaluation of uryupina's coreference resolution features for polish				Bartlomiej Niton	2013		10.1007/978-3-319-43808-5_27		NLP	-30.60183052933058	-77.78420388140893	157119
334db7cbffc537bb4d9b25e0dd128e6434691af6	announcing prague czech-english dependency treebank 2.0		We introduce a substantial update of the Prague Czech-English Dependency Treebank, a parallel corpus manually annotated at the deep syntactic layer of linguistic representation. The English part consists of the Wall Street Journal (WSJ) section of the Penn Treebank. The Czech part was translated from the English source sentence by sentence. This paper gives a high level overview of the underlying linguistic theory (the so-called tectogrammatical annotation) with some details of the most important features like valency annotation, ellipsis reconstruction or coreference.	high-level programming language;parallel text;the wall street journal;treebank	Jan Hajic;Eva Hajicová;Jarmila Panevová;Petr Sgall;Ondrej Bojar;Silvie Cinková;Eva Fucíková;Marie Mikulová;Petr Pajas;Jan Popelka;Jirí Semecký;Jana Sindlerová;Jan Stepánek;Josef Toman;Zdenka Uresová;Zdenek Zabokrtský	2012			linguistics	NLP	-28.614090515068618	-78.88553762548399	157340
1619eb8e4dfc1187f1c5b254066a21ad0256b859	reversible letter-to-sound sound-to-letter generation based on parsing word morphology			mathematical morphology;parsing	Sheri Hunnicutt;Helen M. Meng;Stephanie Seneff;Victor Zue	1993			speech recognition;morphology (linguistics);natural language processing;parsing;computer science;artificial intelligence	NLP	-29.712176013671733	-78.94331353399514	157556
f8c5a9f6a05618f6cf6e92981fca3e8fe578c5d4	automatic keyword annotation system using newspapers	confabulation theory;keyword annotation;the nikkei			Tomoki Takada;Mizuki Arai;Tomohiro Takagi	2014	JACIII	10.20965/jaciii.2014.p0340	natural language processing;computer science;world wide web;information retrieval	NLP	-31.230096494424057	-76.52550683609871	158010
b4c7a796a638bc738b951a3867ca5b4ee895d120	the syntactic process: language, speech, and communication, mark steedman	mark steedman;syntactic process	mark steedman;syntactic process	mark steedman	Raffaella Bernardi	2004	Journal of Logic, Language and Information	10.1007/s10849-004-4005-6	natural language processing;speech recognition;linguistics	NLP	-29.589135228432898	-78.9521806865021	158515
6b7dbac967e09e8b5c7e726517127044c5ce5801	geographical entity annotated corpus of japanese microblogs	corpus annotation;location reference expressions;natural language processing;microblogs			Koji Matsuda;Akira Sasaki;Naoaki Okazaki;Kentaro Inui	2017	JIP	10.2197/ipsjjip.25.121	natural language processing;computer science;microblogging;world wide web;information retrieval	NLP	-31.27554461608232	-76.29511239512087	158587
0d80b96e57945b27a0083023c40182f975e3331f	encoding biomedical resources in tei: the case of the genia corpus		It is well known that standardising the annotation of language resources significantly raises their potential, as it enables re-use and spurs the development of common technologies. Despite the fact that increasingly complex linguistic information is being added to biomedical texts, no standard solutions have so far been proposed for their encoding. This paper describes a standardised XML tagset (DTD) for annotated biomedical corpora and other resources, which is based on the Text Encoding Initiative Guidelines P4, a general and parameterisable standard for encoding language resources. We ground the discussion in the encoding of the GENIA corpus, which currently contains 2,000 abstracts taken from the MEDLINE database, and has almost 100,000 hand-annotated terms marked for semantic class from the accompanying ontology. The paper introduces GENIA and TEI and implements a TEI parametrisation and conversion for the GENIA corpus. A number of aspects of biomedical language are discussed, such as complex tokenisation, prevalence of contractions and complex terms, and the linkage and encoding of ontologies.	blueprint;linkage (software);medline;markup language;ontology (information science);software documentation;text encoding initiative;text corpus;xml;xslt	Tomaz Erjavec;Jin-Dong Kim;Tomoko Ohta;Yuka Tateisi;Jun'ichi Tsujii	2003			natural language processing;speech recognition;computer science;linguistics;information retrieval	NLP	-32.29124130939952	-74.89254228618849	158611
530b83397b2befeff9ce5750ced194ccf88e360d	modeling consensus knowledge from multiple sources based on semantics of concepts				Seppo Puuronen;Vagan Y. Terziyan	1996			semantics;natural language processing;data mining;computer science;artificial intelligence	AI	-32.474144304959964	-77.9307975690338	158789
054ecc862a7860107a92956e9f004e20992196cd	a psycholinguistic model of first and second language learning.	second language learning			Tatiana Slama-Cazacu	1990			language pedagogy;natural language processing;universal networking language;language assessment;second-language acquisition;computer science;language transfer;developmental linguistics;linguistics;second-language attrition;comprehension approach	NLP	-31.734981683292123	-78.92497738092707	159234
b374ee703b9cb27234fab96d5b51b7fa90105aa4	correction to: pires, cavaco and vigário, towards the definition of linguistic metrics for evaluating text readability					2017	Journal of Quantitative Linguistics	10.1080/09296174.2017.1356018		NLP	-30.63309753730253	-77.43474090752495	159780
3a35784f6e413dc68cab29093edf962c85c5e7f7	evaluation in discourse: a corpus-based study		This paper describes the CASOAR corpus, the first manually annotated corpus exploring the impact of discourse structure on sentiment analysis with a study of movie reviews in French and in English as well as letters to the editor in French. While annotating opinions at the expression, sentence, or document level is a well-established task and relatively straightforward, discourse annotation remains difficult, especially for non experts. Therefore, combining opinion and discourse annotations pose several methodological problems that we address here. We propose a multi-layered annotation scheme that includes: the complete discourse structure according to the Segmented Discourse Representation Theory, the opinion orientation of elementary discourse units and opinion expressions, and their associated features (including polarity, strength, etc.). We detail each layer, explore the interactions between them, and discuss our results. In particular, we examine the correlation between discourse and semantic category of opinion expressions, the impact of discourse relations on both subjectivity and polarity analysis, and the impact of discourse on the determination of the overall opinion of a document. Our results demonstrate that discourse is an important cue for sentiment analysis, at least for the corpus genres we have studied.	interaction;sentiment analysis	Farah Benamara;Nicholas Asher;Yvette Yannick Mathieu;Vladimir Popescu;Baptiste Chardon	2016	D&D		natural language processing;computer science;linguistics;communication	NLP	-27.492041883366507	-75.06185580068787	160179
730c2dbc126fb2f47876e8a65d699e609aedd266	scaling up an mt prototype for industrial use - databases and data flow	sprakteknologi sprakvetenskaplig databehandling;language technology computational linguistics	In a cooperative project between Uppsala University, the bus and truck manufacturing company Scania CV AB, and the translation company Explicon AB, issues of scaling up the transfer-based machine translation prototype MULTRA for industrial use is beeing investigated. The project is limited to one domain, automotive service literature, and one translation direction, Swedish to English, but issues concerning the change of domain, translation direction and language pair are also considered. Three focal points of the project work have been the design and implementation of the new MATS system, including the redesign, porting and integration of MULTRA, the redesign and implementation of the dictionaries of the language modules as a lexical database, and the scaling up of the dictionaries and the grammars. The system is currently trained on a corpus of aligned bitexts from the automotive service domain. The coverage of the lexical data is almost complete, and validated by professional translators, but the grammars are still limited. Despite the incomplete state of the grammars, the system already translates more than a third of the segments in the corpus. Preliminary evaluations of system performance and coverage have been made, and further development of evaluation methods and metrics are in progress.	dataflow architecture;dictionary;focal (programming language);image scaling;lexical database;prototype;resource bounded measure;text corpus;transfer-based machine translation;usability	Anna Sågvall Hein;Eva Forsbom;Jörg Tiedemann;Per Weijnitz;Ingrid Almqvist;Leif-Jöran Olsson;Sten Thaning	2002			natural language processing;computer science;data science	NLP	-33.53959220965589	-77.1332544750315	160260
d758ee7ac28a95794c2c6e79076c00ee3831fd45	analysis and reference resolution of bridge anaphora across different text genres	dutch;social sciences;cross genre;bridging;word net;coreference resolution	We discuss bridge relations in Dutch between two textual referents across six di↵erent text genres. After briefly presenting the annotation guidelines and inter-annotation agreement results, we conduct an in-depth manual analysis of the di↵erent types of bridge relations found in our data sets. This analysis reveals that for all genres bridging references stand mostly in a class relationship, which is exactly the kind of information represented in a WordNet hierarchy. This inspired us to investigate to what extent a standard coreference resolution system for Dutch is capable of resolving bridge relations across di↵erent text genres and study the e↵ect of adding semantic features encoding WordNet information. Our results reveal modest improvements when using Dutch WordNet LCS information for all but one genre.	anaphora (linguistics);bridging (networking);computer bridge;error analysis (mathematics);inter-rater reliability;spatial variability;vagueness;wordnet	Iris Hendrickx;Orphée De Clercq;Véronique Hoste	2011		10.1007/978-3-642-25917-3_1	natural language processing;wordnet;bridging;computer science;linguistics	NLP	-27.587726425495813	-74.65169670276103	160320
c1e9ba70f0994c96ae97335baeb753ad1b5b0daa	correction to: revisiting distinctive phonetic features from applied computing perspective: unifying views and analyzing modern arabic speech varieties				Yasser Seddiq;Yousef Ajami Alotaibi;Ali H. Meftah;Sid-Ahmed Selouani;Mansour Al-Ghamdi	2018	I. J. Speech Technology	10.1007/s10772-018-9553-2		NLP	-30.848963789762895	-76.28041406516212	160424
b7f9bd260d3ff557f4169eeffe8dfe5400538e52	toward a lightweight solution for less-resourced languages: creating a pos tagger for alsatian using voluntary crowdsourcing		We present here the results of an experiment aiming at crowdsourcing part-of-speech annotations for a less-resourced French regional language, Alsatian. We used for this purpose a specifically-developed slightly gamified platform, Bisame. It allowed us to gather annotations on a variety of corpora covering some of the language dialectal variations. The quality of the annotations, which reach an averaged F-measure of 93%, enabled us to train a first tagger for Alsatian that is nearly 84% accurate. The platform as well as the produced annotations and tagger are all freely available. The platform can easily be adapted to other languages, thus providing a solution to (some of) the less-resourced languages issue.	brill tagger;crowdsourcing;gamification;part-of-speech tagging;text corpus	Alice Millour;Karën Fort	2018			artificial intelligence;natural language processing;computer science;regional language;crowdsourcing	NLP	-29.51595664778595	-74.20774488959583	160921
d89af1c0c8df4514b72bb8d6114f667033d530a4	a dataset for arabic textual entailment		There are fewer resources for textual entailment (TE) for Arabic than for other languages, and the manpower for constructing such a resource is hard to come by. We describe here a semi-automatic technique for creating a first dataset for TE systems for Arabic using an extension of the ‘headline-lead paragraph’ technique. We also sketch the difficulties inherent in volunteer annotators-based judgment, and describe a regime to ameliorate some of these.	semiconductor industry;test engineer;textual entailment	Maytham Alabbas	2013			natural language processing;speech recognition;textual entailment;computer science;linguistics	NLP	-30.080874603396722	-73.86211692339786	161149
799a907df10b40d1e733e66f9307d00975ca9907	valency and case in computational linguistics	dependency grammar;computational linguistic;natural language processing	Based on the title, you might buy this book to find out how valency and case are used in computational linguistics. If you do, you will get much more than you expected. The title could also lead you to pass up the book if you are interested in case or valency but have no interest in computational linguistics. In that event, you will be missing out. Despite its title, this is a book which is preponderantly about case and valence (73 percent by volume) and less about the applications to natural language processing. The computational part is good and valuable, but the tirst section on case and valency can stand alone, and is worth the purchase price by itself (to the extent that this is true of any academic book these days.) The author’s statement that “For the Theoretical Linguist, here is, for the first time, an extensive survey of the field” @ix) is no hollow claim, though the “new version of Case” which he announces does not on inspection seem all that new. I find the coverage in this volume very impressive. The author is very knowledgeable in the areas of case grammar, valency grammar, dependency grammar and natural language processing (NLP). He has a surprisingly good command of the literature in all of these areas, including original sources written in French, German and Swedish as well as English. I marvel at how he was able to do all that reading and digesting and integrating. In pulling together a lot of threads from various traditions and pointing out parallels and divergences, he has performed a real service for case grammarians and computational linguists alike. Especially valuable I think are the section on locus and time (pp. 148-164) and the chapter on verb features, in which he recognizes and pinpoints the importance of Chafe’s contribution to the field of case grammar and compares it with important related studies by Longacre and linguistic philosophers such as Vendler and Dowty. It is	computational linguistics	Stanley Starosta	1990	Machine Translation	10.1007/BF00310043	natural language processing;subcategorization;generative grammar;link grammar;formal semantics;deep linguistic processing;quantitative linguistics;computer science;computational linguistics;linguistics;relational grammar;word grammar;language technology;stochastic context-free grammar;mildly context-sensitive grammar formalism;computational semantics;dependency grammar	NLP	-31.29611196679231	-74.07477628046335	161651
7ed8b27291103b782fb735a685335f2d4a39931f	word association profiles and their use for automated scoring of essays		We describe a new representation of the content vocabulary of a text we call word association profile that captures the proportions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the distirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writing and word association profiles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays.	automated essay scoring;limbo;microsoft word for mac;vocabulary	Beata Beigman Klebanov;Michael Flor	2013			natural language processing;speech recognition;computer science;artificial intelligence	NLP	-27.11836173409682	-74.1341304967065	161736
8d11ff9ec9400160851440f1b502908f912009ce	pocos - potsdam coreference scheme		This document outlines minimal design principles underlying annotation of coreference relations in PoCoS, a scheme for cross-linguistic anaphoric annotation. We identify language-independent principles for markable identification which are essential for comparability of annotations produced for different languages. We further suggest a clear and motivated structure of annotation stages, the separation of a coarse-grained core and a family of more elaborate extended schemes, and strategies for the systematic treatment of ambiguity. Explicit mark-up of ambiguities is a novel feature. We implemented three instantiations of PoCoS for German, English and Russian applied to corpora of newspaper texts.	anaphora (linguistics);language-independent specification;parallel text;software portability;text corpus	Olga Krasavina;Christian Chiarcos	2007			natural language processing;computer science;data mining;information retrieval	NLP	-29.42834250030491	-75.23030965727772	162327
c8cc2de136f1201c6c5d914df996d425246599ac	an improved automated definition extraction method based on lexicographic and lexico-semantic features		This paper shows that results obtained from extracting definitions in computational lexicography, often have a high recall but a low precision. Herein, we present an improved, automated, rule-based analytical definitions extraction method that uses hypernym identification. This kind of definitions allow us to improve the stateof-the-art precision reported in definitions extracting. Furthermore, this method incorporates a hypernyms extraction module, which has proven to be a necessary first step for generating automated definitions.		Alejandro Pimentel;Gerardo Sierra;Claudio Molina	2016	2016 Fifteenth Mexican International Conference on Artificial Intelligence (MICAI)	10.1109/MICAI-2016.2016.00013		SE	-26.70508891967089	-75.38767204848426	162838
713d36bd26e99b19b32fe3d076289859552bc54e	information content in textual data: revisited for arabic text	information content		text corpus	Nadia H. Hegazi;Nabil Ali;Ehsan Abed	1987	JASIS	10.1002/(SICI)1097-4571(198703)38:2%3C133::AID-ASI8%3E3.0.CO;2-P	natural language processing;self-information;computer science;information retrieval	NLP	-31.09448514195967	-76.86849784021122	164248
0c65dd17855baaea2ccba54999f0ec69cf7eb415	a punjabi to hindi machine translation system	semantic similarity;transliteration;punjabi;word accuracy rate;hindi;rule basedapproach;soundex approach;machine translation	Punjabi and Hindi are two closely related languages as both originated from the same origin and having lot of syntactic and semantic similarities. These similarities make direct translation methodology an obvious choice for Punjabi-Hindi language pair. The purposed system for Punjabi to Hindi translation has been implemented with various research techniques based on Direct MT architecture and language corpus. The output is evaluated by already prescribed methods in order to get the suitability of the system for the Punjabi Hindi language pair.	same-origin policy;subject matter expert turing test;text corpus	Gurpreet Singh Josan;Gurpreet Singh Lehal	2008		10.30019/IJCLCLP.201006.0001	natural language processing;semantic similarity;speech recognition;hindi;transliteration;computer science;linguistics;machine translation	NLP	-28.493791030940827	-74.022543988613	164268
4794296b006ff6ddcc673a2873f6105a5277600c	semantic tree unification grammar: a new formalism for spoken language processing		In this paper we present the Semantic Tree Unification Grammar (STUG) which is a new formalism for parsing spoken language. The main motivation of this formalism is the combination of the robustness and simplicity of the classical semantic grammar to the deepness of the traditional syntactic formalisms. The key properties of STUG are: the direct linearization of the semantic structure, an economical feature structure and the simplicity of the grammar writing and modification. STUG was implemented in the OASIS system which is a system of partial parsing of spontaneous spoken language. The results of our evaluation are encouraging.	formal grammar;parsing;semantics (computer science);spontaneous order;unification (computer science)	Mohamed-Zakaria Kurdi	2000			semantic computing;spoken language;natural language processing;linguistics;natural language;computer science;formalism (philosophy);grammar;unification;artificial intelligence	NLP	-29.96325307425467	-79.63613941652342	164804
329b13de0108eb0e0f44fb137e0574ccfba9a598	formal processes of timbre composition challenging the dualistic paradigm of computer music.			paradigm	Agostino Di Scipio	1994			humanities;psychology;linguistics;communication	ML	-31.65230470299045	-79.45260045219871	164900
8b39946263d581b713cba65db8f23addba93c990	complete complimentary results report of the marf's nlp approach to the deft 2010 competition		This companion paper complements the main DEFT’10 article [15] describing the MARF approach to the DEFT’10 NLP challenge. This paper is aimed to present the complete result sets of all the conducted experiments and their settings in the resulting tables highlighting the approach and the best results, but also showing the worse and the worst and their subsequent analysis. This particular work focuses on application of the MARF’s classical and NLP pipelines to identification tasks within various francophone corpora to identify decades when certain articles were published for the first track (Piste 1) and place of origin of a publication (Piste 2), such as the journal and location (France vs. Quebec). This is the forth iteration of the release of the results.	experiment;iteration;modular audio recognition framework (marf);natural language processing;pipeline (computing);table (database);text corpus	Serguei A. Mokhov	2010	CoRR		natural language processing;artificial intelligence;computer science	NLP	-31.058302786458277	-73.24796317172502	165284
308b421170b840b3fef7fe2d4788a79ee8b52795	integration of speech recognition and language processing in spoken language translation system (sl-trans).	spoken language translation;language processing;speech recognition			Tsuyoshi Morimoto;Kiyohiro Shikano;Hitoshi Iida;Akira Kurematsu	1990			natural language processing;language identification;cued speech;universal networking language;speech;computational linguistics;language transfer;machine translation;natural language;language technology	NLP	-29.703019837307238	-79.42491535439659	165312
d5b9852e2b4a750eb5f12f2cf884e4d4d5e5b853	a novel model for word sense disambiguation			word sense;word-sense disambiguation	Armin Shams Baragh	2010				NLP	-30.51376367152414	-77.14391105704888	165323
0ebaeab3681a02d2ac08c32c8d03848e40e8a5bd	extracting nested collocations	collocations unextracted;increasing number;interrupted collocation;semi-automatic extraction;large textual corpus;various approach;longer collocation;collocation extraction;nested collocation;particular attention	'l?his paper 1)rovidcs an at)l)roa(:h to tim semi-aul;onmtic exl;i'action of (:ollocaIJons f lom eorl)ora using sl;atisti(:s. The growing availability of lm'ge textual cort)ora, and the in(:reasing number of applications of colloeal;ion extra(:tion, has given risc~ 1;o wu.ious apt)roaches on the I;opi(:. In l;his palter, we address the probl(;m of 'ne,stcd collocrd, ions; thai, is, those being l)art of longer colloc;ttions. Most approa(:hes till now, tl'(!al;ed subst;rings of collo(:at;ions as eollocal;ions, only if they apl)eared ffequenl;ly enough 1)y l;hemselves in the cor[)llS. 'Fhese techniques le['l; ~r lot; of collocations mmxl;ra(:l;ed, in this 1)ai)er, we i)rol)oSe an algoril;hln for a semi-aul;oma|;ic exl;ra(;l;ion of nesl;ed uninl;errupl;ed anti inl;errul)l;ed collo(:al;iolls, paying parl;icular al;l;(~lll;ion to nested collocat;ion.	algorithm;automation;collocation;impredicativity;interrupt;learning object metadata;semiconductor industry;terminology extraction	Katerina T. Frantzi;Sophia Ananiadou	1996			natural language processing;linguistics	DB	-30.610339513604277	-74.04785937990279	165481
620124f06dce0544045602e10fa90159c362c61c	漢語語料庫與語法資訊 (chinese corpus and grammatical information) [in chinese]				Ting-Chi Tang	1994				NLP	-30.614470401789884	-78.0270105588439	165533
d72c4ee8ca7cb121091300be670dfa06e3a735cd	compound temporal adverbs in portuguese and in spanish	bilingualism;traductor;maquina estado finito;traducteur;bilinguisme;bilinguismo;spanish;portugues;translator;espagnol;adverbe temps;machine etat fini;finite state transducer;portuguese;finite state machine;portugais;espanol	This paper reports on an ongoing research on temporal adverbs and deals with the problem of processing a family of Portuguese and Spanish compound temporal adverbs, in a contrastive approach, aiming at building finite state transducers to translate them from one language into the other. Because of the large number of combinations involved and their complexity, it is not easy to list them in full. However, their modularity and relative independence from the surrounding sentence make them especially apt for a formal description using a finite state approach.	finite-state transducer	Jorge Baptista;Dolors Català Guitart	2002		10.1007/3-540-45433-0_20	finite state transducer;speech recognition;computer science;finite-state machine;algorithm;spanish;portuguese	NLP	-28.19406672406759	-77.63874087690981	165564
978d26720f703507b6bbcc1db3c6c7774c188205	unsupervised morphological analysis by formal analogy	language use;morphological analysis;information theory	While classical approaches to unsupervised morphology acquisition often rely on metrics based on information theory for identifying morphemes, we describe a novel approach relying on the notion of formal analogy. A formal analogy is a relation between four forms, such as: reader is to doer as reading is to doing. Our assumption is that formal analogies identify pairs of morphologically related words. We first describe an approach which simply identifies all the formal analogies involving words in a lexicon. Despite its promising results, this approach is computationally too expensive. Therefore, we designed a more practical system which learns morphological structures using only a (small) subset of all formal analogies. We tested those two approaches on the five languages used in Morpho Challenge2009.	information theory;lexicon;mathematical morphology;unsupervised learning	Jean-François Lavallée;Philippe Langlais	2009		10.1007/978-3-642-15754-7_74	natural language processing;formal system;information theory;morphological analysis;computer science;linguistics;algorithm	NLP	-26.43352527912546	-73.50242336157031	165587
52b72877f54480f1e5be01ea1438c56248ff28e0	standardization of the formal representation of lexical information for nlp	standardisation	"""1. Complexity of lexical structures and related domains Lexical databases play a central role in all natural language processing applications (Briscoe, 1991), ranging from simple spellcheckers to more complex machine translation systems. In most cases, they constitute the sole parameter information for the corresponding software, and apart from some vary basic methods such as stemming (Lovins, 1968; Frakes, 1992) relying on pure string processing principles and low linguistic requirements, hardly any language technology application can avoid relying on a minimal lexical resource. Even some basic tasks such as word segmentation for languages such as Japanese, Korean or Chinese (Halpern, 2008), in particular in the perspective of accurate named entity recognition, can hardly be carried out without large lexical resources. The same problem actually occurs for the proper identification of multi word units in """" easier """" languages as demonstrated in (Schone and Jurafsky, 2001). Such observations have lead ISO for instance to consider lexical representations as the main tenet of word segmentation processes (ISO/DIS 24614-1). As a result, the cost of development and maintenance of a language technology application highly correlates with the complexity and size of the corresponding lexical database. It is thus essential to be able to standardise the structures and formats of lexical data, taking into consideration that the actual complexity and coverage may deeply fluctuate from one application to another. As a matter of fact, lexical databases can cover many different levels, from simple morphosyntactic descriptions, like in the Multext framework (Ide and Véronis, 1994; Erjavec, 2004), up to multilevel lexical descriptions for machine translation (Lieske et alii, 2001). The degree of generalisation and factorisation in such lexica also impact on their reusability from one application to another and any standardisation effort related to lexical information should be able to cope with multiple types of combinations of linguistic description levels. Finally, it would be difficult to speak about NLP lexica without eliciting the possible relation that these may bear with more human oriented resources. In this respect, we should acknowledge that machine readable dictionaries as well as terminological databases, even if conceived to fulfil other types of requirements, should not be seen"""	dictionary;human-readable medium;language technology;lexical database;lexicon;machine translation;monoid factorisation;named-entity recognition;natural language processing;requirement;stemming;string (computer science);text segmentation	Laurent Romary	2009	CoRR		natural language processing;computer science;linguistics;information retrieval;standardization	NLP	-29.673262513049682	-73.86428194929634	165601
4f0b6dad4d1ef0973b384b1e506ad21104cfb7b7	discriminative statistical approaches for multilingual speech understanding (approches statistiques discriminantes pour l'interprétation sémantique multilingue de la parole) [in french]		RÉSUMÉ Les approches statistiques sont maintenant très répandues dans les différentes applications du traitement automatique de la langue et le choix d'une approche particulière dépend généralement de la tâche visée. Dans le cadre de l'interprétation sémantique multilingue, cet article présente une comparaison entre les méthodes utilisées pour la traduction automatique et celles utilisées pour la compréhension de la parole. Cette comparaison permet de proposer une approche unifiée afin de réaliser un décodage conjoint qui à la fois traduit une phrase et lui attribue ses étiquettes sémantiques. Ce décodage est obtenu par une approche à base de transducteurs à états finis qui permet de composer un graphe de traduction avec un graphe de compréhension. Cette représentation peut être généralisée pour permettre des transmissions d'informations riches entre les composants d'un système d'interaction vocale homme-machine. ABSTRACT Discriminative statistical approaches for multilingual speech understanding Statistical approaches are now widespread in the various applications of natural language processing and the elicitation of an approach usually depends on the targeted task. This paper presents a comparison between the methods used for machine translation and speech understanding. This comparison allows to propose a unified approach to perform a joint decoding which translates a sentence and assign semantic tags to the translation at the same time. This decoding is achieved through a finite-state transducer approach which allows to compose a translation graph with an understanding graph. This representation can be generalized to allow the rich transmission of information between the components of a human-machine vocal interface.	bibliothèque des ecoles françaises d'athènes et de rome;conditional random field;council for educational technology;finite-state transducer;linear algebra;lo que tú quieras oír;machine translation;natural language processing;performance;programmation automatique des formules;speech recognition;theory of conjoint measurement	Bassam Jabaian;Fabrice Lefèvre;Laurent Besacier	2013				NLP	-27.300136646268125	-78.77505826620542	165774
41a96169ea8368473850c30b2429dedfaa365d6e	answer set programming in linguistics		This survey collects scientific works where answer set programming, a declarative knowledge representation and reasoning formalism, is applied to natural language processing and computational linguistics.	answer set programming;authentication;binary prefix;computational linguistics;knowledge representation and reasoning;natural language processing;recursion;scientific literature;semantics (computer science);spatial variability;stable model semantics	Peter Schüller	2018	KI - Künstliche Intelligenz	10.1007/s13218-018-0542-z	descriptive knowledge;natural language processing;computational linguistics;machine learning;answer set programming;artificial intelligence;formalism (philosophy);computer science	AI	-31.17321860653295	-78.14084527312242	165824
364d58b1ae282ade4e09cf12786b69da8b2768a2	generating referring expressions with a unification grammar	knowledge base	A simple formalism is proposed to represent the contexts in which pronouns, definite/indefinite descriptions, and ordinal descriptions (e.g. 'the second book') can be used, and the way in which these expressions change the context. It is shown that referring expressions can be generated by a unification grammar provided that some phrase-structure rules are specially tailored to express entities in the current knowledge base.	entity;knowledge base;ordinal data;phrase structure rules;semantics (computer science);unification (computer science)	Richard Power	1999		10.3115/977035.977038	natural language processing;knowledge base;computer science;linguistics;algorithm	NLP	-31.170027398005406	-79.83415519017248	165944
ca8460d62f3ba810d8086a41d79970b014cc70d2	levels of complexity in discourse for anaphora disambiguation and speech act interpretation	speech acts;speech;psychology;theory;comprehension	Abs t r ac t : This paper presents a discussion of means of describing the discourse and its components which makes speech act i n te rp re ta t i on and anaphora d isambiguat ion possible with minimal search of the knowledge in the database. A p o r t i o n of t h i s paper w i l l cons ider how a f r a m e s representat ion of sentences and common sense knowledge p rov ides a mechanism for represent ing the pos tu la ted discourse components. Finally some discussion of the use of the d i s c o u r s e model and of f rames in a d i s c o u r s e unders tand ing program for a personal assistant w i l l be presented.	anaphora (linguistics);commonsense knowledge (artificial intelligence);integrated development environment;linear algebra;word-sense disambiguation	C. Bullwinkle	1977			natural language processing;comprehension;computer science;speech;linguistics;theory	AI	-32.26426792608179	-79.71696965442328	166180
fa2f6ef053b1afd6d64a4effdd5a939bf79074cd	wide coverage symbolic surface realization	corpus-based system;nlg community;penn treebank;unseen sentence;symbolic surface realization;updated version;computational linguistics;symbolic surface realizer;similar statistics-based generator;wide coverage;recent evaluation technique;similar method	Recent evaluation techniques applied to corpusbased systems have been introduced that can predict quantitatively how well surface realizers will generate unseen sentences in isolation. We introduce a similar method for determining the coverage on the Fuf/Surge symbolic surface realizer, report that its coverage and accuracy on the Penn TreeBank is higher than that of a similar statistics-based generator, describe several bene ts that can be used in other areas of computational linguistics, and present an updated version of Surge for use in the NLG community.	ca-realizer;computation;computational linguistics;natural language generation;open road tolling;treebank	Charles Callaway	2004		10.3115/1219044.1219053	natural language processing;computer science;artificial intelligence;machine learning;algorithm	NLP	-28.17417642110011	-79.33664571727857	166271
d83413f7f785aae067d49332239c3a36c346ba99	estnltk - nlp toolkit for estonian		Although there are many tools for natural language processing tasks in Estonian, these tools are very loosely interoperable, and it is not easy to build practical applications on top of them. In this paper, we introduce a new Python library for natural language processing in Estonian, which provides a unified programming interface for various NLP components. The ESTNLTK toolkit provides utilities for basic NLP tasks including tokenization, morphological analysis, lemmatisation and named entity recognition as well as offers more advanced features such as a clause segmentation, temporal expression extraction and normalization, verb chain detection, Estonian Wordnet integration and rule-based information extraction. Accompanied by a detailed API documentation and comprehensive tutorials, ESTNLTK is suitable for a wide range of audience. We believe ESTNLTK is mature enough to be used for developing NLP-backed systems both in industry and research. ESTNLTK is freely available under the GNU GPL version 2+ license, which is standard for	application programming interface;documentation;gnu;information extraction;interoperability;lemmatisation;logic programming;named-entity recognition;natural language processing;python;tokenization (data security);wordnet	Siim Orasmaa;Timo Petmanson;Alexander Tkachenko;Sven Laur;Heiki-Jaan Kaalep	2016			artificial intelligence;speech recognition;natural language processing;computer science;estonian	NLP	-30.554080211263194	-74.99022441817597	166296
f9cea6b78ef0c88292c2566bb0da19a93fbd3909	the roles of l1 and markedness on mandarin l2ers' construction of syntactic representation			super robot monkey team hyperforce go!	Dong-Bo Hsu	2013			mandarin chinese;markedness;psychology;syntax;linguistics;natural language processing;artificial intelligence	NLP	-30.073271545156715	-79.2604213451866	167008
df21152b4a8e713bb60a9a3d1b635082420aef69	construcción rápida de un sistema de traducción automática español-portugués partiendo de un sistema español-catalán		This paper describes the rapid construction of a Spanish–Portuguese, Portuguese–Spanish machine translation system, starting from an existing system for the Spanish–Catalan pair, developed by the same research group. A team of four developers has produced in six months a useful system having a text coverage above 95 % and a word error rate around 10 % running at thousand of words a second. The process has partly been made easier, on the one hand, by the existence of a translation engine independent from the linguistic data and the avaliability of compilers to turn the linguistic data into the formats used by the engine, and, on the other hand, by the availability of morphological data for both languages.	compiler;machine translation;word error rate	Patricia Gilabert Zarco;Javier Herrero Vicente;Sergio Ortiz-Rojas;Antonio Pertusa Ibánez;Gema Ramírez-Sánchez;Felipe Sánchez-Martínez;Marcial Samper Asensio;Miriam A. Scalco;Mikel L. Forcada	2003	Procesamiento del Lenguaje Natural			NLP	-27.829982731354576	-78.42382891556537	167660
a86b0fe400d2e84625311f7fd3b76cf62ea3e537	emojitalianobot and emojiworldbot - new online tools and digital environments for translation into emoji		"""English. Emojitalianobot and EmojiWorldBot are two new online tools and digital environments for translation into emoji on Telegram, the popular instant messaging platform. Emojitalianobot is the first open and free Emoji-Italian and Emoji-English translation bot based on Unicode descriptions. The bot was designed to support the translation of Pinocchio into emoji carried out by the followers of the """"Scritture brevi"""" blog on Twitter and contains a glossary with all the uses of emojis in the translation of the famous Italian novel. EmojiWorldBot, an off-spring project of Emojitalianobot, is a multilingual dictionary that uses Emoji as a pivot language from dozens of different languages. Currently the emoji-word and word-emoji functions are available for 72 languages imported from the Unicode tables and provide users with an easy search capability to map words in each of these languages to emojis, and vice versa. This paper presents the projects, the background and the main characteristics of these applications. Italiano. Emojitalianobot e EmojiWorldBot sono due applicazioni online per la traduzione in e da emoji su Telegram, la popolare piattaforma di messaggistica istantanea. Emojitalianobot è il primo bot aperto e gratuito di traduzione che contiene i dizionari Emoji-Italiano ed Emoji-Inglese basati sule descrizioni Unicode. Il bot è stato ideato per coadiuvare la traduzione di Pinocchio in emoji su Twitter da parte dei follower del blog Scritture brevi e contiene pertanto anche il glossario con tutti gli usi degli emoji nella traduzione del celebre romanzo per ragazzi. EmojiWorldBot, epigono di Emojitalianobot, è un dizionario multilingue che usa gli emoji come lingua pivot tra dozzine di lingue differenti. Attualmente le funzioni emoji-parola e parola-emoji sono disponibili per 72 lingue importate dalle tabelle Unicode e forniscono agli utenti delle semplici funzioni di ricerca per trovare le corrispondenze in emoji delle parole e viceversa per ciascuna di queste lingue. Questo contributo presenta i progetti, il background e le principali caratteristiche di queste"""	bilingual dictionary;blog;emoji;glossary;instant messaging;linear algebra;naruto shippuden: clash of ninja revolution 3;primos;spring engine;unicode	Johanna Monti;Federico Sangati;Francesca Chiusaroli;Martin Benjamin;Sina Mansour	2016				NLP	-33.02194151781045	-76.68569045013068	168223
205e6ee9e42230492a2339490abfcccad5780863	a language model for human-machine dialog: the reversible semantic grammar.	language model	In this paper we present algorithms for analysis and generation in a human-machine dialog context. The originality of our approach is to base these two algorithms on the same knowledge. The latter combines both semantic and syntactic aspects. The algorithms are based on a double principle: the correspondence between offers and expectations, and the calculation of a heuristic score. We present also some results obtained by performing an evaluation based on the MEDIA French corpus.	algorithm;heuristic;language model;dialog	Jérôme Lehuen;Thierry Lemeunier	2010			grammar systems theory;natural language processing;generative grammar;semantic interpretation;traditional grammar;speech recognition;semantic web rule language;universal networking language;language primitive;object language;second-language acquisition;regular grammar;affix grammar;context-free language;emergent grammar;linguistics;natural language;attribute grammar;mildly context-sensitive grammar formalism;context-sensitive language	NLP	-29.564419518771892	-79.97823770868379	168292
e85a71c8cae795a1b2052a697d5e8182cc8c0655	the stanford corenlp natural language processing toolkit		We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis. This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology. We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.	natural language processing;open-source software;robustness (computer science);software design	Christopher D. Manning;Mihai Surdeanu;John Bauer;Jenny Rose Finkel;Steven Bethard;David McClosky	2014			human–computer interaction;computer science;database	NLP	-33.154294294568764	-73.59679050885022	168797
4befb1766eb8a6b1403c91c6c3473e97f693006d	annotating complex linguistic features in bilingual corpora: the case of multinot		In spite of the current need in the computational community for digital corpora in different languages with complex linguistic annotations going beyond morphosyntactic features, there is not much work within the Digital Humanities community dedicated to this task. In this paper I describe recent work on the development of a bilingual (English-Spanish) corpus consisting of original comparable and parallel texts from a variety of genres and annotated with complex linguistic features such as modality and evidentiality, metadiscourse markers, and thematisation, as carried out within the framework of the MULTINOT project (Lavid et al. 2015).	digital humanities;modality (human–computer interaction);text corpus	Julia Lavid	2017			linguistics;computer science	NLP	-30.52700131890149	-75.53758476959591	169059
1552de286cf6dc42a5c8333422ee293a4f8c8c19	realization of minimum discursive units segmentation of arab oral utterances		Unlike the written texts, discourse segmentation of the Arab oral dialogues is a challenging task that is held back in most cases by the spontaneous character of oral speech. Like any segmentation task, segmentation in minimum discursive units (UDM) aims to cut the different statements of a speech into simple proposals easily usable in subsequent treatment. The majority of the work on the Arabic language was based on extensive syntactic analysis approaches. In this article, we try to show the effectiveness of hybrid approaches combining linguistic and probabilistic processes over purely linguistic approaches. The performance of our segmentation was evaluated on a relatively large size corpus. We built this corpus by using the method of the wizard of Oz.	parsing;spontaneous order;stochastic context-free grammar;text corpus;wizard (software)	Chahira Lhioui;Anis Zouaghi;Mounir Zrigui	2016	Int. J. Comput. Linguistics Appl.		speech recognition;political science;segmentation	NLP	-26.82731671558612	-79.62868887052646	169106
c7eabbaed51b909b8a90cc7c7170c00f0b49044e	defining and representing preposition senses: a preliminary analysis	preposition sense;prepositions use;preliminary analysis;complex polysemous behavior	In this document, we analyze several aspects related to the semantics of prepositions. We propose an approach and elements of a method to organize and to represent prepositions uses and senses. Complex polysemous behaviors are discussed, showing the use and limits of the approach.	interface metaphor;lexicon	Emmanuelle Cannesson;Patrick Saint-Dizier	2002			natural language processing;computer science;linguistics	NLP	-32.905755951814214	-79.8366663535777	169497
3bf8c3cd633b607e660987328279f10e99f8f5b3	an appraisal of univauto – the first discovery program to generate a scientific article	linguistique;linguistic tool;nouveaute;information scientifique technique;novelty;novedad;instrumento linguistico;intelligence artificielle;inteligibilidad;linguistica;anglais;decouverte connaissance;artificial intelligence;descubrimiento conocimiento;english;scientific technical information;inteligencia artificial;informacion cientifica tecnica;intelligibilite;ingles;intelligibility;authoring tool;outil linguistique;knowledge discovery;linguistics	In a companion paper ([14]), I describe UNIVAUTO (UNIVersals AUthoring TOol), a linguistic discovery program that uncovers language universals and can write a report in English on its discoveries. In this contribution, the system is evaluated along a number of parameters that have been suggested in the literature as necessary ingredients of a successful discovery program. These parameters include the novelty, interestingness, plausibility and intelligibility of results, as well as the system’s portability and insightfulness.	biological anthropology;intelligibility (philosophy);logical possibility;natural language generation;natural language processing;plausibility structure;scientific literature;software portability	Vladimir Pericliev	2003		10.1007/978-3-540-39644-4_43	natural language processing;computer science;artificial intelligence;english;knowledge extraction;intelligibility	NLP	-33.36586933434839	-78.88531975798169	169846
2d4f5ae19128badcfbb2ffa822aa993e94f796bb	extracting an arabic lexicon from arabic newspaper text	thesaurus;article de presse;nom propre;traitement automatique des langues naturelles;computacion informatica;noun;analyseur morphologique;humanidades;psicologia y educacion;noun phrase;lexicon;morphological parser;filologias;linguistique appliquee;methode;structure textuelle;humanidades generalidades;proper noun;didacticas aplicadas;text structure;ciencias basicas y experimentales;part of speech;filologias generalidades;arabe;computational linguistics;partie du discours;newspaper article;linguistique informatique;method;grupo b;natural language processing;applied linguistics;lexique	We describe how to build a large comprehensive, integrated Arabic lexicon by automatic parsing of newspaper text. We have built a parser system to read Arabic newspaper articles, isolate the tokens from them, find the part of speech, and the features for each token. To achieve this goal we designed a set of algorithms, we generated several sets of rules, and we developed a set of techniques, and a set of components to carry out these techniques. As each sentence is processed, new words and features are added to the lexicon, so that it grows continuously as the system runs. To test the system we have used 100 articles (80,444 words) from the Al-Raya newspaper. The system consists of several modules: the tokenizer module to isolate the tokens, the type finder system to find the part of speech of each token, the proper noun phrase parser module to mark the proper nouns and to discover some information about them and the feature finder module to find the features of the words.	lexicon	Saleem Abuleil;Martha W. Evens	2002	Computers and the Humanities	10.1023/A:1014368121689	natural language processing;noun;noun phrase;method;speech recognition;part of speech;computer science;computational linguistics;applied linguistics;proper noun;linguistics	NLP	-27.51182988354629	-77.91650691630863	169907
5780592f81f4a65609039cbf7f84436ce12251af	morphological study of albanian words, and processing with nooj	dynamical processes;morphological analysis;natural language processing	We are developing electronic dictionaries and transducers for the automatic processing of the Albanian Language. We will analyze the words inside a linear segment of text. We will also study the relationship between units of sense and units of form. The composition of words takes different forms in Albanian. We have found that morphemes are frequently concatenated or simply juxtaposed or contracted. The inflected grammar of NooJ allows constructing the dictionaries of flexed forms (declensions or conjugations). The diversity of word structures requires tools to identify words created by simple concatenation, or to treat contractions. The morphological tools of NooJ allow us to create grammatical tools to represent and treat these phenomena. But certain problems exceed the morphological analysis and must be represented by syntactical grammars.	concatenation;contraction mapping;dictionary;field electron emission;nooj;pokémon y;transducer	Odile Piton;Klara Lagji	2007	CoRR		natural language processing;morphological analysis;computer science;linguistics	NLP	-28.23419701872356	-78.15685236858737	170226
a9c1192b517f9a1e2fabc8d6ed25cf3a4251f43d	pronunciation and spelling: the case of misspellings in swedish l2 written essays	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;computational linguistics;language technology computational linguistics	This research presents an investigation performed on the ASU corpus. We analyse to what extent does the pronunciation of intended words reflects in spelling errors done by L2 Swedish learners. We also propose a method that helps to automatically discriminate the misspellings affected by pronunciation from other types of misspellings.		Gintare Grigonyte;Björn Hammarberg	2014		10.3233/978-1-61499-442-8-95	natural language processing;clinical linguistics;contrastive linguistics;quantitative linguistics;applied linguistics;linguistics	NLP	-27.635420085496914	-76.32821448814587	170544
0adc6ed1a0c84d7dc4f00b573a1a0914fa3bd931	the university of groningen at qa@clef 2006: using syntactic knowledge for qa	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;computational linguistics;language technology computational linguistics	We describe our system for the monolingual Dutch and multilingual English to Dutch QA tasks. First, we give a brief outline of the architecture of our QA-system, which makes heavy use of syntactic information. Next, we describe the modules that were improved or developed esepcially for the CLEF tasks, i.e. (1) incorporation of syntactic knowledge in the IR-engine, (2) incorporation of lexical equivalences, (3) incorporation of coreference resolution for off-line answer extraction, (4) treatment of temporally restricted questions, (5) treatment of definition questions, and (6) a baseline multilingual (English to Dutch) QA system, which uses a combination of Systran and Wikipedia (for term recognition and translation) for question translation. For non-list questions, 31% of the highest ranked answers returned by the monolingual system were correct and 20% of the answers returned by the multilingual system.	baseline (configuration management);online and offline;systran;software quality assurance;terminology extraction;wikipedia	Gosse Bouma;Ismail Fahmi;Jori Mur;Gertjan van Noord;Lonneke van der Plas;Jörg Tiedemann	2006			natural language processing;speech recognition;computer science;linguistics	NLP	-28.494989106581595	-73.30583900772591	170699
62a53b32aab3b6db6b7c1c7c66634806ec0a62fe	how comparable are parallel corpora? measuring the distribution of general vocabulary and connectives	lexical similarity;sensitive measure;general measure;general vocabulary;specific characteristic;revealing difference;discourse connective;universal tendency;large parallel corpus;various sub-parts;homogeneity;measures;similarity;corpora	In this paper, we question the homogeneity of a large parallel corpus by measuring the similarity between various sub-parts. We compare results obtained using a general measure of lexical similarity based on χ and by counting the number of discourse connectives. We argue that discourse connectives provide a more sensitive measure, revealing differences that are not visible with the general measure. We also provide evidence for the existence of specific characteristics defining translated texts as opposed to nontranslated ones, due to a universal tendency for explicitation.	cohesion (computer science);domain of discourse;europarl corpus;experiment;logical connective;natural language processing;parallel text;semantic similarity;spatial variability;text corpus;vocabulary	Bruno Cartoni;Sandrine Zufferey;Thomas Meyer;Andrei Popescu-Belis	2011			natural language processing;homogeneity;similarity;measure;text corpus;linguistics;statistics	NLP	-27.37542302752436	-75.39854853248552	171273
5d1c8338a149b47d2ad2b2df0a48f9e6df3a5cbc	using alignment for multilingual text compression	text compression;text alignment;coding;compression;multilingual texts	Multilingual text compression exploits the existence of the same text in several languages to compress the second and subsequent copies by reference to the first. We explore the details of this framework and present experimental results for parallel English and French texts.	consortium;data compression;information retrieval;parallel text;pattern matching;text corpus	Ehud S. Conley;Shmuel Tomi Klein	2006		10.1142/S0129054108005553	natural language processing;speech recognition;computer science;mathematics;coding;compression;information retrieval	NLP	-32.31908575169252	-75.4360956154895	171342
8f293edf70a3f8a134da65b4afede32d5102b5d3	observing documentary reading by verbal protocol	artigo			Mariângela Spotti Lopes Fujita;Maria Isabel Asperti Nardi;Silvana Aparecida Fagundes	2003	Inf. Res.		natural language processing;computer science;linguistics;sociology;communication	NLP	-31.869383594181237	-78.56468251400362	171930
07185ed4a8d84d6ee3c5e91c0ef1479c9c20bf8b	resolving ambiguities in sentence boundary detection in russian spontaneous speech		The paper analyses inter-labeller agreement within manual annotations of transcribed spontaneous speech and suggests a way to resolve ambiguities in expert labelling. It argues that the number of controversial sentence boundaries may be reduced if some of them are regarded as “zones”. We describe a technique of detecting these zones and analyse which syntactic structures are the most likely to appear in them. Though the approach is based on Russian language material, it may be applied to oral texts in other languages.	sensor;spontaneous order	Anton Stepikhov	2013		10.1007/978-3-642-40585-3_54	natural language processing;speech recognition;linguistics	NLP	-27.502917523976098	-76.40230823302272	171949
018a8553d272f150567355a2e631956ae291a150	an algorithm for anaphora resolution in spanish texts	analyse de corpus;traitement automatique des langues naturelles;syntax;anaphore pronominale;noun phrase;performance;anaphora resolution;linguistique appliquee;syntaxe;semantic interpretation;info eu repo semantics article;algorithme;algorithm;corpus analysis;interpretation semantique;syntagme nominal;reference;natural language;success rate;computational linguistics;espagnol;pronominal anaphora;antecedent;linguistique informatique;natural language processing;applied linguistics;algoritmo	This paper presents an algorithm for identifying noun phrase antecedents of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) in unrestricted Spanish texts. We define a list of constraints and preferences for different types of pronominal expressions, and we document in detail the importance of each kind of knowledge (lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper also provides a definition for syntactic conditions on Spanish NP-pronoun noncoreference using partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved a success rate of 76.8. We have also implemented four competitive algorithms and tested their performance in a blind evaluation on the same test corpus. This new approach could easily be extended to other languages such as English, Portuguese, Italian, or Japanese.	algorithm;anaphora (linguistics);blue book (cd standard);brill tagger;futures studies;parsing;part-of-speech tagging;point of sale;text corpus;unification (computer science);virtual camera system;wordnet	Manuel Palomar;Antonio Ferrández Rodríguez;Lidia Moreno;Patricio Martínez-Barco;Jesús Peral Cortés;Maximiliano Saiz-Noeda;Rafael Muñoz	2001	Computational Linguistics	10.1162/089120101753342662	natural language processing;personal pronoun;semantic interpretation;noun phrase;speech recognition;syntax;performance;computer science;computational linguistics;applied linguistics;linguistics;english grammar;natural language	NLP	-27.227008711681666	-77.70544387697075	172075
8715817fd014fa1cd1d57d828b242420525b9dd6	representing and parsing terms with accaptability controlled grammar			controlled grammar;parsing	Christian Jacquemin	1993			artificial intelligence;natural language processing;parser combinator;bottom-up parsing;computer science;operator-precedence grammar;parsing;top-down parsing;s-attributed grammar;mildly context-sensitive grammar formalism;top-down parsing language	NLP	-29.536029562155274	-79.30991499312151	172266
ad9f63e4d5859b1e8808ee99beedcbc595694039	correlation of phonetic and morphological systems of indo-european languages		The article is dedicated to a quantitative (correlational and factor) typology of 31 language features (13 phonetic and 31 grammatical) as represented in 38 Indo-European languages and a corresponding differentiation between Slavic, Germanic, Romanic, Indic and Iranian language groups. The language features are grouped into 3 clusters: two contraposed ‘‘polar’’ clusters and an intermediate ‘‘medial’’ cluster. At the basis of the opposition between the two polar clusters 1 and 2 lies and a delimitation of ‘‘vocal’’ and ‘‘consonantal’’ language types, vowels constituting more than 30% of the phonetic system in the first case, in distinction to the second. The diagnostic features of Germanic and Romance languages pertain mainly to cluster 1, those of Slavic languages – to cluster 2. A further differentiation between Germanic and Romance languages is determined by the relevant role of the article, perfect, future-in-the past, diphthongs and long vowels in the first language group and the gerund, continuous tenses, nasalized vowels in the second. Indic languages are positively correlated with the Slavic group on the morphological level and with Germanic languages on the phonetic, Iranian languages correlate with the Slavic group on the phonetic level and are opposed to all the other language groups in their positive correlations with the morphological invariability of the noun, adjective, pronoun and verb. A general conclusion of the investigation is an ascertainment of a high degree of independence of the phonetic and grammatical systems of the languages concerned. A major issue of modern linguistic typology is concerned with the problem of the degree of integratedness of language structure. Are the phonetic, morphological, and others ‘‘levels’’ of a language to be regarded as components of a single unified macrosystem or as autonomous, relatively self-sufficient subsystems on their own account, only loosely interconnected with one another? In other words, are languages to be classified on the basis of a single or several independent sets of criteria, one for each autonomous subsystem? In particular, are phonetic and (grammatical) morphological Address correspondence to: George Silnitsky, Dzerzinskaya str. 8, flat 2, Smolensk, 214000, Russia. Tel.: þ7 (0) 8122 36875; Fax: þ7 (0) 8122 33157; E-mail: rectorat@sci.smolensk.ru D ow nl oa de d by [ L in na eu s U ni ve rs ity ] at 2 3: 56 0 6 O ct ob er 2 01 4 language classifications sufficiently isomorphic to figure in the framework of a single comprehensive typological scheme? On a more generalized level of discussion we deal here with the problem of the relation between the planes of linguistic ‘‘content’’ and ‘‘expression’’ in the typological sphere and the ensuing possibility of elaborating a consistent language classification on the joint basis of linguistic form and meaning. (On a priori grounds, the Saussurian thesis of the mutual independence of signifiant and signifi e implies a negative answer to the question.) Our investigation encompasses 38 Indo-European languages: Russian, Bulgarian, Polish, Czech, Old Church Slavonic, Slovak, Serbo-Croatian, Macedonian, Lithuanian, English, German, Dutch, Norwegian, Swedish, Danish, Old English, Old High German, Old Icelandic, French, Spanish, Portuguese, Italian, Rumanian, Old French, Latin, Ancient Greek, Modern Greek, Armenian, Sanskrit, Vedic, Hindi, Persian, Middle Persian, Baluchi, Khorezmi, Tajiki, Hittite, Lydian. Each language is characterized by the following sets of features:	armenian alphabet;autonomous robot;biological anthropology;fax;indo;iranian.com;medial graph;numerical aperture	George Silnitsky	2003	Journal of Quantitative Linguistics	10.1076/jqul.10.2.129.16717	grammatical gender;fusional language;speech recognition;synthetic language;computer science;linguistics	NLP	-28.071817490923383	-76.07872764708287	172828
71baf861c4a6c986dc26f82579e6ce6bc31185a3	elmolex: connecting elmo and lexicon features for dependency parsing			lexicon;parsing	Ganesh Jawahar;Benjamin Müller;B Bardán-García;Louis Martin;Éric Villemonte de la Clergerie;Benoît Sagot;Djamé Seddah	2018			natural language processing;dependency grammar;lexicon;computer science;artificial intelligence	NLP	-29.762590077258434	-77.55616964643595	173053
b5202f8e2a2bbb024bad63a626c42886214aea1c	semantic interpretation of pragmatic clues: connectives, modal verbs, and indirect speech acts	speech acts;semantic interpretation;natural language	Much work in current research in the field of semantic pragmatic analysis has been concerned with the interpretation of natural language utterances in the context of dialogs. In this paper, however, we will present methods for a primary pragmatic analysis of single utterances. Our investigations involve problems which are not currently well understood, for example how to infer the speaker's intentions by using interpretation of connectives and modal verbs.	logical connective;modal logic;natural language;semantic interpretation	Michael Gerlach;Michael Sprenger	1988		10.3115/991635.991674	natural language processing;semantic interpretation;computer science;linguistics;natural language	NLP	-32.9314011648093	-79.96496878762906	173231
4210c7bc9f86d05cd2d5a8b96a55ca3b3f7c031d	ambiguity resolution in a reductionistic parser		The ENGTWOL morphological analyser is a 55,000 entry Koskenniemi-style morphological description of English that assigns all recognised input word forms with all possible morphological readings as a disjunctive list. Those words not recognised by the ENGTWOL analyser are analysed by a heuristic module; part-ofspeech readings are assigned on the basis of the form of the word (endings etc.). The morphologically analysed sentences are enriched with syntactic and word boundary ambiguities and converted into regular expressions by simple awk programs.	awk;disjunctive normal form;heuristic;morphological parsing;reductionism;regular expression	Pasi Tapanainen;Atro Voutilainen	1993			natural language processing;speech recognition;computer science;bottom-up parsing;linguistics;top-down parsing	NLP	-27.85098259630797	-77.78583506278449	173428
440c12f7a1f60912c252b9e600c06a2110ca67f1	ramble on: tracing movements of popular historical figures		We present RAMBLE ON, an application integrating a pipeline for frame-based information extraction and an interface to track and display movement trajectories. The code of the extraction pipeline and a navigator are freely available; moreover we display in a demonstrator the outcome of a case study carried out on trajectories of notable persons of the XX Century.	graphics pipeline;information extraction	Stefano Menini;Rachele Sprugnoli;Giovanni Moretti;Enrico Bignotti;Sara Tonelli;Bruno Lepri	2017			natural language processing;computer vision;computer science;information extraction;top 100 historical figures of wikipedia;artificial intelligence;tracing	HCI	-33.18145437823859	-75.23529597826322	173577
1d6c79ef200bed12edb82c98e37c6d1ed5ab98e9	utvikling av enkle metoder for teksktsøking med søkeargumenter i naturlig språk (development of simple methods for text search with search arguments in natural language) [in norwegian]			natural language	Tove Fjeldvig	1981			natural language processing;norwegian;natural language;full text search;computer science;artificial intelligence	NLP	-31.56273955131154	-77.74832763223019	174046
12ed6437ffa8672936ebbe6319e87659dba0ada2	automatic spelling correction in galician	grammar;experimental tests;lenguaje natural;linguistique;langage naturel;tratamiento lenguaje;correction automatique;automatic correction;linguistica;language processing;grammaire;natural language;correccion automatica;traitement langage;reparation;reparacion;gramatica;repair;linguistics	We describe a proposal on spelling correction intended to be applied on Galician, a Romance language. Our aim is to put into evidence the flexibility of a novelty technique that provides a quality equivalent to global strategies, but with a significantly minor computational cost. To do it, we take advantage of the grammatical background present in the recognizer, which allows us to dynamically gather information to the right and to the left of the point at which the recognition halts in a word, as long as this information could be considered as relevant for the repair process. The experimental tests prove the validity of our approach in relation to previous ones, focusing on both performance and costs.	acta informatica;algorithm;algorithmic efficiency;automata theory;computation;computational linguistics;dictionary;directed acyclic graph;error detection and correction;experience;finite-state machine;formal system;galaxy morphological classification;lecture notes in computer science;lexicon;nearest neighbor search;parsing;point of interest;spell checker;vocabulary	Manuel Vilares Ferro;Juan Otero Pombo;Francisco-Mario Barcala;Eva Domínguez	2004		10.1007/978-3-540-30228-5_5	natural language processing;computer science;grammar;natural language;algorithm	AI	-27.09835818487214	-78.50197030957914	174225
426241ad5c2522eb21c3f15b87f6a318355467d4	unlexicalized dependency parser for variable word order languages based on local contextual pattern	busqueda informacion;tratamiento automatico;lenguaje natural;traitement automatique des langues naturelles;syntactic parsing;linguistica matematica;deteccion;lexicalization;filtrado;lexical feature;dependency grammar;contextual information;grammaire hors contexte;linguistique appliquee;word order;analyse syntaxique automatique;grammaire de dependance;probabilistic model;categorizacion;modele de langage;contexte free grammar;grammaire formelle;evaluative study;sistema investigacion;modele probabiliste;formal grammar;lexicalisation;computational linguistics;trait lexical;metodo;ordre des mots;linguistique informatique;clasificacion;extraccion informacion;natural language processing;language model;etude evaluative;applied linguistics	We investigate the effect of unlexicalization in a dependency parser for variable word order languages and propose an unlexicalized parser which can utilize some contextual information in order to achieve performance comparable to that of lexicalized parsers. Unlexicalization of an early dependency parser makes performance decrease by 3.6%. However, when we modify the unlexicalized parser into the one which can consider additional contextual information, the parser performs better than some lexicalized dependency parsers, while it requires simpler smoothing processes, less time and space for parsing.	earley parser;smoothing	Hoo-Jung Chung;Hae-Chang Rim	2004		10.1007/978-3-540-24630-5_14	word order;natural language processing;statistical model;parser combinator;speech recognition;lalr parser;canonical lr parser;parsing expression grammar;computer science;computational linguistics;applied linguistics;glr parser;linguistics;formal grammar;recursive descent parser;ll parser;top-down parsing;lr parser;language model;simple lr parser;dependency grammar	NLP	-26.735422943925386	-78.4403186201361	174355
055d79bfa06bb5f4861f7b0ec8d305896eba93c1	un método de aprendizaje semi-supervisado para la modelización semántica en comprensión del habla	language comprehension;apprentissage automatique;traitement automatique des langues naturelles;computacion informatica;statistical modelization;comprehension du langage;filologias;informacion documentacion;linguistica;machine learning;ciencias basicas y experimentales;computational linguistics;semantic classification;grupo a;ciencias sociales;linguistique informatique;grupo b;natural language processing;spoken language understanding	In this paper we present a algorithm for the statistical learning of semantic models, based on a corpus of unaligned pairs of sentences and semantic representations in terms of frames. The objective is automatically associate variable-length segments with their corresponding semantic labels to be used in speech understanding tasks. One advantage of this approach is to avoid the expensive work of segmenting and labeling the whole training corpus, process which is needed by almost all the corpus based methods. Moreover, the discrimination learning ability of this method is specially interesting. We have applied this algorithm to the development of the understanding module of a spoken dialog system, whose task is the access to information about trains. We present experiments that confirm the appropriateness of the methodology.	algorithm;dialog system;experiment;freedom of information laws by country;linear algebra;machine learning;norm (social);semiconductor industry;spoken dialog systems;text corpus	Lucía Ortega;Isabel Galiano;Lluís F. Hurtado;Emilio Sanchis Arnal;Encarna Segarra	2010	Procesamiento del Lenguaje Natural		natural language processing;speech recognition;computer science;computational linguistics;linguistics	NLP	-27.068694781629222	-77.95530198330951	174505
94aab02d7a68a5383fbd676486cf4a4c6313933c	towards a unified framework for bilingual terminology extraction of single-word and multi-word terms		Extracting a bilingual terminology for multi-word terms from comparable corpora has not been widely researched. In this work we propose a unified framework for aligning bilingual terms independently of the term lengths. We also introduce some enhancements to the context-based and the neural network based approaches. Our experiments show the effectiveness of our enhancements over previous works and that the system can be adapted in specialized domains. Title and Abstract in French Vers un systeme unifie pour lu0027extraction terminologique bilingue de termes simples et complexes Lu0027extraction du0027une terminologie bilingue pour les termes complexesacomplexes`complexesa partir de corpus compa-rables nu0027a pasetepas´pasete beaucoupetudieebeaucoup´beaucoupetudiee. Dans ce travail, nous proposons un systeme unifie pour lu0027alignement des termes bilingues independamment de la longueur des termes. De plus nous in-troduisonsegalementtroduisons´troduisonsegalement des ameliorations aux approches basees sur lu0027alignement de contexte et sur un reseau neuronal. Nos experiences montrent lu0027efficacite de nos ameliorations sur les travaux anterieurs, et le fait que le systeme peutetrepeutˆpeutetre adapte en domaines de specialite.	artificial neural network;bibliothèque de l'école des chartes;domain theory;experiment;linear algebra;terminology extraction;text corpus;unified framework	Jingshu Liu;Emmanuel Morin;Sebastián Peña Saldarriaga	2018			natural language processing;artificial intelligence;computer science;terminology extraction;terminology	NLP	-27.1316318892502	-78.3293362271254	174599
33cdbd3da2ac23f445fe59505bafb226bc076ebc	automatic generation of exercises on passive transformation in portuguese	grammar;pragmatics;semantics;syntactics;natural language processing;context	Technology plays a very important role in education and Intelligent Computer-Assisted Language Learning (iCALL) has emerged as a complementary or even alternative method to the conventional language teaching practices. The automatic generation (and correction) of language exercises based on real texts extracted from corpora constitutes a non-trivial challenge to iCALL tutorial systems, and may involve the use of sophisticated Natural Language Processing tools and large-scale linguistic resources. This paper presents the main issues related to the automatic generation of exercises on the Passive transformation, a commonly occurring type of exercises in language textbooks, but also a very complex topic of Portuguese grammar. The paper describes the methods used to produce a large batch of passive-active sentence pairs, where the active sentence was automatically generated from naturally occurring passive sentences, taken from a large-sized, publicly available, corpus. Sentence pairs are ranked by difficulty level. A sample of randomly selected sentence pairs (40 from difficult level, 100 from medium, and 100 from easy level) was manually evaluated by an expert. Results are presented and error analysis is performed. The sentence pairs can be used as prime and correct answer for iCALL systems.	active and passive transformation;algorithm;emoticon;error analysis (mathematics);graphical user interface;natural language processing;parsing;randomness;sentence boundary disambiguation;text corpus;the sentence;vii	Jorge Baptista;Sandra Lourenco;Nuno J. Mamede	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744427	natural language processing;speech recognition;computer science;artificial intelligence;grammar;semantics;pragmatics	NLP	-27.90608969149186	-73.70181076315542	174656
40bb8a6c64c33cbb5136891275a6a1844a455a64	tagging historical corpora - the problem of spelling variation		Spelling issues tend to create relatively minor (though still complex) problems for corpus linguistics, information retrieval and natural language processing tasks that use ‘standard’ or modern varieties of English. For example, in corpus annotation, we have to decide how to deal with tokenisation issues such as whether (i) periods represent sentence boundaries or acronyms and (ii) apostrophes represent quote marks or contractions (Grefenstette and Tapanainen, 1994; Grefenstette, 1999). The issue of spelling variation becomes more problematic when utilising corpus linguistic techniques on non-standard varieties of English, not least because variation can be due to differences in spelling habits, transcription or compositing practices, and morpho-syntactic customs, as well as “misspelling”. Examples of non-standard varieties include:	compositing;corpus linguistics;information retrieval;natural language processing;text corpus;transcription (software)	Paul Rayson;Dawn Archer;Alistair Baron;Nicholas Smith	2006			natural language processing;speech recognition;linguistics	NLP	-30.951283498719118	-74.57818297908493	174771
ea29d03e540218b89aef851a4a69aee1642719ef	smt and hybrid systems of the qtleap project in the wmt16 it-task		This paper presents the description of 12 systems submitted to the WMT16 IT-task, covering six different languages, namely Basque, Bulgarian, Dutch, Czech, Portuguese and Spanish. All these systems were developed under the scope of the QTLeap project, presenting a common strategy. For each language two different systems were submitted, namely a phrasebased MT system built using Moses, and a system exploiting deep language engineering approaches, that in all the languages but Bulgarian was implemented using TectoMT. For 4 of the 6 languages, the TectoMT-based system performs better than the Moses-based one.	moses	Rosa Del Gaudio;Gorka Labaka;Eneko Agirre;Petya Osenova;Kiril Ivanov Simov;Martin Popel;Dieke Oele;Gertjan van Noord;Luís Gomes;João António Rodrigues;Steven Neale;João Ricardo Silva;Andreia Querido;Nuno Rendeiro;António Branco	2016			natural language processing;algorithm	NLP	-29.616230165532812	-75.6908982823853	174788
09707899a3b7762700e5c5b3dd859a07e5c4e915	a formal ontology-based classification of lexemes and its applications		The paper describes the enrichment of OntoSenseNet a verb-centric lexical resource for Indian Languages. A major contribution of this work is preservation of an authentic Telugu dictionary by developing a computational version of the same. It is important because native speakers can better annotate the sense-types when both the word and its meaning are in Telugu. Hence efforts are made to develop the aforementioned Telugu dictionary and annotations are done manually. The manually annotated gold standard corpus consists 8483 verbs, 253 adverbs and 1673 adjectives. Annotations are done by native speakers according to defined annotation guidelines. In this paper, we provide an overview of the annotation procedure and present the validation of the developed resource through inter-annotator agreement. Additional words from Telugu WordNet are added to our resource and are crowdsourced for annotation. The statistics are compared with the sense-annotated lexicon, our resource for more insights.	computation;crowdsourcing;dictionary;formal ontology;gene ontology term enrichment;inter-rater reliability;lexical analysis;lexicon;wordnet	Sreekavitha Parupalli;Navjyoti Singh	2018	CoRR		artificial intelligence;telugu;natural language processing;formal ontology;wordnet;computer science;lexicon;annotation	NLP	-29.438296451140857	-74.8556901281346	175828
708bca84087e7232e04a22e77611ec21fffece60	a frequency dictionary of modern written and oral media arabic			dictionary;word lists by frequency	Orhan Elmaz	2012			arabic;linguistics;computer science	Metrics	-30.54017994155286	-77.83991174435	175956
a9e0918830c96b0934d9c6a23092b7540b2a9686	pergram: a trale implementation of an hpsg fragment of persian	grammars;natural language processing;hpsg grammar of persian;pergram;trale implementation;linguistic literature	In this paper, we discuss an HPSG grammar of Persian (PerGram) that is implemented in the TRALE system. We describe some of the phenomena which are currently covered. While working on the grammar, we developed a test suite with positive and negative examples from the linguistic literature. To be able to test the coverage of the grammar with respect to naturally occurring sentences, we use a subcorpus of a big corpus of Persian.	british national corpus;experiment;head-driven phrase structure grammar;mathematical morphology;minimal recursion semantics;test suite	Stefan Müller;Masood Ghayoomi	2010	Proceedings of the International Multiconference on Computer Science and Information Technology		natural language processing;morphology;computer science;ontology;grammar;semantics;linguistics;programming language;pragmatics	NLP	-28.72028316797226	-76.71445065162958	176262
59d5c3f18ad4cadf8e5e2a8ad9f2f102cbd0a972	the emergence of compositional structure in language evolution and development.				Mary E. Beckman	2015			natural language processing;speech recognition;artificial intelligence;computer science	Robotics	-30.822729072106334	-79.10212119440973	176501
302148720b9c50cc8d8753787ea63d77f659400a	transliterating urdu for a broad-coverage urdu/hindi lfg grammar	inproceedings	In this paper, we present a system for transliterating the Ar abic-based script of Urdu to a Roman transliteration scheme . The system is integrated into a larger system consisting of a morpholog y module, implemented via finite state technologies, and a co mputational LFG grammar of Urdu that was developed with the grammar devel opment platform XLE (Crouch et al. 2008). Our long-term goal is to handle Hindi alongside Urdu; the two languages are very si milar with respect to syntax and lexicon and hence, one gramm r can be used to cover both languages. However, they are not similar c oncerning the script – Hindi is written in Devanagari, while Urdu uses an Arabic-based script. By abstracting away to a common Roman t ransliteration scheme in the respective transliterators, our system can be enabled to handle both languages in parallel. In this pape r, we discuss the pipeline architecture of the Urdu-Roman tr sliterator, mention several linguistic and orthographic issues and pre sent the integration of the transliterator into the LFG pars ing ystem.	lexical functional grammar;lexicon;orthographic projection;pipeline (computing);wilhelm pape	Muhammad Kamran Malik;Tafseer Ahmed;Sebastian Sulger;Tina Bögel;Atif Gulzar;Ghulam Raza;Sarmad Hussain;Miriam Butt	2010			natural language processing;speech recognition;computer science;linguistics	NLP	-28.969591204517968	-78.21193528536612	176560
6ffda5c503c0113e0cbb1ec0e43141658ff4e764	an open source library for semantic-based datetime resolution		In this paper, we introduce an original Python implementation of datetime resolution in French, which we make available as open-source library. Our approach is based on Frame Semantics and Corpus Pattern Analysis in order to provide a precise semantic interpretation of datetime expressions. This interpretation facilitates the contextual resolution of datetime expressions in timestamp format.	lexicon;open-source software;python;semantic interpretation	Aurélie Merlo;Denis Pasin	2016			natural language processing;artificial intelligence;computer science	NLP	-30.197795992414964	-75.57320503538058	176649
0aecffa8a1f1a0b70fed820a98fa7827dcb95a2a	idioms modeling in a computer ontology as a morphosyntactic disambiguation strategy		The article presents the experience of developing computer ontology as one of the tools for Tibetan idioms processing. A computer ontology that contains a consistent specification of meanings of lexical units with different relations between them represents a model of lexical semantics and both syntactic and semantic valencies, reflecting the Tibetan linguistic picture of the world. The article presents an attempt to classify Tibetan idioms, including compounds, which are idiomatized clips of syntactic groups that have frozen inner syntactic relations and are often characterized by omission of grammatical morphemes; and the application of this classification for idioms processing in computer ontology. The article also proposes methods of using computer ontology for avoiding idioms processing ambiguity.		Alexei Dobrov;Anastasia Dobrova;Pavel Grokhovskiy;Maria Smirnova;Nikolay Soms	2018		10.1007/978-3-030-00794-2_8	artificial intelligence;natural language processing;ambiguity;tibetan language;ontology;computer science;corpus linguistics;lexical semantics;morpheme;syntax	NLP	-30.524539271948655	-78.90404233975745	176852
c93bdb439a521e2bed54cbfa93698610c453f828	automatic detection of machine translated text and translation quality estimation		"""The recent success and proliferation of statistical machine translation (MT) systems raise a number of important questions. Prominent among these is how to automatically estimate the translation quality of such a system in various language pairs and domains, as it is crucial in the ongoing process of developing and training newMT systems. Another important question is how to detect machine translated text in an environment containing both human and MT sentences, as commonly found in web-based textual data. This thesis explores the relation between those two tasks and presents a novel approach for machine translation quality estimation based on the correlation between machine translation detection accuracy and translation quality. To begin, we define the problem of machine translation (MT) quality estimation (QE). This is the problem of automatically estimating translation quality at the corpus, sentence or word level, without reference translations or any preliminary information on the expected output. Contrast this with the problem of MT evaluation, which relies on such reference translations in order to evaluate the translation quality. MT detection is defined as the problem of automatically recognizing the MT text portions from within a corpus containing both MT and human generated text, mainly at the paragraph or sentence level. In order to perform MT detection the general features of translated texts are employed, which have been studied widely for many years. Attempts to define their characteristics, often called translation universals, include [38, 6, 2, 15] who showed that the differences between native and translated texts go well beyond systematic translation errors and point to a distinct """"translationese"""" dialect. Other works [5, 26, 21, 25] use text classification techniques in order to distinguish human translated text from native language text at the document or paragraph level, using various linguistic features. Regarding the detection of MT text, Carter and Inkpen [10] conducted detection experiments at the document level, and Arase and Zhou [1] did so at the sentence-level. While previous work has considered MT detection at different levels, the correlation between the quality of the MT text and the ability to detect it has not been studied. It is hypothesized that the quality of a given MT system can be measured by the accuracy with which a classifer can distinguish between human-generated sentences and sentences generated by that MT system. This work shows that while using common linguistic features, such as frequencies of part-of-speech n-grams and function words, it is possible to train classifiers that distinguish MT text from human-translated or native English text. While this is a straightforward and not entirely novel result, the main contribution of this work is to relativize it. It is shown that the success of such classifiers is strongly correlated with the quality of the underlying MT system. Once a classification experiment is performed, the accuracy of classifying the sentences in the corpus is measured. This accuracy will be shown to decrease as the quality of the underlying MT system increases, by measuring the R correlation coefficient. This correlation is strong enough to propose this accuracy measure as a measure of translation quality."""	coefficient;document classification;experiment;grams;n-gram;quadratic equation;statistical machine translation;strongly correlated material;text corpus;web application	Roee Aharoni;Moshe Koppel;Yoav Goldberg	2014			natural language processing;synchronous context-free grammar;speech recognition;transfer-based machine translation;example-based machine translation;computer science;evaluation of machine translation;pattern recognition;rule-based machine translation	NLP	-26.590418914198246	-73.46890782863105	177088
219be22048df896f054173ab237572fbb2e499a2	an integrated amis prototype for automated summarization and translation of newscasts and reports		In this paper we present the results of the integration works on the system designed for automated summarization and translation of newscast and reports. We show the proposed system architectures and list the available software modules. Thanks to well defined interfaces the software modules may be used as building blocks allowing easy experimentation with different summarization scenarios.	automatic summarization;boolean algebra;component-based software engineering;facial recognition system;journal citation reports;machine translation;prototype;python;speech recognition;text-based (computing);video processing	Michal Grega;Kamel Smaïli;Mikolaj Leszczuk;Carlos-Emiliano González-Gallardo;Juan-Manuel Torres-Moreno;Elvys Linhares Pontes;Dominique Fohr;Odile Mella;Mohamed Menacer;Denis Jouvet	2018		10.1007/978-3-319-98678-4_42	machine translation;automatic summarization;information retrieval;software;computer science	NLP	-32.586436283228366	-73.47210457810215	177206
c3823593b01e5fa605a248fb1ee566c438a39a40	build fast and accurate lemmatization for arabic.		In this paper we describe the complexity of building a lemmatizer for Arabic which has a rich and complex morphology, and show some differences between lemmatization and surface stemming, i.e. removing prefixes and suffixes from words. We discuss the need for a fast and accurate lammatization to enhance Arabic Information Retrieval results. We also introduce a new dataset that can be used to test lemmatization accuracy, and an efficient lemmatization algorithm that outperforms state-of-the-art Arabic lemmatization in terms of accuracy and speed. We share the dataset and the code for research purposes.	algorithm;information retrieval;lemmatisation;mathematical morphology;stemming	Hamdy Mubarak	2017	CoRR		lemmatisation;arabic;natural language processing;artificial intelligence;speech recognition;computer science	NLP	-27.99184957280873	-74.06731929479743	177952
f3dd6a07bde670083f5824025757f894d0939e80	determining writing genre: towards a rubric-based approach to automated essay grading	natural language processing writing genre automated essay grading;training;text analysis;speech;natural language processing tools;conference paper;narrative essay;text analysis natural language processing;essay genres;writing kirk field collapse effect natural language processing speech context training;named entity recognition;automated essay grading;kirk field collapse effect;part of speech tagging;speech tagging;expository essay writing genre textual content natural language processing tools named entity recognition speech tagging sentence parsing essay genres narrative essay persuasive essay descriptive essay;expository essay;writing;persuasive essay;writing genre;sentence parsing;natural language processing;context;descriptive essay;textual content	A writing genre can be thought of as the style in which the writer chooses to present textual content to the reader. We distinguish four main types of essay genres namely Narrative, Persuasive, Descriptive and Expository. An essay's writing genre can be identified by searching for salient features present within those genres using various Natural Language Processing tools such as Named Entity Recognition, Part of Speech tagging and Sentence Parsing. This paper explains the more common writing genres in student essays and describes the method in which essays in the narrative genre are identified.	named-entity recognition;natural language processing;parsing expression grammar;part-of-speech tagging	Hon Wai Lam;Tharam S. Dillon;Elizabeth Chang	2011	2011 IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2011.32	natural language processing;text mining;computer science;speech;academic writing;rhetorical modes;writing	NLP	-28.215151484689773	-75.64767830754431	178129
39df62f0f1b87ec1685cf28ce795dc5c26e112fe	using collocation statistics in information extraction	information extraction;relation extraction	"""Our main objective in participating MUC-7 is to investigate and experiment with the use of collocation statistics in information extraction. A collocation is a habitual word combination, such as \weather a storm"""", \ le a lawsuit"""", and \the falling yen"""". Collocation statistics refers to the frequency counts of the collocational relations extracted from a parsed corpus. For example, out of 6577 instances of \addition"""" in a corpus, 5190 was used as the object of \in"""". Out of 3214 instances of \hire"""", 12 of them take \alien"""" as the object."""	collocation;communications security;information extraction;message understanding conference;named entity;text corpus;treebank;word sense;word-sense disambiguation	Dekang Lin	1998			information extraction;relationship extraction;parsing;statistics;collocation;mathematics	NLP	-29.26193277832663	-76.82424304068941	178309
0df25cbccd02f1a2c8b2cf4d4ffc53b8e2e06b5e	persian in multext-east framework	orthographe;ortografia;regle ecriture;modelizacion;lenguaje natural;categorisation;linguistique;lexicon;writing rule;langage naturel;orthography;language resources;modelisation;categorizacion;linguistica;arabic;natural language;part of speech;arabe;lexico;modeling;regla escritura;categorization;lexique;linguistics	Farsi, also known as Persian, is the official language of Iran, Tajikistan and one of the two main languages spoken in Afghanistan. It is an Indo-European agglutinating language, written in Arabic script. This paper presents the first step in creating Farsi basic language resources kit. This Step comprises the specifications for morphosyntactic encoding, which is based on the EAGLES/MULTEXT model and specific resources of MULTEXT-East. This paper introduces the language i.e. Farsi, with an emphasis on its writing system and morphological properties, and its specifications. Two other important issues introduced in this paper are; one, a novel Part of Speech (PoS) categorization and, the other, a unified orthography of Farsi in digital environment. A lexicon and an annotated corpus are under preparation.	categorization;digital environment;indo;lexicon;text corpus	Behrang Q. Zadeh;Saeed Rahimi	2006		10.1007/11816508_54	natural language processing;speech recognition;orthography;part of speech;arabic;natural language;categorization	NLP	-28.43122784355215	-78.29766343433091	179846
647264c3d2befb831e780319d8512f39289f65de	comparing the performance of different nlp toolkits in formal and social media text	004;natural language processing toolkits formal text social media benchmark	Nowadays, there are many toolkits available for performing common natural language processing tasks, which enable the development of more powerful applications without having to start from scratch. In fact, for English, there is no need to develop tools such as tokenizers, partof-speech (POS) taggers, chunkers or named entity recognizers (NER). The current challenge is to select which one to use, out of the range of available tools. This choice may depend on several aspects, including the kind and source of text, where the level, formal or informal, may influence the performance of such tools. In this paper, we assess a range of natural language processing toolkits with their default configuration, while performing a set of standard tasks (e.g. tokenization, POS tagging, chunking and NER), in popular datasets that cover newspaper and social network text. The obtained results are analyzed and, while we could not decide on a single toolkit, this exercise was very helpful to narrow our choice. 1998 ACM Subject Classification I.2.7 Natural Language Processing	finite-state machine;list of toolkits;named-entity recognition;outline of natural language processing;part-of-speech tagging;phrase chunking;shallow parsing;social media;social network;tokenization (data security)	Alexandre Miguel Pinto;Hugo Gonçalo Oliveira;Ana Oliveira Alves	2016		10.4230/OASIcs.SLATE.2016.3	natural language processing;speech recognition;computer science;artificial intelligence;data mining;linguistics;programming language;world wide web	NLP	-28.99845679563589	-74.087055947426	179918
33afc08d2654b9e9eb2c65a2a4e9860c97e7b3d1	querying both time-aligned and hierarchical corpora with nxt search		One problem of the (re-)usability and exchange of annotated corpora is in the lack of standards in corpus formats and corpus query tools. This paper reports on the NXT Search tool, which was used to query two corpora with very different annotation formats. It is shown that with automatic data format conversion both corpora can be accessed and searched with NXT Search.	nxt;text corpus	Ulrich Heid;Holger Voormann;Jan-Torsten Milde;Ulrike Gut;Katrin Erk;Sebastian Padó	2004			artificial intelligence;natural language processing;computer science;usability;annotation	Web+IR	-32.649713822664935	-74.20469249414502	180277
4a9440950d85ae65fa7b65b1491bb2105841ce7b	scaling up from dialogue to multilogue: some principles and benchmarks	long distance resolution possibility;dialogue protocol;possible transformation;transformation yields protocol;non-sentential utterance;british national corpus;dialogue management;multiple conversationalist;issue-based approach	The paper considers how to scale up dialogue protocols to multilogue, settings with multiple conversationalists. We extract two benchmarks to evaluate scaled up protocols based on the long distance resolution possibilities of nonsentential utterances in dialogue and multilogue in the British National Corpus. In light of these benchmarks, we then consider three possible transformations to dialogue protocols, formulated within an issue-based approach to dialogue management. We show that one such transformation yields protocols for querying and assertion that fulfill these benchmarks.	assertion (software development);benchmark (computing);british national corpus;dialog system;incidence matrix;locality of reference;simulation;text corpus	Jonathan Ginzburg;Raquel Fernández	2005			natural language processing;computer science;data science;data mining	NLP	-29.938954873380865	-75.60801795288914	180379
27da37eed4e739a02bf428c8def63840515f5c9c	typed entity and relation annotation on computer science papers			computer science	Yuka Tateisi;Tomoko Ohta;Sampo Pyysalo;Yusuke Miyao;Akiko Aizawa	2016			artificial intelligence;natural language processing;information retrieval;computer science;annotation	Logic	-31.040963374990167	-76.9278228279104	181158
8499dfc950abe26ffe9d7c4a948281cb5e8225ae	beauty before age? applying subjectivity to automatic english adjective ordering		The preferred order of pre-nominal adjectives in English is determined primarily by semantics. Nevertheless, Adjective Ordering (AO) systems do not generally exploit semantic features. This paper describes a system that orders adjectives with significantly abovechance accuracy (73.0%) solely on the basis of semantic features pertaining to the cognitive-semantic dimension of subjectivity. The results indicate that combining such semantic approaches with current methods could result in more accurate and robust AO systems.	ambient occlusion;automatic differentiation	Felix Hill	2012			natural language processing;linguistics	AI	-26.988560112877	-74.5533144732277	181418
17d77d710d3ecb5b0c8640a144d2a2461640da53	rules against the machine: building bridges from text to metadata				José Calvo Tello	2018			database;metadata;computer science	NLP	-31.55374688680202	-76.69106735278805	181448
d8aecee84541ff5644675c0a6d5e87be27ade831	learning subjective language	apprentissage automatique;traitement automatique des langues naturelles;language use;high density;information extraction;low frequency;linguistique appliquee;linguistique de corpus;machine learning;natural language;corpus linguistics;computational linguistics;subjectivite;subjectivity;linguistique informatique;natural language processing;text categorization;extraction;applied linguistics	Subjectivity in natural language refers to aspects of language used to express opinions, evaluations, and speculations. There are numerous natural language processing applications for which subjectivity analysis is relevant, including information extraction and text categorization. The goal of this work is learning subjective language from corpora. Clues of subjectivity are generated and tested, including low-frequency words, collocations, and adjectives and verbs identified using distributional similarity. The features are also examined working together in concert. The features, generated from different data sets using different procedures, exhibit consistency in performance in that they all do better and worse on the same data sets. In addition, this article shows that the density of subjectivity clues in the surrounding context strongly affects how likely it is that a word is subjective, and it provides the results of an annotation study assessing the subjectivity of sentences with high-density features. Finally, the clues are used to perform opinion piece recognition (a type of text categorization and genre detection) to demonstrate the utility of the knowledge acquired in this article.	baseline (configuration management);carlson's theorem;categorization;cluster analysis;collocation;computation;computational linguistics;computer performance;cross-validation (statistics);document classification;information extraction;k-nearest neighbors algorithm;lexical analysis;natural language processing;nick mckeown;objectivity/db;question answering;regular expression;test data;test set;text corpus	Janyce Wiebe;Theresa Wilson;Rebecca F. Bruce;Matthew Bell;Melanie Martin	2004	Computational Linguistics	10.1162/0891201041850885	natural language processing;language identification;extraction;computer science;computational linguistics;applied linguistics;corpus linguistics;linguistics;subjectivity;low frequency;natural language;information extraction	NLP	-27.146318859283937	-76.45073253434941	181989
0ba6ece71191afcb8b93e8715cc052205d0152d9	an annotation scheme and gold standard for dutch-english word alignment	gold standard;english language	The importance of sentence-aligned parallel corpora has been widely acknowledged. Reference corpora in which sub-sentential translational correspondences are indicated manually are more labour-intensive to create, and hence less wide-spread. Such manually created reference alignments – also called Gold Standards – have been used in research projects to develop or test automatic word alignment systems. In most translations, translational correspondences are rather complex; for example word-by-word correspondences can be found only for a limited number of words. A reference corpus in which those complex translational correspondences are aligned manually is therefore also a useful resource for the development of translation tools and for translation studies. In this paper, we describe how we created a Gold Standard for the Dutch-English language pair. We present the annotation scheme, annotation guidelines, annotation tool and inter-annotator results. To cover a wide range of syntactic and stylistic phenomena that emerge from different writing and translation styles, our Gold Standard data set contains texts from different text types. The Gold Standard will be publicly available as part of the Dutch Parallel Corpus.	bitext word alignment;data structure alignment;parallel text;text corpus;translation studies	Lieve Macken	2010			artificial intelligence;speech recognition;natural language processing;syntax;information retrieval;translation studies;computer science;gold standard;annotation	NLP	-29.666149833409964	-73.56027454007922	182112
fbff818183c3fa5c0f20c857a7458fb2176a988b	partial parsing as a method to expedite dependency annotation of a hindi treebank		The paper describes an approach to expedite the process of manual annotation of a Hindi dependency treebank which is currently under development. We propose a way by which consistency among a set of manual annotators could be improved. Furthermore, we show that our setup can also prove useful for evaluating when an inexperienced annotator is ready to start participating in the production of the treebank. We test our approach on sample sets of data obtained from an ongoing work on creation of this treebank. The results asserting our proposal are reported in this paper. We report results from a semi-automated approach of dependency annotation experiment. We find out the rate of agreement between annotators using Cohen’s Kappa. We also compare results with respect to the total time taken to annotate sample data-sets using a completely manual approach as opposed to a semi-automated approach. It is observed from the results that this semi-automated approach when carried out with experienced and trained human annotators improves the overall quality of treebank annotation and also speeds up the process.	experience;parsing;semiconductor industry;treebank	Mridul Gupta;Vineet Yadav;Samar Husain;Dipti Misra Sharma	2010			speech recognition;natural language processing;artificial intelligence;hindi;treebank;parsing;computer science;annotation	NLP	-27.591404143474385	-75.67770853734999	182491
6fdc98746ea42da2f60a90b6c86a42f5d830c3e4	gapping constructions in universal dependencies v2		In this paper, we provide a detailed account of sentences with gapping such as “John likes tea, and Mary coffee” within the Universal Dependencies (UD) framework. We explain how common gapping constructions as well as rare complex constructions can be analyzed on the basis of examples in Dutch, English, Farsi, German, Hindi, Japanese, and Turkish. We further argue why the adopted analysis of these constructions in UD version 2 is better suited for annotating treebanks and parsing than previous proposals, and we discuss how gapping constructions can be analyzed in the enhanced UD representation, a graph-based semantic representation for shallow computational semantics tasks.	calculus of constructions;computational semantics;parsing;treebank;urban dictionary;von neumann universal constructor	Sebastian Schuster;Matthew Lamm;Christopher D. Manning	2017			diva;world wide web;gapping;institutional repository;electronic publishing;drag and drop;publication;social media;computer science	NLP	-30.82837766594552	-75.85984769964276	182571
1013083dd79b9178d7c14b42f4c898f65ce71166	parsing morphologically rich languages: introduction to the special issue	sprakteknologi sprakvetenskaplig databehandling;datorlingvistik;computational linguistics;language technology computational linguistics	Parsing is a key task in natural language processing. It involves predicting, for each natural language sentence, an abstract representation of the grammatical entities in the sentence and the relations between these entities. This representation provides an interface to compositional semantics and to the notions of “who did what to whom.” The last two decades have seen great advances in parsing English, leading to major leaps also in the performance of applications that use parsers as part of their backbone, such as systems for information extraction, sentiment analysis, text summarization, and machine translation. Attempts to replicate the success of parsing English for other languages have often yielded unsatisfactory results. In particular, parsing languages with complex word structure and flexible word order has been shown to require non-trivial adaptation. This special issue reports on methods that successfully address the challenges involved in parsing a range of morphologically rich languages (MRLs). This introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the contributions of the articles in the special issue. These contributions present up-to-date research efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs addresses challenges that transcend particular representational and algorithmic choices.	automatic summarization;entity;information extraction;internet backbone;machine translation;natural language processing;parsing;rich internet application;self-replicating machine;sentiment analysis	Reut Tsarfaty;Djamé Seddah;Sandra Kübler;Joakim Nivre	2013	Computational Linguistics	10.1162/COLI_a_00133	natural language processing;computer science;bottom-up parsing;computational linguistics;parsing;s-attributed grammar;linguistics;programming language	NLP	-30.3710046368393	-74.59563129898109	182950
eb3c4d8615f7f43a743f2e8f90c764e19140fce7	the eclipse annotator: an extensible system for multimodal corpus creation		The Eclipse-Annotator is an extensible tool for the creation of multimodal language resources. It is based on the TASX-Annotator, which has been refactored in order to fit into the plugin based architecture of the new application.	code refactoring;eclipse;multimodal interaction	Fabian Behrens;Jan-Torsten Milde	2006			speech recognition;artificial intelligence;eclipse;natural language processing;computer science;extensibility	NLP	-33.02182274454313	-76.25163159364955	183115
91dc33a2ff5aa5710897ee19414d619e4b399373	learning morphology: algorithms for the identification of the stem changes	suitable feature;stem change;formal classification;automatic recognition;human knowledge;main problem;current work;stem variant;stem pair;automatic recognising	"""The aim of the current work is to create tools' for the automatic recognition of the Estonian stem changing rules'. The main problem consists in bringing together the ,fi)rmal classification,features available to the computer and classification based on human knowledge. This paper introduces two algorithms. First, in STLearn the supervised inductive learning technique is used to find out the suitable jeatures Jor automatic recognising of the stem changes. Two stem variants"""" can be bounded by more than one stem change. The second algorithm is created Jor the identifjdng the whole set of rules Jor stem pairs'. Current work is a part of a project based on the open model of language [Viks94] according to which all regular and productive phenomena of the natural language are represented by different types of rules and irregular phenomena are listed in small dictionaries exception lists. This approach gives opportunity to process the regular words not listed in dictionaries new derivatives, loan-words etc. Subsystem of morphology plays the central role in processing of the morphologically complex languages as the Estonian language is. The number of possible stem variants can strongly vary in Estonian: in some inflection types there are no stem variants at all, in some of them a word can have even five different regular stem variants. Current work presents tools ~br creating a formal description of the Estonian stem changing rules, starting from the pair of the stem variants. The Concise Morphological Dictionary of the Estonian (CMD) [Viks92] serves as a bases for current work and contains over 36 000 headwords, each of them has two stem variants on the averages. The principle types of changes are the following: 1. Stem-grade changes. Stem can occur either in a strong or a weak grade; the grades are differentiated first of all by phonetic quantity (2nd or 3rd degree of quantity marked by') that may be accompanied by various sound changes enfblding the medial sounds. For instance members of the stem pair h6ive-h'~ive are distinguished only by the different phonetic quantity; in case of couple aat2e'aal2e the rewriting rule b --+ p is concurrent with the phonetic quantity change. 2. Stem-end changes. Stem can appear either as a lemmatic stem or an inflection stem; stem variants are differentiated by changes enfolding the final sounds ( e.g. 'aadel-aadli, j'alg \~bot\-j'alga, sipelgas"""" \ant\-sipelga). 3. Secondary changes. These changes are conditioned by the certain context arising after either the stem-end or the stemgrade change (e.g. k'uppel \dome\ --~ * k'uppli --+ k'upli). About 20 % of stems stay changeless, mostly take place the stem-end or stem-grade changes or both at the same time. Formally the recognition of the stem change rules can be reduced to the classification task with string pairs as the objects to classify and possible rules of stem changes as the classes. System has to create class descriptions from the 'available' data: characters and their belongness to the sound classes. The important demand to the classification system is the linguistical"""	algorithm;creative micro designs;galaxy morphological classification;headword;linked list;mathematical morphology;medial graph;morphological dictionary;natural language;rewriting;weak ai	Evelin Kuusik	1996			computer science;artificial intelligence;machine learning;data mining	PL	-28.580031188627196	-77.92088322939104	183619
2c524dfdd10281f3a15f87d05f50a856a6c69870	designing a russian idiom-annotated corpus		This paper describes the development of an idiom-annotated corpus of Russian. The corpus is compiled from freely available resources online and contains texts of different genres. The idiom extraction, annotation procedure, and a pilot experiment using the new corpus are outlined in the paper. Considering the scarcity of publicly available Russian annotated corpora, the corpus is a much-needed resource that can be utilized for literary and linguistic studies, pedagogy as well as for various Natural Language Processing tasks.	compiler;natural language processing;text corpus	Katsiaryna Aharodnik;Anna Feldman;Jing Peng	2018			natural language processing;artificial intelligence;speech recognition;computer science	NLP	-29.662925054175563	-74.77980576538296	184029
208484515fbf84ae05ceb73ae9c5a54f44aba738	natural language processing across time: an empirical investigation on italian	natural language processing	In this paper, we study how existing natural language processing tools for Italian perform on ancient texts. The first goal is to understand to what extent such tools can be used “as they are” for the automatic analysis of old literary works. Indeed, while NLP tools for Italian achieve today good performance, it is not clear if they could be successfully used for the humanities, to support the critical study of historical works. Our analysis will show how tools’ performance systematically vary across different time periods, and within literary movements. As a second goal, we want to verify whether or not simple customization methods can improve the tools performance over the old works.	natural language processing	Marco Pennacchiotti;Fabio Massimo Zanzotto	2008		10.1007/978-3-540-85287-2_36	humanities;natural language processing;art;literature	ML	-30.997306096836333	-74.29647385159109	184156
d5dc1a6545029b89ca9ff4bfcf39219ee5f7de36	koko: an l1 learner corpus for german		We introduce the KoKo corpus, a collection of German L1 learner texts annotated with learner errors, along with the methods and tools used in its construction and evaluation. The corpus contains both texts and corresponding survey information from 1,319 pupils and amounts to around 716,000 tokens. The evaluation of the performed transcriptions and annotations shows an accuracy of orthographic error annotations of approximately 80% as well as high accuracies of transcriptions (> 99%), automatic tokenisation (> 99%), sentence splitting (> 96%) and POS-tagging (> 94%). The KoKo corpus will be published at the end of 2014. It will be the first accessible linguistically annotated German L1 learner corpus and a valuable source for research on L1 learner language as well as for teachers of German as L1, in particular with regards to writing skills.	orthographic projection;part-of-speech tagging	Andrea Abel;Aivars Glaznieks;Lionel Nicolas;Egon Stemle	2014			natural language processing;speech recognition;computer science;transcription (linguistics);artificial intelligence;sentence;german	NLP	-28.44473055779489	-75.29849836753947	184964
1dba1fa6dd287fde87823218d4f03559dde4e15b	natural language annotations for question answering	natural language;question answering	This paper presents strategies and lessons learned from the use of natural language annotations to facilitate question answering in the START information access system.	information access;natural language;question answering	Boris Katz;Gary C. Borchardt;Sue Felshin	2006			artificial intelligence;programming language;natural language processing;natural language;information access;question answering;computer science	NLP	-31.760685213980288	-77.13070560358209	185213
c8f061d657529864404526679c0f684447581d61	representation of original sense of chinese characters by fopc	conference paper	In Natural Language Processing(NLP), the automatic analysis of meaning occupies a very important position. The representation of original sense of Chinese character plays an irreplaceable role in the processing of advanced units of Chinese language such as the processing of syntax and semantics, etc. This paper, by introducing a few important concepts: FOPC, Ontology and Case Grammar, discusses the representation of original sense of Chinese character. The	first-order logic;natural language	Yajun Pei;Zhiwei Feng	2006			humanities;arithmetic;computer science;traditional medicine	NLP	-31.281877334402232	-79.46720443692212	185933
1e59f19a71d4f3db8bdaf55e6aad019af0964890	middleware for creating and combining multi-dimensional nlp markup	multi-dimensional nlp markup;multi-dimensionally annotated text;shallow processing cascade;xml-based integration scenario;robust linguistic markup;nlp-based application;multilingual natural language processing;gold middleware;robust deep-shallow integration;multi-dimensional markup;information extraction	We present the Heart of Gold middleware by demonstrating three XMLbased integration scenarios where multidimensional markup produced online by multilingual natural language processing (NLP) components is combined to deliver rich, robust linguistic markup for use in NLP-based applications like information extraction, question answering and semantic web. The scenarios include (1) robust deep-shallow integration, (2) shallow processing cascades, and (3) treebank storage of multi-dimensionally annotated texts.	information extraction;markup language;middleware;natural language processing;question answering;semantic web;treebank	Ulrich Schäfer	2006			ruleml;natural language processing;xhtml;lexical markup framework;collaborative application markup language;computer science;database;information retrieval	NLP	-32.4675269282889	-76.55532733470257	186130
0c4197cd9ecb074d8447d9077de8e2a52e05f5e5	stepwise mining of multi-word expressions in hindi	multi-word expression;limited linguistic cue;day-to-day conversation;formal textual corpus;conventional statistical method;adequate attention;linguistic knowledge;mwe identification use corpus;machine translation perspective;stepwise mining;machine translation viewpoint	Multi-word expressions (MWEs) play an important role in all tasks that involve natural language processing. MWEs in Hindi are quite varied and many of these are of the types that are not encountered in English. In this paper, we examine different types of MWEs encountered in Hindi. Many of these have not received adequate attention of investigators. For example, ‘vaalaa’ constructs, doublets (word-pairs), replication, and a variety of verb group forms have not been explored as MWEs. We examine these MWEs from machine translation viewpoint. Many of these are frequently used in day-to-day conversations and informal communication but are not that frequently encountered in a formal textual corpus. Most of the conventional statistical methods for MWE identification use corpus with limited linguistic cues. These are found to be inadequate for detecting all types of MWEs that exist in real life. In this paper, we present a stepwise methodology for mining Hindi MWEs using linguistic knowledge. Interpretation and representation for some of these from machine translation perspective have also been explored.	bootstrapping (compilers);dictionary;interpretation (logic);lexicography;machine translation;minimal working example;natural language processing;real life;semiconductor industry;sensor;stepwise regression	Rai Mahesh Sinha	2011			natural language processing;computer science;linguistics;communication	NLP	-26.537310810557404	-80.00448904566146	186219
91a04226efb4849a4e6516c88d2968c14a5ea328	rasa: open source language understanding and dialogue management		We introduce a pair of tools, Rasa NLU and Rasa Core, which are open source python libraries for building conversational software. Their purpose is to make machine-learning based dialogue management and language understanding accessible to non-specialist software developers. In terms of design philosophy, we aim for ease of use, and bootstrapping from minimal (or no) initial training data. Both packages are extensively documented and ship with a comprehensive suite of tests. The code is available at https://github.com/RasaHQ/	bootstrapping (compilers);dialog system;library (computing);machine learning;natural language understanding;open-source software;python;software developer;usability	Tom Bocklisch;Joey Faulkner;Nick Pawlowski;Alan Nichol	2017	CoRR		artificial intelligence;natural language processing;python (programming language);computer science;bootstrapping;software;usability;training set	SE	-32.9966526989232	-73.88072916052764	186599
2c13675cdcbd6efa5304b0ce9e5caf0ca9875085	dima - annotation guidelines for german intonation	intonation;inter annotator reliability;annotation;guidelines;german intonation annotation guidelines inter annotator reliability;inproceedings;german	This paper presents newly developed guidelines for prosodic annotation of German as a consensus system agreed upon by German intonologists. The DIMA system is rooted in the framework of autosegmental-metrical phonology. One important goal of the consensus is to make exchanging data between groups easier since German intonation is currently annotated according to different models. To this end, we aim to provide guidelines that are easy to learn. The guidelines were evaluated running an inter-annotator reliability study on three different speech styles (read speech, monologue and dialogue). The overall high κ between 0.76 and 0.89 (depending on the speech style) shows that the DIMA conventions can be applied successfully.	dima	Frank Kügler;Bernadett Smolibocki;Denis Arnold;Stefan Baumann;Bettina Braun;Martine Grice;Stefanie Jannedy;Jan Michalsky;Oliver Niebuhr;Jörg Peters;Simon Ritter;Christine T. Röhr;Antje Schweitzer;Katrin Schweitzer;Petra Wagner	2015			natural language processing;speech recognition;computer science;linguistics	NLP	-27.809329778203796	-75.80376855087333	187995
5b0d7636681fd2abf3b9001361f953dd972d75a6	unl based bangla natural text conversion - predicate preserving parser approach		Universal Networking Language (UNL) is a declarative formal language that is used to represent semantic data extracted from natural language texts. This paper presents a novel approach to converting Bangla natural language text into UNL using a method known as Predicate Preserving Parser (PPP) technique. PPP performs morphological, syntactic and semantic, and lexical analysis of text synchronously. This analysis produces a semantic-net like structure represented using UNL. We demonstrate how Bangla texts are analyzed following the PPP technique to produce UNL documents which can then be translated into any other suitable natural language facilitating the opportunity to develop a universal language translation method via UNL.	compiler;declarative programming;dictionary;document classification;experiment;formal language;knowledge representation and reasoning;lexical analysis;natural language;parser;semantic network;text mining;universal networking language	Md. Nawab Yousuf Ali;Shamim Ripon;Shaikh Muhammad Allayear	2012	CoRR		natural language processing;speech recognition;universal networking language;computer science;linguistics;programming language	NLP	-30.80687085391512	-80.16153354108081	188693
30545fe538a773b57e06b4217cd495ef84230bc8	knowledge-free induction of inflectional morphologies	language use;gold standard	We propose an algorithm to automatically induce the morphology of inflectional languages using only text corpora and no human input. Our algorithm combines cues from orthography, semantics, and syntactic distributions to induce morphological relationships in German, Dutch, and English. Using CELEX as a gold standard for evaluation, we show our algorithm to be an improvement over any knowledge-free algorithm yet proposed.	algorithm;eur-lex;galaxy morphological classification;text corpus	Patrick Schone;Daniel Jurafsky	2001		10.3115/1073336.1073360	natural language processing;gold standard;computer science;linguistics	NLP	-27.06381551575755	-76.59742679613967	188994
6831fc7b6335584db6d4b4f6a83695fbd5ba2f76	modeling of the meaning: computational interpreting and understanding of natural language fragments.		In this introductory article we present the basics of an approach to implementing computational interpreting of natural language aiming to model the meanings of words and phrases. Unlike other approaches, we attempt to define the meanings of text fragments in a composable and computer interpretable way. We discuss models and ideas for detecting different types of semantic incomprehension and choosing the interpretation that makes most sense in a given context. Knowledge representation is designed for handling context-sensitive and uncertain / imprecise knowledge, and for easy accommodation of new information. It stores quantitative information capturing the essence of the concepts, because it is crucial for working with natural language understanding and reasoning. Still, the representation is general enough to allow for new knowledge to be learned, and even generated by the system. The article concludes by discussing some reasoning-related topics: possible approaches to generation of new abstract concepts, and describing situations and concepts in words (e.g. for specifying interpretation difficulties).	context-sensitive grammar;context-sensitive language;encode;experiment;knowledge representation and reasoning;natural language understanding;semantic network;sensor;dialog	Michael Kapustin;Pavlo Kapustin	2015	CoRR		natural language processing;artificial intelligence;knowledge representation and reasoning;machine learning;natural language understanding;natural language;computer science	AI	-33.44425331237315	-80.1202023742166	189117
073dc43c478331d6984363093e1131a16677a26c	chunking clinical text containing non-canonical language	qa0075 electronic computers computer science	Free text notes typed by primary care physicians during patient consultations typically contain highly non-canonical language. Shallow syntactic analysis of free text notes can help to reveal valuable information for the study of disease and treatment. We present an exploratory study into chunking such text using offthe-shelf language processing tools and pre-trained statistical models. We evaluate chunking accuracy with respect to partof-speech tagging quality, choice of chunk representation, and breadth of context features. Our results indicate that narrow context feature windows give the best results, but that chunk representation and minor differences in tagging quality do not have a significant impact on chunking accuracy.	microsoft windows;shallow parsing;statistical model	Aleksandar Savkov;John A. Carroll;Jackie Cassell	2014		10.3115/v1/W14-3411	natural language processing;speech recognition;computer science;chunking;communication	NLP	-26.615147217890826	-74.96987403570577	189351
9b19e99a69ba3eadece596b58a054c267c5b21a2	wikidition: automatic lexiconization and linkification of text corpora				Alexander Mehler;Rüdiger Gleim;Tim vor der Brück;Wahed Hemati;Tolga Uslu;Steffen Eger	2016	it - Information Technology	10.1515/itit-2015-0035	computer science;embedded system;natural language processing;artificial intelligence;text corpus;text mining;digital edition	NLP	-31.006477194124358	-77.01625482808	189541
20457595da5415af67d579113f2520fa5d7967ec	a high-performance plagiarism detection system - notebook for pan at clef 2011		In this paper we report on our high-performance plagiarism detection system which is able to process the PAN plagiarism corpus for the external plagiarism detection task within relatively short timescales in contrast to previously reported state-of-the-art, and still produce a reasonable degree of performance (PAN 11, 4 place, PlagDet=0.2467329, Recall=0.1500480, Precision=0.7106536, Granularity=1.0058894). At the core of our system is a simple method which avoids the use of hash-type approaches, but about which we are unable to disclose too many details due to a patent application in progress. We optimised our performance using the PAN10 collection, and used the best parameters for the final submission. We anticipated a relatively similar performance at PAN11, modulo changes to the plagiarism cases, and 4 place this year put us between participants who had been 5 and 6 in PAN 10.	modulo operation	Neil Cooke;Lee Gillam;Peter Wrobel;Henry Cooke;Fahad Al-Obaidli	2011			speech recognition;clef;engineering;plagiarism detection	ML	-31.558445826507217	-73.94575758128246	189772
19789d9346c998fc126879f0cf07e9a138d4e3c7	creating and exploiting multimodal annotated corpora	corpus;annotation	The paper presents a project of the Laboratoire Parole et Langage which aims at collecting, annotating and exploiting a corpus of spoken French in a multimodal perspective. The project directly meets the present needs in linguistics where a growing number of researchers become aware of the fact that a theory of communication which aims at describing real interactions should take into account the complexity of these interactions. However, in order to take into account such a complexity, linguists should have access to spoken corpora annotated in different fields. The paper presents the annotation schemes used in phonetics, morphology and syntax, prosody, gestuality at the LPL together with the type of linguistic description made from the annotations seen in two examples.	galaxy morphological classification;multimodal interaction;semantic prosody;text corpus;theory	Philippe Blache;Roxane Bertrand;Gaëlle Ferré	2008			speech recognition;artificial intelligence;syntax;natural language processing;morphology (linguistics);linguistic description;communication theory;phonetics;prosody;computer science;annotation	NLP	-30.700306784361196	-75.54772656169311	189817
c938b9c65ab2191f9c0c1c9f832fa7ed102d5f01	pilot implementation of a bilingual knowledge bank	referentially structured pair;bilingual knowledge bank;translation unit;pilot implementation;corpus-based general-purpose knowledge source;machine translation;computer-aided translation	A Bilingual Knowledge Bank is a syntactically and referentially structured pair of corpora, one being a translation of the other, in which translation units are cross-codexl between the corpora. A pilot implementation is described for a corpus of some 20,000 words each in English, French and Esperanto which has been cross-coded between English and Esperanto and between Esperanto and French. The aim is to develop a corpus-based general-purpose knowledge source for applications in machine translation and computeraided translation.	bikini karate babes;dictionary;general-purpose modeling;human-readable medium;lexicography;machine translation;reversible computing;text corpus;vocabulary	Victor Sadler;Ronald Vendelmans	1990			computer-assisted translation;natural language processing;speech recognition;example-based machine translation;computer science;linguistics;machine translation;rule-based machine translation;machine translation software usability	NLP	-29.6848042212215	-78.076747224257	190002
241f03e2b5f7e26672f2dc1d4bd47168ecf2483c	automated detection of morphemes using distributional measurements		To simply take the distribution of linguistic elements as a basis for analysis was the methodological prime of researchers of the so-called “American Structuralism”. This paper deals with the detection of morphemes from a large corpus of German by simply applying a distributional procedure of counting the number of potential successors of a given sequence of letters of a word, a method reminiscent of proposals by Harris, Shannon and others. Morphemes can be heuristically read off by an increase in the potential successor count. Three different methods of identifying morpheme breaks are discussed and a proposal for improvement of the method by transforming graphemic to partial phonemic representation is put forward.	distributional semantics	Christoph Benden	2004		10.1007/3-540-28084-7_57	prime (order theory);successor cardinal;natural language processing;morpheme;structuralism;mathematics;heuristic;artificial intelligence;german	NLP	-26.98257839924757	-79.92304766531703	190823
82a4f45da92821cb489b2d86fe1304b81f2d2eff	using umls for word sense disambiguation in clinical notes			word sense;word-sense disambiguation	Anna Rumshisky;Rachel Chasin;Özlem Uzuner;Peter Szolovits	2012			semeval;information retrieval;unified medical language system;word-sense disambiguation;computer science	NLP	-30.839077668110228	-76.83121732903629	190856
22bc12180fdae83025ce81ce9d9cb8bfc6b97853	catégorisation automatique de textes basée sur des hiérarchies de concepts		This paper deals with a method for automatic categorisation of texts according to concept hierarchies that describe a domain. This categorisation is based upon two principal components: – the definition of category representatives resulting from learning, – a voting mechanism in order to determine the most suitable categories for a given document. We evaluate the influence of different parameters on the results including the methods used to select the terms to be added to the category representation. The performances that have been obtained using the Reuters-21578 corpus are reported in this paper. MOTS-CLÉS : Recherche d'information, hiérarchies de concepts, catégorisation automatique.	categorization;performance	Jérôme Augé;Kurt Englmeier;Gilles Hubert;Josiane Mothe	2003			natural language processing;voting;principal component analysis;hierarchy;mathematics;artificial intelligence	Vision	-26.841631842920247	-77.25432623502013	191041
591d2df9ddc1236aa8ee3c52a68858f3f7683ff8	annotonia: annotations from browser to tei		"""The Willa Cather Archive (WCA) at the University of Nebraska-Lincoln (UNL) is currently working on transcription and annotation of 1500 letters to be released in 2018. As editors will write several thousand annotations, the workflow logistics are complicated. Annotonia ( a portmanteau of """"annotation"""" and My Ántonia, a 1918 Willa Cather novel) is a solution developed within the Center for Digital Research in Humanities (CDRH) that allows editors to write annotations directly on letters in a browser and insert those annotations into Text Encoding Initiative (TEI) XML files. Multiple editors review annotations, track letters’ annotation statuses, and generate a new TEI file incorporating the annotations, avoiding having to manually edit each file. Annotonia utilizes both pre-existing, customized open source software and new software developed for this project. This paper describes the difficulties faced, the workflow of Annotonia, and its prospects for future annotation work."""	archive;java annotation;logistics;open-source software;text encoding initiative;transcription (software);universal networking language;xml	Gregory John Tunink;Karin Dalziel;Jessica Dussault;Emily Rau	2017				NLP	-33.36226370093118	-74.87424188566943	191240
1f4f83cb41994128440c5458eda2339ba2a568df	build a large-scale syntactically annotated chinese corpus	tratamiento lenguaje;chino;large scale;language processing;traitement langage;analizador sintaxico;parser;chinois;chinese;analyseur syntaxique	This paper reports on our research to build a large-scale Tsinghua Chinese Treebank (TCT). We propose a two-stage approach to reduce manual proofreading labors as much as possible. The insertion of an intermediate functional chunk level creates a good information bridge to link simple chunk annotation with detailed syntactic tree annotation. We describe our chunk and tree annotation schemes, focus on two grammatical relation tag sets designed to give more detailed description for most of the special language phenomena in the Chinese language. We also briefly introduce our current progress in building a Chinese chunk bank with 2,000,000 Chinese characters, developing an efficient Chinese chunk-based parser and building a 1,000,000 words Chinese treebank. All this work lays good foundations for further research project to build a good Chinese parser.	chinese wall;chunking (computing);the coroner's toolkit;treebank	Qiang Zhou	2003		10.1007/978-3-540-39398-6_15	natural language processing;speech recognition;computer science;linguistics;chinese	NLP	-29.586129274912157	-75.88034835924293	191478
ab72bb9ed71e0634ce3534ad59f4cef9572f5608	joe loves lea: transformational analysis of direct transitive sentences		NooJ is capable of both parsing and producing any sentence that matches a given syntactic grammar. We use this functionality to describe direct transitive sentences, and we show that this simple structure of sentence accounts for millions of potential sentences.		Max Silberztein	2015		10.1007/978-3-319-42471-2_5	psychology;speech recognition;linguistics;cognitive science	NLP	-31.498404931551207	-80.03660162574604	191482
dced8c26e14672c67679cc1f84a8799bef1b2710	improving term extraction by system combination using boosting	evaluation performance;evaluation systeme;linguistique;donnee textuelle;analisis estadistico;performance evaluation;extracteur terme;dato textual;evaluacion prestacion;medicina;evaluacion sistema;medecine;system evaluation;linguistica;boosting;statistical analysis;automatic detection;analyse statistique;textual data;term extraction;medicine;learning artificial intelligence;apprentissage intelligence artificielle;linguistics	Term extraction is the task of automatically detecting, from textual corpora, lexical units that designate concepts in thematically restricted domains (e.g. medicine). Current systems for term extraction integrate linguistic and statistical cues to perform the detection of terms. The best results have been obtained when some kind of combination of simple base term extractors is performed 14]. In this paper it is shown that this combination can be further improved by posing an additional learning problem of how to nd the best combination of base term extrac-tors. Empirical results, using AdaBoost in the metalearning step, show that the ensemble constructed surpasses the performance of all individual extractors and simple voting schemes, obtaining signiicantly better accuracy gures at all levels of recall.	adaboost;sensor;terminology extraction;text corpus	Jorge Vivaldi;Lluís Màrquez i Villodre;Horacio Rodríguez	2001		10.1007/3-540-44795-4_44	speech recognition;computer science;artificial intelligence;machine learning;boosting;algorithm	NLP	-26.554318905897848	-77.27946668687473	192204
b1ddc48f032e6de6d61f976d1133500bb56d2cdc	stemming tigrinya words for information retrieval		The increasing penetration of internet into less developed countries has resulted in the increase in the number of digital documents written in many minor languages. However, many of these languages have limited resources in terms of data, language resources and computational tools. Stemming is the reduction of inflected word forms into common basic form. It is an important analysis process in information retrieval and many natural language processing applications. In highly inflected languages such as Tigrinya, stemming is not always straightforward task. In this paper we present the development of stemmer for Tigrinya words to facilitate the information retrieval. We used a hybrid approach for stemming that combines rule based stemming which removes affixes in successively applied steps and dictionary based stemming which reduces stemming errors by verifying the resulting stem based on word distance measures. The stemmer was evaluated using two sets of Tigrinya words. The results show that it achieved an average accuracy of 89.3%.	brill tagger;dictionary;information retrieval;internet;natural language processing;part-of-speech tagging;stemming;verification and validation;web search engine	Omer Osman;Yoshiki Mikami	2012			natural language processing;computer science;stemming;information retrieval;algorithm	Web+IR	-28.72013770041223	-73.97377782403915	192820
bb08e8e57d6c81009b6f032e942838573a66901e	low resource methods for medieval document sections analysis		This paper describes a small but unique digitized collection of medieval Latin charters. This collection consists of 57 charters of 7 types illustrating various purposes of issuance by the Royal Chancellery. Sections in these documents were manually annotated for deeper analysis of the structure of issued charters. This paper also describes two baseline methods for an automatic and semi-automatic analysis and detection of sections of diplomatic documents. The first method is based on an information retrieval paradigm, and the second one is an adaptation of Hidden Markov Models. Both methods were proposed to work with respect to a small amount of available train data. Even though these methods were specifically proposed to work with medieval Latin charters, they can be applied to any documents with partially repetitive character.	baseline (configuration management);document;hidden markov model;information retrieval;markov chain;programming paradigm;semiconductor industry	Petra Galuscáková;Lucie Neuzilova	2018			natural language processing;speech recognition;artificial intelligence;computer science	Web+IR	-30.02358067547482	-76.27620928194195	193700
7c89cbf5d860819c9b5e5217d079dc8aafcba336	recognizing subjectivity: a case study in manual tagging	wall street journal;sentence-level categorization;tagging instruction;recognizing subjectivity;empirical support;basic semantic class;manual tagging;subjective category;case study;final classification	In this paper, we describe a case study of a sentence-level categorization in which tagging instructions are developed and used by four judges to classify clauses from the Wall Street Journal as either subjective or objective. Agreement among the four judges is analyzed, and, based on that analysis, each clause is given a nal classiication. To provide empirical support for the classiications, correlations are assessed in the data between the subjective category and a basic semantic class posited by Quirk et al. (1985).	categorization;quirks mode;the wall street journal	Rebecca F. Bruce;Janyce Wiebe	1999	Natural Language Engineering		natural language processing;artificial intelligence;data mining	NLP	-27.051412729669654	-74.64211817658503	193728
a1db607fcc28587c0a5929ee20ad4ed80153a557	a tool for corpus analysis using partial disambiguation and bootstrapping of the lexicon			bootstrapping (compilers);lexicon;word-sense disambiguation	Kurt Eberle;Ulrich Heid;Manuel Kountz;Kerstin Eckart	2008			bootstrapping;natural language processing;lexicon;artificial intelligence;computer science;pattern recognition	NLP	-29.881121048312117	-77.4502539808035	194123
4c80ec59cb455c902b912de33a577b563a7a2d13	a one-pass search algorithm for continuous speech recognition directed by context-free phrase structure grammar	search algorithm		context-free language;phrase structure grammar;phrase structure rules;search algorithm;speech recognition	Michio Okada	1990			phrase structure grammar;artificial intelligence;speech recognition;pattern recognition;natural language processing;computer science;search algorithm	NLP	-27.486376029610927	-79.74798722639476	194177
c9cee2872771931cb392f49d11bd955d296fca71	translating cross-lingual spelling variants using transformation rules	busqueda informacion;use;transliteration;translating;analisis estadistico;information retrieval;cambio;logique floue;logica difusa;fuzzy logic;change;fuzzy matching;translitteration;traduction;utilisation;statistical analysis;recherche information;target language;uso;proper names;cross language retrieval;analyse statistique;transcripcion;source language;changement;traduccion;multilinguisme;cross language information retrieval;multilingualism;multilinguismo	Technical terms and proper names constitute a major problem in dictionary-based crosslanguage information retrieval (CLIR). However, technical terms and proper names in different languages often share the same Latin or Greek origin, being thus spelling variants of each other. In this paper we present a novel two-step fuzzy translation technique for cross-lingual spelling variants. In the first step, transformation rules are applied to source words to render them more similar to their target language equivalents. The rules are generated automatically using translation dictionaries as source data. In the second step, the intermediate forms obtained in the first step are translated into a target language using fuzzy matching. The effectiveness of the technique was evaluated empirically using five source languages and English as a target language. The two-step technique performed better, in some cases considerably better, than fuzzy matching alone. Even using the first step as such showed promising results.	bilingual dictionary;compiler;cross-language information retrieval;fuzzy concept;source data	Jarmo Toivonen;Ari Pirkola;Heikki Keskustalo;Kari Visala;Kalervo Järvelin	2005	Inf. Process. Manage.	10.1016/j.ipm.2004.02.001	fuzzy logic;natural language processing;speech recognition;approximate string matching;transliteration;computer science;proper noun;linguistics	NLP	-26.800249057812138	-77.77847525419563	194258
4533cd8567c944298ec7edc7c8e58d1e1817a511	senseval-2 the swedish framework		In this paper we describe the organisation and results of the SENSEVAL-2 exercise for Swedish. We present some of the experiences we gained by participating as developers and organisers in the exercise. We particularly focus on the choice of the lexical and corpus material, the annotation process, the scoring scheme, the motivations for choosing the lexical-sample branch of the exercise, the participating systems and the official results.	text corpus	Dimitrios Kokkinakis;Jerker Järborg;Yvonne Cederholm	2001			simulation;computer science;knowledge management;multimedia	NLP	-29.609000243021143	-74.71710742418169	194386
e14772394508b874b03603f11bf1f3a115896db0	measures of word commonness		The main goal of this paper is to investigate methods of how to rank words in a way that corresponds to an intuitive notion of ‘commonness’. Since there is no formal definition of such a notion, our techniques may be considered as a suggestion for such a definition. The commonness of words is sometimes roughly substituted with their frequency in a language corpus. In order to suggest a better measure, we define a quantity, which we call corrected frequency. It depends not only on the frequency of a word in a corpus, but also on its distribution within the corpus. Unlike previous solutions of the same problem, we take the corpus as an uninterrupted sequence of words with no regard to borders between files, texts, genres, or any others. We introduce three different corrected frequencies. Their definitions are based on notions of information theory and analysis of random processes. Their values for individual words depend on the corpus. Hence, it is important to what extent they are stable with respect to th...		Petr Savický;Jaroslava Hlavácová	2002	Journal of Quantitative Linguistics	10.1076/jqul.9.3.215.14124	natural language processing;mathematics;linguistics	NLP	-27.335059053229557	-75.35464903799813	194482
5279148d2182c3e091203afa200acdd8da1020f6	combining semantic annotation of word sense & semantic roles: a novel annotation scheme for verbnet roles on german language data		We present a VerbNet-based annotation scheme for semantic roles which we explore in an annotation study on German language data that combines word sense and semantic role annotation. We reannotate a substantial portion of the SALSA corpus with GermaNet senses and a revised scheme of VerbNet roles. We provide a detailed evaluation of the interaction between sense and role annotation. The resulting corpus will allow us to compare VerbNet role annotation for German to FrameNet and PropBank annotation by mapping to existing role annotations on the SALSA corpus. We publish the annotated corpus and detailed guidelines for the new role annotation scheme.	framenet;germanet;propbank;salsa;text corpus;verbnet;word sense	Éva Mújdricza-Maydt;Silvana Hartmann;Iryna Gurevych;Anette Frank	2016			natural language processing;artificial intelligence;word sense;semantic computing;information retrieval;verbnet;semantic role labeling;temporal annotation;computer science;german;annotation	NLP	-30.28944555933051	-75.65845155560152	194559
2be025804af1dc232188078928591860ad1343b9	corpus based enrichment of germanet verb frames	lexical semantics;artificial intelligent;natural language;document processing	Lexical semantic resources, like WordNet, are often used in real applications of natural language document processing . For example, we integrated GermaNet in our document suite XDOC. In additi on to hypernymy and synonymy relations, we want to exploit Ge rmaNet verb frames for our analysis. In this paper, we outline an app roach for the domain related enrichment of GermaNet verb fra mes by corpus based syntactic and co-occurrence data analyses of real doc uments.	document processing;gene ontology term enrichment;germanet;natural language;wordnet	Manuela Kunze;Dietmar F. Rösner	2004	CoRR		natural language processing;lexical semantics;speech recognition;document processing;computer science;linguistics;natural language	NLP	-30.696161720370494	-76.54997889292996	194654
292a4ece480bcfcddfc9dc625f57f9672daffb81	philippine language resources: trends and directions	speech system;sign language processing;speech tagging;various form;language resource;philippine language resource;formal representation;language grammar;philippine language;various human language;language representation	We present the diverse research activities on Philippine languages from all over the country, with focus on the Center for Language Technologies of the College of Computer Studies, De La Salle University, Manila, where majority of the work are conducted. These projects include the formal representation of Philippine languages and the processes involving these languages. Language representation entails the manual and automatic development of language resources such as lexicons and corpora for various human languages including Philippine languages, across various forms such as text, speech and video files. Tools and applications on languages that we have worked on include morphological processes, part of speech tagging, language grammars, machine translation, sign language processing and speech systems. Future directions are also presented.	language technology;lexicon;linear algebra;machine translation;part-of-speech tagging;text corpus;video file format	Rachel E. O. Roxas;Charibeth Ko Cheng;Nathalie Rose Lim	2009			natural language processing;manually coded language;speech recognition;language interpretation;computer science;linguistics;ontology language;natural language;second-generation programming language	NLP	-33.215348994685165	-76.44191956811167	194718
5829bd059389e02c122099e1b44e7b0a114bb137	grammar-based tools for the creation of tagging resources for an unresourced language: the case of northern sotho		We describe an architecture for the parallel construction o f a tagger lexicon and an annotated reference corpus for the p art-of-speech tagging of Nothern Sotho, a Bantu language of South Africa, f or which no tagged resources have been available so far. Our t ools make use of grammatical properties (morphological and synt actic) of the language. We use symbolic pretagging, followe d by stochastic tagging, an architecture which proves useful not only for th e bootstrapping of tagging resources, but also for the taggi ng of any new text. We discuss the tagset design, the tool architecture and the c urr nt state of our ongoing effort.	brill tagger;lexicon	Ulrich Heid;Elsabé Taljard;Danie J. Prinsloo	2006			architecture;artificial intelligence;syntax;natural language processing;bootstrapping;computer science;linguistics;lexicon;bantu languages;grammar	NLP	-29.94682900881076	-75.9588206817707	194800
69ea2a94de722b5be238b0496d3b6fe512fdbb33	dealing with interpretation errors in tutorial dialogue	computer system;interpretation error;tutorial dialogue system;dialogue system;tutorial dialogue;informationseeking domain;contentful talk;current technology	We describe an approach to dealing with interpretation errors in a tutorial dialogue system. Allowing students to provide explanations and generate contentful talk can be helpful for learning, but the language that can be understood by a computer system is limited by the current technology. Techniques for dealing with understanding problems have been developed primarily for spoken dialogue systems in informationseeking domains, and are not always appropriate for tutorial dialogue. We present a classification of interpretation errors and our approach for dealing with them within an implemented tutorial dialogue system.	computer;dialog system;dialog tree	Myroslava O. Dzikovska;Charles B. Callaway;Elaine Farrow;Johanna D. Moore;Natalie B. Steinhauser;Gwendolyn E. Campbell	2009			natural language processing	NLP	-33.61586799881018	-80.03188613891179	194862
732deae520bf19796b88d5ec01165667209e31e2	integration of lexical and syntactical knowledge in a handwriting-recognition system	linguistique;handwriting recognition;caracter manuscrito;implementation;lexicon;manuscript character;ejecucion;analyse syntaxique;linguistica;automatic recognition;reconnaissance caractere;analisis sintaxico;syntactic analysis;lexico;caractere manuscrit;character recognition;reconocimiento caracter;reconocimiento automatico;reconnaissance automatique;lexique;linguistics	This article presents a research project carried out with the aim of investigating the improvements in recognition performances that result from the use of linguistic information in a handwriting-recognition system. The purpose of the study was to design a postprocessor that would enhance an existing handwriting-recognition system by identifying and correcting words it did not recognize initially. This was done by integrating linguistic information (both lexical and syntactical) into the system. Every sentence containing one or more incorrect words is parsed and all possible grammatical classes for each incorrect word are listed. Then, a lexical enquiry searches for words in the lexicon corresponding to the grammatical class of the word in question. Finally, a string-comparison algorithm selects only the words in the lexicon that are close to the incorrect word. The results of this experimentation show that such a system is more efficient in correcting words (even highly distorted ones) than conventional systems that only integrate lexical information. In conclusion, the integration of linguistic information to correct words not recognized by a handwritingrecognition system is shown to be an effective approach, and one that might be worth pursuing.	algorithm;lexicon;parsing;performance	Stéphanie Clergeau-Tournemire;Réjean Plamondon	1995	Machine Vision and Applications	10.1007/BF01219593	natural language processing;speech recognition;lexical item;computer science;lexical chain;parsing;handwriting recognition;implementation;stop words	NLP	-27.00718229210958	-78.14849279258952	195267
16f86c48f50038be5cc7be8baf676fefe64932e0	symbolic word clustering for medium-size corpora	symbolic word;industrial company;technical corpus;different robust parsers;parse tree;alternative method;medium-size corpus;statistical method;nominal phrase;essential concept	"""When trying to identify essential concepts and relationships in a medium-size corpus, it is not always possible to rely on statistical methods, as the frequencies are too low. We present an alternative method, symbolic, based on the simplification of parse trees. We discuss the resuits on nominal phrases of two technical corpora, analyzed by two different robust parsers used for terminology updating in an industrial company. We compare our results with Hindle's scores of similarity. S u b j e c t s Clustering, ontology development, robust parsing, knowledge acquisition from corpora, computational terminology 1 I d e n t i f y i n g w o r d c l a s s e s i n m e d i u m s i z e c o r p o r a In companies with a wide range of activities, such as EDF, the French electricity company, the rapid evolution of technical domains, the huge amount of textual data involved, its variation in length and style imply building or updating numerous terminologies as NLP resources. In this context, terminology acquisition is defined as a twofold process. On one hand, a terminologist must identify the essential entities of the domain and their relationships, that is its ontology. On the other hand, (s)he must relate these entities and relationships to their linguistic realizations, so as to isolate the lexical entries to be considered as certified terms for the domain. In this paper, we concentrate on the first issue. Automatic exploration of a sublanguage corpus constitutes a first step towards identifying the semantic classes and relationships which are relevant for this sublanguage. In the past five years, important research on the automatic acquisition of word classes based on lexical distribution has been published (Church and Hanks, 1990; Hindle, 1990; Smadja, 1993; Grei~nstette, 1994; Grishman and Sterling, 1994). Most of these approaches, however, need large or even very large corpora in order for word classes to be discovered 1 whereas it is often the case that the data to be processed are insufficient to provide reliable lexical intbrmation. In other words, it is not always possible to resort to statistical methods. On the other hand, medium size corpora (between 100,000 and 500,000 words: typically a reference manual) are already too complex and too long to rely on reading only, even with concordances. For this range of corpora, a pure symbolic approach, which recycles and simplifies analyses produced by robust parsers in order to classify words, offers a viable alternative to statistical methods. We present this approach in section 2. Section 3 describes the results on two technical corpora with two different robust parsers. Section 4 compares our results to Itindle's ones (Hindle, 1990). 2 S i m p l i f y i n g p a r s e t r e e s t o c l a s s i f y w o r d s 2.1 T h e n e e d for n o r m a l i z e d s y n t a c t i c c o n t e x t s As Hindle's work proves it, among others (Grishman and Sterling, 1994; Grefenstette, 1994:), the mere existence of robust syntactic parsers makes it possible to parse large corpora in order to automate the discovery of syntactic patterns in the spirit of Harris's distributional hypothesis. Itowever, Harris' methodology implies also to simplify and transform each parse tree 2 , so as to obtain so-called """"elementary sentences"""" exhibiting the main conceptual classes for the domain (Sager lIa'or instance, Hindle (Hindle, 1990) needs a six million word corpus in order to extract noun similarities from predicate-argunlent structures. 2Changing passive into active sentences, using a verb instead of a nominalization, and so on."""	cluster analysis;distributional semantics;earliest deadline first scheduling;emoticon;entity;harris affine region detector;knowledge acquisition;natural language processing;parse tree;parsing;patrick hanks;sublanguage;symbolic computation;technical standard;text corpus	Benoit Habert;Elie Naulleau;Adeline Nazarenko	1996			natural language processing;computer science;pattern recognition;data mining;linguistics	NLP	-29.52739026151807	-73.74484531122451	195538
0419e9a290986dced926addbf3c610a33968edb8	a system for generating cloze test items from russian-language text		This paper studies the problem of automated educational test generation. We describe a procedure for generating cloze test items from Russian-language text, which consists of three steps: sentence splitting, sentence filtering, and question generation. The sentence filtering issue is discussed as an application of automatic summarization techniques. We describe a simple experimental system which implements cloze question generation and takes into account grammatical features of the Russian language such as gender and number.	automatic summarization;experimental system;texture filtering;the sentence	Andrey Kurtasov	2013			natural language processing;speech recognition;computer science;linguistics	NLP	-28.35380668663935	-75.74909626632426	196106
1c40809d173d7482df2440262feb96c357a6e0b1	a tableau prover for natural logic and language		Modeling the entailment relation over sentences is one of the generic problems of natural language understanding. In order to account for this problem, we design a theorem prover for Natural Logic, a logic whose terms resemble natural language expressions. The prover is based on an analytic tableau method and employs syntactically and semantically motivated schematic rules. Pairing the prover with a preprocessor, which generates formulas of Natural Logic from linguistic expressions, results in a proof system for natural language. It is shown that the system obtains a comparable accuracy (≈81%) on the unseen SICK data while achieving the stateof-the-art precision (≈98%).	automated theorem proving;consortium;decision problem;emoticon;lifting scheme;long division;method of analytic tableaux;natural language understanding;preprocessor;proof calculus;schematic;wordnet;monotone	Lasha Abzianidze	2015			natural language processing;programming language;algorithm	NLP	-28.15614190803625	-79.6283157639179	196380
48d00e89db98cce0a7ec017e5a73867ebd717268	anaphora for limited domain systems	human performance;user needs;english language;noun phrase;performance human;anaphora resolution;data bases;functional equivalence;set theory;computer programming;man computer interface;parsers;cognitive modelling;natural language;artificial intelligence;algorithms;man machine interface;comprehension	"""This paper presents a simple mechanism for the resolution of anaphora in limited domain natural language systems. For such domains, this mechanism provides functionality equivalent to the natural communication mechanism of anaphora as used and understood by people, but without the deep inferencing or cognitive modelling required for full simulation of human performance. The mechanism covers simple pronoun anaphora, and set selection anaphora (e.g. """"last one"""", """"one before"""", """"others""""). It was developed to provide the most efficient and effective communication between system and user, even if this meant diverging significantly from human performance when this performance was impractical to reproduce. In cases of radical divergence, we were careful to make the behaviour of the mechanism very simple and easy to predict. In this way, the user can rely either on his experience of human performance or his knowledge of the artificial, but simple, substitute to predict the behaviour of the system m response to his inputs. An algorithmic description of an implemented version of the mechanism is presented. A similar approach to other aspects of man-machine interfaces is recommended as a promising way to address the problem of habitabiiity that still plagues all natural language computer interfaces."""	algorithm;anaphora (linguistics);ccir system m;cognitive model;human reliability;human–computer interaction;natural language;simulation;stumbleupon;vergence	Philip J. Hayes	1981			natural language processing;comprehension;noun phrase;computer science;artificial intelligence;english;machine learning;computer programming;linguistics;natural language;programming language;set theory	Web+IR	-33.500720111377866	-80.0481806122891	196514
0a289992547c873cb0909852af570514a4123bf4	extracting information for automatic indexing of multimedia material	syntactic analysis;information extraction;indexation	This paper discusses our work on information extraction (IE) from multi-lingual, multi-media, multi-genre Language Resources, in a domain where there are many different event types. This work is being carried out in the context of MUMIS, an EU-funded project that aims at the development of basic technology for the creation of a composite index from multiple and multi-lingual sources. Our approach to IE relies on a finite state machinery provided by GATE, a General Architecture for Text Engineering, pipelined with full syntactic analysis and discourse interpretation implemented in Prolog.	composite index (database);gate;information extraction;parsing;prolog	Horacio Saggion;Hamish Cunningham;Diana Maynard;Kalina Bontcheva;Oana Hamza;Cristian Ursu;Yorick Wilks	2002			natural language processing;information extraction;composite index;artificial intelligence;architecture;information retrieval;automatic indexing;parsing;computer science;indexation;prolog	NLP	-32.0126135898399	-76.29339068629459	196544
9c2d1e7d6c2019e3467384d6b02e34c273256429	realization of discourse relations by other means: alternative lexicalizations	additional expression;syntactic paraphrase method;syntactic type;discourse relation;lexicalized discourse relation annotation;well-defined syntactic class;penn discourse treebank;discourse relation marker;alternative lexicalizations;altlex annotation	Studies of discourse relations have not, in the past, attempted to characterize what serves as evidence for them, beyond lists of frozen expressions, or markers, drawn from a few well-defined syntactic classes. In this paper, we describe how the lexicalized discourse relation annotations of the Penn Discourse Treebank (PDTB) led to the discovery of a wide range of additional expressions, annotated as AltLex(alternative lexicalizations ) in the PDTB 2.0. Further analysis of AltLex annotation suggests that the set of markers is openended, and drawn from a wider variety of syntactic types than currently assumed. As a first attempt towards automatically identifying discourse relation markers, we propose the use of syntactic paraphrase methods.	automatic identification and data capture;categorization;daisy digital talking book;discourse relation;expression (computer science);treebank	Rashmi Prasad;Aravind K. Joshi;Bonnie L. Webber	2010			natural language processing;computer science;linguistics	NLP	-28.651572608455666	-74.94276343902322	196638
0848e7157303b1777c510666fa6a1e799046c18b	statistics-based lexical choice for nlg from quantitative information		We discuss a fully statistical approach to the expression of quantitative information in English. We outline the approach, focussing on the problem of Lexical Choice. An initial evaluation experiment suggests that it is worth investigating the method further.	emoticon;fuzzy logic;lexical choice;natural language generation;regular expression;vagueness	Xiao Li;Kees van Deemter;Chenghua Lin	2016			artificial intelligence;natural language processing;computer science;lexical choice	NLP	-28.744378901604982	-77.28722219466593	196718
30154464f549643e825ccf60072a17a3e55291d3	adaptive multilingual sentence boundary disambiguation	traitement automatique des langues naturelles;punctuation;sentence boundary;linguistique appliquee;segmentation;sentence;automatic recognition;machine learning;language processing;francais;frontiere de phrase;part of speech;allemand;ponctuation;phrase;computational linguistics;desambiguisation;multilinguisme;linguistique informatique;natural language processing;multilingualism;reconnaissance automatique;applied linguistics;systeme satz	The sentence is a standard textual unit in natural language processing applications. In many languages the punctuation mark that indicates the end-of-sentence boundary is ambiguous; thus the tokenizers of most NLP systems must be equipped with special sentence boundary recognition rules for every new text collection. As an alternative, this article presents an efficient, trainable system for sentence boundary disambiguation. The system, called Satz, makes simple estimates of the parts of speech of the tokens immediately preceding and following each punctuation mark, and uses these estimates as input to a machine learning algorithm that then classifies the punctuation mark. Satz is very fast both in training and sentence analysis, and its combined robustness and accuracy surpass existing techniques. The system needs only a small lexicon and training corpus, and has been shown to transfer quickly and easily from English to other languages, as demonstrated on French and German.	algorithm;lexicon;machine learning;natural language processing;sentence boundary disambiguation;text corpus;the sentence;word-sense disambiguation	David D. Palmer;Marti A. Hearst	1997	Computational Linguistics		natural language processing;speech recognition;part of speech;computer science;computational linguistics;sentence boundary disambiguation;applied linguistics;linguistics;punctuation;segmentation	NLP	-27.20143431438668	-78.14015072749297	196795
c6cb80858513c3075bfba5bf296cdde6ac5fbc50	enrichissement du ftb : un treebank hybride constituants/propriétés (enriching the french treebank with properties) [in french]		Enriching the French Treebank with Properties We present in this paper the hybridation of the French Treebank with Property Grammars annotations. This process consists in acquiring a PG grammar from the source treebank and generating the new syntactic encoding on top of the original one. The result is a new resource for French, opening the way to new tools and descriptions. MOTS-CLÉS : Treebank hybride, French Treebank, Grammaires de Propriétés.	attachments;bibliothèque de l'école des chartes;categorial grammar;coherence (physics);computation;computational linguistics;council for educational technology;deep near infrared survey of the southern sky;geforce 6 series;les trophées du libre;linear algebra;nouvelle ai;p (complexity);shallow parsing;text corpus;treebank	Philippe Blache;Stéphane Rauzy	2012				NLP	-30.16608776944991	-78.41249992781457	197088
ae61fef73a8134389dbe26d11c1d7505a491fad3	towards replicability in parsing		We investigate parsing replicability across 7 languages (and 8 treebanks), showing that choices concerning the use of grammatical functions in parsing or evaluation and the influence of the rare word threshold, as well as choices in test sentences and evaluation script options have considerable and often unexpected effects on parsing accuracies. All of those choices need to be carefully documented if we want to ensure replicability.	parsing;treebank	Daniel Dakota;Sandra Kübler	2017		10.26615/978-954-452-049-6_026	artificial intelligence;natural language processing;machine learning;computer science;parsing	NLP	-27.803769927711368	-74.60074801684918	197278
9bcb489cfe993bde6167f1b54de1d63fa4da6efe	on classifying coherent/incoherent romanian short texts.	machine learning;high school;genre classification;text categorization	In this paper we present and discuss the results of a text coherence experiment performed on a small corpus of Romanian text from a number of alternative high school manuals. During the last 10 years, an abundance of alternative manuals for high school was produced and distributed in Romania. Due to the large amount of material and to the relative short time in which it was produced, the question of assessing the quality of this material emerged; this process relied mostly of subjective human personal opinion, given the lack of automatic tools for Romanian. Debates and claims of poor quality of the alternative manuals resulted in a number of examples of incomprehensible / incoherent paragraphs extracted from such manuals. Our goal was to create an automatic tool which may be used as an indication of poor quality of such texts. We created a small corpus of representative texts from Romanian alternative manuals. We manually classified the chosen paragraphs from such manuals into two categories: comprehensible/coherent text and incomprehensible/incoherent text. We then used different machine learning techniques to automatically classify them in a supervised manner. Our approach is rather simple, but the results are encouraging.	coherent;machine learning;supervised learning;text corpus	Anca Dinu	2008			natural language processing;speech recognition;computer science;pattern recognition	NLP	-27.802931812445408	-73.9420908782157	197346
a7439179eab7c1998b2b6513157d0df480972162	editorial: symbol processing in connectionist systems			connectionism	Antony Browne	2000	Expert Systems	10.1111/1468-0394.00122	natural language processing;speech recognition;machine learning	AI	-30.845489358942963	-78.90991722275169	197562
80d3a4c1c25ad11dccbaf0b6bd53645d19ddb112	"""correspondence - """"automatic abstracting research at cas"""" - response to letter"""				Newell E. Gilmour	1976	Journal of Chemical Information and Computer Sciences	10.1021/ci60005a601	computer science;information retrieval;algorithm	Theory	-31.758079950010313	-77.12204164927245	197840
b0331525e233b1c83f7be3d9e087db6e312a8b29	names in novels: an experiment in computational stylistics		Proper names in literary texts have different functions. The most important one in real life, identification, is only one of these. Some others are to make the fiction more ‘real’ or to present ideas about a character by using a name with certain meanings or associations to manipulate the reader’s expectations. A description of the functions of a certain name in a certain text becomes relevant when the researcher can point out how it compares to the functions of other names and names in other texts. The article describes how research into names in literary texts needs a quantitative approach to reach a higher level of relevancy. To get a first impression of what may be normal in literary texts, a corpus of twenty-two Dutch and twenty-two English novels and ten translations into the other language in both sets were gathered. The occurrences of all names in these novels have been tagged for those data categories that seemed useful for the literary stylistic research planned. Some first results of the statistics are presented and the use of the approach is illustrated by means of an analysis of the use of geographical names in the Dutch novel Boven is het stil by Gerbrand Bakker and its English translation by David Colmer, The Twin. In the evaluation of the results, special attention is paid to the status of currently available digital tools for named entity recognition and classification, followed by a wish-list for the tools that this kind of research really needs. .................................................................................................................................................................................	categorization;computation;hall-effect thruster;named-entity recognition;perl;real life;relevance;stylometry;tag (metadata);text corpus;web service	Karina van Dalen-Oskam	2011		10.1093/llc/fqs007	stylistics;linguistics;literature	NLP	-31.29946854270204	-74.24748732232919	197853
a7ffd37d138c98e3aee6f9b2b4f197ca4c236ab8	towards natural natural language processing	ambiguous nlp;natural nlp	An essay about mimicking some aspects of language processing innour heads, using information fusion and competing patterns.	natural language processing	Petr Sojka	2008			natural language processing;language identification;computer science;linguistics;communication	ML	-30.924672116555342	-79.40237802920389	198278
cf61727c5e5c8441bbaac2fbcd954a4ea140d5c2	automatic proofreading of frozen phrases in german	isolated word;grammar checking;frozen phrase;standard level;new level	"""[""""rozen phrases are introduced as a new level of automatic proofieadiltg in between the stm,dard level of spelling verification tfl ist)lated words and tile levcl of grammar checking. The design and the iulplenlentatioll of a corresponding proofleading system are described in detail."""	grammar checker	Ralf Kese;Friedrich Dudda;Marianne Kugler	1992			natural language processing;speech recognition;computer science;linguistics	NLP	-28.366949613641463	-78.40579238330972	198367
27b9a170056c7c1073d729dc891475e1497487b7	lexicalising word order constraints for implemented linearisation grammar	lexicalising word order constraint;lexicalised hpsg grammar;word order;computational parsing system;parsing algorithm;key proposal;word order constraints;word order constraint;additional layer;linearisation grammar;word order domain	This paper presents a way in which a lexicalised HPSG grammar can handle word order constraints in a computational parsing system, without invoking an additional layer of representation for word order, such as Reape’s Word Order Domain. The key proposal is to incorporate into lexical heads the WOC (Word Order Constraints) feature, which is used to constrain the word order of its projection. We also overview our parsing algorithm.	algorithm;head-driven phrase structure grammar;parsing;wan optimization	Yo Sato	2006			word order;natural language processing;speech recognition;computer science;linguistics	NLP	-28.560822200066955	-79.60982876176074	198683
226eba12abad67cc75db9c03d72b1b5bf717450a	lexicalized grammar acquisition	annotated corpus;lexicalized grammar acquisition;consistent acquisition;automatic grammar acquisition;lexicalized grammar formalisms;unique lexicalized grammar	This paper presents a formalization of automatic grammar acquisition that is based on lexicalized grammar formalisms (e.g. LTAG and HPSG). We state the conditions for the consistent acquisition of a unique lexicalized grammar from an annotated corpus.	attribute grammar;categorial grammar;head-driven phrase structure grammar;heuristic;indexed grammar;natural language processing;text corpus	Yusuke Miyao;Takashi Ninomiya;Jun'ichi Tsujii	2003			natural language processing;id/lp grammar;generative grammar;categorial grammar;link grammar;operator-precedence grammar;affix grammar;stochastic grammar;emergent grammar;linguistics;relational grammar;attribute grammar;adaptive grammar;lexical functional grammar;mildly context-sensitive grammar formalism;head-driven phrase structure grammar	NLP	-28.91145989376959	-78.04693497561686	198798
80d4e5088a783540ce1e9cc31d00126843634275	the sounds of the psalter: computational analysis of phonological parallelism in biblical hebrew poetry			computation	Drayton Callen Benner	2013			poetry;literature;art;biblical hebrew	NLP	-31.50682132606175	-78.47593451983444	199287
21d2e1e0cbe2a96b78628e0fb561c76144f68dc7	sentiment/subjectivity analysis survey for languages other than english		Subjective and sentiment analysis have gained considerable attention recently. Most of the resources and systems built so far are done for English. The need for designing systems for other languages is increasing. This paper surveys different ways used for building systems for subjective and sentiment analysis for languages other than English. There are three different types of systems used for building these systems. The first (and the best) one is the language-specific systems. The second type of systems involves reusing or transferring sentiment resources from English to the target language. The third type of methods is based on using language-independent methods. The paper presents a separate section devoted to Arabic sentiment analysis.	compiler;language-independent specification;sentiment analysis	Mohammed Korayem	2016	Social Network Analysis and Mining	10.1007/s13278-016-0381-6	natural language processing;computer science;linguistics;sentiment analysis	NLP	-29.039753475817314	-76.79953594880628	199564
58400790f2acc8bbc628ca1935f1aea792d339b4	implementering av en metode for syntaktisk analyse av norsk (implementation of a method for syntactic analysis of norwegian) [in norwegian].				Knut Hofland	1977			norwegian;parsing;linguistics;computer science	NLP	-31.734162627052154	-78.15720744038313	199825
