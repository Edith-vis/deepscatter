id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
701f81329a25e380ca472b98e93a258ab4bb0d6c	hidden markov model for term weighting in verbose queries	hidden markov model;term weighting;verbose query	It has been observed that short queries generally have better performance than their corresponding long versions when retrieved by the same IR model. This is mainly because most of the current models do not distinguish the importance of different terms in the query. Observed that sentence-like queries encode information related to the term importance in the grammatical structure, we propose a Hidden Markov Model (HMM) based method to extract such information to do term weighting. The basic idea of choosing HMM is motivated by its successful application in capturing the relationship between adjacent terms in NLP field. Since we are dealing with queries of natural language form, we think that HMM can also be used to capture the dependence between the weights and the grammatical structures. Our experiments show that our assumption is quite reasonable and that such information, when utilized properly, can greatly improve retrieval performance.	hidden markov model;markov chain	Xueliang Yan;Guanglai Gao;Xiangdong Su;Hongxi Wei;Xueliang Zhang;Qianqian Lu	2012		10.1007/978-3-642-33247-0_10	computer science;machine learning;hidden semi-markov model;pattern recognition;data mining;information retrieval;hidden markov model	NLP	-33.10085038784751	-61.67473717275055	148795
93c8d1f865ff3d5ea755f2f099e969df8b0f8aee	evaluating topic models for information retrieval	retrieval;information retrieval;information search;evaluation;coarse grained;query expansion;topic model	We explore the utility of different types of topic models, both probabilistic and not, for retrieval purposes. We show that: (1) topic models are effective for document smoothing; (2) more elaborate topic models that capture topic dependencies provide no additional gains; (3) smoothing documents by using their similar documents is as effective as smoothing them by using topic models; (4) topics discovered on the whole corpus are too coarse-grained to be useful for query expansion. Experiments to measure topic models' ability to predict held-out likelihood confirm past results on small corpora, but suggest that simple approaches to topic model are better for large corpora.	experiment;information retrieval;query expansion;smoothing;text corpus;topic model	Xing Yi;James Allan	2008		10.1145/1458082.1458317	natural language processing;query expansion;document clustering;computer science;evaluation;machine learning;data mining;topic model;information retrieval	ML	-27.874859910399067	-61.89146862360252	148799
f14f26a5ce9ed09630c2d0d2fbeda1594ab36ac7	a cross-domain recommender system based on common-sense knowledge bases		A system able to extract and recommend technical terms from various domains is proposed in this paper. The motivation is to provide keywords that users may not be familiar with in the beginning but will be interested in after studying. To acquire domain knowledge, we collect documents from various sources, and the words in the documents are then represented as semantic word vectors. Given queries from users, the system first extracts important terms from given documents and computes the semantic similarity between those terms. Next, we utilize third party common-sense knowledge bases such as ConceptNet and Wikipedia to connect the queries to those extracted keywords through the network structures. Finally, the system will collect all keywords traversed and recommend the top-n of them. We propose and compare four models for the recommendation, and the differences between using ConceptNet and Wikipedia for discovering related knowledge are also investigated in this work.	in the beginning... was the command line;open mind common sense;recommender system;semantic similarity;wikipedia;word embedding	Yi-Ting Tsai;Chih-Shiang Wu;Hsiang-Ling Hsu;Tenniel Liu;Pei-Lin Chen;Wen-Hao Chen;Keng-Te Liao	2017	2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2017.48	recommender system;domain knowledge;information extraction;information retrieval;semantic similarity;commonsense knowledge;computer science	Web+IR	-28.403418306034823	-63.420804752738455	149795
719dee5116371668b2e60e70c93ff3aba7372b42	a visual approach for interactive keyterm-based clustering		The keyterm-based approach is arguably intuitive for users to direct text-clustering processes and adapt results to various applications in text analysis. Its way of markedly influencing the results, for instance, by expressing important terms in relevance order, requires little knowledge of the algorithm and has predictable effect, speeding up the task. This article first presents a text-clustering algorithm that can easily be extended into an interactive algorithm. We evaluate its performance against state-of-the-art clustering algorithms in unsupervised mode. Next, we propose three interactive versions of the algorithm based on keyterm labeling, document labeling, and hybrid labeling. We then demonstrate that keyterm labeling is more effective than document labeling in text clustering. Finally, we propose a visual approach to support the keyterm-based version of the algorithm. Visualizations are provided for the whole collection as well as for detailed views of document and cluster relationships. We show the effectiveness and flexibility of our framework, Vis-Kt, by presenting typical clustering cases on real text document collections. A user study is also reported that reveals overwhelmingly positive acceptance toward keyterm-based clustering.	algorithm;cluster analysis;preprocessor;relevance;sequence labeling;text corpus;usability testing;user-centered design;wikipedia	Seyednaser Nourashrafeddin;Ehsan Sherkat;Rosane Minghim;Evangelos E. Milios	2018	TiiS	10.1145/3181669	distributed computing;visualization;machine learning;computer science;visual approach;cluster analysis;artificial intelligence;text mining;document clustering	Web+IR	-26.55860958850643	-60.324505907863234	150139
db82c8e83d2ef333250126655511498b8114b18e	robust textual inference via graph matching	common source;tf-idf model;robust textual inference;present result;recognizing textual entailment dataset;dependency parser;approximate entailment;semantic relationship;semantic content;graph matching;directed graph;bag of words	We present an automated system for deciding whether a given sentence is entailed from a body of text. Each sentence is represented as a directed graph (extracted from a dependency parser) in which the nodes represent words or phrases, and the links represent syntactic and semantic relationships. A learned graph matching cost is used to measure how much of the semantic content of the sentence is contained in the text. We present results on the Recognizing Textual Entailment (RTE) dataset (Dagan et al., 2005), compare to other approaches, discuss common classes of errors, and discuss directions for improvement.	directed graph;matching (graph theory);textual entailment	Aria Haghighi;Andrew Y. Ng;Christopher D. Manning	2005			natural language processing;text graph;dependency graph;textual entailment;directed graph;computer science;bag-of-words model;machine learning;pattern recognition;graph;graph database;matching	NLP	-27.10185923203259	-65.18075876477593	151571
4a57873ff448b6015b8f4edd9e26b1b1ce40d02f	multilingual web retrieval experiments with field specific indexing strategies for clef 2006 at the university of hildesheim	web documents;multilingual information retrieval;automatic generation;indexation;web retrieval;relevance feedback	For WebCLEF 2006 we experimented with the analysis and extraction of the HTML structure of the web documents. In addition, blind relevance feedback was applied in the search process. As in 2005, the experiments wer e carried out with a language independent indexing strategy. We experimented with HTML title, H1 element and other elements emphasizing text. Our index containe d title and H1, emphasized elements, full and partial content. Blind relevance fe dback was implemented for all index fields except for the full content. The best results with the WebCLEF 2005 topics were achieved with a strong weight on the ti tl -element accomplishing a marginal improvement over the best post submission ru s for the mixedmonolingual task at WebCLEF 2005. For the WebCLEF 2 006 topics, improved results were achieved with the manually generated t opics, while those automatically generated led to results far below average. The bes t performance for manual topics for CLEF 2006 was achieved with a strong weight on b th HTML title as well as H1 elements, and a decreased weight for the other elem ents. Blind relevance feedback could not yet improve the results.	experiment;field electron emission;html;marginal model;relevance feedback;web page	Ben Heuwing;Thomas Mandl;Robert Strötgen	2006		10.1007/978-3-540-74999-8_105	computer science;multimedia;world wide web;information retrieval	Web+IR	-32.50137691510577	-63.129083560903325	151852
6d21c0d313fedd491d37b66bd638459a26dd237e	pagerank without hyperlinks: structural reranking using links induced by language models	vector space model;structural reranking;language modeling;high accuracy retrieval;hits;pagerank;social network;authorities;graph based retrieval;social networks;hubs;web search;language model	The ad hoc retrieval task is to find documents in a corpus that are relevant to a query. Inspired by the PageRank and HITS (hubs and authorities) algorithms for Web search, we propose a structural reranking approach to ad-hoc retrieval that applies to settings with no hyperlink information. We reorder the documents in an initially retrieved set by exploiting implicit asymmetric relationships among them. We consider  generation links , which indicate that the language model induced from one document assigns high probability to the text of another. We study a number of reranking criteria based on measures of centrality in the graphs formed by generation links, and show that integrating centrality into standard language-model-based retrieval is quite effective at improving precision at top ranks; the best resultant performance is comparable, and often superior, to that of a state-of-the-art pseudo-feedback-based retrieval approach. In addition, we demonstrate the merits of our language-model-based method for inducing interdocument links by comparing it to previously suggested notions of interdocument similarities (e.g., cosines within the vector-space model).We also show that ourmethods for inducing centrality are substantially more effective than approaches based on document-specific characteristics, several of which are novel to this study.	hyperlink;language model;pagerank	Oren Kurland;Lillian Lee	2010	ACM Trans. Inf. Syst.	10.1145/1852102.1852104	computer science;machine learning;data mining;database;world wide web;information retrieval;language model;social network	Security	-28.119453605685948	-61.974069506085144	151969
936147515ebeded721e8d725b8ee5dc7a87298ba	abstractive text summarization based on improved semantic graph approach		The goal of abstractive summarization of multi-documents is to automatically produce a condensed version of the document text and maintain the significant information. Most of the graph-based extractive methods represent sentence as bag of words and utilize content similarity measure, which might fail to detect semantically equivalent redundant sentences. On other hand, graph based abstractive method depends on domain expert to build a semantic graph from manually created ontology, which requires time and effort. This work presents a semantic graph approach with improved ranking algorithm for abstractive summarization of multi-documents. The semantic graph is built from the source documents in a manner that the graph nodes denote the predicate argument structures (PASs)—the semantic structure of sentence, which is automatically identified by using semantic role labeling; while graph edges represent similarity weight, which is computed from PASs semantic similarity. In order to reflect the impact of both document and document set on PASs, the edge of semantic graph is further augmented with PAS-to-document and PAS-to-document set relationships. The important graph nodes (PASs) are ranked using the improved graph ranking algorithm. The redundant PASs are reduced by using maximal marginal relevance for re-ranking the PASs and finally summary sentences are generated from the top ranked PASs using language generation. Experiment of this research is accomplished using DUC-2002, a standard dataset for document summarization. Experimental findings signify that the proposed approach shows superior performance than other summarization approaches.	algorithm;automatic summarization;bag-of-words model;crc-based framing;computer science;formal concept analysis;marginal model;master of science in information technology;maximal set;natural language generation;relevance;semantic role labeling;semantic similarity;sensor;similarity measure;subject-matter expert;taxonomy (general);text corpus	Atif Khan;Naomie Salim;Haleem Farman;Murad Khan;Bilal Jan;Awais Ahmad;Imran Ahmed;Anand Paul	2018	International Journal of Parallel Programming	10.1007/s10766-018-0560-3	semantic similarity;theoretical computer science;automatic summarization;semantic equivalence;semantic role labeling;similarity measure;bag-of-words model;computer science;artificial intelligence;sentence;pattern recognition;ranking	NLP	-26.903707179910977	-64.7861135959025	152162
c36cb34fd59e2c83d28aa2324e01b0477deca43c	using query reformulation and keywords in the geographic information retrieval task	query reformulation;geographic information retrieval	This paper describes the use of query reformulation to improve the Geographic Information Retrieval (GIR) task. This technique also includes the geographic expansion of the topics. Moreover, several experiments related to the use of keywords and hyponyms in the filtering process are performed. We also use a new approach in the re-ranking process based on the original position of each document in the ranking. The results obtained show that our query reformulation sometimes retrieves valid documents that the default query is not able to find, but on average it does not improve the baseline case. The best result is obtained considering the geographic entities in the traditional retrieval process.	geographic information system;information retrieval	José Manuel Perea Ortega;Luis Alfonso Ureña López;Manuel García Vega;Miguel Ángel García Cumbreras	2008		10.1007/978-3-642-04447-2_112	query optimization;query expansion;web query classification;ranking;computer science;data mining;database;web search query;information retrieval;query language	Web+IR	-30.561432251239076	-59.99620978225681	152380
977b8752e7ae123a2215fd9375d0f3ee8c6394f1	lia at trec 2011 web track: experiments on the combination of online resources		In this paper, we report the experiments we conducted for our participation to the TREC 2011 Web Track. The experiments we conducted this year aim at discovering how the combination of specific external resources in a language modeling fashion can help web search. We use Wikipedia and Google as external resources for different search contexts.	baseline (configuration management);email filtering;experiment;hyperlink;iso/iec 10967;language model;relevance feedback;significant figures;spamming;web search engine;wikipedia	Romain Deveaud;Eric SanJuan;Patrice Bellot	2011			computer science;data mining;information retrieval;language model	Web+IR	-30.608147262752272	-62.87051028498243	152428
f971bf00097e90e71dce51c65fc266504dece86a	expertise drift and query expansion in expert search	pseudo relevance feedback;information retrieval;expert finding;topic drift;query expansion;expertise modelling;expert search information retrieval	Pseudo-relevance feedback, or query expansion, has been shown to improve retrieval performance in the adhoc retrieval task. In such a scenario, a few top-ranked documents are assumed to be relevant, and these are then used to expand and refine the initial user query, such that it retrieves a higher quality ranking of documents. However, there has been little work in applying query expansion in the expert search task. In this setting, query expansion is applied by assuming a few top-ranked candidates have relevant expertise, and using these to expand the query. Nevertheless, retrieval is not improved as expected using such an approach. We show that the success of the application of query expansion is hindered by the presence of topic drift within the profiles of experts that the system considers. In this work, we demonstrate how topic drift occurs in the expert profiles, and moreover, we propose three measures to predict the amount of drift occurring in an expert's profile. Finally, we suggest and evaluate ways of enhancing query expansion in expert search using our new insights. Our results show that, once topic drift has been anticipated, query expansion can be successfully applied in a general manner in the expert search task.	baseline (configuration management);cohesion (computer science);it baseline protection;norm (social);query expansion;relevance feedback;web search engine	Craig MacDonald;Iadh Ounis	2007		10.1145/1321440.1321490	sargable;query optimization;query expansion;web query classification;ranking;computer science;machine learning;concept search;data mining;database;world wide web;information retrieval;query language	NLP	-29.948202129013026	-59.424285956961604	152750
75efbb7dfe9b1bfb777f6ed8a395bbd7de00406e	a case for usage of case usage in case-based reasoning	nearest neighbor;case base reasoning	Some of the issues in case retrieval and maintenance of case-bases are discussed. Conventionally, nearest neighbor algorithm is applied for retrieval of similar cases. Uniqueness of feature-values is not rewarded in such similarity matching functions. In order to refine the retrieval process, a metric based on successful case usage in the past is incorporated. This metric is shown to be useful in case-base maintenance. Applications in Homoeopathy remedy and personal tour selection are presented with comparative results.	case-based reasoning;k-nearest neighbors algorithm	Mohamed A. K. Sadiq;Deepak Khemani	2005			uniqueness;artificial intelligence;machine learning;computer science;k-nearest neighbors algorithm;case-based reasoning;pattern recognition	AI	-32.54766763652667	-59.25362628837085	152811
359c3ea51638c0f2d923012f2c8ecad1640fe4a2	matching resumes and jobs based on relevance models	relevance model;large scale;resume;job matching;relevance models;language model	We investigate the difficult problem of matching semi-structured resumes and jobs in a large scale real-world collection. We compare standard approaches to Structured Relevance Models (SRM), an extensionof relevance-based language model for modeling and retrieving semi-structured documents. Preliminary experiments show that the SRM approach achieved promising performance and performed better than typical unstructured relevance models.	consistency model;experiment;language model;relevance;semiconductor industry	Xing Yi;James Allan;W. Bruce Croft	2007		10.1145/1277741.1277920	computer science;machine learning;data mining;world wide web;information retrieval;language model	NLP	-28.881375744873264	-62.471490060780134	153126
30b0d994990dffdf82c56e9428f0e83e001d3fd3	a study into annotation ranking metrics in community contributed image corpora		Community contributed datasets are becoming increasing common in automated image annotation systems. One important issue with community image data is that there is no guarantee that the associated metadata is relevant. A method is required that can accurately rank the semantic relevance of community annotations. This should enable the extracting of relevant subsets from potentially noisy collections of these annotations. Having relevant, non-heterogeneous tags assigned to images should improve community image retrieval systems, such as Flickr, which are based on text retrieval methods. In the literature, the current state of the art approach to ranking the semantic relevance of Flickr tags is based on the widely used tf-idf metric. In the case of datasets containing landmark images, however, this metric is inefficient and can be improved upon. In this paper, we present a landmark recognition framework, that provides end-to-end automated recognition and annotation. In our study into automated annotation, we evaluate 5 alternate approaches to tf-idf to rank tag relevance in community contributed landmark image corpora. We carry out a thorough evaluation of each of these ranking metrics and results of this evaluation demonstrate that four of these proposed techniques outperform the current commonly-used tf-idf approach for this task. Our best performing evaluated approach achieves a significant F-Measure increase of .19 over tf-idf.	text corpus	Mark Hughes;Gareth J. F. Jones;Noel E. O'Connor	2012		10.1007/978-3-319-12093-5_8	data science;data mining;information retrieval	Vision	-27.503462812506992	-63.4576552548606	153420
b00449fb52e38a3d232a41a06408180d8f5ef91f	a computing model for concept fusing and document classification	document handling;semantic representation;computer model;knowledge management;hybrid approach;statistical analysis;pattern classification;statistical analysis document handling formal concept analysis knowledge based systems knowledge management pattern classification;document classification;knowledge based systems;formal concept analysis;semantic overlay computing model document classification concept fusing knowledge management semantic representation statistical measurements content segments formal concept analysis fca standard concept identifiers document semantic links	Effective document classification is a long-pursued goal in knowledge management. This paper proposes a novel hybrid approach of semantic representation and statistical measurements. Document is divided into content segments first. By Formal Concept Analysis (FCA), their semantic links with standard concept identifiers are built up whose weights are calculated statistically. In this way, effective concept fusing and document classification can be achieved. In addition, a semantic overlay for specific documents will be constructed via concept fusing. Experiments show our approach is feasible and effective.	document classification;e-science;experiment;formal concept analysis;identifier;knowledge management;semantic network;semantic similarity;statistical classification;statistical model	Nan Zhang;Chao He	2006		10.1109/SKG.2006.1	natural language processing;semantic computing;computer science;formal concept analysis;artificial intelligence;knowledge-based systems;machine learning;data mining;information retrieval	AI	-27.652339960487943	-63.71918258920937	153543
00cc825915256af3ea40a0ccb15a9b1320e7c320	fuzzy keyword ontology for annotating and searching event reports	fuzzy partonomy;knowledge mobilisation;fuzzy ontology;semantic web;fuzzy reasoning schemes	This paper defines and applies a fuzzy keyword ontology to annotate and search event reports in a database. The ontology is developed by superimposing a fuzzy partonomy on fuzzy classifications. The claim is that fuzzy keywords will help us find event reports even if the event description is incomplete or imprecise and that this will provide benefits in finding the relevant problem reports. This will save time and costs when working with queries on large dataand knowledge bases.	database;description logic;fuzzy concept;fuzzy logic;knowledge base;meronomy;semantic web;web standards	Juhani Hirvonen;Teemu Tommila;Antti Pakonen;Christer Carlsson;Mario Fedrizzi;Robert Fullér	2010			computer science;artificial intelligence;semantic web;data mining;database;information retrieval	AI	-33.245782048230836	-59.85818483017217	153556
c831bba9b91b3ee71a8bc02081ee98d8da91f058	extending a single-document summarizer to multi-document: a hierarchical approach		The increasing amount of online content motivated the development of multi-document summarization methods. In this work, we explore straightforward approaches to extend single-document summarization methods to multi-document summarization. The proposed methods are based on the hierarchical combination of single-document summaries, and achieves state of the art results.	automatic summarization;cosine similarity;multi-document summarization;upsampling;web content	Luís Marujo;Ricardo Ribeiro;David Martins de Matos;João Paulo da Silva Neto;Anatole Gershman;Jaime G. Carbonell	2015			computer science;data science;data mining;world wide web	AI	-27.173068742187077	-63.16127687874995	154517
b4f5685b57afaffd11b75c2b9a427c94c2eb22b3	query driven hypothesis generation for answering queries over nlp graphs	degrades geometrically;ground truth graph;lower confidence result;hypothesizing link;query result;nlp component;conjunctive query;hypothesis generation;natural language processing;nlp graph	It has become common to use RDF to store the results of Natural Language Processing (NLP) as a graph of the entities mentioned in the text with the relationships mentioned in the text as links between them. These NLP graphs can be measured with Precision and Recall against a ground truth graph representing what the documents actually say. When asking conjunctive queries on NLP graphs, the Recall of the query is expected to be roughly the product of the Recall of the relations in each conjunct. Since Recall is typically less than one, conjunctive query Recall on NLP graphs degrades geometrically with the number of conjuncts. We present an approach to address this Recall problem by hypothesizing links in the graph that would improve query Recall, and then attempting to find more evidence to support them. Using this approach, we confirm that in the context of answering queries over NLP graphs, we can use lower confidence results from NLP components if they complete a query result.	conjunctive query;end-to-end principle;entity;ground truth;information needs;knowledge representation and reasoning;natural language processing;precision and recall;resource description framework	Christopher A. Welty;Ken Barker;Lora Aroyo;Shilpa Arora	2012		10.1007/978-3-642-35173-0_15	natural language processing;computer science;data mining;database;information retrieval	NLP	-28.404749796566637	-65.00516642924738	154551
5e6d2829c23423fa57a732b57bc88554b106034f	entity linking with multiple knowledge bases: an ontology modularization approach	linked data;entity linking;ontology modularization	The recognition of entities in text is the basis for a series of applications. Synonymy and Ambiguity are among the biggest challenges in identifying such entities. Both challenges are addressed by Entity Linking, the task of grounding entity mentions in textual documents to Knowledge Base entries. Entity Linking has been based in the use of single cross-domain Knowledge Bases as source for entities. This PhD research proposes the use of multiple Knowledge Bases for Entity Linking as a way to increase the number of entities recognized in text. The problem of Entity Linking with Multiple Knowledge Bases is addressed by using textual and Knowledge Base features as contexts for Entity Linking, Ontology Modularization to select the most relevant subset of entity entries, and Collective Inference to decide the most suitable entity entry to link with each mention.		Bianca Pereira	2014		10.1007/978-3-319-11915-1_33	natural language processing;computer science;knowledge management;linked data;data mining;entity linking;world wide web;weak entity;sgml entity	Web+IR	-29.533755273682896	-65.59393637065695	154631
bfeaf7687529efb8d7c7e3cf5a09c7cd2125e0b6	creating a novel geolocation corpus from historical texts		This paper describes the process of annotating a historical US civil war corpus with geographic reference. Reference annotations are given at two different textual scales: individual place names and documents. This is the first published corpus of its kind in document-level geolocation, and it has over 10,000 disambiguated toponyms, double the amount of any prior toponym corpus. We outline many challenges and considerations in creating such a corpus, and we evaluate baseline and benchmark toponym resolution and document geolocation systems on it. Aspects of the corpus suggest several recommendations for proper annotation procedure for the tasks.	baseline (configuration management);benchmark (computing);display resolution;geolocation;spatial reference system;text corpus;the new york times;toponym resolution	Grant DeLozier;Benjamin Wing;Jason Baldridge;Scott Nesbit	2016			artificial intelligence;natural language processing;computer science;multimedia;geolocation	NLP	-30.512612620384107	-64.37490176109243	154847
6d38a644b5af6bda50cc0c447571db2eef2ba618	report on the trec-4 experiment: combining probabilistic and vector-space schemes		This paper describes and evaluates a retrieval scheme combining the OKAPI probabilistic retrieval model with various vector-space schemes. In this study, each retrieval strategy represents both queries and documents using the same set of single terms; however they weight them differently. To combine these search schemes, we do not apply a given combination operator on the retrieval status nor the rank of each retrieved record (e.g., sum, average, max., etc.). We think that each retrieval strategy may perform well for a set of queries and poorly for other requests. Thus, based on a given query's statistical characteristics, our search model first selects the more appropriate retrieval scheme and then retrieves information based on the selected search mechanism. Since the selection procedure is done before any search operation, our approach has the advantage of limiting the search time to one retrieval algorithm instead of retrieving items using various retrieval schemes, and then combining the given results.	algorithm;document	Jacques Savoy;Melchior Ndarugendamwo;Dana Vrajitoru	1995				Web+IR	-33.03885335788256	-60.80306939515751	155030
e303909c5a618a07d3349763e5818114a3732743	a better approach to ontology integration using clustering through global similarity measure		Corresponding Author: Ashwin Makwana Department of Computer Engineering, Charotar University of Science and Technology, CHARUSAT Campus, Changa, Gujarat, India Email: ashwinmakwana.ce@charusat.ac.in Abstract: Knowledge representation is a crucial area of work in the intelligent system, especially in query answering system development. Ontology is used to represent shared knowledge of a particular domain for query answering system. Domain-specific ontology can be designed and developed by many groups and researchers, because of which there is heterogeneity in the knowledgebase. Ontology integration or merging is necessary in order to solve this problem of mixed knowledge. Finding similarity between two ontologies is crucial to achieve integration or merging of ontology. In this study, we present a method to generate a cluster of ontologies using global similarity measure of two ontologies. Ontology matching tools are used to find matched classes between two ontologies. Output of ontology matching tool is mapping between two ontologies and is used for generating clusters of ontology. We use Jaccard Similarity Index as a global similarity measure for clustering. Based on this measure, the popular k-means clustering algorithm is used to perform clustering of ontologies. Bins of ontologies are generated from each cluster. From each bin, all ontologies are finally merged into a single ontology, which helps us in reducing search effort in querying knowledge in query processing. The outcome of this research paper to provide better solution for merging ontology. Here, we use agriculture domain ontology corpus from the standard dataset for experimentation.	algorithm;artificial intelligence;cluster analysis;computer engineering;database;domain-specific language;email;jaccard index;k-means clustering;knowledge base;knowledge representation and reasoning;ontology (information science);ontology alignment;question answering;similarity measure;web ontology language	Ashwin Makwana;Amit Ganatra	2018	JCS	10.3844/jcssp.2018.854.867	artificial intelligence;machine learning;computer science;ontology;knowledge representation and reasoning;data mining;jaccard index;ontology alignment;ontology (information science);semantic web;similarity measure;cluster analysis	AI	-27.951696773479483	-60.31712525250227	155253
2dd6a7618f48d08ac16e1df637ea40b83beee236	a comparison of methods for web document classification	web documents	WebDoc is an automated classification system that assigns Web documents to appropriate Library of Congress subject headings based upon the text in the documents. We have used different classification methods in different versions of WebDoc. One classification method is a statistical approach that counts the number of occurrences of a given noun phrase in documents assigned to a particular subject heading as the basis for determining the weights to be assigned to the candidate indexes. The second classification method uses a naïve Bayes approach. In this case, we experimented with the use of smoothing to dampen the effect of having a large number of 0s in our feature vectors. The third classification method is a k-nearest neighbors approach. With this approach, we tested two different ways of determining the similarity of feature vectors. In this paper, we report the performance of each of the versions of WebDoc in terms of recall, precision, and F-measures.	course (navigation);document classification;feature vector;k-nearest neighbors algorithm;library of congress subject headings;naive bayes classifier;smoothing	Julia E. Hodges;Yong Wang;Bo Tang	2005			web mining;web query classification;data web;web standards;computer science;semantic web;web page;semantic web stack	Web+IR	-30.13309912139977	-63.35450421546144	155382
f37b2a313ee2b3380e82917db663debc851fe53b	applying two-level reinforcement ranking in query-oriented multidocument summarization	summarization;ranking;weighting	Sentence ranking is the issue of most concern in document summarization today. While traditional featurebased approaches evaluate sentence significance and rank the sentences relying on the features that are particularly designed to characterize the different aspects of the individual sentences, the newly emerging graphbased ranking algorithms (such as the PageRank-like algorithms) recursively compute sentence significance using the global information in a text graph that links sentences together. In general, the existing PageRank-like algorithms can model well the phenomena that a sentence is important if it is linked by many other important sentences. Or they are capable of modeling the mutual reinforcement among the sentences in the text graph. However, when dealing with multidocument summarization these algorithms often assemble a set of documents into one large file. The document dimension is totally ignored. In this article we present a framework to model the two-level mutual reinforcement among sentences as well as documents. Under this framework we design and develop a novel ranking algorithm such that the document reinforcement is taken into account in the process of sentence ranking.The convergence issue is examined. We also explore an interesting and important property of the proposed algorithm.When evaluated on the DUC 2005 and 2006 query-oriented multidocument summarization datasets, significant results are achieved.	algorithm;automatic summarization;pagerank;recursion;reinforcement learning;text graph;upsampling	Furu Wei;Wenjie Li;Qin Lu;Yanxiang He	2009	JASIST	10.1002/asi.21127	natural language processing;ranking;computer science;automatic summarization;data mining;weighting;world wide web;information retrieval	Web+IR	-26.55345175792973	-62.85681970224851	155545
f7ba5be674249635ae1b0bd8edabe966e1fa4908	an approach to graph-based analysis of textual documents	settore inf 01 informatica;text analysisgraph modelmulti document summarization;text analysis;multi document summarization;technology and engineering;graph model	In this paper a new graph-based model is proposed for the representation of textual documents. Graph-structures are obtained from textual documents by making use of the well-known Part-OfSpeech (POS) tagging technique. More specifically, a simple rule-based (re)classifier is used to map each tag onto graph vertices and edges. As a result, a decomposition of textual documents is obtained where tokens are automatically parsed and attached to either a vertex or an edge. It is shown how textual documents can be aggregated through their graph-structures and finally, it is shown how vertex-ranking methods can be used to find relevant tokens.1.	automatic summarization;baseline (configuration management);cycle rank;graph operations;logic programming;parsing;part-of-speech tagging;rule-based system;vertex (geometry);vertex (graph theory);whole earth 'lectronic link	Antoon Bronselaer;Gabriella Pasi	2013		10.2991/eusflat.2013.96	natural language processing;text graph;computer science;data mining;information retrieval	NLP	-26.749314071454343	-64.57905276930691	155786
c2abb75c1e0d3c34422c57f7ca6e5fd7d6592ace	a pipeline tweet contextualization system at inex 2013		This article describes a pipeline system and preliminary results for Tweet Contextualization at INEX 2013. The system consists of three steps: tweet analysis, passage retrieval and summarization. For each tweet, key phrases are first extracted by making use of ArkTweet toolkit and employing several heuristics. They are then submitted as queries to Indri search engine to retrieve relevant passages. Finally, a multi-document summarization system (MEAD) is used to generate the output document with a limit of 500 words. The preliminary results show that the approach does not work well where our run was ranked 22nd out of 24 runs. We discuss our observations for these results and some further possible improvements.	automatic summarization;heuristic (computer science);multi-document summarization;web search engine	Khaled Hossain Ansary;Anh Tuan Tran;Nam Khanh Tran	2013			information retrieval;automatic summarization;contextualization;heuristics;search engine;ranking;computer science	AI	-31.383136839840223	-62.916651792207624	156257
2040c8ce2eda6a2e05bbe4a6cbda6cf0b10010e4	using minimaps to enable toponym resolution with an effective 100% rate of recall	minimap;precision;recall;toponym resolution;geotagging	A number of systems have been recently constructed that make use of a map query interface to access documents by the locations that they mention. These mentions are often ambiguous in the sense that many interpretations exist for the locations which are not always expressed along with all the necessary qualifiers. In other words, users are assumed to be able to make the appropriate identification based either on knowledge of prior queries or the nature of the document containing the references as well as knowledge of the target audience. The disambiguation process is known as toponym resolution. The map query interface results in the placement of icons and links to the appropriate documents at the corresponding location on the map. Assuming that all toponyms have been recognized (i.e., 100% rate of recall for toponym recognition), it is shown how to achieve an effective 100% rate of recall for toponym resolution for all interpretations of a toponym that the toponym recognition process associates with at least one document. This is done with the aid of a minimap that shows all of these interpretations which means that a user has access to all documents that mention a specific location as long as the textual specification to the location has been recognized as a location rather than as the name of another entity such as a person, company, organization, etc. It also assumes that the user is capable of determining the correct interpretation of each toponym. This is important as it enables the determination of precision and recall.	document;precision and recall;toponym resolution;word-sense disambiguation	Hanan Samet	2014		10.1145/2675354.2675698	computer science;data mining;recall;accuracy and precision;geotagging;world wide web;information retrieval;statistics	Web+IR	-30.450677257587916	-60.271862206267144	156737
5cbf97f6a39cf4177d0c8c797e4cc2161a3b6438	multilingual information retrieval in the language modeling framework	multilingual information retrieval;multilingual feedback;kl divergence framework;multilingual language models;language modeling framework	Multilingual information retrieval (MLIR) provides results that are more comprehensive than those of mono- and cross-lingual retrieval. Methods for MLIR are categorized as: (1) Fusion-based methods that merge results from multiple retrieval runs, and (2) Direct methods that build a unique index for the entire collection. Merging results of individual runs reduces the overall effectiveness, while more effective direct methods suffer from either time complexity and memory overhead, or over-weighting of index terms. In this paper, we propose a direct MLIR approach by using the language modeling framework that includes a novel multilingual language model estimation for documents, and a new way to globally estimate word statistics. These contributions enable ranking documents in multiple languages in one retrieval phase without having the problems of the previous direct methods. Moreover, our approach has the advantage of accommodating multilingual feedback information which helps to prevent query drift, and consequently to improve the performance. Finally, we effectively address the common case of incomplete coverage of translation resources in our proposed estimation methods. Experimental results show that the proposed approach outperforms the previous MLIR approaches.	categorization;dictionary;feedback;ietf language tag;information retrieval;kullback–leibler divergence;language model;n-gram;overhead (computing);query language;time complexity	Razieh Rahimi;Azadeh Shakery;Irwin King	2015	Information Retrieval Journal	10.1007/s10791-015-9255-1	natural language processing;speech recognition;computer science;machine learning;world wide web;information retrieval	Web+IR	-28.784722934543172	-61.62640627193998	157128
de91671e6798535cb0e6c795ff41f28c87877601	entity-based keyword search in web documents		In document search, documents are typically seen as a flat list of keywords. To deal with the syntactic interoperability, i.e., the use of different keywords to refer to the same real world entity, entity linkage has been used to replace keywords in the text with a unique identifier of the entity to which they are referring. Yet, the flat list of entities fails to capture the actual relationships that exist among the entities, information that is significant for a more effective document search. In this work we propose to go one step further from entity linkage in text, and model the documents as a set of structures that describe relationships among the entities mentioned in the text. We show that this kind of representation is significantly improving the effectiveness of document search. We describe the details of the implementation of the above idea and we present an extensive set of experimental results that prove our point.	entity;experiment;identifier;interoperability;linkage (software);response time (technology);unique key;web page;web search engine	Enrico Sartori;Yannis Velegrakis;Francesco Guerra	2016	Trans. Computational Collective Intelligence	10.1007/978-3-662-49521-6_2	computer science;data mining;entity linking;world wide web;weak entity;information retrieval	Web+IR	-28.61494348821349	-59.939800049242976	157418
ea9f2ec8265e0153d3a6a9940994e073ded23a5e	summarizing answers for community question answer services		This paper presents a novel answer summarization approach for community Question Answering services (cQAs) to address the problem of “incomplete answer”, i.e., missing valuable information from the “best answer” of a complex multi-sentence question, which can be obtained from other answers to the same question. Our method automatically generate a novel and non-redundant summary from cQA answers using structured determinantal point processes (SDPP). Experimental evaluation on sample dataset from Yahoo Answers shows significant improvement over baseline approaches.	automatic summarization;baseline (configuration management);graph (discrete mathematics);question answering;yahoo! answers	Vinay Pande;Tanmoy Mukherjee;Vasudeva Varma	2013		10.1007/978-3-642-40722-2_16	data science;data mining	NLP	-27.220811780138458	-63.0305233508503	158137
1473d0459efd26cfee35b02a07efd35adf76532d	a novel view on information content of concepts in a large ontology and a view on the structure and the quality of the ontology	information content;semantic distance	Semantic distance and semantic similarity are two important information retrieval measures used in word sense disambiguation as well as for the assessment of how relevant concepts are with respect to the documents in which they are found. A variety of calculation methods have been proposed in the literature, whereby methods taking into account the information content of an individual concept outperform those that do not. In this paper, we present a novel recursive approach to calculate a concept's information content based on the information content of the concepts to which it relates. The method is applicable to extremely large ontologies containing several million concepts and relationships amongst them. It is shown that a concept's information content as calculated by this method provides additional information with respect to an ontology that cannot be approximated by hierarchical edge-counting or human insight. In addition, it is suggested that the method can be used for quality control within large ontologies and that it can give you an impression on the structure and the quality of the ontology.	approximation algorithm;implantable cardioverter-defibrillator;information retrieval;ontology (information science);population parameter;recursion;self-information;semantic similarity;word sense;word-sense disambiguation;fetal tachycardia	Carl Van Buggenhout;Werner Ceusters	2005	International journal of medical informatics	10.1016/j.ijmedinf.2004.03.009	upper ontology;semantic similarity;self-information;computer science;knowledge management;ontology;data mining;information quality;ontology-based data integration;information retrieval;process ontology;dishin	Web+IR	-31.936869563122254	-60.33565067876389	158164
120217addea70a9a203dbb8b94ac95b1a7e6fa44	explicit scientific knowledge comparison based on semantic description matching	semantic description;scientific knowledge	Researchers begin new research by acquiring pre-existing explicit scientific knowledge that is potentially relevant to the research subject. In order to find some potentially relevant explicit scientific knowledge items, such as knowledge whose content is similar to the targeted research, a researcher must examine the semantics of each item. In this paper, after reviewing related work, an automated semantic description matching-based approach is presented for comparing items of explicit scientific knowledge. This approach obtains a matching score between semantic descriptions of two items of explicit scientific knowledge that indicates their similarity. Three dimensions are considered in this approach: matching granularity, similarity scale for instance classes, and logic similarity scale. In order to match two semantic descriptions, a six-step method is presented: creation of atomic queries, generalization of query classes, generalization of query properties, addition of rules, creation of instances implied by complex class definition, and semi-automatic pruning of matching results. Finally, some conclusions regarding the approach are presented together with plans for future work.	query language;semiconductor industry	Weisen Guo;Steven B. Kraines	2008		10.1002/meet.2008.1450450210	natural language processing;semantic similarity;computer science;data mining;sociology of scientific knowledge;information retrieval	AI	-32.59775098840032	-61.22906431145422	158442
8c9a0ca3ce8d2f351428e9e4aa00e145d72d1732	xml schema clustering with semantic and hierarchical similarity measures	semantic similarity;hierarchical structure;xml schema;data mining;data representation;schema matching;document mining;semi structured data;clustering;xml;similarity measure;clustered data;structural similarity	With the growing popularity of XML as the data representation language, collections of XML data have exploded in numbers. The methods are required to manage and discover the useful information from them for improved document handling. We present a schema clustering process by organising heterogeneous XML schemas into groups. The methodology considers not only the linguistic and the context of the elements but also the hierarchical structure similarity. We support our findings with experiments and analysis.	categorization;class hierarchy;cluster analysis;data (computing);digital library;experiment;heterogeneous system architecture;library (computing);maximal set;norm (social);path expression;preprocessor;semantic similarity;structural similarity;web page;xhtml;xml namespace;xml schema	Richi Nayak;Wina Iryadi	2007	Knowl.-Based Syst.	10.1016/j.knosys.2006.08.006	well-formed document;data exchange;xml validation;simple api for xml;semantic similarity;semi-structured data;xml;xml schema;computer science;document structure description;structural similarity;xml framework;data mining;xml database;xml schema;database;external data representation;document schema definition languages;cluster analysis;xml schema editor;information retrieval;efficient xml interchange	DB	-27.931887617922243	-59.3680862278835	159219
1e2ad2b2177437164aa7743050537b0c4b1c6fb6	theoretical benchmarks of xml retrieval	search and retrieval;matching function;xml retrieval;meta evaluation	This poster investigates the use of theoretical benchmarks to describe the matching functions of XML retrieval systems and the properties of specificity and exhaustivity in XML retrieval. Theoretical benchmarks concern the formal representation of qualitative properties of IR models. To this end, Situation Theory framework for the meta-evaluation of XML retrieval is presented.	sensitivity and specificity;xml retrieval	Tobias Blanke;Mounia Lalmas	2006		10.1145/1148170.1148281	xml validation;simple api for xml;computer science;document structure description;data mining;xml schema;database;information retrieval;human–computer information retrieval	Web+IR	-33.45777114927234	-60.666558415071364	159464
30ecaeac58456f6598e2c2dd262575e01e0ae647	source code retrieval using conceptual similarity	distance measure;information extraction;conceptual model;universiteitsbibliotheek;retrieval model;source code;structured documents	We propose a method for retrieving segments of source code from a large repository. The method is based on conceptual modeling of the code, combining information extracted from the structure of the code and standard informationdistance measures. Our results show an improvement over traditional retrieval models, indicating that, for this type of highly-structured documents, usage of structure is indeed beneficial for retrieval.	conceptual graph;embedded system;experiment;mathematical optimization;programming language;similarity measure	Gilad Mishne;Maarten de Rijke	2004			visual word;conceptual model;data mining;information extraction;source code;divergence-from-randomness model	SE	-29.245765000306065	-61.51856304378669	159710
af1ac20bff13f185d148cf040116dff4ed3244ab	relational indexing using a grammarless parser	conceptual knowledge extraction relational indexing grammarless parser natural language parsing predefined structure inter word relations parsing linguistic problems ir applications index expressions feedback mechanisms query by navigation;feedback mechanism;conceptual knowledge;information retrieval;natural languages;natural language parsing;indexing natural languages feedback navigation robustness data mining classification tree analysis;indexing;close relationships;knowledge acquisition;indexation;computational linguistics;indexing natural languages word processing computational linguistics information retrieval knowledge acquisition;word processing	This article proposes an alternate view on natural language parsing. Instead of looking for some predefined (phrase) structure it takes inter-word relations as startingpoint. The reason for this is twofold: firstly it circumvents traditional parsing and linguistic problems and secondly it offers a possibility to extract information specifically needed by IR applications. The close relationship with index expressions opens the door to feedback mechanisms like ‘Query By Navigation’ [1] and conceptual knowledge extraction [2]. The presented ideas are accompanied by an implementation and a small scale experiment.	parsing	F. A. Grootjen	2001		10.1109/ICSMC.2001.971947	natural language processing;search engine indexing;parser combinator;computer science;bottom-up parsing;computational linguistics;parsing;s-attributed grammar;feedback;natural language;top-down parsing;information retrieval	NLP	-33.35230669841115	-62.98911331126658	160247
6e8a7869e83d1b45a47128199f33994656208be0	dutir at trec 2011 microblog track	feedback;language model;microblog retrieval;vsm	In TREC 2011 Microblog Track, we explore the use of pseudo relevance feedback to expand original query terms in topics. Hyperlink is used to enhance the performance of the retrieval results. And we set a threshold of entropy to filter retrieval results. Microblog is a Realtime Adhoc Task, so we make use of average querytweettime that comes from pseudo relevance feedback to change retrieval score. We combine two models to improve retrieval results. The results show that our model is effective at realtime relevance retrieval.	entropy (information theory);hyperlink;relevance feedback	Cunhui Shi;Kejiang Ren;Hongfei Lin;Shaowu Zhang	2011			relevance;computer science;data mining;world wide web;information retrieval	Web+IR	-31.12423053221092	-62.38570244814345	161045
de144dbce1cfdb0ab480d3cdc88c4f67eb473d13	reference metadata extraction from korean research papers		A large amount of research papers are published in various fields and the ability to accurately extract metadata from a list of references is becoming increasingly important. Moreover, metadata extraction is crucial for measuring the influence of a particular study or researcher. However, it is difficult to automatically extract data from most lists of references because they consist of unstructured strings with bibliographies structured in various formats depending on the proceedings. Thus, this paper presents an effective and accurate method for extracting metadata, such as author name, title, publication year, volume, issue, page numbers, and journal name from heterogeneous references using the conditional random fields model. To conduct an experiment measuring the effectiveness of the proposed model, 1,415 references from 93 different academic papers published in Korea were used and a high accuracy of 97.10% was obtained.		Jae-Wook Seol;Won-Jun Choi;Hee-Seok Jeong;Hye-Kyong Hwang;Hwa-Mook Yoon	2018		10.1007/978-3-030-05918-7_5	information retrieval;metadata;conditional random field;computer science;author name	EDA	-30.208054289141018	-65.49135414564776	161346
a293b7a26a7d768e3248f2b0284ec50c4d375e34	constructing search as a service towards non-deterministic and not validated resource environment with a positive-negative strategy		Internet resources are non-deterministic, non-guaranteed and ultra-complex. We provide a progressive search approach towards problems with positive and negative tendencies aiming at improving the credibility of resources through multi times progressive searching. Meanwhile, we introduce Knowledge Graph as a resource process architecture to organize resources on the network and analyze the tendency of searchers for retrieving information by semantic analysis. We calculate entropy of resources according to searching times and amount of items of each search to represent the reliability of resources with positive and negative tendencies. Resources with ambiguous tendency and false information will be eliminated during the process of progressive search and quality of searching results will be improved while avoiding dead loop of searching towards infinite and complex problems. We apply the searching strategy to a medical resource processing system that provides high precision medical resource retrieval service for medical workers to verify the feasibility of our approach.		Yucong Duan;Lixu Shao;Xiaobing Sun;Li-zhen Cui;Donghai Zhu;Zhengyang Song	2017		10.1007/978-3-030-00916-8_34	bidirectional search;computer science;process architecture;distributed computing;the internet;credibility;graph	HPC	-33.054162698924294	-59.45258632707052	161372
60d3238ee48c53aa0303b0e496871a7b311d6132	product specifications summarization and product ranking system using user's requests			requests	Kazutaka Shimada;Tsutomu Endo	2003			discrete mathematics;automatic summarization;product design specification;data mining;mathematics;theoretical computer science;multi-document summarization;ranking	Robotics	-29.327462184773058	-64.14270949156048	162992
749c4534fee817ceb846d372df69bcc48d785efd	sanom results for oaei 2017		Simulated annealing-based ontology matching (SANOM) participates for the second time at the ontology alignment evaluation initiative (OAEI) 2018. This paper contains the configuration of SANOM and its results on the anatomy and conference tracks. In comparison to the OAEI 2017, SANOM has improved significantly, and its results are competitive with the state-of-the-art systems. In particular, SANOM has the highest recall rate among the participated systems in the conference track, and is competitive with AML, the best performing system, in terms of F-measure. SANOM is also competitive with LogMap on the anatomy track, which is the best performing system in this track with no usage of particular biomedical background knowledge. SANOM has been adapted to the HOBBIT platfrom and is now available for the registered users. abstract environment.	ontology alignment;sensitivity and specificity;simulated annealing	Majid Mohammadi;Amir Ahooye Atashin;Wout Hofman;Yao-Hua Tan	2017			ontology alignment;information retrieval;ontology;simulated annealing;informatics;similarity measure;computer science	Visualization	-32.312187173900156	-65.87867274364241	163388
627e25dd1fa1762c150b30b01801993aac2db2c3	damsel: the dsto/macquarie system for entity-linking		This paper describes the DSTO/Macquarie University System for Entity Linking (DAMSEL), which competed in the 2009 Text Acquisition Conference Knowledge Base Population task. The system achieves 73.5% accuracy. For a given named entity mention, the system selects a set of candidate entities from the knowledge base and selects the most likely candidate based on the similarity between the document in which the mention was found and the candidate’s Wikipedia article. The best-performing candidate selection strategy took advantage of Wikipedia redirection and disambiguation data. The best-performing similarity measure was the cosine metric.	cosine similarity;entity linking;knowledge base;named entity;similarity measure;wikipedia;word-sense disambiguation	Matthew Honnibal;Robert Dale	2009			named entity;information retrieval;entity linking;computer science;knowledge base;university system;similarity measure;population	NLP	-30.182526810723655	-64.0943523637935	164294
a19394edf53d4c19a5facd56aaa2cc3050a35c0f	phrasal graph-based method for abstractive vietnamese paragraph compression		Text compression is the task of identifying the main information in the source text to form a short single sentence. A broad approach is to find a path containing common vertices in the word graph model. The first issue of this approach is that the path finding algorithm can separate words from the phrase expressing a content. This leads to create new sentences having different meaning from the original ones. The second issue is that when an information is expressed by different words or phrases, called co-reference situations. Due to lacking of mechanism for handling this situation, the compression will be missing information. We propose in this paper a method to overcome the above issues. The core of new method is the improved graph model in which each vertex illustrates a phrase with its corresponding Part-of-Speech label. The intersection vertices of branches are results of mechanism for handling co-references. The compressing algorithm reduces the graph and forms the final sentence. We use ROUGE measure to compare with two word graph-based baselines. The experiment result shows that our method creates short sentences containing rich information.	algorithm;baseline (configuration management);pathfinding;vertex (geometry)	Dang Tuan Nguyen;Trung Tran	2017		10.1145/3155133.3155177	vietnamese;source text;paragraph;rouge;vertex (geometry);compression (physics);mathematics;phrase;artificial intelligence;pattern recognition;sentence	NLP	-27.07179636674512	-65.89817890492532	164761
32fa0a32c552e18106b92933b765ca0871803253	snumedinfo at clefehealth2013 task 3		This paper describes the participation of the SNUMedinfo team at the CLEFeHealth2013 task 3. We submitted 7 runs in total: 1 baseline run using query likelihood model in Indri search engine; 3 runs using passage based language model; 3 runs using passage based language model with lexical query expansion. We tried to incorporate passage-based score into ranking model to reflect the degree of query term cohesion per each document.	baseline (configuration management);cohesion (computer science);language model;query expansion;web search engine	Sungbin Choi;Jinwook Choi	2013			speech recognition;computer science;data mining;database	NLP	-30.785620664430446	-63.313217685510764	165840
2df0f80ac9bfa6a5e292018405752c648989387c	model fusion experiments for the clsr task at clef 2007	information retrieval system;speech retrieval;data fusion;indexation;retrieval models;query expansion;relevance feedback;machine translation	This paper presents the participation of the University of Ottawa group in the Cross-Language Speech Retrieval (CL-SR) task at CLEF 2007. We present the results of the submitted runs for the English collection. We have used two Information Retrieval systems in our experiments: SMART and Terrier, with two query expansion techniques: one based on a thesaurus and the second one based on blind relevant feedback. We proposed two novel data fusion methods for merging the results of several models (retrieval schemes available in SMART and Terrier). Our experiments showed that the combination of query expansion methods and data fusion methods helps to improve the retrieval performance. We also present cross-language experiments, where the queries are automatically translated by combining the results of several online machine translation tools. Experiments on indexing the manual summaries and keywords gave the best retrieval results.	computer law & security review;experiment;information retrieval;machine translation;query expansion;smart;thesaurus	Muath Alzghool;Diana Inkpen	2007		10.1007/978-3-540-85760-0_88	natural language processing;query expansion;computer science;data mining;sensor fusion;machine translation;world wide web;information retrieval;human–computer information retrieval	Web+IR	-32.456973671812584	-63.395102830890885	165910
0b674e8356abd076f4f01261927e536eb82b3aac	rough sets based reasoning and pattern mining for a two-stage information filtering system	filtering models;mismatch problems;thresholds;specific information;information filtering;pattern mining;weighting schema;theory;decision rules;information filtering system;decision;analysis models;information need;rough set;experimentation;decision rule;two stage	"""This paper presents a novel two-stage information filtering model which combines the merits of term-based and pattern- based approaches to effectively filter sheer volume of infor- mation. In particular, the first filtering stage is supported by a novel rough analysis model which efficiently removes a large number of irrelevant documents, thereby addressing the overload problem. The second filtering stage is empow- ered by a semantically rich pattern taxonomy mining model which effectively fetches incoming documents according to the specific information needs of a user, thereby addressing the mismatch problem. The experiments have been conducted to compare the proposed two-stage filtering (T-SM) model with other possible """"term-based + pattern-based"""" or """"term-based + term-based"""" IF models. The results based on the RCV1 corpus show that the T-SM model significantly outperforms other types of """"two-stage"""" IF models."""	data mining;experiment;information filtering system;information needs;relevance;rough set	Xujuan Zhou;Yuefeng Li;Peter Bruza;Yue Xu;Raymond Y. K. Lau	2010		10.1145/1871437.1871639	computer science;information filtering system;machine learning;data mining;decision rule;database;information retrieval;statistics	Web+IR	-29.536111416079983	-59.93015583430147	166067
607b522c32d61474f2f05b2b23b7bc8e205cf821	unsupervised graph-based topic labelling using dbpedia	dbpedia;latent dirichlet allocation;conference paper;topic labelling;graph centrality measures	Automated topic labelling brings benefits for users aiming at analysing and understanding document collections, as well as for search engines targetting at the linkage between groups of words and their inherent topics. Current approaches to achieve this suffer in quality, but we argue their performances might be improved by setting the focus on the structure in the data. Building upon research for concept disambiguation and linking to DBpedia, we are taking a novel approach to topic labelling by making use of structured data exposed by DBpedia. We start from the hypothesis that words co-occuring in text likely refer to concepts that belong closely together in the DBpedia graph. Using graph centrality measures, we show that we are able to identify the concepts that best represent the topics. We comparatively evaluate our graph-based approach and the standard text-based approach, on topics extracted from three corpora, based on results gathered in a crowd-sourcing experiment. Our research shows that graph-based analysis of DBpedia can achieve better results for topic labelling in terms of both precision and topic coverage.	centrality;crowdsourcing;dbpedia;linkage (software);performance;text corpus;text-based (computing);web search engine;word-sense disambiguation	Ioana Hulpus;Conor Hayes;Marcel Karnstedt;Derek Greene	2013		10.1145/2433396.2433454	latent dirichlet allocation;natural language processing;computer science;machine learning;data mining;world wide web;information retrieval	NLP	-27.119838000828988	-64.83847336371446	166837
a5689fdf140621d99627a36a8c119046f0009c13	an information extraction method from different structural web sites by word distances between a user instantiated label and similar entity	word distances;different structure;information extraction;information retrieval;different structure web sites information extraction user instantiated label word distances link selection;web sites;web sites data mining optical sensors adaptive optics digital cameras pattern matching monitoring;web sites information retrieval;link selection;target information information extraction method structural web sites word distances user instantiated example decision making;user instantiated label	"""This paper addresses an information extraction from different structural web sites by using a user instantiated example. A user instantiated example consists of labels as criteria for decision making on purchasing a target product or service and instances related to the labels. When information extraction method outputs the information in table form, labels are used as column heading of table and instances are used as instances filled in the table. Because there are various labels and information that does not correspond to the target on the web site, it is difficult to extract the target information related to the target. Information of the target tends to be written in a similar string to the instances that is called """"similar entity"""". And target information is written close to the labels. So, the proposed method extracts information using the number of words among a user instantiated label and similar entities. Additionally, in order to extract a piece of information described across the web sites, the proposed method extracts information from linked web sites that are similar to the web site used for a user instantiated example. Experimental results show that the proposed method can extract information at recall rate of 65% and precision rate of 91%."""	course (navigation);entity;information extraction;purchasing;sensitivity and specificity	Daisuke Nakajima;Yuki Mitsui;Masaki Samejima;Masanori Akiyoshi	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.29	computer science;data mining;world wide web;information extraction;information retrieval	DB	-30.404570927210354	-60.16690619826236	166843
c0c17e89c6eac1d9b1c90c22937ecb47a792fedd	keyphrase extraction-based query expansion in digital libraries	unsupervised learning;document handling;pos;query processing;pseudo relevance feedback;disjunctive normal form;average precision;digital library;query formulation;keyphrase extraction;medline collection keyphrase extraction unsupervised query expansion technique digital libraries pseudorelevance feedback retrieval performance pos phrase categorization documents retrieval disjunctive normal form query refomulation ontologies;ontologies artificial intelligence;wordnet;software libraries feedback information retrieval data mining information science ontologies educational institutions proteins testing speech;query expansion;information gain;unsupervised learning document handling ontologies artificial intelligence query formulation query processing relevance feedback;relevance feedback;query expansion pos wordnet information gain keyphrase extraction	In pseudo-relevance feedback, the two key factors affecting the retrieval performance most are the source from which expansion terms are generated and the method of ranking those expansion terms. In this paper, we present a novel unsupervised query expansion technique that utilizes keyphrases and POS phrase categorization. The keyphrases are extracted from the retrieved documents and weighted with an algorithm based on information gain and co-occurrence of phrases. The selected keyphrases are translated into Disjunctive Normal Form (DNF) based on the POS phrase categorization technique for better query refomulation. Furthermore, we study whether ontologies such as WordNet and MeSH improve the retrieval performance in conjunction with the keyphrases. We test our techniques on TREC 5, 6, and 7 as well as a MEDLINE collection. The experimental results show that the use of keyphrases with POS phrase categorization produces the best average precision.	algorithm;categorization;digital library;disjunctive normal form;information gain in decision trees;information retrieval;kullback–leibler divergence;library (computing);medline;ontology (information science);query expansion;relevance feedback;text retrieval conference;wordnet	Min Song;Il-Yeol Song;Robert B. Allen;Zoran Obradovic	2006	Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)	10.1145/1141753.1141800	natural language processing;unsupervised learning;wordnet;query expansion;digital library;computer science;data mining;kullback–leibler divergence;point of sale;disjunctive normal form;world wide web;information retrieval	NLP	-29.39363326907507	-64.91790539298307	166995
2dd4bd85763d178d444816713859fbbe478554fe	measure-based metasearch	ucl;average precision;discovery;theses;conference proceedings;metasearch;digital web resources;ucl discovery;open access;retrieval evaluation;ucl library;book chapters;open access repository;ucl research	We propose a simple method for converting many standard measures of retrieval performance into metasearch algorithms. Our focus is both on the analysis of retrieval measures themselves and on the development of new metasearch algorithms. Given the conversion method proposed, our experimental results using TREC data indicate that system-oriented measures of overall retrieval performance (such as average precision) yield good metasearch algorithms whose performance equals or exceeds that of benchmark techniques such as CombMNZ and Condorcet.	algorithm;benchmark (computing);information retrieval;text retrieval conference	Javed A. Aslam;Virgil Pavlu;Emine Yılmaz	2005		10.1145/1076034.1076133	metasearch engine;computer science;world wide web;information retrieval	Web+IR	-31.854125619839287	-62.398816609287884	167048
8ae0bffe6c49273bd3b29ce68c5096be272ef5b1	clueweb09 and trec diversity		The TREC Web Track explores and evaluates Web retrieval technologies. The TREC 2009 Web Track included both a traditional adhoc retrieval task and a new diversity task. The goal of this diversity task is to return a ranked list of pages that together provide complete coverage for a query, while avoiding excessive redundancy in the result list. Both tasks will continue at TREC 2010, which will also include a new Web spam task. The track uses the ClueWeb09 dataset as its document collection. This collection consists of roughly 1 billion web pages in multiple languages, comprising approximately 25TB of uncompressed data crawled from the general Web during January and February 2009. For TREC 2009, topics for the track were created from the logs of a commercial search engine, with the aid of tools developed at Microsoft Research. Given a target query, these tools extracted and analyzed groups of related queries, using co-clicks and other information, to identify clusters of queries that highlight different aspects and interpretations of the target query. These clusters were employed by NIST for topic development. For use by the diversity task, each resulting topic is structured as a representative set of subtopics, each related to a different user need. Documents were judged with respect to the subtopics, as well as with respect to the topic as a whole. In 2009, a total of 18 groups submitted runs to the diversity task. To evaluate these runs, the task used two primary effectiveness measures: -nDCG as defined by Clarke et al. (SIGIR 2008) and an “intent aware” version of precision, based on the work of Agrawal et al. (WSDM 2009). Developing and validating metrics for diversity tasks continues to be a goal of the track. For TREC 2010, we will report a number of additional evaluation measures that have been proposed over the past year, including an intent aware version of the ERR measure described by Chapelle et al. (CIKM 2009). Nick Craswell from Microsoft serves as the track co-coordinator. Ian Soboroff is the NIST contact. The ClueWeb09 collection was created through the efforts of Jamie Callan and Mark Hoy at the Language Technologies Institute, Carnegie Mellon University. More information may be found on the track Web page: http://plg.uwaterloo.ca/~trecweb/2010.html. Bio Charles Clarke is a professor in the David R. Cheriton School of Computer Science at the University of Waterloo, Canada. He has published on a wide range of topics within the area of information retrieval, including papers related to evaluation, efficiency, ranking, parallel systems, security, question answering, document structure, and XML. He was a Program Co-Chair of SIGIR 2007 and General CoChair of SIGIR 2003. From 2004 to 2006 he was the coordinator of the TREC Terabyte Retrieval track. Since 2009 he has been a cocoordinator of the TREC Web Track. He is a co-author of the book Information Retrieval: Implementing and Evaluating Search Engines (MIT Press, 2010). He has previously held software development positions at a number of computer consulting and engineering firms. In 2006 he spent a sabbatical at Microsoft, where he was involved in their search engine development effort.	archive;computer science;edmund m. clarke;ian cullimore;information needs;information retrieval;jamie wilkinson;language technologies institute;language technology;learning to rank;microsoft research;question answering;software development;spamdexing;spamming;terabyte;web services distributed management;web page;web search engine;xml	Charles L. A. Clarke	2010			learning to rank;web page;xml;question answering;nist;search engine;information retrieval;spamdexing;software development;computer science	Web+IR	-31.600865586119696	-63.11405993882978	167556
0ac1d86fcd8825217e778457e425c143c434641a	feeling lucky?: multi-armed bandits for ordering judgements in pooling-based evaluation	pooling;information retrieval;multi armed bandits	Evaluation is crucial in Information Retrieval. The Cranfield paradigm allows reproducible system evaluation by fostering the construction of standard and reusable benchmarks. Each benchmark or test collection comprises a set of queries, a collection of documents and a set of relevance judgements. Relevance judgements are often done by humans and thus expensive to obtain. Consequently, relevance judgements are customarily incomplete. Only a subset of the collection, the pool, is judged for relevance. In TREC-like campaigns, the pool is formed by the top retrieved documents supplied by systems participating in a certain evaluation task. With multiple retrieval systems contributing to the pool, an exploration/exploitation trade-off arises naturally. Exploiting effective systems could find more relevant documents, but exploring weaker systems might also be valuable for the overall judgement process. In this paper, we cast document judging as a multi-armed bandit problem. This formal modelling leads to theoretically grounded adjudication strategies that improve over the state of the art. We show that simple instantiations of multi-armed bandit models are superior to all previous adjudication strategies.	benchmark (computing);information retrieval;multi-armed bandit;programming paradigm;relevance;text retrieval conference	David E. Losada;Javier Parapar;Alvaro Barreiro	2016		10.1145/2851613.2851692	computer science;artificial intelligence;machine learning;data mining;pooling	Web+IR	-30.877981801326342	-61.423269822782366	167645
011d771859634c6d3be9f759cd71b16a96b94f5e	determining the spatial reader scopes of news sources using local lexicons	search engine;information sources;spatial reader scope;automatic generation;local lexicon;spatial relatedness;semantic relatedness;spatial relation;news search engine;geotagging	Information sources on the Internet (e.g., Web versions of newspapers) usually have an implicit spatial reader scope, which is the geographical location for which the content has been primarily produced. Knowledge of the spatial reader scope facilitates the construction of a news search engine that provides readers a set of news sources relevant to the location in which they are interested. In particular, it plays an important role in disambiguating toponyms (e.g., textual specifications of geographical locations) in news articles, as the process of selecting an interpretation for the toponym often reduces to one of selecting an interpretation that seems natural in the context of the spatial reader scope. The key to determining the spatial reader scope of news sources is the notion of local lexicon, which for a location s is a set of concepts such as, but not limited to, names of people, landmarks, and historical events, that are spatially related to s. Techniques to automatically generate the local lexicon of a location by using the link structure of Wikipedia are described and evaluated. A key contribution is the improvement of existing methods used in the semantic relatedness domain to extract concepts spatially related to a given location from the Wikipedia. Results of experiments are presented that indicate that the knowledge of the spatial reader scope significantly improves the disambiguation of textually specified locations in news articles and that using local lexicons is an effective method to determine the spatial reader scopes of news sources.	algorithm;effective method;experiment;geotagging;internet;lex (software);lexicon;location (geography);semantic similarity;text mining;web search engine;wikipedia;word-sense disambiguation	Gianluca Quercini;Hanan Samet;Jagan Sankaranarayanan;Michael D. Lieberman	2010		10.1145/1869790.1869800	spatial relation;semantic similarity;computer science;operating system;data mining;geotagging;world wide web;information retrieval;search engine;remote sensing	Web+IR	-30.401669844522456	-60.08179166941347	167812
26d354f6408ecf495139dad7a0d7871ea930d201	a refined methodology for automatic keyphrase assignment to digital documents		AbstrAct: Keyphrases precisely express the primary topics and themes of documents and are valuable for cataloging and classification. Manually assigning keyphrases to existing documents is a tedious task; therefore, automatic keyphrase generation has been extensively used to classify digital documents. Existing automatic keyphrase generation algorithms are limited in assigning semantically relevant keyphrases to documents. In this paper we have proposed a methodology to refine the result set of automatically generated keyphrases by Keyphrase Extraction Algorithm (KEA++), so that the key-phrases accurately and precisely represent the content of the document. Our approach is an additional layer at the top of KEA++ and exploits semantic relationships and hierarchical structure of the controlled vocabulary to filter out irrelevant keyphrases from the result set generated by KEA++. The methodology was applied on different sets of academic publications for evaluation. Evaluation demonstrates that the proposed refinement methodology improves the quality of generated keyphrases.	algorithm;controlled vocabulary;refinement (computing);relevance;result set	Sharifullah Khan;Iram Fatima;Rabia Irfan;Khalid Latif	2011	JDIM		information retrieval;data mining;computer science	AI	-29.639310227884582	-65.81705058385398	168014
51b2afe664c24f5c520ebd500698863c197aac31	tldret: a temporal semantic facilitated linked data retrieval framework		Temporal features, such as date and time or time of an event, employ concise semantics for any kind of information retrieval, and therefore for linked data information retrieval. However, we have found that most linked data information retrieval techniques pay little attention on the power of temporal feature inclusion. We propose a keyword-based linked data information retrieval framework, called TLDRet, that can incorporate temporal features and give more concise results. Preliminary evaluation of our system shows promising performance.	data retrieval;linked data	Md-Mizanur Rahoman;Ryutaro Ichise	2013		10.1007/978-3-319-06826-8_18	relevance;cognitive models of information retrieval;computer science;data mining;database;term discrimination;information retrieval;human–computer information retrieval	Web+IR	-30.55072502678913	-59.29450732310532	168086
0070f021a753159246dd9893bef0e4a91ed21c6d	detecting near-duplicate text documents with a hybrid approach	duplicate detection;shingling;jaro distance;data cleansing;data quality	Near duplicate data not only increase the cost of information processing in big data, but also increase decision time. Therefore, detecting and eliminating nearly identical information is vital to enhance overall business decisions. To identify near-duplicates in large-scale text data, the shingling algorithm has been widely used. This algorithm is based on occurrences of contiguous subsequences of tokens in two or more sets of information, such as in documents. In other words, if there is a slight variation among documents, the overall performance of the algorithm decreases. Therefore, to increase the efficiency and accuracy performances of the shingling algorithm, we propose a hybrid approach that embeds Jaro distance and statistical results of word usage frequency for fixing the ill-defined data. In a real text dataset, the proposed hybrid approach improved the shingling algorithm's accuracy performance by 27% on average and achieved above 90% common shingles.	sensor	Cihan Varol;Sairam Hari	2015	J. Information Science	10.1177/0165551515577912	data quality;computer science;data mining;database;data cleansing;world wide web;information retrieval	NLP	-27.868837960119198	-60.313310820377055	168109
ec70bbb65f858642b11a9068e1e176c48d5a10d0	resolving polysemy and pseudonymity in entity linking with comprehensive name and context modeling	entity linking;polysemy;name modeling;pseudonymity;coreference;context modeling	Names are important atomic information carriers in unstructured text. Matching names that refer to the same entities is an important issue in text analysis and a key component in many real world applications. Generally referred to as entity linking, it is defined as a task that aligns a name mentioned in free text to its corresponding entry in a Knowledge Base (KB). The difficulty of the task lies in the many-to-many correspondence between names and entities, causing the pseudonymity and polysemy issues. Existing work usually focuses on resolving polysemy by aggregating large numbers of loosely arranged features in supervised learning frameworks, with very few targeting the pseudonymity or both issues with the same depth. In this work, we tackle both issues by comprehensive modeling of an enti-ty's name and context: we tackle the pseudonymity by modeling name variants on the query name and the KB title; and polysemy by modeling heterogeneous aspects of the query and KB context. Specially, we harness entity coreferences within query and KB documents together with the external alias resources for modeling name variants, and further use the name variants to identify focused context. Moreover, we propose a recall-boosted retrieval method for efficient candidate entity generation. Experimental results show that our proposed approach outperforms the state-of-the-art systems on the benchmark data. Names, such as person, organization and location names, are important atomic information carriers in unstructured text, such as newspaper articles. The referents that these names (or the rigid designators, as defined by Kripke [39]) stand for are called ''Named Entities'' in text processing research. On the other hand, the definitions and other detailed information about the entities are usually manually compiled and stored as entries in structured Knowledge Base (KB) such as dictionaries or encyclopedias. Entity linking is therefore a task that automatically aligns a name mentioned in unstructured text to its corresponding entry in a knowledge base. Entity linking has been found useful in many real-world applications. A direct application is in an educational environment where entity linking can provide fast access to reference knowledge in study materials such as lecture notes and assignments. In encyclopedia itself, a new knowledge entry's content can be cross-referenced to existing entries, so as to build comprehensive knowledge interlinking. Wikify [51] is a successful system toward the goal of enriching Wikipedia articles with interlinks. Other systems include the Microsoft Smart Tags in later versions of Microsoft Word and …	benchmark (computing);compiler;dictionary;entity linking;knowledge base;many-to-many;microsoft word for mac;pseudonymity;rigid designator;smart tag (microsoft);supervised learning;text mining;wikipedia	Zhaoyan Ming;Tat-Seng Chua	2015	Inf. Sci.	10.1016/j.ins.2015.02.025	natural language processing;coreference;computer science;pseudonymity;data mining;entity linking;database;context model	NLP	-29.37593333399071	-65.75997793462848	168184
008377b51dd3bbe0164a9de60d7b2b8489fa4517	olap textual aggregation approach using the google similarity distance	semantic similarity;keywords;online analytical processing;olap;textual aggregation;google similarity distance;k means clustering	Data warehousing and On-Line Analytical Processing (OLAP) are essential elements to decision support. In the case of textual data, decision support requires new tools, mainly textual aggregation functions, for better and faster high level analysis and decision making. Such tools will provide textual measures to users who wish to analyse documents online. In this paper, we propose a new aggregation function for textual data in an OLAP context based on the K-means method. This approach will highlight aggregates semantically richer than those provided by classical OLAP operators. The distance used in K-means is replaced by the Google similarity distance which takes into account the semantic similarity of keywords for their aggregation. The performance of our approach is analyzed and compared to other methods such as Topkeywords, TOPIC, TuBE and BienCube. The experimental study shows that our approach achieves better performances in terms of recall, precision,F-measure complexity and runtime.	aggregate function;algorithm;big data;decision support system;emoticon;experiment;f1 score;high-level programming language;k-means clustering;online analytical processing;performance;semantic similarity;text corpus	Mustapha Bouakkaz;Sabine Loudcher;Youcef Ouinten	2016	IJBIDM	10.1504/IJBIDM.2016.076425	online analytical processing;computer science;data mining;database;information retrieval	DB	-28.75881607323148	-60.35511509839176	168546
53d1f0e4a26445644eed14ab3e869fe10fe289a2	summarization and expansion of search facets	summarization;selection of search facets;expansion	We present a novel method for summarization and expansion of search facets. To dynamically extract key facets, the ranked list of search results generated from a keyword search is coupled with the spatial distribution of relevant documents in a hierarchical taxonomy of subject classes. An evaluation of the method based on the relevance and diversity of the produced facets indicates its effectiveness for both summarization and expansion.	automatic summarization;relevance;search algorithm;taxonomy (general)	Aparna Venkat;Marie-Francine Moens	2013			automatic summarization;pattern recognition;data mining;mathematics;information retrieval	Web+IR	-29.794345633461834	-60.36739156351591	168696
82fd5065d1d92c905836eb7340f7276625c602c5	a hybrid arabic text summarization technique based on text structure and topic identification	text summarization;text extraction;arabic text summarization;topic identification	We present a hybrid approach to the problem of Arabic text summarization. Our approach focuses on segment extraction and ranking using heuristic methods that assign weighted scores to segments of text. Also, we use a text categorization system and the Arabic WordNet to identify the thematic structure of the input text in order to select the most relevant sentences obtained from the statistical analysis process. We use a tokenizer, a stemmer and other statistical tools borrowed from traditional information retrieval to identify relevant segments in the text. The source document is segmented into its major units (title, paragraphs and lines) and then, text-lines are interpreted to extract relevant segments for inclusion in the summary. The summarization system was tested by 1200 human evaluators, who were each given a copy of a newspaper article and a system-generated summary and asked to classify them as “rejected,” ”not-related,” “satisfactory,” “good,” or “accepted.” 76.92% of the summaries were judged to be “good” or “accepted” and 92.34% were judged to be “satisfactory,” or “good,” or “accepted.” These results confirm the viability of using this hybrid approach to tackle the problem of Arabic text summarization.	automatic summarization;categorization;document classification;heuristic;information retrieval;lexical analysis;wordnet	Bassam H. Hammo;Hani Abu-Salem;Martha W. Evens	2011	Int. J. Comput. Proc. Oriental Lang.	10.1142/S1793840611002206	natural language processing;text graph;text mining;speech recognition;multi-document summarization;computer science;automatic summarization;information retrieval	NLP	-28.9494475452696	-65.89014652196587	169769
6500981d76d46f2d9475880c3d1afd9ade71a187	the semeval-2007 weps evaluation: establishing a benchmark for the web people search task	web people search task;task definition;actual entity;comparative result;semeval-2007 weps evaluation;ambiguous person name;semeval-2007 evaluation exercise	This paper presents the task definition, resources, participation, and comparative results for the Web People Search task, which was organized as part of the SemEval-2007 evaluation exercise. This task consists of clustering a set of documents that mention an ambiguous person name according to the actual entities referred to using that name.	benchmark (computing);cluster analysis;entity;opensearch;pure function;semeval;spatial variability;test case;testbed;world wide web	Javier Artiles;Julio Gonzalo;Satoshi Sekine	2007			computer science;data mining;task analysis;multimedia;world wide web	NLP	-30.46077612834175	-63.56682276948016	170206
da055733130bd33eb5029501750ace084d4f7734	entailment graphs for text exploration		Taxonomy-based representations are widely used to model compactly large amounts of textual data. While current methods allow organizing knowledge at the lexical level (keywords/concepts/topics), there is an increasing demand to move towards more informative representations, which express properties of concepts and relations among them. This demand triggered our research on statement entailment graphs. In these graphs, nodes are natural language statements (propositions), comprising of predicates with their arguments and modifiers, while edges represent entailment relations between nodes. In this talk we report initial research that defines the properties of entailment graphs and their potential applications. Particularly, we show how entailment graphs can be profitably used for both knowledge acquisition and text exploration. Beyond providing a rich and informative representation, statement entailment graphs allow integrating multiple semantic inferences. So far, textual inference research focused on single, mutually independent, entailment judgments. However, in many scenarios there are dependencies among Text/Hypothesis pairs, which need to be captured consistently. This calls for global optimization algorithms for inter-dependent entailment judgments, taking advantage of the overall entailment graph structure (e.g. ensuring entailment graph transitivity). From the applied perspective, we are experimenting with entailment graphs in the context of the EXCITEMENT project industrial scenarios. We focus on the text analytics domain, and particularly on the analysis of customer interactions across multiple channels, including speech, email, chat and social media, and multiple languages (English, German, Italian). For example, we would like to recognize that the complaint they charge too much for sandwiches entails food is too expensive, and allow an analyst to compactly navigate through an entailment graph that consolidates the information structure of a large number of customer statements. Our eventual applied goal is to develop a new generation of inference-based text exploration applications, which will enable businesses to better analyze their diverse and often unpredicted client content. This task will be exemplified with data collected from real customer interactions, while referring to the EXCITEMENT Open Platform that we developed as a generic open source framework for textual inferences.	algorithm;email;evolutionary taxonomy;experiment;global optimization;information;interaction;knowledge acquisition;mathematical optimization;natural language;open platform;open-source software;organizing (structure);social media;text corpus;text mining;vertex-transitive graph	Ido Dagan;Bernardo Magnini	2013			combinatorics;logical consequence;mathematics;graph	NLP	-27.167769429559517	-61.05061967581549	170252
430c03e01c902bbd38e29ceaa03daf68a0389153	topical pagerank: a model of scientific expertise for bibliographic search		We model scientific expertise as a mixture of topics and authority. Authority is calculated based on the network properties of each topic network. ThemedPageRank, our combination of LDA-derived topics with PageRank differs from previous models in that topics influence both the bias and transition probabilities of PageRank. It also incorporates the age of documents. Our model is general in that it can be applied to all tasks which require an estimate of document–document, document– query, document–topic and topic–query similarities. We present two evaluations, one on the task of restoring the reference lists of 10,000 articles, the other on the task of automatically creating reading lists that mimic reading lists created by experts. In both evaluations, our system beats state-of-the-art, as well as Google Scholar and Google Search indexed againt the corpus. Our experiments also allow us to quantify the beneficial effect of our two proposed modifications to PageRank.	bibliometrics;cold start;collaborative filtering;experiment;f1 score;google scholar;google search;hyperlink;local-density approximation;markov chain;pagerank;portable document format;qiqqa;rife;reference implementation;roland gs;sparse matrix;tf–idf	James Gregory Jardine;Simone Teufel	2014			natural language processing;computer science;data mining;world wide web;information retrieval	ML	-28.104055697347682	-62.22807598643383	170945
51b5cecc3881f1608ce53c4229682f55e1787fa6	fast and space-efficient entity linking for queries	wikipedia;entity linking;web search;queries	Entity linking deals with identifying entities from a knowledge base in a given piece of text and has become a fundamental building block for web search engines, enabling numerous downstream improvements from better document ranking to enhanced search results pages. A key problem in the context of web search queries is that this process needs to run under severe time constraints as it has to be performed before any actual retrieval takes place, typically within milliseconds.  In this paper we propose a probabilistic model that leverages user-generated information on the web to link queries to entities in a knowledge base. There are three key ingredients that make the algorithm fast and space-efficient. First, the linking process ignores any dependencies between the different entity candidates, which allows for a O(k2) implementation in the number of query terms. Second, we leverage hashing and compression techniques to reduce the memory footprint. Finally, to equip the algorithm with contextual knowledge without sacrificing speed, we factor the distance between distributional semantics of the query words and entities into the model.  We show that our solution significantly outperforms several state-of-the-art baselines by more than 14% while being able to process queries in sub-millisecond times---at least two orders of magnitude faster than existing systems.	algorithm;baseline (configuration management);distributional semantics;downstream (software development);entity linking;knowledge base;memory footprint;ranking (information retrieval);statistical model;user-generated content;web search engine;web search query	Roi Blanco;Giuseppe Ottaviano;Edgar Meij	2015		10.1145/2684822.2685317	web query classification;computer science;machine learning;data mining;brand;entity linking;web search query;world wide web;information retrieval	Web+IR	-28.297567312799774	-62.76172513950142	171078
0e55c18eade8d183ca0ce4330ec1459ec3546db6	predict ranking of object summaries with hidden markov model	databases;probability;query processing;hidden markov model;query result;data subject;relational database;time series;data mining;adaptive model;hidden markov model ranking object summaries;hidden markov models;ranking;keyword search;probability distribution;relational databases hidden markov models probability query processing;object summaries;predictive models;hidden markov models keyword search internet spatial databases predictive models web pages computer science image databases image storage network topology;relational databases;object summary ranking prediction;ranking position;markov processes;experimental evaluation;rank scores;probability estimation;probability estimation object summary ranking prediction hidden markov model keyword search query result relational database data subject ranking position adaptive model rank scores	Ranking of Object Summaries [3] proposed a Keyword Search paradigm which produces, as a query result, a ranked list of Object Summaries (OSs) [2] in top-k and size-l; each OS summaries all data held in the relational database about a particular Data Subject (DS). This paper further investigates the volatility of the ranking position, and a robust, adaptive model is developed with probability terms basing on the Hidden Markov Model (HMM) approach. The parameters of HMM are trained by calculating the rank scores of each tuple in time series, and then this model is used to guide the ranking of OSs for further high accuracy depending on probability estimations. Preliminary Experimental evaluation on Microsoft Northwind and DBLP Databases are presented, which proves that HMM has superior discriminative properties.	dbl-browser;experiment;hidden markov model;markov chain;monte carlo method;operating system;programming paradigm;real-time clock;relational database;search algorithm;time series;volatility	Le Peng;Zhi Cai;Guowen Wu	2009	2009 International Conference on Computational Intelligence and Security	10.1109/CIS.2009.154	relational database;computer science;machine learning;pattern recognition;data mining;hidden markov model;statistics	DB	-28.561421314437318	-61.1879646182686	171170
57e7030622e0b37e7c62c73a0d94ad5cbf0a4d8e	enhancing search result clustering with semantic indexing	semantic similarity;semantic indexing;information retrieval system;digital library;document representation;web search engine;domain knowledge;semantic information;mesh;snippet clustering;search result clustering;semantic search;rough set;domain ontology;pubmed;domain specificity	Semantic search results clustering is one of the most wanted functionalities of many information retrieval systems including general web search engines as well as domain specific article portals or digital libraries. It may advice the users to describe the need for information in a more precise way. In this paper, we discuss a framework of document description extension which utilizes domain knowledge and semantic similarity. Our idea is based on application of Tolerance Rough Set Model, semantic information extracted from source text and domain ontology to approximate concepts associated with documents and to enrich the vector representation. Some document representation models including document meta-data, citations and semantic information build using MeSH ontology. We compare those models in a search result clustering problem over the freely accessed biomedical research articles from Pubmed Cetral (PMC) portal. The experimental results are showing the advantages of the proposed models.	approximation algorithm;cluster analysis;digital library;information retrieval;library (computing);ontology (information science);portals;pubmed;resource description framework;rough set;semantic search;semantic similarity;web search engine	Sinh Hoa Nguyen;Grzegorz Jaskiewicz;Wojciech Swieboda;Hung Son Nguyen	2012		10.1145/2350716.2350729	semantic similarity;semantic computing;semantic integration;explicit semantic analysis;document clustering;semantic search;computer science;data mining;semantic web stack;world wide web;information retrieval	Web+IR	-29.904720483268253	-61.31147605500357	172333
ce3bad298359cea068c5a3dde5451bcc741f1fea	a content-based link detection approach using the vector space model	best entry point;vector space model;reference model;universiteitsbibliotheek	Link detection can be seen as a special application of Focused Retrieval. This paper presents a content-based link detection approach using the Vector Space Model. We present our results, and conclude by discussing the merits and deficiencies of our approach.		Junte Zhang;Jaap Kamps	2008		10.1007/978-3-642-03761-0_40	reference model;computer science;theoretical computer science;machine learning;mathematics;vector space model	SE	-32.60823354902579	-60.09323880064845	172747
34a33cc76a80318d7c977c029c7882cd048f7088	zzisti at trec2013 temporal summarization track		Our team submitted runs for the first running of the TREC Temporal Summarization track. TS Track at TREC2013 contains two tasks, namely Sequential update Summarization and value tracking. Our Systems to each task are described in this paper respectively. In particular, Stanford CoreNLP was applied to extract the event attributes.	automatic summarization;do not track;stanford bunny	Yaoyi Xi;Bicheng Li;Jie Zhou;Yongwang Tang	2013			computer science;automatic summarization;data mining;database;information retrieval	NLP	-31.359584771021506	-63.5681331354519	173047
1b89b9813bb3796bf649f0cb2ee916db5e9265e2	topic detection based on the pagerank's clustering property		This paper introduces a method to cluster graphs of semantically related terms from texts using PageRank calculations for use in the field of text mining, e.g. to automatically discover different topics in a text corpus. It is evaluated by providing empirical results of tests by applying this method on real text corpora. It is shown that this application of the PageRank formula realizes suitable clustering such that the mean similarity between the terms in the clusters reaches a high level. A special state transition in the mean term similarity is discussed when analysing texts with stopwords.	algorithm;cluster analysis;computation;high-level programming language;observable;pagerank;state transition table;text corpus;text mining;world wide web	Mario Kubek;Herwig Unger	2011			cluster analysis;pagerank;artificial intelligence;pattern recognition;computer science	ML	-27.061213751290488	-64.07115792608224	173564
c190c15c57b0a65df061a17c786143b32aa045e9	improving the efficiency of retrieval effectiveness evaluation: finding a few good topics with clustering?		We consider the issue of using fewer topics in the effectiveness evaluation of information retrieval systems. Previous work has shown that using fewer topics is theoretically possible; one of the main issues that remains to be solved is how to find such a small set of a few good topics. To this aim, in this paper we try a novel approach based on clustering of topics. We consider various algorithms, metrics, and various features of topics that can be helpful in identifying such a set.	algorithm;cluster analysis;computer cluster;information retrieval	Kevin Roitero;Stefano Mizzaro	2016			computer science;data science;data mining;information retrieval	Web+IR	-27.45041885002338	-59.20277206657042	173691
1cbb72c45ed25c88932338cb62bb96d5876b13bc	topic identification using wikipedia graph centrality	graph-centrality algorithm;simpler baseline;wikipedia graph centrality;encyclopedic graph;external encyclopedic knowledge;automatic topic identification;wikipedia	This paper presents a method for automatic topic identification using a graph-centrality algorithm applied to an encyclopedic graph derived from Wikipedia. When tested on a data set with manually assigned topics, the system is found to significantly improve over a simpler baseline that does not make use of the external encyclopedic knowledge.	algorithm;baseline (configuration management);biased graph;centrality;experiment;wikipedia	Kino Coursey;Rada Mihalcea	2009			computer science;data science;data mining;brand;information retrieval	NLP	-27.206829780449766	-64.74414975446487	174044
d497b0c07c66cc10b874c3f5ad313608addf5a7d	deeptext2go: improving large-scale protein function prediction with deep semantic text representation	large-scale protein function prediction;text classification	UniProtKB has collected more than 88 million protein sequences by July 2017. Less than 0.2% of these proteins, however, have added experimental GO annotations. To reduce this huge gap, automatic protein function prediction (AFP) becomes increasingly important. Results on CAFA (the Critical Assessment of protein Function Annotation algorithms) benchmark demonstrates that sequence homology based methods are highly competitive in AFP. One imperative issues will be incorporating other information sources other than sequence for AFP. In contrast to using BOW (bag of words) representation in traditional text-based AFP, we proposed a new method called DeepText2GO to improve large-scale AFP by using deep semantic text representation instead. Furthermore, DeepText2GO integrates both text-based and sequence homology-based methods through a consensus approach. Extensive experiments on the benchmark dataset extracted from UniProt/SwissProt have demonstrated that DeepText2GO significantly outperformed both text-based and sequence homology-based methods, validating its superiority.	algorithm;bag-of-words model;benchmark (computing);critical assessment of function annotation;experiment;imperative programming;peptide sequence;protein function prediction;sequence homology;text-based (computing);uniprot	Ronghui You;Shanfeng Zhu	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217622	computer science;uniprot;bioinformatics;ontology (information science);protein function prediction;bag-of-words model;semantics;protein engineering;benchmark (computing);annotation	Vision	-29.681175797236566	-64.94818595768902	174417
0fdd2259effe5bfef6f49f27a34815d8eee70886	patent retrieval experiments in the context of the clef ip track 2009	patent retrieval;intellectual property;information retrieval;indexation	At CLEF 2009 the University of Hildesheim focused on the main task of the Intellectual Property Track which aims at finding prior art for a specified patent [cf. Information Retrieval Facility 2009]. The experiments of the University of Hildesheim concentrated on a baseline approach including stopword elimination, stemming and simple term queries. Furthermore only title and claim were included into the index as especially the second one is considered to be the most important patent part during a prior art search [cf. Graf/Azzopardi 2008: 64].	baseline (configuration management);experiment;information retrieval facility;stemming	Daniela Becks;Christa Womser-Hacker;Thomas Mandl;Ralph Kölle	2009		10.1007/978-3-642-15754-7_59	computer science;data mining;world wide web;information retrieval;intellectual property	Web+IR	-32.25160190102239	-63.20107765917163	174948
05c1b1ecaeb962070fc3d257a4cc9bcf9f652cf9	crf-based authors' name tagging for scanned documents	information extraction;hidden markov model;digital library;digital libraries;optical character recognition;conditional random fields crf;conditional random field	Authors' names are a critical bibliographic element when searching or browsing academic articles stored in digital libraries. Therefore, those creating metadata for digital libraries would appreciate an automatic method to extract such bibliographic data from printed documents. In this paper, we describe an automatic author name tagger for academic articles scanned with optical character recognition (OCR) mark-up. The method uses conditional random fields (CRF) for labeling the unsegmented character strings in authors' blocks as those of either an author or a delimiter. We applied the tagger to Japanese academic articles. The results of the experiments showed that it correctly labeled more than 99% of the author name strings, which compares favorably with the under 96% correct rate of our previous tagger based on a hidden Markov model (HMM).	brill tagger;conditional random field;delimiter;digital library;experiment;hidden markov model;library (computing);markov chain;optical character recognition;printing;tag (metadata)	Manabu Ohta;Atsuhiro Takasu	2008		10.1145/1378889.1378935	digital library;speech recognition;computer science;optical character recognition;world wide web;conditional random field;information retrieval	NLP	-30.72012887221514	-66.01391117892966	175286
068ce8fadaaabd4659d8c9f2b3f8909283dd3db8	the university college london at trec 2008 enterprise track	information systems;information retrieval;searching;united kingdom;documents	The University College London Information Retrieval Group participated in both the Expert Search and Document Search tasks in the TREC2008 Enterprise Track. We used a generic two-stage approach, which consists of a document retrieval stage followed by an expert association discovery stage, for expert finding. Since document search is an integral part of our expert finding approach, we have studied the relationship between document search and expert search. Due to the existence of rich features that can potentially contribute to expert finding, our expert finding approach integrates these features including anchor texts, indegree, and multiple levels of associations between experts and query terms. Our experimental results show that the introduction of features has helped improve the expert finding performance. 1. I%TRODUCTIO% Same as in TREC2007 Enterprise Track, the domain for TREC2008 Enterprise Track is the website of the CSIRO (Australian Commonwealth Scientific and Research Organization). The topics were developed in order to reflect the requests of information received by the CSIRO Enquiries staffers. The aim of the two tasks is to find a number of key pages and experts on a topic that can help the staffers to answer each request. For example, find key experts and key pages to answer the request for information on “cane toad”. Based on our approach that integrates multiple features in a two-stage expert finding model [4], we have continued investigating the effects of these features as follows in expert finding. Anchor texts: anchor texts of a document often highlight its key topic. Sometimes, keywords for identifying a document’s topic may even be missing in the document itself but exist in its anchor texts, e.g. the BMW homepage does not mention “car”, but anchor texts pointing to the page often do. We have studied the effect of anchor texts in both expert and document search. Indegree: Typically, the number of inlinks of a document is an indicator of the document’s authority. Previous work shows that there is a strong correlation between the number of inlinks and PageRank [1], and PageRank and indegree help document search on the Web [2]. We will study the effect of indegree in both document and expert search. Multiple levels of associations: We have continued using our multiple window based cooccurrence model [4]. The assumption is that there are multiple levels of associations between an expert and query terms in documents. We give higher weights to co-occurrences in smaller windows and lower weights to co-occurrences in larger windows. We have studied different window selections and combinations. In [3], we studied the relationship between ad hoc retrieval and expert finding via three parameters, namely, a background smoothing parameter in a language model, and anchor texts and indegree. Our experiments on the TREC 2007 Enterprise Track CSIRO dataset have shown that improvement in document retrieval does not necessarily lead to improvement in expert finding. Firstly, smoothing language model by a background collection model can significantly improve ad hoc retrieval performance, but does not help or even hurt expert finding. Accordingly, we give background smoothing different weights for expert and document search, respectively. Secondly, anchor text does not help document retrieval, and hurts document retrieval when weighted high in document retrieval, and indegree only slight helps ad hoc retrieval. Therefore, anchor texts and indegree have different effect in intranet search than in Web search [2]. The reason might be that, in document retrieval, documents are largely judged as relevant or not regardless of their authoritativeness, and anchor text and indegree may introduce more noise than useful information in document retrieval. However, both anchor text and indegree help expert finding. Since people appearing in authoritaReport Documentation Page Form Approved OMB No. 0704-0188 Public reporting burden for the collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Washington Headquarters Services, Directorate for Information Operations and Reports, 1215 Jefferson Davis Highway, Suite 1204, Arlington VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to a penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number. 1. REPORT DATE NOV 2008 2. REPORT TYPE 3. DATES COVERED 00-00-2008 to 00-00-2008 4. TITLE AND SUBTITLE The University College London at TREC 2008 Enterprise Track 5a. CONTRACT NUMBER	anchor text;association rule learning;backlink;directed graph;document retrieval;documentation;experiment;failure;floor and ceiling functions;hoc (programming language);information retrieval;intranet;language model;microsoft windows;pagerank;request for information;smoothing;va kernel;web search engine;world wide web	Jianhan Zhu	2008			computer science;data science;data mining;world wide web;information retrieval;information system	Web+IR	-31.255322753002986	-62.80535619560963	175534
48c68a94281f1af63764fe4c5b13d088c8bc5123	scatter of library and information science topics among bibliographic databases	library and information science;relevance information retrieval;cluster analysis;information science	Researchers and educators in library and information science borrow heavily from other disciplines and other disciplines make use of LIS concepts in organizing their own literatures. Materials relevant to LIS are scattered in the journals of many fields, and a search of one or two databases may miss relevant items. The purpose of this research was to begin to gather data to construct a mapping of LIS topics in non-LIS databases. Subject terms were taken at random from LISA, categorized, and put into BRSu0027s CROSS database. A cluster analysis was conducted on the resulting 10.8 million postings to discover possible subject relationships among the databases. A search strategy was then developed using words which describe libraries, librarianship, and information science, and real searches conducted. A total of 168,673 hits were obtained and 2655 abstracts analyzed as to relevance. Precision estimates for each database tended to verify the clustering and gave further clues as to possibly fruitful search paths. The results showed that the clustering process is a useful starting point to characterize databases, and that there are many documents relevant to LIS in non-LIS databases. © 1990 John Wiley u0026 Sons, Inc.	bibliographic database;library and information science	A. Neil Yerkey;Maryruth Glogowski	1990	JASIS	10.1002/(SICI)1097-4571(199006)41:4%3C245::AID-ASI3%3E3.0.CO;2-8	data mining;world wide web;relevance (information retrieval);computer science;bibliographic database;information retrieval;library science;database;cluster analysis;content analysis;information science	DB	-27.526853568966942	-59.3079252673259	176710
8685c75b2cb34bca0c144cc2d604211c5bbeba3f	a new web page summarization method	web pages;maximum marginal relevance;graph based ranking algorithm;multi document summarization;ranking algorithm	In this paper, we present a novel multi-webpage summarization algorithm. It adds the graph based ranking algorithm into the framework of Maximum Marginal Relevance (MMR) method, to not only capture the main topic of the web pages but also eliminate the redundancy existing in the sentences of the summary result. The experiment result indicates that the new approach has the better performance than the previous methods.	algorithm;marginal model;multi-master replication;relevance;web page	Qian Diao;Jiulong Shan	2006		10.1145/1148170.1148294	multi-document summarization;computer science;automatic summarization;web page;data mining;world wide web;information retrieval	NLP	-27.054470115747044	-62.237758423782154	177053
8049c72cba1403d8c6297aea71493e1d8efd2758	manifold-ranking based topic-focused multi-document summarization	multi document summarization;user profile;greedy algorithm	Topic-focused multi-document summarization aims to produce a summary biased to a given topic or user profile. This paper presents a novel extractive approach based on manifold-ranking of sentences to this summarization task. The manifold-ranking process can naturally make full use of both the relationships among all the sentences in the documents and the relationships between the given topic and the sentences. The ranking score is obtained for each sentence in the manifold-ranking process to denote the biased information richness of the sentence. Then the greedy algorithm is employed to impose diversity penalty on each sentence. The summary is produced by choosing the sentences with both high biased information richness and high information novelty. Experiments on DUC2003 and DUC2005 are performed and the ROUGE evaluation results show that the proposed approach can significantly outperform existing approaches of the top performing systems in DUC tasks and baseline approaches.	automatic summarization;baseline (configuration management);greedy algorithm;machine learning;multi-document summarization;upsampling;user profile	Xiaojun Wan;Jianwu Yang;Jianguo Xiao	2007			greedy algorithm;multi-document summarization;computer science;automatic summarization;machine learning;pattern recognition;data mining;information retrieval	AI	-26.576827066189246	-62.41203568887493	177814
6dab289a12ade34f4abedd209ad36d57131bb7a9	single document summarization with document expansion	document summarization	Existing methods for single document summarization usually make use of only the information contained in the specified document. This paper proposes the technique of document expansion to provide more knowledge to help single document summarization. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and then the graphranking based algorithm is applied on the expanded document set for extracting sentences from the single document, by making use of both the within-document relationships between sentences of the specified document and the cross-document relationships between sentences of all documents in the document set. The experimental results on the DUC2002 dataset demonstrate the effectiveness of the proposed approach based on document expansion. The cross-document relationships between sentences in the expanded document set are validated to be very important for single document summarization.	algorithm;automatic summarization;web page	Xiaojun Wan;Jianwu Yang	2007			well-formed document;multi-document summarization;document clustering;computer science;automatic summarization;document layout analysis;data mining;tf–idf;world wide web;information retrieval;design document listing	AI	-27.97941032864482	-63.85335232049876	177843
251b579a7a44ac44edac67cc9c4a4ebf6c7ca9c4	clustering of rough set related documents with use of knowledge from dbpedia	dbpedia;text mining;semantic clustering;rough sets;document grouping	A case study of semantic clustering of scientific articles related to Rough Sets is presented. The proposed method groups the documents on the basis of their content and with assistance of DBpedia knowledge base. The text corpus is first treated with Natural Language Processing tools in order to produce vector representations of the content and then matched against a collection of concepts retrieved from DBpedia. As a result, a new representation is constructed that better reflects the semantics of the texts. With this new representation, the documents are hierarchically clustered in order to form partition of papers that share semantic relatedness. The steps in textual data preparation, utilization of DBpedia and clustering are explained and illustrated with results of experiments performed on a corpus of scientific documents about rough sets.	dbpedia;rough set	Marcin S. Szczuka;Andrzej Janusz;Kamil Herba	2011		10.1007/978-3-642-24425-4_52	natural language processing;text mining;rough set;computer science;machine learning;data mining;information retrieval	ML	-27.94972540195595	-64.2169727611136	177911
5dcaab36fd83c75324baa46042d8c39ef8a46353	evaluation of thesaurus on sociopolitical life as information-retrieval tool		In the paper we present description of Thesaurus on Sociopolitical life, which was constructed as a tool for automatic text processing of large text collections. Specific features of the thesaurus in comparison to conventional information-retrieval thesauri for manual indexing are described. Evaluation of thesaurus-based information retrieval for short queries showed considerable improvement of the model in comparison to vector model.	comparison of raster-to-vector conversion software;information retrieval;thesaurus (information retrieval)	Natalia V. Loukachevitch;Boris V. Dobrov	2002			information retrieval;artificial intelligence;natural language processing;computer science;search engine indexing;text processing	Web+IR	-33.27799328776378	-65.06923572754205	177971
9b2b3065b99771383a46fe2c8c0d7c76c7fd102a	plagiarism detection using feature-based neural networks	cheating;neural networks;features;introductory computer science;plagiarism detection;neural network	This paper focuses on the use of code features for automatic plagiarism detection. Instead of the text-based analyses employed by current plagiarism detectors, we propose a system that is based on properties of assignments that course instructors use to judge the similarity of two submissions. This system uses neural network techniques to create a feature-based plagiarism detector and to measure the relevance of each feature in the assessment. The system was trained and tested on assignments from an introductory computer science course, and produced results that are comparable to the most popular plagiarism detectors.	artificial neural network;capacitor plague;computer science;kerrison predictor;neural network software;relevance;sensor;text-based (computing)	Steve Engels;Vivek Lakshmanan;Michelle Craig	2007		10.1145/1227310.1227324	feature;computer science;artificial intelligence;data science;data mining;artificial neural network	NLP	-31.463760601617608	-65.50084674897029	178190
d097ff6fc65258669912c34d0678ac74cd544214	trusting the results in crosslingual keyword-based image retrieval		This paper gives a brief description of the starting points for the experiments the SICS rnteam has performed in the 2006 interactive CLEF campaign.	experiment;image retrieval;swedish institute of computer science;trust (emotion)	Jussi Karlgren;Fredrik Olsson	2006			computer science;internet privacy;world wide web;information retrieval	Vision	-32.662953230774384	-62.796016923987665	179127
371c1a6575fe39cf2aa955b9556f5e8dab2b5d47	ontology-based multilingual information retrieval	multilingual information retrieval;vector space model;serveur institutionnel;archive institutionnelle;open access;indexation;archive ouverte unige;cybertheses;institutional repository	For our first participation in the CLEF evaluation campaign, our aim is to explore a translation-free technique for multilingual information retrieval. This technique is based on an ontological representation of documents and queries. We use a multilingual ontology for documents/queries representation. For each language, we use the multilingual ontology to map a term to its corresponding concept. The same mapping is applied to each document and each query. Then, we use a classic vector space model for the indexing and the querying. The main advantages of our approach are: no merging phase is required, no dependency on automatic translators between all pairs of languages exists, and adding a new language only requires a new mapping dictionary to the multilingual ontology.	dictionary;information retrieval;ontology (information science)	Jacques Guyot;Saïd Radhouani;Gilles Falquet	2005			computer science;machine learning;database;world wide web;vector space model;information retrieval	Web+IR	-32.492267515145144	-66.04473219737875	179209
628dab2640d292698c49260e4fa4d29c4c564622	semantic and bayesian profiling services for textual resource retrieval.	integrated approach;lexical database;user profile;knowledge systems	This paper presents an integrated approach to textual resource retrieval, which combines logical inference services with user profiles, in which a structured representation of the user interests is maintained. Learning is performed on documents which have been disambiguated by exploiting the WordNet lexical database, in an attempt to discover concepts describing user interests. The proposed approach relies on several additional features compared to classical lexical knowledge systems, including: structured user recommendation, numeric value management, definition of strict and negotiable constraints and keywords to retrieve potential interesting resources w.r.t. both user request and profile.	categorization;document classification;experiment;knowledge management;knowledge-based systems;lexical database;prototype;semantic search;similarity measure;text-based (computing);user profile;web search engine;wordnet	Eufemia Tinelli;Pierpaolo Basile;Eugenio Di Sciascio;Giovanni Semeraro	2006			computer science;artificial intelligence;knowledge-based systems;data mining;database;information retrieval	Web+IR	-29.54020657930812	-59.35304243436661	179465
2b56b3eee1fbb5eadf3e04e6b02c40008b611f07	trec 2009 at the university of buffalo: interactive legal e-discovery with enron emails		For the TREC 2009, the team from University at Buffalo, the State University of New York participated in the Legal E-Discovery track, working on the interactive search task. We explored indexing and searching at both the record level and the document level with the Enron email collection. We studied the usefulness of fielded search and document presentation features such as clustering documents based on email threads. For query formulation for the selected search topic, we combined a precision-oriented Specific Query method that a recall-oriented Generic Query method. Future evaluation of the effectiveness of these query techniques is still needed.	buffalo network-attached storage series;cluster analysis;email;knowledge discovery metamodel	Jianqiang Wang;Ying Sun;Paul Thompson	2009			web query classification;information retrieval;data mining;web search query;computer science;cluster analysis;thread (computing);search engine indexing	Web+IR	-31.86564514339906	-63.00602255492773	179617
75965655c48b7e7d45acdbf3a3c53bc65a1d8fb4	a hybrid genetic-bootstrapping approach to link resources in the web of data		In the Web of Data, real-world entities are represented by means of resources, for instance the southern Spanish city “Seville” that is represented by means of the resource that is available at http://es.dbpedia.org/page/Sevilla in the DBpedia dataset. Link rules are intended to link resources that are different, but represent the same real-world entities; for instance the resource that is available at https://www.wikidata.org/wiki/Q8717 represents exactly the same real-world entity as the resource aforementioned. A link rule may establish that two resources that represent cities should be linked as long as the GPS coordinates are the same. Such rules are then paramount to integrating web data, because otherwise programs would deal with every resource independently from the other. Knowing that the previous resources represent the same real-world entity allows them to merge the information that they provide independently (which is commonly known as integrating link data). State-of-the-art link rules are learnt by genetic programming systems and build on comparing the values of the attributes of the resources. Unfortunately, this approach falls short in cases in which resources have similar values for their attributes, but represent different real-world entities. In this paper, we present a proposal that hybridises a genetic programming system that learns link rules and an ad-hoc filtering technique that bootstraps them to decide whether the links that they produce must be selected or not. Our analysis of the literature reveals that our approach is novel and our experimental analysis confirms that it helps improve the (F_1) score, which is defined in the literature as the harmonic mean of precision and recall, by increasing precision without a significant penalty on recall.	semantic web;world wide web	Andrea Cimmino;Rafael Corchuelo	2018		10.1007/978-3-319-92639-1_13	world wide web;machine learning;artificial intelligence;genetic programming;filter (signal processing);merge (version control);precision and recall;computer science;recall;bootstrapping;harmonic mean	Web+IR	-29.465651894403344	-61.19686384191558	180128
f23dae2eced0c7e30809f1e522a06f2c678c8d5c	a multilingual approach to discover cross-language links in wikipedia		Wikipedia is a well-known public and collaborative encyclopaedia consisting of millions of articles. Initially in English, the popular website has grown to include versions in over 288 languages. These versions and their articles are interconnected via cross-language links, which not only facilitate navigation and understanding of concepts in multiple languages, but have been used in natural language processing applications, developments in linked open data, and expansion of minor Wikipedia language versions. These applications are the motivation for an automatic, robust, and accurate technique to identify cross-language links. In this paper, we present a multilingual approach called EurekaCL to automatically identify missing cross-language links in Wikipedia. More precisely, given a Wikipedia article the source EurekaCL uses the multilingual and semantic features of BabelNet 2.0 in order to efficiently identify a set of candidate articles in a target language that are likely to cover the same topic as the source. The Wikipedia graph structure is then exploited both to prune and to rank the candidates. Our evaluation carried out on 42,000 pairs of articles in eight language versions of Wikipedia shows that our candidate selection and pruning procedures allow an effective selection of candidates which significantly helps the determination of the correct article in the target language version.	wikipedia	Nacéra Bennacer;Mia Johnson Vioulès;Maximiliano Ariel López;Gianluca Quercini	2015		10.1007/978-3-319-26190-4_36	natural language processing;computer science;database;world wide web;information retrieval	ML	-29.060422914874607	-64.2332162289014	180212
628c15434d3f0d3a02d8ecab22148d9caeb2935b	finding relevant answers in question answering system contest		We have investigated the potential use of question answering systems and participated in the Question Answering Challenge (QAC) of National institute of informatics Test Collection for Information Retrieval systems (NTCIR). In this paper, we describe our question answering system, the preliminary results of our experiments to the contest and some possible improvement to question answering systems.	experiment;informatics;information retrieval;question answering	David Ramamonjisoa	2004			data mining;information retrieval;document retrieval;question answering;contest;informatics;computer science	NLP	-31.980295014903135	-63.36481227358121	180339
24f896e584033e5aa9a20327e25de1f387c3f430	diversity by proportionality: an election-based approach to search result diversification	search result diversification;novelty;proportionality;proportional representation;redundancy;sainte lague;political parties	This paper presents a different perspective on diversity in search results: diversity by proportionality. We consider a result list most diverse, with respect to some set of topics related to the query, when the number of documents it provides on each topic is proportional to the topic's popularity. Consequently, we propose a framework for optimizing proportionality for search result diversification, which is motivated by the problem of assigning seats to members of competing political parties. Our technique iteratively determines, for each position in the result ranked list, the topic that best maintains the overall proportionality. It then selects the best document on this topic for this position. We demonstrate empirically that our method significantly outperforms the top performing approach in the literature not only on our proposed metric for proportionality, but also on several standard diversity measures. This result indicates that promoting proportionality naturally leads to minimal redundancy, which is a goal of the current diversity approaches.	diversification (finance)	Van Dang;W. Bruce Croft	2012		10.1145/2348283.2348296	proportionality;proportional representation;artificial intelligence;data mining;redundancy	Web+IR	-26.604186629418507	-61.98305075964892	180671
34f4022ef46e1390456e599eea0c9cbef4f6128b	document clustering using small world communities	document clustering;scale free networks;small worlds;semantic clustering;information retrieval;scale free network;small world;community structure;natural language;small world network	Words in natural language documents exhibit a small world network structure. Thus the physics community provides us with an extensive supply of algorithms for extracting community structure. We present a novel method for semantically clustering a large collection of documents using small world communities. This method combines modified physics algorithms with traditional information retrieval techniques. A term network is generated from the document collection, the terms are clustered into small world communities, the semantic term clusters are used to generate overlapping document clusters. The algorithm combines the speed of single link with the quality of complete link. Clustering takes place in nearly real-time and the results are judged to be coherent by expert users. Our algorithm occupies a middle ground between speed and quality of document clustering.	algorithm;archive;cluster analysis;coherence (physics);information retrieval;natural language;real-time clock;real-time computing	Brant W. Chee;Bruce R. Schatz	2007		10.1145/1255175.1255186	data stream clustering;document clustering;fuzzy clustering;computer science;data science;scale-free network;data mining;cluster analysis;brown clustering;world wide web	Web+IR	-26.619039105413748	-59.2015425812994	181126
6b24d165583ca702458262d520b2730438ea1f40	structural relevance feedback in xml retrieval	inex;line of descent matrix;user needs;query reformulation;information retrieval system;query optimization;xml retrieval;xml;structured documents;relevance feedback	Contrarily to classical information retrieval systems, the systems that treat structured documents include the structural dimension through the document and query comparison. Thus, the retrieval of relevant results means the retrieval of document fragments that match the user need rather than the whole document. So, the structure notion should be taken into account during the retrieval process as well as during the reformulation.#R##N##R##N#In this paper we propose an approach of query reformulation based on structural relevance feedback. We start from the original query on one hand and the fragments judged as relevant by the user on the other. Structure hints analysis allows us to identify nodes that match the user query and to rebuild it during the relevance feedback step. The main goal of this paper is to show the impact of structural hints in XML query optimization. Some experiments have been undertaken into a dataset provided by INEX to show the effectiveness of our proposals.	relevance feedback;xml retrieval	Inès Kamoun Fourati;Mohamed Tmar;Abdelmajid Ben Hamadou	2009		10.1007/978-3-642-04957-6_15	document retrieval;query optimization;query expansion;web query classification;xml;ranking;relevance;computer science;concept search;data mining;database;web search query;information retrieval;query language;human–computer information retrieval	Web+IR	-33.69492575432308	-59.834933465558834	181292
3ef33e00807f35e384ddfdc368f23c1b322d06e1	prediction of performance on cross-lingual information retrieval by regression models		The purpose of this paper is to examine empirically factors having effects on performance of cross-lingual information retrieval. In order to obtain experimental data, at the NTCIR-4 CLIR task, we submitted search results of Japanese monolingual run and three bilingual runs retrieving the Japanese document collection (i.e., Chinese-Japanese, Korean-Japanese and English-Japanese runs). It turns out that a regression model of which independent variables are “quality” of query translation and “difficulty” of the search in itself explains well variations of values of average precision by CLIR runs. The “quality” of translations was measured as a score assigned by a human assessor based on the degree to which each translation is coincident with the corresponding term in the Japanese topic that the task organizers provided, and the “difficulty” of the search was represented as a value of average precision by a run using the Japanese topic (i.e., monolingual run).	archive;cross-language information retrieval	Kazuaki Kishida;Kazuko Kuriyama;Noriko Kando;Koji Eguchi	2004			experimental data;regression analysis;coincident;information retrieval;variables;pattern recognition;artificial intelligence;computer science	Web+IR	-30.98778161433689	-63.57610501685811	181763
86f4232734ccbe139dbbdee681ef0a2bc436d702	spoken document retrieval experiments for spokendoc-2 at ryukoku university (rysdt)		In this paper, we describe spoken document retrieval systems in Ryukoku University, which were participated in NTCIR-10 IR for Spoken Documents (“SpokenDoc-2”) task. In NTCIR-10 “SpokenDoc-2” task, there are two subtasks: “spoken term detection (STD) subtask” and “ad-hoc spoken content retrieval (SCR) subtask”. We participated in the SCR subtask as team RYSDT. In this paper, our SCR systems are described.	document retrieval;hoc (programming language)	Hiroaki Nanjo;Tomohiro Nishio;Takehiko Yoshimi	2013				Web+IR	-32.57531292220718	-63.65647207843552	181937
b403606e1b0eb05c1ae4e8620e4797075735dd00	chinese information retrieval based on terms and ontology	information retrieval;chinese information retrieval;indexation;term extraction;query expansion	In this paper, we describe our approach for single language information retrieval (SLIR) on Chinese language of NTCIR4 tasks. Firstly, we automatically extract terms (short-terms and long terms) from document set and use them to build indexes; secondly, for a query, we use short terms in the query and documents to do initial retrieval; thirdly, we build an ontology for the query to do query expansion and implement second retrieval. Finally, we use long terms to reorder the top N retrieved documents. Experiments show that the method achieves good results for both T-run and D-Run SLIR tasks of Chinese language.	document;information retrieval;query expansion	Lingpeng Yang;Dong-Hong Ji;Li Tang	2004			query optimization;query expansion;visual word;ranking;computer science;concept search;data mining;database;rdf query language;information retrieval;query language;human–computer information retrieval	Web+IR	-33.18318154234812	-62.987930637798314	182658
0298facf8c5c3599cb4d9f12630db4472110c949	the university of amsterdam at clef@qa 2006		We describe WiQA 2006, a pilot task aimed at studying question answering using Wikipedia. Going beyond traditional factoid questions, the task considered at WiQA 2006 was to return—given an source page from Wikipedia—to identify snippets from other Wikipedia pages, possibly in languages different from the language of the source page, that add new and important information to the source page, and that do so without repetition. A total of 7 teams took part, submitting 20 runs. Our main findings are twofold: (i) while challenging, the tasks considered at WiQA are do-able as participants achieved impressive scores as measured in terms of yield, mean reciprocal rank, and precision, (ii) on the bilingual task, substantially higher scores were achieved than on the monolingual tasks.	question answering;wikipedia	Valentin Jijkoun;Joris van Rantwijk;David Ahn;Erik F. Tjong Kim Sang;Maarten de Rijke	2006				NLP	-31.287582166176755	-64.10214787563274	182804
ae45979821da5b8e30880688a3d3440a7cd035bd	understanding combination of evidence using generative probabilistic models for information retrieval (abstract only)	information retrieval;xml retrieval;probabilistic model;geographic information retrieval;statistical language model;information need;structured documents;question answering	Structured documents, rich information needs, and detailed information about users are becoming more pervasive within everyday computing usage. Applications such as Question Answering, reading tutors, and XML retrieval demand more robust retrieval on richly annotated documents. In order to effectively serve these applications, the community will need a better understanding of the combination of evidence. In this work, I propose that the use of simple generative probabilistic models will be an effective framework for these problems. Statistical language models, which are a special case of generative probabilistic models, have been used extensively within recent Information Retrieval research. Their flexibility has been very effective in adapting to numerous tasks and problems. I propose to extend the statistical language modeling framework to handle rich information needs and documents with structural and linguistic annotations. Much of the prior work on combination of evidence has had few well-studied theoretical contributions, so I also propose to develop a sounder theoretical basis which gives more predictable results.	information needs;information retrieval;language model;pervasive informatics;question answering;xml retrieval	Paul Ogilvie	2004		10.1145/1008992.1009151	natural language processing;statistical model;information needs;question answering;relevance;cognitive models of information retrieval;computer science;data mining;world wide web;information retrieval;human–computer information retrieval	Web+IR	-29.21150701983104	-61.512293031427504	183574
3cb842d0ea41f67d1481d1d60c5aebdbd3e2c0f0	distributed multisearch and resource selection for the trec million query track	resource selection;information systems;information retrieval;data processing;searching;documents;algorithms	A distributed information retrieval system with resource‐selection and result‐set merging capability was used to search subsets of the GOV2 document corpus for the 2008 TREC Million Query Track. The GOV2 collection was partitioned into host‐name subcollections and distributed to multiple remote machines. The Multisearch demonstration application restricted each search to a fraction of the available sub‐collections that was pre‐determined by a resource‐selection algorithm. Experiment results from topic‐by‐topic resource selection and aggregate topic resource selection are compared. The sensitivity of Multisearch retrieval performance to variations in the resource selection algorithm is discussed. The information processing research group at ARSC works on problems affecting the performance of distributed information retrieval applications such as metasearch [1], federated search [2], and collection sampling [3]. An ongoing goal of this research is to guide the selection of standards and reference implementations for Grid Information Retrieval (GIR) applications [4]. Prototype GIR applications developed at ARSC help to evaluate theoretical research and gain experience with the capabilities and limitations of existing APIs, middleware, and security requirements. The TREC experiments provide an additional context to test and develop distributed IR technology. Prior TREC Terabyte (TB) and Million Query (MQ) Track experiments performed at ARSC have explored the IR performance and search efficiency of result‐set merging and ranking across small numbers of heterogeneous systems and large numbers of homogeneous systems. In the 2005 TREC [5] TB Track [6], the ARSC IR group used a variant of the logistic regression merging strategy [7], modified for efficiency, to merge results from a metasearch‐ style application that searched and merged results from two indexed copies of the GOV2 corpus [8]. One index was constructed with the Lucene Toolkit and the other index was constructed with the Amberfish application [9]. For the 2006 TREC [10] TB Track [11], the GOV2 corpus was partitioned into approximately 17,000 collections by grouping documents with identical URL host names [12]. Each query was searched against every collection and the ranked results from each collection were merged using the logistic regression algorithm used in the 2005 TB track. The large number of document collections used in the 2006 experiment, coupled with the distribution of collection size (measured in number of documents contained) that spanned five orders of magnitude, introduced significant wall‐clock, bandwidth, and IR performance problems. Follow‐up work found that the bandwidth performance could be improved somewhat without sacrificing IR performance by	aggregate data;experiment;federated search;information processing;information retrieval;logistic regression;middleware;prototype;requirement;result set;sampling (signal processing);selection algorithm;terabyte;text retrieval conference;text corpus	Christopher T. Fallen;Gregory B. Newby;Kylie McCormick	2008			data processing;computer science;data mining;database;information retrieval;information system	Web+IR	-31.565316250432925	-61.45428155844106	183981
2ec0f171622e7d3665e1df96f96e950de970b688	transfer learning for bibliographic information extraction		This paper discusses the problems of analyzing title page layouts and extracting bibliographic information from academic papers. Information extraction is an important task for easily using digital libraries. Sequence analyzers are usually used to extract information from pages. Because we often receive new layouts and the layouts also usually change, it is necessary to have a machenism for self-trainning a new analyzer to achieve a good extraction accuracy. This also makes the management becomes easier. For example, when the new layout is inputed, There is a problem of how we can learn automatically and efficiently to create a new analyzer. This paper focuses on learning a new sequence analyzer automatically by using transfer learning approach. We evaluated the efficiency by testing three academic journals. The results show that the proposed method is effective to self-train a new sequence analyer.	conditional random field;digital library;icpram;information extraction;library (computing);pattern recognition;semiconductor industry	Quang-Hong Vuong;Atsuhiro Takasu	2015			digital library;transfer of learning;computer science;data science;machine learning;world wide web;conditional random field;information retrieval	AI	-30.6444766011601	-65.93702174646114	184196
84bc7bddaeaeb0f8e67234358a754bec70f620a1	characterizing infrastructure damage after earthquake: a split-query based ir approach		Retrieving relevant information from social media based on specific requirements has become a focus area for researchers. In this paper, we propose a framework for online retrieval of tweets providing information about possible infrastructure damages, caused due to earthquakes and use the same to determine a damage score for the possibly affected locations. Identifying such tweets would not only provide a holistic view of the affected areas but would also help in taking necessary relief actions. Existing works on this topic fail to effectively capture the semantic variation in the tweets, possibly due to poor content quality, thereby providing scopes for further improvement in the mechanisms involved. Our proposed technique relies on a novel split-query based mechanism along with a pseudo-relevance feedback approach to identify the relevant tweets. The pseudo-relevance feedback approach expands on an initial set of seed tweets obtained using a semi-automatic query generation mechanism that couples topic based clustering with human annotation. Empirical validation of our proposed method on a manually annotated ground truth data reveals a considerable improvement in precision, recall and mean average precision over several baseline methods.	baseline (configuration management);cluster analysis;ground truth;holism;information retrieval;multimodal interaction;query expansion;relevance feedback;requirement;semantic similarity;semiconductor industry;social media;text corpus;word2vec	Shalini Priya;Manish Bhanu;Sourav Kumar Dandapat;Kripabandhu Ghosh;Joydeep Chandra	2018	2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1109/ASONAM.2018.8508752	data mining;cluster analysis;social media;ground truth;annotation;computer science	SE	-27.583196557491426	-64.78216910458592	184505
779d08b9fc2fa89d9087f6053ceabff0c71fa6a4	bruja system. the university of jaén at the spanish task of clefqa 2006		This paper presents our first participation in the bilingual English-Spanish track at CLEF QA 2006. The Multilingual BRUJA system is presented, a Question Answering (QA) system that works with questions in several languages and also collections in several languages. The BRUJA system is currently in its first phase of develop, so we have only run one official experiment with questions into English and the collection into Spanish. The results obtained shown that the prototype and its answer extraction phase, have to be finished and improved. An overall accuracy of 20.53% in not a good result and the system is in progress.	prototype;question answering;software quality assurance	Miguel Ángel García Cumbreras;Luis Alfonso Ureña López;Fernando Martínez Santiago;José Manuel Perea Ortega	2006				NLP	-32.02714567375852	-64.39777020939407	184568
dd8740f6b9434a1c03e44da8b71e0afb6ee00b78	automatic typing of dbpedia entities	golden standard;dns ultra lite class;graph pattern;corresponding wikipedia page;wordnet supersenses;top-level ontology;dbpedia entity;automatic typing;natural language definition;appropriate type;selected user	We present Tı̀palo, an algorithm and tool for automatically typing DBpedia entities. T̀ıpalo identifies the most appropriate types for an entity by interpreting its natural language definition, which is extracted from its corresponding Wikipedia page abstract. Types are identified by means of a set of heuristics based on graph patterns, disambiguated to WordNet, and aligned to two top-level ontologies: WordNet supersenses and a subset of DOLCE+DnS Ultra Lite classes. The algorithm has been tuned against a golden standard that has been built online by a group of selected users, and further evaluated in a user study.	adobe flash lite;algorithm;crowdsourcing;dbpedia;entity;experiment;heuristic (computer science);natural language;ontology (information science);performance;typing;usability testing;wikipedia;wordnet	Aldo Gangemi;Andrea Giovanni Nuzzolese;Valentina Presutti;Francesco Draicchio;Alberto Musetti;Paolo Ciancarini	2012		10.1007/978-3-642-35176-1_5	computer science;data mining;world wide web;information retrieval	NLP	-29.313572699550754	-64.40922826505411	185500
501b62c26fa044b56e3e088f58da5618a4078a3d	task-specific query expansion (multitext experiments for trec 2003)	question answering;query expansion;information retrieval	I. I NTRODUCTION For TREC 2003 the MultiText Project focused its efforts on the Genomics and Robust tracks. We also submitted passageretrieval runs for the QA track. For the Genomics Track primary task, we used an amalgamation of retrieval and query expansion techniques, including tiering, term re-wr iting and pseudo-relevance feedback. For the Robust Track, we examined the impact of pseudo-relevance feedback on retrie val effectiveness under the new robustness measures. All of our TREC runs were generated by the MultiText System, a collection of tools and techniques for informatio n retrieval, question answering and structured text search. The MultiText Project at the University of Waterloo has been developing this system since 1993 and has participated in TREC annually since TREC-4 in 1995. In the next section, we briefly review the retrieval methods used in our TREC 2003 runs. Depending on the track, various combinations of these methods were used to generate our runs. The remaining sections describe our activities for th e individual tracks, with the bulk of the report covering our Genomics Track results.	asch conformity experiments;query expansion;question answering;relevance feedback;structured text	David L. Yeung;Charles L. A. Clarke;Gordon V. Cormack;Thomas R. Lynam;Egidio L. Terra	2003			relevance (information retrieval);query expansion;data mining;information retrieval;query language;computer science;question answering	Web+IR	-31.91530418832622	-62.86990305011304	186306
85746d7980a4b18e4213d36c127afe5952b4d598	sinai at clef ehealth 2018 task 3. using ctakes to remove noise from expanding queries with google.		In this paper we present our participation as SINAI research group from the Universidad de Jaén at Task 3 “Consumer Health Search” specifically in sub-task 1 “Ad-hoc Search”. The main objective of the task is to provide relevant information to people seeking health advice on the web. We apply the query expansion technique using the most famous search engine at the moment: Google. We search additional information related to the query using the search engine. We identify the medical concepts in Google results using cTAKES. This recognizer provides UMLS concepts from a given text. In this way, we avoid introducing noise with words that are not related to the user query. Our system improves NDCG@10 measurement by 48% over the previous year. We also significantly reduces the response time of the Information Retrieval System (IRS) by 90% compared to previous years.	finite-state machine;hoc (programming language);query expansion;response time (technology);web search engine;ctakes	Manuel Carlos Díaz-Galiano;Pilar López-Úbeda;María Teresa Martín-Valdivia;Luis Alfonso Ureña López	2018			information retrieval;ehealth;clef;computer science	Web+IR	-32.16881201800458	-63.11197160346698	186579
2be4abe1ab34ce05b7b0fe283a14d0c3e9948609	ranked feature fusion models for ad hoc retrieval	vector space model;information re trieval;score normalization;term frequency;metasearch;in formation retrieval;system design;feature fusion;feature ranking;relevance feedback;matching model;language model	"""We introduce the Ranked Feature Fusion framework for information retrieval system design. Typical information retrieval formalisms such as the vector space model, the best-match model and the language model first combine features (such as term frequency and document length) into a unified representation, and then use the representation to rank documents. We take the opposite approach: Documents are first ranked by the relevance of a single feature value and are assigned scores based on their relative ordering within the collection. A separate ranked list is created for every feature value and these lists are then fused to produce a final document scoring. This new """"rank then combine"""" approach is extensively evaluated and is shown to be as effective as traditional """"combine then rank"""" approaches. The model is easy to understand and contains fewer parameters than other approaches. Finally, the model is easy to extend (integration of new features is trivial) and modify. This advantage includes but is not limited to relevance feedback and distribution flattening."""	hoc (programming language);information retrieval;language model;relevance feedback;systems design;tf–idf	Jeremy Pickens;Gene Golovchinsky	2008		10.1145/1458082.1458200	computer science;machine learning;pattern recognition;data mining;database;tf–idf;world wide web;vector space model;information retrieval;feature model;language model;systems design	Web+IR	-33.2649373255597	-61.65422825895422	186674
ec1dad94ea0f964eaed871941454fbd1edbf2707	isti@trec microblog track 2011: exploring the use of hashtag segmentation and text quality ranking		In the first year of the TREC Micro Blog track, our participation has focused on building from scratch an IR system based on the Whoosh IR library. Though the design of our system (CipCipPy) is pretty standard it includes three ad-hoc solutions for the track: (i) a dedicated indexing function for hashtags that automatically recognizes the distinct words composing an hashtag, (ii) expansion of tweets based on the title of any referred Web page, and (iii) a tweet ranking function that ranks tweets in results by their content quality, which is compared against a reference corpus of Reuters news. In this preliminary paper we describe all the components of our system, and the efficacy scored by our runs. The CipCipPy system is available under a GPL license.	blog;hard coding;hashtag;hoc (programming language);information retrieval;level of detail;ranking (information retrieval);social network;software propagation;web page;weight function	Giacomo Berardi;Andrea Esuli;Diego Marcheggiani;Fabrizio Sebastiani	2011			computer science;multimedia;world wide web;information retrieval	Web+IR	-30.00929291711169	-63.347893762407054	187090
7e1bdf8c9822a735fa2c0cf41275eba4deaaef16	semcluster: unsupervised automatic keyphrase extraction using affinity propagation		Keyphrases provide important semantic metadata for organizing and managing free-text documents. As data grow exponentially, there is a pressing demand for automatic and efficient keyphrase extraction methods. We introduce in this paper SemCluster, a clustering-based unsupervised keyphrase extraction method. By integrating an internal ontology (i.e., WordNet) with external knowledge sources, SemCluster identifies and extracts semantically important terms from a given document, clusters the terms, and, using the clustering results as heuristics, identifies the most representative phrases and singles them out as keyphrases. SemCluster is evaluated against two baseline unsupervised methods, TextRank and KeyCluster, over the Inspec dataset under an F1-measure metric. The evaluation results clearly show that SemCluster outperforms both methods.	affinity propagation;approximation algorithm;babelfy;baseline (configuration management);big data;cluster analysis;computation;f1 score;heuristic (computer science);organizing (structure);semantic similarity;social network;software propagation;text corpus;video post-processing;word sense;word-sense disambiguation;wordnet	Hassan H. Alrehamy;Coral Walker	2017		10.1007/978-3-319-66939-7_19	cluster analysis;metadata;heuristics;wordnet;affinity propagation;artificial intelligence;pattern recognition;computer science	NLP	-27.05644153337073	-65.00680327935387	188218
59e5ec57d1e469d44b00efb20a37dfd65895a294	automatically characterizing salience using readers' feedback	informacion documentacion;ciencias sociales	Salience is an important characteristic of information influencing users’ cognitive and emotional states. For example, salient parts of a document are those that readers will find moving or provoking. This article analyzes the main characteristics of salience and the different meanings of the concept in information retrieval and linguistics. It also presents a generic approach for identifying linguistically salient segments in a text using readers’ textual feedback. The method supports any kind of text and textual feedback. We evaluated the effectiveness of the method with a corpus of blog posts and readers’ comments. Our preliminary experiments show that the method has promising results with an fscore of 0.65. The method could also be used on 90% of commented posts which proves that it can be used on a large scale. Key-words: Salience, Information Retrieval, Feedback	blog;documentation;experiment;feedback;information retrieval;location-based service;text corpus;unfair contract terms act 1977;wiki;wikipedia	Jean-Yves Delort	2009	J. Digit. Inf.		computer science;world wide web;information retrieval	NLP	-28.387449924704605	-65.58607276566764	189124
52d57bcfaaadca9c0b1e0610e0a1347454db7833	facilitating query decomposition in query language modeling by association rule mining using multiple sliding windows	term relationship;query language;relevance model;association rule mining;information flow;document segmentation;association rule;book chapters;query expansion;language model;sliding window	This paper presents a novel framework to further advance the recent trend of using query decomposition and high-order term relationships in query language modeling, which takes into account terms implicitly associated with different subsets of query terms. Existing approaches, most remarkably the language model based on the Information Flow method are however unable to capture multiple levels of associations and also suffer from a high computational overhead. In this paper, we propose to compute association rules from pseudo feedback documents that are segmented into variable length chunks via multiple sliding windows of different sizes. Extensive experiments have been conducted on various TREC collections and our approach significantly outperforms a baseline Query Likelihood language model, the Relevance Model and the Information Flow model.	association rule learning;baseline (configuration management);experiment;information flow;language model;microsoft windows;overhead (computing);query language;relevance	Dawei Song;Qiang Huang;Stefan M. Rüger;Peter Bruza	2008		10.1007/978-3-540-78646-7_31	sargable;query optimization;query expansion;web query classification;ranking;association rule learning;boolean conjunctive query;data control language;computer science;query by example;machine learning;data mining;database;rdf query language;world wide web;information retrieval;query language;language model	Web+IR	-28.033412130021016	-61.952651522586486	189167
1c5366b7d4ada115051d798fd1688affe540c88a	an information extraction system for heterogeneous web source	web page retrieval module;web mining topical crawler web page structure classification information extraction;web pages;support vector machines;information extraction;search engines;web page structure classification module;chinese universities;structure formation;information updating module;web page structure classification;information updating module information extraction system heterogeneous web source computer science teachers chinese universities web page retrieval module web page structure classification module;data mining;topical crawler;information integration;web design;computer science teachers;web pages data mining classification algorithms educational institutions crawlers support vector machines search engines;classification algorithms;web mining;crawlers;information analysis;web design data mining information analysis;information extraction system;heterogeneous web source	Information Extraction is the task of identifying information in texts and converting it into a predefined format. In this paper, we build an information integration system which focuses on the information of computer science teachers in Chinese universities. The target of the system is to automatically extract the useful information from heterogeneous sources and re-organize them into structured format. The system includes 4 main modules: web pages retrieval module, web pages' structure classification module, information extraction module and information updating module. We have successfully applied the system to deal with 107 universities in China which shows the effect of the proposed system.	computer science;focused crawler;information extraction;moe;natural language processing;php;thesaurus;web crawler;web page;web search engine;world wide web	Ting Zhou;Chengjie Sun;Lei Lin;Bingquan Liu	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580698	support vector machine;web mining;web design;structure formation;computer science;information integration;information filtering system;web page;data mining;data analysis;world wide web;information extraction;information retrieval	AI	-29.402937758972598	-64.71862282775997	190340
30feade758d15844a5746fd0de7983d5a0e4af02	rouge 2.0: updated and improved measures for evaluation of summarization tasks		Evaluation of summarization tasks is extremely crucial to determining the quality of machine generated summaries. Over the last decade, ROUGE has become the standard automatic evaluation measure for evaluating summarization tasks. While ROUGE has been shown to be effective in capturing n-gram overlap between system and human composed summaries, there are several limitations with the existing ROUGE measures in terms of capturing synonymous concepts and coverage of topics. Thus, often times ROUGE scores do not reflect the true quality of summaries and prevents multi-faceted evaluation of summaries (i.e. by topics, by overall content coverage and etc). In this paper, we introduce ROUGE 2.0, which has several updated measures of ROUGE: ROUGE-N+Synonyms, ROUGE-Topic, ROUGETopic+Synonyms, ROUGE-TopicUniq and ROUGE-TopicUniq+Synonyms; all of which are improvements over the core ROUGE measures.	automatic summarization;faceted classification;n-gram;rouge (metric)	Kavita Ganesan	2018	CoRR		information retrieval;rouge;data mining;automatic summarization;computer science;synonym	NLP	-26.757178260881282	-65.50711601641329	190352
d53b4515a1dd4974f235694e2ea4e36b7f495e64	bayesian network-based probabilistic xml keywords filtering	slca;probabilistic xml;keywords filtering;期刊论文;bayesian networks	Data uncertainty appears in many important XML applications. Recent probabilistic XML models represent different dependency correlations of sibling nodes by adding various kinds of distributional nodes, while there does not exist a uniform probability calculation method for different dependency correlations. Since Bayesian Networks can denote various dependency correlations among nodes just by conditional probability table(CPT), this paper proposes the Bayesian Networks based probabilistic XML model PrXML-BN, and combines SLCA semantic meaning of keyword query into Bayesian Networks, then implements keywords filtering on SLCA semantic meaning. To optimize the performance of keywords filtering, two optimization strategies are proposed in this paper. In the end, experiments verify the performance of keywords filtering algorithm based on SLCA in model PrXML-BN.	bayesian network;xml	Chenjing Zhang;Kun Yue;Jinghua Zhu;Xiaoling Wang;Aoying Zhou	2012		10.1007/978-3-642-29023-7_28	computer science;machine learning;bayesian network;data mining;database;information retrieval	Web+IR	-28.384200563538084	-61.54430860460568	190423
af9105b051abb52525a4d6aae4e60ecbc680e874	rarity-oriented information retrieval: social bookmarking vs. word co-occurrence		We propose rarity-oriented retrieval methods for serendipity using two approaches. We define rare information as relevant and atypical information. We propose two approaches. In the first approach, we use social bookmark data. We introduce tag estimation to our previous work. The second approach is based on word co-occurrence in a dataset. In both approaches, we use conditional probabilities to express relevancy and atypicality. In experiments, we compared our methods with the relevance-oriented method, the diversity-oriented method, and another rarity-oriented method. Our methods using word co-occurrence obtained better nDCG scores than the other methods.	information retrieval	Takayuki Yumoto;Takahiro Yamanaka;Manabu Nii;Naotake Kamiura	2016		10.1007/978-3-319-49304-6_11	information retrieval	Web+IR	-27.944211842652336	-61.016270158731885	190838
c7e991bc555e9dacac637cef3a5649fc49b2ea54	wb-jrc-ut's participation in tac 2009: update summarization and aesop tasks		In this paper we describe our participation in the Summarization track at the 2009 Text Analysis Conference (TAC). The Summarization track was composed of two tasks: Update Summarization and Automatically Evaluating Summaries of Peers (AESOP). We submitted two runs for the former and four for the latter. In the following sections we describe our runs and discuss the results attained.	automatic summarization;color balance;windows update	Josef Steinberger;Mijail A. Kabadjov;Ralf Steinberger;Bruno Pouliquen;Massimo Poesio	2009			automatic summarization;information retrieval;data mining;computer science	NLP	-31.402044176869847	-63.726947863256484	191623
ddcacb00bbee9937b6dfb34188523ea43587d75b	recommending program committee candidates for academic conferences	expert finding;social network analysis;entity recommendation	Establishing a respectful and well-functional program committee (PC) consisting of capable PC members is one of the most important tasks for conference organizers. However, little research has been done for automatic recommendation of PC candidates. PC member finding is a complex task, which could be influenced by many factors such as the candidates' research interests' match with conference topics, the candidates' social closeness with PC chairs, the candidates' authoritativeness, as well as the candidates' publication history in the conference. To examine the importance of each feature, we build a dataset that consists of papers from four conferences: KDD, SIGIR, JCDL and GIS (2007-2011) and split it into the training and testing subsets based on the temporal information. The results show that: i) the publication history is the strongest indicator of being PC members; ii) recommendations based on the social closeness also produce reasonable good results; iii) recommend high authority researchers as PC members fails to predict the real PC because there are a large proportion of PC members who actually only have low authority values (we use the PageRank value in coauthor networks to simulate researcher's authority); and iv) applying simple linear combination of different features can make reasonable improvements.	centrality;functional programming;geographic information system;joint conference on digital libraries;pagerank;simulation	Shuguang Han;Jiepu Jiang;Zhen Yue;Daqing He	2013		10.1145/2508497.2508498	public relations;computer science;data mining;world wide web	Web+IR	-29.970719846431837	-62.23970996172876	191784
fa68f426d2cef14515abc32547cf497fc50a9b2a	a multiobjective optimization based entity matching technique for bibliographic databases		With the increasing use of on-line resources, the size of the bibliographic database is growing day by day. The available huge amount of data belong to various entities. It is difficult to automatically identify the records which belong to a particular entity. Mapping the records to the corresponding entity is termed as the entity matching problem. In bibliographic database many attributes change over time. For example affiliation of an author changes frequently. Many authors generally use different email-ids. The names of co-authors also change with time. All these aspects have made the entity matching problem challenging. Generally an entity matching task is carried out by constructing a feature vector to represent a record, then a classifier is trained to classify each feature vector. But for bibliographic database it is very difficult and time consuming to generate some manually annotated labeled data to train a classifier. Inspired by this observation, we have proposed an unsupervised approach for entity matching problem using non-dominated sorting genetic algorithm-II (NSGA-II). A new encoding strategy is used to encode the clusters in the form of a chromosome. New mutation and crossover operators are proposed which are suitable for bibliographic data clustering. Different distance measures are used to measure the dissimilarities between records. Finally, solutions are evolved using the search capability of NSGA-II. Experimental evaluations are carried out with 247 different combinations of eight objective functions for eight different bibliographic datasets. A comparative analysis with two existing systems DBLP and ArnetMiner, shows that the proposed technique can produce better results in many cases. © 2016 Elsevier Ltd. All rights reserved.	bibliographic database;cluster analysis;dbl-browser;encode;email;entity;feature vector;genetic algorithm;mathematical optimization;multi-objective optimization;online and offline;qualitative comparative analysis;sorting;statistical classification;unsupervised learning	Sumit Mishra;Sriparna Saha;Samrat Mondal	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.07.043	computer science;artificial intelligence;machine learning;data mining;information retrieval	Web+IR	-27.36456157206067	-59.90553467917684	192152
5b9184b548ed4768d6679db1ae978a8ea9094075	a semantic metadata-translation method for multilingual cross-language information retrieval			cross-language information retrieval	Xing Chen;Yasushi Kiyoki;Takashi Kitagawa	2000			human–computer information retrieval;concept search;universal networking language;mathematics;semantic equivalence;semantic grid;information retrieval;cross-language information retrieval;semantic compression;semantic web stack	ML	-32.909921501968604	-65.0600399831863	192365
8d59d9249a93b520870d41a3dfdb9cf1f1ee1eda	literal node matching based on image features toward linked data integration		Linked Open Data (LOD) has a graph structure in which nodes are represented by Uniform Resource Identifiers (URIs), and thus LOD sets are connected and searched through different domains. In fact, however, 5% of the values are literal (string without URI) even in DBpedia, which is a de facto hub of LOD. Since the literal becomes a terminal node, and we need to rely on regular expression matching, we cannot trace the links in the LOD graphs during searches. Therefore, this paper proposes a method of identifying and aggregating literal nodes that have the same meaning in order to facilitate cross-domain search through links in LOD. The novelty of our method is that part of the LOD graph structure is regarded as a block image, and then image features of LOD are extracted. In experiments, we created about 30,000 literal pairs from a Japanese music category of DBpedia Japanese and Freebase, and confirmed that the proposed method correctly determines literal identity with F-measure of 99%.	linked data;literal (mathematical logic)	Takahiro Kawamura;Shinichi Nagano;Akihiko Ohsuga	2014		10.1007/978-3-319-09912-5_15	computer science;theoretical computer science;pattern recognition;algorithm	Vision	-28.162908689290227	-65.50574806691408	192725
d9c42a06d0bad48432c070d5c95138420dc19545	an approach to the main task of qa4mre-2013		This article describes the participation of a group from the University of Évora in the CLEF2013 QA4MRE main task. Our system has a superficial text analysis based approach. The methodology starts with the preprocessing of background collection documents, whose texts are lemmatized and then indexed. Named entities and numerical expressions are sought in questions and their candidate answers. Then the lemmatizer is applied and stop words are removed. Answer patterns are formed for each question+answer pair, with a search query for document retrieval. Original search terms are expanded with synonyms and hyperonyms. Finally, the texts retrieved for each candidate response are segmented and scored for answer selection. Considering only the main questions, the system best result was obtained in the third run, having answered to 206 questions, with 0.24 c@1 and 51 correct answers. When evaluating main and auxiliary questions, the final run continued to have our better results, being answered 245 questions, with 64 right answers and 0.26 for c@1. The use of hypernyms proved to be an improvement factor in the third run, which results had a 12% increase of correct answers and a 0.02 gain in c@1.	anaphora (linguistics);document retrieval;entity;failure cause;google questions and answers;lemmatisation;numerical analysis;preprocessor;the superficial;word-sense disambiguation	Marilia Santos;José Saias;Paulo Quaresma	2013			computer science;artificial intelligence;data mining;information retrieval	NLP	-31.537271425384887	-64.5969082539719	192808
e0072b0cf8989ae4cdf8daf738cf5f547652b293	a textual-based similarity approach for efficient and scalable external plagiarism analysis - lab report for pan at clef 2010.		In this paper we present an approach to detect external plagiarism based on textual similarity. This is an efficient and precise method that can be applied over large sets of documents. The system that we have developed contains a first phase of document selection that uses a variant of tf -idf applied over the terms that appear in the two documents of the pair being compared. After this is done, we apply a more complex and accurate function based on character n-grams over the subset of documents resulting from the first step in order to extract the plagiarized passages, or matches. Once all matches for a given document are extracted, we perform a greedy match merging operation to allow in-between text in order to be compatible with certain levels of plagiarism obfuscation. In our participation in the 2nd International Competition on Plagiarism Detection, we achieved an overall score of 0.2222, ranking 11 out of 18 participants.	grams;greedy algorithm;n-gram	Daniel Micol;Óscar Ferrández;Fernando Llopis;Rafael Muñoz	2010			computer science;data mining;world wide web;information retrieval	Web+IR	-31.12528850273712	-64.73146869082943	193111
fd4049fcfe0f05b15d5c831197a5627665d2cac6	feed distillation using adaboost and topic maps	topic maps	This paper retains the experiences by participating in TREC 2007 Blog Track ‘Feed Distillation’. To perform the run various classifiers are combined, which analyze title-, contentand splog-specific features to predict the relevance of a feed related to a topic, based on the idea of AdaBoost. The implemented classifiers utilize keywords retrieved from different thesauri such as Wordnet and Wortschatz, as well as from websites providing hierarchical organized ‘ontology’ such as the ‘Open Directory Project’ and Yahoo Directory. To structure the keywords, Topic Maps are utilized according to ISO/IEC 13250:2000.	adaboost;apple open directory;blog;directory (computing);iso/iec 42010;relevance;thesaurus;topic maps;wordnet	Wai-Lung Lee;Andreas Lommatzsch;Christian Scheel	2007			main bearing;information retrieval;gas compressor;data mining;casing;cylinder;computer science;piston;electric motor;compression (physics);drive shaft	Web+IR	-30.5463229199611	-63.161914413752754	193927
bcf294ea5e521ad0de8125ac9c9bb57b0ab83d6a	adaptive maximum marginal relevance based multi-email summarization	maximum marginal relevance;content cohesion;adaptive model;multi email summarization	By analyzing the inherent relationship between the maximum marginal relevance (MMR) model and the content cohesion of emails with the same subject, this paper presents an adaptive maximum marginal relevance based multi-email summarization method. Due to the adoption of approximate computing of email content cohesion, the adaptive MMR is able to automatically adjust the parameters according to the changing of the email sets. The experimental results have shown that the email summarizing system based on this technique can increase the precision while reducing the redundancy of the automatic summary results, consequently improve the average quality of email summaries.	email;marginal model;relevance	Baoxun Wang;Bingquan Liu;Chengjie Sun;Xiaolong Wang;Bo Li	2009		10.1007/978-3-642-05253-8_46	computer science;data mining;world wide web;information retrieval	NLP	-26.806968471339836	-62.263942064223606	194299
2fa063ab950f2d039fd943258035f9c195915ddc	on microblog dimensionality and informativeness: exploiting microblogs' structure and dimensions for ad-hoc retrieval	state machine;dimensions;ranking;microblog;ad hoc retrieval	In recent years, microblog services such as Twitter have gained increasing popularity, leading to active research on how to effectively exploit its content. Microblog documents such as tweets differ in morphology with respect to more traditional documents such as web pages. Particularly, tweets are considerably shorter (140 characters) than web documents and contain contextual tags regarding the topic (hashtags), intended audience (mentions) of the document as well as links to external content(URLs).  Traditional and state of the art retrieval models perform rather poorly in capturing the relevance of tweets, since they have been designed under very different conditions. In this work, we define a microblog document as a high-dimensional entity and study the structural differences between those documents deemed relevant and those non-relevant. Secondly we experiment with enhancing the behaviour of the best observed performing retrieval model by means of a re-ranking approach that accounts for the relative differences in these dimensions amongst tweets. Additionally we study the interactions between the different dimensions in terms of their order within the documents by modelling relevant and non-relevant tweets as state machines. These state machines are then utilised to produce scores which in turn are used for re-ranking.  Our evaluation results show statistically significant improvements over the baseline in terms of precision at different cut-off points for both approaches. These results confirm that the relative presence of the different dimensions within a document and their ordering are connected with the relevance of microblogs.	baseline (configuration management);galaxy morphological classification;hashtag;hoc (programming language);interaction;relevance;web page	Jesus A. Rodriguez Perez;Joemon M. Jose	2015		10.1145/2808194.2809466	computer science;data mining;world wide web;information retrieval	Web+IR	-28.067572844270277	-61.73764901929863	194525
5da55b35248c494f1eec116e001f1b7d6dc5141f	york university at trec 2009: relevance feedback track	information retrieval;weighting functions;knowledge management;symposia;canada;feedback;mathematical models;algorithms	We describe a series of experiments conducted in our participation in the Relevance Feedback Track. We evaluate two traditional weighting models (BM25 and DFR) for the phase 1 task, which are widely used in text retrieval domain. We also evaluate a statistics-based feedback model and our proposed feedback model for the phase 2 task. Currently, we are waiting for the overview paper to facilitate further analyses.	divergence-from-randomness model;document retrieval;experiment;okapi bm25;relevance feedback	Zheng Ye;Xiangji Huang;Ben He;Hongfei Lin	2009			computer science;data science;mathematical model;data mining;feedback;world wide web;information retrieval	Web+IR	-32.18422009754141	-62.87756771785532	194951
8b8d4748b057b034d2806eb8e88307fe9f4d1182	fine-grain web site structure discovery	researcher;academic research;web information system;information extraction;web modeling;digital library;structure formation;bibliography;web information systems;reference software;library software;research paper;clustering;academic software;library management;research tool;academics;wrapper induction	Several techniques have been recently proposed to automatically derive web wrappers, i.e., programs that extract data from HTML pages, and transform them into a more structured format, typically in XML syntax. These techniques automatically induce a wrapper from a set of sample pages that share a common HTML template. An open issue, however, is how to collect suitable classes of sample pages to feed the wrapper inducer. Presently, the pages are chosen manually.In this paper, we tackle the problem of automatically discovering the main classes of pages offered by a site by exploring only a small, representative, portion of it. The web site model we propose describes the structure of the site as a graph whose nodes are classes of pages that share a common structure, and whose edges represent links among instances of the page classes. Using this model, we have developed an algorithm that accepts the url of an entry point to the target web site, visits a limited portion of the site, and produces an accurate model of the site structure. We also report on preliminary experiments performed on actual web sites, that have produced encouraging results.	algorithm;entry point;experiment;html;world wide web;wrapper library;xml	Valter Crescenzi;Paolo Merialdo;Paolo Missier	2003		10.1145/956699.956703	static web page;site map;digital library;computer science;machine learning;data mining;database;world wide web;website parse template;information extraction;information retrieval;research	DB	-31.42137862362326	-61.021694131036895	194995
10ce81dadc2e07d69c8a4f0bbdf7d14b3f37882e	the use of mmr, diversity-based reranking for reordering documents and producing summaries	selected works;text summarization;text retrieval;bepress;document retrieval	This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization. The Maximal Marginal Relevance (MMR) criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization. Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization. The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems. However, the clearest advantage is demonstrated in constructing non-redundant multi-document summaries, where MMR results are clearly superior to non-MMR passage selection.	automatic summarization;document retrieval;marginal model;maximal set;multi-master replication;relevance feedback	Jaime G. Carbonell;Jade Goldstein-Stewart	1998		10.1145/290941.291025	natural language processing;document retrieval;computer science;data mining;information retrieval	Web+IR	-31.39775623985849	-61.624691961974946	195011
c8299be43f40e346c09c1d09c3a80770d987a197	ucas at trec-2014 microblog track	information retrieval;learning machines;logistics;communications networks;regression analysis;china	University of Chinese Academy of Sciences (UCAS) participated in the first task, namely temporally-anchored ad hoc retrieval in Microblog track, aiming to efficiently and effectively retrieve tweets. Based on the conventional application of learning to rank, we incorporated a machine learning approach, such as logistic regression for selecting high-quality training data for improving the effectiveness. Except for the tweets' content features, we also used the features of the web information, external evidence, which is related with the URLS to improve the effectiveness.	academy;hoc (programming language);learning to rank;logistic regression;machine learning	Qingli Ma;Ben He;Dongxing Li	2014			logistics;computer science;data science;data mining;world wide web;china;information retrieval;regression analysis	Web+IR	-30.343408688885166	-62.800175077413755	195247
34bca9752668e509f51ba4331169ee5fbf950997	etter solutions research group trecvid 2007		"""Etter Solutions Research Group participated in the TRECVID conference for the first time in 2007. We submitted five runs in the area of fully automatic search. F_A_1_ESRG_A1_1 The first run is a required baseline, using only ASR/MT features. A sliding window of three shots is used to create a """" bag of words """" representation of the shot. F_A_1_ESRG_KN2_2 The second run is a required baseline, using no text from the ASR/MT output. This run uses visual keypoint features with shot description expansion using a semantic network. F_A_2_ESRG_AN3_3 The third run is an optional run combining ASR/MT features with shot term expansion using a semantic network. F_A_2_ESRG_AKN4_4 The fourth run is an optional run using a weighted fusion model with the ASR/MT and visual keypoint features. Shot description expansion using a semantic network is applied to both feature sets. F_A_2_ESRG_AKNF5_5 The fifth run is an optional run using a weighted fusion model with the ASR and visual keypoint features, shot description expansion, and automatic relevance feedback. Our primary focus in TRECVID 2007 is the evaluation of retrieval utilities on a video search system. We apply both automatic relevance feedback and shot (document) expansion using a semantic network. These utilities have proven to be very effective in the text retrieval domain. Our top MAP was found in the second run, consisting of visual keypoint features with shot description expansion. Runs using only the ASR/MT features performed poorly, but showed slight improvement in MAP when combined with shot description expansion."""	automated system recovery;bag-of-words model;baseline (configuration management);document retrieval;relevance feedback;semantic network	David Etter	2007			speech recognition;computer science;machine learning;data mining	Vision	-31.686614422225443	-64.0209233283116	195410
65fcaf868eaa31918137b7f24ef6b680d4326f65	a formal concept analysis-based domain-specific thesaurus and its application in document representation	document clustering;vector space model;information retrieval;document representation;concept lattice;background knowledge;document retrieval;query expansion;domain specificity;formal concept analysis	"""Many techniques in the process of document retrieval and clustering, based on the vector space model, represent documents by vectors. They ignore the conceptual relationships of terms such as synonyms, hypernyms and hyponyms and, especially, treat terms as a bag of terms. The application of conceptual relationships of terms has been proved by generating improved results for document clustering in previous studies. For those studies, thesauri like WordNet were used to provide the information of relationships between terms. However, some domain-specific terms like """"query expansion"""" and """"document clustering"""" cannot be found in these thesauri. These terms are thought of as important features in domain-specific documents. In this paper, we propose an automatic domain-specific thesaurus building approach based on Formal Concept Analysis (FCA) dealing with the problem with general thesauri. We also apply the domain-specific thesaurus as background knowledge to represent documents by concept dimension vectors. In the evaluation, an improved result by our method compared to traditional approaches is shown."""	formal concept analysis;thesaurus	Jihn-Chang J. Jehng;Shihchieh Chou;Chin-Yi Cheng	2010		10.1007/978-3-642-12179-1_36	natural language processing;document retrieval;query expansion;document clustering;computer science;formal concept analysis;data mining;vector space model;information retrieval	NLP	-28.041904467466964	-60.10395648903197	195909
7644980732b696a1351f2dcad2a3b3511be9c31e	global resources for peer-to-peer text retrieval	information retrieval;term weighting;text retrieval;query expansion;peer to peer;peer to peer information retrieval;query routing	The thesis presented in this paper tackles selected issues in unstructured peer-to-peer information retrieval (P2PIR) systems, using world knowledge for solving P2PIR problems. A first part uses so-called reference corpora for estimating global term weights such as IDF instead of sampling them from the distributed collection. A second part of the work will be dedicated to the question of query routing in unstructured P2PIR systems using peer resource descriptions and world knowledge for query expansion.	commonsense knowledge (artificial intelligence);document retrieval;information retrieval;peer-to-peer;query expansion;routing;sampling (signal processing);text corpus;tf–idf	Hans Friedrich Witschel	2007		10.1145/1277741.1277992	document retrieval;query expansion;computer science;concept search;data mining;database;information retrieval;query language;human–computer information retrieval	Web+IR	-29.743041936125536	-61.067690737011475	196326
00be4465076d0a1656d33d866845c8a7ffc192c7	content-based relevance estimation on the web using inter-document similarities	web search;inter document similarities	"""In adversarial and noisy search settings as the Web, the document-query surface level similarity can be a highly misleading relevance signal. Thus, devising content-based relevance estimation (ranking) approaches becomes highly challenging. We address this challenge using two methods that utilize inter-document similarities in an initially retrieved list. The first removes documents from the list that exhibit high query similarity, but for which there is insufficient additional support for relevance that is based on inter-document similarities. The method is based on a probabilistic model that decouples document-query similarities from relevance estimation. The second method re-ranks the list by """"rewarding"""" documents that exhibit high similarity both to the query and to other documents in the list. Both methods incorporate, in addition, at the model level, query-independent document quality estimates. Extensive empirical evaluation demonstrates the merits of our methods."""	location-based service;relevance;semantic similarity;statistical model;world wide web	Fiana Raiber;Oren Kurland;Moshe Tennenholtz	2012		10.1145/2396761.2398514	ranking;computer science;data mining;world wide web;information retrieval	Web+IR	-29.549229314392814	-59.709969019102985	196391
17c95dd89157024d82687a842aad1c3928c572b6	learning patterns to answer open domain questions on the web	web pages;fusion;distributed retrieval;expertise model;expert finding;probabilistic approach;natural language;question answering	"""While being successful in providing keyword based access to web pages, commercial search portals still lack the ability to answer questions expressed in a natural language. We present a probabilistic approach to automated question answering on the Web, based on trainable patterns, answer triangulation and semantic filtering. In contrast to the other """"shallow"""" approaches, our approach is entirely self-learning. It does not require any manually created scoring and filtering rules while still performing comparably. It also performs better than other fully trainable approaches."""	natural language;portals;question answering;web page	Dmitri Roussinov;Jose Antonio Robles-Flores	2004		10.1145/1008992.1009090	natural language processing;question answering;fusion;computer science;machine learning;web page;data mining;natural language;world wide web;information retrieval	Web+IR	-28.58147057910019	-64.15100038458041	196932
968ce0f3ce4b0013466e9479276379c7af9592c4	paraphrasing adaptation for web search ranking		Mismatch between queries and documents is a key issue for the web search task. In order to narrow down such mismatch, in this paper, we present an in-depth investigation on adapting a paraphrasing technique to web search from three aspects: a search-oriented paraphrasing model; an NDCG-based parameter optimization algorithm; an enhanced ranking model leveraging augmented features computed on paraphrases of original queries. Experiments performed on the large scale query-document data set show that, the search performance can be significantly improved, with +3.28% and +1.14% NDCG gains on dev and test sets respectively.	algorithm;conditional random field;mathematical optimization;relevance feedback;web search engine	Chenguang Wang;Nan Duan;Ming Zhou;Ming Zhang	2013			computer science;data mining;world wide web;information retrieval	NLP	-28.56498196403555	-61.58652993396928	197298
b1fd4b765fb10de0225e3afbafc0fbcff759da6a	maverick: a system for discovering exceptional facts from knowledge graphs		This paper presents Maverick, a system for discovering exceptional facts about entities in knowledge graphs. Maverick is built upon a beam-search based algorithmic framework which we proposed in a research paper that is published in SIGMOD 2018. In this demonstration proposal, we showcase an end-to-end system that includes a user-facing portal and a cache server. In Maverick, an exceptional fact about an entity of interest is modeled as a contextsubspace pair, in which the subspace is a set of attributes and the context is defined by a graph query pattern of which the entity is a match, together with other matching entities. The entity is exceptional among the entities in the context, with regard to the subspace. The portal allows users to search entities in a knowledge graph and explores exceptional facts about the entities of interest. It presents exceptional facts to users in forms of natural language sentences and illustration charts, for better interpretability of the discovered exceptional facts. The cache server stores intermediate computation results, such as pattern evaluations, exceptionality calculations, and candidate patterns. It is built for sharing computation across entities, such that repetitive computation across entities can be avoided. PVLDB Reference Format: Gensheng Zhang, and Chengkai Li. Maverick: A System for Discovering Exceptional Facts from Knowledge Graphs. PVLDB, 11 (12): 1934 1937, 2018. DOI: https://doi.org/10.14778/3229863.3236228	beam search;chart;computation;end system;end-to-end principle;entity;knowledge graph;maverick.net;natural language;server (computing);web cache	Gensheng Zhang;Chengkai Li	2018	PVLDB	10.14778/3229863.3236228	data mining;computer science;graph	DB	-29.01291640849482	-62.781966476159525	197750
49dbb51642fd70e26069e6ee2068fa93bfc08402	multimedia information retrieval based on late semantic fusion approaches: experiments on a wikipedia image collection	imageclef wikipedia collection multimedia information retrieval late semantic fusion approaches wikipedia image collection textual prefiltering semantic gap;information retrieval;multimedia computing;internet;filtering theory;textual based information retrieval content based information retrieval multimedia information fusion multimedia retrieval;image retrieval;multimedia computing filtering theory image retrieval information retrieval internet	Main goal of this work is to show the improvement of using a textual pre-filtering combined with an image re-ranking in a Multimedia Information Retrieval task. The defined three step-based retrieval processes and a well-selected combination of visual and textual techniques help the developed Multimedia Information Retrieval System to overcome the semantic gap in a given query. In the paper, five different late semantic fusion approaches are discussed and experimented in a realistic scenario for multimedia retrieval like the one provided by the publicly available ImageCLEF Wikipedia Collection.	algorithm;computation;content-based image retrieval;experiment;information retrieval;modality (human–computer interaction);wikipedia	Xaro Benavent;Ana M. García-Serrano;Ruben Granados;Joan Benavent;Esther de Ves	2013	IEEE Transactions on Multimedia	10.1109/TMM.2013.2267726	visual word;the internet;explicit semantic analysis;relevance;image retrieval;computer science;concept search;adversarial information retrieval;multimedia;world wide web;vector space model;information retrieval;human–computer information retrieval	Web+IR	-30.6992977493901	-61.49058861749658	197828
22880c97be10433a0fc1269cec3228792129439b	predictive mining of comparable entities from the web	link prediction;user experience;query logs	Comparing entities is an important part of decision making. Several approaches have been reported for mining comparable entities from Web sources to improve user experience in comparing entities online. However, these efforts extract only entities explicitly compared in the corpora, and may exclude entities that occur less-frequently but potentially comparable. To build a more complete comparison machine that can infer such missing relations, here we develop a solution to predict transitivity of known comparable relations. Named CLIQUEGROW, our approach predicts missing links given a comparable entity graph obtained from versus query logs. Our approach achieved the highest F1-score among five link prediction approaches and a commercial comparison engine provided by Yahoo!.	algorithm;cluster analysis;decision theory;entity;f1 score;microsoft research;shattered world;text corpus;user experience;vertex-transitive graph;web search engine;world wide web	Myungha Jang;Jin-Woo Park;Seung-won Hwang	2012			user experience design;computer science;data science;data mining;world wide web	Web+IR	-27.389610237797534	-64.70375931773536	198833
6aec5526bef89842c20b68e7471d3220ddfbf0d7	relevance feedback at inex 2005	relevance feedback	Relevance feedback in the INEX environment is addressed by several papers in the proceedings of the INEX 2005 Workshop [1]. The most extensive discussion is provided by Schenkel and Theobald [2]. Mihajlovic, et. al. [3], and Sauvagnat, et. al. [4], describe approaches to relevance feedback in specific sections of papers devoted to discussions of multiple INEX tasks. For detailed descriptions of the systems and methods used and results obtained in each case, see the papers cited.	relevance feedback	Carolyn J. Crouch	2006	SIGIR Forum	10.1145/1147197.1147208	computer science;artificial intelligence;multimedia;information retrieval	NLP	-32.11715245309534	-62.927890943476996	199415
0adc23fdd59b13afe9872d6d77b62d1318120015	infrastructure support for evaluation as a service	trec microblog;tweet search	"""How do we conduct large-scale community-wide evaluations for information retrieval if we are unable to distribute the document collection? This was the challenge we faced in organizing a task on searching tweets at the Text Retrieval Conference (TREC), since Twitter's terms of service forbid redistribution of tweets. Our solution, which we call """"evaluation as a service"""", was to provide an API through which the collection can be accessed for completing the evaluation task. This paper describes the infrastructure underlying the service and its deployment at TREC 2013. We discuss the merits of the approach and potential applicability to other evaluation scenarios."""	application programming interface;archive;information retrieval;organizing (structure);software deployment;terms of service;text retrieval conference	Jimmy J. Lin;Miles Efron	2014		10.1145/2567948.2577014	computer science;data mining;world wide web;information retrieval	Web+IR	-31.29500049322496	-62.74752997580156	199537
70597716614b2887b167b1863dd651bfdc2d0a70	open set evaluation of web genre identification		Web genre detection is a task that can enhance information retrieval systems by providing rich descriptions of documents and enabling more specialized queries. Most of previous studies in this field adopt the closed-set scenario where a given palette comprises all available genre labels. However this is not a realistic setup since web genres are constantly enriched with new labels and existing web genres are evolving in time. Open-set classification, where some pages used in the evaluation phase do not belong to any of the known genres, is a more realistic setup for this task. In this case, all pages not belonging to known genres can be seen as noise. This paper focuses on systematic evaluation of open-set web genre identification when the noise is either structured or unstructured. Two open-set methods combined with alternative text representation schemes and similarity measures are tested based on two benchmark corpora. Moreover, we adopt the openness test for web genre identification that enables the observation of effectiveness for a varying number of known/unknown labels.		Dimitrios A. Pritsos;Efstathios Stamatatos	2018	Language Resources and Evaluation	10.1007/s10579-018-9418-y	artificial intelligence;natural language processing;computer science;open set	NLP	-29.352718712858405	-61.15147161850661	199625
