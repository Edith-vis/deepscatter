id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
6dce2e5eb87c8ff89353b1fb9a4dd1534d9aa878	method for novelty recommendation using topic modelling		Content-based filtering methods fall short in situations where there are many similar items to recommend from, for instance when recommending articles from multiple news portals. To deal with this problem, we can consider the novelty of recommendations. Detecting novelty is usually implemented as finding the most dissimilar articles. We propose a method that uses topic modelling to find the novelty of articles. Our method ranks topics by their importance and novelty to the user and recommends articles according to their topics. We evaluate our method and compare it to other approaches to novelty recommendation and also to a method that doesn’t take novelty into account. The results show that our method was more successful than the other approaches to novelty detection in recommending relevant articles that the users were interested in. It also showed a better click-through rate than the method that didn’t incorporate novelty, although the order of its recommendations was less optimal.	novelty detection;portals;recommender system;topic model	Matús Tomlein;Jozef Tvarozek	2014			topic model;data mining;novelty;novelty detection;computer science	Web+IR	-26.5494251570165	-52.59125876531016	149617
285fc3c43337cce4d02d19f1129feae09cb253d5	to divide and conquer search ranking by learning query difficulty	query difficulty;query reformulation;information retrieval;large scale;feature extraction;web search;learning to rank;divide and conquer	Learning to rank plays an important role in information retrieval. In most of the existing solutions for learning to rank, all the queries with their returned search results are learnt and ranked with a single model. In this paper, we demonstrate that it is highly beneficial to divide queries into multiple groups and conquer search ranking based on query difficulty. To this end, we propose a method which first characterizes a query using a variety of features extracted from user search behavior, such as the click entropy, the query reformulation probability. Next, a classification model is built on these extracted features to assign a score to represent how difficult a query is. Based on this score, our method automatically divides queries into groups, and trains a specific ranking model for each group to conquer search ranking. Experimental results on RankSVM and RankNet with a large-scale evaluation dataset show that the proposed method can achieve significant improvement in the task of web search ranking.	information retrieval;learning to rank;search algorithm;web search engine	Zeyuan Allen Zhu;Weizhu Chen;Tao Wan;Chenguang Zhu;Gang Wang;Zheng Chen	2009		10.1145/1645953.1646255	sargable;query optimization;query expansion;divide and conquer algorithms;web query classification;ranking;feature extraction;computer science;machine learning;concept search;data mining;web search query;ranking svm;information retrieval;query language;learning to rank	Web+IR	-27.93857219413514	-54.45274779374476	149675
3627c334d2b417ad67c66b92758d115fa09bc579	content based re-ranking scheme for video queries on the web	ranking function;user preference;ranking svm;domain-based concept;video search engine;video queries;video domain;user query;video retrieval system;video retrieval;re-ranking scheme;support vector machines;search engines;user interfaces;internet	We present a novel content-based re-ranking scheme for enhancing the precision of video retrieval on the web. We use ontology specified knowledge of the video domain to map user queries to domain-based concepts. The user preferences are learned implicitly from the web logs of users' interaction with a video search engine. A ranking SVM is trained for each concept to learn the ranking function which incorporates user preferences for the concept. The videos are represented by a set of ingeniously derived content based features which are based on MPEG-7 descriptors. Our re-ranking scheme thus effectively re-ranks results for new text queries submitted to our video retrieval system, leading to better satisfaction of the users' information need.	audio description;blog;high- and low-level;information needs;mpeg-7;natural language;ranking (information retrieval);ranking svm;user (computing);web search engine;world wide web	Anupama Mallik;Santanu Chaudhury;Ankur Jain;Mansi Matela;Pasumarthi Poornachander	2007	2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops		document retrieval;information needs;support vector machine;the internet;computer science;machine learning;multimedia;user interface;law;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-28.508942216940277	-52.4196199513208	149770
1ce55e58009f4ff3644d98c4f936a0aa1cd4f222	a music search engine built upon audio-based and web-based similarity measures	search engine;web pages;context based retrieval;information re trieval;vector space;cross media retrieval;music similarity;large scale;natural language;music information retrieval;web retrieval;similarity measure;music search engine	An approach is presented to automatically build a search engine for large-scale music collections that can be queried through natural language. While existing approaches depend on explicit manual annotations and meta-data assigned to the individual audio pieces, we automatically derive descriptions by making use of methods from Web Retrieval and Music Information Retrieval. Based on the ID3 tags of a collection of mp3 files, we retrieve relevant Web pages via Google queries and use the contents of these pages to characterize the music pieces and represent them by term vectors. By incorporating complementary information about acous tic similarity we are able to both reduce the dimensionality of the vector space and improve the performance of retrieval, i.e. the quality of the results. Furthermore, the usage of audio similarity allows us to also characterize audio pieces when there is no associated information found on the Web.	information retrieval;mp3;monkey's audio;natural language;web application;web page;web search engine;world wide web	Peter Knees;Tim Pohle;Markus Schedl;Gerhard Widmer	2007		10.1145/1277741.1277818	cognitive models of information retrieval;vector space;computer science;web page;multimedia;natural language;world wide web;information retrieval;search engine;human–computer information retrieval	Web+IR	-30.040458855655213	-57.20506301552433	150043
6b19ac4c10ed22cf96c830d4642eb64a3dfe2fd0	beyond hyperlinks: organizing information footprints in search logs to support effective browsing	search engine;web pages;exploratory search;topic maps;information space;information footprints;multi resolution topic maps;beyond hyperlinks;effective browsing;information need;multi resolution;information search and retrieval	"""While current search engines serve known-item search such as homepage finding very well, they generally cannot support exploratory search effectively. In exploratory search, users do not know their information needs precisely and also often lack the needed knowledge to formulate effective queries, thus querying alone, as supported by the current search engines, is insufficient, and browsing into related information would be very useful. Currently, browsing is mostly done by following hyperlinks embedded on Web pages. In this paper, we propose to leverage search logs to allow a user to browse beyond hyperlinks with a multi-resolution topic map constructed based on search logs. Specifically, we treat search logs as """"footprints"""" left by previous users in the information space and build a multi-resolution topic map to semantically capture and organize them in multiple granularities. Such a topic map can support a user to zoom in, zoom out, and navigate horizontally over the information space, and thus provide flexible and effective browsing capabilities for end users. To test the effectiveness of the proposed methods of supporting browsing, we rely on real search logs and a commercial search engine to implement our proposed methods. Our experimental results show that the proposed topic map is effective to support browsing beyond hyperlinks."""	browsing;embedded system;exploratory search;hyperlink;information needs;organizing (structure);topic maps;web page;web search engine	Xuanhui Wang;Bin Tan;Azadeh Shakery;ChengXiang Zhai	2009		10.1145/1645953.1646110	topic maps;information needs;computer science;web page;data mining;database;world wide web;information retrieval;search engine	Web+IR	-30.482229457773226	-53.25783382945762	150468
0cb842c71fd76d57fbbe4c77531649d1aa54e320	webput: efficient web-based data imputation	web based data imputation;webput;incomplete data;080604 database management;080107 natural language processing;080000 information and computing sciences;data quality	In this paper, we present WebPut, a prototype system that adopts a novel web-based approach to the data imputation problem. Towards this, Webput utilizes the available information in an incomplete database in conjunction with the data consistency principle. Moreover, WebPut extends effective Information Extraction (IE) methods for the purpose of formulating web search queries that are capable of effectively retrieving missing values with high accuracy. WebPut employs a confidence-based scheme that efficiently leverages our suite of data imputation queries to automatically select the most effective imputation query for each missing value. A greedy iterative algorithm is also proposed to schedule the imputation order of the different missing values in a database, and in turn the issuing of their corresponding imputation queries, for improving the accuracy and efficiency of WebPut. Experiments based on several real-world data collections demonstrate that WebPut outperforms existing approaches.	dirty data;experiment;geo-imputation;greedy algorithm;information extraction;iterative method;missing data;prototype;web application;web search engine;web search query;world wide web	Zhixu Li;Mohamed A. Sharaf;Laurianne Sitbon;Shazia Wasim Sadiq;Marta Indulska;Xiaofang Zhou	2012		10.1007/978-3-642-35063-4_18	data quality;computer science;data mining;database;information retrieval	DB	-30.50479133751937	-59.10469768717415	150958
abfd04d0ca8d6c2a1ceeceb3537da1660811cd52	term-based approach for linking digital news stories		The World Wide Web has become a platform for news publication in the past few years. Many television channels, magazines and newspapers have started publishing digital versions of the news stories online. It is observed that recommendation systems can automatically process lengthy articles and identify similar articles to readers based on a predefined criteria i.e. collaborative filtering, content-based filtering approach. The paper presents a content-based similarity measure for linking digital news stories published in various newspapers during the preservation process. The study compares similarity of news articles based on human judgment with a similarity value computed automatically using common ratio measure for stories. The results are generalized by defining a threshold value based on multiple experimental results using the proposed approach.		Muzammil Khan;Arif Ur Rahman;Muhammad Daud Awan	2018		10.1007/978-3-319-73165-0_13	recommender system;collaborative filtering;information retrieval;computer science;newspaper;similarity measure;publishing;text processing	NLP	-27.427112918027746	-55.41380180693975	151039
995117a878684bcb50b92e567de032098e82a7fa	combining link and content information in web search		As the World Wide Web has grown, search engines have become the preferred method for finding information on the Web; it is now almost impossible to find specific information without them. This has given rise to a problem: How can we automatically determine the quality and relevance of a Web page to a particular query? The original search engines used the content of a page to determine its relevance, but recently it was found that results could be greatly improved by incorporating information gleaned from the link structure as well. In this chapter, we describe two of the most well-known algorithms that do this: HITS [17] and PageRank [18], and also survey some of their improvements. We then introduce our algorithm, Query-Dependent PageRank, which maintains query-time efficiency while alleviating the problem of topic drift. Experiments on two large subsets of the Web indicate that our algorithm significantly outperforms PageRank in the (human-rated) quality of the pages returned, while remaining efficient enough to be used in today’s large search engines. After presenting these results and a discussion of scalability, we leave the reader with some open questions and possible directions for future work.	web search engine	Matthew Richardson;Pedro M. Domingos	2004			web service;web mining;static web page;web development;data web;web mapping;web design;web search engine;semantic search;web standards;web crawler;web navigation;web page;semantic web stack;web intelligence;web search query;world wide web;search engine	Web+IR	-31.3238983174549	-54.092760801330044	151332
752c45e471d0093fbaebd4709aaae1784a336229	pming distance: a collaborative semantic proximity measure	information extraction;context modeling pming distance collaborative semantic proximity measure semantics approach ontologies semantic annotation search engines web information source semantic similarity collaborative proximity measure indexing information semantic content extraction proximity distance relatedness degree collaborative change semantic association clustering human perception modeling;data mining;semantic similarity measure;semantic similarity measure data mining information extraction;search engines groupware ontologies artificial intelligence pattern clustering	One of the main problems that emerges in the classic approach to semantics is the difficulty in acquisition and maintenance of ontologies and semantic annotations. On the other hand, the flow of data and documents which are accessible from the Web is continuously fueled by the contribution of millions of users who interact digitally in a collaborative way. Search engines, continually exploring the Web, are therefore the natural source of information on which to base a modern approach to semantic annotation. A promising idea is that it is possible to generalize the semantic similarity, under the assumption that semantically similar terms behave similarly, and define collaborative proximity measures based on the indexing information returned by search engines. In this work PMING, a new collaborative proximity measure based on search engines, which uses the information provided by search engines, is introduced as a basis to extract semantic content. PMING is defined on the basis of the best features of other state-of-the-art proximity distances which have been considered. It defines the degree of relatedness between terms, by using only the number of documents returned as result for a query, then the measure dynamically reflects the collaborative change made on the web resources. Experiments held on popular collaborative and generalist engines (e.g. Flickr, Youtube, Google, Bing, Yahoo Search) show that PMING outperforms state-of-the-art proximity measures (e.g. Normalized Google Distance, Flickr Distance etc.), in modeling contexts, modeling human perception, and clustering of semantic associations.	cluster analysis;dataflow;experiment;flickr;information source;learning to rank;normalized google distance;ontology (information science);semantic similarity;web resource;web search engine;world wide web	Valentina Franzoni;Alfredo Milani	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.226	semantic similarity;semantic computing;semantic integration;semantic search;proximity search;semantic grid;computer science;social semantic web;data mining;semantic web stack;semantic technology;world wide web;information extraction;information retrieval;semantic analytics	Web+IR	-28.992446090119774	-58.211841350999684	153334
ee449e69087c2f42cae25f92f0d2bbcb7d6895f1	are wikipedia resources useful for discovering answers to list questions within web snippets	search engine;resource use;question answering system;web mining;question answering	This paper presents LiSnQA, a list question answering system that extracts answers to list queries from the short descriptions of web-sites returned  by search engines, called web snippets. LiSnQA mines Wikipedia resources in order to obtain valuable information that assists in the extraction of these answers. The interesting  facet of LiSnQA is, that in contrast to current systems, it does not account for lists in Wikipedia, but for its redirections, categories,  sandboxes, and first definition sentences. Results show that these resources strengthen the answering process.  	wikipedia	Alejandro Figueroa	2008		10.1007/978-3-642-01344-7_13	question answering;computer science;data mining;world wide web;information retrieval;search engine	AI	-30.92403123665373	-54.76348543579036	154266
08e25da40f797ff9b18c26633fb39677b69b95fa	personalized search support for networked document retrieval using link inference	personalized search;article letter to editor;document retrieval	Constructing a query consisting of a set of terms or descriptors is often an iterative process. To the user, the starting query and the nal result could be strongly related. These two queries could even be worthy of a link between them. This paper presents a method for deciding when a link between two descriptors is justiied. The decision hinges on the way in which the user has moved from one to the other. In order to allow for users with dierent levels of experience and diierent backgrounds, we introduce a number of parameters with which the inference process can be controlled.	document retrieval;iteration;personalized search	F. C. Berger;Patrick van Bommel	1996		10.1007/BFb0034732	document retrieval;document clustering;computer science;data mining;world wide web;information retrieval	Web+IR	-33.68224779260764	-58.89344301195796	154755
9c42300494639d3dd537ba0b8f48e335b1d9a5eb	lods: a linked open data based similarity measure	linked open data based similarity measure lod based similarity measure classification dimensions property dimensions lod resources ontological dimensions;semantics;ontologies semantics encyclopedias electronic publishing internet knowledge based systems;internet;ontologies;electronic publishing;pattern classification ontologies artificial intelligence;encyclopedias;knowledge based systems	With the rapid evolution of Linked Open Data (LOD), researchers are exploiting it to solve particular problems such as semantic similarity assessment. Existing LOD-based semantic similarity approaches attach compared data (terms or concepts) to LOD resources to exploit their semantic descriptions and relationships with other resources and estimate the degree of overlap between resources. Current approaches suffer from two limitations: they focus on the analysis of links between resources and ignore the important taxonomic structure of concepts and categories used to describe resources. On the other hand, they do not exploit interlinks between LOD resources in order to enrich data used to compute the similarity score. In this paper, we overcome the above limitations by proposing a new LOD-based similarity measure based on the combination of ontological, classification and property dimensions of LOD resources.	computation;experiment;gene ontology term enrichment;linked data;semantic similarity;similarity measure;statistical classification;web services discovery;web service	Nasredine Cheniki;Abdelkader Belkhir;Yacine Sam;Nizar Messai	2016	2016 IEEE 25th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises (WETICE)	10.1109/WETICE.2016.58	semantic similarity;the internet;computer science;ontology;artificial intelligence;data mining;database;semantics;electronic publishing;world wide web;information retrieval;encyclopedia	SE	-28.848479898027545	-58.427266986755534	155254
f481ddbc50c5496cb70007b133a431af9736b5dc	data extraction for search engine using safe matching	information extraction;search engines;automatic wrapper	Our study shows that algorithms used to check the similarity of data records affect the efficiency of a wrapper. A closer examination indicates that the accuracy of a wrapper can be improved if the DOM Tree and visual properties of data records can be fully utilized. In this paper, we develop algorithms to check the similarity of data records based on the distinct tags and visual cue of the tree structure of data records and the voting algorithm which can detect the similarity of data records of a relevant data region which may contain irrelevant information such as search identifiers to distinguish the potential data regions more correctly and eliminate data region only when necessary. Experimental results show that our wrapper performs better than state of the art wrapper WISH and it is highly effective in data extraction. This wrapper will be useful for meta search engine application, which needs an accurate tool to locate its source of information.	web search engine	Jer Lang Hong;Ee Xion Tan;Fariza Fauzi	2011		10.1007/978-3-642-25832-9_77	computer science;data mining;world wide web;information retrieval	AI	-29.755148954329744	-55.01029487433568	155386
9ac67920546a64671f3bee21e669e4566b3fe184	an improved shark-search algorithm based on multi-information	textual information;search engine;focused crawling;web pages;search engines;search algorithm;improved shark search algorithm;internet;uniform resource locators web pages educational institutions marine animals computer science web sites search engines crawlers information science heuristic algorithms;search engines internet;world wide web;general purpose search engines;multiinformation;web pages improved shark search algorithm multiinformation world wide web general purpose search engines focused crawling textual information	With the enormous growth of world wide web, existing general-purpose search engines have presented much more limitations. Focused crawling is increasingly seen as a potential solution. The key of focused crawling is how to accurately predict the relevance of the unvisited web pages pointed to by known URLs to a given topic. A formalized description of the predicting process is introduced. Then, four policies are proposed to predict the relevance of unvisited pages to a topic. Further the combinations of these policies are used to improve the Shark-Search, which is a classic focused crawling algorithm mainly based on the textual information of Web pages. A large number of experiments were carried out to identify the optimized combination and verify that the improved Shark-Search is more effective than the original one.	experiment;focused crawler;general-purpose modeling;rm-odp;relevance;search algorithm;total correlation;web page;web search engine;world wide web	Zhumin Chen;Jun Ma;Jingsheng Lei;Bo Yuan;Li Lian	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.166	web modeling;site map;web search engine;computer science;web crawler;web navigation;distributed web crawling;data mining;hits algorithm;web search query;world wide web;website parse template;information retrieval;search engine	DB	-29.769887221182625	-55.39057454521682	155689
2fe8a99feef93ba13880f8291cde61b804b76f70	custom ordering on digital library information retrieval	o;information retrieval;digital library;otilde;reordenacao;recuperacao de informacoes;atilde;user profile;es;personaliza ccedil;reordena ccedil;recupera ccedil;bibliotecas digitais;o de informa ccedil;personalizacao	Suitable ranking of results is crucial when searching large collections available in Digital Libraries (BDs) so the first document presented to users are the most relevant to a search criterion. Several works acknowledge the need to consider the user's preferences for ranking search results, but it is hard to determine the user profile model and how much weight individual preferences should have in each application. This paper proposes a technique for re-ordering the results of searches on BDs that combines the relevance of each result item for the posed query with the individual user's preferences. In this proposal, the user profile maintains the characteristics of the documents that he/she prefers. This profile is built dynamically and transparently, based on the values of some metadata elements describing the documents accessed by the user. The proposal was validated in a BD providing access to a collection of literature documents.	digital library;information retrieval	Carlos A. Furtado;Roberto Willrich;Renato Fileto;Fernando de L. Siqueira;Saïd Tazi	2009		10.1145/1858477.1858505	digital library;ranking;computer science;data mining;database;world wide web;information retrieval	NLP	-32.17668005098876	-55.61860908075276	155753
f90309f387ae6d582017b206fb85cc6fd4c5bcbb	ranking web pages using collective knowledge		Indexing is a crucial technique for dealing with the massive amount of data present on the web. Indexing can be performed based on words or on phrases. Our approach aims to efficiently index web documents by employing a hybrid technique in which web documents are indexed in such a way that knowledge available in the Wikipedia and in meta-content is efficiently used. Our preliminary experiments on the TREC dataset have shown that our indexing scheme is a robust and efficient method for both indexing and for retrieving relevant web pages. We ranked term queries in different ways, depending if they were found in Wikipedia pages or not. This paper presents our preliminary algorithm and experiments for the ad-hoc and diversity tasks of the TREC 2011 Web track. We ran our system on the subset B (50 million web documents) from the ClueWeb09 dataset. Categories and Subject Description Web Information Retrieval: Content Analysis, Indexing, and Ranking	algorithm;automatic summarization;email filtering;experiment;faceted classification;hoc (programming language);index (publishing);information retrieval;money;web page;web search engine;wikipedia	Falah Hassan Al-akashi;Diana Inkpen	2011			data mining;information retrieval;web page;collective intelligence;search engine indexing;computer science;ranking;content analysis	Web+IR	-28.85151503023721	-57.244521217643566	156799
1ab758492347723ae8ad20257715f3fd49e75c27	beyond dcg: user behavior as a predictor of a successful search	search engine evaluation;web pages;query log analysis;search sessions;web search engine;user behavior models;keyword search;web search;user behavior;information need;user satisfaction	Web search engines are traditionally evaluated in terms of the relevance of web pages to individual queries. However, relevance of web pages does not tell the complete picture, since an individual query may represent only a piece of the user's information need and users may have different information needs underlying the same queries. In this work, we address the problem of predicting user search goal success by modeling user behavior. We show empirically that user behavior alone can give an accurate picture of the success of the user's web search goals, without considering the relevance of the documents displayed. In fact, our experiments show that models using user behavior are more predictive of goal success than those using document relevance. We build novel sequence models incorporating time distributions for this task and our experiments show that the sequence and time distribution models are more accurate than static models based on user behavior, or predictions based on document relevance.	definite clause grammar;experiment;information needs;kerrison predictor;relevance;web page;web search engine	Ahmed Hassan Awadallah;Rosie Jones;Kristina Lisa Klinkner	2010		10.1145/1718487.1718515	information needs;web query classification;organic search;user modeling;computer user satisfaction;web design;web search engine;computer science;user requirements document;web page;data mining;search analytics;web search query;world wide web;information retrieval;search engine	Web+IR	-33.401706440138994	-52.53350082589386	157254
fb56b37f94137e029243fb83048711c44ec50fd4	semantic bookworm: mining literary resources revisited	google;text mining;text analysis data mining semantic web;semantics;text analysis;data mining;conference contribution;computer architecture;visualization;semantic concepts semantic bookworm mining literary resources revisited scholarly text analysis text based bookworm tool;data mining semantic analysis digital humanities text mining;digital humanities;semantics visualization time frequency analysis text analysis google computer architecture;computer science;time frequency analysis;semantic analysis	In this paper, we describe Semantic Bookworm-a tool that supports scholarly text analysis. In contrast to the text-based Bookworm tool, the Semantic Bookworm identifies semantic concepts.	text-based (computing)	Annika Hinze;Michael Coleman;Sally Jo Cunningham;David Bainbridge	2016	2016 IEEE/ACM Joint Conference on Digital Libraries (JCDL)	10.1145/2910896.2925444	semantic computing;text mining;time–frequency analysis;visualization;computer science;data science;data mining;semantic web stack;information retrieval	SE	-27.21056745783151	-56.7946486743699	157270
da6a94d7845aa267237598dd1946338c2be54991	web content extraction using clustering with web structure		Web content extraction is an essential part of data preprocessing in web information system. An algorithm for web content extraction based on clustering with web structure is proposed. The whole process can be divided in two steps. In the first step, clustering with the web pages collected from different websites. During this processing, similarity measurement of web page based on dynamic programming of weight is used. First, the web page is parsed to DOM tree; second, the weight is assigned to every node according to the position of the node and the amount of nodes in same depth and the depth of the DOM tree; third, calculating the similarity of two pages according to the given formula. When the first step is finished, web pages with similar structure would be divided into a set. In the second step, pages in the same set are compared and the same parts of pages will be removed, thus the remain is the web content. Experiments show that the proposed algorithm works with great effectiveness and accuracy.	web content	Xiaotao Huang;Yan Gao;Liqun Huang;Zhizhao Zhang;Yuhua Li;Fen Wang;Ling Kang	2017		10.1007/978-3-319-59072-1_12	web page;artificial intelligence;information retrieval;web mining;machine learning;data pre-processing;cluster analysis;computer science;parsing;web information system;document object model;web content	Web+IR	-27.837099275829335	-56.07700364426751	157510
c20ff7b741b1ed57c9d6b976fa80a855f9193253	automatically selecting answer templates to respond to customer emails	contact center	Contact center agents typically respond to email queries from customers by selecting predefined answer templates that relate to the questions present in the customer query. In this paper we present a technique to automatically select the answer templates corresponding to a customer query email. Given a set of query-response email pairs we find the associations between the actual questions and answers within them and use this information to map future questions to their answer templates. We evaluate the system on a small subset of the publicly available Pine-Info discussion list email archive and also on actual contact center data comprising customer queries, agent responses and templates.	archive;email;google questions and answers;pine;text corpus	Rahul Malik;L. Venkata Subramaniam;Saroj Kaushik	2007			computer science;data mining;internet privacy;world wide web	AI	-27.429554658854983	-54.95833424356891	158352
e1994db3ca11c72d9f98d674c3bb98ec82640d20	"""when naïve is not enough: bringing naïve bayes text categorization to """"surface"""""""		Since information has become more and more available in digital format, especially on the World Wide Web, organizing and classifying digital documents, making them accessible and presenting them in a proper way are becoming important issues. Digital Library Management Systems (DLMSs) are an example of systems that manage collections of multi-media digitalized data and include components that perform the storage, access, retrieval, and analysis of the collections of data. Solutions for content organization, access and interaction are required in order to let users express their information needs, especially when the specific request is not clear in their mind [2]. Automatic categorization systems may be used to build classification schemes (or taxonomies, or subject hierarchies), in order to browse, explore, and retrieve resources from collections of digital objects. Visualization frameworks that give straightforward graphical explanation of organization and classification of data have been successfully designed and implemented [3]. However, this visualization approaches have been rarely, if never, applied in the area of textual document categorization. However, these visualization approaches have been rarely applied in the area of textual document categorization. In fact, the representation of textual information is particularly challenging: how can the semantics of textual documents be captured and represented through graphs? A recent probabilistic approach on Automated Text Categorization (ATC) that represent documents on the two– dimensional space [1, 4] has shown to be a valid visualization tool to understand the relationships between categories of textual documents, and help users to visually audit the classifier and identify suspicious training data. In this work, we apply the same idea of the twodimensional representation of documents to the case of the Naı̈ve Bayes (NB) classifier. 2 Naı̈ve Bayes Categorization	browsing;categorization;digital library;digital rights management;document classification;graph (discrete mathematics);graphical user interface;information needs;management system;mind;naive bayes classifier;naivety;organizing (structure);taxonomy (general);world wide web	Giorgio Maria Di Nunzio	2007			naive bayes classifier;information retrieval;golf ball;acoustics;categorization;detector;transmitter;ultrasonic sensor;computer science;artificial intelligence;ball (bearing);pattern recognition	Web+IR	-33.13847078167012	-56.86681234208106	158963
ea9cf2880125eadcccc42c3d31a1188c7d45a629	personalized query expansion for web search using social keywords	query suggestion;social networks;temporal indexing	A person is generally motivated by the thoughts of a set of people in his social network and he has different degree of interest in each of those people considering the common interest, trust, philosophy and several other factors between them. In this work, we model the social context of the person as the status messages generated by those socially associated people and propose a method to use his social context to improve the web search query expansion process for him. Our method extracts and ranks keywords from the status messages, which are relevant with the initial search query that is to be expanded. The selected keyword is then appended with the initial query to form socially expanded query. We show that useful search queries can be formed in terms of specialization and parallel movement, if we use the socially expanded query for further expansion using traditional expansion processes. Our method ensures privacy by keeping the social network data segregated from search engine vendors. Moreover, we provide directions for implementing this method without the intervention of search engine vendors. Nevertheless, the background search process is considered to be provided by search engine vendors in the form of Application Program Interface (API).	application programming interface;list of code lyoko episodes;partial template specialization;privacy;query expansion;social network;web search engine;web search query	Sheikh Muhammad Sarwar;Md. Anowarul Abedin;A. H. M. Sofi Ullah;Abdullah Al Mamun	2013		10.1145/2539150.2539266	search-oriented architecture;sargable;query optimization;query expansion;web query classification;computer science;data mining;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-31.983152866698283	-55.95747249921346	159103
30636caf768ca6ce621bbb545e508518d2ff381f	recombination operators in genetic algorithm - based crawler: study and experimental appraisal		A focused crawler traverses the web selecting out relevant pages according to a predefined topic. While browsing the internet it is difficult to identify relevant pages and predict which links lead to high quality pages. This paper proposes a topical crawler for Vietnamese web pages using greedy heuristic and genetic algorithms. Our crawler based on genetic algorithms uses different recombination operators in the genetic algorithms to improve the crawling performance. We tested our algorithms on Vietnamese newspaper VnExpress websites. Experimental results show the efficiency and the viability of our approach.	display resolution;focused crawler;genetic algorithm;greedy algorithm;heuristic;web crawler;web page	Huynh Thi Thanh Binh;Ha Minh Long;Tran Duc Khanh	2013		10.1007/978-3-642-34300-1_23	simulation;engineering;artificial intelligence;machine learning	DB	-28.15207331851729	-55.606900092955286	160907
71a52917e4965190462a806b1521fde53c8b6c57	cloudalicious: folksonomy over time	tagging uniform resource locators tag clouds data visualization libraries information retrieval information systems vocabulary documentation internet;text analysis;folksonomy;classification;data visualisation;visualization;internet;visualization categorization classification folksonomy tagging;diagonal graph movement online visualization tool text label online bookmark url tag cloud;text analysis data visualisation internet;categorization;tagging	Cloudalicious is an online visualization tool that has been designed to give insight into how Òtag cloudsÓ, or folksonomies, develop over time. A folksonomy is an organic system of text labels attributed to an object by the users of that object. The most common object so far to be the subject of this tagging has been the online bookmark. Stabilization of a URL's tag cloud over time is the clearest result of this type of visualization. Any diagonal movement on the graphs, indicative of a change in the tags being used to describe a URL, should garner further discussion.	folksonomy;tag cloud	Terrell Russell	2006	Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital Libraries (JCDL '06)	10.1145/1141753.1141859	text mining;the internet;information visualization;visualization;biological classification;computer science;data mining;world wide web;information retrieval;data visualization;categorization	Visualization	-28.643029881301565	-52.95622325270045	161101
de217025f8bd17d98224a87968696d2334c54040	extraction of the contents in the web texts by content-density distribution	web information retrieval;content density distribution;soft data paradigms;web page recognition;knowledge engineering	In recent years, users use result snippets of a web search engine to grasp the content of web pages, when users search for useful information on the internet. However, they are sometimes unable to notice the content of web pages by reading the result snippets because these snippets are so short that they cannot determine whether the content of each web page is relevant. To address this problem, we propose a method for grasping the content of each web page and extracting a part of the web page concerned to query keywords. This method is more effective than conventional methods based on snippets, because we regard the content as a set of words in the text of a web page, and we generate the content-density distribution by using both the position and the influence of the word. In the result of our experiments, we found that our method is useful for gasping the influence of extracted web text.		Saori Kitahara;Koya Tamura;Kenji Hatano	2011	IJKESDP	10.1504/IJKESDP.2011.045723	web service;backlink;static web page;site map;web query classification;framing;web mapping;web design;page view;computer science;artificial intelligence;web navigation;knowledge engineering;web page;printer-friendly;database;web search query;web 2.0;world wide web;information retrieval;mashup	NLP	-29.130282351509912	-54.69743549635832	162076
32782bd14201b5f80048000daf069dc7d40c52ca	using explicit word co-occurrences to improve term-based text retrieval	vector space;text retrieval;profitability	Reaching high precision and recall rates in the results of term-based queries on text collections is becoming more and more cru- cial, as long as the amount of available documents increases and their quality tends to decrease. In particular, retrieval techniques based on the strict correspondence between terms in the query and terms in the doc- uments miss important and relevant documents where it just happens that the terms selected by their authors are slightly different than those used by the final user that issues the query. Our proposal is to explicitly consider term co-occurrences when building the vector space. Indeed, the presence in a document of different but related terms to those in the query should strengthen the confidence that the document is relevant as well. Missing a query term in a document, but finding several terms strictly related to it, should equally support the hypothesis that the doc- ument is actually relevant. The computational perspective that embeds such a relatedness consists in matrix operations that capture direct or indirect term co-occurrence in the collection. We propose two different approaches to enforce such a perspective, and run preliminary experi- ments on a prototypical implementation, suggesting that this technique is potentially profitable.	document retrieval	Stefano Ferilli;Marenglen Biba;Teresa Maria Altomare Basile;Floriana Esposito	2010		10.1007/978-3-642-15850-6_13	query expansion;ranking;vector space;computer science;data mining;database;world wide web;information retrieval;profitability index	NLP	-33.368201905015624	-58.67868528588722	163499
52a1b1bc1fb84f985b483e4c336c2a9338b802d7	supporting the automatic construction of entity aware search engines	search engine;resource discovery;entity aware search engines;web exploration;semantic web	Several web sites deliver a large number of pages, each publishing data about one instance of some real world entity, such as an athlete, a stock quote, a book. Although it is easy for a human reader to recognize these instances, current search engines are unaware of them. Technologies for the Semantic Web aim at achieving this goal; however, so far they have been of little help in this respect, as semantic publishing is very limited.  We have developed a method to automatically search on the web for pages that publish data representing an instance of a certain conceptual entity. Our method takes as input a small set of sample pages: it automatically infers a description of the underlying conceptual entity and then searches the web for other pages containing data representing the same entity. We have implemented our method in a system prototype, which has been used to conduct several experiments that have produced interesting results.	experiment;prototype;semantic web;semantic publishing;web search engine	Lorenzo Blanco;Valter Crescenzi;Paolo Merialdo;Paolo Papotti	2008		10.1145/1458502.1458526	semantic search;computer science;semantic web;social semantic web;data mining;semantic web stack;database;world wide web;information retrieval;search engine	DB	-30.72660636521874	-54.393411257655	163875
e97b3a4c4e7bd23096f28710972378d5bf14ac65	efficient watcher based web crawler design	search engine;information retrieval;crawling algorithm;static crawler;crawler re visiting policies;ajax crawler	Purpose – The purpose of this paper is to design a watcher-based crawler (WBC) that has the ability of crawling static and dynamic web sites, and can download only the updated and newly added web pages. Design/methodology/approach – In the proposed WBC crawler, a watcher file, which can be uploaded to the web sites servers, prepares a report that contains the addresses of the updated and the newly added web pages. In addition, the WBC is split into five units, where each unit is responsible for performing a specific crawling process. Findings – Several experiments have been conducted and it has been observed that the proposed WBC increases the number of uniquely visited static and dynamic web sites as compared with the existing crawling techniques. In addition, the proposed watcher file not only allows the crawlers to visit the updated and newly web pages, but also solves the crawlers overlapping and communication problems. Originality/value – The proposed WBC performs all crawling processes in the sense ...	web crawler	Saed Alqaraleh;Omar Ramadan;Muhammed Salamah	2015	Aslib J. Inf. Manag.	10.1108/AJIM-02-2015-0019	spider trap;computer science;web crawler;distributed web crawling;database;focused crawler;internet privacy;world wide web	DB	-29.93504892405339	-53.46989044864811	164350
7c128d2dcf64d18cbbc3abb395a0583ba3e1fbdb	early exit optimizations for additive machine learned ranking systems	web documents;web search engine;learning system;machine learning;web search;optimization;query logs;early exit	Some commercial web search engines rely on sophisticated machine learning systems for ranking web documents. Due to very large collection sizes and tight constraints on query response times, online efficiency of these learning systems forms a bottleneck. An important problem in such systems is to speedup the ranking process without sacrificing much from the quality of results. In this paper, we propose optimization strategies that allow short-circuiting score computations in additive learning systems. The strategies are evaluated over a state-of-the-art machine learning system and a large, real-life query log, obtained from Yahoo!. By the proposed strategies, we are able to speedup the score computations by more than four times with almost no loss in result quality.	bottleneck (engineering);computation;learning to rank;machine learning;mathematical optimization;quality of results;real life;speedup;utility functions on indivisible goods;web page;web search engine	Berkant Barla Cambazoglu;Hugo Zaragoza;Olivier Chapelle;Jiang Chen;Ciya Liao;Zhaohui Zheng;Jon Degenhardt	2010		10.1145/1718487.1718538	web query classification;ranking;web search engine;computer science;machine learning;data mining;database;web search query;ranking svm;world wide web;active learning;information retrieval	Web+IR	-27.632322469838773	-53.71174815184836	164637
3443c4cdff41f15ccf60257f1999e220fdc34b3a	site-searching strategies of searchers referred from search engines	query reformulation;web queries;web searching;site search;search strategies	In this research, we analyze the referral queries and associated site-search queries at the session level from searchers coming from web search engines. Findings are based on a random sample of 10,000 from a total of 327,261 searching sessions of an online Spanish entertainment business collected over the course of a five month period from March 23, 2012 to August 26, 2012. We find six searching strategies that are correlated with the type of referral keywords (i.e., search terms) used at the major search engine. Of the six, the three major searching strategies are (1) the explorers who submit a broad query on the major search engine and then submit multiple broad queries on the site-search engine, (2) the navigators who submit a query to the major search engine that is part of a URL and then submit specific queries to the site-search engine, and (3) the persisters who submit the exact type of query on both the search engine and the site search. Implications for this research include developing better internal searching features, sponsored search keyword generation, and personalization of website content.		Adan Ortiz-Cordova;Bernard J. Jansen	2013		10.1002/meet.14505001050	search-oriented architecture;database search engine;web query classification;organic search;metasearch engine;semantic search;search engine optimization;computer science;phrase search;data mining;search analytics;web search query;queries per second;world wide web;information retrieval;search engine	Web+IR	-32.91469078128539	-53.9490345376594	165026
d2662c5f2b4aca17f7e9ce70074d2eb829b4f38d	entity set expansion via knowledge graphs		The entity set expansion problem is to expand a small set of seed entities to a more complete set of similar entities. It can be applied in applications such as web search, item recommendation and query expansion. Traditionally, people solve this problem by exploiting the co-occurrence of entities within web pages, where latent semantic correlation among seed entities cannot be revealed. We propose a novel approach to solve the problem using knowledge graphs, by considering the deficiency (e.g., incompleteness) of knowledge graphs. We design an effective ranking model based on the semantic features of seeds to retrieve the candidate entities. Extensive experiments on public datasets show that the proposed solution significantly outperforms the state-of-the-art techniques.	angular defect;entity;experiment;extensible storage engine;knowledge graph;query expansion;seeds (cellular automaton);web page;web search engine	Xiangling Zhang;Yueguo Chen;Jun Chen;Xiaoyong Du;Ke Wang;Ji-Rong Wen	2017		10.1145/3077136.3080732	information retrieval;query expansion;computer science;data mining;web page;small set;ranking;graph	Web+IR	-27.0928756974644	-58.08660823380722	165053
3ba74b3289aea219610dd82de915ac42fc081194	"""semantically enriched recommender engine: a novel collaborative filtering approach using """"user-to-user fast xor bit operation"""""""	k nearest neighbor classifier;filtering;groupware;search engines;information retrieval;information filtering;collaboration;semantics;data mining;ontologies artificial intelligence;recommender engine;boosting;user to user fast xor bit operation;collaborative filtering;top n precision;semantic web;top n recall;recommender systems;recommender search engine	In this paper, we focus on Collaborative Filtering to provide recommendations to users that fit their profiles. We employed two methods: (1) K-Nearest Neighbors classifier, and (2) a fast implementation of Collaborative Filtering approach: “user-to-user fast XOR bit operation”. Both techniques serve the same objective, which is modifying the user's ontology profile (semantic profile). Technically, Collaborative Filtering extends the user's ontology profile based on the interests of a community of similar users. Also, we describe the implementation of the recommender system on a real platform, known as Hyper Many Media at Western Kentucky University. Finally, we evaluate the system based on Top-n-Recall and Top-n-Precision. The results show an improvement in Recall and Precsion using Collaborative Filtering.	bitwise operation;collaborative filtering;exclusive or;k-nearest neighbors algorithm;personalization;personalized search;precision and recall;recommender system;semantic search;user (computing);web search engine	Leyla Zhuhadar;Olfa Nasraoui;Robert Wyatt	2010	2010 IEEE Fourth International Conference on Semantic Computing	10.1109/ICSC.2010.80	filter;computer science;collaborative filtering;information filtering system;machine learning;semantic web;data mining;database;semantics;world wide web;information retrieval;boosting;recommender system;collaboration	DB	-28.155989163068543	-55.48137791021638	165433
99ed9711fd09cc65749ea9d67c8caa4a417946b5	mining web logs: an automated approach	log files;web mining;pattern analysis;data warehouse;adaptive web site;web logs	Web Log files need to be analyzed to identify usage and access trends and to provide useful information to web site developers and administrators so as to create adaptive web sites. In this work we propose a general automated approach towards collecting and mining useful information from web log files that follow the Extended Log File Format proposed by W3C. The novelty of this work involves proposing of optimized pre-processing, storing and mining procedures and utilization of relevant tools for pattern analysis.	blog;data logger;pattern recognition;preprocessor;world wide web	G. Sudhamathy	2010		10.1145/1858378.1858435	web service;web mining;static web page;web development;web modeling;site map;data web;web analytics;web mapping;web design;computer science;web navigation;web log analysis software;web page;data mining;database;web intelligence;web 2.0;world wide web	ML	-29.509947411662075	-52.506621379542665	165950
0f6551ff4145dcde2f359df8111842cfb70eaa3e	evaluating different information retrieval algorithms on real-world data	information retrieval	More and more data is produced in the form of videos, which are opaque to textual queries. To allow searching in video data collections, two problems have to be solved: The automatic generation of a searchable index, and the effective search in the automatically produced and therefore imperfect index. The ISL View4You system is a prototype of a video indexing and retrieval system which both generates the index and provides a search engine to access it. An end to end evaluation was carried out using real-world data and queries from naive subjects. From the results it can be concluded, errors of the overall system are not due to the index generation, but are introduced by the information retrieval engine (the search). Therefore, the focus of this paper is a comparison of two di erent search algorithms, LSI (latent semantic indexing) and Okapi (a avor of the traditional classic vector model approach). The evaluation is carried out on the automatically produced index on a relatively small database, which allows for full manual relevance judgement.	database;extended precision;information retrieval;interactivity;latent semantic analysis;prototype;regular expression;relevance;search algorithm;video;web search engine;isl	Manfred Weber;Thomas Kemp	2000			concept search;query expansion;human–computer information retrieval;visual word;relevance (information retrieval);information retrieval;adversarial information retrieval;retrievability;computer science;vector space model	Web+IR	-32.496862070231074	-57.85705689486913	167265
2bcf2cd9f1525de76c8b0e0b7f43a21c067913a1	focusing web crawls on location-specific content	web crawling	Retrieving relevant data for location-sensitive keyword queries is a challenging task that has so far been addressed as a problem of automatically determining the geographical orientation of web searches. Unfortunately, identifying localizable queries is not sufficient per se for performing successful location-sensitive searches, unless there exists a geo-referenced index of data sources against which localizable queries are searched. In this paper, we propose a novel approach towards the automatic construction of a geo-referenced search engine index. Our approach relies on a geo-focused crawler that incorporates a structural parser and uses GeoWordNet as a knowledge base in order to automatically deduce the geo-spatial information that is latent in the pages’ contents. Based on location-descriptive elements in the page URLs and anchor text, the crawler directs the pages to a location-sensitive downloader. This downloading module resolves the geographical references of the URL location elements and organizes them into indexable hierarchical structures. The location-aware URL hierarchies are linked to their respective pages, resulting into a georeferenced index against which location-sensitive queries can be answered.	anchor text;download;focused crawler;geonetwork opensource;knowledge base;location awareness;location-based service;web crawler;web search engine	Lefteris Kozanidis;Sofia Stamou;George Spiros	2009	IJWA		computer science;web crawler;world wide web	Web+IR	-29.65435077826519	-53.97561754286985	167493
230eefe0b95b979d277fe5da9d2523e434d235e3	a method study of online publication time extraction for chinese web news		In Web search, the publication time of Web page plays an important role, because the return result is time-based in general. Besides, it is also used to locate the occurrence time of news event and further track the event evolution. We propose an efficient method to extract the publication time of Chinese Web news online in this paper. Focusing on the extraction, the method carries out from two aspects: one is that temporal information is generally hidden in Web news URL, which is exactly the publication time and the other is that the publication time is one of all the text nodes belonging to the DOM parsing tree of the HTML document of the corresponding Web page or part of it usually. Given 20 keywords for 3 mainstream Chinese search engines, we will get 1200 items of Web news URLs, which are used to conduct the experiment and then analyze the accuracy and efficiency of publication time extraction. The experimental result shows that the precision of extraction is up to 96.3% and the time consumption is just 60 seconds.	document object model;feature vector;html;internet;machine learning;named-entity recognition;overhead (computing);parse tree;parsing;web page;web search engine	Liangliang Wang;Gongqing Wu	2017	2017 IEEE International Conference on Big Knowledge (ICBK)	10.1109/ICBK.2017.57	world wide web;web page;static web page;parsing;information retrieval;search engine;computer science	Web+IR	-27.854980288920263	-55.437815585026314	167711
0ebfccec4aacde1f6aef9d4f772bc4b8bbb91214	ranking scientific publications with similarity-preferential mechanism	citation network;pagerank;similarity;robustness	Along with the advance of internet and fast updating of information, nowadays it is much easier to search and acquire scientific publications. To identify the high quality articles from the paper ocean, many ranking algorithms have been proposed. One of these methods is the famous PageRank algorithm which was originally designed to rank web pages in online systems. In this paper, we introduce a preferential mechanism to the PageRank algorithm when aggregating resource from different nodes to enhance the effect of similar nodes. The validation of the new method is performed on the data of American Physical Society journals. The results indicate that the similarity-preferential mechanism improves the performance of the PageRank algorithm in terms of ranking effectiveness, as well as robustness against malicious manipulations. Though our method is only applied to citation networks in this paper, it can be naturally used in many other real systems, such as designing search engines in the World Wide Web and revealing the leaderships in social networks.	algorithm;display resolution;internet;malware;pagerank;scientific literature;social network;web page;web search engine;world wide web	Jianlin Zhou;An Zeng;Ying Fan;Zengru Di	2015	Scientometrics	10.1007/s11192-015-1805-1	similarity;computer science;data mining;world wide web;information retrieval;robustness	AI	-26.704051570050815	-57.252077267524854	168030
41a399250059b9e190cdc3578e38893d3811750f	lessons learned from indexing close word pairs	indexes;information retrieval;ranking	We describe experiments with proximity-aware ranking functions that use indexing of word pairs. Our goal is to evaluate a method of “mild” pruning of proximity information, which would be appropriate for a moderately loaded retrieval system, e.g., an enterprise search engine. We create an index that includes occurrences of close word pairs, where one of the words is frequent. This allows one to efficiently restore relative positional information for all non-stop words within a certain distance. It is also possible to answer phrase queries promptly. We use two functions to evaluate relevance: a modification of a classic proximity-aware function and a logistic function that includes a linear combination of relevance features. Additionally, we use the spam scores provided by the University of Waterloo.	experiment;relevance;spamming;web search engine	Leonid Boytsov;Anna Belova	2010			computer science;data mining;information retrieval;linear combination;search engine;search engine indexing;logistic function;phrase;ranking	Web+IR	-31.543780942159447	-59.00072787762867	168350
e46d593e313a5aaf9da1ad0d4cdc78b756c30a57	on the effectiveness of anonymizing networks for web search privacy	search engine;query obfuscation;anonymizing networks;web search privacy;machine learning;web search;off the shelf	"""Web search has emerged as one of the most important applications on the internet, with several search engines available to the users. There is a common practice among these search engines to log and analyse the user queries, which leads to serious privacy implications. One well known solution to search privacy involves issuing the queries via an anonymizing network, such as Tor, thereby hiding one's identity from the search engine. A fundamental problem with this solution, however, is that user queries are still obviously revealed to the search engine, although they are """"mixed"""" among the queries issued by other users of the same anonymization service.  In this paper, we consider the problem of identifying the queries of a user of interest (UOI) within a pool of queries received by a search engine over an anonymizing network. We demonstrate that an adversarial search engine can extract the UOI's queries, when it is equipped with only a short-term user search query history, by utilizing only the query content information and off-the-shelf machine learning classifiers. More specifically, by treating a selected set of 60 users --- from the publicly-available AOL search logs --- as the users of interest performing web search over an anonymizing network, we show that each user's queries can be identified with 25.95% average accuracy, when mixed with queries of 99 other users of the anonymization service. This average accuracy drops to 18.95% when queries of 999 other users of the anonymization service are mixed together. Though the average accuracies are not so high, our results indicate that few users of interest could be identified with accuracies as high as 80--98%, even when their queries are mixed among queries of 999 other users. Our results cast serious doubts on the effectiveness of anonymizing web search queries by means of anonymizing networks."""	data anonymization;machine learning;privacy;search algorithm;tor messenger;web search engine;web search query	Sai Teja Peddinti;Nitesh Saxena	2011		10.1145/1966913.1966984	web query classification;metasearch engine;semantic search;computer science;data mining;internet privacy;search analytics;web search query;queries per second;world wide web;search engine	Web+IR	-27.747172283261314	-53.680141025468274	168455
35f8246932ee9c7285536c647d2be4e0a3fddf2d	lensingwikipedia: parsing text for the interactive visualization of human history	word processing data visualisation grammars information retrieval natural language processing reviews text analysis web sites;pragmatics;information retrieval;semantics;text analysis;human history articles lensingwikipedia text parsing interactive human history visualization information extraction bag of words word clusters valuable linguistic information textual information visualization approach state of the art natural language processing tools nlp tools web based interactive visual browser;data mining;data visualisation;grammars;internet;encyclopedias electronic publishing internet semantics data mining pragmatics;web sites;electronic publishing;reviews;encyclopedias;natural language processing;word processing	Extracting information from text is challenging. Most current practices treat text as a bag of words or word clusters, ignoring valuable linguistic information. Leveraging this linguistic information, we propose a novel approach to visualize textual information. The novelty lies in using state-of-the-art Natural Language Processing (NLP) tools to automatically annotate text which provides a basis for new and powerful interactive visualizations. Using NLP tools, we built a web-based interactive visual browser for human history articles from Wikipedia.	bag-of-words model;interactive visualization;natural language processing;parsing;web application;wikipedia	Ravikiran Vadlapudi;Maryam Siahbani;Anoop Sarkar;John Dill	2012	2012 IEEE Conference on Visual Analytics Science and Technology (VAST)	10.1109/VAST.2012.6400530	natural language processing;the internet;computer science;data mining;semantics;electronic publishing;world wide web;information retrieval;data visualization;encyclopedia;pragmatics	NLP	-27.224398808153445	-56.74193016812751	168851
14fc33b3b1ac90f699956ccd567fdb139e3c3d93	mining user preference using spy voting for search engine personalization	search engine;user preferences;personalization;ranking function;clickthrough data	This article addresses search engine personalization. We present a new approach to mining a user's preferences on the search results from clickthrough data and using the discovered preferences to adapt the search engine's ranking function for improving search quality. We develop a new preference mining technique called SpyNB, which is based on the practical assumption that the search results clicked on by the user reflect the user's preferences but does not draw any conclusions about the results that the user did not click on. As such, SpyNB is still valid even if the user does not follow any order in reading the search results or does not click on all relevant results. Our extensive offline experiments demonstrate that SpyNB discovers many more accurate preferences than existing algorithms do. The interactive online experiments further confirm that SpyNB and our personalization approach are effective in practice. We also show that the efficiency of SpyNB is comparable to existing simple preference mining algorithms.	algorithm;experiment;online and offline;personalization;ranking (information retrieval);web search engine	Wilfred Ng;Lin Deng;Dik Lun Lee	2007	ACM Trans. Internet Techn.	10.1145/1278366.1278368	organic search;computer science;data mining;personalization;search analytics;world wide web;information retrieval;search engine	Web+IR	-32.11091451185695	-52.39218298773491	169066
c6e7696da20b431d3187e5c984f37892c632d278	evolution of the chilean web structure composition	life cycle;search engines;statistical analysis web sites internet search engines natural languages;natural languages;internet;statistical analysis;web sites;statistical analysis chilean web structure composition web site life cycle internet search engines;computer science predictive models search engines navigation	In this paper we present the evolution of the structure of the Chilean Web between 2000 and 2002. Our results show that although the Web grows as expected, also a significant part of it disappears. In addition, some components are much more stable than others. We also compare the expected life cycle of a Web site in the structure with the actual real data.	world wide web	Ricardo A. Baeza-Yates;Barbara Poblete	2003		10.1109/LAWEB.2003.1250276	biological life cycle;web mining;web development;web modeling;the internet;simulation;data web;web analytics;web design;web search engine;web standards;computer science;web crawler;web navigation;social semantic web;multimedia;natural language;law;world wide web;search engine;statistics	Web+IR	-29.71092603924084	-56.64657893548457	169636
021e36ca7ba6fc2925c59aca0baf0d0b1fe937ae	folksonomy implementation based on the art-1 neural network	portable document format;neural networks databases support vector machine classification vectors memory management internet dictionaries;ieee xplore	This document describes the sample implementation of a very popular classification method in the modern internet web applications — folksonomy. This method forces users to assign a particular keywords to the content that they are uploading. Basing on the assigned keywords it is possible to find a similar content. In this paper, there is described the method of finding similar content basing on the ART-1 neural network. Such solution allows to perform a background clustering of a content and speed up the process of retrieving the related data. In case of a heavy exploitation of an internet application, this might be a big advantage.	artificial neural network;cluster analysis;folksonomy;reference implementation;rich internet application;upload;web application	Adam Sobaniec;Bohdan Macukow	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		computer science;artificial intelligence;machine learning;data mining;database;world wide web;information retrieval	ML	-28.56685755950357	-56.75260712157285	170004
e0e8a4845c3f7ac3ebfe591d0b09ea1d7e59a497	a method of web search result clustering based on rough sets	cluster algorithm;pattern clustering;document handling;clustering rough sets snippet;information retrieval;rough set theory;internet;web search rough sets clustering algorithms search engines mathematics web pages navigation algorithm design and analysis scalability set theory;pattern clustering information retrieval internet document handling rough set theory;clustering;search result clustering;tolerance rough set model web search document clustering vector representation snippet representation enrichment;rough sets;web search;rough set;snippet	Due to the enormous size of the web and low precision of user queries, finding the right information from the web can be difficult if not impossible. One approach that tries to solve this problem is using clustering techniques for grouping similar document together in order to facilitate presentation of results in more compact form and enable thematic browsing of the results set. The main problem of many web search result (snippet) clustering algorithm is based on the poor vector representation of snippets. In this paper, we present a method of snippet representation enrichment using Tolerance Rough Set model. We applied the proposed method to construct a rough set based search result clustering algorithm and compared it with other recent methods.	algorithm;cluster analysis;gene ontology term enrichment;rough set;web search engine	Chi Lang Ngo;Hung Son Nguyen	2005	The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05)	10.1109/WI.2005.7	correlation clustering;rough set;computer science;canopy clustering algorithm;machine learning;pattern recognition;data mining;cluster analysis;information retrieval	DB	-28.786878465210904	-56.749123560237386	170322
0869da3e874ff23b54667b5086d4914b2c077cd1	a study of query reformulation for patent prior art search with partial patent applications	query reformulation;patent search;conference paper	Patents are used by legal entities to legally protect their inventions and represent a multi-billion dollar industry of licensing and litigation. In 2014, 326,033 patent applications were approved in the US alone -- a number that has doubled in the past 15 years and which makes prior art search a daunting, but necessary task in the patent application process. In this work, we seek to investigate the efficacy of prior art search strategies from the perspective of the inventor who wishes to assess the patentability of their ideas prior to writing a full application. While much of the literature inspired by the evaluation framework of the CLEF-IP competition has aimed to assist patent examiners in assessing prior art for complete patent applications, less of this work has focused on patent search with queries representing partial applications. In the (partial) patent search setting, a query is often much longer than in other standard IR tasks, e.g., the description section may contain hundreds or even thousands of words. While the length of such queries may suggest query reduction strategies to remove irrelevant terms, intentional obfuscation and general language used in patents suggests that it may help to expand queries with additionally relevant terms. To assess the trade-offs among all of these pre-application prior art search strategies, we comparatively evaluate a variety of partial application search and query reformulation methods. Among numerous findings, querying with a full description, perhaps in conjunction with generic (non-patent specific) query reduction methods, is recommended for best performance. However, we also find that querying with an abstract represents the best trade-off in terms of writing effort vs. retrieval efficacy (i.e., querying with the description sections only lead to marginal improvements) and that for such relatively short queries, generic query expansion methods help.	entity;internet protocol suite;marginal model;partial application;query expansion;relevance;software patent	Mohamed Reda Bouadjenek;Scott Sanner;Gabriela Ferraro	2015		10.1145/2746090.2746092	query expansion;web query classification;computer science;artificial intelligence;data science;data mining;information retrieval	Web+IR	-32.67174950029725	-58.57163862638037	171147
69bc3dfd4a0835840fc7d64e4f284f3ee6c5ed0b	an automated algorithm for extracting website skeleton	extraction information;navegacion;lien hypertexte;red www;esqueleto;information extraction;enlace hipertexto;reseau web;hyperlink;high precision;skeleton;envolvero;navigation;internet;data extraction;precision elevee;enveloppeur;precision elevada;squelette;world wide web;information system;extraccion informacion;wrapper	The huge amount of information available on the Web has attracted many research efforts into developing wrappers that extract data from webpages. However, as most of the systems for generating wrappers focus on extracting data at page-level, data extraction at site-level remains a manual or semi-automatic process. In this paper, we study the problem of extracting website skeleton, i.e. extracting the underlying hyperlink structure that is used to organize the content pages in a given website. We propose an automated algorithm, called the Sew algorithm, to discover the skeleton of a website. Given a page, the algorithm examines hyperlinks in groups and identifies the navigation links that point to pages in the next level in the website structure. The entire skeleton is then constructed by recursively fetching pages pointed by the discovered links and analyzing these pages using the same process. Our experiments on real life websites show that the algorithm achieves a high recall with	algorithm;experiment;hyperlink;information extraction;real life;recursion;semiconductor industry;world wide web	Zehua Liu;Wee Keong Ng;Ee-Peng Lim	2004		10.1007/978-3-540-24571-1_70	navigation;the internet;computer science;data mining;database;hyperlink;skeleton;world wide web;information extraction;information retrieval;information system	Web+IR	-29.569851283756137	-53.92892032546473	171258
2c5af7b80c9f6235790d0f521338f26559bdbdf2	grabex: a graph-based method for web site block classification and its application on mining breadcrumb trails	graph theory;recall grabex web site block classification breadcrumb trail mining page blocks header navigation bar content area hyperlink functionality menu block content hierarchy navigational block mining visual feature analysis graph based block extraction method navigational block classification graph based link analysis css class attributes precision;web site block classification;breadcrumb mining;page segmentation;navigation feature extraction visualization html data mining web sites vegetation;data mining;web sites;pattern classification;page segmentation web site block classification breadcrumb mining;user interfaces;web sites data mining graph theory pattern classification user interfaces	In order to interact with a Web site, humans must be able to distinguish and understand the purposes of different page blocks, e.g. header, navigation bar or content area. In case of navigational blocks, the block type determines the functionality of the hyperlinks it contains. For example, the hyperlinks in the main menu block represent the main topics of a site while the hyperlinks in a breadcrumb trail show the location in the content hierarchy. Hence, mining navigational blocks of specific types can provide valuable input for applications in the fields of crawling, ranking or presenting search results. However, analyzing visual features in order to identify specific navigational blocks as humans do is a difficult, resource-consuming task and a general solution does not exist yet. In this paper, we propose a novel approach to the problem and present the Graph-based block extraction method (GRABEX) that can be adapted to classify different types of navigational blocks. The fundamental concept is that a separate graph-based link-analysis is conducted for groups of blocks. Each block group consists of blocks from different pages that have similar CSS class attributes. This allows discovering navigational blocks of specific types, e.g. breadcrumb trails, without analyzing any presentational features. We apply our method to mine breadcrumb trails and are the first to describe an applicable solution to this problem. In an extensive evaluation including 700 different sites, the GRABEX-method performed with perfect precision and high recall.	breadcrumb (navigation);cascading style sheets;hyperlink;link analysis;navigation bar	Matthias Keller;Hannes Hartenstein	2013	2013 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT)	10.1109/WI-IAT.2013.42	computer science;graph theory;data mining;database;user interface;world wide web	ML	-28.892886552278675	-53.65797182577685	172079
2e0c5ec56bf204c0302dc81a0ab3c2d44c76e373	partial-match retrieval with structure-reflected indices at the ntcir-10 math task		To attain fast and accurate response in math formulae search, an index should be prepared which holds structure information of math expressions; a different indexing for full text search. Although some previous research has been done by this approach, the size of indices tends to become huge on memory. This paper proposes a partial match retrieval system for math formulae with two kinds of indices. The first one is an inverted index constructed from paths to the root node from each node seeing formula as an expression tree. The other index is a table which stores the parent node and the text string for each node in the expression trees. A hundred thousand documents in the NTCIR-10 Math Task (formula search) containing 36 million math formulae were used for evaluation. The number of nodes was about 291 million and the number of path kinds in the inverted index was about 9 million. Experimental results showed that the search time grows linearly to the number of retrieved documents. Concretely, the search time ranges from 10 milliseconds to 1.2 seconds; the simpler formulae tend to need more search time.	binary expression tree;inverted index;string (computer science);string searching algorithm;tree (data structure)	Hiroya Hagino;Hiroaki Saito	2013			inverted index;arithmetic;binary expression tree;search engine indexing;full text search;expression (mathematics);mathematics	Web+IR	-32.26608901210305	-58.35267374827345	172138
ca5c150320a1d220fbfd2bc842d0d6b2fe269329	geo-word centric association rule mining	spatial data;data mining;spatial database;association rule mining;location dependent and sensitive;spatial databases and gis;web mining	"""Association rule mining is a well known data mining technique that also applicable to spatial data. Here we systematically elaborate spatial association rule mining which focusing on the """"geo-word"""" in order to analyze location related preferences of human users. We propose several categories of mining methodologies based on the items in the rule and rule generation process. Novel interestingness metrics are derived out of those mining methodologies to identify location specific characteristics such as local specialties and common words. An experiment on real access logs from a commercial yellowpage site is conducted to examine the effectiveness of those methodologies"""	association rule learning;data mining;entropy (information theory);microsoft word for mac	Katsumi Takahashi;Iko Pramudiono;Masaru Kitsuregawa	2005		10.1145/1071246.1071290	concept mining;web mining;association rule learning;computer science;data science;data mining;spatial analysis;data stream mining;k-optimal pattern discovery;spatial database;information retrieval	ML	-28.57283498414817	-53.49306454141453	173588
22be5b86959a477422cca888106e5135599382f5	toward efficient peer-to-peer information retrieval based on textual entailment	p2p system;text analysis information retrieval internet natural language processing peer to peer computing;service provider;peer to peer network;information retrieval;text analysis;peer to peer system;computer network;large scale;peer to peer computing information retrieval search engines tellurium internet natural language processing intelligent agent information science ip networks computer networks;combinatorial capacity metric;internet;data discovery;textual documents;peer to peer computing;time based clustering;shannon languages;peer to peer information retrieval;natural language processing;internet community;textual entailment approach;textual entailment approach peer to peer information retrieval data discovery internet community textual documents natural language processing	Peer-to-Peer networks are gaining increasing attention from both the scientific and the larger Internet user community. Peer-to-peer systems are very large computer networks, where peers collaborate to provide a common service. Providing large-scale Information retrieval, like searching the Internet, is an attractive application for P2P systems. Data discovery and retrieval is of great importance to computer users and the broad Internet community. In this paper, we propose a novel approach to retrieve not only textual documents that have specified keywords, but also to discover semantically equivalent or entailed documents from given keywords. This approach is based on the recent natural language processing approach called the Textual Entailment Approach.	information retrieval;natural language processing;peer-to-peer;textual entailment;user (computing);virtual community	Yasser Kotb	2006	2006 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology Workshops	10.1109/WI-IATW.2006.132	service provider;text mining;the internet;computer science;database;world wide web;information retrieval	DB	-29.61008088511824	-57.22599698956655	174528
bd33adcd86e12cd4471b0712d042335d7f19ebfd	creating temporally dynamic web search snippets	search result snippets;temporal dynamics;user study;query dynamics;temporal information;web search	Content on the Internet is always changing. We explore the value of biasing search result snippets towards new webpage content. We present results from a user study comparing traditional query-focused snippets with snippets that emphasize new page content for two query types: general and trending. Our results indicate that searchers prefer the inclusion of temporal information for trending queries but not for general queries, and that this is particularly valuable for pages that have not been recently crawled.	biasing;internet;temporal logic;usability testing;web page;web search engine	Krysta Marie Svore;Jaime Teevan;Susan T. Dumais;Anagha Kulkarni	2012		10.1145/2348283.2348461	computer science;data mining;world wide web;information retrieval	Web+IR	-33.538335093757304	-52.55925915311608	175638
4d1b7b87b676a2bcc49cadcb31462cede767060d	an efficient e-mail filtering using time priority measurement	business education;filtering method;time priority;pattern matching;pattern matching machine;multi attribute rules;e mail systems	Although E-mail systems are one of the most useful communication tools for business, education, etc., missing important E-mail messages become a very serious problem. It is very useful filtering supports for users to pick up important messages or to neglect unnecessary messages. This paper presents a method of determining the time priority for E-mail messages. Multi-attribute rules are defined to detect complex time expressions and a set pattern-matching machine is proposed. It enables us to protect missing messages with important time information because the presented method can classify and rank them according to time priority measurement automatically. From the simulation results of determining time priority, the presented pattern-matching method is about 4 times faster than the traditional string pattern-matching method. From the results of filtering 5172 sentences, it is verified that precision and recall of the presented method becomes 95% and 96%, respectively. From the experimental results of determining 10 highest messages among 100 E-mail, it is verified that filtering time by the proposed measurement is from 9.7 to 16.6 faster than a non-filtering method. 2003 Elsevier Inc. All rights reserved.	algorithm;email filtering;pattern matching;precision and recall;regular expression;rule 90;sensor;simulation	Yuki Kadoya;Masao Fuketa;El-Sayed Atlam;Kazuhiro Morita;Shinkaku Kashiji;Jun-ichi Aoe	2004	Inf. Sci.	10.1016/j.ins.2003.12.003	business education;computer science;theoretical computer science;machine learning;pattern matching;data mining;programming language;world wide web;algorithm	DB	-29.254660239036536	-55.22875993561041	176687
d583d3ec72f13cf775f05607d736cd5bc4154a8a	url as starting point for www document categorization	machine learning;search engine;side effect	Information about the category (type) of a WWW page can be helpful for the user within search, filteri ng, as well as navigation tasks. We propose a multidimensional categorisation scheme, with bibliographic dimension as the primary one. We examine the possibilities and limit s of performing such categorisation based on inform ation extracted from URL, which is particularly useful fo r certain on-line applications such as meta-search or navigation support. In addition, we describe the pr oblem of ambiguity of URL terms, and suggest a meth od for its partial overcoming by means of machine learning. As a side–effect, we show that general purpose WWW se arch engines can be used for providing input data for bo th human and computational analysis of the web.	call of duty: black ops;categorization;document classification;machine learning;online and offline;side effect (computer science);www	Vojtech Svátek;Petr Berka	2000				ML	-33.2583748638839	-56.601163558554376	176786
253637e93acdf5437bac37791dc4caa749dccd35	bayesian semantics incorporation to web content for natural language information retrieval		For the present work, we endeavor with the important aspect of information retrieval of Web content using natural language queries. Currently, markup languages and formalisms do not fully provide mechanisms for effective and accurate analysis of Web content but rather provide means for describing the content in a more human-centric approach. As a result, natural language queries cannot be handled by the Internet search engines. Other approaches use grammar markup labels that attempt to fully match an unforeseen query. For the purposes of this paper, we introduce the theoretical and implementation issues of a novel, statistical framework that can cope with Web content analysis and information retrieval using natural language. The framework is based on Bayesian networks, a tool for knowledge representation and reasoning under conditions of uncertainty. The Web page designer provides the lexical items that contain useful information and labels the corresponding semantic interpretation, from a pre-defined set of domain categories. This knowledge is used for learning the structure and the parameters of a Bayesian network. At the time a user’s query is encountered, the network is used in order to return pages that contain the most related semantic content to the user’s query.	bayesian network;information retrieval;knowledge representation and reasoning;markup language;natural language;semantic interpretation;web content;web page;web search engine;world wide web	Manolis Maragoudakis;Nikos Fakotakis	2004			knowledge representation and reasoning;information extraction;artificial intelligence;computational semantics;natural language processing;semantic interpretation;semantics;computer science;search engine;question answering;information retrieval;web content	Web+IR	-31.671133014660555	-58.311658328146144	176896
71a8ffcbae09f1fb2ff4f2da7d02670fb75941be	hide: a tool for unrestricted literature based discovery		As the quantity of publications increases daily, researchers are forced to narrow their attention to their own specialism and are therefore less likely to make new connections with other areas. Literature based discovery (LBD) supports the identification of such connections. A number of LBD tools are available, however, they often suffer from limitations such as constraining possible searches or not producing results in real-time. We introduce HiDE (Hidden Discovery Explorer), an online knowledge browsing tool which allows fast access to hidden knowledge generated from all abstracts in Medline. HiDE is fast enough to allow users to explore the full range of hidden connections generated by an LBD system. The tool employs a novel combination of two approaches to LBD: a graph-based approach which allows hidden knowledge to be generated on a large scale and an inference algorithm to identify the most promising (most likely to be non trivial) information. Available at https://skye.shef.ac.uk/kdisc	algorithm;medline;real-time locating system	Judita Preiss;Mark Stevenson	2018			data mining;natural language processing;artificial intelligence;inference;computer science;medline;graph;literature-based discovery	Comp.	-30.979009417687887	-53.32235487279591	177210
2b0f34ee0841bbf9d8b46a027eba48bf1d1a7253	approach for name ambiguity problem using a multiple-layer clustering	pattern matching extraction methods;pattern clustering;coauthorship relationship;fuzzy logic clustering name ambiguity package merge;coauthorship relationship name ambiguity problem multiple layer clustering digital library package merge algorithm pattern matching extraction methods fuzzy logic rule;search engines;digital library;rule based;package merge;fuzzy logic rule;dh hemts;data mining;fuzzy set theory;fuzzy logic;ground penetrating radar;multiple layer clustering;clustering;pattern matching;package merge algorithm;name ambiguity problem;search engines bibliographic systems fuzzy set theory pattern clustering;bibliographic systems;ground penetrating radar geophysical measurement techniques;name ambiguity;geophysical measurement techniques;extraction method	Name ambiguity refers to the problem of attributing a publication to a proper author. This is a common issue in digital library. It is a difficult problem as the same author's name may be written in different ways and different authors may share the same name. In this paper, we examine a multiple-layer clustering approach which is based on a limited amount of associated information with each publication. It combines the Package-Merge algorithm, pattern-matching extraction methods, as well as a fuzzy logic rule based concept. This experimental study uses the DBLP collection as a case study, and the three attributes used are Email addresses, the Co-Authorship relationship and Paper title similarity. Our experiments show that this approach can distinguish authors and classify papers on the test dataset more accurately than the previous studies.	cluster analysis;dbl-browser;digital library;email;experiment;fuzzy logic;merge algorithm;package-merge algorithm;pattern matching	Wenrong Jiang;Anbao Wang;Cuihong Wu;Jian Chen;Jihong Yan	2009	2009 International Conference on Computational Science and Engineering	10.1109/CSE.2009.110	fuzzy logic;digital library;ground-penetrating radar;computer science;artificial intelligence;machine learning;pattern matching;data mining;database;fuzzy set;cluster analysis;information retrieval	SE	-26.814880813429212	-58.603698357496086	177264
0446172d255f18da46376e284bd968cf5d03a104	designing a value based niche search engine using evolutionary strategies	search engines evolutionary computation clustering algorithms industrial engineering information retrieval testing taxonomy genetics business character generation;search engine;document handling;electronic commerce;evolutionary computation;search engines;information retrieval;e commerce;search algorithm;search algorithm value based niche search engine evolutionary strategy e commerce corporate intranets organizational repository unstructured document collections retrieval accuracy evolutionary algorithm document collection;evolutionary strategy;evolutionary algorithm;document handling search engines evolutionary computation electronic commerce information retrieval	The advent of e-commerce and corporate intranets has led to the growth of organizational repositories containing large, fragmented, and unstructured document collections. Though it is difficult to retrieve relevant documents from such collections, it is relatively less cumbersome to define categories broadly classifying the information contained in the collection. Such categories lend value to the information contained in the collection. This research addresses the issue of improving retrieval accuracy of search engines that retrieve documents from organizational repositories using a value based approach. We test an evolutionary algorithm approach on a document collection. The precision of the search algorithm improved from 40% in generation 1 of the algorithm to nearly 90% in generation 10,000.	archive;document;e-commerce;evolutionary algorithm;information retrieval;intranet;relevance;search algorithm;value (ethics);web search engine	Sourav Sengupta;Bernard J. Jansen	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.125	document retrieval;document clustering;computer science;data mining;world wide web;information retrieval;search engine	Web+IR	-28.769852895628933	-57.378175145515286	177645
2c773dd3f31a4e8a45d5e7cc3edd7ada9c7cb329	ranking function optimization for effective web search by genetic programming: an empirical study	support vector machines search engines genetic algorithms hypermedia markup languages;hypermedia markup languages;search engine;empirical study;genetic program;support vector machines;search engines;ranking function;web search engine;web search;genetic algorithms;support vector machine function optimization genetic programming web search engines structural information html documents;support vector machine;web search genetic programming search engines information systems computer science internet electronic mail delay html support vector machines	Web search engines have become indispensable in our daily life to help us find the information we need. Although search engines are very fast in search response time, their effectiveness in finding useful and relevant documents at the top of the search hit list needs to be improved. In this paper, we report our experience applying genetic programming (GP) to the ranking function discovery problem leveraging the structural information of HTML documents. Our empirical experiments using the Web track data from recent TREC conferences show that we can discover better ranking functions than existing well-known ranking strategies from IR, such as Okapi, Ptfidf. The performance is even comparable to those obtained by support vector machine.	experiment;genetic programming;html;mathematical optimization;ranking (information retrieval);response time (technology);search engine optimization;support vector machine;web search engine;windows rally;world wide web	Weiguo Fan;Michael D. Gordon;Praveen Pathak;Wensi Xi;Edward A. Fox	2004	37th Annual Hawaii International Conference on System Sciences, 2004. Proceedings of the	10.1109/HICSS.2004.1265279	beam search;support vector machine;search engine indexing;ranking;metasearch engine;web search engine;semantic search;computer science;spamdexing;web crawler;data mining;database;search analytics;web search query;ranking svm;world wide web;information retrieval;search engine	Web+IR	-29.92215513278812	-55.184540911144644	177682
25d8b142d7c033ded55d0c660c507b09d13c952f	web pages clustering: a new approach	search engine;english language;web pages;information retrieval;standardisation	The rapid growth of web has resulted in vast volume of information. Information availability at a rapid speed to the user is vital. English language (or any for that matter) has lot of ambiguity in the usage of words. So there is no guarantee that a keyword based search engine will provide the required results. This paper introduces the use of dictionary (standardised) to obtain the context with which a keyword is used and in turn cluster the results based on this context. These ideas can be merged with a metasearch engine to enhance the search efficiency.	component-based software engineering;curve fitting;data mining;dictionary;earthbound;factor analysis;generalized least squares;learning to rank;principal component analysis;relevance;web search engine	E. JeevanH.;P. PrashanthP.;N. PunithKumarS.;Vinay Hegde	2011	CoRR		metasearch engine;web search engine;computer science;english;web page;database;keyword density;search analytics;world wide web;information retrieval;standardization;search engine	ML	-28.63115890342292	-57.47012151503989	177778
a1f8c175bac3359b64f0225869f8023d41d6c7d6	clustering web services to facilitate service discovery	journal	Clustering Web services would greatly boost the ability of Web service search engine to retrieve relevant services. The performance of traditional Web service description language (WSDL)-based Web service clustering is not satisfied, due to the singleness of data source. Recently, Web service search engines such as Seekda! allow users to manually annotate Web services using tags, which describe functions of Web services or provide additional contextual and semantical information. In this paper, we cluster Web services by utilizing both WSDL documents and tags. To handle the clustering performance limitation caused by uneven tag distribution and noisy tags, we propose a hybrid Web service tag recommendation strategy, named WSTRec, which employs tag co-occurrence, tag mining, and semantic relevance measurement for tag recommendation. Extensive experiments are conducted based on our real-world dataset, which consists of 15,968 Web services. The experimental results demonstrate the effectiveness of our proposed service clustering and tag recommendation strategies. Specifically, compared with traditional WSDL-based Web service clustering approaches, the proposed approach produces gains in both precision and recall for up to 14 % in most cases.	cluster analysis;collaborative filtering;experiment;precision and recall;relevance;service discovery;social network;tag (metadata);tag cloud;tag system;web services description language;web search engine;web service;world wide web	Jian Wu;Liang Chen;Zibin Zheng;Michael R. Lyu;Zhaohui Wu	2013	Knowledge and Information Systems	10.1007/s10115-013-0623-0	web service;web application security;web development;web modeling;data web;web analytics;web mapping;web design;web standards;computer science;ws-policy;social semantic web;data mining;semantic web stack;database;web intelligence;ws-i basic profile;web 2.0;world wide web;information retrieval;universal description discovery and integration	Web+IR	-28.323751204239176	-58.210874404632655	178307
8362b9850a1d38c872463866952b39c1a39911ab	web template extraction based on hyperlink analysis	content extraction;template extraction;information retrieval;articulo	Web templates are one of the main development resources for website engineers. Templates allow them to increase productivity by plugin content into already formatted and prepared pagelets. For the final user templates are also useful, because they provide uniformity and a common look and feel for all webpages. However, from the point of view of crawlers and indexers, templates are an important problem, because templates usually contain irrelevant information such as advertisements, menus, and banners. Processing and storing this information is likely to lead to a waste of resources (storage space, bandwidth, etc.). It has been measured that templates represent between 40% and 50% of data on the Web. Therefore, identifying templates is essential for indexing tasks. In this work we propose a novel method for automatic template extraction that is based on similarity analysis between the DOM trees of a collection of webpages that are detected using menus information. Our implementation and experiments demonstrate the usefulness of the technique.	circuit complexity;common look and feel;document object model;experiment;hyperlink;plug-in (computing);relevance;software engineer;web template system;world wide web	Julián Alarte;David Insa;Josep Silva;Salvador Tamarit	2014		10.4204/EPTCS.173.2	computer science;data mining;world wide web;information retrieval	Web+IR	-30.70781348614313	-55.481676799446205	178657
15a68c730e95fbe938f70273bec36188f024b09a	an approach of multi-path segmentation clustering based on web usage mining	pattern clustering;data mining;clustering algorithms data mining pattern analysis machine learning algorithms educational institutions information analysis web server algorithm design and analysis web sites clustering methods;internet;clustering method;web usage mining;web mining;web log mining;pattern clustering data mining internet;web site structure multipath segmentation clustering web usage mining web server log mining	Web log mining is applying web mining in analysis of web server log in order to solve problems mentioned above. Based on web log of a university, this paper focuses on analyzing and researching technology of web log mining; bringing forward a multi-path segmentation cluster method, which segments and clusters according to user access path to improve efficiency. What is more, this paper discusses how to improve website structure with achievements of web log mining.	association rule learning;blog;cluster analysis;login;prototype;sequential access;server (computing);server log;web mining;web server;world wide web	Houqun Yang;Jingsheng Lei;Fa Fu	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.145	web mining;the internet;data web;web analytics;web mapping;computer science;data science;data mining;data stream mining;web intelligence;world wide web	DB	-26.48519036282841	-55.72727810654779	178824
0acc671c1832877e24f0c56eeb826dfbfd093bb0	theme-based retrieval of web news	image database searching;search behavior;searching for images;image retrieval	We introduce an information system for organization and retrieval of news articles from Web publications, incorporating a classification framework based on Support Vector Machines. We present the data model for storage and management of news data and the system architecture for news retrieval, classification and generation of topical collections. We also discuss the classification results obtained with a collection of news articles gathered from a set of online newspapers.	support vector machine	Nuno Maria;Mário J. Silva	2000		10.1145/345508.345648	visual word;image retrieval;computer science;data mining;world wide web;information retrieval	Web+IR	-30.761167583672723	-58.127138817146744	179097
19af92153ef7babcbf983f7b7dc515b6ba2fabad	log mining to improve the performance of site search	generalized association rule;search engine;web pages;chaos;search engines;search methods;log mining;testing;satisfiability;taxonomy;web search;computer science;search engines web pages chaos computer science asia radio access networks taxonomy testing search methods web search;asia;radio access networks	In despite of the popularity of current search engines, people still suffer search failure and lots of non-relevant results when finding some specific information from a specific website. This is because the site search performance is not satisfying as the whole Web search. This paper analyzes the specialty of site search compared with traditional Web search, and the non-applicability of link-based re-ranking techniques such as HITS and PageRank. In this paper, we propose to use log mining to improve the site search performance. With the help of website taxonomy, a generalized association rule mining technique is applied to users’ log to abstract the user’s access patterns at different levels, and the mining results are then applied to re-ranking the retrieved pages. Our mining algorithm tackles the diversity problem of user’s access behavior and mines out general patterns. The experimental results show that our proposed method outperforms keyword-based method by 15% and DirectHit by 13% respectively.	algorithm;association rule learning;newton's method;pagerank;web search engine;web search query	Gui-Rong Xue;Hua-Jun Zeng;Zheng Chen;Wei-Ying Ma;Chao-Jun Lu	2002		10.1109/WISEW.2002.1177868	web mining;site map;web search engine;semantic search;search engine optimization;computer science;spamdexing;artificial intelligence;web crawler;data mining;database;search analytics;web search query;world wide web;information retrieval;search engine;taxonomy	Web+IR	-29.818646731862046	-55.26707205840011	179166
8e362bf98ae1217e386ee9f7deed8f286738e59f	learning models for ranking aggregates	learning model;rank aggregation;difference set;test collection	Aggregate ranking tasks are those where documents are not the final ranking outcome, but instead an intermediary component. For instance, in expert search, a ranking of candidate persons with relevant expertise to a query is generated after consideration of a document ranking. Many models exist for aggregate ranking tasks, however obtaining an effective and robust setting for different aggregate ranking tasks is difficult to achieve. In this work, we propose a novel learned approach to aggregate ranking, which combines different document ranking features as well as aggregate ranking approaches. We experiment with our proposed approach using two TREC test collections for expert and blog search. Our experimental results attest the effectiveness and robustness of a learned model for aggregate ranking across different settings.	aggregate data;aggregate function;blog;ensemble forecasting;learning to rank;ranking (information retrieval);robustness (computer science);text retrieval conference	Craig MacDonald;Iadh Ounis	2011		10.1007/978-3-642-20161-5_52	ranking;computer science;machine learning;data mining;ranking svm;information retrieval;difference set	Web+IR	-27.636491213480355	-54.111612518636946	179238
9f91d91f4c78690c58d147cb9cad0d7b798d0d10	ct-rank: a time-aware ranking algorithm for web search	ranking algorithm;tf-idf;time information;web page;web search	Time plays important roles in Web search, because most Web pages contain time information and a lot of Web queries are time-related. However, traditional search engines such as Google have little consideration on the time information in Web pages. In particular, they do not take into account the time information of Web pages when ranking searching results. In this paper, we present a new timeaware ranking algorithm for Web search, which is called CT-Rank (Content-Time-based Ranking). The algorithm uses three factors of a Web page, namely the Pagerank value, the title ranking score, and the time-constrained keyword ranking score, to sort search results, and we develop a two-stage algorithm to realize the time-based ranking. We conduct a comprehensive experiment on 6,500 Web pages which is manually collected through Google, and compare the performance of CT-Rank with other four competitor algorithms including Pagerank, vector space model based ranking, update time based ranking, and Google’s ranking algorithm. The experimental result shows that CT-Rank has the best performance under different temporal textual queries.	algorithm;ct scan;pagerank;web page;web search engine	Peiquan Jin;Xiaowen Li;Hong Chen;Lihua Yue	2010	JCIT		beam search;ranking;computer science;best-first search;web search query;ranking svm;search algorithm	Web+IR	-27.807487322154884	-55.24403417547776	179333
6c224bb53cdd05698209a28104689cf7ba273f4e	information retrieval and deduplication for tourism recommender sightsplanner	duplicate detection;conceptnet;web pages;information retrieval;data mining;recommender system;machine learning;web portal;genetic algorithms;optimization;query expansion	This paper is about scraping web pages for tourism objects and resolving duplicates for a tourism recommender system Sightsplanner. Gathering information from different web portals, we end up having several versions of the same object in our database. It is very important that we can find out which objects are duplicates and merge those. Only unique objects are presented to the end user. The main focus of this paper is therefore on deduplication problem. We have implemented a duplication detection system and tuned the parameters manually to get up to 85% accuracy. In this paper we present a machine learning setup which we used to improve deduplication accuracy of tourism attractions by 13 percentage points to achieve 98% accuracy. All the steps in the process are presented along with problems we tackled.	data deduplication;database;information retrieval;machine learning;portals;recommender system;web page	Ago Luberg;Michael Granitzer;Honghan Wu;Priit Järv;Tanel Tammet	2012		10.1145/2254129.2254191	computer science;database;world wide web;information retrieval	DB	-30.35429851066116	-54.65714689754078	179457
edd07ce75e90e39bc65c3d6239e6259700a4f413	folksonomies and science communication - a mash-up of professional science databases and web 2.0 services		Folksonomies complete the methods of indexing scientific documents. Now scientists in their function as readers may play an active role in science communication as well, since they can tag documents with terms taken from their professional or personal environment. Folksonomies allow the indexing of documents by everyone without following any rules. Besides the benefits of folksonomies there are severe problems, e.g. the tags’ lack of precision. In order to overcome the shortcomings of this collaborative indexing method we introduce natural language processing of tags and a relevance ranking algorithm which is based on specific tag distributions, on aspects of collaboration and on the actions of the “prosumers”. This article is a plea for the combination of the “old” science databases and the benefits of the folksonomies.	algorithm;database;folksonomy;mash-1;natural language processing;relevance;science communication;web 2.0	Wolfgang G. Stock	2007	Inf. Services and Use			DB	-30.86728767198855	-57.18722299005903	179616
c13e2d2cf735f934a6ac9621d2f2beacf5a4b206	information filtering based on wiki index database	information retrieval;knowledge extraction;indexation;information flow	In this paper we present a profile-based approach to information filtering by an analysis of the content of text documents. The Wikipedia index database is created and used to automatically generate the user profile from the user’s document collection. The problem-oriented Wikipedia subcorpora are created (using knowledge extracted from the user profile) for each topic of user interests. The index databases of these subcorpora are applied to filtering information flow (e.g., mails, news). Thus, the analyzed texts are classified into several topics explicitly presented in the user profile. The paper concentrates on the indexing part of the approach. The architecture of an application implementing the Wikipedia indexing is described. The indexing method is evaluated using the Russian and Simple English Wikipedia.	archive;database;information filtering system;user profile;wiki;wikipedia	Alexander V. Smirnov;Andrew Krizhanovsky	2008	CoRR		information flow;computer science;information filtering system;data mining;knowledge extraction;world wide web;information retrieval	Web+IR	-28.803405912324312	-56.4793369549666	179679
3c94147f2c8490889e86f38e42312d7d994c79e5	ranking algorithms for digital forensic string search hits	digital forensics;string search;ranking;relevancy;ranked list	This research proposes eighteen quantifiable characteristics of allocated files, unallocated clusters, and string search hits contained therein, which can be used to relevancy rank string search output. We executed a 36-term query across four disks in a synthetic case (“M57 Patents” from DigitalCorpora.org), which produced over two million search hits across nearly 50,000 allocated files and unallocated clusters. We sampled 21,400 search hits from the case, extracted the proposed feature values, trained binary class (relevant/ not-relevant) support vector machine (SVM) models, derived two relevancy ranking functions from the resultant model feature weights, and empirically tested the ranking algorithms. We achieved 81.02% and 85.97% prediction accuracies for the allocated and unallocated models, respectively. Further research is needed to validate these algorithms in a broader set of real-world cases, and/or adapt the algorithms to improve their robustness. Nonetheless, this research provides an important starting point for research into digital forensic search hit relevancy ranking algorithms. We proposed an initial set of relevancy ranking features and obtained very promising empirical results. The ability to achieve rank-ordered list output for search queries in digital forensics, similar to what web browsing and digital library users enjoy, is extremely important for digital forensic practitioners to reduce the analytical burden of text string searching e a valuable analytical technique. © 2014 Digital Forensics Research Workshop. Published by Elsevier Ltd. All rights reserved.	digital library;forensic search;library (computing);relevance;resultant;string (computer science);string searching algorithm;support vector machine;synthetic intelligence;test case;web search engine;web search query	Nicole Beebe;Lishu Liu	2014	Digital Investigation	10.1016/j.diin.2014.05.007	ranking;computer science;digital forensics;data mining;ranking svm;world wide web;computer security;information retrieval;string searching algorithm	Web+IR	-32.45155906428488	-58.708151136433145	179717
797991a7c0e5f2c36eca2221d7f748eb89d5ef56	identifying keywords to improve a web site text content		The steadily increasing competition of internet web sites makes it both more difficult and more important to attract and retain users. However, it is not always possible to determine beforehand which content is most appropriate to reach this goal, since the behavior and requirements of users can be heterogeneous and changing over time. In order to improve a web site text content, it is necessary to better use the words that have proven to attract the user interest. In this paper we introduce a method to identify such keywords in the text content of an operating web site. The effectiveness of the method was tested in a real web site, showing its benefits.	algorithm;cluster analysis;internet;point of view (computer hardware company);requirement;web page	Juan D. Velásquez;Sebastián A. Ríos;Alejandro Bassi;Hiroshi Yasuda;Terumasa Aoki	2004			world wide web;the internet;information retrieval;computer science	ML	-28.662578216920966	-54.88927150243427	179963
166b3bcc0aae16e503be41ed15708569a5935f76	intent models for contextualising and diversifying query suggestions	diversity;contextualisation;query suggestions	The query suggestion or auto-completion mechanisms help users to type less while interacting with a search engine. A basic approach that ranks suggestions according to their frequency in query logs is suboptimal. Firstly, many candidate queries with the same prefix can be removed as redundant. Secondly, the suggestions can also be personalised based on the user's context. These two directions to improve the mechanisms' quality can be in opposition: while the latter aims to promote suggestions that address search intents that a user is likely to have, the former aims to diversify the suggestions to cover as many intents as possible. We introduce a contextualisation framework that utilises a short-term context using the user's behaviour within the current search session, such as the previous query, the documents examined, and the candidate query suggestions that the user has discarded. This short-term context is used to contextualise and diversify the ranking of query suggestions, by modelling the user's information need as a mixture of intent-specific user models. The evaluation is performed offline on a set of approximately 1.0M test user sessions. Our results suggest that the proposed approach significantly improves query suggestions compared to the baseline approach.	baseline (configuration management);information needs;interaction;online and offline;redundancy (engineering);session (web analytics);test set;web search engine	Eugene Kharitonov;Craig MacDonald;Pavel Serdyukov;Iadh Ounis	2013		10.1145/2505515.2505661	sargable;query expansion;web query classification;ranking;computer science;data mining;world wide web;information retrieval	Web+IR	-27.66918914667394	-54.345380748671836	180255
014c3d1716a618b2920dde64b231afab90de2d13	what snippets say about pages in federated web search	ibcn;technology and engineering	What is the likelihood that a Web page is considered relevant to a query, given the relevance assessment of the corresponding snippet? Using a new federated IR test collection that contains search results from over a hundred search engines on the internet, we are able to investigate such research questions from a global perspective. Our test collection covers the main Web search engines like Google, Yahoo!, and Bing, as well as a number of smaller search engines dedicated to multimedia, shopping, etc., and as such reflects a realistic Web environment. Using a large set of relevance assessments, we are able to investigate the connection between snippet quality and page relevance. The dataset is strongly inhomogeneous, and although the assessors’ consistency is shown to be satisfying, care is required when comparing resources. To this end, a number of probabilistic quantities, based on snippet and page relevance, are introduced and evaluated.	approximation algorithm;book;java collections framework;online search;relevance;terminator 2: judgment day;usability testing;web page;web search engine	Thomas Demeester;Dong Nguyen;Dolf Trieschnigg;Chris Develder;Djoerd Hiemstra	2012		10.1007/978-3-642-35341-3_21	web search engine;computer science;database;world wide web;information retrieval	Web+IR	-32.717533180610744	-53.47373042189724	180564
32be1289ec2422dfc5353cf1b134bb6cfb36ca40	towards designing an efficient crawling window to analysis and annotate changes in linked data sources	linked data;event annotation;web architecture;content analysis;data dependence;levenshtein distance;data quality;similarity measure;crawling	Today the popularity of data quality is increasing in linked data, and its changes are being annotated. Linked data consuming applications need to be aware of changes in a dataset. Changes such as update, remove or creation links may occur for a time so it is necessary to detect them to update local data dependencies where this annotation is made by detecting changes systems. Updated or removed links can be detected using a syntactic change similarity measure, and it is simply done using the Levenshtein distance measure. However, a specific event classification of updated event and removed event, which is created by developing detecting changes systems, does not exist based on content analysis. A Hash number comparison approach over each linked data resource is developed to create a more specific classification of the initial updated and removed event. It is computed over RDF triples and is used to enrich the resources, annotating the new classification of the initial updated event and removed event. Annotations on the modification time are made in linked data resource, and making an average time study about when these specific events change, and using a Crawling Window to analyze a portion of Linked Data graph could be improved the crawling techniques for a domain. In this paper a study based in depth of child nodes analyzed in Linked Data graph is made to estimate what is the Crawling Window design more efficient.	data dependency;data quality;levenshtein distance;linked data;sensor;similarity measure;time and motion study;tree (data structure)	David Urdiales-Nieto;José Francisco Aldana Montes	2011		10.1145/1966901.1966904	computer science;data mining;world wide web;information retrieval	Web+IR	-28.54074980009832	-55.65371067426987	180778
e8dd0cf7eafa2c16833ef4ace929b9e5634b2848	a generalized links and text properties based forum crawler	information retrieval;information retrieval forum crawler clustering;clustering;web sites data mining information retrieval text analysis;forum crawler;light weight crawling method generalized link based web forum crawler text property based web forum crawler information gathering information mining user generated content generic web crawler redundant page elimination duplicate page elimination	Web forums have become a major source of information gathering/mining due to a large amount of user generated content. Crawling of web forums is necessary to gather/mine the information from them. However, a generic web crawler is unable to efficiently and effectively crawl the web forums because of the existence of many redundant and duplicate pages. In addition, there exists a crawling relationship among the useful pages that need to be considered. So, for efficient crawling, we need to intelligently crawl the web forums by eliminating redundant and duplicate pages, and understanding the crawling relationship. Existing works in forum crawling use visual pattern recognition based methods, which make them extremely computational expensive. In this paper, we propose a novel light-weight crawling method using text and links properties of the pages in web forums. Theoretical analysis and experimental results show the effectiveness and efficiency of the proposed method.	algorithm;cluster analysis;computation;experiment;information source;pattern recognition;redundancy (engineering);sampling (signal processing);tree traversal;user-generated content;virtual instrument software architecture;web crawler	Amit Sachan;Wee-Yong Lim;Vrizlynn L. L. Thing	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.213	static web page;web search engine;computer science;web crawler;distributed web crawling;web page;data mining;focused crawler;cluster analysis;world wide web;information retrieval	Web+IR	-29.029601655197162	-54.943869110318964	180816
5a107a653172b02b0aa2a0fca9d5532ff2e9bcff	evaluating the use of social networks in author name disambiguation in digital libraries	name disambiguation;digital library;social network	Digital libraries have become an important source of information for scientific communities. However, by gathering data from different sources, the problem of duplicate and ambiguous information about author names arises. Traditional methods of name disambiguation use syntactic attribute information. However, recently the use of relationship networks has been studied in data deduplication. This article presents a study of the impact of adding social network analysis to traditional methods in the name disambiguation problem in digital libraries. Through experiments using subsets of real libraries, we show that the use of social network analysis significantly improves the quality of results.	data deduplication;digital library;experiment;information source;internationalized domain name;library (computing);quality of results;social network analysis;word-sense disambiguation	Felipe Hoppe Levin;Carlos Alberto Heuser	2009	JIDM		computer science;data mining;world wide web;information retrieval	AI	-26.757338517101115	-58.40955738891836	180852
8bdfdbbf7f7d68c396c97addf1079ea45b7c9017	distributed web search efficiency by truncating results	filtering;web documents;web pages;information retrieval;digital library;domain name;relevance ranking;host name;merging;world wide web;web search;ranked distributions	"""A large set of Web documents (the TREC GOV2 collection) comes from many separate Internet hosts, such as www.nih.gov and travel.state.gov. There is considerable variability in the number of Web pages (i.e., documents) from each host. In this paper, we present and evaluate a method for setting a maximum number of """"hits"""" that may be presented for each web host. Federated search environments are increasingly common components of digital libraries and in these environments, the benefit of such a maximum is that it can reduce the number of possibly relevant documents presented by each subcollection, without hurting early precision measures such as P@20. Derivation of a maximum number, which is proportional to the subcollection size but not sensitive to different search topics, is made possible by an analysis of patterns of relevance judgment across approximately 17,000 web hosts in GOV2."""	digital library;federated search;host (network);library (computing);relevance;spatial variability;text retrieval conference;truncation;web hosting service;web page;web search engine	Christopher T. Fallen;Gregory B. Newby	2007		10.1145/1255175.1255214	filter;web service;web mining;digital library;data web;web analytics;web mapping;web search engine;computer science;semantic web;social semantic web;web page;data mining;web search query;web 2.0;world wide web;information retrieval;web server	Web+IR	-31.6641045775396	-55.450668318537346	181167
014ed1cfc7ffab995d8a416a7ce7057d807acb5a	mining interesting meta-paths from complex heterogeneous information networks	educational institutions encyclopedias electronic publishing internet gold data mining;data mining;gold;internet;electronic publishing;wikipedia meta paths mining complex heterogeneous information networks dblp imdb complex type systems meta path queries knowledge discovery pattern mining recall statistics;web sites complex networks data mining information networks pattern clustering query processing statistics;similarity information networks meta paths;encyclopedias	Meta-paths in heterogeneous information networks are almost always hand created and have, so far, only been attempted on data sets with very small type systems like DBLP, IMDB, etc. Most real-world heterogeneous information networks have large and complex type systems. As the size and complexity of the type-system grows it becomes more and more difficult for humans to form reasonable meta-path queries. This work introduces a new technique to discover a new market for data called interesting meta-paths from complex heterogeneous information networks. Our interestingness measure is based on classical knowledge discovery principles, but have been applied in such a way that only interesting meta-paths are mined from the hundreds-of-thousands of possible choices. As in classical pattern mining literature, precision and recall statistics are difficult to obtain, instead we evaluate the effectiveness of our results using a quantitative node-similarity analysis as well as a large user study. Finally, we apply the newly discovered interesting meta-paths to find similar nodes on the Wikipedia heterogeneous information networks.	algorithm;communication endpoint;dbl-browser;data mining;generative model;incisive;internet movie database (imdb);mined;precision and recall;running with rifles;snapshot (computer storage);type system;usability testing;wikipedia	Baoxu Shi;Tim Weninger	2014	2014 IEEE International Conference on Data Mining Workshop	10.1109/ICDMW.2014.25	gold;the internet;computer science;data science;machine learning;data mining;electronic publishing;world wide web;encyclopedia	DB	-29.27860645825624	-54.39770474920236	181420
670c3ac1a0adad1b00c0270fed5a229c61465e96	personal web revisitation by context and content keywords with relevance feedback	web revisitation access context page content relevance feedback;web pages;history;search engines;semantics;browsers;probabilistic logic;context;context web pages history semantics probabilistic logic search engines browsers	Getting back to previously viewed web pages is a common yet uneasy task for users due to the large volume of personally accessed information on the web. This paper leverages human's natural recall process of using episodic and semantic memory cues to facilitate recall, and presents a personal web revisitation technique called  WebPagePrev through context and content keywords. Underlying techniques for context and content memories’ acquisition, storage, decay, and utilization for page re-finding are discussed. A relevance feedback mechanism is also involved to tailor to individual's memory strength and revisitation habits. Our 6-month user study shows that: (1) Compared with the existing web revisitation tool Memento, History List Searching method, and Search Engine method, the proposed WebPagePrev delivers the best re-finding quality in finding rate (92.10 percent), average F1-measure (0.4318), and average rank error (0.3145). (2) Our dynamic management of context and content memories including decay and reinforcement strategy can mimic users’ retrieval and recall mechanism. With relevance feedback, the finding rate of  WebPagePrev increases by 9.82 percent, average F1-measure increases by 47.09 percent, and average rank error decreases by 19.44 percent compared to stable memory management strategy. Among time, location, and activity context factors in WebPagePrev, activity is the best recall cue, and context+content based re-finding delivers the best performance, compared to context based re-finding and content based re-finding.	elegant degradation;f1 score;memento pattern;memory management;organizing (structure);relevance feedback;usability testing;web page;web search engine	Li Jin;Ling Feng;Gang-Li Liu;Changping Wang	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2672747	computer science;memory management;web page;data mining;probabilistic logic;semantic memory;semantics;information retrieval;search engine;recall;relevance feedback	Web+IR	-33.39473814840151	-52.70728539075612	181721
98d43e5a3e82e33dd9a70902f4f34f9b493860a1	news vertical search: when and what to display to users	news vertical;qa75 electronic computers computer science;web search;user generated content	News reporting has seen a shift toward fast-paced online reporting in new sources such as social media. Web Search engines that support a news vertical have historically relied upon articles published by major newswire providers when serving news-related queries. In this paper, we investigate to what extent real-time content from newswire, blogs, Twitter and Wikipedia sources are useful to return to the user in the current fast-paced news search setting. In particular, we perform a detailed user study using the emerging medium of crowdsourcing to determine when and where integrating news-related content from these various sources can better serve the user's news need. We sampled approximately 300 news-related search queries using Google Trends and Bitly data in real-time for two time periods. For these queries, we have crowdsourced workers compare Web search rankings for each, with similar rankings integrating real-time news content from sources such as Twitter or the blogosphere. Our results show that users exhibited a preference for rankings integrating newswire articles for only half of our queries, indicating that relying solely on newswire providers for news-related content is now insufficient. Moreover, our results show that users preferred rankings that integrate tweets more often than those that integrate newswire articles, showing the potential of using social media to better serve news queries.	blog;blogosphere;crowdsourcing;real-time locating system;real-time transcription;real-time web;social media;usability testing;web search engine;web search query;wikipedia	Richard McCreadie;Craig MacDonald;Iadh Ounis	2013		10.1145/2484028.2484080	computer science;multimedia;internet privacy;user-generated content;world wide web;information retrieval	Web+IR	-32.62271824451674	-53.419098343372816	181996
03febdec0f92c046c7a4da118c0076f679bd6e01	comparison of pagination algorithms based-on large data sets	search engine;large data sets	  At present, many kinds of databases are widely used in Web applications. When Search Engines find tens of thousands of results  for a keyword, it’s very important to spit out and display the results in multiple pages quickly and efficiently instead of  just putting them all in one long page. This paper firstly presents several kinds of pagination algorithms based-on Large  Data Sets which belongs to the database-driven method. Then, we will experiment on the large data sets and discuss different  pagination approaches. The experimental results show that proc_Rownumber() method can greatly improve the performance on the  query speed of the pagination.    	algorithm	Junkuo Cao;Weihua Wang;Yuanzhong Shu	2010		10.1007/978-3-642-19853-3_56	data mining;information retrieval	ML	-29.389095967796013	-54.81555170742107	182650
4a97429f58075e871f1029af7ce354ca64dbf1db	exploiting community behavior for enhanced link analysis and web search	explicit rate;search engine;web pages;004;query refinement;satisfiability;proof of concept;link analysis;markov model;negative feedback;query logs link analysis markov reward model;web search;user behavior;query logs	Methods for Web link analysis and authority ranking such as PageRank are based on the assumption that a user endorses a Web page when creating a hyperlink to this page. There is a wealth of additional user-behavior information that could be considered for improving authority analysis, for example, the history of queries that a user community posed to a search engine over an extended time period, or observations about which query-result pages were clicked on and which ones were not clicked on after a user saw the summary snippets of the top-10 results. This paper enhances link analysis methods by incorporating additional user assessments based on query logs and click streams, including negative feedback when a query-result page does not satisfy the user demand or is even perceived as spam. Our methods use various novel forms of advanced Markov models whose states correspond to users and queries in addition to Web pages and whose links also reflect the relationships derived from query-result clicks, query refinements, and explicit ratings. Preliminary experiments are presented as a proof of concept.	algorithm;experiment;hyperlink;link analysis;markov chain;markov model;negative feedback;pagerank;spamming;virtual community;web page;web search engine	Julia Luxenburger;Gerhard Weikum	2006			query expansion;web query classification;computer science;data mining;web search query;world wide web;information retrieval	Web+IR	-33.16358375835326	-52.58803008991748	183382
cec58d31bf7b4242bbd4f084e0aaa16e63dede72	an empirical evaluation of techniques for ranking semantic associations		Searching for associations between entities is needed in many domains like national security and bioinformatics. In recent years, it has been facilitated by the emergence of graph-structured semantic data on the Web, which offers structured semantic associations more explicit than those hiding in unstructured text for computers to discover. The increasing volume of semantic data often produces excessively many semantic associations, and requires ranking techniques to identify the more important ones for users. Despite the fruitful theoretical research on innovative ranking techniques, there is a lack of comprehensive empirical evaluation of these techniques. In this article, we carry out an extensive evaluation of eight techniques for ranking semantic associations, including two novel ones we propose. The practical effectiveness of these techniques is assessed based on 1,200 ground-truth rankings created by 30 human experts for real-life semantic associations and the explanations given by the experts. Our findings also suggest a number of directions in improving existing techniques and developing novel techniques for future work.	bioinformatics;centrality;computer;diagram;emergence;entity;ground truth;long tail;real life;semantic web;world wide web	Gong Cheng;Fei Shao;Yuzhong Qu	2017	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2017.2735970	data mining;computer science;social semantic web;semantic data model;semantic computing;artificial intelligence;machine learning;semantic similarity;semantic analytics;semantic grid;semantic search;ontology (information science);information retrieval	DB	-26.933500854278414	-57.784729648453	183773
865c3fc6b5bf2f70cb83049c2d183da49242e1ad	a self-adaptive cross-domain query approach on the deep web	top k;cross domain;deep web	"""As integration systems of data sources in the same domain become more and more, another application comes up with the tide of them. Because of correlation of pairs of domains, when we do some queries involving multiple domains, such as """"find a post named software development engineer on job web and look for apartments for rental around the company having been chosen"""", we note that general-purpose search engines and general integration frameworks fail to answer cross-domain queries. This paper presents SCDQ, an approach providing fully automated support for cross-domain queries. More specifically, for SCDQ, (i) find which domains are correlated based on data sources having been clustered according to domain. (ii) Recommend different cross-domain paths to meet user's all possible intentions when query arrives."""	deep web	Yingjun Li;Derong Shen;Tiezheng Nie;Ge Yu;Jing Shan;Yue Kou	2011		10.1007/978-3-642-23535-1_6	computer science;operating system;machine learning;data mining;database;world wide web;deep web	ML	-27.577693892485158	-55.00472419216942	185020
14ae81644d714bb4dde7bc93202edf83794e128c	505-998-0800 x116 robert t. access innovations jr., e.-mail: bob_kasenchak@accessinn. com lawlorbonnie kasenchak: scholarly triage: advances in manuscript submission using text analytics techniques		The drastic increase in the volume of submissions for inclusion in scholarly journals offers new challenges for scholarly publishers. These include sorting through thousands of new manuscripts to prioritize those most likely to be published first and detecting dubious research. Although recent advances in peer review and editorial management platforms have advanced processes to help alleviate the problem, new solutions to prioritize high-value papers – and to flag suspect ones – are emerging. Using text analytics methods grounded in natural language processing (NLP) and other techniques to augment the submission review process can include leveraging taxonomy-based indexing terms to match manuscripts to appropriate reviewers and editors, preventing fraud by detecting machine-generated entries, screening for irreproducible research practices, and predicting the likelihood of acceptance by examining non-content factors.	natural language processing;sensor;sorting;text mining	Robert T. Kasenchak	2017	Inf. Services and Use	10.3233/ISU-170847	triage;data science;information retrieval;text mining;computer science	Web+IR	-33.18231760897098	-58.42448148527055	186016
33c14f666325f00b5e8ae8f75395d961225681d0	the volume and evolution of web page templates	boilerplate;web pages;information retrieval;data mining;link analysis;randomized algorithm;web mining;algorithms;data cleaning;templates	Web pages contain a combination of unique content and template material, which is present across multiple pages and used primarily for formatting, navigation, and branding. We study the nature, evolution, and prevalence of these templates on the web. As part of this work, we develop new randomized algorithms for template extraction that perform approximately twenty times faster than existing approaches with similar quality. Our results show that 40--50% of the content on the web is template content. Over the last eight years, the fraction of template content has doubled, and the growth shows no sign of abating. Text, links, and total HTML bytes within templates are all growing as a fraction of total content at a rate of between 6 and 8% per year. We discuss the deleterious implications of this growth for information retrieval and ranking, classification, and link analysis.	byte;cache (computing);html;hyperlink;information retrieval;internet branding;link analysis;no symbol;randomized algorithm;web page	David Gibson;Kunal Punera;Andrew Tomkins	2005		10.1145/1062745.1062763	web mining;boilerplate;link analysis;computer science;web page;data mining;database;randomized algorithm;world wide web;website parse template;information retrieval	Web+IR	-32.46021935266954	-54.49051671986082	186171
19a78645466c208981dd37bf35f45b6be75ef36a	query recommendation based on query relevance graph		With the explosive and diverse growth of web contents, query recommendation is a critical aspect of the search engine. Different kind of re c mmendation like query, image, movie, music and book etc. are used every day . Different kinds of data are used for the recommendations. If we model the data into various kinds of graphs then we can build a general method for any recommend atio . This paper presents a general method to recommend queries by combining tw o graphs: 1) query click graph which uses the knowledge of link between user input query and clicked URLs and 2) query text similarity graph which finds the similarity between two queries using Jaccard similarity. The proposed method prov i es literally as well as semantically relevant queries for users’ need. Exper iment results show that the proposed algorithm outperforms heat diffusion method by providing more number of relevant queries. It is also useful to other recomm endations like image, query and product recommendation.		D. Sejal;K. G. Shailesh;V. Tejaswi;Dinesh Anvekar;K. R. Venugopal;S. Sitharama Iyengar;Lalit M. Patnaik	2016	Trans. MLDM		ranking (information retrieval);query expansion;computer science;web search query;artificial intelligence;data mining;web query classification;sargable;information retrieval;rdf query language;pattern recognition;query language;query optimization	Web+IR	-29.01291905783364	-54.315054129296	186651
19210e809cd0bfa760b949dd3ab01d6e24353b4d	distribution of relevant documents in domain-level aggregates for topic distillation	distribution of relevant documents;aggregates;web ir	In this paper, we study the distribution of relevant documents in aggregates, formed by grouping the retrieved documents according to their domain. For each aggregate, we take into account its size, and a measure of the correlation between its incoming and outgoing hyperlinks. We report on a preliminary experiment with two TREC topic distillation tasks, where we find that larger aggregates, or those aggregates with correlated hyperlinks, are more likely to contain relevant documents. This result shows that the distribution of domain-level aggregates is potentially useful for finding relevant documents.	aggregate data;hyperlink;text retrieval conference	Vassilis Plachouras;Iadh Ounis	2004		10.1145/1013367.1013481	data mining;multimedia	Web+IR	-32.41276687948119	-54.553747449235104	187165
a8e2c4eaa318e06466c18f7b5ec5b2881670ac71	coniunge et impera: multiple-graph mining for query-log analysis	search engine;implicit feedback;query log analysis;graph mining;semantic information;graph representation;query logs	Query logs of search engines record a huge amount of data about the actions of the users who search for information on the Web. Hence, they contain a wealth of valuable knowledge about the users’ interests and preferences, as well as the implicit feedback that Web searchers provide when they click on the results obtained for their queries. In this paper we propose a general and completely unsupervised methodology for query-log analysis, which consists of aggregating multiple graph representations of a query log, tailored to capturing different semantic information. The combination is carried out by applying simple but efficient graph-mining techniques. We show that our approach achieves very good performance for two different applications, which are classifying query transitions and recognizing spam queries.		Ilaria Bordino;Debora Donato;Ricardo A. Baeza-Yates	2010		10.1007/978-3-642-15880-3_17	sargable;query optimization;query expansion;web query classification;computer science;data mining;graph;web search query;world wide web;information retrieval;query language;search engine	Web+IR	-29.26344086289778	-53.774589530397314	187888
a9bd6f291fded5b33972beb3b7189e3114b947c5	siren - security information retrieval and extraction engine		Domain specific search engines (DSSE) are gaining popularity because of better search relevance and domain specificity. The growth of IT and internet led to the increase of cyber attacks, however, lack of DSSE for Security is making users refer multiple sites for security information. We demonstrate SIREN, a search engine for ‘Information and Cyber Security’ with coverage and classification at subdomain level and ranking search results based on site credibility. As part of our demonstration, we automated identification of seed URLs (34,007) and related child URLs (400,726) of the security domain using Artificial Bee Colony algorithm. Also, we evaluated functional and non-functional parameters of available open source software stack that can be used for building other DSSEs.	information retrieval	Lalit Mohan Sanagavarapu;Neeraj Mathur;Shriyansh Agrawal;Y. Raghu Reddy	2018		10.1007/978-3-319-76941-7_81	data mining;artificial bee colony algorithm;the internet;information retrieval;software;search engine;security domain;credibility;popularity;computer science;ranking	Crypto	-29.663805131367372	-55.52806636245711	188353
acbae4349b0b6881532b7ff675f861d3b733b170	metrics for ranking ontologies	search engine;knowledge structure;domain ontology	Representing knowledge using domain ontologies has shown to be a useful mechanism and format for managing and exchanging information. Due to the difficulty and cost of building ontologies, a number of ontology libraries and search engines are coming to existence to facilitate reusing such knowledge structures. The need for ontology ranking techniques is becoming crucial as the number of ontologies available for reuse is continuing to grow. In this paper we present AKTiveRank, a prototype system for ranking ontologies based on the analysis of their structures. We describe the metrics used in the ranking system and present an experiment on ranking ontologies returned by a popular search engine for an example query.	library (computing);ontology (information science);prototype;web search engine	Harith Alani;Christopher Brewster	2006			upper ontology;idef5;ranking;ontology components;computer science;data science;data mining;database;world wide web;information retrieval;process ontology;search engine	AI	-30.622004634673303	-56.25424230161749	188687
af18557736c3ed3ab36dbf9d4040e70c8d4ddac2	efficient storage and retrieval of probabilistic latent semantic information for information retrieval	thesaurus;information systems;probabilistic latent semantic indexing;information retrieval;term frequency;information retrieval and web search;journal article;semantic information;indexation;electronic information storage and retrieval services;query expansion;probabilistic latent semantic analysis	Probabilistic latent semantic analysis (PLSA) is a method for computing term and document relationships from a document set. The probabilistic latent semantic index (PLSI) has been used to store PLSA information, but unfortunately the PLSI uses excessive storage space relative to a simple term frequency index, which causes lengthy query times. To overcome the storage and speed problems of PLSI, we introduce the probabilistic latent semantic thesaurus (PLST); an efficient and effective method of storing the PLSA information. We show that through methods such as document thresholding and term pruning, we are able to maintain the high precision results found using PLSA while using a very small percent (0.15%) of the storage space of PLSI.	data structure;effective method;experiment;information retrieval;probabilistic latent semantic analysis;tfi-5;tf–idf;thesaurus;thresholding (image processing)	Laurence Anthony F. Park;Kotagiri Ramamohanarao	2008	The VLDB Journal	10.1007/s00778-008-0093-2	computer science;data mining;database;probabilistic latent semantic analysis;information retrieval;divergence-from-randomness model	Web+IR	-30.887533942159948	-58.97186706697387	189509
a6021021c5679246c95f3753da89c2672530b267	estimating similarity of rich internet pages using visual information		Traditional text-based web page similarity measures fail to handle rich-information-embedded modern web pages. Current approaches regard web pages as either DOM trees or images. However, the former only focuses on the web page structure, while the latter ignores the inner connections among different web page features. Therefore, they are not suitable for modern web pages. Hence, the idea of a block tree is introduced, which contains both structural and visual information of web pages. A visual similarity metric is proposed as the edit distance between two block trees. Finally, an experiment is undertaken, by cross-comparing 500 web pages, illustrating that the model appears to be highly accurate, empirically demonstrating that the metric is highly promising.		Zhen Xu;James Miller	2017	Int. J. Web Eng. Technol.	10.1504/IJWET.2017.086415	computer science;edit distance;world wide web;the internet;biconnected component;web page;information retrieval	Web+IR	-33.37930859464261	-55.059593223545136	189551
b7571e398787045191edc78d903c6e291279bac4	using wikipedia-based conceptual contexts to calculate document similarity	search engines information retrieval wikipedia crawlers humans clustering algorithms content based retrieval organizing advertising weight measurement;search engine;search engines;information retrieval;vocabulary;vocabulary search engines;information services;data mining;semantic relatedness;internet;text similarity;semantic relatedness document similarity text similarity;wikipedia hypertext corpus wikipedia based conceptual contexts document similarity search engine results;electronic publishing;encyclopedias;context;document similarity	Rating the similarity of two or more text documents is an essential task in information retrieval. For example, document similarity can be used to rank search engine results, cluster documents according to topics etc. A major challenge in calculating document similarity originates from the fact that two documents can have the same topic or even mean the same, while they use different wording to describe the content. A sophisticated algorithm therefore will not directly operate on the texts but will have to find a more abstract representation that captures the texts' meaning. In this paper, we propose a novel approach for calculating the similarity of text documents. It builds on conceptual contexts that are derived from content and structure of the Wikipedia hypertext corpus.	algorithm;experiment;hyperlink;hypertext;information retrieval;microsoft outlook for mac;relational database management system;speedup;web search engine;weight function;wikipedia;word sense;word-sense disambiguation	Fabian Kaiser;Holger Schwarz;Mihály Jakob	2009	2009 Third International Conference on Digital Society	10.1109/ICDS.2009.7	natural language processing;computer science;electronic publishing;world wide web;information retrieval;search engine	Web+IR	-29.637525161131297	-57.2902248603295	189578
26ec0b0a8fae543e19c8bb8cad8fe279a70df89f	diversifying web search results	query ambiguity;result diversity;keyword search;expectation maximization;web search;search diversity;query logs	Result diversity is a topic of great importance as more facets of queries are discovered and users expect to find their desired facets in the first page of the results. However, the underlying questions of how 'diversity' interplays with 'quality' and when preference should be given to one or both are not well-understood. In this work, we model the problem as expectation maximization and study the challenges of estimating the model parameters and reaching an equilibrium. One model parameter, for example, is correlations between pages which we estimate using textual contents of pages and click data (when available). We conduct experiments on diversifying randomly selected queries from a query log and the queries chosen from the disambiguation topics of Wikipedia. Our algorithm improves upon Google in terms of the diversity of random queries, retrieving 14% to 38% more aspects of queries in top 5, while maintaining a precision very close to Google. On a more selective set of queries that are expected to benefit from diversification, our algorithm improves upon Google in terms of precision and diversity of the results, and significantly outperforms another baseline system for result diversification.	baseline (configuration management);clickstream;diversification (finance);expectation–maximization algorithm;experiment;information retrieval;randomness;web search engine;wikipedia;word-sense disambiguation	Davood Rafiei;Krishna Bharat;Anand Shukla	2010		10.1145/1772690.1772770	web query classification;expectation–maximization algorithm;computer science;machine learning;data mining;database;world wide web;information retrieval	Web+IR	-32.39442461704433	-54.364760364974195	190298
8772dfc60f114b0cb51af90f3792f0b77286d767	an interactive retrieval framework for online health information	web pages;prototypes;semantics;resource description framework;medical services;search problems;conferences	People and patients are increasingly using the internet to search for health information, but finding the right terms to search requires iterative search and query reformulation [1], which may be difficult for lay persons who do not have much medical knowledge. To solve the problem, we propose an interactive searching mechanism where resources are annotated using existing healthcare database and search result could be automatically updated based on users' feedback.	interactivity;iterative method	Mingkun Gao;Liu Yuan;Wai-Tat Fu	2016	2016 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2016.45	computer science;data mining;web search query;world wide web;information retrieval;search engine	Robotics	-30.084650700410435	-52.56198833192024	191049
b8482de98d2fcdf60a4783b20e51193fa5798864	which one is better: presentation-based or content-based math search?		Mathematical content is a valuable information source and retrieving this content has become an important issue. This paper compares two searching strategies for math expressions: presentation-based and content-based approaches. Presentation-based search uses state-ofthe-art math search system while content-based search uses semantic enrichment of math expressions to convert math expressions into their content forms and searching is done using these content-based expressions. By considering the meaning of math expressions, the quality of search system is improved over presentation-based systems.	futures studies;gene ontology term enrichment;information source;markup language;semantic html	Minh-Quoc Nghiem;Giovanni Yoko Kristianto;Goran Topic;Akiko Aizawa	2014		10.1007/978-3-319-08434-3_15	computer science;multimedia;information retrieval	AI	-30.258039259268706	-58.34388438030093	191672
76fd4508a0b4de252f890d719b91db70b5aa0e44	modeling and predicting the task-by-task behavior of search engine users	query log analysis;task recommendation;task discovery	Web search engines answer user needs on a query-by-query fashion, namely they retrieve the set of the most relevant results to each issued query, independently. However, users often submit queries to perform multiple, related tasks. In this paper, we first discuss a methodology to discover from query logs the latent tasks performed by users. Furthermore, we introduce the Task Relation Graph (TRG) as a representation of users’ search behaviors on a task-by-task perspective. The task-by-task behavior is captured by weighting the edges of TRG with a relatedness score computed between pairs of tasks, as mined from the query log. We validate our approach on a concrete application, namely a task recommender system, which suggests related tasks to users on the basis of the task predictions derived from the TRG. Finally, we show that the task recommendations generated by our solution are beyond the reach of existing query suggestion schemes, and that our method recommends tasks that user will likely perform in the near future.	mined;recommender system;web search engine	Claudio Lucchese;Salvatore Orlando;Raffaele Perego;Fabrizio Silvestri;Gabriele Tolomei	2013			query expansion;web query classification;computer science;data mining;web search query;world wide web;information retrieval	Web+IR	-27.83586478464359	-54.24270433796211	191860
64fde5ab4403d7a45dae112052c9548890ff5340	controlled automatic query expansion based on a new method arisen in machine learning for detection of semantic relationships between terms	pseudo relevance feedback;information retrieval;least square method;semantic relationships;automatic query expansion	With the proliferation of textual data on the web, efficient access to relevant information to meet the user's needs has become an important problem in the information retrieval tasks. This problem is specially due to the short queries submitted usually by users to an information retrieval system to describe their needs. These systems have to complete the user needs with related terms in order to disambiguate the user query and better meet the user's needs. This paper presents a new method to define semantic relationships between terms of the relevant returned documents for a given query in order to improve the description of the user's needs, by expanding automatically the original query with related terms, and to improve the search results. Some experiments have been performed on the CLEF 2014 collection to show the effectiveness of our method.	document-term matrix;experiment;information retrieval;machine learning;query expansion;relevance feedback;rocchio algorithm;text corpus;the matrix	Nesrine Ksentini;Mohamed Tmar;Faïez Gargouri	2015	2015 15th International Conference on Intelligent Systems Design and Applications (ISDA)	10.1109/ISDA.2015.7489214	query expansion;relevance;cognitive models of information retrieval;computer science;concept search;data mining;database;least squares;information retrieval;query language;human–computer information retrieval	DB	-30.349302104858022	-59.07907225645299	191872
c836f6b9fd3b06948eb41b2907bf134f650c3fb9	research of people disambiguation by combining multiple knowledges		With the rapid development of Internet and many related technology, Web has become the main source of information. For many search engines, there are many different identities in the returned results of character information query. Thus the Research of People disambiguation is important. In this paper we attempt to solve this problem by combing different knowledge. As people usually have different kind of careers, so we first utilize this knowledge to classify people roughly. Then we use social context of people to identify different person. The experimental results show that these knowledge are helpful for people disambiguation.	information source;internet;web search engine;word-sense disambiguation	Erlei Ma;Yuanchao Liu	2010			computer science	AI	-27.76098969618665	-54.5531217510152	193082
17a7f66e726212d18c2a3487306f6434dec27411	a revised simrank approach for query expansion	search engine;information retrieval;computational complexity;normal weight;term extraction;query expansion;query logs;test collection	Query expansion technologies based on pseudo-relevance documents have been proven to be effective in many information retrieval tasks. One prob- lem with these methods is that some of the expansion terms extracted from feedback documents are irrelevant to the query, which may hurt the retrieval performance. In this paper, we proposed a normalized weight SimRank (NWS) approach for query expansion, with query logs collected by a practical search engine. Analyzing the relationship between queries and URLs, we create a query-click graph, and a term-relationship graph is constructed by several trans- formations. In order to reduce the computational complexity of NWS, strategies of pruning and radius limit were used to optimize the algorithm. Experimental results on two TREC test collections show that our approach can discover the qualified terms effectively and improve queries' accuracy.	query expansion;simrank	Yunlong Ma;Hongfei Lin;Song Jin	2010		10.1007/978-3-642-17187-1_53	sargable;query optimization;query expansion;web query classification;ranking;boolean conjunctive query;computer science;data mining;database;computational complexity theory;web search query;world wide web;information retrieval;query language;search engine;spatial query	NLP	-30.096479682349667	-58.94719928938843	193627
1dd1b0405cdc495407a9110b7194fc1929a5e1b3	discovering interesting relationships among deep web databases: a source-biased approach	present value;deep web database;database ranking;probing;biased discovery;deep web;performance optimization	The escalation of deep web databases has been phenomenal over the last decade, spawning a growing interest in automated discovery of interesting relationships among available deep web databases. Unlike the “surface” web of static pages, these deep web databases provide data through a web-based query interface and account for a huge portion of all web content. This paper presents a novel source-biased approach to efficiently discover interesting relationships among web-enabled databases on the deep web. Our approach supports a relationship-centric view over a collection of deep web databases through source-biased database analysis and exploration. Our source-biased approach has three unique features: First, we develop source-biased probing techniques, which allow us to determine in very few interactions whether a target database is relevant to the source database by probing the target with very precise probes. Second, we introduce source-biased relevance metrics to evaluate the relevance of deep web databases discovered, to identify interesting types of source-biased relationships for a collection of deep web databases, and to rank them accordingly. The source-biased relationships discovered not only present value-added metadata for each deep web database but can also provide direct support for personalized relationship-centric queries. Third, but not least, we also develop a performance optimization using source-biased probing with focal terms to further improve the effectiveness of the basic source-biased model. A prototype system is designed for crawling, probing, and supporting relationship-centric queries over deep web databases using the source-biased approach. Our experiments evaluate the effectiveness of the proposed source-biased analysis and discovery model, showing that the source-biased approach outperforms query-biased probing and unbiased probing.	algorithm;association rule learning;cluster analysis;database;deep web;dictionary;experiment;focal (programming language);interaction;mathematical optimization;named-entity recognition;performance tuning;personalization;privilege escalation;prototype;relevance;unbiased rendering;web application;web content;web crawler	James Caverlee;Ling Liu;Daniel Rocco	2006	World Wide Web	10.1007/s11280-006-0227-7	web mining;web modeling;data web;computer science;social semantic web;data mining;web intelligence;present value;web search query;world wide web;information retrieval;deep web	DB	-29.736258523226322	-54.20184875939707	194877
84a71ada5fb85f5781428e889b659e02fb62a398	indexing the web - a challenge for supercomputers	eigenvalues and eigenfunctions;personal communication networks;search engines;indexing supercomputers search engines computer architecture web search delay personal communication networks fault tolerance eigenvalues and eigenfunctions internet;computer architecture;internet;indexing;indexation;fault tolerance;web search;supercomputers	Since January 2002, the Google search engine has been powering an average of 150 million web searches a day, with a peark of over 2000 searches per second. These searches are performed over an index of over 2 billion documents, over 300 million images, and over 700 million Usenet messages. To guarantee fast user response time, Google performs these searches on a cluster of over 10,000 PCs. The main challenages with this architecture are fault-tolerance and the quality of search results. Replication solves the former and the PageRank score is used to advance the latter. The PageRank score is based on an eigenvalue computation of a large matrix that is derived from the web graph and is one of the main contributor to very high quality search results. As Internet use continues to grow, so does the use of the Google search engine. The Google architecture is designed to scale to accommodate the growth in useage as well as the growth of the web. Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER’02) 0-7695-1745-5/02 $17.00 © 2002 IEEE	computation;computer cluster;display resolution;fault tolerance;google search;internet;pagerank;proceedings of the ieee;response time (technology);supercomputer;usenet;web search engine;webgraph;world wide web	Monika Henzinger	2002		10.1109/CLUSTR.2002.1137763	search engine indexing;fault tolerance;parallel computing;the internet;web search engine;computer science;spamdexing;theoretical computer science;web crawler;operating system;database;world wide web;search engine	HPC	-31.769983422386254	-55.23909388712279	195830
64ca5f4888630263af2f2bbe638cd532f9e1686d	using search session context for named entity recognition in query	context aware;context information;search session;information retrieval;crf;named entity recognition;conditional random field;topic model	Recently, the problem of Named Entity Recognition in Query (NERQ) is attracting increasingly attention in the field of information retrieval. However, the lack of context information in short queries makes some classical named entity recognition (NER) algorithms fail. In this paper, we propose to utilize the search session information before a query as its context to address this limitation. We propose to improve two classical NER solutions by utilizing the search session context, which are known as Conditional Random Field (CRF) based solution and Topic Model based solution respectively. In both approaches, the relationship between current focused query and previous queries in the same session are used to extract novel context aware features. Experimental results on real user search session data show that the NERQ algorithms using search session context performs significantly better than the algorithms using only information of the short queries.	algorithm;conditional random field;information retrieval;named entity;named-entity recognition;session (web analytics);topic model	Junwu Du;Zhimin Zhang;Jun Yan;Yan Cui;Zheng Chen	2010		10.1145/1835449.1835605	query expansion;computer science;machine learning;data mining;topic model;world wide web;conditional random field;information retrieval	Web+IR	-28.324540300587273	-58.439435933068985	195978
f2bae85b963deb737aa19553577754051122210b	fairscholar: balancing relevance and diversity for scientific paper recommendation		In this paper, we present (mathsf {FairScholar}), a novel scientific paper recommendation system that aims at balancing both relevance and diversity while searching for research papers in response to keyword queries. Our system performs a vertex reinforced random-walk, a time heterogeneous random-walk on the citation graph of papers in order to factor in diversity while serving recommendations. To incorporate semantically similar items in the search results, it uses a query expansion step that finds similar keywords using community detection. An online demo of our search engine is available at http://www.cnergres.iitkgp.ac.in/FairScholar/.	relevance	Ankesh Anand;Tanmoy Chakraborty;Amitava Das	2017		10.1007/978-3-319-56608-5_76	data mining;query expansion;information retrieval;recommender system;search engine;computer science;citation graph	ML	-27.056592809479007	-57.164287016822335	196016
b2b34a2a0a5674dd8b5f180a73d9fc250f85f4cc	query-oriented clustering: a multi-objective approach	document clustering;cluster algorithm;information retrieval;information access;query oriented clustering;multiobjective optimization;point of view;document similarity	Document clustering techniques have been widely applied in Information Retrieval to reorganize results furnished as a response to user's queries. Following the Cluster Hypothesis which states that relevant documents tend to be more similar one to each other than to non-relevant ones, most of relevant documents are likely to be gathered in a single cluster. Usually, systems organizing search results as a set of clusters consider this tendency as a very advantageous phenomenon, since it allows to filter the results provided by the initial search. Adopting a different point of view, we rather consider the Cluster Hypothesis as a hindrance to the information access since it prevents the emergence of the various aspects of the query. The risk induced is to restrict the perception of the subject to an unique point of view. Therefore, we propose to rather distribute the relevant documents over clusters by orienting the organization of the clusters according to the user's topic. The aim is to attract the clusters around the latter in order to highlight the thematic differences between documents which are strongly connected to the query. Rather than modifying the inter-documents similarity computation as it is the case in several studies, we propose to directly act on the organization of the clusters by using a multi-objective evolutionary clustering algorithm which, besides the classical internal cohesion, also optimizes the query proximity of the clusters. First experimental results highlight the great benefit which may be gained by our way of query consideration.	algorithm;cluster analysis;cluster hypothesis;computation;emergence;information access;information retrieval;organizing (structure);strongly connected component	Sylvain Lamprier;Tassadit Amghar;Frédéric Saubion;Bernard Levrat	2010		10.1145/1774088.1774467	correlation clustering;query expansion;web query classification;ranking;document clustering;fuzzy clustering;computer science;multi-objective optimization;machine learning;data mining;database;cluster analysis;world wide web;information retrieval	Web+IR	-28.175009187127156	-57.845526312286836	196251
fecaa680284ad3b3cbb4de88e3a372c57ed3c848	framework for location-aware search engine		Nowadays, a large part of the multimedia data on the Internet is generated with devices that automatically annotate them with location information However, free-form content on websites does not implicitly contain any geographical information. This is the biggest challenge for building a location-aware search engine. In this paper, we study how to extract location-aware information from the web. The key challenges are to detect location from a web page and to extract relevant information related to that location. We detect locations by identifying postal addresses using freely available gazetteers. Additional information for summarising the search results are titles and representative images, which we mine from the content using simple rule-based approaches utilising the structure of web pages. This information can be used to personalise search results for mobile users so that the results are relevant to their location.	blog;experiment;internet;linear algebra;location awareness;logic programming;nl (complexity);phantomjs;postal;web page;web search engine	Andrei Tabarcea;Najlah Gali;Pasi Fränti	2017	J. Location Based Services	10.1080/17489725.2017.1407001	data mining;computer science;search engine;database search engine	Web+IR	-29.46677534157285	-53.915533271173345	196691
5a434e7edc86f581b1345f04c640581eb580390e	a search engine for natural language applications	web documents;search engine;variables;noun;information extraction;space time;web search engine;large scale;corpus;indexing;language processing;natural language;indexation;query;language;natural language processing	"""Many modern natural language-processing applications utilize search engines to locate large numbers of Web documents or to compute statistics over the Web corpus. Yet Web search engines are designed and optimized for simple human queries---they are not well suited to support such applications. As a result, these applications are forced to issue millions of successive queries resulting in unnecessary search engine load and in slow applications with limited scalability.In response, this paper introduces the Bindings Engine (BE), which supports queries containing typed variables and  string-processing functions. For example, in response to the query  """"powerful ‹noun›"""" BE will return all the nouns in its index that immediately follow the word """"powerful"""", sorted by frequency. In response to the query  """"Cities such as ProperNoun(Head(‹NounPhrase›))"""", BE will return a list of proper nouns likely to be city names.BE's novel  neighborhood index enables it to do so with O(k) random disk seeks and O(k) serial disk reads, where k is the number of non-variable terms in its query. As a result, BE can yield several orders of magnitude speedup for large-scale language-processing applications. The main cost is a modest increase in space to store the index. We report on experiments validating these claims, and analyze how BE's space-time tradeoff scales with the size of its index and the number of variable types. Finally, we describe how a BE-based application extracts thousands of facts from the Web at interactive speeds in response to simple user queries."""	algorithm;database;experiment;information extraction;natural language processing;query language;scalability;space–time tradeoff;speedup;web page;web search engine;world wide web	Michael J. Cafarella;Oren Etzioni	2005		10.1145/1060745.1060811	variables;natural language processing;noun;search engine indexing;web search engine;computer science;machine learning;space time;database;language;natural language;world wide web;information extraction;information retrieval;search engine	Web+IR	-32.68090395456395	-57.48138092818779	197046
f04867ef0b89b6321a92b0e757ffd00d36501cce	an advanced partitioning approach of web page clustering utilizing content & link structure	hits algorithm;k-mediod;clustering	Clustering of non-homogenous documents has become an increasing challenge and opportunity with the huge proliferation of World Wide Web. It has become difficult to retrieve the desired information without proper clustering of Web-page with the increase in information on the WWW. Several new ideas have been proposed in recent years. Among them partitioning approach is still widely used clustering approach for its simplicity. This paper proposes a partitioning approach to cluster the Web-page based on information provided by the hyperlink structure of Web-pages and also by the content of the Web-pages. The proposed approach of Web-page clustering exhibits better result than K-medoid partitioning clustering approach as the centroids are chosen by HITS Algorithm. The partitioning approach like Kmediod, K-means require number of clusters apriori. It has been observed that the performance of these approaches depend on the initial selection centroids of the clusters. These two problems have been solved by the approach proposed in this paper. Experimental result supports our approach as better concept.	apriori algorithm;cluster analysis;equivalence partitioning;hyperlink;k-means clustering;k-medoids;medoid;regular expression;www;web page;world wide web	Ruma Dutta;Indranil Ghosh;Anirban Kundu;Debajyoti Mukhopadhyay	2009	JCIT		correlation clustering;constrained clustering;fuzzy clustering;computer science;machine learning;web page;data mining;database;cluster analysis;world wide web;information retrieval;k-means clustering	AI	-28.28533494059387	-56.913144755183495	197238
dc9699e2073bb1f3b81d3b798f52a4c317227d3a	an effective semantic search technique using ontology	model category;ranking;semantic web;semantic relationship;semantic search;information system;ontology	In this paper, we present a semantic search technique considering the type of desired Web resources and the semantic relationships between the resources and the query keywords in the ontology. In order to effectively retrieve the most relevant top-k resources, we propose a novel ranking model. To do this, we devise a measure to determine the weight of the semantic relationship. In addition, we consider the number of meaningful semantic relationships between a resource and keywords, the coverage of keywords, and the distinguishability of keywords. Through experiments using real datasets, we observe that our ranking model provides more accurate semantic search results compared to existing ranking models.	experiment;semantic search;web resource	Jihyun Lee;Jun-Ki Min;Chin-Wan Chung	2009		10.1145/1526709.1526854	upper ontology;semantic similarity;semantic computing;semantic integration;semantic search;semantic grid;ranking;computer science;model category;semantic web;ontology;social semantic web;data mining;semantic web stack;semantic compression;database;semantic equivalence;semantic technology;probabilistic latent semantic analysis;world wide web;owl-s;information retrieval;information system;semantic analytics	Web+IR	-29.3098457022156	-58.854507520312644	197277
0173aadcf82762abbb7d023d003268ea00c803e8	a multimedia content management and retrieval system based on metadata and ontologies	content management;multimedia content management;domain ontology multimedia content management metadata information retrieval query processing semantic ranking broadcast tv programs multimedia content retrieval;multimedia content retrieval;query processing content management meta data multimedia computing ontologies artificial intelligence;metadata;query processing;information retrieval;broadcast tv programs;ontologies artificial intelligence;multimedia computing;meta data;multimedia systems content management content based retrieval ontologies information retrieval query processing prototypes multimedia communication tv broadcasting digital multimedia broadcasting;system architecture;domain ontology;semantic ranking	This paper presents a system architecture and information retrieval strategy for multimedia content which exploits descriptive metadata as well as domain ontology. We propose a query processing model including a semantic ranking scheme which can retrieve multimedia objects semantically relevant to the user query and provide users with a search result categorized by concepts and ordered by their semantic relevance to the query. By experiments on a prototype system using a repository of broadcast TV programs, we show effectiveness of the ontology-based multimedia search approach proposed in the paper.	categorization;database;experiment;information retrieval;ontology (information science);prototype;relevance;systems architecture	Hangkyu Kim;Chang-Sup Park;Ji-Youn Park;Byunghee Jung;Yoon-Joon Lee	2007	2007 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2007.4284710	query expansion;ranking;computer science;concept search;database;web search query;metadata;world wide web;information retrieval;systems architecture	DB	-28.5775060190222	-52.44214768264686	197286
d26bf65c5314862bb66467216af469035f4d8e1b	mining web content outliers using structure oriented weighting techniques and n-grams	web documents;web pages;n grams;dissimilarity measure;information retrieval;data capture;hot spot;web contents;information management;web mining;topic identification;optimal algorithm;text categorization	Classifying text into predefined categories is a fundamental task in information retrieval (IR). IR and web mining techniques have been applied to categorize web pages to enable users to manage and use the huge amount of information available on the web. Thus, developing user-friendly and automated tools for managing web information has been on a higher demand in web mining and information retrieval communities. Text categorization, information routing, identification of junk materials, topic identification and structured search are some of the hot spots in web information management. A great deal of techniques exists for classifying web documents into categories. Interestingly, almost none of the existing algorithms consider documents having 'varying contents' from the rest of the documents taken from the same domain (category) called web content outliers. In this paper, we take advantage of the HTML structure of web and n-gram technique for partial matching of strings and propose an n-gram-based algorithm for mining web content outliers. To reduce the processing time, the optimized algorithm uses only data captured in <Meta> and <Title> tags. Experimental results using planted motifs indicate the proposed n-gram-based algorithm is capable of finding web content outliers. In addition, using texts captured in <Meta> and <Title> tags gave the same results as using text embedded in <Meta>, <Title>, and <Body> tags.	algorithm;categorization;category theory;document classification;embedded system;grams;html element;information management;information retrieval;n-gram;routing;sequence motif;usability;web content;web mining;web page	Malik Agyemang;Ken Barker;Reda Alhajj	2005		10.1145/1066677.1066788	n-gram;web mining;web modeling;data web;web analytics;web mapping;computer science;social semantic web;web page;data mining;semantic web stack;database;automatic identification and data capture;information management;web intelligence;world wide web;hot spot;information retrieval	Web+IR	-28.796931619955334	-56.87702470658835	197604
7aead20d736c22f37440694fae88f49e2b3a09b1	accessing accurate documents by mining auxiliary document information	document clustering;text mining	Earlier techniques of text mining included algorithms like k-means, Naïve Bayes, SVM which classify and cluster the text document for mining relevant information about the documents. The need for improving the mining techniques has us searching for techniques using the available algorithms. This paper proposes one technique which uses the auxiliary information that is present inside the text documents to improve the mining. This auxiliary information can be a description to the content. This information can be either useful or completely useless for mining. The user should assess the worth of the auxiliary information before considering this technique for text mining. In this paper, a combination of classical clustering algorithms is used to mine the datasets. The algorithm runs in two stages which carry out mining at different levels of abstraction. The clustered documents would then be classified based on the necessary groups. The proposed technique is aimed at improved results of document clustering.	algorithm;blog;cluster analysis;k-means clustering;naive bayes classifier;principle of abstraction;support vector machine;text mining	Jinju Joby;Jyothi Korra	2015	2015 Second International Conference on Advances in Computing and Communication Engineering		concept mining;text mining;document clustering;computer science;pattern recognition;data mining;cluster analysis;tf–idf;world wide web;information retrieval	ML	-27.414183667839097	-57.94796255992424	197656
52011565e6a80b0226d96ece9e396c903e8ef640	on the equilibrium of query reformulation and document retrieval		In this paper, we study jointly query reformulation and document relevance estimation, the two essential aspects of information retrieval (IR). Their interactions are modelled as a two-player strategic game: one player, a query formulator, taking actions to produce the optimal query, is expected to maximize its own utility with respect to the relevance estimation of documents produced by the other player, a retrieval modeler; simultaneously, the retrieval modeler, taking actions to produce the document relevance scores, needs to optimize its likelihood from the training data with respect to the refined query produced by the query formulator. Their equilibrium or equilibria will be reached when both are the best responses to each other. We derive our equilibrium theory of IR using normal-form representations: when a standard relevance feedback algorithm is coupled with a retrieval model, they would share the same objective function and thus form a partnership game; by contrast, pseudo relevance feedback pursues a rather different objective than that of retrieval models, therefore the interaction between them would lead to a general-sum game (though implicitly collaborative). Our game-theoretical analyses not only yield useful insights into the two major aspects of IR, but also offer new practical algorithms for achieving the equilibrium state of retrieval which have been shown to bring consistent performance improvements in both text retrieval and item recommendation.	algorithm;baseline (configuration management);collaborative filtering;document retrieval;information retrieval;interaction;loss function;machine learning;mathematical optimization;nash equilibrium;online advertising;optimization problem;rate of convergence;reinforcement learning;relevance feedback;social network;utility	Shihao Zou;Guanyu Tao;Weinan Zhang;Dell Zhang	2018		10.1145/3234944.3234962	game theory;relevance feedback;thermodynamic equilibrium;computer science;information retrieval;general equilibrium theory;training set;document retrieval;partnership game	Web+IR	-27.14966569935784	-53.16053177996054	197684
4b8c814216ed02ca2dfe9981493e1c4c311a839c	mining web multi-resolution community-based popularity for information retrieval	information systems;web community;negative matrix factorisation;information re trieval;information retrieval;symmetric non negative matrix factorisation;pagerank;multi resolution;conference proceeding	The PageRank algorithm is used in Web information retrieval to calculate a single list of popularity scores for each page in the Web. These popularity scores are used to rank query results when presented to the user. By using the structure of the entire Web to calculate one score per document, we are calculating a general popularity score, not particular to any community. Therefore, the PageRank scores are more suited to general queries. In this paper, we introduce a more general form of PageRank, using Web multi-resolution community-based popularity scores, where each document obtains a popularity score dependent on a given Web community. When a query is related to a specific community, we choose the associated set of popularity scores and order the query results accordingly. Using Web-community based popularity scores, we achieved an 11% increase in precision over PageRank.	algorithm;archive;information retrieval;monoid factorisation;pagerank;paging;relevance;web page;world wide web	Laurence Anthony F. Park;Kotagiri Ramamohanarao	2007		10.1145/1321440.1321517	computer science;data mining;world wide web;information retrieval;information system	Web+IR	-31.78418728785808	-54.82011838331172	197912
1ebbf8fb60f4618a91979766b419d7086683db3d	query-free news search	query-free search;web information retrieval	Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics.	algorithm;relevance;stemming;tf–idf	Monika Henzinger;Bay-Wei Chang;Brian Milch;Sergey Brin	2003	World Wide Web	10.1007/s11280-004-4870-6	computer science;data mining;tf–idf;world wide web;information retrieval	Web+IR	-27.848467309339426	-55.129508610797664	198157
b738e6208b7968378feefeed9b8e4690d9f8fdb6	an aiding system for internet surfings by associations: get one through chances	databases;information resources;search keywords;search engine;fatigue;serendipity;keywords;web pages;search engines;information retrieval;search methods;information retrieval internet search engines online front ends information resources;internet surfing by association;potential interest;data mining;search keywords internet surfing by association internet users search engines serendipity potential interest keywords;online front ends;visualization;internet users;internet;roads;internet search engines web pages visualization roads databases search methods information retrieval fatigue data mining	Most Internet users use Search, Engines to get information with holding some intentions. However, there are not chances along the stmight road to a goal; that is, chances exist aside from a right way. I n this paper, we suggest an aiding system for Internet surfings which employs a search engine, although most users who surf on the net may not use search engines. This system reminds users of a potential user interest by supplying keywords related to search keywords. As many of words come beyond the user's thought, a user can continue to discover some words which express a user's potential interest and continue to get information using those keywords. Associations of the keywords are occured as relationships between new supplied keywords and newly entered search keywords. Therefore, a user can take pleasure in his/her endless surfings and take many chances.	game engine;internet;speeded up robust features;web search engine	Wataru Sunayama;Masahiko Yachida	2000		10.1109/KES.2000.884169	engineering;multimedia;world wide web;information retrieval	ML	-31.900013612174522	-52.98538943733744	198323
0c6f52da47de1830e0a1129978ff8043d780e355	web service information mining and correlation calculation method study		With the development of Internet technology, more and more web services are published on the Internet. How to efficiently and accurately obtain the specific services for users become more important. Based on this, web service vertical search engine emerged. This vertical search engine can improve the service retrieval efficiency compared with the traditional search engine. However there are still several deficiencies: required services are difficult to be filtered from the limited information that the users can refer; Moreover, sort principle of the search results are not transparent to users, they cannot reorder the search results according to their needs. This paper aims to solve this problem. Through mining service information, multi-dimensional information can be referred; through correlation calculation, users can search personalized information according to their needs, which enhances the power of web services retrieve and improves user experience of the search engine system.	web service	Huahua Ning;Feng Chen;Pan Deng;Yao Zhao;Wei Yuan;Chaofan Bi;Biying Yan	2014		10.1007/978-3-319-16050-4_21	data mining;world wide web;information retrieval	HPC	-30.357507965566644	-54.16767906421831	199226
c4b4807f43aac57a6627352e3f266edd7c169a75	geosearcher: geospatial ranking of search engine results	search engine;search engines;information retrieval;evaluation methods;web sites;world wide web;article;geography	Abstract#R##N##R##N#Current search engines build indices based on keyword occurrence and frequency and use this information along with link and usage analysis to rank the results of Boolean query negotiation. This approach is fast, robust, and generic but produces the same ranked order of the results no matter the intent of the user. Some queries are related to physical locations and distances. Queries of this type are common including; finding activities, online browsing for shopping, finding schools, planning trips, and, perhaps, finding the closest Italian restaurant.#R##N##R##N##R##N##R##N#In this paper, we describe a prototype system that provides dynamic ranking of search engine results for queries with a geospatial dimension based on the URL of the host site. We evaluate this approach using both user queries and random web pages. This work makes a contribution to the retrieval experience on the web by providing an alternative ranking order for search engine results. This means that users with queries with a geospatial concern can more readily exploit the results of general search engine results.		Carolyn R. Watters;Ghada Amoudi	2002		10.1002/meet.1450390145	search-oriented architecture;search engine indexing;database search engine;organic search;ranking;metasearch engine;web search engine;semantic search;computer science;spamdexing;data mining;search analytics;web search query;queries per second;world wide web;information retrieval;search engine	DB	-32.8647882687423	-54.0563433021294	199688
f711e75155a04360dc8e0089a236bfd8d4b3488b	intelligent string search processor to accelerate text information retrieval	information retrieval	This paper describes an intelligent string search processor (ISSP) developed for faster text information retrieval and also considers its application system design concept. This ISSP has such string matching functions as anchor/non-anchor, strict/approximate, and fixed­ length/variable-length don't-care matching. These functions are indispensable for getting certain given information from a full text search. Each ISSP stores 64 or less variable-length keywords to be compared with a text inputted at 10 million characters per second. A new application system design concept allows this ISSP to accumulate an increased number of user	information retrieval	K. Takahashi;Hisao Yamada;H. Nagai;M. Hirata	1987		10.1007/978-1-4613-1679-4_22	computer science;theoretical computer science;world wide web;information retrieval	Theory	-32.67969205432826	-57.77355490577276	199880
