id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7b0094e32efe7f93f6c19d6e6b64ec675807d177	principles for annotating and reasoning with spatial information		In this paper we present the first phase of the ongoing SpaceBank project that attempts to create a linguistic resource for annotating and reasoning with spatial information from text. SpaceBank is the spatial counterpart of TimeBank, an electronic resource for temporal semantics and reasoning. The paper focuses on building an ontology of lexicalized spatial concepts. The textual occurrences of the concepts in this ontology will be annotated using the SpaceML language, briefly described here. SpaceBank is designed to be integrated with TimeBank, for a spatio-temporal model of the textual information.	geographic information system;ontology (information science)	Paul Morarescu	2006			data mining;natural language processing;ontology;artificial intelligence;semantics;computer science;spatial analysis	AI	-31.62461802893621	-70.6780194960293	130657
4590c09dd27b33817caeb306860ff51617ca2743	a multiple-domain ontology builder	domain ontology	The interpretation of a multiple-domain text corpus as a single ontology leads to misconceptions. This is because some concepts may be syntactically equal; though, they are semantically lopsided in different domains. Also, the occurrences of a domain concept in a large multipledomain corpus may not gauge correctly the concept significance. This paper tackles the mentioned problems and proposes a novel ontology builder to extract separate domain specific ontologies from such a corpus. The builder contribution is to sustain each domain specific concepts and relations to get precise answers for user questions. We extend a single ontology builder named Text2Onto to apply our thought. We fruitfully enhance it to answer, more precisely, questions on a subset of AQUAINT corpus.	information retrieval;kaon;ontology (information science);randomness extractor;text retrieval conference;text corpus	Sara Salem;Samir AbdelRahman	2010			natural language processing;upper ontology;computer science;ontology;data mining;ontology-based data integration;information retrieval;process ontology	NLP	-27.815097673876394	-70.90970252106372	131485
a4bbf5b543c93580512ef8b29b09de8a113dcf16	x.ent: r package for entities and relations extraction based on unsupervised learning and document structure		Relation extraction with accurate precision is still a challenge when processing full text databases. We propose an approach based on cooccurrence analysis in each document for which we used document organization to improve accuracy of relation extraction. This approach is implemented in a R package called x.ent. Another facet of extraction relies on use of extracted relation into a querying system for expert end-users. Two datasets had been used. One of them gets interest from specialists of epidemiology in plant health. For this dataset usage is dedicated to plant-disease exploration through agricultural information news. An open-data platform exploits exports from x.ent and is publicly available.	browsing;database;diagram;dictionary;exploit (computer security);finite-state machine;information system;information visualization;named entity;parallel coordinates;r language;relationship extraction;unsupervised learning;usability	Nicolas Turenne;Tien T. Phan	2015	CoRR		computer science;data science;machine learning;data mining;information retrieval	NLP	-31.784442080020664	-68.62972807240824	131524
cc34e90fbdb61bc3473a732059b682dfd8e23cfb	the multital nlp tool infrastructure		This paper gives an overview of the MultiTal project, which aims to create a research infrastructure that ensures long-term distribution of NLP tools descriptions. The goal is to make NLP tools more accessible and usable to end-users of different disciplines. The infrastructure is built on a meta-data scheme modelling and standardising multilingual NLP tools documentation. The model is conceptualised using an OWL ontology. The formal representation of the ontology allows us to automatically generate organised and structured documentation in different languages for each represented tool.	as-easy-as;data model;documentation;natural language processing;pipeline (computing);tagged architecture;text corpus;web ontology language	Driss Sadoun;Satenik Mkhitaryan;Damien Nouvel;Mathieu Valette	2016			natural language processing;computer science;systems engineering;data mining	NLP	-33.18999277309801	-72.0891947400674	131797
5cb8fffb528cf6a0837d21937a4f06d630189f3a	temporality in relation with discourse structure		Temporal relations between events and times are often difficult to discover, time-consuming and expensive. In this paper a corpus study is performed to derive a strong relation between discourse structure, as revealed by Veins theory, and the temporal links between entities, as addressed in the TimeML annotation standard. The data interpretation helps us gain insight on how Veins theory can improve the manual and even (semi-) automatic detection of temporal relations.	entity;timeml	Corina Forascu;Ionut Pistol;Dan Cristea	2006			experimental data;natural language processing;rhetorical structure theory;artificial intelligence;treebank;timeml;notation;computer science;temporal annotation;expression (mathematics);annotation	NLP	-28.38187020311554	-72.76689759957465	131916
2f2f8f247507411e0b5b1eeec879d6ef04de33d7	recognizing confinement in web texts	web text;japanese-language web text;recognizing textual entailment;new semantic relation;web text;recognizing confinement;confinement relation;sentence pair;useful information;semantic template;partial contradiction sentence pair;semantic relation	In the Recognizing Textual Entailment (RTE) task, sentence pairs are classified into one of three semantic relations: ENTAILMENT, CONTRADICTION or UNKNOWN. While we find some sentence pairs hold full entailments or contradictions, there are a number of pairs that partially entail or contradict one another depending on a specific situation. These partial contradiction sentence pairs contain useful information for opinion mining and other such tasks, but it is difficult for Internet users to access this knowledge because current frameworks do not differentiate between full contradictions and partial contradictions. In this paper, under current approaches to semantic relation recognition, we define a new semantic relation known as CONFINEMENT in order to recognize this useful information. This information is classified as either CONTRADICTION or ENTAILMENT. We provide a series of semantic templates to recognize CONFINEMENT relations in Web texts, and then implement a system for recognizing CONFINEMENT between sentence pairs. We show that our proposed system can obtains a F-score of 61% for recognizing CONFINEMENT in Japanese-language Web texts, and it outperforms a baseline which does not use a manually compiled list of lexico-syntactic patterns to instantiate the semantic templates.	baseline (configuration management);compiler;error analysis (mathematics);lexico;ontology components;sentence boundary disambiguation;textual entailment;world wide web	Megumi Ohki;Eric Nichols;Suguru Matsuyoshi;Koji Murakami;Junta Mizuno;Shouko Masuda;Kentaro Inui;Yuji Matsumoto	2011			natural language processing;data mining;mathematics;communication	NLP	-26.89440612820465	-70.55732381313625	132032
469acf8b67bab10f959372669b74bee4053e14fe	experiments for tuning the values of lexical features in question answering for spanish	language technology;pattern recognition;natural language processing;question answering	This paper describes the prototype developed by the Language Technologies Laboratory at INAOE for Spanish monolingual QA evaluation task at CLEF 2005. Our approach is centered in the use of lexical features in order to identify possible answers to factual questions. Such method is supported by an alternative one based on pattern recognition in order to identify candidate answers to definition questions. The methods applied at different stages of the system and prototype architecture for question answering are described. The paper shows and discusses the results achieved with this approach.	experiment;language technology;pattern matching;pattern recognition;prototype;question answering;server name indication;software quality assurance	Manuel Alberto Pérez-Coutiño;Manuel Montes-y-Gómez;Aurelio López-López;Luis Villaseñor Pineda	2005			natural language processing;computer science;linguistics;information retrieval	NLP	-30.612465100157277	-71.98198020078394	132412
ddf89b6fac2451d93a7256c6b9551266d5c903e2	chinese word spelling correction based on n-gram ranked inverted index list		Spelling correction can assist individuals to input text data with machine using written language to obtain relevant information efficiently and effectively in. By referring to relevant applications such as web search, writing systems, recommend systems, document mining, typos checking before printing is very close to spelling correction. Individuals can input text, keyword, sentence how to interact with an intelligent system according to recommendations of spelling correction. This work presents a novel spelling error detection and correction method based on N-gram ranked inverted index is proposed to achieve this aim, spelling correction. According to the pronunciation and the shape similarity pattern, a dictionary is developed to help detect the possible spelling error detection. The inverted index is used to map the potential spelling error character to the possible corresponding characters either in character or word level. According to the N-gram score, the ranking in the list corresponding to possible character is illustrated. Herein, E-How net is used to be the knowledge representation of tradition Chinese words. The data sets provided by SigHan 7 bakeoff are used to evaluate the proposed method. Experimental results show the proposed methods can achieve accepted performance in subtask one, and outperform other approaches in subtask two.	algorithm;artificial intelligence;dictionary;error detection and correction;inverted index;knowledge representation and reasoning;n-gram;printing;sensor;spell checker;text corpus;web search engine	Jui-Feng Yeh;Sheng-Feng Li;Mei-Rong Wu;Wen-Yi Chen;Mao-Chuan Su	2013			natural language processing;speech recognition;computer science;pattern recognition	NLP	-29.104578336690764	-68.48344490398952	132467
9f88d70079773e0d07e073aaa1ec25813609228f	learning regular expressions for the extraction of product attributes from e-commerce microdata	entity linking;feature extraction;microdata	A large number of e-commerce websites have started to markup their products using standards such as Microdata, Microformats, and RDFa. However, the markup is mostly not as fine-grained as desirable for applications and mostly consists of free text properties. This paper discusses the challenges that arise in the task of matching descriptions of electronic products from several thousand e-shops that offer Microdata markup. Specifically, our goal is to extract product attributes from product offers, by means of regular expressions, in order to build well structured product specifications. For this purpose we present a technique for learning regular expressions. We evaluate our attribute extraction approach using 1.9 million product offers from 9,240 e-shops which we extracted from the Common Crawl 2012, a large public Web corpus. Our results show that with our approach we are able to reach a similar matching quality as with manually defined regular expressions.	cooperative breeding;e-commerce;feature extraction;iteration;knowledge base;markup language;microdata (html);microformat;parsing;postal;rdfa;rank (j programming language);regular expression;semantic analysis (machine learning);semiconductor industry	Petar Petrovski;Volha Bryl;Christian Bizer	2014			microdata;computer science;data mining;database;world wide web	DB	-31.445856106253395	-68.57471181659255	132705
0dd9ce60b65246a8e762c1fa39a0151f19efc221	don't classify, translate: multi-level e-commerce product categorization via machine translation		E-commerce platforms categorize their products into a multi-level taxonomy tree with thousands of leaf categories. Conventional methods for product categorization are typically based on machine learning classification algorithms. These algorithms take product information as input (e.g., titles and descriptions) to classify a product into a leaf category. In this paper, we propose a new paradigm based on machine translation. In our approach, we translate a product’s natural language description into a sequence of tokens representing a root-to-leaf path in a product taxonomy. In our experiments on two large real-world datasets, we show that our approach achieves better predictive accuracy than a state-of-the-art classification system for product categorization. In addition, we demonstrate that our machine translation models can propose meaningful new paths between previously unconnected nodes in a taxonomy tree, thereby transforming the taxonomy into a directed acyclic graph (DAG). We discuss how the resultant taxonomy DAG promotes user-friendly navigation, and how it is more adaptable to new products.		Maggie Yundi Li;Stanley Kok;Liling Tan	2018	CoRR			NLP	-31.373726255146895	-68.73441884453725	133419
8582b05f9fcbd665b737fc5928d14f11279fdea3	embedding wikipedia title based on its wikipedia text and categories		Distributed word representation is widely used in many NLP tasks and knowledge-based resources also provide valuable information. Comparing to conventional knowledge bases, Wikipedia provides semi-structural data other than structural data. We argue that a Wikipedia titleu0027s categories can help complement the titleu0027s meaning besides Wikipedia text, so the categories should be utilized to improve the titleu0027s embedding. We propose two directions of using categories, cooperating with conventional context-based approaches, to generate embeddings of Wikipedia titles. We conduct extensively large scale experiments on the generated title embeddings on Chinese Wikipedia. Experiments on word similarity task and analogical reasoning task show that our approaches significantly outperform conventional context-based approaches.	category theory;experiment;knowledge base;knowledge-based systems;natural language processing;semiconductor industry;wikipedia	Chi-Yen Chen;Wei-Yun Ma	2017	2017 International Conference on Asian Language Processing (IALP)	10.1109/IALP.2017.8300566	word embedding;natural language processing;artificial intelligence;knowledge base;embedding;computer science	NLP	-27.886792284224594	-67.64033055002311	133434
1619888b6f68bcda0104f8a8dbe630ec1e2e0a44	ontology population from unstructured and semi-structured texts	information technology;logic;information search;data mining;ontology population;learning systems;semantic information;machine learning;semantic web;world wide web;ontologies;humans;ontology construction;high performance;ontologies semantic web data mining humans logic machine learning information technology costs world wide web learning systems	Legacy information search systems have limitation that it does not consider semantic information but just lexical information such as keywords. A semantic web is expected to solve such limitation of present systems. In constructing semantic web, an ontology is believed to be a must. However, the ontology construction is very difficult. It requires great human efforts, since the creation of individuals is a time consuming task. Thus, there is a potential need for automatic or semiautomatic ontology population system, which greatly alleviates the human efforts. This paper proposes a method for an ontology population, in which the population is processed by computing the overlap between instances and concepts. This method is very simple but shows high performance.	semantic web;semiconductor industry	Hee-Geun Yoon;Yong-Jin Han;Seong-Bae Park;Se-Young Park	2007	Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007)	10.1109/ALPIT.2007.30	upper ontology;ontology alignment;semantic computing;bibliographic ontology;semantic search;ontology inference layer;web standards;computer science;ontology;data science;semantic web;social semantic web;data mining;semantic web stack;web intelligence;ontology-based data integration;owl-s;information retrieval;process ontology;semantic analytics;suggested upper merged ontology	AI	-31.72372016377088	-67.34482069286018	133583
c207693372eaba47e52a53b60c3e271a31dc3ca5	evaluating a topic modelling approach to measuring corpus similarity		Web corpora are often constructed automatically, and their contents are therefore often not well understood. One technique for assessing the composition of such a web corpus is to empirically measure its similarity to a reference corpus whose composition is known. In this paper we evaluate a number of measures of corpus similarity, including a method based on topic modelling which has not been previously evaluated for this task. To evaluate these methods we use known-similarity corpora that have been previously used for this purpose, as well as a number of newly-constructed known-similarity corpora targeting differences in genre, topic, time, and region. Our findings indicate that, overall, the topic modelling approach did not improve on a chi-square method that had previously been found to work well for measuring corpus similarity.	off topic;semantic similarity;text corpus;topic model	Richard Fothergill;Paul Cook;Timothy Baldwin	2016			natural language processing;artificial intelligence;topic model;computer science	NLP	-26.42987172959227	-66.93822201104038	133610
181b15539cbafff1af3d2ea9ddad1eeba8d21fdb	unsupervised relation extraction for automatic generation of multiple-choice questions		In this paper, we investigate an unsupervised approach to Relation Extraction to be applied in the context of automatic generation of multiple-choice questions (MCQs). The approach aims to identify the most important semantic relations in a document without assigning explicit labels to them in order to ensure broad coverage, unrestricted to predefined types of relations. The paper examines three different surface pattern types, each implementing different assumptions about linguistic expression of semantic relations between named entities. Our main findings indicate that the approach is capable of achieving high precision rates and its enhancement with linguistic knowledge helps to produce significantly better patterns. The intended application for the method is an e-learning system for automatic assessment of students’ comprehension of training texts; however it can also be applied to other NLP scenarios, where it is necessary to recognise important semantic relations without any prior knowledge as to their types.	named entity;natural language processing;relationship extraction;unsupervised learning	Naveed Afzal;Viktor Pekar	2009			natural language processing;computer science;machine learning;data mining	NLP	-26.973465591791932	-70.2581627224438	134176
445714f99c4c09a2ea10477fd71a074ea0682a63	towards semi-automatic methods for improving wordnet	semi-automatic method;lower level;towards semi-automatic method;major lexical resource;semantic constraint;ontological principle;previous effort	WordNet is extensively used as a major lexical resource in NLP. However, its quality is far from perfect, and this alters the results of applications using it. We propose here to complement previous efforts for “cleaning up” the top-level of its taxonomy with semi-automatic methods based on the detection of errors at the lower levels. The methods we propose test the coherence of two sources of knowledge, exploiting ontological principles and semantic constraints.	natural language processing;plasma cleaning;semiconductor industry;wordnet	Nervo Verdezoto;Laure Vieu	2011			natural language processing;computer science;data mining;information retrieval	NLP	-29.107612922777353	-71.09728511463754	134480
5114374b16f50b0c0dfaef3e649c38f2e3304751	discovering and understanding word level user intent in web search queries	intent words;query understanding;co occurrence entropy;query intent	Identifying and interpreting user intent are fundamental to semantic search. In this paper, we investigate the association of intent with individual words of a search query. We propose that words in queries can be classified as either content or intent, where content words represent the central topic of the query, while users add intent words to make their requirements more explicit. We argue that intelligent processing of intent words can be vital to improving result quality, and in this work we focus on intent word discovery and understanding. Our approach towards intent word detection is motivated by the hypotheses that query intent words satisfy certain distributional properties in large query logs similar to function words in natural language corpora. Following this idea, we first prove the effectiveness of our corpus distributional features, namely, word co-occurrence counts and entropies, towards function word detection for five natural languages. Next, we show that reliable detection of intent words in queries is possible using these same features computed from query logs. To make the distinction between content and intent words more tangible, we additionally provide operational definitions of content and intent words as those words that should match, and those that need not match, respectively, in the text of relevant documents. In addition to a standard evaluation against human annotations, we also provide an alternative validation of our ideas using clickthrough data. Concordance of the two orthogonal evaluation approaches provide further support to our original hypothesis of the existence of two distinct word classes in search queries. Finally, we provide a taxonomy of intent words derived through rigorous manual analysis of large query logs.	concordance (publishing);corpus linguistics;entropy (information theory);natural language;operational definition;requirement;semantic search;taxonomy (general);text corpus;web search engine;web search query	Rishiraj Saha Roy;Rahul Katare;Niloy Ganguly;Srivatsan Laxman;Monojit Choudhury	2015	J. Web Sem.	10.1016/j.websem.2014.07.010	natural language processing;semantic search;computer science;data mining;database;information retrieval;stop words	Web+IR	-28.278837485643606	-67.94089695624386	134663
7646207b652ba9d0dc8afae92b1a882148108e28	the statistical analysis of morphosyntactic distributions		This paper describes methods for the statistical analysis of quantitative data on the distribution of morphosyntactic features. A key problem is the large amount of ambiguity in automatically extracted data. In the paper, I argue for a conservative approach that treats ambiguous instances as counter-evidence. It is nonetheless possible to obtain detailed morphosyntactic information from the corpus data with the help of partial disambiguation and by exploiting systematic ambiguity classes.	word-sense disambiguation	Stefan Evert	2004			artificial intelligence;natural language processing;computer science;ambiguity	NLP	-28.130336499349326	-72.90470379438915	136481
2ad4f0b5705a76925e8cc0a8fd567e813bf9d583	measuring the compositionality of nv expressions in basque by means of distributional similarity techniques		We present several experiments aiming at measuring the semantic compositionality of NV expressions in Basque. Our approach is based on the hypothesis that compositionality can be related to distributional similarity. The contexts of each NV expression are compared with the contexts of its corresponding components, by means of different techniques, as similarity measures usually used with the Vector Space Model (VSM), Latent Semantic Analysis (LSA) and some measures implemented in the Lemur Toolkit, as Indri index, tf-idf, Okapi index and Kullback-Leibler divergence. Using our previous work with cooccurrence techniques as a baseline, the results point to improvements using the Indri index or Kullback-Leibler divergence, and a slight further improvement when used in combination with cooccurrence measures such as t-score, via rank-aggregation. This work is part of a project for MWE extraction and characterization using different techniques aiming at measuring the properties related to idiomaticity, as institutionalization, non-compositionality and lexico-syntactic fixedness.	baseline (configuration management);experiment;kullback–leibler divergence;latent semantic analysis;lexico;minimal working example;nv network;regular expression;tf–idf;viable system model	Antton Gurrutxaga;Iñaki Alegria	2012			natural language processing;principle of compositionality;divergence;vector space model;artificial intelligence;expression (mathematics);computer science;pattern recognition;latent semantic analysis	NLP	-26.65657379463921	-71.49717609469872	136608
8ab32e6a67d49649385502fd69c29f892b564acb	speeding up multilingual grammar development by exploiting linked data to generate pre-terminal rules		The development of grammars, e.g. for spoken dialog systems, is a timeand effort-intensive process. Especially the crafting of rules that list all relevant instances of a non-terminal, e.g. Greek cities or Automobile companies, possibly in multiple languages, is costly. In order to automatize and speed up the generation of multilingual terminal lists, we present a tool that uses linked data sources such as DBpedia in order to retrieve all entities that satisfy a relevant semantic restriction. We briefly describe the architecture of the system and explain how it can be used by means of an online web service.	dbpedia;dialog system;entity;linked data;spoken dialog systems;web service	Sebastian Walter;Christina Unger;Philipp Cimiano	2014		10.1007/978-3-319-07983-7_34	natural language processing;computer science;machine learning;data mining;linguistics	NLP	-30.775624361637334	-70.82739376448045	137288
3168bcb678bf06bead0b0e27fb7185e95dbe4576	automatic generation method of relation matrix for ism	ism;automatic generation;visualisation;full text search;problem structuring;co occurrence probability	This article proposes an automatic generation method of a relation matrix for Interpretive Structural Modelling (ISM). In ISM, keywords related to the problem are selected. Then the relation between two arbitrary keyword pairs is examined. When there is a relation between these two keywords, a non-zero value such as 1 is assigned to this relation. After examining the relations of all pairs of keywords, an n×n relation matrix is generated where n stands for the number of keywords. Then a reachable matrix is calculated from the relation matrix. By using the reachable matrix, the hierarchy structure of the problem is constructed. Building a relation matrix requires much time and labour. This article proposes an automatic generation method of relation matrix based on the co-occurrence probability of keywords in primary materials. As the proposed method is fully automatic, less labour is necessary than in the original ISM method. This method is evaluated by using newspaper articles.	algorithm;connection-oriented ethernet;diagram;implicit shape model;information system;possible world;sfiaplus;synthetic intelligence	Nobuaki Kanazawa;Toshiaki Sasahira;Shigeo Kaneda;Hirohide Haga	2006			full text search;computer science;artificial intelligence;theoretical computer science;algorithm	NLP	-29.238328890160016	-69.34082694873867	137638
8c252489e2691b7bd5c2bc96bc08f7ff5471bec7	pagerank and generic entity summarization for rdf knowledge bases		Ranking and entity summarization are operations that are tightly connected and recurrent in many different domains. Possible application fields include information retrieval, question answering, named entity disambiguation, co-reference resolution, and natural language generation. Still, the use of these techniques is limited because there are few accessible resources. PageRank computations are resource-intensive and entity summarization is a complex research field in itself. We present two generic and highly re-usable resources for RDF knowledge bases: a component for PageRank-based ranking and a component for entity summarization. The two components, namely PageRankRDF and summaServer, are provided in form of open source code along with example datasets and deployments. In addition, this work outlines the application of the components for PageRank-based RDF ranking and entity summarization in the question answering project WDAqua.	automatic summarization;computation;entity linking;information retrieval;natural language generation;open-source software;pagerank;question answering;word-sense disambiguation	Dennis Diefenbach;Andreas Thalhammer	2018		10.1007/978-3-319-93417-4_10	information retrieval;rdf;automatic summarization;data mining;natural language generation;linked data;entity linking;pagerank;computer science;question answering;ranking	NLP	-32.67324094867477	-67.14175585487779	137679
1787cd0323b3cb04ee0c6afc735afb4fff431757	bringing structure into summaries: crowdsourcing a benchmark corpus of concept maps		Concept maps can be used to concisely represent important information and bring structure into large document collections. Therefore, we study a variant of multidocument summarization that produces summaries in the form of concept maps. However, suitable evaluation datasets for this task are currently missing. To close this gap, we present a newly created corpus of concept maps that summarize heterogeneous collections of web documents on educational topics. It was created using a novel crowdsourcing approach that allows us to efficiently determine important elements in large document collections. We release the corpus along with a baseline system and proposed evaluation protocol to enable further research on this variant of summarization.	automatic summarization;baseline (configuration management);benchmark (computing);concept map;crowdsourcing;docs (software);text corpus;web page	Tobias Falke;Iryna Gurevych	2017			natural language processing;computer science;data science;data mining;information retrieval	NLP	-31.352096063765018	-67.02651086399035	137745
7dec047ebf2f8245ed89962de8e227b5aa1a1266	consistency among domain analysts in selecting domain documents and creating vocabularies	articulo;consistency among domain analysts in selecting domain documents and creating vocabularies	A study is reported on the consistency of the domain vocabularies created and the source documents selected by domain analysts for domain analysis using DARE (Domain Analysis and Reuse Environment). Consistency was analyzed by measuring the pairwise overlap scores between the domain analysts. The overlap scores of the vocabularies and the source documents were both found to be significantly greater than zero. The effect sizes were large. A positive correlation was also observed between overlap scores of the vocabularies and overlap scores of the source documents. The variability of domain vocabularies created automatically was compared to the variability of domain vocabularies produced manually by domain engineers. The variability of automatic and manual vocabularies was found to be significantly different. The difference was of medium effect size.	algorithm;coefficient;domain analysis;experiment;heart rate variability;spatial variability;subject-matter expert;vocabulary	Chaitanya Nemmallapudi;William B. Frakes;Reghu Anguswamy	2013		10.1007/978-3-642-38977-1_15	computer science;pattern recognition;data mining;information retrieval	NLP	-27.90488922265366	-67.92317858821808	138288
9d1f4778bff9440c0597d866a87cfaa664e8278e	renewing and revising semlink		This research describes SemLink, a comprehensive resource for Natural Language Processing that maps and unifies several highquality lexical resources: PropBank, VerbNet, FrameNet, and the recently added OntoNotes sense groupings. Each of these resources was created for slightly different purposes, and therefore each carries unique strengths and limitations. SemLink allows users to leverage the strengths of each resource and provides the groundwork for incorporating these lexical resources effectively into linked data resources. SemLink and the resources included therein are discussed with a focus on the value of using lexical resources in a complementary fashion. Recent improvements to SemLink, including the addition of a new resource, the OntoNotes sense groupings, are described. Work to address future goals, including further expansion of SemLink, is also discussed.	framenet;lexicon;linked data;map;natural language processing;propbank;semantic web;unification (computer science);verbnet	Claire Bonial;Kevin Stowe;Martha Palmer	2013			verbnet;linked data;leverage (finance);propbank;natural language processing;framenet;artificial intelligence;computer science	NLP	-31.912136044154913	-70.91325385517227	138427
4d61ff4b81965d54ec1b9d488e80d0e8bf1c67e8	the hocr microformat for ocr workflow and results	search engines;ground truth	Large scale scanning and document conversion efforts have led to a renewed interest in OCR systems and workflows. This paper describes a new format for representing both intermediate and final OCR results, developed in response to the needs of a newly developed OCR system and ground truth data release. The format embeds OCR information invisibly inside the HTML and CSS standards and therefore can represent a wide range of linguistic and typographic phenomena with already well-defined, widely understood markup and can be processed using widely available and known tools. The format is based on a new, multi-level abstraction of OCR results based on logical markup, common typesetting models, and OCR engine-specific markup, making it suitable both for the support of existing workflows and the development of future model-based OCR engines.	cascading style sheets;comparison of optical character recognition software;ground truth;hocr;html;markup language;microformat;open-source software	Thomas M. Breuel	2007	Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)	10.1109/ICDAR.2007.249	speech recognition;ground truth;computer science;database;world wide web;information retrieval	DB	-32.63565155695869	-68.42219239507845	138437
b85acaae9fe8bbfe4a99ad8a94ed001e62b1d2c6	building a domain knowledge base from wikipedia: a semi-supervised approach		Knowledge bases are becoming indispensable to software engineering and knowledge engineering. However, the existing domain knowledge bases are always artificially constructed and small-scale. In this paper, we propose a semi-supervised approach to domain concepts detection and software engineering knowledge base construction from Wikipedia. First, the approach selects domain relevant tags from Stackoverflow. Then, it matches Wikipedia entities and expands the concept set through an improved label propagation algorithm. A rule-based method is designed to discover semantic relations including relate, subclassOf and equal by analyzing structural information of Wikipedia. A relation derivation mechanism is presented to optimize the relation set. We finally construct SEBase, a domainspecific knowledge base of software engineering. Experimental results show the high accuracy of the integrated concepts and relations. Compared with other knowledge bases, SEBase has the widest coverage of concepts and relations in software engineering.	deep learning;display resolution;entity;experiment;keyword extraction;knowledge base;knowledge engineering;label propagation algorithm;logic programming;semi-supervised learning;semiconductor industry;software engineering;software propagation;stack overflow;statistical classification;topic model;wikipedia	Kai Chen;Xiang Dong;Jiangang Zhu;Beijun Shen	2016		10.18293/SEKE2016-051	domain analysis;domain;computer science;knowledge management;artificial intelligence;data science;body of knowledge;feature-oriented domain analysis;domain engineering;data mining;domain knowledge	SE	-31.23638427169647	-67.157012788831	138562
b1dcbecac691e5a69624c5ba543e292d587aee7d	measuring semantic coverage	semantic coverage;prominent nlp system;corpus coverage;semantic representation;computational semantics;computational treatment;general purpose nlp system;knowledge-base size;natural language processing system;nlp system;natural language processing;knowledge base	"""The developlnent of natural language processing systems is currently driven to a large extent by measures of knowledgebase size and coverage of individual phenomena relative to a corpus. While these measures have led to significant advances for knowledge-lean applications, they do not adequately motivate progress in computat ional semantics leading to the development of large-scale, general purpose NLP systems. In this article, we argue tha t depth of semantic representat ion is essential for covering a broad range of phenomena in the computational t rea tment of language and propose (lepth as an impor tan t additional dimension for measuring the semantic coverage of NLP systems. We propose an operationalization of this measure and show how to characterize an NLP system along the dimensions of size, corpus coverage, and depth. The proposed framework is illustrated using sever~fl prominent NLP systems. We hope the preliminary proposals made in this article will lead to prolonged debates in the field and will continue to be refined. 1 Measures of Size versus Measures of Depth Evaluation of current and potential performance of' an NLP system or method is of crucial importance to researchers, developers and users. Current performance of systems is directly measured using a variety of tests and techniques. Often, as in the case of machine translation or information extraction, an entire """"industry"""" of evaluation gets developed (see, for example, ARPA MT Evaluation; MUC-4 ). Measuring the performance of an NLP method, approach or technique (and through it the promise of a system based on it) is more difficult, as judgments must be made about """"blame assigmnent"""" and the impact of improving a variety of system components on the overall future performance. One of the widely accepted measures of potential performance improvement is the feasibility of scaling up the static knowledge sources of an NLP system its grammars , lexicons, worht knowledge bases and other sets of language descriptions (the reasoning being that the larger the system's grammars and lexicons, the greater percentage of input they would be able to match and, therefore, the better the performance of the systeml) . As a result, a system would be considered very promising if its knowledge sources could be significantly scaled up at a reasonable expense. Natm'ally, the expense is lowest if acquisition is performed automatically. This consideration and the recent resurgence of corpus-based methods heighten the interest in the automat ion of knowledge acquisition, llowever, we believe that such acquisition should not 1)e judged solely by the utility of acquired knowledge for ~ particular application. A preliminary to the sealability estimates is a judgment of the current coverage of a system's static knowledge sources. Unfortunately, judgments based purely on size ace often misleading. While they may be sufficiently straightforward for less km)wledgeAntensive methods used in such applications as information extraction and retrieval, part of speech tagging, bilingual corpus alignment, and so on, the saute is not true about more ruleand knowledge-based methods (such as syntactic parsers, semantic analyzers, semantic lexicons, ontological world models, etc.). It ix widely accepted, for instance, that judgments of the coverage of a syntactic g rammar in terms of the number of rules are tlawed. It is somewhat less self-evident, however, that the number of lexicon entries or ontology concepts is not an adequate measure of the quality or coverage of NLP a Incidentally, this consideration eontributes to evMuation of current perforntance as well. In the absence of actual evaluation results, it is customary to c|aim the utility of the system by simply mentioning tit(: size of its knowledge sources (e.g., """"over 550 grammar rules, over 50,000 concepts in the ontology and over 100,00(I word senses in the dictionary"""")."""	automatic computing engine;compiler;dictionary;image scaling;independence day: resurgence;information extraction;knowledge acquisition;knowledge base;lexicon;machine translation;natural language processing;os-tan;parsing;part-of-speech tagging;resources, events, agents (accounting model);semantic data model	Sergei Nirenburg;Kavi Mahesh;Stephen Beale	1996			natural language processing;knowledge base;semantic computing;multinet;computer science;artificial intelligence;data mining;semantic compression;linguistics;computational semantics	NLP	-28.011106133649452	-71.82195238774544	138942
b5d7797d55437ec92688cf0772ded2868549dc62	impact of corpus size and dimensionality of lsa spaces from wikipedia articles on autotutor answer evaluation		Latent Semantic Analysis (LSA) plays an important role in analyzing text data from education settings. LSA represents meaning of words and sets of words by vectors from a k-dimensional space generated from a selected corpus. While the impact of the value of k has been investigated by many researchers, the impact of the selection of documents and the size of the corpus has never been systematically investigated. This paper tackles this problem based on the performance of LSA in evaluating learners’ answers to AutoTutor, a conversational intelligent tutoring system. We report the impact of document sources (Wikipedia vs TASA), selection algorithms (keyword based vs random), corpus size (from 2000 to 30000 documents) and number of dimensions (from 2 to 1000). Two AutoTutor tasks are used to evaluate the performance of different LSA spaces: a phrase level answer assessment (responses to focal prompt questions) and a sentence level answer assessment (responses to hints). We show that a sufficiently large (e.g., 20,000 to 30,000 documents) randomly selected Wikipedia corpus with high enough dimensions (about 300) could provide a reasonably good space. A specifically selected domain corpus could have significantly better performance with a relatively smaller corpus size (about 8000 documents) and much lower dimensionality (around 17). The widely used TASA corpus (37,651 documents scientifically sampled) performs equally well as a randomly selected large Wikipedia corpus (20,000 to 30,000) with a sufficiently high dimensionality (e.g., k>=300).	algorithm;focal (programming language);latent semantic analysis;randomness;spaces;text corpus;wikipedia	Zhiqiang Cai;Art Graesser;Leah Windsor;Qinyu Cheng;David W. Shaffer;Xiangen Hu	2018			artificial intelligence;machine learning;natural language processing;discourse analysis;computer science;computational linguistics;semantics;phrase structure rules;curse of dimensionality;collaborative writing	NLP	-27.991796404425983	-68.29694353497055	139837
16117a2c9fc2c778a448b5487f732887c94961d5	semantic interpretation of temporal information by abductive inference	semantic interpretation;temporal information	Besides temporal information explicitly available in verbs and adjuncts, the temporal interpretation of a text also depends on general world knowledge and default assumptions. We will present a theory for describing the relation between, on the one hand, verbs, their tenses and adjuncts and, on the other, the eventualities and periods of time they represent and their relative temporal locations. The theory is formulated in logic and is a practical implementation of the concepts described in Ness Schelkens et al. (this volume). We will show how an abductive resolution procedure can be used on this representation to extract temporal information from texts.	abductive reasoning;commonsense knowledge (artificial intelligence);earthbound;experiment;first-order logic;head-driven phrase structure grammar;knowledge representation and reasoning;natural language processing;semantic interpretation;semantics (computer science)	Sven Verdoolaege;Marc Denecker;Ness Schelkens;Danny De Schreye;Frank Van Eynde	1999	CoRR		natural language processing;semantic interpretation;computer science;artificial intelligence;machine learning	NLP	-31.09743050765852	-71.16653923345721	140183
1f8f15527f82ee2c23c1c3550d618a774e2d7f76	utilizing domain-specific keywords for discovering public sparql endpoints: a life-sciences use-case	web of data;linked open data lod cloud;conference paper;sparql;healthcare and life sciences	"""The LOD cloud comprises of billions of facts covering hundreds of datasets. In accordance with the Linked Data principles, these datasets are connected by a variety of typed links, forming an interlinked """"Web of Data"""". The growing diversity of the Web of Data makes it more and more challenging for publishers to find relevant datasets that could be linked to, particularly in specialist domain-specific settings. This paper thus proposes a baseline method to automatically identify a list of public SPARQL endpoints whose content is deemed relevant to a local dataset based on queries generated from a local set of domain-specific keywords."""	baseline (configuration management);data web;linked data;sparql;world wide web	Muntazir Mehdi;Aftab Iqbal;Ali Hasnain;Yasar Khan;Stefan Decker;Ratnesh Sahay	2014		10.1145/2554850.2555146	named graph;computer science;sparql;data mining;database;world wide web;information retrieval	Web+IR	-32.35937473537344	-66.88524115383764	140318
8a599292762edd1e3dea8444b6af9aebb6b6ccea	acquiring semantic relations using the web for constructing lightweight ontologies	hybrid approach;coarse grained;semantic relations	Common techniques for acquiring semantic relations rely on static domain and linguistic resources, predefined patterns, and the presence of syntactic cues. We propose a hybrid approach which brings together established and novel techniques in lexical simplification, word disambiguation and association inference for acquiring coarse-grained relations between potentially ambiguous and composite terms using only dynamic Web resources. Our experiments using terms from two different domains demonstrate potential preliminary results.	cpu cache;cluster analysis;dictionary;diversification (finance);endeavour software project management;experiment;level of detail;lexical simplification;lightweight ontology;ontology (information science);ontology learning;text simplification;the australian;web resource;web search engine;wikipedia;word-sense disambiguation;world wide web	Wilson Wong;Wei Liu;Mohammed Bennamoun	2009		10.1007/978-3-642-01307-2_26	natural language processing;computer science;data mining	NLP	-29.64134137487378	-67.23266490966999	140558
69de073523a16ee81a7951f46bc15f56864390a8	extracting causal knowledge from a medical database using graphical patterns	syntactic parsing;knowledge extraction;causal relation;knowledge discovery	This paper reports the first part of a project that aims to develop a knowledge extraction and knowledge discovery system that extracts causal knowledge from textual databases. In this initial study, we develop a method to identify and extract cause-effect information that is explicitly expressed in medical abstracts in the Medline database. A set of graphical patterns were constructed that indicate the presence of a causal relation in sentences, and which part of the sentence represents the cause and which part represents the effect. The patterns are matched with the syntactic parse trees of sentences, and the parts of the parse tree that match with the slots in the patterns are extracted as the cause or the effect.	causal filter;database;discovery system;graphical user interface;medline;parse tree;parsing	Christopher S. G. Khoo;Syin Chan;Yun Niu	2000		10.3115/1075218.1075261	natural language processing;computer science;data mining;knowledge extraction;information retrieval	NLP	-32.87520813937077	-69.5430155311445	141072
743f3041effeabfbae3e8e179bf993386f2876a9	n³ - a collection of datasets for named entity recognition and disambiguation in the nlp interchange format		Extracting Linked Data following the Semantic Web principle from unstructured sources has become a key challenge for scientific research. Named Entity Recognition and Disambiguation are two basic operations in this extraction process. One step towards the realization of the Semantic Web vision and the development of highly accurate tools is the availability of data for validating the quality of processes for Named Entity Recognition and Disambiguation as well as for algorithm tuning. This article presents three novel, manually curated and annotated corpora (N). All of them are based on a free license and stored in the NLP Interchange Format to leverage the Linked Data character of our datasets.	algorithm;free license;linked data;named-entity recognition;natural language processing;semantic web;text corpus;word-sense disambiguation	Michael Röder;Ricardo Usbeck;Sebastian Hellmann;Daniel Gerber;Andreas Both	2014				NLP	-32.64343270814954	-68.09909001467372	141695
1cecb886b2b27d02cbe32714bf1c57136e719d4a	ntcir-4 qac experiments at matsushita	answer extraction;named entity extraction;information retrieval;pattern matching;named entity;test collection;question answering	This paper investigates our experimental results for NTCIR-4 QAC2, the second attempt to evaluate the technology of Japanese question answering (QA). Our basic approach is a combination of information retrieval and named entity (NE) extraction based on pattern matching. The results show that the accuracy of NE extraction crucially affects the overall performance of our system. Additional experiments show the effects of refinements of answer extraction. We also analyze the QAC2 test collection to identify features relevant for measuring the difficulty of the questions in the collection. Based on the analysis, we make some proposals for the future QAC tasks, as regards to the measurement of difficulty of the test collection and definition of tasks.	comparison and contrast of classification schemes in linguistics and metadata;experiment;information retrieval;named entity;pattern matching;question answering;software quality assurance	Masako Nomoto;Yoshio Fukushige;Mitsuhiro Sato;Hiroyuki Suzuki	2002			computer science;data mining;database;information retrieval	NLP	-28.38433344349144	-67.36662912082362	142040
bec56e7acff527bc0f37babdcc5491ded27e239a	language independent query focused snippet generation	language independent query;english snippet;sentence scoring;query dependent snippet;subjective evaluation;sentence ranking;overall average evaluation score;snippet generation module;best evaluation score;sentence extraction	language independent query;english snippet;sentence scoring;query dependent snippet;subjective evaluation;sentence ranking;overall average evaluation score;snippet generation module;best evaluation score;sentence extraction		Pinaki Bhaskar;Sivaji Bandyopadhyay	2012		10.1007/978-3-642-33247-0_16	natural language processing;computer science;database;information retrieval	DB	-28.171118939328533	-68.09965571922105	142242
83bbb8e552914a1e7cdda006d5267724a3ea3553	integration of dependency analysis with semantic analysis referring to the context	conference paper	This paper describes how to perform syntactic parsing and semantic analysis using the contextual information as a language understanding component in a dialog system. Although syntactic parsing and semantic analysis are often conducted independently of each other, correct parsing of a sentence often requires the semantic information on the input and/or the contextual information prior to the input. We therefore merge syntactic parsing with semantic analysis, which enables syntactic parsing to take advantage of the semantic content of an input and its contextual information. To use contextual information, the semantic representation of an input should have a comparable form to the semantic content of the preceding context. Accordingly, we employ a framework for semantic representations that achieves such comparison. We take dialogs of hotel search and reservation for example, and demonstrate the effectiveness of the proposed method. The experimental results confirm that the proposed system achieves high accuracy in parsing and generation of semantic representations.	attribute–value pair;dependence analysis;dialog system;entity;natural language understanding;parsing;semantic analysis (compilers);stable model semantics	Yuki Ikegaya;Yasuhiro Noguchi;Satoru Kogure;Tatsuhiro Konishi;Makoto Kondo;Hideki Asoh;Akira Takagi;Yukihiro Itoh	2005			semantic computing;database;linguistics;information retrieval	NLP	-28.154883614756972	-69.86256712113386	142381
2ed164a624809c4dc339f973a7e12c8dc847da47	short text similarity with word embeddings	short text similarity;word embeddings	Determining semantic similarity between texts is important in many tasks in information retrieval such as search, query suggestion, automatic summarization and image finding. Many approaches have been suggested, based on lexical matching, handcrafted patterns, syntactic parse trees, external sources of structured semantic knowledge and distributional semantics. However, lexical features, like string matching, do not capture semantic similarity beyond a trivial level. Furthermore, handcrafted patterns and external sources of structured semantic knowledge cannot be assumed to be available in all circumstances and for all domains. Lastly, approaches depending on parse trees are restricted to syntactically well-formed texts, typically of one sentence in length.  We investigate whether determining short text similarity is possible using only semantic features---where by semantic we mean, pertaining to a representation of meaning---rather than relying on similarity in lexical or syntactic representations. We use word embeddings, vector representations of terms, computed from unlabelled data, that represent terms in a semantic space in which proximity of vectors can be interpreted as semantic similarity.  We propose to go from word-level to text-level semantics by combining insights from methods based on external sources of semantic knowledge with word embeddings. A novel feature of our approach is that an arbitrary number of word embedding sets can be incorporated. We derive multiple types of meta-features from the comparison of the word vectors for short text pairs, and from the vector means of their respective word embeddings. The features representing labelled short text pairs are used to train a supervised learning algorithm. We use the trained model at testing time to predict the semantic similarity of new, unlabelled pairs of short texts   We show on a publicly available evaluation set commonly used for the task of semantic similarity that our method outperforms baseline methods that work under the same conditions.	automatic summarization;baseline (configuration management);distributional semantics;information retrieval;parse tree;parsing;semantic similarity;string searching algorithm;supervised learning;well-formed element;word embedding	Tom Kenter;Maarten de Rijke	2015		10.1145/2806416.2806475	natural language processing;semantic similarity;explicit semantic analysis;semeval;computer science;pattern recognition;semantic property;information retrieval;dishin	NLP	-27.144893104774244	-70.55592984359949	142944
370cd18564387649f6b636f631a7320c7491acbc	comparison of collocation extraction measures for document indexing	indexation	Automatic extraction of collocations from a corpus is a well-known problem in the field of natural language processing. It is typically carried out by employing some kind of a statistical measure that indicates whether or not two words occur together more often than by chance. As there is an abundance of these measures proposed by various authors, we have compared some of them on a task of extracting collocations from a corpus of Croatian legal documents for the purpose of document indexing. We propose and evaluate extensions of these measures for collocations consisting of three words	coefficient of determination;collocation extraction;heuristic (computer science);ll parser;matthews correlation coefficient;natural language processing;randomness;sørensen–dice coefficient	Sasa Petrovic;Jan Snajder;Bojana Dalbelo Basic;Mladen Kolar	2006	28th International Conference on Information Technology Interfaces, 2006.		natural language processing;search engine indexing;speech recognition;computer science;computational linguistics;database;linguistics;natural language	NLP	-28.454302939766297	-66.97452845378098	143233
4b01c6873f0860c2f274dfe0713d54fa54f7efef	information extraction as an ontology population task and its application to genic interactions	genic interactions;biology computing;relational machine learning information extraction ontology population task genic interactions text extraction lexical layer natural language processing;information extraction;information retrieval;inductive logic programming;text analysis;text extraction;biology;bacillus subtilis;data mining;ontologies artificial intelligence;domain knowledge;inference rule;ontology population;relational machine learning;inductive logic programming information extraction ontology population ontology learning genic interactions;ontology learning;rna;proteins;machine learning;data mining ontologies machine learning artificial intelligence natural language processing pipelines thesauri knowledge representation scattering databases;biological information theory;ontologies;lexical layer;text analysis biology computing information retrieval learning artificial intelligence natural language processing ontologies artificial intelligence;learning artificial intelligence;polymers;natural language processing;ontology population task	Ontologies are a well-motivated formal representation to model knowledge needed to extract and encode data from text. Yet, their tight integration with Information Extraction (IE) systems is still a research issue, a fortiori with complex ones that go beyond hierarchies. In this paper, we introduce an original architecture where IE is specified by designing an ontology, and the extraction process is seen as an Ontology Population (OP) task. Concepts and relations of the ontology define a normalized text representation. As their abstraction level is irrelevant for text extraction, we introduced a Lexical Layer (LL) along with the ontology, i.e. relations and classes at an intermediate level of normalization between raw text and concepts. On the contrary to previous IE systems, the extraction process only involves normalizing the outputs of Natural Language Processing (NLP) modules with instances of the ontology and the LL. All the remaining reasoning is left to a query module, which uses the inference rules of the ontology to derive new instances by deduction. In this context, these inference rules subsume classical extraction rules or patterns by providing access to appropriate abstraction level and domain knowledge. To acquire those rules, we adopt an Ontology Learning (OL) perspective, and automatically acquire the inference rules with relational Machine Learning (ML). Our approach is validated on a genic interaction extraction task from a Bacillus subtilis bacterium text corpus. We reach a global recall of 89.3% and a precision of 89.6%, with high scores for the ten conceptual relations in the ontology.	abstraction layer;encode;information extraction;interaction;knowledge representation and reasoning;ll parser;machine learning;natural deduction;natural language processing;ontology (information science);ontology learning;relevance;text corpus;web ontology language	Alain-Pierre Manine;Érick Alphonse;Philippe Bessières	2008	2008 20th IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2008.117	natural language processing;upper ontology;rna;ontology inference layer;computer science;ontology;artificial intelligence;machine learning;data mining;ontology-based data integration;information extraction;information retrieval;process ontology;domain knowledge;rule of inference;suggested upper merged ontology	AI	-31.38025239442297	-70.07690517586396	143600
d80cc0e769070fbc70b99b4359abc36dba96adf1	extraction, selection and ranking of field association (fa) terms from domain-specific corpora for building a comprehensive fa terms dictionary	terminology extraction;information retrieval;or phrases;field association fa terms;term weighting;passage retrieval;part of speech;experimental evaluation;document classification;terms weighting and selection;domain specificity	Field Association (FA) Terms—words or phrases that serve to identify document fields are effective in document classification, similar file retrieval and passage retrieval. But the problem lies in the lack of an effective method to extract and select relevant FA Terms to build a comprehensive dictionary of FA Terms. This paper presents a new method to extract, select and rank FA Terms from domain-specific corpora using part-of-speech (POS) pattern rules, corpora comparison and modified tf-idf weighting. Experimental evaluation on 21 fields using 306 MB of domain-specific corpora obtained from English Wikipedia dumps selected up to 2,517 FA Terms (single and compound) per field at precision and recall of 74–97 and 65–98. This is better than the traditional methods. The FA Terms dictionary constructed using this method achieved an average accuracy of 97.6% in identifying the fields of 10,077 test documents collected from Wikipedia, Reuters RCV1 corpus and 20 Newsgroup data set.	dictionary;document classification;effective method;information retrieval;machine translation;megabyte;natural language processing;ontology engineering;part-of-speech tagging;precision and recall;text corpus;tf–idf;wikipedia	Tshering C. Dorji;El-Sayed Atlam;Susumu Yata;Masao Fuketa;Kazuhiro Morita;Jun-ichi Aoe	2010	Knowledge and Information Systems	10.1007/s10115-010-0296-x	natural language processing;speech recognition;part of speech;computer science;information retrieval	NLP	-27.858137604230233	-67.19809180001636	143654
0c1f33db302e727f7fbafa8ce3708f5e94a0cad6	"""building on redundancy: factoid question answering, robust retrieval and the """"other"""""""	noun;conference_paper;information technology;or phrases;question answering system;world wide web;computer science;question answering	We have explored how redundancy based techniques can be used in improving factoid question answering, definitional questions (“other”), and robust retrieval. For the factoids, we explored the meta approach: we submit the questions to the several open domain question answering systems available on the Web and applied our redundancy-based triangulation algorithm to analyze their outputs in order to identify the most promising answers. Our results support the added value of the meta approach: the performance of the combined system surpassed the underlying performances of its components. To answer definitional (“other”) questions, we were looking for the sentences containing re-occurring pairs of noun entities containing the elements of the target. For robust retrieval, we applied our redundancy based Internet mining technique to identify the concepts (single word terms or phrases) that were highly related to the topic (query) and expanded the queries with them. All our results are above the mean performance in the categories in which we have participated, with one of our robust runs being the best in its category among all 24 participants. Overall, our findings support the hypothesis that using as much as possible textual data, specifically such as mined from the World Wide Web, is extre mely promising. FACTOID QUESTION ANSWERING The Natural Language Processing (NLP) task, which is behind Question Answering (QA) technology, is known to be Artificial Intelligence (AI) complete: it requires the computers to be as intelligent as people, to understand the deep semantics of human communication, and to be capable of common sense reasoning. As a result, different systems have different capabilities. They vary in the range of tasks that they support, the types of questions they can handle, and the ways in which they present the answers. By following the example of meta search engines on the Web (Selberg & Etzioni, 1995), we advocate combining several fact seeking engines into a single “Meta” approach. Meta search engines (sometimes called metacrawlers) can take a query consisting of keywords (e.g. “Rotary engines”), send them to several portals (e.g. Google, MSN, etc.), and then combine the results. This allows them to provide better coverage and specialization. The examples are MetaCrawler (Selberg & Etzioni, 1995), 37.com (www.37.com), and Dogpile (www.dogpile.com). Although, the keyword based meta search engines have been suggested and explored in the past, we are not aware of the similar approach tried for the task of open domain/corpus question answering (fact seeking). The practical benefits of the meta approach are justified by general consideration: eliminating “weakest link” dependency. It does not rely on a single system which may fail or may simply not be designed for a specific type of tasks (questions). The meta approach promises higher coverage and recall of the correct answers since different QA engines may cover different databases or different parts of the Web. In addition, the meta approach can reduce subjectivity by querying several engines; like in the real-world, one can gather the views from several people in order to make the answers more accurate and objective. The speed provided by several systems queried in parallel can also significantly exceed those obtained by working with only one system, since their responsiveness may vary with the task and network traffic conditions. In addition, the meta approach fits nicely into a becoming-popular Web services model, where each service (QA engine) is independently developed and maintained and the meta engine integrates them together, while still being organizationally independent from them. Since each engine may be provided by a commercial company interested in increasing their advertising revenue or a research group showcasing their cutting edge technology, the competition mechanism will also ensure quality and diversity among the services. Finally, a meta engine can be customized for a particular portal such as those supporting business intelligence, education, serving visually impaired or mobile phone users. Figure 1. Example of START output. Figure 2. Example of Btainboost output. Meta Approach Defined We define a fact seeking meta engine as the system that can combine, analyze, and represent the answers that are obtained from several underlying systems (called answer services throughout our paper). At least some of these underlying services (systems ) have to be capable of providing candidate answers to some types of questions asked in a natural language form, otherwise the overall architecture would not be any different from a single fact seeking engine which are typically based on a commercial keyword search engines, e.g. Google. The technology behind each of the answer services can be as complex as deep semantic NLP or as simple as shallow pattern matching. Fact Seeking Service Web address Output Format Organization/System Performance in our evaluation (MRR) START start.csail.mit.edu Single answer sentence Research Prototype 0.049** AskJeeves www.ask.com Up to 200 ordered snippets Commercial 0.397** BrainBoost www.brainboost.com Up to 4 snippets Commercial 0.409* ASU QA on the Web qa.wpcarey.asu.edu Up to 20 ordered sentences Research Prototype 0.337** Wikipedia en.wikipedia.org Narrative Non profit 0.194** ASU Meta QA http://qa.wpcarey.asu.edu/ Precise answer Research Prototype 0.435 Table 1. The fact seeking services involved, their characteristics and performances in the evaluation on the 2004 questions. * and ** indicate 0.1 and .05 levels of statistical significance of the difference from the best accordingly. Challenges Faced and Addressed Combing multiple fact seeking engines also faces several challenges. First, the output formats of them may differ : some engines produce exact answer (e.g. START), some other present one sentence or an entire snippet (several sentences) simi lar to web search engines, as shown in Figures 1-4. Table 1 summarizes those differences and other capabilities for the popular fact seeking engines. Second, the accuracy of responses may differ overall and have even higher variability depending on a specific type of a question. And finally, we have to deal with multiple answers, thus removing duplicates, and resolving answer variations is necessary. The issues with merging search results from multiple engines have been already explored by MetaCrawler (Selberg & Etzioni, 1995) and fusion studies in information retrieval (e.g. Vogt & Cottrell, 1999) but only in the context or merging lists of retrieved text documents. We argue that the task of fusing multiple short answers, which may potentially conflict or confirm each other, is fundamentally different and poses a new challenge for the researchers. For example, some answer services (components) may be very precise (e.g. START), but cover only a small proportion of questions. They need to be backed up by less precise services that have higher coverage (e.g. AskJeeves). However, backing up may easily result in diluting the answer set by spurious (wrong) answers. Thus, there is a need for some kind of triangulation of the candidate answers provided by the different services or multiple candidate answers provided by the same service. Figure 3. Example of Ask Jeeves output. Figure 4. Example of ASU QA output. Triangulation, a term which is widely used in intelligence and journalism, stands for confirming or disconfirming facts, by using multiple sources. Roussinov et al. (2004) went one step further than using the frequency counts explored earlier by Dumais et al. (2002) and groups involved in TREC competitions. They explored a more fine-grained triangulation process which we also used in our prototype. Their algorithm can be demonstrated by the following intuitive example. Imagine that we have two candidate answers for the question “What was the purpose of the Manhattan Project?”: 1) “To develop a nuclear bomb” 2) “To create an atomic weapon”. These two answers support (triangulate) each other since they are semantically similar. However, a straightforward frequency count approach would not pick this similarity. The advantage of triangulation over simple frequency counting is that it is more powerful for less “factual” questions, such as those that may allow variations in the correct answers. In order to enjoy the full power of triangulation with factoid questions (e.g. Who is the CEO of IBM?), the candidate answers have to be extracted from their sentences (e.g. Samuel Palmisano), so they can be more accurately compared with the other candidate answers (e.g. Sam Palmisano). That is why the meta engine needs to possess answer understanding capabilities as well, including such crucial capability as question interpretation and semantic verification of the candidate answers to check that they belong to a desired category (person in the example above). Figure 5. The Meta approach to fact seeking. Fact Seeking Engine Meta Prototype: Underlying Technologies and Architecture In the first version of our prototype, we included several freely available demonstrational prototypes and popular commercial engines on the Web that have some QA (fact seeking) capabilities, specifically START, AskJeeves, BrainBoost and ASU QA (Table 1, Figures 1-4). We also added Wikipedia to the list. Although it does not have QA capabilities, it provides good quality factual information on a variety of topics, which adds power to our triangulation mechanism. Google was not used directly as a service but BrainBoost and ASU QA are already using it among the other major keyword search engines. The meta-search part of our system was based on the MetaSpider architecture (Chau et al., 2001; Chen et al., 2001). Multithreads are launched to submit the query to fetch the candidate answers from each service. After these results are obtained, the system performs answer extraction, triangulation and semantic verifi	artificial intelligence;backup;commonsense reasoning;computer;database;definition;document retrieval;entity;entity–relationship model;fits;information retrieval;language model;machine learning;mined;mobile phone;natural language processing;network traffic control;partial template specialization;pattern matching;performance;portals;prototype;question answering;ranking (information retrieval);responsiveness;robustness (computer science);search algorithm;software quality assurance;spatial variability;stable model semantics;text retrieval conference;text corpus;triangulation (geometry);triangulation (topology);web search engine;web service;wikipedia;word lists by frequency;world wide web	Dmitri Roussinov;Elena Filatova;Michael Chau;Jose Antonio Robles-Flores	2005			natural language processing;noun;question answering;computer science;world wide web;information retrieval	Web+IR	-28.32932502878232	-69.52600444622186	144190
3946979cb1ac2737c14208958e92f8cd2698d44d	interesting nuggets and their impact on definitional question answering	human interest;definitional question answering;question answering	"""Current approaches to identifying definitional sentences in the context of Question Answering mainly involve the use of linguistic or syntactic patterns to identify informative nuggets. This is insufficient as they do not address the novelty factor that a definitional nugget must also possess. This paper proposes to address the deficiency by building a """"Human Interest Model"""" from external knowledge. It is hoped that such a model will allow the computation of human interest in the sentence with respect to the topic. We compare and contrast our model with current definitional question answering models to show that interestingness plays an important factor in definitional question answering."""	angular defect;computation;definition;information;question answering	Kian-Wei Kor;Tat-Seng Chua	2007		10.1145/1277741.1277800	natural language processing;question answering;computer science;data mining;information retrieval	Web+IR	-27.54871071761152	-68.94329842824915	144714
c61a9a2b37f8be63e08035fe708f4135c8c0dbd0	a study of probabilistic and algebraic methods for semantic similarity		We study and propose in this article several novel solutions to the task of semantic similarity between two short texts. The proposed solutions are based on the probabilistic method of Latent Dirichlet Allocation (LDA) and on the algebraic method of Latent Semantic Analysis (LSA). Both methods, LDA and LSA, are completely automated methods used to discover latent topics or concepts from large collection of documents. We propose a novel word-to-word similarity measure based on LDA as well as several text-totext similarity measures. We compare these measures with similar, known measures based on LSA. Experiments and results are presented on two data sets: the Microsoft Research Paraphrase corpus and the User Language Paraphrase corpus. We found that the novel word-to-word similarity measure based on LDA is extremely promising.	experiment;latent dirichlet allocation;latent semantic analysis;linear algebra;microsoft research;semantic similarity;similarity measure	Vasile Rus;Nobal B. Niraula;Rajendra Banjade	2013			artificial intelligence;natural language processing;latent dirichlet allocation;machine learning;semantic similarity;computer science;probabilistic logic;probabilistic latent semantic analysis;probabilistic method;similarity measure;paraphrase;latent semantic analysis	ML	-26.598021399223402	-67.73207625407834	146344
4b81b16778c5d2dbf2cee511f7822fa6ae3081bf	designing a tag-based statistical math word problem solver with reasoning and explanation		Background Since Big Data mainly aims to explore the correlation between surface features but not their underlying causality relationship, the Big Mechanism 2 program has been proposed by DARPA to find out “why” behind the “Big Data”. However, the pre-requisite for it is that the machine can read each document and learn its associated knowledge, which is the task of Machine Reading (MR). Since a domain-independent MR system is complicated and difficult to build, the math word problem (MWP) [1] is frequently chosen as the first test case to study MR (as it usually uses less complicated syntax and requires less amount of domain knowledge). According to the framework for making the decision while there are several candidates, previous MWP algebra solvers can be classified into: (1) Rule-based approaches with logic inference [2-7], which apply rules to get the answer (via identifying entities, quantities, operations, etc.) with a logic inference engine. (2) Rule-based approaches without logic inference [8-13], which apply rules to get the answer without a logic inference engine. (3) Statistics-based approaches [14, 15], which use statistical models to identify entities, quantities, operations, and get the answer. To our knowledge, all the statistics-based approaches do not adopt logic inference. The main problem of the rule-based approaches mentioned above is that the coverage rate problem is serious, as rules with wide coverage are difficult and expensive to construct. Also, since they adopt Go/No-Go approach (unlike statistical approaches which can adopt a large Top-N to have high including rates), the error accumulation problem would be severe. On the other hand, the main problem of those approaches without adopting logic inference is that they usually need to implement a new handling procedure for each new type of problems (as the general logic inference mechanism is not adopted). Also, as there is no inference engine to generate the reasoning chain [16], additional effort would be required for	big data;big mechanism;causality;entity;inference engine;logic programming;solver;statistical model;test case;tree accumulation	Chien-Tsung Huang;Yi-Chung Lin;Chao-Chun Liang;Kuang-Yi Hsu;Shen-Yun Miao;Wei-Yun Ma;Lun-Wei Ku;Churn-Jung Liau;Keh-Yih Su	2015	IJCLCLP		computer science;artificial intelligence;machine learning;algorithm	AI	-29.558193565274017	-70.12972903023991	146465
6643ec699ec45e3ca7bb291a9baf54c846e5e9cf	hauss: incrementally building a summarizer combining multiple techniques	automatic summarization;legal documents;knowledge acquisition;natural language processing	The idea of automatic summarization dates back to 1958, when Luhn invented the “auto abstract” (Luhn, 1958). Since then, many diverse automatic summarization approaches have been proposed, but no single technique has solved the increasingly urgent need for automatic summarization. Rather than proposing one more such technique, we suggest that the best solution is likely a system able to combine multiple summarization techniques, as required by the type of documents being summarized. Thus, this paper presents HAUSS: a framework to quickly build specialized summarizers, integrating several base techniques into a single approach. To recognize relevant text fragments, rules are created that combine frequency, centrality, citation and linguistic information in a context-dependent way. An incremental knowledge acquisition framework strongly supports the creation of these rules, using a training corpus to guide rule acquisition, and produce a powerful knowledge base specific to the domain. Using HAUSS, we created a knowledge base for catchphrase extraction in legal text. The system outperforms existing state-of-the-art general-purpose summarizers and machine learning approaches. Legal experts rated the extracted summaries similar to the original catchphrases given by the court. Our investigation of knowledge acquisition methods for summarization therefore demonstrates that it is possible to quickly create effective special-purpose summarizers, which combine multiple techniques, into a single context-	automatic summarization;centrality;citation analysis;context-sensitive language;exception handling;general-purpose modeling;graham scan;information extraction;interaction;knowledge acquisition;knowledge base;knowledge-based systems;list of toolkits;luhn algorithm;machine learning;natural language processing;rouge (metric);restrictive design rules;statistical model;test set;text corpus;tf–idf;trusted computer system evaluation criteria	Filippo Galgani;Paul Compton;Achim G. Hoffmann	2014	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2014.03.002	natural language processing;multi-document summarization;computer science;artificial intelligence;automatic summarization;machine learning;data mining	NLP	-28.971314766809343	-68.734561807905	146782
c4233abe5887b18bd659fb728c26e30dbfcda247	automated unsupervised authorship analysis using evidence accumulation clustering		Authorship Analysis aims to extract information about the authorship of documents from features within those documents. Typically, this is performed as a classification task with the aim of identifying the author of a document, given a set of documents of known authorship. Alternatively, unsupervised methods have been developed primarily as visualisation tools to assist the manual discovery of clusters of authorship within a corpus by analysts. However, there is a need in many fields for more sophisticated unsupervised methods to automate the discovery, profiling and organisation of related information through clustering of documents by authorship. An automated and unsupervised methodology for clustering documents by authorship is proposed in this paper. The methodology is named NUANCE, for n-gram Unsupervised Automated Natural Cluster Ensemble. Testing indicates that the derived clusters have a strong correlation to the true authorship of unseen documents.	cluster analysis;n-gram;stylometry;tree accumulation;unsupervised learning	Robert Layton;Paul A. Watters;Richard Dazeley	2013	Natural Language Engineering	10.1017/S1351324911000313	computer science;data science;data mining;information retrieval	NLP	-27.873862013517563	-67.85770910311011	146982
506270b9e786a8fcc1d5eea748f1bab9a9b86881	cross lingual snippet generation using snippet translation system	statistical machine translation;snippet generation;cross lingual snippet generation;snippet translation	Multi Lingual Snippet Generation MLSG systems provide the users with snippets in multiple languages. But collecting and managing documents in multiple languages in an efficient way is a difficult task and thereby makes this process more complicated. Fortunately, this requirement can be fulfilled in another way by translating the snippets from one language to another with the help of Machine Translation MT systems. The resulting system is called Cross Lingual Snippet Generation CLSG system. This paper presents the development of a CLSG system by Snippet Translation when documents are available only in one language. We consider the English-Bengali language pair for snippet translation in one direction English to Bengali. In this work, a major concentration is given towards translating snippets with simpler but excluding deeper MT concepts. In experimental results, an average BLEU score of 14.26 and NIST score of 4.93 are obtained.		Pintu Lohar;Pinaki Bhaskar;Santanu Pal;Sivaji Bandyopadhyay	2014		10.1007/978-3-642-54903-8_28	natural language processing;speech recognition;computer science;world wide web	NLP	-28.67090088133642	-72.00645044308662	147440
c684fd6c99b319e3da26f03d5d284b108e05083d	merging controlled vocabularies through semantic alignment based on linked data		In this paper, a methodology is presented that aids in finding equivalent terms between semantically similar controlled vocabularies. It is based both on lexical similarities discovery and semantic alignment through external LOD datasets. The proposed methodology has been deployed for the identification of equivalent terms within two datasets consisting of subject headings, namely Dione and NYT and facilitated through the employment of the LOD datasets of DBpedia and WordNet. The effectiveness of the methodology is assessed through a comparative evaluation between the deployment of the methodology presented in this paper and the deployment of a lexical similarities-based algorithm presented in previous work.	controlled vocabulary;linked data	Ioannis Papadakis;Konstantinos Kyprianos	2013		10.1007/978-3-319-03437-9_32	data mining;database;information retrieval	NLP	-31.255162978087974	-68.07384697364853	147667
a636e6680c658e328af29ee8cdb376e3b12a2e8b	on design of a question-answering interface for hindi in a restricted domain	heuristics-based query analysis.;natural language interface;question-answering;hindi-english mixing;hindi;information retrieval;system performance;question answering	The paper presents a schema for developing natural language interface for question-answering in Hindi in a specific domain and discusses issues involved therein. The system performs a shallow syntactic and semantic analysis of the input system. After identifying certain keywords in the query, the system triggers a reasoning process to determine the type of query and the answer slots that are required for reply generation. The reply generation is in the form of slot-fillings as identified through the reasoning process and by triggering information retrieval from the domain database. We outline strategy for incorporating inter-query contexts and for generating back-query to the user.	cluster analysis;information retrieval;natural language user interface;query optimization;question answering;semantic analysis (compilers);semiconductor industry;shallow parsing;systems design	Raveendranatha P. Mahesh;Koustuv Sinha	2006			hindi;natural language processing;natural language user interface;question answering;artificial intelligence;computer science	AI	-32.43837930961049	-71.4206974680375	148192
fa658b113f3b21420cd9de58b3b0c3acdc694029	jvn-tdt entity linking systems at tac-kbp2012		We present two methods for entity linking in two of our systems submitted to TAC-KBP 2012. The first one, implemented in JVNTDT1 system, learns coherence among cooccurrence entities referred to within a text by exploiting Wikipedia’s link structure and the second one, implemented in JVN_TDT2 system, combines some heuristics with a statistical model, for entity linking. The method implemented in JVN-TDT1 exploits two features to train a classifier and exploits coreference relations among co-occurring mentions for entity linking. The method implemented in JVNTDT2 is a hybrid method that performs entity linking in two phases. The first phase is a rulebased phase that filters candidates and, if possible, it disambiguates mentions with high reliability. The second phase employs a statistical model to rank the candidates of each remaining mention and choose the one with the highest ranking as the right referent of that mention. Experiments are conducted to evaluate two methods on two datasets – TAC-KBP2011 and TAC-KBP2012 datasets.	dia;entity linking;experiment;heuristic (computer science);information retrieval;knowledge base;natural language processing;question answering;semantic web;semantic similarity;statistical model;time-domain reflectometry;wikipedia	Hien T. Nguyen;Huy H. Minh;Tru H. Cao;Trong T. Nguyen	2012			computer science;machine learning;data mining;algorithm	NLP	-26.61564646462578	-66.88236461725276	148488
44c2f941ae52313f0b4256a83aa5b432d48c671c	an ontology-based method for an efficient acquisition of relation extraction training and testing examples	relation extraction;corpus;polish;ontology;cyc	In this paper, we describe an ontology-based method of selection of test examples for relation extraction, as well as a method of their validation apt to be carried out by ordinary language-speakers. The results will be used to validate performance of various relation extraction algorithms. In performed tests we utilize the ResearchCyc ontology and demonstrate the method's performance in gathering examples from Polish texts.	information extraction;relationship extraction	Aleksander Smywinski-Pohl	2011		10.1007/978-3-642-25261-7_25	natural language processing;computer science;ontology;data mining;polish;ontology-based data integration;information retrieval	NLP	-30.072562879999484	-69.21073807495614	149348
ad8ba469defc8804d2af46fd0f80fd76b5a57d6e	blue-lite: a knowledge-based lexical entailment system for rte6		"""In this paper we present our RTE6 system, BLUE-Lite, and the results of experiments with it. Unlike our earlier RTE5 system, called BLUE, BLUE-Lite uses only a lexical (""""bag of words"""") representation of the sentences. To compare lexical items, BLUE-Lite exploits linguistic and world knowledge drawn from WordNet and the DIRT paraphrase database. To take context into account, BLUE-Lite also looks in the preceding sentence (with reduced confidence) if an H word does not match T. In addition, the entailment theshold is varied between topics to account for the fact that some topics are harder to find entailments in than others. Our results show that WordNet, DIRT, and these two techniques all improved performance (producing an overall F=0.44), and also that a relatively simple baseline (""""match all but one"""") without any of these techniques achieved a surprisingly high score (F=0.40). Finally, we discuss the role of structural information, why it is challenging to yield advantage from it (in particular in this year's challenge), but why ultimately it must be taken into account for further improvements in performance."""	adobe flash lite;bag-of-words model;baseline (configuration management);blue gene;commonsense knowledge (artificial intelligence);experiment;wordnet	Peter Clark;Philip Harrison	2010			natural language processing;lexical functional grammar;information retrieval;computer science;bag-of-words model;wordnet;paraphrase;artificial intelligence;logical consequence;sentence;lexical item	NLP	-26.42109624000111	-69.30572827728889	149463
07ddffa6436958a888abefc8cbddccfce517092e	applicability of automatically generated thesauri to text classification in specific domains		The paper is devoted to comparison of the quality of text classification with the use of manually and automatically generated thesauri. For this purpose, the authors applied the BM25 algorithm with word features based on thesaurus’s relations between terms. The experiments, conducted with text corpora from three specific domains (medicine, economics, and sport), showed that using an automatically generated thesaurus provides nearly the same classification quality as the manually created one. These results make the authors’ approach promising for text classification in many specific domains where no thesaurus is available, as it allows to avoid consumption of high amount of resources for manual thesaurus creation.	algorithm;document classification;experiment;okapi bm25;statistical classification;text corpus;thesaurus	Ksenia Lagutina;Ivan Shchitov;Nadezhda Lagutina;Ilya Paramonov	2017				Web+IR	-33.471721446093184	-69.03835478483482	149669
640f225db8951052675b2b442244f6a18d74552c	incremental topic representations	relevant topic relation;relevant term;previously-proposed topic representation;topic representation;multi-document summarization;information extraction;incremental topic representation;topic theme;nlp application;topic signature;incremental enhancement	"""y z1{}|~Ez z1E|GEz}+|"""":|Wz}~E ~|"""" """"|G~#n|E z |GE{E{}Ez>~z^y z#Ez}{nz~ """" nz¡J¢ | ~E{z}z}~P3""""£z}~E¤""""~{}z}z~W|"""" z|GE{L~¤"""" Ez ~P|WE{z¥W¦ §© ̈;~a~¤¬«£| ­W¦P®T ̄°°G°G±32 z ́μ z}~E¤~E{}z}z}~P{}|~EzF|GE{¶zEz}z}~P3 |~L~uz |zz­G""""~W |{¬z}""""|~>~EμzG+|z}z}­~P z} 2 z!z}{}|G~Lz~¤""""~{}zz}~PT ¤""""zF|~L3""""~·W~#Ez |{ zz}} 2;|G{¶z}Ezz}~P3""""|G~E """"z>~Wz3""""z} ̧~1J¢ | oT ̈;»1⁄4E{""""|~}1⁄2¶3⁄4J~|"""" """"|G~À¿fW3{|G~À""""~¤ÀÁ EÂ ÃT|P{}Ez}~P ÄW  Å}""""|G~H Æ&ETzEnzz~W£|¢u"""" ~{}|""""n|3 ~Ez J¢ |z}~~{z#z}z}z}~P3""""|G~E&~n|"""" E{""""|~ |WEE{}z} μ3""""~PÇ""""^E| ­Czz}~P¬| ­Cz Ez­|Â¦W N|Gn|z |{Fz}z}z}~P3""""|~¤ {~anz {}ÈWEÂz|G  {}Â¦P"""		Sanda M. Harabagiu	2004			natural language processing;computer science;machine learning;information retrieval	NLP	-27.49931642580432	-66.21042205488308	149674
97f94bf7b0b6c91961bd1ed90e79f542be4cc9e2	towards a common evaluation strategy for table structure recognition algorithms	table detection;precision;recall;system development;evaluation;ground truth;table structure recognition;table recognition	A number of methods for evaluating table structure recognition systems have been proposed in the literature, which have been used successfully for automatic and manual optimization of their respective algorithms. Unfortunately, the lack of standard, ground-truthed datasets coupled with the ambiguous nature of how humans interpret tabular data has made it difficult to compare the obtained results between different systems developed by different research groups.  With reference to these approaches, we describe our experiences in comparing our algorithm for table detection and structure recognition to another recently published system using a freely available dataset of 75 PDF documents. Based on examples from this dataset, we define several classes of errors and propose how they can be treated consistently to eliminate ambiguities and ensure the repeatability of the results and their comparability between different systems from different research groups.	algorithm;mathematical optimization;repeatability;table (information)	Tamir Hassan	2010		10.1145/1860559.1860617	ground truth;computer science;artificial intelligence;evaluation;data mining;database;recall;accuracy and precision;algorithm	Vision	-29.20236785075692	-69.26273120220841	150372
845d50ff7b2f0c356ee299014faca80809c6abe1	semantic annotations as complimentary to underspecified semantic representations		This paper presents a new perspective on the use of semantic annotations. We argue that semantic annotations should (1) capture semantic information that is complimentary to the information that is expressed in the source text; (2) have a formal interpretation. If these conditions are fullfilled, then the information in semantic annotations can be effectively combined with the information in the source text by interpreting a semantic annotation language through the translation of annotations into the same formalism as underspecified semantic representations obtained through compositional semantic analysis.	interpretation (logic);knuth–morris–pratt algorithm;machine learning;ontology (information science);pipelines;semantics (computer science);stochastic process;text corpus;unification (computer science)	Harry Bunt	2009			natural language processing;semantic interoperability;semantic interpretation;semantic similarity;semantic computing;semantic integration;explicit semantic analysis;semantic search;semantic grid;computer science;social semantic web;data mining;semantic web stack;semantic compression;semantic equivalence;semantic property;semantic technology;information retrieval;semantic analytics	NLP	-31.445002343500576	-71.05729780154172	151492
c812f7409256f2dd437661cae6d9d20731457cf4	augmenting wordnet-like lexical resources with distributional evidence. an application-oriented perspective	semantic similarity;generic point	The paper deals with the issue of how and to what extent WordNet-like resources provide the necessary information for an assessment of semantic similarity which is useful for practical applications. The general point is made that taxonomical information should be complemented with distributional evidence. The claim is substantiated through experimental a~t8 and an illustration of a word sense disambiguation system (SENSE) capable of using contextually-relevant semantic similarity.	context-sensitive grammar;entity;eurowordnet;inferential theory of learning;natural language processing;relevance;semantic similarity;taxonomy (general);web services for devices;word sense;word-sense disambiguation;wordnet	Simonetta Montemagni;Vito Pirrelli	1998			artificial intelligence;semantic similarity;experimental data;natural language processing;pattern recognition;computer science;semeval;word-sense disambiguation;wordnet;generic point	NLP	-27.933869614022452	-72.29971019324088	151606
dcbc6d398ed21b5321ad22f734e95f0fe86de760	abbreviation expander - a web-based system for easy reading of technical documents		Abbreviations and acronyms are a part of textual communication in most domains. However, abbreviations are not necessarily defined in documents that employ them. Understanding all abbreviations used in a given document often requires extensive knowledge of the target domain and the ability to disambiguate based on context. This creates considerable entry barriers to newcomers and difficulties in automated document processing. Existing abbreviation expansion systems or tools require substantial technical knowledge for set up or make strong assumptions which limit their use in practice. Here, we present Abbreviation Expander, a system that builds on state of the art methods for identification of abbreviations, acronyms and their definitions and a novel disambiguator for abbreviation expansion in an easily accessible web-based solution.	document processing;feedback;web application;word embedding	Manuel R. Ciosici;Ira Assent	2018			information retrieval;natural language processing;artificial intelligence;web application;technical documentation;computer science	NLP	-30.86811661767768	-67.53856559176862	151613
5144f55509085220f4d93ae6314a236d8f36e079	on semantic spaces		This contribution gives an overview about different approaches to semantic spaces. It is not a exhaustive survey, but rather a personal view on different approaches which use metric spaces for the representation of meanings of linguistic units. The aim is to demonstrate the similarities of apparently different approaches and to inspire the generalisation of semantic spaces tailored to the representation of texts to arbitrary semiotic artefacts. I assume that the primary purpose of a semiotic system is communication. A semiotic system S̃ consists of signs s. Signs fulfil a communicative function f (s) within the semiotic system in order to meet the communicative requirements of system’s user. There are different similarity relations between functions of signs. In its most general form a semantic space can be defined as follows:	requirement;semiotics;spaces	Edda Leopold	2005	LDV Forum		computer science;semantic similarity;semantic computing;natural language processing;information retrieval;multinet;social semantic web;artificial intelligence;semantic equivalence;semantic search;semantic web stack;semantic compression	NLP	-32.84698706051486	-70.56740975321935	151779
5753b1c8802785642a34159753838259aefbaea8	second order co-occurrence pmi for determining the semantic similarity of words		This paper presents a new corpus-based method for calculating the semantic similarity of two target words. Our method, called Second Order Co-occurrence PMI (SOC-PMI), uses Pointwise Mutual Information to sort lists of important neighbor words of the two target words. Then we consider the words which are common in both lists and aggregate their PMI values (from the opposite list) to calculate the relative semantic similarity. Our method was empirically evaluated using Miller and Charlers (1991) 30 noun pair subset, Rubenstein and Goodenoughs (1965) 65 noun pairs, 80 synonym test questions from the Test of English as a Foreign Language (TOEFL), and 50 synonym test questions from a collection of English as a Second Language (ESL) tests. Evaluation results show that our method outperforms several competing corpus-based methods.	aggregate data;landauer's principle;natural language processing;pointwise mutual information;semantic similarity;sensor;similarity measure;speech recognition;text corpus	Aminul Islam;Diana Inkpen	2006			noun;natural language processing;co-occurrence;pointwise mutual information;artificial intelligence;synonym;computer science;semantic similarity;sort;test of english as a foreign language;pattern recognition	NLP	-26.469514158485037	-68.41251102436196	151916
de9b2dd84b1da9489c232ac779aebdef64d8a42c	towards learning based strategy for improving the recall of the servomap matching system.		In order to solve interoperability issues among he terogeneous knowledge based applications, it is important to fi nd correspondences between their underlying ontologies. This is the aim of the S rvOMap system, a generic approach for large scale ontologies matching. Howev er, although achieving good results on the official Ontology Alignment Eva luation Initiative dataset, ServOMap performance remains to be improved in term of recall. We describe in this paper a strategy based on Machine Learning technique for improving the discovery of more possible candidate mappings among input ontology entities.	entity;interoperability;machine learning;ontology (information science);ontology alignment	Gayo Diallo;Amal Kammoun	2013			machine learning;pattern recognition;information retrieval	ML	-31.31787479536663	-66.61650157834553	153017
0690e89a374c0c0b1aff6b56439f50e9692c500b	truly exploring multiple references for machine translation evaluation		Multiple references in machine translation evaluation are usually under-explored: they are ignored by alignment-based metrics and treated as bags of n-grams in string matching evaluation metrics, none of which take full advantage of the recurring information in these references. By exploring information on the n-gram distribution and on divergences in multiple references, we propose a method of ngram weighting and implement it to generate new versions of the popular BLEU and NIST metrics. Our metrics are tested in two into-English machine translation datasets. They lead to a significant increase in Pearson’s correlation with human fluency judgements at system-level evaluation. The new NIST metric also outperforms the standard NIST for documentlevel evaluation.	bleu;grams;machine translation;n-gram;nist (metric);string searching algorithm	Ying Qin;Lucia Specia	2015			natural language processing;computer science;theoretical computer science;programming language	NLP	-27.20181668340344	-68.23265244226778	153044
25946d1a0d138e2c934465ad2e366b91a517314b	semantic disambiguation in folksonomy: a case study	qa076 computer software	Social annotation systems such as del.icio.us, Flickr and others have gained tremendous popularity among Web 2.0 users. One of the factors of success was the simplicity of the underlying model, which consists of a resource (e.g., a web page), a tag (e.g., a text string), and a user who annotates the resource with the tag. However, due to the syntactic nature of the underlying model, these systems have been criticised for not being able to take into account the explicit semantics implicitly encoded by the users in each tag. In this article we: a) provide a formalisation of an annotation model in which tags are based on concepts instead of being free text strings; b) describe how an existing annotation system can be converted to the proposed model; c) report on the results of such a conversion on the example of a del.icio.us dataset; and d) show how the quality of search can be improved by the semantic in the converted dataset.	circa;emoticon;flickr;folksonomy;html element;lambda calculus;mathematical model;provisioning;semantics (computer science);string (computer science);vocabulary;web 2.0;web page;word-sense disambiguation;wordnet	Pierre Andrews;Juan Pane;Ilya Zaihrayeu	2009		10.1007/978-3-642-23160-5_8	computer science;data mining;world wide web;information retrieval	Web+IR	-30.31728100360603	-66.66675259411903	153294
22651623b11cffd90c99d92430d3af41c7b9fd80	towards detecting anomalies in the content of standardized lmf dictionaries		Dictionaries are reference resources for learning and diffusing natural languages. Their contents must be enriched carefully due to their importance. However, such contents might contain errors and inconsistencies that are hard to detect manually. Several researches have been made in recent years in order to perform this step automatically. However, they have dealt with the problem in a superficial way. The present paper deals with the detection of anomalies in the content of LMF-standardized dictionaries that covers lexical knowledge at the morphological, syntactic and semantic levels. Thus, we are proposing an approach based on a typological study of the potential anomalies that can occur in editorial dictionaries in general. This approach takes advantage of the LMF fine structure that highlights all kinds of relationships between entries’ knowledge and distinguishes the role of each available text such as giving definitions and examples. An experiment of the proposed approach was carried out on an available LMFstandardized dictionary of the Arabic language. This experiment has been related to the morphological and syntactic levels.	data dictionary;lexical markup framework;lexicon;natural language;sensor;the superficial;verification and validation	Wafa Wali;Bilel Gargouri;Abdelmajid Ben Hamadou	2013			natural language processing;speech recognition;computer science;machine learning;linguistics	NLP	-29.047093133003564	-72.22715293181277	153417
8d7b9efab86b0353047718adf56ec3ab10edb81e	an assessment methodology of short and open answer questions soaqs		aThe short and open answer is a response written by a learner in natural language (until 50 words). We will propose a novel method of SOAQ assessment using concept maps and semantic similarity measure.		Safa Ben Salem;Lilia Cheniti-Belcadhi;Rafik Braham	2014		10.1007/978-3-319-11200-8_65	natural language processing;computer science;data mining;information retrieval	Theory	-32.82537994168154	-69.63704842549221	153734
5408b5a8762644451c2d7b98d581f1db26b5df9f	applying wikipedia's multilingual knowledge to cross-lingual question answering	cross language evaluation forum;wikipedia;cross lingual question answering;cross lingual;noun;info eu repo semantics article;inter lingual index;named entity;eurowordnet;question answering	The application of the multilingual knowledge encoded in Wikipedia to an open–domain Cross–Lingual Question Answering system based on the Inter Lingual Index (ILI) module of EuroWordNet is proposed and evaluated. This strategy overcomes the problems due to ILI’s low coverage on proper nouns (Named Entities). Moreover, as these are open class words (highly changing), using a community–based up– to–date resource avoids the tedious maintenance of hand–coded bilingual dictionaries. A study reveals the importance to translate Named Entities in CL–QA and the advantages of relying on Wikipedia over ILI for doing this. Tests on questions from the Cross–Language Evaluation Forum (CLEF) justify our approach (20% of these are correctly answered thanks to Wikipedia’s Multilingual Knowledge).	bilingual dictionary;entity;eurowordnet;ibm open class;question answering;wikipedia	Sergio Ferrández;Antonio Toral;Óscar Ferrández;Antonio Ferrández Rodríguez;Rafael Muñoz	2007		10.1007/978-3-540-73351-5_31	natural language processing;noun;question answering;computer science;brand;linguistics;information retrieval	NLP	-28.384924125809132	-71.24080506010046	153769
0ca3096914bb30effad37018c10b66680912fe83	background knowledge for ontology construction	text mining;document representation;machine learning;semi automatic ontology construction;background knowledge;ontology construction	In this paper we describe a solution for incorporating background knowledge into the OntoGen system for semi-automatic ontology construction. This makes it easier for different users to construct different and more personalized ontologies for the same domain. To achieve this we introduce a word weighting schema to be used in the document representation. The weighting schema is learned based on the background knowledge provided by user. It is than used by OntoGen's machine learning and text mining algorithms.	algorithm;machine learning;ontology (information science);personalization;semiconductor industry;text mining	Blaz Fortuna;Marko Grobelnik;Dunja Mladenic	2006		10.1145/1135777.1135959	upper ontology;text mining;bibliographic ontology;computer science;knowledge management;ontology;body of knowledge;machine learning;open knowledge base connectivity;data mining;ontology-based data integration;information retrieval;process ontology;suggested upper merged ontology	AI	-31.783891037615614	-67.83043967649222	153777
d93a6a226aab6d5c5c06d0ead75621b4d2b8cc39	semantic automatic error-detecting for chinese text based on semantic dependency relationship	automatic proofreading for chinese text;dependency grammar;semantic collocation;semantic errordetecting	Automatic error-detecting for Chinese text is one of the important issues in the field of Chinese Information Processing. The current study of text error-detecting focuses on words-level and syntactic-level, but semantic error-detecting has not got enough attention. For this reason, we studied semantic collocations of sentences, and built up a words dependency knowledge base; at meanwhile, combined with the description of concept of words in HowNet, we proposed a semantic error-detecting strategy for Chinese text. Experimental results show that this method has a better performance in semantic error-detecting.		Jiayuan Li;Yangsen Zhang;Jinjin Zhu;Zewei Zhang	2013		10.1007/978-3-642-45185-0_43	natural language processing;computer science;linguistics;information retrieval	NLP	-27.465662570791135	-68.21252941058911	154250
a72e1d56ed9c97afa2ee9da78361c2bc6efebe0d	using topic salience and connotational drifts to detect candidates to semantic change	semantic change;semantic change candidate;cohesion measure;adaptable tool;semantic representation;topic salience;connotational drift;flexible semantic change detection;semantic atlas;geometrical model;co-occurrence pattern	Semantic change has mostly been studied by historical linguists and typically at the scale of centuries. Here we study semantic change at a finer-grained level, the decade, making use of recent newspaper corpora. We detect semantic change candidates by observing context shifts which can be triggered by topic salience or may be independent from it. To discriminate these phenomena with accuracy, we combine variation filters with a series of indices which enable building a coherent and flexible semantic change detection model. The indices include widely adaptable tools such as frequency counts, co-occurrence patterns and networks, ranks, as well as model-specific items such as a variability and cohesion measure and graphical representations. The research uses ACOM, a co-occurrence based geometrical model, which is an extension of the Semantic Atlas. Compared to other models of semantic representation, it allows for extremely detailed analysis and provides insight as to how connotational drift processes unfold.	agency.com;coherence (physics);concept drift;graphical user interface;spatial variability;text corpus	Armelle Boussidan;Sabine Ploux	2011			natural language processing;semantic similarity;computer science;data mining;communication	NLP	-31.5337192715533	-70.65443207866429	154314
31f5feaebd5af0baf7717668cd7940ed9daecfc3	on the automated generation of scholarly publishing linked datasets: the case of ceur-ws proceedings		The availability of highly-informative semantic descriptions of scholarly publishing contents enables an easier sharing and reuse of research findings as well as a better assessment of the quality of scientific productions. In the context of the ESWC2015 Semantic Publishing Challenge, we present a system that automatically generates rich RDF datasets from CEUR-WS workshop proceedings and exposes them as Linked Data. Web pages of proceedings and textual contents of papers are analyzed through proper text processing pipelines. Semantic annotations are added by a set of SVM classifiers and refined by heuristics, gazetteers and rule-based grammars. Web services are exploited to link annotations to external datasets like DBpedia, CrossRef, FundRef and Bibsonomy. Finally, the data is modelled and published as an RDF graph.		Francesco Ronzano;Beatríz Fisas;Gerard Casamayor;Horacio Saggion	2015		10.1007/978-3-319-25518-7_15	library science;computer science;data science;information retrieval	NLP	-32.406502920068895	-67.10768916287171	154338
5f0a228eaf82a92721300fd89b7dffe9c0a1dc13	exploiting lists of names for named entity identification of financial institutions from unstructured documents		There is a wealth of information about financial systems that is embedded in document collections. In this paper, we focus on a specialized text extraction task for this domain. The objective is to extract mentions of names of financial institutions, or FI names, from financial prospectus documents, and to identify the corresponding real world entities, e.g., by matching against a corpus of such entities. The tasks are Named Entity Recognition (NER) and Entity Resolution (ER); both are well studied in the literature. Our contribution is to develop a rule-based approach that will exploit lists of FI names for both tasks; our solution is labeled Dict-based NER and Rank-based ER. Since the FI names are typically represented by a root, and a suffix that modifies the root, we use these lists of FI names to create specialized root and suffix dictionaries. To evaluate the effectiveness of our specialized solution for extracting FI names, we compare Dict-based NER with a general purpose rule-based NER solution, ORG NER. Our evaluation highlights the benefits and limitations of specialized versus general purpose approaches, and presents additional suggestions for tuning and customization for FI name extraction. To our knowledge, our proposed solutions, Dict-based NER and Rank-based ER, and the root and suffix dictionaries, are the first attempt to exploit specialized knowledge, i.e., lists of FI names, for rule-based NER and ER. CCS Concepts: rInformation systems→ Extraction, transformation and loading; rComputing methodologies→ Information extraction;	cloud computing;data dictionary;embedded system;erdős–rényi model;heuristic (computer science);information extraction;information quality;ksd-64;logic programming;named entity;named-entity recognition;partial template specialization;regular expression;scoring functions for docking;superuser;synergy;text corpus	Zheng Xu;Douglas Burdick;Louiqa Raschid	2016	CoRR		natural language processing;computer science;artificial intelligence;machine learning;data mining	DB	-31.35227340710267	-69.4600317827169	154346
0a72cb974c5f0c627b87dcb8ea75337e0235873b	follow-up question handling in the imix and ritel systems: a comparative study	dialogue system;comparative study;user interaction;question answering	One of the basic topics of QA dialogue systems is how follow-up questions should be interpreted by a QA system. In this paper, we shall discuss our experience with the IMIX and Ritel systems, for both of which a follow-up question handling scheme has been developed, and corpora have been collected. These two systems are each other’s opposites in many respects: IMIX is multimodal, non-factoid, black-box QA, while Ritel is speech, factoid, keyword-based QA. Nevertheless, we will show that they are quite comparable, and that it is fruitful to examine the similarities and differences. We shall look at how the systems are composed, and how real, non-expert, users interact with the systems. We shall also provide comparisons with systems from the literature where possible, and indicate where open issues lie and in what areas existing systems may be improved. We conclude that most systems have a common architecture with a set of common subtasks, in particular detecting follow-up questions and finding referents for them. We characterise these tasks using the typical techniques used for performing them, and data from our corpora. We also identify a special type of follow-up question, the discourse question, which is asked when the user is trying to understand an answer, and propose some basic methods for handling it.	black box;dialog system;internet mix;multimodal interaction;question answering;sensor;software quality assurance;text corpus	Boris W. van Schooten;Rieks op den Akker;Sophie Rosset;Olivier Galibert;Aurélien Max;Gabriel Illouz	2009	Natural Language Engineering	10.1017/S1351324908004920	natural language processing;question answering;computer science;comparative research;data mining;information retrieval	NLP	-28.379261109291225	-72.92071025728748	154428
1d1774f6e3312ae010aad9a9aa421ffca4852bfe	predicting learner levels for online exercises of hebrew	feature selection;online exercise;individual error;two-step process;language teaching situation;data sparsity;learner level;linguistic construction;language learner;easy integration;two-phase classification process;targeted language data	We develop a system for predicting the level of language learners, using only a small amount of targeted language data. In particular, we focus on learners of Hebrew and predict level based on restricted placement exam exercises. As with many language teaching situations, a major problem is data sparsity, which we account for in our feature selection, learning algorithm, and in the setup. Specifically, we define a two-phase classification process, isolating individual errors and linguistic constructions which are then aggregated into a second phase; such a two-step process allows for easy integration of other exercises and features in the future. The aggregation of information also allows us to smooth over sparse features.	algorithm;construction grammar;feature selection;microsoft outlook for mac;parse tree;parsing;sparse matrix;two-phase commit protocol	Markus Dickinson;Sandra Kübler;Anthony Meyer	2012			natural language processing;speech recognition;computer science;machine learning;linguistics	ML	-27.022042879965714	-73.03592731964521	154535
97757729080140a4101c0c7ac1c7b7dc5bd5fbcb	an inverted index for storing and retrieving grammatical dependencies.	search engine;inverted index;indexing terms;indexation;open source	Web count statistics gathered from search engines have been widely use d as a resource in a variety of NLP tasks. For some tasks, however, the information they exploit is not fine-grained enough. We pr opose an inverted index over grammatical relations as a fast and reliable resource to access more general and also more detailed fr equency information. To build the index, we use a dependency parser to parse a large corpus. We extract binary dependency relatio ns, such ashe-subj-say (he is the subject of say) as index terms and construct the index using publicly available open-source indexing softwa re. The unit we index over is the sentence. The index can be used to extract grammatical relations and frequency counts for these r elations. The framework also provides the possibility to search for partial dependencies (say, the frequency of he occurring in subject position), words, strings and a combination of these . On possible application is the disambiguation of syntactic structures.	context-free grammar;inverted index;natural language processing;open-source software;parsing;string (computer science);text corpus;web search engine;word-sense disambiguation	Michaela Atterer;Hinrich Schütze	2008			inverted index;index term;computer science;database;world wide web;information retrieval;search engine	NLP	-29.58977501780601	-68.05321575748539	155250
6637c98cf8de21ce65e013f83682da5d2b20a256	distributed distributional similarities of google books over the centuries		This paper introduces a distributional thesaurus and sense clusters computed on the complete Google Syntactic N-grams, which is extracted from Google Books, a very large corpus of digitized books published between 1520 and 2008. We show that a thesaurus computed on such a large text basis leads to much better results than using smaller corpora like Wikipedia. We also provide distributional thesauri for equal-sized time slices of the corpus. While distributional thesauri can be used as lexical resources in NLP tasks, comparing word similarities over time can unveil sense change of terms across different decades or centuries, and can serve as a resource for diachronic lexicography. Thesauri and clusters are available for download.	book;distributional semantics;download;grams;lexicography;n-gram;natural language processing;text corpus;thesaurus;wikipedia	Martin Riedl;Richard Steuer;Christian Biemann	2014			world wide web;information retrieval	NLP	-29.93125055159679	-67.7512705859687	155854
5b11285e4379578c8a34bf72843824762b691f22	chinese term extraction using minimal resources		dentify fea relatively stable and domain independent term delimiters rather than that of the terms. For term verification, a link analysis based method is proposed to calculate the relevance between term candidates and the sentences in the domain specific corpus from which the candidates are extracted. The proposed approach requires no prior domain knowledge, no general corpora, no full segmentation and minimal adaptation for new domains. Consequently, the method can be used in any domain corpus and it is especially useful for resource-limited domains. Evaluations conducted on two different domains for Chinese term extraction show quite significant improvements over existing techniques and also verify the efficiency and relative domain independent nature of the approach. Experiments on new term extraction also indicate that the approach is quite effective for identifying new terms in a domain making it useful for domain knowledge update.	delimiter;link analysis;relevance;terminology extraction;text corpus	Yuhang Yang;Qin Lu;Tiejun Zhao	2008			natural language processing;computer science;data mining;information retrieval	Web+IR	-28.739635628852266	-70.32848727442968	156205
164ec5e735a93023399bc2b8db953ed20953788e	language identification in document images.		This paper presents a system dedicated to automatic language identification of text regions in heterogeneous and complex documents. This system is able to process documents with mixed printed and handwritten text and various layouts. To handle such a problem, we propose a system that performs the following sub-tasks: writing type identification (printed/handwritten), script identification and language identification. The methods for the writing type recognition and the script discrimination are based on the analysis of the connected components while the language identification approach relies on a statistical text analysis, which requires a recognition engine. We evaluate the system on a new public dataset and present detailed results on the three tasks. Our system outperforms the Google plug-in evaluated on the ground-truth transcriptions of the same dataset.	connected component (graph theory);language identification;plug-in (computing);printing	Philippine Barlas;David Hebert;Clément Chatelain;Sébastien Adam;Thierry Paquet	2016		10.2352/ISSN.2470-1173.2016.17.DRR-058	computer vision;language identification;artificial intelligence;computer science	NLP	-30.675924862608287	-70.12278890259019	157641
17170f8af3c7fbddfd82f4417ab9bae451caf1a6	a parallel non-negative sparse large matrix factorization		This paper proposes parallel methods of non-negative large sparse matrix factorization - a very popular technique in computational linguistics. Memory usage and data transmitting necessity of factor- ization algorithm was analysed and optimized. The described effective GPU-based and distributed algorithms were implemented, tested and compared by means of large sparse matrices processing. Non-negative matrix and tensor factorization are very popular techniques in computational linguistics. With the help of non-negative matrix and tensor fac- torization within the paradigm of latent semantic analysis (1) computational linguists are capable of solving practical problems such as classification, clus- tering of texts and terms (2, 3)), construction of semantic similarity measures (4, 5)), automatic extraction of linguistic structures and relations (Selectional Preferences) and Verb Sub-Categorization Frames), etc. (6) This paper describes the construction of a model for parallel non-negative factorization of a large sparse matrix. Such a model can be used in large NLP systems not limited to narrow domains. The problem of non-negative factorization for a sparse large matrix emerged in the development of a measure of semantic similarity between words with La- tent Semantic Analysis usage. To cover a wide range of topics a great amount of articles from the English Wikipedia was processed to construct the similarity measure. Lexical analysis of the various Wikipedia articles was performed to cal- culate the frequency of using words and collocations. As a result, a large matrix Terms × Articles was constructed. It contains frequency estimation of the terms in the texts. The precise size of the matrix equals to 2,437,234 terms × 4,475,180 articles of the English Wikipedia. The frequency threshold T=3 was set to re- move the noise. The resulting matrix contains 156,236,043 non-zero elements. To factorize a sparse matrix of such size it is necessary to develop a specific model for parallelizing matrix computations. The model has been implemented due to the usage of distributed and parallel computing on the GPU. Recently a	sparse	Anatoly Anisimov;Oleksandr Marchenko;Emil Nasirov;Stepan Palamarchuk	2014		10.1007/978-3-319-10888-9_14	natural language processing;latent semantic analysis;sparse matrix;computer science;theoretical computer science;machine learning;document-term matrix;matrix-free methods;non-negative matrix factorization	Vision	-30.259781057338923	-69.85823221810584	158711
205c9a4664a8c05f15e9743aacdc78cc2d6649ed	building an evaluation scale using item response theory		Evaluation of NLP methods requires testing against a previously vetted gold-standard test set and reporting standard metrics (accuracy/precision/recall/F1). The current assumption is that all items in a given test set are equal with regards to difficulty and discriminating power. We propose Item Response Theory (IRT) from psychometrics as an alternative means for gold-standard test-set generation and NLP system evaluation. IRT is able to describe characteristics of individual items - their difficulty and discriminating power - and can account for these characteristics in its estimation of human intelligence or ability for an NLP task. In this paper, we demonstrate IRT by generating a gold-standard test set for Recognizing Textual Entailment. By collecting a large number of human responses and fitting our IRT model, we show that our IRT model compares NLP systems with the performance in a human population and is able to provide more insight into system performance than standard evaluation metrics. We show that a high accuracy score does not always imply a high IRT score, which depends on the item characteristics and the response pattern.	evaluation function;item response theory;ninl gene;natural language processing;psychometrics;test set;textual entailment	John P. Lalor;Hao Wu;Hong Yu	2016	Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing		data mining;statistics	NLP	-28.308201340236433	-68.9867634349542	159288
76f6c1079e43bea5497471bd79dc9f470480fdfe	supporting resource-based learning on the web using automatically extracted large-scale taxonomies from multiple wikipedia versions	hyponymy detection;resource based learning;tel recommender;wikipedia mining	CROKODIL is a platform for the support of collaborative resource-based learning with Web resources. It enables the building of learning communities in which learners annotate their relevant resources using tags. In this paper, we propose the use of automatically generated large-scale taxonomies in different languages to cope with two challenges in CROKODIL: The multilingualism of the resources, i.e. web resources are in different languages and the connectivity of the semantic network, i.e. learners do not tag resources on the same topic with identical tags. More specifically, we describe a set of features that can be used for detecting hyponymy relations from the category system of Wikipedia.	is-a;taxonomy (general);wikipedia;world wide web	Renato Domínguez García;Philipp Scholl;Christoph Rensing	2011		10.1007/978-3-642-25813-8_34	computer science;data mining;world wide web;information retrieval	NLP	-29.2447895954408	-66.35963139152162	160504
6b9d4427ca1eb526ddb38ddcdcdb3f792a62e750	inferring semantic relations by user feedback		In the last ten years, ontology-based recommender systems have been shown to be effective tools for predicting user pre fer nces and suggesting items. There are however some issues associated wit h the ontologies adopted by these approaches, such as: 1) their crafting is not a cheap process, being time consuming and calling for specialist expertise; 2) they may not represent accurately the viewpoint of the targeted user commu nity; 3) they tend to provide rather static models, which fail to keep tr ack of evolving user perspectives. To address these issues, we propose K link UM, an approach for extracting emergent semantics from user feedbacks, with the aim of tailoring the ontology to the users and improving the recomme ndations accuracy. Klink UM uses statistical and machine learning techniques for finding hierarchical and similarity relationships between keywords assoc iated with rated items and can be used for: 1) building a conceptual taxonomy from scratch, 2) enriching and correcting an existing ontology, 3) providing a numerical estimate of the intensity of semantic relationships according to th e users. The evaluation shows that Klink UM performs well with respect to handcra fted ontologies and can significantly increase the accuracy of suggestions in content-based recommender systems.	acknowledgement (data networks);algorithm;emergence;heuristic (computer science);machine learning;numerical analysis;ontology (information science);prospective search;recommender system;relevance;scratch (programming language);sensor;sparse matrix;subsumption architecture;unified model	Francesco Osborne;Enrico Motta	2014		10.1007/978-3-319-13704-9_27	computer science;knowledge management;artificial intelligence;data mining;world wide web	AI	-31.739153564923427	-67.29440411858484	161075
6b49731e7e2853347548be1262dcdeb745a2e870	semantic enrichment of journal articles using chemical named entity recognition	real publishing workflow;semantic enrichment;possible application;entity recognition;oscar work;chemical structure;journal article;enriched article;general ner;biomedical ontology term	We describe the semantic enrichment of journal articles with chemical structures and biomedical ontology terms using Oscar, a program for chemical named entity recognition (NER). We describe how Oscar works and how it can been adapted for general NER. We discuss its implementation in a real publishing workflow and possible applications for enriched articles.	gene ontology term enrichment;named-entity recognition;oscar	Colin R. Batchelor;Peter T. Corbett	2007			natural language processing;computer science;data mining;entity linking;chemical structure;information retrieval	Web+IR	-33.65808886809066	-67.66716645759453	161122
5bac7d84ba21969ea69649d3349086f76f6ea5c4	towards integration of ontology and text-extracted data for event coreference reasoning		Recently, systems for automatic extraction of semantic information about events from large textual resources have been made available. These tools generate RDF datasets about the events described in the texts, enabling logical reasoning over the extracted information.. Ontological reasoning can be exploited to implement tasks that improve the quality of the extracted information, as, for example in event coreference (i.e., recognizing whether two textual descriptions refer to the same event). Starting from the observation that state of the art tools for event coreference do not exploit ontological information, in this paper, we propose a method to enrich event coreference detection on text-extracted event data by semantic-based rule reasoning.		Stefano Borgo;Loris Bozzato;Alessio Palmero Aprosio;Marco Rospocher;Luciano Serafini	2017		10.1145/3019612.3019916	rdf;ontology;data mining;natural language processing;coreference;logical reasoning;complex event processing;exploit;artificial intelligence;computer science	AI	-29.67577747187555	-66.59956833855244	162265
37d6f356d039b08c2c4919c23064abe6a5dfde57	compression-based similarity measures in symbolic, polyphonic music		We present a novel compression-based method for measuring similarity between sequences of symbolic, polyphonic music. The method is based on mapping the values of binary chromagrams extracted from MIDI files to tonal centroids, then quantizing the tonal centroid representation values to sequences, and finally measuring the similarity between the quantized sequences using Normalized Compression Distance (NCD). The method is comprehensively evaluated with a test set of classical music variations, and the highest achieved precision and recall values suggest that the proposed method can be applied for similarity measuring. Also, we analyze the performance of the method and discuss what should be taken into consideration when applying the method for measurement tasks.	network computing devices;precision and recall;test set	Teppo E. Ahonen;Kjell Lemström;Simo Linkola	2011			machine learning;precision and recall;artificial intelligence;speech recognition;test set;polyphony;midi;normalized compression distance;quantization (signal processing);binary number;mathematics;centroid;pattern recognition	ML	-29.602679395478408	-70.26339228085304	162317
a7d9f1d24e5b49dd3766073980a8ffa3aad78af0	low-bias extraction of domain-specific concepts	domain specificity	The availability of domain-specific knowledge models in various forms has led to the development of several tools and applications specialized on complex domains such as bio-medecine, tourism and chemistry. Yet, most of the current approaches to the extraction of domain-specific knowledge from text are limited in their portability to other domains and languages. In this paper, we present and evaluate an approach to the low-bias extraction of domain-specific concepts. Our approach is based on graph clustering and makes no use of a-priori knowledge about the language or the domain to process. Therefore, it can be used on virtually any language. The evaluation is carried out on two data sets of different cleanness and size.	british informatics olympiad;cluster analysis;knowledge representation and reasoning;software portability	Axel-Cyrille Ngonga Ngomo	2009	Informatica (Slovenia)		computer science	AI	-32.90549994458884	-67.69067460644	162415
27be2078e9c846087efd646f9f87746d68038f5a	named entity translation using anchor texts		This work describes a process to extract Named Entity (NE) translations from the text available in web links (anchor texts). It translates a NE by retrieving a list of web documents in the target language, extracting the anchor texts from the links to those documents and finding the best translation from the anchor texts, using a combination of features, some of which, are specific to anchor texts. Experiments performed on a manually built corpora, suggest that over 70% of the NEs, ranging from unpopular to popular entities, can be translated correctly using sorely anchor texts. Tests on a Machine Translation task indicate that the system can be used to improve the quality of the translations of state-of-the-art statistical machine translation systems.	compiler;experiment;named entity;statistical machine translation;web page	Wang Ling;Pável Calado;Bruno Martins;Isabel Trancoso;Alan W. Black;Luísa Coheur	2011			machine translation;named entity;entity linking;natural language processing;ranging;computer science;artificial intelligence	NLP	-28.208279690578298	-69.6622490535761	162723
6ad51ac518e97ba9eaf8e264aebcbe5b88ccc3fe	cl research's knowledge management system	cl research;topic-based summarization;document exploration functionality;document exploration;different user;knowledge management system;dynamic ontology creation;key research issue;text summarization;information extraction;user model;semantic similarity;knowledge management;question answering	CL Research began experimenting with massive XML tagging of texts to answer questions in TREC 2002. In DUC 2003, the experiments were extended into text summarization. Based on these experiments, The Knowledge Management System (KMS) was developed to combine these two capabilities and to serve as a unified basis for other types of document exploration. KMS has been extended to include web question answering, both general and topic-based summarization, information extraction, and document exploration. The document exploration functionality includes identification of semantically similar concepts and dynamic ontology creation. As development of KMS has continued, user modeling has become a key research issue: how will different users want to use the information they identify.	automatic summarization;experiment;information extraction;knowledge management;management system;question answering;software repository;upsampling;user modeling;web ontology language;xml	Kenneth C. Litkowski	2005			natural language processing;semantic similarity;user modeling;question answering;multi-document summarization;computer science;automatic summarization;data mining;world wide web;information extraction;information retrieval	AI	-33.349640561842214	-66.51687574714654	163569
3917d807f1b88002602128134177280688d1a369	contextuality and beyond: investigating an online diary corpus		Heylighen & Dewaele’s (2002) F-score, a measure of formality developed based on categorical frequencies of word types, is used as a starting point for an investigation of an online diary corpus. Comparisons are made between results in the main corpus of diary entries, a smaller corpus of diary comments, and with previously calculated F-scores for similar types of data (Nowson, Oberlander & Gill, 2005). While the overall F-score is similar in these two corpora, results show that internal make-up of the categories upon which the calculation is based can differ. This suggests that while the F-score is a good measure of formality/contextuality and is useful in distinguishing between genres on a large scale, more detailed analyses are required to more completely describe and situate genres with respect to one another.	f1 score;online diary;situated cognition;text corpus	Laura Teddiman	2009			natural language processing;speech recognition	NLP	-27.37344530677217	-71.2605621581199	163612
76ea8a16454a878b5613f398a62e022097cab39c	generating keyword queries for natural language queries to alleviate lexical chasm problem		In recent years, the task of reformulating natural language queries has received considerable attention from both industry and academic communities. Because of the lexical chasm problem between natural language queries and web documents, if we directly use natural language queries as inputs for retrieval, the results are usually unsatisfactory. In this work, we formulated the task as a translation problem to convert natural language queries into keyword queries. Since the nature language queries users input are diverse and multi-faceted, general encoder-decoder models cannot effectively handle low-frequency words and out-of-vocabulary words. We propose a novel encoder-decoder method with two decoders: the pointer decoder firstly extracts query terms directly from the source text via copying mechanism, then the generator decoder generates query terms using two attention modules simultaneously considering the source text and extracted query terms. For evaluation and training, we also proposed a semi-automatic method to construct a large-scale dataset about natural language query-keyword query pairs. Experimental results on this dataset demonstrated that our model could achieve better performance than the previous state-of-the-art methods.	artificial neural network;copying mechanism;deep learning;encoder;faceted classification;natural language user interface;pointer (computer programming);semiconductor industry;vocabulary;web page	Xiaoyu Liu;Zheming Gao;Qi Zhang;Yu-Gang Jiang;Xuanjing Huang	2018		10.1145/3269206.3271727	source text;information retrieval;pointer (computer programming);natural language;computer science;copying mechanism	NLP	-28.5317361627094	-69.59732516377169	163784
0519af74e6386a54c9e261bb8812eda3f35fe734	machine learning based semantic inference: experiments and observations at rte-3	rte-3 test dataset;semantic inference task;dependent content word pair;current average precision measure;baseline dummy system;textual entailment recognition;average distance;state-of-the-art nlp technique;effective feature;lexical semantic similarity	Textual Entailment Recognition is a semantic inference task that is required in many natural language processing (NLP) applications. In this paper, we present our system for the third PASCAL recognizing textual entailment (RTE-3) challenge. The system is built on a machine learning framework with the following features derived by state-of-the-art NLP techniques: lexical semantic similarity (LSS), named entities (NE), dependent content word pairs (DEP), average distance (DIST), negation (NG), task (TK), and text length (LEN). On the RTE-3 test dataset, our system achieves the accuracy of 0.64 and 0.6488 for the two official submissions, respectively. Experimental results show that LSS and NE are the most effective features. Further analyses indicate that a baseline dummy system can achieve accuracy 0.545 on the RTE-3 test dataset, which makes RTE-3 relatively easier than RTE-2 and RTE-1. In addition, we demonstrate with examples that the current Average Precision measure and its evaluation process need to be changed.	baseline (configuration management);distance (graph theory);dummy variable (statistics);executable space protection;information retrieval;machine learning;named entity;natural language processing;precision and recall;semantic similarity;textual entailment	Baoli Li;Joseph Irwin;Ernest V. Garcia;Ashwin Ram	2007			natural language processing;computer science;machine learning;data mining;linguistics	NLP	-26.85231027921117	-70.61146329046372	164047
2a5cf7007aa58b7fd1345553cf09b3a9ee6484d7	automatic acquisition of phrasal knowledge for english-chinese bilingual information retrieval	maps;information retrieval system;information retrieval;digital library;keyphrase extraction;visualization;knowledge acquisition;proper names;user interaction;domain specificity;language model	Extraction of phrasal knowledge, such as proper names, domain-specific keyphrases and lexical templates from a domain-specific text collection are significant for developing effective information retrieval systems for the Internet. In this paper, we are going to introduce our ongoing research on automatic phrasal knowledge acquisition for English-Chinese bilingual texts. The underlying techniques consist of adaptive keyphrase extraction, lexical template extraction, phrase translation extraction and high-order Markov language model construction. In addition to the increase of retrieval effectiveness, IR systems based on these techniques are expected able to perform much better in many aspects, such as automatic term suggestion, information filtering, text classification and cross-language information retrieval, etc.	cross-language information retrieval;document classification;information filtering system;knowledge acquisition;language model;markov chain	Ming-Jer Lee;Lee-Feng Chien	1998		10.1145/290941.291041	natural language processing;visual word;digital library;question answering;visualization;relevance;computer science;proper noun;data mining;information retrieval;language model;human–computer information retrieval	Web+IR	-30.308600664023313	-68.37356722781881	164374
604809686a977df714f76b823d4fce48e67ee3ba	better summarization evaluation with word embeddings for rouge		ROUGE is a widely adopted, automatic evaluation measure for text summarization. While it has been shown to correlate well with human judgements, it is biased towards surface lexical similarities. This makes it unsuitable for the evaluation of abstractive summarization, or summaries with substantial paraphrasing. We study the effectiveness of word embeddings to overcome this disadvantage of ROUGE. Specifically, instead of measuring lexical overlaps, word embeddings are used to compute the semantic similarity of the words used in summaries instead. Our experimental results show that our proposal is able to achieve better correlations with human judgements when measured with the Spearman and Kendall rank co-	automatic summarization;microsoft word for mac;semantic similarity;word embedding	Jun-Ping Ng;Viktoria Abrecht	2015			natural language processing;speech recognition;automatic summarization;machine learning;pattern recognition;linguistics	NLP	-26.754857118712213	-68.0894965737045	164506
84a4fe8081a90fba25ea7869babe49e5ce8a8e70	models to represent linguistic linked data		As the interest of the Semantic Web and computational linguistics communities in linguistic linked data (LLD) keeps increasing and the number of contributions that dwell on LLD rapidly grows, scholars (and linguists in particular) interested in the development of LLD resources sometimes find it difficult to determine which mechanism is suitable for their needs and which challenges have already been addressed. This review seeks to present the state of the art on the models, ontologies and their extensions to represent language resources as LLD by focusing on the nature of the linguistic content they aim to encode. Four basic groups of models are distinguished in this work: models to represent the main elements of lexical resources (group 1), vocabularies developed as extensions to models in group 1 and ontologies that provide more granularity on specific levels of linguistic analysis (group 2), catalogues of linguistic data categories (group 3) and other models such as corpora models or service-oriented ones (group 4). Contributions encompassed in these four groups are described, highlighting their reuse by the community and the modelling challenges that are still to be faced.	linked data	Julia Bosque-Gil;Jorge Gracia;Elena Montiel-Ponsoda;Asunción Gómez-Pérez	2018	Natural Language Engineering	10.1017/S1351324918000347	encode;artificial intelligence;natural language processing;linguistics;computational linguistics;reuse;granularity;computer science;ontology (information science);linked data;semantic web	NLP	-32.42821885426416	-70.66858496095367	164518
5740db598f6547149865162bf6cad100d4d5213a	what's in this paper?: combining rhetorical entities with linked open data for semantic literature querying	semantic publishing;semantic web;natural language processing	Finding research literature pertaining to a task at hand is one of the essential tasks that scientists face on daily basis. Standard information retrieval techniques allow to quickly obtain a vast number of potentially relevant documents. Unfortunately, the search results then require significant effort for manual inspection, where we would rather select relevant publications based on more fine-grained, semantically rich queries involving a publication's contributions, methods, or application domains. We argue that a novel combination of three distinct methods can significantly advance this vision: (i) Natural Language Processing (NLP) for Rhetorical Entity (RE) detection; (ii) Named Entity (NE) recognition based on the Linked Open Data (LOD) cloud; and (iii) automatic generation of RDF triples for both NEs and REs using semantic web ontologies to interconnect them. Combined in a single workflow, these techniques allow us to automatically construct a knowledge base that facilitates numerous advanced use cases for managing scientific documents.	application domain;information retrieval;knowledge base;linked data;named entity;natural language processing;ontology (information science);scientific literature;semantic web	Bahar Sateli;René Witte	2015		10.1145/2740908.2742022	natural language processing;semantic computing;computer science;semantic web;linked data;data mining;semantic web stack;world wide web;information retrieval	AI	-32.96170804882541	-67.51569090729058	164522
49b59ec1eecb79e98feef6823b1dc80e48a433cf	bootstrapping dictionaries for cross-language information retrieval	automatic generation;lexical acquisition;parallel corpora;cross language information retrieval	The bottleneck for dictionary-based cross-language information retrieval is the lack of comprehensive dictionaries, in particular for many different languages. We here introduce a methodology by which multilingual dictionaries (for Spanish and Swedish) emerge automatically from simple seed lexicons. These seed lexicons are automatically generated, by cognate mapping, from (previously manually constructed) Portuguese and German as well as English sources. Lexical and semantic hypotheses are then validated and new ones iteratively generated by making use of co-occurrence patterns of hypothesized translation synonyms in parallel corpora. We evaluate these newly derived dictionaries on a large medical document collection within a cross-language retrieval setting.	archive;cross-language information retrieval;dictionary;lexicon;parallel text;text corpus	Kornél G. Markó;Stefan Schulz;Olena Medelyan;Udo Hahn	2005		10.1145/1076034.1076124	natural language processing;speech recognition;computer science;information retrieval	Web+IR	-29.68894096160909	-71.50718821669558	164826
c49f774e8741fa4df83547227d57de5c0a63ede7	automatic term extraction for sentiment classification of dynamically updated text collections into three classes		This paper presents an automatic term extraction approach for building a vocabulary that is constantly updated. A prepared dictionary is used for sentiment classification into three classes (positive, neutral, negative). In addition, the results of sentiment classification are described and the accuracy of methods based on various weighting schemes is compared. The paper also demonstrates the computational complexity of generating representations for N dynamic documents depending on the weighting scheme used.		Yuliya Rubtsova	2014		10.1007/978-3-319-11716-4_12	pattern recognition;data mining;information retrieval	NLP	-26.770649086221752	-66.18377451715551	165065
22a00d7934cb19e3c0598a32aa6a5231ad2f4845	semi-automatic recognition of noun modifier relationships	large semantic lexicon;semi-automatic recognition;complex nominal;semi-automatic system;noun modifier relationship;current nominal;precoded noun;semantic relationship;lexical clue;noun phrase;adjective semantics;lexical semantics	Semantic relationships among words and phrases are often marked by explicit syntactic or lexical clues that help recognize such rela~ tionships in texts. Within complex nominals, however, few overt clues are available. Systems that analyze such nominals must compensate for the lack of surface clues with other information. One way is to load the system with lexical semantics for nouns or adjectives. This merely shifts the problem elsewhere: how do we define the lexical semantics and build large semantic lexicons? Another way is to find constructions similar to a given complex nominal, for which the relationships are already known. This is the way we chose, but it too has drawbacks. Similarity is not easily assessed, similar analyzed constructions may not exist, and if they do exist, their analysis may not be appropriate for the current nominal. We present a semi-automatic system that identifies semantic relationships in noun phrases without using precoded noun or adjective semantics. Instead, partial matching on previously analyzed noun phrases leads to a tentative interpretation of a new input. Processing can start without prior analyses, but the early stage requires user interaction. As more noun phrases are analyzed, the system learns to find better interpretations and reduces its reliance on the user. In experiments on English technical texts the system correctly identiffed 60-70% of relationships automatically.	experiment;lexicon;modifier key;semiconductor industry	Ken Barker;Stan Szpakowicz	1998			natural language processing;noun;nominalization;lexical semantics;noun phrase;computer science;specifier;proper noun;linguistics	NLP	-27.907204295398145	-71.83911596736485	166266
1cfe2ad21cb1eb11becc6cd0dc857b7b3ea9eb06	automated generalization of phrasal paraphrases from the web		Rather than creating and storing thousands of paraphrase examples, paraphrase templates have strong representation capacity and can be used to generate many paraphrase examples. This paper describes a new template representation and generalization method. Combing a semantic dictionary, it uses multiple semantic codes to represent a paraphrase template. Using an existing search engine to extend the word clusters and generalize the examples. We also design three metrics to measure our generalized templates. The experimental results show that the representation method is reasonable and the generalized templates have a higher precision and coverage.	code;computer cluster;dictionary;web search engine	Weigang Li;Ting Liu;Yu Zhang;Sheng Li;Wei He	2005			machine learning;template;search engine;combing;artificial intelligence;computer science;paraphrase	AI	-26.84227919349519	-67.48892552752136	166958
16c7e2808a8de7d69e5a9d2155b19c116746f0e5	corpus-driven thematic hierarchy induction		Thematic role hierarchy is a linguistic tool used to describe interactions between semantic roles and their syntactic realizations. Despite decades of dedicated research and numerous thematic hierarchy suggestions in the literature, this concept has not been used in NLP so far due to incompatibility and limited scope of existing hierarchies. We introduce an empirical framework for thematic hierarchy induction and evaluate several role ranking strategies on English and German corpus data. We hypothesize that inducing a thematic hierarchy is feasible, that a hierarchy can be induced from small amounts of data and that resulting hierarchies apply cross-lingually. We evaluate these assumptions empirically.		Ilia Kuznetsov;Iryna Gurevych	2018			natural language processing;artificial intelligence;machine learning;computer science;hierarchy;thematic map	NLP	-27.448681997822373	-72.9368211192112	166975
5c721143cefd2e6ae499154f1df08f4f527ed4cf	general purpose word sense disambiguation methods for nouns in portuguese		Word Sense Disambiguation (WSD) aims at determining the appropriate sense of a word in a particular context. Although it is a highly relevant task for Natural Language Processing, there are few works for Portuguese, which are tailored to specific applications, such as translation and information retrieval. In this work, we report our investigation of some general purpose WSD methods for nouns in Portuguese, tackling two additional challenges: using Princeton Wordnet (for English) as the sense repository and applying/customizing a WSD method for multi-document applications, which, to the best of our knowledge, has not been addressed before. In this paper, we also report our efforts on building a sense annotated corpus (for nouns, only), which was used for evaluating the investigated WSD methods.	information retrieval;natural language processing;web services for devices;word sense;word-sense disambiguation;wordnet	Fernando Antônio Asevedo Nóbrega;Thiago Alexandre Salgueiro Pardo	2014		10.1007/978-3-319-09761-9_9	semeval	NLP	-28.06739751871904	-71.27028392965475	167492
e007284b753fbba9b7efaefc612f39be5af1668c	extraction of definitions for bulgarian		We participated at the Monolingual Bulgarian QA task at CLEF-2006 with a definition extraction system based on linguistic templates and keywords. Our system uses a partial syntactic parser for Bulgarian to detect noun phrases as candidates for definitions. Our system answered correctly to 28% of the definition questions.	parsing;software quality assurance	Hristo Tanev	2006			natural language processing;syntax;noun phrase;bulgarian;parsing;speech recognition;artificial intelligence;engineering	NLP	-28.131337803041706	-72.89589935641476	168469
50b6c9bda98a86372bd304b3ebf0e3bb0962ad08	modeling deliberative argumentation strategies on wikipedia		This paper studies how the argumentation strategies of participants in deliberative discussions can be supported computationally. Our ultimate goal is to predict the best next deliberative move of each participant. In this paper, we present a model for deliberative discussions and we illustrate its operationalization. Previous models have been built manually based on a small set of discussions, resulting in a level of abstraction that is not suitable for move recommendation. In contrast, we derive our model statistically from several types of metadata that can be used for move description. Applied to six million discussions from Wikipedia talk pages, our approach results in a model with 13 categories along three dimensions: discourse acts, argumentative relations, and frames. On this basis, we automatically generate a corpus with about 200,000 turns, labeled for the 13 categories. We then operationalize the model with three supervised classifiers and provide evidence that the proposed categories can be predicted.	argumentation framework;mediawiki;supervised learning;text corpus;wikipedia	Khalid Al Khatib;Henning Wachsmuth;Kevin Lang;Jakob Herpel;Matthias Hagen;Benno Stein	2018			argumentation theory;natural language processing;artificial intelligence;computer science	NLP	-27.317521407189858	-70.51013161024504	168563
a75ad5eefb48ae7800a073b71f57d11bcf59991c	arabic medical terms compilation from wikipedia	knowledge source arabic medical terms compilation domain terms nlp processors domain specific tasks wikipedia graph structure;web sites natural language processing text analysis;semantics;data mining;internet;encyclopedias electronic publishing internet terminology semantics data mining;terminology;electronic publishing;encyclopedias	Domain terms are a useful mean for tuning both resources and NLP processors to domain specific tasks. This paper proposes an improved method for obtaining terms from potentially any domain using the Wikipedia graph structure as a knowledge source.	central processing unit;experiment;natural language processing;terminology extraction;wikipedia	Jorge Vivaldi;Horacio Rodríguez	2014	2014 Third IEEE International Colloquium in Information Science and Technology (CIST)	10.1109/CIST.2014.7016627	natural language processing;computer science;data mining;information retrieval	NLP	-31.062312671833805	-66.89394631952834	168572
758c3a5d9a6e76c0152214a7ce7521d611ced1cd	mining name translations from entity graph mapping	transliteration-based approach;bilingual co-occurrences;monolingual corpus;corpus-based work;monolingual co-occurrences;monolingual entity co-occurrences;corpus-based approach;entity search engine;mining name translation;mining entity translation;holistic approach;entity graph mapping	This paper studies the problem of mining entity translation, specifically, mining English and Chinese name pairs. Existing efforts can be categorized into (a) a transliterationbased approach leveraging phonetic similarity and (b) a corpus-based approach exploiting bilingual co-occurrences, each of which suffers from inaccuracy and scarcity respectively. In clear contrast, we use unleveraged resources of monolingual entity co-occurrences, crawled from entity search engines, represented as two entity-relationship graphs extracted from two language corpora respectively. Our problem is then abstracted as finding correct mappings across two graphs. To achieve this goal, we propose a holistic approach, of exploiting both transliteration similarity and monolingual co-occurrences. This approach, building upon monolingual corpora, complements existing corpus-based work, requiring scarce resources of parallel or comparable corpus, while significantly boosting the accuracy of transliteration-based work. We validate our proposed system using real-life datasets.	algorithm;categorization;emoticon;entity–relationship model;holism;real life;text corpus;web search engine	Gae-won You;Seung-won Hwang;Young-In Song;Long Jiang;Zaiqing Nie	2010			natural language processing;speech recognition;computer science;information retrieval	AI	-28.310885035201316	-69.77337943566968	168841
7e7f34cf9a8398d31abc0f4f21ba04d3732e4e8f	lextec ― a rich language resource for technical domains in portuguese		The growing amount of available information and the importance given to the access to technical information enhance the potential role of NLP applications in enabling users to deal with information for a variety of knowledge domains. In this process, language resources are crucial. This paper presents Lextec, a rich computational language resource for technical vocabulary in Portuguese. Encoding a representative set of terms for ten different technical domains, this concept-based relational language resource combines a wide range of linguistic information by integrating each entry in a domain-specific wordnet and associating it with a precise definition for each lexicalization in the technical domain at stake, illustrative texts and information for translation into English.	natural language processing;vocabulary;wordnet	Palmira Marrafa;Raquel Amaro;Sara Mendes	2014				NLP	-31.89383901594241	-71.11619507810427	170219
688c494efe456f0cf5598beaf7f5ff198aac1017	building a common pipeline for rule-based document classification	data mining;classification;natural language processing	Instance-based classification of clinical text is a widely used natural language processing task employed as a step for patient classification, document retrieval, or information extraction. Rule-based approaches rely on concept identification and context analysis in order to determine the appropriate class. We propose a five-step process that enables even small research teams to develop simple but powerful rule-based NLP systems by taking advantage of a common UIMA AS based pipeline for classification. Our proposed methodology coupled with the general-purpose solution provides researchers with access to the data locked in clinical text in cases of limited human resources and compact timelines.	apache uima;document classification;document retrieval;general-purpose modeling;information extraction;logic programming;natural language processing;patients;timeline;teams	Olga V Patterson;Thomas Ginter;Scott L. DuVall	2013	Studies in health technology and informatics	10.3233/978-1-61499-289-9-1211	pattern recognition;data mining;information retrieval;library classification	AI	-31.341811646474508	-69.71163244926912	170358
0f543c637318c7fcb12025a88e6a378e7313919b	probabilistic evaluation of process model matching techniques	004 informatik	Process model matching refers to the automatic identification of corresponding activities between two process models. It represents the basis for many advanced process model analysis techniques such as the identification of similar process parts or process model search. A central problem is how to evaluate the performance of process model matching techniques. Often, not even humans can agree on a set of correct correspondences. Current evaluation methods, however, require a binary gold standard, which clearly defines which correspondences are correct. The disadvantage of this evaluation method is that it does not take the true complexity of the matching problem into account and does not fairly assess the capabilities of a matching technique. In this paper, we propose a novel evaluation method for process model matching techniques. In particular, we build on the assessment of multiple annotators to define probabilistic notions of precision and recall. We use the dataset and the results of the Process Model Matching Contest 2015 to assess and compare our evaluation method. We find that our probabilistic evaluation method assigns different ranks to the matching techniques from the contest and allows to gain more detailed insights into their performance.	automatic identification and data capture;f1 score;lazy evaluation;precision and recall;process modeling;utility;whole earth 'lectronic link	Elena Kuss;Henrik Leopold;Han van der Aa;Heiner Stuckenschmidt;Hajo A. Reijers	2016		10.1007/978-3-319-46397-1_22	computer science;machine learning;data mining	Web+IR	-28.910898389823032	-69.09901997383237	170682
991fb4a612c7f387f8eadb730164f2ee95fdca72	evaluation of automatic hypernym extraction from technical corpora in english and dutch	languages and literatures;terminology;hypernym extraction;semantic relations	In this research, we evaluate different approaches for the automatic extraction of hypernym relations from English and Dutch technical text. The detected hypernym relations should enable us to semantically structure automatically obtained term lists from domainand userspecific data. We investigated three different hypernymy extraction approaches for Dutch and English: a lexico-syntactic pattern-based approach, a distributional model and a morpho-syntactic method. To test the performance of the different approaches on domain-specific data, we collected and manually annotated English and Dutch data from two technical domains, viz. the dredging and financial domain. The experimental results show that especially the morpho-syntactic approach obtains good results for automatic hypernym extraction from technical and domain-specific texts.	automatic control;domain-specific language;lexico;text corpus;viz: the computer game	Els Lefever;Marjan Van de Kauter;Véronique Hoste	2014			natural language processing;computer science;data mining;linguistics;terminology;information retrieval	NLP	-29.79895839660273	-72.41610875908452	171392
78c9626e288a767362fdc64d0996015a794a215d	purposenet: a knowledge base organized around purpose		We show how purpose can be used as a central guiding principle for organizing knowledge about artifacts. It allows the actions in which the artifact participates to be related naturally to other objects. Similarly, the structure or parts of artifact can also be related to actions. A knowledgebase called PurposeNet has been built using these principles. A comparison with other knowledgebases shows that it is a superior method in terms of coverage. It also makes it possible for automatic extraction of simple facts (or information) from text for populating a richly structured knowledgebase. An experiment in domain-specific question-answering from a given passage shows that PurposeNet used along with scripts (or knowledge of stereotypical situations), can lead to substantially higher accuracy in question answering. In the domain of car racing, individually they produce correct answers to 50% and 37.5% questions respectively, but together they produce 89% of correct answers.	artifact (software development);knowledge base;organizing (structure);population;question answering	Rajeev Sangal;Soma Paul;P. Kiran Mayee	2013		10.1007/978-3-642-35786-2_3	knowledge base;computer science;knowledge management;artificial intelligence;knowledge-based systems;machine learning;data mining;knowledge extraction;world wide web;algorithm	NLP	-32.437130737989946	-69.48529144398123	171508
93646e0d3619b744511ccbb3d37f56dc65598408	question answering over linked data: what is difficult to answer? what affects the f scores?		We present a fine-grained analysis of the Question Answering over Linked Data (QALD-6) challenge. We divide the QALD-6 questions into 8 main categories and compare state-of-the-art questions answering (QA) systems over Linked Data against the individual categories. We show the difficulty (in terms of overall F scores of the QA systems) of each category. We show the effect of various natural language and SPARQL features such as the number of triple patterns, number of keywords, the answer size, the type of answers, the effect of aggregate functions, and the SPARQL query forms on the overall F scores of the QA systems.	aggregate data;linked data;natural language;question answering;sparql;software quality assurance;triplestore	Muhammad Saleem;Samaneh Nazari Dastjerdi;Ricardo Usbeck;Axel-Cyrille Ngonga Ngomo	2017			linked data;information retrieval;data mining;question answering;computer science	NLP	-28.386948421159264	-66.18002968901577	171557
65c058602990317d56612ccaeebbd0187892e91f	extrinsic corpus evaluation with a collocation dictionary task	corpus;collocation;evaluation	The NLP researcher or application-builder often wonders “what corpus should I use, or should I build one of my own? If I build one of my own, how will I know if I have done a good job?” Currently there is very little help available for them. They are in need of a framework for evaluating corpora. We develop such a framework, in relation to corpora which aim for good coverage of ‘general language’. The task we set is automatic creation of a publicationquality collocations dictionary. For a sample of 100 headwords of Czech and 100 of English, we identify a gold standard dataset of (ideally) all the collocations that should appear for these headwords in such a dictionary. The datasets are being made available alongside this paper. We then use them to determine precision and recall for a range of corpora, with a range of parameters.	collocation;dictionary;headword;natural language processing;precision and recall;text corpus	Adam Kilgarriff;Pavel Rychlý;Milos Jakubícek;Vojtech Kovár;Vít Baisa;Lucia Kocincová	2014			natural language processing;speech recognition;collocation;computer science;evaluation;data mining;linguistics	NLP	-29.645080301001173	-72.9269482939999	172262
0e6035917472431d057cb0aa164c95cd20f1e767	generating and using probabilistic morphological resources for the biomedical domain		In most Indo-European languages, many biomedical terms are rich morphological structures composed of several constituents mainly originating from Greek or Latin. The interpretation of these compounds are keystones to access information. In this paper, we present morphological resources aiming at coping with these biomedical morphological compounds. Following previous work (Claveau and Kijak, 2011; Claveau, 2012), these resources are automatically built using Japanese terms in Kanjis as a pivot language and alignment techniques. We show how these alignment information can be used for segmenting compounds, attaching semantic interpretation to each part, proposing definitions (gloses) of the compounds... When possible, these tasks are compared with state-of-the-art tools, and the results show the interest of our automatically built probabilistic resources.	indo;semantic interpretation	Vincent Claveau;Ewa Kijak	2014			artificial intelligence;natural language processing;market segmentation;semantic interpretation;computer science;probabilistic logic;pivot language	NLP	-29.479842497558824	-71.43702389608555	172574
d0b7ed414d7922cce47a295b111b7d449b8cfdd3	web as a corpus: going beyond the n-gram	syntactic parsing;noun phrase coordination;noun compound bracketing;web as a corpus;prepositional phrase attachment;paraphrases;surface features	The 60-year-old dream of computational linguistics is to make computers capable of communicating with humans in natural language. This has proven hard, and thus research has focused on sub-problems. Even so, the field was stuck with manual rules until the early 90s, when computers became powerful enough to enable the rise of statistical approaches. Eventually, this shifted the main research attention to machine learning from text corpora, thus triggering a revolution in the field. Today, the Web is the biggest available corpus, providing access to quadrillions of words; and, in corpus-based natural language processing, size does matter. Unfortunately, while there has been substantial research on the Web as a corpus, it has typically been restricted to using page hit counts as an estimate for n-gram word frequencies; this has led some researchers to conclude that the Web should be only used as a baseline. We show that much better results are possible for structural ambiguity problems, when going beyond the n-gram.		Preslav Nakov	2014		10.1007/978-3-319-25485-2_5	natural language processing;nominalization;noun phrase;speech recognition;determiner phrase;linguistics	Web+IR	-27.23353893889332	-72.73329858989929	173182
3348d7c09f0a9eea2f8a530d73f4200d161cb650	an ontology-based approach to learnable focused crawling	focused crawling;web pages;ontology focused crawling;unified medical language system;learnable focused crawling;knowledge representation;breadth first search;ontology;domain specificity;harvest rate;artificial neural network	Focused crawling is aimed at selectively seeking out pages that are relevant to a predefined set of topics. Since an ontology is a well-formed knowledge representation, ontology-based focused crawling approaches have come into research. However, since these approaches utilize manually predefined concept weights to calculate the relevance scores of web pages, it is difficult to acquire the optimal concept weights to maintain a stable harvest rate during the crawling process. To address this issue, we proposed a learnable focused crawling framework based on ontology. An ANN (artificial neural network) was constructed using a domain-specific ontology and applied to the classification of web pages. Experimental results show that our approach outperforms the breadth-first search crawling approach, the simple keyword-based crawling approach, the ANN-based focused crawling approach, and the focused crawling approach that uses only a domain-specific ontology.	artificial intelligence;focused crawler;ontology (information science)	Hai-Tao Zheng;Bo-Yeong Kang;Hong-Gee Kim	2008	Inf. Sci.	10.1016/j.ins.2008.07.030	breadth-first search;computer science;artificial intelligence;machine learning;ontology;web page;data mining;unified medical language system;information retrieval;artificial neural network	AI	-31.627713930678134	-67.2883548782889	173223
c5a57c2c96960e544b70691f825b5becc8945ac2	computing the data semantics of wsdl specifications via gradient boosting	data semantics	This paper proposes a method for the semi-automatic semantic annotation of WSDL specifications, given ontologies related to the domain of services. The proposed method uses a synthesis of mapping methods to map input/output messages' parameters to ontology classes. Exploiting validated results provided by humans, the method learns via the gradient boosting learning algorithm to combine the individual mapping methods towards improving its accuracy. The aim is to mitigate difficulties concerning mappings and address limitations of other approaches, even in challenging cases, so as to assist human annotators to perform their work. The paper presents experimental results of the proposed methods.	gradient boosting;web services description language	Alexandros G. Valarakos;George A. Vouros	2010		10.3233/978-1-60750-606-5-503	computer science;theoretical computer science;machine learning;data mining;database	DB	-31.62489910492194	-67.53710309509006	173459
ca496aea6927e444ad2eaddd079cd2d9cc9a4bd3	a linguistic light approach to multilingualism in lexical layers for ontologies	databases;analytical models;language resources lexical layers semantic web ontologies text analytics applications ontology based information extraction internal conceptual data structures subsequent knowledge acquisition;information retrieval;ontologies artificial intelligence;ontologies information analysis natural languages semantic web data mining spine data structures engines knowledge acquisition data engineering;automata;indexes;semantic web information retrieval knowledge acquisition linguistics ontologies artificial intelligence;knowledge acquisition;target language;ontology engineering;semantic web;ontologies;data structure;lexical analysis;knowledge base;knowledge engineering;linguistics	Semantic Web ontologies are being increasingly used in modern text analytics applications and ontology-based information extraction (OBIE) as a means to provide a semantic backbone either for modelling the internal conceptual data structures of the text analytics (TA) engine or to model the knowledge base to drive the analysis of unstructured information in raw text and subsequent Knowledge acquisition and population. creating and targeting language resources (LR)s from a TA to an ontology can be time consuming and costly.The authors describe a user-friendly method for ontology engineers to augment an ontologies with a lexical layer which provides a flexible framework to identify term mentions of ontology concepts in raw text. In this paper we explore multilinguality in these lexical layers using the same framework. We discuss a number of potential issues for the ldquolinguistic lightrdquo lexical extensions for ontologies (LEON) approach when looking at languages more morphologically rich and which have more complex linguistic constraints than English. We show how the LEON approach can cope with these phenomena once the morphological normaliser used in the lexical analysis process is able to generalise sufficiently well for the language concerned.	data structure;information extraction;internet backbone;knowledge acquisition;knowledge base;leon;lexical analysis;ontology (information science);semantic web;text mining;usability	Alexander Troussov;John Judge;Mikhail Sogrin;Amine Akrout;Brian Davis;Siegfried Handschuh	2008	2008 International Multiconference on Computer Science and Information Technology	10.1109/IMCSIT.2008.4747268	natural language processing;upper ontology;database index;knowledge base;data structure;lexical analysis;computer science;ontology;artificial intelligence;semantic web;knowledge engineering;data mining;database;automaton;information retrieval;process ontology;semantic analytics	NLP	-32.77099669217444	-68.5591090729916	174783
7175e717134c72160f7835a8c8ae8ce95945b5ea	construction of an infrastructure for providing users with suitable language resources	article publisher;language resources;large scale;dublin core	Our research organization has been constructing a large scale database named SHACHI by collecting detailed meta information on language resources (LRs) in Asia and Western countries. The metadata database contains more than 2,000 compiled LRs such as corpora, dictionaries, thesauruses and lexicons, forming a large scale metadata of LRs archive. Its metadata, an extended version of OLAC metadata set conforming to Dublin Core, have been collected semi-automatically. This paper explains the design and the structure of the metadata database, as well as the realization of the catalogue search tool.	archive;compiler;dictionary;dublin core;lexicon;semiconductor industry;text corpus;thesaurus	Hitomi Tohyama;Shunsuke Kozawa;Kiyotaka Uchimoto;Shigeki Matsubara;Hitoshi Isahara	2008			natural language processing;computer science;database;database catalog;metadata;world wide web;meta data services;information retrieval;metadata repository	NLP	-33.00023983840404	-72.62647410422166	175121
531afb3430accfc24b28d2a946389ae530726804	how much should you ask? on the question structure in qa systems.		Datasets that boosted state-of-the-art solutions for Question Answering (QA) systems prove that it is possible to ask questions in natural language manner. However, users are still used to query-like systems where they type in keywords to search for answer. In this study we validate which parts of questions are essential for obtaining valid answer. In order to conclude that, we take advantage of LIME - a framework that explains prediction by local approximation. We find that grammar and natural language is disregarded by QA. State-of-the-art model can answer properly even if u0027askedu0027 only with a few words with high coefficients calculated with LIME. According to our knowledge, it is the first time that QA model is being explained by LIME.	approximation;coefficient;natural language;question answering;software quality assurance	Dominika Basaj;Barbara Rychalska;Przemysław Biecek;Anna Wróblewska	2018	CoRR		natural language processing;machine learning;artificial intelligence;natural language;ask price;question answering;computer science;grammar	NLP	-27.279737703931374	-70.85547190480835	175122
5d70b7ccd49066176dda02298e577950ff1e6fc1	keyword extraction from arabic legal texts	grammar;information systems;information extraction;knowledge management systems;vocabulary;knowledge management;human indexing;periodicals;foreign countries;validity;indexing;keyword extraction;semitic languages;local grammar;computational linguistics	Purpose – The purpose of this paper is to apply local grammar (LG) to develop an indexing system which automatically extracts keywords from titles of Lebanese official journals. Design/methodology/approach – To build LG for our system, the first word that plays the determinant role in understanding the meaning of a title is analyzed and grouped as the initial state. These steps are repeated recursively for the whole words. As a new title is introduced, the first word determines which LG should be applied to suggest or generate further potential keywords based on a set of features calculated for each node of a title. Findings – The overall performance of our system is 67 per cent, which means that 67 per cent of the keywords extracted manually have been extracted by our system. This empirical result shows the validity of this study’s approach after taking into consideration the below-mentioned limitations. Research limitations/implications – The system has two limitations. First, it is applied to a sample of 5,747 titles and it can be developed to generate all finite state automata for all titles. The other limitation is that named entities are not processed due to their varieties that require specific ontology. Originality/value – Almost all keyword extraction systems apply statistical, linguistic or hybrid approaches to extract keywords from texts. This paper contributes to the development of an automatic indexing system to replace the expensive human indexing by taking advantages of LG, which is mainly applied to extract time, date and proper names from texts.	automata theory;finite-state machine;keyword extraction;named entity;named-entity recognition;recursion;thesaurus;vocabulary	Mahmoud Rammal;Zeinab Bahsoun;Mona Al Achkar Jabbour	2015	Interact. Techn. Smart Edu.	10.1108/ITSE-11-2013-0030	natural language processing;search engine indexing;speech recognition;computer science;computational linguistics;grammar;semitic languages;linguistics;communication;world wide web;information extraction;information system;validity	NLP	-29.984428809059033	-68.86919557025057	175740
a7341ceb0239b91225d0f7c8bf1963310439ab35	semantic representation analysis: a general framework for individualized, domain-specific and context-sensitive semantic processing		Language agnostic methods for semantic extraction, encoding, and applications are an increasingly active research area in computational linguistics. This paper introduces an analytic framework for vector-based semantic representation called semantic representation analysis (SRA). The rationale for this framework is considered, as well as some successes and future challenges that must be addressed. A cloud-based implementation of SRA as a domain-specific semantic processing portal has been developed. Applications of SRA in three different areas are discussed: analysis of online text streams, analysis of the impression formation over time, and a virtual learning environment called V-CAEST that is enhanced by a conversation-based intelligent tutoring system. These use-cases show the flexibility of this approach across domains, applications, and languages.		Xiangen Hu;Benjamin Nye;Chuang Gao;Xudong Huang;Jun Xie;Keith T. Shubeck	2014		10.1007/978-3-319-07527-3_4	semantic data model;natural language processing;semantic interoperability;semantic similarity;semantic computing;multinet;semantic integration;explicit semantic analysis;semantic search;semantic grid;social semantic web;data mining;semantic compression;semantic equivalence;semantic technology;information retrieval;semantic analytics;semantic gap	NLP	-33.59014296963049	-67.06115536267096	175939
a1acc7e33d961873c46e59c7c0a44bf042bb54aa	multidocument question answering text summarization using topic signatures	text summarization;question answering			Maria Biryukov;Roxana Angheluta;Marie-Francine Moens	2005	JDIM		text graph;computer science;information retrieval	NLP	-27.34667642384336	-66.3383037489687	176085
329947aa599c968941ffc17a3f0df9f632ab9a62	unsupervised software-specific morphological forms inference from informal discussions		Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.	correctness (computer science);dictionary;knowledge base;lemmatisation;natural language processing;outline of software;plasma cleaning;software engineering;spell checker;stack overflow;stemming;text normalization;thesaurus;vocabulary;wordnet	Chunyang Chen;Zhenchang Xing;Ximing Wang	2017	2017 IEEE/ACM 39th International Conference on Software Engineering (ICSE)		natural language processing;computer science;data mining;database;programming language	SE	-29.13139746879587	-71.74434469993706	176391
e15adb74863254f944d047a372c02287041fd546	word sense disambiguation using extended relevant domains resource		An improved document registration system in a copier having a platen for exposing documents includes a registration edge at the forward end of the platen and at least one foam document aligning belt located adjacent to and above the registration edge. The alignment belt is adapted to extend a short distance over and conform to the registration edge so that documents can be driven toward the registration edge with positive control and thereby reduce up-curl and registration edge jumping.	word sense;word-sense disambiguation	Sonia Vázquez;Andrés Montoyo;Zornitsa Kozareva	2007			natural language processing;word-sense disambiguation;semeval;artificial intelligence;computer science	NLP	-28.86430001326283	-70.96917307123076	176516
fba28688404c090661d29cd8b90acf1cb74d959c	which coreference evaluation metric do you trust? a proposal for a link-based entity aware metric		Interpretability and discriminative power are the two most basic requirements for an evaluation metric. In this paper, we report the mention identification effect in the B3, CEAF, and BLANC coreference evaluation metrics that makes it impossible to interpret their results properly. The only metric which is insensitive to this flaw is MUC, which, however, is known to be the least discriminative metric. It is a known fact that none of the current metrics are reliable. The common practice for ranking coreference resolvers is to use the average of three different metrics. However, one cannot expect to obtain a reliable score by averaging three unreliable metrics. We propose LEA, a Link-based Entity-Aware evaluation metric that is designed to overcome the shortcomings of the current evaluation metrics. LEA is available as branch LEA-scorer in the reference implementation of the official CoNLL scorer.	addressing mode;entity;expect;flaw hypothesis methodology;message understanding conference;precision and recall;reference implementation;requirement	Nafise Sadat Moosavi;Michael Strube	2016			artificial intelligence;computer science;natural language processing;data mining;coreference	Web+IR	-28.751431140046275	-69.13642238189007	176607
03d05feb242e36a4102d83c3c64e26942997c48f	use of domain knowledge in the automatic extraction of structured representations from patient-related texts	semantic representation;information extraction;domain knowledge;structured data	Domain knowledge is essential resource in Information Extraction (IE) from free text since it supports the decisions about structuring the extracted text objects into domain statements. Thus manually-created conceptual structures enable the semantic representation of textual information. This paper discusses the role of domain knowledge in information extraction of structured data from patient related texts. The article shows that domain knowledge is encoded not only in the conceptual structures, which provide the ontological framework for the IE task, but also in the IE templates that are designed to capture domain semantics. A prototype system and IE examples of domain knowledge usage are considered together with results of the current prototype evaluation.		Galia Angelova	2010		10.1007/978-3-642-14197-3_6	domain analysis;natural language processing;relationship extraction;data model;computer science;artificial intelligence;body of knowledge;domain engineering;data mining;knowledge extraction;information extraction;information retrieval;domain knowledge	NLP	-32.39715358976757	-68.74999337255773	176760
e9f80bb1352cb9f725fd2e629eb21b021b473580	using question-answer pairs in extractive summarization of email conversations	sentence extraction;automatic detection;question answering	While sentence extraction as an approach to summarization has been shown to work in documents of certain genres, because of the conversational nature of email communication, sentence extraction may not result in a coherent summary. In this paper, we present our work on augmenting extractive summaries of threads of email conversations with automatically detected question-answer pairs. We compare various appraoches to integrating questionanswer pairs in the extractive summaries, and show that their use improves the quality of email summaries. We also describe the email summarization interface we have developed that allows users to summarize email conversations from email clients such as Mircosoft Outlook.	automatic summarization;coherence (physics);html email;microsoft outlook for mac;sentence extraction	Kathleen McKeown;Lokesh Shrestha;Owen Rambow	2007		10.1007/978-3-540-70939-8_48	natural language processing;question answering;computer science;data mining;information retrieval	NLP	-27.519027495713225	-70.01899957366005	177189
fc9199162332076cfe219e30d4dc33a17d75df79	trans-ez at ntcir-2 : synset co-occurrence method for english-chinese cross-lingual information retrieval		In this paper, a new method for English-Chinese cross-lingual information retrieval is proposed and evaluated in NTCIR-II project. We use the bilingual resources and contextual information to deal with the word sense disambiguation (WSD) and translation disambiguation for query translation. An EnglishChinese WordNet and a synset co-occurrence model are adopted to solve the problem of word sense ambiguity. And the translation ambiguity and target polysemy are also resolved using such co-occurrence relationship of synsets. The experimental results are discussed to analyze the effects of ambiguity in source language and target language.	compiler;cross-language information retrieval;entity–relationship model;european conference on information retrieval;experiment;synonym ring;word sense;word-sense disambiguation;wordnet	Guo-Wei Bian;Chi-Ching Lin	2001			co-occurrence;word sense;information retrieval;ambiguity;polysemy;wordnet;word-sense disambiguation;computer science	NLP	-28.596263756623827	-69.4260907437693	177828
c7e80174e5c49c94bd6051fc7589381710ca91f9	interpretation of nominal compounds: combining domain-independent and domain-specific information	extensive use;semantic characteristic;domain-specific semantic information;nominal constituent;automated interpretation;generalizable semantic principle;predicative information;nominal compound;complementary semantic information;domain independent model;domain-specific information	A domain independent model is proposed for the automated interpretation of nominal compounds in English. This model is meant to account for productive rules of interpretation which are inferred from the morpho-syntact ic and semantic characteristics of the nominal constituents. In particular, we make extensive use of Pustejovsky's principles concerning the predicative information associated with nominals. We argue that it is necessary to draw a line between generalizable semantic principles and domainspecific semantic information. We explain this distinction and we show how this model may be applied to the interpretat ion of compounds in real texts, provided that complementary semantic information are retrieved.	domain-specific language;impredicativity;james pustejovsky	Cécile Fabre	1996			natural language processing;semantic interpretation;semantic similarity;computer science;information retrieval	NLP	-28.284272742128977	-72.02449841071814	178162
aec3ac49ebfbd46b5c9cab60d2a64c720fa798d3	a multi-strategy approach for lexicalizing linked open data		This paper aims at exploiting Linked Data for generating natural text, often referred to as lexicalization. We propose a framework that can generate patterns which can be used to lexicalize Linked Data triples. Linked Data is structured knowledge organized in the form of triples consisting of a subject, a predicate and an object. We use DBpedia as the Linked Data source which is not only free but is currently the fastest growing data source organized as Linked Data. The proposed framework utilizes the Open Information Extraction (OpenIE) to extract relations from natural text and these relations are then aligned with triples to identify lexicalization patterns. We also exploit lexical semantic resources which encode knowledge on lexical, semantic and syntactic information about entities. Our framework uses VerbNet and WordNet as semantic resources. The extracted patterns are ranked and categorized based on the DBpedia ontology class hierarchy. The pattern collection is then sorted based on the score assigned and stored in an index embedded database for use in the framework as well as for future lexical resource. The framework was evaluated for syntactic accuracy and validity by measuring the Mean Reciprocal Rank (MRR) of the first correct pattern. The results indicated that framework can achieve 70.36% accuracy and a MRR value of 0.72 for five DBpedia ontology classes generating 101 accurate lexicalization patterns.	categorization;class hierarchy;dbpedia;data mining;display resolution;encode;embedded database;embedded system;entity;fastest;gene ontology term enrichment;information extraction;linked data;tag cloud;verbnet;wordnet	Rivindu Perera;Parma Nand	2015		10.1007/978-3-319-18117-2_26	natural language processing;computer science;data mining;database;programming language;information retrieval	NLP	-29.517110282468483	-66.38319799538974	178389
5e79701fdda39f0a7d255739cd9a138451da7676	robust semantic construction	robust semantic construction	Recent years have seen a surge in interest for robust at analysis, i.e. NLP systems with fairly limited supply of linguistic knowledge but with vast coverage. The paper describes a module that serves as a back-end to such at analysis methods and transforms their output into full semantic representations as constructed by deep analysis methods. In particular, the module has been designed so as to process input from	natural language processing	Michael Schiehlen	2000			natural language processing;semantic interoperability;semantic similarity;semantic computing;multinet;semantic integration;semantic web rule language;explicit semantic analysis;semantic search;semantic grid;computer science;social semantic web;pattern recognition;semantic web stack;semantic compression;semantic equivalence;probabilistic latent semantic analysis;information retrieval;semantic gap	Web+IR	-30.86669063431343	-67.5292767101968	178494
39a866179e2c4b06b83f0c71430defc988c74ca7	identifying and analyzing brazilian portuguese complex predicates	limited list;annotated expression;semantic role;brazilian portuguese;verb construction;pos tag;nlp tool;complex predicates;original resource;annotation task;brazilian portuguese complex predicate	Semantic Role Labeling annotation task depends on the correct identification of predicates, before identifying arguments and assigning them role labels. However, most predicates are not constituted only by a verb: they constitute Complex Predicates (CPs) not yet available in a computational lexicon. In order to create a dictionary of CPs, this study employs a corpus-based methodology. Searches are guided by POS tags instead of a limited list of verbs or nouns, in contrast to similar studies. Results include (but are not limited to) light and support verb constructions. These CPs are classified into idiomatic and less idiomatic. This paper presents an in-depth analysis of this phenomenon, as well as an original resource containing a set of 773 annotated expressions. Both constitute an original and rich contribution for NLP tools in Brazilian Portuguese that perform tasks involving semantics.	brown corpus;dictionary;lexicon;natural language processing;point of sale;programming idiom;semantic role labeling;text corpus	Magali Sanches Duran;Carlos Ramisch;Sandra M. Aluísio;Aline Villavicencio	2011			natural language processing;computer science;linguistics;communication	NLP	-28.577297843398085	-73.07866643892856	178764
47e34da73faae407d353873923a47861177bbbaf	answering and questioning for machine reading	study design;evaluation methodology;elementary school;natural language processing	Machine reading can be defined as the automatic understanding of text. One way in which human understanding of text has been gauged is to measure the ability to answer questions pertaining to the text. In this paper, we present a brief study designed to explore how a natural language processing component for the recognition of textual entailment bears on the problem of answering questions in a basic, elementary school reader. An alternative way of testing human understanding is to assess one's ability to ask sensible questions for a given text. We survey current computational systems that are capable of generating questions automatically, and suggest that understanding must comprise not only a grasp of semantic equivalence, but also an assessment of the importance of information conveyed by the test. We suggest that this observation should contribute in the design of an overall evaluation methodology for machine reading.	anaphora (linguistics);commonsense knowledge (artificial intelligence);computation;natural language processing;natural language understanding;question answering;textual entailment;trusted computer system evaluation criteria;turing completeness	Lucy Vanderwende	2007			natural language processing;computer science;artificial intelligence;machine learning;clinical study design	NLP	-28.077479625298423	-72.61785610870659	178817
2b0c45601569e534c9d872403cdd6f41fb0560e9	domain-specific query translation for multilingual information access using machine translation augmented with dictionaries mined from wikipedia	information retrieval	Accurate high-coverage translation is a vital component of reliable cross language information access (CLIA) systems. While machine translation (MT) has been shown to be effective for CLIA tasks in previous evaluation workshops, it is not well suited to specialized tasks where domain specific translations are required. We demonstrate that effective query translation for CLIA can be achieved in the domain of cultural heritage (CH). This is performed by augmenting a standard MT system with domainspecific phrase dictionaries automatically mined from the online Wikipedia. Experiments using our hybrid translation system with sample query logs from users of CH websites demonstrate a large improvement in the accuracy of domain specific phrase detection and translation.	dictionary;domain-specific language;information access;information retrieval;machine translation;mined;wikipedia	Gareth J. F. Jones;Fabio Fantino;Eamonn Newman	2008			computer-assisted translation;natural language processing;computer science;machine translation;rule-based machine translation;machine translation software usability;world wide web;information retrieval	NLP	-32.253807027994185	-72.54179802848644	179723
effe9198a7e181a8653cb917fbb1f0cc74a2ce7a	hierarchical context supplementation for consecutive question answering		Question-answering (QA) systems have recently shown impressive results in terms of accurately answering user questions in such situations as domain specific user questions. However, we have identified many real situations where QA systems must cope with not a single question-answering situation but rather a sequence of consecutive questions. In such cases, users often ask questions on the basis of the previous answer they have received, so the context of the questions changes on a certain level. The commonly used method to handle this problem when using a QA system is to append the current question to the previous question (append method). However, the append method is not designed to detect such context changes. To deal with such context changes, we have designed a hierarchical context supplementation QA System (HCSQ). The HCSQ handles consecutive questions by matching the current question with the hierarchical domain knowledge database structure of the previous answer and then supplements the context of the current question with the required keywords. We also show that our method can be further applied to the initial question to supplement omitted context. Experimental results show that our method substantially outperforms the state-of-the-art methods.	append;natural language;question answering	Kohtaroh Miyamoto;Hironori Takeuchi;Satoshi Masuda;Futoshi Iwama	2017	2017 IEEE International Conference on Service Operations and Logistics, and Informatics (SOLI)	10.1109/SOLI.2017.8120959	marketing;data mining;domain knowledge;question answering;engineering;append;training set	SE	-26.620592321504116	-69.50408716270317	180032
218ec099f74b9492a4ce749a51e27bd3ec1e9e0b	suggesting missing relations in biomedical ontologies based on lexical regularities		The number of biomedical ontologies has increased significantly in recent years. Many of such ontologies are the result of efforts of communities of domain experts and ontology engineers. The development and application of quality assurance (QA) methods should help these communities to develop useful ontologies for both humans and machines. According to previous studies, biomedical ontologies are rich in natural language content, but most of them are not so rich in axiomatic terms. Here, we are interested in studying the relation between content in natural language and content in axiomatic form. The analysis of the labels of the classes permits to identify lexical regularities (LRs), which are sets of words that are shared by labels of different classes. Our assumption is that the classes exhibiting an LR should be logically related through axioms, which is used to propose an algorithm to detect missing relations in the ontology. Here, we analyse a lexical regularity of SNOMED CT, congenital stenosis, which is reported as problematic by the SNOMED CT maintenance team.	arterial pulse quality:type:pt:xxx:nom:palpation;axiomatic system;class;community;engineering;exhibits as topic;lr parser;lexical group unique identifier;lexicon;license;natural language;ontology (information science);pet/ct scan;software quality assurance;subject-matter expert;systematized nomenclature of medicine;algorithm	Manuel Quesada-Martínez;Jesualdo Tomás Fernández-Breis;Daniel Karlsson	2016	Studies in health technology and informatics	10.3233/978-1-61499-678-1-384	knowledge management;open biomedical ontologies;ontology;ontology (information science);medicine	AI	-33.23075993835996	-70.08160907652847	180268
1e2b5c2964810de196081042c62a4d612388e8ec	a systematic study on latent semantic analysis model parameters for mining biomedical literature	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;latent semantic analysis;europe pmc;biomedical research;microarrays;bioinformatics	Background and rationale Latent semantic analysis (LSA) is considered to be an efficient text mining technique [1] but most approaches developed on this paradigm are based on adhoc principles. A systematic study on the parameters affecting the performance of LSA is expected to provide guidelines to objectively select the LSA model parameters in a way that is consistent with the data and the application. In this study, empirical analyses were conducted using a previously published 50 gene data set [2] to examine the effects of the following parameters (outlined in Figure 1): Parameters are: (i) stemming, stop-words and word counts (to discard abstract with not enough information), (ii) corpus content (e.g., abstracts with and without titles), (iii) inclusion or exclusion of the dc component or 1st Eigen vector (that adds bias to the model), (iv) objective criteria to choose the number of factors (Eigen vectors) to create the model, (v) information theoretic criteria to select features (words in the corpus) instead of considering complete set of features.	design rationale;eigen (c++ library);emoticon;information theory;latent semantic analysis;programming paradigm;stemming;text corpus;text mining	Mohammed Yeasin;Haritha Malempati;Ramin Homayouni;Mohammad S. Sorower	2009		10.1186/1471-2105-10-S7-A6	biology;dna microarray;latent semantic analysis;computer science;bioinformatics;data science;data mining;probabilistic latent semantic analysis	NLP	-27.874511119455995	-67.86820150327884	180573
b6b6d2504fd57d27a0467654fa62169cc7dedbdd	an overview of microsoft academic service (mas) and applications	academic search;entity conflation;recommender systems	In this paper we describe a new release of a Web scale entity graph that serves as the backbone of Microsoft Academic Service (MAS), a major production effort with a broadened scope to the namesake vertical search engine that has been publicly available since 2008 as a research prototype. At the core of MAS is a heterogeneous entity graph comprised of six types of entities that model the scholarly activities: field of study, author, institution, paper, venue, and event. In addition to obtaining these entities from the publisher feeds as in the previous effort, we in this version include data mining results from the Web index and an in-house knowledge base from Bing, a major commercial search engine. As a result of the Bing integration, the new MAS graph sees significant increase in size, with fresh information streaming in automatically following their discoveries by the search engine. In addition, the rich entity relations included in the knowledge base provide additional signals to disambiguate and enrich the entities within and beyond the academic domain. The number of papers indexed by MAS, for instance, has grown from low tens of millions to 83 million while maintaining an above 95% accuracy based on test data sets derived from academic activities at Microsoft Research. Based on the data set, we demonstrate two scenarios in this work: a knowledge driven, highly interactive dialog that seamlessly combines reactive search and proactive suggestion experience, and a proactive heterogeneous entity recommendation.	data mining;entity;internet backbone;knowledge base;lionsolver;microsoft research;prototype;scalability;test data;venue (sound system);web index;web search engine;world wide web;dialog	Arnab Sinha;Zhihong Shen;Yang Song;Hao Ma;Darrin Eide;Bo-June Paul Hsu;Kuansan Wang	2015		10.1145/2740908.2742839	computer science;data mining;database;world wide web;recommender system	Web+IR	-32.34569060948861	-66.47613207058168	180831
3e75ebce191c3554b1fb7b34db99e352b0521d49	a simple method for citation metadata extraction using hidden markov models	hidden markov model;digital library;citation management;metadata extraction;viterbi algorithm	This paper describes a simple method for extracting metadata fields from citations using hidden Markov models. The method is easy to implement and can achieve levels of precision and recall for heterogeneous citations comparable to or greater than other HMM-based methods. The method consists largely of string manipulation and otherwise depends only on an implementation of the Viterbi algorithm, which is widely available, and so can be implemented by diverse digital library systems.	digital library;hidden markov model;markov chain;precision and recall;string (computer science);viterbi algorithm	Erik Hetzner	2008		10.1145/1378889.1378937	maximum-entropy markov model;digital library;viterbi algorithm;computer science;data mining;database;world wide web;hidden markov model	NLP	-30.643917977552356	-68.79712955413676	180906
b199fafbe70d2a2accf906b6ca9cf4d4649b80e5	a linguistically grounded graph model for bilingual lexicon extraction	graph model	We present a new method, based on graph theory, for bilingual lexicon extraction without relying on resources with limited availability like parallel corpora. The graphs we use represent linguistic relations between words such as adjectival modification. We experiment with a number of ways of combining different linguistic relations and present a novel method, multi-edge extraction (MEE), that is both modular and scalable. We evaluate MEE on adjectives, verbs and nouns and show that it is superior to cooccurrence-based extraction (which does not use linguistic analysis). Finally, we publish a reproducible baseline to establish an evaluation benchmark for bilingual lexicon extraction.	algorithm;baseline (configuration management);benchmark (computing);graph theory;lexicon;limited availability;list of algorithms;multiple edges;parallel text;scalability;text corpus	Florian Laws;Lukas Michelbacher;Beate Dorow;Christian Scheible;Ulrich Heid;Hinrich Schütze	2010			natural language processing;speech recognition;computer science;linguistics	NLP	-27.391740419219214	-67.3177581317346	180971
85b837ca165c88a033b0168786dc1490a0522cd7	an enhanced approach to query performance prediction using reference lists		We address the problem of query performance prediction (QPP) using reference lists. To date, no previous QPP method has been fully successful in generating and utilizing several pseudo-effective and pseudo-ineffective reference lists. In this work, we try to fill the gaps. We first propose a novel unsupervised approach for generating and selecting both types of reference lists using query perturbation and statistical inference. We then propose an enhanced QPP approach that utilizes both types of selected reference lists.	performance prediction;reference implementation;unsupervised learning	Haggai Roitman	2017		10.1145/3077136.3080665	information retrieval;computer science;data mining;performance prediction;statistical inference	Web+IR	-27.47702013224007	-67.89764820201363	181477
6afba6e012c801a46a498defae63b1e5874ac502	toward finding semantic relations not written in a single sentence: an inference method using auto-discovered rules		Recent advances in automatic knowledge acquisition methods have enabled us to construct massive knowledge bases of semantic relations. Most previous work has focused on semantic relations explicitly expressed in single sentences. Our goal in this work is to obtain valid non-single sentence relation instances, which are not written in any single sentence and may not be even written in a large corpus. We develop a method to infer new semantic relation instances by applying auto-discovered inference rules, and show that our method inferred a considerable number of valid instances that were not written in single sentences even in 600 million Web pages.	knowledge acquisition;ontology components;text corpus;web page	Masaaki Tsuchida;Kentaro Torisawa;Stijn De Saeger;Jong-Hoon Oh;Jun'ichi Kazama;Chikara Hashimoto;Hayato Ohwada	2011			natural language processing;computer science;pattern recognition;data mining	NLP	-27.719213045752436	-70.65557290889092	182351
3bc5e17172b8ba3b6901924ed99c0dfc47d45e2f	toward an arabic ontology for arabic word sense disambiguation based on normalized dictionaries	query reformulation;information retrieval system;arabic language;lmf;word sense disambiguation;arabic dictionary;arabic ontology	In this paper, we propose an approach for constructing Arabic Ontology based on normalized dictionaries. This approach mainly consists in transforming non structured Arabic dictionaries into LMF Lexical Markup Framework based-normalized ones. We are basically exploiting Arabic dictionaries of Hadith for experimentation. Then, from an Arabic normalized dictionary of Hadith, an ontology will be constructed. It represents hidden knowledge in Hadith texts. It will be next integrated into an information retrieval and navigation system. We will take advantage of it to semantically disambiguate Arabic terms of both the formulated user query and/or the Arabic texts with application on texts of Hadith.	dictionary;word sense;word-sense disambiguation	Nadia Soudani;Ibrahim Bounhas;Bilel Elayeb;Yahya Slimani	2014		10.1007/978-3-662-45550-0_68	natural language processing;speech recognition;computer science;linguistics	NLP	-29.914239024664653	-69.59527322378997	182549
09861eaf41a3a7b945744c5fadf79eaa1dafacdd	extending a crf-based named entity recognition model for turkish well formed text and user generated content		Named entity recognition (NER), which provides useful information for many high level NLP applications and semantic web technologies, is a well-studied topic for most of the languages and especially for English. However the studies for Turkish, which is a morphologically richer and lesser-studied language, have fallen behind these for a long while. In recent years, Turkish NER intrigued researchers due to its scarce data resources and the unavailability of high-performing systems. Especially, the need to discover named entities occurring in Web datasets initiated many studies in this field. This article presents the enhancements made to a Turkish named entity recognition model [5] (based on conditional random fields (CRFs) and originally tailored for well formed texts) in order to extend its covered named entity types, and also to process extra challenging user generated content coming with Web 2.0. The article introduces the re-annotation of the available datasets to extend the covered named entity types, and a brand new dataset from Web 2.0. The introduced approach reveals an exact match F1 score of 92% on a dataset collected from Turkish news articles and ∼65% on different datasets collected from Web 2.0.	compiler;conditional random field;f1 score;feature engineering;high-level programming language;mike lesser;named entity;natural language processing;performance;semantic web;social media;software as a service;timex sinclair;turkish informatics olympiad;unavailability;user-generated content;web 2.0;web server	Gökhan Akin Seker;Gülsen Eryigit	2017	Semantic Web	10.3233/SW-170253		Web+IR	-33.44686808731444	-70.91289515990954	183395
37a4d52617bef3bd5d8106cc2e2b97d3b764f4fd	using the semantic web as background knowledge for ontology mapping	ontology mapping;background knowledge;semantic web;resource availability	While current approaches to ontology mapping produce good results by mainly relying on label and structure based similarity measures, there are several cases in which they fail to discover important mappings. In this paper we describe a novel approach to ontology mapping, which is able to avoid this limitation by using background knowledge. Existing approaches relying on background knowledge typically have one or both of two key limitations: 1) they rely on a manually selected reference ontology; 2) they suffer from the noise introduced by the use of semi-structured sources, such as text corpora. Our technique circumvents these limitations by exploiting the increasing amount of semantic resources available online. As a result, there is no need either for a manually selected reference ontology (the relevant ontologies are dynamically selected from an online ontology repository), or for transforming background knowledge in an ontological form. The promising results from experiments on two real life thesauri indicate both that our approach has a high precision and also that it can find mappings, which are typically missed by existing approaches.	experiment;ontology (information science);real life;semantic web;semantic integration;semiconductor industry;text corpus;thesaurus;upper ontology;web ontology language	Marta Sabou;Mathieu d'Aquin;Enrico Motta	2006			upper ontology;ontology alignment;bibliographic ontology;computer science;knowledge management;ontology;data mining;ontology-based data integration;owl-s;information retrieval;process ontology;suggested upper merged ontology	AI	-30.58502090418618	-67.11068352416521	183642
8de25b499a682fb389f8b51f299139df01e1fe0a	deconstructing nuggets: the stability and reliability of complex question answering evaluation	human judgments;complex information needs;information need;question answering;trec	"""A methodology based on """"information nuggets"""" has recently emerged as the de facto standard by which answers to complex questions are evaluated. After several implementations in the TREC question answering tracks, the community has gained a better understanding of its many characteristics. This paper focuses on one particular aspect of the evaluation: the human assignment of nuggets to answer strings, which serves as the basis of the F-score computation. As a byproduct of the TREC 2006 ciQA task, identical answer strings were independently evaluated twice, which allowed us to assess the consistency of human judgments. Based on these results, we explored simulations of assessor behavior that provide a method to quantify scoring variations. Understanding these variations in turn lets researchers be more confident in their comparisons of systems."""	computation;experiment;golden nugget 64;question answering;simulation;software quality assurance;text retrieval conference	Jimmy J. Lin;Pengyi Zhang	2007		10.1145/1277741.1277799	natural language processing;information needs;question answering;computer science;artificial intelligence;data mining;world wide web;information retrieval	Web+IR	-28.230540770852766	-68.66853839393966	184274
8ed8c0c52e09e9bb8b81a81bc7b6e3440011e1b8	automatic classification of scientific records using the german subject heading authority file (swd)	schlagwortkatalog;dewey dezimalklassifikation;linked data;text mining;schlagwortnormdatei;machine learning;evaluation measure;automatic classification;subject headings;notation	The following paper deals with an automatic text classification method which does not require training documents. For this method the German Subject Heading Authority File (SWD), provided by the linked data service of the German National Library is used. Recently the SWD was enriched with notations of the Dewey Decimal Classification (DDC). In consequence it became possible to utilize the subject headings as textual representations for the notations of the DDC. Basically, we we derive the classification of a text from the classification of the words in the text given by the thesaurus. The method was tested by classifying 3826 OAI-Records from 7 different repositories. Mean reciprocal rank and recall were chosen as evaluation measure. Direct comparison to a machine learning method has shown that this method is definitely competitive. Thus we can conclude that the enriched version of the SWD provides high quality information with a broad coverage for classification of German scientific articles.	algorithm;dewey decimal classification;display resolution;document classification;gene ontology term enrichment;ground truth;library (computing);linked data page;machine learning;scientific literature;thesaurus	Christian Wartena;Maike Sommer	2012			natural language processing;computer science;data mining;information retrieval;library classification	NLP	-28.62447200024268	-67.36422321843308	184321
016bb83ccce1ee294c810dd35bbcda948dc1a65d	improving question answering by combining multiple systems via answer validation	question answering system;question answering	Nowadays there exist several kinds of question answering systems. According to recent evaluation results, most of these systems are complementary (i.e., each one is better than the others in answering some specific type of questions). This fact indicates that a pertinent combination of various systems may allow improving the best individual result. This paper focuses on this problem. It proposes using an answer validation method to handle this combination. The main advantage of this approach is that it does not rely on internal system’s features nor depend on external answer’s redundancies. Experimental results confirm the appropriateness of our proposal. They mainly show that it outperforms individual system’s results as well as the precision obtained by a redundancy-based combination strategy.	answer set programming;data validation;edit distance;ensemble forecasting;existential quantification;norm (social);question answering;relevance;software quality assurance;textual entailment	Alberto Téllez-Valero;Manuel Montes-y-Gómez;Luis Villaseñor Pineda;Anselmo Peñas	2008		10.1007/978-3-540-78135-6_47	natural language processing;question answering;computer science;data mining;information retrieval;algorithm	NLP	-27.44414712885463	-66.99808254747953	184624
76e7b9dd7313f94f8a17c8b23c9d6bd115ff7b55	local rephrasing suggestions for supporing the work of writers	parallel corpora	In this article, we present a framework for obtaining rephrasings for short text spans. Good candidates include paraphrases, but also more generally phrases that could help a writer revise a text with some shifts in meaning. The presented framework uses as its knowledge source bilingual aligned phrases learnt from parallel corpora. We present several models for selecting rephrasings, and we evaluate the selection power of candidate rephrasings on  grammaticality ,  meaning preservation and  authoring value . The approach is then discussed and future work is described.		Aurélien Max	2008		10.1007/978-3-540-85287-2_31	natural language processing;computer science;artificial intelligence;linguistics	HCI	-29.451499722263783	-72.53051302642385	184680
11ef4ffd493c238a019496e44cc30c2c792da88f	open information extraction via contextual sentence decomposition	context thyristors semantics accuracy information retrieval educational institutions data mining;information retrieval;open information extraction;contextual sentence decomposition;recall aspect open information extraction contextual sentence decomposition technique csd technique high precision semantic search implicit verb explicit verb csd ie system reverb system ollie system clausie system accuracy aspect minimality aspect coverage aspect;semantic search;semantic search open information extraction contextual sentence decomposition	"""We show how contextual sentence decomposition (CSD), a technique originally developed for high-precision semantic search, can be used for open information extraction (OIE). Intuitively, CSD decomposes a sentence into the parts that semantically """"belong together"""". By identifying the (implicit or explicit) verb in each such part, we obtain facts like in OIE. We compare our system, called CSD-IE, to three state-of-the-art OIE systems: ReVerb, OLLIE, and ClausIE. We consider the following aspects: accuracy (does the extracted triple express a meaningful fact, which is also expressed in the original sentence), minimality (can the extracted triple be further decomposed into smaller meaningful triples), coverage (percentage of text contained in at least one extracted triple), and number of facts extracted. We show how CSD-IE clearly outperforms ReVerb and OLLIE in terms of coverage and recall, but at comparable accuracy and minimality, and how CSD-IE achieves precision and recall comparable to ClausIE, but at significantly better minimality."""	cambridge structural database;error analysis (mathematics);information extraction;parse tree;parsing;precision and recall;preprocessor;semantic search;vertex-transitive graph	Hannah Bast;Elmar Haussmann	2013	2013 IEEE Seventh International Conference on Semantic Computing	10.1109/ICSC.2013.36	natural language processing;semantic search;computer science;data mining;information retrieval	NLP	-26.83840696438413	-69.9573999973794	185007
fd7161968b5d4fc4326260656536fa744d9e9ef2	short-text similarity measurement using word sense disambiguation and synonym expansion	digital repository;word sense disambiguation;semantic information;la trobe university research online;semantic relations;similarity measure	Measuring the similarity between text fragments at the sentence level is made difficult by the fact that two sentences that are semantically related may not contain any words in common. This means that standard IR measures of text similarity, which are based on word co-occurrence and designed to operate at the document level, are not appropriate. While various sentence similarity measures have been recently proposed, these measures do not fully utilise the semantic information available from lexical resources such as WordNet. In this paper we propose a new sentence similarity measure which uses word sense disambiguation and synonym expansion to provide a richer semantic context to measure sentence similarity. Evaluation of the measure on three benchmark datasets shows that as a stand-alone sentence similarity measure, the method achieves better results than other methods recently reported in the literature.	algorithm;algorithmic efficiency;automatic summarization;benchmark (computing);binary classification;cluster analysis;computation;similarity learning;similarity measure;text mining;web services for devices;word sense;word-sense disambiguation;wordnet	Khaled Abdalgader;Andrew Skabar	2010		10.1007/978-3-642-17432-2_44	natural language processing;semantic similarity;digital library;semeval;computer science;pattern recognition;information retrieval;similarity heuristic	NLP	-27.297098912586822	-67.35907509341274	186378
ec6786748459403d39df2e3923720fa442102c91	an unsupervised framework for topological relations extraction from geographic documents		In this paper, we face the problem of extracting spatial relationships from geographical entities mentioned in textual documents. This is part of a research project which aims at geo-referencing document contents, hence making the realization of a Geographical Information Retrieval system possible. The driving factor of this research is the huge amount of Web documents which mention geographic places and relate them spatially. Several approaches have been proposed for the extraction of spatial relationships. However, they all assume the availability of either a large set of manually annotated documents or complex hand-crafted rules. In both cases, a rather tedious and time-consuming activity is required by domain experts. We propose an alternative approach based on the combined use of both a spatial ontology, which defines the topological relationships (classes) to be identified within text, and a nearest-prototype classifier, which helps to recognize instances of the topological relationships. This approach is unsupervised, so it does not need annotated data. Moreover, it is based on an ontology, which prevents the hand-crafting of ad hoc rules. Experimental results on real datasets show the viability of this approach.		Corrado Loglisci;Dino Ienco;Mathieu Roche;Maguelonne Teisseire;Donato Malerba	2012		10.1007/978-3-642-32597-7_5	data mining;computer science;ontology;spatial relation;geographic information system;classifier (linguistics);topology	NLP	-31.04928519292706	-68.08097153394293	186634
50541488ac8c63318625a338d1fad76be505a39e	using aobp for definitional question answering	information retrieval;integrable system;language model adaptation;adaptive optical back propagation aobp;back propagation;definitional question answering;language model;question answering;adaptive optics;neural network	This paper presents an integrated system for the task of definitional question answering. Firstly, we extract question-related knowledge as much as possible which include 3 categories. The first is based on language model. We train our language model on four different corpora. The second resource is the syntax dependency relations, which are extracted by Minipar. And the third resource contains only one feature, the document score provided by Information Retrieval (IR) engine. After that, we use a novel Adaptive Optical Back-Propagation (AOBP) neural network to score candidate sentences by using above extracted knowledge. The topk candidates will be selected as the final question answers. We experiment on the task of definitional question answering in TREC2006&2005. The experimental results show that our method can greatly improve the performance.	definition;question answering	Junkuo Cao;Weihua Wang;Yuanzhong Shu	2010		10.1007/978-3-642-13498-2_12	natural language processing;integrable system;question answering;computer science;backpropagation;machine learning;data mining;adaptive optics;information retrieval;artificial neural network;language model	NLP	-27.339161440339492	-68.57908102356501	186928
ca539821b9226f460793be3453fc2e937251403a	text structures in medical text processing: empirical evidence and a text understanding prototype	empirical evidence;medical records;empirical study;front end;linguistics;knowledge base;natural language processing	We consider the role of textual structures in medical texts. In particular, we examine the impact the lacking recognition of text phenomena has on the validity of medical knowledge bases fed by a natural language understanding front-end. First, we review the results from an empirical study on a sample of medical texts considering, in various forms of local coherence phenomena (anaphora and textual ellipses). We then discuss the representation bias emerging in the text knowledge base that is likely to occur when these phenomena are not dealt with--mainly the emergence of referentially incoherent and invalid representations. We then turn to a medical text understanding system designed to account for local text coherence.	anaphora (linguistics);emergence;knowledge bases;knowledge base;natural language understanding;prototype	Udo Hahn;Martin Romacker	1997	Proceedings : a conference of the American Medical Informatics Association. AMIA Fall Symposium		empirical research;text linguistics;natural language processing;natural language understanding;knowledge base;text processing;front and back ends;empirical evidence;artificial intelligence;computer science	NLP	-28.26181613747842	-71.80500165220946	187203
30e85623c962617a8d19b187cb58af4e86b18f2c	retrofitting word vectors of mesh terms to improve semantic similarity measures		Estimation of the semantic relatedness between biomedical concepts has utility for many informatics applications. Automated methods fall into two broad categories: methods based on distributional statistics drawn from text corpora, and methods based on the structure of existing knowledge resources. In the former case, taxonomic structure is disregarded. In the latter, semantically relevant empirical information is not considered. In this paper, we present a method that retrofits the context vector representation of MeSH terms by using additional linkage information from UMLS/MeSH hierarchy such that linked concepts have similar vector representations. We evaluated the method relative to previously published physician and coder’s ratings on sets of MeSH terms. Our experimental results demonstrate that the retrofitted word vector measures obtain a higher correlation with physician judgments. The results also demonstrate a clear improvement on the correlation with experts’ ratings from the retrofitted vector representation in comparison to the vector representation without retrofitting.	approximation;artificial neural network;distributional semantics;informatics;linkage (software);semantic similarity;similarity learning;similarity measure;taxonomy (general);text corpus;word embedding	Zhiguo Yu;Trevor Cohen;Byron C. Wallace;Elmer V. Bernstam;Todd R. Johnson	2016		10.18653/v1/W16-6106	natural language processing;theoretical computer science;engineering drawing	NLP	-27.080490533788588	-68.62813344160625	187216
20499f3c6fe9f84a12c9def941e2e12846a00c77	the conll-2014 shared task on grammatical error correction		The CoNLL-2013 shared task was devoted to grammatical error correction. In this paper, we give the task definition, present the data sets, and describe the evaluation metric and scorer used in the shared task. We also give an overview of the various approaches adopted by the participating teams, and present the evaluation results.	error detection and correction	Hwee Tou Ng;Siew Mei Wu;Ted Briscoe;Christian Hadiwinoto;Raymond Hendy Susanto;Christopher Bryant	2013				NLP	-31.16989159001456	-72.9099623487283	187300
f77e743b868879479ff6cdc76af7a7fdbdc0b7a1	an effective corpus-based question answering pipeline for italian		Question Answering is a longevous field in computer science, aimed at realizing systems able to answer questions expressed in natural language. However, building Question Answering systems for Italian and able to extract answers from a corpus pertaining a closed domain is still an open research problem. Indeed, extracting clues from a question to generate a query for the information retrieval engine as well as determining the likelihood that a candidate answer is correct are two very thorny tasks. To face these issues, the paper presents a Question Answering pipeline for Italian and based on a corpus of documents pertaining a closed domain. In particular, this pipeline exhibits functionalities for: (i) analyzing natural language questions in Italian by using lexical features; (ii) handling both factoid and description answer types and, depending on them, filtering contextual stop words from questions; (iii) scoring and selecting candidate answers with respect to their type in order to determine the best one. The proposed solution has been subject to an evaluation of its performance using standard metrics, showing promising results.		Emanuele Damiano;Raffaele Spinelli;Massimo Esposito;Giuseppe De Pietro	2017		10.1007/978-3-319-59480-4_9	natural language;factoid;information retrieval;data mining;question answering;open research;stop words;computer science	NLP	-27.756857924471007	-71.77544486430178	187396
3d894a0c37c9bc2fcf5d321da55d9e7e43a03332	enabling users to create their own web-based machine translation engine	data sharing;cloud service;web pages;letsmt;statistical machine translation;online collaboration;moses;european union;quality of data;parallel corpora;named entity;domain specificity;machine translation	"""This paper presents European Union co-funded projects to advance the development and use of machine translation (MT) that will benefit from the possibilities provided by the Web. Current mass-market and online MT systems are of a general nature and perform poorly for smaller languages and domain specific texts. The ICT-PSP Programme project LetsMT! develops a user-driven machine translation """"factory in the cloud"""" enabling web users to get customized MT that better fits their needs. Harnessing the huge potential of the web together with open statistical machine translation (SMT) technologies LetsMT! has created an innovative online collaborative platform for data sharing and building MT. Users can upload their parallel corpora to an online repository and generate user-tailored SMT systems based on user selected data. FP7 Programme project ACCURAT researches new methods for accumulating more data from the Web to improve the quality of data-driven machine translation systems. ACCURAT has created techniques and tools to use comparable corpora such as news feeds and multinational web pages. Although the majority of these texts are not direct translations, they share a lot of common paragraphs, sentences, phrases, terms and named entities in different languages which are useful for machine translation."""	cloud computing;fits;named entity;parallel text;statistical machine translation;text corpus;upload;web application;web page;world wide web	Andrejs Vasiljevs;Raivis Skadins;Indra Samite	2012		10.1145/2187980.2188032	computer-assisted translation;cloud computing;computer science;web page;data mining;database;machine translation;machine translation software usability;world wide web	NLP	-32.62820652760536	-72.59640658645837	187769
6a342a4e9e3cb3e3865aae260c970d884fa6205d	automatic acronym acquisition and term variation management within domain-specific texts.	molecular biology;morphological analysis	In this paper we present a framework for the effective management of terms and their variants that are automatically acquired from domain-specific texts. In our approach, the term variant recognition is incorporated in the automatic term retrieval process by taking into account orthographical, morphological, syntactic, lexico-semantic and pragmatic term variations. In particular, we address acronyms as a common way of introducing term variants in scientific papers. We describe a method for the automatic acquisition of newly introduced acronyms and the mapping to their ‘meanings’, i.e. the corresponding terms. The proposed three-step procedure is based on morpho-syntactic constraints that are commonly used in acronym definitions. First, acronym definitions containing an acronym and the corresponding term are retrieved. These two elements are matched in the second step by performing morphological analysis of words and combining forms constituting the term. The problems of acronym variation and acronym ambiguity are addressed in the third step by establishing classes of term variants that correspond to specific concepts. We present the results of the acronym acquisition in the domain of molecular biology: the precision of the method ranged from 94% to 99% depending on the size of the corpus used for evaluation, whilst the recall was 73%. * This research is a part of the BioPATH research project coordinated by LION BioScience (http://www.lionbioscience.com) and funded by German Ministry of Research.	domain-specific language;lexico;scientific literature;text corpus;while	Goran Nenadic;Irena Spasić;Sophia Ananiadou	2002				NLP	-29.617724341696	-71.49837487425533	188020
096949cc3c2f9d67973e2556132999deb0801090	schema normalization for improving schema matching	noun;schema matching;heterogeneous data sources	Schema matching is the problem of finding relationships among concepts across heterogeneous data sources (heterogeneous in format and in structure). Starting from the “hidden meaning” associated to schema labels (i.e. class/attribute names) it is possible to discover relationships among the elements of different schemata. Lexical annotation (i.e. annotation w.r.t. a thesaurus/lexical resource) helps in associating a “meaning” to schema labels. However, accuracy of semi-automatic lexical annotation methods on real-world schemata suffers from the abundance of non-dictionary words such as compound nouns and word abbreviations. In this work, we address this problem by proposing a method to perform schema labels normalization which increases the number of comparable labels. Unlike other solutions, the method semi-automatically expands abbreviations and annotates compound terms, without a minimal manual effort. We empirically prove that our normalization method helps in the identification of similarities among schema elements of different data sources, thus improving schema matching accuracy.	database schema;dictionary;html attribute;semiconductor industry;thesaurus	Serena Sorrentino;Sonia Bergamaschi;Maciej Gawinecki;Laura Po	2009		10.1007/978-3-642-04840-1_22	natural language processing;noun;information schema;schema;computer science;conceptual schema;star schema;data mining;database;database schema;information retrieval	DB	-29.796747325762453	-66.54829132968705	188094
51adbe886c63a3d64886f0bdfbf79e10df806b40	analyse de la robustesse des algorithmes de méta-recherche discriminante		This paper studies the sensitivity of four metasearch engines under different situations. The focus of this analysis is on trainable metasearch engines. Our main contribution is a large scale systematic analysis of the performance and behavior of these methods on several corpora. Firstly, we analyze how the choice and normalization of the relevance score delivered by base search engines influence the performance of metasearch. We then study the effectiveness of the metasearch engines on individual queries. We also analyze the robustness of the metasearch engines with regard to the variability in the training and test corpora. We finally present a preliminary analysis of active learning for query selection. All of these experiments demonstrate that the learned search engines are quite effective since they are uniformly better than both heuristic metasearch techniques and indivual engines. They are also particularly robust to changes in the training data sets and to the encoding of base search engines scores. MOTS-CLÉS : méta-recherche, BordaFuse, RankSVM, RankBoost, analyse de la robustesse.	active learning (machine learning);bibliothèque de l'école des chartes;experiment;heuristic;linear algebra;relevance;spatial variability;text corpus;web search engine	Huyen-Trang Vu;Patrick Gallinari	2008		10.24348/coria.2008.87	history;performance art	Web+IR	-28.674319916419215	-67.87922394625649	188540
fc96842c9589bad730b0dc0941319cf007a68034	towards very large ontologies for medical language processing.		We describe an ontology engineering methodology by which conceptual knowledge is extracted from an informal medical thesaurus (UMLS) and automatically converted into a formal description logics system. Our approach consists of four steps: concept definitions are automatically generated from the UMLS source, integrity checking of taxonomic and partonomic hierarchies is performed by the terminological classifier, cycles and inconsistencies are eliminated, and incremental refinement of the evolving knowledge base is performed by a domain expert. We report on experiments with a knowledge base composed of 164,000 concepts and 76,000 relations.	description logic;experiment;knowledge base;meronomy;natural language processing;ontology (information science);ontology engineering;refinement (computing);subject-matter expert;thesaurus	Udo Hahn;Stefan Schulz	2002			natural language processing;linguistics;programming language	AI	-32.161287639652606	-69.58208003228904	188740
30d77d6ccdf498fc538eef059c016454893fc213	thomson legal and regulatory at ntcir-3: japanese, chinese and english retrieval experiments	perforation;query formulation;indexation	Thomson Legal and Regulatory participated in the CLIR task of the NTCIR-3 workshop. We submitted formal runs for monolingual retrieval in Japanese and Chinese, and for bilingual retrieval from English to Japanese. Our main focus was in Japanese retrieval. We compared word-based and character-based indexing, as well as query formulation using characters and character bigrams. Our results show that wordbased and bigram-based retrieval show similar performance for most query formulation approaches, while they outperform character-based retrieval. For Chinese retrieval, we compared using single characters with using character bigrams. We also introduced a structured query to leverage both. Our results are consistent with previous work, where character bigrams were shown to have better performance than single characters. The structured query approach is promising, but requires more analysis. In our bilingual runs, queries were translated using a machine-readable dictionary. Translated terms were resegmented to match indexing units. Our results, so far, are inconclusive, as we experienced unexpected query formulation issues especially in our word-based approach.	bigram;boosting (machine learning);cross-language information retrieval;human-readable medium;machine-readable dictionary;microsoft word for mac;text-based (computing)	Isabelle Moulinier;Hugo Molina-Salgado;Peter Jackson	2002			natural language processing;speech recognition;computer science;linguistics	Web+IR	-29.402860042002274	-69.19371834942781	189077
8fab1999948eeee51a06737f05bf0192ea0babea	addressing the resource bottleneck to create large-scale annotated texts	expert annotation;annotated corpus;general web population;complex linguistic phenomenon;high-quality annotation;critical mass;game-like approach;large-scale annotated text;resource bottleneck;anaphora resolution;linguistic annotation;anawiki project	Large-scale linguistically annotated resources have become available in recent years. This is partly due to sophisticated automatic and semi-automatic approaches that work well on specific tasks such as part-of-speech tagging. For more complex linguistic phenomena like anaphora resolution there are no tools that result in high-quality annotations without massive user intervention. Annotated corpora of the size needed for modern computational linguistics research cannot however be created by small groups of hand annotators. The ANAWIKI project strikes a balance between collecting high-quality annotations from experts and applying a game-like approach to collecting linguistic annotation from the general Web population. More generally, ANAWIKI is a project that explores to what extend expert annotations can be substituted by a critical mass of non-expert judgements.	anaphora (linguistics);computation;computational linguistics;part-of-speech tagging;semiconductor industry;text corpus	Jon Chamberlain;Massimo Poesio;Udo Kruschwitz	2008			natural language processing;computer science;data mining;information retrieval	NLP	-30.29829562858782	-73.01142653395047	189180
20ef262404c4cbf361ac31bc9f5bcdfac1c31152	domain-specific ir for german, english and russian languages	search engine;english language;pseudo relevance feedback;search strategy;indexation;divergence from randomness;statistical language model;domain specificity;test collection	In participating in this domain-specific track, our first objective is to propose and evaluate a light stemmer for the Russian language. Our second objective is to measure the relative merit of various search engines used for the German and to a lesser extent the English languages. To do so we evaluated the tf ·idf , Okapi, IR models derived from the Divergence from Randomness (DFR) paradigm, and also a language model (LM). For the Russian language, we find that word-based indexing using our light stemming procedure results in better retrieval effectiveness than does the 4-gram indexing strategy (relative difference around 30%). Using the German corpus, we examine certain variations in retrieval effectiveness after applying the specialized thesaurus to automatically enlarge topic descriptions. In this case, the performance variations were relatively small and usually non significant.	divergence-from-randomness model;domain-specific language;language model;mike lesser;programming paradigm;randomness;relative change and difference;stemming;text corpus;tf–idf;thesaurus;web search engine	Claire Fautsch;Ljiljana Dolamic;Samir Abdou;Jacques Savoy	2007		10.1007/978-3-540-85760-0_26	natural language processing;speech recognition;computer science;english;linguistics;world wide web;information retrieval;search engine	NLP	-28.650336667841277	-68.16596171201445	189386
8c84fd100cfbe7d1887e4d7522bc7d44ea336a4f	automatically detecting corresponding edit-turn-pairs in wikipedia		In this study, we analyze links between edits in Wikipedia articles and turns from their discussion page. Our motivation is to better understand implicit details about the writing process and knowledge flow in collaboratively created resources. Based on properties of the involved edit and turn, we have defined constraints for corresponding edit-turn-pairs. We manually annotated a corpus of 636 corresponding and non-corresponding edit-turn-pairs. Furthermore, we show how our data can be used to automatically identify corresponding edit-turn-pairs. With the help of supervised machine learning, we achieve an accuracy of .87 for this task.	baseline (configuration management);machine learning;supervised learning;text corpus;wikipedia	Johannes Daxenberger;Iryna Gurevych	2014			natural language processing;computer science;machine learning;data mining;world wide web;information retrieval	NLP	-27.48237148088784	-70.19133589133605	189841
426d809408674b6034b8315eb772bd3495bf017c	a hybrid approach towards information expansion based on shallow and deep metadata	hybrid approach	The exponential growth of the World Wide Web in the last decade, brought an explosion in the information space, with important consequences also in the area of scientific research. Lately, finding relevant work in a particular field and exploring links between relevant publications, became a cumbersome task. In this paper we propose a hybrid approach to automatic extraction of semantic metadata from scientific publications that can help to alleviate, at least partially, the above mentioned problem. We integrated the extraction mechanisms in an application targeted to early stage researchers. The application harmoniously combines the metadata extraction with information expansion and visualization for the seamless exploration of the space surrounding scientific publications.	anaphora (linguistics);document engineering;fagan inspection;high- and low-level;iteration;open-source software;scientific literature;seamless3d;time complexity;world wide web	Tudor Groza;Siegfried Handschuh	2009			computer science;database;world wide web;information retrieval	HPC	-32.98359386860115	-67.67215435397351	190109
de60cc0cdf128d4144bea8dc91336404767feefe	semnews: a semantic news framework	semantic news framework	SemNews is a semantic news service that monitors different RSS news feeds and provides structured representations of the meaning of news. As new content appears, SemNews extracts the summary from the RSS description and processes it using OntoSem, which is a sophisticated text understanding system. The OntoSem environment is a rich and extensive tool for extracting and representing meaning in a language independent way. OntoSem performs a syntactic, semantic, and pragmatic analysis of the text, resulting in its text meaning representation or TMR. The TMRs are represented using a constructed world model or an ontology that consists of about 8000 Concepts. The ontology is also supported by an Onomasticon (Nirenburg & Raskin 2005) of about 400K terms, which is a lexicon of proper names. The learned instances from the text are stored in a fact repository which essentially forms the knowledge base of OntoSem. As the news items get processed by SemNews, the fact repository and the TMR are translated and published in Semantic Web representation language OWL. Although significant number of documents already exist on the Semantic Web in representations such as RDF and OWL (Li Ding et al. 2004), a vast majority of content on the Web remains as natural language text. By integrating language understanding agents into the Semantic Web through the SemNews framework, we have been able to demonstrate the potential of large scale semantic annotation and automatic metadata generation. OntoSem belongs to a general class of traditional, framebased knowledge representation systems. Migrating such a system to newer web based representations like OWL, poses certain challenges. While doing a complete and faithful translation of knowledge from OntoSem’s native representation into OWL is not feasible, we found the problems to be manageable in practice for a large subset of OntoSem features (Java, Finin, & Nirenburg 2005). Unlike information extraction tools that provide features such as named entity detection and basic noun phrase markup, the SemNews application aims to exports facts and learned instances of ontological concepts. By publishing	information extraction;java;knowledge base;knowledge representation and reasoning;lexicon;markup language;named entity;natural language understanding;ontology (information science);rss;resource description framework;semantic web;triple modular redundancy;web ontology language;world wide web	Akshay Java;Timothy W. Finin;Sergei Nirenburg	2006			semantic interoperability;semantic computing;semantic web stack;semantic analytics	Web+IR	-30.53993183822974	-67.14830195125822	190180
4948d5f16cd1f6733f2d989577119fdd18c83d02	semi-automatic terminology ontology learning based on topic modeling		Abstract Ontologies provide features like a common vocabulary, reusability, machine-readable content, and also allows for semantic search, facilitate agent interaction and ordering u0026 structuring of knowledge for the Semantic Web (Web 3.0) application. However, the challenge in ontology engineering is automatic learning, i.e., the there is still a lack of fully automatic approach from a text corpus or dataset of various topics to form ontology using machine learning techniques. In this paper, two topic modeling algorithms are explored, namely LSI u0026 SVD and Mr.LDA for learning topic ontology. The objective is to determine the statistical relationship between document and terms to build a topic ontology and ontology graph with minimum human intervention. Experimental analysis on building a topic ontology and semantic retrieving corresponding topic ontology for the useru0027s query demonstrating the effectiveness of the proposed approach.	algorithm;human-readable medium;integrated circuit;linear discriminant analysis;machine learning;ontology (information science);ontology engineering;ontology learning;semantic web;semantic search;semiconductor industry;singular value decomposition;text corpus;topic model;vocabulary	Monika Rani;Amit Kumar Dhar;O. P. Vyas	2017	Eng. Appl. of AI	10.1016/j.engappai.2017.05.006	process ontology;ontology learning;ontology inference layer;natural language processing;ontology-based data integration;computer science;ontology (information science);data mining;ontology engineering;suggested upper merged ontology;upper ontology;artificial intelligence	AI	-31.876583744081945	-67.9391521018531	190898
a110c5992b92827deb1aa31d3803ba80822a3c6f	automatic acquisition of sense tagged corpora	search engine;statistical method;word sense disambiguation;internet use;information gathering;natural language	An important problem in Natural Language Processing is identifying the correct sense of a word in a particular context. Thus far, statistical methods have been considered the best techniques in word sense disambiguation. Unfortunately, these methods produce high accuracy results only for a small number of preselected words. The reduced applicability of statistical methods is due basically to the lack of widely available semantically tagged corpora. In this paper we present a method which enables the automatic acquisition of sense tagged corpora. It is based on (1) the information provided in WordNet, particularly the word definitions found within the glosses and (2) the information gathered from Internet using existing search engines.	definition;natural language processing;tagged architecture;text corpus;web search engine;word sense;word-sense disambiguation;wordnet;world wide web	Rada Mihalcea;Dan I. Moldovan	1999			natural language processing;speech recognition;semeval;computer science;linguistics;natural language;information retrieval;search engine	NLP	-27.377438076117194	-71.52472196656785	191234
c13e49e4765264439d89f4cd0ae57bb5f6e4df07	rich ontology extraction and wikipedia expansion using language resources	conceptual knowledge;ontology extraction;information extraction;language resources;web ontology;consistency checking;semantic web;semantic relations;data classification	Existing social collaboration projects contain a host of conceptual knowledge, but are often only sparsely structured and hardly machine-accessible. Using the well known Wikipedia as a showcase, we propose new and improved techniques for extracting ontology data from the wiki category structure. Applications like information extraction, data classification, or consistency checking require ontologies of very high quality and with a high number of relationships. We improve upon existing approaches by finding a host of additional relevant relationships between ontology classes, leveraging multi-lingual relations between categories and semantic relations between terms.	display resolution;information extraction;knowledge base;ontology (information science);ontology learning;social collaboration;web ontology language;whole earth 'lectronic link;wiki;wikipedia	Christian Schönberg;Helmuth Pree;Burkhard Freitag	2010		10.1007/978-3-642-14246-8_17	upper ontology;ontology alignment;ontology components;bibliographic ontology;ontology inference layer;computer science;ontology;semantic web;data mining;database;ontology-based data integration;owl-s;information extraction;information retrieval;process ontology;suggested upper merged ontology	AI	-31.66626658231472	-66.63820038441025	191731
c2bdc4197c5adadfbedc45edbac399aef6f1a855	capturing syntactico-semantic regularities among terms: an application of the framenet methodology to terminology		Terminological databases do not always provide detailed information on the linguistic behaviour of terms, although this is important for potential users such as translators or students. In this paper we describe a project that aims to fill this gap by proposing a method for annotating terms in sentences based on that developed within the FrameNet project (Ruppenhofer et al. 2010) and by implementing it in an online resource called DiCoInfo. We focus on the methodology we devised, and show with some preliminary results how similar actantial (i.e. argumental) structures can provide evidence for defining lexical relations in specific languages and capturing cross-linguistic equivalents. The paper argues that the syntactico-semantic annotation of the contexts in which the terms occur allows lexicographers to validate their intuitions concerning the linguistic behaviour of terms as well as interlinguistic relations between them. The syntactico-semantic annotation of contexts could, therefore, be considered a good starting point in terminology work that aims to describe the linguistic functioning of terms and offer a sounder basis to define interlinguistic relationships between terms that belong to different languages.	database;framenet;lexicography	Marie-Claude L'Homme;Janine Pimentel	2012			natural language processing;linguistics	NLP	-28.696457629127945	-72.80742891214521	191814
4367060682b830f8f4a0e2f21c0ad9314789e022	investigating subspace distances in semantic spaces	information retrieval;semantic space	Semantic space models of word meaning derived from cooccurrence statistics within a corpus of documents, such as the Hyperspace Analogous to Language (HAL) model, have been proposed in the past. While word similarity can be computed using these models, it is not clear how semantic spaces derived from different sets of documents can be compared. In this paper, we focus on this problem, and we revisit the proposal of using semantic subspace distance measurements [1]. In particular, we outline the research questions that still need to be addressed to investigate and validate these distance measures. Then, we describe our plans for future research.	hal;spaces	Guido Zuccon;Leif Azzopardi;Enrico Gasco	2011			natural language processing;semantic similarity;semantic computing;semantic integration;explicit semantic analysis;data mining;semantic compression;mathematics;semantic equivalence;information retrieval	NLP	-28.325701518386875	-67.03435232775585	191867
7467971afbf7a595fcf450324bba0b3fd0d40028	cross-lingual entity matching for heterogeneous online wikis		Knowledge bases play an increasing important role in many applications. However, many knowledge bases mainly focus on English knowledge, and have only a few knowledge for low-resource languages (LLs). If we can map the entities in LLs to these in high-resource languages (HLs), many knowledge such as relation between entities can be transferred from HLs to LLs.	wiki	Weiming Lu;Huan Wang;Jiahui Liu;Hao Dai;Baogang Wei	2017		10.1007/978-3-319-73618-1_78		DB	-29.030736753842547	-67.0330718701059	192480
a34e877398b0725bb0cb19eeda6df6f38697500c	the risk of sub-optimal use of open source nlp software: ukb is inadvertently state-of-the-art in knowledge-based wsd		UKB is an open source collection of programs for performing, among other tasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-thebox in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise reproducibility instructions.	algorithm;download;end-to-end principle;natural language processing;open-source software;preprocessor;software release life cycle;status message (instant messaging);web services for devices;word sense;word-sense disambiguation	Eneko Agirre;Oier López de Lacalle;Aitor Soroa	2018	CoRR		artificial intelligence;natural language processing;word-sense disambiguation;software;computer science	NLP	-33.541133770983926	-71.64072769090707	192584
bb6a172bf496f48cbdfdbb9dd3c9e40a54c56ff0	leveraging crowdsourcing for the thematic annotation of the qur'an	disambiguation;semantic web;thematic annotation;ontology;crowdsourcing;qur an;knowledge engineering	In this paper, we illustrate how we leverage crowdsourcing to create workflows for knowledge engineering in specialized and knowledge intensive domains. We undertake the special case of the Arabic script of the Qur’an, a widely studied manuscript, and attempt to employ crowdsourcing methods for its thematic annotation at the sub-verse level, for which, there is no standardized knowledge model available to date. We demonstrate that our proposed method presents feasibility to achieve reliable annotations in an efficient and scalable manner. The proposed methodology and framework is meant to be generalizable to other knowledge intensive and specialized domains.	crowdsourcing;knowledge engineering;knowledge representation and reasoning;scalability;verse protocol	Amna Basharat;Ismailcem Budak Arpinar;Khaled Rasheed	2016		10.1145/2872518.2889409	computer science;data science;semantic web;knowledge engineering;ontology;data mining;world wide web;crowdsourcing;information retrieval	Web+IR	-32.80536779862441	-67.35746595745111	193287
73ea84739243d977261e793a90904b8f27f0f35d	phrase discovery for english and cross-language retrieval at trec 6		Berkeley's experiments in TREC-6 center around phrase discovery in topics and documents. The technique of ranking bigram term pairs by their expected mutual information value was utilized for English phrase discovery as well as Chinese seg-mentation. This diierentiates our phrase-nding method from the mechanistic one of using all bigrams which appear at least 25 times in the collection. Phrase nd-ing presents an interesting interaction with stop words and stop word processing. English phrase discovery proved very important in a dictionary-based English to German cross language run. Our participation in the ltering track was marked with an interesting strictly Boolean retrieval as well as some experimentation with maximum utility thresholds on probabilistically ranked retrieval.	bigram;dictionary;energy (psychological);experiment;mutual information;standard boolean model	Fredric C. Gey;Aitao Chen	1997				Web+IR	-28.78007469949083	-70.16324147394953	193816
486009fcb67c80837748b2603bd467ca1de17fb3	study on english learning system based on the difficulty of english texts	difficulty of text;english learning;aided learning system;multi-model and multi-feature	In order to effectively improve the learning efficiency of English learners, the difficulty of English texts based on multi-model and multi-feature fusion is studied. Based on the difficulty classification system of new concept English, a training and testing corpus was constructed. Effective features of English text difficulty are extracted. Based on the research on the method of determining the difficulty of English texts, an English assisted learning system with practice mechanism was designed and implemented. It can meet the needs of different users for learning. The system is mainly composed of five core modules: search module, fixed collocation module, keyword query module, new word module and single exercise module. The process of building a system was introduced. The result shows that this system has a good auxiliary effect on English learning.		Xiaoyan Wu	2018	Wireless Personal Communications	10.1007/s11277-018-5391-4	computer science;real-time computing;natural language processing;collocation;artificial intelligence	NLP	-27.575426744862423	-69.72879689213006	194127
d7e1944a69c89e213756f15935b5f3ae1dd63b14	enhancing automatic term recognition through recognition of variation		Terminological variation is an integral part of the linguistic ability to realise a concept in many ways, but it is typically considered an obstacle to automatic term recognition (ATR) and term management. We present a method that integrates term variation in a hybrid ATR approach, in which term candidates are recognised by a set of linguistic filters and termhood assignment is based on joint frequency of occurrence of all term variants. We evaluate the effectiveness of incorporating specific types of term variation by comparing it to the performance of a baseline method that treats term variants as separate terms. We show that ATR precision is enhanced by considering joint termhoods of all term variants, while recall benefits by the introduction of new candidates through consideration of different variation types. On a biomedical test corpus we show that precision can be increased by 20–70% for the top ranked terms, while recall improves generally by 2–25%.	answer to reset;automatic target recognition;baseline (configuration management);floor and ceiling functions;terminology extraction	Goran Nenadic;Sophia Ananiadou;John McNaught	2004			computer science;artificial intelligence;algorithm	NLP	-28.907878625870154	-71.4056989189556	194372
705bf66ccbd12fefe8bf0823650c2cb734618728	knowledge node and relation detection		A bottleneck problem in detecting knowledge nodes and their relations is how to extract accurately and correctly and codify the complex knowledge assertions from full-text documents (human intelligence) into the format of “machine intelligence” (computer-processable knowledge assertions). This paper reports a preliminary study that aims at this bottleneck problem by starting from the fundamentals of KR—representing knowledge from full-text documents by using knowledge node and relation recognition methods and tools. We collected data from full-text biomedical research publications and used manual and automatic tools to investigate the strengths and limitations of these methods. The findings show that MetaMap did a better job in detecting concepts from texts while SemRep is capable of extract relations between k-nodes. The paper presents the findings from the perspectives of degree of abstraction, types of k-nodes and relations, and linguistic structures and the evaluation results using the BLEU and cosine similarity measures.	artificial intelligence;bleu;cosine similarity;sensor	Jian Qin;Bei Yu;Liya Wang	2018				NLP	-27.625410358912447	-71.50759215175354	194600
fa4279ccf3515ddf91429d8b01bc726eb2e1cd4b	unsupervised acquisition of axioms to paraphrase noun compounds and genitives	background knowledge acquisition;paraphrasing;noun compounds;proposition stores	A predicate is usually omitted from text when it is highly predictable from the context. This omission is due to the effort optimization that humans perform during the language generation process. Authors omit the information that they know the addressee is able to recover effortlessly. Most noun-noun structures including genitives and compounds are result of this process. The goal of this work is to generate automatically and without supervision the paraphrases that make explicit the omitted predicate in these noun-noun structures. The method is general enough to address also the cases were components are Named Entities. The resulting paraphrasing axioms are necessary for recovering the semantics of a text, and therefore, useful for applications such as Question Answering.		Anselmo Peñas;Ekaterina Ovchinnikova	2012		10.1007/978-3-642-28604-9_32	natural language processing;computer science;linguistics	NLP	-27.26424495225859	-72.01184805970148	195132
88ee55142ebaab9a5da281a8ca53b329c233d77d	representation of texts in structured form	learning;text processing;natural language understanding;regular languages;structured representation	Although the existing knowledge representation techniques, ranging from the relational databases to the most recent Semantic web languages, are successfully applied in numerous practical applications, they are still unable to represent the information contained in text documents and web pages in structured form, suitable for productive text processing. Text files can represent text documents with no loss of information, however, this information is represented in an unstructured form. Various knowledge formalisms used in different phases of Natural Language Understanding, such as lexical, syntactic, semantic, pragmatic and discourse analysis, are still unable to represent texts in structured form with no loss of information. In this paper, we define the crucial requirements for structured text representation and then, we give a brief introduction to a representation technique that fulfills all these requirements, including the basic data types and learning techniques used to create, maintain and interpret the resulting representation formalism.		Mladen Stanojevic;Sanja Vranes	2012	Comput. Sci. Inf. Syst.	10.2298/CSIS100901038S	natural language processing;regular language;computer science;data mining;database;programming language;information retrieval	AI	-32.11950520116834	-69.7300757752169	195701
360ce6b9cf8079859370d992c5d95a5072992b82	towards automatic structuring and semantic indexing of legal documents	legislation;legal text analysis;natural language processing	Over the last years there has been a great increase on the number of freely available legal resources. Portals that allow users to search for legislation, using keywords are now a common place. However, in the vast majority of those portals, legal documents are not stored in a structured format with a rich set of meta data, but in presentation oriented manifestation, making impossible for the end users to inquiry semantics about the documents, such as date of enactment, date of repeal, jurisdiction, etc. or to reuse information and establish an interconnection with similar repositories. In this paper, we present an approach for extracting a machine readable semantic representation of legislation, from unstructured document formats. Our method exploits common formats of legal documents to identify blocks of structural and semantic information and models them according to a popular legal meta-schema. Our proposed method is highly extensible and achieves high accuracy for a variety of legal and para legal documents, especially legislation. Our evaluation results reveal that our methodology can be of great assistance for the automatic structuring and semantic indexing of legal resources.	automatic taxonomy construction;document;human-readable medium;interconnection;portals	Marios Koniaris;George Papastefanatos;Yannis Vassiliou	2016		10.1145/3003733.3003801	computer science;data mining;database;world wide web;computer security	Web+IR	-32.48523126391144	-67.58770268382274	195776
05cf923d3eb295d74e827018e75fbf03f78c9197	finding duplicates of your yet unwritten bug report	proposals indexes computer bugs white spaces java measurement histograms;duplicate detection;bug report;stack trace;text analysis;eclipse project software project testing effort development team duplicate recognition natural language processing text similarity measure bug report duplicate detection annotation stack trace machine learning algorithm;bug report stack trace duplicate detection;program debugging;learning artificial intelligence;natural language processing;text analysis learning artificial intelligence natural language processing program debugging	Software projects often use bug-tracking tools to keep track of reported bugs and to provide a communication platform to discuss possible solutions or ways to reproduce failures. The goal is to reduce testing efforts for the development team. However, often, multiple bug reports are committed for the same bug, which, if not recognized as duplicates, can result in work done multiple times by the development team. Duplicate recognition is, in turn, tedious, requiring to examine large amounts of bug reports. Previous work addresses this problem by employing natural-language processing and text similarity measures to automate bug-report duplicate detection. The downside of these techniques is that, to be applicable, they require a reporting user to go through the time-consuming process of describing the problem, just to get informed that the bug is already known. To address this problem, we propose an approach that only uses stack traces and their structure as input to machine-learning algorithms for detecting bug-report duplicates. The key advantage is that stack traces are available without a written bug report. Experiments on bug reports from the Eclipse project show that our approach performs as good as state-of-the-art techniques, but without requiring the whole text corpus of a bug report to be available.	algorithm;bug tracking system;documentation;eclipse;experiment;human-readable medium;machine learning;natural language processing;printing;sensitivity and specificity;sensor;software bug;stack trace;text corpus;text-based (computing);tracing (software);web search query	Johannes Lerch;Mira Mezini	2013	2013 17th European Conference on Software Maintenance and Reengineering	10.1109/CSMR.2013.17	stack trace;text mining;software bug;computer science;operating system;data mining;database;programming language;software regression;world wide web	SE	-31.639398185680193	-66.2683536986506	196293
4286968553d28b3719ba879fa56b75575b228e00	what happened to esfinge in 2007?	question reformulation;anaphor resolution;question answering system;named entity recognizer;answer choice;portuguese;question answering;wikipedia processing	Esfinge is a general domain Portuguese question answering system which uses the information available on the Web as an additional resource when searching for answers. Other external resources and tools used are a broad coverage parser, a morphological analyser, a named entity recognizer and a Web-based database of word co-occurrences. In this fourth participation in CLEF, in addition to the new challenges posed by the organization (topics and anaphors in questions and the use of Wikipedia to search and support answers), we experimented with a multiple question and multiple answer approach in QA.	finite-state machine;morphological parsing;named entity;question answering;wikipedia;world wide web	Luís Miguel Cabral;Luís Fernando Costa;Diana Santos	2007		10.1007/978-3-540-85760-0_31	natural language processing;question answering;computer science;data mining;information retrieval;portuguese	NLP	-28.96278599352695	-71.18700831192172	196332
1ad7b7187c62cb475539f27197c2ebb8e9765721	expressmatch: a system for creating ground-truthed datasets of online mathematical expressions	online mathematical expressions;mathematics computing;handwriting recognition;performance evaluation;online handwritten mathematical expression recognition expressmatch ground truthed datasets performance evaluation performance comparison;system design;automatic annotation;labeling data models writing handwriting recognition integrated circuit modeling computational modeling performance evaluation;ground truthed dataset;ground truth;mathematics computing handwriting recognition;performance evaluation online mathematical expressions ground truthed dataset	In recognition domains, publicly available ground-truthed datasets are essential to perform effective performance evaluation and comparison of existing methods and systems. However, in the field of online handwritten mathematical expression recognition, datasets are quite scarce and their creation is one of the current challenging issues. In this paper, we present Express Match, a system designed to help creation and management of online mathematical expression datasets with ground-truth data. In this system, handwritten model expressions can be input and manually annotated with ground-truth data, transcriptions of these expressions can be automatically annotated by matching them to the respective models. Additional metadata can also be attached to each sample expression. To test the system, a dataset consisting of 56 model expressions and 910 sample expressions with a total of 20,010 symbols, written by 25 different writers, has been created. This dataset, as well as Express Match, will be made publicly available.	ground truth;performance evaluation	Frank D. Julca-Aguilar;Nina Sumiko Tomita Hirata	2012	2012 10th IAPR International Workshop on Document Analysis Systems	10.1109/DAS.2012.38	speech recognition;ground truth;computer science;artificial intelligence;machine learning;data mining;handwriting recognition;systems design	Visualization	-31.09744011446052	-70.25638695985266	196347
d0c5b6c798e608095a0b48cccc835d7165971794	producing a large-scale encyclopedic corpus over the web		Encyclopedias,which describegeneral/tech nical terms,arevaluablelanguageresources(LRs). As with othertypesof LRs relying on humanintrospectionandsupervision,constructingencyclopediasis quiteexpensi ve. To resolvethisproblem,weautomatically produced a large-scaleencyclopedic corpusover theWorld Wide Web. We first searchedtheWebfor pagescontaininga termin question.Then we usedlinguistic patternsandHTML structuresto extract text fragmentsdescribingthe term. Finally, we organizedextractedterm descriptionsbasedon domains.Theresultantcorpuscontainsapproximately 100,000terms.We alsoevaluatedthequality of 2,000test terms,andfoundthatcorrectdescriptionswereobtainedfor 65%of testterms.	world wide web	Atsushi Fujii;Katunobu Itou;Tetsuya Ishikawa	2002			world wide web;natural language processing;encyclopedia;introspection;artificial intelligence;computer science	NLP	-29.776417648902406	-67.57512020599329	196550
91786e94d1aee7f38222679b7d841fa844fc234a	using local grammar for entity extraction from clinical reports	health and wellbeing;tecnologias generalidades;tecnologias;media digital technology and the creative economy	Information Extraction (IE) is a natural language processing (NLP) task whose aim is to analyze texts written in natural language to extract structured and useful information such as named entities and semantic relations linking these entities. Information extraction is an important task for many applications such as bio-medical literature mining, customer care, community websites, and personal information management. The increasing information available in patient clinical reports is difficult to access. As it is often in an unstructured text form, doctors need tools to enable them access to this information and the ability to search it. Hence, a system for extracting this information in a structured form can benefits healthcare professionals. The work presented in this paper uses a local grammar approach to extract medical named entities from French patient clinical reports. Experimental results show that the proposed approach achieved an F-Measure of 90. 06%.	entity bean;named-entity recognition	Aicha Ghoulam;Fatiha Barigou;Ghalem Belalem;Farid Meziane	2015	IJIMAI	10.9781/ijimai.2015.332	computer science;knowledge management;data mining;multimedia;information extraction	NLP	-33.60028668076186	-70.63303322253897	196573
eab8aa72315e63d6ca5a83cb37b419d4e09ef146	ontology-based view of natural language meaning: the case of humor detection	language understanding;text comprehension;humor detection;semantics;computational linguistic;natural language;success rate;ontologies;text meaning representation;analyzer	This paper deals with computational detection of humor. It assumes that computational humor is an useful task for any number of reasons and in many applications. Besides these applications, it also shows that recognition of humor is a perfect test platform for an advanced level of language understanding by a computer. It discusses the computational linguistic/semantic preconditions for computational humor and an ontological semantic approach to the task of humor detection, based on direct and comprehensive access to meaning rather than on trying to guess it with statistical-cum-syntactical keyword methods. The paper is informed by the experience of designing and implementing a humor detection model, whose decent success rate confirmed some of the assumptions while its flaws made other ideas prominent, including the necessity of full text comprehension. The bulk of the paper explains how the comprehensive meaning access technology makes it possible for unstructured natural language text to be automatically translated into the ontologically defined text meaning representations that can be used then to detect humor in them, if any, automatically. This part is informed by the experience, subsequent to humor detection, of designing, implementing, and testing an ontological semantic text analyzer that takes an English sentence as input and outputs its text meaning representation (TMR). Every procedure mentioned in the paper has either been implemented or proven to be implementable within the approach.	formal ontology;natural language	Julia M. Taylor	2010	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-010-0014-2	natural language processing;spectrum analyzer;computer science;ontology;artificial intelligence;semantics;natural language	NLP	-28.003762054703262	-71.85803574935622	196688
02acdcea57da5eb0ca758050d3284f8a27d05fd7	english- vietnamese cross-language paraphrase identification method		Paraphrase identification is a very important problem and is used in many natural language processing tasks such as machine translation, bilingual information retrieval, plagiarism detection, etc. With the development of information technology and the internet, the requirement of textual comparing is not only in the same language but also in many different language pairs. Especially in Vietnamese, the need to detect paraphrase in English-Vietnamese pair of sentences is very large because English is a most popular foreign language in Vietnam. However, the in-depth studies on cross-language paraphrase identification task between English and Vietnamese are still limited. In this paper, we propose a method to identify the English-Vietnamese cross-language paraphrase cases using a fuzzy-based method and the BabelNet semantic network. We identify if the pair of sentences is paraphrased by using feature classes and then combine these results into a final one using a mathematical formula. The experimental results show that our model achieves 77.1% F-measure accuracy and has the advantage of the processing speed compared to other methods which have equivalent quality.	babelnet;f1 score;information retrieval;machine translation;natural language processing;programming language;semantic network	Le Thanh Nguyen;Dinh Dien	2017		10.1145/3155133.3155187	machine translation;vietnamese;natural language processing;semantic similarity;foreign language;semantic network;plagiarism detection;paraphrase;information technology;artificial intelligence;computer science	NLP	-28.591154164789106	-71.45501187040254	196724
7e13cdc1d2c7737b8856e8592e5585867e440691	from natural language descriptions in clinical guidelines to relationships in an ontology	clinical practice guideline;clinical guideline;umls;relation extraction;clinical practice guidelines;natural language;ontologies;natural language processing;knowledge engineering	Knowledge Engineering allows to automate entity recognition and relation extraction from clinical texts, which in turn can be used to facilitate clinical practice guideline (CPG) modeling. This paper presents a method to recognize diagnosis and therapy entities, and to identify relationships between these entities from CPG free-text documents. Our approach applies a sequential combination of several basic methods classically used in knowledge engineering (natural language processing techniques, manually authored grammars, lexicons and ontologies), to gradually map sentences describing diagnostic and therapeutic procedures to an ontology. First, using a standardized vocabulary, our method automatically identifies guideline concepts. Next, for each sentence, it determines the patient conditions under which the descriptive knowledge of the sentence is valid. Then, it detects the central information units in the sentence, in order to match the sentence with a small set of predefined relationships. The approach enables automated extraction of relationships about findings that have manifestation in a disease, and procedures that diagnose or treat a disease.	central pattern generator;entity;knowledge engineering;lexicon;natural language processing;ontology (information science);relationship extraction;vocabulary	Maria Taboada;Maria Meizoso;David Riaño;Albert Alonso;Diego Martínez Hernández	2009		10.1007/978-3-642-11808-1_3	natural language processing;computer science;ontology;artificial intelligence;knowledge engineering;data mining;database;unified medical language system;natural language	AI	-33.10715999281854	-69.463961206044	197104
0c708675c6a836ce84cbf97742ed8e27cde55869	a system for extracting study design parameters from nutritional genomics abstracts		The extraction of study design parameters from biomedical journal articles is an important problem in natural language processing (NLP). Such parameters define the characteristics of a study, such as the duration, the number of subjects, and their profile. Here we present a system for extracting study design parameters from sentences in article abstracts. This system will be used as a component of a larger system for creating nutrigenomics networks from articles in the nutritional genomics domain. The algorithms presented consist of manually designed rules expressed either as regular expressions or in terms of sentence parse structure. A number of filters and NLP tools are also utilized within a pipelined algorithmic framework. Using this novel approach, our system performs extraction at a finer level of granularity than comparable systems, while generating results that surpass the current state of the art.		Cassidy Kelly;Hui Yang	2013	Journal of integrative bioinformatics	10.2390/biecoll-jib-2013-222	computer science;bioinformatics;data science;machine learning;data mining;database	NLP	-31.124076162346103	-70.32913872353899	198112
0034d2619797d26e7dda21237913d36fab7e8219	empirical methods in information extraction	empirical method;natural language;information extraction	Most corpus-basedmethods in natural language processing (NLP)were developed toprovide an arbitrary text-understanding application with one or more general-purpose linguistic capabilities. This is evident from the articles in this issue of AI Magazine. Charniak and Ng/Zelle, for example, describe techniques for part-of-speech tagging, parsing, and word-sense disambiguation. These techniques were created with no specific domain or high-level language-processing task in mind. In contrast, this article surveys the use of empirical methods for a particular natural language understanding task that is inherently domain-specific. The task is information extraction. Very generally, an information extraction system takes as input an unrestricted text and “summarizes” the text with respect to a prespecified topic or domain of interest: it finds useful information about the domain and encodes that information in a structured form, suitable for populating databases. In contrast to in-depth natural language understanding tasks, information extraction systems effectively skim a text to find relevant sections and then focus only on these sections in subsequent processing. The information extraction system in Figure 1, for example, summarizes stories about natural disasters, extracting for each such event the type of disaster, the date and time that it occurred, and data on any property damage or human injury caused by the event. Information extraction has figured prominently in the field of empirical NLP: The first largescale, head-to-head evaluations of NLP systems on the same text-understanding tasks were the DARPA-sponsored MUC performance evaluations of information extraction systems (Lehnert and Sundheim, 1991; Chinchor et al., 1993). Prior to each evaluation, all participating sites receive a corpus of texts from a predefined domain and the corresponding “answer keys” to use for system development. The answer keys are manually encoded templates — much like that of Figure 1 — that capture all information from the corresponding source text that is relevant to the domain, as specified in a set of written guidelines. After a short development phase, the NLP systems are evaluated by comparing the summaries each produces with the summaries generated by human experts for the same test set of previously unseen texts. The comparison is performed using an automated scoring program that rates each system according to measures of recall and precision. Recallmeasures the amount of the relevant information that theNLP system correctly extracts from the test collection while precisionmeasures the reliability of the information extracted: recall = (# correct slot-fillers in output template) / (# slot-fillers in answer key) precision = (# correct slot-fillers in output template) / (# slot-fillers in output template)	database;general-purpose markup language;high- and low-level;high-level programming language;information extraction;message understanding conference;natural language processing;natural language understanding;parsing;part-of-speech tagging;population;precision and recall;smart common input method;test set;text corpus;word sense;word-sense disambiguation	Claire Cardie	1997	AI Magazine		natural language processing;computer science;data science;data mining;natural language;empirical research;information extraction	NLP	-28.278903220148457	-71.82719775840096	198443
512fba47dc812e560ef668a25e0e2a22f0001084	language identification in web pages	tokenization;web documents;search engine;web pages;performance evaluation;optical character recognition;part of speech tagging;language identification;sentence boundary detection;similarity measure	"""This paper discusses the problem of automatically identifying the language of a given Web document. Previous experiments in language guessing focused on analyzing """"coherent"""" text sentences, whereas this work was validated on texts from the Web, often presenting harder problems. Our language """"guessing"""" software uses a well-known n-gram based algorithm, complemented with heuristics and a new similarity measure. Both fast and robust, the software has been in use for the past two years, as part of a crawler for a search engine. Experiments show that it achieves very high accuracy in discriminating different languages on Web pages."""	algorithm;coherence (physics);experiment;heuristic (computer science);language identification;n-gram;similarity measure;web crawler;web page;web search engine;world wide web	Bruno Martins;Mário J. Silva	2005		10.1145/1066677.1066852	natural language processing;language identification;tokenization;html;computer science;web page;database;optical character recognition;world wide web;information retrieval;search engine	NLP	-28.605384571455286	-67.88914476220116	199271
b1b2a1db535e90b9b192c92b55a5e9e0375f2f0f	the agreementmakerlight ontology matching system	settore inf 01 informatica	AgreementMaker is one of the leading ontology matching systems, thanks to its combination of a flexible and extensible framework with a comprehensive user interface. In many domains, such as the biomedical, ontologies are becoming increasingly large thus presenting new challenges. We have developed a new core framework, AgreementMakerLight, focused on computational efficiency and designed to handle very large ontologies, while preserving most of the flexibility and extensibility of the original AgreementMaker framework. We evaluated the efficiency of AgreementMakerLight in two OAEI tracks: Anatomy and Large Biomedical Ontologies, obtaining excellent run time results. In addition, for the Anatomy track, AgreementMakerLight is now the best system as measured in terms of F-measure. Also in terms of F-measure, AgreementMakerLight is competitive with the best OAEI performers in two of the three tasks of the Large Biomedical Ontologies track that match whole ontologies.	algorithm;display resolution;extensibility;f1 score;fma instruction set;fastest;matching (graph theory);nc (complexity);ontology (information science);ontology alignment;run time (program lifecycle phase);scalability;semantic similarity;systematized nomenclature of medicine;user interface	Daniel Faria;Catia Pesquita;Emanuel Santos;Matteo Palmonari;Isabel F. Cruz;Francisco M. Couto	2013		10.1007/978-3-642-41030-7_38	computer science;artificial intelligence;operating system;data mining;database;world wide web	Web+IR	-32.528355389008404	-66.41872964119943	199786
