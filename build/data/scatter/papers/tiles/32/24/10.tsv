id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
d77db10fb8640162d81b2297faae5427b76f5538	visual-inertial navigation for pinpoint planetary landing using scale-based landmark matching	navigation;precision;inertial;moon;landing;vision	Landing an autonomous spacecraft within 100 meters of a mapped target is a navigation challenge in planetary exploration. Vision-based approaches attempt to pair 2D features detected in camera images with 3D mapped landmarks to reach the required precision. This paper presents a visionaided inertial navigation system for pinpoint planetary landing called LION (Landing Inertial and Optical Navigation). It can fly over any type of terrain, whatever the topography. LION uses measurements from a novel image-tomap matcher in order to update through a tight data fusion scheme the state of an extended Kalman filter propagated with inertial data. The image processing uses the state and covariance predictions from the filter to determine the regions and extraction scales in which to search for non-ambiguous landmarks in the image. The image scale management process operates per landmark and greatly improves the repeatability rate between the map and descent images. A lunar-representative optical test bench called Visilab was also designed in order to test LION. The observability of absolute navigation performances in Visilab is evaluated with a model developed specifically for this purpose. Finally, the system performances are evaluated at a number of altitudes along with its robustness to off-nadir camera angle, illumination changes, a different map generation process and non-planar topography. The Email addresses: jeff.delaune@centraliens-nantes.net (J. Delaune), guy.le_besnerais@onera.fr (G. Le Besnerais), thomas.voirin@esa.int (T. Voirin), jean-loup.farges@onera.fr (J.L. Farges), clement.bourdarias@astrium.eads.net (C. Bourdarias) Preprint submitted to Robotics and Autonomous Systems June 13, 2015 Manuscript Click here to view linked References	autonomous robot;autonomous system (internet);email;extended kalman filter;image processing;inertial navigation system;performance;planetary scanner;repeatability;robotics;test bench;topography	Jeff Delaune;Guy Le Besnerais;Thomas Voirin;Jean-Loup Farges;Clément Bourdarias	2016	Robotics and Autonomous Systems	10.1016/j.robot.2016.01.007	vision;computer vision;inertial frame of reference;navigation;simulation;natural satellite;accuracy and precision	Robotics	53.91764498090782	-36.15601856186375	198405
5079e786d664549b88003cc2210904c1ba6c2c5c	prediction of vibrations as a measure of terrain traversability in outdoor structured and natural environments	gaussian process regression;texture features;acceleration;autonomous mobile robots;terrain traversability	Terrain recognition is an important task that a mobile robot has to accomplish autonomously to navigate in hazardous territories safely with no additional human monitoring. For this, sensory information should be employed to construct a good model to estimate the degree of traversability of upcoming terrains. In this paper, a regression-based method is proposed to estimate mobile robot vibration from terrain images as a description for terrain traversability. Texture attributes obtained from evaluation of the fractal dimension to describe the terrains were combined with appropriate acceleration features for function approximation using Gaussian Process regression GP. Results showed effectiveness of the method to predict motion data for different terrain configurations in structured and rough environments.		Mohammed Abdessamad Bekhti;Yuichi Kobayashi	2015		10.1007/978-3-319-29451-3_23	acceleration;computer vision;simulation;kriging;statistics	Robotics	53.761447097162794	-31.723804805133017	198467
317d8ca041da67075594286749f2f16051f17841	vision-based localization of an underwater robot in a structured environment	underwater robots;local algorithm;submersibles control systems;vehicles submergibles sistemes de control;underwater vehicles;mobile robots;three dimensional;info eu repo semantics article;position control underwater vehicles mobile robots robot vision;robot vision;position control;12 5 hz vision based localization absolute localization map based localization underwater robot structured environment onboard down looking camera landmark detection landmark tracking real time computation three dimensional position vehicle orientation drift free estimate accuracy velocity based low level controller localization algorithm;robots mobils;robots submarins;robot vision systems cameras underwater vehicles velocity control underwater tracking acoustic sensors motion estimation robot kinematics informatics optical feedback;real time computing	AbstracIThis paper presents a vision-based localization approach for an underwater robot in a ~ t ~ e t u r r d environment. The system is based on a coded pattern placed on the bottom of a water tank and an onhoard downlooking camera. Main features are, absolute and map-based localization, landmark detection and tracking, and real-time computation (12.5 Hz). The proposed system provides threedimensional position and orientation of the vehicle along with its velocity. Accuracy of the drift-free estimates is very high, allowing them to he used as feedback measures of a velocity-based low level controller. The paper details the localization algorithm, by showing some graphical results, and the accuracy of the system.	algorithm;computation;course (navigation);image stitching;real-time clock;robot;velocity (software development)	Marc Carreras;Pere Ridao;Rafael García;Tudor Nicosevici	2003		10.1109/ROBOT.2003.1241718	control engineering;mobile robot;three-dimensional space;computer vision;simulation;computer science;engineering;artificial intelligence	Robotics	55.136540119977695	-35.71791422328569	199380
9e3e58927fc21528fcae5250dcdef17727af668c	minimalistic sensor design in visual-inertial structure from motion	observability;robot sensing systems;standards;gravity;accelerometers;cameras;units measurement control system synthesis observability sensors;uio problem minimalistic sensor design visual inertial sensor fusion camera extrinsic calibration inertial sensor bias complete inertial measurement unit orthogonal accelerometers orthogonal gyroscopes control theory unknown input observability;quaternions;observability cameras accelerometers robot sensing systems standards gravity quaternions	This paper presents a theoretical investigation in the framework of visual-inertial sensor fusion and the results here provided are the extension of our previous contribution in [10], [11]. The general goal of this research is to establish minimalistic visual-inertial sensors settings, which still provide full information even in the most challenging situations, i.e., in the case of unknown camera extrinsic calibration, unknown magnitude of the gravity, unknown inertial sensor bias and when only a single point feature is available. The investigation here provided allows us to conclude that, even in the case of a single point feature, the information provided by a sensor suit composed by a monocular camera and two inertial sensors (along two independent axes and where at least one is an accelerometer) is the same as in the case of a complete inertial measurement unit (i.e., when the inertial sensors consist of three orthogonal accelerometers and three orthogonal gyroscopes). To derive this result, an observability analysis of systems with only one and two inertial sensors is performed. This analysis requires to approach an open problem in control theory, called the Unknown Input Observability (UIO). In this paper we adopt the same method introduced in [10], [11] to solve this UIO problem. The method has been here improved in order to deal with systems more complex than the ones analyzed in [10], [11]. The paper also provides a general discussion on UIO and in particular on the proposed solution.	control theory;sensor;structure from motion	Agostino Martinelli	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7139656	control engineering;inertial measurement unit;inertial reference unit;observability;simulation;gravity;engineering;control theory;accelerometer;quaternion	Robotics	56.49725801316383	-37.557603224554526	199448
190b8a114e0b218ef4ea7e4b11c481b560bfe45d	a visually guided mobile robot acting in indoor environments	ccd camera;obstacle detection;mobile robots indoor environments navigation robot vision systems cameras parallel robots charge coupled devices charge coupled image sensors visual perception process control;mobile robot;path planning;mobile robots;stereo image processing computerised navigation mobile robots parallel processing path planning;control architecture;indoor environment;stereo image processing;visual perception;partially structured environment six wheeled trc labmate obstacle avoidance trajectory following visually guided mobile robot indoor environments vision based navigation system mobile robot ccd cameras stereo pair ground plane obstacle detection;navigation system;parallel processing;computerised navigation	The paper describes the practical implementation of a vision-based navigation system for a mobile robot operating in indoor environments. The robot acquires visual information by means of three CCD cameras mounted on board. A stereo pair is used for ground plane obstacle detection and avoidance, while the third camera is used to locate landmarks and compute the robot's position. Odometric readings are used to guide visual perception by simple 'where to look next' strategies. The whole processing and the control architecture determining the overall behaviour of the robot are mainly implemented on a parallel MIMD machine. Some examples are presented, showing how the robot moves in a partially structured environment reaching the specified goal points with a fair degree of accuracy, avoiding unpredicted obstacles and following trajectories obtained through the cooperation of the various navigation modules running in parallel. >	mobile robot	Marco Fossa;Enrico Grosso;F. Ferrari;M. Magrassi;Giulio Sandini;M. Zapendouski	1992		10.1109/ACV.1992.240298	mobile robot;embedded system;parallel processing;computer vision;simulation;computer science;robot control;mobile robot navigation	Robotics	57.86550467752434	-31.922910005404823	199579
b35ec1779ce55d92e72864a4203e926401c7dad5	robust 3d vision based control and planning	manipulators;image motion analysis;path planning;robust control;robot vision;robust control visual servoing cameras uncertainty calibration robust stability displacement control robustness sliding mode control error correction;displacement control;robot vision image motion analysis robust control path planning manipulators cameras calibration displacement control;planning and control;3d vision;camera calibration;visual servoing;manipulator robust 3d vision eye in hand camera image camera calibration error visually servoed motion robust control path planning;calibration;cameras	In this paper, we present a method to plan and control the 6 DOF displacements of a manipulator from an eye-in-hand camera image. With this method, even in the presence of camera calibration errors, we can formally guarantee that the system is stable and that the entire target remains visible during any visually servoed motion.	camera resectioning;nvidia 3d vision	Philippe Zanne;Guillaume Morel;Franck Plestan	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1302414	robust control;control engineering;computer vision;camera auto-calibration;calibration;camera resectioning;computer science;control theory;motion planning;visual servoing	Robotics	60.697160010186316	-32.35342282363201	199591
33c829000a6a3e280c694f7e0dd8cdceaf583ab9	constant turn model for converted doppler measurement kalman filter	doppler measurement mathematical model position measurement radar tracking target tracking kalman filters equations;kalman filters filtering theory;tracking doppler measurement conversion kalman filter pseudostate equation;position measurement errors constant turn model converted doppler measurement kalman filter discrete temporal evolution equation pseudo state vector constant turn ct motion linear state equation linear kalman filter information extraction pseudo measurements cdmkf converted position measurement kalman filter parallel filtering structure prl cmkf	In this paper, we derive the discrete temporal evolution equation of the pseudo state vector, defined by the converted Doppler (the productive of target true range and range rate) and its first derivative, for the constant turn (CT) motion. The resulted linear state equation allows using of linear Kalman filter to extract information from the pseudo measurements (the productive of range and Doppler measurements) of a target moves with constant speed and constant turn rate. The method is referred to as converted Doppler measurement Kalman filter (CDMKF) and is used in parallel with the converted position measurement Kalman filter (CPMKF) to establish a parallel filtering structure (PRL-CMKF). The validity of the proposed CT model is demonstrated by assessing the performance of the CDMKF and PRL-CMKF. Comparative results show the superior performance of the proposed method especially in challenging scenario with large position measurement errors.	kalman filter;pattern recognition letters	Gongjian Zhou;Bin Zhao;Changjun Yu;Taifan Quan	2014	2014 IEEE Workshop on Statistical Signal Processing (SSP)	10.1109/SSP.2014.6884646	control engineering;electronic engineering;invariant extended kalman filter;ensemble kalman filter;fast kalman filter;control theory;mathematics;extended kalman filter;moving horizon estimation;alpha beta filter	Robotics	58.89209641182423	-37.35035493546971	199926
4125c87cba132df925e7d30f49b2bc5dacc51d18	classification-based wheel slip detection and detector fusion for outdoor mobile robots	detectors;motion control;detection algorithms;support vector machines;mobile robot;dynamic model;mobile robots;robotics;autonomous mobile robot;feature space;orbital robotics;wheels detectors mobile robots support vector machines support vector machine classification extraterrestrial measurements statistics velocity measurement orbital robotics detection algorithms;robot immobilization;dynamic model based algorithm;detection algorithm;robotteknik och automation;outdoor mobile robots;pattern classification;statistics;support vector machine classification;signal recognition;positional information;support vector machine;sensor fusion;dynamic model based algorithm cassification based wheel slip detection detector fusion outdoor mobile robots signal recognition autonomous mobile robot robot immobilization support vector machine;velocity measurement;extraterrestrial measurements;cassification based wheel slip detection;detector fusion;autonomous robot;wheels;support vector machines mobile robots motion control pattern classification sensor fusion	This paper introduces a signal-recognition based approach for detecting autonomous mobile robot immobilization on outdoor terrain. The technique utilizes a support vector machine classifier to form class boundaries in a feature space composed of statistics related to inertial and (optional) wheel speed measurements. The proposed algorithm is validated using experimental data collected with an autonomous robot operating in an outdoor environment. Additionally, two detector fusion techniques are proposed to combine the outputs of multiple immobilization detectors. One technique is proposed to minimize false immobilization detections. A second technique is proposed to increase overall detection accuracy while maintaining rapid detector response. The two fusion techniques are demonstrated experimentally using the detection algorithm proposed in this work and a dynamic model-based algorithm. It is shown that the proposed techniques can be used to rapidly and robustly detect mobile robot immobilization in outdoor environments, even in the absence of absolute position information.	algorithm;autonomous robot;encoder;experiment;feature vector;immobiliser;mathematical model;mobile robot;sensor;support vector machine;tachometer	Chris C. Ward;Karl Iagnemma	2007	Proceedings 2007 IEEE International Conference on Robotics and Automation	10.1109/ROBOT.2007.363878	control engineering;mobile robot;support vector machine;computer vision;simulation;computer science;engineering;artificial intelligence;robotics	Robotics	56.58395913729948	-34.67760750979372	199943
