id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
6c21586b08651aa129b34ea8a1dc56e5a2dadc6b	virtual camera modeling for multi-view simulation of surveillance scenes		A recent trend in research is to leverage on advanced simulation frameworks for the implementation and validation of video surveillance and ambient intelligence algorithms. However, in order to guarantee a seamless transferability between the virtual and real worlds, the simulator is required to represent the real-world target scenario in the best way possible. This includes on the one hand the appearance of the scene and the motion of objects, and, on the other hand, it should be accurate with respect to the sensing equipment that will be used in the acquisition phase. This paper focuses on the latter problem related to camera modeling and control, discussing how noise and distortions can be handled, and implementing an engine for camera motion control in terms of pan, tilt, and zoom, with particular attention to the video surveillance scenario.		Niccoló Bisagno;Nicola Conci	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553409	motion control;computer vision;aperture;ambient intelligence;transferability;zoom;distortion;computer science;artificial intelligence;lens (optics);image resolution	Robotics	53.78523834771624	-45.1370443473045	190757
d7df589ddf2d1a420a8b8a153b1d92372de85a02	projected light beams tracking for efficient 3d reconstruction	ccd camera;image matching;computer vision;optical tracking;image sequences projected light beam tracking 3d image reconstruction machine vision system ccd camera light beam matrix image spot labelling image spot motion motion prediction;image representation;machine vision;image reconstruction;labeling structural beams laser beams layout charge coupled image sensors charge coupled devices cameras image reconstruction machine vision focusing;image sequences optical tracking image reconstruction computer vision image representation image matching;3d representation;3d reconstruction;image sequences	This article outlines how display holograms can be combined with interactive computer graphics. Digitally projected light is used for replaying the holographic content synchronized to the rendering of autostereoscopic or stereoscopic graphics. Modifying the local intensity of the projected light beam allows creating consistent occlusion and shading effects between both—graphics and hologram. This, however, requires depth information of the holographic recording. While flatbed scanners are suitable for estimating surface depth of small to medium size white-light holograms, range scanning is preferred for large-scale holograms. While the integrated graphical elements allow interactivity that is not supported by analog display holograms, the holograms can provide a visual quality that is not possible with today’s three-dimensional displays.	3d reconstruction;3d scanner;autostereoscopy;computer graphics;graphical user interface;holography;human–computer interaction;image scanner;interactivity;shading;stereoscopy	Frédéric Lerasle;Patrick Danès	2001		10.1109/ICIP.2001.958282	3d reconstruction;iterative reconstruction;image warping;image texture;image restoration;computer vision;feature detection;scale space;machine vision;binary image;image processing;computer science;time delay and integration;mathematics;charge-coupled device;motion field;real image;digital image;computer graphics (images)	Graphics	59.03654237063307	-50.96601708027956	191165
506c421d823e0db2b8760280a7a152734fef5e53	3d modeling for mine roadway from laser scanning point cloud	3d modeling mine roadway point cloud cylinder projection poisson equation;reconstruction algorithms;surface reconstruction;three dimensional displays mathematical model poisson equations laser modes solid modeling surface reconstruction reconstruction algorithms;three dimensional displays;solid modeling;mathematical model;structural engineering computing geotechnical engineering mining poisson equation production engineering computing roads;underground tunnel modeling 3d modeling laser scanning point cloud mine roadway surface reconstruction cylinder projection method poisson equation reconstruction method 3d roadway model laser point cloud data roadway laser scanning data;laser modes;poisson equations	For the mine roadway surface reconstruction based on laser scanning point cloud, the cylinder projection method and Poisson reconstruction method were adopted to build the roadway model in this paper. The two methods were introduced in detail including the three modeling parts of the cylinder projection method and the five parts of the Poisson equation reconstruction method. On this basis, two methods were applied to construct approximately real 3D roadway model by processing laser point cloud data and modeling the model of the roadway based on laser scanning point cloud automatically. Finally, the projection method was compared with Poisson reconstruction method in modeling time and modeling effect based on the actual roadway laser scanning data, in which the two methods showed their respective advantages in efficiency and precision. The methods proposed in this paper could also provide reference for the adjacent applications, such as underground tunnel modeling, etc.	3d modeling;cylinder seal;cylinder-head-sector;point cloud;projection method (fluid dynamics)	Jiateng Guo;Jizhou Jiang;Lixin Wu;Wenhui Zhou;Lianhuan Wei	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730160	computer vision;simulation;surface reconstruction;mathematical model;solid modeling;statistics;computer graphics (images)	Visualization	58.496349952303945	-49.1008037442975	191523
b887c6e6fdadda78c854b277c6099854e40e60aa	object surface recovery using a multi-light photometric stereo technique for non-lambertian surfaces subject to shadows and specularities	ultrasound optical imaging;shadows;photometric stereo method;linear least square;height map;surface reflectance;specular reflection;linear least squares solution;photometric stereo;rough surface;surface orientation;reverse engineering	This paper presents a new multi-light source photometric stereo system for reconstructing images of various characteristics of non-Lambertian rough surfaces with widely varying texture and specularity. Compared to the traditional three-light photometric stereo method, extra lights are employed using a hierarchical selection strategy to eliminate the effects of shadows and specularities, and to make the system more robust. We also show that six lights is the minimum needed in order to apply photometric stereo to the entire visible surface of any convex object. Experiments on synthetic and real scenes demonstrate that the proposed method can extract surface reflectance and orientation effectively, even in the presence of strong shadows and highlights. Hence, the method offers advantages in the recovery of dichromatic surfaces possessing rough texture or deeply relieved topographic features, with applications in reverse engineering and industrial surface inspection. Experimental results are presented in the paper.	lambertian reflectance;photometric stereo	Jiuai Sun;Melvyn L. Smith;Lyndon N. Smith;P. Sagar Midha;Jeff Bamber	2007	Image Vision Comput.	10.1016/j.imavis.2006.04.025	computer vision;specular reflection;shadow;photometric stereo;computer science;heightmap;reverse engineering	Vision	58.33328630558276	-51.88144819303762	191701
cee4b7bb01deaa80f46a61db6243d88f1f956265	towards a unified approach between digitizations of linear objects and discrete analytical objects				Christoph Lincke;Charles Albert Wüthrich	2000			computer vision;artificial intelligence;computer science	Theory	60.4583345802753	-46.58502237901037	191767
edb256396985fe67539019d8790e85b10ad9952b	interpolating vertical parallax for an autostereoscopic three-dimensional projector array	projection systems	We present a technique for achieving tracked vertical parallax for multiple users using a variety of autostereoscopic projector array setups, including frontand rear-projection and curved display surfaces. This hybrid parallax approach allows for immediate horizontal parallax as viewers move left and right and tracked parallax as they move up and down, allowing cues such as three-dimensional (3-D) perspective and eye contact to be conveyed faithfully. We use a low-cost RGB-depth sensor to simultaneously track multiple viewer head positions in 3-D space, and we interactively update the imagery sent to the array so that imagery directed to each viewer appears from a consistent and correct vertical perspective. Unlike previous work, we do not assume that the imagery sent to each projector in the array is rendered from a single vertical perspective. This lets us apply hybrid parallax to displays where a single projector forms parts of multiple viewers’ imagery. Thus, each individual projected image is rendered with multiple centers of projection, and might show an object from above on the left and from below on the right. We demonstrate this technique using a dense horizontal array of pico-projectors aimed into an anisotropic vertical diffusion screen, yielding 1.5 deg angular resolution over 110 deg field of view. To create a seamless viewing experience for multiple viewers, we smoothly interpolate the set of viewer heights and distances on a per-vertex basis across the array’s field of view, reducing image distortion, cross talk, and artifacts from tracking errors. © 2014 SPIE and IS&T [DOI: 10.1117/1.JEI.23.1.011005]	angularjs;autostereoscopy;color;distortion;graphics;interactivity;interpolation;mcop;movie projector;multi-user;parallax;range imaging;seamless3d;smoothing;stereo display;video card;video projector	Andrew Jones;Koki Nagano;Jing Liu;Jay Busch;Xueming Yu;Mark T. Bolas;Paul E. Debevec	2014	J. Electronic Imaging	10.1117/1.JEI.23.1.011005	computer vision;computer science;computer graphics (images)	HCI	59.11974324888229	-50.918557807145405	192899
58229454f2c85684bd0e38834eac04fbfb1750d5	symmetry-based completion		Acquired images often present missing, degraded or occluded parts. Inpainting techniques try to infer lacking information, usually from valid information nearby. This work introduces a new method to complete missing parts from an image using structural information of the image. Since natural and human-made objects present several symmetries, the image structure is described in terms of axial symmetries, and extrapolating the symmetries of the valid parts completes the missing ones. In particular, this allows inferring both the edges and	database;extrapolation;inpainting;rendering (computer graphics)	Thiago Pereira;Renato Paes Leme;Luiz Velho;Thomas Lewiner	2009			quadrangle;computer science;computer vision;artificial intelligence;mechanical engineering;lever	Vision	57.37470416040402	-50.54257366084564	192948
c0a45ebd5a18721553cdb8593c0dbbb395b4c634	registration of arbitrary multi-view 3d acquisitions	3d imaging;close range photogrammetry;3d registration;multi view registration	Registration of 3D meshes of smooth surfaces is performed by tracking the acquisition system. The tracking is performed using photogrammetric techniques. Careful calibration of all objects in play enable a registration accuracy of *** . Targets are used to asses the precision of the registration, but the method does not rely on the use of targets and can be used for the registration of featureless surfaces.	3d computer graphics;photogrammetry	Camille Simon;Rainer Schütze;Frank Boochs;Franck Marzani	2013	Computers in Industry	10.1016/j.compind.2013.03.017	stereoscopy;computer vision;simulation;computer graphics (images)	Vision	55.414814583347834	-47.03629902120355	193368
605b00a1329963204ad7b11ca0319d5eb8906c39	a self-calibrating method for photogeometric acquisition of 3d objects	modelizacion;digital projector;a priori calibrated systems;analisis escena;modele geometrique;analyse scene;acquisition processing 3d objects self calibrating photogeometric method a priori calibrated systems digital projector virtual camera multiviewpoint 3d acquisition method surface position registration;normal superficie;digitizing;surface normal;3d objects;multiviewpoint 3d acquisition method;digital camera;structured light;surface position registration;photometry digital cameras hardware light sources robustness surface reconstruction solid modeling shape image reconstruction image analysis;numerisation;color model;indexing terms;surface reconstruction;computer vision;modelisation;digital cameras;self calibrating photogeometric method;light source;shape;photometry;image reconstruction;image registration;solid modeling;image registration calibration cameras computer vision;source lumineuse;fuente luminosa;normale surface;geometric modeling;numerizacion;robustness;photometrie;image analysis;geometric model;digitization and image capture;acquisition processing;geometric modeling digitization and image capture scene analysis;source lumiere;quality model;modeling;off the shelf;calibration;fotometria;cameras;light sources;geometrical model;scene analysis;hardware;virtual camera;modelo geometrico	We present a self-calibrating photogeometric method using only off-the-shelf hardware that enables quickly and robustly obtaining multimillion point-sampled and colored models of real-world objects. Some previous efforts use a priori calibrated systems to separately acquire geometric and photometric information. Our key enabling observation is that a digital projector can be simultaneously used as either an active light source or as a virtual camera (as opposed to a digital camera, which cannot be used for both). We present our self-calibrating and multiviewpoint 3D acquisition method, based on structured light, which simultaneously obtains mutually registered surface position and surface normal information and produces a single high-quality model. Acquisition processing freely alternates between using a geometric setup and using a photometric setup with the same hardware configuration. Further, our approach generates reconstructions at the resolution of the camera and not only the projector. We show the results of capturing several high-quality models of real-world objects.	calibration;digital camera;lambertian reflectance;normal (geometry);numerous;photometric stereo;photometry;physical object;pixel;preparation;published comment;registration;sampling - surgical action;structured light;surface detail;video projector;virtual camera system	Daniel G. Aliaga;Yi Xu	2010	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2009.202	computer vision;image analysis;computer science;geometric modeling;computer graphics (images)	Vision	57.44454457987494	-49.661866255709235	193384
0bca8d9ce6d3c7b640639356db12f56b08c54670	geometry-corrected light field rendering for creating a holographic stereogram	printing;photogrammetry;image resolution;query processing;geometry;arrays;cameras geometry arrays lighting printing image resolution rendering computer graphics;stereo image processing;holography;lighting;rendering computer graphics;stereo image processing cameras holography photogrammetry query processing rendering computer graphics;relevant rays querying geometry corrected light field rendering printed holographic stereogram depth of field maximization depth dependent surface detail angular resolution light field data digital still camera 2d translation stage hogel generation holographic elements light field reprojection photogrammetrically recovered object model;cameras	We present a technique to record and process a light field of an object in order to produce a printed holographic stereogram. We use a geometry correction process to maximize the depth of field and depth-dependent surface detail even when the array of viewpoints comprising the light field is coarsely sampled with respect to the angular resolution of the printed hologram. We capture the light field data of an object with a digital still camera attached to a 2D translation stage, and generate hogels (holographic elements) for printing by reprojecting the light field onto a photogrammetrically recovered model of the object and querying the relevant rays to be produced by the hologram with respect to this geometry. This results in a significantly clearer image of detail at different depths in the printed holographic stereogram.	angularjs;digital camera;holographic display;holographic principle;holography;image resolution;light field;linear stage;photogrammetry;printing;stereoscopy;surface detail	Joel Jurik;Thomas Burnett;Michael Klug;Paul E. Debevec	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239344	computer vision;image resolution;rendering;computer science;lighting;geometry;holography;photogrammetry;computer graphics (images)	Vision	58.289780814034714	-50.78846791624497	193710
a082cbde3c42406e25dba7dbbd22af1b6f0b505a	a feature points matching method based on window unique property of pseudo-random coded image	structured light;coded structured light;random coding;machine vision;feature points matching;window unique property;3d reconstruction;matching method	Proposed a method for object feature points matching used in active machine vision technique. By projecting pseudo-random coded structured light onto the object to add code information, its feature points can be easily identified exclusively taking advantage of the window unique property of pseudo-random array. Then, the 3D coordinates of object both on camera plane and code plane can be obtained by decoding process, which will provide the foundation for further 3D reconstruction. Result of simulation shows that this method is easy to operate, calculation simple and of high matching precision for object feature points matching.		Hui Chen;Shiwei Ma;Hao Zhang;Zhonghua Hao;Junfeng Qian	2010		10.1007/978-3-642-15597-0_37	3d reconstruction;computer vision;structured light;machine vision;computer science;pattern recognition;mathematics	EDA	56.887821993624826	-49.51829110507791	193966
ecb093209569344a1fd580db30ef996010c8314b	depth estimation for semi-automatic 2d to 3d conversion	motion estimation;computer vision;2d to 3d conversion;depth estimation	The conversion of monoscopic footage into stereoscopic or multiview content is a difficult and time consuming task. A number of semi-automatic methods have been developed to speed up the process and provide some control to the user. However these methods require that the user provide detailed labels indicating the relative depth of objects in the scene. In this paper we present a method to automatically estimate depth in such a way that it is amenable to semi-automatic conversion. The method is designed to simplify the depth labelling task so that the user does not have to provide as many depth labels.	2d to 3d conversion;2d-plus-depth;semiconductor industry;stereoscopy	Richard Rzeszutek;Raymond Phan;Dimitrios Androutsos	2012		10.1145/2393347.2396320	computer vision;simulation;computer science;motion estimation;2d to 3d conversion;computer graphics (images)	HCI	59.61856624547385	-50.685147899127664	194734
3fa03c62d9a39f87f1a499ad970643b0469f6b57	photo repair and 3d structure from flatbed scanners	photometric stereo.;photo repair;scanners;3d reconstruction;photometric stereo;texture synthesis	We introduce a technique that allows 3D information to be captured from a conventional flatbed scanner. The technique requires no hardware modification and allows untrained users to easily capture 3D datasets. Once captured, these datasets can be used for interactive relighting and enhancement of surface detail on physical objects. We have also found that the method can be used to scan and repair damaged photographs. Since the only 3D structure on these photographs will typically be surface tears and creases, our method provides an accurate procedure for automatically detecting these flaws without any user intervention. Once detected, automatic techniques, such as infilling and texture synthesis, can be leveraged to seamlessly repair such damaged areas. We first present a method that is able to repair damaged photographs with minimal user interaction and then show how we can achieve similar results using a fully automatic process.	align (company);image rectification;image scanner;interactive media;justin (robot);list of hewlett-packard products;palo;pixel;printing;raw image format;rectifier;sensor;surface detail;texture synthesis	Ruggero Pintus;Thomas Malzbender;Oliver Wang;Ruth Bergman;Hila Nachlieli;Gitit Ruckenstein	2009		10.1007/978-3-642-11840-1_24	computer vision;computer graphics (images)	Graphics	60.25793711088571	-49.993940674915805	194961
c2d6fcb629baeadf3f7336a8d639274dd7b281d8	resolving the generalized bas-relief ambiguity by entropy minimization	minimisation;light source strength;entropy minimization;image resolution;albedo distribution;snow;reflectivity;entropy light sources geometry reflectivity photometry surface reconstruction lighting snow image resolution machine vision;geometry;computational geometry;inference mechanisms;prior knowledge;uncalibrated photometric stereo;surface reconstruction;generalized bas relief ambiguity;computer vision;surface geometry;statistical distributions;gbr ambiguity;photometry;photometric stereo;machine vision;gbr transformation;stereo image processing computational geometry computer vision inference mechanisms minimisation statistical distributions;stereo image processing;linear transformation;entropy;lighting;intuitive reasoning;intuitive reasoning generalized bas relief ambiguity gbr ambiguity uncalibrated photometric stereo entropy minimization surface geometry light source strength light source direction lambertian object gbr transformation albedo distribution machine vision;light source direction;light sources;lambertian object	It is well known in the photometric stereo literature that uncalibrated photometric stereo, where light source strength and direction are unknown, can recover the surface geometry of a Lambertian object up to a 3-parameter linear transform known as the generalized bas relief (GBR) ambiguity. Many techniques have been proposed for resolving the GBR ambiguity, typically by exploiting prior knowledge of the light sources, the object geometry, or non-Lambertian effects such as specularities. A less celebrated consequence of the GBR transformation is that the albedo at each surface point is transformed along with the geometry. Thus, it should be possible to resolve the GBR ambiguity by exploiting priors on the albedo distribution. To the best of our knowledge, the only time the albedo distribution has been used to resolve the GBR is in the case of uniform albedo. We propose a new prior on the albedo distribution : that the entropy of the distribution should be low. This prior is justified by the fact that many objects in the real-world are composed of a small finite set of albedo values.	lambertian reflectance;photometric stereo;subpixel rendering	Neil Gordon Alldrin;Satya P. Mallick;David J. Kriegman	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383208	probability distribution;computer vision;minimisation;entropy;snow;photometric stereo;image resolution;surface reconstruction;machine vision;photometry;computational geometry;computer science;lighting;mathematics;reflectivity;linear map	Vision	55.29478900172553	-51.96207617683845	196136
fb59d8df6a71db1846343d1fcb300ec73e76ae6f	3d face texture stitching based on differential coordinates	texture stitching;differential coordinates;3d face model	In this paper, we present a new method for 3D face texture stitching based on Differential Coordinates in which 3D texture patch from different face samples can be seamless stitched patch together. Usually, traditional stitching methods for geometric textures involve two works: stitching on shape and stitching on textures. None of them combine these two works. 3D face model is composed of two parts information: surface geometry and texture elements. A natural and realistic face model can be created by interactions between them. So, the approach presented in this paper can get good result. We demonstrate that our approach enables to edit texture elements while considering the shape of face sample.		Yun Ge;Baocai Yin;Yanfeng Sun;Hengliang Tang	2011	Trans. Edutainment	10.1007/978-3-642-22639-7_3	computer vision;geography;image stitching;engineering drawing;computer graphics (images)	Graphics	57.589626449045106	-49.77299385164575	196223
1a86e03c229adb5b94e1f43f8508f033f74e94ae	datasets and benchmarks for densely sampled 4d light fields	inproceedings	Light field datasets thirteen high quality densely sampled light fields seven CG generated datasets with ground truth disparity four of these with ground truth segmentation six real world datasets captured using a gantry one transparency dataset with ground truth disparity for both the surface as well as an object behind it Suitable for the evaluation of continuous methods for light field analysis based on epipolar plane images X Code and benchmarks	benchmark (computing);binocular disparity;display resolution;epipolar geometry;ground truth;light field	Sven Wanner;Stephan Meister;Bastian Goldlücke	2013		10.2312/PE.VMV.VMV13.225-226	computer science	Vision	55.321506213223486	-47.00712077671327	196490
b91c606c463749b40ab1dda381b17225e3064092	auto-calibration - kruppa's equations and the intrinsic parameters of a camera		Auto-calibration may be defined as the process of finding the intrinsic parameters of a camera from real image data. Recent techniques for finding these parameters rely upon solving equations which relate the epipolar geometry of two camera positions with the intrinsic parameters, equations known as Kruppa's equations[4, 2]. These techniques involve a very time consuming numerical process, and yet only produce two of the intrinsic parameters, the focal length and the aspect ratio, to an acceptable degree of accuracy. Further processes which, for example, compute the camera's movement need to assume standard values for the other parameters[4]. In this paper, we present a method of solving Kruppa's equations for the focal length and the aspect ratio which is suitable for a real-time system, together with details of experiments using simulated noisy data which show that its accuracy is comparable with the previous method.	camera resectioning;epipolar geometry;equation solving;experiment;focal (programming language);numerical analysis;real-time clock;real-time computing;signal-to-noise ratio;simulation	S. D. Hippisley-Cox;John Porrill	1994		10.5244/C.8.76		Vision	53.9203789055625	-49.23065424029219	197107
7b34b3f51c873c11aea217eef4b76322a8ac9ec1	accurate sensing of scene geo-context via mobile visual localization	scene parsing;image geo tagging;mobile visual localization	Image geo-tagging has drawn a great deal of attention in recent years. The geographic information associated with images can be used to promote potential applications such as location recognition or virtual navigation. In this paper, we propose a novel approach for accurate mobile image geo-tagging in urban areas. The approach is able to provide a comprehensive set of geo-context information based on the current image, including the real location of the camera and the viewing angle, as well as the location of the captured scene. Moreover, the parsed building facades and their geometric structures can also be estimated. First, for the image to be geo-tagged, we perform partial duplicate image retrieval to filter crowd-sourced images capturing the same scene. We then employ the structure-from-motion technique to reconstruct a sparse 3D point cloud of the scene. Meanwhile, the geometric structure of the query image is analyzed to extract building facades. Finally, by combining the reconstructed 3D scene model and the extracted structure information, we can register the camera location and viewing direction to a real-world map. The captured building location and facade orientation are also aligned. The effectiveness of the proposed system is demonstrated by experiment results.	crowdsourcing;experiment;geotagging;image retrieval;parsing;point cloud;sparse matrix;structure from motion;viewing angle;viewing cone	Heng Liu;Houqiang Li;Tao Mei;Jiebo Luo	2013	Multimedia Systems	10.1007/s00530-013-0344-y	computer vision;multimedia;computer graphics (images)	Vision	55.80687048434687	-45.493822384080985	197240
e2ed37d49ca0c643d168d64a551ac50639bf79fc	generalised linear pose estimation	psi_visics;robot navigation;indoor environment;pose estimation	This paper investigates several aspects of 3D-2D camera pose estimation, aimed at robot navigation in poorly-textured scenes. The major contribution is a fast, linear algorithm for the general case with six or more points. We show how to specialise this to work with only four or five points, which is of utmost importance in a test and hypothesis framework. Our formulation allows for an easy inclusion of lines, as well as the handling of other camera geometries, such as stereo rigs. We also treat the special case of planar motion, a valid restriction for most indoor environments. We conclude the paper with extensive simulated tests and a real test case, which substantiate the algorithm’s usability for our application domain.	3d pose estimation;algorithm;application domain;matlab;planar (computer graphics);robotic mapping;simulation;stereoscopy;test case;usability	Andreas Ess;Alexander Neubeck;Luc Van Gool	2007		10.5244/C.21.22	computer vision;simulation;pose;3d pose estimation;computer science;computer graphics (images)	Robotics	53.83439295387352	-47.85736263457788	197333
dd55567ef6ac4300fa110442bdb809cabe248d99	post-processing of uav-captured images for enhanced mapping by image stitching	optical distortion;image processing;distortion;lenses;matrix converters;cameras;aircraft	This report introduces 3 post-processing techniques that can be used on images captured from an unmanned aerial vehicle (UAV) to distort the images in such a way as to make all the images appear as though they were taken from a uniform altitude, pitch, roll and yaw and from a rectilinear lens, meaning that when the images are stitched together the resulting map is accurate. The recent increase in demand for UAV technologies in the marketplace has been illustrated by a host of new products from major companies. The work in this report adds to the field of imaging from UAV's - for example, the creation of an aerial map. A current limitation in technology is the distortion of images captured by UAVs. This distortion is caused by turbulence as well as the necessary pitch and roll rotations required to move horizontally. This report introduces techniques which make images captured from fisheye lenses appear rectilinear; make all images appear of uniform scale, regardless of the height at which they were captured; and correct for any skew distortions caused by the pitch, roll and yaw of the UAV. All algorithms were developed in Matlab and tested on actual UAV images. This report focuses on rotary wing UAV's such as quad-copters and hexa-copters, although some of the principles discussed could be applied to fixed-wing aircraft as well.	aerial photography;algorithm;distortion;fisheye;image stitching;matlab;regular grid;rotary woofer;turbulence;unmanned aerial vehicle;video post-processing;yaws	Jacques F. Nel	2015	2015 IEEE 5th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2015.7391322	computer vision;simulation;engineering;computer graphics (images)	Robotics	56.8311143951373	-49.001433410063555	197344
ab70531811eb17feda1ef6ac81e35f7cd45ee113	a camera self-calibration method based on dual constraints of multi-view images	reprojection error;cameras calibration geometry three dimensional displays feature extraction genetic algorithms computer vision;dual constraints;computer vision;epipolar geometry;multi view images;cameras calibration;genetic algorithm;reprojection error multi view images camera calibration dual constraints genetic algorithm epipolar geometry;camera calibration;calibration;3d reconstruction;cameras;fitness function;camera parameters camera self calibration method dual constraints multi view images 3d information 3d reconstruction genetic algorithm geometry matching error reprojection error;no reference	Camera calibration is the essential step of obtaining 3D information from 2D views in the field of computer vision, which is widely used in the area of 3D reconstruction, navigation, visual supervision, etc. A camera self-calibration method based on dual constraints of multi-view images is proposed to make calibration possible when there is no reference calibration block or the movement of camera is arbitrary. In this method, multi-view images are preprocessed first to select three images which are most suitable for camera self-calibration, then we use the method based on the genetic algorithm, the camera parameters are finally estimated by using epipolar geometry matching error and reprojection error as fitness functions as two steps. Experimental results show that the proposed camera self-calibration method is correct and effective.	3d reconstruction;camera resectioning;computer vision;epipolar geometry;fitness function;genetic algorithm;real-time computing;real-time transcription;reprojection error;requirement;scale-invariant feature transform;self-similarity;software release life cycle;virtual reality headset	Feng Liu;Weiwei Han;Qiong Xu;Qiongjie Lin;Jingjing Fan	2011	2011 International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2011.6096839	triangulation;stereo camera;computer vision;camera auto-calibration;camera matrix;camera resectioning;mathematics;optics;pinhole camera model;epipolar geometry;computer graphics (images)	Vision	54.310649433047324	-48.84907644567374	197897
00420eab85bdb70c0f74a6b889dbe303c439c22d	a rational function lens distortion model for general cameras	lens distortion;wide angle lenses;photographic lenses;linear estimation;kernel;lens geometry;image coordinates;rational functions photographic lenses nonlinear distortion cameras motion estimation geometry;3d scene;geometry;motion estimation;layout;lenses cameras solid modeling nonlinear distortion geometry radio frequency motion estimation layout pixel kernel;epipolar geometry;nonlinear distortion;epipolar geometry rational function lens distortion model radial lens distortion wide angle lenses catadioptric lenses linear motion estimation lens geometry 3d scene image coordinates nonlinear image distortion estimation;radio frequency;radial lens distortion;pixel;solid modeling;lenses;catadioptric lenses;rational functions;rational function;rational function lens distortion model;cameras;linear motion estimation;nonlinear image distortion estimation;nonlinear model	"""We introduce a new rational function (RF) model for radial lens distortion in wide-angle and catadioptric lenses, which allows the simultaneous linear estimation of motion and lens geometry from two uncalibrated views of a 3D scene. In contrast to existing models which admit such linear estimates, the new model is not specialized to any particular lens geometry, but is sufficiently general to model a variety of extreme distortions. The key step is to define the mapping between image (pixel) coordinates and 3D rays in camera coordinates as a linear combination of nonlinear functions of the image coordinates. Like a """"kernel trick"""", this allows a linear algorithm to estimate nonlinear models, and in particular offers a simple solution to the estimation of nonlinear image distortion. The model also yields an explicit form for the epipolar curves, allowing correspondence search to be efficiently guided by the epipolar geometry. We show results of an implementation of the RF model in estimating the geometry of a real camera lens from uncalibrated footage, and compare the estimate to one obtained using a calibration grid."""	aeron chair;algorithm;distortion;epipolar geometry;general-purpose modeling;kernel method;nonlinear system;pixel;radial (radio);radio frequency;webcam	David Claus;Andrew W. Fitzgibbon	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.43	computer vision;rational function;computer science;mathematics;geometry;pinhole camera model;epipolar geometry	Vision	54.65567424754649	-50.04576980915087	198865
7022a6dc626e8593f21ef2ebb98a4cd9f95f4ab6	wound detection and reconstruction using rgb-d camera	databases;histograms;wounds;three dimensional displays;image color analysis;image reconstruction;cameras	The advent of inexpensive RGB-D sensors pioneered by the original Kinect sensor, has paved the way for a lot of innovations in computer and robot vision applications. In this article, we propose a system which uses the new Kinect 2 sensor in a medical application for the purpose of detection and 3D reconstruction of chronic wounds. Wound detection is based on a per block classification of wound tissue using color histograms and the nearest neighbor approach. The 3D reconstruction is similar to KinectFusion where ICP is used for determining the rigid body transformation, color enhanced TSDF is applied for scene fusion, while the marching cubes algorithm is used for creating a surface mesh. The entire system is implemented in CUDA which enables real-time operation. The end result of the developed system is a precise 3D colored model which can be used for determining a correct therapy and treatment of chronic wounds.	3d reconstruction;algorithm;cuda;kinect;marching cubes;olap cube;real-time clock;sensor	Damir Filko;Emmanuel Karlo Nyarko;Robert Cupec	2016	2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2016.7522325	iterative reconstruction;computer vision;simulation;histogram;statistics;computer graphics (images)	Robotics	60.066097630007874	-47.59149417328709	198960
ec450ac95699b7f927c597d0ddf81f4b72c1c8ec	high-coverage 3d scanning through online structured light calibration	settore inf 01 informatica;surface coverage high coverage 3d scanning online structured light calibration 3d scanning techniques calibrated imaging cameras structured light source cameras geometry projector optics online calibration method seamless process scanning shots unconstrained imaging model;cameras calibration;cameras three dimensional displays calibration accuracy monitoring standards interpolation	Many 3D scanning techniques rely on two or more well calibrated imaging cameras and a structured light source. Within these setups the light source does not need any calibration. In fact the shape of the target surface can be inferred by the cameras geometry alone, while the structured light is only exploited to establish stereo correspondences. Unfortunately, this approach requires each reconstructed point to exhibit an unobstructed line of sight from three independent points of views. This requirement limits the amount of scene points that can be effectively captured with each shot. To overcome this restriction, several systems that combine a single camera with a calibrated projector have been proposed. However, this type of calibration is more complex to be performed and its accuracy is hindered by both the indirect measures involved and the lower precision of projector optics. In this paper we propose an online calibration method for structured light sources that computes the projector parameters concurrently with regular scanning shots. This results in an easier and seamless process that can be applied directly to most current scanning systems without modification. Moreover, we attain high accuracy by adopting an unconstrained imaging model that is able to handle well even less accurate optics. The improved surface coverage and the quality of the measurements are thoroughly assessed in the experimental section.	3d scanner;experiment;seamless3d;structured light;video projector	Andrea Albarelli;Luca Cosmo;Filippo Bergamasco;Andrea Torsello	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.699	structured-light 3d scanner;computer vision;computer science;computer graphics (images)	Vision	58.09939490938252	-50.067218339982226	199081
4b7f6700c5b45cef4e95ff77918398adad9441e4	robust motion from space curves and 3d reconstruction from multiviews using perpendicular double stereo rigs	shape from silhouette;plane curve;silhouette;measurement error;surface representation;quantization noise;space curves;motion estimation;perpendicular double stereo;three dimensional;multiple views;3d model;unique points;image sequence;visual hull;curve matching;structure and motion;structure from motion;3d reconstruction	This paper proposes a robust curve based method to reconstruct 3D model of an object from image sequences captured by two perpendicular stereo rigs. First, corresponding points and the geometry of points are computed in stereo images by extracting unique space curves. A new algorithm is proposed to extract unique space curves from plane curves in stereo images based on curvature and torsion consistency. The proposed method provides accurate geometry of the curve points with extremely reduced number of outliers. Contrarily to the standard sparse approaches that need sub-pixel accuracy to compute structure and motion, the proposed curve matching method deals with pixel accuracy information. More importantly, it finds the correspondence based on curve shape and does not use any photometric information. This property makes the matching process very robust against the color and intensity maladjustment of stereo rigs. Second, the recovered space curves are employed to estimate robust motion by minimizing the curve distance in the next sequence of stereo images. An efficient structure of stereo rigs - perpendicular double stereo - is proposed to improve accuracy of motion estimation. We discuss and prove its properties mathematically. Third, a set of calibrated virtual cameras are constructed from estimated motion information to take advantage of the shape-from-silhouette using multiple views and extract the object's visual hull as fine as possible. As a whole, a complete automatic and practical system of three-dimensional modeling from raw images captured by calibrated perpendicular double stereo rigs to surface representation is proposed. Fine motion estimation is the main advantage of the proposed method using perpendicular stereo rigs, which makes use of the space curves in a large base line camera setup. While the previous methods of motion estimation suffer from the statistical bias due to quantization noise, measurement error, and outliers in the input data set, the proposed method overcomes the bias problem even in pixel-level information. Experimental results demonstrate the privileged performance of the proposed method for a variety of object shapes and textures.	3d reconstruction	Hossein Ebrahimnezhad;Hassan Ghassemian	2008	Image Vision Comput.	10.1016/j.imavis.2008.01.002	3d reconstruction;computer stereo vision;three-dimensional space;computer vision;plane curve;structure from motion;quantization;computer science;motion estimation;mathematics;geometry;silhouette;computer graphics (images);observational error	Vision	55.56563133171234	-50.31466619756042	199099
6741c0f0f980397730e8a232a38713d6f4140df9	image-based pose estimation of an endoscopic instrument	biomedical measurements;instruments;solid modelling cameras endoscopes medical image processing pose estimation rendering computer graphics;instruments endoscopes rendering computer graphics estimation surgery biomedical measurements;three dimensional;computer graphic;position estimation image based pose estimation flexible endoscopic instrument endoscopic images three dimensional rendering endoscopic camera colon anatomical model electromagnetic tracker;estimation;medical image processing;endoscopes;position estimation;surgery;rendering computer graphics;cameras;solid modelling;pose estimation	This video shows a system that estimates the pose of a flexible endoscopic instrument, based on the endoscopic images. A three-dimensional rendering of the instrument is matched to the actual instrument that is observed through the endoscopic camera. This system was evaluated in an anatomical model of a colon. The estimated position of the tip of the instrument was compared to measurements performed with an electromagnetic tracker. The errors of the position estimation were 2 mm, 2.2 mm and 1.7 mm in the horizontal (x), vertical (y) and away-from-camera (z) directions, respectively.	3d pose estimation;colon classification;emoticon;laser tracker	Rob Reilink;Stefano Stramigioli;Sarthak Misra	2012	2012 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2012.6224923	three-dimensional space;computer vision;estimation;simulation;pose;computer science;statistics;computer graphics (images)	Robotics	55.89313054746616	-48.865928161792795	199310
dcf12f05e908ae1657b53f5a5cd771392ef41e2c	can the sun's direction be estimated from an image prior to the computation of object shape?	image irradiance equation;shape from shading;computational techniques;prior knowledge;journal article;feasibility;estimation;light source direction;domain specificity	Various computational techniques have been developed that performreasonably well in inferring shape from shading. However, thesetechniques typically require substantial prerequisite information ifthey are to evolve an estimate of surface shape. It is thereforeinteresting to consider how depth might be inferred from shadinginformation without prior knowledge of various scene conditions. Oneapproach has been to undertake a pre-processing step ofestimating the light-source direction, thereby providing input tothe computation of shape from shading. In this paper, we presentevidence that a versatile light-source-direction estimator isunattainable, and propose that, in the absence of domain-specificknowledge, shape and light-source direction should be determined ina coupled manner	computation;photometric stereo;preprocessor;shading	Wojciech Chojnacki;Michael J. Brooks;Danny Gibbins	1997	Journal of Mathematical Imaging and Vision	10.1023/A:1008201505044	feasibility study;computer vision;estimation;photometric stereo;mathematics;statistics	Vision	55.91297614148904	-50.83360682563453	199451
