id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
22029beb936c9871757813758c5ae3e5820260c9	proximity distribution kernels for geometric context in category recognition	object recognition;vector quantized local feature descriptors;photometric information;graz 02;support vector machines;geometrically unconstrained approaches;mercer condition;kernel photometry computer vision lighting statistics layout shape solid modeling data systems computer science;proximity distribution kernels;pascal challenge proximity distribution kernels geometric context category recognition vector quantized local feature descriptors object recognition photometric information mercer condition support vector machine visual categorization geometrically unconstrained approaches over constrained affine procrustes graz 01 graz 02;local features;vector quantisation object recognition support vector machines;visual categorization;pascal challenge;graz 01;vector quantizer;geometric context;support vector machine;vector quantisation;category recognition;over constrained affine procrustes	"""We propose using the proximity distribution of vector- quantized local feature descriptors for object and category recognition. To this end, we introduce a novel """"proximity distribution kernel"""" that naturally combines local geometric as well as photometric information from images. It satisfies Mercer's condition and can therefore be readily combined with a support vector machine to perform visual categorization in a way that is insensitive to photometric and geometric variations, while retaining significant discriminative power. In particular, it improves on the results obtained both with geometrically unconstrained """"bags of features"""" approaches, as well as with over-constrained """"affine procrustes."""" Indeed, we test this approach on several challenging data sets, including Graz-01, Graz-02, and the PASCAL challenge. We registered the average performance at 91.5% on Graz-01, 82.7% on Graz-02, and 74.5% on PASCAL. Our approach is designed to enforce and exploit geometric consistency among objects in the same category; therefore, it does not improve the performance of existing algorithms on datasets where the data is already roughly aligned and scaled. Our method has the potential to be extended to more complex geometric relationships among local features, as we illustrate in the experiments."""	affine shape adaptation;algorithm;best, worst and average case;categorization;experiment;kernel (operating system);pascal;support vector machine	Haibin Ling;Stefano Soatto	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4408859	support vector machine;computer vision;computer science;machine learning;pattern recognition;mathematics;geometry	Vision	30.948884054123667	-47.316559121254656	159238
d1ca34311d6572ab27a85d7f03ce8fed2d2fb89d	monocular total capture: posing face, body, and hands in the wild		We present the first method to capture the 3D total motion of a target person from a monocular view input. Given an image or a monocular video, our method reconstructs the motion from body, face, and fingers represented by a 3D deformable mesh model. We use an efficient representation called 3D Part Orientation Fields (POFs), to encode the 3D orientations of all body parts in the common 2D image space. POFs are predicted by a Fully Convolutional Network (FCN), along with the joint confidence maps. To train our network, we collect a new 3D human motion dataset capturing diverse total body motion of 40 subjects in a multiview system. We leverage a 3D deformable human model to reconstruct total body pose from the CNN outputs by exploiting the pose and shape prior in the model. We also present a texture-based tracking method to obtain temporally coherent motion capture output. We perform thorough quantitative evaluations including comparison with the existing body-specific and hand-specific methods, and performance analysis on camera viewpoint and human pose changes. Finally, we demonstrate the results of our total body motion capture on various challenging in-the-wild videos. Our code and newly collected human motion dataset will be publicly shared.		Donglai Xiang;Hanbyul Joo;Yaser Sheikh	2018	CoRR			Vision	31.44383638916318	-50.8071007524496	159694
e44b8d6df0ec8f92f5cac9a8294c67a180a42ec0	multi feature deconvolutional faster r-cnn for precise vehicle detection in aerial imagery		Accurate detection of objects in aerial images is an important task for many applications such as traffic monitoring, surveillance, reconnaissance and rescue tasks. Recently, deep learning based detection frameworks clearly improved the detection performance on aerial images compared to conventional methods comprised of hand-crafted features and a classifier within a sliding window approach. These deep learning based detection frameworks use the output of the last convolutional layer as feature map for localization and classification. Due to the small size of objects in aerial images, only shallow layers of standard models like VGG-16 or small networks are applicable in order to provide a sufficiently high feature map resolution. However, high-resolution feature maps offer less semantic and contextual information, which results in approaches being more prone to false alarms due to objects with similar shapes especially in case of tiny objects. In this paper, we extend the Faster R-CNN detection framework to cope this issue. Therefore, we apply a deconvolutional module that up-samples low-dimensional feature maps of deep layers and combines the up-sampled features with the features of shallow layers while the feature map resolution is kept sufficiently high to localize tiny objects. Our proposed deconvolutional framework clearly outperforms state-of-the-art methods on two publicly available datasets.	aerial photography;deep learning;image resolution;map;website monitoring	Lars Wilko Sommer;Arne Schumann;Tobias Schuchert;Jürgen Beyerer	2018	2018 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2018.00075	task analysis;sliding window protocol;computer vision;artificial intelligence;deep learning;feature extraction;computer science;image resolution;similarity (geometry);pattern recognition	Vision	28.626676334734388	-51.03498654111356	159879
c0a0adb7f02d5509969e6107c914f7cc6e9ec881	semantic instance segmentation via deep metric learning		We propose a new method for semantic instance segmentation, by first computing how likely two pixels are to belong to the same object, and then by grouping similar pixels together. Our similarity metric is based on a deep, fully convolutional embedding model. Our grouping method is based on selecting all points that are sufficiently similar to a set of “seed points’, chosen from a deep, fully convolutional scoring model. We show competitive results on the Pascal VOC instance segmentation benchmark.	benchmark (computing);end-to-end principle;pixel;region growing	Alireza Fathi;Zbigniew Wojna;Vivek Rathod;Hyun Oh Song;Sergio Guadarrama;Kevin P. Murphy	2017	CoRR		machine learning;pixel;artificial intelligence;pattern recognition;computer science;embedding;segmentation	Vision	29.68411083356281	-52.06980488114331	159959
b5ca8d4f259f35c1f3edfd9f108ce29881e478b0	disentangled representation learning gan for pose-invariant face recognition		The large pose discrepancy between two face images is one of the key challenges in face recognition. Conventional approaches for pose-invariant face recognition either perform face frontalization on, or learn a pose-invariant representation from, a non-frontal face image. We argue that it is more desirable to perform both tasks jointly to allow them to leverage each other. To this end, this paper proposes Disentangled Representation learning-Generative Adversarial Network (DR-GAN) with three distinct novelties. First, the encoder-decoder structure of the generator allows DR-GAN to learn a generative and discriminative representation, in addition to image synthesis. Second, this representation is explicitly disentangled from other face variations such as pose, through the pose code provided to the decoder and pose estimation in the discriminator. Third, DR-GAN can take one or multiple images as the input, and generate one unified representation along with an arbitrary number of synthetic images. Quantitative and qualitative evaluation on both controlled and in-the-wild databases demonstrate the superiority of DR-GAN over the state of the art.	code;dr-dos;database;discrepancy function;discriminator;encoder;facial recognition system;image fusion;machine learning;rendering (computer graphics);synthetic intelligence	Luan Tran;Xi Yin;Xiaoming Liu	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.141	computer vision;face detection;pattern recognition;three-dimensional face recognition;computer science;decoding methods;discriminator;facial recognition system;machine learning;invariant (mathematics);pose;artificial intelligence;feature learning	Vision	25.62933580279415	-49.69053078162352	159989
69a41c98f6b71764913145dbc2bb4643c9bc4b0a	learning match kernels on grassmann manifolds for action recognition		Action recognition has been extensively researched in computer vision due to its potential applications in a broad range of areas. The key to action recognition lies in modeling actions and measuring their similarity, which however poses great challenges. In this paper, we propose learning match kernels between actions on Grassmann manifold for action recognition. Specifically, we propose modeling actions as a linear subspace on the Grassmann manifold; the subspace is a set of convolutional neural network (CNN) feature vectors pooled temporally over frames in semantic video clips, which simultaneously captures local discriminant patterns and temporal dynamics of motion. To measure the similarity between actions, we propose Grassmann match kernels (GMK) based on canonical correlations of linear subspaces to directly match videos for action recognition; GMK is learned in a supervised way via kernel target alignment, which is endowed with a great discriminative ability to distinguish actions from different classes. The proposed approach leverages the strengths of CNNs for feature extraction and kernels for measuring similarity, which accomplishes a general learning framework of match kernels for action recognition. We have conducted extensive experiments on five challenging realistic data sets including Youtube, UCF50, UCF101, Penn action, and HMDB51. The proposed approach achieves high performance and substantially surpasses the state-of-the-art algorithms by large margins, which demonstrates the great effectiveness of proposed approach for action recognition.	aggregate data;algorithm;artificial neural network;binary prefix;biological neural networks;class;computer vision;convolutional neural network;deep learning;discriminant;experiment;feature extraction;frame (physical object);gm2-klh vaccine/qs21;kernel (operating system);manifold regularization;pooled sample;statistical manifold;video clip	Lei Zhang;Xiantong Zhen;Ling Shao;Jingkuan Song	2019	IEEE Transactions on Image Processing	10.1109/TIP.2018.2866688	grassmannian;convolutional neural network;kernel (linear algebra);discriminative model;linear subspace;feature extraction;feature vector;artificial intelligence;pattern recognition;mathematics;subspace topology	Vision	26.896348093561627	-50.62323703574328	160054
64b41f3a7e6051f478da98f2c7c4f63fc223abb7	robust activity recognition for aging society		Human activity recognition (HAR) is widely applied to many industrial applications. In the context of Industry 4.0, driven by the same demand of machines’ self-organizing ability, HAR can also be adopted in elderly healthcare. However, HAR should be adaptive to the application scenarios in elderly healthcare. In this paper, we propose a nonintrusive activity recognition method that can be applied to long-term and unobtrusive monitoring for elderlies. The method is robust to obstruction and nontarget object interference. Skeleton sequence is estimated from RGB images. Based on two activity continuity metrics, an interframe matching algorithm is proposed to filter nontarget objects. In order to make full use of spatial-temporal information, we propose a novel activity encoding method based on the interframe joints distances. A convolutional neural network is used to learn the distinguishing features automatically. A specific data augmentation method is designed to avoid the overfitting problem on small-scale datasets. The experiments are performed on two public activity datasets and a newly released noisy activity dataset (NAD). The NAD contains obstruction, nontarget object interference. The experimental results show that the proposed method achieves the state-of-the-art performance while only using one ordinary camera. The proposed method is robust to a realistic environment.	activity recognition;activity tracker;algorithm;articular system;artificial neural network;biological neural networks;convolutional neural network;distance;entity name part qualifier - adopted;experiment;industry 4.0;information retrieval;interference (communication);matching;network access device;obstruction;organizing (structure);overfitting;physical object;robustness (computer science);scott continuity;self-organization;silo (dataset)	Yi Chen;Li Yu;Kaoru Ota;Mianxiong Dong	2018	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2018.2819182	overfitting;convolutional neural network;artificial intelligence;rgb color model;blossom algorithm;noise measurement;pattern recognition;activity recognition;inter frame;computer science;skeleton (computer programming)	Vision	31.027434575761983	-51.411363135301436	160830
67da607541b8e380c1665c2158e5e0dd4a6f0e49	learning to localize sound source in visual scenes		Visual events are usually accompanied by sounds in our daily lives. We pose the question: Can the machine learn the correspondence between visual scene and the sound, and localize the sound source only by observing sound and visual scene pairs like human? In this paper, we propose a novel unsupervised algorithm to address the problem of localizing the sound source in visual scenes. A two-stream network structure which handles each modality, with attention mechanism is developed for sound source localization. Moreover, although our network is formulated within the unsupervised learning framework, it can be extended to a unified architecture with a simple modification for the supervised and semi-supervised learning settings as well. Meanwhile, a new sound source dataset is developed for performance evaluation. Our empirical evaluation shows that the unsupervised method eventually go through false conclusion in some cases. We also show that even with a few supervision, i.e., semi-supervised setup, false conclusion is able to be corrected effectively.		Arda Senocak;Tae Hyun Oh;Jun-Sik Kim;Ming-Hsuan Yang;In-So Kweon	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00458	computer vision;machine learning;visualization;unsupervised learning;artificial intelligence;architecture;network architecture;acoustic source localization;computer science	Vision	27.63393551545935	-49.798567701245865	160969
0632a9ace74f540e8793f89a84bb7555ba9deece	weakly supervised localization and learning with generic knowledge	conditional random fields;transfer learning;weakly supervised learning;object detection	Learning a new object class from cluttered training images is very challenging when the location of object instances is unknown, i.e. in a weakly supervised setting. Many previous works require objects covering a large portion of the images. We present a novel approach that can cope with extensive clutter as well as large scale and appearance variations between object instances. To make this possible we exploit generic knowledge learned beforehand from images of other classes for which location annotation is available. Generic knowledge facilitates learning any new class from weakly supervised images, because it reduces the uncertainty in the location of its object instances. We propose a conditional random field that starts from generic knowledge and then progressively adapts to the new class. Our approach simultaneously localizes object instances while learning an appearance model specific for the class. We demonstrate this on several datasets, including the very challenging Pascal VOC 2007. Furthermore, our method allows training any state-of-the-art object detector in a weakly supervised fashion, although it would normally require object location annotations.	clutter;computation;conditional random field;imagenet;instance (computer science);internationalization and localization;markov chain;microsoft windows;state space;supervised learning;test set	Thomas Deselaers;Bogdan Alexe;Vittorio Ferrari	2012	International Journal of Computer Vision	10.1007/s11263-012-0538-3	computer vision;method;transfer of learning;computer science;machine learning;pattern recognition;conditional random field	Vision	31.544330026740653	-48.84128891825964	161099
206487852f814334bb32f0418d8f63c86e4fcb2d	generic online animal activity recognition on collar tags		Animal behaviour is a commonly-used and sensitive indicator of animal welfare. Moreover, the behaviour of animals can provide rich information about their environment. For online activity recognition on collar tags of animals, fundamental challenges include: limited energy resources, limited CPU and memory availability, and heterogeneity of animals. In this paper, we propose to tackle these challenges with a framework that employs Multitask Learning for embedded platforms. We train the classifiers with shared training data and a shared feature-representation. We show that Multitask Learning has a significant positive effect on the performance of the classifiers. Furthermore, we compare 7 types of classifiers in terms of resource usage and activity recognition performance on real-world movement data from goats and sheep. A Deep Neural Network could obtain an accuracy of 94% when tested with the data from both species. Our results show that a Deep Neural Network performs the best among the compared classifiers in terms of complexity versus performance. This work supports the development of a robust generic classifier that can run on a small embedded system with good performance, as well as sustain the lifetime of online activity recognition systems.	activity recognition;central processing unit;computer multitasking;deep learning;embedded system;experiment;learning classifier system;server farm;smart tv;statistical classification	Jacob W. Kamminga;Helena C. Bisby;Duc V. Le;Nirvana Meratnia;Paul J. M. Havinga	2017		10.1145/3123024.3124407	collar;training set;activity recognition;artificial neural network;machine learning;computer science;multi-task learning;artificial intelligence	HCI	26.779363575873436	-47.64905573082088	161545
c67eee5e0ecc020264fae089a08a1abf8c0b6240	multifeature analysis and semantic context learning for image classification	semantic context modeling;image classification;journal article;multifeature fusion;object detection	This article introduces an image classification approach in which the semantic context of images and multiple low-level visual features are jointly exploited. The context consists of a set of semantic terms defining the classes to be associated to unclassified images. Initially, a multiobjective optimization technique is used to define a multifeature fusion model for each semantic class. Then, a Bayesian learning procedure is applied to derive a context model representing relationships among semantic classes. Finally, this context model is used to infer object classes within images. Selected results from a comprehensive experimental evaluation are reported to show the effectiveness of the proposed approaches.	computer vision;high- and low-level;mathematical optimization;multi-objective optimization	Qianni Zhang;Ebroul Izquierdo	2013	TOMCCAP	10.1145/2457450.2457454	computer vision;contextual image classification;semantic similarity;semantic computing;computer science;machine learning;pattern recognition;data mining	Vision	26.112186871726237	-46.04115038156851	162428
702eeab7164fca800bcdd17a582f136860322ce9	multi-label audio concept detection using correlated-aspect gaussian mixture model	multi label classification;aspect gaussian mixture model;audio concept detection;concept correlation;probabilistic latent semantic analysis	As an essentially multi-label classification problem, audio concept detection is normally solved by treating concepts independently. Since in this process the original useful concept correlation information is missing, this paper proposes a new model named Correlated-Aspect Gaussian Mixture Model (C-AGMM) to take advantage of such a clue for enhancing multi-label audio concept detection. Originating from Aspect Gaussian Mixture Model (AGMM) which improves GMM by incorporating it into probabilistic Latent Semantic Analysis (pLSA), C-AGMM still learns a probabilistic model of the whole audio clip by regarding concepts as its component elements. However, different from AGMM that assumes concepts independent with each other, C-AGMM considers their distribution on a sub-manifold embedded in the ambient space. With an assumption that if two concepts are close in the intrinsic geometry of this distribution then their conditional probability distributions are likely to show similarity, a graph regularizer is exploited to model the correlation between these concepts. Following the Maximum Likelihood Estimate principle, model parameters of C-AGMM encoding the concept correlation clue are derived and used directly as the detection criterion. Experiments on two datasets show the effectiveness of our proposed model.	embedded system;essence;experiment;mixture model;multi-label classification;multiclass classification;probabilistic latent semantic analysis;statistical model;streaming media;video clip	Cencen Zhong;Zhenjiang Miao	2013	Multimedia Tools and Applications	10.1007/s11042-013-1842-9	computer science;machine learning;pattern recognition;data mining;probabilistic latent semantic analysis;statistics	AI	25.61024146685358	-46.49298472035012	162688
cd9ce1db5ee59d582a3e95e32094fd2a1eec7410	scale-recurrent network for deep image deblurring		"""In single image deblurring, the """"coarse-to-fine"""" scheme, i.e. gradually restoring the sharp image on different resolutions in a pyramid, is very successful in both traditional optimization-based methods and recent neural-network-based approaches. In this paper, we investigate this strategy and propose a Scale-recurrent Network (SRN-DeblurNet) for this deblurring task. Compared with the many recent learning-based approaches in [25], it has a simpler network structure, a smaller number of parameters and is easier to train. We evaluate our method on large-scale deblurring datasets with complex motion. Results show that our method can produce better quality results than state-of-the-arts, both quantitatively and qualitatively."""		Xin Tao;Hongyun Gao;Yi Wang;Xiaoyong Shen;Jue Wang;Jiaya Jia	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00853	computer vision;task analysis;kernel (linear algebra);image restoration;pattern recognition;artificial intelligence;computer science;deblurring;convolution;image resolution	Vision	25.299388308290872	-51.47323885417757	162949
008e8ab75d75c93eb70d8d562bdc857dd739cbc0	training inter-related classifiers for automatic image classification and annotation	inter related classifier training;visual concept network;structural learning;large scale image classification	A structural learning algorithm is developed in this paper to achieve more effective training of large numbers of inter-related classifiers for supporting large-scale image classification and annotation. A visual concept network is constructed for characterizing the inter-concept visual correlations intuitively and determining the inter-related learning tasks automatically in the visual feature space rather than in the label space. By partitioning large numbers of object classes and image concepts into a set of groups according to their inter-concept visual correlations, the object classes and image concepts in the same group will share similar visual properties and their classifiers are strongly inter-related while the object classes and image concepts in different groups will contain various visual properties and their classifiers can be trained independently. By leveraging the inter-concept visual correlations for inter-related classifier training, our structural learning algorithm can train the inter-related classifiers jointly rather than independently, which can enhance their discrimination power significantly. Our experiments have also provided very positive results on large-scale image classification and annotation. & 2012 Elsevier Ltd. All rights reserved.	algorithm;computer vision;experiment;f1 score;feature vector;linear discriminant analysis;scale-invariant feature transform;semantic similarity;vector quantization;wordnet	Peixiang Dong;Kuizhi Mei;Nanning Zheng;Hao Lei;Jianping Fan	2013	Pattern Recognition	10.1016/j.patcog.2012.10.029	computer vision;computer science;machine learning;pattern recognition	Vision	26.769446182988638	-46.587075704648704	163669
5cd2b28b9e88b2be0b8c9d9ddce8440656477d1c	person re-identification with deep features and transfer learning		Person re-identification is an important technique towards automatic search of a person's presence in a surveillance video. Two fundamental problems are critical for person re-identification:feature representation and metric learning. At present, there are many methods in the study of person re-identification, which has achieved remarkable results. Due to the difference of the data distribution in different scenarios, the performance of the person re-identification in the new scene is significantly decreased. In order to avoid the tedious manual annotation, and to make full use of the original detector and labeled samples, the research of person re-identification based on transfer learning has received more and more attention. Existing approaches adopt a fixed metric for matching all the subjects. In this work, we propose a Feature Net (FN) architecture with Convolution Neural Networks (CNNs) to learn the pedestrian feature, reserved more useful information. And use Cosine distance to measure the each image pair's similarity directly which is more efficient but uncomplicated than others. Our method can be applied to different scenarios and improved the recognition performance. Experiments on the challenging datasets show the effectiveness of our methods, especially on cuhk03 dataset, we achieve the state-of-the-art result.	artificial neural network;closed-circuit television;convolution;cosine similarity;experiment;feature extraction;transfer-based machine translation	Shengke Wang;Shan Wu;Lianghua Duan;Changyin Yu;Yujuan Sun;Junyu Dong	2017	22017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)	10.1109/CSE-EUC.2017.136	transfer of learning;visualization;feature extraction;pedestrian;artificial neural network;computer science;convolution;machine learning;artificial intelligence;detector;annotation	Vision	30.105250183204674	-51.4768579205705	163697
276d832793679013bf6a89ee5492d8768e7bf16d	manifold regularized multitask learning for semi-supervised multilabel image classification	manifolds complexity theory vectors nose visualization kernel reliability;computer science and information systems;manifolds;mir dataset manifold regularized multitask learning algorithm semisupervised multilabel image classification data distribution high dimensional visual features mrmtl algorithm search volume pascal voc07 dataset;image classification;journal article;shared subspace image classification manifold multilabel semi supervised;learning artificial intelligence;manifolds image classification learning artificial intelligence	It is a significant challenge to classify images with multiple labels by using only a small number of labeled samples. One option is to learn a binary classifier for each label and use manifold regularization to improve the classification performance by exploring the underlying geometric structure of the data distribution. However, such an approach does not perform well in practice when images from multiple concepts are represented by high-dimensional visual features. Thus, manifold regularization is insufficient to control the model complexity. In this paper, we propose a manifold regularized multitask learning (MRMTL) algorithm. MRMTL learns a discriminative subspace shared by multiple classification tasks by exploiting the common structure of these tasks. It effectively controls the model complexity because different tasks limit one another's search volume, and the manifold regularization ensures that the functions in the shared hypothesis space are smooth along the data manifold. We conduct extensive experiments, on the PASCAL VOC'07 dataset with 20 classes and the MIR dataset with 38 classes, by comparing MRMTL with popular image classification algorithms. The results suggest that MRMTL is effective for image classification.	algorithm;binary classification;class;computer multitasking;computer vision;discriminative model;experiment;learning disorders;manifold regularization;semi-supervised learning;silo (dataset);tracer	Yong Luo;Dacheng Tao;Bo Geng;Chao Xu;Stephen J. Maybank	2013	IEEE Transactions on Image Processing	10.1109/TIP.2012.2218825	contextual image classification;manifold;computer science;machine learning;pattern recognition;data mining;mathematics;manifold alignment	ML	24.810847898183383	-45.21587730226908	163791
4fc8229dd322910205949ace43e67808fc1d63ed	embedding geometry in generative models for pose estimation of object categories		Pose estimation for object classes is central in many Computer Vision tasks. Many approaches have been proposed to estimate the pose of an unknown object from a given category, and those based on local features have shown to be very effective. While some use 3D information obtained through CAD models [4] or 3D reconstructions [2], others have shown that coupling feature regression and view labeling efficiently solves this task [1, 5]. However, they rely solely on the discriminative power of local features, and this is problematic if objects have similar appearance in different views, as Figure 1 shows. To handle these situations they need to resort to external coarse-grained pose estimators for disambiguation. We propose a method that solves this problem by integrating feature regression and graph matching in a unified probabilistic framework. The former predicts the descriptor of each patch in a query pose, while the latter evaluates the geometrical consistency between pairs of matches. As a consequence, our approach does not resort to external pose pre-processing and in addition experimentally shows to be more accurate in comparison. This permits to avoid any initial hard decision, postponing it to a later stage when more data is available. Feature regression allows to treat pose estimation as a continuous problem, unlike most methods that provide only discrete values for the pose [3, 4]. Graph matching permits to softly align the unknown object to the class model, bringing additional consistency and precision to the solution. In a nutshell, our method retains the benefits of regression-based methods, like continuity and generality, while favoring geometrically consistent results through graph matching. Our feature regression method leverages [1]. Regression functions model feature descriptors as a function of the pose. Given a patch i, t i = {( f i 1,α i 1),( f i 2,α i 2), . . . ,( f i n,α i n)}, i.e., t i is a set of feature descriptors f i j labelled by their corresponding viewing angle α j. For each t i, a generative feature model F i is defined as a linear combination of Gaussian kernels centered at the training poses,	3d pose estimation;align (company);computer vision;computer-aided design;experiment;feature model;matching (graph theory);model f keyboard;preprocessor;scott continuity;viewing angle;word-sense disambiguation	Michele Fenzi;Jörn Ostermann	2014			generative grammar;computer science;computer vision;artificial intelligence;matching (graph theory);estimator;pattern recognition;3d pose estimation;probabilistic logic;embedding;generality;pose;geometry	Vision	29.774064712041383	-49.37159712338545	164231
710c3aaffef29730ffd909a63798e9185f488327	the gist of aligning faces	reliability;training;data mining;learning systems;shape;feature extraction;head	We propose a novel supervised initialization scheme for cascaded face alignment by searching nearest neighbors based on global image descriptors. Unlike existing schemes which resort to additional large training data sets for learning features, our method does not require additional training steps; thus making our method low computational. Moreover, we found that it is sufficient to use a simple low-dimensional global image descriptor that is easy to extract. In particular, in this work we use the GIST features as our global image descriptor. The proposed initialization scheme outperforms existing initialization schemes for face alignment and improves on the state-of-the-art methods on two challenging datasets, 300-W and COFW.	computation;gist;global illumination;scheme;supervised learning;test set;visual descriptor	Siqi Yang;Arnold Wiliem;Brian C. Lovell	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900095	computer vision;feature extraction;shape;computer science;machine learning;pattern recognition;data mining;reliability;geometry;head	Vision	29.451895016696565	-49.04568987456321	164341
de5e7586e534f2bcec3cfed4c41a48787225868c	rgbd co-saliency detection via multiple kernel boosting and fusion		RGBD co-saliency detection, which aims at extracting common salient objects from a group of RGBD images with the additional depth information, has become an emerging branch of saliency detection. In this regard, this paper proposes a novel framework via multiple kernel boosting (MKB) and co-saliency quality based fusion. First, on the basis of pre-segmented regions at multiple scales, the regional clustering by feature bagging is exploited to generate the base co-saliency maps. Then the clustering-based samples selection is performed to select the most similar regions with high saliency from different images in the image set. The selected samples are utilized to learn a MKB-based regressor, which is applied to all regions at multiple scales to generate the MKB-based co-saliency maps. Finally, to make full use of both MKB and clustering-based co-saliency maps, a co-saliency quality criterion is proposed for adaptive fusion to generate the final co-saliency maps. Experimental results on a public RGBD co-saliency detection dataset demonstrate that the proposed co-saliency model outperforms the state-of-the-art co-saliency models.	cluster analysis;co-np;kernel (operating system);map	Lishan Wu;Zhi Liu;Hangke Song;Olivier Le Meur	2017	Multimedia Tools and Applications	10.1007/s11042-017-5576-y	computer science;kernel (linear algebra);artificial intelligence;boosting (machine learning);fusion;pattern recognition;salience (neuroscience);cluster analysis	Vision	27.14573029736588	-46.54850130841208	164429
23c662b6f05e110d7d3daef32a2825c79a6bb6ef	automatic image annotation and retrieval using hybrid approach	continuous plsa;automatic image annotation;hybrid approach;semantic learning;image retrieval	We firstly propose continuous probabilistic latent semantic analysis (PLSA) to model continuous quantity. In addition, corresponding ExpectationMaximization (EM) algorithm is derived to determine the model parameters. Furthermore, we present a hybrid framework which employs continuous PLSA to model visual features of images in generative learning stage and uses ensembles of classifier chains to classify the multi-label data in discriminative learning stage. Since the framework combines the advantages of generative and discriminative learning, it can predict semantic annotation precisely for unseen images. Finally, we conduct a series of experiments on a standard Corel dataset. The experiment results show that our approach outperforms many state-of-theart approaches.	automatic image annotation;classifier chains;expectation–maximization algorithm;experiment;html5 in mobile devices;multi-label classification;probabilistic latent semantic analysis	Zhixin Li;Weizhong Zhao;Zhiqing Li;Zhiping Shi	2012		10.1007/978-3-642-32891-6_43	image retrieval;computer science;machine learning;pattern recognition;probabilistic latent semantic analysis;information retrieval	ML	26.134812555771166	-47.188209858384525	164686
7e736f25911c91cda343c000aabc773ed9a94fdf	accurate and efficient video de-fencing using convolutional neural networks and temporal information		De-fencing is to eliminate the captured fence on an image or a video, providing a clear view of the scene. It has been applied for many purposes including assisting photographers and improving the performance of computer vision algorithms such as object detection and recognition. However, the state-of-the-art de-fencing methods have limited performance caused by the difficulty of fence segmentation and also suffer from the motion of the camera or objects. To overcome these problems, we propose a novel method consisting of segmentation using convolutional neural networks and a fast/robust recovery algorithm. The segmentation algorithm using convolutional neural network achieves significant improvement in the accuracy of fence segmentation. The recovery algorithm using optical flow produces plausible de-fenced images and videos. The proposed method is experimented on both our diverse and complex dataset and publicly available datasets. The experimental results demonstrate that the proposed method achieves the state-of-the-art performance for both segmentation and content recovery.	algorithm;artificial neural network;computer vision;convolutional neural network;fast fourier transform;object detection;optical flow	Chen Du;Byeongkeun Kang;Zheng Xu;Ji Dai;Truong Nguyen	2018	2018 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2018.8486522	convolutional neural network;artificial intelligence;computer vision;robustness (computer science);computer science;motion estimation;adaptive optics;image segmentation;object detection;pattern recognition;fencing;optical flow	Vision	31.63624290167189	-51.330833929311396	164907
098fa9b4c3f7fb41c7a178d36f5dbb50a3ffa377	dense optical flow prediction from a static image	neural nets image sequences motion estimation;optical imaging videos predictive models optical losses neural networks context trajectory;dense optical flow prediction future motion prediction motion prediction cnn convolutional neural network static image	Given a scene, what is going to move, and in what direction will it move? Such a question could be considered a non-semantic form of action prediction. In this work, we present a convolutional neural network (CNN) based approach for motion prediction. Given a static image, this CNN predicts the future motion of each and every pixel in the image in terms of optical flow. Our CNN model leverages the data in tens of thousands of realistic videos to train our model. Our method relies on absolutely no human labeling and is able to predict motion based on the context of the scene. Because our CNN model makes no assumptions about the underlying scene, it can predict future optical flow on a diverse set of scenarios. We outperform all previous approaches by large margins.	algorithm;artificial neural network;autostereogram;convolutional neural network;graphics processing unit;ibm notes;maximum flow problem;optical flow;pixel;tesla (microarchitecture)	Jacob Walker;Abhinav Gupta;Martial Hebert	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.281	computer vision;simulation;computer science;machine learning	Vision	27.27393915990933	-51.15450268154659	165193
7acbf0b060e948589b38d5501ca217463cfd5c2f	learning multiple relative attributes with humans in the loop	relative attributes active learning learning to rank humans in the loop image recognition;vectors optimization kernel training data training semantics image retrieval;real image data sets semantic attributes image content image annotation high quality training data multiple relative attributes learning loop minimal additional guidance joint active learning pairwise supervision ranking functions pairwise queries;semantic networks human factors image recognition image retrieval learning artificial intelligence	Semantic attributes have been recognized as a more spontaneous manner to describe and annotate image content. It is widely accepted that image annotation using semantic attributes is a significant improvement to the traditional binary or multiclass annotation due to its naturally continuous and relative properties. Though useful, existing approaches rely on an abundant supervision and high-quality training data, which limit their applicability. Two standard methods to overcome small amounts of guidance and low-quality training data are transfer and active learning. In the context of relative attributes, this would entail learning multiple relative attributes simultaneously and actively querying a human for additional information. This paper addresses the two main limitations in existing work: 1) it actively adds humans to the learning loop so that minimal additional guidance can be given and 2) it learns multiple relative attributes simultaneously and thereby leverages dependence amongst them. In this paper, we formulate a joint active learning to rank framework with pairwise supervision to achieve these two aims, which also has other benefits such as the ability to be kernelized. The proposed framework optimizes over a set of ranking functions (measuring the strength of the presence of attributes) simultaneously and dependently on each other. The proposed pairwise queries take the form of which one of these two pictures is more natural? These queries can be easily answered by humans. Extensive empirical study on real image data sets shows that our proposed method, compared with several state-of-the-art methods, achieves superior retrieval performance while requires significantly less human inputs.	addresses (publication format);algorithm;automatic image annotation;equilibrium;humans;image retrieval;kernel method;learning to rank;machine learning;ordered pair;programming paradigm;question (inquiry);spontaneous order;benefit	Buyue Qian;Xiang Wang;Nan Cao;Yu-Gang Jiang;Ian Davidson	2014	IEEE Transactions on Image Processing	10.1109/TIP.2014.2365952	semi-supervised learning;computer vision;image retrieval;computer science;machine learning;pattern recognition;automatic image annotation	Vision	25.10430488791116	-46.43813423970567	165320
661c9ed7c5341f4f3173b5ed7c775153186a12b4	adaptation of visual models with cross-modal regularization	jose costa;cbir;retrieval;regularization;san diego nuno vasconcelos pereira;computer science adaptation of visual models with cross modal regularization university of california;adaptation;computer science;electrical engineering;cross modal	Semantic representations of images have been widely adopted in ComputerVision. A vocabulary of concepts of interest is first identified and classifiers arelearned for the detection of those concepts. Images are classified and mapped to aspace where each feature is a score for the detection of a concept. This representationbrings several advantages. First, the generalization from low-level featuresto concept-level enables similarity measures that correlate much better with userexpectations. Second, because semantic features are, by definition, discriminantfor tasks like image categorization, the semantic representation enables a solutionfor such tasks with low-dimensional classifiers. Third, the semantic representationis naturally aligned with recent interest on contextual modeling. This is ofimportance for tasks such as object recognition, where detection of contextuallyrelated objects has been shown to improve detection of certain objects of interest,or semantic segmentation, where the coherence of segment semantics can be exploitedto achieve more robust segmentations. Lastly, due to their abstract nature,semantic spaces enable a unified representation for data from different contentmodalities, e.g. images, text, or audio. This opens up a new set of possibilities formultimedia processing, enabling operations such as cross-modal retrieval, or imagede-noising by text regularization. This unified representation for multi-modal datais the starting point of the proposed framework on adaptation of visual modelswith cross-modal regularization.We start by pointing the problems in computing similarity on heterogeneousdata, proposing two fundamental hypotheses to deal with those issues. One,learning a space that maximizes the correlation on the (heterogeneous) data; two,learning a representation where data lies at a higher level of abstraction. Empiricalevidence is shown in favor of each hypothesis; furthermore the hypotheses areshown to be complementary. We follow on the (semantic) abstraction hypothesisfor a deeper understanding on the robustness of these representations and to studythe richness of this space, as it highly influences the discriminative power of suchdescriptors.It has been shown that categories unknown to the semantic space, whenrepresented in it, exhibit a pattern of co-occurring concepts that describe them accuratelyand sensibly; e.g. the concept of fishing might not belong to the semanticspace and instead be represented by the set water, boat, people and gear. Eventhough the amount of labeled data continues to increase with ongoing efforts fromdifferent research communities, it is a challenging task to build a semantic spacethat is universal. We show evidence towards robustness of representations in thesemantic space.Noting that images are frequently published on the web together withloosely related text, we use the semantic representations described above to introducethe theoretical principles to a feature regularizer for image semantic representationsbased on auxiliary data. This proves very effective on improving retrievalprecision and recall in the task of content-based image retrieval (CBIR). It’s resultsare compared to recently developed methods, achieving significant gains inthree benchmark datasets, raising the bar of state-of-the-art performance for imageretrieval.	matrix regularization;modal logic	Jose Maria C. Costa Pereira	2015			semantic similarity;semantic computing;semantic grid;computer science;artificial intelligence;machine learning;data mining	ML	27.633239885970653	-48.83561971333646	165998
636280d54ea08cbe37cb4aba3efcec59349eaefa	dense information flow for neural machine translation		Recently, neural machine translation has achieved remarkable progress by introducing well-designed deep neural networks into its encoder-decoder framework. From the optimization perspective, residual connections are adopted to improve learning performance for both encoder and decoder in most of these deep architectures, and advanced attention connections are applied as well. Inspired by the success of the DenseNet model in computer vision problems, in this paper, we propose a densely connected NMT architecture (DenseNMT) that is able to train more efficiently for NMT. The proposed DenseNMT not only allows dense connection in creating new features for both encoder and decoder, but also uses the dense attention structure to improve attention quality. Our experiments on multiple datasets show that DenseNMT structure is more competitive and efficient 1.	artificial neural network;computer vision;deep learning;encoder;experiment;information flow;mathematical optimization;neural machine translation	Yanyao Shen;Xu Tan;Di He;Tao Qin;Tie-Yan Liu	2018			architecture;machine translation;encoder;residual;artificial neural network;machine learning;information flow (information theory);computer science;artificial intelligence	AI	24.977334552414764	-52.007580923011446	166270
087337fdad69caaab8ebd8ae68a731c5bf2e8b14	fully convolutional networks for semantic segmentation	semantic segmentation convolutional networks deep learning transfer learning;image segmentation;fuses;convolution;fully convolutional networks semantic segmentation visual models correspondingly sized output spatially dense prediction tasks contemporary classification networks learned representations coarse layer fine layer pascal voc nyudv2 sift flow pascal context;training;semantics;semantics image segmentation training convolution computer architecture proposals fuses;computer architecture;feedforward neural nets image classification image representation image resolution image segmentation learning artificial intelligence transforms;deep learning;transfer learning semantic segmentation convolutional networks deep learning;transfer learning;semantic segmentation;proposals;convolutional networks	Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, improve on the previous best result in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional networks achieve improved segmentation of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT Flow, and PASCAL-Context, while inference takes one tenth of a second for a typical image.	convolutional neural network;end-to-end principle;ephrin type-b receptor 1, human;inference;instruction unit;pixel;biologic segmentation;tenth	Evan Shelhamer;Jonathan Long;Trevor Darrell	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/TPAMI.2016.2572683	fuse;computer vision;transfer of learning;computer science;theoretical computer science;machine learning;semantics;deep learning;image segmentation;convolution	Vision	25.74899760582143	-51.65751042205629	166786
50b38a985fbeae260d9d59b35479d161c6d9d1bd	sparsity cue in image copy detection	copy detection;sparsity;online dictionary learning;spare representation;attack	Image copy detection is an art of searching duplicates from a target database. Computationally efficient and robust detection is still a challenging issue. Inspired by the recent study of sparsity in the context of compressed sensing, we propose a sparse representation-based image copy detection method exploiting sparsity as the cue for searching duplicates. We find that although sparse representation can describe an image in a compact manner, the inherent discriminable features, as far as we know, are not entirely explored. In this paper, we study the discrimination ability inherent in sparsity via online dictionary learning and compact feature descriptor representation. Experimental results show that our method, compared with state-of-the-art, is computationally efficient and attains better or comparable detection performance measured in terms of precision and recall rates.	algorithmic efficiency;compressed sensing;database;dictionary;machine learning;precision and recall;sparse approximation;sparse matrix;visual descriptor	Huan-Cheng Hsu;Chun-Rong Huang;Chun-Shien Lu	2012		10.1145/2393347.2396351	computer vision;attack;computer science;machine learning;pattern recognition;sparsity-of-effects principle	ML	28.53794033732922	-46.55744865499909	168040
06f7e0aee7fc5807ab862432a4e5ade2cda73c4b	flowing convnets for human pose estimation in videos	heating optical imaging videos training adaptive optics computer architecture;video pose estimation convnets human pose estimation temporal context optical flow spatial fusion layer heatmap prediction parametric pooling layer pooled confidence map;video signal processing image sequences neural nets pose estimation	The objective of this work is human pose estimation in videos, where multiple frames are available. We investigate a ConvNet architecture that is able to benefit from temporal context by combining information across the multiple frames using optical flow. To this end we propose a network architecture with the following novelties: (i) a deeper network than previously investigated for regressing heatmaps, (ii) spatial fusion layers that learn an implicit spatial model, (iii) optical flow is used to align heatmap predictions from neighbouring frames, and (iv) a final parametric pooling layer which learns to combine the aligned heatmaps into a pooled confidence map. We show that this architecture outperforms a number of others, including one that uses optical flow solely at the input layers, one that regresses joint coordinates directly, and one that predicts heatmaps without spatial fusion. The new architecture outperforms the state of the art by a large margin on three video pose estimation datasets, including the very challenging Poses in the Wild dataset, and outperforms other deep methods that don't use a graphical model on the single-image FLIC benchmark (and also [5, 35] in the high precision region).	3d pose estimation;align (company);autostereogram;benchmark (computing);convolutional neural network;direct method in the calculus of variations;expectation propagation;flic (file format);graphical model;heat map;human computer;markov random field;network architecture;optical flow;refinement (computing)	Tomas Pfister;James Charles;Andrew Zisserman	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.222	computer vision;simulation;3d pose estimation;computer science;machine learning;pattern recognition	Vision	27.04926702908813	-50.84578642729464	168739
2e286fe30e2dac87fe340853941eb35851f9f326	cross-modality distillation: a case for conditional generative adversarial networks		In this paper, we propose to use a Conditional Generative Adversarial Network (CGAN) for distilling (i.e. transferring) knowledge from sensor data and enhancing low-resolution target detection. In unconstrained surveillance settings, sensor measurements are often noisy, degraded, corrupted, and even missing/absent, thereby presenting a significant problem for multi-modal fusion. We therefore specifically tackle the problem of a missing modality in our attempt to propose an algorithm based on CGANs to generate representative information from the missing modalities when given some other available modalities. Despite modality gaps, we show that one can distill knowledge from one set of modalities to another. Moreover, we demonstrate that it achieves better performance than traditional approaches and recent teacher-student models.	algorithm;generative adversarial networks;modal logic;modality (human–computer interaction)	Siddharth Roheda;Benjamin S. Riggan;Hamid Krim;Liyi Dai	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8462082	adversarial system;modalities;generative grammar;artificial intelligence;object detection;pattern recognition;computer science	Vision	28.56607139068838	-49.93125836473022	169141
69215ccb336c2175d69db9f489331f404d6623f4	hierarchical structured dictionary learning for image categorization		A novel Hierarchical Structured Dictionary Learning (HSDL) algorithm is proposed in this paper. It aims to learn classs-pecific dictionaries for all classes simultaneously in a hierarchical structure. A discriminative term based on Fisher discrimination criterion is jointly considered for both the classs-pecific dictionaries in the lower level and the shared dictionaries in the upper level to enhance the discrimination of dictionaries. The experimental results evaluated on the ImageNet database have shown the superior performance of HSDL over the state-of-the-art dictionary learning methods.	algorithm;categorization;dictionary;imagenet;machine learning	Tzu-Chan Chuang;Chen-Kuo Chiang;Shang-Hong Lai	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952359	discriminative model;pattern recognition;visualization;categorization;computer science;feature extraction;sparse matrix;machine learning;artificial intelligence	Vision	25.52206782141722	-45.65999181487405	169181
dce5289a0974fbd0695ceb547f97eecb5158e7a5	deep 3d human pose estimation under partial body presence		This paper addresses the problem of 3D human pose estimation when not all body parts are present in the input image, i.e., when some body joints are present while other joints are fully absent (we exclude self-occlusion). State-of-the-art is not designed and thus not effective for such cases. We propose a deep CNN to regress the human pose directly from an input image; we design and train this network to work under partial body presence. Parallel to this, we train a detection network to classify the presence or absence of each of the main body joints in the input image. The outputs of our detection and regression networks are a) joints that are present and b) joints that are absent. With these outputs, our method reconstructs the full body skeleton. Evaluations on the Hu-man3.6M dataset yield promising results compared to related work.	3d modeling;3d pose estimation;emoticon;gomory–hu tree;hidden surface determination;holographic principle	Saeid Vosoughi;Maria A. Amer	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451031	computer vision;pose;regression;artificial intelligence;pattern recognition;body joints;computer science	Robotics	29.730837383093817	-49.68680617312519	169388
096eb8b4b977aaf274c271058feff14c99d46af3	multi-observation visual recognition via joint dynamic sparse representation	object recognition;object recognition face recognition image classification image representation;object classification multi observation visual recognition joint dynamic sparse representation visual classification face recognition;training;heterogeneous data;image classification;joints;computer vision;face anatomy;visualization;recognition;face recognition;vectors;image representation;heuristic algorithms;state of the art;visual perception;face;object classification;sparse representation;joints vectors heuristic algorithms face training visualization face recognition;vector analysis;heuristic algorithm;physical properties	We address the problem of visual recognition from multiple observations of the same physical object, which can be generated under different conditions, such as frames at different time instances or snapshots from different viewpoints. We formulate the multi-observation visual recognition task as a joint sparse representation model and take advantage of the correlations among the multiple observations for classification using a novel joint dynamic sparsity prior. The proposed joint dynamic sparsity prior promotes shared joint sparsity pattern among the multiple sparse representation vectors at class-level, while allowing distinct sparsity patterns at atom-level within each class in order to facilitate a flexible representation. The proposed method can handle both homogenous as well as heterogenous data within the same framework. Extensive experiments on various visual classification tasks including face recognition and generic object classification demonstrate that the proposed method outperforms existing state-of-the-art methods	experiment;facial recognition system;ibm notes;modal logic;sparse approximation;sparse matrix;statistical classification;visual basic[.net]	Haichao Zhang;Nasser M. Nasrabadi;Yanning Zhang;Thomas S. Huang	2011	2011 International Conference on Computer Vision	10.1109/ICCV.2011.6126293	facial recognition system;face;heuristic;computer vision;contextual image classification;visualization;vector calculus;visual perception;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;sparse approximation;mathematics;geometry;physical property	Vision	26.027609284532186	-45.2306468970668	169654
898ff1bafee2a6fb3c848ad07f6f292416b5f07d	face alignment via regressing local binary features	detectors;training;vegetation;shape;feature extraction;face detection local binary feature regression face alignment locality principle facial landmark linear regression mobile phone face detector;face;regression analysis face recognition feature extraction mobile handsets;local binary feature face alignment tracking random forest;face shape vegetation detectors feature extraction regression tree analysis training;regression tree analysis	This paper presents a highly efficient and accurate regression approach for face alignment. Our approach has two novel components: 1) a set of local binary features and 2) a locality principle for learning those features. The locality principle guides us to learn a set of highly discriminative local binary features for each facial landmark independently. The obtained local binary features are used to jointly learn a linear regression for the final output. This approach achieves the state-of-the-art results when tested on the most challenging benchmarks to date. Furthermore, because extracting and regressing local binary features are computationally very cheap, our system is much faster than previous methods. It achieves over 3000 frames per second (FPS) on a desktop or 300 FPS on a mobile phone for locating a few dozens of landmarks. We also study a key issue that is important but has received little attention in the previous research, which is the face detector used to initialize alignment. We investigate several face detectors and perform quantitative evaluation on how they affect alignment accuracy. We find that an alignment friendly detector can further greatly boost the accuracy of our alignment method, reducing the error up to 16% relatively. To facilitate practical usage of face detection/alignment methods, we also propose a convenient metric to measure how good a detector is for alignment initialization.	alignment;anatomic structures;benchmark (computing);decision tree;desktop computer;detectors;disease regression;experiment;face detection;floating point systems;frame (physical object);locality of reference;mobile phone;principle of locality;trees (plant)	Shaoqing Ren;Xudong Cao;Yichen Wei;Jian Sun	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2518867	face;computer vision;detector;feature extraction;shape;computer science;machine learning;pattern recognition;mathematics;vegetation	Vision	29.949187412347143	-49.10685577935811	169708
10d3f77225eca1d576268ba84ed83f230a5e47c4	crafting a multi-task cnn for viewpoint estimation		Convolutional Neural Networks (CNNs) were recently shown to provide state-of-theart results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [28]. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.	ambiguous name resolution;baseline (configuration management);bibliothèque des ecoles françaises d'athènes et de rome;computer multitasking;convolutional neural network;imagenet;synthetic data;teaching method	Francisco Massa;Renaud Marlet;Mathieu Aubry	2016	CoRR		computer vision;simulation;computer science;artificial intelligence;machine learning;data mining;statistics	Vision	25.379641385553274	-48.62405669733406	169932
d01e65df6592366d09a1fb8c765ecefcb0026ce3	a classification-oriented dictionary learning model: explicitly learning the particularity and commonality across categories	particularity;commonality;image classification;dictionary learning;sparse coding	Empirically, we find that despite the most exclusively discriminative features owned by one specific object category, the various classes of objects usually share some common patterns, which do not contribute to the discrimination of them. Concentrating on this observation and motivated by the success of dictionary learning (DL) framework, in this paper, we propose to explicitly learn a class-specific dictionary (called particularity) for each category that captures the most discriminative features of this category, and simultaneously learn a common pattern pool (called commonality), whose atoms are shared by all the categories and only contribute to representation of the data rather than discrimination. In this way, the particularity differentiates the categories while the commonality provides the essential reconstruction for the objects. Thus, we can simply adopt a reconstruction-based scheme for classification. By reviewing the existing DL-based classification methods, we can see that our approach simultaneously learns a classification-oriented dictionary and drives the sparse coefficients as discriminative as possible. In this way, the proposed method will achieve better classification performance. To evaluate our method, we extensively conduct experiments both on synthetic data and real-world benchmarks in comparison with the existing DL-based classification algorithms, and the experimental results demonstrate the effectiveness of our method. HighlightsWe propose a discriminative dictionary learning method for image classification.Our method learns class-specific feature sub-dictionaries and a common pattern pool.The class-specific dictionary captures the most discriminative features of the class.The common pattern pool complements the representation of images over the dictionary.We provide the explanation of our model and the comparisons with other methods.	dictionary;machine learning	Donghui Wang;Shu Kong	2014	Pattern Recognition	10.1016/j.patcog.2013.08.004	contextual image classification;k-svd;computer science;machine learning;pattern recognition;data mining;neural coding	Vision	24.972071121326472	-45.64874021570368	169980
06b7413b210425bd9bda80b4aecc7501adaf8250	scene-dependent proposals for efficient person detection		Abstract In this paper, we present a new method that provides a substantial speed-up of person detection while showing high classification accuracy. Our method learns a Gaussian Mixture Model of locations and scales of the persons in the scene under observation. The model is learnt in an unsupervised way from a set of detections extracted from a small number of frames, so that each component of the mixture represents the expectation of finding a target in a region of the image at a specific scale. At runtime, the windows that most likely contain a person are sampled from the components and evaluated by the classifier. Experimental results show that replacing the classic sliding window approach with our scene-dependent proposals in state of the art person detectors allows us to drastically reduce the computational complexity while granting equal or higher performance in terms of accuracy.		Federico Bartoli;Giuseppe Lisanti;Svebor Karaman;Alberto Del Bimbo	2019	Pattern Recognition	10.1016/j.patcog.2018.10.008	machine learning;mixture model;sliding window protocol;computational complexity theory;small number;artificial intelligence;pattern recognition;mathematics	Vision	31.45082536067684	-48.26138288064094	170297
9370afb66743ae5567edc8a4474de442ea797c18	an integrated statistical model for multimedia evidence combination	semantic concept detection;learning algorithm;generic model;model based fusion;latent variable;average precision;evidence fusion;prior knowledge;system performance;statistical model;domain knowledge;graphical model;em algorithm	Given the rich content-based features of multimedia (e.g., visual, text, or audio) and the development of various approaches to automatic detectors (e.g., SVM, Adaboost, HMM or GMM, etc), can we find an efficient approach to combine these evidences? In the paper, we address this issue by proposing an Integrated Statistical Model (ISM) to combine diverse evidences extracted from the domain knowledge of detectors, the intrinsic structure of modality distribution and inter-concept associations. The ISM provides a unified framework for evidence fusion, having the following unique advantages: 1) the intrinsic modes in the modality distribution are discovered and modeled by a generative model; 2) each mode is a partial description of structure of the modality and the mode configuration, i.e. a set of modes, and is a new representation of the document content; 3) mode discrimination is automatically learned; 4) prior knowledge such as detector correlations and inter-concept relations can be explicitly described and integrated. More importantly, an efficient pseudo-EM algorithm is realized for training the statistical model. The learning algorithm relaxes the computational cost due to the normalized factor and latent variables in the graphical model. We evaluate system performance of our multimedia semantic concept detection with the TRECVID 2005 development dataset, in terms of efficiency and capacity. Our experimental results demonstrate that the ISM fusion outperforms the SVM based discriminative fusion method.	adaboost;algorithmic efficiency;computation;expectation–maximization algorithm;generative model;google map maker;graphical model;hidden markov model;implicit shape model;latent variable;modality (human–computer interaction);oracle fusion middleware;sensor;statistical model;support vector machine;unified framework	Sheng Gao;Joo-Hwee Lim;Qibin Sun	2007		10.1145/1291233.1291432	latent variable;statistical model;computer vision;expectation–maximization algorithm;computer science;machine learning;pattern recognition;data mining;computer performance;graphical model;domain knowledge	AI	26.181107397322315	-46.99200197229617	170710
b658c42485eab5f2314f720e5e95edb89f5555c1	application of semi-supervised learning with voronoi graph for place classification	support vector machines;path planning;training;computational geometry;mobile robots;testing;electric breakdown;accuracy;support vector machines computational geometry educational institutions indoor environment learning artificial intelligence mobile robots path planning pattern classification;indoor environment;classification algorithms;pattern classification;accuracy support vector machines training classification algorithms testing semisupervised learning electric breakdown;learning artificial intelligence;universities voronoi graph place classification space representation geometric information semantic information high level tasks complex environments onboard sensors based environments mobile robots svm based solutions crf based approaches connectivity samples information complex dependencies complementary strengths support vector machine svm conditional random field crf indoor environments diversified testing data semisupervised learning strategy real world maps;semisupervised learning;conference proceeding	Representation of spaces including both geometric and semantic information enables a robot to perform high-level tasks in complex environments. Therefore, in recent years identifying and semantically labeling the environments based on onboard sensors has become an important competency for mobile robots. Supervised learning algorithms have been extensively used for this purpose with SVM-based solutions showing good generalization properties. The CRF-based approaches take the advantage of connectivity information of samples thereby provide a mechanism to capture complex dependencies. Blending the complementary strengths of Support Vector Machine (SVM) and Conditional Random Field (CRF), there have been algorithms to exploit the advantages of both to enhance the overall accuracy of place classification in indoor environments. However, experiments show that none of the above approaches deal well with diversified testing data. In this paper, we focus mainly on the generalization ability of the model and propose a semi-supervised learning strategy, which essentially improves the performance of the system. Experiments have been carried out on six real-world maps from different universities around the world and the results from rigorous testing demonstrate the feasibility of the approach.	algorithm;alpha compositing;apple maps;bayesian network;co-training;conditional random field;experiment;high- and low-level;machine learning;map;mobile robot;semi-supervised learning;semiconductor industry;sensor;sparse matrix;supervised learning;support vector machine	Luping Shi;Sarath Kodagoda;Gamini Dissanayake	2012	2012 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2012.6385549	mobile robot;support vector machine;computer vision;computational geometry;computer science;artificial intelligence;machine learning;pattern recognition;motion planning;accuracy and precision;software testing	Robotics	31.106198062802964	-46.330987114522586	170825
a1e07c31184d3728e009d4d1bebe21bf9fe95c8e	on looking at faces in an automobile: issues, algorithms and evaluation on naturalistic driving dataset	detectors;training;head;vehicles;lighting;face detection;pose estimation	Face detection is a vital step in the process of extracting semantic information about the driver's state, such as distraction and fatigue, from pixel values in images looking at the driver. Therefore, in the context of time and safety critical situation like driving, efficient use of time and reliable detection of faces is essential. While challenges like lighting and occlusion are prevalent in the vehicle cockpit and disruptive for time and reliabilities sake, the automobile cabin has a unique and advantageous environment for face detection. In this study we introduce a deep CNN based face detection method with discrete head pose estimation which address key challenges such as lighting conditions, occlusions, varying view points. One of the vital points in training the CNN based system is the compilation of positive samples via real-world dataset and synthetic data augmentation useful for in-vehicular settings. Performance evaluation on publicly available naturalistic driving data set, called VIVA-Face Dataset, shows promising results compared to baseline methods.	3d pose estimation;algorithm;baseline (configuration management);compiler;convolutional neural network;driving simulator;face detection;performance evaluation;pixel;synthetic data	Kevan Yuen;Sujitha Martin;Mohan Manubhai Trivedi	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7900056	computer vision;detector;face detection;simulation;pose;computer science;lighting;head;computer graphics (images)	Vision	30.446780772212456	-52.038171252876865	171897
68ad01f0b01919e9fadb08248940ad8b6deffd30	online adaptation for joint scene and object classification		The proposed active learning framework learns scene and object classification models simultaneously. Both scene and object classification models take advantage of the interdependence between them to select the most informative samples with the least manual labeling cost. To the best of our knowledge, any previous work using active learning to classify scene and objects together is unknown. Leveraging upon the inter-relationships between scene and objects, we propose a new information-theoretic sample selection strategy. [Figure] This figure presents a pictorial representation of the proposed framework. Overview of Our Joint Active Learning Framework	active learning (machine learning);image;information theory;interdependence	Jawadul H. Bappy;Sujoy Paul;Amit K. Roy-Chowdhury	2016		10.1007/978-3-319-46484-8_14	computer vision;pattern recognition	Vision	25.429154411780466	-46.70549278656392	172975
121a81683a8711e8e796ad4f2243a9961c74b964	multi-view transfer learning with adaboost	algorithm design and analysis machine learning accuracy vectors hafnium sun prediction algorithms;multi view learning;prediction algorithms;classification;accuracy;vectors;machine learning;adaboost;classification transfer learning adaboost multi view learning;sun;transfer learning;learning artificial intelligence;algorithm design;algorithm design and analysis;hafnium;mv tladaboost multiview transfer learning adaboost machine learning labeled data	Transfer learning, serving as one of the most important research directions in machine learning, has been studied in various fields in recent years. In this paper, we integrate the theory of multi-view learning into transfer learning and propose a new algorithm named Multi-View Transfer Learning with Adaboost (MV-TL Adaboost). Different from many previous works on transfer learning, we not only focus on using the labeled data from one task to help to learn another task, but also consider how to transfer them in different views synchronously. We regard both the source and target task as a collection of several constituent views and each of these two tasks can be learned from every views at the same time. Moreover, this kind of multi-view transfer learning is implemented with adaboost algorithm. Furthermore, we analyze the effectiveness and feasibility of MV-TL Adaboost. Experimental results also validate the effectiveness of our proposed approach.	adaboost;algorithm;binary classification;experiment;mv-algebra;machine learning;transform, clipping, and lighting	Zhijie Xu;Shiliang Sun	2011	2011 IEEE 23rd International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2011.65	semi-supervised learning;algorithm design;multi-task learning;computer science;machine learning;pattern recognition;data mining;inductive transfer;active learning;boosting;generalization error	AI	25.794148878976454	-45.27077666573031	173412
0a511058edae582e8327e8b9d469588c25152dc6	memory constrained face recognition	memory management;discriminatory power;streaming data classification;real time recognition;face face recognition streaming media training data models memory management tagging;classification performance;online face recognition memory constrained face recognition real time recognition scarce memory computing resources classification performance classifier training limited storage resources streaming data classification discriminatory power nearest neighbor classifiers;training;image classification;classifier training;face recognition;streaming media;limited storage resources;nearest neighbor classifiers;computing resources;scarce memory;face;memory constrained face recognition;learning artificial intelligence;online face recognition;learning artificial intelligence face recognition image classification;data models;tagging	Real-time recognition may be limited by scarce memory and computing resources for performing classification. Although, prior research has addressed the problem of training classifiers with limited data and computation, few efforts have tackled the problem of memory constraints on recognition. We explore methods that can guide the allocation of limited storage resources for classifying streaming data so as to maximize discriminatory power. We focus on computation of the expected value of information with nearest neighbor classifiers for online face recognition. Experiments on real-world datasets show the effectiveness and power of the approach. The methods provide a principled approach to vision under bounded resources, and have immediate application to enhancing recognition capabilities in consumer devices with limited memory.	baseline (configuration management);computation;computer vision;expected utility hypothesis;experiment;facial recognition system;greedy algorithm;k-nearest neighbors algorithm;mobile phone;programming paradigm;real-time transcription;sampling (signal processing);stream (computing);streaming media	Ashish Kapoor;Simon Baker;Sumit Basu;Eric Horvitz	2012	2012 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2012.6247971	facial recognition system;face;data modeling;computer vision;contextual image classification;speech recognition;computer science;machine learning;pattern recognition;geometry;memory management	Vision	26.846569955596422	-47.441604298332784	173554
6fb8f2a8ed6e7185294714b81159588aba6803d9	transfer subspace learning for cross-dataset facial expression recognition	facial expression recognition;multi view learning;subspace learning;biometrics;transfer learning	In this paper, we propose a transfer subspace learning approach cross-dataset facial expression recognition. To our best knowledge, this problem has been seldom addressed in the literature. While many facial expression recognition methods have been proposed in recent years, most of them assume that face images in the training and testing sets are collected under the same conditions so that they are independently and identically distributed. In many real applications, this assumption does not hold as the testing data are usually collected online and are generally more uncontrollable than the training data. Hence, the testing samples are likely different from the training samples. In this paper, we define this problem as cross-dataset facial expression recognition as the training and testing data are considered to be collected from different datasets due to different acquisition conditions. To address this, we propose a transfer subspace learning approach to learn a feature subspace which transfers the knowledge gained from the source domain (training samples) to the target domain (testing samples) to improve the recognition performance. To better exploit more complementary information for multiple feature representations of face images, we develop a multi-view transfer subspace learning approach where multiple different yet related subspaces are learned to transfer information from the source domain to the target domain. Experimental results are presented to demonstrate the efficacy of these proposed methods for the cross-dataset facial expression recognition task.		Haibin Yan	2016	Neurocomputing	10.1016/j.neucom.2015.11.113	multi-task learning;computer vision;transfer of learning;computer science;machine learning;pattern recognition;biometrics	Vision	26.991710491731062	-45.23252373922518	173566
cb474ea7c5f6d3c25a75a653baaad19cd12c8536	three-layer spatial sparse coding for image classification	pyramidal cell;minimization;image to category similarity;image coding;image classification;multiple scales;image coding artificial neural networks encoding visualization pixel minimization image reconstruction;visualization;artificial neural networks;computational complexity;image reconstruction;pixel;image to category similarity three layer spatial sparse coding image classification tssc spatial configuration spatial pyramid;tssc;image coding image classification;encoding;sparse coding;spatial configuration;three layer spatial sparse coding image classification sparse coding;three layer spatial sparse coding;spatial pyramid	In this paper, we propose a three-layer spatial sparse coding (TSSC) for image classification, aiming at three objectives: naturally recognizing image categories without learning phase, naturally involving spatial configurations of images, and naturally counteracting the intra-class variances. The method begins by representing the test images in a spatial pyramid as the to-be-recovered signals, and taking all sampled image patches at multiple scales from the labeled images as the bases. Then, three sets of coefficients are involved into the cardinal sparse coding to get the TSSC, one to penalize spatial inconsistencies of the pyramid cells and the corresponding selected bases, one to guarantee the sparsity of selected images, and the other to guarantee the sparsity of selected categories. Finally, the test images are classified according to a simple image-to-category similarity defined on the coding coefficients. In experiments, we test our method on two publicly available datasets and achieve significantly more accurate results than the conventional sparse coding with only a modest increase in computational complexity.	coefficient;computation;computational complexity theory;computer vision;experiment;multitier architecture;neural coding;sparse matrix	Dengxin Dai;Wen Yang;Tianfu Wu	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.155	iterative reconstruction;computer vision;contextual image classification;visualization;computer science;machine learning;pattern recognition;mathematics;neural coding;computational complexity theory;artificial neural network;pixel;encoding	Vision	25.678624656029392	-47.092286288133835	174048
dddfc10d9649a936cc440c1f3590b14e51a81daa	bringing background into the foreground: making all classes equal in weakly-supervised video semantic segmentation		Pixel-level annotations are expensive and timeconsuming to obtain. Hence, weak supervision using only image tags could have a significant impact in semantic segmentation. Recent years have seen great progress in weakly-supervised semantic segmentation, whether from a single image or from videos. However, most existing methods are designed to handle a single background class. In practical applications, such as autonomous navigation, it is often crucial to reason about multiple background classes. In this paper, we introduce an approach to doing so by making use of classifier heatmaps. We then develop a two-stream deep architecture that jointly leverages appearance and motion, and design a loss based on our heatmaps to train it. Our experiments demonstrate the benefits of our classifier heatmaps and of our two-stream architecture on challenging urban scene datasets and on the YouTube-Objects benchmark, where we obtain state-of-the-art results.	autonomous robot;autostereogram;benchmark (computing);experiment;heat map;optical flow;pixel;semantics (computer science);statistical classification	Fatemehsadat Saleh;Mohammad Sadegh Ali Akbarian;Mathieu Salzmann;Lars Petersson;Jose M. Alvarez	2017	2017 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2017.232	optical imaging;machine learning;artificial intelligence;computer vision;pattern recognition;semantics;architecture;computer science;segmentation;classifier (linguistics);image segmentation	Vision	28.132002282815957	-50.915933452112164	174069
325723a7fa69f9976feeab5ba9abd3c11e3f7c80	beyond textures: learning from multi-domain artistic images for arbitrary style transfer		We propose a fast feed-forward network for arbitrary style transfer, which can generate stylized image for previously unseen content and style image pairs. Besides the traditional content and style representation based on deep features and statistics for textures, we use adversarial networks to regularize the generation of stylized images. Our adversarial network learns the intrinsic property of image styles from large-scale multi-domain artistic images. The adversarial training is challenging because both the input and output of our generator are diverse multi-domain images. We use a conditional generator that stylized content by shifting the statistics of deep features, and a conditional discriminator based on the coarse category of styles. Moreover, we propose a mask module to spatially decide the stylization level and stabilize adversarial training by avoiding mode collapse. As a side effect, our trained discriminator can be applied to rank and select representative stylized images. We qualitatively and quantitatively evaluate the proposed method, and compare with recent style transfer methods.	discriminator;feedforward neural network;input/output;texture mapping	Zheng Xu;Michael J. Wilber;Chen Fang;Aaron Hertzmann;Hailin Jin	2018	CoRR		stylized fact;pattern recognition;machine learning;adversarial system;intrinsic and extrinsic properties (philosophy);artificial intelligence;discriminator;computer science;input/output	Vision	25.02298730647299	-49.88802560197258	174643
27784c7d525b338f236de37a824a010d636102cf	detecting rare events using kullback-leibler divergence: a weakly supervised approach	kullback leibler divergence;anomaly detection;event detection;weakly supervised learning	Video surveillance infrastructure has been widely installed in public places for security purposes. However, live video feeds are typically monitored by human staff, making the detection of important events as they occur difficult. As such, an expert system that can automatically detect events of interest in surveillance footage is highly desirable. Although a number of approaches have been proposed, they have significant limitations: supervised approaches, which can detect a specific event, ideally require a large number of samples with the event spatially and temporally localised; while unsupervised approaches, which do not require this demanding annotation, can only detect whether an event is abnormal and not specific event types. To overcome these problems, we formulate a weakly-supervised approach using Kullback– Leibler (KL) divergence to detect rare events. The proposed approach leverages the sparse nature of the target events to its advantage, and we show that this data imbalance guarantees the existence of a decision boundary to separate samples that contain the target event from those that do not. This trait, combined with the coarse annotation used by weakly supervised learning (that only indicates approximately when an event occurs), greatly reduces the annotation burden while retaining the ability to detect specific events. Furthermore, the proposed classifier requires only a decision threshold, simplifying its use compared to other weakly supervised approaches. We show that the proposed approach outperforms state-of-the-art methods on a popular real-world traffic surveillance dataset, while preserving real time performance. © 2016 Elsevier Ltd. All rights reserved.	closed-circuit television;decision boundary;expert system;kullback–leibler divergence;rare events;sparse matrix;supervised learning;unsupervised learning	Jingxin Xu;Simon Denman;Clinton Fookes;Sridha Sridharan	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.01.035	anomaly detection;computer science;machine learning;pattern recognition;data mining;kullback–leibler divergence;statistics	AI	31.422408101586417	-47.359401198166964	174893
5e7f2b84cdbe5654f0e2c56b9c9b7db2a704a3c2	hierarchical semantic image matching using cnn feature pyramid		Abstract Image matching remains an important and challenging problem in computer vision, especially for the dense correspondence estimation between images with high category-level similarity. The effectiveness of image matching largely depends on the advance of image descriptors. Inspired by the success of Convolutional Neural Network(CNN), we propose a hierarchal image matching method using the CNN feature pyramid, named as CNN Flow. The feature maps output by different layers of CNN tend to encode different information of the input image, such as the semantic information extracted from higher layers and the structural information extracted from lower layers. This nature of CNN feature pyramid is suitable to build the hierarchical image matching framework, which detects the patterns of different levels in an implicit coarse-to-fine manner. In particular, we take advantage of the complementarity of different layers using guidance from higher layer to lower layer. The high-layer features present semantic patterns to cope with the intra-class variations, and the guidance from high layers can resist the semantic ambiguity of low-layer features due to small receptive fields. The bottom-level matching utilize the low-layer features with more structural information to achieve finer matching. On one hand, extensive experiments and analysis demonstrate the superiority of CNN Flow in image dense matching under challenging variations. On the other hand, CNN Flow is demonstrated through various applications, such as fine alignment for intra-class object, scene label transfer and facial expression transfer.	image registration	Wei Yu;Xiaoshuai Sun;Kuiyuan Yang;Yong Rui;Hongxun Yao	2018	Computer Vision and Image Understanding	10.1016/j.cviu.2018.01.001	computer vision;mathematics;artificial intelligence;convolutional neural network;visual descriptors;ambiguity;pyramid;complementarity (molecular biology);receptive field	Vision	29.619864241500025	-51.97438286183264	175038
62fddae74c553ac9e34f511a2957b1614eb4f937	action recognition based on efficient deep feature learning in the spatio-temporal domain	action recognition deep feature learning spatiotemporal domain domain knowledge data driven feature learning method 2 d convolutional neural network concatenated 3 d network raw video data content based video recognition pretrained network data capture feature extraction;three dimensional displays computational modeling feature extraction convolution robots data models optical imaging;feature extraction image capture image recognition learning artificial intelligence neural nets video signal processing;convolution;computational modeling;optical imaging;computer vision for automation recognition visual learning;three dimensional displays;feature extraction;robots;feature extraction action recognition deep feature learning spatiotemporal domain domain knowledge data driven feature learning method 2 d convolutional neural network concatenated 3 d network raw video data content based video recognition pretrained network data capture;video signal processing feature extraction image capture image recognition learning artificial intelligence neural nets;visual learning computer vision for automation recognition;data models	Hand-crafted feature functions are usually designed based on the domain knowledge of a presumably controlled environment and often fail to generalize, as the statistics of real-world data cannot always be modeled correctly. Data-driven feature learning methods, on the other hand, have emerged as an alternative that often generalize better in uncontrolled environments. We present a simple, yet robust, 2-D convolutional neural network extended to a concatenated 3-D network that learns to extract features from the spatio-temporal domain of raw video data. The resulting network model is used for content-based recognition of videos. Relying on a 2-D convolutional neural network allows us to exploit a pretrained network as a descriptor that yielded the best results on the largest and challenging ILSVRC-2014 dataset. Experimental results on commonly used benchmarking video datasets demonstrate that our results are state-of-the-art in terms of accuracy and computational time without requiring any preprocessing (e.g., optic flow) or a priori knowledge on data capture (e.g., camera motion estimation), which makes it more general and flexible than other approaches. Our implementation is made available.	artificial neural network;autostereogram;baseline (configuration management);central processing unit;computation;computer vision;concatenation;convolution;convolutional neural network;epoch (reference date);experiment;feature learning;graphics processing unit;motion estimation;network model;network planning and design;optical flow;preprocessor;test set;time complexity;uncompressed video;uncontrolled format string	Farzad Husain;Babette Dellen;Carme Torras	2016	IEEE Robotics and Automation Letters	10.1109/LRA.2016.2529686	neural gas;robot;data modeling;feature learning;computer vision;feature;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;optical imaging;time delay neural network;deep learning;convolution;computational model;feature	Vision	27.32364484304497	-51.669786212923526	175188
9125903bf7aa68920fdca8296f703b9013a877ed	deep spatial pyramid for person re-identification		Re-identification refers to the task of finding the same subject across a network of surveillance cameras. This task must deal with appearance changes caused by variations in illumination, a person's pose, camera viewing angle and background clutter. State-of-the-art approaches usually focus either on feature modeling — designing image descriptors that are robust to changes in imaging conditions, or dissimilarity functions — learning effective metrics to compare images from different cameras. Typically, with novel deep architectures both approaches can be merged into a single end-to-end training, but to become effective, this requires annotating thousands of subjects in each camera pair. Unlike standard CNN-based approaches, we introduce a spatial pyramid-like structure to the image and learn CNNs for image sub-regions at different scales. When training a CNN using only image sub-regions, we force the model to recognize not only the person's identity but also the spatial location of the sub-region. This results in highly effective feature representations, which when combined with Mahalanobis-like metric learning significantly outperform state-of-the-art approaches.	closed-circuit television;clutter;de-identification;deep learning;end-to-end principle;feature model;pose (computer vision);stereo camera;viewing angle;visual descriptor	Slawomir Bak Peter Carr	2017	2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2017.8078492	computer vision;artificial intelligence;robustness (computer science);pattern recognition;viewing angle;pyramid;artificial neural network;feature extraction;visual descriptors;computer science;clutter	Vision	31.64553832585438	-50.95823223352316	175291
04f4679765d2f71576dd77c1b00a2fd92e5c6da4	part detector discovery in deep convolutional neural networks		Current fine-grained classification approaches often rely on a robust localization of object parts to extract localized feature representations suitable for discrimination. However, part localization is a challenging task due to the large variation of appearance and pose. In this paper, we show how pre-trained convolutional neural networks can be used for robust and efficient object part discovery and localization without the necessity to actually train the network on the current dataset. Our approach called “part detector discovery” (PDD) is based on analyzing the gradient maps of the network outputs and finding activation centers spatially related to annotated semantic parts or bounding boxes. This allows us not just to obtain excellent performance on the CUB2002011 dataset, but in contrast to previous approaches also to perform detection and bird classification jointly without requiring a given bounding box annotation during testing and ground-truth parts during training. The code is available at http://www.inf-cv.uni-jena.de/part_ discovery and https://github.com/cvjena/PartDetectorDisovery.	activation function;artificial neural network;convolutional neural network;gradient;ground truth;map;minimum bounding box	Marcel Simon;Erik Rodner;Joachim Denzler	2014		10.1007/978-3-319-16808-1_12	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics	ML	29.02678939981613	-50.612937656822915	175867
440349a3c81780ef08254f5a59229cad74f54904	dreaming more data: class-dependent distributions over diffeomorphisms for learned data augmentation		Data augmentation is a key element in training highdimensional models. In this approach, one synthesizes new observations by applying pre-specified transformations to the original training data; e.g. new images are formed by rotating old ones. Current augmentation schemes, however, rely on manual specification of the applied transformations, making data augmentation an implicit form of feature engineering. Working towards true end-to-end learning, we suggest to learn the applied transformations on a per-class basis. Particularly, we align image pairs within each class under the assumption that the spatial transformation between images belongs to a large class of diffeomorphisms. For each class, we then build a probabilistic generative model of the transformations in a Riemannian submanifold of the Lie group of diffeomorphisms. We demonstrate significant performance improvements in training deep neural nets over manuallyspecified augmentation schemes.	align (company);artificial neural network;convolutional neural network;deep learning;end-to-end principle;feature engineering;generative model	Søren Hauberg;Oren Freifeld;Anders Boesen Lindbo Larsen;John W. Fisher;Lars Kai Hansen	2016			computer vision;discrete mathematics;machine learning;mathematics;geometry	ML	24.9712472648586	-49.28320035024921	175959
38f44f3fee43b4cdf4379397d519bc60aef59d24	fish species recognition from video using svm classifier	multimedia and multimodal retrieval;specialized information retrieval;video search	To build a detailed knowledge of the biodiversity, the geographical distribution and the evolution of the alive species is essential for a sustainable development and the preservation of this biodiversity. Massive databases of underwater video surveillance have been recently made available for supporting designing algorithms targeting the identification of fishes. However these video datasets are rather poor in terms of video resolution, pretty challenging regarding both the natural phenomena to be considered such as murky water, seaweed moving the water current, etc, and the huge amount of data to be processed. We have designed a processing chain based on background segmentation, selection keypoints with an adaptive scale, description with OpponentSift and learning of each species by a binary linear Support Vector Machines classifier.  Our algorithm has been evaluated in the context of our participation to the Fish task of the LifeCLEF2014 challenge. Compared to the baseline designed by the LifeCLEF challenge organizers, our approach reaches a better precision but a worse recall. Our performances in terms of species recognition (based only on the correctly detected bounding boxes) is comparable to the baseline, but our bounding boxes are often too large and our score is so penalized. Our results are thus really encouraging.	algorithm;baseline (configuration management);closed-circuit television;database;display resolution;performance;support vector machine	Katy Blanc;Diane Lingrand;Frédéric Precioso	2014		10.1145/2661821.2661827	computer vision;computer science;data mining;communication	AI	28.582301861984476	-50.586706800517106	176056
ab758c7d163dba039f1b1badaa9ea72064c887ba	deeproad: gan-based metamorphic testing and input validation framework for autonomous driving systems		While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.   In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.	application domain;autonomous car;autonomous robot;cns;data validation;generative adversarial networks;ibm notes;machine learning;metamorphic testing;synthetic intelligence;test suite;unsupervised learning	Mengshi Zhang;Yuqun Zhang;Lingming Zhang;Cong Liu;Sarfraz Khurshid	2018		10.1145/3238147.3238187	theoretical computer science;robustness (computer science);application domain;computer science;artificial neural network;data validation;metamorphic testing;machine learning;test suite;compromise;artificial intelligence	SE	27.342420710275054	-50.07622700491261	176097
b760e57499dfbddf5c86ddb1c3bdd8da10188f61	learning group-based dictionaries for discriminative image representation	discriminative image representation;image classification;structural learning;bag of visual words;group based dictionary learning	Dictionary learning is a critical issue for achieving discriminative image representation in many computer vision tasks such as object detection and image classification. In this paper, a new algorithm is developed for learning discriminative group-based dictionaries, where the inter-concept (category) visual correlations are leveraged to enhance both the reconstruction quality and the discrimination power of the group-based discriminative dictionaries. A visual concept network is first constructed for determining the groups of visually similar object classes and image concepts automatically. For each group of such visually similar object classes and image concepts, a group-based dictionary is learned for achieving discriminative image representation. A structural learning approach is developed to take advantage of our group-based discriminative dictionaries for classifier training and image classification. The effectiveness and the discrimination power of our group-based discriminative dictionaries have been evaluated on multiple popular visual benchmarks. HighlightsA visual concept network is built to characterize the inter-concept correlations.An automatic algorithm is proposed for identifying the visually similar groups.A new algorithm is developed to learn group-based discriminative dictionaries.A structural method is developed for classifier training and image classification.Our proposed algorithms are evaluated on multiple popular visual benchmarks.	dictionary	Hao Lei;Kuizhi Mei;Nanning Zheng;Peixiang Dong;Ning Zhou;Jianping Fan	2014	Pattern Recognition	10.1016/j.patcog.2013.07.016	computer vision;contextual image classification;computer science;machine learning;pattern recognition;bag-of-words model in computer vision	Vision	26.460385336789585	-46.09662373968156	176159
b1dc24d4eedd75cd9096acfb95f0de515b9a8a06	a unified online dictionary learning framework with label information for robust object tracking	object tracking dictionaries image classification learning artificial intelligence;optimal linear multiclassifier unified online dictionary learning framework label information robust object tracking supervised approach structured sparse discriminative representation discriminative dictionary ideal code regularization term high quality dictionary;target tracking dictionaries robustness object tracking encoding classification algorithms;object tracking;optimal linear multi classifier label information the unified objective function for online dictionary learning;dictionaries;classification algorithms;robustness;target tracking;encoding;the unified objective function for online dictionary learning;label information;optimal linear multi classifier	In this paper, a supervised approach to online learn a structured sparse and discriminative representation for object tracking is presented. Label information from training data is incorporated into the dictionary learning process to construct a robust and discriminative dictionary. This is accomplished by adding an ideal-code regularization term and classification error term to the unified objective function. By minimizing the unified objective function we learn the high quality dictionary and optimal linear multi-classifier jointly. Combined with robust sparse coding, the learned classifier is employed directly to separate the object from background. As the tracking continues, the proposed algorithm alternates between robust sparse coding and dictionary updating. Experimental evaluations on the challenging sequences show that the proposed algorithm performs favorably against state-of-the-art methods in terms of effectiveness, accuracy and robustness.	algorithm;dictionary;display resolution;loss function;machine learning;neural coding;optimization problem;robotics;sparse matrix;supervised learning;television	Baojie Fan;Jing Sun;Yang Cong;Yingkui Du	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.401	statistical classification;k-svd;computer science;machine learning;video tracking;pattern recognition;data mining;encoding;robustness	Vision	25.885609281555816	-45.099789832129275	176680
ab368594b9bd569e8d0fcf5c6010f1c31e3aa39e	node-adapt, path-adapt and tree-adapt: model-transfer domain adaptation for random forest		Random Forest (RF) is a successful paradigm for learning classifiers due to its ability to learn from large feature spaces and seamlessly integrate multi-class classification, as well as the achieved accuracy and processing efficiency. However, as many other classifiers, RF requires domain adaptation (DA) provided that there is a mismatch between the training (source) and testing (target) domains which provokes classification degradation. Consequently, different RF-DA methods have been proposed, which not only require target-domain samples but revisiting the source-domain ones, too. As novelty, we propose three inherently different methods (Node-Adapt, Path-Adapt and Tree-Adapt) that only require the learned sourcedomain RF and a relatively few target-domain samples for DA, i.e. source-domain samples do not need to be available. To assess the performance of our proposals we focus on image-based object detection, using the pedestrian detection problem as challenging proof-of-concept. Moreover, we use the RF with expert nodes because it is a competitive patch-based pedestrian model. We test our Node-, Pathand Tree-Adapt methods in standard benchmarks, showing that DA is largely achieved.	adaptive multi-rate audio codec;benchmark (computing);domain adaptation;elegant degradation;multiclass classification;object detection;pedestrian detection;programming paradigm;radio frequency;random forest;virtual world	Azadeh Sadat Mozafari;David Vázquez;Mansour Jamzad;Antonio M. López	2016	CoRR		simulation;artificial intelligence;machine learning;pattern recognition	ML	28.504386413737915	-49.97803871213643	177610
b437bd0ae14cf314424d470e2da1b4a81c7cb6ca	efficiently consistent affinity propagation for 3d shapes co-segmentation		Unsupervised co-segmentation for a set of 3D shapes is a challenging problem as no prior information is provided. The accuracy of the current approaches is necessarily restricted by the accuracy of the unsupervised face classification, which is used to provide an initialization for the following optimization to improve the consistency between adjacent faces. However, it is exceedingly difficult to obtain a satisfactory initialization pre-segmentation owing to variation in topology and geometry of 3D shapes. In this study, we consider the unsupervised 3D shape co-segmentation as an exemplar-based clustering problem, aimed at simultaneously discovering optimal exemplars and obtaining co-segmentation results. Therefore, we introduce a novel exemplar-based clustering method based on affinity propagation for 3D shape co-segmentation, which can automatically identify representative exemplars and patterns in 3D shapes considering the high-order statistics, yielding consistent and accurate co-segmentation results. Experiments using various datasets, especially large sets with 200 or more shapes that would be challenging to manually segment, demonstrate that our method exhibits a better performance compared to state-of-the-art methods.	affinity propagation;cluster analysis;experiment;mathematical optimization;software propagation;unsupervised learning	Xiaogang Wang;Bin Zhou;Zongji Wang;Dongqing Zou;Xiaowu Chen;Qinping Zhao	2018	The Visual Computer	10.1007/s00371-018-1538-2	artificial intelligence;computer vision;computer science;cluster analysis;initialization;shape analysis (digital geometry);affinity propagation;segmentation;pattern recognition	AI	29.59425612183595	-48.649369268891526	177722
75b1d6e486a86f43f105338b55876fc1fbb98de2	multimodal ground-based cloud classification using joint fusion convolutional neural network		The accurate ground-based cloud classification is a challenging task and still under development. The most current methods are limited to only taking the cloud visual features into consideration, which is not robust to the environmental factors. In this paper, we present the novel joint fusion convolutional neural network (JFCNN) to integrate the multimodal information for ground-based cloud classification. To learn the heterogeneous features (visual features and multimodal features) from the ground-based cloud data, we designed the proposed JFCNN as a two-stream structure which contains the vision subnetwork and multimodal subnetwork. We also proposed a novel layer named joint fusion layer to jointly learn two kinds of cloud features under one framework. After training the proposed JFCNN, we extracted the visual and multimodal features from the two subnetworks and integrated them using a weighted strategy. The proposed JFCNN was validated on the multimodal ground-based cloud (MGC) dataset and achieved remarkable performance, demonstrating its effectiveness for ground-based cloud classification task.	convolutional neural network;multimodal interaction	Shuang Liu;Mei Li;Zhong Zhang;Baihua Xiao;Xiaozhong Cao	2018	Remote Sensing	10.3390/rs10060822	data mining;convolutional neural network;geology;computer vision;joint fusion;artificial intelligence;cloud computing;subnetwork	Mobile	28.09661939600024	-51.27482435448758	177946
80b9bd013bbfbb1aafdb3da5f3f7afd95739fda6	applying latent semantic analysis to large-scale medical image databases	cbir;lsa;data fusion;classification;svd;text retrieval;feature selection;image retrieval	Latent Semantic Analysis (LSA) although has been used successfully in text retrieval when applied to CBIR induces scalability issues with large image collections. The method so far has been used with small collections due to the high cost of storage and computational time for solving the SVD problem for a large and dense feature matrix. Here we present an effective and efficient approach of applying LSA skipping the SVD solution of the feature matrix and overcoming in this way the deficiencies of the method with large scale datasets. Early and late fusion techniques are tested and their performance is calculated. The study demonstrates that early fusion of several composite descriptors with visual words increase retrieval effectiveness. It also combines well in a late fusion for mixed (textual and visual) ad hoc and modality classification. The results reported are comparable to state of the art algorithms without including additional knowledge from the medical domain.		Spyridon Stathopoulos;Theodore Kalamboukis	2015	Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society	10.1016/j.compmedimag.2014.05.009	visual word;biological classification;image retrieval;computer science;machine learning;pattern recognition;data mining;sensor fusion;singular value decomposition;feature selection;information retrieval	Vision	29.7910615309982	-45.71782804557285	178827
62d88fa54a864629c5a2ec4e3a0fcd2c5cffc248	aid++: an updated version of aid on scene classification		Aerial image scene classification is a fundamental problem for understanding high-resolution remote sensing images and has become an active research task in the field of remote sensing due to its important role in a wide range of applications. However, the limitations of existing datasets for scene classification, such as the small scale and low-diversity, severely hamper the potential usage of the new generation deep convolutional neural networks (CNNs). Although huge efforts have been made in building large-scale datasets very recently, e.g., the Aerial Image Dataset (AID) which contains 10,000 image samples, they are still far from sufficient to fully train a high-capacity deep CNN model. To this end, we present a larger-scale dataset in this paper, named as AID++, for aerial scene classification based on the AID dataset. The proposed AID++ consists of more than 400,000 image samples that are semi-automatically annotated by using the existing the geographical data. We evaluate several prevalent CNN models on the proposed dataset, and the results show that our dataset can be used as a promising benchmark for scene classification.	aerial photography;artificial neural network;benchmark (computing);convolutional neural network;image resolution;scene graph;semiconductor industry	Pu Jin;Gui-Song Xia;Fan Hu;Qikai Lu;Liangpei Zhang	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518882	computer vision;convolutional neural network;task analysis;machine learning;artificial intelligence;data modeling;aerial image;computer science	Vision	28.251532019606685	-50.82788994283736	178904
d783d04bdbef9de86f64222025b69a5dc49160af	weakly-supervised man-made object recognition in underwater optimal image through deep domain adaptation		Underwater man-made object recognition in optical images plays important roles in both image processing and oceanic engineering. Deep learning methods have received impressive performances in many recognition tasks in in-air images, however, they will be limited in the proposed task since it is tough to collect and annotate sufficient data to train the networks. Considered that large-scale in-air images of man-made objects are much easier to acquire in the applications, one can train a network on in-air images and directly applying it on underwater images. However, the distribution mismatch between in-air and underwater images will lead to a significant performance drop. In this work, we propose an end-to-end weakly-supervised framework to recognize underwater man-made objects with large-scale labeled in-air images and sparsely labeled underwater images. And a novel two-level feature alignment approach, is introduced to a typical deep domain adaptation network, in order to tackle the domain shift between data generated from two modalities. We test our methods on our newly simulated datasets containing two image domains, and achieve an improvement of approximately 10 to 20 % points in average accuracy compared to the best-performing baselines.		Chaoqi Chen;Weiping Xie;Yue Huang;Xian Yu;Xinghao Ding	2018		10.1007/978-3-030-04221-9_28	image processing;modalities;deep learning;artificial intelligence;pattern recognition;domain adaptation;computer science;underwater;cognitive neuroscience of visual object recognition	Vision	28.23389359109936	-51.075184226673564	179096
b4bb131ac41c8a997e1d57f76ff1cc9e72b9fbcf	odd: an algorithm of online directional dictionary learning for sparse representation		Recently, some sparse representation based image reconstruction methods have demonstrated with a learnt dictionary. In this paper, we propose a block-based image sparse representation approach with an online directional dictionary (ODD). Unlike the conventional dictionary learning approaches for image sparse representation aims at learning some signal patterns from a large set of training image patches, the proposed joint dictionary for each patch is composed by an original offline or online trained sub-dictionary from a training set and an novel adaptive directional sub-dictionary estimated from the reconstructed nearby pixels of the patch itself. A joint dictionary with ODD has two main advantages compared with the conventional dictionaries. First, for each patch to be sparse represented, not only the most general contents, but also the most possible directional textures of the image patch are considered to improve the reconstruction performance. Second, in order to save storage costs, only the original trained sub-dictionary should be stored, the proposed ODD can be obtained consistently. Experimental results show that the reconstruction performance of the proposed approach exceeds other competitive dictionary learning based image sparse representation methods, validating the superiority of our approach.	algorithm;dictionary;machine learning;sparse approximation	Dan Xu;Xinwei Gao;Xiaopeng Fan;Debin Zhao;Wen Gao	2017		10.1007/978-3-319-77383-4_92	iterative reconstruction;artificial intelligence;computer science;pixel;sparse approximation;pattern recognition;training set	AI	28.481434110976792	-46.03124476991551	179152
a3b00cd64541fc2b9d2ede118263d7e9f94043cb	locality constraint neighbour embedding via reference patch	databases;manifolds;training;reference patch;search problems face recognition image reconstruction image resolution;position patch;image reconstruction;dictionaries;super resolution;ip networks;locality constraints;face;face hallucination;locality constraints face hallucination super resolution position patch reference patch;visual quality locality constraint neighbour embedding reference patch fh methods face hallucination methods search criteria k nearest neighbors k nn human facial features low resolution patch lr patch high resolution patches hr patch discriminant locality constraints reconstruction weights position patch schemes reconstruction error;face image reconstruction databases manifolds dictionaries training ip networks	Recently, face hallucination (FH) methods using position priors have gained popularity; however position priors might not always be the best due to the intrinsic rigidness of faces collected from uncontrollable environment. Therefore, we improve the search criteria for K-nearest neighbors (K-NN) to address the variations in human facial features. Meanwhile, the limitations of the manifold assumption are taken into consideration to refine the neighborhood of the low-resolution (LR) patch by using the information from the high-resolution (HR) patches. For each input patch, we search the local neighborhood of its corresponding position patch in each training image to find the best-matched neighbor “Reference Patches”. Reference patches and their HR counterparts are taken to construct LR and HR patch dictionaries. The proposed method is composed of two steps. For an input LR patch, first we construct its initial HR patch using conventional FH methods. Secondly, we search the initial HR patch's nearest neighbors in HR manifold to extract the discriminant locality constraints. Then the corresponding LR reference patches are taken as refined K-NN of the input patch. These refined reference patches better optimize the reconstruction weights, thus the performance is improved. Extensive experiments show that our method outperforms recent position patch schemes in reconstruction error and visual quality.	dictionary;discriminant;experiment;face hallucination;high-resolution scheme;image resolution;k-nearest neighbors algorithm;lr parser;locality of reference;reconstruction filter;web search engine	Javaria Ikram;Lu Yao;Danfeng Wan;Jianwu Li	2015	2015 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2015.7177383	iterative reconstruction;face;computer vision;manifold;computer science;machine learning;pattern recognition;mathematics;face hallucination;superresolution	Vision	28.72526657829442	-45.934294385843145	179191
ff12a6aabd72310cc346d1c32e80c8661b3b48c6	adversarial training for dual-stage image denoising enhanced with feature matching		We propose a dual-stage convolutional neural network, augmented with adversarial training, to address the shortcoming of current convolutional neural networks in image denoising. Our dual-stage approach, coupled with feature matching, is especially effective in recovering fine detail under high noise level. First, we use residual learning denoising to output a preliminary denoised reference image. Then, an image reconstruction denoiser uses a multi-scale feature selection layer, which deploys skip-connections and ResNet blocks to recover the image detail based on the noisy image and the reference image. This dual-stage denoising is augmented with the feedback from a discriminator, which forms an adversarial training framework and guides the denoising towards a clean image construction. The feature matching process embedded in the discriminator ensures that the framework can be generalized to a diverse collection of image content. Experimental results show better denoising performance in public benchmark datasets compared with the state-of-the-art approaches.		Xinyao Sun;Navaneeth Kamballur Kottayil;Subhayan Mukherjee;L. Irene Cheng	2018		10.1007/978-3-030-04375-9_30	computer science;systems engineering;convolutional neural network;residual;iterative reconstruction;adversarial system;noise reduction;feature selection;discriminator;residual neural network;artificial intelligence;pattern recognition	Vision	25.11521848184281	-51.31737848947541	179403
65539436abf0eedabeb915a52f787b962722c99a	satellite image classification via two-layer sparse coding with biased image representation	satellite communication;image to category similarity;image coding;satellite image classification;support vector machines;coding coefficient;image classification;journal;visual attention satellite image classification two layer sparse coding tsc;visualization;image color analysis;image representation;feature extraction;satellites;two layer sparse coding tsc;satellite image;satellites image classification image coding image representation biological system modeling layout support vector machines support vector machine classification humans image reconstruction;visual attention;encoding;satellite image database satellite image classification two layer sparse coding model biased image representation image to category similarity coding coefficient;sparse coding;satellite image database;biased image representation;two layer sparse coding model;satellite communication encoding image classification image representation	This letter presents a method for satellite image classification aiming at the following two objectives: 1) involving visual attention into the satellite image classification; biologically inspired saliency information is exploited in the phase of the image representation, making our method more concentrated on the interesting objects and structures, and 2) handling the satellite image classification without the learning phase. A two-layer sparse coding (TSC) model is designed to discover the “true” neighbors of the images and bypass the intensive learning phase of the satellite image classification. The underlying philosophy of the TSC is that an image can be more sparsely reconstructed via the images (sparse I) belonging to the same category (sparse II). The images are classified according to a newly defined “image-to-category” similarity based on the coding coefficients. Requiring no training phase, our method achieves very promising results. The experimental comparisons are shown on a real satellite image database.	coefficient;computer vision;neural coding;remote desktop services;sparse matrix	Dengxin Dai;Wen Yang	2011	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2010.2055033	support vector machine;computer vision;contextual image classification;visualization;feature extraction;computer science;machine learning;pattern recognition;neural coding;communications satellite;satellite;encoding	Vision	29.693166285322004	-45.23714056489501	179544
de5b1423d5c7104227ce55073e79744d62f8db50	dynamic scene deblurring using spatially variant recurrent neural networks		Due to the spatially variant blur caused by camera shake and object motions under different scene depths, deblurring images captured from dynamic scenes is challenging. Although recent works based on deep neural networks have shown great progress on this problem, their models are usually large and computationally expensive. In this paper, we propose a novel spatially variant neural network to address the problem. The proposed network is composed of three deep convolutional neural networks (CNNs) and a recurrent neural network (RNN). RNN is used as a deconvolution operator performed on feature maps extracted from the input image by one of the CNNs. Another CNN is used to learn the weights for the RNN at every location. As a result, the RNN is spatially variant and could implicitly model the deblurring process with spatially variant kernels. The third CNN is used to reconstruct the final deblurred feature maps into restored image. The whole network is end-to-end trainable. Our analysis shows that the proposed network has a large receptive field even with a small model size. Quantitative and qualitative evaluations on public datasets demonstrate that the proposed method performs favorably against state-of-the-art algorithms in terms of accuracy, speed, and model size.		Jingshuai Zhang;Jinshan Pan;Jimmy S. J. Ren;Yibing Song;Linchao Bao;Rynson W. H. Lau;Ming-Hsuan Yang	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00267	computer vision;kernel (linear algebra);convolutional neural network;image segmentation;deconvolution;artificial neural network;image restoration;artificial intelligence;recurrent neural network;pattern recognition;computer science;deblurring	Vision	25.822875272482573	-51.46194991460564	179721
989332c5f1b22604d6bb1f78e606cb6b1f694e1a	recurrent face aging	latent representation recurrent face aging human face cross age face verification cross age face recognition rfa framework recurrent neural network one step face feature transformation two layer gated recurrent unit;recurrent neural nets age issues face recognition feature extraction image representation;face aging prototypes lighting recurrent neural networks dictionaries computational modeling	Modeling the aging process of human face is important for cross-age face verification and recognition. In this paper, we introduce a recurrent face aging (RFA) framework based on a recurrent neural network which can identify the ages of people from 0 to 80. Due to the lack of labeled face data of the same person captured in a long range of ages, traditional face aging models usually split the ages into discrete groups and learn a one-step face feature transformation for each pair of adjacent age groups. However, those methods neglect the in-between evolving states between the adjacent age groups and the synthesized faces often suffer from severe ghosting artifacts. Since human face aging is a smooth progression, it is more appropriate to age the face by going through smooth transition states. In this way, the ghosting artifacts can be effectively eliminated and the intermediate aged faces between two discrete age groups can also be obtained. Towards this target, we employ a twolayer gated recurrent unit as the basic recurrent module whose bottom layer encodes a young face to a latent representation and the top layer decodes the representation to a corresponding older face. The experimental results demonstrate our proposed RFA provides better aging faces over other state-of-the-art age progression methods.	artificial neural network;codec;color gradient;encoder;hidden variable theory;optical flow;recurrent neural network;rollover (key)	Wei Wang;Zhen Cui;Yuqing Chen;Jiashi Feng;Shuicheng Yan;Xiangbo Shu;Nicu Sebe	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.261	computer vision;speech recognition;artificial intelligence	Vision	25.60072522484313	-50.699327607852474	180119
a2379e503a5e66c79b29a3732413a6759f105a38	mining 3d key-pose-motifs for action recognition		Recognizing an action from a sequence of 3D skeletal poses is a challenging task. First, different actors may perform the same action in various styles. Second, the estimated poses are sometimes inaccurate. These challenges can cause large variations between instances of the same class. Third, the datasets are usually small, with only a few actors performing few repetitions of each action. Hence training complex classifiers risks over-fitting the data. We address this task by mining a set of key-pose-motifs for each action class. A key-pose-motif contains a set of ordered poses, which are required to be close but not necessarily adjacent in the action sequences. The representation is robust to style variations. The key-pose-motifs are represented in terms of a dictionary using soft-quantization to deal with inaccuracies caused by quantization. We propose an efficient algorithm to mine key-pose-motifs taking into account of these probabilities. We classify a sequence by matching it to the motifs of each class and selecting the class that maximizes the matching score. This simple classifier obtains state-of the-art performance on two benchmark datasets.	algorithm;benchmark (computing);dictionary;ibm notes;motif;overfitting	Chun-yu Wang;Yizhou Wang;Alan L. Yuille	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.289	computer science;machine learning;pattern recognition;data mining	Vision	31.74382626985725	-47.835043565815816	180599
b5bc03b29235a1fdfe316b88628eafd303b3ddf3	multi-modal embedding for main product detection in fashion		We present an approach to detect the main product in fashion images by exploiting the textual metadata associated with each image. Our approach is based on a Convolutional Neural Network and learns a joint embedding of object proposals and textual metadata to predict the main product in the image. We additionally use several complementary classification and overlap losses in order to improve training stability and performance. Our tests on a large-scale dataset taken from eight e-commerce sites show that our approach outperforms strong baselines and is able to accurately detect the main product in a wide diversity of challenging fashion images.	artificial neural network;convolutional neural network;cross entropy;e-commerce;grammar-based code;minimum bounding box;modal logic;network architecture	LongLong Yu;Edgar Simo-Serra;Francesc Moreno-Noguer;Antonio Rubio	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.261	convolutional neural network;deep learning;metadata;pattern recognition;computer science;embedding;artificial intelligence	Vision	28.005125458008443	-50.62710828014797	180868
f187b0ed2224b2861442a73ad2966c1789afc09a	zero-shot learning via revealing data distribution		This paper presents a method of zero-shot learning (ZSL) which poses ZSL as the missing data problem, rather than the missing label problem. While most popular methods in ZSL focus on learning the mapping function from the image feature space to the label embedding space, the proposed method explores a simple yet effective transductive framework in the reverse mapping. Our method estimates data distribution of unseen classes in the image feature space by transferring knowledge from the label embedding space. It assumes that data of each seen and unseen class follow Gaussian distribution in the image feature space and utilizes Gaussian mixture model to model data. The signature is introduced to describe the data distribution of each class. In experiments, our method obtains 87.38% and 61.08% mean accuracies on the Animals with Attributes (AwA) and the Caltech-UCSD Birds-200-2011 (CUB) datasets respectively, which outperforms the runner-up methods significantly by 4.95% and 6.38%. In addition, we also investigate the extension of our method to open-set classification.	experiment;feature (computer vision);feature vector;missing data;mixture model;ucsd pascal/p-system	Bo Zhao;Botong Wu;Tianfu Wu;Yizhou Wang	2016	CoRR		machine learning;pattern recognition;data mining;mathematics;statistics	ML	24.80532003946055	-46.37320564947731	181300
fd70574919db642b85f260d63eef1341ca041682	feature combination and the knn framework in object classification	kernel;memory management;support vector machines;k nearest neighbors framework feature combination knn framework object classification multiple complementary features multiple kernel learning mkl optimization process average combination weighted average combination nearly sparse combination;learning systems;accuracy;weighted average combination dominant sets dsets feature combination k nearest neighbors knns multiple kernel learning mkl;optimisation feature extraction image classification learning artificial intelligence;optimization;kernel accuracy support vector machines learning systems optimization encoding memory management;encoding	In object classification, feature combination can usually be used to combine the strength of multiple complementary features and produce better classification results than any single one. While multiple kernel learning (MKL) is a popular approach to feature combination in object classification, it does not always perform well in practical applications. On one hand, the optimization process in MKL usually involves a huge consumption of computation and memory space. On the other hand, in some cases, MKL is found to perform no better than the baseline combination methods. This observation motivates us to investigate the underlying mechanism of feature combination with average combination and weighted average combination. As a result, we empirically find that in average combination, it is better to use a sample of the most powerful features instead of all, whereas in one type of weighted average combination, the best classification accuracy comes from a nearly sparse combination. We integrate these observations into the k-nearest neighbors (kNNs) framework, based on which we further discuss some issues related to sparse solution and MKL. Finally, by making use of the kNN framework, we present a new weighted average combination method, which is shown to perform better than MKL in both accuracy and efficiency in experiments. We believe that the work in this paper is helpful in exploring the mechanism underlying feature combination.	arabic numeral 0;baseline (configuration management);behavior;computation;dspace;experiment;k-nearest neighbors algorithm;kernel (operating system);learning disorders;math kernel library;mathematical optimization;multiple kernel learning;personnameuse - assigned;sensorineural hearing loss (disorder);sparse matrix;weight	Jian Hou;Huijun Gao;Qi Xia;Naiming Qi	2016	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2015.2461552	support vector machine;kernel;computer science;machine learning;pattern recognition;data mining;mathematics;accuracy and precision;encoding;statistics;memory management	Web+IR	24.79036854427355	-45.39630859059843	182359
9827ec586a00f8156df4033470c747caf54348e8	no matter where you are: flexible graph-guided multi-task learning for multi-view head pose classification under target motion	graph theory;head pose classification multi task learning multi view;video surveillance;pose classification flexible graph guided multitask learning multiview head pose classification target motion fega mtl field of view surveillance cameras facial appearance camera perspective grid partitions camera geometry;head cameras magnetic heads geometry training three dimensional displays computational modeling;motion estimation;image sensors;face recognition;multi task learning;learning artificial intelligence;multi view;head pose classification;video surveillance face recognition graph theory image sensors learning artificial intelligence motion estimation pose estimation;pose estimation	We propose a novel Multi-Task Learning framework (FEGA-MTL) for classifying the head pose of a person who moves freely in an environment monitored by multiple, large field-of-view surveillance cameras. As the target (person) moves, distortions in facial appearance owing to camera perspective and scale severely impede performance of traditional head pose classification methods. FEGA-MTL operates on a dense uniform spatial grid and learns appearance relationships across partitions as well as partition-specific appearance variations for a given head pose to build region-specific classifiers. Guided by two graphs which a-priori model appearance similarity among (i) grid partitions based on camera geometry and (ii) head pose classes, the learner efficiently clusters appearance wise related grid partitions to derive the optimal partitioning. For pose classification, upon determining the target's position using a person tracker, the appropriate region specific classifier is invoked. Experiments confirm that FEGA-MTL achieves state-of-the-art classification with few training data.	closed-circuit television;distortion;electromagnetically induced transparency;experiment;grid (spatial index);homology-derived secondary structure of proteins;monoidal t-norm logic;multi-task learning;space partitioning;tik	Yuqing Chen;Elisa Ricci;Subramanian Ramanathan;Oswald Lanz;Nicu Sebe	2013	2013 IEEE International Conference on Computer Vision	10.1109/ICCV.2013.150	facial recognition system;multi-task learning;computer vision;pose;3d pose estimation;computer science;graph theory;machine learning;pattern recognition;motion estimation;image sensor;articulated body pose estimation	Vision	30.417928402194796	-47.53715603893056	182792
82dd5dae92a27970cdac4af122bc1350927ba98c	dilated fcn for multi-agent 2d/3d medical image registration		2D/3D image registration to align a 3D volume and 2D X-ray images is a challenging problem due to its ill-posed nature and various artifacts presented in 2D X-ray images. In this paper, we propose a multi-agent system with an auto attention mechanism for robust and efficient 2D/3D image registration. Specifically, an individual agent is trained with dilated Fully Convolutional Network (FCN) to perform registration in a Markov Decision Process (MDP) by observing a local region, and the final action is then taken based on the proposals from multiple agents and weighted by their corresponding confidence levels. The contributions of this paper are threefold. First, we formulate 2D/3D registration as a MDP with observations, actions, and rewards properly defined with respect to X-ray imaging systems. Second, to handle various artifacts in 2D X-ray images, multiple local agents are employed efficiently via FCN-based structures, and an auto attention mechanism is proposed to favor the proposals from regions with more reliable visual cues. Third, a dilated FCN-based training mechanism is proposed to significantly reduce the Degree of Freedom in the simulation of registration environment, and drastically improve training efficiency by an order of magnitude compared to standard CNN-based training method. We demonstrate that the proposed method achieves high robustness on both spine cone beam Computed Tomography data with a low signal-to-noise ratio and data from minimally invasive spine surgery where severe image artifacts and occlusions are presented due to metal screws and guide wires, outperforming other state-of-the-art methods (single agent-based and optimization-based) by a large margin.	agent-based model;align (company);backpropagation;ct scan;computer vision;cone beam computed tomography;experiment;general computer corporation;image registration;markov chain;markov decision process;mathematical optimization;motion estimation;multi-agent system;optical flow;pixel;radiography;region of interest;signal-to-noise ratio;simulation;software propagation;teaching method;triplet state;visual artifact;visual descriptor;well-posed problem	Shun Miao;Sebastien Piat;Peter Walter Fischer;Ahmet Tuysuzoglu;Philip Walter Mewes;Tommaso Mansi;Rui Liao	2018			pattern recognition;artificial intelligence;robustness (computer science);sensory cue;cone beam computed tomography;degrees of freedom (statistics);computer science;guide wires;markov decision process;image registration;minimally invasive spine surgery	Vision	26.826883871598397	-51.10731840533464	182958
e9923b9b6f5731c38e67bed62d9b09f3d55f19e9	objects, relationships, and context in visual data		For decades, we are interested in detecting objects and classifying them into a fixed vocabulary of lexicon. With the maturity of these low-level vision solutions, we are hunger for a higher-level representation of the visual data, so as to extract visual knowledge rather than merely bags of visual entities, allowing machines to reason about human-level decision-making and even manipulate the visual data at the pixel-level. In this tutorial, we will introduce a various of machine learning techniques for modeling visual relationships (e.g., subject-predicate-object triplet detection) and contextual generative models (e.g., generating photo-realistic images using conditional generative adversarial networks). In particular, we plan to start from fundamental theories on object detection, relationship detection, generative adversarial networks, to more advanced topics on referring expression visual grounding, pose guided person image generation, and context based image inpainting.	capability maturity model;entity;generative adversarial networks;glossary of computer graphics;high- and low-level;inpainting;lexicon;machine learning;object detection;pixel;sensor;theory;triplet state;vocabulary	Hanwang Zhang;Qianru Sun	2018		10.1145/3206025.3210496	generative grammar;artificial intelligence;adversarial system;machine learning;inpainting;computer science;object detection;referring expression;lexicon;vocabulary	Vision	26.94814436857842	-50.63348705315757	183316
78569509e61269f5d2276b80f4fd41c22617ccc4	localization guided learning for pedestrian attribute recognition		Pedestrian attribute recognition has attracted many attentions due to its wide applications in scene understanding and person analysis from surveillance videos. Existing methods try to use additional pose, part or viewpoint information to complement the global feature representation for attribute classification. However, these methods face difficulties in localizing the areas corresponding to different attributes. To address this problem, we propose a novel Localization Guided Network which assigns attribute-specific weights to local features based on the affinity between proposals pre-extracted proposals and attribute locations. The advantage of our model is that our local features are learned automatically for each attribute and emphasized by the interaction with global features. We demonstrate the effectiveness of our Localization Guided Network on two pedestrian attribute benchmarks (PA-100K and RAP). Our result surpasses the previous state-of-the-art in all five metrics on both datasets.	affinity analysis;feature extraction;internationalization and localization;pose (computer vision);rapid refresh	Pengze Liu;Xihui Liu;Junjie Yan;Jing Shao	2018			pattern recognition;machine learning;artificial intelligence;computer science;pedestrian	AI	30.085559968494767	-51.60646664835944	183459
f9d171019bfeb71733fe36f7fae14f342ca9e51c	hough forests revisited: an approach to multiple instance tracking from multiple cameras		Tracking multiple objects in parallel is a difficult task, especially if instances are interacting and occluding each other. To alleviate the arising problems multiple camera views can be taken into account, which, however, increases the computational effort. Evoking the need for very efficient methods, often rather simple approaches such as background subtraction are applied, which tend to fail for more difficult scenarios. Thus, in this work, we introduce a powerful multi-instance tracking approach building on Hough Forests. By adequately refining the time consuming building blocks, we can drastically reduce their computational complexity without a significant loss in accuracy. In fact, we show that the test time can be reduced by one to two orders of magnitude, allowing to efficiently process the large amount of image data coming from multiple cameras. Furthermore, we adapt the pre-trained generic forest model in an online manner to train an instance-specific model, making it well suited for multi-instance tracking. Our experimental evaluations show the effectiveness of the proposed efficient Hough Forests for object detection as well as for the actual task of multi-camera tracking.	background subtraction;computation;computational complexity theory;computer vision;hough transform;interaction;match moving;microsoft outlook for mac;object detection	Georg Poier;Samuel Schulter;Sabine Sternig;Peter M. Roth;Horst Bischof	2014		10.1007/978-3-319-11752-2_41	mathematical optimization;discrete mathematics;theoretical computer science;mathematics	Vision	31.18815123975046	-48.930036542245944	183890
040d39a42a0ec0f7307bfeb88c7a6a6695535e8e	unsupervised deep features for remote sensing image matching via discriminator network		The advent of deep perceptual networks brought about a paradigm shift in machine vision and image perception. Image apprehension lately carried out by hand-crafted features in the latent space have been replaced by deep features acquired from supervised networks for improved understanding. However, such deep networks require strict supervision with a substantial amount of the labeled data for authentic training process. These methods perform poorly in domains lacking labeled data especially in case of remote sensing image retrieval. Resolving this, we propose an unsupervised encoder-decoder feature for remote sensing image matching (RSIM). Moreover, we replace the conventional distance metrics with a deep discriminator network to identify the similarity of the image pairs. To the best of our knowledge, discriminator network has never been used before for solving RSIM problem. Results have been validated with two publicly available benchmark remote sensing image datasets. The technique has also been investigated for content-based remote sensing image retrieval (CBRSIR); one of the widely used applications of RSIM. Results demonstrate that our technique supersedes the state-of-the-art methods used for unsupervised image matching with mean average precision (mAP) of 81%, and image retrieval with an overall improvement in mAP score of about 12%.	autoencoder;benchmark (computing);content-based image retrieval;discriminator;encoder;image registration;image retrieval;information retrieval;iterative method;machine vision;programming paradigm;relevance feedback;unsupervised learning;visual descriptor	Geanna Capitan;Hao-Jun Jia;Murtaza Taj	2018	CoRR		labeled data;pattern recognition;artificial intelligence;remote sensing;computer science;discriminator;machine vision;image retrieval	Vision	28.117331736833037	-50.13608381467884	184130
7587ddc0dcefe0c406aad2a92de3c0d7e5f3f46d	brain inspired cognitive model with attention for self-driving cars		Perception-driven approach and end-to-end system are two major vision-based frameworks for self-driving cars. However, it is difficult to introduce attention and historical information of autonomous driving process, which are the essential factors for achieving human-like driving into these two methods. In this paper, we propose a novel model for self-driving cars named brain-inspired cognitive model with attention (CMA). This model consists of three parts: a convolutional neural network for simulating human visual cortex, a cognitive map built to describe relationships between objects in complex traffic scene and a recurrent neural network that combines with the realtime updated cognitive map to implement attention mechanism and long-short term memory. The benefit of our model is that can accurately solve three tasks simultaneously: i) detection of the free space and boundaries of the current and adjacent lanes. ii)estimation of obstacle distance and vehicle attitude, and iii) learning of driving behavior and decision making from human driver. More significantly, the proposed model could accept external navigating instructions during an end-to-end driving process. For evaluation, we build a large-scale roadvehicle dataset which contains more than forty thousand labeled road images captured by three cameras on our self-driving car. Moreover, human driving activities and vehicle states are recorded in the meanwhile.	artificial neural network;autonomous car;cma-es;cognition;cognitive map;cognitive model;convolutional neural network;end system;end-to-end principle;non-volatile memory;recurrent neural network;simulation	Shi-tao Chen;Songyi Zhang;Jinghao Shang;Badong Chen;Nanning Zheng	2017	CoRR		computer vision;simulation;artificial intelligence;machine learning	AI	27.707238575043178	-51.328466782740406	184311
29c721f628803b54bbc0eacd27e60d94772cc2e4	learning hierarchical semantic description via mixed-norm regularization for image understanding	optimisation;electronic mail;semantics;statistical learning image representation large scale systems pattern analysis semantic web;visualization semantics image representation dictionaries electronic mail laboratories information processing;visualization;statistical learning;image representation;optimisation image representation learning artificial intelligence;dictionaries;information processing;image reranking hierarchical semantic description learning mixed norm regularization vicept representation visual polysemia large scale semantic image understanding membership probability distribution visual appearances image representation group sparse coding weighted sum dictionary elements image level discriminative vicept descriptions structural sparsity optimization problem concept membership distribution vicept distance multilevel separability analysis image search image annotation;semantic web;pattern analysis;learning artificial intelligence;large scale systems	This paper proposes a new perspective-Vicept representation to solve the problem of visual polysemia and concept polymorphism in the large-scale semantic image understanding. Vicept characterizes the membership probability distribution between visual appearances and semantic concepts, and forms a hierarchical representation of image semantic from local to global. In the implementation, incorporating group sparse coding, visual appearance is encoded as a weighted sum of dictionary elements, which could obtain more accurate image representation with sparsity at the image level. To obtain discriminative Vicept descriptions with structural sparsity, mixed-norm regularization is adopted in the optimization problem for learning the concept membership distribution of visual appearance. Furthermore, we introduce a novel image distance measurement based on the hierarchical Vicept description, where different levels of Vicept distance are fused together by multi-level separability analysis. Finally, the wide applications of Vicept description are validated in our experiments, including large-scale semantic image search, image annotation, and semantic image re-ranking.	a picture is worth a thousand words;algorithm;automatic image annotation;computation;computer vision;dictionary;experiment;image retrieval;linear separability;mathematical optimization;matrix regularization;neural coding;optimization problem;scalability;sparse matrix;test set;web application;weight function	Liang Li;Shuqiang Jiang;Qingming Huang	2012	IEEE Transactions on Multimedia	10.1109/TMM.2012.2194993	computer vision;visualization;information processing;computer science;machine learning;semantic web;pattern recognition;mathematics;semantics	Vision	26.16494866751798	-46.066698029980145	184804
698162ea361b5149b1aaaf87faa59695b08153ea	detection of co-salient objects by looking deep and wide	bayesian framework;co saliency detection;domain adaptive convolutional neural network	In this paper, we propose a unified co-salient object detection framework by introducing two novel insights: (1) looking deep to transfer higher-level representations by using the convolutional neural network with additional adaptive layers could better reflect the sematic properties of the co-salient objects; (2) looking wide to take advantage of the visually similar neighbors from other image groups could effectively suppress the influence of the common background regions. The wide and deep information are explored for the object proposal windows extracted in each image. The window-level co-saliency scores are calculated by integrating the intra-image contrast, the intra-group consistency, and the inter-group separability via a principled Bayesian formulation and are then converted to the superpixel-level co-saliency maps through a foreground region agreement strategy. Comprehensive experiments on two existing and one newly established datasets have demonstrated the consistent performance gain of the proposed approach.	algorithm;artificial neural network;convolutional neural network;experiment;linear separability;map;microsoft windows;object detection;restricted boltzmann machine;supervised learning;video processing	Dingwen Zhang;Junwei Han;Chao Li;Jingdong Wang;Xuelong Li	2016	International Journal of Computer Vision	10.1007/s11263-016-0907-4	computer vision;computer science;machine learning;pattern recognition	Vision	29.02788772661362	-51.148229565193134	184916
144d19b3f820c96b7816842356dadebf16678a8e	ranus: rgb and nir urban scene dataset for deep scene parsing		In this letter, we present a data-driven method for scene parsing of road scenes to utilize single-channel near-infrared (NIR) images. To overcome the lack of data problem in non-RGB spectrum, we define a new color space and decompose the task of deep scene parsing into two subtasks with two separate CNN architectures for chromaticity channels and semantic masks. For chromaticity estimation, we build a spatially-aligned RGB-NIR image database (40k urban scenes) to infer color information from RGB-NIR spectrum learning process and leverage existing scene parsing networks trained over already available RGB masks. From our database, we sample key frames and manually annotate them (4k ground truth masks) to finetune the network into the proposed color space. Hence, the key contribution of this work is to replace multispectral scene parsing methods with a simple yet effective approach using single NIR images. The benefits of using our algorithm and dataset are confirmed in the qualitative and quantitative experiments.	algorithm;color space;experiment;ground truth;key frame;multispectral image;parsing;pixel	Gyeongmin Choe;Seong-Heum Kim;Sunghoon Im;Joon-Young Lee;Srinivasa G. Narasimhan;I. Kweon	2018	IEEE Robotics and Automation Letters	10.1109/LRA.2018.2801390	chromaticity;computer vision;parsing;multispectral image;rgb color model;image segmentation;control engineering;engineering;ground truth;color space;artificial intelligence	Vision	27.737966644132584	-51.28459092023016	185061
314d24aa3c611fa8311b71dd2597e28e3f606bf9	contactless and partial 3d fingerprint recognition using multi-view deep representation		Contactless 3D fingerprint identification has gained significant attentions in recent years as it can offer more hygienic, accurate and ubiquitous personal identification. Despite such advantages, contactless 3D imaging often results in partial 3D fingerprints as it requires relatively higher cooperation from users during the contactless 3D imaging. Such contactless 3D fingerprint images significantly degrade matching accuracy due to partial 3D fingerprint imaging. This paper proposes an end-to-end contactless 3D fingerprint representation learning model based on convolutional neural network (CNN). The proposed model includes one fully convolutional network for fingerprint segmentation and three Siamese networks to learn multi-view 3D fingerprint feature representation. Contactless partial 3D fingerprint identification is a more challenging problem due to its high degree of freedom during contactless 3D fingerprint acquisition and is also addressed by using proposed model. We therefore investigate multi-view 3D fingerprint recognition and partial 3D fingerprint using proposed approach. Comparative experimental results, presented in this paper using state-of-the-art 3D fingerprint recognition method, demonstrate the effectiveness of the proposed multiview approach and illustrate a significant improvement of state-of-the-art 3D fingerprint recognition methods.	3d reconstruction;artificial neural network;contactless smart card;convolutional neural network;database;end-to-end principle;feature learning;fingerprint recognition;free viewpoint television;machine learning;minutiae;performance evaluation;software deployment;stereoscopy;test set;time complexity	Chenhao Lin;Ajay Kumar	2018	Pattern Recognition	10.1016/j.patcog.2018.05.004	convolutional neural network;fingerprint;fingerprint recognition;mathematics;pattern recognition;artificial intelligence;feature learning	Vision	29.44944544258884	-50.78178998283612	185381
21445604cc6b41f2242c6a55a5c1ed559b3f55cd	temporal residual networks for dynamic scene recognition		This paper combines three contributions to establish a new state-of-the-art in dynamic scene recognition. First, we present a novel ConvNet architecture based on temporal residual units that is fully convolutional in spacetime. Our model augments spatial ResNets with convolutions across time to hierarchically add temporal residuals as the depth of the network increases. Second, existing approaches to video-based recognition are categorized and a baseline of seven previously top performing algorithms is selected for comparative evaluation on dynamic scenes. Third, we introduce a new and challenging video database of dynamic scenes that more than doubles the size of those previously available. This dataset is explicitly split into two subsets of equal size that contain videos with and without camera motion to allow for systematic study of how this variable interacts with the defining dynamics of the scene per se. Our evaluations verify the particular strengths and weaknesses of the baseline algorithms with respect to various scene classes and camera motion parameters. Finally, our temporal ResNet boosts recognition performance and establishes a new state-of-the-art on dynamic scene recognition, as well as on the complementary task of action recognition.	algorithm;baseline (configuration management);categorization;convolution;convolutional neural network;cross-validation (statistics);futures studies;increment and decrement operators;spatial network	Christoph Feichtenhofer;Axel Pinz;Richard P. Wildes	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.786	spacetime;kernel (linear algebra);computer vision;residual;computer science;artificial intelligence;pattern recognition;architecture;residual neural network;strengths and weaknesses;convolution	Vision	30.76232855077902	-50.89851504630661	185392
6b4da897dce4d6636670a83b64612f16b7487637	learning from simulated and unsupervised images through adversarial training		With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.	3d pose estimation;algorithm;autostereogram;cobham's thesis;discriminator;generative adversarial networks;graphics;image;simulation;synthetic data;synthetic intelligence;unsupervised learning;usability testing	Ashish Shrivastava;Tomas Pfister;Oncel Tuzel;Josh Susskind;Wenda Wang;Russell Webb	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.241	pattern recognition;artificial intelligence;machine learning;computer vision;real image;adversarial system;artificial neural network;discriminator;computer science;data modeling;pose;annotation	Vision	26.653338664351352	-49.85263626656141	185782
6f64358855d51d97769c03df17002d5ad3321aeb	dynamic dictionary optimization for sparse-representation-based face classification using local difference images		Abstract In this study, we present a new sparse-representation-based face-classification algorithm that exploits dynamic dictionary optimization on an extended dictionary using synthesized faces. More specifically, given a dictionary consisting of face examples, we first augment the dictionary with a set of virtual faces generated by calculating the image difference of a pair of faces. This results in an extended dictionary with hybrid training samples, which enhances the capacity of the dictionary to represent new samples. Second, to reduce the redundancy of the extended dictionary and improve the classification accuracy, we use a dictionary-optimization method. We truncate the extended dictionary with a more compact structure by discarding the original samples with small contributions to represent a test sample. Finally, we perform sparse-representation-based face classification using the optimized dictionary. Experimental results obtained using the AR and FERRET face datasets demonstrate the superiority of the proposed method in terms of accuracy, especially for small-sample-size problems.	algorithm;ar (unix);dictionary;feret (facial recognition technology);information processing;intelligent control;mathematical optimization;sparse approximation;sparse matrix;truncation	Chang-Bin Shao;Xiaoning Song;Zhen-Hua Feng;Xiaojun Wu;Yuhui Zheng	2017	Inf. Sci.	10.1016/j.ins.2017.02.017	redundancy (engineering);truncate;machine learning;k-svd;sparse approximation;artificial intelligence;computer science;pattern recognition	Vision	27.916471351823237	-45.320674255248726	186155
ea8805ce575f45630e02ac3760643db19d2e37b7	hierarchical multimodal metric learning for multimodal classification		Multimodal classification arises in many computer vision tasks such as object classification and image retrieval. The idea is to utilize multiple sources (modalities) measuring the same instance to improve the overall performance compared to using a single source (modality). The varying characteristics exhibited by multiple modalities make it necessary to simultaneously learn the corresponding metrics. In this paper, we propose a multiple metrics learning algorithm for multimodal data. Metric of each modality is a product of two matrices: one matrix is modality specific, the other is enforced to be shared by all the modalities. The learned metrics can improve multimodal classification accuracy and experimental results on four datasets show that the proposed algorithm outperforms existing learning algorithms based on multiple metrics as well as other approaches tested on these datasets. Specifically, we report 95.0% object instance recognition accuracy, 89.2% object category recognition accuracy on the multi-view RGB-D dataset and 52.3% scene category recognition accuracy on SUN RGB-D dataset.	algorithm;computer vision;experiment;feature learning;image retrieval;machine learning;modal logic;modality (human–computer interaction);multimodal interaction	Heng Zhang;Vishal M. Patel;Rama Chellappa	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.312	modalities;pattern recognition;computer vision;artificial intelligence;support vector machine;feature extraction;computer science;image retrieval;machine learning;rgb color model	Vision	26.714107620042746	-46.740224854792075	186572
63b7a821f11e6ad466be5bb1a3f18549d68a9cb2	a cross-modal distillation network for person re-identification in rgb-depth		Person re-identification involves the recognition over time of individuals captured using multiple distributed sensors. With the advent of powerful deep learning methods able to learn discriminant representations for visual recognition, cross-modal person re-identification based on different sensor modalities has become viable in many challenging applications in, e.g., autonomous driving, robotics and video surveillance. Although some methods have been proposed for re-identification between infrared and RGB images, few address depth and RGB images. In addition to the challenges for each modality associated with occlusion, clutter, misalignment, and variations in pose and illumination, there is a considerable shift across modalities since data from RGB and depth images are heterogeneous. In this paper, a new cross-modal distillation network is proposed for robust person re-identification between RGB and depth sensors. Using a two-step optimization process, the proposed method transfers supervision between modalities such that similar structural features are extracted from both RGB and depth modalities, yielding a discriminative mapping to a common feature space. Our experiments investigate the influence of the dimensionality of the embedding space, compares transfer learning from depth to RGB and vice versa, and compares against other state-of-the-art cross-modal re-identification methods. Results obtained with BIWI and RobotPKU datasets indicate that the proposed method can successfully transfer descriptive structural features from the depth modality to the RGB modality. It can significantly outperform state-of-the-art conventional methods and deep neural networks for cross-modal sensing between RGB and depth, with no impact on computational complexity.	artificial neural network;autonomous car;autonomous robot;closed-circuit television;clutter;computational complexity theory;de-identification;deep learning;discriminant;experiment;feature vector;illumination (image);map;mathematical optimization;modal logic;modality (human–computer interaction);radar;robotics;sensor;sparse matrix;test set	Marc Comas‐Cufí;Amran Bhuiyan;Julian F. P. Kooij;Eric Granger	2018	CoRR		discriminative model;modalities;feature vector;pattern recognition;artificial neural network;deep learning;rgb color model;computer science;versa;curse of dimensionality;artificial intelligence	AI	29.426242233353978	-50.22765509140757	187001
915988f35ce07fdb82fe32673e94bdc237fbcd43	deep cross-domain flying object classification for robust uav detection		Recent progress in the development of unmanned aerial vehicles (UAVs) causes serious safety issues for mass events and safety-sensitive locations like prisons or airports. To address these concerns, robust UAV detection systems are required. In this work, we propose an UAV detection framework based on video images. Depending on whether the video images are recorded by static cameras or moving cameras, we initially detect regions that are likely to contain an object by median background subtraction or a deep learning based object proposal method, respectively. Then, the detected regions are classified into UAV or distractors, such as birds, by applying a convolutional neural network (CNN) classifier. To train this classifier, we use our own dataset comprised of crawled and self-acquired drone images, as well as bird images from a publicly available dataset. We show that, even across a significant domain gap, the resulting classifier can successfully identify UAVs in our target dataset. We evaluate our UAV detection framework on six challenging video sequences that contain UAVs at different distances as well as birds and background motion.		Arne Schumann;Lars Wilko Sommer;Johannes Klatte;Tobias Schuchert;Jürgen Beyerer	2017	2017 14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)	10.1109/AVSS.2017.8078558	convolutional neural network;computer vision;robustness (computer science);deep learning;pattern recognition;computer science;artificial intelligence;background subtraction;image resolution	Vision	31.305354044011036	-51.94079186498708	187417
5a55b8262229a3e95ed9824bd5d9ecf6aead3a0c	zero-shot deep domain adaptation		Domain adaptation is an important tool to transfer knowledge about a task (e.g. classification) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from taskirrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classifier for classification tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, FashionMNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classification tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classification task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the first domain adaptation and sensor fusion method which requires no taskrelevant target-domain data. The underlying principle is not particular to computer vision data, but should be extensible to other domains.	computer vision;computer-aided design;domain adaptation;experiment;loss function;mnist database;rgd;relevance;simulation;statistical classification	Kuan-Chuan Peng;Ziyan Wu;Jan Ernst	2018		10.1007/978-3-030-01252-6_47	pattern recognition;artificial intelligence;machine learning;computer science;nist;domain adaptation;mnist database;rgb color model;sensor fusion;extensibility;training set	Vision	25.15586356637865	-47.93903869185743	187496
65ef33636f07d4d1aa1b22a5b67f1f402d6a5900	partbook for image parsing	detectors;object recognition;pattern clustering;support vector machines;image matching;training;image classification;heating;support vector machines image classification image matching image representation object recognition pattern clustering;visualization;vectors;image representation;optimization;detectors support vector machines heating optimization vectors training visualization;object identification partbook image parsing selective representation invariant representation intra class variation inter class variation codebook bag of visual words representation part based models manual object level labeling intra class invariance inter class selectivity heat map like representations svm classifier region clustering dense matching based similarity part detector mid level patterns	Effective image parsing needs a representation that is both selective (to inter-class variations) and invariant (to intra-class variations). CodeBook from bag-of-visual-words representation addresses the invariance, and part-based models can potentially address the selectivity. However, existing part-based approaches either require expensive manual object-level labeling or make strong assumptions not applicable to real-world images. In this paper, we propose a PartBook approach that simultaneously overcomes the above two difficulties. Furthermore, we present an effective framework that integrates CodeBook and PartBook, which achieves both intra-class invariance and inter-class selectivity. Specifically, a set of candidate regions are first selected from heat map-like representations obtained by a SVM classifier trained for each category. Then the regions are clustered based on the dense matching-based similarity, and a part detector is learned from each cluster and further refined by utilizing a latent SVM. The learned PartBook summarizes the most representative mid-level patterns of each category, and can be readily used for image parsing tasks to identify not only objects but also different parts of an object. Extensive experimental results on real-world images show that the automatically learned parts are semantically meaningful, and demonstrate the effectiveness of ParkBook in image parsing tasks at different levels.	algorithm;bag-of-words model in computer vision;benchmark (computing);bottom-up parsing;codebook;heat map;image processing;part-based models;selectivity (electronic);top-down and bottom-up design	Kuiyuan Yang;Lei Zhang;Yong Rui;HongJiang Zhang	2012	2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops	10.1109/CVPRW.2012.6239169	support vector machine;computer vision;detector;contextual image classification;visualization;u-matrix;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;mathematics	Vision	29.71427860456336	-50.77728459732757	187551
22fec74ec6e11d4bc61eef5c8f3cac10bd23c856	auxiliary image regularization for deep cnns with noisy labels	paper;neural networks;computer vision;cuda;deep learning;nvidia;computer science;cnn	Precisely-labeled data sets with sufficient amount of samples are very important for training deep convolutional neural networks (CNNs). However, many of the available real-world data sets contain erroneously labeled samples and those errors substantially hinder the learning of very accurate CNN models. In this work, we consider the problem of training a deep CNN model for image classification with mislabeled training samples – an issue that is common in real image data sets with tags supplied by amateur users. To solve this problem, we propose an auxiliary image regularization technique, optimized by the stochastic Alternating Direction Method of Multipliers (ADMM) algorithm, that automatically exploits the mutual context information among training images and encourages the model to select reliable images to robustify the learning process. Comprehensive experiments on benchmark data sets clearly demonstrate our proposed regularized CNN model is resistant to label noise in training data.	algorithm;artificial neural network;augmented lagrangian method;benchmark (computing);computer vision;convolutional neural network;experiment;matrix regularization;robustification;synthetic intelligence	Samaneh Azadi;Jiashi Feng;Stefanie Jegelka;Trevor Darrell	2015	CoRR		computer vision;computer science;artificial intelligence;machine learning;pattern recognition;deep learning;artificial neural network	ML	26.96462806046982	-48.9852251892333	187765
4ca0b6f185f92ce90ac79ae70ca3085fe495b1d9	a parameter partial-sharing cnn architecture for cross-domain clothing retrieval	triplet network;cross domain;parameter partial sharing;siamese network;clothing retrieval	Cross-domain clothing retrieval is a challenging task due to significant differences between online shop images taken in controlled conditions of clean backgrounds, good lighting, and fixed poses, and street photos captured in uncontrollable conditions. In recent years, Convolutional Neural Networks (CNNs) have demonstrated its effectiveness for various computer vision problems including image retrieval. There are two mainstream CNNs based models addressing image retrieval tasks: triplet network models [1] and siamese network models [2]. In this paper, we first make a thorough comparison between the two types of models, and investigate the impact of different domain adaptation schemes including parameter sharing, non-sharing, and a new partial-sharing strategy between the street domain and the shop domain. Extensive experiments have revealed that the proposed partial-sharing scheme is able to reduce the number of parameters by a significant margin, while achieving comparable retrieval accuracy as the state-of-the-art scheme using triplet loss with non-sharing parameters.	computer vision;convolutional neural network;domain adaptation;experiment;image registration;image retrieval;network model;online shopping;triplet state	Yichao Xiong;Ning Liu;Zhe Xu;Ya Zhang	2016	2016 Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2016.7805463	computer vision;simulation;telecommunications;computer science;artificial intelligence;machine learning;algorithm	Vision	29.477850813929386	-50.86734984198488	187850
b45e2580f40059205f2ba018428f6b8f33f44436	scene recognition and object detection in a unified convolutional neural network on a mobile manipulator		Environment understanding, object detection and recognition are crucial skills for robots operating in the real world. In this paper, we propose a Convolutional Neural Network with multi-task objectives: object detection and scene classification in one unified architecture. The proposed network reasons globally about an image to understand the scene, hypothesize object locations, and encodes global scene features with regional object features to improve object recognition. We evaluate our network on the standard SUN RGBD dataset. Experiments show that our approach outperforms state-of-the-arts. Network predictions are further transformed into continuous robot beliefs to ensure temporal coherence and extended to 3D space for robotics applications. We embed the whole framework in Robot Operating System, and evaluate its performance on a real robot for semantic mapping and grasp detection.	algorithm;cloud computing;coherence (physics);computation;computer multitasking;convolutional neural network;dr-dos;mobile manipulator;object detection;outline of object recognition;point cloud;robot operating system;robotics;semantic mapper	Hao Sun;Zehui Meng;Pey Yuen Tao;Marcelo H. Ang	2018	2018 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2018.8460535	semantic mapping;convolutional neural network;computer vision;architecture;object detection;mobile manipulator;feature extraction;control engineering;engineering;grasp;artificial intelligence;robotics	Robotics	29.57641025257283	-49.45113116120812	188014
86204fc037936754813b91898377e8831396551a	dense face alignment		Face alignment is a classic problem in the computer vision field. Previous works mostly focus on sparse alignment with a limited number of facial landmark points, i.e., facial landmark detection. In this paper, for the first time, we aim at providing a very dense 3D alignment for large-pose face images. To achieve this, we train a CNN to estimate the 3D face shape, which not only aligns limited facial landmarks but also fits face contours and SIFT feature points. Moreover, we also address the bottleneck of training CNN with multiple datasets, due to different landmark markups on different datasets, such as 5, 34, 68. Experimental results show our method not only provides high-quality, dense 3D face fitting but also outperforms the state-of-the-art facial landmark detection methods on challenging datasets. Our model can run at real time during testing and it's available at http:///cvlab.cse.msu.edu/project-pifa.html.	align (company);artificial neural network;computer vision;deep learning;fits;facial recognition system;landmark point;manifold alignment;scale-invariant feature transform;sparse matrix	Yaojie Liu;Amin Jourabloo;William Ren;Xiaoming Liu	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.190	computer vision;artificial intelligence;pattern recognition;bottleneck;solid modeling;computer science;scale-invariant feature transform	Vision	30.214095559865253	-49.65150907141967	188196
079f1c028f76e810101bfee2629d06d755eae62b	anytime recognition of objects and scenes	anytime;visual perception computer vision feature extraction image classification object recognition;human visual perception object recognition scene recognition visual recognition deployments anytime recognition ability computer vision research visual architectures feature computation;computer vision;budgeted classification;recognition;budgeted classification visual recognition anytime;computer science;visual recognition;vectors training hafnium visualization feature extraction logistics computer vision	Humans are capable of perceiving a scene at a glance, and obtain deeper understanding with additional time. Similarly, visual recognition deployments should be robust to varying computational budgets. Such situations require Anytime recognition ability, which is rarely considered in computer vision research. We present a method for learning dynamic policies to optimize Anytime performance in visual architectures. Our model sequentially orders feature computation and performs subsequent classification. Crucially, decisions are made at test time and depend on observed data and intermediate results. We show the applicability of this system to standard problems in scene and object recognition. On suitable datasets, we can incorporate a semantic back-off strategy that gives maximally specific predictions for a desired level of accuracy, this provides a new view on the time course of human visual perception.	anytime algorithm;artificial neural network;cognition;computation;computer vision;convolutional neural network;experiment;feature selection;humans;ibm notes;markov chain;markov decision process;outline of object recognition;software deployment;visual computing	Sergey Karayev	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.80	computer vision;feature;intelligent character recognition;computer science;artificial intelligence;machine learning;3d single-object recognition	Vision	27.819831760128533	-48.388875139528686	188753
b570e441357e49f5518743d4e82df25ac5a394f5	evolvement constrained adversarial learning for video style transfer		Video style transfer is a useful component for applications such as augmented reality, non-photorealistic rendering, and interactive games. Many existing methods use optical flow to preserve the temporal smoothness of the synthesized video. However, the estimation of optical flow is sensitive to occlusions and rapid motions. Thus, in this work, we introduce a novel evolve-sync loss computed by evolvements to replace optical flow. Using this evolve-sync loss, we build an adversarial learning framework, termed as Video Style Transfer Generative Adversarial Network (VST-GAN), which improves upon the MGAN method for image style transfer for more efficient video style transfer. We perform extensive experimental evaluations of our method and show quantitative and qualitative improvements over the state-of-the-art methods.	augmented reality;non-photorealistic rendering;optical flow;time complexity;unbiased rendering	Wenbo Li;Longyin Wen;Xiao Bian;Siwei Lyu	2018	CoRR			Vision	26.44643964246256	-50.64581433174308	188890
aff45718297e1af95d1e80720d52b90f8cc92ce8	captcha image generation systems using generative adversarial networks			captcha;generative adversarial networks	Hyun Kwon;Yongchul Kim;Hyunsoo Yoon;Daeseon Choi	2018	IEICE Transactions		generative grammar;adversarial system;computer vision;computer science;machine learning;captcha;artificial intelligence	Vision	24.68831461819976	-49.764355838898005	189479
c2a7175c3031e4b38a7a51d40d9e12bbc59a94a6	convolutional neural networks for movement prediction in videos		In this work we present a convolutional neural network-based (CNN) model that predicts future movements of a ball given a series of images depicting the ball and its environment. For training and evaluation, we use artificially generated images sequences. Two scenarios are analyzed: Prediction in a simple table tennis environment and a more challenging squash environment. Classical 2D convolution layers are compared with 3D convolution layers that extract the motion information of the ball from contiguous frames. Moreover, we investigate whether networks with stereo visual input perform better than those with monocular vision only. Our experiments suggest that CNNs can indeed predict physical behaviour with small error rates on unseen data but the performance drops for very complex underlying movements.	artificial neural network;convolution;convolutional neural network;experiment;image resolution;network architecture	Alexander Warnecke;Timo Lüddecke;Florentin Wörgötter	2017		10.1007/978-3-319-66709-6_18	convolutional neural network;deep learning;monocular vision;convolution;physical behaviour;pattern recognition;computer science;artificial intelligence	Vision	27.47409748064138	-51.44176026219004	189494
2bc76e536ef3b3a90666ec6e5508691c79431d31	hallucination from noon to night images using cnn	adversarial network;deep learning;color transfer	Given that the deep neural networks (DNNs) can now achieve almost human level performance in the object classification task, a question arises whether DNNs can execute meaningful color transforms of images. The specific goal of our color transfer is to imitate time transfer from noon to night on an image. We eliminate the demand for training pairs of noon and night images by adopting adversarial network [Goodfellow et al. 2014].	almost human;artificial neural network;color mapping	Junyong Lee;Seungyong Lee	2016		10.1145/3005274.3005320	computer vision;computer science;deep learning	Vision	25.711286687812027	-50.02483089720679	190773
ae97076afe9ac710bca3f2adbdf1cca6dbe4d8fe	learning spatiotemporal and geometric features with isa for video-based facial expression recognition		Many appearance-based and geometry-based approaches have been proposed in facial expression recognition. In this paper, we propose a method of learning and combining spatiotemporal features and geometric features for video-based expression recognition. Specifically, we first adopt a multi-layer independent subspace analysis (ISA) network to learn spatiotemporal features directly from videos, and then use another single layer ISA network to learn geometric features from the trajectories of the facial landmark points. The learned spatiotemporal features and geometric features are concatenated to be the final representation for the input video. We use a linear SVM in classification. Experiments on CK+ and MMI facial expression databases show that recognition performance can be improved effectively by incorporating geometric features into spatiotemporal features. Furthermore, comparison results with other related methods demonstrate that the overall accuracy of our method is comparable to some deep learning based methods and the learned features outperform popular hand-crafted features.		ChenHan Lin;Fei Long;Junfeng Yao;Ming-Ting Sun;Jinsong Su	2017		10.1007/978-3-319-70090-8_45	support vector machine;machine learning;deep learning;artificial intelligence;concatenation;pattern recognition;computer science;facial expression;subspace topology	Vision	28.788909698468657	-51.368330036844185	191052
b5ff7f7f4ecd3eb11e05b52108fbf2be50083581	unsupervised natural image patch learning		Learning a metric of natural image patches is an important tool for analyzing images. An efficient means is to train a deep network to map an image patch to a vector space, in which the Euclidean distance reflects patch similarity. Previous attempts learned such an embedding in a supervised manner, requiring the availability of many annotated images. In this paper, we present an unsupervised embedding of natural image patches, avoiding the need for annotated images. The key idea is that the similarity of two patches can be learned from the prevalence of their spatial proximity in natural images. Clearly, relying on this simple principle, many spatially nearby pairs are outliers, however, as we show, the outliers do not harm the convergence of the metric learning. We show that our unsupervised embedding approach is more effective than a supervised one or one that uses deep patch representations. Moreover, we show that it naturally leads itself to an efficient self-supervised domain adaptation technique onto a target domain that contains a common foreground object.	code;domain adaptation;embedded system;euclidean distance;patch (computing);refinement (computing);rough set;supervised learning;triplet state;unsupervised learning	Dov Danon;Hadar Averbuch-Elor;Ohad Fried;Daniel Cohen-Or	2018	CoRR		euclidean distance;pattern recognition;computer science;machine learning;artificial intelligence;domain adaptation;outlier;vector space;convergence (routing);embedding	ML	25.64179437761093	-47.557531977475946	191055
062723093e77d942471a77f1a89614d05216a275	dave: a unified framework for fast vehicle detection and annotation		Vehicle detection and annotation for streaming video data with complex scenes is an interesting but challenging task for urban traffic surveillance. In this paper, we present a fast framework of Detection and Annotation for Vehicles (DAVE), which effectively combines vehicle detection and attributes annotation. DAVE consists of two convolutional neural networks (CNNs): a fast vehicle proposal network (FVPN) for vehicle-like objects extraction and an attributes learning network (ALN) aiming to verify each proposal and infer each vehicle’s pose, color and type simultaneously. These two nets are jointly optimized so that abundant latent knowledge learned from the ALN can be exploited to guide FVPN training. Once the system is trained, it can achieve efficient vehicle detection and annotation for real-world traffic surveillance data. We evaluate DAVE on a new self-collected UTS dataset and the public PASCAL VOC2007 car and LISA 2010 datasets, with consistent improvements over existing algorithms.	algorithm;apple lisa;artificial neural network;convolutional neural network;streaming media;uts	Yi Zhou;Li Liu;Ling Shao;Matt Mellor	2016		10.1007/978-3-319-46475-6_18	computer vision;simulation;computer science;machine learning;data mining	Vision	28.53639730659269	-51.51231501586983	191223
97f3d35d3567cd3d973c4c435cdd6832461b7c3c	unleash the black magic in age: a multi-task deep neural network approach for cross-age face verification		Facial aging is a complicated process which usually affects the facial appearance (e.g., wrinkles). Variations of facial appearance pose a big challenge to the automatic face recognition problem. How to eliminate the influence of aging factors to the verification performance is a very challenging problem. Multi-task learning has provided a principled framework for jointly learning multiple related tasks to improve generalization performance. In this paper, we leverage this powerful technique to improve the task of cross-age face verification. We present an end-to-end learning framework for cross-age face verification by designing a multi-task deep neural network architecture that exploits the intrinsic low-dimensional representation shared between the tasks of face verification and age estimation. We show that the algorithm effectively balances feature sharing and feature exclusion between the two given tasks. We evaluate the proposed framework on two standard benchmarks. Experimental results demonstrate that our algorithm has significant improvement over the state-of-theart (2.2% EER on MORPH and 7.8% EER on FG-NET, by more than 50.0% and 59.70% performance gain respectively).	algorithm;artificial neural network;benchmark (computing);computer multitasking;deep learning;end-to-end principle;enhanced entity–relationship model;facial electromyography;facial recognition system;job control (unix);loss function;multi-task learning;network architecture	Xiaolong Wang;Yin Zhou;Deguang Kong;Jon Currey;Dawei Li;Jiayu Zhou	2017	2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017)	10.1109/FG.2017.75	architecture;facial recognition system;artificial neural network;black magic;artificial intelligence;machine learning;exploit;computer science	Vision	26.23361613945217	-49.950579016150556	191306
38abaa549c4f398079dc5b1e5957315f66918e23	a fast method for estimating transient scene attributes	databases;training;transient analysis;transient analysis clouds webcams training meteorology databases;clouds;object detection image classification natural scenes neural net architecture;webcams;meteorology;object detection deep convolutional neural networks transient scene attribute estimation scene classification network architecture	We propose the use of deep convolutional neural networks to estimate the transient attributes of a scene from a single image. Transient scene attributes describe both the objective conditions, such as the weather, time of day, and the season, and subjective properties of a scene, such as whether or not the scene seems busy. Recently, convolutional neural networks have been used to achieve state-of-the-art results for many vision problems, from object detection to scene classification, but have not previously been used for estimating transient attributes. We compare several methods for adapting an existing network architecture and present state-of-the-art results on two benchmark datasets. Our method is more accurate and significantly faster than previous methods, enabling real-world applications.	artificial neural network;autostereogram;benchmark (computing);convolutional neural network;network architecture;object detection;scene statistics	Ryan Baltenberger;Menghua Zhai;Connor Greenwell;Scott Workman;Nathan Jacobs	2016	2016 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2016.7477713	computer vision;computer science;machine learning;data mining	Vision	27.150439649499873	-51.1472195749701	191700
750f9e76e730805bcca76eaf3ea1d4c512fb2ac5	a multi-task collaborative learning method based on auxiliary training and geometric constraints		In facial landmark detection, cascaded deep convolutional neural networks have high model complexity, inadequate training of underlying parameters and difficult network initialization. To address these problems, a multi-task collaborative learning method is proposed based on auxiliary training and geometric constraints. Firstly, by virtue of the correlation between facial landmark localization and head pose estimation, a deep network model was designed based on joint optimization of these two tasks to simultaneously estimate the landmark coordinates and pose angles. Secondly, the auxiliary training technique was applied to enhance the feature learning ability of the network model through adding a back-propagation layer. Then, the geometric constraint algorithm was employed to pre-train the model and its related parameters are used for the initialization of the constructed model, in order to make the model to extract the invariant features of pose variation effectively. Finally, the effectiveness of the proposed method was evaluated on the public database 300W. Experimental results showed that the proposed method was better than the single-task and multi-task learning methods, and yielded that the AUC0.2 value of facial landmark detection is 0.1406 and the error rates of head pose estimation in three-dimensional space are 4.75%, 8.39% and 5.75% respectively.	3d pose estimation;artificial neural network;backpropagation;computer multitasking;constraint algorithm;convolutional neural network;database;feature learning;mathematical optimization;multi-task learning;network model;software propagation	Gaoyuan Mu;Qingshan She;Zhuo Tian;Haitao Gan;Peng Jiang	2018	2018 IEEE Industrial Cyber-Physical Systems (ICPS)	10.1109/ICPHYS.2018.8387641	control engineering;convolutional neural network;deep learning;initialization;network model;engineering;constraint algorithm;collaborative learning;feature learning;artificial intelligence;pattern recognition;pose	Vision	26.391207221037234	-49.22891259920534	192411
404a95075de80ea6ba795943edf0f73b7f7df341	a hybrid holistic/semantic approach for scene classification	semantics matching pursuit algorithms dictionaries rivers lakes accuracy vectors;scene classification;semantic representation;holistic representation;natural scenes feature extraction image classification image representation;semantic spatial pyramid scene classification holistic representation semantic representation;semantic spatial pyramid;benchmark natural scene dataset hybrid holistic semantic approach scene classification global features internal object configuration holistic strategy semantic strategy deep learning algorithm scene representation spatial object configuration	There are two main strategies to tackle scene classification: holistic and semantic. The former characterizes a scene using its global features, while the latter represents a scene by modeling its internal object configuration. Holistic strategy is good at representing scenes with simple contents, but it does not represent well complex scenes that consist of multiple objects. By contrast, semantic strategy is advantageous at recognizing scenes with complex objects, but it does not work well for simple scenes. In this paper, we propose to integrate holistic and semantic strategies to cope with scene classification. In particular, we exploit a deep learning algorithm to learn features for scene representation in the holistic way. For the semantic strategy, we explore a semantic spatial pyramid to represent the spatial object configuration of scenes. The holistic and semantic strategies are integrated using a method proposed by us. Experimental results on a benchmark natural scene dataset demonstrate the effectiveness of our proposed hybrid approach for scene classification, by comparing to several state-of-the-art algorithms.	algorithm;benchmark (computing);deep learning;holism;scene graph	Zenghai Chen;Zheru Chi;Hong Fu	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.399	computer vision;semantic computing;scene statistics;computer science;machine learning;pattern recognition	Vision	28.620650465120416	-49.27509435080073	192574
171a4ef673e40d09d7091082c7fd23b3758fc3c2	video-based face recognition using ensemble of haar-like deep convolutional neural networks		Growing number of surveillance and biometric applications seek to recognize the face of individuals appearing in the viewpoint of video cameras. Systems for video-based FR can be subjected to challenging operational environments, where the appearance of faces captured with video cameras varies significantly due to changes in pose, illumination, scale, blur, expression, occlusion, etc. In particular, with still-to-video FR, a limited number of high-quality facial images are typically captured for enrollment of an individual to the system, whereas an abundance facial trajectories can be captured using video cameras during operations, under different viewpoints and uncontrolled conditions. This paper presents a deep learning architecture that can learn a robust facial representation for each target individual during enrollment, and then accurately compare the facial regions of interest (ROIs) extracted from a still reference image (of the target individual) with ROIs extracted from live or archived videos. An ensemble of deep convolutional neural networks (DCNNs) named HaarNet is proposed, where a trunk network first extracts features from the global appearance of the facial ROIs (holistic representation). Then, three branch networks effectively embed asymmetrical and complex facial features (local representations) based on Haar-like features. In order to increase the discriminativness of face representations, a novel regularized triplet-loss function is proposed that reduces the intra-class variations, while increasing the inter-class variations. Given the single reference still per target individual, the robustness of the proposed DCNN is further improved by fine-tuning the HaarNet with synthetically-generated facial still ROIs that emulate capture conditions found in operational environments. The proposed system is evaluated on stills and videos from the challenging COX Face and Chokepoint datasets according to accuracy and complexity. Experimental results indicate that the proposed method can significantly improve performance with respect to state-of-the-art systems for video-based FR.	archive;artificial neural network;biometrics;chroma subsampling;closed-circuit television;convolutional neural network;decibel;deep learning;experiment;face detection;facial recognition system;gaussian blur;haar wavelet;holism;information;inter-process communication;loss function;region of interest;synthetic data;triplet state;trunk (software);uncontrolled format string	Mostafa Parchami;Saman Bashbaghi;Eric Granger	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966443	machine learning;architecture;convolutional neural network;robustness (computer science);facial recognition system;computer science;deep learning;feature extraction;pattern recognition;computer vision;artificial intelligence	Vision	30.898693028691035	-51.45144929803619	192886
140c93362da0f0f88ebb78e6bec041677d245ae8	conditioned regression models for non-blind single image super-resolution	kernel image resolution training dictionaries adaptation models computer vision neural networks;regression analysis computer vision image resolution image restoration learning artificial intelligence neural nets;random forests conditioned regression models nonblind single image superresolution computer vision machine learning algorithms high resolution images low resolution images fixed blur kernel image formation process convolutional neural networks	Single image super-resolution is an important task in the field of computer vision and finds many practical applications. Current state-of-the-art methods typically rely on machine learning algorithms to infer a mapping from low-to high-resolution images. These methods use a single fixed blur kernel during training and, consequently, assume the exact same kernel underlying the image formation process for all test images. However, this setting is not realistic for practical applications, because the blur is typically different for each test image. In this paper, we loosen this restrictive constraint and propose conditioned regression models (including convolutional neural networks and random forests) that can effectively exploit the additional kernel information during both, training and inference. This allows for training a single model, while previous methods need to be re-trained for every blur kernel individually to achieve good results, which we demonstrate in our evaluations. We also empirically show that the proposed conditioned regression models (i) can effectively handle scenarios where the blur kernel is different for each image and (ii) outperform related approaches trained for only a single kernel.	algorithm;artificial neural network;computer vision;consistency model;convolutional neural network;experiment;gaussian blur;image formation;image resolution;kernel (operating system);machine learning;random forest;standard test image;super-resolution imaging	Gernot Riegler;Samuel Schulter;Matthias Rüther;Horst Bischof	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.67	computer vision;kernel embedding of distributions;radial basis function kernel;mean-shift;computer science;machine learning;pattern recognition;tree kernel;polynomial kernel	Vision	27.15470076319609	-49.17343158852593	193067
ae97dcdd096b734456fd9266a40703dc40245d64	measuring and predicting tag importance for image retrieval	visualization semantics image retrieval predictive models visual databases training motorcycles;cross domain learning multimodal image retrieval mir image retrieval semantic gap tag importance importance measure importance prediction	Textual data such as tags, sentence descriptions are combined with visual cues to reduce the semantic gap for image retrieval applications in today's Multimodal Image Retrieval (MIR) systems. However, all tags are treated as equally important in these systems, which may result in misalignment between visual and textual modalities during MIR training. This will further lead to degenerated retrieval performance at query time. To address this issue, we investigate the problem of tag importance prediction, where the goal is to automatically predict the tag importance and use it in image retrieval. To achieve this, we first propose a method to measure the relative importance of object and scene tags from image sentence descriptions. Using this as the ground truth, we present a tag importance prediction model to jointly exploit visual, semantic and context cues. The Structural Support Vector Machine (SSVM) formulation is adopted to ensure efficient training of the prediction model. Then, the Canonical Correlation Analysis (CCA) is employed to learn the relation between the image visual feature and tag importance to obtain robust retrieval performance. Experimental results on three real-world datasets show a significant performance improvement of the proposed MIR with Tag Importance Prediction (MIR/TIP) system over other MIR systems.	abnormal degeneration;approximation algorithm;automatic image annotation;body dysmorphic disorders;description;entity name part qualifier - adopted;fbn2 wt allele;ground truth;image retrieval;inference;multimodal interaction;natural language processing;object detection;question (inquiry);support vector machine;text corpus;canonical correlation analysis	Shangwen Li;Sanjay Purushotham;Chen Chen;Yuzhuo Ren;C.-C. Jay Kuo	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2017.2651818	support vector machine;semantic gap;artificial intelligence;computer vision;visual word;information retrieval;image retrieval;sensory cue;pattern recognition;ground truth;computer science;visualization;sentence	Vision	31.103081869054435	-51.37118079548841	194062
6284d5af891faafa6e0415cd1573d1dea82f1057	fast aircraft detection using end-to-end fully convolutional network	aircraft detection;fully convolutional network;end to end	Aircraft detection from remote sensing images of complex background is a challenging task. Existing aircraft detection methods usually consist of two separated stages: proposal generation and window classification, which may be suboptimal for the aircraft detection task. To overcome this shortcoming, we propose a unified aircraft detection framework to simultaneously predict aircraft bounding boxes and class probabilities directly from an arbitrary-sized remote sensing image. Specifically, an end-to-end fully convolutional network (FCN) replaces the fully connected layers in traditional convolutional neural network (CNN). This can greatly reduce the model size while obtaining the comparable detection accuracy. To directly detect aircrafts under multiple scales and different aspect ratios, multiple referenced boxes are introduced. The whole framework can be optimized end-to-end by minimizing a multi-task loss. Extensive experiments on a common dataset demonstrate that the proposed method yields much lower false alarm rates at different recall rates than the state-of-the-art methods, and its speed is more than 35 times faster than the compared methods.	artificial neural network;computer multitasking;convolutional neural network;end-to-end principle;experiment;feature model;instance (computer science);object detection;sensitivity and specificity	Ting-Bing Xu;Guang-Liang Cheng;Jie Yang;Cheng-Lin Liu	2016	2016 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2016.7868532	embedded system;real-time computing;simulation;computer science	Vision	29.22600397874651	-51.63914123624775	194089
aab3561acbd19f7397cbae39dd34b3be33220309	quantization mimic: towards very tiny cnn for object detection		In this paper, we propose a simple and general framework for training very tiny CNNs (e.g. VGG with the number of channels reduced to 1 32 ) for object detection. Due to limited representation ability, it is challenging to train very tiny networks for complicated tasks like detection. To the best of our knowledge, our method, called Quantization Mimic, is the first one focusing on very tiny networks. We utilize two types of acceleration methods: mimic and quantization. Mimic improves the performance of a student network by transfering knowledge from a teacher network. Quantization converts a full-precision network to a quantized one without large degradation of performance. If the teacher network is quantized, the search scope of the student network will be smaller. Using this feature of the quantization, we propose Quantization Mimic. It first quantizes the large network, then mimic a quantized small network. The quantization operation can help student network to better match the feature maps from teacher network. To evaluate our approach, we carry out experiments on various popular CNNs including VGG and Resnet, as well as different detection frameworks including Faster R-CNN and R-FCN. Experiments on Pascal VOC and WIDER FACE verify that our Quantization Mimic algorithm can be applied on various settings and outperforms state-of-the-art model acceleration methods given limited computing resouces.	algorithm;elegant degradation;experiment;map;map matching;object detection;quantization (signal processing)	Yi Wei;Xinyu Pan;Hongwei Qin;Wanli Ouyang;Junjie Yan	2018		10.1007/978-3-030-01237-3_17	computer vision;computer science;acceleration;artificial intelligence;quantization (signal processing);object detection;residual neural network;pattern recognition;quantization (physics);communication channel	Vision	26.472241460274287	-50.78854947650652	194180
876c8435c9d69cab2cecb76c08fd31d6dbfacba7	eyelash detection model for accurate iris segmentation.	iris recognition	 In this paper, we present a novel eyelash detection model based on three criterions: 1) separable eyelash condition, 2) non-informative condition and 3) connective criterion. The first condition handles separable eyelash and the second condition manages multiple eyelashes. The last criterion avoids misclassification of strong iris texture as a single and separable eyelash. A number of images are selected to evaluate the accuracy and necessity of the eyelash detection model. The results are encouraging.	information;logical connective	W. K. Kong;Diwei Zhang	2001			computer science;iris recognition	AI	30.638903737636774	-45.70188605025132	194929
1fb5cc711a2f0c2c65bd0146edf18d7156c391ea	minimizing dataset bias: discriminative multi-task sparse coding through shared subspace learning for image classification	encoding dictionaries image coding accuracy training algorithm design and analysis feature extraction;shared subspace;dataset bias;dataset bias multi task sparse coding shared subspace;learning artificial intelligence image classification image coding;heterogeneous datasets image classification discriminative multi task sparse coding shared subspace learning image processing analysis sparse coding method;sparse coding;multi task	Sparse coding was shown to be able to find succinct representations of stimuli. Recently, it has been successfully applied to a variety of problems in image processing analysis. Sparse coding models data vectors as a linear combination of a few elements from a dictionary. However, most existing sparse coding methods are applied for a single task on a single dataset. The learned dictionary is then possibly biased towards the specific dataset and lacks of generalization abilities. In light of this, in this paper we propose a multitask sparse coding approach by uncovering a shared subspace among heterogeneous datasets. The proposed multi-task coding strategy leverages the commonality benefit from different datasets. Moreover, our multi-task coding framework is capable of direct classification by incorporating label information. Experimental results show that the dictionary learned by our approach has more generalization abilities and our model performs better classification compared to the model learned from only one dataset or the model learned from simply pooling different datasets together.	computer multitasking;computer vision;dictionary;image processing;neural coding;sparse approximation;sparse matrix	Gaowen Liu;Yuqing Chen;Jingkuan Song;Nicu Sebe	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025580	k-svd;computer science;machine learning;pattern recognition;sparse approximation;data mining;neural coding	Vision	24.974986376934748	-45.62995687723139	195411
4c28ca360096d1ebf9a0c1d5ac10bedfa35819b4	coupled auto-associative neural networks for heterogeneous face recognition	latent features coupled autoassociative neural networks correlated representation learning source modalities target modalities target to source image representation learning cross modal transformation hidden units information preservation heterogeneous face recognition databases empirical analysis competitive cross modal face recognition softmax classifier training source domain target domain;neural networks cross modality heterogeneous face recognition common latent features biometrics;neural networks;biometrics;cross modality;visual databases face recognition feature extraction image classification learning artificial intelligence neural nets;face recognition;feature extraction;coupled autoassociative neural networks correlated representation learning source modalities target modalities target to source image representation learning cross modal transformation hidden units information preservation heterogeneous face recognition databases empirical analysis competitive cross modal face recognition softmax classifier training source domain target domain latent features;face recognition feature extraction biometrics neural networks cross modality;cross modality heterogeneous face recognition common latent features biometrics neural networks;face recognition feature extraction image classification learning artificial intelligence neural nets visual databases	Several models have been previously suggested for learning correlated representations between source and target modalities. In this paper, we propose a novel coupled autoassociative neural network for learning a target-to-source image representation for heterogenous face recognition. This coupled network is unique, because a cross-modal transformation is learned by forcing the hidden units (latent features) of two neural networks to be as similar as possible, while simultaneously preserving information from the input. The effectiveness of this model is demonstrated using multiple existing heterogeneous face recognition databases. Moreover, the empirical results show that the learned image representation-common latent features-by the coupled auto-associative produces competitive cross-modal face recognition results. These results are obtained by training a softmax classifier using only the latent features from the source domain and testing using only the latent features from the target domain.	autoassociative memory;database;facial recognition system;modal logic;neural networks;softmax function	Benjamin S. Riggan;Christopher Reale;Nasser M. Nasrabadi	2015	IEEE Access	10.1109/ACCESS.2015.2479620	facial recognition system;speech recognition;feature;feature extraction;computer science;machine learning;pattern recognition;three-dimensional face recognition;time delay neural network;artificial neural network;biometrics	Vision	24.915934538736906	-48.173432738685975	196154
b9604da5a1648473bc6d337e723f9d2ad60d7df8	towards effective codebookless model for image classification	image classification;bag of features;riemannian manifold;codebookless model	The bag-of-features (BoF) model for image classification has been thoroughly studied over the last decade. Different from the widely used BoF methods which modeled images with a pre-trained codebook, the alternative codebook free image modeling method, which we call Codebookless Model (CLM), attracted little attention. In this paper, we present an effective CLM that represents an image with a single Gaussian for classification. By embedding Gaussian manifold into a vector space, we show that the simple incorporation of our CLM into a linear classifier achieves very competitive accuracy compared with state-of-the-art BoF methods (e.g., Fisher Vector). Since our CLM lies in a highdimensional Riemannian manifold, we further propose a joint learning method of low-rank transformation with support vector machine (SVM) classifier on the Gaussian manifold, in order to reduce computational and storage cost. To study and alleviate the side effect of background clutter on our CLM, we also present a simple yet effective partial background removal method based on saliency detection. Experiments are extensively conducted on eight widely used databases to demonstrate the effectiveness and efficiency o f our CLM method.	channel length modulation;clutter;codebook;computer vision;database;experiment;linear classifier;support vector machine	Qilong Wang;Peihua Li;Lei Zhang;Wangmeng Zuo	2016	Pattern Recognition	10.1016/j.patcog.2016.03.004	computer vision;contextual image classification;speech recognition;computer science;artificial intelligence;machine learning;mathematics	AI	28.185251975215746	-47.09940312997664	196318
47ec44b8d6c3a9566a6c4d81dbf27131d670ee85	discriminatively activated sparselets		Shared representations are highly appealing due to their potential for gains in computational and statistical efficiency. Compressing a shared representation leads to greater computational savings, but can also severely decrease performance on a target task. Recently, sparselets (Song et al., 2012) were introduced as a new shared intermediate representation for multiclass object detection with deformable part models (Felzenszwalb et al., 2010a), showing significant speedup factors, but with a large decrease in task performance. In this paper we describe a new training framework that learns which sparselets to activate in order to optimize a discriminative objective, leading to larger speedup factors with no decrease in task performance. We first reformulate sparselets in a general structured output prediction framework, then analyze when sparselets lead to computational efficiency gains, and lastly show experimental results on object detection and image classification tasks. Our experimental results demonstrate that discriminative activation substantially outperforms the previous reconstructive approach which, together with our structured output prediction formulation, make sparselets broadly applicable and significantly more effective.	benchmark (computing);computation;computer vision;discriminative model;experiment;intermediate representation;object detection;sparse matrix;speedup	Ross B. Girshick;Hyun Oh Song;Trevor Darrell	2013			efficiency;discriminative model;machine learning;intermediate language;pattern recognition;artificial intelligence;object detection;speedup;computer science;contextual image classification	ML	26.241979370716482	-48.225595043926276	196985
1e2026f1301f24c16ba1e563f2045ca535d99588	predicting multiple structured visual interpretations	predictive models labeling adaptation models computer vision semantics inference algorithms;pose estimation computer vision image segmentation inference mechanisms learning artificial intelligence optimisation;image foreground background segmentation multiple structured visual interpretation structured visual output monocular pose estimation semantic scene segmentation inference procedure learning procedure structured output predictor diverse prediction structured learning submodular maximization literature vision task monocular pose estimation	We present a simple approach for producing a small number of structured visual outputs which have high recall, for a variety of tasks including monocular pose estimation and semantic scene segmentation. Current state-of-the-art approaches learn a single model and modify inference procedures to produce a small number of diverse predictions. We take the alternate route of modifying the learning procedure to directly optimize for good, high recall sequences of structured-output predictors. Our approach introduces no new parameters, naturally learns diverse predictions and is not tied to any specific structured learning or inference procedure. We leverage recent advances in the contextual submodular maximization literature to learn a sequence of predictors and empirically demonstrate the simplicity and performance of our approach on multiple challenging vision tasks including achieving state-of-the-art results on multiple predictions for monocular pose-estimation and image foreground/background segmentation.	3d pose estimation;baseline (configuration management);black box;display resolution;expectation–maximization algorithm;experiment;institute for operations research and the management sciences;kerrison predictor;misra c;marginal model;structured prediction;submodular set function	Debadeepta Dey;Varun Ramakrishna;Martial Hebert;J. Andrew Bagnell	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.337	computer vision;computer science;machine learning;pattern recognition	Vision	27.991957567254083	-48.074011356463366	197041
6754c98ba73651f69525c770fb0705a1fae78eb5	joint cascade face detection and alignment		We present a new state-of-the-art approach for face detection. The key idea is to combine face alignment with detection, observing that aligned face shapes provide better features for face classification. To make this combination more effective, our approach learns the two tasks jointly in the same cascade framework, by exploiting recent advances in face alignment. Such joint learning greatly enhances the capability of cascade detection and still retains its realtime performance. Extensive experiments show that our approach achieves the best accuracy on challenging datasets, where all existing solutions are either inaccurate or too slow.	cascade framework;experiment;face detection	Dong Chen;Shaoqing Ren;Yichen Wei;Xudong Cao;Jian Sun	2014		10.1007/978-3-319-10599-4_8	machine learning;computer vision;face detection;artificial intelligence;computer science;cascade	Vision	30.496044069608004	-50.31498347784839	197798
e4094280b441c171994175a511a063091e3924e2	deep learning for logo recognition	flickrlogos 32;data augmentation;cognitive neuroscience;deep learning;logo recognition;artificial intelligence;convolutional neural network;computer science applications1707 computer vision and pattern recognition	In this paper we propose a method for logo recognition using deep learning. Our recognition pipeline is composed of a logo region proposal followed by a Convolutional Neural Network (CNN) specifically trained for logo classification, even if they are not precisely localized. Experiments are carried out on the FlickrLogos-32 database, and we evaluate the effect on recognition performance of synthetic versus real data augmentation, and image pre-processing. Moreover, we systematically investigate the benefits of different training choices such as class-balancing, sample-weighting and explicit modeling the background class (i.e. no-logo regions). Experimental results confirm the feasibility of the proposed method, that outperforms the methods in the state of the art.	application domain;convolutional neural network;deep learning;distortion;explicit modeling;logo;preprocessor;sensor;synthetic data;visual artifact	Simone Bianco;Marco Buzzelli;Davide Mazzini;Raimondo Schettini	2017	Neurocomputing	10.1016/j.neucom.2017.03.051	convolutional neural network;machine learning;logo;artificial intelligence;deep learning;pattern recognition;cognitive neuroscience;computer science	AI	25.958225620437673	-51.62352017714223	198171
ef6ec57bc6fb74dea23d5e99885fa4152a4afefb	enhancing multi-label classification based on local label constraints and classifier chains	classifier chains;multi label classification;label constraints;multilabel learning system local label constraint correlation information multilabel classification method training label set segmentation mutual information metric local tree structure label constraint classifier chain strategy;trees mathematics learning artificial intelligence pattern classification;data models measurement multimedia communication clustering algorithms algorithm design and analysis;classifier chains multi label classification label constraints	In the multi-label classification issue, some implicit constraints and dependencies are always existed among labels. Exploring the correlation information among different labels is important for many applications. It not only can enhance the classifier performance but also can help to interpret the classification results for some specific applications. This paper presents an improved multi-label classification method based on local label constraints and classifier chains for solving multi-label tasks with large number of labels. Firstly, in order to exploit local label constraints in multi-label problem with large number of labels, clustering approach is utilized to segment training label set into several subsets. Secondly, for each label subset, local tree-structure constraints among different labels are mined based on mutual information metric. Thirdly, based on the mined local tree-structure label constraints, a variant of classifier chain strategy is implemented to enhance the multi-label learning system. Experiment results on five multi-label benchmark datasets show that the proposed method is a competitive approach for solving multi-label classification tasks with large number of labels.	benchmark (computing);classifier chains;cluster analysis;error detection and correction;mined;multi-label classification;mutual information;statistical classification;tree structure	Benhui Chen;Weite Li;Yuqing Zhang;Jinglu Hu	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727370	computer science;machine learning;pattern recognition;data mining	AI	25.98148543361642	-45.65094728560586	198431
e83b017eaedfbf19d598799d8a1884c57c634782	genlr-net: deep framework for very low resolution face and object recognition with generalization to unseen categories		Matching very low resolution images of faces and objects with high resolution images in the database has important applications in surveillance scenarios, street-to-shop matching for general objects, etc. Matching across huge resolution difference along with variations in illumination, view-point, etc. makes the problem quite challenging. The problem becomes even more difficult if the testing objects have not been seen during training. In this work, we propose a novel deep convolutional neural network architecture to address these problems. We systematically introduce different kinds of constraints at different stages of the architecture so that the approach can recognize low resolution images as well as generalize well to images of unseen categories. The reason behind each additional step along with its effect on the overall performance is thoroughly analyzed. Extensive experiments are conducted on two face and object datasets which justifies the effectiveness of the proposed approach for handling these real-life challenging scenarios.		Sivaram Prasad Mudunuri;Soubhik Sanyal;Soma Biswas	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2018.00090	computer vision;convolutional neural network;architecture;task analysis;pattern recognition;artificial intelligence;cognitive neuroscience of visual object recognition;computer science;image resolution	Vision	28.988174543733283	-50.6928772633846	198445
24312e48adaafd54ab076815c327141351a630f9	seeing in the dark with recurrent convolutional neural networks.		Classical convolutional neural networks (cCNNs) are very good at categorizing objects in images. But, unlike human vision which is relatively robust to noise in images, the performance of cCNNs declines quickly as image quality worsens. Here we propose to use recurrent connections within the convolutional layers to make networks robust against pixel noise such as could arise from imaging at low light levels, and thereby significantly increase their performance when tested with simulated noisy video sequences. We show that cCNNs classify images with high signal to noise ratios (SNRs) well, but are easily outperformed when tested with low SNR images (high noise levels) by convolutional neural networks that have recurrency added to convolutional layers, henceforth referred to as gruCNNs. Addition of Bayes-optimal temporal integration to allow the cCNN to integrate multiple image frames still does not match gruCNN performance. Additionally, we show that at low SNRs, the probabilities predicted by the gruCNN (after calibration) have higher confidence than those predicted by the cCNN. We propose to consider recurrent connections in the early stages of neural networks as a solution to computer vision under imperfect lighting conditions and noisy environments; challenges faced during real-time video streams of autonomous driving at night, during rain or snow, and other non-ideal situations.	artificial neural network;autonomous car;autonomous robot;categorization;computer vision;convolutional neural network;database;deep learning;experiment;feed forward (control);feedforward neural network;ground truth;humans;image quality;imagenet;pixel;real-time data;real-time locating system;recurrent neural network;signal-to-noise ratio;streaming media	Till S. Hartmann	2018	CoRR		pattern recognition;pixel;streams;convolutional neural network;artificial intelligence;image quality;artificial neural network;computer science;signal-to-noise ratio	Vision	27.70093919326016	-51.1790357273959	198746
38f634a488a2fbb4801a2878304450aa5aa5e6c4	rbnet: a deep neural network for unified road and road boundary detection		Accurately detecting road and its boundary on the images is an essential task for vision-based autonomous driving systems. However, prevailing methods either only detect road or add an extra processing stage to detect road boundary. In this work, we introduce a deep neural network, called Road and road Boundary detection Network (RBNet), that can detect both road and road boundary in a single process. In specific, we first investigate the contextual relationship between the road structure and its boundary arrangement and then model them with a Bayesian network. By implementing the Bayesian model, the RBNet can learn to simultaneously estimate the probabilities of a pixel on the image belonging to the road and road boundary. Comprehensive evaluations are carried out based on the well-known road benchmark, which can demonstrate the compelling performance of the proposed method.	deep learning	Zhe Chen;Zijing Chen	2017		10.1007/978-3-319-70087-8_70	machine learning;artificial intelligence;pattern recognition;computer science;deep learning;artificial neural network;bayesian network;bayesian inference	Robotics	30.534130706075718	-50.14429736098337	199003
b7ac537d97efcb968ca8e353ff5b0563e26b9dbe	object-aware dense semantic correspondence		This work aims to build pixel-to-pixel correspondences between images from the same visual class but with different geometries and visual similarities. This task is particularly challenging because (i) their visual content is similar only on the high-level structure, and (ii) background clutters keep bringing in noises. To address these problems, this paper proposes an object-aware method to estimate per-pixel correspondences from semantic to low-level by learning a classi?er for each selected discriminative grid cell and guiding the localization of every pixel under the semantic constraint. Specifically, an Object-aware Hierarchical Graph (OHG) model is constructed to regulate matching consistency from one coarse grid cell containing whole object(s), to fine grid cells covering smaller semantic elements, and finally to every pixel. A guidance layer is introduced as the semantic constraint on local structure matching. In addition, we propose to learn the important high-level structure for each grid cell in an objectness-driven way as an alternative to handcrafted descriptors in de?ning a better visual similarity. The proposed method has been extensively evaluated on various challenging benchmarks and real-world images. The results show that our method signi?cantly outperforms the state-of-the-arts in terms of semantic flow accuracy.	benchmark (computing);data descriptor;discriminative model;high- and low-level;level structure;pattern recognition;pixel	Fan Yang;Xin Li;Hong Cheng;Jianping Li;Leiting Chen	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.442	artificial intelligence;pixel;computer vision;grid;visualization;semantic computing;discriminative model;semantics;pattern recognition;computer science;semantic similarity;machine learning;graph	Vision	30.971627545087753	-52.05435911412517	199315
82d2b42ea4ee9997480a1fd39e3ec59a78f42ef2	defocus blur detection via multi-stream bottom-top-bottom fully convolutional network		Defocus blur detection (DBD) is the separation of in-focus and out-of-focus regions in an image. This process has been paid considerable attention because of its remarkable potential applications. Accurate differentiation of homogeneous regions and detection of low-contrast focal regions, as well as suppression of background clutter, are challenges associated with DBD. To address these issues, we propose a multi-stream bottom-top-bottom fully convolutional network (BTBNet), which is the first attempt to develop an end-to-end deep network for DBD. First, we develop a fully convolutional BTBNet to integrate low-level cues and high-level semantic information. Then, considering that the degree of defocus blur is sensitive to scales, we propose multi-stream BTBNets that handle input images with different scales to improve the performance of DBD. Finally, we design a fusion and recurrent reconstruction network to recurrently refine the preceding blur detection maps. To promote further study and evaluation of the DBD models, we construct a new database of 500 challenging images and their pixel-wise defocus blur annotations. Experimental results on the existing and our new datasets demonstrate that the proposed method achieves significantly better performance than other state-of-the-art algorithms.		Wenhong Zhao;Fan Zhao;Dong Wang;Huchuan Lu	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00325	task analysis;computer vision;iterative reconstruction;pattern recognition;feature extraction;homogeneous;clutter;artificial intelligence;computer science;image resolution	Vision	28.35252100183866	-51.57129273805805	199822
0f26cb93ade95dee2d21f5cb44b1a80264750ffd	a joint appearance-spatial distance for kernel-based image categorization	universal reference model;image features;kernel;distance measure;semantic level image retrieval;asia image retrieval upper bound spatial coherence multimedia computing automation size measurement kernel design methodology embedded computing;reference model;image classification;adaption learning;spatial structure;joints;information theoretic discrimination;upper bound;distance measurement;computational modeling;adaptation model;hidden markov models;recursive formulation;adaptive learning;kernel based image categorization;image retrieval image classification;appearance spatial image features;computational efficiency;universal reference model appearance spatial distance kernel based image categorization semantic level image retrieval appearance spatial image features information theoretic discrimination recursive formulation adaption learning;information theoretic;appearance spatial distance;image retrieval	The goal of image categorization is to classify a collection of unlabeled images into a set of predefined classes to support semantic-level image retrieval. The distance measures used in most existing approaches either ignored the spatial structures or used them in a separate step. As a result, these distance measures achieved only limited success. To address these difficulties, in this paper, we propose a new distance measure that integrates joint appearance-spatial image features. Such a distance measure is computed as an upper bound of an information-theoretic discrimination, and can be computed efficiently in a recursive formulation that scales well to image size. In addition, the upper bound approximation can be further tightened via adaption learning from a universal reference model. Extensive experiments on two widely-used data sets show that the proposed approach significantly outperforms the state-of-the-art approaches.	algorithm;approximation;categorization;experiment;image resolution;image retrieval;information theory;kernel (operating system);recursion;reference model	Guo-Jun Qi;Xian-Sheng Hua;Yong Rui;Jinhui Tang;Zheng-Jun Zha;HongJiang Zhang	2008	2008 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2008.4587379	computer vision;contextual image classification;kernel;reference model;image retrieval;computer science;machine learning;pattern recognition;mathematics;distance transform;upper and lower bounds;computational model;adaptive learning;feature	Vision	30.920732119180293	-46.982558048720875	199947
