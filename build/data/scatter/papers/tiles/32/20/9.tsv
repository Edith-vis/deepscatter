id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
03a0537685fae7cd4712071540735d04878692b2	thematic information extraction from ikonos imagery based on object and various features	object orientated methodology;kernel;classification algorithm;high resolution;object oriented methods;image segmentation;support vector machines;information extraction;supervised classification;image classification;support vector machine very high resolution object oriented multi feature fuzzy system;support vector machine method land cover thematic information extraction ikonos imagery high resolution remote sensing object orientated methodology feature configuration feature classification algorithm image segmentation image classification fuzzy system;feature classification algorithm;data mining;feature space;terrain mapping fuzzy systems geophysics computing image classification image segmentation object oriented methods;distance measurement;remote sensing imagery;ikonos imagery;geophysics computing;multi feature;object oriented;very high resolution;remote sensing;classification algorithms;data mining support vector machines support vector machine classification image classification image resolution classification algorithms spatial resolution multispectral imaging image segmentation information analysis;terrain mapping;support vector machine;multi spectral;feature configuration;land cover thematic information extraction;classification accuracy;support vector machine method;land cover;fuzzy systems;fuzzy system;high resolution remote sensing	As to high-resolution remote sensing imagery classification based on object-orientated methodology, the precision is related to feature configuration and classification algorithm. In this paper, the IKONOS multi-spectral image is selected as sample data, and segmentated into many image objects. Firstly, the supervising classification is applied to extract land cover thematic information, based on nearest distance and SVM methods respectively, and the total classification accuracy is analysed and compared with each other. Follow the feature space is changed into different dimensionality, and the relation between feature configuration and classification accuracy is discussed respectively with nearest distance and SVM methods. The results show that the SVM classifier can well relax the relationship between image classification and representative feature configuration, and improved the insufficiency of nearest distance methodology during nonlinearity classification processing.	algorithm;computer vision;convergence insufficiency;feature vector;image resolution;information extraction;multispectral image;nonlinear system	Deyong Hu;Wenji Zhao;Huili Gong;Xiaojuan Li;Jiacun Li	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779847	support vector machine;computer vision;computer science;machine learning;linear classifier;pattern recognition;k-nearest neighbors algorithm;feature;fuzzy control system	Robotics	31.69665537142678	-43.988621160926186	159282
2a735a90919b451b4bc436b48cc1311f41b266f7	fusion and community detection in multi-layer graphs	symmetric matrices;image edge detection;matrix decomposition;linear programming;clustering algorithms;robustness;optimization	Relational data arising in many domains can be represented by networks (or graphs) with nodes capturing entities and edges representing relationships between these entities. Community detection in networks has become one of the most important problems having a broad range of applications. Until recently, the vast majority of papers have focused on discovering community structures in a single network. However, with the emergence of multi-view network data in many real-world applications and consequently with the advent of multilayer graph representation, community detection in multi-layer graphs has become a new challenge. Multi-layer graphs provide complementary views of connectivity patterns of the same set of vertices. Fusion of the network layers is expected to achieve better clustering performance. In this paper, we propose two novel methods, coined as WSSNMTF (Weighted Simultaneous Symmetric Non-Negative Matrix Tri-Factorization) and NG-WSSNMTF (Natural Gradient WSSNMTF), for fusion and clustering of multi-layer graphs. Both methods are robust with respect to missing edges and noise. We compare the performance of the proposed methods with two baseline methods, as well as with three state-of-the-art methods on synthetic and three real-world datasets. The experimental results indicate superior performance of the proposed methods.	adjacency matrix;baseline (configuration management);cluster analysis;emergence;entity;gradient;graph (abstract data type);graph (discrete mathematics);information geometry;layer (electronics);loss function;non-negative matrix factorization;optimization problem;synthetic data;synthetic intelligence	Vladimir Gligorijevic;Yannis Panagakis;Stefanos P. Zafeiriou	2016	2016 23rd International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2016.7899821	computer science;linear programming;theoretical computer science;machine learning;data mining;mathematics;cluster analysis;matrix decomposition;robustness;symmetric matrix	ML	24.870053217154364	-43.07661822399463	159862
0f8c925e0855a37c918bf71cd66b1b3d4ed864aa	incremental kernel non-negative matrix factorization for hyperspectral unmixing	kernel;memory management;matrix decomposition kernel hyperspectral imaging partitioning algorithms memory management correlation;hyperspectral image incremental kernel nonnegative matrix factorization iknmf hyperspectral unmixing nonlinear dependency feature data matrix partition matrix theory;matrix decomposition;nonlinear spectral unmixing incremental algorithm block matrix kernel non negative matrix factorization knmf;correlation;hyperspectral imaging;partitioning algorithms;matrix decomposition feature extraction hyperspectral imaging image processing	In this paper, we proposed an incremental kernel non-negative matrix factorization (IKNMF) to reduce the computing scale in hyperspectral unmixing. Kernel non-negative matrix factorization (KNMF) is an extended non-negative matrix factorization (NMF) able to capture nonlinear dependency features in data matrix through kernel functions. In KNMF algorithm, the size of kernel matrices is closely associated with the input data scale. To reduce calculation and storage of large matrices, we extend KNMF by introducing partition matrix theory. The decomposition results of data matrices are derived from smaller scale matrices incrementally. Experiments are conducted on synthetic hyperspectral images with multiple sizes, and the experimental results show that the proposed algorithm have effect in saving calculation and memory resource without degrading the unmixing performance.	algorithm;experiment;kernel (operating system);non-negative matrix factorization;nonlinear system;synthetic intelligence	Risheng Huang;Xiaorun Li;Liaoying Zhao	2016	2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2016.7730716	mathematical optimization;kernel;sparse matrix;hyperspectral imaging;machine learning;pattern recognition;mathematics;matrix decomposition;kernel;correlation;remote sensing;memory management	AI	27.312991990895245	-39.66273460402537	161623
080c26bfa8754525278bb62eb3cc32f35d645e76	fast pixel/part selection with sparse eigenvectors	eigenvalues and eigenfunctions;fast pixel part selection;backward elimination stage;bayesian classification;image resolution;state of the art recognition;marginal likelihood;state of the art recognition fast pixel part selection sparse eigenvectors backward elimination stage gender classification feret data mnist data bayesian classification automatic relevance determination;bayes methods;gender classification;image classification;matrix inversion;sparse eigenvectors;automatic relevance determination;linear discriminant analysis principal component analysis input variables independent component analysis partitioning algorithms sparse matrices bayesian methods statistics computer vision eigenvalues and eigenfunctions;mnist data;feret data;image resolution bayes methods eigenvalues and eigenfunctions image classification;eigenvectors	"""We extend the """"Sparse LDA"""" algorithm of [7] with new sparsity bounds on 2-class separability and efficient partitioned matrix inverse techniques leading to 1000-fold speed-ups. This mitigates the 0(n4) scaling that has limited this algorithm's applicability to vision problems and also prioritizes the less-myopic backward elimination stage by making it faster than forward selection. Experiments include """"sparse eigenfaces"""" and gender classification on FERET data as well as pixel/part selection for OCR on MNIST data using Bayesian (GP) classification. Sparse- LDA is an attractive alternative to the more demanding Automatic Relevance Determination. State-of-the-art recognition is obtained while discarding the majority of pixels in all experiments. Our sparse models also show a better fit to data in terms of the """"evidence"""" or marginal likelihood."""	algorithm;eigenface;experiment;feret (facial recognition technology);image scaling;linear discriminant analysis;linear separability;local-density approximation;mnist database;marginal model;pixel;relevance;requirement prioritization;sparse matrix;stepwise regression	Bernard Moghaddam;Yair Weiss;Shai Avidan	2007	2007 IEEE 11th International Conference on Computer Vision	10.1109/ICCV.2007.4409093	contextual image classification;naive bayes classifier;image resolution;eigenvalues and eigenvectors;marginal likelihood;computer science;machine learning;pattern recognition;sparse approximation;statistics	Vision	28.11449588904458	-44.11403264283728	161759
c63b614865bd9e5b4944894083e5e9d4aba82d86	large scale similarity learning using similar pairs for person verification	large scale similarity learning;face verification;person re identification	In this paper, we propose a novel similarity measure and then introduce an efficient strategy to learn it by using only similar pairs for person verification. Unlike existing metric learning methods, we consider both the difference and commonness of an image pair to increase its discriminativeness. Under a pairconstrained Gaussian assumption, we show how to obtain the Gaussian priors (i.e., corresponding covariance matrices) of dissimilar pairs from those of similar pairs. The application of a log likelihood ratio makes the learning process simple and fast and thus scalable to large datasets. Additionally, our method is able to handle heterogeneous data well. Results on the challenging datasets of face verification (LFW and PubFig) and person re-identification (VIPeR) show that our algorithm outperforms the state-of-the-art methods.	algorithm;gaussian blur;scalability;similarity learning;similarity measure	Yang Yang;Shengcai Liao;Zhen Lei;Stan Z. Li	2016			machine learning;pattern recognition;data mining	AI	25.89167290286304	-44.26647980535227	161770
0234a8b7d43b22fb4f3998e768f668b38d362770	modeling appearances with low-rank svm	modeling appearances;support vector machines;coupled subspace analysis;spatial relations;matrix algebra;low rank matrices;discriminative learning stage;spatial relation;discrimination learning;image reconstruction;support vector machines image reconstruction matrix algebra;image measurements;low rank matrices modeling appearances low rank svm spatial relations image measurements coupled subspace analysis discriminative learning stage low rank separators;low rank separators;support vector machines pixel principal component analysis matrix decomposition computer science image analysis particle separators multidimensional systems algorithm design and analysis concatenated codes;low rank svm	"""Several authors have noticed that the common representation of images as vectors is sub-optimal. The process of vectorization eliminates spatial relations between some of the nearby image measurements and produces a vector of a dimension which is the product of the measurements' dimensions. It seems that images may be better represented when taking into account their structure as a 2D (or multi-D) array. Our work bears similarities to recent work such as 2DPCA or Coupled Subspace Analysis in that we treat images as 2D arrays. The main difference, however, is that unlike previous work which separated representation from the discriminative learning stage, we achieve both by the same method. Our framework, """"low-rank separators """", studies the use of a separating hyperplane which are constrained to have the structure of low-rank matrices. We first prove that the low-rank constraint provides preferable generalization properties. We then define two """"low-rank SVM problems"""" and propose algorithms to solve these. Finally, we provide supporting experimental evidence for the framework."""	algorithm;array data structure;automatic vectorization;boosting (machine learning);feature selection;low-rank approximation;mathematical optimization;synthetic data;the matrix;vc dimension	Lior Wolf;Hueihan Jhuang;Tamir Hazan	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383099	spatial relation;combinatorics;computer science;machine learning;pattern recognition;mathematics	Vision	27.35466194313772	-43.06153713277658	163244
cdc3f7a123b7adb9b5bbe3740915e2e6755f561a	multi-layer age regression for face age estimation		Face features convey many personal information that promote and regulate our social linkages. Age prediction using single layer estimation such as aging subspace or a hybrid pattern is limited due to the complexity of human faces. In this work, we propose Multilayer Age Regression (MAR) where the face age is predicted based on a coarse-to-fine estimation using global and local features. In the first layer, Support Vector Regression (SVR) performs a between group prediction by the parameters of Facial Appearance Model (FAM). In the second layer, a within group estimation is performed using FAM, Bio-Inspired Features (BIF), Kernel-based Local Binary Patterns (KLBP) and Multi-scale Wrinkle Patterns (MWP). The performance of MAR is assessed on four benchmark datasets: FGNET, MORPH, FERET and PAL. Results showed that MAR outperforms the state of the art on FERET with a Mean Absolute Error (MAE) of 3.00 (±4.14).	algorithm;benchmark (computing);computation;feret (facial recognition technology);fuzzy associative matrix;local binary patterns;pal;personally identifiable information;sudoku solving algorithms;support vector machine;the diamond age;time complexity	Choon-Ching Ng;Yi-Tseng Cheng;Gee-Sern Hsu;Moi Hoon Yap	2017	2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)	10.23919/MVA.2017.7986859	support vector machine;artificial intelligence;kernel (linear algebra);local binary patterns;computer vision;pattern recognition;mathematics;mean absolute error;statistics;active appearance model;subspace topology;age regression;feret	Vision	26.583274272996757	-42.72954343458547	163440
e273fb43ca209a33668ce389c929db42b74b0968	uncorrelated discriminant nearest feature line analysis for face recognition	nearest feature line nfl;uncorrelated constraint face recognition feature extraction nearest feature line nfl;subspace learning;uncorrelated constraint;face recognition feature extraction signal processing algorithms learning systems face detection computer vision principal component analysis linear discriminant analysis algorithm design and analysis pattern analysis;face databases;computer vision;learning systems;face recognition;geometrical information;feature extraction;principal component analysis;subspace learning method;learning artificial intelligence face recognition feature extraction;pattern analysis;learning artificial intelligence;face databases uncorrelated discriminant nearest feature line analysis face recognition subspace learning method geometrical information feature extraction;face detection;signal processing algorithms;linear discriminant analysis;uncorrelated discriminant nearest feature line analysis;algorithm design and analysis;nearest feature line	We propose in this letter a new subspace learning method, called uncorrelated discriminant nearest feature line analysis (UDNFLA), for face recognition. Motivated by the fact that existing nearest feature line (NFL) can effectively characterize the geometrical information of face samples, and uncorrelated features are desirable for many pattern analysis applications, we propose using the NFL metric to seek a feature subspace such that the within-class feature line (FL) distances are minimized and between-class FL distances are maximized simultaneously in the reduced subspace, and impose an uncorrelated constraint to make the extracted features statistically uncorrelated. Experimental results on two widely used face databases demonstrate the efficacy of the proposed method.	database;discriminant;discriminative model;feret (facial recognition technology);facial recognition system;feature vector;nfl;pattern recognition	Jiwen Lu;Yap-Peng Tan	2010	IEEE Signal Processing Letters	10.1109/LSP.2009.2035017	algorithm design;computer vision;face detection;speech recognition;feature extraction;computer science;machine learning;pattern recognition;linear discriminant analysis;principal component analysis	Vision	27.52401809272684	-42.82233246299152	163742
812f6682458b1b0cd4c460968f3422664e1572d8	spectral and spatial classification of hyperspectral images based on ica and reduced morphological attribute profiles	feature extraction hyperspectral imaging data mining matrix decomposition training spectral analysis;training;state of the art techniques hyperspectral image spectral classification hyperspectral image spatial classification reduced morphological attribute profiles spectral resolutions spatial resolutions land cover classification supervised hyperspectral image classification feature reduction strategy independent component analysis reconstruction error contextual information fusion hyperspectral data sets;data mining;supervised classification dimensionality reduction hyperspectral images independent component analysis ica mathematical morphology mm reduced attribute profiles raps remote sensing rs;matrix decomposition;feature extraction;land cover feature extraction geophysical image processing geophysical techniques hyperspectral imaging image classification image fusion;spectral analysis;hyperspectral imaging	The availability of hyperspectral images with improved spectral and spatial resolutions provides the opportunity to obtain accurate land-cover classification. In this paper, a novel methodology that combines spectral and spatial information for supervised hyperspectral image classification is proposed. A feature reduction strategy based on independent component analysis is the main core of the spectral analysis, where the exploitation of prior information coupled to the evaluation of the reconstruction error assures the identification of the best class-informative subset of independent components. Reduced attribute profiles (APs), which are designed to address well-known issues related to information redundancy that affect the common morphological APs, are then employed for the modeling and fusion of the contextual information. Four real hyperspectral data sets, which are characterized by different spectral and spatial resolutions with a variety of scene typologies (urban, agriculture areas), have been used for assessing the accuracy and generalization capabilities of the proposed methodology. The obtained results demonstrate the classification effectiveness of the proposed approach in all different scene typologies, with respect to other state-of-the-art techniques.	artificial intelligence;computer vision;database schema;dimensionality reduction;experiment;feature extraction;feature model;feature vector;genetic algorithm;icl distributed array processor;independent computing architecture;independent component analysis;information;loss function;maxima and minima;reduction strategy (lambda calculus);redundancy (information theory);scene graph;spectrum analyzer;supervised learning;thinning	Nicola Falco;Jon Atli Benediktsson;Lorenzo Bruzzone	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2015.2436335	full spectral imaging;computer vision;feature extraction;hyperspectral imaging;machine learning;pattern recognition;matrix decomposition;remote sensing	Vision	30.965344289797795	-44.30961256836198	163945
439483ba272e574fc834fbd2fded7e381392b5b2	on affinity matrix normalization for graph cuts and spectral clustering	feature space;spectral clustering;kernel k means;affinity matrix	Graph-based spectral clustering algorithms involve the analysis of an affinity matrix. The latter defines the pairwise similarity relations among data points. Popular graph partitioning algorithms typically involve a normalization step that reflects itself onto an affinity matrix normalization step in spectral clustering algorithms. In this paper, we show that affinity matrix normalization with constant row/column sum guarantees the invariance of the size-weighted sum of the betweenand within-cluster graph association; a property conceptually equivalent to the data variance decomposition exploited by the standard k-means algorithm. From this observation, we demonstrate that the solution of numerous spectral clustering methods can be obtained using the standard graph ratio cut objective function. We have identified in the literature 7 such affinity matrix normalization schemes relevant to spectral clustering. Clustering experiments performed with these 7 normalization schemes on 17 benchmark datasets are presented. As a general rule, it is observed that the appropriate normalization method depends on the dataset. A geometric interpretation in the feature space (FS) of such a normalization scheme for k-way spectral clustering is also presented. Crown Copyright © 2015 Published by Elsevier B.V. All rights reserved.	affinity analysis;algorithm;benchmark (computing);cluster analysis;crown group;cut (graph theory);data point;experiment;feature vector;graph partition;k-means clustering;loss function;optimization problem;processor affinity;spectral clustering;weight function	Mario Beauchemin	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.08.020	graph energy;correlation clustering;combinatorics;feature vector;fuzzy clustering;computer science;machine learning;pattern recognition;mathematics;cluster analysis;spectral clustering;affinity propagation;adjacency matrix	AI	27.072084076203485	-38.91429003743813	164960
27669571f9f5540627023a6ca1c105d41d909507	detection of correlated co-clusters in tensor data based on the slice-wise factorization		As the extension of matrices, tensors are very powerful tools to model the heterogeneous multidimensional arrays in applications. Similar to bi-clustering in matrices, co-clustering can simultaneously extract the coherent patterns along all modes of tensors. In recent years, various techniques have been developed to identify co-clusters in tensor. In this paper, a novel co-clustering method based on the slice-wise full rank factorization (SFRF) is proposed. We first develop the workhorse alternating least squares (ALS) with the flexible constraint of the full rank decomposition of sliced matrices. The computation time can be greatly reduced with the compression technique based on matrix singular value decomposition (SVD). Next it is proven that the original identification of the co-clusters can be transformed to the linear grouping in the row space of every factor matrix. We use a linear grouping algorithm (LGA) to detect the geometrical patterns of the points in the corresponding row space. Finally the combination of the point index on the hyperplanes successfully supports the detection of the co-clusters in a tensor. This paper attempts to provide a flexible and fast algorithm to identify the co-clusters of tensor data based on the full rank tensor decomposition. Extensive simulations provide the empirical evidence of validity and efficiency of the proposed algorithm.	algorithm;biclustering;cluster analysis;coherence (physics);collaborative product development;computation;computational complexity theory;experiment;land grid array;least squares;negativity (quantum mechanics);simulation;singular value decomposition;time complexity	Hongya Zhao;Zhenghong Wei;Hong Yan	2017	2017 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2017.8107762	artificial intelligence;tensor;mathematical optimization;rank (linear algebra);computer science;pattern recognition;stress (mechanics);factorization;matrix decomposition;singular value decomposition;matrix (mathematics);least squares	ML	27.837404513074762	-39.405198511008614	166444
680823be9ba5343968b58d68206346ea4602ef12	a convergence analysis of distributed dictionary learning based on the k-svd algorithm	topology;convergence;dictionaries encoding algorithm design and analysis distributed databases convergence sparse matrices topology;dictionaries;distributed databases;singular value decomposition big data cloud computing convergence distributed algorithms learning artificial intelligence;encoding;interconnection topology convergence analysis distributed dictionary learning distributed algorithm cloud k svd algorithm data adaptive representation problem big distributed data geographically distributed interconnected sites local data collaborative learning dictionary sparsifying global data;sparse matrices;algorithm design and analysis	This paper provides a convergence analysis of a recent distributed algorithm, termed cloud K-SVD, that solves the problem of data-adaptive representations for big, distributed data. It is assumed that a number of geographically-distributed, interconnected sites have massive local data and they are collaboratively learning a sparsifying dictionary underlying these data using cloud K-SVD. This paper provides a rigorous analysis of cloud K-SVD that gives insights into its properties as well as deviations of the dictionaries learned at individual sites from a centralized solution in terms of different measures of local/global data and topology of the interconnections.	centralized computing;converge;dictionary;distributed algorithm;k-svd;machine learning;power iteration;singular value decomposition	Haroon Raja;Waheed Uz Zaman Bajwa	2015	2015 IEEE International Symposium on Information Theory (ISIT)	10.1109/ISIT.2015.7282843	algorithm design;distributed algorithm;convergence;sparse matrix;k-svd;computer science;theoretical computer science;machine learning;data mining;mathematics;distributed database;encoding	Theory	28.329636948821737	-39.51689405354605	166768
6fe25d6532763715c45bf9b2b3ef58735bfea23c	incremental learning of locally orthogonal subspaces for set-based object recognition	optimal solution;object recognition;high dimensionality;incremental learning;canonical correlation analysis;principal component	Orthogonal subspaces are effective models to represent object image sets (generally any high-dimensional vector sets). Canonical correlation analysis of the orthogonal subspaces provides a good solution to discriminate objects with sets of images. In such a recognition task involving image sets, an efficient learning over a large volume of image sets, which may be increasing over time, is important. In this paper, an incremental learning method of orthogonal subspaces is proposed by updating the principal components of the class correlation and total correlation matrices separately, yielding the same solution as the batch computation with far lower computational cost. A novel concept of local orthogonality is further proposed to cope with non-linear manifolds of data vectors and find a more optimal solution of orthogonal subspaces for a certain neighbouring object image sets. In the experiments using 700 face image sets, the locally orthogonal subspaces outperformed the orthogonal subspaces as well as relevant state-of-the-art methods in accuracy. Note that the locally orthogonal subspaces are also amenable to incremental updating due to their linear property.	computation;computational complexity theory;disk image;experiment;nonlinear system;outline of object recognition;total correlation	Tae-Kyun Kim;Josef Kittler;Roberto Cipolla	2006		10.5244/C.20.58	canonical correlation;combinatorics;discrete mathematics;computer science;cognitive neuroscience of visual object recognition;machine learning;mathematics;statistics;principal component analysis	Vision	26.532730607659467	-43.06761146578838	166987
cc44552e18bd7f18b2bdf4cddda83bde78cb0ad7	a kernel-based l2 norm regularized least square algorithm for vehicle logo recognition	adaptive online dictionary kernel technique l 2 norm regularized least square algorithm;vehicles dictionaries signal processing algorithms kernel conferences digital signal processing atomic measurements;smoothing methods computer vision edge detection intelligent transportation systems learning artificial intelligence least squares approximations;adaptive online dictionary kernel based l2 norm regularized least square algorithm rls algorithm kernel technique vehicle logo recognition vlr	We consider the problem of automatically recognizing the vehicle logos from the frontal views with varying illumination, as well as certain corruption. To better address the problem, a kernel-based l2 norm regularized least square (RLS) algorithm is proposed in the paper. Kernel technique is smoothly combined with the l2 norm RLS algorithm to enhance the performance of vehicle logo recognition (VLR). As an extension, the improvement of dictionary is also considered. A simple mechanism of constructing an adaptive online dictionary has been presented and experimented. Experimental results show that our proposed algorithm outperforms the original l2 norm RLS algorithm and the l1 norm based algorithms.	algorithm;computation;computer simulation;dictionary;entity–relationship model;kernel (operating system);lu decomposition;logo;recursive least squares filter;smoothing;sparse approximation;sparse matrix;taxicab geometry	Weiyang Liu;Yandong Wen;Kai Pan;Hui Li;Yuexian Zou	2014	2014 19th International Conference on Digital Signal Processing	10.1109/ICDSP.2014.6900742	speech recognition;kernel embedding of distributions;radial basis function kernel;kernel adaptive filter;computer science;machine learning;pattern recognition	Vision	30.579218128274086	-39.85115901849262	167351
a47f44f0ae8bb1e5be3e51f9e9df8c9f4f3e23e9	the effect of bant selection to success of artificial neural network in hyperspectral classification		In recent years, clasification with hyperspectral images is becoming very popular. Development of camera technology is increasing number of researchers who work in this area. Thanks to hyperspectral imaging technology, specific spectral signatures of objects can be handled. Especially, vegetation clasification is possible by using the spectral information of hyperspectral images. However due to the change of spectral signatures of vegetations by time and existence of vegetation which exhibit similar spectral properties, finding a generic clasification method that can work time independent is difficult. In this study, a neural network-based method which uses bant selected training data is proposed in order to classify corn and cotton vegetation in the land cover. In this study, the effect of band selection for training data set on the success of neural network method is investigated. Results showed that training network with band-selected data is increasing the classification success and makes classification process time-independent.	antivirus software;artificial neural network;imaging technology;test set;type signature	Ismail Karakaya;Yucel Cimtay	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960553	computer science;vegetation;artificial intelligence;computer vision;pattern recognition;land cover;artificial neural network;training set;hyperspectral imaging;machine learning;spectral signature	ML	30.68382261954917	-44.928098399762064	167527
ed932e978a3dc5f8c077ee0d8a97351eaf52da72	incremental manifold learning algorithm using pca on overlapping local neighborhoods for dimensionality reduction	high dimensionality;manifold learning;topological properties;local pca;large scale;dimensionality reduction;least squares fit;incremental learning;least squares problem;least square;synthetic data;linear space;dimensional reduction;divide and conquer	A novel manifold learning algorithm called LPcaML is proposed in this paper. Based on the geometric intuition that d-dimensional manifold locally lies on or close to d-dimensional linear space, LPcaML first finds an  ***  -TSLN of the whole high-dimensional input data set and then obtains the low-dimensional local coordinates of each neighborhood in the  ***  -TSLN using classical PCA technique while preserving the local geometric and topological property of each neighborhood. At last LPcaML transforms each local coordinates to a unified global low-dimensional representation by processing each neighborhood in their order appeared in  ***  -TSLN. And the transformation function of each neighborhood is obtained by solving a least square problem via the overlapped examples. By using the divide and conquer strategy, LPcaML can learn from incremental data and discover the underlying manifold efficiently even if the data set is large scale. Experiments on both synthetic data sets and real face data sets demonstrate the effectiveness of our LPcaML algorithm. Moreover the proposed LPcaML can discover the manifold from sparsely sampled data sets where other manifold learning algorithms can't.	algorithm;algorithmic learning theory;dimensionality reduction	Yubin Zhan;Jianping Yin;Guomin Zhang;En Zhu	2008		10.1007/978-3-540-92137-0_45	local tangent space alignment;mathematical optimization;machine learning;pattern recognition;mathematics;manifold alignment	ML	27.051974035817132	-40.241193461094305	167708
ce46dc42257cd2c8d09192ca51eb831704ebf87a	on the equivalence between algorithms for non-negative matrix factorization and latent dirichlet allocation.				Thiago de Paulo Faleiros;Alneu de Andrade Lopes	2016			equivalence (measure theory);latent dirichlet allocation;pattern recognition;artificial intelligence;matrix decomposition;computer science;mathematical optimization;non-negative matrix factorization	AI	29.48585310041419	-39.514581650600384	167798
82b586ef1296dccc013fcc031aa93dc6aec6764e	globally-preserving based locally linear embedding	image sampling;eigenvalues and eigenfunctions;image recognition;locally linear embedding;manifolds training principal component analysis eigenvalues and eigenfunctions estimation image recognition laplace equations;manifolds;training;manifold learning;power method;globally preserving;local neighborhood;embedded systems;laplace equations;dimensionality estimation dimensionality reduction manifold learning globally preserving locally linear;dimensionality reduction;estimation;nonlinear dimensionality reduction;principal component analysis;locally linear;pattern classification;image sampling locally linear embedding nonlinear dimensionality reduction globally preserving based lle algorithm local neighborhood;dimensional reduction;local linear embedding;globally preserving based lle algorithm;pattern classification embedded systems;dimensionality estimation	The locally linear embedding (LLE) algorithm is considered as a powerful method for the problem of nonlinear dimensionality reduction. In this paper, a new method called globally-preserving based LLE (GPLLE) is proposed. It not only preserves the local neighborhood, but also keeps those distant samples still far away, which solves the problem that LLE may encounter, i.e. LLE only makes local neighborhood preserving, but can’t prevent the distant samples from nearing. Moreover, GPLLE can estimate the intrinsic dimensionality d of the manifold structure. The experiment results show that GPLLE always achieves better classification performances than LLE based on the estimated d.	algorithm;arc diagram;intrinsic dimension;intrinsic function;nonlinear dimensionality reduction;nonlinear system;performance	Kanghua Hui;Chunheng Wang;Baihua Xiao	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.135	mathematical optimization;topology;computer science;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction	Vision	27.509592533635832	-40.79993939337997	168183
31f2f7336e30de9170453f8480d06401fe2a797e	face image modeling by multilinear subspace analysis with missing values	higher order;face recognition;facial age estimation;multilinear subspace analysis;missing values;face image;image modeling;multilinear subspace analysis msa	The main difficulty in face image modeling is to decompose those semantic factors contributing to the formation of the face images, such as identity, illumination and pose. One promising way is to organize the face images in a higher-order tensor with each mode corresponding to one contributory factor. Then, a technique called Multilinear Subspace Analysis (MSA) is applied to decompose the tensor into the mode-$n$ product of several mode matrices, each of which represents one semantic factor. In practice, however, it is usually difficult to obtain such a complete training tensor since it requires a large amount of face images with all possible combinations of the states of the contributory factors. To solve the problem, this paper proposes a method named M$^2$SA, which can work on the training tensor with massive missing values. Thus M$^2$SA can be used to model face images even when there are only a small number of face images with limited variations which will cause missing values in the training tensor). Experiments on face recognition show that M$^2$SA can work reasonably well with up to $70\%$ missing values in the training tensor.	experiment;facial recognition system;illumination (image);missing data;multilinear subspace learning	Xin Geng;Kate Smith-Miles;Zhi-Hua Zhou;Liang Wang	2009		10.1145/1631272.1631373	facial recognition system;computer vision;multilinear principal component analysis;higher-order logic;computer science;machine learning;pattern recognition;multilinear subspace learning	Vision	26.442062290974135	-43.978678364712636	169322
4cb05da4dd30e01c9b1197032d0ca36fc18d7cd4	crim-fcho: sar image two-stage segmentation with multifeature ensemble	synthetic aperture radar sar image fuzzy clustering image segmentation multifeature ensemble region merging similarity measures;image segmentation;feature extraction gradient methods image fusion image segmentation optimisation radar imaging synthetic aperture radar;synthetic aperture radar image segmentation feature extraction brightness merging algorithm design and analysis clustering algorithms;fuzzy clustering image segmentation multifeature ensemble region merging similarity measures synthetic aperture radar sar image;brightness;gradient steepest descent high dimension feature space hybrid optimization fuzzy clustering algorithm image domain context based region iterative merging algorithm fine classification stage coarse merging stage similarity measure discriminability heterogeneous feature extraction feature fusion synthetic aperture radar image segmentation multifeature ensemble sar image two stage segmentation crim fcho;feature extraction;crim fcho gradient steepest descent high dimension feature space hybrid optimization fuzzy clustering algorithm image domain context based region iterative merging algorithm fine classification stage coarse merging stage similarity measure discriminability heterogeneous feature extraction feature fusion synthetic aperture radar image segmentation multifeature ensemble sar image two stage segmentation;merging;clustering algorithms;synthetic aperture radar feature extraction gradient methods image fusion image segmentation optimisation radar imaging;algorithm design and analysis;synthetic aperture radar	This paper investigates the synthetic aperture radar (SAR) image segmentation in terms of feature analysis and fusion and develops a new algorithm based on multifeature ensemble accordingly. This paper is characterized by two aspects. First, multiple heterogeneous features are extracted to accurately describe the objects in SAR images. These features are then integrated in the feature level and the similarity level, respectively, to avoid the mutual influences between different kinds of features and maximize the discriminability of the similarity measure between objects. Second, a two-stage algorithm consisting of a coarse merging stage and a fine classification stage is proposed. In the coarse merging stage, a context-based region iterative merging algorithm is designed to merge most of the unambiguous superpixels in image domain at a high speed. In the fine classification stage, a fuzzy clustering algorithm incorporating hybrid optimization is developed to balance the efficiency and the robustness of the algorithm by simultaneously searching heuristically in the complete high-dimension feature space and searching along the direction of the gradient steepest descent in each feature subspace. The effectiveness of the proposed method has been successfully validated on synthetic and real SAR images.	algorithm;algorithm design;algorithmic efficiency;aperture (software);chief security officer;cluster analysis;computation;ensemble kalman filter;ensemble forecasting;experiment;feature vector;fuzzy clustering;gradient descent;heuristic;image segmentation;iterative method;loss function;mathematical optimization;optimization problem;pixel;preprocessor;similarity measure;synthetic data;synthetic intelligence;time complexity;with high probability	Hang Yu;Licheng Jiao;Fang Liu	2016	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2015.2501162	algorithm design;computer vision;feature detection;synthetic aperture radar;feature extraction;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;cluster analysis;scale-space segmentation;brightness	Vision	30.876208495909225	-44.29918553185461	169892
cc25841c5fbabaaaf34e044dba575014680d70a1	comparative study of dimensionality reduction methods for remote sensing images interpretation		Hyperspectral imagery is widely used for the identification and monitoring of earth surface, which in turn need good classification performances. However, the high spectral dimensionality of hyperspectral images degrades classification accuracy and increases computational complexity. To overcome these issues, dimensionality reduction has become an essential preprocessing step in order to enhance classifiers performances using hyperspectral images. Dimensionality reduction tackles the problem of the high dimensionality, but also the high correlation between the spectral bands of hyperspectral images. In this paper, we first review the main dimensionality reduction approaches and compare their performances when used for the classification task using the Support Vector Machines classifier. We also propose a combination of feature extraction and band selection for classification. We report the performances of all these methods using real hyperspectral images and show their efficiency for hyperspectral image classification.	computational complexity theory;computer vision;dimensionality reduction;experiment;feature extraction;horizontal situation indicator;kernel principal component analysis;nonlinear system;performance;preprocessor;support vector machine	Akrem Sellami;Mohamed Farah	2018	2018 4th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP)	10.1109/ATSIP.2018.8364490	support vector machine;spectral bands;feature extraction;computational complexity theory;dimensionality reduction;hyperspectral imaging;contextual image classification;curse of dimensionality;computer science;artificial intelligence;pattern recognition	Robotics	30.25790616436497	-43.16317857784113	170274
bae412cf960f854ec1f2d4fe11b38423460edab9	learning a multiple kernel similarity metric for kinship verification		Abstract Kinship Verification (KV) has recently caught much attention in the computer vision community due to its potential applications ranging from missing children search to social media analysis. Most of the related work focuses either on developing hand-crafted feature representations to describe the faces or on learning the Mahalanobis distance metric to measure the similarity between facial images. Instead, in this paper, we propose a novel Multiple Kernel Similarity Metric (MKSM), in which, different from the Mahalanobis metric, the similarity computation is essentially based on an implicit nonlinear feature transformation. The overall MKSM is a weighted combination of basic similarities and therefore possesses the capacity for feature fusion. The basic similarities are derived from base kernels and local features, and the weights are obtained by solving a constrained linear programming (LP) problem that originates from a Large margin (LM) criterion. Particularly, the LM criterion not only guarantees the generalization on unseen samples when the training set is small, but also leads to sparsity in the weight vector which in turn boosts the efficiency at the prediction stage. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed method.	kernel (operating system)	Yanguo Zhao;Zhan Song;Feng Zheng;Ling Shao	2018	Inf. Sci.	10.1016/j.ins.2017.11.048	kernel (linear algebra);polynomial kernel;mathematics;artificial intelligence;discrete mathematics;mahalanobis distance;machine learning;ranging;weight;nonlinear system;linear programming;pattern recognition;radial basis function kernel	AI	25.12870084554177	-42.229954911050726	171124
43231db041797822aafeee83b8e3ad2852a0b23e	active learning for classification of remote sensing images	remote sensing image;geophysical image processing;model selection;machine learning remote sensing;support vector machines geophysical image processing image classification learning artificial intelligence remote sensing;support vector machines;active learning;image classification;machine learning remote sensing automatic classification semisupervised learning active learning;machine learning;very high resolution;remote sensing;uncertainty estimation;machine learning remote sensing images active learning techniques image classification support vector machines stochastic analysis unlabeled samples high resolution image automatic classification semisupervised learning;support vector machine;learning artificial intelligence;automatic classification;remote sensing machine learning support vector machines support vector machine classification uncertainty labeling image analysis computer science electronic mail learning systems;semisupervised learning	This paper presents an analysis of active learning techniques for the classification of remote sensing images and proposes a novel active learning method based on support vector machines (SVMs). The proposed method exploits a query function for the inclusion of batches of unlabeled samples in the training set, which is based on the evaluation of two criteria: uncertainty and diversity. This query function adopts a stochastic approach to the selection of unlabeled samples, which is based on a function of uncertainty estimated from the distribution of errors on the validation set (which is assumed available for the model selection of the SVM classifier). Experimental results carried out on a very high resolution image confirm the effectiveness of the proposed active learning technique, which results more accurate than standard methods.	active learning (machine learning);image resolution;model selection;support vector machine;test set	Lorenzo Bruzzone;Claudio Persello	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5417857	semi-supervised learning;support vector machine;computer vision;computer science;machine learning;pattern recognition;active learning	Robotics	31.090333627154468	-43.29514838557695	171185
4ab0d77862353cfc1c5d39d4a85fc12c5764843b	hyperspectral image classification with robust sparse representation	matching pursuit algorithms;training;image representation hyperspectral imaging image classification;dictionaries;robustness;optimization;hyperspectral imaging;jrsrc hyperspectral image classification robust sparse representation sparse representation based classification joint robust sparsity model;sparse representation based classification src hyperspectral classification joint rsrc jrsrc outliers robust src rsrc;training hyperspectral imaging robustness matching pursuit algorithms optimization	Recently, the sparse representation-based classification (SRC) methods have been successfully used for the classification of hyperspectral imagery, which relies on the underlying assumption that a hyperspectral pixel can be sparsely represented by a linear combination of a few training samples among the whole training dictionary. However, the SRC-based methods ignore the sparse representation residuals (i.e., outliers), which may make the SRC not robust for outliers in practice. To overcome this problem, we propose a robust SRC (RSRC) method which can handle outliers. Moreover, we extend the RSRC to the joint robust sparsity model named JRSRC, where pixels in a small neighborhood around the test pixel are simultaneously represented by linear combinations of a few training samples and outliers. The JRSRC can also deal with outliers in hyperspectral classification. Experiments on real hyperspectral images demonstrate that the proposed RSC and JRSRC have better performances than the orthogonal matching pursuit (OMP) and simultaneous OMP, respectively. Moreover, the JRSRC outperforms some other popular classifiers.	convolutional code;dictionary;horizontal situation indicator;matching pursuit;openmp;performance;pixel;sample rate conversion;sparse approximation;sparse matrix	Chang Li;Yong Ma;Xiaoguang Mei;Chengyin Liu;Jiayi Ma	2016	IEEE Geoscience and Remote Sensing Letters	10.1109/LGRS.2016.2532380	computer vision;computer science;hyperspectral imaging;machine learning;pattern recognition;robustness;remote sensing	Vision	29.524834966834998	-43.816205398206364	171716
30d3caf5c57bef0e7f8376c7a2bf66dd3ec3692f	the dynamical neighborhood selection based on the sampling density and manifold curvature for isometric data embedding	tangent space;protection information;learning algorithm;coeficiente correlacion;learning;echantillonnage;isometrie;data embedding;isomap algorithm;sampling density;metodo subespacio;manifold learning;algorithme apprentissage;dynamical neighborhood;journal;algorithme isomap;geodesic distance;methode sous espace;sampling;aprendizaje;geodesique;apprentissage;isometria;proteccion informacion;geodesic;information protection;geodesico;subspace method;manifold curvature;synthetic data;muestreo;correlation coefficient;algoritmo aprendizaje;isometry;coefficient correlation	0167-8655/$ see front matter 2010 Elsevier B.V. A doi:10.1016/j.patrec.2010.08.005 ⇑ Corresponding author at: College of Computer a Shanxi University, Shanxi, Taiyuan 030006, China. E-mail addresses: gxfhtp@mail.sxu.cn (X. Gao), ljy@ The construction of the neighborhood is a critical problem of manifold learning. Most of manifold learning algorithms use a stable neighborhood parameter (such as k-NN), but it may not work well for the entire manifold, since manifold curvature and sampling density may vary over the manifold. Although some dynamical neighborhood algorithms have been proposed, they are limited by either another global parameter or an assumption. This paper proposes a new approach to select the dynamical neighborhood for each point while constructing the tangent subspace based on the sampling density and the manifold curvature. And the parameters of the approach can be automatically determined by computing the correlation coefficient of the matrices of geodesic distances between pairs of points in input and output spaces. When we apply it to ISOMAP, the results of experiments on the synthetic data as well as the real world patterns demonstrate that the proposed approach can efficiently maintain an accurate low dimensional representation of the manifold data with less distortion, and give higher average classification rate compared to others. 2010 Elsevier B.V. All rights reserved.	coefficient;data visualization;distortion;emoticon;experiment;input/output;isomap;isometric projection;k-nearest neighbors algorithm;machine learning;nonlinear dimensionality reduction;sampling (signal processing);synthetic data;time complexity	Xiaofang Gao;Jiye Liang	2011	Pattern Recognition Letters	10.1016/j.patrec.2010.08.005	center manifold;local tangent space alignment;statistical manifold;mathematical optimization;geodesic;stable manifold theorem;topology;atlas;mathematics;geometry;nonlinear dimensionality reduction;simplicial manifold;stable manifold;periodic point;pseudo-riemannian manifold;manifold alignment	ML	29.64207803478562	-38.41725862058064	171863
8dcb61952032fb4778316cbe52645c7ff43e61af	forward basis selection for pursuing sparse representations over a dictionary	gaussian graphical models;image segmentation;convex programming;gaussian processes;prefixed dictionary;greedy algorithms;coordinate wise sparse learning problems;matrix algebra;convex sparse representation setup;learning nonnegative setup;low rank subspace segmentation;iterative methods;data analysis;forward greedy selection algorithm;high dimensional data analysis;sparse precision matrix estimation;subspace segmentation;image representation;dictionaries;objective function minimization;greedy selection;optimization;forward basis selection;learning artificial intelligence;pursuing sparse representations;sparse representation;sparse matrices	The forward greedy selection algorithm of Frank and Wolfe has recently been applied with success to coordinate-wise sparse learning problems, characterized by a tradeoff between sparsity and accuracy. In this paper, we generalize this method to the setup of pursuing sparse representations over a prefixed dictionary. Our proposed algorithm iteratively selects an atom from the dictionary and minimizes the objective function over the linear combinations of all the selected atoms. The rate of convergence of this greedy selection procedure is analyzed. Furthermore, we extend the algorithm to the setup of learning nonnegative and convex sparse representation over a dictionary. Applications of the proposed algorithms to sparse precision matrix estimation and low-rank subspace segmentation are investigated with efficiency and effectiveness validated on benchmark datasets.	atom;base;benchmark (computing);computation;converge;dictionary [publication type];experiment;function-behaviour-structure ontology;genetic selection;gradient;greedy algorithm;iteration;iterative method;loss function;low-rank approximation;mathematical optimization;optimization problem;overhead (computing);preparation;rate of convergence;relaxation;selection algorithm;sparse approximation;sparse matrix;weight;biologic segmentation	Xiao-Tong Yuan;Shuicheng Yan	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.85	mathematical optimization;greedy algorithm;convex optimization;sparse matrix;k-svd;computer science;machine learning;pattern recognition;sparse approximation;gaussian process;mathematics;iterative method;image segmentation;data analysis	ML	25.950244058608394	-39.304264823352185	172748
89a8980e8b67a19d897e707fae8786d2cc0a3994	dictionary learning based on laplacian score in sparse coding	unsupervised learning;clustering;laplacian score;dictionary learning;sparse coding	Sparse coding, which is represented a vector based on sparse linear combination of a dictionary, is widely applied on signal processing, data mining and neuroscience. How to get a proper dictionary is a problem, which is data dependent and computational cost. In this paper, we treat dictionary learning in the unsupervised learning view and proposed Laplacian score dictionary (LSD). This new method uses local geometry information to select atoms for dictionary. Comparison experiments with competitive clustering based dictionary learning methods are established. We also compare LSD with full-training-data-dictionary and others classic methods in the experiments. The results on binary classes datasets and multi class datasets from UCI repository demonstrate the effectiveness and efficiency of our method.	cluster analysis;computational complexity theory;data dictionary;data mining;experiment;machine learning;neural coding;signal processing;sparse matrix;unsupervised learning	Jin Xu;Hong Man	2011		10.1007/978-3-642-23199-5_19	unsupervised learning;speech recognition;k-svd;computer science;machine learning;pattern recognition;cluster analysis;neural coding	ML	24.64955132000973	-42.3832628129596	173245
f2984733c33dfcbec0cf403b3bf581f000e4be22	a maximum entropy approach to pairwise data clustering	data partitionings;maximum entropy methods;cost function;np hard combinatorial optimization problem;combinatorial optimization problem;d dimensional euclidian space;psychology;noise measurement;data partitioning;data clustering;symmetric matrices;optimization problem;data analysis;embedding problem;noise reduction;data visualization;entropy;maximum entropy approach;extraterrestrial measurements;entropy extraterrestrial measurements embedded computing symmetric matrices cost function data analysis psychology noise reduction noise measurement data visualization;variational principle;maximum entropy;embedding problem maximum entropy approach pairwise data clustering np hard combinatorial optimization problem variational principle data partitionings d dimensional euclidian space;embedded computing;pairwise data clustering	Partitioning a set of data points which are characterized by their mutual dissimilarities instead of an explicit coordinate representation is a difficult, NP-hard combinatorial optimization problem. We formulate this optimization problem of a pairwise clustering cost function in the maximum entropy framework using a variational principle to derive corresponding data partitionings in a d-dimensional Euclidian space. This approximation solves the embedding problem and the grouping of these data into clusters simultaneously and in a selfconsistent fashion.	3d projection;approximation algorithm;best, worst and average case;calculus of variations;cluster analysis;combinatorial optimization;data point;distance matrix;loss function;mathematical optimization;maxima and minima;minimum description length;np-hardness;nonlinear system;optimization problem;pattern recognition;principal component analysis;principle of maximum entropy;sparse matrix;variational method (quantum mechanics);variational principle	Joachim M. Buhmann;Thomas Hofmann	1994		10.1109/ICPR.1994.576905	embedding problem;correlation clustering;optimization problem;entropy;mathematical optimization;combinatorics;variational principle;computer science;noise measurement;principle of maximum entropy;machine learning;noise reduction;mathematics;cluster analysis;data analysis;data visualization;statistics;symmetric matrix	ML	27.673815038844875	-39.498933759699206	173682
b3b98b6f84b5007e7e927bcac7ec5ffd6610ae91	a graph laplacian matrix learning method for fast implementation of graph fourier transform		In this paper, we propose an efficient graph learning approach for fast graph Fourier transform. We consider a maximum likelihood problem with additional constraints based on a matrix factorization of the graph Laplacian matrix, such that its eigenmatrix is a product of a block diagonal matrix and a butterfly-like matrix. We show that a special case of this problem reduces to a learning problem with constraints enforcing certain graph symmetries. Then, we provide an efficient approximation approach for the general problem without enforcing any symmetry constraint. We use this approach to design a fast nonseparable transform for intra predictive residual blocks in video compression. The resulting transform achieves a better rate-distortion performance than the 2D DCT and the hybrid DCT/ADST transform.	approximation;data compression;discrete cosine transform;distortion;laplacian matrix	Keng-Shih Lu;Antonio Ortega	2017	2017 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2017.8296567	algorithm;fourier transform;laplacian matrix;sparse matrix;discrete cosine transform;artificial intelligence;matrix (mathematics);data compression;block matrix;pattern recognition;matrix decomposition;computer science	Vision	27.086942902500866	-40.544858797446956	173767
5a98f0f05e4ebbd2e15b8d2fbc086e27b8501b61	asymmetric kernel in gaussian processes for learning target variance		This work incorporates the multi-modality of the data distribution into a Gaussian Process regression model. We approach the problem from a discriminative perspective by learning, jointly over the training data, the target space variance in the neighborhood of a certain sample through metric learning. We start by using data centers rather than all training samples. Subsequently, each center selects an individualized kernel metric. This enables each center to adjust the kernel space in its vicinity in correspondence with the topology of the targets — a multi-modal approach. We additionally add descriptiveness by allowing each center to learn a precision matrix. We demonstrate empirically the reliability of the model. © 2018 Elsevier B.V. All rights reserved. t G m m i n I f u d p s a e a e 2	data center;gaussian process;kernel (operating system);kriging;modal logic;modality (human–computer interaction);user space	Silvia L. Pintea;Jan C. van Gemert;Arnold W. M. Smeulders	2018	Pattern Recognition Letters	10.1016/j.patrec.2018.02.026	kernel (linear algebra);mathematics;artificial intelligence;discriminative model;pattern recognition;gaussian process;matrix (mathematics);kriging;training set	AI	27.358357477466715	-43.56870157183377	174073
7956a6815b57396d0a8021b9b49d678c491b66dd	discriminative multi-modality non-negative sparse graph model for action recognition	multi modality;sparse graph;shared coefficients;benchmark datasets discriminative multimodality nonnegative sparse graph model action recognition dmns graph model method mahalanobis space transformation learning shared coefficients labeled data propagation unlabeled sample data prediction;mahalanobis space;discriminative;discriminative sparse graph mahalanobis space multi modality shared coefficients;feature extraction measurement youtube vectors mathematical model sparse matrices training;object recognition graph theory	A discriminative multi-modality non-negative sparse (DMNS) graph model is proposed in this paper. In the model, features in each modality are first projected into the Mahalanobis space by a transformation learned for this modality, a multi-modality non-negative sparse graph is then constructed in the Mahalanobis space with shared coefficients across modalities. Both the labeled and unlabeled data can be introduced into the graph, and label propagation can then be performed to predict labels of the unlabeled samples. Extensive experiments over two benchmark datasets demonstrate the advantages of the proposed DMNS-graph method over the state-of-the-art methods.	benchmark (computing);coefficient;experiment;modality (human–computer interaction);software propagation;sparse graph code;sparse matrix	Yuanbo Chen;Yanyun Zhao;Bojin Zhuang;Anni Cai	2014	2014 IEEE Visual Communications and Image Processing Conference	10.1109/VCIP.2014.7051502	speech recognition;dense graph;computer science;machine learning;pattern recognition;mathematics;discriminative model	Vision	25.62002699032441	-44.77206398045673	174472
b45a6d33ec3ef34776792c3ae49966a9ff3d3cc6	automatic threshold selection for morphological attribute profiles	geophysical image processing;clustering attribute profiles threshold selection classification contextual information;attribute profiles;image classification;contextual information;classification;image classification geophysical image processing geophysical techniques;threshold selection;clustering;gray scale manuals hyperspectral imaging educational institutions vectors clustering algorithms;classification experiment automatic threshold selection morphological attribute profiles automatized procedure threshold informative values unsupervised classification clustering algorithm image scaling effect;geophysical techniques	In this article, an automatized procedure for selecting informative values of the thresholds, essential for the construction of morphological attribute profiles, is proposed. To this end, connected component analysis is performed on a preliminary supervised or unsupervised classification result that does not involve contextual information. Subsequently, after extracting the relevant attributes from each of the connected components, the threshold values are found by grouping the attribute vectors using a clustering algorithm. In our experiments, we demonstrate the effect of image scaling on the selected thresholds. In addition, we show the advantage of using our automatic threshold selection approach with respect to manual selection, by both monitoring redundancy and performing a classification experiment.	algorithm;cluster analysis;connected component (graph theory);connected-component labeling;experiment;image scaling;information;unsupervised learning;xslt/muenchian grouping	Zahid Mahmood;Guy Thoonen;Paul Scheunders	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6352502	contextual image classification;biological classification;computer science;machine learning;pattern recognition;data mining;cluster analysis	Vision	31.262413547288965	-43.819036359399384	174782
88cbde3694d70226523fd36a43726747147a4224	spectral-spatial rotation forest for hyperspectral image classification		Rotation Forest (RoF) is a decision tree ensemble classifier, which uses random feature selection and data transformation techniques to improve both the diversity and accuracy of base classifiers. Traditional RoF only considers data transformation on spectral information. In order to further improve the performance of RoF, we introduce spectral-spatial data transformation into RoF and thus propose a spectral-spatial Rotation Forest (SSRoF). The proposed method is experimentally investigated on a hyperspectral remote sensing image collected by the Airborne Visible/Infrared Imaging Spectrometer (AVIRIS) sensor. Experimental results indicate that the proposed methodology achieves excellent performance.		Junshi Xia;Lionel Bombrun;Yannick Berthoumieu;Christian Germain;Peijun Du	2017	IEEE J Sel. Topics in Appl. Earth Observ. and Remote Sensing	10.1109/JSTARS.2017.2720259	full spectral imaging;computer vision;feature extraction;computer science;hyperspectral imaging;pattern recognition;radio frequency;physics;remote sensing	Vision	30.495754417900613	-43.583232759476594	174866
01c7a778cde86ad1b89909ea809d55230e569390	a supervised low-rank method for learning invariant subspaces	sparse matrices matrix decomposition training measurement training data testing robustness;matrix decomposition computer vision face recognition image classification image representation learning artificial intelligence;face recognition supervised low rank method invariant subspace learning sparse representation low rank matrix decomposition approaches computer vision problems uniform interclass separation robust classification simple nearest neighbor approach local metric learning	Sparse representation and low-rank matrix decomposition approaches have been successfully applied to several computer vision problems. They build a generative representation of the data, which often requires complex training as well as testing to be robust against data variations induced by nuisance factors. We introduce the invariant components, a discriminative representation invariant to nuisance factors, because it spans subspaces orthogonal to the space where nuisance factors are defined. This allows developing a framework based on geometry that ensures a uniform inter-class separation, and a very efficient and robust classification based on simple nearest neighbor. In addition, we show how the approach is equivalent to a local metric learning, where the local metrics (one for each class) are learned jointly, rather than independently, thus avoiding the risk of overfitting without the need for additional regularization. We evaluated the approach for face recognition with highly corrupted training and testing data, obtaining very promising results.	algorithm;computer vision;facial recognition system;mathematical optimization;nuisance variable;overfitting;sparse;supervised learning;time complexity	Farzad Siyahjani;Ranya Almohsen;Sinan Sabri;Gianfranco Doretto	2015	2015 IEEE International Conference on Computer Vision (ICCV)	10.1109/ICCV.2015.480	computer vision;machine learning;pattern recognition;mathematics	Vision	27.12199261832396	-44.22040151125599	174932
451c42da244edcb1088e3c09d0f14c064ed9077e	using subclasses in discriminant non-negative subspace learning for facial expression recognition	decomposition;face recognition;image classification;image enhancement;learning (artificial intelligence);matrix decomposition;pattern clustering;nmf;clustering based discriminant analysis;cost function;discriminant nonnegative subspace learning method;facial expression recognition problem;image processing problem;image recognition;nonnegative matrix factorization;databases;vectors;algorithm design and analysis	Non-negative Matrix Factorization (NMF) is among the most popular subspace methods, widely used in a variety of image processing problems. To achieve an efficient decomposition of the provided data to its discriminant parts, thus enhancing classification performance, we regard that data inside each class form clusters and use criteria inspired by Clustering based Discriminant Analysis. The proposed method combines these discriminant criteria as constraints in the NMF decomposition cost function in order to address the problem of finding discriminant projections that enhance class separability in the reduced dimensional projection space. The developed algorithm has been applied to the facial expression recognition problem and experimental results verified that it successfully identified discriminant facial parts, thus enhancing recognition performance.	.cda file;algorithm;computer cluster;image processing;linear discriminant analysis;linear separability;loss function;non-negative matrix factorization	Symeon Nikitidis;Anastasios Tefas;Ioannis Pitas	2011	2011 19th European Signal Processing Conference		speech recognition;machine learning;pattern recognition;mathematics;linear discriminant analysis;multiple discriminant analysis	AI	25.52277622202076	-41.432702901364436	174979
41ddf4d2b973cc850bf109875c4c74372cf914c2	face recognition using position-dictionaries and region covariance feature		In this paper, a new sparsity formulation called position-dictionary based sparse representation is developed for frontal face recognition. Different from the sparse representation based classification (SRC) method and the Gabor-feature based SRC (GSRC) method which both employ a global dictionary to decompose image patches, the proposed method constructs a position-dictionary for each location using training patches in the corresponding location since they resemble each other and are more likely to favor the same atoms. Sparse coefficients of each position-patch can be obtained by solving an \(l_{1}\)-norm minimization problem. For each face image, sparse coefficients of position-patches are pooled to construct a discriminative upper level feature to represent face image. PCA is used to perform dimension reduction. Each testing sample is represented as a sparse linear combination of all training samples, and recognition is accomplished by evaluating which class of training samples leads to the minimum reconstruction error. We compared the proposed method with SRC and GSRC method on three benchmark face databases. Experimental results show that the proposed method achieves higher recognition rates and is robust to a certain degree of occlusions.	dictionary;facial recognition system	Yuhua Li;Chun Qi	2014	Signal, Image and Video Processing	10.1007/s11760-014-0647-2	computer vision;machine learning;pattern recognition;sparse approximation;mathematics	Vision	26.639404580279518	-43.49678359607613	175391
4d5c3157b6b46fa4e94fa98f9029c2311aeec0af	fast clustering and topic modeling based on rank-2 nonnegative matrix factorization		The importance of unsupervised clustering and topic modeling is well recognized with everincreasing volumes of text data. In this paper, we propose a fast method for hierarchical clustering and topic modeling called HierNMF2. Our method is based on fast Rank-2 nonnegative matrix factorization (NMF) that performs binary clustering and an efficient node splitting rule. Further utilizing the final leaf nodes generated in HierNMF2 and the idea of nonnegative least squares fitting, we propose a new clustering/topic modeling method called FlatNMF2 that recovers a flat clustering/topic modeling result in a very simple yet significantly more effective way than any other existing methods. We implement highly optimized open source software in C++ for both HierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic modeling of document data sets. Substantial experimental tests are presented that illustrate significant improvements both in computational time as well as quality of solutions. We compare our methods to other clustering methods including K-means, standard NMF, and CLUTO, and also topic modeling methods including latent Dirichlet allocation (LDA) and recently proposed algorithms for NMF with separability constraints. Overall, we present efficient tools for analyzing large-scale data sets, and techniques that can be generalized to many other data analytics problem domains.	algorithm;blas;big data;c++;cluster analysis;computation;ground truth;hierarchical clustering;k-means clustering;latent dirichlet allocation;linear separability;marginal model;matrix multiplication;network security;non-negative least squares;non-negative matrix factorization;open-source software;overhead (computing);problem domain;scalability;sparse matrix;text corpus;text mining;time complexity;topic model;tree (data structure)	Da Kuang;Barry J. Drake;Haesun Park	2015	CoRR		correlation clustering;constrained clustering;data stream clustering;document clustering;fuzzy clustering;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;brown clustering;biclustering	ML	24.655486639514134	-38.235199983754526	175486
49c512fbaf97bff90994248d93ba5b8cab30359e	discriminative transformation for multi-dimensional temporal sequences	electronic mail;training;hidden markov models;transforms;adaptation models;character recognition;data models	Feature space transformation techniques have been widely studied for dimensionality reduction in vector-based feature space. However, these techniques are inapplicable to sequence data because the features in the same sequence are not independent. In this paper, we propose a method called max–min inter-sequence distance analysis (MMSDA) to transform features in sequences into a low-dimensional subspace such that different sequence classes are holistically separated. To utilize the temporal dependencies, MMSDA first aligns features in sequences from the same class to an adapted number of temporal states, and then, constructs the sequence class separability based on the statistics of these ordered states. To learn the transformation, MMSDA formulates the objective of maximizing the minimal pairwise separability in the latent subspace as a semi-definite programming problem and provides a new tractable and effective solution with theoretical proofs by constraints unfolding and pruning, convex relaxation, and within-class scatter compression. Extensive experiments on different tasks have demonstrated the effectiveness of MMSDA.	algorithm;alignment;altretamine;appendix;c date and time functions;class;cobham's thesis;compression;confusion;dimensionality reduction;experiment;feature vector;hamming distance;hidden markov model;holism;linear programming relaxation;linear separability;matroid rank;maxima and minima;name;neodymium-doped yttrium aluminum garnet lasers;nonlinear system;performance;programming in the large and programming in the small;projections and predictions;semiconductor industry;semidefinite programming;unfolding (dsp implementation);xfig;emotional dependency	Bing Su;Xiaoqing Ding;Changsong Liu;Hao Wang;Ying Wu	2017	IEEE Transactions on Image Processing	10.1109/TIP.2017.2704438	data modeling;speech recognition;computer science;machine learning;pattern recognition;mathematics;hidden markov model;statistics	DB	25.5080045909376	-43.96121737382479	175699
b02b838f96797b23091fbc18ba974b19c3b3e764	multi-level low-rank approximation-based spectral clustering for image segmentation	image segmentation;spectral clustering;matrix approximation	Spectral clustering is a well-known graph-theoretic approach of finding natural groupings in a given dataset, and has been broadly used in image segmentation. Nowadays, High-Definition (HD) images are widely used in television broadcasting and movies. Segmenting these high resolution images presents a grand challenge to the current spectral clustering techniques. In this paper, we propose an efficient spectral method, Multi-level Low-rank Approximation-based Spectral Clustering (MLASC), to segment high resolution images. By integrating multi-level low-rank matrix approximations, i.e., the approximations to the affinity matrix and its subspace, as well as those for the Laplacian matrix and the Laplacian subspace, MLASC gains great computational and spacial efficiency. In addition, the proposed fast sampling strategy make it possible to select sufficient data samples in MLASC, leading to accurate approximation and segmentation. From a theoretical perspective, we mathematically prove the correctness of MLASC, and provide detailed analysis on its computational complexity. Through experiments performed on both synthetic and real datasets, we demonstrate the superior performance of MLASC. 2012 Elsevier B.V. All rights reserved.	approximation algorithm;approximation error;cluster analysis;column (database);computation;computational complexity theory;correctness (computer science);data point;experiment;grand challenges;graph theory;image resolution;image segmentation;jt (visualization format);laplacian matrix;low-rank approximation;maxima and minima;multistage interconnection networks;nonlinear system;outline of television broadcasting;pixel;processor affinity;sampling (signal processing);semiconductor industry;singular value decomposition;spectral clustering;spectral method;synthetic intelligence;time complexity;universal quantification	Lijun Wang;Ming Dong	2012	Pattern Recognition Letters	10.1016/j.patrec.2012.07.024	correlation clustering;computer vision;mathematical optimization;data stream clustering;computer science;machine learning;segmentation-based object categorization;pattern recognition;mathematics;image segmentation;cluster analysis;scale-space segmentation;spectral clustering;statistics	AI	27.510730320178038	-38.57369763891172	176799
65ee4de888e5b934429dcb126ee0ae544156c9bd	face recognition using linear representation ensembles	ensemble learning;face recognition;linear representation	In the past decade, linear representation based face recognition has become a very popular research subject in computer vision. This method assumes that faces belonging to one individual reside in a lowdimensional linear subspace. In real-world applications, however, face images usually are of degraded quality due to expression variations, disguises, and partial occlusions. These problems undermine the validity of the subspace assumption and thus the recognition performance deteriorates significantly. In this work, we propose a simple yet effective framework to address the problem. Observing that the linear subspace assumption is more reliable on certain face patches rather than on the holistic face, Probabilistic Patch Representations (PPRs) are randomly generated, according to the Bayesian theory. We then train an ensemble model over the patch-representations by minimizing the empirical risk w.r.t. the “leave-one-out margins”, which we term Linear Representation Ensemble (LRE). In the test stage, to handle the non-facial or novel face patterns, we design a simple inference method to dynamically tune the ensemble weights according to the proposed Generic Face Confidence (GFC). Furthermore, to accommodate immense PPR sets, a boosting-like algorithm is also derived. In addition, we theoretically prove two desirable property of the proposed learning methods. We extensively evaluate the proposed methods on four public face dataset, i.e., Yale-B, AR, FRGC and LFW, and the results demonstrate the superiority of both our two methods over many other state-of-the art algorithms, in terms of both recognition accuracy and computational efficiency. & 2015 Elsevier Ltd. All rights reserved.	adaptive filter;algorithm;ar (unix);boosting (machine learning);central processing unit;computer vision;consistency model;ensemble learning;facial recognition system;gradient boosting;heuristic (computer science);holism;matlab;portland pattern repository;procedural generation;randomness;real life	Hanxi Li;Fumin Shen;Chunhua Shen;Yang Yang;Yongsheng Gao	2016	Pattern Recognition	10.1016/j.patcog.2015.12.011	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;ensemble learning;algorithm;statistics	Vision	25.785387772228265	-42.99559677123433	177536
4bcf004e28d2f268bac1c58e0e259dff0e93b775	magnitude bounded matrix factorisation for recommender systems		Low rank matrix factorisation is often used in recommender systems as a way of extracting latent features. When dealing with large and sparse datasets, traditional recommendation algorithms face the problem of acquiring large, unrestrained, fluctuating values over predictions especially for users/items with very few corresponding observations. Although the problem has been somewhat solved by imposing bounding constraints over its objectives, and/or over all entries to be within a fixed range, in terms of gaining better recommendations, these approaches have two major shortcomings that we aim to mitigate in this work: one is they can only deal with one pair of fixed bounds for all entries, and the other one is they are very time-consuming when applied on large scale recommender systems. In this paper, we propose a novel algorithm named Magnitude Bounded Matrix Factorisation (MBMF), which allows different bounds for individual users/items and performs very fast on large scale datasets. The key idea of our algorithm is to construct a model by constraining the magnitudes of each individual user/item feature vector. We achieve this by converting from the Cartesian to Spherical coordinate system with radii set as the corresponding magnitudes, which allows the above constrained optimisation problem to become an unconstrained one. The Stochastic Gradient Descent (SGD) method is then applied to solve the unconstrained task efficiently. Experiments on synthetic and real datasets demonstrate that in most cases the proposed MBMF is superior over all existing algorithms in terms of accuracy and time complexity.	algorithm;experiment;feature vector;intelligent platform management interface;mathematical optimization;monoid factorisation;non-negative matrix factorization;quantum fluctuation;recommender system;singular value decomposition;sparse matrix;stochastic gradient descent;synthetic intelligence;time complexity	Shuai Jiang;Kan Li;Richard Y. D. Xu	2018	CoRR		mathematical optimization;recommender system;mathematics;time complexity;machine learning;artificial intelligence;low-rank approximation;magnitude (mathematics);bounded function;feature vector;matrix (mathematics);stochastic gradient descent	AI	26.48896095139842	-39.28172165028417	177909
73f38ffa54ca4dff09d42cb18461187b9315a735	fast inference in sparse coding algorithms with applications to object recognition	object recognition;efficient algorithm;natural images;science learning;pattern recognition;visual object recognition;sparse representation;optimal algorithm;sparse coding	Adaptive sparse coding methods learn a possibly overcomplete set of basis functions, such that natural image patches can be reconstructed by linearly combining a small subset of these bases. The applicability of these methods to visual object recognition tasks has been limited because of the prohibitive cost of the optimization algorithms required to compute the sparse representation. In this work we propose a simple and efficient algorithm to learn basis functions. After training, this model also provides a fast and smooth approximator to the optimal representation, achieving even better accuracy than exact sparse coding algorithms on visual object recognition tasks.	algorithm;algorithmic efficiency;approximation algorithm;basis function;code;iterative method;mathematical optimization;neural coding;outline of object recognition;real-time clock;real-time computing;sparse approximation;sparse matrix	Koray Kavukcuoglu;Marc'Aurelio Ranzato;Yann LeCun	2008	CoRR		computer vision;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;sparse approximation;3d single-object recognition	ML	27.261005072776005	-44.35205839162244	178121
e433fc9d5cc6846fcc84a7c3252a637c43f5736f	vectorial approximations of infinite-dimensional covariance descriptors for image classification		The class of symmetric positive definite (SPD) matrices, especially in the form of covariance descriptors (CovDs), have been receiving increased interest for many computer vision tasks. Covariance descriptors offer a compact way of robustly fusing different types of features with measurement variations. Successful examples of applying CovDs addressing various classification problems include object recognition, face recognition, human tracking, texture categorization, visual surveillance, etc. As a novel data descriptor, CovDs encode the second-order statistics of features extracted from a finite number of observation points (e.g., the pixels of an image) and capture the relative correlation of these features along their powers as a means of representation. In general, CovDs are SPD matrices and it is well known that the space of SPD matrices (denoted by Sym+) is not a subspace in Euclidean space but a Riemannian manifold with nonpositive curvature. As a consequence, conventional learning methods based on Euclidean geometry are not the optimal choice for CovDs, as proven in several prior studies. In order to better cope with the Riemannian structure of CovDs, many methods based on nonEuclidean metrics (e.g., affine-invariant metrics, logEuclidean metrics, Bregman divergence, and Stein metrics) have been proposed over the last few years. In particular, the log-Euclidean metric possesses several desirable properties which are beneficial for classification: (i) it is fast to compute; (ii) it defines a true geodesic on Sym+; and (iii) it comes up with	approximation;bregman divergence;categorization;computer vision;data descriptor;encode;euclidean distance;facial recognition system;outline of object recognition;pixel	Jie-Yi Ren;Xiaojun Wu	2017	Computational Visual Media	10.1007/s41095-017-0094-4	approximations of π;pattern recognition;mathematics;machine learning;contextual image classification;covariance;artificial intelligence	Vision	27.76061212903454	-44.26305537701391	178217
75ec2d8bb4bdb5d1b6576df3695b45fc028e8084	largest-eigenvalue-theory for incremental principal component analysis	eigenvalues and eigenfunctions;image processing eigenvalues and eigenfunctions principal component analysis covariance matrices;cmu;image processing;incremental principal component analysis;face database largest eigenvalue theory incremental principal component analysis symmetry matrix iterative algorithm covariance matrix digits database;eigenvalues;iterative algorithm;covariance matrices;largest eigenvalue;principal component analysis;principal component analysis iterative algorithms eigenvalues and eigenfunctions covariance matrix state estimation image databases face detection image converters computer vision face recognition;eigenvectors;covariance matrix	In this paper, we present a novel algorithm for incremental principal component analysis. Based on the largest-eigenvalue-theory, i.e. the eigenvector associated with the largest eigenvalue of a symmetry matrix can be iteratively estimated with any initial value, we propose an iterative algorithm, referred as LET-IPCA, to incrementally update the eigenvectors corresponding to the leading eigenvalues. LET-IPCA is covariance matrix free and seamlessly connects the estimations of the leading eigenvectors by cooperatively preserving the most dominating information, as opposed to the state-of-the-art algorithm CCIPCA, in which the estimation of each eigenvector is independent. The experiments on both the MNIST digits database and the CMU PIE face database show that our proposed algorithm is much superior to CCIPCA in both convergency speed and accuracy.	algorithm;experiment;iterative method;mnist database;principal component analysis	Shuicheng Yan;Xiaoou Tang	2005	IEEE International Conference on Image Processing 2005	10.1109/ICIP.2005.1529967	estimation of covariance matrices;mathematical optimization;sparse pca;non-linear iterative partial least squares;image processing;eigenvalues and eigenvectors;pattern recognition;mathematics;eigenface;statistics;principal component analysis	Robotics	26.946501460846175	-40.65365961315946	178781
b54c477885d53a27039c81f028e710ca54c83f11	semi-supervised kernel mean shift clustering	mean shift clustering;biological patents;log det bregman divergence;pattern clustering;clustering procedure;biomedical journals;nonparametric technique;text mining;europe pubmed central;semisupervised kernel mean shift clustering;citation search;algorithms clustering similarity measures computer vision applications pattern recognition computing methodologies;semisupervised framework;null space;matrix algebra;initial kernel matrix;citation networks;computer vision;vectors clustering algorithms null space symmetric matrices clustering methods computer vision;symmetric matrices;vectors;research articles;clustering;semisupervised kernel mean shift clustering synthetic datasets real datasets initial kernel matrix linear transformation clustering procedure semisupervised framework semisupervised clustering methods underlying cluster structure nonparametric technique;abstracts;open access;semi supervised kernel clustering;life sciences;clinical guidelines;pattern recognition;similarity measures;linear transformation;clustering algorithms;algorithms;semisupervised clustering methods;full text;learning artificial intelligence;pattern clustering learning artificial intelligence matrix algebra;clustering methods;real datasets;computing methodologies;rest apis;applications;orcids;synthetic datasets;europe pmc;biomedical research;bioinformatics;literature search;underlying cluster structure	Mean shift clustering is a powerful nonparametric technique that does not require prior knowledge of the number of clusters and does not constrain the shape of the clusters. However, being completely unsupervised, its performance suffers when the original distance metric fails to capture the underlying cluster structure. Despite recent advances in semi-supervised clustering methods, there has been little effort towards incorporating supervision into mean shift. We propose a semi-supervised framework for kernel mean shift clustering (SKMS) that uses only pairwise constraints to guide the clustering procedure. The points are first mapped to a high-dimensional kernel space where the constraints are imposed by a linear transformation of the mapped points. This is achieved by modifying the initial kernel matrix by minimizing a log det divergence-based objective function. We show the advantages of SKMS by evaluating its performance on various synthetic and real datasets while comparing with state-of-the-art semi-supervised clustering algorithms.	algorithm;cluster analysis;gene distance metric;kernel (operating system);loss function;mean shift;mental suffering;optimization problem;semi-supervised learning;semiconductor industry;synthetic intelligence;unsupervised learning;user space;diethyltoluamide;mapped;statistical cluster	Saket Anand;Sushil Mittal;Oncel Tuzel;Peter Meer	2014	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.190	correlation clustering;constrained clustering;data stream clustering;text mining;k-medians clustering;fuzzy clustering;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;statistics;clustering high-dimensional data	Vision	26.147892077963583	-38.65513664423677	178955
6b514c45cc5919c7c644e94a5f625074d4a5794e	ordering and elimination based component learning method	support vector machines image classification pattern clustering;pattern clustering;kernel;optics;support vector machines;image classification;feature space;data distribution;learning systems;distance measurement;learning methods;visualization technique;data visualization;support vector clustering;learning systems indium phosphide data visualization testing optical devices pattern recognition support vector machines support vector machine classification gaussian distribution performance analysis;support vector machine;synthetic datasets elimination based component learning method ordering based component learning method gaussian components data distribution visualization technique optics classification clustering linear support vector machines;gaussian distribution	In this paper, we propose a component learning method to learn a set of Gaussian components that fit the given data distribution. An ordering and visualization technique called OPTICS and tests of multinormality are used in this method. We consider the applications of the proposed method to the tasks of classification and clustering. Here, the components are used to define a feature space to which the data points are transformed. In that feature space, classification is performed using linear support vector machines and clustering is performed using support vector clustering. The performance of the component learning method and its application to classification and clustering is demonstrated on synthetic datasets.	cluster analysis;data point;feature vector;optics algorithm;support vector machine;synthetic intelligence	Sheetal Reddy Pamudurthy;Chellu Chandra Sekhar	2009	2009 Seventh International Conference on Advances in Pattern Recognition	10.1109/ICAPR.2009.103	correlation clustering;constrained clustering;feature learning;data stream clustering;fuzzy clustering;flame clustering;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;clustering high-dimensional data;conceptual clustering	Robotics	30.15467903223153	-43.100806769955454	178986
77de6656872a7bf64353e8fe031e30a4632de11b	structured sparsity via alternating directions methods		We consider a class of sparse learning problems in high dimen sional feature space regularized by a structured sparsity-inducing norm that incorporates pri or knowledge of the group structure of the features. Such problems often pose a considerable chall enge to optimization algorithms due to the non-smoothness and non-separability of the regulari zation term. In this paper, we focus on two commonly adopted sparsity-inducing regularization erms, the overlapping Group Lasso penaltyl1/l2-norm and thel1/l∞-norm. We propose a unified framework based on the augmented Lagrangian method, under which problems with both types of r egularization and their variants can be efficiently solved. As one of the core building-blocks of this framework, we develop new algorithms using a partial-linearization/splitting tech nique and prove that the accelerated versions of these algorithms require O( 1 ε ) iterations to obtain anε-optimal solution. We compare the performance of these algorithms against that of the alterna ing direction augmented Lagrangian and FISTA methods on a collection of data sets and apply them t o two real-world problems to compare the relative merits of the two norms.	algorithm;augmented lagrangian method;feature vector;iteration;lasso;linear separability;mathematical optimization;matrix regularization;sparse matrix;synthetic data;test data;unified framework	Zhiwei Qin;Donald Goldfarb	2012	Journal of Machine Learning Research			ML	25.620389105246517	-39.839617702558506	179159
776befb4b1a51f3e00c7fe677748ae6069d4eb4f	ℓp-norm multiple kernel learning with low-rank kernels	kernels;multiple kernel learning;svm;low rank approximation	Kernel-based learning algorithms are well-known to poorly scale to large-scale applications. For such large tasks, a common solution is to use low-rank kernel approximation. Several algorithms and theoretical analyses have already been proposed in the literature, for low-rank Support Vector Machine or low-rank Kernel Ridge Regression but not for multiple kernel learning. The proposed method bridges this gap by addressing the problem of scaling lp norm multiple kernel for large learning tasks using low-rank kernel approximations. Our contributions stand on proposing a novel optimization problem, which takes advantage of the low-rank kernel approximations and on introducing a proximal gradient algorithm for solving that optimization problem. We also provide partial theoretical results on the impact of the low-rank approximations over the kernel combination weights. Experimental evidences show that the proposed approach scales better than the SMO-MKL algorithm for tasks involving about several hundred thousands of examples. Experimental comparisons with interior point methods also prove the efficiency of the algorithm we propose. & 2014 Elsevier B.V. All rights reserved.	algorithm;convex function;convex set;emoticon;gramian matrix;image scaling;interior point method;jetflash;kernel (operating system);kernel method;loss function;low-rank approximation;machine learning;math kernel library;mathematical optimization;maxima and minima;min/max kd-tree;multiple kernel learning;optimization problem;perturbation function;proximal gradient methods for learning;quadratic programming;scalability;sequential minimal optimization;social inequality;support vector machine;the matrix	Alain Rakotomamonjy;Sukalpa Chanda	2014	Neurocomputing	10.1016/j.neucom.2014.06.019	kernel;principal component regression;support vector machine;least squares support vector machine;kernel method;mathematical optimization;string kernel;kernel embedding of distributions;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;low-rank approximation;kernel smoother	ML	25.102135466644715	-38.10956188075137	179553
4378933d37f266756f15f55749370879b572b4a1	spectral-spatial classification for hyperspectral imagery: a novel combination method based on affinity scoring		Recently, a general framework for spectral-spatial classification has caught the attention of the hyperspectral imagery (HSI) society. It consists of three parts: classification, segmentation and combination of the former results to make a refined labeled map. Seeing the potentials of the last part, we derive a novel combination rule based on affinity scoring (CRAS). The core of the system is affinity score (AS), which is derived from fuzzy logic. Every AS measures the degree, i.e., the affinity, by which a pixel belongs to a class. The score is essentially decided by three factors: local spatial consistency, spectral similarity, and prior knowledge. The method is compatible with basic classification and segmentation tools, thus saving the trouble of designing complex techniques for the other parts in the framework. Experimental results show that CRAS excels several basic techniques as well as various state-of-the-art methods in the area of spectral-spatial classification. 近年来, 空谱结合的高光谱图像分类方法受到重视。 在该领域内, 存在一种分类-分割-合并的框架。 为了提升该框架中合并环节的作用, 本文提出一种新型的基于隶属度评分的新的合并方法, 用于空谱结合的分类。 该算法的核心是隶属度评分, 其概念由模糊数学中的 “隶属度” 发展而来。 一个隶属度分数实际上衡量的是一个像素属于某一个类别的程度。 隶属度评分主要受到三个因素的影响: 局部空间一致性, 光谱相似性以及先验知识。 提出的合并算法可与基本的经典的分类和分割算法结合使用。 实验证明提出的合并算法在分类精度上优于多种不同的空谱结合的经典算法及以当前先进的空谱结合的算法。	affinity analysis;algorithm;ecology;fuzzy logic;horizontal situation indicator;image segmentation;machine learning;pixel;processor affinity;semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Zhao Chen;Bin Wang	2016	Science China Information Sciences	10.1007/s11432-016-5576-y	computer science;machine learning;pattern recognition;data mining	Vision	30.74422052378459	-44.74330986377518	179920
ac5ee48955ce97433cadef011bedd8e65a574255	image classification using kernel collaborative representation with regularized least square	image recognition;kernel;classification;journal;crc_rls;src	Sparse representation based classification (SRC) has received much attention in computer vision and pattern recognition. SRC codes a testing sample by sparse linear combination of all the training samples and classifies the testing sample into the class with the minimum representation error. Recently, Zhang analyzes the working mechanism of SRC and points out that it is the collaborative representation but not the L1-norm sparsity that makes SRC powerful. Based on the analysis, they propose a very simple and much more efficient classification scheme, called collaborative representation based classification with regularized least square (CRC_RLS). CRC_RLS is a linear method in nature. Here we propose a kernel collaborative representation based classification with regularized least square (Kernel CRC_RLS, KCRC_RLS) by implicitly mapping the sample into high-dimensional space via kernel tricks. Our approach is highly motivated by the kernel methods which can capture the nonlinear similarity among samples and have been successfully applied in pattern recognition and machine learning. The experimental results on the CENPAMI handwritten digital database, ETH80 database, FERET face database, ORL database, AR face database, demonstrate that Kernel CRC_RLS is effective in classification, leading to promising performance. 2013 The Authors. Published by Elsevier Inc. All rights reserved.	code;comparison and contrast of classification schemes in linguistics and metadata;computer vision;feret (facial recognition technology);kernel (operating system);kernel method;machine learning;nonlinear system;pattern recognition;return loss;sample rate conversion;sparse matrix;taxicab geometry	Wankou Yang;Zhenyu Wang;Jun Yin;Changyin Sun;Karl Ricanek	2013	Applied Mathematics and Computation	10.1016/j.amc.2013.07.024	kernel;speech recognition;biological classification;computer science;machine learning;pattern recognition;mathematics;proto-oncogene tyrosine-protein kinase src;statistics;algebra	AI	25.59560518526041	-42.00865768101452	180178
18b09b3260aa110cf5b358cafa95b7440f26be7d	image classification using correlation tensor analysis	reconnaissance visage;correlation tensor analysis;similarity metric;metodo correlacion;evaluation performance;learning algorithm;performance evaluation;image processing;subspace learning;manifolds;learning;subspace learning correlation tensor analysis cta discriminant analysis face recognition image classification;tensor calculus;correlation method;biometrie;evaluacion prestacion;isometrie;biometrics;biometria;procesamiento imagen;image classification;graph embedded correlational mapping;face recognition image classification correlation tensor analysis data structure correlation based similarity metric supervised multilinear discriminant subspace learning graph embedded correlational mapping discriminant analysis low dimensional data representation;informacion fisher;metodo subespacio;algorithme apprentissage;algorithms artificial intelligence biometry discriminant analysis face humans image enhancement image interpretation computer assisted imaging three dimensional pattern recognition automated reproducibility of results sensitivity and specificity;data representation;carta de datos;traitement image;similitude;data distribution;low dimensional data representation;methode sous espace;aprendizaje;discriminant analysis;analyse discriminante;calculo tensorial;analisis discriminante;apprentissage;automatic recognition;face recognition;isometria;data structures;mappage;robustesse;estructura datos;high dimensional data;signal classification;classification image;classification algorithms;similarity;analyse correlation;pattern recognition;subspace method;correlation based similarity metric;classification signal;correlation tensor analysis cta;robustness;structure donnee;mapping;graph embedding;reconnaissance forme;supervised multilinear discriminant subspace learning;similitud;correlation;classification automatique;learning artificial intelligence;reconocimiento patron;automatic classification;isometric embedding;algoritmo aprendizaje;isometry;clasificacion automatica	Images, as high-dimensional data, usually embody large variabilities. To classify images for versatile applications, an effective algorithm is necessarily designed by systematically considering the data structure, similarity metric, discriminant subspace, and classifier. In this paper, we provide evidence that, besides the Fisher criterion, graph embedding, and tensorization used in many existing methods, the correlation-based similarity metric embodied in supervised multilinear discriminant subspace learning can additionally improve the classification performance. In particular, a novel discriminant subspace learning algorithm, called correlation tensor analysis (CTA), is designed to incorporate both graph-embedded correlational mapping and discriminant analysis in a Fisher type of learning manner. The correlation metric can estimate intrinsic angles and distances for the locally isometric embedding, which can deal with the case when Euclidean metric is incapable of capturing the intrinsic similarities between data points. CTA learns multiple interrelated subspaces to obtain a low-dimensional data representation reflecting both class label information and intrinsic geometric structure of the data distribution. Extensive comparisons with most popular subspace learning methods on face recognition evaluation demonstrate the effectiveness and superiority of CTA. Parameter analysis also reveals its robustness.	algorithm;algorithmic efficiency;bayesian information criterion;cancer/testis antigen;class;computation;data (computing);data point;data structure;dimensionality reduction;euclidean distance;experiment;facial recognition system;feature extraction;graph - visual representation;graph embedding;isometric projection;linear discriminant analysis;multilinear subspace learning;robustness (computer science);similarity measure;statistical classification;benefit	Yun Fu;Thomas S. Huang	2008	IEEE Transactions on Image Processing	10.1109/TIP.2007.914203	algorithm design;computer vision;tensor calculus;contextual image classification;graph embedding;tensor;similarity;isometry;data structure;manifold;computer science;fisher information;similitude;machine learning;pattern recognition;mathematics;external data representation;linear discriminant analysis;multilinear subspace learning;correlation;biometrics;statistics;robustness;clustering high-dimensional data	ML	29.319159586743186	-39.63521865710153	180387
7202996a6078916ca430b81b3bffb6744ce60799	nonlinear dimensionality reduction for discriminative analytics of multiple datasets		Principal component analysis (PCA) is widely used for feature extraction and dimensionality reduction, with documented merits in diverse tasks involving high-dimensional data. PCA copes with one dataset at a time, but it is challenged when it comes to analyzing multiple datasets jointly. In certain data science settings however, one is often interested in extracting the most discriminative information from one dataset of particular interest (a.k.a. target data) relative to the other(s) (a.k.a. background data). To this end, this paper puts forth a novel approach, termed discriminative (d) PCA, for such discriminative analytics of multiple datasets. Under certain conditions, dPCA is proved to be least-squares optimal in recovering the latent subspace vector unique to the target data relative to background data. To account for nonlinear data correlations, (linear) dPCA models for one or multiple background datasets are generalized through kernel-based learning. Interestingly, all dPCA variants admit an analytical solution obtainable with a single (generalized) eigenvalue decomposition. Finally, substantial dimensionality reduction tests using synthetic and real datasets are provided to corroborate the merits of the proposed methods.		Jia Chen;Gang Wang;Georgios B. Giannakis	2019	IEEE Transactions on Signal Processing	10.1109/TSP.2018.2885478	kernel (linear algebra);discriminative model;mathematical optimization;principal component analysis;dimensionality reduction;feature extraction;nonlinear dimensionality reduction;mathematics;subspace topology;pattern recognition;artificial intelligence;analytics	ML	24.686523925349565	-41.744926870247156	180727
47889e36448aff6dc0ac0f1ecb556de8d48cc53a	a conic section classifier and its application to image datasets	health research;focusing;uk clinical guidelines;biological patents;protocols;high dimensionality;supervised learning;cancer;distance measure;information science;application software;europe pubmed central;prototypes;citation search;feature space;computer vision;face recognition;uk phd theses thesis;life sciences;computer vision cancer application software supervised learning protocols information science prototypes focusing face recognition epilepsy;uk research reports;medical journals;off the shelf;europe pmc;biomedical research;bioinformatics;epilepsy	Many problems in computer vision involving recognition and/or classification can be posed in the general framework of supervised learning. There is however one aspect of image datasets, the high-dimensionality of the data points, that makes the direct application of off-the-shelf learning techniques problematic. In this paper, we present a novel concept class and a companion tractable algorithm for learning a suitable classifier from a given labeled dataset, that is particularly suited to high-dimensional sparse datasets. Each member class in the dataset is represented by a prototype conic section in the feature space, and new data points are classified based on a distance measure to each such representative conic section that is parameterized by its focus, directrix and eccentricity. Learning is achieved by altering the parameters of the conic section descriptor for each class, so as to better represent the data. We demonstrate the efficacy of the technique by comparing it to several well known classifiers on multiple public domain datasets.	algorithm;cobham's thesis;computer vision;concept class;critical section;data point;distance (graph theory);feature vector;multiclass classification;numerous;prototype;resultant;silo (dataset);sparse matrix;supervised learning;tracer	Arunava Banerjee;Santhosh Kodipaka;Baba C. Vemuri	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.20	facial recognition system;communications protocol;computer vision;application software;feature vector;information science;computer science;data science;machine learning;data mining;prototype;supervised learning;statistics;cancer	Vision	26.840932267379852	-44.69241482083124	181429
f657a680c19c8e151af6a0df5394c5464e9b1728	kernelized multiview projection for robust action recognition	g400 computer science;sequential distance learning;multiple view fusion;dimensionality reduction;human action recognition;spectral coding	Conventional action recognition algorithms adopt a single type of feature or a simple concatenation of multiple features. In this paper, we propose to better fuse and embed different feature representations for action recognition using a novel spectral coding algorithm called Kernelized Multiview Projection (KMP). Computing the kernel matrices from different features/views via time-sequential distance learning, KMP can encode different features with different weights to achieve a low-dimensional and semantically meaningful subspace where the distribution of each view is sufficiently smooth and discriminative. More crucially, KMP is linear for the reproducing kernel Hilbert space, which allows it to be competent for various practical applications. We demonstrate KMP’s performance for action recognition on five popular action datasets and the results are consistently superior to state-of-the-art techniques.	algorithm;computer vision;concatenation;dimensionality reduction;encode;hilbert space;kernel (operating system);kernel method;klee's measure problem;krylov subspace;semi-supervised learning	Ling Shao;Li Liu;Mengyang Yu	2015	International Journal of Computer Vision	10.1007/s11263-015-0861-6	computer vision;computer science;machine learning;pattern recognition;mathematics;dimensionality reduction	Vision	25.32943505214302	-44.49032484758755	181485
6dca6497183f0b054076d95a2f5dda90146182e4	sensible functional linear discriminant analysis		Summary The focus of this paper is to extend Fisher’s linear discriminant analysis (LDA) to both densely recorded functional data and sparsely observed longitudinal data for general c-category classification problems. We propose an efficient approach to identify the optimal LDA projections in addition to managing the noninvertibility issue of the covariance operator emerging from this extension. A conditional expectation technique is employed to tackle the challenge of projecting sparse data to the LDA directions. We study the asymptotic properties of the proposed estimators and show that asymptotically perfect classification can be achieved in certain circumstances. The performance of this new approach is further demonstrated with numerical examples.	linear discriminant analysis	Lu-Hung Chen;Ci-Ren Jiang	2018	Computational Statistics & Data Analysis	10.1016/j.csda.2018.04.005	machine learning;pattern recognition;mathematics;statistics	ML	25.06801287937843	-38.412704465229595	182349
341a9b13184d20f3246b893d3f1c57157dbc221f	hyperspectral data feature extraction using deep learning hybrid model	hyperspectral imaging;feature extraction;deep learning;hybrid model;contractive autoencoder;restricted boltzman machines	 The original hyperspectral data served as the initial features has the characteristics of high dimension and redundancy, which is not suitable for the subsequent analysis, so extracting feature information is needed. The deep learning model has a strong ability in feature learning, but if the model has too many layers which will lead to the original information loss in the process of layer-by-layer feature learning and reduce the subsequent classification accuracy. To solve this problem, the paper proposed a deep learning model of hybrid structure with the contractive autoencoder and restricted boltzmann machine to extract the hyperspectral data feature information. First, through pre-processing the spectral data, the 2d spectrum data is converted into a one dimensional vector. Then, a hybrid model is constructed for unsupervised training and supervised learning for the hyperspectral data, and features are extracted from bottom to top gradually according to the hybrid model. Finally, the SVM classifier is adopted to enhance the classification ability of spectral data. The paper uses the hybrid model proposed to test for extracting features with two sets of AVIRIS data and compares with PCA and GCA methods. The experiment results show that the feature extraction algorithm based on hybrid depth model can get the better features, and have strong distinguish performance, and can get better classification accuracy by the SVM algorithm. 	deep learning;feature extraction	Xinhua Jiang;Heru Xue;Lina Zhang;Xiaojing Gao;Yanqing Zhou;Jie Bai	2018	Wireless Personal Communications	10.1007/s11277-018-5389-y	supervised learning;support vector machine;real-time computing;autoencoder;computer science;feature extraction;deep learning;feature learning;hyperspectral imaging;pattern recognition;restricted boltzmann machine;artificial intelligence	Mobile	29.936749268974893	-44.86813707224647	182602
8e95af0cb74450a6ca38a5f6ae33d1ed44257318	global coordination based on matrix neural gas for dynamic texture synthesis	neural gas;dynamic texture;principal component	Matrix neural gas has been proposed as a mathematically well-founded extension of neural gas networks to represent data in terms of prototypes and local principal components in a smooth way. The additional information provided by local principal directions can directly be combined with charting techniques such that a nonlinear embedding of a data manifold into low dimensions results for which an explicit function as well as an approximate inverse exists. In this paper, we show that these ingredients can be used to embed dynamic textures in low dimensional spaces such that, together with a traversing technique in the low dimensional representation, efficient dynamic texture synthesis can be obtained.		Banchar Arnonkijpanich;Barbara Hammer	2010		10.1007/978-3-642-12159-3_8	neural gas;computer vision;computer science;machine learning;pattern recognition;principal component analysis	Robotics	28.095546892480275	-40.79899118371081	182669
a6c0881a1fe3dde65c1ae35dd6a1a7300552b6a1	the kernel common vector method: a novel nonlinear subspace classifier for pattern recognition	nonlinear subspace classifier;modified common vector method;vectors pattern classification;indexing terms;feature space;common vector cv;kernel mapping function kernel common vector method nonlinear subspace classifier pattern recognition modified common vector method;vectors;kernel common vector method;feature extraction;principal component analysis;subspace classifier common vector cv kernel based subspace method pattern recognition;pattern classification;word recognition;pattern recognition;subspace method;kernel based subspace method;kernel pattern recognition support vector machines support vector machine classification image recognition data mining eigenvalues and eigenfunctions covariance matrix testing performance evaluation;support vector machine classification;kernel method;support vector machine;algorithms artificial intelligence computer simulation decision support techniques image interpretation computer assisted models theoretical nonlinear dynamics pattern recognition automated;kernel mapping function;eigenvectors;covariance matrix;subspace classifier	The common vector (CV) method is a linear subspace classifier method which allows one to discriminate between classes of data sets, such as those arising in image and word recognition. This method utilizes subspaces that represent classes during classification. Each subspace is modeled such that common features of all samples in the corresponding class are extracted. To accomplish this goal, the method eliminates features that are in the direction of the eigenvectors corresponding to the nonzero eigenvalues of the covariance matrix of each class. In this paper, we introduce a variation of the CV method, which will be referred to as the modified CV (MCV) method. Then, a novel approach is proposed to apply the MCV method in a nonlinearly mapped higher dimensional feature space. In this approach, all samples are mapped into a higher dimensional feature space using a kernel mapping function, and then, the MCV method is applied in the mapped space. Under certain conditions, each class gives rise to a unique CV, and the method guarantees a 100% recognition rate with respect to the training set data. Moreover, experiments with several test cases also show that the generalization performance of the proposed kernel method is comparable to the generalization performances of other linear subspace classifier methods as well as the kernel-based nonlinear subspace method. While both the MCV method and its kernel counterpart did not outperform the support vector machine (SVM) classifier in most of the reported experiments, the application of our proposed methods is simpler than that of the multiclass SVM classifier. In addition, it is not necessary to adjust any parameters in our approach.	class;experiment;extraction;feature vector;generalization (psychology);kernel (operating system);kernel method;mobile television;nonlinear system;pattern recognition;performance;statistical classification;support vector machine;test case;test set;mapped	Hakan Cevikalp;Marian Neamtu;Atalay Barkana	2007	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2007.896011	random subspace method;support vector machine;kernel method;covariance matrix;index term;feature vector;feature extraction;eigenvalues and eigenvectors;word recognition;computer science;machine learning;pattern recognition;data mining;mathematics;principal component analysis	ML	28.205735969942634	-42.215667925544395	183008
daa4f19a1846ee73243fbe0dbbcd16034b9eb7a6	robust periocular recognition by fusing sparse representations of color and geometry information	elastic net regularization;texture decomposition;periocular recognition;color;total variation;sparse representation	In this paper, we propose a re-weighted elastic net (REN) model for biometric recognition. The new model is applied to data separated into geometric and color spatial components. The geometric information is extracted using a fast cartoon texture decomposition model based on a dual formulation of the total variation norm allowing us to carry information about the overall geometry of images. Color components are defined using linear and nonlinear color spaces, namely the red-green-blue (RGB), chromaticitybrightness (CB) and hue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparse representations for classification purposes are obtained. The scheme is numerically solved using a gradient projection (GP) algorithm. In the empirical validation of the proposed model, we have chosen the periocular region, which is an emerging trait known for its robustness against low quality data. Our results were obtained in the publicly available FRGC and UBIRIS.v2 data sets and show consistent improvements Juan C. Moreno jcmb@ubi.pt V. B. Surya Prasath prasaths@missouri.edu Gil Santos gmelfe@ubi.pt Hugo Proença hugomcp@di.ubi.pt 1 IT-Instituto de Telecomunicações, Department of Computer Science, University of Beira Interior, 6201-001 Covilha, Portugal 2 Department of Computer Science, University of Missouri-Columbia, MO 65211, USA in recognition effectiveness when compared to related stateof-the-art techniques.	algorithm;biometrics;color space;columbia (supercomputer);computer science;dvd region code;elastic map;elastic net regularization;gradient;hgnc;nonlinear system;numerical analysis;sparse matrix	Juan Carlos Moreno;V. B. Surya Prasath;Gil Melfe Mateus Santos;Hugo Proença	2016	Signal Processing Systems	10.1007/s11265-015-1023-3	computer vision;computer science;machine learning;pattern recognition;sparse approximation;mathematics;total variation;elastic net regularization	Vision	27.206577940854434	-42.070121363637305	183224
96a3349d89581fc3e0d4355c674c77d21468e07e	class-imbalance learning based discriminant analysis	databases;databases feature extraction vectors correlation handwriting recognition training educational institutions;image features;image recognition;handwriting recognition;training;image database;class imbalance;image classification;linear discriminate analysis;vectors feature extraction image classification image recognition learning artificial intelligence;lda class imbalance learning pattern recognition class specific idea multiclass feature extraction multiclass feature recognition binary class problems class imbalance problem minority class specific class majority class discriminative information class balanced discrimination orthogonal cbd linear discriminant analysis discriminative vector extraction orthogonal constraint public image databases image feature extraction image recognition method;discriminant analysis;vectors;image feature extraction and recognition class balanced discrimination cbd orthogonal cbd ocbd class imbalance learning discriminant analysis;feature extraction;class imbalance learning;pattern recognition;image feature extraction and recognition;class balanced discrimination cbd;correlation;learning artificial intelligence;orthogonal cbd ocbd;approaches to learning	Feature extraction is an important research topic in the field of pattern recognition. The class-specific idea tends to recast a traditional multi-class feature extraction and recognition task into several binary class problems, and therefore inevitably class imbalance problem, where the minority class is the specific class, and the majority class consists of all the other classes. However, discriminative information from binary class problems is usually limited, and imbalanced data may have negative effect on the recognition performance. For solving these problems, in this paper, we propose two novel approaches to learn discriminant features from imbalanced data, named class-balanced discrimination (CBD) and orthogonal CBD (OCBD). For a specific class, we select a reduced counterpart class whose data are nearest to the data of specific class, and further divide them into smaller subsets, each of which has the same size as the specific class, to achieve balance. Then, each subset is combined with the minority class, and linear discriminant analysis (LDA) is performed on them to extract discriminative vectors. To further remove redundant information, we impose orthogonal constraint on the extracted discriminant vectors among correlated classes. Experimental results on three public image databases demonstrate that the proposed approaches outperform several related image feature extraction and recognition methods.	component-based software engineering;computer vision;database;feature (computer vision);feature extraction;linear discriminant analysis;pattern recognition	Xiao-Yuan Jing;Chao Lan;Min Li;Yong-Fang Yao;David Zhang;Jingyu Yang	2011	The First Asian Conference on Pattern Recognition	10.1109/ACPR.2011.6166659	speech recognition;computer science;machine learning;pattern recognition	AI	27.570368822540914	-43.59414303589559	183243
7feb9ca090ff9a2bdbd1d1bc5ab9d04f5f0e0727	"""hierarchical classification of hyperspectral images by using svms and """"same class neighborhood property"""""""	classification algorithm;decision tree;information sources;support vector machines;tree architecture;hyperspectral sensors;independent component analysis;hierarchical classification;feature extraction;principal component analysis;remote sensing;classification algorithms;hyperspectral data;hyperspectral remote sensing;support vector machine classification;classification tree analysis;support vector machine;hyperspectral imaging;classification accuracy;hyperspectral image;binary tree	With the increase of information sources and data volume along with the limited number of training samples of hyperspectral images, the classification of hyperspectral images by feature extraction algorithms like PCA, ICA, PP, DBFE, DAFE and Wavelet with use of statistical classifiers like ML-classifier is considered useless. In this paper a two stages classification algorithm to fuse spatial and spectral information is proposed. In the first stage the hyperspectral images with a classical classification algorithm is classified. Then primary classes of each pixel and its eight immediate neighbors are identified. In the second stage each pixel is labeled by a hierarchical classifier. Hierarchical classifier that is used in this research is a binary decision tree. Two primary classes of a pixel and its neighbors are compared in each node of decision tree by a SVM. The proposed SVM based binary tree takes advantage of both the efficient computation of the tree architecture and the high classification accuracy of SVM. The hyperspectral data set used in our experiments is a scene taken over NW Indiana’s Indian Pine by the AVIRIS sensor. The obtained results show that the problem of limited training samples can be mitigated using the proposed classification algorithm; moreover the proposed algorithm reduced significantly the computational time of hyperspectral classification using SVMs. This suggests that SVM based binary tree could be a promising tool to classify hyperspectral remote-sensing images. KeywordsSupport Vector Machine, Classification, Hyperspectral image, Remote Sensing.	algorithm;binary tree;computation;decision tree;experiment;feature extraction;hierarchical classifier;independent computing architecture;information theory;netware;pp (complexity);pixel;principal component analysis;statistical classification;time complexity;wavelet	Ahmad Keshavarz;Hassan Ghassemian;Hamid Dehghani	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1526526	statistical classification;support vector machine;computer science;hyperspectral imaging;machine learning;pattern recognition;data mining	ML	30.391470260308626	-44.25795470266022	183598
274ba758d399ec7eb847045750d4148d0ceaedcb	multi-modal remote sensing image classification for low sample size data		Recently, multiple and heterogeneous remote sensing images have provided a new development opportunity for Earth observation research. utilizing deep learning to gain the shared representative information between different modalities is important to resolve the problem of geographical region classification. In this paper, a CNN-based multi-modal framework for low-sample-size data classification of remote sensing images is introduced. This method has three main stages. Firstly, features are extracted from high- and low-resolution remote sensing images separately using multiple convolution layers. Then, the two types of features are fused at the fusion algorithm layer. Finally, the fused features are used to train a classifier. The novelty of this method is that not only it considers the complementary relationship between the two modalities, but enhances the value of a small number of samples. Based on our experiments, the proposed model can obtain a state-of-the-art performance, being more accurate than the comparable architectures, such as single-modal LeNet, NanoNets and multi-modal H&L-LeNet that are trained with a double size of samples.	algorithm;convolution;deep learning;experiment;modal logic;statistical classification	Qi He;Yao Tung Lee;Dongmei Huang;Shengqi He;Wei Song;Yanling Du	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489351	data classification;pattern recognition;deep learning;remote sensing;feature extraction;earth observation;statistical classification;small number;contextual image classification;computer science;image resolution;artificial intelligence	Robotics	30.760875167099726	-44.83863405899268	183761
36fd702e5686f91b7e45434f8e2f6ef51feb2d54	kernel-pca analysis of surface normals for shape-from-shading	face reconstruction shape from shading kernel principal component analysis normals;kernel principal component analysis;face reconstruction;shape from shading;kernel vectors electronics packaging robustness principal component analysis ip networks manifolds;normals;principal component analysis computational geometry differential geometry image representation;kernel pca analysis kernel based framework surface normals component analysis mapping functions azimuthal equidistant projection aep principal geodesic analysis pga cosine distance noisy training sets shape from shading algorithm sfs algorithm spherical normal representation cosine kernel robust subspace analysis technique kernel principal component analysis	We propose a kernel-based framework for computing components from a set of surface normals. This framework allows us to easily demonstrate that component analysis can be performed directly upon normals. We link previously proposed mapping functions, the azimuthal equidistant projection (AEP) and principal geodesic analysis (PGA), to our kernel-based framework. We also propose a new mapping function based upon the cosine distance between normals. We demonstrate the robustness of our proposed kernel when trained with noisy training sets. We also compare our kernels within an existing shape-from-shading (SFS) algorithm. Our spherical representation of normals, when combined with the robust properties of cosine kernel, produces a very robust subspace analysis technique. In particular, our results within SFS show a substantial qualitative and quantitative improvement over existing techniques.	algorithm;angularjs;asymptotic equipartition property;clustered file system;cosine similarity;digital television adapter;discrete cosine transform;expectation propagation;kernel (operating system);kernel principal component analysis;normal (geometry);photometric stereo;principal geodesic analysis;shading;unified framework	Patrick Snape;Stefanos P. Zafeiriou	2014	2014 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2014.139	kernel;principal component regression;computer vision;mathematical optimization;kernel embedding of distributions;photometric stereo;radial basis function kernel;kernel principal component analysis;computer science;machine learning;pattern recognition;principal geodesic analysis;mathematics;variable kernel density estimation	Vision	27.02617701846993	-42.954026627447206	184160
3a63667284dc8b9687ed1620406030bfe39af3c9	active-metric learning for classification of remotely sensed hyperspectral images	remote sensing feature extraction hyperspectral imaging image classification;measurement;training;feature extraction;active metric learning k nearest neighbor classification lmnn metric learning principle large margin nearest neighbor feature space active learning process feature extraction dimensionality reduction spectral data remote sensing hyperspectral image classification;optimization;metric learning active learning al classification dimensionality reduction hyperspectral images large margin nearest neighbor lmnn;hyperspectral imaging;training feature extraction hyperspectral imaging measurement optimization	Classification of remotely sensed hyperspectral images via supervised approaches is typically affected by high dimensionality of the spectral data and a limited number of labeled samples. Dimensionality reduction via feature extraction and active learning (AL) are two approaches that researchers have investigated independently to deal with these two problems. In this paper, we propose a new method in which the feature extraction and AL steps are combined into a unique framework. The idea is to learn and update a reduced feature space in a supervised way at each iteration of the AL process, thus taking advantage of the increasing labeled information provided by the user. In particular, the computation of the reduced feature space is based on the large-margin nearest neighbor (LMNN) metric learning principle. This strategy is applied in conjunction with k-nearest neighbor ( k-NN) classification, for which a new sample selection strategy is proposed. The methodology is validated experimentally on four benchmark hyperspectral data sets. Good improvements in terms of classification accuracy and computational time are achieved with respect to the state-of-the-art strategies that do not combine feature extraction and AL.	active learning (machine learning);benchmark (computing);computation;dimensionality reduction;ensemble learning;experiment;feature extraction;feature vector;information theory;iteration;k-nearest neighbors algorithm;large margin nearest neighbor;local-density approximation;overhead (computing);performance;pine;preprocessor;regular expression;requirement;supervised learning;time complexity;unsupervised learning	Edoardo Pasolli;Hsiuhan Lexie Yang;Melba M. Crawford	2016	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2015.2490482	large margin nearest neighbor;computer vision;feature extraction;hyperspectral imaging;machine learning;pattern recognition;mathematics;k-nearest neighbors algorithm;physics;measurement;remote sensing;dimensionality reduction	ML	29.98650023619305	-43.3840779902299	184296
724413561e5acdb366affbb4fbb8b4f5dac2de7f	view-based 3d model retrieval via supervised multi-view feature learning	3d model retrieval;multi-view;feature learning;feature dimensionality reduction;svd;zernike moments	With the development of the processing technologies of 3D model and the increasing of 3D model in different application flieds, 3D model retrieval is attracting more and more people’s attention. In order to handle this problem, most of approaches focus on the feature extraction form different virtual view. It is hard to guarantee the robustness and also ignore the correlation between both views. Thus, we propose an effective view-based 3D model retrieval method via supervised multi-view feature learning (SMFL). First, the subspace dimension of viusal feature is generated through Singular Value Decomposition (SVD) algorithm. This step is used to select main information from multi-view in order to reduce the final amount of calculation; Secondly, we consider the relationship of multi-view from same class and the correlation between two different classes to make the feature mapping in order to reduce the different of views from the same class and increase the different of views from the difference class; Finally, the projection mapping corresponding to the inner product of each 3D model helps to calculate the similarities between two different 3D models. The extensive experiments are conducted on popular ETH, NTU, MV-RED and PSB 3D model datasets with Zernike moments. The comparative results or The experimental results with existing 3D model retrieval methods show the superiority of the proposed method.	3d modeling;algorithm;effective method;experiment;feature extraction;feature learning;feature model;loss function;network interface device;optimization problem;pacific symposium on biocomputing;polygonal modeling;singular value decomposition;supervised learning	Anan Liu;Yang Shi;Weizhi Nie;Yuting Su	2017	Multimedia Tools and Applications	10.1007/s11042-017-5076-0	computer science;computer vision;robustness (computer science);artificial intelligence;pattern recognition;zernike polynomials;feature extraction;subspace topology;singular value decomposition;machine learning;projection mapping;feature learning;correlation	AI	26.454893141838248	-43.86059363981573	184371
acb9bb0e2046d63bee5289cbfac254dd1d7aa6f9	random sampling lda for face recognition	conventional approach;gabor responses;lda classifier;n-lda classifier;training samples;feature vector;face recognition;discriminative information;robust face recognition system;random subspace;linear discriminant analysis;random sampling lda;image sampling;feature extraction;image classification;high dimensional face data;feature extraction technique;different overfitting problem;random sampling;face recognition system;null space lda;principal component analysis;fusion rule;multiple stabilized fisherface;system integration	Linear discriminant analysis (LDA) is a popular feature extraction technique for face recognition. However, It often suffers from the small sample size problem when dealing with the high dimensional face data. Fisherface and null space LDA (N-LDA) are two conventional approaches to address this problem. But in many cases, these LDA classifiers are overfitted to the training set and discard some useful discriminative information. In this paper, by analyzing different overfitting problems for the two kinds of LDA classifiers, we propose an approach using random subspace and bagging to improve them respectively. By random sampling on feature vector and training samples, multiple stabilized Fisherface and N-LDA classifiers are constructed. The two kinds of complementary classifiers are integrated using a fusion rule, so nearly all the discriminative information is preserved. We also apply this approach to the integration of multiple features. A robust face recognition system integrating shape, texture and Gabor responses is finally developed.	bootstrap aggregating;facial recognition system;feature extraction;feature vector;kernel (linear algebra);linear discriminant analysis;local-density approximation;overfitting;sampling (signal processing);test set;texture mapping	Xiaogang Wang;Xiaoou Tang	2004	Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004.	10.1109/CVPR.2004.200	facial recognition system;random subspace method;sample size determination;sampling;computer vision;contextual image classification;multivariate random variable;feature vector;facies;vector space;image processing;feature extraction;biological classification;computer science;machine learning;pattern recognition;mathematics;texture;system integration;principal component analysis	Vision	27.746608764470537	-43.53955722036578	184596
1e54fca5d68611ca0bbab5a465d1038793bb7549	nonnegative matrix underapproximation for robust multiple model fitting		In this work, we introduce a highly efficient algorithm to address the nonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix factorization (NMF) with an additional underapproximation constraint. NMU results are interesting as, compared to traditional NMF, they present additional sparsity and part-based behavior, explaining unique data features. To show these features in practice, we first present an application to the analysis of climate data. We then present an NMU-based algorithm to robustly fit multiple parametric models to a dataset. The proposed approach delivers state-of-the-art results for the estimation of multiple fundamental matrices and homographies, outperforming other alternatives in the literature and exemplifying the use of efficient NMU computations.	algorithm;autonomous robot;computation;curve fitting;fundamental matrix (computer vision);non-negative matrix factorization;sparse matrix	Mariano Tepper;Guillermo Sapiro	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.77	econometrics;mathematical optimization;machine learning;mathematics;statistics	Vision	28.49469206771735	-38.2074218789441	184869
15ca15c6afbb1fad1e9967ca4fa459fe6781c437	kernel sparse subspace clustering	image processing kernel sparse subspace clustering data points data representation nonlinear manifolds kernel trick kernel sparse representations nonlinear mappings computer vision;kernel methods;non linear subspace clustering subspace clustering sparse subspace clustering kernel methods;sparse subspace clustering;pattern clustering computer vision data handling;subspace clustering;kernel computer vision clustering algorithms conferences manifolds pattern recognition signal processing algorithms;non linear subspace clustering	Subspace clustering refers to the problem of grouping data points that lie in a union of low-dimensional subspaces. One successful approach for solving this problem is sparse subspace clustering, which is based on a sparse representation of the data. In this paper, we extend SSC to non-linear manifolds by using the kernel trick. We show that the alternating direction method of multipliers can be used to efficiently find kernel sparse representations. Various experiments on synthetic as well real datasets show that non-linear mappings lead to sparse representation that give better clustering results than state-of-the-art methods.	augmented lagrangian method;cluster analysis;clustering high-dimensional data;data point;experiment;kernel (operating system);kernel method;nonlinear system;sql server compact;sparse approximation;sparse matrix;synthetic intelligence	Vishal M. Patel;René Vidal	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025576	correlation clustering;kernel method;mathematical optimization;data stream clustering;kernel embedding of distributions;k-svd;computer science;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;sparse approximation;mathematics;cluster analysis;brown clustering;clustering high-dimensional data	Vision	26.37988446505765	-40.48459018794912	185010
99de5503f87248fda9e1cff5b71b8667d2c82fa2	sparse-representation-based graph embedding for traffic sign recognition	graph theory;machine learning algorithms;image recognition;feature extraction algorithm design and analysis sparse matrices principal component analysis neural networks machine learning algorithms machine learning;traffic signs;neural networks;projection matrix traffic sign recognition machine learning algorithms supervised multicategory classification problem unbalanced class frequencies graph embedding algorithm local manifold structures global discriminative information between class discriminative information discriminative subspace l 2 1 norm sparse representation property;matrix algebra;classification;traffic engineering computing graph theory image recognition image representation learning artificial intelligence matrix algebra;sparse representation dimensionality reduction graph embedding machine learning;dimensionality reduction;machine learning;image representation;feature extraction;principal component analysis;algorithms;traffic engineering computing;graph embedding;learning artificial intelligence;detection and identification systems;sparse representation;sparse matrices;algorithm design and analysis	Researchers have proposed various machine learning algorithms for traffic sign recognition, which is a supervised multicategory classification problem with unbalanced class frequencies and various appearances. We present a novel graph embedding algorithm that strikes a balance between local manifold structures and global discriminative information. A novel graph structure is designed to depict explicitly the local manifold structures of traffic signs with various appearances and to intuitively model between-class discriminative information. Through this graph structure, our algorithm effectively learns a compact and discriminative subspace. Moreover, by using L2, 1-norm, the proposed algorithm can preserve the sparse representation property in the original space after graph embedding, thereby generating a more accurate projection matrix. Experiments demonstrate that the proposed algorithm exhibits better performance than the recent state-of-the-art methods.	algorithm;baseline (configuration management);experiment;graph embedding;machine learning;sparse approximation;sparse matrix;subspace gaussian mixture model;traffic sign recognition;unbalanced circuit	Ke Lu;Zhengming Ding;Sam Ge	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2220965	algorithm design;graph embedding;sparse matrix;feature extraction;biological classification;computer science;graph theory;theoretical computer science;machine learning;pattern recognition;sparse approximation;mathematics;moral graph;dimensionality reduction;principal component analysis	Vision	25.38961364351691	-42.58356428705264	185816
2cab2dabc8c7d21e73a265525f46c149f286c230	wifi-based human identification via convex tensor shapelet learning				Han Zou;Yuxun Zhou;Jianfei Yang;Weixi Gu;Lihua Xie;Costas J. Spanos	2018			artificial intelligence;machine learning;tensor;mathematical optimization;regular polygon;computer science	AI	26.36451341921198	-41.581004272607025	186162
cbf3e848c5d2130dd640d9bd546403b8d78ce0f9	local linear discriminant analysis with composite kernel for face recognition	local linear transforms;kernel;interpolation;composite kernel;over training problem;nonlinear pattern recognition task;local linear model;local linear models;kernel function;implicit kernel mapping;nonlinear problems;polynomials;nonlinear discriminant analysis;composite kernel based discriminant analysis form;face data set;face recognition;dimensionality reduction;complex nonlinear problem decomposition;kernel polynomials interpolation face recognition transforms face;dimensionality reduction linear discriminant analysis generalized discriminant analysis support vector machine local linear model composite kernel;transforms;generalized discriminant analysis;face;support vector machine;transforms face recognition interpolation;local linear discriminant analysis;linear discriminant analysis;local linear transforms local linear discriminant analysis face recognition nonlinear discriminant analysis local linear models interpolation complex nonlinear problem decomposition nonlinear classification composite kernel based discriminant analysis form nonlinear problems generalized discriminant analysis nonlinear pattern recognition task kernel function implicit kernel mapping over training problem face data set;nonlinear classification	This paper presents a method for nonlinear discriminant analysis utilizing a composite kernel which is derived from a combination of local linear models with interpolation. The underlying idea is to decompose a complex nonlinear problem into a set of simpler local linear problems. Combining with the theory of nonlinear classification based on kernels, the local linear models with interpolation can be formulated as a composite kernel based discriminant analysis form. In face recognition, linear discriminant analysis (LDA) has been widely adopted owing to its efficiency, but it fails to solve nonlinear problems. Conventional kernel based approaches such as generalized discriminant analysis (GDA) has been successfully applied to extend LDA to nonlinear pattern recognition tasks. However, selecting an appropriate kernel function is usually difficult. Utilizing an implicit kernel mapping may face potential over-training problems for some complex and noised tasks. Our proposed method gives an alternative solution for nonlinear discriminant analysis while the conventional linear and nonlinear approaches are difficult to achieve a satisfactory results. Experiments on both synthetic data and face data set show the effectiveness of the proposed methods.	experiment;facial recognition system;feature vector;gnome-db;interpolation;kernel (operating system);kernel method;linear discriminant analysis;linear model;nonlinear system;pattern recognition;synthetic data	Zhan Shi;Jinglu Hu	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252385	kernel;face;support vector machine;mathematical optimization;kernel fisher discriminant analysis;kernel;kernel embedding of distributions;interpolation;kernel principal component analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;variable kernel density estimation;multiple discriminant analysis;polynomial;dimensionality reduction	Vision	25.176907002189324	-40.64423091474765	186594
d8bf32e0936bae857cbca669f3ebf21455cf9d10	face recognition using flda with single training image per person	singular value decomposition svd;fisher linear discriminant analysis;analisis numerico;decomposition valeur singuliere;matematicas aplicadas;mathematiques appliquees;singular value;singular value decomposition;reconocimiento;analyse numerique;experimental result;recognition;numerical analysis;face recognition;feature extraction;discriminant;resultado experimental;scattering matrix;decomposicion valor singular;fisher linear discriminant analysis flda;resultat experimental;applied mathematics;reconnaissance;single training image per person	Keywords: Face recognition Fisher linear discriminant analysis (FLDA) Single training image per person Singular value decomposition (SVD) a b s t r a c t Fisher linear discriminant analysis (FLDA) has been widely used for feature extraction in face recognition. However, it cannot be used when each object has only one training sample because the intra-class variations cannot be statistically measured in this case. In this paper, a novel method is proposed to solve this problem by evaluating the within-class scatter matrix from the available single training image. By using singular value decomposition (SVD), we decompose the face image into two complementary parts: a smooth general appearance image and a difference image. The later is used to approximately evaluate the within-class scatter matrix and thus the FLDA can be applied to extract the discrimi-nant face features. Experimental results show that the proposed method is efficient and it can achieve higher recognition accuracy than many existing schemes. Face recognition has been extensively studied for many years and it is still attracting much attention because of its big potential in security, surveillance and human-computer intelligent interaction, etc. A key issue in face recognition is to find sufficient and discriminative features for face representation. Many approaches have been proposed and subspace analysis method (SAM) has become one of the most popular methods. SAM seeks for a set of basis vectors according to some criteria and extracts the features by projecting face images onto the subspace spanned by those basis vectors [1–3]. Principal component analysis (PCA), which tries to find a set of optimal orthogonal bases in the sense of minimum mean square error, and Fisher linear discriminant analysis (FLDA), which tries to find a set of optimal projection vectors by maximizing the ratio between the determinants of the between-class and the within-class scatter matrices of the training samples , are the two most representative methods in SAM. By first applying PCA to face recognition, Kirby and Sirovich [4] found that a face image could be reconstructed approximately as a weighted sum of a small collection of basis face images plus a mean face image. Based on this work, Turk and Pentland [5] developed the well-known Eigenface method. Since then, PCA has been extensively investigated and many PCA-based algorithms have been developed [6,7]. Although PCA enables sufficient reconstruction, it may not be optimal for classification because its optimality is in the sense of minimum …	algorithm;basis (linear algebra);eigenface;feret (facial recognition technology);facial recognition system;feature extraction;linear discriminant analysis;mean squared error;nant;optimal projection equations;principal component analysis;singular value decomposition;speech recognition;statistical classification;the turk;weight function;whole earth 'lectronic link;yang	Quanxue Gao;Lei Zhang;David Zhang	2008	Applied Mathematics and Computation	10.1016/j.amc.2008.05.019	facial recognition system;s-matrix;speech recognition;feature extraction;numerical analysis;machine learning;pattern recognition;mathematics;singular value decomposition;singular value;discriminant;algebra	Vision	25.49761673684385	-40.73914506591855	186780
b61cf5daf24c5d66110cf0ec6c32ca39f252c0ae	similarity metric learning for face verification using sigmoid decision function	mahalanobis distance;bilinear similarity;sigmoid function;face verification;similarity metric learning	In this paper, we consider the face verification problem, which is to determine whether two face images belong to the same subject or not. Although many research efforts have been focused on this problem, it still remains a challenging problem due to large intra-personal variations in imaging conditions, such as illumination, pose, expression, and occlusion. Our proposed method is based on the idea that we would like the similarity between positive pairs larger than negative pairs, and obtain a similarity estimation of two images. We construct our decision function by incorporating bilinear similarity and Mahalanobis distance to the sigmoid function. The constructed decision function makes our method discriminative for inter-personal differences and invariant to intra-personal variations such as pose/lighting/expression. What is more, our formulated objective function is convex, which guarantees global minimum. Our method belongs to nonlinear metric which is more robust to handle heterogeneous data than linear metric. We evaluate our proposed verification method on the challenging labeled faces in the wild (LFW) database. Experimental results demonstrate that the proposed method outperforms state-of-the-art methods such as Joint Bayesian under the unrestricted setting of LFW.	bilinear filtering;constant term;convex function;experiment;hidden surface determination;loss function;mathematical optimization;maxima and minima;nonlinear system;optimization problem;pose (computer vision);sl (complexity);sigmoid function;similarity measure;term (logic)	Xiao-Nan Hou;Shouhong Ding;Lizhuang Ma;Chengjie Wang;Ji-Lin Li;Feiyue Huang	2015	The Visual Computer	10.1007/s00371-015-1079-x	mathematical optimization;computer science;artificial intelligence;mahalanobis distance;machine learning;pattern recognition;mathematics;sigmoid function;statistics	Vision	25.372024152578486	-42.709528090834304	186839
2d569666768f41fb571725056d5db4b9b7da5707	human age estimation by metric learning for regression problems	distance metric learning;gaussian process regression;prior knowledge;manifold learning;metric learning;optimization problem;regression;real world application;age estimation;linear transformation	The estimation of human age from face images has many real-world applications. However, how to discover the intrinsic aging trend is still a challenging problem. We proposed a general distance metric learning scheme for regression problems, which utilizes not only data themselves, but also their corresponding labels to strengthen the credibility of distances. This metric could be learned by solving an optimization problem. Via the learned metric, it is easy to find the intrinsic variation trend of data by a relative small amount of samples without any prior knowledge of the structure or distribution of data. Furthermore, the test data could be projected to this metric by a simple linear transformation and it is easy to be combined with manifold learning algorithms to improve the performance. Experiments are conducted on the public FG-NET database by Gaussian process regression in the learned metric to validate our framework, which shows that its performance is improved over traditional regression methods.		Leting Pan	2009		10.1007/978-3-642-03641-5_34	optimization problem;mathematical optimization;regression;computer science;machine learning;pattern recognition;mathematics;linear map;nonlinear dimensionality reduction;kriging;statistics	ML	26.334669781047538	-43.30415006619607	187044
5745d7e662543e4c638f0102883dfab41e891a19	learning filter functions in regularisers by minimising quotients		Learning approaches have recently become very popular in the field of inverse problems. A large variety of methods has been established in recent years, ranging from bi-level learning to high-dimensional machine learning techniques. Most learning approaches, however, only aim at fitting parametrised models to favourable training data whilst ignoring misfit training data completely. In this paper, we follow up on the idea of learning parametrised regularisation functions by quotient minimisation as established in [3]. We extend the model therein to include higher-dimensional filter functions to be learned and allow for fit- and misfit-training data consisting of multiple functions. We first present results resembling behaviour of well-established derivative-based sparse regularisers like total variation or higher-order total variation in one-dimension. Our second and main contribution is the introduction of novel families of non-derivative-based regularisers. This is accomplished by learning favourable scales and geometric properties while at the same time avoiding unfavourable ones.		Martin Benning;Guy Gilboa;Joana Sarah Grah;Carola-Bibiane Schoenlieb	2017		10.1007/978-3-319-58771-4_41	machine learning	Robotics	24.92298435815912	-39.605688751209605	187359
552a37b43d01045be8b18304c65a647a6bf24c84	an ensemble constructed using spectral distribution and its efficiency in categorizing hard-to-discriminate features	accuracy earth spatial resolution remote sensing diversity reception neural networks training;neural networks;earth;training;object detection image classification;hard to discriminate image objects spectral distribution ensemble method efficiency ensemble method competence spectral information single classifier spectral channels earth feature spectral signature ensemble members classification accuracy;diversity reception;accuracy;remote sensing;diversity ensemble method spectral distribution hard to discriminate objects;spatial resolution	In the present paper, efficiency and competence of an ensemble method is explored in the context of large number of available spectral information. Classification results of ensemble method are compared with the results generated by a single classifier utilizing all spectral channels. In the present study, an ensemble committee is constructed by distributing spectral channels among five members of the committee to satisfy diversity criteria. Each spectral channel is representative of a particular wavelength and each Earth feature has its own spectral signature to a specific wavelength. Taking advantage of this fact, the present study attempts to explore the possibility of constructing diverse ensemble members in addition to achieving improved classification accuracy with respect to hard-to-discriminate image objects. Classification results obtained are promising. Overall classification accuracy is better through ensemble method. Some hard-to-discriminate objects are correctly identified. However, in some cases we obtained mixed results.	categorization	Laxminarayana Eeti;Krishna Mohan Buddhiraju	2015	2015 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)	10.1109/IGARSS.2015.7326464	image resolution;machine learning;pattern recognition;data mining;accuracy and precision;earth;artificial neural network;physics;remote sensing	Vision	30.995876387794652	-44.51584124516345	187726
99aff7c5c51989ac35aca8a162554267f00ec341	multi-class linear feature extraction by nonlinear pca	eigenvalues and eigenfunctions;optimisation;eigenvector feature extraction nonlinear pca fisher mapping class conjunctions covariance matrices principal component analysis optimisation;normal distribution;covariance matrices;feature extraction;principal component analysis;pattern classification;feature extraction principal component analysis covariance matrix scattering neural networks pattern recognition physics information technology laboratories data mining;eigenvalues and eigenfunctions pattern classification feature extraction optimisation covariance matrices principal component analysis;neural network	s ces, phthe Abstract The traditional way to find a linear solution to the feature extraction problem is based on the maximization of the class-between scatter over the class-within scatter (Fisher mapping). For the multi-class problem this is, however, sub-optimal due to class conjunctions, even for the simple situation of normal distributed classes with identical covariance matrices. We propose a novel, equally fast method, based on nonlinear PCA. Although still sub-optimal, it may avoid the class conjunction. The proposed method is experimentally compared with Fisher mapping and with a neural network based approach to nonlinear PCA. It appears to outperform both methods, the first one even in a dramatic way.	artificial neural network;expectation–maximization algorithm;experiment;feature extraction;nonlinear system;principal component analysis	Robert P. W. Duin;Marco Loog;Reinhold Häb-Umbach	2000		10.1109/ICPR.2000.906096	normal distribution;sparse pca;feature extraction;computer science;machine learning;pattern recognition;mathematics;artificial neural network;statistics;dimensionality reduction;principal component analysis	ML	24.61781576854796	-39.6879243543441	188636
53c5dc7448195c129b5802ed7b718529e4849e05	an efficient hierarchical hyperspectral image classification using binary quaternion-moment-preserving thresholding technique	geophysical image processing;image features;pattern clustering;high dimensionality;image classification;aviris images hierarchical hyperspectral image classification binary quaternion moment preserving thresholding technique unsupervised classification maximum correlation band clustering high dimensional image data low dimensional image features spectral characteristics;remote sensing;unsupervised classification;hyperspectral image;remote sensing geophysical image processing geophysical techniques image classification pattern clustering;hyperspectral imaging image classification hyperspectral sensors feature extraction principal component analysis clustering algorithms partitioning algorithms remote sensing image resolution multispectral imaging;geophysical techniques	In the study, we propose a novel unsupervised classification technique for hyperspectral images, which consists of two algorithms, referred to as the maximum correlation band clustering (MCBC) and hierarchical binary quaternion-moment-preserving (BQMP) thresholding technique. By the MCBC, we partition the bands into groups and transfer the high-dimensional image data into low-dimensional image features. Afterwards, the hierarchical BQMP approach partitions the feature image into proper regions according to the spectral characteristics. Simulation results performed on AVIRIS images have demonstrated the efficiency of the proposed approaches.	algorithm;cluster analysis;computer vision;simulation;thresholding (image processing);unsupervised learning	Lena Chang;Ching-Min Cheng;Yang-Lang Chang	2009	2009 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2009.5418068	computer vision;contextual image classification;feature detection;computer science;pattern recognition;feature;remote sensing	Robotics	30.895874286933857	-44.00654306108409	189201
5bc1766eee3daaaa6478d654ffe5ce9ffdb51b31	sign correlation subspace for face alignment		Face alignment is an essential task for facial performance capture and expression analysis. Current methods such as random subspace supervised descent method, stage-wise relational dictionary and coarse-to-fine shape searching can ease multi-pose face alignment problem, but no method can deal with the multiple local minima problem directly. In this paper, we propose a sign correlation subspace method for domain partition in only one reduced low-dimensional subspace. Unlike previous methods, we analyze the sign correlation between features and shapes and project both of them into a mutual sign correlation subspace. Each pair of projected shape and feature keeps their signs consistent in each dimension of the subspace, so that each hyper octant holds the condition that one general descent exists. Then a set of general descents are learned from the samples in different hyperoctants. Requiring only the feature projection for domain partition, our proposed method is effective for face alignment. We have validated our approach with the public face datasets which include a range of poses. The validation results show that our method can reveal their latent relationships to poses. The comparison with state-of-the-art methods demonstrates that our method outperforms them, especially in uncontrolled conditions with various poses, while enjoying the comparable speed.		Dansong Cheng;Yongqiang Zhang;Feng Tian;Ce Liu;Xiaofang Liu	2019	Soft Comput.	10.1007/s00500-018-3389-1	artificial intelligence;machine learning;partition (number theory);computer science;maxima and minima;motion capture;sparse approximation;subspace topology;correlation	Vision	26.0885829615558	-43.1996537269438	190483
7da07348d6fcb2b292da76285f3a371b02b8296b	mercer kernel-based clustering in feature space	libraries;eigenvalues and eigenfunctions;engineering;unsupervised learning;unsupervised partitioning;pattern clustering;kernel;inherent clusters;high dimensionality;nonlinear data transformation;electrical electronic;technology;data generation;feature space partitioning;computationally simple iterative procedure;kernel scattering data structures unsupervised learning data analysis radial basis function networks costs clustering methods councils libraries;scattering;matrix algebra;theory methods;feature space;transformed space;data partitioning;hardware architecture;data clustering;radial basis function networks;high dimensional feature space;data analysis;linear separability;unsupervised learning mercer kernel based clustering unsupervised partitioning inherent clusters data generation nonlinear data transformation high dimensional feature space linear separability transformed space data structure eigenvectors kernel matrix implicit mapping computationally simple iterative procedure feature space partitioning data clustering data partitioning;science technology;data structures;mercer kernel based clustering;data transformation;number of clusters;nonlinear component analysis;councils;matrix algebra unsupervised learning data analysis pattern clustering eigenvalues and eigenfunctions;artificial intelligence;computer science;clustering methods;data structure;eigenvectors;implicit mapping;kernel matrix	The article presents a method for both the unsupervised partitioning of a sample of data and the estimation of the possible number of inherent clusters which generate the data. This work exploits the notion that performing a nonlinear data transformation into some high dimensional feature space increases the probability of the linear separability of the patterns within the transformed space and therefore simplifies the associated data structure. It is shown that the eigenvectors of a kernel matrix which defines the implicit mapping provides a means to estimate the number of clusters inherent within the data and a computationally simple iterative procedure is presented for the subsequent feature space partitioning of the data.	binary space partitioning;cluster analysis;data structure;feature vector;iterative method;kernel;linear separability;nonlinear system;statistical cluster	Mark A. Girolami	2002	IEEE transactions on neural networks	10.1109/TNN.2002.1000150	mathematical optimization;kernel;data structure;computer science;machine learning;pattern recognition;mathematics	DB	27.643556387072145	-40.385280459267555	190703
a83f1767abdc819102602e089581c79fc6035b6f	detection and correction of mislabeled training samples for hyperspectral image classification		In this paper, a novel method is introduced to detect and correct mislabeled training samples for hyperspectral image classification. First, domain transform recursive filtering-based feature extraction is used to improve the separability of the training samples. Then, constrained energy minimization-based object detection is performed on the training set with each training sample serving as the object spectrum. Finally, the label of each training sample is verified or corrected based on the averaged detection probabilities of different classes. Experiments performed on real hyperspectral data sets demonstrate the effectiveness of the proposed method in improving classification performance with respect to the classifier trained with the original training set that contains a number of mislabeled samples.	computer vision;energy minimization;feature extraction;ground truth;linear separability;object detection;recursion;semi-supervised learning;sensor;statistical classification;test set	Xudong Kang;Puhong Duan;Xuanlin Xiang;Shutao Li;Jon Atli Benediktsson	2018	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2018.2823866	artificial intelligence;mathematics;computer vision;filter (signal processing);feature extraction;object detection;contextual image classification;hyperspectral imaging;training set;data set	Vision	29.853246921336357	-43.57630339931925	190801
913d35879dc566f9d0bb8204db19a923f97d834f	combining unmixing and deep feature learning for hyperspectral image classification		Image classification is one of the critical tasks in hyperspectral remote sensing. In recent years, significant improvement have been achieved by various classification methods. However, mixed spectral responses from different ground materials still create confusions in complex scenes. In this regard, unmixing approaches are being successfully carried out to decompose mixed pixels into a collection of spectral signatures. Considering the usefulness of these techniques, we propose to utilize the unmixing results as an input to classifiers for better classification accuracy. We propose a novel band group based structure preserving nonnegative matrix factorization (NMF) method to estimate the individual spectral responses from different materials within different ranges of wavelengths. Then we train a convolutional neural network (CNN) with the unmixing results to generate powerful features and eventually classify the data. This method is evaluated on a new dataset and compared with several state-of-the-art models, which shows the promising potential of our method.	artificial neural network;computer vision;convolutional neural network;deep learning;feature learning;map;non-negative matrix factorization;pixel;statistical classification;type signature	Fahim Irfan Alam;Jun Zhou;Lei Tong;Alan Wee-Chung Liew;Yongsheng Gao	2017	2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2017.8227419	artificial intelligence;pattern recognition;computer vision;convolutional neural network;computer science;pixel;hyperspectral imaging;image resolution;contextual image classification;non-negative matrix factorization;feature learning;spectral signature	Vision	30.51391142241745	-44.85981197510765	191140
98d1230dcf357fdfeaaf12514fd1077dd96902a6	a property of learning chunk data using incremental kernel principal component analysis	eigenvalues and eigenfunctions;chunk incremental kpca;kernel principal component analysis;eigenvalue problem;earth;principal component analysis eigenvalues and eigenfunctions learning artificial intelligence pattern recognition;satellites earth remote sensing learning systems principal component analysis;online feature extraction;eigenvalue decomposition;independent data selection;learning systems;learning system;incremental learning;feature extraction;principal component analysis;remote sensing;satellites;pattern recognition;independent data selection learning chunk data incremental kernel principal component analysis incremental learning algorithm chunk incremental kpca online feature extraction pattern recognition eigenvalue problem;incremental learning algorithm;learning artificial intelligence;learning chunk data;incremental kernel principal component analysis	An incremental learning algorithm of Kernel Principal Component Analysis (KPCA) called Chunk Incremental KPCA (CIKPCA) has been proposed for online feature extraction in pattern recognition. CIKPCA can reduce the number of times to solve the eigenvalue problem compared with the conventional incremental KPCA when a small number of data are simultaneously given as a stream of data chunks. However, our previous work suggests that the computational costs of the independent data selection in CIKPCA could dominate over those of the eigenvalue decomposition when a large chunk of data are given. To verify this, we investigate the influence of the chunk size to the learning time in CIKPCA. As a result, CIKPCA requires more learning time than IKPCA unless a large chunk of data are divided into small chunks (e.g., less than 50).	algorithm;feature extraction;feature vector;kernel principal component analysis;pattern recognition	Takaomi Tokumoto;Seiichi Ozawa	2012	2012 IEEE Conference on Evolving and Adaptive Intelligent Systems	10.1109/EAIS.2012.6232796	speech recognition;computer science;machine learning;pattern recognition	Robotics	29.07715933033629	-42.459530178929406	191666
9ee3a6209636b816919b9f851529d455616c7dff	a maximum noise fraction transform with improved noise estimation for hyperspectral images	noise estimation;liu xiang zhang bing gao lianru chen dongmei 超光谱图像 噪声估计 分数 噪音 a maximum noise fraction transform with improved noise estimation for hyperspectral images;image classification;maximum noise fraction;feature extraction;principal component transform;maximum noise fraction transform;classification accuracy;hyperspectral image;covariance matrix	Feature extraction is often performed to reduce spectral dimension of hyperspectral images before image classification. The maximum noise fraction (MNF) transform is one of the most commonly used spectral feature extraction methods. The spectral features in several bands of hyperspectral images are submerged by the noise. The MNF transform is advantageous over the principle component (PC) transform because it takes the noise information in the spatial domain into consideration. However, the experiments described in this paper demonstrate that classification accuracy is greatly influenced by the MNF transform when the ground objects are mixed together. The underlying mechanism of it is revealed and analyzed by mathematical theory. In order to improve the performance of classification after feature extraction when ground objects are mixed in hyperspectral images, a new MNF transform, with an improved method of estimating hyperspectral image noise covariance matrix (NCM), is presented. This improved MNF transform is applied to both the simulated data and real data. The results show that compared with the classical MNF transform, this new method enhanced the ability of feature extraction and increased classification accuracy.	computer vision;experiment;feature extraction;image noise	Xiang Liu;Bing Zhang;Lianru Gao;Dongmei Chen	2009	Science in China Series F: Information Sciences	10.1007/s11432-009-0156-z	covariance matrix;contextual image classification;speech recognition;feature extraction;computer science;machine learning;pattern recognition;mathematics;statistics	Vision	30.76709562514646	-44.0778372368242	192235
239d367972c59dc8995f14d27ff8680ee72ebf74	pursuing informative projection on grassmann manifold	optimal method;scattering;information criteria;conjugate gradient method;computer vision;rotation invariance;linear discriminant analysis principal component analysis mutual information feature extraction computer vision information theory scattering information analysis multidimensional systems entropy;grassmann manifold;feature extraction;principal component analysis;mutual information;entropy;linear discriminant analysis;information analysis;multidimensional systems;information theory	Inspired by the underlying relationship between classification capability and the mutual information, in this paper, we first establish a quantitative model to describe the information transmission process from feature extraction to final classification and identify the critical channel in this propagation path, and then propose a Maximum Effective Information Criteria for pursuing the optimal subspace in the sense of preserving maximum information that can be conveyed to final decision. Considering the orthogonality and rotation invariance properties of the solution space, we present a Conjugate Gradient method constrained on a Grassmann manifold to exploit the geometric traits of the solution space for enhancing the efficiency of optimization. Comprehensive experiments demonstrate that the framework integrating the Maximum Effective Information Criteria and Grassmann manifold-based optimization method significantly improves the classification performance.	algorithm;channel (communications);computer vision;conjugate gradient method;database;experiment;feret (facial recognition technology);facial recognition system;feasible region;feature extraction;information theory;manifold regularization;mathematical optimization;music encoding initiative;mutual information;pattern recognition;software propagation;toy problem	Dahua Lin;Shuicheng Yan;Xiaoou Tang	2006	2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)	10.1109/CVPR.2006.231	entropy;mathematical optimization;multidimensional systems;information theory;feature extraction;computer science;grassmannian;machine learning;pattern recognition;mathematics;conjugate gradient method;mutual information;linear discriminant analysis;scattering;data analysis;statistics;principal component analysis	Vision	26.61964280455847	-41.33442689130299	192452
350d8609d2d5eea4dccff337b00f61fd116f6314	linear multilayer independent component analysis for large natural scenes	high dimensionality;natural images;independent component analysis;stochastic gradient;numerical experiment;independent component;natural scenes	In this paper, linear multilayer ICA (LMICA) is proposed for extracting independent components from quite high-dimensional observed signals such as large-size natural scenes. There are two phases in each layer of LMICA. One is the mapping phase, where a one-dimensional mapping is formed by a stochastic gradient algorithm which makes more highlycorrelated (non-independent) signals be nearer incrementally. Another is the local-ICA phase, where each neighbor (namely, highly-correlated) pair of signals in the mapping is separated by the MaxKurt algorithm. Because LMICA separates only the highly-correlated pairs instead of all ones, it can extract independent components quite efficiently from appropriate observed signals. In addition, it is proved that LMICA always converges. Some numerical experiments verify that LMICA is quite efficient and effective in large-size natural image processing.	algorithm;approximation;data mining;dataspaces;experiment;gradient;image processing;independent computing architecture;independent component analysis;numerical analysis;pixel;text mining;whitening transformation	Yoshitatsu Matsuda;Kazunori Yamaguchi	2004			independent component analysis;computer vision;computer science;machine learning;pattern recognition;mathematics	ML	28.69978119761675	-42.662187738776574	192925
9dd17f0d6b2e4aa3a5e2ee9c33008b9e407b196e	regularization and improved interpretation of linear data mappings and adaptive distance measures	upper bound;learning artificial intelligence data handling;training vectors eigenvalues and eigenfunctions measurement null space symmetric matrices prototypes;data handling;learning artificial intelligence;regularization mechanism linear data mappings adaptive distance measurement linear data transformations machine learning algorithms finite data set;data models	Linear data transformations are essential operations in many machine learning algorithms, helping to make such models more flexible or to emphasize certain data directions. In particular for high dimensional data sets linear transformations are not necessarily uniquely determined, though, and alternative parameterizations exist which do not change the mapping of the training data. Thus, regularization is required to make the model robust to noise and more interpretable for the user. In this contribution, we characterize the group of transformations which leave a linear mapping invariant for a given finite data set, and we discuss the consequences on the interpretability of the models. We propose an intuitive regularization mechanism to avoid problems in under-determined configurations, and we test the approach in two machine learning models.	algorithm;machine learning;matrix regularization	Marc Strickert;Barbara Hammer;Thomas Villmann;Michael Biehl	2013	2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)	10.1109/CIDM.2013.6597211	semi-supervised learning;regularization perspectives on support vector machines;data modeling;mathematical optimization;computer science;artificial intelligence;theoretical computer science;machine learning;group method of data handling;data mining;mathematics;upper and lower bounds;statistics	ML	27.485899068659425	-40.035815892640635	193329
77869f274d4be4d4b4c438dbe7dff4baed521bd8	face recognition with pose variations and misalignment via orthogonal procrustes regression	databases;manifolds;training;collaboration;face recognition;lfw database face recognition pose variations orthogonal procrustes regression misalignment linear regression based method sparse representation collaborative representation based classifiers regression analysis based methods orthogonal procrustes problem optimal linear transformation face alignment cmu pie database cmu multipie database;regression analysis face recognition;face;face alignment face recognition regression analysis pose variations orthogonal procrustes problem opp;face face recognition databases training manifolds collaboration	A linear regression-based method is a hot topic in face recognition community. Recently, sparse representation and collaborative representation-based classifiers for face recognition have been proposed and attracted great attention. However, most of the existing regression analysis-based methods are sensitive to pose variations. In this paper, we introduce the orthogonal Procrustes problem (OPP) as a model to handle pose variations existed in 2D face images. OPP seeks an optimal linear transformation between two images with different poses so as to make the transformed image best fits the other one. We integrate OPP into the regression model and propose the orthogonal Procrustes regression (OPR) model. To address the problem that the linear transformation is not suitable for handling highly non-linear pose variation, we further adopt a progressive strategy and propose the stacked OPR. As a practical framework, OPR can handle face alignment, pose correction, and face representation simultaneously. We optimize the proposed model via an efficient alternating iterative algorithm, and experimental results on three popular face databases, such as CMU PIE database, CMU Multi-PIE database, and LFW database, demonstrate the effectiveness of our proposed method.	algorithm;alignment;attention deficit hyperactivity disorder;database;estimated;fits;facial recognition system;handling (psychology);iterative method;name;nonlinear system;open prosthetics project;published comment;regression analysis;sparse approximation;sparse matrix	Ying Tai;Jian Yang;Yigong Zhang;Lei Luo;Jianjun Qian;Yu Chen	2016	IEEE Transactions on Image Processing	10.1109/TIP.2016.2551362	facial recognition system;face;computer vision;manifold;computer science;machine learning;pattern recognition;mathematics;collaboration	Vision	26.092772202626563	-42.793253297025075	193370
4b2dcc591a44c38cc37ecf2822965ab146c50321	single-sample aeroplane detection in high-resolution optimal remote sensing imagery		In remote sensing images, detecting aeroplanes of special shapes is difficult due to limited number of samples. Without enough training samples, most supervised learning based algorithms will fail. Focusing on the specially-shaped aeroplanes in high-resolution optical remote sensing imagery, this paper presents a single-sample approach. The proposed approach takes one sample as input and directly searches for similar matches from the image. Unlike the supervised learning algorithms which extracts information from positive and negative samples, the hyperspectral algorithm estimates the statistics of background by analyzing the global information of the target image, needless to provide negative samples. Furthermore, this algorithm tries to find a hyperplane projected on which the background is compressed while the target is preserved, making it more data-adaptive than the conventional similarity measurements. Experiments on real data have presented the robustness of the proposed method.	algorithm;experiment;image resolution;machine learning;sensor;supervised learning	Bin Pan;Liming Wang;Xinran Yu;Zhenwei Shi	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518820	microsoft windows;remote sensing;robustness (computer science);hyperplane;kernel (linear algebra);supervised learning;computer vision;artificial intelligence;feature extraction;computer science;hyperspectral imaging	Vision	29.8299144486825	-43.41092688574342	194111
c34e48d637705ffb52360c2afb6b03efdeb680bf	subclass discriminant nonnegative matrix factorization for facial image analysis	facial expression recognition;subclass discriminant analysis;multiplicative updates;face recognition;nonnegative matrix factorization	Nonnegative Matrix Factorization (NMF) is among the most popular subspace methods, widely used in a variety of image processing problems. Recently, a discriminant NMF method that incorporates Linear Discriminant Analysis inspired criteria has been proposed, which achieves an efficient decomposition of the provided data to its discriminant parts, thus enhancing classification performance. However, this approach possesses certain limitations, since it assumes that the underlying data distribution is unimodal, which is often unrealistic. To remedy this limitation, we regard that data inside each class have a multimodal distribution, thus forming clusters and use criteria inspired by Clustering based Discriminant Analysis. The proposed method incorporates appropriate discriminant constraints in the NMF decomposition cost function in order to address the problem of finding discriminant projections that enhance class separability in the reduced dimensional projection space, while taking into account subclass information. The developed algorithm has been applied for both facial expression and face recognition on three popular databases. Experimental results verified that it successfully identified discriminant facial parts, thus enhancing recognition performance. & 2012 Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;database;facial recognition system;image analysis;image processing;linear discriminant analysis;linear separability;loss function;multimodal interaction;non-negative matrix factorization	Symeon Nikitidis;Anastasios Tefas;Nikos Nikolaidis;Ioannis Pitas	2012	Pattern Recognition	10.1016/j.patcog.2012.04.030	facial recognition system;mathematical optimization;kernel fisher discriminant analysis;computer science;machine learning;pattern recognition;optimal discriminant analysis;mathematics;linear discriminant analysis;non-negative matrix factorization;multiple discriminant analysis	AI	25.375116981221197	-41.34882735896454	194571
6353266413c38fa9288d82decaf08853b537c1db	multiple feature learning for hyperspectral image classification	nonlinear class boundaries multiple feature learning hyperspectral image classification feature extraction multiple kernel learning linear class boundaries;multiple feature learning hyperspectral imaging linear and nonlinear features;remote sensing feature extraction geophysical image processing hyperspectral imaging image classification learning artificial intelligence;kernel hyperspectral imaging feature extraction educational institutions training	Hyperspectral image classification has been an active topic of research in recent years. In the past, many different types of features have been extracted (using both linear and nonlinear strategies) for classification problems. On the one hand, some approaches have exploited the original spectral information or other features linearly derived from such information in order to have classes which are linearly separable. On the other hand, other techniques have exploited features obtained through nonlinear transformations intended to reduce data dimensionality, to better model the inherent nonlinearity of the original data (e.g., kernels) or to adequately exploit the spatial information contained in the scene (e.g., using morphological analysis). Special attention has been given to techniques able to exploit a single kind of features, such as composite kernel learning or multiple kernel learning, developed in order to deal with multiple kernels. However, few approaches have been designed to integrate multiple types of features extracted from both linear and nonlinear transformations. In this paper, we develop a new framework for the classification of hyperspectral scenes that pursues the combination of multiple features. The ultimate goal of the proposed framework is to be able to cope with linear and nonlinear class boundaries present in the data, thus following the two main mixing models considered for hyperspectral data interpretation. An important characteristic of the presented approach is that it does not require any regularization parameters to control the weights of considered features so that different types of features can be efficiently exploited and integrated in a collaborative and flexible way. Our experimental results, conducted using a variety of input features and hyperspectral scenes, indicate that the proposed framework for multiple feature learning provides state-of-the-art classification results without significantly increasing computational complexity.	computational complexity theory;computer vision;feature learning;kernel (operating system);linear separability;matrix regularization;multiple kernel learning;nonlinear system	Jun Li;Xin Huang;Paolo Gamba;Jos&#x00E9; M. Bioucas-Dias;Liangpei Zhang;Jon Atli Benediktsson;Antonio J. Plaza	2015	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2014.2345739	computer vision;feature detection;feature extraction;machine learning;pattern recognition;feature	ML	29.72908765643921	-44.477736797018835	195028
bd82d53a1e3b5e0836c62a6d92dceba5cd19cda6	improving dictionary learning using the itakura-saito divergence	sparse matrices error statistics iterative methods learning artificial intelligence minimisation signal representation;sparsity constraint dictionary learning nonnegative sparse representation itakura saito divergence euclidean distance;dictionaries measurement uncertainty sparse matrices atomic measurements noise minimization algorithm design and analysis;sparsity constraint itakura saito divergence nonnegative dictionary learning algorithm nonnegative sparse representation nnsr error measure euclidean distance euc coefficient matrix sparseness 1 1 norm minimization	This paper presents an improved and efficient algorithm for overcomplete, nonnegative dictionary learning for nonnegative sparse representation (NNSR) of signals. We adopt the Itakura-Saito (IS) divergence as the error measure, which is quite different from the conventional dictionary learning methods using the Euclidean (EUC) distance as the error measure. In addition, for enforcing the sparseness of coefficient matrix, we impose ℓ1-norm minimization as the sparsity constraint. Numerical experiments on recovery of a dictionary show that the proposed dictionary learning algorithm performs better than other currently available algorithms which use Euclidean distance as the error measure.	algorithm;coefficient;dictionary;end-user computing;euclidean distance;experiment;inpainting;itakura–saito distance;machine learning;neural coding;noise reduction;sparse approximation;sparse matrix;synthetic intelligence;taxicab geometry	Zhenni Li;Shuxue Ding;Yujie Li;Zunyi Tang;Wuhui Chen	2014	2014 IEEE China Summit & International Conference on Signal and Information Processing (ChinaSIP)	10.1109/ChinaSIP.2014.6889341	mathematical optimization;k-svd;machine learning;pattern recognition;mathematics	Vision	26.40975600381411	-39.57786118691956	195122
6fe15fc9f080402481718d165b1da9bfe4466d31	refined-graph regularization-based nonnegative matrix factorization		Nonnegative matrix factorization (NMF) is one of the most popular data representation methods in the field of computer vision and pattern recognition. High-dimension data are usually assumed to be sampled from the submanifold embedded in the original high-dimension space. To preserve the locality geometric structure of the data, k-nearest neighbor (k-NN) graph is often constructed to encode the near-neighbor layout structure. However, k-NN graph is based on Euclidean distance, which is sensitive to noise and outliers. In this article, we propose a refined-graph regularized nonnegative matrix factorization by employing a manifold regularized least-squares regression (MRLSR) method to compute the refined graph. In particular, each sample is represented by the whole dataset regularized with ℓ2-norm and Laplacian regularizer. Then a MRLSR graph is constructed based on the representative coefficients of each sample. Moreover, we present two optimization schemes to generate refined-graphs by employing a hard-thresholding technique. We further propose two refined-graph regularized nonnegative matrix factorization methods and use them to perform image clustering. Experimental results on several image datasets reveal that they outperform 11 representative methods.	cluster analysis;coefficient;computer vision;data (computing);encode;embedded system;euclidean distance;iteration;k-nearest neighbors algorithm;least squares;locality of reference;mathematical optimization;matrix regularization;non-negative matrix factorization;pattern recognition;refinement (computing);similarity measure;software bug;thresholding (image processing);truncation	Xuelong Li;Guosheng Cui;Yongsheng Dong	2017	ACM TIST	10.1145/3090312	adjacency matrix;machine learning;euclidean distance;cluster analysis;submanifold;artificial intelligence;computer science;k-nearest neighbors algorithm;mathematical optimization;pattern recognition;laplace operator;non-negative matrix factorization;least squares	AI	26.70474798413565	-40.233300673639675	195573
8c4f15623ee4aa8bff0e7ad02d9435f8889e0411	an asymptotically convex approach to discriminative coding	convex programming;dictionaries noise encoding vectors support vector machine classification dh hemts;atr mutual exclusivity discrimination asymptotically convex signal classification sparsity;mutual exclusivity operator asymptotically convex approach discriminative coding convex programming;dh hemts;sparsity;vectors;mutual exclusivity;dictionaries;signal classification;atr;support vector machine classification;signal classification convex programming encoding;asymptotically convex;encoding;discrimination;noise	We introduce a novel methodology for calculating discriminative codes for different classes of vectors with respect to the same dictionary. This is accomplished by introducing and quantifying the concept of `mutual exclusivity' between two classes of vectors (endowed possibly with different probabilistic structures) in a manner amenable to convex programming. We study theoretical properties of our mutual exclusivity operator and experimentally demonstrate its capability in generating effective discriminative codes that successfully incorporate both intra-class and inter-class characteristics. We conclude with a brief discussion of a generalization our mutual exclusivity operator to handle arbitrary number of classes, together with future directions emanating from this work.	code;convex optimization;dictionary;experiment	Raghu G. Raj	2012	2012 IEEE Statistical Signal Processing Workshop (SSP)	10.1109/SSP.2012.6319811	mathematical optimization;machine learning;pattern recognition;mathematics	Vision	28.392031583079174	-41.95707762225177	196637
960ee55bbd639af36cbec46f3022ed349eec078e	analysis of different subspace mixture models in handwriting recognition	principal component analysis laplace equations face recognition covariance matrix character recognition feature extraction vectors;handwriting recognition;gaussian processes;handwritten kannada characters subspace mixture model handwriting recognition principal component analysis fisher linear discriminant analysis gaussian mixture model decorrelated feature space cluster density gaussian distribution function laplacian transformed space;lpp;handwritten character recognition gaussian processes handwriting recognition;analysis;fld;mixture models;pca;handwritten character recognition;analysis handwriting recognition pca fld lpp mixture models	In this paper we explore, analyze and propose the idea of subspace mixture models such as Principal Component Analysis (PCA), Fisher's Linear Discriminant Analysis (FLD) and Laplacian in handwriting recognition. Statistically, Gaussian Mixture Models (GMMs) are among the most suppurate methods for clustering (though they are also used intensively for density estimation). By modeling each class into a mixture of several components and by performing the classification in the compact and decorrelated feature space it may result in better performance. To do this, each character class is partitioned into several clusters and each cluster density is estimated by a Gaussian distribution function in the PCA, FLD and Laplacian transformed space. The analysis of different mixture models are experimented out on handwritten Kannada characters.	cluster analysis;feature vector;handwriting recognition;linear discriminant analysis;mixture model;principal component analysis	V. N. Manjunath Aradhya;S. K. Niranjan	2012	2012 International Conference on Frontiers in Handwriting Recognition	10.1109/ICFHR.2012.178	speech recognition;computer science;machine learning;pattern recognition;mixture model;analysis;gaussian process;handwriting recognition;principal component analysis	ML	31.098714356963093	-38.93620031004947	198414
0a8cc6332d3f9a7605c9dde6f3b78d0b579761c0	contextual svm for hyperspectral classification using hilbert space embedding	support vector machines;hilbert spaces;hilbert space embedding;hilbert space embedding contextual support vector machine hyperspectral classification;image classification;support vector machines kernel hyperspectral imaging hilbert space standards training;support vector machines hilbert spaces image classification;hyperspectral classification;svm separating hyperplane contextual svm hyperspectral classification support vector machine technique hilbert space embedding principle local hyperspectral data distribution reproducing kernel hilbert space rkhs local spatial information spectral spatial information hyperspectral image hse;contextual support vector machine	In this paper, a contextual Support Vector Machine (SVM) technique based on the principle of Hilbert Space Embedding (HSE) of a local hyperspectral data distribution into an Reproducing Kernel Hilbert Space (RKHS) is proposed to optimally exploit the spectral and local spatial information of the hyperspectral image. The idea of embedding is to map hyperspectral pixels in a local neighborhood into a single point in the RKHS that can uniquely represent those pixels collectively. Previously, the authors have employed an HSE called empirical mean map to build the contextual SVM. In this work, a weighted empirical mean map is utilized to exploit the similarities and variation in the local spatial information. For every pixel, a small set of the neighboring pixels in a hyperspectral image are mapped into an RKHS induced by a certain kernel (Eg. Gaussian RBF kernel) and then, the embedded point of these group of pixels is obtained by calculating the weighted empirical mean of these mapped points. The weights are determined based on the distance between the pixel in consideration and its neighbors. An SVM separating hyperplane is built to maximize the margin between classes formed by weighted empirical means. The proposed technique showed significant improvement over the existing contextual and composite kernels on two hyperspectral image data sets.	embedded system;gaussian (software);hilbert space;kernel (operating system);pixel;radial basis function kernel;support vector machine	Prudhvi Gurram;Heesung Kwon	2012	2012 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2012.6352368	support vector machine;contextual image classification;topology;computer science;machine learning;pattern recognition;mathematics;hilbert space	Vision	28.69317556739818	-43.687290604623136	198553
1f050c1eeeda8d494bf1fb83d3cc8095719542eb	approximate sampling method for locally linear embedding	gaussian random fields framework;nonlinear manifold learning problem;gaussian processes;gaussian random field;sampling methods manifolds eigenvalues and eigenfunctions machine learning principal component analysis biological neural networks usa councils iterative algorithms neuroscience cognitive science;manifold learning;approximation theory;embedded systems;locally linear embedding system approximate sampling method nonlinear manifold learning problem gaussian random fields framework;sampling methods approximation theory embedded systems gaussian processes learning artificial intelligence;high dimensional data;approximate sampling method;learning artificial intelligence;sampling methods;local linear embedding;locally linear embedding system	We deal with the nonlinear manifold learning problem to find a low-dimensional structure in high-dimensional data. Based on Gaussian random fields framework, we propose an approximate sampling method for coordinates on the manifolds. Experimentally the mean of samples are shown to be almost equal to the coordinates obtained by locally linear embedding where the generated set of samples of coordinates show interesting variety.	approximation algorithm;arc diagram;experiment;linear algebra;nonlinear dimensionality reduction;nonlinear system;sampling (signal processing)	Hyun-Chul Kim;Kyu-Hwan Jung;Jaewook Lee	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371023	gaussian random field;sampling;action-angle coordinates;mathematical optimization;combinatorics;computer science;machine learning;gaussian process;mathematics;nonlinear dimensionality reduction;statistics;generalized coordinates;orthogonal coordinates;clustering high-dimensional data;approximation theory	ML	28.711457553729364	-38.43426604875464	198608
7fb74f5abab4830e3cdaf477230e5571d9e3ca57	polyhedral conic classifiers for visual object detection and classification		We propose a family of quasi-linear discriminants that outperform current large-margin methods in sliding window visual object detection and open set recognition tasks. In these tasks the classification problems are both numerically imbalanced u0026#x2013; positive (object class) training and test windows are much rarer than negative (non-class) ones u0026#x2013; and geometrically asymmetric u0026#x2013; the positive samples typically form compact, visually-coherent groups while negatives are much more diverse, including anything at all that is not a well-centred sample from the target class. It is difficult to cover such negative classes using training samples, and doubly so in open set applications where run-time negatives may stem from classes that were not seen at all during training. So there is a need for discriminants whose decision regions focus on tightly circumscribing the positive class, while still taking account of negatives in zones where the two classes overlap. This paper introduces a family of quasi-linear polyhedral conic discriminants whose positive regions are distorted L1 balls. The methods have properties and run-time complexities comparable to linear Support Vector Machines (SVMs), and they can be trained from either binary or positive-only samples using constrained quadratic programs related to SVMs. Our experiments show that they significantly outperform both linear SVMs and existing one-class discriminants on a wide range of object detection, open set recognition and conventional closed-set classification tasks.	coherence (physics);convex function;discriminant;epcc;experiment;feature vector;microsoft windows;numerical analysis;object detection;outline of object recognition;polyhedral;portable c compiler;proof-carrying code;scalability;stochastic gradient descent;stress ball;support vector machine;t-norm;taxicab geometry	Hakan Cevikalp;Bill Triggs	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.438	pattern recognition;support vector machine;robustness (computer science);sliding window protocol;computer science;open set;conic section;object detection;visualization;artificial intelligence;machine learning;binary number	Vision	27.374647839718467	-44.76574267494083	198848
b38c43e133bb608a09310ff26cca0b29d35b86d8	learning a mahalanobis distance metric for data clustering and classification	optimisation sous contrainte;constrained optimization;sistema interactivo;mahalanobis distance;evaluation performance;face pose estimation;learning algorithm;distance metric learning;image segmentation;performance evaluation;image processing;learning;distance de mahalanobis;evaluacion prestacion;mesure position;procesamiento imagen;metric;optimum global;natural images;algorithme apprentissage;global optimum;traitement image;journal;medicion posicion;systeme conversationnel;aprendizaje;optimizacion con restriccion;data clustering;apprentissage;machine learning;interactive system;segmentation image;signal classification;position measurement;distance metric;classification signal;metrico;interactive image segmentation;global optimization;classification automatique;automatic classification;side information;algoritmo aprendizaje;constrained optimization problem;clasificacion automatica;optimo global;metrique;pose estimation	Distance metric is a key issue in many machine learning algorithms. This paper considers a general problem of learning from pairwise constraints in the form of must-links and cannot-links. As one kind of side information, a must-link indicates the pair of the two data points must be in a same class, while a cannot-link indicates that the two data points must be in two different classes. Given must-link and cannot-link information, our goal is to learn a Mahalanobis distance metric. Under this metric, we hope the distances of point pairs in must-links are as small as possible and those of point pairs in cannot-links are as large as possible. This task is formulated as a constrained optimization problem, in which the global optimum can be obtained effectively and efficiently. Finally, some applications in data clustering, interactive natural image segmentation and face pose estimation are given in this paper. Experimental results illustrate the effectiveness of our algorithm.	binary search algorithm;cluster analysis;constrained optimization;constraint (mathematics);euclidean distance;iteration;loss function;mathematical optimization;optimization problem;performance;tracing (software)	Shiming Xiang;Feiping Nie;Changshui Zhang	2008	Pattern Recognition	10.1016/j.patcog.2008.05.018	computer vision;constrained optimization;metric;image processing;metric k-center;computer science;intrinsic metric;machine learning;pattern recognition;mathematics;k-server problem;global optimization	AI	30.18369581990759	-39.79882952352443	199080
c1e77440b7bb41238ef2398193156fa2641172b7	optimal feature selection for robust classification via l2,1-norms regularization	sparse matrices convergence of numerical methods face recognition feature selection image classification iterative methods learning artificial intelligence optimisation;conference paper;convergence robust classification l 2 1 norm regularization dimensionality reduction optimal feature selection classification ofsc orthogonal subspace learning jointly sparse feature selection jointly sparse feature representation iterative algorithm jointly sparse projection matrix jointly sparse representation matrix public face datasets action dataset;training face recognition robustness face convergence accuracy vectors	This paper aims to explore the optimal feature selection with dimensionality reduction and jointly sparse representation scheme for classification. The proposed method is called Optimal Feature Selection Classification (OFSC). Our model simultaneously learns an orthogonal subspace for jointly sparse feature selection and representation via l2,1-norms regularization. To solve the proposed model, an alternately iterative algorithm is proposed to optimize both the jointly sparse projection matrix and representation matrix. Experimental results on three public face datasets and one action dataset validate the quick convergence of our algorithm and show that the proposed method is more competitive than the state-of-the-art methods.	algorithm;data point;dimensionality reduction;feature selection;iterative method;manifold regularization;matrix regularization;return loss;sparse approximation;sparse matrix;vergence	Jiajun Wen;Wai Keung Wong;Jinrong Cui;Minghua Wan	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.99	mathematical optimization;machine learning;pattern recognition;sparse approximation;mathematics	Vision	25.763942999548302	-41.65526051528656	199331
110919f803740912e02bb7e1424373d325f558a9	statistical inference of gaussian-laplace distribution for person verification		Metric learning is an important issue in the person verification problem, which is to identify whether a pair of face or human body images is about the same person. Due to low running cost, the non-iterative statistical inference methods for metric learning show their efficiency and effectiveness to large scale datasets and on-line updating person verification applications. The KISSME method is a typical one that constructs the metric based on two assumptions that both of the discrepancy spaces of negative pairs and positive pairs should be Gaussian structures. However, we find that, in fact, the distribution of discrepancies of positive pairs might tend to the Laplace distribution rather than the Gaussian distribution. Based on this finding, we propose a metric learning method by exploiting Gaussian-Laplace distribution statistical inference, where the Gaussian distribution of negative discrepancies and the Laplace distribution of positive discrepancies are considered together. Experiments conducted on two human body datasets (VIPeR and Market-1501) and one face dataset (LFW) show its superiority in terms of effectiveness and efficiency as compared with the state-of-the-art approaches, no matter the appearance description is handcrafted or deep learned.	discrepancy function;experiment;iterative method;online and offline	Zheng Wang;Ruimin Hu;Yi Yu;Junjun Jiang;Jiayi Ma;Shin'ichi Satoh	2017		10.1145/3123266.3123421	body images;statistical inference;computer science;gaussian;laplace distribution;pattern recognition;artificial intelligence	AI	26.258123938138404	-44.108577840524916	199540
1a14f549e72d06704ee72e6e9f03228f43da4613	gabor wavelet based feature extraction and fusion for hyperspectral and lidar remote sensing data		In recent years, it has been found that the fusion processing of remote sensing data produced by multiple sensors is often effective for material classification. Specifically, the joint use of hyperspectral image (HSI) and Light Detection And Ranging (LiDAR) data for classification has been an active topic of research in remote sensing field. Since hyperspectral and LiDAR data provide complementary information (spectral reflectance, and vertical structure, respectively), one promising and challenging approach is to fuse these data in the information extraction procedure. In this paper, we propose an efficient feature extraction and fusion method based on Gabor wavelet, leading to a fusion of the spectral, spatial and elevation data. The core idea of the proposed fusion approach is stacking elevation and intensity data of LiDAR as additional channels to spectral bands. Our strategies are based on the Gabor feature stack structure, which are natural and effective. The features extracted by Gabor wavelets have proved to be discriminant features when considered for thematic classification in remote sensing applications especially when dealing with hyperspectral images due to their ability to extract joint spatial and spectrum information from HSI. Experimental results on the real hyperspectral image data have shown the better discriminative power of our approach for classification.	discriminant;feature extraction;gabor wavelet;horizontal situation indicator;information extraction;sensor;stacking	Sen Jia;Meng Zhang;Jiasong Zhu	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518351	remote sensing;elevation;gabor wavelet;information extraction;computer vision;feature extraction;artificial intelligence;remote sensing application;lidar;ranging;hyperspectral imaging;computer science	Robotics	30.735845853026778	-44.55367152035782	199643
