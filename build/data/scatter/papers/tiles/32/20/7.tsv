id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
26d70e3290ee1b974cc22bd9024c0ab3a2f587f0	artistic style transfer for videos and spherical images	style transfer;deep networks;artistic videos;video stylization	Manually re-drawing an image in a certain artistic style takes a professional artist a long time. Doing this for a video sequence single-handedly is beyond imagination. We present two computational approaches that transfer the style from one image (for example, a painting) to a whole video sequence. In our first approach, we adapt to videos the original image style transfer technique by Gatys et al. based on energy minimization. We introduce new ways of initialization and new loss functions to generate consistent and stable stylized video sequences even in cases with large motion and strong occlusion. Our second approach formulates video stylization as a learning problem. We propose a deep network architecture and training procedures that allow us to stylize arbitrary-length videos in a consistent and stable way, and nearly in real time. We show that the proposed methods clearly outperform simpler baselines both qualitatively and quantitatively. Finally, we propose a way to adapt these approaches also to 360$$^\circ $$ ∘ images and videos as they emerge with recent virtual reality hardware.	arbitrary code execution;baseline (configuration management);energy minimization;image;loss function;network architecture;virtual reality	Manuel Ruder;Alexey Dosovitskiy;Thomas Brox	2018	International Journal of Computer Vision	10.1007/s11263-018-1089-z	stylized fact;machine learning;computer vision;network architecture;initialization;artificial intelligence;computer science;imagination	Vision	25.44162168314582	-53.45980796630765	153482
4c97f3c66236649f1723e210104833278fb7f84e	language independent single document image super-resolution using cnn for improved recognition		Recognition of document images have important applications in restoring old and classical texts. The problem involves quality improvement before passing it to a properly trained OCR to get accurate recognition of the text. The image enhancement and quality improvement constitute important steps as subsequent recognition depends upon the quality of the input image. There are scenarios when high resolution images are not available and our experiments show that the OCR accuracy reduces significantly with decrease in the spatial resolution of document images. Thus the only option is to improve the resolution of such document images. The goal is to construct a high resolution image, given a single low resolution binary image, which constitutes the problem of single image super-resolution. Most of the previous work in super-resolution deal with natural images which have more information-content than the document images. Here, we use Convolution Neural Network to learn the mapping between low and the corresponding high resolution images. We experiment with different number of layers, parameter settings and non-linear functions to build a fast end-to-end framework for document image super-resolution. Our proposed model shows a very good PSNR improvement of about 4 dB on 75 dpi Tamil images, resulting in a 3% improvement of word level accuracy by the OCR. It takes less time than the recent sparse based natural image super-resolution technique, making it useful for real-time document recognition applications.	artificial neural network;autostereogram;bicubic interpolation;binary image;convolution;decibel;dots per inch;end-to-end principle;experiment;image editing;image resolution;linear function;nonlinear system;optical character recognition;peak signal-to-noise ratio;real-time clock;real-time locating system;resolution (logic);sparse matrix;super-resolution imaging	Ram Krishna Pandey;A. G. Ramakrishnan	2017	CoRR		computer vision;speech recognition;computer science;machine learning;pattern recognition;data mining;sub-pixel resolution	Vision	24.829399612451667	-52.091373196327744	153820
ab6c09ee2e466ceef2492f16472aeb76cd34009a	data augmentation for cnn-based people detection in aerial images		Much more than ever, many important places have deployed surveillance cameras for early detection of abnormal events and suspects. However, the monitoring ability of fixed cameras is significantly limited due to the low flexibility, blind spot, and obstacle occlusion. With high mobility, drones have high potential for supporting security surveillance. On the other hand, people detection plays a key role in intelligent surveillance system, and increasing deep learning-based methods show great results. However, the training data for aerial images are still few, even though there are many public datasets available. Thus, in this paper we research on data augmentation, try transforming general images to be aerial image-like, and make an attempt to improving the performance of deep learning-based people detection with existing datasets. The experiments conducted on the real aerial images collected by a camera drone show encouraging results.		Hua-Tsung Chen;C. H. Bryan Liu;Wen-Jiin Tsai	2018	2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)	10.1109/ICMEW.2018.8551501	computer vision;blind spot;artificial intelligence;obstacle;computer science;deep learning;aerial image;training set	Vision	29.732164576296164	-52.80308738047881	154455
48614aa9c01bf60cf47ad3db30fc69d67edd9714	a comparison of face and facial feature detectors based on the viola–jones general object detection framework	opencv;human computer interaction;haar wavelet;face datasets;haar wavelets;face and facial feature detection;info eu repo semantics article;public domain;system integration;120304 inteligencia artificial;detection rate;facial feature detection;facial features;face detection;object detection;open source	The human face provides useful information during interaction; therefore, any system integrating Vision-Based Human Computer Interaction requires fast and reliable face and facial feature detection. Different approaches have focused on this ability but only open source implementations have been extensively used by researchers. A good example is the Viola–Jones object detection framework that particularly in the context of facial processing has been frequently used. The OpenCV community shares a collection of public domain classifiers for the face detection scenario. However, these classifiers have been trained in different conditions and with different data but rarely tested on the same datasets. In this paper, we try to fill that gap by analyzing the individual performance of all those public classifiers presenting their pros and cons with the aim of defining a baseline for other approaches. Solid comparisons will also help researchers to choose a specific classifier for their particular scenario. The experimental setup also describes some heuristics to increase the facial feature detection rate while reducing the face false detection rate.	algorithmic efficiency;baseline (configuration management);computation;experiment;face detection;feature detection (computer vision);feature detection (web development);heuristic (computer science);human computer;human–computer interaction;image scaling;jones calculus;naruto shippuden: clash of ninja revolution 3;open-source software;opencv;real life;region of interest;sensor;statistical classification;viola–jones object detection framework	Modesto Castrillón Santana;Oscar Déniz-Suárez;Daniel Hernández-Sosa;Javier Lorenzo-Navarro	2010	Machine Vision and Applications	10.1007/s00138-010-0250-7	computer vision;face detection;public domain;speech recognition;object-class detection;computer science;viola–jones object detection framework;machine learning;pattern recognition;system integration	Vision	31.21319887647261	-58.34759411827294	154506
691796f86dbd7f076c8c86feef7144ac580f0acb	faster r-cnn features for instance search		Image representations derived from pre-trained Convolutional Neural Networks (CNNs) have become the new state of the art in computer vision tasks such as instance retrieval. This work explores the suitability for instance retrieval of image-and region-wise representations pooled from an object detection CNN such as Faster R-CNN. We take advantage of the object proposals learned by a Region Proposal Network (RPN) and their associated CNN features to build an instance search pipeline composed of a first filtering stage followed by a spatial reranking. We further investigate the suitability of Faster R-CNN features when the network is fine-tuned for the same objects one wants to retrieve. We assess the performance of our proposed system with the Oxford Buildings 5k, Paris Buildings 6k and a subset of TRECVid Instance Search 2013, achieving competitive results.	artificial neural network;baseline (configuration management);computer vision;convolutional neural network;geforce 200 series;geforce 6 series;geforce 900 series;image processing;national information infrastructure;object detection;reverse polish notation;titan rain;universal product code	Amaia Salvador;Xavier Giró;Ferran Marqués;Shin'ichi Satoh	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.56	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Vision	27.373729068988936	-54.1019701619688	154550
153889209bd5b44a70a5a58d607bf38d42898b0a	multimodal vehicle detection: fusing 3d-lidar and color camera data		Abstract Most of the current successful object detection approaches are based on a class of deep learning models called Convolutional Neural Networks (ConvNets). While most existing object detection researches are focused on using ConvNets with color image data, emerging fields of application such as Autonomous Vehicles (AVs) which integrates a diverse set of sensors, require the processing for multisensor and multimodal information to provide a more comprehensive understanding of real-world environment. This paper proposes a multimodal vehicle detection system integrating data from a 3D-LIDAR and a color camera. Data from LIDAR and camera, in the form of three modalities, are the inputs of ConvNet-based detectors which are later combined to improve vehicle detection. The modalities are: (i) up-sampled representation of the sparse LIDAR’s range data called dense-Depth Map (DM), (ii) high-resolution map from LIDAR’s reflectance data hereinafter called Reflectance Map (RM), and (iii) RGB image from a monocular color camera calibrated wrt the LIDAR. Bounding Box (BB) detections in each one of these modalities are jointly learned and fused by an Artificial Neural Network (ANN) late-fusion strategy to improve the detection performance of each modality. The contribution of this paper is two-fold: (1) probing and evaluating 3D-LIDAR modalities for vehicle detection (specifically the depth and reflectance map modalities), and (2) joint learning and fusion of the independent ConvNet-based vehicle detectors (in each modality) using an ANN to obtain more accurate vehicle detection. The obtained results demonstrate that (1) DM and RM are very promising modalities for vehicle detection, and (2) experiments show that the proposed fusion strategy achieves higher accuracy compared to each modality alone in all the levels of increasing difficulty (easy, moderate, hard) in KITTI object detection dataset.		Alireza Asvadi;Luis Garrote;Cristiano Premebida;Paulo Peixoto;Urbano Jose C. Nunes	2018	Pattern Recognition Letters	10.1016/j.patrec.2017.09.038	computer vision;mathematics;pattern recognition;deep learning;artificial neural network;convolutional neural network;object detection;lidar;rgb color model;color image;artificial intelligence;minimum bounding box	Vision	29.938334727986373	-53.15853395512444	154674
cc5d91b20c8769d1f040ff9a5166f76cc19d2d55	self-supervised learning of visual features through embedding images into text topic spaces		End-to-end training from scratch of current deep architectures for new computer vision problems would require Imagenet-scale datasets, and this is not always possible. In this paper we present a method that is able to take advantage of freely available multi-modal content to train computer vision algorithms without human supervision. We put forward the idea of performing self-supervised learning of visual features by mining a large scale corpus of multi-modal (text and image) documents. We show that discriminative visual features can be learnt efficiently by training a CNN to predict the semantic context in which a particular image is more probable to appear as an illustration. For this we leverage the hidden semantic structures discovered in the text corpus with a well-known topic modeling technique. Our experiments demonstrate state of the art performance in image classification, object detection, and multi-modal retrieval compared to recent self-supervised or natural-supervised approaches.	algorithm;computer vision;experiment;feature learning;imagenet;modal logic;object detection;spaces;supervised learning;text corpus;topic model	Lluís Gómez i Bigorda;Yash Patel;Marçal Rusiñol;Dimosthenis Karatzas;C. V. Jawahar	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.218	supervised learning;computer vision;topic model;artificial intelligence;visualization;scratch;pattern recognition;computer science;object detection;semantics;machine learning;text corpus;contextual image classification	Vision	26.41152046526894	-52.94301776280929	154925
6a8b1c533dcae83a93b2a33a34de9d1353154058	end-to-end bloody video recognition by audio-visual feature fusion		With the rapid development of Internet technology, the spread of bloody video has become increasingly serious, causing huge harm to society. In this paper, a bloody video recognition method based on audio-visual feature fusion is proposed to complement the limitation of the single vision-modality methods. In the absence of open bloody video data, this paper first constructed a database of bloody videos through web crawlers and data augmentation methods; then it used CNN and LSTM methods to extract the spatiotemporal features of visual channels. Meanwhile, the audio channel features were extracted directly from the original waveforms using the 1D convolutional network. Finally, the neural network based on the audio-visual feature fusion layer was constructed to achieve the early fusion of multimodal cues. The accuracy of the proposed method on the bloody video test data is 95%. The experimental results on self-built bloody video databases demonstrate that the extracted audio-visual feature representations are effective and the proposed multimodal fusion model can obtain the better and discriminative recognition performance than the single-channel model.		Congcong Hou;Xiaoyu Wu;Ge Wang	2018		10.1007/978-3-030-03398-9_43	the internet;fusion;artificial neural network;feature extraction;end-to-end principle;test data;web crawler;pattern recognition;computer science;communication channel;artificial intelligence	Vision	26.37642044538188	-55.63802240806378	155201
e9d94b2d61b0b3e06751cccad5e400102ca147c1	cumulative nets for edge detection		Lots of recent progress have been made by using Convolutional Neural Networks (CNN) for edge detection. Due to the nature of hierarchical representations learned in CNN, it is intuitive to design side networks utilizing the richer convolutional features to improve the edge detection. However, different side networks are isolated, and the final results are usually weighted sum of the side outputs with uneven qualities. To tackle these issues, we propose a Cumulative Network (C-Net), which learns the side network cumulatively based on current visual features and low-level side outputs, to gradually remove detailed or sharp boundaries to enable high-resolution and accurate edge detection. Therefore, the lower-level edge information is cumulatively inherited while the superfluous details are progressively abandoned. In fact, recursively Learningwhere to remove superfluous details from the current edge map with the supervision of a higher-level visual feature is challenging. Furthermore, we employ atrous convolution (AC) and atrous convolution pyramid pooling (ASPP) to robustly detect object boundaries at multiple scales and aspect ratios. Also, cumulatively refining edges using high-level visual information and lower-lever edge maps is achieved by our designed cumulative residual attention (CRA) block. Experimental results show that our C-Net sets new records for edge detection on both two benchmark datasets: BSDS500 (i.e., .819 ODS, .835 OIS and .862 AP) and NYUDV2 (i.e., .762 ODS, .781 OIS, .797 AP). C-Net has great potential to be applied to other deep learning based applications, e.g., image classification and segmentation.	benchmark (computing);computer vision;convolution;convolutional neural network;credit bureau;deep learning;edge detection;feature learning;high- and low-level;image resolution;map;operational data store;pyramid (geometry);recursion;weight function	Jingkuan Song;Zhilong Zhou;Lianli Gao;Xing Xu;Heng Tao Shen	2018		10.1145/3240508.3240688	convolutional neural network;computer vision;recursion;deep learning;convolution;pyramid;contextual image classification;segmentation;edge detection;artificial intelligence;computer science	Vision	26.483882897184188	-52.443564505644304	155324
818483b3ff8c424d80b5b78d27b1df1865474a4f	advanced satellite image classification of various resolution image using a novel approach of deep neural network classifier	satellite image;deep neural network;multiview image registration	Image registration is computationally intensive and applied in a variety of applications, for example, multispectral classification, change recognition, climate prediction and multi-view analysis in GIS and medicine. There are three types of registration namely multi-view, multimodal and multi-temporal. In multi-view based registration, the images of the same scene taken at different viewpoints are analyzed and modeled for the requirement. Hence stereoscopic image sequences of the same view are acquired, and accurate comparison for the image classification is essential. This paper presents a robust method which has three steps. The first phase includes obtaining hyper spectral (satellite) images and preprocessing of them, the second period subdivides into image blocks for alignment, and the final step focuses on classification based on hyper graph structure using deep learning approach. For processing of satellite images, a new method linear iterative clustering and deep neural network classification are employed. Previous works in remote sensing applications involve training samples and hence prior knowledge of image sets which incurs more computational time. The implementation of this method shows an automatic, achieving better accuracy and dynamic reconfigurable image registration in reduced complexity. The mathematical model used is hidden markov chain model for clustering which provides region-wise feature construction for evaluating region shape and contextual information. The work yields classification accuracy of 94.12% which is far better than past outcomes in this engaged field of research. The execution of the usage is examined, a comparison is additionally influenced regarding false classification ratio, time complexity and clustering accuracy is demonstrated.		S. Jayanthi;C. Vennila	2019	Wireless Personal Communications	10.1007/s11277-018-6024-7	computer vision;time complexity;computer science;real-time computing;deep learning;multispectral image;artificial neural network;hidden markov model;contextual image classification;cluster analysis;image registration;artificial intelligence	Vision	28.650626182505743	-53.43430455248903	155738
09b2dd48ca6e0e045590c507cd1789abf698a038	is convolutional network competitive for vision method in the furniture dowel quality control?		The paper presents a method of the furniture dowel quality control based on the convolutional neural network. We applied AlexNet, testing the ability of transfer learning. The paper presents details of the method and experiments performed to justify our solution on images datasets collected from real dowels production. The outcomes are compared with the results of baseline vision method. The experiments show that both methods have their strengths and weaknesses, but they complement each other.	algorithm;artificial neural network;baseline (configuration management);control system;convolutional neural network;experiment;graphics processing unit;mind;nvidia tesla;situated;test set	Urszula Markowska-Kaczmar;Michal Skiba	2018	2018 Innovations in Intelligent Systems and Applications (INISTA)	10.1109/INISTA.2018.8466279	transfer of learning;convolutional neural network;visualization;dowel;feature extraction;machine learning;strengths and weaknesses;computer science;artificial intelligence	Robotics	24.79254806865192	-55.329892465399524	155906
c36ae7c5e9f9f992a5939e07283183707ee0a787	stuffnet: using ‘stuff’ to improve object detection	image segmentation;training;object detection proposals feature extraction training image segmentation semantics context;semantics;feature extraction;proposals;context;object detection	We propose a Convolutional Neural Network (CNN) based algorithm – StuffNet – for object detection. In addition to the standard convolutional features trained for region proposal and object detection [33], StuffNet uses convolutional features trained for segmentation of objects and 'stuff' (amorphous categories such as ground and water). Through experiments on Pascal VOC 2010, we show the importance of features learnt from stuff segmentation for improving object detection performance. StuffNet improves performance from 18.8% mAP to 23.9% mAP for small objects. We also devise a method to train StuffNet on datasets that do not have stuff segmentation labels. Through experiments on Pascal VOC 2007 and 2012, we demonstrate the effectiveness of this method and show that StuffNet also significantly improves object detection performance on such datasets.	algorithm;computer multitasking;convolutional neural network;experiment;internationalization and localization;multi-task learning;object detection;reverse polish notation	Samarth Brahmbhatt;Henrik I. Christensen;James Hays	2017	2017 IEEE Winter Conference on Applications of Computer Vision (WACV)	10.1109/WACV.2017.109	computer vision;object-class detection;feature extraction;computer science;viola–jones object detection framework;machine learning;segmentation-based object categorization;pattern recognition;semantics;image segmentation;scale-space segmentation	Vision	31.589917190552327	-54.45546845741088	156161
94e52b257c5feb97f5fd522bc626a1ad928c4e4a	analysis and fusion of 2d and 3d images applied for detection and recognition of traffic signs using a new method of features extraction in conjunction with deep learning		This paper presents a system for detection and recognition of traffic signs using 2D images and 3D scene data. The detection and recognition of the 3D structures (pole and signs) and the classification of the traffic signs is based on a new 3D features extraction method and the use of a Deep Learning method. The traffic sign recognition aims to increase road safety considering autonomous and semi-autonomous intelligent robotic vehicles in structured environments with traffic rules (urban streets or roads/highways). The proposed system can be used as an Advanced Driver Assistance System (ADAS) to help human drivers to increase safety and to respect the traffic rules while driving the car. It can also be adopted in fully autonomous vehicles, in the task of detecting traffic signs, making it possible to adapt the vehicle navigation control according to the local traffic rules. The system must be able to detect traffic signs, using 3D data in point clouds, and classify several different traffic signs, using 2D data considering colors, textures and shapes information (e.g. maximum speed allowed, stop, slow down, turn ahead, pedestrian crossing). The results are promising and very satisfactory, we obtained an accuracy of 97.64% in the 2D classification task and 76% accuracy in the single frame 3D detection task. These results were obtained using for testing a well-known dataset of street scenes with traffic signs, The KITTI Vision Benchmark Suite, and also another traffic sign benchmark dataset, the INI - German Traffic Sign Benchmark.	3d computer graphics;architecture design and assessment system;artificial neural network;autonomous car;autonomous robot;benchmark (computing);cambridge structural database;color;computer vision;deep learning;fuzzy logic;learning relationship management;object detection;point cloud;real-time computing;semiconductor industry;sensor;traffic sign recognition	Diego Renan Bruno;Daniel O. Sales;Jean Amaro;Fernando Santos Osório	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489538	task analysis;point cloud;traffic sign recognition;pedestrian crossing;pattern recognition;feature extraction;fusion;deep learning;artificial neural network;artificial intelligence;computer science	Robotics	31.770079378366493	-55.914685761894475	156375
4dfed1de379a46d5f3693bcc1e35bdc3329167be	content-based image retrieval using growing hierarchical self-organizing quadtree map	hierarchical structure;incremental learning;self organization;content based image retrieval;growing hierarchical self organizing quadtree map;relevance feedback;neural network;image distance	In this paper, a growing hierarchical self-organizing quadtree map (GHSOQM) is proposed and used for a content-based image retrieval (CBIR) system. The incorporation of GHSOQM in a CBIR system organizes images in a hierarchical structure. The retrieval time by GHSOQM is less than that by using direct image comparison using a flat structure. Furthermore, the ability of incremental learning enables GHSOQM to be a prospective neural-network-based approach for CBIR systems. We also propose feature matrices, image distance and relevance feedback for region-based images in the GHSOQM-based CBIR system. Experimental results strongly demonstrate the effectiveness of the proposed system. 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	artificial neural network;content-based image retrieval;organizing (structure);pattern recognition;prospective search;quadtree;radio frequency;relevance feedback;self-organization;self-organizing map;tree structure	Sitao Wu;Md Mahmudur Rahman;Tommy W. S. Chow	2005	Pattern Recognition	10.1016/j.patcog.2004.10.005	computer vision;self-organization;computer science;machine learning;pattern recognition;artificial neural network	Vision	27.810985076144252	-56.810864726309795	156544
18c592385640853bca7eb26c19ee82edaeae86cf	face liveness detection scheme with static and dynamic features		Face liveness detection is a significant research topic in face-based online authentication. The current face liveness detection approaches utilize either static or dynamic features, but not both. In fact, the dynamic and static features have different advantages in face liveness detection. In this paper, we propose a scheme combining dynamic and static features to capture merits of them for face liveness detection. First, the dynamic maps are captured from the inter-frame motion in the video, which investigates motion information of the face in the video. Then, with a Convolutional Neural Network (CNN), the dynamic and static features are extracted from the dynamic maps and the frame images, respectively. Next, in CNN, the fully connected layers containing the dynamic and static features are concatenated to form a fused feature. Finally, the fused features are used to train a binary Support Vector Machine (SVM) classifier, which classifies the frames into two categories, i.e. frame with real or fake face...	liveness	Lifang Wu;Yaowen Xu;Meng Jian;Xiao Xu;Wei Qi	2018	IJWMIP	10.1142/S0219691318400015	mathematical optimization;support vector machine;convolutional neural network;mathematics;concatenation;deep learning;liveness;binary number;pattern recognition;artificial intelligence	Logic	29.002853842758675	-52.172527291601654	156627
6c54e47202e7365c47c6fd68508e3efd48892d2b	scene nudity level detection with deep nets	support vector machines;convolution;support vector machines multimedia communication internet digital multimedia broadcasting convolution visualization transform coding;lear human scene nudity level detection deep net configurations deep convolution layers net modelling net configuration support vector machine svm success rate test stage training stage regular level semi nudity level full nudity level sun2012;transform coding;visualization;digital multimedia broadcasting;support vector machines feedforward neural nets object detection;internet;deep nets visual nudity detection;multimedia communication	In this paper, we present an approach that can detect scene nudity level with high precision using different deep net configurations. For this purpose, a recent approach [1] which has intense and very deep convolution layers is used. During net modelling, we strive to obtain most successful net configuration by comparing different Dropout models and image sizes -64 × 64, 128 × 128-. Additionally, leveraging the generalization capability of Support Vector Machine (SVM), improvement on success rate is demonstrated by retraining the features obtained at different output levels of the nets with SVM. At test and training stages, scene is investigated under three nudity levels: regular, semi-nudity and full-nudity. In order to evaluate false alarm rates of the net models, tests are conducted on different datasets which are SUN2012, LEAR Human and a dataset contains only semi-nudity samples besides validation set determined for each class. The results indicate that high precision rates can be achieved with low false alarm rate exploiting deep net models.	convolution;deep web;dropout (neural networks);semiconductor industry;support vector machine	Savas Özkan;Ersin Esen;Ilkay Atil;Gozde Bozdagi Akar	2016	2016 24th Signal Processing and Communication Application Conference (SIU)	10.1109/SIU.2016.7496178	simulation;computer science;theoretical computer science;machine learning	ML	26.36311012904174	-54.15981268372241	157498
322a7dad274f440a92548faa8f2b2be666b2d01f	pyramid scene parsing network		Scene parsing is challenging for unrestricted open vocabulary and diverse scenes. In this paper, we exploit the capability of global context information by different-region-based context aggregation through our pyramid pooling module together with the proposed pyramid scene parsing network (PSPNet). Our global prior representation is effective to produce good quality results on the scene parsing task, while PSPNet provides a superior framework for pixel-level prediction. The proposed approach achieves state-of-the-art performance on various datasets. It came first in ImageNet scene parsing challenge 2016, PASCAL VOC 2012 benchmark and Cityscapes benchmark. A single PSPNet yields the new record of mIoU accuracy 85.4% on PASCAL VOC 2012 and accuracy 80.2% on Cityscapes.	benchmark (computing);imagenet;parsing expression grammar;pixel;vocabulary	Hengshuang Zhao;Jianping Shi;Xiaojuan Qi;Xiaogang Wang;Jiaya Jia	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2017.660	computer vision;machine learning;artificial intelligence;pooling;pattern recognition;artificial neural network;feature extraction;parsing;pyramid;image segmentation;computer science;vocabulary	Vision	27.072613370106097	-52.409248518429386	158367
8899094797e82c5c185a0893896320ef77f60e64	non-local neural networks		Both convolutional and recurrent operations are building blocks that process one local neighborhood at a time. In this paper, we present non-local operations as a generic family of building blocks for capturing long-range dependencies. Inspired by the classical non-local means method [4] in computer vision, our non-local operation computes the response at a position as a weighted sum of the features at all positions. This building block can be plugged into many computer vision architectures. On the task of video classification, even without any bells and whistles, our nonlocal models can compete or outperform current competition winners on both Kinetics and Charades datasets. In static image recognition, our non-local models improve object detection/segmentation and pose estimation on the COCO suite of tasks. Code will be made available.		Xiaolong Wang;Ross B. Girshick;Abhinav Gupta;Kaiming He	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00813	machine learning;convolutional code;artificial intelligence;computer vision;computer science;pose;pattern recognition;artificial neural network;object detection;convolution;coco	Vision	27.143953357046815	-53.4586460649357	158762
090079e20d2d5a79f972f46ec6974c5d4b80428b	object detection with convolutional context features		A novel extension to Hızlı B-ESA object detection algorithm is proposed in order to learn convolutional context features for determining boundaries of objects better. For input images, the hypothesis windows and their context around those windows are learned through convolutional layers as two pa­rallel networks. The resulting object and context feature maps are combined in such a way that they preserve their spatial relationship. The proposed algorithm is trained and evaluated on PASCAL VOC 2007 detection benchmark dataset and yielded improvements in performance over state-of-the-art, for almost all classes, especially the ones with distinctive context.	algorithm;benchmark (computing);esa;feature model;map;microsoft windows;object detection	Emre Can Kaya;A. Aydin Alatan	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960502	object-class detection;computer vision;computer science;artificial intelligence;pattern recognition;feature extraction;object detection;viola–jones object detection framework	Vision	31.742450318746528	-53.25058163225179	158818
e263acc5282260248c1e8df73149c410f4757835	road user abnormal trajectory detection using a deep autoencoder		In this paper, we focus on the development of a method that detects abnormal trajectories of road users at traffic intersections. The main difficulty with this is the fact that there are very few abnormal data and the normal ones are insufficient for the training of any kinds of machine learning model. To tackle these problems, we proposed the solution of using a deep autoencoder network trained solely through augmented data considered as normal. By generating artificial abnormal trajectories, our method is tested on four different outdoor urban users scenes and performs better compared to some classical outlier detection methods.	anomaly detection;autoencoder;convolutional neural network;film-type patterned retarder;interactivity;machine learning	Pankaj Raj Roy;Guillaume-Alexandre Bilodeau	2018		10.1007/978-3-030-03801-4_65	autoencoder;trajectory;computer science;pattern recognition;artificial intelligence	AI	26.260255455813343	-54.23837707075792	160313
d488dad9fa81817c85a284b09ebf198bf6b640f9	fchd: a fast and accurate head detector		In this paper, we propose FCHD-Fully Convolutional Head Detector, which is an end-to-end trainable head detection model, which runs at 5 fps and with 0.70 average precision (AP), on a very modest GPU. Recent head detection techniques have avoided using anchors as a starting point for detection especially in the cases where the detection has to happen in the wild. The reason is poor performance of anchorbased techniques under scenarios where the object size is small. We argue that a good AP can be obtained with carefully designed anchors, where the anchor design choices are made based on the receptive field size of the hidden layers. Our contribution is two folds. 1) A simple fully convolutional anchor based model which is end-to-end trainable and has a very low inference time. 2) Carefully chosen anchor sizes which play a key role in getting good average precision. Our model achieves comparable results than many other baselines on challenging head detection dataset like BRAINWASH. Along with accuracy, our model has least runtime among all the baselines along with modest hardware requirements which makes it suitable for edge deployments in surveillance applications. The code is made open-source at https://github.com/aditya-vora/FCHDFully-Convolutional-Head-Detector.	end-to-end principle;graphics processing unit;html element;information retrieval;nvidia quadro;open-source software;requirement;sensor	Aditya Vora	2018	CoRR		pattern recognition;artificial intelligence;computer science;receptive field;detector;inference	Vision	27.87300958519663	-52.963142841508436	160535
c12291be31a4cc17097719f5de60f7e3bb157174	svm-based human action recognition and its remarkable motion features discovery algorithm	generalization error;support vector;action recognition;parameter optimization	Motivation, Problem Statement, Related Works This paper proposes a discovery algorithm of knowledge of remarkable motion features in daily life action recognition based on Support Vector Machine. The main characteristics of the proposed method are 1)basic scheme of the algorithm is based on Support Vector Learning and its generalization error, 2)remarkable motion features are discovered in response to kernel parameters optimization through generalization error minimization. Experimental results show that the proposed algorithm makes the recognition system robust and finds remarkable motion features that are intuitive for human.. Recognizing human actions has potential to contribute to intuitive communication between human and machine, such as human-computer interaction, search engine for multi-media databases, or intelligent video editing. It may be applied to design some level of humanoid actions efficiently. It is proper to divide the process of action recognition into the following two phases. The former is to get time series of 3D body motion structurally from some instruments. The latter is to symbolize these kinds of motion to action names. There are many researches which use video image as input. For example, Starner et al. constructed a sign language recognition based on HMM[1], and Wilson et.al made a gesture recognition system[2]. However, usually the main work of such researches is on how to acquire motion robustly, since the time series image processing is still a difficult problem.	algorithm;database;generalization error;gesture recognition;human–computer interaction;image processing;mathematical optimization;support vector machine;thad starner;time series;web search engine	Taketoshi Mori;Masamichi Shimosaka;Tomomasa Sato	2004		10.1007/11552246_2	support vector machine;computer science;artificial intelligence;machine learning;pattern recognition;generalization error	AI	30.258336321083494	-58.994047448934296	160875
540c3e3b6945ca3ce5c54608ed50355594399cb2	esir: end-to-end scene text recognition via iterative image rectification.		Automated recognition of various texts in scenes has been a research challenge for years, largely due to arbitrary text appearance variations in term of perspective distortion, text line curvature, text styles as well as different types of imaging artifacts. The recently developed Convolutional Neural Networks (CNNs) are capable of learning robust representations under the presence of imaging artifacts and changes in text fonts, sizes and colors, but still face various problems while dealing with scene texts with perspective and curvature distortions. In this work, we propose an end-to-end trainable scene text recognition system via iterative rectification (ESIR), which iteratively removes perspective distortion and text line curvature as driven by better scene text recognition performance. An innovative rectification network is developed, where a line-fitting transformation is designed to estimate the pose of text lines in scenes. In addition, an iterative rectification framework is developed which corrects perspective distortion and text line curvature iteratively towards a fronto-parallel view and optimal scene text recognition. Further, the ESIR is robust to parameter initialization and convenient to train, where the training needs only scene text images and word-level annotations as required by most scene text recognition systems. Extensive experiments over a number of public datasets show that the ESIR is capable of correcting various scene text distortions accurately, achieving superior scene text recognition performance for both normal scene text images and those suffering from severe perspective and curvature distortions.		Fangneng Zhan;Shijian Lu	2018	CoRR		perspective distortion;image rectification;artificial intelligence;rectification;pattern recognition;initialization;curvature;computer science;end-to-end principle	Vision	30.8230641831023	-53.69398634024392	161150
15703730a4a0f1675e390ea63eab8d316084bc28	cooperative distributed object classification for multiple robots with audio features	real time visualization;computers;signal classification cepstral analysis multi robot systems object tracking pattern classification;feature vector;mel frequency cepstral coefficient;training data;cepstral analysis;distributed objects;robots training data mel frequency cepstral coefficient speech recognition support vector machine classification tiles computers;robots;object tracking;signal classification;multi robot systems;classification system;knn classifier cooperative distributed object classification multiple robot system audio feature audio classifier real time visual object tracking system mel frequency cepstral coefficient dynamic time warping dtw approach k nearest neighbor classifier;pattern classification;speech recognition;support vector machine classification;k nearest neighbor;tiles;object classification;support vector machine;dynamic time warping	This paper explains the methodology for an object classification system using audio features for the purpose of integrating the audio classifier into a real time visual object tracking system in order to more accurately track and describe objects of interest. Four objects are classified by the sounds they produce using Mel-Frequency Cepstral Coefficients (MFCC). These features are classified using a Dynamic Time Warping (DTW) approach along with a k Nearest Neighbor (kNN) classifier. In particular, this paper improves upon the best method of a survey [2] that uses MFCC and DTW. In this paper we propose a method that builds on using only MFCC and DTW. We suggest that once the costs from MFCC and DTW are computed they be used as feature vectors to be classified by another classification method. The results show a 24% improvement over using only MFCC with DTW. These results prove the usefulness of the joint classification system which can be integrated into a multiple robot system.	coefficient;distributed object;dynamic time warping;feature vector;mel-frequency cepstrum;robot;statistical classification;tracking system	Daniel McGibney;Takayuki Umeda;Kousuke Sekiyama;Hiro Mukai;Toshio Fukuda	2011	2011 International Symposium on Micro-NanoMechatronics and Human Science	10.1109/MHS.2011.6102174	speech recognition;computer science;machine learning;dynamic time warping;pattern recognition	Robotics	31.77638654121542	-58.86871696194982	161823
bbdf3a802314fb96a820a0ddaa510a322e1a38f1	utilizing google images for training classifiers in crf-based semantic segmentation	conditional random fields;saliency filter;classifier learning;semantic segmentation;google images		conditional random field	Rizki Perdana Rangkuti;Vektor Dewanto;Aprinaldi;Wisnu Jatmiko	2016	JACIII	10.20965/jaciii.2016.p0455	computer vision;computer science;machine learning;pattern recognition;scale-space segmentation;conditional random field	Vision	29.716602126947176	-57.342843362986336	162293
7b6b49adf60d56d1b33b428fdf66aff7426fca6e	survey on deep learning techniques for person re-identification task		Intelligent video-surveillance is currently an active research field in computer vision and machine learning techniques. It provides useful tools for surveillance operators and forensic video investigators. Person reidentification (PReID) is one among these tools. It consists of recognizing whether an individual has already been observed over a camera in a network or not. This tool can also be employed in various possible applications such as off-line retrieval of all the video-sequences showing an individual of interest whose image is given a query, and online pedestrian tracking over multiple camera views. To this aim, many techniques have been proposed to increase the performance of PReID. Among the systems, many researchers utilized deep neural networks (DNNs) because of their better performance and fast execution at test time. Our objective is to provide for future researchers the work being done on PReID to date. Therefore, we summarized state-of-the-art DNN models being used for this task. A brief description of each model along with their evaluation on a set of benchmark datasets is given. Finally, a detailed comparison is provided among these models followed by some limitations that can work as guidelines for future research.		Bahram Lavi;Mehdi Fatan Serj;Ihsan Ullah	2018	CoRR		machine learning;operator (computer programming);deep learning;artificial neural network;computer science;artificial intelligence	Vision	29.3101053910562	-52.78497977381845	162459
8c5330bd2045cbce0b5cff79cdc613bb91c161e6	using convolutional networks and satellite imagery to identify patterns in urban environments at a large scale		Urban planning applications (energy audits, investment, etc.) require an understanding of built infrastructure and its environment, i.e., both low-level, physical features (amount of vegetation, building area and geometry etc.), as well as higher-level concepts such as land use classes (which encode expert understanding of socio-economic end uses). This kind of data is expensive and labor-intensive to obtain, which limits its availability (particularly in developing countries). We analyze patterns in land use in urban neighborhoods using large-scale satellite imagery data (which is available worldwide from third-party providers) and state-of-the-art computer vision techniques based on deep convolutional neural networks. For supervision, given the limited availability of standard benchmarks for remote-sensing data, we obtain ground truth land use class labels carefully sampled from open-source surveys, in particular the Urban Atlas land classification dataset of $20$ land use classes across $~300$ European cities. We use this data to train and compare deep architectures which have recently shown good performance on standard computer vision tasks (image classification and segmentation), including on geospatial data. Furthermore, we show that the deep representations extracted from satellite imagery of urban environments can be used to compare neighborhoods across several cities. We make our dataset available for other machine learning researchers to use for remote-sensing applications.	artificial neural network;computer vision;convolutional neural network;encode;ground truth;high- and low-level;limited availability;machine learning;open-source software	Adrian Albert;Jasleen Kaur;Marta C. Gonzalez	2017		10.1145/3097983.3098070	convolutional neural network;machine learning;geospatial analysis;vegetation;computer science;data mining;artificial intelligence;land use;ground truth;satellite imagery;contextual image classification;urban planning	ML	27.897933506126886	-53.98399920130533	162663
bfdd0227cec48acf93d66f10298fc5078024a5f5	adatest: an efficient statistical test framework for test escape screening	standards;training;semiconductor device measurement;runtime;production;robustness;real time systems	Statistical analyses based on production test data can help identify test escapes, which are chips that pass the test program but fail later at system-level test or in field. Such analyses do not require extra physical measurements and can be referred to as statistical tests. For designing effective statistical tests, this paper investigates the use of a learning framework based on Adaptive Boosting, which has demonstrated great success in real-time face and object recognition. The framework is composed of a cascade of AdaBoost classifiers, each of which uses a small set of most relevant features that are automatically selected in the training phase, to identify a subset of test escapes. This framework therefore generates only the features that are most relevant for classification and significantly reduces the runtime and memory usage for statistical tests during test application. We also propose a new feature set to characterize the chips under test and demonstrate that including the new feature set as input to the proposed feature selection framework could reveal more test escapes.	adaboost;feature selection;html5 in mobile devices;outline of object recognition;real-time clock;real-time computing;test automation;test data	Fan Lin;Chun-Kai Hsu;Kwang-Ting Cheng	2015	2015 IEEE International Test Conference (ITC)	10.1109/TEST.2015.7342391	simulation;computer science;engineering;machine learning;data mining;test case;statistics;robustness	EDA	25.55735334076344	-58.757071562446065	163297
43e7bc2ec96e234e28da73a7116ba2dbd2c2495d	selecting suitable image retargeting methods with multi-instance multi-label learning	multi instance multi label learning;会议论文;image retargeting;method selection;image characteristic analysis	Althogh the diversity of mobile devices brings in image retargeting technique to effectively display images on various screens, no existing image retargeting method can handle all images well. In this paper, we propose a novel approach to select suitable image retargeting methods solely based on original image characteristic, which can obtain acceptable selection accuracy with low computation cost. First, the original image is manually annotated with several simple features. Then, suitable methods are automatically selected from candidate image retargeting methods using multi-instance multi-label learning. Finally, target images are generated by the selected methods. Experiments demonstrate the effectiveness of the proposed approach.	computation;experiment;mobile device;multi-label classification;retargeting;seam carving;simple features	Muyang Song;Tongwei Ren;Yan Liu;Jia Bei;Zhihong Zhao	2013		10.1007/978-3-319-02753-1_42	computer vision;computer science;machine learning;multimedia	Vision	31.557223823686616	-52.887309684036104	163392
55f3cc98359c980bf325899e9aaec1e6c95a2bea	deepshape: deep-learned shape descriptor for 3d shape retrieval	3d shape retrieval heat kernel signature heat diffusion auto encoder fisher discrimination criterion;kernel;shape three dimensional displays heating kernel feature extraction neurons solid modeling;heating;shape;fisher discrimination criterion 3d shape retrieval heat kernel signature heat diffusion auto encoder;three dimensional displays;feature extraction;solid modeling;neurons	Complex geometric variations of 3D models usually pose great challenges in 3D shape matching and retrieval. In this paper, we propose a novel 3D shape feature learning method to extract high-level shape features that are insensitive to geometric deformations of shapes. Our method uses a discriminative deep auto-encoder to learn deformation-invariant shape features. First, a multiscale shape distribution is computed and used as input to the auto-encoder. We then impose the Fisher discrimination criterion on the neurons in the hidden layer to develop a deep discriminative auto-encoder. Finally, the outputs from the hidden layers of the discriminative auto-encoders at different scales are concatenated to form the shape descriptor. The proposed method is evaluated on four benchmark datasets that contain 3D models with large geometric variations: McGill, SHREC’10 ShapeGoogle, SHREC’14 Human and SHREC’14 Large Scale Comprehensive Retrieval Track Benchmark datasets. Experimental results on the benchmark datasets demonstrate the effectiveness of the proposed method for 3D shape retrieval.	3d modeling;autoencoder;benchmark (computing);concatenation;data descriptor;encoder device component;feature learning;high- and low-level;anatomical layer	Jin Xie;Guoxian Dai;Fan Zhu;Edward K. Wong;Yi Fang	2017	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2016.2596722	active shape model;computer vision;computer science;pattern recognition;artificial intelligence;autoencoder;discriminative model;shape analysis (digital geometry);heat kernel signature;feature extraction;feature learning;solid modeling	Vision	29.937895012380576	-54.986621619485284	163492
1c6e067098fa86ee3f96365f28669b06f9ce0c7a	object detection from video tubelets with convolutional neural networks	video tubelets temporal convolution network general object tracking still image object detection vid task object location imagenet detection frameworks semantic segmentation image classification vision tasks cnns deep convolution neural networks;proposals object detection object tracking detectors feature extraction neural networks support vector machines;object tracking image classification image segmentation neural nets object detection	Deep Convolution Neural Networks (CNNs) have shown impressive performance in various vision tasks such as image classification, object detection and semantic segmentation. For object detection, particularly in still images, the performance has been significantly increased last year thanks to powerful deep networks (e.g. GoogleNet) and detection frameworks (e.g. Regions with CNN features (RCNN)). The lately introduced ImageNet [6] task on object detection from video (VID) brings the object detection task into the video domain, in which objects' locations at each frame are required to be annotated with bounding boxes. In this work, we introduce a complete framework for the VID task based on still-image object detection and general object tracking. Their relations and contributions in the VID task are thoroughly studied and evaluated. In addition, a temporal convolution network is proposed to incorporate temporal information to regularize the detection results and shows its effectiveness for the task. Code is available at https://github.com/ myfavouritekk/vdetlib.	artificial neural network;computer vision;convolution;convolutional neural network;imagenet;object detection;sensor;video	Kai Kang;Wanli Ouyang;Hongsheng Li;Xiaogang Wang	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.95	computer vision;object-class detection;computer science;viola–jones object detection framework;machine learning;video tracking;pattern recognition	Vision	28.439195481766017	-52.429956309813846	164408
2ff3da21d3495c5fa55778e70db5fe0a2f67a423	multimodal gesture recognition based on the resc3d network		Gesture recognition is an important issue in computer vision. Recognizing gestures with videos remains a challenging task due to the barriers of gesture-irrelevant factors. In this paper, we propose a multimodal gesture recognition method based on a ResC3D network. One key idea is to find a compact and effective representation of video sequences. Therefore, the video enhancement techniques, such as Retinex and median filter are applied to eliminate the illumination variation and noise in the input video, and a weighted frame unification strategy is utilized to sample key frames. Upon these representations, a ResC3D network, which leverages the advantages of both residual and C3D model, is developed to extract features, together with a canonical correlation analysis based fusion scheme for blending features. The performance of our method is evaluated in the Chalearn LAP isolated gesture recognition challenge. It reaches 67.71% accuracy and ranks the 1st place in this challenge.	alpha compositing;computer vision;gesture recognition;key frame;median filter;multimodal interaction;pattern recognition;relevance;support vector machine;unification (computer science);velocity (software development)	Qiguang Miao;Yunan Li;Wanli Ouyang;Zhenxin Ma;Xin Xu;Weikang Shi;Xiaochun Cao	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.360	residual;artificial intelligence;gesture recognition;computer vision;median filter;pattern recognition;feature extraction;hidden markov model;computer science;gesture;color constancy;unification	Vision	30.8464298968883	-58.45472592577494	164664
f57891b2e5860f42c9cbe3c58e926b891270277e	50 years of object recognition: directions forward	object recognition;object representations;cognitive vision systems;object learning;dynamic vision;active vision	Object recognition systems constitute a deeply entrenched and omnipresent component of modern intelligent systems. Research on object recognition algorithms has led to advances in factory and office automation through the creation of optical character recognition systems, assembly-line industrial inspection systems, as well as chip defect identification systems. It has also led to significant advances in medical imaging, defence and biometrics. In this paper we discuss the evolution of computer-based object recognition systems over the last fifty years, and overview the successes and failures of proposed solutions to the problem. We survey the breadth of approaches adopted over the years in attempting to solve the problem, and highlight the important role that active and attentive approaches must play in any solution that bridges the semantic gap in the proposed object representations, while simultaneously leading to efficient learning and inference algorithms. From the earliest systems which dealt with the character recognition problem, to modern visually-guided agents that can purposively search entire rooms for objects, we argue that a common thread of all such systems is their fragility and their inability to generalize as well as the human visual system can. At the same time, however, we demonstrate that the performance of such systems in strictly controlled environments often vastly outperforms the capabilities of the human visual system. We conclude our survey by arguing that the next step in the evolution of object recognition algorithms will require radical and bold steps forward in terms of the object representations, as well as the learning and inference algorithms used. 2013 Elsevier Inc. All rights reserved.	active vision;algorithmic efficiency;antivirus software;artificial intelligence;binary prefix;biometrics;bridging (networking);capability maturity model;categorization;cell signaling;circular shift;cluster analysis;coefficient;color space;computation;computer vision;convex optimization;database normalization;depth-first search;discrete cosine transform;discriminative model;exptime;edge detection;emoticon;enumerated type;expect;experiment;feature extraction;feature model;feature vector;fisher information;fisher kernel;forward kinematics;gamma correction;gaussian blur;generative model;gradient;grayscale;greedy algorithm;ground truth;harris affine region detector;heuristic (computer science);high- and low-level;histogram of oriented gradients;human visual system model;image noise;internationalization and localization;invariant (computer science);laplacian matrix;latent dirichlet allocation;latent variable;logistic regression;loss function;mpeg-7;marginal model;mathematical optimization;maxima and minima;medical imaging;microsoft windows;mined;minimum bounding box;mixture model;multistage interconnection networks;multitier architecture;naive bayes classifier;nonlinear system;optical character recognition;optimization problem;organizing (structure);outline of object recognition;pascal;pattern recognition;pedestrian detection;pixel;portable document format;preprocessor;product binning;quadratic function;radial (radio);randomness;receiver operating characteristic;region of interest;repeatability;robotics;robustness (computer science);spin;sampling (signal processing);scale-invariant feature transform;self-organization;self-organizing map;sensor;signed zero;simulation;smoothing;sobel operator;software bug;spatial variability;standard test image;support vector machine;test set;texture mapping;type signature;unified model;visual descriptor;vocabulary;xfig	Alexander Andreopoulos;John K. Tsotsos	2013	Computer Vision and Image Understanding	10.1016/j.cviu.2013.04.005	computer vision;method;active vision;object model;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;3d single-object recognition	AI	30.6178947527147	-56.91203101860659	164787
037f97afda1991ff5e3af5373e491faeab6c6364	simultaneous recognition of horizontal and vertical text in natural images		Recent state-of-the-art scene text recognition methods have primarily focused on horizontal text in images. However, in several Asian countries, including China, large amounts of text in signs, books, and TV commercials are vertically directed. Because the horizontal and vertical texts exhibit different characteristics, developing an algorithm that can simultaneously recognize both types of text in real environments is necessary. To address this problem, we adopted the direction encoding mask (DEM) and selective attention network (SAN) methods based on supervised learning. DEM contains directional information to compensate in cases that lack text direction; therefore, our network is trained using this information to handle the vertical text. The SAN method is designed to work individually for both types of text. To train the network to recognize both types of text and to evaluate the effectiveness of the designed model, we prepared a new synthetic vertical text dataset and collected an actual vertical text dataset (VTD142) from the Web. Using these datasets, we proved that our proposed model can accurately recognize both vertical and horizontal text and can achieve state-of-the-art results in experiments using benchmark datasets, including the street view test (SVT), IIIT-5k, and ICDAR. Although our model is relatively simple as compared to its predecessors, it maintains the accuracy and is trained in an end-to-end manner.		Chankyu Choi;Youngmin Yoon;Junsu Lee;Junseok Kim	2018	CoRR		artificial intelligence;machine learning;supervised learning;pattern recognition;horizontal and vertical;computer science	NLP	28.784076838950654	-54.73490300136003	164823
f3276ec26e9d347a5f325703dd1bf246d8363c18	leveraging robust signatures for mobile robot semantic localization		This paper describes the participation of the CIII UTN FRC team in the ImageCLEF 2012 Robot Vision Challenge. The challenge was focused on the problem of visual place classification in indoor environments. During the competition, participants were asked to classify images according to the room in which they were acquired, using the information provided by RGB and depth images only. We based our approach on the Fisher Vector representation –a robust signature recently proposed in the literature– and the use of efficient linear classifiers. In order to exploit the information provided by different information channels, we adopted a simple fusion strategy and generated classification scores for each image in the sequence. Two tasks were proposed during the competition: in the first, images had to be classified independently of one another while, in the second, it was possible to exploit the temporal continuity of the stream. For the first task, we adopted a simple threshold based classification scheme. For the second, we considered the classification of groups of images instead of single frames. These groups, i.e. temporal segments, were automatically generated based on the visual similarity of the images in the sequence. Our team ranked first on both tasks, showing the effectiveness of the proposed schemes.	comparison and contrast of classification schemes in linguistics and metadata;computer vision;electronic signature;frame rate control;image segmentation;linear classifier;mobile robot;scott continuity;statistical classification	Javier Redolfi;Jorge Sánchez	2012			computer vision;computer science;machine learning;data mining	Vision	25.593387871897367	-57.05901028749382	164973
1b85b3e0d68b8bff0c69a72cc9f40e0935c0c896	real-time image-based parking occupancy detection using deep learning		Parking Guidance and Information (PGI) systems have a potential to reduce the congestion in crowded areas by providing real-time indications of occupancy of parking spaces. To date, such systems are mostly implemented for indoor environments using costly sensor-based techniques. Consequently, with the increasing demand for PGI systems in outdoor environments, inexpensive image-based detection methods have become a focus of research and development recently. Motivated by the remarkable performance of Convolutional Neural Networks (CNNs) in various image category recognition tasks, this study presents a robust parking occupancy detection framework by using a deep CNN and a binary Support Vector Machine (SVM) classifier to detect the occupancy of outdoor parking spaces from images. The classifier was trained and tested by the features learned by the deep CNN from public datasets (PKLot) having different illuminance and weather conditions. Subsequently, we evaluate the transfer learning performance (the ability to generalise results to a new dataset) of the developed method on a parking dataset created for this research. We report detection accuracies of 99.7% and 96.7% for the public dataset and our dataset respectively, which indicates the great potential of this method to provide a low-cost and reliable solution to the PGI systems in outdoor environments.	convolutional neural network;deep learning;network congestion;neural networks;performance evaluation;real-time clock;real-time transcription;sensor;support vector machine	Debaditya Acharya;Weilin Yan;Kourosh Khoshelham	2018			computer vision;occupancy;deep learning;computer science;artificial intelligence	Robotics	29.223118978185337	-53.76535128608518	165123
e189c21576985fca021f85233d75cffaec574cc4	object segmentation using multiple neural networks for commercial offers visual search		We describe a web application that takes advantage of new computer vision techniques to allow the user to make searches based on visual similarity of color and texture related to the object of interest. We use a supervised neural network strategy to segment different classes of objects. A strength of this solution is the high speed in generalization of the trained neural networks, in order to obtain an object segmentation in real time. Information about the segmented object, such as color and texture, are extracted and indexed as text descriptions. Our case study is the online commercial offers domain where each offer is composed by text and images. Many successful experiments were done on real datasets in the fashion field.	algorithm;color;computer vision;experiment;neural networks;texture mapping;units of information;web application;web search engine	Ignazio Gallo;Angelo Nodari;Marco Vanetti	2011		10.1007/978-3-642-23957-1_24	computer vision;computer science;machine learning;data mining	Vision	27.101387108130776	-55.65664033173291	165184
3ae9f5832a1974166b5aea02976169b36e4c010f	discriminative sparse image models for class-specific edge detection and image interpretation	cost function;edge detection;image understanding;image classification;texture segmentation;computer vision;image interpretation;least square;feature selection;multiscale method;computational efficiency;image modeling	Sparse signal models learned from data are widely used in audio, image, and video restoration. They have recently been generalized to discriminative image understanding tasks such as texture segmentation and feature selection. This paper extends this line of research by proposing a multiscale method to minimize least-squares reconstruction errors and discriminative cost functions under l0 or l1 regularization constraints. It is applied to edge detection, category-based edge selection and image classification tasks. Experiments on the Berkeley edge detection benchmark and the PASCAL VOC’05 and VOC’07 datasets demonstrate the computational efficiency of our algorithm and its ability to learn local image descriptions that effectively support demanding computer vision tasks.	algorithm;benchmark (computing);circuit restoration;coefficient;computation;computer vision;edge detection;feature selection;least squares;mathematical optimization;matrix regularization;microsoft windows;modernpascal;object detection;preprocessor;sparse;sparse matrix	Julien Mairal;Marius Leordeanu;Francis R. Bach;Martial Hebert;Jean Ponce	2008		10.1007/978-3-540-88690-7_4	image texture;computer vision;contextual image classification;feature detection;scale space;edge detection;binary image;image processing;computer science;machine learning;pattern recognition;image segmentation;feature selection;least squares;automatic image annotation;feature	Vision	29.645350020383884	-56.75563217041657	165207
123972d5b8da47a7c60c7e702c4c96f2acb59254	kit at mediaeval 2015 - evaluating visual cues for affective impact of movies task		We present the approach and results of our system on the MediaEval Affective Impact of Movies Task. The challenge involves two primary tasks: affect classification and violence detection. We test the performance of multiple visual features followed by linear SVM classifiers. Inspired by successes in different vision fields, we use (i) GIST features used in scene modeling, (ii) features extracted from a deep convolutional neural network trained on object recognition, and (iii) improved dense trajectory features encoded using Fisher vectors commonly used in action recognition.	artificial neural network;convolutional neural network;fisher information;gist;outline of object recognition;statistical classification	P. Vlastelica MarinVlastelica;Sergey Hayrapetyan;Makarand Tapaswi;Rainer Stiefelhagen	2015			computer vision;speech recognition;engineering;communication	Vision	26.456345401898588	-56.81626495732462	166105
c216c58f63f5f31c04b329b8eebf72bcd12ed6c4	human action recognition based on recognition of linear patterns in action bank features using convolutional neural networks	neural networks;linear local pattern recognition human action recognition linear pattern recognition action bank features convolutional neural networks deep convolutional network architecture action bank videos ucf50 dataset demonstrates;convolution;data;action bank features human action recognition deep convolutional network;computer vision;computer programming;computer architecture;videos pattern recognition feature extraction neural networks convolution computer architecture computer vision;feature extraction;human action recognition;pattern recognition;deep convolutional network;video signal processing convolution feature extraction neural nets object recognition;programs;action bank features;videos	In this paper, we proposed a deep convolutional network architecture for recognizing human actions in videos using action bank features. Action bank features computed against of a predefined set of videos known as an action bank, contain linear patterns representing the similarity of the video against the action bank videos. Due to the independence of the patterns across action bank features, a convolutional neural network with linear masks is considered to capture the local patterns associated with each action. The knowledge gained through training is used to assign an action label to videos during testing. Experiments conducted on UCF50 dataset demonstrates the effectiveness of the proposed approach in capturing and recognizing these linear local patterns.	artificial neural network;convolutional neural network;network architecture;uncompressed video	Earnest Paul Ijjina;Chalavadi Krishna Mohan	2014	2014 13th International Conference on Machine Learning and Applications	10.1109/ICMLA.2014.33	computer vision;feature extraction;computer science;machine learning;pattern recognition;computer programming;convolution;artificial neural network;data	Vision	27.141701556181957	-52.14172523222661	166145
24215403b6565fa3a3bb71139482d08f23118b0d	novelty detection on metallic surfaces by gmm learning in gabor space	free surface;gaussian mixture;gaussian mixtures;integrable system;multiscale analysis;gabor filters;novelty detection;gabor filter;gaussian mixture model;computational complexity;metallic surface;defect detection	Defect detection on painted metallic surfaces is a challenging task in inspection due to the varying illuminative and reflective structure of the surface. This paper proposes a novelty detection scheme that models the defect-free surfaces by using Gaussian Mixture Models (GMMs) trained in Gabor space. It is shown that training using the texture representations obtained by Gabor filtering takes the advantage of multiscale analysis while reducing the computational complexity. Test results reported on defected metallic surfaces including pinhole, crater, hav, dust, scratch, and mound type of abnormalities demonstrate the superiority of developed integrated system with respect to the stand alone Gabor filtering as well as the spatial domain GMM classification.	google map maker;novelty detection	Yigitcan Savran;Bilge Günsel	2010		10.1007/978-3-642-13775-4_33	integrable system;computer vision;computer science;machine learning;pattern recognition;mixture model;mathematics;free surface;computational complexity theory	Vision	30.07576812094793	-56.24808240468102	166899
05f4d907ee2102d4c63a3dc337db7244c570d067	face recognition from a single image per person: a survey	small sample size;numerical technique;data collection;system evaluation;face recognition;single training image per person	One of the main challenges faced by the current face recognition techniques lies in the difficulties of collecting samples. Fewer samples per person mean less laborious effort for collecting them, lower cost for storing and processing them. Unfortunately, many reported face recognition techniques rely heavily on the size and representative of training set, and most of them will suffer serious performance drop or even fail to work if only one training sample per person is available to the systems. This situation is called “one sample per person” problem: given a stored database of faces, the goal is to identify a person from the database later in time in any different and unpredictable poses, lighting, etc. from just one image. Such a task is very challenging for most current algorithms due to the extremely limited representative of training sample. Numerous techniques have been developed to attack this problem, and the purpose of this paper is to categorize and evaluate these algorithms. The prominent algorithms are described and critically analyzed. Relevant issues such as data collection, the influence of the small sample size, and system evaluation are discussed, and several promising directions for future research are also proposed in this paper. 2006 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.	algorithm;autostereogram;categorization;facial recognition system;pattern recognition;test set	Xiaoyang Tan;Songcan Chen;Zhi-Hua Zhou;Fuyan Zhang	2006	Pattern Recognition	10.1016/j.patcog.2006.03.013	facial recognition system;computer vision;simulation;computer science;data mining;mathematics;statistics;data collection	Vision	29.833622624798853	-53.72061274984654	168655
26ec90ea38976f9e1734709a1bdef9e1c12ebcc6	deep forest with local experts based on elm for pedestrian detection		Despite recent significant advances, pedestrian detection continues to be an extremely challenging problem in real scenarios. Recently, some authors have shown the advantages of using combinations of part/patch-based detectors in order to cope with the large variability of poses and the existence of partial occlusions. In the beginning of 2017, deep forest is put forward to make up the blank of the decision tree in the field of deep learning. Deep forests have much less parameters than deep neural network and the advantages of higher classification accuracy. In this paper, we propose a novel pedestrian detection approach that combines the flexibility of a part-based model with the fast execution time of a deep forest classifier. In this proposed combination, the role of the part evaluations is taken over by local expert evaluations at the nodes of the decision tree. We first do feature select based on extreme learning machines to get feature sets. Afterwards we use the deep forest to classify the feature sets to get the score which is the results of the local experts. We tested the proposed method with well-known challenging datasets such as TUD and INRIA. The final experimental results on two challenging pedestrian datasets indicate that the proposed method achieves the state-of-the-art or competitive performance.	elm;pedestrian detection	Wenbo Zheng;Sisi Cao;Xin Jin;Shaocong Mo;Han Gao;Yili Qu;Chengfeng Zheng;Sijie Long;Jia Shuai;Zefeng Xie;Wei Jiang;Hang Du;Yongsheng Zhu	2018		10.1007/978-3-030-00767-6_74	computer science;deep learning;decision tree;artificial neural network;pattern recognition;pedestrian detection;artificial intelligence	Logic	28.646946367041963	-54.01066755921935	168914
4d814f9c9412abf782fbc173d0d112c19bd6f0ee	deep belief networks for spam filtering	frequency analysis;caption beginning frame;video signal processing;videos wavelet analysis indexing information retrieval strips discrete cosine transforms data mining frequency domain analysis feature extraction redundancy;image classification;discrete cosine transform;news videos;wavelet transforms;discrete cosine transforms;feature extraction;wavelet transforms character recognition discrete cosine transforms feature extraction image classification video signal processing;caption candidate strip;frequency domain;character recognition;text block classifier;caption localization;text block classifier caption localization news videos frequency analysis caption beginning frame caption candidate strip discrete cosine transform wavelet feature extraction;wavelet feature extraction	This paper proposes a novel approach for spam filtering based on the use of Deep Belief Networks (DBNs). In contrast to conventional feedfoward neural networks having one or two hidden layers, DBNs are feedforward neural networks with many hidden layers. Until recently it was not clear how to initialize the weights of deep neural networks, which resulted in poor solutions with low generalization capabilities. A greedy layer-wise unsupervised algorithm was recently proposed to tackle this problem with successful results. In this work we present a methodology for spam detection based on DBNs and evaluate its performance on three widely used datasets. We also compare our method to Support Vector Machines (SVMs) which is the state-of-the-art method for spam filtering in terms of classification performance. Our experiments indicate that using DBNs to filter spam e-mails is a viable methodology, since they achieve similar or even better performance than SVMs on all three datasets.	anti-spam techniques;artificial neural network;bayesian network;deep learning;email filtering;experiment;feedforward neural network;greedy algorithm;spamming;support vector machine;unsupervised learning	Grigorios Tzortzis;Aristidis Likas	2007	19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)	10.1109/ICTAI.2007.65	computer vision;contextual image classification;speech recognition;feature extraction;computer science;machine learning;discrete cosine transform;pattern recognition;frequency analysis;frequency domain;algorithm;wavelet transform	ML	26.198884261078305	-55.34308799091339	169427
025a64339d26f9f9b89a97a54b133636cfe3587b	recognition algorithm for topographic features	selenological and engineering explorer;image recognition;recognition algorithm;generalized hough transform;selene;digital terrain models;digital terrain model;impact craters;terrain mapping hough transforms image recognition;grabens;linear topographic features;hough transforms;general hough transform recognition algorithm impact craters wrinkle ridges grabens digital terrain models selene selenological and engineering explorer linear topographic features;general hough transform;terrain mapping;wrinkle ridges;moon digital elevation models stress mars information technology planets surfaces classification algorithms cameras remote sensing	This research describes development of recognition algorithm for topographic features such as impact craters, wrinkle ridges, and grabens on Digital Terrain Models of SELENE (SELenological and Engineering Explorer). Because SELENE provides massive volumes of lunar data, its data sets require semi-automatic labor saving analysis. We try to recognize circular and linear topographic features with General Hough Transform. Although the transform is one of the most popular techniques, this should be optimized for our targeted features. Experimental results of this algorithm show that typical craters and topographic lineaments can be extracted.	algorithm;digital elevation model;hough transform;semiconductor industry;topography	Naoto Harada;Takafumi Hayashi;Naru Hirata;Hirohide Demura;Noriaki Asada	2007	7th IEEE International Conference on Computer and Information Technology (CIT 2007)	10.1109/CIT.2007.151	computer vision;digital elevation model;graben;impact crater;computer graphics (images)	Vision	28.75672185759505	-55.91383175845536	169683
3edcd645a3846c491396314cd90baaa799c4cfbe	human motion classification using r transform	ℛ transform;conditional model;feature extraction;linear-chain conditional random fields (lcrfs);motion classification	linear-chain conditional random fields lcrfs;feature extraction;ℜ transform;motion classification;conditional model.		Qing Wei;Hao Zhang;Haiyong Zhao;Zhijing Liu	2010		10.1007/978-3-642-16339-5_22	computer vision;speech recognition;machine learning;mathematics	Vision	29.86639993232408	-57.57077795206439	169803
13289ccba23cee528b23f3eaf63596df54bd1611	deep age distribution learning for apparent age estimation		Apparent age estimation has attracted more and more researchers since its potential applications in the real world. Apparent age estimation differs from chronological age estimation that in apparent age estimation each facial image is labelled by multiple individuals, the mean age is the ground truth age and the uncertainty is introduced by the standard deviation. In this paper, we propose a novel method called Deep Age Distribution Learning(DADL) to deal with such situation. According to the given mean age and the standard deviation, we generate a Gaussian age distribution for each facial image as the training target instead of the single age. DADL first detects the facial region in image and aligns the facial image. Then, it uses deep Convolutional Neural Network(CNN) pre-trained based on the VGGFace and fine-tuned on the age dataset to extract the predicted age distribution. Finally it uses ensemble method to get the result. Our DADL method got a good performance in ChaLearn Looking at People 2016-Track 1: Age Estimation and ranked the 2nd place among 105 registered participants.	convolutional neural network;ground truth	Zeng-Wei Huo;Xu Yang;Chao Xing;Ying Zhou;Peng Hou;Jiaqi Lv;Xin Geng	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2016.95	econometrics;artificial intelligence;statistics	Vision	28.322862172519095	-58.72560817199581	169877
49e659ffc8922d1c9aeef57bbdb55b115cfd8216	face recognition: pre-processing techniques for linear autoassociators	face recognition		facial recognition system;linear algebra;preprocessor	E. Drege;Fan Yang;Michel Paindavoine;Hervé Abdi	1998			artificial intelligence;face detection;pattern recognition;three-dimensional face recognition;facial recognition system;computer science	Crypto	30.756786037132787	-57.75203523930741	170172
4c1d46fade29029bfc750888c6f747c4182bf8bf	sports video classification from multimodal information using deep neural networks		The work presents a methodology for classification of sports videos using both audio and visual information by applying deep learning algorithms. We show a methodology to combine multiple deep learning architectures through higher layers. Our method learns two separate models trained on audio and visual part of the data. We have trained the model for the audio part of the multimedia input using two stacked layers of CRBMs forming a CDBN. We also train two layered ISA network to extract features from video part of the data. We then train deep stacked autoencoder over both audio and visual features with discriminative fine tuning. Our results show that by combining both audio and visual features we get better accuracy as compared to single type of features.	algorithm;autoencoder;deep learning;industry standard architecture;machine learning;multimodal interaction	Devendra Singh Sachan;Umesh Tekwani;Amit Sethi	2013			computer vision;speech recognition;multimedia	HCI	26.09855518471026	-53.16234928554322	170882
2dd853b617c176810e3dda008f7cacea6473f0ae	image captioning using deep neural architectures		Automatically creating the description of an image using any natural language sentences is a very challenging task. It requires expertise of both image processing as well as natural language processing. This paper discusses about different available models for image captioning task. We have discussed about how the advancement in the task of object recognition and machine translation has greatly improved the performance of image captioning model in recent years. In addition to that we have discussed how this model can be implemented. At the end, we have also evaluated the performance of model using standard evaluation matrices.		Parth Shah;Vishvajit Bakarola;Supriya Pati	2017	2017 International Conference on Innovations in Information, Embedded and Communication Systems (ICIIECS)	10.1109/ICIIECS.2017.8276124	artificial intelligence;machine learning;image processing;task analysis;machine translation;communications system;natural language;computer science;closed captioning	Robotics	24.681300830166116	-54.4453943431117	171197
882f1ce75a533e3d133c9c34d3de11c75919d6e6	the neural-sift feature descriptor for visual vocabulary object recognition	training;classifier feedback neural sift feature descriptor visual vocabulary object recognition image semantic content computer vision feature extraction scale invariant feature transform adaptive neural network feature descriptor visual words approach initialization methods classification error neural network classifier training samples well performing descriptor;visualization;transforms feature extraction image classification neural nets object recognition;feature extraction;feature extraction visualization training	Recognizing the semantic content of an image is a challenging problem in computer vision. Many researchers attempt to apply local image descriptors to extract features from an image, but choosing the best type of feature to use is still an open problem. Some of these systems are only trained once using a fixed descriptor, like the Scale Invariant Feature Transform (SIFT). In most cases these algorithms show good performance, but they do not learn from their mistakes once training is completed. In this paper a continuous deep neural network feedback system is proposed which consists of an adaptive neural network feature descriptor, the bag of visual words approach and a neural classifier. Two initialization methods for the neural network feature descriptor were compared, one where it was trained on SIFT descriptor output and one where it was randomly initialized. After initial training, the system propagates the classification error from the neural network classifier through the entire pipeline, updating not only the classifier itself, but also the type of features to extract. Results show that for both initialization methods the feedback system increased accuracy substantially when regular training was not able to increase it any further. The proposed neural-SIFT feature descriptor performs better than the SIFT descriptor itself even with a limited number of training instances. Initializing on an existing feature descriptor is beneficial when not a lot of training samples are available. However, when there are a lot of training samples the system is able to construct a well-performing descriptor, solely based on classifier feedback.	algorithm;artificial neural network;bag-of-words model in computer vision;convolutional neural network;deep learning;feedback;outline of object recognition;randomness;scale-invariant feature transform;visual descriptor;vocabulary	Sybren Jansen;Amirhosein Shantia;Marco Wiering	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280660	computer vision;local binary patterns;visualization;feature extraction;gloh;computer science;machine learning;pattern recognition;feature	Vision	26.99763082757894	-55.20366918401867	171967
e3355011a25ecdb3a92297fd2509b622f0fb2331	automated quantification of eye blink rate using viola-jones algorithm		In﻿ this﻿ article,﻿ we﻿ have﻿ proposed﻿ a﻿ novel﻿ tool﻿ that﻿ helps﻿ to﻿ objectively﻿ quantify﻿ eye﻿ blink﻿ rate.﻿ Using﻿the﻿proposed﻿algorithm,﻿a﻿threshold﻿for﻿normal﻿blink﻿rate﻿can﻿be﻿set﻿to﻿test﻿those﻿who﻿have﻿to﻿ reduce﻿eye﻿blink﻿rate﻿and﻿are﻿prone﻿to﻿ocular﻿surface﻿dryness.﻿The﻿statistical﻿results﻿show﻿excellent﻿ agreement﻿between﻿software-detected﻿number﻿of﻿blinks﻿and﻿visually﻿measured﻿with﻿90%﻿accuracy﻿ for﻿the﻿participants.﻿In﻿addition,﻿the﻿comparison﻿between﻿our﻿tool﻿and﻿other﻿approaches﻿of﻿eye﻿blink﻿ monitoring﻿shows﻿that﻿our﻿tool﻿is﻿competitive﻿with﻿only﻿5%﻿wasted﻿blinks. KEywORDS Dry Eyes Disease, Eye Blinking, Eye Detection, Image Processing, Tracking, Viola-Jones		Mohammad Hamdan;Hisham A. Shehadeh	2018	IJTD	10.4018/IJTD.2018100102	speech recognition;computer science;management science;viola–jones object detection framework	ML	29.89863615388888	-59.02010617200478	172725
e0ba06ca457672c5de0c254c52b853d304412967	digital recognition from lip texture analysis	digital recognition;liveness detection;deep learning;lipreading	Digital recognition with lip images has become a key step of the interactive liveness detection for Chinese banking systems. However, the problem of the digital recognition is very challenging due to intra class variation of lip images, head pose variations, and uncontrolled illumination. This paper studies a deep learning architecture to model the appearance and the spatial-temporal information of lip texture. The lip texture in still image frames and the spatial-temporal relationship between these frames are jointly modeled by convolutional neural networks and long short-term memory. Two strategies are further exploited to find effective groups of ten digitals for training the deep models. As a result, more information can be utilized for accurate recognition based on lip texture analysis. Besides, two datasets of isolated digits in Chinese are established to simulate real-world liveness detection environments together with various attacks. Extensive experiments have been done to analyze the recognition accuracy of each digit and to provide some clues for determining appropriate digits for interactive liveness detection.	artificial neural network;convolutional neural network;deep learning;experiment;liveness;long short-term memory;simulation;uncontrolled format string	Wenkai Dong;Ran He;Shu Zhang	2016	2016 IEEE International Conference on Digital Signal Processing (DSP)	10.1109/ICDSP.2016.7868603	computer vision;speech recognition;engineering;communication	Vision	30.700187202940572	-52.899843869247135	172982
2921dbe59e26ed57eaf65b174dab62457766c5d7	sketch-based cross-domain image retrieval via heterogeneous network		The development of image diversity has led multiple fields' application of cross-domain image retrieval. In this paper, we propose a heterogeneous dual network (two different networks) based on sketches and images. End-to-end cross-domain image retrieval is realized by limiting the similarity of features extracted from the two networks through the method of combining the contrastive loss with the triplet ranking loss. We also study how the order of drawings affects the sketch retrieval. Compared to the Siamese network, we avoid edge extraction for preprocessing, and the effect of fine-grained retrieval is further enhanced.	deconvolution;experiment;feature vector;image retrieval;preprocessor;sketch;triplet state	Hao Zhang;Chuang Zhang;Ming Wu	2017	2017 IEEE Visual Communications and Image Processing (VCIP)	10.1109/VCIP.2017.8305153	computer vision;artificial intelligence;computer science;preprocessor;heterogeneous network;image retrieval;limiting;sketch;ranking	Vision	26.56751188470195	-55.024054346501686	173202
1863d67b653090151cd67daac7a635f3e1034a54	shape completion enabled robotic grasping		This work provides an architecture to enable robotic grasp planning via shape completion. Shape completion is accomplished through the use of a 3D convolutional neural network (CNN). The network is trained on our own new open source dataset of over 440,000 3D exemplars captured from varying viewpoints. At runtime, a 2.5D pointcloud captured from a single point of view is fed into the CNN, which fills in the occluded regions of the scene, allowing grasps to be planned and executed on the completed object. Runtime shape completion is very rapid because most of the computational costs of shape completion are borne during offline training. We explore how the quality of completions vary based on several factors. These include whether or not the object being completed existed in the training data and how many object models were used to train the network. We also look at the ability of the network to generalize to novel objects allowing the system to complete previously unseen objects at runtime. Finally, experimentation is done both in simulation and on actual robotic hardware to explore the relationship between completion quality and the utility of the completed mesh model for grasping.	2.5d;artificial neural network;columbia (supercomputer);convolutional neural network;generative adversarial networks;online and offline;open-source software;point of view (computer hardware company);robot;run time (program lifecycle phase);simulation	Jacob Varley;Chad DeChant;Adam Richardson;Avinash Nair;Joaquín Ruales;Peter K. Allen	2017	2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2017.8206060	convolutional neural network;computer science;simulation;artificial intelligence;computer vision;viewpoints;architecture;training set;machine learning;grasp	Robotics	25.634202895851985	-53.55998707364425	173719
0d3bb75852098b25d90f31d2f48fd0cb4944702b	a data-driven approach to cleaning large face datasets	outlier detection;face recognition	Large face datasets are important for advancing face recognition research, but they are tedious to build, because a lot of work has to go into cleaning the huge amount of raw data. To facilitate this task, we describe an approach to building face datasets that starts with detecting faces in images returned from searches for public figures on the Internet, followed by discarding those not belonging to each queried person. We formulate the problem of identifying the faces to be removed as a quadratic programming problem, which exploits the observations that faces of the same person should look similar, have the same gender, and normally appear at most once per image. Our results show that this method can reliably clean a large dataset, leading to a considerable reduction in the work needed to build it. Finally, we are releasing the FaceScrub dataset that was created using this approach. It consists of 141,130 faces of 695 public figures and can be obtained from http://vintage.winklerbros.net/facescrub.html.	facial recognition system;internet;plasma cleaning;quadratic programming;sensor;sputter cleaning	Hongwei Ng;Stefan Winkler	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025068	facial recognition system;computer vision;computer science;artificial intelligence;data science;machine learning;data mining	Vision	31.829007430590636	-52.38976219487185	174621
689aea08d84183bd79fd5865e890996fdbf9a709	a novel mutual authentication based on data embedding technique	seals;computers;phishing;learning process;message authentication image retrieval;transformation model;multiple instance learning;authentication;data embedding;vehicle retrieval system;data mining;servers;registers;region of interest;mutual authentication;authentication vehicles information retrieval image databases spatial databases image retrieval shape data mining visual databases training data;message authentication;data embedding technique;multiple instance learning mutual authentication data embedding technique image retrieval multiple instance learning vehicle retrieval system;data embedding mutual authentication phishing;image retrieval	This paper presents a novel approach for retrieving images from databases using eigen color and the concept of multiple instance learning. Usually, vehicles have various colors and shapes under different viewpoints, weathers, and lighting conditions. All the variations will increase many difficulties and challenges in selecting a general feature to describe vehicles. Thus, traditional methods to retrieve vehicles require their orientations or colors being fixed. To tackle this problem, this paper proposes a novel vehicle retrieval system for effectively retrieving vehicles from databases no matter what orientations and colors they are. First of all, this paper proposes a novel color transform model, which is global and does not need to be re-estimated for any new vehicles or new images, to extract different regions of interest (or vehicle analogues) from databases. Then, to more accurately locate desired vehicle images, this paper uses the MIL(multiple-instance learning) method to learn specific visual properties of vehicles from query images. However, the MIL technique requires the positive training data being strongly positive and the negative ones being strongly negative. This requirement is too constrained in real cases and will lead to lots of false detection. This problem can be easily tackled if an eigen color transform is introduced. The extra consideration “eigen color” will add more capabilities to the MIL learner for capturing the embedded concept more accurately. Furthermore, during the learning process, since no time-consuming optimization process is involved, all the desired visual concept can be obtained immediately and adapted to different user’s requests. Experimental results reveal the feasibility and high accuracy of the proposed approach in vehicle retrieval system.	anomaly detection;autostereogram;color management;database;eigen (c++ library);embedded system;mathematical optimization;multiple instance learning;multiple-instance learning;mutual authentication;phishing;prototype;region of interest;server (computing)	Chia-Chen Lin;Po-Hsuan Chiang	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.305	message authentication code;computer vision;phishing;image retrieval;computer science;machine learning;data mining;authentication;processor register;computer security;information retrieval;server;region of interest	Robotics	25.912520600759404	-58.784119089926264	175247
1ad45747a121055b6fd3be6ff8f3f933f88ab659	microscopy cell segmentation via adversarial neural networks		We present a novel method for cell segmentation in microscopy images which is inspired by the Generative Adversarial Neural Network (GAN) approach. Our framework is built on a pair of two competitive artificial neural networks, with a unique architecture, termed Rib Cage, which are trained simultaneously and together define a min-max game resulting in an accurate segmentation of a given image. Our approach has two main strengths, similar to the GAN, the method does not require a formulation of a loss function for the optimization process. This allows training on a limited amount of annotated data in a weakly supervised manner. Promising segmentation results on real fluorescent microscopy data are presented. The code is freely available at: https://github.com/arbellea/DeepCellSeg.git	artificial neural network;loss function;mathematical optimization;maxima and minima;supervised learning	Assaf Arbelle;Tammy Riklin-Raviv	2018	2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)	10.1109/ISBI.2018.8363657	artificial neural network;computer science;computer vision;architecture;machine learning;artificial intelligence;pattern recognition;microscopy;segmentation	Vision	25.667625197187522	-54.16652328876199	175388
919a0935d8d127aa5cea44327fc8f47f9724e5d9	face detection		Face detection in still images and videos has been extensively studied over the last two decades. Attributed to the recent proliferation of cameras in consumer applications, research in face detection has gradually transformed into more unconstrained settings, with the goal of achieving performance close to humans. This presents two main challenges: (i) in addition to modeling the facial characteristics, understanding the information portrayed by the surrounding scene is important in resolving visual ambiguities, and (ii) the computational time needed for decision making should be compatible for real-time applications, since detection is primarily a front-end process on which additional knowledge extraction is built upon. This chapter begins with a review of recent work in modeling face-specific information, including appearance-based methods used by sliding window classifiers, concepts from learning and local interest-point descriptors, and then focuses on representing the contextual information shared by faces with the surrounding scene. To provide better understanding of working concepts, we discuss our proposed method for learning the semantic context shared by the face with other human body parts that facilitates reasoning under occlusion, and then present an image representation which efficiently encodes contour information to enable fast detection of faces. We conclude the chapter by briefly discussing some existing challenges. Raghuraman Gopalan Department of Electrical and Computer Engineering, University of Maryland, College Park MD 20742 USA, e-mail: raghuram@umiacs.umd.edu William Robson Schwartz Institute of Computing, University of Campinas, Campinas-SP, Brazil, 13084-971, e-mail: wschwartz@liv.ic.unicam.br Rama Chellappa Department of Electrical and Computer Engineering, and UMIACS, University of Maryland, College Park MD 20742 USA, e-mail: rama@umiacs.umd.edu Ankur Srivastava Department of Electrical and Computer Engineering, and Institute for Systems Research, University of Maryland, College Park MD 20742 USA, e-mail: ankurs@umd.edu	computation;computer engineering;email;face detection;intel turbo memory;molecular dynamics;real-time transcription;time complexity	Raghuraman Gopalan;William Robson Schwartz;Rama Chellappa;Ankur Srivastava	2011		10.1007/978-0-85729-997-0_5	face detection;computer vision;image retrieval;object-class detection;artificial intelligence;3d single-object recognition;content-based image retrieval;facial recognition system;three-dimensional face recognition;ranging;computer science	Vision	30.662003408608992	-56.67481718860185	175645
05caf9e66947d863bbbc5ff72d6bd84705954111	dag-recurrent neural networks for scene labeling		In image labeling, local representations for image units are usually generated from their surrounding image patches, thus long-range contextual information is not effectively encoded. In this paper, we introduce recurrent neural networks (RNNs) to address this issue. Specifically, directed acyclic graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which enables the network to model long-range semantic dependencies among image units. Our DAG-RNNs are capable of tremendously enhancing the discriminative power of local representations, which significantly benefits the local classification. Meanwhile, we propose a novel class weighting function that attends to rare classes, which phenomenally boosts the recognition accuracy for non-frequent classes. Integrating with convolution and deconvolution layers, our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow, CamVid and Barcelona benchmarks.	artificial neural network;convolution;core image;deconvolution;directed acyclic graph;experiment;gist;graphical user interface;interaction;map;recurrent neural network;weight function	Bing Shuai;Zhen Zuo;Gang Wang;Bing Wang	2016	2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2016.394	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Vision	26.495117135935185	-52.55135066086241	175898
080efc3f664cb31f95a8e7a255c5802f0d91031d	combining fully convolutional and recurrent neural networks for 3d biomedical image segmentation		Segmentation of 3D images is a fundamental problem in biomedical image analysis. Deep learning (DL) approaches have achieved state-of-the-art segmentation performance. To exploit the 3D contexts using neural networks, known DL segmentation methods, including 3D convolution, 2D convolution on planes orthogonal to 2D image slices, and LSTM in multiple directions, all suffer incompatibility with the highly anisotropic dimensions in common 3D biomedical images. In this paper, we propose a new DL framework for 3D image segmentation, based on a combination of a fully convolutional network (FCN) and a recurrent neural network (RNN), which are responsible for exploiting the intra-slice and inter-slice contexts, respectively. To our best knowledge, this is the first DL framework for 3D image segmentation that explicitly leverages 3D image anisotropism. Evaluating using a dataset from the ISBI Neuronal Structure Segmentation Challenge and in-house image stacks for 3D fungus segmentation, our approach achieves promising results comparing to the known DL-based 3D segmentation approaches.	3d film;artificial neural network;binary delta compression;convolution;convolutional neural network;deep learning;ibm notes;image analysis;image segmentation;long short-term memory;programming paradigm;random neural network;recurrent neural network;software incompatibility	Jianxu Chen;Lin Yang;Yizhe Zhang;Mark S. Alber;Danny Ziyi Chen	2016			computer vision;computer science;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation	Vision	25.449106781451018	-52.59960072183486	176133
6f3a1216e984c78e7374b3b7ab05612544d345eb	derpn: taking a further step toward more general object detection		Most current detection methods have adopted anchor boxes as regression references. However, the detection performance is sensitive to the setting of the anchor boxes. A proper setting of anchor boxes may vary significantly across different datasets, which severely limits the universality of the detectors. To improve the adaptivity of the detectors, in this paper, we present a novel dimension-decomposition region proposal network (DeRPN) that can perfectly displace the traditional Region Proposal Network (RPN). DeRPN utilizes an anchor string mechanism to independently match object widths and heights, which is conducive to treating variant object shapes. In addition, a novel scale-sensitive loss is designed to address the imbalanced loss computations of different scaled objects, which can avoid the small objects being overwhelmed by larger ones. Comprehensive experiments conducted on both general object detection datasets (Pascal VOC 2007, 2012 and MS COCO) and scene text detection datasets (ICDAR 2013 and COCO-Text) all prove that our DeRPN can significantly outperform RPN. It is worth mentioning that the proposed DeRPN can be employed directly on different models, tasks, and datasets without any modifications of hyperparameters or specialized optimization, which further demonstrates its adaptivity. The code will be released at https://github.com/HCIILAB/DeRPN.	box;computation;detectors;entity name part qualifier - adopted;evaluation;experiment;height;ibm notes;international conference on document analysis and recognition;latex;large;mathematical optimization;object detection;performance;physical object;pineal region yolk sac tumor;reverse polish notation;sensor;universality probability;variant object;format;newton per square metre	Lele Xie;Yuliang Liu;Lianwen Jin;Zecheng Xie	2018	CoRR		pattern recognition;regression;universality (philosophy);object detection;artificial intelligence;hyperparameter;computation;computer science;detector	Vision	30.302184414262832	-54.04683571150175	176534
ae425a2654a1064c2eda29b08a492c8d5aab27a2	an incremental face recognition system based on deep learning		In recent years, face recognition technologies develop rapidly especially in those systems based on deep learning. However, the models of the general face recognition system are fixed in use after trained, which is difficult to adapt to the new data collected during use. In this paper, we design a face recognition system based on OpenFace. Different from other systems, we propose an intelligent model training method S-DDL(self detection, decision and learning) using incremental SVM algorithm which makes our system able to update the classification model in real time during execution. With this method, the accuracy of our system will increase on specific human groups and the time consumption of training new models can be limited with the incremental SVM algorithm. The results show that our face recognition system has a good real time performance and accuracy. The S-DDL method can obviously improve the accuracy within little time during the execution of system and the incremental SVM algorithm has a better performance than the traditional one.	algorithm;deep learning;experiment;facial recognition system;feature extraction;incremental compiler;pattern recognition;run time (program lifecycle phase);security descriptor definition language;teaching method	Lufan Li;Zhang Jun;Jiawei Fei;Shuohao Li	2017	2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)	10.23919/MVA.2017.7986845	pattern recognition;support vector machine;computer vision;population-based incremental learning;deep learning;facial recognition system;machine learning;artificial intelligence;computer science	Robotics	26.569305108353255	-54.96513280017577	176948
fa6e7c4043172f59864d18bbbcc5d66e35087a03	dynamic scene recognition based on improved visual vocabulary model	visualization vocabulary feature extraction image recognition noise computational modeling clustering algorithms;gaussian model scene recognition visual vocabulary soft assignment	In this paper, we present a scene recognition framework, which could process the images and recognize the scene in the images. We demonstrate and evaluate the performance of our system on a dataset of Oxford typical landmarks. In this paper, we put forward a novel method of local k-meriod for building a vocabulary and introduce a novel quantization method of soft-assignment based on the Gaussian mixture model. Then we also introduced the Gaussian model in order to classify the images into different scenes by calculating the probability of whether an image belongs to the scene, and we further improve the model by drawing out the consistent features and filtering out the noise features. Our experiment proves that these methods actually improve the classifying performance.	approximation;cluster analysis;computation;data point;emoticon;high- and low-level;k-means clustering;mixture model;mobile device;need to know;outline of object recognition;receiver operating characteristic;soul;vocabulary;yang	Lin Yan-Hao;Lu-Fang Gao	2014	2014 International Conference on Computer Vision Theory and Applications (VISAPP)	10.5220/0004736105570565	natural language processing;speech recognition;computer science;pattern recognition	Vision	30.41946323591324	-55.46873294144088	177147
74e0fd0059e3438b3945c5c0bac22a7c528baa3c	video object detection with an aligned spatial-temporal memory		We introduce Spatial-Temporal Memory Networks for video object detection. At its core, a novel Spatial-Temporal Memory module (STMM) serves as the recurrent computation unit to model long-term temporal appearance and motion dynamics. The STMM’s design enables full integration of pretrained backbone CNN weights, which we find to be critical for accurate detection. Furthermore, in order to tackle object motion in videos, we propose a novel MatchTrans module to align the spatial-temporal memory from frame to frame. Our method produces state-of-the-art results on the benchmark ImageNet VID dataset, and our ablative studies clearly demonstrate the contribution of our different design choices. We release our code and models at http://fanyix.cs. ucdavis.edu/project/stmn/project.html.		Fanyi Xiao;Yong Jae Lee	2018		10.1007/978-3-030-01237-3_30	memory module;computer vision;machine learning;artificial intelligence;computation;computer science;object detection	Vision	27.923820844349038	-52.15251259263832	177535
e9ac109c395ededb23dfc78fe85d76eeb772ee7e	a multilevel mixture-of-experts framework for pedestrian classification	support vector machines;image matching;computer model;multilayer perceptrons;training;mixture of experts;image classification;shape recognition;histograms of oriented gradients;multilayer perceptron;local binary pattern;universiteitsbibliotheek;multimodality dataset multilevel mixture of experts framework pedestrian classification pedestrian recognition chamfer shape matching image intensity depth flow histograms of oriented gradients local binary patterns hog lbp multilayer perceptrons mlp linear support vector machines linsvm;research purpose;computational modeling;optical imaging;shape;shape matching;pedestrian classification mixture of experts object detection;pixel;detection rate;traffic engineering computing image classification image matching multilayer perceptrons object detection shape recognition support vector machines;cluster analysis fuzzy logic humans image processing computer assisted pattern recognition automated support vector machines;traffic engineering computing;support vector machine;pedestrian classification;training shape cameras support vector machines optical imaging computational modeling pixel;false positive;cameras;object detection	Notwithstanding many years of progress, pedestrian recognition is still a difficult but important problem. We present a novel multilevel Mixture-of-Experts approach to combine information from multiple features and cues with the objective of improved pedestrian classification. On pose-level, shape cues based on Chamfer shape matching provide sample-dependent priors for a certain pedestrian view. On modality-level, we represent each data sample in terms of image intensity, (dense) depth, and (dense) flow. On feature-level, we consider histograms of oriented gradients (HOG) and local binary patterns (LBP). Multilayer perceptrons (MLP) and linear support vector machines (linSVM) are used as expert classifiers. Experiments are performed on a unique real-world multi-modality dataset captured from a moving vehicle in urban traffic. This dataset has been made public for research purposes. Our results show a significant performance boost of up to a factor of 42 in reduction of false positives at constant detection rates of our approach compared to a baseline intensity-only HOG/linSVM approach.	baseline (configuration management);belief propagation;chamfer;experiment;expert system;extraction;gradient;local binary patterns;modality (human–computer interaction);multilayer perceptron;numerous;overfitting;pedestrian detection;quad flat no-leads package;shape context;silo (dataset);support vector machine;benefit	Markus Enzweiler;Dariu Gavrila	2011	IEEE Transactions on Image Processing	10.1109/TIP.2011.2142006	computer simulation;support vector machine;computer vision;computer science;machine learning;pattern recognition	Vision	31.803125628258396	-55.22719970896321	177761
8e0cd9bedf768de8a280f4854177251a368b29ba	automatic feature extraction to an mpeg-7 content model	feature extraction		feature extraction;mpeg-7	Minaz J. Parmar;Marios C. Angelides	2005			feature extraction;content model;artificial intelligence;pattern recognition;computer science	NLP	31.034934332884966	-58.12925219364907	178651
8e5804b8dddff600ee7646b8c33bebad4b550b31	convolutional neural network-based periocular recognition in surveillance environments		Visible light surveillance cameras are currently deployed on a large scale to prevent crime and accidents in public urban environments. For this reason, various human identification studies using biometric data are underway in surveillance environments. The most active research area is face recognition, which generally shows excellent performance; however, aging, changes in facial expression, and occlusions by accessories cause a rapid decline in recognition performance. To resolve these problems, we propose a periocular recognition method in surveillance environments that is based on the convolutional neural network. In this paper, experiments were performed using the custom-made Dongguk periocular database and the open database of ChokePoint database. It was confirmed that the proposed method performs better than existing techniques used in periocular recognition. It was also found to perform better than conventional techniques in face recognition when an occlusion is present.	artificial neural network;biometrics;closed-circuit television;convolutional neural network;experiment;facial recognition system;hidden surface determination	Moonil Kim;Ja Hyung Koo;Se Woon Cho;Na Rae Baek;Kang Ryoung Park	2018	IEEE Access	10.1109/ACCESS.2018.2874056	convolutional neural network;feature extraction;biometrics;distributed computing;computer vision;computer science;iris recognition;facial recognition system;artificial intelligence;image resolution	Vision	30.97623031972134	-52.595357317615644	178751
f66d1d68eabae50d67e23a8a4ef3a69c53fca534	vehicle classification for large-scale traffic surveillance videos using convolutional neural networks	vehicle classification;cnn;googlenet;vehicledataset;pre-training;fine-tuning	Vehicle classification plays an important role in intelligent transport system. However, because the conventional vehicle classification methods are not robust to variations such as illumination, weather, noise, and the classification accuracy cannot meet the requirements of practical applications. Therefore, a new vehicle classification method using Convolutional Neural Networks is proposed in this paper, which consists of two steps: pre-training and fine-tuning. In pre-training, GoogLeNet is pre-trained on ILSVRC-2012 dataset to obtain the initial model with the corresponding connection weights. In fine-tuning, the initial model is further fine-tuned on VehicleDataset which is constructed with 13,700 images in this paper to obtain the final classification model. All images in the VehicleDataset are extracted from real highway surveillance videos, including variations of illumination, noise, resolution, angle of video cameras and weather. The vehicles are divided into six categories, i.e., bus, car, motorcycle, minibus, truck and van. The performance evaluation is carried out on the VehicleDataset. The experimental results show that the proposed method can avoid the complicated process of manually extracting features and the average classification accuracy is up to 98.26%, which is 3.42% higher than the conventional methods using “Feature + Classifier”.	algorithm;artificial neural network;convolutional neural network;neural network software;performance evaluation;requirement	Li Zhuo;Liying Jiang;Ziqi Zhu;Jiafeng Li;Jing Zhang;Hai-Xia Long	2017	Machine Vision and Applications	10.1007/s00138-017-0846-2	pattern recognition;artificial intelligence;computer vision;fine-tuning;convolutional neural network;computer science;truck	AI	29.24206999824627	-54.78055592670007	180592
ec78ac52b95f5019337350a880c2ac216022cffe	traffic sign recognition using deep convolutional networks and extreme learning machine		Traffic sign recognition is an important but challenging task, especially for automated driving and driver assistance. Its accuracy depends on two aspects: feature exactor and classifier. Current popular algorithms mainly use convolutional neural networks (CNN) to execute feature extraction and classification. Such methods could achieve impressive results but usually on the basis of an extremely huge and complex network. What’s more, since the fully-connected layers in CNN form a classical neural network classifier, which is trained by conventional gradient descent-based implementations, the generalization ability is limited. The performance could be further improved if other favorable classifiers are used instead and extreme learning machine (ELM) is just the candidate. In this paper, a novel CNN-ELM model is proposed, which integrates the CNN’s terrific capability of feature learning with the outstanding generalization performance of ELM. Firstly CNN learns deep and robust features and then ELM is used as classifier to conduct a fast and excellent classification. Experiments on German traffic sign recognition benchmark (GTSRB) demonstrate that the proposed method can obtain competitive results with state-of-the-art algorithms with less computation time.		Yujun Zeng;Xin Xu;Yuqiang Fang;Kun Zhao	2015		10.1007/978-3-319-23989-7_28	speech recognition;engineering;artificial intelligence;machine learning;deep learning	ML	28.36069402736459	-54.064684154726024	181196
b7a5fcb9ddaf3232b33aead57141577aaf8e7c2e	action recognition via spatio-temporal local features	g400 computer science;performance evaluation;feature coding;fisher kernel;spatio temporal local features;action recognition;nbnn;match kernels;vlad;sparse coding;bag of words	Local methods based on spatio-temporal interest points (STIPs) have shown their effectiveness for human action recognition. The bag-of-words (BoW) model has been widely used and dominated in this field. Recently, a large number of techniques based on local features including improved variants of the BoW model, sparse coding (SC), Fisher kernels (FK), vector of locally aggregated descriptors (VLAD) as well as the naive Bayes nearest neighbor (NBNN) classifier have been proposed and developed for visual recognition. However, some of them are proposed in the image domain and have not yet been applied to the video domain and it is still unclear how effectively these techniques would perform on action recognition. In this paper, we provide a comprehensive study on these local methods for human action recognition. We implement these techniques and conduct comparison under unified experimental settings on three widely used benchmarks, i.e., the KTH, UCF-YouTube and HMDB51 datasets. We discuss insightfully the findings from the experimental results and draw useful conclusions, which are expected to guide practical applications and future work for the action recognition community.	bag-of-words model in computer vision;fisher information;forward kinematics;naive bayes classifier;neural coding;sparse matrix	Xiantong Zhen;Ling Shao	2016	Image Vision Comput.	10.1016/j.imavis.2016.02.006	computer vision;speech recognition;computer science;bag-of-words model;machine learning;pattern recognition;neural coding;fisher kernel	Vision	30.868794692378806	-54.6389685137753	181453
9911ee1c6114051ac1ec53fa4ed2546e20faee9a	unsupervised relational feature learning for vision			feature learning;unsupervised learning	Kishore Reddy Konda	2016				Vision	29.05712202330234	-57.00185443657665	181759
eaf821a48963cbae3d065dbeecab2149f713d8ff	learning from ambiguous examples for object detection and image categorization			categorization;object detection	Gunawan Herman	2012				Vision	30.45283183457796	-56.745041798226815	182528
4bfd755bca9578475fa220d065b026316eeb95c4	camera-based optical music recognition using a convolutional neural network		Optical Music Recognition (OMR) consists in recognizing images of music scores. Contrary to expectation, the current OMR systems usually fail when recognizing images of scores captured by digital cameras and smartphones. In this work, we propose a camera-based OMR system based on Convolutional Neural Networks, showing promising preliminary results.	convolutional neural network;digital camera;optical mark recognition;smartphone	Adria Rico Blanes;Alicia Fornés Bisquerra	2017	2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)	10.1109/ICDAR.2017.261	computer vision;convolutional neural network;computer science;task analysis;optical music recognition;artificial intelligence;optical imaging;image segmentation;pattern recognition	Robotics	27.446155164533405	-58.96884894626798	182988
9add1803827f6c82fe47a882289f6a3f7d97f170	vfh-color and deep belief network for 3d point cloud recognition		With the invention of Microsoft Kinect sensor, 3D object recognition has become an important task in computer vision research in recent years. The Viewpoint Feature Histogram (VFH) is a Point Cloud Library (PCL) descriptor that encodes only geometry and viewpoint of 3D point cloud data. In this paper, we propose a new approach to representing and learning 3D point cloud classes. First, we develop a new descriptor called VFH-Color that combines the original version of VFH descriptor with the color quantization histogram, thus adding the appearance information that would improve the recognition rate. Then, we use those features for training deep learning algorithm called Deep Belief Network (DBN). We have also tested our approach on Washington RGBD dataset and have obtained highly promising results.	deep belief network;point cloud;vector field histogram	Nabila Zrira;Mohamed Hannat;El-Houssine Bouyakhf	2017		10.1007/978-3-319-58838-4_49	deep belief network;computer vision;point cloud;artificial intelligence;pattern recognition;color quantization;deep learning;computer science;histogram	Vision	30.53659851758869	-56.10353687147478	183239
bcaefdc357b4e90e27c9d713cc984ed31f66c0d6	license plate recognition and super-resolution from low-resolution videos by convolutional neural networks		The paper proposes Convolutional Neural Network (CNN) for License Plate Recognition (LPR) from low-resolution videos. The CNN accepts arbitrary long sequence of geometrically registered license plate (LP) images and outputs a distribution over a set of strings with an admissible length. Evaluation on 31k low-resolution videos shows that the proposed CNN significantly outperforms both baseline methods and humans by a large margin. Our second contribution is a CNN based super-resolution generator of LP images. The generator converts input low-resolution LP image into its high-resolution counterpart which i) preserves the structure of the input and ii) depicts a string that was previously recognized from video.	automatic number plate recognition;baseline (configuration management);convolutional neural network;end-to-end principle;image resolution;lr parser;lr-attributed grammar;neural networks;string (computer science);super-resolution imaging	Vojtech Vasek;Vojtech Franc;Martin Urban	2018			computer science;computer vision;artificial intelligence;convolutional neural network;license;superresolution	Vision	25.42467700398558	-53.78685934822632	183507
57c270a9f468f7129643852945cf3562cbb76e07	multi-label convolutional neural network based pedestrian attribute classification		Recently, pedestrian attributes like gender, age, clothing etc., have been used as soft biometric traits for recognizing people. Unlike existing methods that assume the independence of attributes during their prediction, we propose a multi-label convolutional neural network (MLCNN) to predict multiple attributes together in a unified framework. Firstly, a pedestrian image is roughly divided into multiple overlapping body parts, which are simultaneously integrated in the multi-label convolutional neural network. Secondly, these parts are filtered independently and aggregated in the cost layer. The cost function is a combination of multiple binary attribute classification cost functions. Experiments show that the proposed method significantly outperforms the SVM based method on the PETA database.	artificial neural network;biometrics;convolutional neural network;effective method;expectation propagation;experiment;loss function;multi-label classification;pixel;test engineer;unified framework	Jianqing Zhu;Shengcai Liao;Zhen Lei;Stan Z. Li	2017	Image Vision Comput.	10.1016/j.imavis.2016.07.004	computer science;machine learning;pattern recognition;data mining	AI	30.984727159704608	-52.352018235580516	185333
eb3e0a14851b681ee254ff0c2be5c6b29025c823	pornographic image recognition by strongly-supervised deep multiple instance learning	detectors;image recognition;training;breast;indexes;feature extraction;graphics	In this paper, we propose a principled framework for pornographic image recognition. Specifically, we present our definition of pornographic images, which characterizes the pornographic contents in images as the exposure of private body parts. As the private body parts often lie in local image regions, we model each image as a bag of local image patches (instances), and assume that for each pornographic image at least one instance accounts for the pornographic content within it. This treatment allows us to cast the model training as a Multiple Instance Learning (MIL) problem. Furthermore, we propose a strongly-supervised setting for MIL by identifying the most likely pornographic instances in positive bags, which effectively prevents the algorithm from getting trapped in a bad local optima. Last but not least, we formulate our strongly-supervised MIL under the deep CNN framework to learn deep representations; hence we call it Strongly-supervised Deep MIL (SD-MIL). We demonstrate that our SD-MIL based system produces remarkable accuracy with 97.01% TPR at 1% FPR, testing on 117K pornographic images and 117K normal images from our newly-collected large scale dataset.	algorithm;computer vision;film-type patterned retarder;local optimum;multiple instance learning;patch (computing);secure digital	Yuhui Wang;Xin Jin;Xiaoyang Tan	2016	2016 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2016.7533195	database index;computer vision;detector;feature extraction;computer science;graphics;machine learning;pattern recognition	Vision	28.136842708565787	-56.23664702912854	186240
500f3eb4eb0beae497c0247224bfb088efd67621	detecting tampered videos with multimedia forensics and deep learning		User-Generated Content (UGC) has become an integral part of the news reporting cycle. As a result, the need to verify videos collected from social media and Web sources is becoming increasingly important for news organisations. While video verification is attracting a lot of attention, there has been limited effort so far in applying video forensics to real-world data. In this work we present an approach for automatic video manipulation detection inspired by manual verification approaches. In a typical manual verification setting, video filter outputs are visually interpreted by human experts. We use two such forensics filters designed for manual verification, one based on Discrete Cosine Transform (DCT) coefficients and a second based on video requantization errors, and combine them with Deep Convolutional Neural Networks (CNN) designed for image classification. We compare the performance of the proposed approach to other works from the state of the art, and discover that, while competing approaches perform better when trained with videos from the same dataset, one of the proposed filters demonstrates superior performance in cross-dataset settings. We discuss the implications of our work and the limitations of the current experimental setup, and propose directions for future research in this area.		Markos Zampoglou;Fotini Markatopoulou;Grégoire Mercier;Despoina Touska;Evlampios E. Apostolidis;Symeon Papadopoulos;Roger Cozien;Ioannis Patras;Vasileios Mezaris;Yiannis Kompatsiaris	2019		10.1007/978-3-030-05710-7_31	filter (video);convolutional neural network;computer science;computer vision;artificial intelligence;discrete cosine transform;deep learning;contextual image classification;social media	Vision	24.666179131045894	-55.21707984941152	188109
f64fbefe2126aa0665b4671ff6f589f5d3159ec4	human action recognition based on improved motion history image and deep convolutional neural networks		In order to make full use of video color image sequences for human action recognition, we proposed an approach to recognize human action based on motion history image (MHI) and deep convolution networks. Firstly, part of frames from the beginning and end of the motion video are removed and the gray MHI are extracted from the rest. Then we colorize them into 3 channels RGB by the rainbow encoding. Finally, we train a deep convolutional neural network to classify the RGB MHI by fine tuning a pretrained model. The experimental results indicate that the proposed method improves the recognition accuracy rate by 13% remarkably.	artificial neural network;color image;convolution;convolutional neural network;mathematical optimization;uniform theory of diffraction;video	Qiuping Chun;Erhu Zhang	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302061	grayscale;convolutional neural network;computer vision;fine-tuning;color image;pattern recognition;feature extraction;artificial intelligence;computer science;rgb color model;convolution	Vision	28.029844085361102	-52.748647220308676	188930
f96bdd1e2a940030fb0a89abbe6c69b8d7f6f0c1	comparison of human and computer performance across face recognition experiments	human performance;algorithm performance;face recognition;challenge problem	a r t i c l e i n f o Since 2005, human and computer performance has been systematically compared as part of face recognition competitions, with results being reported for both still and video imagery. The key results from these competitions are reviewed. To analyze performance across studies, the cross-modal performance analysis (CMPA) framework is introduced. The CMPA framework is applied to experiments that were part of face a recognition competition. The analysis shows that for matching frontal faces in still images, algorithms are consistently superior to humans. For video and difficult still face pairs, humans are superior. Finally, based on the CMPA framework and a face performance index, we outline a challenge problem for developing algorithms that are superior to humans for the general face recognition problem.	algorithm;computer performance;experiment;facial recognition system;modal logic	P. Jonathon Phillips;Alice J. O'Toole	2014	Image Vision Comput.	10.1016/j.imavis.2013.12.002	facial recognition system;human performance technology;computer vision;simulation;computer science;artificial intelligence;machine learning;three-dimensional face recognition	Vision	26.240380947560233	-57.19965663365448	190182
27a432cfd44c1397b6d5f498f632dbfca73fb280	jointly learning shape descriptors and their correspondence via deep triplet cnns			algorithm;artificial neural network;bayesian network;complex network;convolutional neural network;deep belief network;deep learning;information;loss function;overfitting;point-to-point protocol;shape analysis (digital geometry);spectral method;test set;triplet state;unsupervised learning	Mingjia Chen;Changbo Wang;Hong Qin	2018	Computer Aided Geometric Design	10.1016/j.cagd.2018.03.002		EDA	28.512627777554666	-56.87606675630301	190259
dbb7f37fb9b41d1aa862aaf2d2e721a470fd2c57	face image analysis with convolutional neural networks		In this work, we present the problem of automatic appearance-based facial analysis with machine learning techniques and describe common specific subproblems like face detection, facial feature detection and face recognition which are the crucial parts of many applications in the context of indexation, surveillance, access-control or human-computer interaction. To tackle this problem, we particularly focus on a technique called Convolutional Neural Network (CNN) which is inspired by biological evidence found in the visual cortex of mammalian brains and which has already been applied to many different classification problems. Existing CNN-based methods, like the face detection system proposed by Garcia and Delakis, show that this can be a very effective, efficient and robust approach to non-linear image processing tasks such as facial analysis. An important step in many automatic facial analysis applications, e.g. face recognition, is face alignment which tries to translate, scale and rotate the face image such that specific facial features are roughly at predefined positions in the image. We propose an efficient approach to this problem using CNNs and experimentally show its very good performance on difficult test images. We further present a CNN-based method for automatic facial feature detection. The proposed system employs a hierarchical procedure which first roughly localizes the eyes, the nose and the mouth and then refines the result by detecting 10 different facial feature points. The detection rate of this method is 96% for the AR database and 87% for the BioID database tolerating an error of 10% of the inter-ocular distance. Finally, we propose a novel face recognition approach based on a specific CNN architecture learning a non-linear mapping of the image space into a lowerdimensional sub-space where the different classes are more easily separable. We applied this method to several public face databases and obtained better recognition rates than with classical face recognition approaches based on PCA or LDA. Moreover, the proposed system is particularly robust to noise and partial occlusions. We also present a CNN-based method for the binary classification problem of gender recognition with face images and achieve a state-of-the-art accuracy. The results presented in this work show that CNNs perform very well on various facial image processing tasks, such as face alignment, facial feature detection and face recognition and clearly demonstrate that the CNN technique is a versatile, efficient and robust approach for facial image analysis.	artificial neural network;binary classification;convolutional neural network;database;experiment;face detection;facial recognition system;feature detection (computer vision);feature detection (web development);feature recognition;human–computer interaction;image analysis;image noise;image processing;local-density approximation;machine learning;nonlinear system;principal component analysis;sensor	Stefan Duffner	2008			convolutional neural network;machine learning;deep learning;neocognitron;artificial intelligence;computer science	Vision	31.42371153915039	-58.44521550764254	191060
55327a1f45f27f34f58858b4a45a94cc01ced887	fast ray features for learning irregular shapes	image features;detectors;medical imagery;ray feature set;haar features;shape recognition feature extraction haar transforms object detection ray tracing;training;microscopy;shape recognition;histograms of oriented gradients;serveur institutionnel;adaboost image features ray feature set haar features face detection;shape;archive institutionnelle;image edge detection;feature extraction;open access;shape computational efficiency costs robustness image recognition microscopy histograms neurons face detection detectors;ray tracing;adaboost;archive ouverte unige;face detection;cybertheses;haar transforms;institutional repository;object detection	We introduce a new class of image features, the Ray feature set, that consider image characteristics at distant contour points, capturing information which is difficult to represent with standard feature sets. This property allows Ray features to efficiently and robustly recognize deformable or irregular shapes, such as cells in microscopic imagery. Experiments show Ray features clearly outperform other powerful features including Haar-like features and Histograms of Oriented Gradients when applied to detecting irregularly shaped neuron nuclei and mitochondria. Ray features can also provide important complementary information to Haar features for other tasks such as face detection, reducing the number of weak learners and computational cost. Ray features can be efficiently precomputed to reduce cost, just as precomputing integral images reduces the overall cost of Haar features. While Rays are slightly more expensive to precompute, their computational cost is less than that of Haar features for scanning an AdaBoost-based detector window across an image at run-time.	adaboost;algorithmic efficiency;birefringence;computation;face detection;haar wavelet;image gradient;neuron;precomputation;sensor	Kevin Smith;Alan Carleton;Vincent Lepetit	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459210	adaboost;ray tracing;computer vision;detector;face detection;haar-like features;feature extraction;shape;computer science;microscopy;machine learning;pattern recognition;feature	Vision	31.882724745246204	-53.91255770752581	191299
11fa8734e310fa919f165e33f2b3734b7be234c3	detection of human faces using neural networks		Human face detection is a key technology in machine vision applications including human recognition, access control, security surveillance and so on. This research proposes a precise scheme for human face detection using a hybrid neural network. The system is based on visual information of the face image sequences and is commenced with estimation of the skin area depending on color components. In this paper we have considered HSV and YCbCr color space to extract the visual features. These features are used to train the hybrid network consisting of a bidirectional associative memory BAM and a back propagation neural network BPNN. The BAM is used for dimensional reduction and the multi-layer BPNN is used for training the facial color features. Our system provides superior performance comparable to the existing methods in terms of both accuracy and computational efficiency. The low computation time required for face detection makes it suitable to be employed in real time applications.	artificial neural network;neural network software	Mozammel Chowdhury;Junbin Gao;Rafiqul Islam	2016		10.1007/978-3-319-46672-9_77	face detection;artificial intelligence;machine learning;bidirectional associative memory;hybrid neural network;hsl and hsv;object-class detection;machine vision;artificial neural network;computer science;pattern recognition;computer vision;color space	ML	29.885571595695684	-58.53524489501353	191452
25b925d211c9a0eac788bdc60be5cfb978a504a8	a convolutional neural network for automatic analysis of aerial imagery	shape recognition entropy image classification image colour analysis learning artificial intelligence neural nets object detection;tensile stress;image processing;neural networks;training;marine mammals;logistics;080104 computer vision;vectors;machine learning;three dimensional displays;image color analysis;negative training example selection method convolutional neural network automatic aerial imagery analysis marine species detection machine learning approach classifier color features entropy shape analysis;training tensile stress neural networks three dimensional displays logistics vectors image color analysis;convolutional neural network	This paper introduces a new method to automate the detection of marine species in aerial imagery using a Machine Learning approach. Our proposed system has at its core, a convolutional neural network. We compare this trainable classifier to a handcrafted classifier based on color features, entropy and shape analysis. Experiments demonstrate that the convolutional neural network outperforms the handcrafted solution. We also introduce a negative training example-selection method for situations where the original training set consists of a collection of labeled images in which the objects of interest (positive examples) have been marked by a bounding box. We show that picking random rectangles from the background is not necessarily the best way to generate useful negative examples with respect to learning.	adaboost;aerial photography;algorithm;artificial neural network;boosting (machine learning);computation;convolutional neural network;experiment;image resolution;learning classifier system;machine learning;metaheuristic;microsoft windows;minimum bounding box;pixel;sensor;shape analysis (digital geometry);statistical classification;test set	Frédéric Maire;Luis Mejías Alvarez;Amanda Hodgson	2014	2014 International Conference on Digital Image Computing: Techniques and Applications (DICTA)	10.1109/DICTA.2014.7008084	logistics;computer vision;image processing;computer science;machine learning;pattern recognition;time delay neural network;deep learning;convolutional neural network;stress	Vision	28.447986224708423	-56.31794350380006	191792
d87c8174b81f8926e8663600890c7b7235db4281	building robust industrial applicable object detection models using transfer learning and single pass deep learning architectures		The uprising trend of deep learning in computer vision and artificial intelligence can simply not be ignored. On the most diverse tasks, from recognition and detection to segmentation, deep learning is able to obtain state-of-the-art results, reaching top notch performance. In this paper we explore how deep convolutional neural networks dedicated to the task of object detection can improve our industrial-oriented object detection pipelines, using state-of-the-art open source deep learning frameworks, like Darknet. By using a deep learning architecture that integrates region proposals, classification and probability estimation in a single run, we aim at obtaining real-time performance. We focus on reducing the needed amount of training data drastically by exploring transfer learning, while still maintaining a high average precision. Furthermore we apply these algorithms to two industrially relevant applications, one being the detection of promotion boards in eye tracking data and the other detecting and recognizing packages of warehouse products for augmented advertisements.	algorithm;artificial intelligence;artificial neural network;computer vision;convolutional neural network;darknet;deep learning;eye tracking;information retrieval;object detection;open-source software;pipeline (computing);real-time clock;sensor	Steven Puttemans;Timothy Callemein;Toon Goedemé	2018		10.5220/0006562002090217	transfer of learning;machine learning;object detection;deep learning;computer science;artificial intelligence	AI	26.930901078556403	-54.32051641072589	193869
8268803a432474dc33f1282d56d997007dfdd40f	an eye detection method based on convolutional neural networks and support vector machines			artificial neural network;convolutional neural network;support vector machine	Mingxin Yu;Xiaoying Tang;Yingzi Lin;David Schmidt;Xiangzhou Wang;Yikang Guo;Bo Liang	2018	Intell. Data Anal.	10.3233/IDA-173361	artificial intelligence;machine learning;support vector machine;convolutional neural network;computer science;pattern recognition	ML	29.759008701065877	-57.675656556929056	194231
0ebd5316edeb7e25dd06890f12aa33d60a7c2138	team tactics estimation in soccer videos via deep extreme learning machine based on players formation		A method of team tactics estimation in soccer videos is presented in this paper. Our method enables estimation of basic tactics in each team on the basis of the Deep-Extreme Learning Machine (DELM) by using features of players formation. In the soccer games, team tactics relate to each other team. Therefore, the proposed method obtains final estimation results by utilizing two DELMs of each team and their relationship. Since the proposed method takes into consideration the relevance of the estimated tactics in each team, we realize accurate tactics estibimation. Experimental results using actual soccer videos showed the effectiveness of our method.		Genki Suzuki;Sho Takahashi;Takahiro Ogawa;Miki Haseyama	2018	2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2018.8574814	simulation;deep learning;extreme learning machine;artificial intelligence;computer science	Robotics	24.755098425196074	-56.554415550932674	194265
97cdd7249dbf6c57cc106bb64314cbe824e7c942	on the importance of multi-dimensional information in gender estimation from face images	performance improvement;face image;state-of-the-art approach;human face demography;multi-dimensional approach;elderly face;gender variable;relevant application;demographic variable;traditional gender classifier;multi-dimensional information;gender estimation;gender classifier	performance improvement;face image;state-of-the-art approach;human face demography;multi-dimensional approach;elderly face;gender variable;relevant application;demographic variable;traditional gender classifier;multi-dimensional information;gender estimation;gender classifier		Juan Bekios-Calfa;José Miguel Buenaposada;Luis Baumela	2011		10.1007/978-3-642-25085-9_31	data mining	Vision	30.290793171861218	-58.40032717426837	194865
ec4b9a8f9c8d1bb57ad89a6325d7161683d32863	neural video compression based on surf scene change detection algorithm			algorithm;data compression;speeded up robust features	Rafal Grycuk;Michal Knop	2015		10.1007/978-3-319-23814-2_13	data compression;change detection;computer vision;speech recognition;computer science;artificial intelligence	Vision	29.341403099760598	-58.01592800766763	195125
f5373133883c7c4d1ac77fb8400e83b30ae5889a	global-local feature fusion for image classification of flood affected roads from social multimedia				Benjamin Bischke;Patrick Helber;Andreas Dengel	2018				Vision	28.74400589793108	-57.76743941180717	195148
5ac3fc081121933e2f41cd412d62983090f3c96f	reliability and data density in high capacity color barcodes	color;classification;barcode;settore ing inf 05 sistemi di elaborazione delle informazioni	2D color barcodes have been introduced to obtain larger storage capabilities than traditional black and white barcodes. Unfortunately, the data density of color barcodes is substantially limited by the redundancy needed for correcting errors, which are due not only to geometric but also to chromatic distortions introduced by the printing and scanning process. The higher the expected error rate, the more redundancy is needed for avoiding failures in barcode reading, and thus, the lower the actual data density. Our work addresses this trade-off between reliability and data density in 2D color barcodes and aims at identifying the most effective algorithms, in terms of byte error rate and computational overhead, for decoding 2D color barcodes. In particular, we perform a thorough experimental study to identify the most suitable color classifiers for converting analog barcode cells to digital bit streams. To accomplish this task, we implemented a prototype capable of decoding 2D color barcodes by using different methods, including clustering algorithms and machine learning classifiers. We show that, even if state-of-art methods for color classification could be successfully used for decoding color barcodes in the desktop scenario, there is an emerging need for new color classification methods in the mobile scenario. In desktop scenarios, our experimental findings show that complex techniques, such as support vector machines, does not seem to pay off, as they do not achieve better accuracy in classifying color barcode cells. The lowest error rates are indeed obtained by means of clustering algorithms and probabilistic classifiers. From the computational viewpoint, classification with clustering seems to be the method of choice. In mobile scenarios, simple and efficient methods (in terms of computational time) such as the Euclidean and the K-means classifiers are not effective (in terms of error rate), while, more complex methods are effective but not efficient. Even if a few color barcode designs have been proposed in recent studies, to the best of our knowledge, there is no previous research that addresses a comparative and experimental analysis of clustering and machine learning methods for color classification in 2D color barcodes.	algorithm;areal density (computer storage);barcode;bit error rate;byte;cluster analysis;computation;desktop computer;distortion;euclidean distance;experiment;k-means clustering;machine learning;mobile device;overhead (computing);printing;probabilistic turing machine;prototype;support vector machine;time complexity	Marco Querini;Giuseppe F. Italiano	2014	Comput. Sci. Inf. Syst.	10.2298/CSIS131218054Q	computer vision;biological classification;computer science;data mining;computer graphics (images)	ML	26.687465027982093	-58.11167498152977	195252
5882f42ae73b15dc2cac181ecb10f35e3a74b7d5	deep residual net based compact feature representation for image retrieval		Deep learning technology has been introduced into many multimedia processing tasks, including multimedia retrieval. In this paper, we propose a deep residual net (ResNet) based compact feature representation improve the content-based image retrieval (CBIR) performance. The proposed method integrates ResNet and hashing networks to convert the raw images into binary codes. The binary codes of images in query set and that of the database are compared using Hamming distance for retrieval. Comprehensive experiments are executed on three public databases. The results show that the proposed method outperforms state-of-the-art methods. Furthermore, the impact of the deep convolutional network (DCNN)’s depth on the performance is investigated.	image retrieval	Cong Bai;Jian Chen;Qing Ma;Zhi Liu;Shengyong Chen	2018		10.1007/978-3-030-00767-6_68	binary code;residual;computer vision;hamming distance;deep learning;content-based image retrieval;hash function;artificial intelligence;image retrieval;computer science;residual neural network;pattern recognition	Vision	25.642495002818766	-52.89554700912773	195310
c1d1b1197667facd360196059796ab8cb7b46e96	polymapper: extracting city maps using polygons		We propose a method to leapfrog pixel-wise, semantic segmentation of (aerial) images and predict objects in a vector representation directly. PolyMapper predicts maps of cities from aerial images as collections of polygons with a learnable framework. Instead of the usual multi-step procedure of semantic segmentation, shape improvement, conversion to polygons, and polygon refinement, our approach learns mappings with a single network architecture and directly outputs maps. We demonstrate that our method is capable of drawing polygons of buildings and road networks that very closely approximate the structure of existing online maps such as OpenStreetMap, and it does so in a fully automated manner. Validation on existing and novel large scale data sets of several cities show that our approach achieves good levels of performance.		Zhi Qiang Li;Jan Dirk Wegner;Aur'elien Lucchi	2018	CoRR			Vision	28.38735156446582	-53.469497404859844	195398
eb5cc4604cab6399c8a2b10a7367725810450925	learning affective features based on vip for video affective content analysis		Video affective computing aims to recognize, interpret, process, and simulate human affective of videos from visual, textual, and auditory sources. An intrinsic challenge is how to extract effective representations to analyze affection. In view of this problem, we propose a new video affective content analysis framework. In this paper, we observe the fact that only a few actors play an important role in video, leading the trend of video emotional developments. We provide a novel solution to distinguish the important one and call it the very important person (VIP). Meanwhile, we design a novel keyframes selection strategy to select the keyframes including the VIPs. Furthermore, scale invariant feature transform (SIFT) features corresponding to a set of patches are first extracted from each VIP keyframe, which forms a SIFT feature matrix. Next, the feature matrix is fed to a convolutional neural network (CNN) to learn discriminative representations, which make CNN and SIFT complement each other. Experimental results on two public audio-visual emotional datasets, including the classical LIRIS-ACCEDE and the PMSZU dataset we built, demonstrate the promising performance of the proposed method and achieve better performance than other compared methods.		Yingying Zhu;Min Tong;Tinglin Huang;Zhenkun Wen;Qi Tian	2018		10.1007/978-3-030-00764-5_64	video content analysis;convolutional neural network;discriminative model;artificial intelligence;affective computing;computer science;affect (psychology);content analysis;pattern recognition;scale-invariant feature transform	Vision	26.458369238060047	-55.35723015232864	195641
56fe66b5daaa2d9fd8c1d6243de5a3964882f338	dictionaries for image-based recognition	domain adaptation image based recognition dictionary learning image processing video data processing object recognition nonlinear kernel sparse representation computer vision problem multimodal biometrics recognition;object recognition biometrics access control computer vision dictionaries image recognition image representation learning artificial intelligence;dictionaries iris recognition training kernel vectors optimization	In recent years, Sparse Representation (SR) and Dictionary Learning (DL) have emerged as powerful tools for efficiently processing of image and video data in non-traditional ways. An area of promise for these theories is object recognition. In this paper, we present an overview of SR and DR and examine several interesting object recognition approaches using these theories. We will also explore the use of non-linear kernel SR as well as DL methods in many computer vision problems including object recognition, multimodal biometrics recognition, and domain adaptation.	biometrics;computer vision;dictionary;domain adaptation;machine learning;multimodal interaction;nonlinear system;object detection;outline of object recognition;sparse;sparse approximation;sparse matrix;supervised learning;theory	Vishal M. Patel;Qiang Qiu;Rama Chellappa	2013	2013 Information Theory and Applications Workshop (ITA)	10.1109/ITA.2013.6502927	computer vision;feature;intelligent character recognition;computer science;machine learning;pattern recognition;three-dimensional face recognition;3d single-object recognition;sketch recognition	Vision	30.090789981426916	-57.0184624410966	196750
7cef4e214b2c9f0a950ea938d50ebb4f2ae250f0	automated detection of geological landforms on mars using convolutional neural networks	mars;transverse aeolian ridges;support vector machines;volcanic rootless cones;convolutional neural networks	The large volume of high-resolution images acquired by the Mars Reconnaissance Orbiter has opened a new frontier for developing automated approaches to detecting landforms on the surface of Mars. However, most landform classifiers focus on crater detection, which represents only one of many geological landforms of scientific interest. In this work, we use Convolutional Neural Networks (ConvNets) to detect both volcanic rootless cones and transverse aeolian ridges. Our system, named MarsNet, consists of five networks, each of which is trained to detect landforms of different sizes. We compare our detection algorithm with a widely used method for image recognition, Support Vector Machines (SVMs) using Histogram of Oriented Gradients (HOG) features. We show that ConvNets can detect a wide range of landforms and has better accuracy and recall in testing data than traditional classifiers based on SVMs.		Leon F. Palafox;Christopher W. Hamilton;Stephen P. Scheidt;Alexander M. Alvarez	2017	Computers & geosciences	10.1016/j.cageo.2016.12.015	support vector machine;mars exploration program;hydrology;computer science;machine learning;convolutional neural network;remote sensing	ML	28.154363862823704	-54.2701308812279	196983
7eb089194d9d98248d6d11ccc29e2c10c73e87a1	a low-power neuromorphic system for real-time visual activity recognition		We describe a high-accuracy, real-time, neuromorphic method and system for activity recognition in streaming or recorded videos from static and moving platforms that can detect even small objects and activities with high-accuracy. Our system modifies and integrates multiple independent algorithms into an end-to-end system consisting of five primary modules: object detection, object tracking, convolutional neural network image feature extractor, recurrent neural network sequence feature extractor, and an activity classifier. We also integrate neuromorphic principles of foveated detection similar to how the retina works in the human visual system and the use of contextual knowledge about activities to filter the activity recognition results. We mapped the complete activity recognition pipeline to the COTS NVIDIA Tegra TX2 development kit and demonstrate real-time activity recognition from streaming drone videos at less than 10 W power consumption.	activity recognition;neuromorphic engineering;real-time transcription	Deepak Khosla;Ryan Uhlenbrock;Yang Chen	2018		10.1007/978-3-030-03801-4_10	pattern recognition;artificial intelligence;computer vision;computer science;convolutional neural network;video tracking;deep learning;object detection;recurrent neural network;human visual system model;activity recognition;neuromorphic engineering	HCI	27.822045873976144	-52.93467236081401	197092
9197c062dfabe90c350ef4c06012320e55bd76e7	locate, segment and match: a pipeline for object matching and registration		Image registration requires simultaneous processing of multiple images to match the keypoints or landmarks of the contained objects. These images often come from different modalities for example CT and Ultrasound (US), and pose the challenge of establishing one to one correspondence. In this work, a novel pipeline of convolutional neural networks is developed to perform the desired registration. The complete objective is divided into three parts: localization of the object of interest, segmentation and matching transformation. Most of the existing approaches skip the localization step and are prone to fail in general scenarios. We overcome this challenge through detection which also establishes initial correspondence between the images. A modified version of single shot multibox detector is used for this purpose. The detected region is cropped and subsequently segmented to generate a mask corresponding to the desired object. The mask is used by a spatial transformer network employing thin plate spline deformation to perform the desired registration. Initial experiments on MNIST and Caltech-101 datasets show that the proposed model is able to accurately match the segmented images. The proposed framework is extended to the registration of CT and US images, which is free from any data specific assumptions and has better generalization capability as compared to the existing rule based/classical approaches.		Deepak Mishra;Rajeev Ranjan;Santanu Chaudhury;Mukul Sarkar;Arvinder Singh Soin	2018	CoRR		pattern recognition;convolutional neural network;bijection;rule-based system;computer science;artificial intelligence;mnist database;detector;thin plate spline;image registration	Vision	30.118932258183797	-52.252269384409225	197140
4ae1a837c6b74f48d6a048c2d687c5387b9d70cb	dilated convolutions for image classification and object localization		Yu et al.[1] showed that dilated convolutions are very effective in dense prediction problems such as semantic segmentation. In this work, we propose a new ResNet[2] based convolutional neural network model using dilated convolutions and show that this model can achieve lower error rate for image classification than ResNet with reduction of the number of the parameters of the network by 94% and that this model has high ability to localize objects despite being trained on image-level labels. We evaluated this model on ImageNet[5] which has 50 class labels randomly selected from 1000 class labels.	artificial neural network;computer vision;convolution;convolutional neural network;network model;randomness	Yasunori Kudo;Yoshimitsu Aoki	2017	2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)	10.23919/MVA.2017.7986898	mathematics;word error rate;convolutional neural network;computer vision;artificial intelligence;pattern recognition;residual neural network;contextual image classification;convolution;segmentation	Vision	25.67952229200771	-52.66805058874183	197258
9e6c15150179ce848402e89bd245831d9935f4f9	bi-modal face recognition - how combining 2d and 3d clues can increase the precision		This paper introduces a bi-modal face recognition approach. The objective is to study how combining depth and intensity information can increase face recognition precision. In the proposed approach, local features based on LBP (Local Binary Pattern) and DLBP (Depth Local Binary Pattern) are extracted from intensity and depth images respectively. Our approach combines the results of classifiers trained on extracted intensity and depth cues in order to identify faces. Experiments are performed on three datasets: Texas 3D face dataset, BOSPHORUS 3D face dataset and FRGC 3D face dataset. The obtained results demonstrate the enhanced performance of the proposed method compared to mono-modal (2D or 3D) face recognition. Most processes of the proposed system are performed automatically. It leads to a potential prototype of face recognition using the latest RGB-D sensors, such as Microsoft Kinect or Intel RealSense 3D Camera.	3d computer graphics;binary pattern (image generation);depth perception;experiment;facial recognition system;intel realsense;kinect;local binary patterns;modal logic;prototype;sensor;three-dimensional face recognition	Amel Aissaoui;Jean Martinet	2015		10.5220/0005359605590564	computer science;computer vision;artificial intelligence;facial recognition system;pattern recognition;local binary patterns;rgb color model;three-dimensional face recognition;depth perception	Vision	30.932614459411514	-55.85658785165663	198252
724cddb758017457dfe4ff89af9e0a56245ad8ef	learning primitive and scene semantics of images for classification and retrieval	scene classification;supervised learning;machine learning;image semantics;self organization;natural scenes;level 1;image retrieval	We present a learning-based semantics approach for classifying and retrieving images. Our approach defines semantics at two levels: (1) primitive semantics at the patch level, extracted automatically from pixel characteristics of patches with supervised learning; and (2) scene semantics at the image level, recognized from the association of primitive semantics of the patches in a self-organizing manner. Images are classified and retrieved according to the similarity in scene semantics. Our experiments so far have yielded highly accurate scene classification results and very promising retrieval performance on a set of diverse natural scene images.	douglas comer;experiment;organizing (structure);pixel;scene graph;seaside;self-organization;supervised learning;waterfall model	Cheong Yiu Fung;Kia-Fock Loe	1999		10.1145/319878.319881	computer vision;visual word;self-organization;image retrieval;computer science;machine learning;pattern recognition;supervised learning;automatic image annotation	Vision	30.399981065185887	-52.82467281482452	198362
8a96500a20d0ae89c7fbe953c2c3f21fd8114d1f	action recognition using low-rank sparse representation			sparse approximation	Shilei Cheng;Song Gu;Maoquan Ye;Mei Xie	2018	IEICE Transactions		pattern recognition;artificial intelligence;computer science;sparse approximation	Vision	29.39192797150258	-56.9754392815417	198602
c62d2ef736106f1e0065aadebb5b721870aacc73	resilient face expressional recognitions using geometry and behavioural traits	gas;facial expressions;geometry methods;behavioural characteristics;biometrics;face recognition;genetic algorithms	Tremendous growth in the use of biometric technologies in high-security applications has motivated the requirement for highly dependable face recognition systems with expressional robustness. The previous works try to achieve insensitivity to such variations leads to the motivation in classifying the physiological properties of the facial expressions and mapping it to the behavioural traits underlying the expression. The system developed in this paper describes and evaluates a resilient face expressional recognition system using a geometric structural representation of the various expressions like sad, angry, disguise, happy, etc. and mapping it to the behavioural traits stored in the form of either hidden or exposed property in the genes adopted in genetic algorithm model. The experimental evaluations are conducted with initial taking of ten expression samples of each and every faces of 63 humans (21 men and 42 women). The behaviour pattern mapping of the expressions are associated with the genetic prope...		J. K. Kani Mozhi;R. S. D. Wahida Banu	2010	IJMEI	10.1504/IJMEI.2010.029805	facial recognition system;computer vision;genetic algorithm;computer science;facial expression;biometrics	Vision	30.99601898739844	-58.819111334368884	198763
15d5d41fc7c8b5c6bada450e231c9c0d7d089dc6	a new pyramid architecture for bottom-up image analysis	image analysis;bottom up		image analysis;top-down and bottom-up design	Kuo-Liang Chung	1986	J. Inf. Sci. Eng.		computer science;pyramid (image processing);distributed computing;architecture;pyramid;computer vision;top-down and bottom-up design;artificial intelligence	DB	30.230404684342083	-57.17832784695159	199276
02b0d53532e383f2a829eaabccdceef99a442d41	facial feature discovery for ethnicity recognition				Cunrui Wang;Qingling Zhang;Wanquan Liu;Yu Liu;Lixin Miao	2019	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.1278	ethnic group;machine learning;neural coding;artificial intelligence;sparse approximation;computer science	ML	30.32104099344758	-58.17865715996471	199306
