id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
411d1d9108348825df35b5d3683c27e0d5be5197	repitools: an r package for the analysis of enrichment-based epigenomic data	software;genomics;software tool;dato;data;gene regulation;epigenetique;analyse;histones;enrichment;article letter to editor;donnee;enrichissement;epigenesis genetic;epigenetica;software package;histone modification;dna methylation;analysis;dna sequence;enriquecimiento;epigenetics;oligonucleotide array sequence analysis;analisis	SUMMARY Epigenetics, the study of heritable somatic phenotypic changes not related to DNA sequence, has emerged as a critical component of the landscape of gene regulation. The epigenetic layers, such as DNA methylation, histone modifications and nuclear architecture are now being extensively studied in many cell types and disease settings. Few software tools exist to summarize and interpret these datasets. We have created a toolbox of procedures to interrogate and visualize epigenomic data (both array- and sequencing-based) and make available a software package for the cross-platform R language.   AVAILABILITY The package is freely available under LGPL from the R-Forge web site (http://repitools.r-forge.r-project.org/)   CONTACT mrobinson@wehi.edu.au.	biopolymer sequencing;diploid cell;epigenetic process;epigenomics;forge;gene expression regulation;gene ontology term enrichment;histones;r language;web site;anatomical layer;study of epigenetics	Aaron L. Statham;Dario Strbenac;Marcel W. Coolen;Clare Stirzaker;Susan J. Clark;Mark D. Robinson	2010		10.1093/bioinformatics/btq247	biology;molecular biology;bioinformatics;epigenetics;histone;analysis;genetics	Comp.	-2.956827765934397	-57.622163112630304	117991
7c73ad4dd8ca3b4f50ca1b38507556ee2465aa41	peptide finder: mapping measured molecular masses to peptides and proteins	peptides;proteine;bioinformatique;peptido;molecular mass;peptide;proteina;bioinformatica;protein;bioinformatics	UNLABELLED The identification of unknown amino acid sequences of peptides as well as protein identification is of great significance in proteomics. Here, we present a publicly available web application that facilitates a high resolution mapping of measured molecular masses to peptides and proteins, irrespectively of the enzyme/digestion method used. Furthermore, multi-filtering may be applied in terms of measured mass tolerance, molecular mass and isoelectric point range as well as pattern matching to refine the results. This approach serves complementary to the existing solutions for protein identification and gives insights in novel peptides discovery and protein identification at the cases where the identification scores from the other approaches may be below significance threshold. Peptide Finder has been proven useful in proteomics procedures with experimental data from MALDI-TOF.   AVAILABILITY Peptide Finder web-application is available at http://bioserver-1.bioacademy.gr/Bioserver/PeptideFinder/.	amino acid sequence;amino acids;isoelectric point;molecular mass;numerous;pattern matching;protein digestion (research activity);proteomics;solutions;spectrometry, mass, matrix-assisted laser desorption-ionization;web application	Anastasia Alexandridou;George T. Tsangaris;Konstantinos N. Vougas;Konstantina S. Nikita;George M. Spyrou	2008	Bioinformatics	10.1093/bioinformatics/btn413	peptide spectral library;peptide bond;biology;biochemistry;molecular biology;bioinformatics;molecular mass;peptide mass fingerprinting	Comp.	1.123665966359914	-58.340601266583704	117992
77601b8bc78fe0ce7f48b96ed1a5d0bc41590b6b	dash: localising dynamic programming for order of magnitude faster, accurate sequence alignment	magnitude speed improvement;sensitivity benchmark;improved sensitivity;similar speed;magnitude faster;sensitivity gain;moderate speed;superior sensitivity;companion paper;localising dynamic programming;complete dp;sensitivity result;accurate sequence alignment;molecular biophysics;sequence alignment;nucleotides;genetics;proteins;dynamic programming;exponential growth	In this paper we present our genomic and proteomic sequence alignment algorithm, DASH, which results in order of magnitude speed improvement when compared to NCBI-BLAST 2.2.6, with superior sensitivity. Dynamic programming (DP) is the predominant contributor to search time for algorithms such as BLAST and FastA/P. Improving the efficiency of DP provides an opportunity to increase sensitivity, or significantly reduce search times and help offset the effects of the continuing exponential growth in database sizes. Specifically, for nucleotide searching we have demonstrated an order of magnitude speed improvement with significantly improved sensitivity, or alternatively moderate speed up with further sensitivity gains, depending on the parameters selected. Smith-Waterman complete DP is used as the sensitivity benchmark. Similar speed and sensitivity results are presented for protein searching. Since our algorithm is highly parallel, we have developed dedicated hardware which we will present in a companion paper, and a distributed version of our software (DDASH), which we expect to provide linear speedup on a cluster.	blast;benchmark (computing);dynamic programming;fasta;proteomics;sequence alignment;smith–waterman algorithm;speedup;time complexity	Paul Gardner-Stephen;Greg Knowles	2004	Proceedings. 2004 IEEE Computational Systems Bioinformatics Conference, 2004. CSB 2004.	10.1109/CSB.2004.1332562	biology;exponential growth;nucleotide;computer science;bioinformatics;theoretical computer science;machine learning;dynamic programming;sequence alignment;genetics;algorithm;alignment-free sequence analysis;molecular biophysics	HPC	-1.7269910573203688	-52.97114483535675	118183
c8de801ec240b91dbcd9d491916891144222f4d6	simulation-based estimation of branching models for ltr retrotransposons		Motivation LTR retrotransposons are mobile elements that are able, like retroviruses, to copy and move inside eukaryotic genomes. In the present work, we propose a branching model for studying the propagation of LTR retrotransposons in these genomes. This model allows us to take into account both the positions and the degradation level of LTR retrotransposons copies. In our model, the duplication rate is also allowed to vary with the degradation level.   Results Various functions have been implemented in order to simulate their spread and visualization tools are proposed. Based on these simulation tools, we have developed a first method to evaluate the parameters of this propagation model. We applied this method to the study of the spread of the transposable elements ROO, GYPSY and DM412 on a chromosome of Drosophila melanogaster .   Availability and Implementation Our proposal has been implemented using Python software. Source code is freely available on the web at https://github.com/SergeMOULIN/retrotransposons-spread .   Contact serge.moulin@univ-fcomte.fr.   Supplementary information are available at Bioinformatics online.	bioinformatics;copy (object);dna transposable elements;elegant degradation;gene duplication abnormality;genome;imagery;list of python software;long terminal repeat;radiology information systems;retrotransposons;retroviridae;simulation;software propagation;source code	Serge Moulin;Nicolas Seux;Stéphane Chrétien;Christophe Guyeux;Emmanuelle Lerat	2017	Bioinformatics	10.1093/bioinformatics/btw622	biology;bioinformatics;genetics	Comp.	-1.8139371111884988	-56.36508913390555	118385
0d5bffc8ae879dd573008146a6d5a7132dd25ddc	statistical distributions of sequencing by synthesis with probabilistic nucleotide incorporation	probability;combinatorics;nucleotides;sequencing by synthesis;next generation dna sequencing;sequence analysis;statistical distribution	Sequencing by synthesis is used in many next-generation DNA sequencing technologies. Some of the technologies, especially those exploring the principle of single-molecule sequencing, allow incomplete nucleotide incorporation in each cycle. We derive statistical distributions for sequencing by synthesis by taking into account the possibility that nucleotide incorporation may not be complete in each flow cycle. The statistical distributions are expressed in terms of nucleotide probabilities of the target sequences and the nucleotide incorporation probabilities for each nucleotide. We give exact distributions both for fixed number of flow cycles and for fixed sequence length. Explicit formulas are derived for the mean and variance of these distributions. The results are generalizations of our previous work for pyrosequencing. Incomplete nucleotide incorporation leads to significant change in the mean and variance of the distributions, but still they can be approximated by normal distributions with the same mean and variance. The results are also generalized to handle sequence context dependent incorporation. The statistical distributions will be useful for instrument and software development for sequencing by synthesis platforms.	approximation algorithm;biopolymer sequencing;boyce–codd normal form;data interpretation, statistical;generalization (psychology);nucleotides;partial;probability;pyrosequencing;sample variance;software development;statistical distributions	Yong Kong	2009	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2008.0215	probability distribution;biology;molecular biology;nucleotide;bioinformatics;sequence analysis;probability;mathematics;genetics;statistics	Comp.	2.6678567189057802	-54.85749410515638	118461
94f2f4a86452e93f6f0ecf28f9f748f9ac3b0763	t2prhd: a tool to study the patterns of repeat evolution	evolution molecular;software;evolutionary model;phylogeny;gene family evolution;large dataset;phylogenetic reconstruction;chromosome mapping;sequence analysis dna;repetitive sequences nucleic acid;computational biology bioinformatics;dna mutational analysis;biological evolution;phylogenetic tree;gene family;model development;command line interface;algorithms;pattern recognition automated;positional information;molecular sequence data;combinatorial libraries;base sequence;computer appl in life sciences;multigene family;qp physiology elettan;microarrays;bioinformatics	"""The models developed to characterize the evolution of multigene families (such as the birth-and-death and the concerted models) have also been applied on the level of sequence repeats inside a gene/protein. Phylogenetic reconstruction is the method of choice to study the evolution of gene families and also sequence repeats in the light of these models. The characterization of the gene family evolution in view of the evolutionary models is done by the evaluation of the clustering of the sequences with the originating loci in mind. As the locus represents positional information, it is straightforward that in the case of the repeats the exact position in the sequence should be used, as the simple numbering according to repeat order can be misleading. We have developed a novel rapid visual approach to study repeat evolution, that takes into account the exact repeat position in a sequence. The """"pairwise repeat homology diagram"""" visualizes sequence repeats detected by a profile HMM in a pair of sequences and highlights their homology relations inferred by a phylogenetic tree. The method is implemented in a Perl script (t2prhd) available for downloading at http://t2prhd.sourceforge.net and is also accessible as an online tool at http://t2prhd.brc.hu . The power of the method is demonstrated on the EGF-like and fibronectin-III-like (Fn-III) domain repeats of three selected mammalian Tenascin sequences. Although pairwise repeat homology diagrams do not carry all the information provided by the phylogenetic tree, they allow a rapid and intuitive assessment of repeat evolution. We believe, that t2prhd is a helpful tool with which to study the pattern of repeat evolution. This method can be particularly useful in cases of large datasets (such as large gene families), as the command line interface makes it possible to automate the generation of pairwise repeat homology diagrams with the aid of scripts."""	cessation of life;cluster analysis;command-line interface;download;ephrin type-b receptor 1, human;evolution;evolutionary algorithm;gene family;hidden markov model;homologous gene;homology (biology);inference;locus;mammals;perl;phylogenetic tree;phylogenetics;sourceforge;voronoi diagram;statistical cluster	Botond Sipos;Kálmán Somogyi;István Andó;Zsolt Pénzes	2007	BMC Bioinformatics	10.1186/1471-2105-9-27	biology;command-line interface;phylogenetic tree;dna microarray;bioinformatics;gene family;genetics;phylogenetics	Comp.	0.7726093885952827	-57.45279472209578	119189
244061fbb6cc13e1f3e837d0d01643a731eb7f36	a fuzzy guided genetic algorithm for operon prediction	prediccion;functional annotation;experimental method;regulatory network;score function;bacterie;receiver operator characteristic;algoritmo borroso;reconstruction;estructura;reseau;multiple criteria;computational method;algoritmo genetico;bacillus subtilis;red;genoma a;genome a;fuzzy algorithm;algorithme genetique;roc curve;algorithme flou;genetic algorithm;operon;bacteria;a genome;bacillaceae;prediction;structure;bacillales;network;genome sequence;reconstruccion	MOTIVATION The operon structure of the prokaryotic genome is a critical input for the reconstruction of regulatory networks at the whole genome level. As experimental methods for the detection of operons are difficult and time-consuming, efforts are being put into developing computational methods that can use available biological information to predict operons.   METHOD A genetic algorithm is developed to evolve a starting population of putative operon maps of the genome into progressively better predictions. Fuzzy scoring functions based on multiple criteria are used for assessing the 'fitness' of the newly evolved operon maps and guiding their evolution.   RESULTS The algorithm organizes the whole genome into operons. The fuzzy guided genetic algorithm-based approach makes it possible to use diverse biological information like genome sequence data, functional annotations and conservation across multiple genomes, to guide the organization process. This approach does not require any prior training with experimental operons. The predictions from this algorithm for Escherchia coli K12 and Bacillus subtilis are evaluated against experimentally discovered operons for these organisms. The accuracy of the method is evaluated using an ROC (receiver operating characteristic) analysis. The area under the ROC curve is around 0.9, which indicates excellent accuracy.   CONTACT roschen_csir@rediffmail.com.	bacillus subtilis;bio-informatics;bioinformatics;clinical prediction rule;experiment;gene regulatory network;genetic algorithm;genome;high-level programming language;map;operon;receiver operator characteristics;receiver operating characteristic;rule (guideline);score;scoring functions for docking;wdfy2 wt allele	Elizabeth Jacob;Roschen Sasikumar;K. N. Ramachandran Nair	2005	Bioinformatics	10.1093/bioinformatics/bti156	biology;computer science;bioinformatics;genetics;receiver operating characteristic;statistics	Comp.	1.0768800073082814	-55.98029592721974	119512
7952f2102c34603fd2c72003fad7ebce8ebdf5c6	aligning dynamic networks with dynawave		Motivation Network alignment (NA) aims to find similar (conserved) regions between networks, such as cellular networks of different species. Until recently, existing methods were limited to aligning static networks. However, real-world systems, including cellular functioning, are dynamic. Hence, in our previous work, we introduced the first ever dynamic NA method, DynaMAGNA++, which improved upon the traditional static NA. However, DynaMAGNA++ does not necessarily scale well to larger networks in terms of alignment quality or runtime.   Results To address this, we introduce a new dynamic NA approach, DynaWAVE. We show that DynaWAVE complements DynaMAGNA++: while DynaMAGNA++ is more accurate yet slower than DynaWAVE for smaller networks, DynaWAVE is both more accurate and faster than DynaMAGNA++ for larger networks. We provide a friendly user interface and source code for DynaWAVE.   Availability and implementation https://www.nd.edu/∼cone/DynaWAVE/.   Contact tmilenko@nd.edu.   Supplementary information Supplementary data are available at Bioinformatics online.	alignment;bioinformatics;complement system proteins;geographic information systems;large;small;source code;user interface device component;world-system	Vipin Vijayan;Tijana Milenkovi&#x0107;	2018	Bioinformatics	10.1093/bioinformatics/btx841	data mining;computer science;source code;distributed computing;user interface;cellular network	Comp.	-0.8215456744789618	-58.239062551231925	119894
3e3fb4258278357e2e31b2d50232bfc848d1bec2	high-precision function prediction using conserved interactions	protein protein interaction;functional genomics	The recentavailability of large datasetsof proteinprotein-interactions (PPIs)from variousspeciesoffersnew opportunitiesfor functionalgenomicsandproteomics. We describea methodfor exploiting conserved and connectedsubgraphs (CCSs)in thePPInetworksof multiple speciesfor thepredictionof proteinfunction. Structuralconservationiscombinedwith functionalconservationusingaGeneOntologybasedscoringscheme.Weappliedourmethodto thePPInetworksof fivespecies, i.e., E. coli, D. melanogaster , M. musculus,H. sapiensandS.cerevisiae.We detectedsurprisingly largeCCSsfor groupsof threespeciesbut not beyond. A manualanalysis of thebiologicalcoherenceof exemplarysubgraphstronglysupportsacloserelationship betweenstructuralandfunctionalconservation. Basedon this observation, we devised an algorithmfor function predictionbasedon CCS.Using our method,for instance,we predictnew functionalannotationsfor humanbasedon mouseproteins with a precisionof 70%.	interaction	Samira Jaeger;Ulf Leser	2007			functional genomics;protein function prediction;bioinformatics;protein structure prediction;protein–protein interaction;structural genomics;critical assessment of function annotation;proteomics;biology	Vision	1.0291011671415997	-58.64176693446439	119930
bcc591b8061a0b7025af56bfa74c65d72cf31924	bioclipse-r: integrating management and visualization of life science data with statistical analysis	software;quantitative structure activity relationship;data interpretation statistical;bioinformatics and systems biology;bioinformatik och systembiologi;computer graphics;bioinformatik;biological science disciplines;antineoplastic agents;bioinformatik berakningsbiologi;mutagenesis;systems integration;bioinformatics computational biology;programming languages;bioinformatics	SUMMARY Bioclipse, a graphical workbench for the life sciences, provides functionality for managing and visualizing life science data. We introduce Bioclipse-R, which integrates Bioclipse and the statistical programming language R. The synergy between Bioclipse and R is demonstrated by the construction of a decision support system for anticancer drug screening and mutagenicity prediction, which shows how Bioclipse-R can be used to perform complex tasks from within a single software system.   AVAILABILITY AND IMPLEMENTATION Bioclipse-R is implemented as a set of Java plug-ins for Bioclipse based on the R-package rj. Source code and binary packages are available from https://github.com/bioclipse and http://www.bioclipse.net/bioclipse-r, respectively.   CONTACT martin.eklund@farmbio.uu.se   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	accessibility;bachelor of science in law;bioclipse;bioconductor;bioinformatics;biological science disciplines;boost;cdk;cns disorder;decision support system;ephrin type-b receptor 1, human;essence;fertilization;graphical user interface;imagery;jchempaint;java programming language;jmol;plug (physical object);r language;seizures;software system;source code;statistical model;synergy;usability;user interface device component;workbench	Ola Spjuth;Valentin Georgiev;Lars Carlsson;Jonathan Alvarsson;Arvid Berg;Egon L. Willighagen;Jarl E. S. Wikberg;Martin Eklund	2013		10.1093/bioinformatics/bts681	biology;mutagenesis;computer science;bioinformatics;theoretical computer science;computer graphics;quantitative structure–activity relationship;genetics;system integration	Comp.	-3.063284973082819	-58.370423741733376	120901
9c8ba6af36e3c72d3ac03ac2fccc8a0ba26fd4c7	at excursion: a new approach to predict replication origins in viral genomes by locating at-rich regions	prediction method;genome viral;replication origin;chromosome mapping;statistical significance;sequence analysis dna;computational method;dna replication;public domain;computational biology bioinformatics;double strand;at rich sequence;algorithms;molecular sequence data;herpesviridae;combinatorial libraries;molecular mechanics;base sequence;computer appl in life sciences;genome sequence;microarrays;bioinformatics	Replication origins are considered important sites for understanding the molecular mechanisms involved in DNA replication. Many computational methods have been developed for predicting their locations in archaeal, bacterial and eukaryotic genomes. However, a prediction method designed for a particular kind of genomes might not work well for another. In this paper, we propose the AT excursion method, which is a score-based approach, to quantify local AT abundance in genomic sequences and use the identified high scoring segments for predicting replication origins. This method has the advantages of requiring no preset window size and having rigorous criteria to evaluate statistical significance of high scoring segments. We have evaluated the AT excursion method by checking its predictions against known replication origins in herpesviruses and comparing its performance with an existing base weighted score method (BWS1). Out of 43 known origins, 39 are predicted by either one or the other method and 26 origins are predicted by both. The excursion method identifies six origins not predicted by BWS1, showing that the AT excursion method is a valuable complement to BWS1. We have also applied the AT excursion method to two other families of double stranded DNA viruses, the poxviruses and iridoviruses, of which very few replication origins are documented in the public domain. The prediction results are made available as supplementary materials at [1]. Preliminary investigation shows that the proposed method works well on some larger genomes too. The AT excursion method will be a useful computational tool for identifying replication origins in a variety of genomic sequences.	acoustic radiation force;archive;artificial neural network;checking (action);choi response criteria in the evaluation of gastrointestinal stromal tumors;complement system proteins;dna replication;herpesviridae;high-speed serial interface;hoc (programming language);html link type - copyright;iridovirus;keratoconus posticus circumscriptus;large;machine learning;manuscripts;national origin;neoplasms;neural network simulation;p-value;pml wt allele;paper;peer review;poppy seed (dietary);poxviridae;pubmed central;replication origin;spag9 gene;scientific publication;score;support vector machine;united states national institutes of health;viral genome;window function;citation;funding grant	David S. H. Chew;Ming-Ying Leung;Kwok Pui Choi	2006	BMC Bioinformatics	10.1186/1471-2105-8-163	biology;public domain;whole genome sequencing;dna microarray;molecular mechanics;bioinformatics;statistical significance;dna replication;genetics	Comp.	1.2395626658703178	-55.53220455817629	120994
11cd7c4b13d339820832f9de418d8a44901cd792	pathway analyst automated metabolic pathway prediction		Metabolic pathways are crucial to our understanding of biology. The speed at which new organisms are being sequenced is outstripping our ability to experimentally determine their metabolic pathway information. In recent years several initiatives have been successful in automating the annotations of individual proteins in these organisms, either experimentally or by prediction. However, to leverage the success of metabolic pathways we need to automate their identification in our rapidly growing list of sequenced organisms. We present a prototype system for predicting the catalysts of important reactions and for organizing the predicted catalysts and reactions into previously defined metabolic pathways. We compare a variety of predictors that incorporate sequence similarity (BLAST), hidden Markov models (HMM) and Support Vector Machines (SVM). We found that there is an advantage to using different predictors for different reactions. We validate our prototype on 10 metabolic pathways across 13 organisms for which we obtained a cross-validation precision of 71.5% and recall of 91.5% in predicting the catalyst proteins of all reactions.	algorithm;blast;centralized computing;cross-validation (statistics);experiment;gene regulatory network;hidden markov model;homology (biology);markov chain;online and offline;organizing (structure);prototype;support vector machine;web application	Luca Pireddu;Brett Poulin;Duane Szafron;Paul Lu;David S. Wishart	2005	2005 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology		biology;support vector machine;computer science;bioinformatics;machine learning;data mining;hidden markov model;statistics	Comp.	1.7360158097535858	-58.66733166912477	121037
aef572dbc6dbfcaf6710d47d625703b86e1eb05b	pepsite: prediction of peptide-binding sites from protein surfaces	peptides;exanthema	Complex biological functions emerge through intricate protein-protein interaction networks. An important class of protein-protein interaction corresponds to peptide-mediated interactions, in which a short peptide stretch from one partner interacts with a large protein surface from the other partner. Protein-peptide interactions are typically of low affinity and involved in regulatory mechanisms, dynamically reshaping protein interaction networks. Due to the relatively small interaction surface, modulation of protein-peptide interactions is feasible and highly attractive for therapeutic purposes. Unfortunately, the number of available 3D structures of protein-peptide interfaces is very limited. For typical cases where a protein-peptide structure of interest is not available, the PepSite web server can be used to predict peptide-binding spots from protein surfaces alone. The PepSite method relies on preferred peptide-binding environments calculated from a set of known protein-peptide 3D structures, combined with distance constraints derived from known peptides. We present an updated version of the web server that is orders of magnitude faster than the original implementation, returning results in seconds instead of minutes or hours. The PepSite web server is available at http://pepsite2.russelllab.org.	access network;amino acid sequence;application programming interface;binding sites;bio-informatics;bioinformatics;community-acquired infections;exanthema;experiment;interaction network;interactivity;internet;mandibular right second molar tooth;membrane proteins;microsoft windows;modulation;peptide sequence;pipeline (computing);processor affinity;server (computer);server (computing);staphylococcal protein a;subgroup;web server;webgl;orders - hl7publishingdomain;protein protein interaction;receptor	Leonardo G. Trabuco;Stefano Lise;Evangelia Petsalaki;Robert B. Russell	2012		10.1093/nar/gks398	biology;bioinformatics	Comp.	-0.10984107960843943	-58.84372035745887	121488
3fbb4ab543ea8959557069fdd97087507e763024	rgb: a scriptable genome browser for r		Thanks to its free licensing and the development of initiatives like Bioconductor, R has become an essential part of the bioinformatics toolbox in the past years, and is more and more confronted with genomically located data. While separate solutions are available to manipulate and visualize such data, no R package currently offers the efficiency required for computationally intensive tasks such as interactive genome browsing. The package proposed here fulfills this specific need, providing a multi-level interface suitable for most needs, from a completely interfaced genome browser to low-level classes and methods. Its time and memory efficiency have been challenged in a human dataset, where it outperformed existing solutions by several orders of magnitude.	bioconductor;bioinformatics;class;high- and low-level;interface device component;memory management;numerous;solutions;structured product labeling licensing terminology;orders - hl7publishingdomain	Sylvain Mareschal;Sydney Dubois;Thierry Lecroq;Fabrice Jardin	2014	Bioinformatics	10.1093/bioinformatics/btu185	computer science;bioinformatics;operating system;database;world wide web	Comp.	-2.848602499605826	-58.44535114207679	121579
fe3b59f264a3ce87f7ae7c9fe5c968f697206e8d	phylogenetically enhanced statistical tools for rna structure prediction	prediccion;metodo estadistico;sequence evolution;alignement sequence;phylogeny;structure secondaire;phylogenese;statistical method;secuencia nucleotido;alineacion secuencia;mathematical analysis;nucleotide sequence;sequence nucleotide;rna structure;rna;estructura secundaria;methode statistique;structure moleculaire;secondary structure;filogenesis;mutual information;sequence alignment;phylogenetic relationship;estructura molecular;prediction;multiple alignment;tests of independence;molecular structure	MOTIVATION Methods that predict the structure of molecules by looking for statistical correlation have been quite effective. Unfortunately, these methods often disregard phylogenetic information in the sequences they analyze. Here, we present a number of statistics for RNA molecular-structure prediction. Besides common pair-wise comparisons, we consider a few reasonable statistics for base-triple predictions, and present an elaborate analysis of these methods. All these statistics incorporate phylogenetic relationships of the sequences in the analysis to varying degrees, and the different nature of these tests gives a wide choice of statistical tools for RNA structure prediction.   RESULTS Starting from statistics that incorporate phylogenetic information only as independent sequence evolution models for each position of a multiple alignment, and extending this idea to a joint evolution model of two positions, we enhance the usual purely statistical methods (e.g. methods based on the Mutual Information statistic) with the use of phylogenetic information available in the sequences. In particular, we present a joint model based on the HKY evolution model, and consequently a X(2) test of independence for two positions. A significant part of this work is devoted to some mathematical analysis of these methods. We tested these statistics on regions of 16S and 23S rRNA, and tRNA.	mathematics;multiple sequence alignment;mutual information;phylogenetics;rna;statistic (data)	Viatcheslav R. Akmaev;Scott T. Kelley;Gary D. Stormo	2000	Bioinformatics	10.1093/bioinformatics/16.6.501	biology;nucleic acid structure;rna;prediction;molecule;multiple sequence alignment;nucleic acid sequence;bioinformatics;sequence alignment;mutual information;genetics;statistics;protein secondary structure;phylogenetics	Comp.	-3.4280606427943257	-54.584320044502434	121847
2044493ffec74af228a22d38f62a5bc37f9397c7	pnpprobs: a better multiple sequence alignment tool by better handling of guide trees	computational biology bioinformatics;algorithms;computer appl in life sciences;article;microarrays;bioinformatics	This paper describes a new MSA tool called PnpProbs, which constructs better multiple sequence alignments by better handling of guide trees. It classifies sequences into two types: normally related and distantly related. For normally related sequences, it uses an adaptive approach to construct the guide tree needed for progressive alignment; it first estimates the input’s discrepancy by computing the standard deviation of their percent identities, and based on this estimate, it chooses the better method to construct the guide tree. For distantly related sequences, PnpProbs abandons the guide tree and uses instead some non-progressive alignment method to generate the alignment. To evaluate PnpProbs, we have compared it with thirteen other popular MSA tools, and PnpProbs has the best alignment scores in all but one test. We have also used it for phylogenetic analysis, and found that the phylogenetic trees constructed from PnpProbs’ alignments are closest to the model trees. By combining the strength of the progressive and non-progressive alignment methods, we have developed an MSA tool called PnpProbs. We have compared PnpProbs with thirteen other popular MSA tools and our results showed that our tool usually constructed the best alignments.	computation (action);discrepancy function;estimated;handling (psychology);mesna;multiple sequence alignment;phylogenetic tree;phylogenetics;sense of identity (observable entity);standard deviation;tpo wt allele;thirteen;trees (plant)	Yongtao Ye;Tak Wah Lam;Hing-Fung Ting	2016		10.1186/s12859-016-1121-7	biology;dna microarray;computer science;bioinformatics;theoretical computer science;data mining;alignment-free sequence analysis	Comp.	0.9939658019488593	-54.215030504423034	121885
b5f2bba60f6f5e96fa65e36eb499fe19727dec48	identification of deleterious synonymous variants in human genomes		Motivation: The prioritization and identification of disease-causing mutations is one of the most significant challenges in medical genomics. Currently available methods address this problem for non-synonymous single nucleotide variants (SNVs) and variation in promoters/ enhancers; however, recent research has implicated synonymous (silent) exonic mutations in a number of disorders. Results: We have curated 33 such variants from literature and developed the Silent Variant Analyzer (SilVA), a machine-learning approach to separate these from among a large set of rare polymorphisms. We evaluate SilVA’s performance on in silico ‘infection’ experiments, in which we implant known disease-causing mutations into a human genome, and show that for 15 of 33 disorders, we rank the implanted mutation among the top five most deleterious ones. Furthermore, we apply the SilVA method to two additional datasets: synonymous variants associated with Meckel syndrome, and a collection of silent variants clinically observed and stratified by a molecular diagnostics laboratory, and show that SilVA is able to accurately predict the harmfulness of silent variants in these datasets. Availability: SilVA is open source and is freely available from the pro	genome	Orion J. Buske;AshokKumar Manickaraj;Seema Mital;Peter N. Ray;Michael Brudno	2015	Bioinformatics	10.1093/bioinformatics/btu765	biology;bioinformatics;genetics	NLP	2.4772950097214226	-56.44549878507701	121894
cab9067630664fe657e833ac814993250f1fa4eb	a novel estimation algorithm of evolutionary distance based on the word frequency variation of sequence segment		Based on the model of evolution, studies on the genome sequence of organisms use alignment method to calculate evolutionary distances between organisms. However, there is clear disadvantages in that it requires high complexity of the whole algorithm and necessity of perfect sequence assembly in the conventional studies. Therefore, in recent years, alignment-free methods for analyzing the relationship between sequences by using fragmentary information such as word frequency or common substring of each sequence without progressing alignment for the entire sequence are being researched. Nevertheless, the alignment-free method has not been widely used yet. This is because the alignment free methods so far have presented algorithms which are solely experimental rather than mathematical. Therefore, we propose a mathematical estimation method of the evolutionary distance based on the model of evolution and prove that it shows superior performance compared with various alignment-free methods.	algorithm;models of dna evolution;sequence assembly;substring;word lists by frequency	Yong-Joon Song;Dong-Ho Cho	2018	2018 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)	10.1109/BHI.2018.8333425	genomics;sequence assembly;substring;algorithm;word lists by frequency;computer science	DB	-0.13599871299897556	-53.14709486984301	122067
1dc7723a933804171eff4e1260bdcd998106a0af	mobidb-lite: fast and highly specific consensus prediction of intrinsic disorder in proteins		MOTIVATION Intrinsic disorder (ID) is established as an important feature of protein sequences. Its use in proteome annotation is however hampered by the availability of many methods with similar performance at the single residue level, which have mostly not been optimized to predict long ID regions of size comparable to domains. Here, we have focused on providing a single consensus-based prediction, MobiDB-lite, optimized for highly specific (i.e. few false positive) predictions of long disorder. The method uses eight different predictors to derive a consensus which is then filtered for spurious short predictions. Consensus prediction is shown to outperform the single methods when annotating long ID regions. MobiDB-lite can be useful in large-scale annotation scenarios and has indeed already been integrated in the MobiDB, DisProt and InterPro databases.   AVAILABILITY MobiDB-lite is available as part of the MobiDB database from URL: http://mobidb.bio.unipd.it/ An executable can be downloaded from URL: http://protein.bio.unipd.it/mobidblite/ CONTACT: silvio.tosatto@unipd.it.	amino acid sequence;annotation;auditory processing disorder;benzalkonium chloride 1 mg/ml topical foam [lite 'n foamy];database;disprot;executable;interpro;mood disorders;peptide sequence;uniform resource locator	Marco Necci;Damiano Piovesan;Zsuzsanna Dosztányi;Silvio C. E. Tosatto	2017	Bioinformatics	10.1093/bioinformatics/btx015	instrumental and intrinsic value;computer science;bioinformatics	Comp.	1.813188978608816	-57.16012462642485	122295
07824cbcc22a533878d2edcccfabab04618de35e	scoring hidden markov models	software;sequence comparison;metodo estadistico;secuencia aminoacido;alignement sequence;proteine;sequence aminoacide;modelo markov;aminoacid sequence;logiciel;computerized processing;analisis estructural;tratamiento informatico;hidden markov model;estudio comparativo;statistical method;alineacion secuencia;statistical model;etude comparative;markov model;proteins;methode statistique;comparative study;null model;logicial;proteina;sequence alignment;geometric mean;modele markov;analyse structurale;structural analysis;ucsc;traitement informatique	MOTIVATION Statistical sequence comparison techniques, such as hidden Markov models and generalized profiles, calculate the probability that a sequence was generated by a given model. Log-odds scoring is a means of evaluating this probability by comparing it to a null hypothesis, usually a simpler statistical model intended to represent the universe of sequences as a whole, rather than the group of interest. Such scoring leads to two immediate questions: what should the null model be, and what threshold of log-odds score should be deemed a match to the model.   RESULTS This paper analyses these two issues experimentally. Within the context of the Sequence Alignment and Modeling software suite (SAM), we consider a variety of null models and suitable thresholds. Additionally, we consider HMMer's log-odds scoring and SAM's original Z-scoring method. Among the null model choices, a simple looping null model that emits characters according to the geometric mean of the character probabilities in the columns modeled by the hidden Markov model (HMM) performs well or best across all four discrimination experiments.	column (database);experiment;hidden markov model;markov chain;null value;null model;personality character;probability;s-adenosylmethionine;score;sequence alignment;software suite;statistical model	Christian Barrett;Richard Hughey;Kevin Karplus	1997	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/13.2.191	statistical model;geometric mean;null model;computer science;comparative research;hidden semi-markov model;sequence alignment;structural analysis;markov model;algorithm;hidden markov model;statistics	Comp.	-3.452903289348545	-54.714128172830016	123039
556498c37a28ec2882d4a7c9b19ef981c71cd3f6	an approach to phylogenomic analysis of bacterial pathogens	pathogenic bacteria;genomics;genetic variability;comparative analysis;dissimilarity measure;biomedical engineering;bioinformatics genomics microorganisms phylogeny reliability pathogens conferences;diseases;biological techniques;bacterial genome;environmental change;microorganisms biological techniques biomedical engineering diseases genomics;microorganisms;genome sequence;phylogenomic refinement bacterial pathogen phylogenomic analysis microbial genome sequencing human pathogens sequencing technologies pathogenic bacteria genetic variability whole genome sequencing bacterial adaptation virulence mechanisms bacterial genome datasets phylogenetic classification ncbi proteobacteria;phylogenetic diversity	From the beginning of the microbial genome sequencing era, researchers have shown a commendable commitment to phylogenetic diversity. The completion of one genome from each prokaryotic division or phylum is still a frequently articulated community goal. However, largely because of the interest in human pathogens and advances in sequencing technologies, there are also now a number of very closely related genomes whose organization and gene content can be directly compared. Studying genetic variability of pathogenic bacteria using whole-genome sequencing provides a way to understanding the mechanism of bacterial adaptation to rapid environmental changes and can be a source of useful information on virulence mechanisms. The bacterial genome datasets available in public archives represent a large collection of genome at different levels of sequence quality and assembly. A fast and reliable method of phylogenetic classification based on genome sequences provides a necessary foundation for a more detailed comparative analysis. NCBI has developed an approach of grouping bacterial organisms into phylogenetic clades using a genome dissimilarity measure based on the comparison of universally conserved markers. Special adjustments have been made to compensate for data inaccuracy and incompleteness. Tests performed on complete and draft genomes from phylum Proteobacteria demonstrated that the proposed robust genomic distance allows stable and reliable species-level clustering and can be used for forming phylogenetic clades. Since the tradeoff for the increased robustness of the method is its limited sensitivity at a very fine level, a phylogenomic refinement could be done within each constructed clade when file-level phylogenetic resolution of close genomes is necessary.	archive;assembly language;clade;cluster analysis;phylogenetic nomenclature;phylogenetics;qualitative comparative analysis;refinement (computing);spatial variability;whole genome sequencing	Leonid Zaslavsky;Vyacheslav Chetvernin;Dmitry Dernovoy;Boris Fedorov;William Klimke;Alexandre Souvorov;Igor Tolstoy;Tatiana A. Tatusova;David J. Lipman	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW)	10.1109/BIBMW.2011.6112529	genetic variability;qualitative comparative analysis;biology;phylogenetic diversity;genomics;whole genome sequencing;environmental change;bioinformatics;microorganism;genetics;bacterial genome size	Comp.	0.8008780154674975	-54.367212755076686	123044
77245ef93a00d7b4bad5d1febc2c822c9758444a	quality assessment of peptide tandem mass spectra	databases;ion trap;biology computing;peptides;search engine;fisher linear discriminant analysis;mass spectrometer;search engines;database management systems;mass spectra;ions;trained classifier;spectrum;sequest;spectral filtering;feature vector;quality assessment peptides search engines mass spectroscopy throughput proteomics vectors linear discriminant analysis nonlinear filters testing;distance measurement;tandem mass spectrometry;peptide tandem mass spectra quality;quality assessment;proteomic studies;proteins;vectors;computer experiment;molecular biophysics;amino acids;spectrochemical analysis biochemistry biological techniques biology computing database management systems mass spectra mass spectroscopy molecular biophysics proteins search engines;spectral dataset;mass spectroscopy;biological techniques;high throughput;ion trap mass spectrometers quality assessment peptide tandem mass spectra quality tandem mass spectrometry proteomic studies search engines sequest feature vector fisher linear discriminant analysis trained classifier spectral filtering spectral dataset;biochemistry;spectrochemical analysis;ion trap mass spectrometers	"""Tandem mass spectrometry has emerged as a cornerstone of high throughput proteomic studies owing in part to various high throughput search engines which are used to interpret these tandem mass spectra. However, majority of experimental tandem mass spectra cannot be interpreted by any existing search engines or other methods. There are many reasons why this happens. However, one of the most important reasons is that majority of experimental spectra are of too poor quality to be interpretable. It wastes time to interpret these """"uninterpretable"""" spectra by search engines or other methods. Therefore, if a powerful filter that could eliminate those spectra with poor quality is applied before any interpretations, it could significantly save the interpretation time of a whole set of spectra using search engines such as SEQUEST. This paper proposes a novel method to assess the quality of tandem mass spectra, and then use this method to develop a powerful filter that can eliminate majority of poor quality spectra while losing very minority of high quality spectra. First, a number of features are proposed to describe the quality of tandem mass spectra. The proposed method maps each tandem spectrum into a feature vector. Then Fisher linear discriminant analysis (FLDA) is employed to construct the classifier (the filter) which discriminates the high quality spectra from the poor quality ones. The proposed method has been tested on two tandem mass spectra datasets acquired by ion trap mass spectrometers. Computational experiments illustrate that the proposed method outperforms existing ones. The proposed method is generic, and is expected to be applicable to assessing the quality of spectra acquired by instruments other than ion trap mass spectrometers."""	rca spectra 70	Fang-Xiang Wu;Pierre Gagné;Arnaud Droit;Guy G. Poirier	2006		10.1109/IMSCCS.2006.109	chromatography;chemistry;bioinformatics;analytical chemistry	NLP	0.9302808363889506	-53.23922245086695	123140
deee0d2545a1f5dd78e92a4149f214b5fe574a82	interest: intron-exon retention estimator	alternative splicing;bioconductor;expression analysis;intron retention;rna;rna-seq;u12-type introns;u2-type introns	In-depth study of the intron retention levels of transcripts provide insights on the mechanisms regulating pre-mRNA splicing efficiency. Additionally, detailed analysis of retained introns can link these introns to post-transcriptional regulation or identify aberrant splicing events in human diseases. We present IntEREst, Intron–Exon Retention Estimator, an R package that supports rigorous analysis of non-annotated intron retention events (in addition to the ones annotated by RefSeq or similar databases), and support intra-sample in addition to inter-sample comparisons. It accepts binary sequence alignment/map (.bam) files as input and determines genome-wide estimates of intron retention or exon-exon junction levels. Moreover, it includes functions for comparing subsets of user-defined introns (e.g. U12-type vs U2-type) and its plotting functions allow visualization of the distribution of the retention levels of the introns. Statistical methods are adapted from the DESeq2, edgeR and DEXSeq R packages to extract the significantly more or less retained introns. Analyses can be performed either sequentially (on single core) or in parallel (on multiple cores). We used IntEREst to investigate the U12- and U2-type intron retention in human and plant RNAseq dataset with defects in the U12-dependent spliceosome due to mutations in the ZRSR2 component of this spliceosome. Additionally, we compared the retained introns discovered by IntEREst with that of other methods and studies. IntEREst is an R package for Intron retention and exon-exon junction levels analysis of RNA-seq data. Both the human and plant analyses show that the U12-type introns are retained at higher level compared to the U2-type introns already in the control samples, but the retention is exacerbated in patient or plant samples carrying a mutated ZRSR2 gene. Intron retention events caused by ZRSR2 mutation that we discovered using IntEREst (DESeq2 based function) show considerable overlap with the retained introns discovered by other methods (e.g. IRFinder and edgeR based function of IntEREst). Our results indicate that increase in both the number of biological replicates and the depth of sequencing library promote the discovery of retained introns, but the effect of library size gradually decreases with more than 35 million reads mapped to the introns.		Ali Oghabian;Dario Greco;Mikko J. Frilander	2018		10.1186/s12859-018-2122-5	computational biology;refseq;spliceosome;rna splicing;intron;genetics;exon;rna;rna-seq;alternative splicing;biology	ML	2.0074934106627134	-55.887246202687635	123514
f6cdbcaab09e5d9a9c281b56c5528717321073ff	invited: a protein domain-centric approach to translational bioinformatics	prediction of domain domain interactions;patient diagnosis;tumor suppressor;complex disease;prediction of domain domain interactions protein domain translational bioinformatics coevolution;evolution biological;protein domains;tumours;statistical method;coevolution;genetics;proteins;statistical analysis;molecular biophysics;translational bioinformatics;large scale sequencing protein domain centric approach translational bioinformatics molecular disruption complex diseases personalized disease therapy gene analysis gene centric approach domain mapping dmdm protein domain database human coding mutations gene centric visualization tools statistical method domain significance scores disease mutation clusters ds scores human data oncogenes tumor suppressors mendelian diseases domain domain interactions molecular underpinning computational tools protein interactions evolutionary distances evolutionary constraints evolutionary trees binding sites mirrortree method coevolutionary signal domain sequence compensatory mutations protein coevolution disease diagnosis;diseases;patient treatment;protein domain;proteins diseases bioinformatics humans context data visualization marine vehicles;bioinformatics;tumours bioinformatics diseases evolution biological genetics molecular biophysics patient diagnosis patient treatment proteins statistical analysis	Identifying the functional context for key molecular disruptions in complex diseases is a major goal of modern medicine that will lead to improved preventive clinical approaches, earlier diagnosis and more effective personalized disease therapies. Most available resources for visualization and analysis of disease mutations centered on gene analysis and do not leverage information about similarity on the functional context of the mutation. In addition, gene-centric approaches are confounded because genes may share some functional sub-units, or protein domains, but not others. We have built a resource for domain mapping of disease mutations, DMDM, a protein domain database in which each disease mutation can be displayed by its protein domain location. DMDM provides a unique domain-level view where human coding mutations are mapped to protein domains by highlighting molecular relationships among mutations from different diseases that might not have been discovered with traditional gene-centric visualization tools. We have also developed a statistical method, the domain significance scores (DSScores), to assess the significance of disease mutation clusters on protein domains. When we applied the DS-Scores to human data and identified domain hotspots in oncogenes, tumor suppressors, and genes associated with Mendelian diseases. Since most proteins need to interact to perform their function, the identification domains of clinical relevance needs to be complemented by the identification of domain-domain interactions to provide a better understanding about the molecular underpinning of disease. Computational tools to predict domain-domain interactions provide a detailed molecular view of the protein interactions and complements expensive and laborious experimental techniques to identify such interactions. The evolutionary distances of interacting proteins often display a higher level of similarity than those of non-interacting proteins. This finding indicates that interacting proteins are subject to common evolutionary constraints and constitute the basis of a method to predict protein interactions known as mirrortree. It has been difficult, however, to identify the direct cause of the observed similarities between evolutionary trees. One possible explanation is the existence of compensatory mutations between partners binding sites to maintain proper binding. This explanation, however, has been recently challenged. It has been suggested that the signal of correlated evolution uncovered by the mirrortree method is unrelated to any correlated evolution between binding sites. We have addressed this controversial debate in the field by studying the contribution of binding sites to the correlation between evolutionary trees of interacting domains. We showed that binding neighborhoods of interacting proteins have, on average, higher co-evolutionary signal compared to the regions outside binding sites; although when the binding neighborhood was removed, the remaining domain sequence still contained some co-evolutionary signal. These results provide evidence of the role of compensatory mutations in protein co-evolution and contribute to our understanding of co-evolution of interacting proteins. Our domain-centric methods have the potential to be incorporated into translational bioinformatics tools for functional characterization of rare and common human variants from large-scale sequencing studies.	bioinformatics;computation;design of experiments;interaction;personalization;phylogenetic tree;relevance	Maricel G. Kann	2012	2012 IEEE 2nd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)	10.1109/ICCABS.2012.6182622	biology;bioinformatics;protein domain;genetics;molecular biophysics	Comp.	2.614416952749385	-58.97938987626488	123525
399b09fb25d42b7b461c823a3e191aa7ae62357c	database challenges for genome information in the post sequencing phase	complex analysis;genome data;genomic data;post sequencing phase;genome sequencing project;new information;information management system;genome information;computer scientist;resulting data set;scientists complete record;effective information management system	Genome sequencing projects are making available to scientists complete records of the genetic make-up of organisms. The resulting data sets, along with the results of experiments that seek systematically to nd new information on the functions of genes, will present numerous opportunities and challenges to biologists. However, the complexity and variety of both the data and the analyses required over such data sets also pose signi cant challenges to computer scientists charged with providing e ective information management systems for use with genome data. This paper presents models for the sorts of information that are being produced on genomes and genome-wide experiments, and outlines a project developing an information management system aimed at supporting analyses over genomic data. This information management system replicates data from other sources, with a view to providing an integrated environment for performing complex analyses.	comparison of command shells;computer scientist;experiment;information management system (ims);whole genome sequencing	Fouzia Moussouni;Norman W. Paton;Andy Hayes;Steve Oliver;Carole A. Goble;Andy Brass	1999		10.1007/3-540-48309-8_50	personal genomics;bioinformatics	Comp.	-3.148174560791562	-58.22272200054324	123837
0f558805a2505df39d4ffa5dad3cb4789e5b40a4	neat: a framework for building fully automated ngs pipelines and analyses	genomics;chipseq;journal article;computational biology bioinformatics;ngs pipelines;high throughput sequencing;rnaseq;algorithms;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	The analysis of next generation sequencing (NGS) has become a standard task for many laboratories in the life sciences. Though there exists several tools to support users in the manipulation of such datasets on various levels, few are built on the basis of vertical integration. Here, we present the NExt generation Analysis Toolbox (NEAT) that allows non-expert users including wet-lab scientists to comprehensively build, run and analyze NGS data through double-clickable executables without the need of any programming experience. In comparison to many publicly available tools including Galaxy, NEAT provides three main advantages: (1) Through the development of double-clickable executables, NEAT is efficient (completes within <24 hours), easy to implement and intuitive; (2) Storage space, maximum number of job submissions, wall time and cluster-specific parameters can be customized as NEAT is run on the institution’s cluster; (3) NEAT allows users to visualize and summarize NGS data rapidly and efficiently using various built-in exploratory data analysis tools including metagenomic and differentially expressed gene analysis. To simplify the control of the workflow, NEAT projects are built around a unique and centralized file containing sample names, replicates, conditions, antibodies, alignment-, filtering- and peak calling parameters as well as cluster-specific paths and settings. Moreover, the small-sized files produced by NEAT allow users to easily manipulate, consolidate and share datasets from different users and institutions. NEAT provides biologists and bioinformaticians with a robust, efficient and comprehensive tool for the analysis of massive NGS datasets. Frameworks such as NEAT not only allow novice users to overcome the increasing number of technical hurdles due to the complexity of manipulating large datasets, but provide more advance users with tools that ensure high reproducibility standards in the NGS era. NEAT is publically available at https://github.com/pschorderet/NEAT .	biological science disciplines;canonical account;centralized computing;clickable;communications satellite;customize;executable;laboratory;massively-parallel sequencing;metagenomics;neat chipset;name;neuroevolution of augmenting topologies;peak calling;pipeline (computing);standards characteristics	Patrick Schorderet	2016		10.1186/s12859-016-0902-3	biology;dna sequencing;genomics;dna microarray;rna-seq;computer science;bioinformatics;data science;data mining;chip-sequencing	PL	-2.3949740517608373	-58.08738274349835	123905
e0fe05f68055fe2adc0a4ed1af881445264927e5	calign: aligning sequences with restricted affine gap penalties	dna;software;alignement sequence;logiciel;computerized processing;tratamiento informatico;methode;secuencia nucleotido;alineacion secuencia;nucleotide sequence;sequence nucleotide;algorithme;algorithm;logicial;sequence alignment;metodo;traitement informatique;method;algoritmo	MOTIVATION Given a genomic DNA sequence, it is still an open problem to determine its coding regions, i.e. the region consisting of exons and introns. The comparison of cDNA and genomic DNA helps the understanding of coding regions. For such an application, it might be adequate to use the restricted affine gap penalties which penalize long gaps with a constant penalty.   RESULTS Several techniques developed for solving the approximate string-matching problem are employed to yield efficient algorithms for computing the optimal alignment with restricted affine gap penalties. In particular, efficient algorithms can be derived based on the suffix automaton with failure transitions and on the diagonalwise monotonicity of the cost tables. We have implemented the above methods in C on Sun workstations running SunOS Unix. Preliminary experiments show that these approaches are very promising for aligning a cDNA sequence with a genomic DNA sequence.   AVAILABILITY Calign is available free of charge by anonymous ftp at: iubio.bio. indiana.edu, directory: molbio/align, files: calign.driver.c calign. c. Another URL reference for the files is http://iubio.bio.indiana.edu/soft/molbio/align/+ ++calign.c.	align (company);approximate string matching;approximation algorithm;computation (action);dna, complementary;data table;directory (computing);exons;experiment;gap penalty;introns;published directory;sequence alignment;suffix automaton;sunos;uniform resource locator;unix;workstation	Kun-Mao Chao	1999	Bioinformatics	10.1093/bioinformatics/15.4.298	biology;method;gap penalty;nucleic acid sequence;computer science;bioinformatics;sequence alignment;genetics;dna;algorithm	Comp.	-4.127363337519599	-55.80030551266917	124498
9dde7a12292c9d1cc185ca96c707197e557d6f90	mapper: an intelligent restriction mapping tool	dna;software;carte restriction;logiciel;computerized processing;tratamiento informatico;automatisation;intelligence artificielle;automatizacion;algorithme;algorithm;restriction map;artificial intelligence;logicial;inteligencia artificial;mapa restriccion;traitement informatique;algoritmo;automation	MOTIVATION To determine the most powerful artificial intelligence techniques for automated restriction mapping, and use them to create a powerful multiple-enzyme restriction mapping tool.   RESULTS The most effective search engine utilized model-driven exhaustive search and a form of binary logic pruning based on Pratt's separation theory. Additional experimentation led to the development of an input preprocessing module which significantly speeds up searches, and an output post-processing module which enables users to analyze large solution sets and reduce their apparent complexity.   AVAILABILITY An executable version of the resultant tool, Mapper, can be downloaded from our Web site (http://www.ai.eecs.uic.edu) by selecting the 'Software' option.   CONTACT nelson@eecs.uic.edu (http://www.ai.eecs.uic.edu/ñelson).	artificial intelligence;brute-force search;executable;knuth–morris–pratt algorithm;mapper;model-driven integration;preprocessor;restriction mapping;resultant;speed (motion);video post-processing;web search engine;search a word	J. A. Inglehart;P. C. Nelson;Yan Zou	1998	Bioinformatics	10.1093/bioinformatics/14.2.101	biology;computer science;bioinformatics;artificial intelligence;restriction map;automation;genetics;dna;algorithm	AI	-4.339680205440305	-55.82625281273859	124638
126fdeb9b627ffe0f2c0741cf13e8b24ff331bb8	inferring piecewise ancestral history from haploid sequences	polymorphism;information content;population size;genetics;genetic variation	There has been considerable recent interest in the use of haplotype structure to aid in the design and analysis of case-control association studies searching for genetic predictors of human disease. The use of haplotype structure is based on the premise that genetic variations that are physically close on the genome will often be predictive of one another due to their frequent descent intact through recent evolution. Understanding these correlations between sites should make it possible to minimize the amount of redundant information gathered through assays or examined in association tests, improving the power and reducing the cost of the studies. In this work, we evaluate the potential value of haplotype structure in this context by applying it to two key subproblems: inferring hidden polymorphic sites in partial haploid sequences and choosing subsets of variants that optimally capture the information content of the full set of sequences. We develop methods for these approaches based on a prior method we developed for predicting piece-wise shared ancestry of haploid sequences. We apply these methods to a case study of two genetic regions with very different levels of sequence diversity. We conclude that haplotype correlations do have considerable potential for these problems, but that the degree to which they are useful will be strongly dependent on the population sizes available and the specifics of the genetic regions examined.	self-information	Russell Schwartz;Andrew G. Clark;Sorin Istrail	2002		10.1007/978-3-540-24719-7_5	biology;polymorphism;population size;self-information;bioinformatics;genetic variation;genetics	Comp.	2.6400111490829246	-58.98720614496741	124733
dbce70bdbd048cb28f8536ebe22fa93e02e856a8	kdetrees: non-parametric estimation of phylogenetic tree distributions		MOTIVATION Although the majority of gene histories found in a clade of organisms are expected to be generated by a common process (e.g. the coalescent process), it is well known that numerous other coexisting processes (e.g. horizontal gene transfers, gene duplication and subsequent neofunctionalization) will cause some genes to exhibit a history distinct from those of the majority of genes. Such 'outlying' gene trees are considered to be biologically interesting, and identifying these genes has become an important problem in phylogenetics.   RESULTS We propose and implement kdetrees, a non-parametric method for estimating distributions of phylogenetic trees, with the goal of identifying trees that are significantly different from the rest of the trees in the sample. Our method compares favorably with a similar recently published method, featuring an improvement of one polynomial order of computational complexity (to quadratic in the number of trees analyzed), with simulation studies suggesting only a small penalty to classification accuracy. Application of kdetrees to a set of Apicomplexa genes identified several unreliable sequence alignments that had escaped previous detection, as well as a gene independently reported as a possible case of horizontal gene transfer. We also analyze a set of Epichloë genes, fungi symbiotic with grasses, successfully identifying a contrived instance of paralogy.   AVAILABILITY AND IMPLEMENTATION Our method for estimating tree distributions and identifying outlying trees is implemented as the R package kdetrees and is available for download from CRAN.	anatomy, regional;clade;computational complexity theory;download;estimated;fungi;gene duplication abnormality;gene transfer;homology (biology);image histogram;phylogenetic tree;phylogenetics;poaceae;polynomial;r language;scientific publication;sequence alignment;simulation;trees (plant)	Grady Weyenberg;Peter Huggins;Christopher L. Schardl;Daniel K. Howe;Ruriko Yoshida	2014	Bioinformatics	10.1093/bioinformatics/btu258	biology;bioinformatics;phylogenetic network;ecology;genetics	Comp.	1.9258316087628553	-53.34769490502885	124790
f9692de1e3f63d4bf2bea0a8dfbf1b28b07f1b43	rust-bio: a fast and safe bioinformatics library	software;algorithms;sequence analysis;humans;computational biology;programming languages	SUMMARY We present Rust-Bio, the first general purpose bioinformatics library for the innovative Rust programming language. Rust-Bio leverages the unique combination of speed, memory safety and high-level syntax offered by Rust to provide a fast and safe set of bioinformatics algorithms and data structures with a focus on sequence analysis.   AVAILABILITY AND IMPLEMENTATION Rust-Bio is available open source under the MIT license at https://rust-bio.github.io.   CONTACT koester@jimmy.harvard.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithm;bioinformatics;british informatics olympiad;data structure;high- and low-level;memory safety;open-source software;programming language;rust;sequence analysis	Johannes Köster	2016	Bioinformatics	10.1093/bioinformatics/btv573	biology;computer science;bioinformatics;theoretical computer science;sequence analysis	Comp.	-2.2312861714169565	-56.32882591584358	124900
adb5d491f70da9f0001dcdf984fd4efc7bd17a76	fast alignment of fragmentation trees	metabolomics;mass spectrometry;dynamic program;exact algorithm;algorithms;databases factual;high throughput;computational biology;integer linear program	MOTIVATION Mass spectrometry allows sensitive, automated and high-throughput analysis of small molecules such as metabolites. One major bottleneck in metabolomics is the identification of 'unknown' small molecules not in any database. Recently, fragmentation tree alignments have been introduced for the automated comparison of the fragmentation patterns of small molecules. Fragmentation pattern similarities are strongly correlated with the chemical similarity of the molecules, and allow us to cluster compounds based solely on their fragmentation patterns.   RESULTS Aligning fragmentation trees is computationally hard. Nevertheless, we present three exact algorithms for the problem: a dynamic programming (DP) algorithm, a sparse variant of the DP, and an Integer Linear Program (ILP). Evaluation of our methods on three different datasets showed that thousands of alignments can be computed in a matter of minutes using DP, even for 'challenging' instances. Running times of the sparse DP were an order of magnitude better than for the classical DP. The ILP was clearly outperformed by both DP approaches. We also found that for both DP algorithms, computing the 1% slowest alignments required as much time as computing the 99% fastest.	alignment;chemical similarity;computation (action);davis–putnam algorithm;dynamic programming;fastest;fragmentation (computing);high-throughput computing;linear iga bullous dermatosis;linear programming;mass spectrometry;metabolite;metabolomics;ninety nine;small molecule;sparse matrix;strongly correlated material;throughput;trees (plant);xiap gene	Franziska Hufsky;Kai Dührkop;Florian Rasche;Markus Chimani;Sebastian Böcker	2012		10.1093/bioinformatics/bts207	high-throughput screening;mass spectrometry;computer science;bioinformatics;theoretical computer science;metabolomics;algorithm	Comp.	-0.8575573034910935	-52.82557783773382	125441
5010c05b675b1af7d38c2cf70f4fd5411262c3f2	a survey of error-correction methods for next-generation sequencing		UNLABELLED Error Correction is important for most next-generation sequencing applications because highly accurate sequenced reads will likely lead to higher quality results. Many techniques for error correction of sequencing data from next-gen platforms have been developed in the recent years. However, compared with the fast development of sequencing technologies, there is a lack of standardized evaluation procedure for different error-correction methods, making it difficult to assess their relative merits and demerits. In this article, we provide a comprehensive review of many error-correction methods, and establish a common set of benchmark data and evaluation criteria to provide a comparative assessment. We present experimental results on quality, run-time, memory usage and scalability of several error-correction methods. Apart from providing explicit recommendations useful to practitioners, the review serves to identify the current state of the art and promising directions for future research.   AVAILABILITY All error-correction programs used in this article are downloaded from hosting websites. The evaluation tool kit is publicly available at: http://aluru-sun.ece.iastate.edu/doku.php?id=ecr.	benchmark (computing);biopolymer sequencing;bit error rate;bittorrent;communications satellite;computation;decade (log scale);error detection and correction;evaluation;gen1 wt allele;genetic polymorphism;haplotypes;ions;list of toolkits;manuscripts;massively-parallel sequencing;mast/stem cell growth factor receptor kit, human;memory footprint;metagenomics;reading (activity);run time (program lifecycle phase);sampling - surgical action;scalability;silo (dataset);throughput;whole genome sequencing;algorithm;cellular targeting;error correction	Xiao Yang;Sriram P. Chockalingam;Srinivas Aluru	2013	Briefings in bioinformatics	10.1093/bib/bbs015	bioinformatics;data mining;management science	Comp.	0.16432882440910737	-55.704721426694036	125690
dc6abfa9ac45c89606c3150b86ee3101d253f93a	accurate annotation of metagenomic data without species-level references	databases;genomics;taxonomy;probabilistic logic;encoding;microorganisms;bioinformatics	Taxonomic annotation is a critical first step for analysis of metagenomic data. Despite a lot of tools being developed, the accuracy is still not satisfactory, in particular, when a close species-level reference does not exist in the database. In this paper, we propose a novel annotation tool, MetaAnnotator, to annotate metagenomic reads, which outperforms all existing tools significantly when only genus-level references exist in the database. From our experiments, MetaAnnotator can assign 87.5% reads correctly (67.5% reads are assigned to the exact genus) with only 8.5% reads wrongly assigned. The best existing tool (MetaCluster-TA) can only achieve 73.4% correct read assignment (with only 50.9% reads assigned to the exact genus and 22.6% reads wrongly assigned). The speed of MetaAnnotator is also the second faster (1 hour for 20 million reads). The core concepts behind MetaAnnotator includes: (i) we only consider exact k-mers in coding regions of the references as they should be more significant and accurate; (ii) to assign reads to taxonomy nodes, we construct genome and taxonomy specific probabilistic models from the reference database; and (iii) using the BWT data structure to speed up the k-mer matching process.	bibliographic database;burrows–wheeler transform;data structure;experiment;genus (mathematics);k-mer;mer;metagenomics	Haobin Yao;Tak Wah Lam;Hing-Fung Ting;Siu-Ming Yiu;Yadong Wang;Bo Liu	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822493	biology;genomics;computer science;bioinformatics;data mining;probabilistic logic;microorganism;world wide web;taxonomy;encoding	DB	-0.5444838447160364	-53.818425873127616	126019
7a055d89c73a730be126c284d9594d70d575f285	miscore: a new scoring function for characterizing dna regulatory motifs in promoter sequences	simulation and modeling;systems biology;physiological cellular and medical topics;computational biology bioinformatics;nucleotide motifs;promoter regions genetic;algorithms;computational biology;bioinformatics	Computational approaches for finding DNA regulatory motifs in promoter sequences are useful to biologists in terms of reducing the experimental costs and speeding up the discovery process of de novo binding sites. It is important for rule-based or clustering-based motif searching schemes to effectively and efficiently evaluate the similarity between a k-mer (a k-length subsequence) and a motif model, without assuming the independence of nucleotides in motif models or without employing computationally expensive Markov chain models to estimate the background probabilities of k-mers. Also, it is interesting and beneficial to use a priori knowledge in developing advanced searching tools. This paper presents a new scoring function, termed as MISCORE, for functional motif characterization and evaluation. Our MISCORE is free from: (i) any assumption on model dependency; and (ii) the use of Markov chain model for background modeling. It integrates the compositional complexity of motif instances into the function. Performance evaluations with comparison to the well-known Maximum a Posteriori (MAP) score and Information Content (IC) have shown that MISCORE has promising capabilities to separate and recognize functional DNA motifs and its instances from non-functional ones. MISCORE is a fast computational tool for candidate motif characterization, evaluation and selection. It enables to embed priori known motif models for computing motif-to-motif similarity, which is more advantageous than IC and MAP score. In addition to these merits mentioned above, MISCORE can automatically filter out some repetitive k-mers from a motif model due to the introduction of the compositional complexity in the function. Consequently, the merits of our proposed MISCORE in terms of both motif signal modeling power and computational efficiency will make it more applicable in the development of computational motif discovery tools.	analysis of algorithms;binding sites;cluster analysis;computation (action);de novo transcriptome assembly;embedding;evaluation;k-mer;logic programming;markov chain;mer;nucleotides;probability;score;scoring functions for docking;sequence motif;whole earth 'lectronic link;emotional dependency;statistical cluster	Dianhui Wang;Sarwar Tapan	2012		10.1186/1752-0509-6-S2-S4	biology;computer science;bioinformatics;data mining;genetics;systems biology	Comp.	2.709260540643647	-55.01799208332305	126445
aab212c618a9983ef30f6c7cc50111d3cf717519	using a bioinformatics approach to generate accurate exploit-based signatures for polymorphic worms	one byte invariant;exploit based signature generation;simplified regular expression;polymorphism;sequence alignment;multiple sequence alignment;distance restriction;regular expression;polymorphic worms	In this paper, we propose Simplified Regular Expression (SRE) signature, which uses multiple sequence alignment techniques, drawn from bioinformatics, in a novel approach to generating more accurate exploit-based signatures. We also provide formal definitions of what is ‘‘a more specific’’ and what is ‘‘the most specific’’ signature for a polymorphic worm and show that the most specific exploit-based signature generation is NP-hard. The approach involves three steps: multiple sequence alignment to reward consecutive substring extractions, noise elimination to remove noise effects, and signature transformation to make the SRE signature compatible with current IDSs. Experiments on a range of polymorphic worms and real-world polymorphic shellcodes show that our bioinformatics approach is noise-tolerant and as that because it extracts more polymorphic worm characters, like one-byte invariants and distance restrictions between invariant bytes, the signatures it generates are more accurate and precise than those generated by some other exploit-based signature generation schemes. a 2009 Elsevier Ltd. All rights reserved.	antivirus software;bioinformatics;byte;multiple sequence alignment;np-hardness;regular expression;substring;type signature	Yong Tang;Bin Xiao;Xicheng Lu	2009	Computers & Security	10.1016/j.cose.2009.06.003	polymorphism;multiple sequence alignment;computer science;bioinformatics;theoretical computer science;sequence alignment;regular expression;algorithm	Security	-4.377038686785403	-52.54698766495098	126466
47e32e48af8bdf14e8d8ba03d5e690229d6b46d2	affygcqc: a web-based interface to detect outlying genechips with extreme studentized deviate tests.	web tool;affymetrix genechip;outlier detection;quality control	Affymetrix GeneChip oligonucleotide arrays are dedicated to analyzing gene expression differences across distinct experimental conditions. Data production for such arrays is an elaborate process with many potential sources of variability unrelated to biologically relevant gene expression variations. Therefore, rigorous data quality assessment is fundamental throughout the process for downstream biologically meaningful analyses. We have developed a program named AffyGCQC, which is the acronym for a bioinformatics tool designed to perform Affymetrix GeneChip Quality Control. This program implements a graphical representation of QC metrics recommended by Affymetrix for GeneChip oligonucleotide array technology. Most importantly, it performs extreme studentized deviate statistical tests for the set of arrays being compared in a given experiment, thus providing an objective measure for outlier detection. AffyGCQC has been designed as an easy-to-use Web-based interface (online supplementary information: http://www.transcriptome.ens.fr/AffyGCQC/; contact: affygcqc@biologie.ens.fr).	acronyms;affymetrix genechip operating software;anomaly detection;bioinformatics;data quality;downstream (software development);electronic supplementary materials;gene expression;interface device component;name;spatial variability;statistical test	José Osorio y Fortéa;Éric Prina;Thierry Lang;Geneviève Milon;Clarisse Davory;Jean-Yves Coppée;Béatrice Regnault	2008	Journal of bioinformatics and computational biology	10.1142/S0219720008003400	quality control;anomaly detection;affymetrix genechip operating software;computer science;bioinformatics;data science;machine learning;data mining	Comp.	-2.024186432725928	-57.09343252738385	126884
12d7fa271d47d400ce9d342a7fb2636bbaa99ef2	graspx: efficient homolog-search of short peptide metagenome database through simultaneous alignment and assembly	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Metagenomics is a cultivation-independent approach that enables the study of the genomic composition of microbes present in an environment. Metagenomic samples are routinely sequenced using next-generation sequencing technologies that generate short nucleotide reads. Proteins identified from these reads are mostly of partial length. On the other hand, de novo assembly of a large metagenomic dataset is computationally demanding and the assembled contigs are often fragmented, resulting in the identification of protein sequences that are also of partial length and incomplete. Annotation of an incomplete protein sequence often proceeds by identifying its homologs in a database of reference sequences. Identifying the homologs of incomplete sequences is a challenge and can result in substandard annotation of proteins from metagenomic datasets. To address this problem, we recently developed a homology detection algorithm named GRASP (Guided Reference-based Assembly of Short Peptides) that identifies the homologs of a given reference protein sequence in a database of short peptide metagenomic sequences. GRASP was developed to implement a simultaneous alignment and assembly algorithm for annotation of short peptides identified on metagenomic reads. The program achieves significantly improved recall rate at the cost of computational efficiency. In this article, we adopted three techniques to speed up the original version of GRASP, including the pre-construction of extension links, local assembly of individual seeds, and the implementation of query-level parallelism. The resulting new program, GRASPx, achieves >30X speedup compared to its predecessor GRASP. At the same time, we show that the performance of GRASPx is consistent with that of GRASP, and that both of them significantly outperform other popular homology-search tools including the BLAST and FASTA suites. GRASPx was also applied to a human saliva metagenome dataset and shows superior performance for both recall and precision rates. In this article we present GRASPx, a fast and accurate homology-search program implementing a simultaneous alignment and assembly framework. GRASPx can be used for more comprehensive and accurate annotation of short peptides. GRASPx is freely available at http://graspx.sourceforge.net/ .	alignment;amino acid sequence;annotation;blast;biopolymer sequencing;de novo transcriptome assembly;entity name part qualifier - adopted;ephrin type-b receptor 1, human;fasta;grasp gene;homologous gene;homology (biology);massively-parallel sequencing;metagenome;metagenomics;nucleotides;parallel computing;partial;peptide sequence;plant seeds;precision and recall;question (inquiry);reading (activity);search engine;sensitivity and specificity;silo (dataset);sourceforge;speedup;algorithm	Cuncong Zhong;Youngik Yang;Shibu Yooseph	2016		10.1186/s12859-016-1119-1	biology;dna microarray;computer science;bioinformatics;data science;data mining	Comp.	0.10676842884577367	-55.031917294851745	127129
67e3a3c979b960c9d8bfec9387845eae58848f54	coco: a web application to display, store and curate chip-on-chip data integrated with diverse types of gene expression data	embl;expression profile;data integrity;gene expression data;chip;transcription factor;open source	MOTIVATION CoCo, ChIP-on-Chip online, is an open-source web application that supports the annotation and curation of regulatory regions and associated target genes discovered in ChIP-on-chip experiments. CoCo integrates ChIP-on-chip results with diverse types of gene expression data (expression profiling, in situ hybridization) and displays them within a genomic context. Regulatory relationships between the transcription factor-bound regions and putative target genes can be stored and expanded throughout different sessions.   AVAILABILITY http://furlonglab.embl.de/methods/tools/coco.	annotation;chip-on-chip;digital curation;experiment;gene expression profiling;in situ hybridization;molecular profiling;nucleic acid hybridization;open-source software;regulatory sequences, nucleic acid;transcription factor;transcription (software);web application	Charles Girardot;Oleg Sklyar;Sophie Grosz;Wolfgang Huber;Eileen E. M. Furlong	2007	Bioinformatics	10.1093/bioinformatics/btl641	chip;biology;computer science;bioinformatics;data integrity;database;world wide web;genetics;transcription factor	Comp.	-1.3402145903275147	-59.06475851838837	127514
2f3637ae88e3b12f336c180e1406f7538067bae6	learning from the data: mining of large high-throughput screening databases	high throughput screening;data mining	High-throughput screening (HTS) campaigns in pharmaceutical companies have accumulated a large amount of data for several million compounds over a couple of hundred assays. Despite the general awareness that rich information is hidden inside the vast amount of data, little has been reported for a systematic data mining method that can reliably extract relevant knowledge of interest for chemists and biologists. We developed a data mining approach based on an algorithm called ontology-based pattern identification (OPI) and applied it to our in-house HTS database. We identified nearly 1500 scaffold families with statistically significant structure-HTS activity profile relationships. Among them, dozens of scaffolds were characterized as leading to artifactual results stemming from the screening technology employed, such as assay format and/or readout. Four types of compound scaffolds can be characterized based on this data mining effort: tumor cytotoxic, general toxic, potential reporter gene assay artifact, and target family specific. The OPI-based data mining approach can reliably identify compounds that are not only structurally similar but also share statistically significant biological activity profiles. Statistical tests such as Kruskal-Wallis test and analysis of variance (ANOVA) can then be applied to the discovered scaffolds for effective assignment of relevant biological information. The scaffolds identified by our HTS data mining efforts are an invaluable resource for designing SAR-robust diversity libraries, generating in silico biological annotations of compounds on a scaffold basis, and providing novel target family specific scaffolds for focused compound library design.		S. Frank Yan;Frederick J. King;Yun He;Jeremy S. Caldwell;Yingyao Zhou	2006	Journal of chemical information and modeling	10.1021/ci060102u	combinatorial chemistry;reporter gene;chemistry;bioinformatics;database;data mining;scaffold;high-throughput screening	ML	0.4340626878446447	-58.790757501140156	128057
bb30c3d0671b73c38ba0337e742fcdae4570ac2d	an efficient normalized maximum likelihood algorithm for dna sequence compression	normalized maximum likelihood model;efficient algorithm;first order;normalized maximum likelihood;human genome;compression ratio;approximate matching;is success;approximate sequence matching;dna sequence;dna compression	This article presents an efficient algorithm for DNA sequence compression, which achieves the best compression ratios reported over a test set commonly used for evaluating DNA compression programs. The algorithm introduces many refinements to a compression method that combines: (1) encoding by a simple normalized maximum likelihood (NML) model for discrete regression, through reference to preceding approximate matching blocks, (2) encoding by a first order context coding and (3) representing strings in clear, to make efficient use of the redundancy sources in DNA data, under fast execution times. One of the main algorithmic features is the constraint on the matching blocks to include reasonably long contiguous matches, which not only reduces significantly the search time, but also can be used to modify the NML model to exploit the constraint for getting smaller code lengths. The algorithm handles the changing statistics of DNA data in an adaptive way and by predictively encoding the matching pointers it is successful in compressing long approximate matches. Apart from comparison with previous DNA encoding methods, we present compression results for the recently published human genome data.	approximation algorithm;dna microarray;database;encoder;internet;lossless compression;poisson regression;regular expression;requirement;test set	Gergely Korodi;Ioan Tabus	2005	ACM Trans. Inf. Syst.	10.1145/1055709.1055711	data compression;dna sequencing;human genome;computer science;theoretical computer science;machine learning;compression ratio;pattern recognition;first-order logic;lossless compression;incremental encoding	ML	-2.2234769001396826	-53.415075376069204	128330
0135ebaafab3543021cb520cca4b95983e0d0acf	homology-driven assembly of non-redundant protein sequence sets (nomess) for mass spectrometry	mathematics;biotechnology applied microbiology;biochemistry molecular biology;mathematical computational biology;computer science	UNLABELLED To enable mass spectrometry (MS)-based proteomic studies with poorly characterized organisms, we developed a computational workflow for the homology-driven assembly of a non-redundant reference sequence dataset. In the automated pipeline, translated DNA sequences (e.g. ESTs, RNA deep-sequencing data) are aligned to those of a closely related and fully sequenced organism. Representative sequences are derived from each cluster and joined, resulting in a non-redundant reference set representing the maximal available amino acid sequence information for each protein. We here applied NOmESS to assemble a reference database for the widely used model organism Xenopus laevis and demonstrate its use in proteomic applications.   AVAILABILITY AND IMPLEMENTATION NOmESS is written in C#. The source code as well as the executables can be downloaded from http://www.biochem.mpg.de/cox Execution of NOmESS requires BLASTp and cd-hit in addition.   CONTACT cox@biochem.mpg.de   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	alignment;amino acid sequence;amino acids;blast;bibliographic database;bioinformatics;executable;expressed sequence tags;homologous gene;homology (biology);homology modeling;mass spectrometry;maximal set;proteomics;rna;representative sequences;silo (dataset);source code;thrombocytopenia	Tikira Temu;Matthias Mann;Markus Räschle;Juergen Cox	2016		10.1093/bioinformatics/btv756	biology;computer science;bioinformatics;data mining;genetics	Comp.	-0.47663892320252227	-58.26005729937776	128342
44137d0e73d2f0e2a771c98103968d6521be8f7b	poretools: a toolkit for analyzing nanopore sequence data	software;sequence analysis dna;nanopores	MOTIVATION Nanopore sequencing may be the next disruptive technology in genomics, owing to its ability to detect single DNA molecules without prior amplification, lack of reliance on expensive optical components, and the ability to sequence long fragments. The MinION™ from Oxford Nanopore Technologies (ONT) is the first nanopore sequencer to be commercialized and is now available to early-access users. The MinION™ is a USB-connected, portable nanopore sequencer that permits real-time analysis of streaming event data. Currently, the research community lacks a standardized toolkit for the analysis of nanopore datasets.   RESULTS We introduce poretools, a flexible toolkit for exploring datasets generated by nanopore sequencing devices from MinION™ for the purposes of quality control and downstream analysis. Poretools operates directly on the native FAST5 (an application of the HDF5 standard) file format produced by ONT and provides a wealth of format conversion utilities and data exploration and visualization tools.   AVAILABILITY AND IMPLEMENTATION Poretools is an open-source software and is written in Python as both a suite of command line utilities and a Python application programming interface. Source code is freely available in Github at https://www.github.com/arq5x/poretools.	application programming interface;biopolymer sequencing;command-line interface;downstream (software development);early access;genomics;hierarchical data format;imagery;interface device component;license;microsequencer;network interface device;open-source software;optical devices;python;real-time transcription;source code;usb	Nicholas J. Loman;Aaron R. Quinlan	2014		10.1093/bioinformatics/btu555	nanopore;computer science;bioinformatics;data mining;world wide web	Comp.	-2.4001082732428403	-57.60330752534511	128421
ba5dc4bc9299d136d88e3abbf0d9451412067123	identifying property based sequence motifs in protein families and superfamilies: application to dnase-1 related endonucleases	functional annotation;score function;protein family;metal ion;sequence similarity;amino acid;protein sequence;enzyme;local structure;chemical properties;dna repair;expert knowledge;sequence motif;active site;structural similarity	MOTIVATION Identification of short conserved sequence motifs common to a protein family or superfamily can be more useful than overall sequence similarity in suggesting the function of novel gene products. Locating motifs still requires expert knowledge, as automated methods using stringent criteria may not differentiate subtle similarities from statistical noise.   RESULTS We have developed a novel automatic method, based on patterns of conservation of 237 physical-chemical properties of amino acids in aligned protein sequences, to find related motifs in proteins with little or no overall sequence similarity. As an application, our web-server MASIA identified 12 property-based motifs in the apurinic/apyrimidinic endonuclease (APE) family of DNA-repair enzymes of the DNase-I superfamily. Searching with these motifs located distantly related representatives of the DNase-I superfamily, such as Inositol 5'-polyphosphate phosphatases in the ASTRAL40 database, using a Bayesian scoring function. Other proteins containing APE motifs had no overall sequence or structural similarity. However, all were phosphatases and/or had a metal ion binding active site. Thus our automated method can identify discrete elements in distantly related proteins that define local structure and aspects of function. We anticipate that our method will complement existing ones to functionally annotate novel protein sequences from genomic projects.   AVAILABILITY MASIA WEB site: http://www.scsb.utmb.edu/masia/masia.html   SUPPLEMENTARY INFORMATION The dendrogram of 42 APE sequences used to derive motifs is available on http://www.scsb.utmb.edu/comp_biol.html/DNA_repair/publication.html	amino acid sequence;amino acids;annotation;antivirus software;complement (complexity);complement system proteins;conserved sequence;cynthia dwork;dendrogram;deoxyribonuclease i;deoxyribonucleases;homology (biology);inositol;ions;manuscripts;metal ion binding;pcp theorem;prosite;peptide sequence;phencyclidine;phosphoric monoester hydrolases;preparation;protein family;superfamily;sequence alignment;sequence database;sequence motif;server (computing);staphylococcal protein a;structural similarity;web server;world wide web;chemical properties;endonuclease;hemoglobin f kotobuki	Venkatarajan S. Mathura;Catherine H. Schein;Werner Braun	2003	Bioinformatics	10.1093/bioinformatics/btg164	consensus sequence;biology;enzyme;chemical property;amino acid;dna repair;bioinformatics;active site;structural similarity;protein sequencing;score;metal;protein family;genetics;sequence motif	Comp.	1.3222246969352596	-58.344530328852855	129533
9e315e79a8e62679ccaaad3f20cc1604bfee5e55	aligning lc peaks by converting gradient retention times to retention index of peptides in proteomic experiments	peptides;stationary phase;bioinformatique;retention time;peptido;tandem mass spectrometry;liquid chromatography;retention index;machine learning;organic solvent;proteomique;peptide;bioinformatica;proteomics;proteomica;bioinformatics	MOTIVATION Liquid chromatography-tandem mass spectrometry (LC-MS/MS) is a powerful tool in proteomics studies, but when peptide retention information is used for identification purposes, it remains challenging to compare multiple LC-MS/MS runs or to match observed and predicted retention times, because small changes of LC conditions unavoidably lead to variability in retention times. In addition, non-contiguous retention data obtained with different LC-MS instruments or in different laboratories must be aligned to confirm and utilize rapidly accumulating published proteomics data.   RESULTS We have developed a new alignment method for peptide retention times based on linear solvent strength (LSS) theory. We found that log k(0) (logarithm of retention factor for a given organic solvent) in the LSS theory can be utilized as a 'universal' retention index of peptides (RIP) that is independent of LC gradients, and depends solely on the constituents of the mobile phase and the stationary phases. We introduced a machine learning-based scheme to optimize the conversion function of gradient retention times (t(g)) to log k(0). Using the optimized function, t(g) values obtained with different LC-MS systems can be directly compared with each other on the RIP scale. In an examination of Arabidopsis proteomic data, the vast majority of retention time variability was removed, and five datasets obtained with various LC-MS systems were successfully aligned on the RIP scale.		Kosaku Shinoda;Masaru Tomita;Yasushi Ishihama	2008	Bioinformatics	10.1093/bioinformatics/btn240	peptide bond;tandem mass spectrometry;bioinformatics;proteomics;kovats retention index	ML	-0.4659873772238003	-55.470775653598075	130454
fff1e960ab8471252f2ba4a188332ee04d51722a	uniref: comprehensive and non-redundant uniprot reference clusters	functional annotation;sequence similarity;protein sequence;sequence space;biological database;similarity search	MOTIVATION Redundant protein sequences in biological databases hinder sequence similarity searches and make interpretation of search results difficult. Clustering of protein sequence space based on sequence similarity helps organize all sequences into manageable datasets and reduces sampling bias and overrepresentation of sequences.   RESULTS The UniRef (UniProt Reference Clusters) provide clustered sets of sequences from the UniProt Knowledgebase (UniProtKB) and selected UniProt Archive records to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. Currently covering >4 million source sequences, the UniRef100 database combines identical sequences and subfragments from any source organism into a single UniRef entry. UniRef90 and UniRef50 are built by clustering UniRef100 sequences at the 90 or 50% sequence identity levels. UniRef100, UniRef90 and UniRef50 yield a database size reduction of approximately 10, 40 and 70%, respectively, from the source sequence set. The reduced redundancy increases the speed of similarity searches and improves detection of distant relationships. UniRef entries contain summary cluster and membership information, including the sequence of a representative protein, member count and common taxonomy of the cluster, the accession numbers of all the merged entries and links to rich functional annotation in UniProtKB to facilitate biological discovery. UniRef has already been applied to broad research areas ranging from genome annotation to proteomics data analysis.   AVAILABILITY UniRef is updated biweekly and is available for online search and retrieval at http://www.uniprot.org, as well as for download at ftp://ftp.uniprot.org/pub/databases/uniprot/uniref.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	accession number (identifier);accession number (bioinformatics);amino acid sequence;annotation;archive;bioinformatics;biological database;cluster analysis;consortium;databases;download;knowledge bases;knowledge base;linkage (software);lithium;merge;online search;peptide sequence;phylogenetics;pierre robin syndrome;protein family;proteomics;published database;sampling (signal processing);sampling - surgical action;sequence alignment;taxonomy;thrombocytopenia;uniprot;bacterium u01;genetic linkage;statistical cluster	Baris E. Suzek;Hongzhan Huang;Peter B. McGarvey;Raja Mazumder;Cathy H. Wu	2007	Bioinformatics	10.1093/bioinformatics/btm098	biology;biological database;bioinformatics;protein sequencing;sequence database;sequence space	Comp.	-0.23884613231795523	-56.5779834496703	130832
450f35d57f2c6d1a06976ce53851e3f7e5f6b407	chip-array: combinatory analysis of chip-seq/chip and microarray gene expression data to discover direct/indirect targets of a transcription factor	software;animals;mice;high throughput nucleotide sequencing;transcription factors;binding sites;chromatin immunoprecipitation;regulatory elements transcriptional;humans;article;gene expression profiling;oligonucleotide array sequence analysis	Chromatin immunoprecipitation (ChIP) coupled with high-throughput techniques (ChIP-X), such as next generation sequencing (ChIP-Seq) and microarray (ChIP-chip), has been successfully used to map active transcription factor binding sites (TFBS) of a transcription factor (TF). The targeted genes can be activated or suppressed by the TF, or are unresponsive to the TF. Microarray technology has been used to measure the actual expression changes of thousands of genes under the perturbation of a TF, but is unable to determine if the affected genes are direct or indirect targets of the TF. Furthermore, both ChIP-X and microarray methods produce a large number of false positives. Combining microarray expression profiling and ChIP-X data allows more effective TFBS analysis for studying the function of a TF. However, current web servers only provide tools to analyze either ChIP-X or expression data, but not both. Here, we present ChIP-Array, a web server that integrates ChIP-X and expression data from human, mouse, yeast, fruit fly and Arabidopsis. This server will assist biologists to detect direct and indirect target genes regulated by a TF of interest and to aid in the functional characterization of the TF. ChIP-Array is available at http://jjwanglab.hku.hk/ChIP-Array, with free access to academic users.	aquaporin 1;binding sites;chip-on-chip;dna microarray chip;dna binding site;drosophila <fruit fly, genus>;gene expression profiling;high-throughput computing;massively-parallel sequencing;molecular profiling;sequence number;server (computing);throughput;transcription (software);web server;chromatin immunoprecipitation;transcription factor binding	Jing Qin;Mulin Jun Li;Panwen Wang;Michael Q. Zhang;Junwen Wang	2011		10.1093/nar/gkr332	biology;molecular biology;chromatin immunoprecipitation;rip-chip;bioinformatics;binding site;gene expression profiling;genetics;transcription factor	Comp.	0.5808361588400965	-58.2852620976197	130837
6682622997271509dda30cdcc1f8e9928cd1215d	jmol smiles and jmol smarts: specifications and applications	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	"""BACKGROUND SMILES and SMARTS are two well-defined structure matching languages that have gained wide use in cheminformatics. Jmol is a widely used open-source molecular visualization and analysis tool written in Java and implemented in both Java and JavaScript. Over the past 10 years, from 2007 to 2016, work on Jmol has included the development of dialects of SMILES and SMARTS that incorporate novel aspects that allow new and powerful applications.   RESULTS The specifications of """"Jmol SMILES"""" and """"Jmol SMARTS"""" are described. The dialects most closely resemble OpenSMILES and OpenSMARTS. Jmol SMILES is a superset of OpenSMILES, allowing a freer format, including whitespace and comments, the addition of """"processing directives"""" that modify the meaning of certain aspects of SMILES processing such as aromaticity and stereochemistry, a more extensive treatment of stereochemistry, and several minor additions. Jmol SMARTS similarly adds these same modifications to OpenSMARTS, but also adds a number of additional """"primitives"""" and elements of syntax tuned to matching 3D molecular structures and selecting their atoms. The result is an expansion of the capabilities of SMILES and SMARTS primarily for use in 3D molecular analysis, allowing a broader range of matching involving any combination of 3D molecular structures, SMILES strings, and SMARTS patterns. While developed specifically for Jmol, these dialects of SMILES and SMARTS are independent of the Jmol application itself.   CONCLUSIONS Jmol SMILES and Jmol SMARTS add value to standard SMILES and SMARTS. Together they have proven exceptionally capable in extracting valuable information from 3D structural models, as demonstrated in Jmol. Capabilities in Jmol enabled by Jmol SMILES and Jmol SMARTS include efficient MMFF94 atom typing, conformational identification, SMILES comparisons without canonicalization, identification of stereochemical relationships, quantitative comparison of 3D structures from different sources (including differences in Kekulization), conformational flexible fitting, and atom mapping used to synchronize interactive displays of 2D structures, 3D structures, and spectral correlations, where data are being drawn from multiple sources."""	atom;cheminformatics;gain;imagery;java programming language;javascript;jmol;matching;merck molecular force field;molecular analysis;molecular structure;open-source software;programming languages;simplified molecular input line entry specification;simplified molecular-input line-entry system;smiles arbitrary target specification;stereochemistry (discipline)	Robert M. Hanson	2016		10.1186/s13321-016-0160-4	computational science;computer science;theoretical computer science	Visualization	-3.226776036263823	-57.62112256514628	132767
4a7ace9e2e759201635effbdd2d0111e13464cd3	advances in dna storage		With decreasing costs for DNA synthesis and sequencing, ultra-dense DNA storage is an emerging, viable technology. The original proof of concept [1]–[3] has yielded several experiments of larger scale demonstrating archival storage in DNA molecules [4]–[7]. In particular, a recent collaboration by Harvard and Technicolor announced the storage of 22 MB of data in synthetic DNA [4]. Primarily, existing storage systems utilize high-fidelity synthesizers. For synthesizers which incur non-negligible insertions and deletions, a large fraction of the oligonucleotide segments produced have unequal, variable lengths. This talk overviews methods to correct for synchronization errors in variable-length segments using synchronization codes (e.g., [8], [9]).	archive;code;dhrystone;experiment	Naveen Goela;Jean-Chrysostome Bolot	2017	2017 Information Theory and Applications Workshop (ITA)	10.1109/ITA.2017.8023453	theoretical computer science;oligonucleotide;dna synthesis;discrete mathematics;proof of concept;dna;synthetic dna;computer science;synchronization;bioinformatics	Theory	-1.136591412124822	-54.518606646803406	132899
09335c9ddbc2d31215a776900da470352705682b	dario: a ncrna detection and analysis tool for next-generation sequencing experiments	software;high throughput nucleotide sequencing;non coding rna;large data sets;datasets;web service;internet;rna;micro rna;rna untranslated;genome;massively parallel genome sequencing;molecule;user computer interface;sequence analysis rna;high throughput;next generation sequencing;quality control;microrna;small rna	Small non-coding RNAs (ncRNAs) such as microRNAs, snoRNAs and tRNAs are a diverse collection of molecules with several important biological functions. Current methods for high-throughput sequencing for the first time offer the opportunity to investigate the entire ncRNAome in an essentially unbiased way. However, there is a substantial need for methods that allow a convenient analysis of these overwhelmingly large data sets. Here, we present DARIO, a free web service that allows to study short read data from small RNA-seq experiments. It provides a wide range of analysis features, including quality control, read normalization, ncRNA quantification and prediction of putative ncRNA candidates. The DARIO web site can be accessed at http://dario.bioinf.uni-leipzig.de/.	biopolymer sequencing;convenient vector space;experiment;high-throughput computing;massively-parallel sequencing;quantitation;rna;rna, untranslated;sequence number;throughput;web site;web service	Mario Fasold;David Langenberger;Hans Binder;Peter F. Stadler;Steve Hoffmann	2011		10.1093/nar/gkr357	biology;molecular biology;bioinformatics;genetics;microrna	Comp.	-0.6508708824414152	-57.91078251349253	133162
f4e7ac8c73ed1e381156178ba10a0de810fa94df	practical software for big genomics data	dna;biology computing;genomics;statistical analysis biology computing cancer cloud computing dna genetics genomics molecular biophysics molecular configurations;cancer;molecular configurations;genetics;software computers software algorithms genomics bioinformatics computer science educational institutions;statistical analysis;rna sequencing dataset preprocessing practical software big genomics data second generation dna sequencers high resolution window biology genetics human disease per instrument sequencing throughput computer speed computer science cloud enabled scalable software pipelines crossbow myrna cancer gene detection statistical models differential expression recount database;molecular biophysics;cloud computing	Second-generation DNA sequencers provide an inexpensive and high-resolution window on diverse aspects of biology, genetics, and human disease. In recent years, improvements in per-instrument sequencing throughput have far outpaced improvements in computer speed. This necessitates a computer-science counterattack on two fronts: (1) faster algorithms that make better use of a fixed amount of compute power, and (2) scalable algorithms that make the best possible use of large collections of computers. Here I will discuss past work on both these fronts, concentrating on the cloud-enabled, scalable software pipelines Crossbow and Myrna. Crossbow is a cloud-enabled tool for aligning short second-generation sequence reads and calling SNP variants. Myrna is a cloud-enabled tool for aligning second-generation sequence reads from two groups (e.g. cancer and normal) and detecting which genes are differentially expressed between the groups. Myrna is an example of how scalable software tools can be used to derive new scientific results (in this case, about the usefulness of certain statistical models for differential expression) from large, already-published datasets. This is further exemplified by ReCount, a database of pre-processed RNA sequencing datasets from 18 different published studies comprising 475 samples and over 8 billion reads.	algorithm;computer science;image resolution;pipeline (computing);snp annotation;scalability;second-generation programming language;sensor;statistical model;throughput	Ben Langmead	2013	2013 IEEE 3rd International Conference on Computational Advances in Bio and medical Sciences (ICCABS)	10.1109/ICCABS.2013.6629241	computational biology;biology;genomics;cloud computing;computer science;bioinformatics;theoretical computer science;genetics;dna;cancer;molecular biophysics	Comp.	-1.632705114228174	-53.998470590827274	133179
bd8cd2c708e884fa0cdf3eb6d2a676dab4fd4a07	diana mirpath v.2.0: investigating the combinatorial effect of micrornas in pathways	software;computer graphics;databases genetic;gene expression;cluster analysis;internet;gene expression regulation;algorithms;humans;micrornas;polymorphism single nucleotide	MicroRNAs (miRNAs) are key regulators of diverse biological processes and their functional analysis has been deemed central in many research pipelines. The new version of DIANA-miRPath web server was redesigned from the ground-up. The user of DNA Intelligent Analysis (DIANA) DIANA-miRPath v2.0 can now utilize miRNA targets predicted with high accuracy based on DIANA-microT-CDS and/or experimentally verified targets from TarBase v6; combine results with merging and meta-analysis algorithms; perform hierarchical clustering of miRNAs and pathways based on their interaction levels; as well as elaborate sophisticated visualizations, such as dendrograms or miRNA versus pathway heat maps, from an intuitive and easy to use web interface. New modules enable DIANA-miRPath server to provide information regarding pathogenic single nucleotide polymorphisms (SNPs) in miRNA target sites (SNPs module) or to annotate all the predicted and experimentally validated miRNA targets in a selected molecular pathway (Reverse Search module). DIANA-miRPath v2.0 is an efficient and yet easy to use tool that can be incorporated successfully into miRNA-related analysis pipelines. It provides for the first time a series of highly specific tools for miRNA-targeted pathway analysis via a web interface and can be accessed at http://www.microrna.gr/miRPathv2.	algorithm;biochemical pathway;cluster analysis;diana (intermediate language);dendrogram;experiment;gene regulatory network;genetic polymorphism;heat map;hierarchical clustering;interface device component;micrornas;nucleotides;pathway analysis;pipeline (computing);server (computer);server (computing);single nucleotide polymorphism;single-chain antibodies;user interface;web server;statistical cluster	Ioannis S. Vlachos;Nikos Kostoulas;Thanasis Vergoulis;Georgios Georgakilas;Martin Reczko;Manolis Maragkakis;Maria D. Paraskevopoulou;Kostantinos Prionidis;Theodore Dalamagas;Artemis G. Hatzigeorgiou	2012		10.1093/nar/gks494	biology;the internet;regulation of gene expression;gene expression;bioinformatics;cluster analysis;computer graphics;genetics;microrna	Comp.	-1.1645002174817118	-58.86331161355663	133641
4b345cc25daf2900fe2b9f1633f61e3517138caf	a statistical method for correlating trna sequence with amino acid specificity	computers;software;investigation method;metodo estadistico;mathematics;methode etude;computerized processing;tratamiento informatico;amino acid;statistical method;rna transfert;transfer rna;traduction;metodo estudio;aminoacido;translation;methode statistique;aminoacid;amino acids;arn transferencia;traduccion;specificity;specificite;especifidad;base sequence;traitement informatique;aminoacide;rna transfer;t rna;rna transfer amino acyl	A statistical method for finding the nucleotide positions in tRNA sequences that correlate with amino acid specificity has been developed. The procedure involves finding the subset of nucleotide positions and groups of positions where the marginal density of one amino acid tRNA class does not overlap that of any other amino acid class. The procedure is an application of a statistical method known as the Expectation Maximization algorithm.	amino acids;expectation–maximization algorithm;marginal model;nucleotides;pseudo amino acid composition;sensitivity and specificity;subgroup	T. Atilgan;H. B. Nicholas;W. H. McClain	1986	Nucleic acids research	10.1093/nar/14.1.375	biology;biochemistry;transfer rna;amino acid;genetics	Comp.	-3.139957369571192	-54.86353859750883	133709
1ae85229dd386b44843401e7fff9aec6f3d523b9	psiblast_pairwisestatsig: reordering psi-blast hits using pairwise statistical significance	coulure plante;plant blast;algorithme psi blast;statistical significance;algorithme blast;corrimiento planta	We present an add-on to BLAST and PSI-BLAST programs to reorder their hits using pairwise statistical significance. Using position-specific substitution matrices to estimate pairwise statistical significance has been recently shown to give promising results in terms of retrieval accuracy, which motivates its use to refine PSI-BLAST results, since PSI-BLAST also constructs a position-specific substitution matrix for the query sequence during the search. The obvious advantage of the approach is more accurate estimates of statistical significance because of pairwise statistical significance, along with the advantage of BLAST/PSI-BLAST in terms of speed.	add-ons for firefox;blast;estimated;p-value;question (inquiry);substitution matrix	Ankit Agrawal;Xiaoqiu Huang	2009	Bioinformatics	10.1093/bioinformatics/btp089	econometrics;computer science;bioinformatics;statistical significance;statistics	HPC	-2.2374254177219766	-53.82702431834562	133911
49aa49b32aebf1e1457da869f958536d13ef03c0	fast protein structure alignment using gaussian overlap scoring of backbone peptide fragment similarity		MOTIVATION Aligning and comparing protein structures is important for understanding their evolutionary and functional relationships. With the rapid growth of protein structure databases in recent years, the need to align, superpose and compare protein structures rapidly and accurately has never been greater. Many structural alignment algorithms have been described in the past 20 years. However, achieving an algorithm that is both accurate and fast remains a considerable challenge.   RESULTS We have developed a novel protein structure alignment algorithm called 'Kpax', which exploits the highly predictable covalent geometry of C(α) atoms to define multiple local coordinate frames in which backbone peptide fragments may be oriented and compared using sensitive Gaussian overlap scoring functions. A global alignment and hence a structural superposition may then be found rapidly using dynamic programming with secondary structure-specific gap penalties. When superposing pairs of structures, Kpax tends to give tighter secondary structure overlays than several popular structure alignment algorithms. When searching the CATH database, Kpax is faster and more accurate than the very efficient Yakusa algorithm, and it gives almost the same high level of fold recognition as TM-Align while being more than 100 times faster.	algorithm;align (company);alignment;cath;databases;dynamic programming;frame (physical object);greater than;high-level programming language;internet backbone;normal statistical distribution;peptide fragments;protein structure database;protein structure prediction;protein, organized by structure;score;scoring functions for docking;superposition principle;threading (protein sequence);vertebral column	David W. Ritchie;Anisah W. Ghoorah;Lazaros Mavridis;Vishwesh Venkatraman	2012	Bioinformatics	10.1093/bioinformatics/bts618	structural alignment;computer science;bioinformatics;theoretical computer science	Comp.	-2.4469913183923597	-52.30041020166681	135186
f096c64190de0e7bef2fa94608823afade3b3187	essamem: finding maximal exact matches using enhanced sparse suffix arrays	biology and life sciences	We have developed essaMEM, a tool for finding maximal exact matches that can be used in genome comparison and read mapping. essaMEM enhances an existing sparse suffix array implementation with a sparse child array. Tests indicate that the enhanced algorithm for finding maximal exact matches is much faster, while maintaining the same memory footprint. In this way, sparse suffix arrays remain competitive with the more complex compressed suffix arrays.	algorithm;maximal set;memory footprint;sparse matrix;suffix array	Michaël Vyverman;Bernard De Baets;Veerle Fack;Peter Dawyndt	2013	Bioinformatics	10.1093/bioinformatics/btt042	biology;computer science;theoretical computer science;machine learning;compressed suffix array;algorithm	HPC	-1.5594077503410053	-52.08213486700384	135430
5aebfdb5c2853f60444273be5d0e048778932eb4	rnaseqviewer: visualization tool for rna-seq data		SUMMARY With the advances of RNA sequencing technologies, scientists need new tools to analyze transcriptome data. We introduce RNAseqViewer, a new visualization tool dedicated to RNA-Seq data. The program offers innovative ways to represent transcriptome data for single or multiple samples. It is a handy tool for scientists who use RNA-Seq data to compare multiple transcriptomes, for example, to compare gene expression and alternative splicing of cancer samples or of different development stages.   AVAILABILITY AND IMPLEMENTATION RNAseqViewer is freely available for academic use at http://bioinfo.au.tsinghua.edu.cn/software/RNAseqViewer/   CONTACT zhangxg@tsinghua.edu.cn   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	alternative splicing;bioinformatics;gene expression;handy board;imagery;rna splicing;transcriptome	Xavier Rogé;Xuegong Zhang	2014	Bioinformatics	10.1093/bioinformatics/btt649	computer science;bioinformatics;data science;data mining	Comp.	-2.3937443802138274	-57.93995466593733	135476
ced7e0034e3525e50e32658aa424cfba1744a439	raiphy: phylogenetic classification of metagenomics samples using iterative refinement of relative abundance index profiles	software;iterative refinement;classification algorithm;phylogeny;sequence similarity;sequence analysis dna;computational biology bioinformatics;metagenomics;relative abundance;indexation;acid mine drainage;algorithms;computer analysis;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Computational analysis of metagenomes requires the taxonomical assignment of the genome contigs assembled from DNA reads of environmental samples. Because of the diverse nature of microbiomes, the length of the assemblies obtained can vary between a few hundred bp to a few hundred Kbp. Current taxonomic classification algorithms provide accurate classification for long contigs or for short fragments from organisms that have close relatives with annotated genomes. These are significant limitations for metagenome analysis because of the complexity of microbiomes and the paucity of existing annotated genomes. We propose a robust taxonomic classification method, RAIphy, that uses a novel sequence similarity metric with iterative refinement of taxonomic models and functions effectively without these limitations. We have tested RAIphy with synthetic metagenomics data ranging between 100 bp to 50 Kbp. Within a sequence read range of 100 bp-1000 bp, the sensitivity of RAIphy ranges between 38%-81% outperforming the currently popular composition-based methods for reads in this range. Comparison with computationally more intensive sequence similarity methods shows that RAIphy performs competitively while being significantly faster. The sensitivity-specificity characteristics for relatively longer contigs were compared with the PhyloPythia and TACOA algorithms. RAIphy performs better than these algorithms at varying clade-levels. For an acid mine drainage (AMD) metagenome, RAIphy was able to taxonomically bin the sequence read set more accurately than the currently available methods, Phymm and MEGAN, and more accurately in two out of three tests than the much more computationally intensive method, PhymmBL. With the introduction of the relative abundance index metric and an iterative classification method, we propose a taxonomic classification algorithm that performs competitively for a large range of DNA contig lengths assembled from metagenome data. Because of its speed, simplicity, and accuracy RAIphy can be successfully used in the binning process for a broad range of metagenomic data obtained from environmental samples.	algorithm;bin;clade;genome;homology (biology);iterative method;iterative refinement;megan;metagenomics;microbiome;phylogenetics;pierre robin syndrome;product binning;reading (activity);refinement (computing);sensitivity and specificity;synthetic data;tacrolimus binding proteins;taxonomy (general);thioctic acid	Özkan U. Nalbantoglu;Samuel F. Way;Steven H. Hinrichs;Khalid Sayood	2010		10.1186/1471-2105-12-41	biology;relative species abundance;dna microarray;bioinformatics;data mining;genetics;metagenomics	Comp.	0.9647426419961598	-54.48097694210345	135816
7712cfc4c87740f3cb9ecb25d8c668c6c5952f40	comparative visualization of genetic and physical maps with strudel	genetique;software;genomics;visualizacion;genetica;computer graphics;mapa fisico;genetic mapping;genetics;physical chromosome mapping;visualization;visualisation;genome;carte genetique;carte physique;mapa genetico;user computer interface;computational biology;physical map	UNLABELLED Data visualization can play a key role in comparative genomics, for example, underpinning the investigation of conserved synteny patterns. Strudel is a desktop application that allows users to easily compare both genetic and physical maps interactively and efficiently. It can handle large datasets from several genomes simultaneously, and allows all-by-all comparisons between these.   AVAILABILITY AND IMPLEMENTATION Installers for Strudel are available for Windows, Linux, Solaris and Mac OS X at http://bioinf.scri.ac.uk/strudel/.	calviria solaris;data visualization;desktop computer;exercise;genome;imagery;interactivity;linux;map;microsoft windows;operating system;synteny	Micha Bayer;Iain Milne;Gordon Stephen;Paul D. Shaw;Linda Cardle;Frank Wright;A. David Marshall	2011		10.1093/bioinformatics/btr111	biology;genomics;visualization;bioinformatics;genetics	Visualization	-3.617899829899067	-57.67851537791301	135979
23999f9bc2380be2738880179ce23b2e5917a602	application of whole genome data for in silico evaluation of primers and probes routinely employed for the detection of viral species by rt-qpcr using dengue virus as a case study	blast;dengue virus;rt-qpcr;virus detection	BACKGROUND Viral infection by dengue virus is a major public health problem in tropical countries. Early diagnosis and detection are increasingly based on quantitative reverse transcriptase real-time polymerase chain reaction (RT-qPCR) directed against genomic regions conserved between different isolates. Genetic variation can however result in mismatches of primers and probes with their targeted nucleic acid regions. Whole genome sequencing allows to characterize and track such changes, which in turn enables to evaluate, optimize, and (re-)design novel and existing RT-qPCR methods. The immense amount of available sequence data renders this however a labour-intensive and complex task.   RESULTS We present a bioinformatics approach that enables in silico evaluation of primers and probes intended for routinely employed RT-qPCR methods. This approach is based on analysing large amounts of publically available whole genome data, by first employing BLASTN to mine the genomic regions targeted by the RT-qPCR method(s), and afterwards using BLASTN-SHORT to evaluate whether primers and probes will anneal based on a set of simple in silico criteria. Using dengue virus as a case study, we evaluated 18 published RT-qPCR methods using more than 3000 publically available genomes in the NCBI Virus Variation Resource, and provide a systematic overview of method performance based on in silico sensitivity and specificity.   CONCLUSIONS We provide a comprehensive overview of dengue virus RT-qPCR method performance that will aid appropriate method selection allowing to take specific measures that aim to contain and prevent viral spread in afflicted regions. Notably, we find that primer-template mismatches at their 3' end may represent a general issue for dengue virus RT-qPCR detection methods that merits more attention in their development process. Our approach is also available as a public tool, and demonstrates how utilizing genomic data can provide meaningful insights in an applied public health setting such as the detection of viral species in human diagnostics.	blast;bioinformatics;clinical use template;dengue fever;dengue virus;heterozygote detection;labor (childbirth);ncbi taxonomy;nucleic acids;primer;polymerase chain reaction;rna-directed dna polymerase;real-time clock;rendering (computer graphics);scientific publication;sensitivity and specificity;whole genome sequencing;windows rt	Kevin Vanneste;Linda Garlant;Sylvia Broeders;Steven Van Gucht;Nancy Roosens	2018		10.1186/s12859-018-2313-0	dna microarray;genome;genetic variation;bioinformatics;biology;primer (molecular biology);polymerase chain reaction;dengue virus;whole genome sequencing;reverse transcriptase	Comp.	0.26935908554828336	-57.37636506180043	136896
5851fe8d2db4c4d9169113738f2e88c09dea9393	a new approach to fragment assembly in dna sequencing	generalization error;information retrieval;time resolved;large scale;prosite;pattern matching;bit parallelism;computational biology;dna sequence	For the last twenty years fragment assembly in DNA sequencing followed the “overlap - layout - consensus” paradigm that is used in all currently available assembly tools. Although this approach proved to be useful in assembling clones, it faces difficulties in genomic shotgun assembly: the existing algorithms make assembly errors and are often unable to resolve repeats even in prokaryotic genomes. Biologists are well-aware of these errors and are forced to carry additional experiments to verify the assembled contigs. We abandon the classical “overlap - layout - consensus” approach in favor of a new Eulerian Superpath approach that, for the first time, resolves the problem of repeats in fragment assembly. Our main result is the reduction of the fragment assembly to a variation of the classical Eulerian path problem. This reduction opens new possibilities for repeat resolution and allows one to generate error-free solutions of the large-scale fragment assembly problems. The major improvement of EULER over other algorithms is that it resolves all repeats except long perfect repeats that are theoretically impossible to resolve without additional experiments.	algorithm;eulerian path;experiment;programming paradigm	Pavel A. Pevzner;Haixu Tang;Michael S. Waterman	2001		10.1145/369133.369230	dna sequencing;prosite;computer science;bioinformatics;theoretical computer science;pattern matching;genetics;hybrid genome assembly;algorithm;generalization error	Comp.	-0.21688687383658015	-53.43417589419647	137096
e85290a3563039fc9321165c8600a7374860178c	3d protein structure predicted from sequence	experimental method;protein family;sequence similarity;synthetic biology;three dimensional;genetics;g protein coupled receptor;protein structure;data analysis;structural genomics;protein structure prediction;structure prediction;protein folding;maximum entropy model;homology modeling;multiple sequence alignment;sequence space;3d structure	The evolutionary trajectory of a protein through sequence space is constrained by function and three-­‐ dimensional (3D) structure. Residues in spatial proximity tend to co-­‐evolve, yet attempts to invert the evolutionary BLOCKIN BLOCKIN record BLOCKIN BLOCKIN to BLOCKIN BLOCKIN identify BLOCKIN BLOCKIN these BLOCKIN BLOCKIN constraints BLOCKIN BLOCKIN and BLOCKIN BLOCKIN use BLOCKIN BLOCKIN them BLOCKIN BLOCKIN to BLOCKIN BLOCKIN computationally BLOCKIN BLOCKIN fold BLOCKIN BLOCKIN proteins BLOCKIN BLOCKIN have BLOCKIN BLOCKIN so BLOCKIN BLOCKIN far been unsuccessful. Here, we show that co-­‐variation of residue pairs, observed in a large protein family, provides sufficient information to determine 3D protein structure. Using a data-­‐constrained maximum entropy model of the multiple sequence alignment, we identify pairs of statistically coupled residue positions which are expected to be close in the protein fold, termed contacts inferred from evolutionary information BLOCKIN BLOCKIN (EICs).	multiple sequence alignment;principle of maximum entropy;protein family	Debora S. Marks;Lucy J. Colwell;Robert P. Sheridan;Thomas A. Hopf;Andrea Pagnani;Riccardo Zecchina;Chris Sander	2011	CoRR		threading;protein folding;structural genomics;biology;three-dimensional space;protein structure;structural alignment;homology modeling;multiple sequence alignment;bioinformatics;g protein-coupled receptor;principle of maximum entropy;loop modeling;protein structure prediction;data analysis;protein family;sequence space;genetics;synthetic biology	Comp.	0.8956046688972971	-53.8196781885991	137300
805cc44573c1d4deb59582cc7c2504561e155311	automatic profile extraction based on frequency distribution of atoms for retrieving similar interaction protein	biology computing;protein function;protein retrieval automatic profile extraction frequency distribution protein interaction;protein structure;proteins;feature extraction;molecular biophysics;frequency proteins data mining peptides databases information retrieval amino acids information science functional analysis drugs;biology computing proteins feature extraction molecular biophysics	Protein function is closely related to the structure of interaction site of the protein. Proteins that have similar structure at interaction sites often bind to the same compound. We have proposed a method for retrieving similar interaction protein using profiles that represent the features of the interaction site binding to a certain compound. Since it takes a long time to extract a profile manually, automatic extraction of a profile from the protein structure data is highly required. This paper presents a method for automatic profile extraction based on frequency distribution of atoms. The profile extracted by using the proposed method was closely similar to the profile extracted manually. Retrieval experiment shows that the accuracy of identifying correct interaction sites using a profile generated by the proposed method is equivalent to the result using a manually constructed profile.		Mariko Matsumoto;Yusuke Nonomura;Takenao Ohkawa	2005	5th International Conference on Intelligent Systems Design and Applications (ISDA'05)	10.1109/ISDA.2005.29	protein structure;feature extraction;computer science;bioinformatics;data mining;information retrieval;molecular biophysics	Robotics	1.5917997871991687	-58.50108402499057	137532
27c72987ac236e633cfd80a9101f592d9dec5000	a hybrid method for the exact planted (l, d) motif finding problem and its parallelization	software;sequence analysis dna;computational biology bioinformatics;nucleotide motifs;algorithms;combinatorial libraries;computational biology;computer appl in life sciences;microarrays;bioinformatics	Given a set of DNA sequences s1, ..., s t , the (l, d) motif problem is to find an l-length motif sequence M , not necessary existing in any of the input sequences, such that for each sequence s i , 1 ≤ i ≤ t, there is at least one subsequence differing with at most d mismatches from M. Many exact algorithms have been developed to solve the motif finding problem in the last three decades. However, the problem is still challenging and its solution is limited to small values of l and d. In this paper we present a new efficient method to improve the performance of the exact algorithms for the motif finding problem. Our method is composed of two main steps: First, we process q ≤ t sequences to find candidate motifs. Second, the candidate motifs are searched in the remaining sequences. For both steps, we use the best available algorithms. Our method is a hybrid one, because it integrates currently existing algorithms to achieve the best running time. In this paper, we show how the optimal value of q is determined to achieve the best running time. Our experimental results show that there is about 24% speed-up achieved by our method compared to the best existing algorithm. Furthermore, we also present a parallel version of our method running on shared memory architecture. Our experiments show that the performance of our algorithm scales linearly with the number of processors. Using the parallel version, we were able to solve the (21, 8) challenging instance using 8 processors in 20.42 hours instead of 6.68 days of the serial version. Our method speeds up the solution of the exact motif problem. Our method is generic, because it can accommodate any new faster algorithm based on traditional methods. We expect that our method will help to discover longer motifs. The software we developed is available for free for academic research at http://www.nubios.nileu.edu.eg/tools/hymotif .	algorithm;central processing unit;emoticon;expect;experiment;generic drugs;motif;optimization problem;parallel computing;search - action;shared memory;time complexity	Mostafa M. Abbas;Mohamed Abouelhoda;Hazem M. Bahig	2012		10.1186/1471-2105-13-S17-S10	biology;dna microarray;computer science;bioinformatics;theoretical computer science;planted motif search;algorithm	Comp.	-1.3774635053217608	-52.608698727204974	137729
aa258c60361c42e6fe714216a61b36284f42f796	identification of consensus patterns in unaligned dna sequences known to be functionally related	dna;computadora;software;computer aided analysis;analyse assistee;logiciel;computerized processing;tratamiento informatico;ordinateur;consensus sequence;motif structural;sequence consensus;binding site;secuencia nucleotido;computer;nucleotide sequence;sequence nucleotide;algorithme;motivo estructural;site fixation;algorithm;methode matricielle;identification;structural unit;matrix method;proteine de liaison dna;metodo matriz;logicial;identificacion;analisis asistido;proteina de enlace dna;dna sequence;traitement informatique;sitio fijacion;dna binding protein;algoritmo	We have developed a method for identifying consensus patterns in a set of unaligned DNA sequences known to bind a common protein or to have some other common biochemical function. The method is based on a matrix representation of binding site patterns. Each row of the matrix represents one of the four possible bases, each column represents one of the positions of the binding site and each element is determined by the frequency the indicated base occurs at the indicated position. The goal of the method is to find the most significant matrix--i.e. the one with the lowest probability of occurring by chance--out of all the matrices that can be formed from the set of related sequences. The reliability of the method improves with the number of sequences, while the time required increases only linearly with the number of sequences. To test this method, we analysed 11 DNA sequences containing promoters regulated by the Escherichia coli LexA protein. The matrices we found were consistent with the known consensus sequence, and could distinguish the generally accepted LexA binding sites from other DNA sequences.	base;binding sites;consensus sequence;matrix representation;randomness;the matrix;promoter	G. Z. Hertz;G. W. Hartzell;Gary D. Stormo	1990	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/6.2.81	consensus sequence;identification;matrix method;biology;dna sequencing;dna-binding protein;nucleic acid sequence;bioinformatics;binding site;structural unit;genetics;dna	Comp.	-3.808682301349752	-55.19041795702089	137882
0080fce0fdc3be86f95b6201ead7e5221027bf00	code optimization of the subroutine to remove near identical matches in the sequence database homology search tool psi-blast	code optimization;homology search;gapped sequence alignment	A central task in protein sequence characterization is the use of a sequence database homology search tool to find similar protein sequences in other individuals or species. PSI-BLAST is a widely used module of the BLAST package that calculates a position-specific score matrix from the best matching sequences and performs iterated searches using a method to avoid many similar sequences for the score. For some queries and parameter settings, PSI-BLAST may find many similar high-scoring matches, and therefore up to 80% of the total run time may be spent in this procedure. In this article, we present code optimizations that improve the cache utilization and the overall performance of this procedure. Measurements show that, for queries where the number of similar matches is high, the optimized PSI-BLAST program may be as much as 2.9 times faster than the original program.		Mats Aspnäs;Kimmo Mattila;Kristoffer Osowski;Jan Westerholm	2010	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2008.0053	computer science;bioinformatics;theoretical computer science;program optimization;data mining	Comp.	-1.8516121045353722	-52.71934003382634	139178
845e745ac68f102b1648d98b237f3a8e6927d51f	design of multiple sequence alignment algorithms on parallel, distributed memory supercomputers	distributed memory;genomics;random access memory;algorithms base sequence computers mainframe dna bacterial genome bacterial molecular sequence data sequence alignment sequence analysis dna software software design;hidden markov model;comparative genomics;large data sets;genomics algorithm design and analysis random access memory bioinformatics supercomputers educational institutions hidden markov models;large scale;hidden markov models;multiple sequence alignment;bacterial genome;data structure;algorithm design;algorithm design and analysis;supercomputers;genome sequence;bioinformatics	The challenge of comparing two or more genomes that have undergone recombination and substantial amounts of segmental loss and gain has recently been addressed for small numbers of genomes. However, datasets of hundreds of genomes are now common and their sizes will only increase in the future. Multiple sequence alignment of hundreds of genomes remains an intractable problem due to quadratic increases in compute time and memory footprint. To date, most alignment algorithms are designed for commodity clusters without parallelism. Hence, we propose the design of a multiple sequence alignment algorithm on massively parallel, distributed memory supercomputers to enable research into comparative genomics on large data sets. Following the methodology of the sequential progressiveMauve algorithm, we design data structures including sequences and sorted k-mer lists on the IBM Blue Gene/P supercomputer (BG/P). Preliminary results show that we can reduce the memory footprint so that we can potentially align over 250 bacterial genomes on a single BG/P compute node. We verify our results on a dataset of E.coli, Shigella and S.pneumoniae genomes. Our implementation returns results matching those of the original algorithm but in 1/2 the time and with 1/4 the memory footprint for scaffold building. In this study, we have laid the basis for multiple sequence alignment of large-scale datasets on a massively parallel, distributed memory supercomputer, thus enabling comparison of hundreds instead of a few genome sequences within reasonable time.	algorithm;align (company);anatomic node;benchmark (computing);blue gene;computational complexity theory;computer cluster;data structure;distributed memory;ethanol 0.62 ml/ml topical gel;gain;genome;genomics;hearing loss, high-frequency;hidden markov model;inclusion body myositis (disorder);job control (unix);k-mer;memory footprint;mer;multiple sequence alignment;parallel computing;run time (program lifecycle phase);scalability;silo (dataset);supercomputer;verification of theories	Philip C. Church;Andrzej M. Goscinski;Kathryn Holt;Michael Inouye;Amol Ghoting;Konstantin Makarychev;Matthias Reumann	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6090208	algorithm design;computer science;bioinformatics;theoretical computer science;data mining;hidden markov model;alignment-free sequence analysis	HPC	-1.3333553587703422	-52.51768397177364	139898
9416749aa2e606a5d82ff1fa4047d42e6920a3c8	forrepeats: detects repeats on entire chromosomes and between genomes	complete genome;heuristic method;arabidopsis thaliana;genome comparison;dna sequence;data structure	MOTIVATION As more and more whole genomes are available, there is a need for new methods to compare large sequences and transfer biological knowledge from annotated genomes to related new ones. BLAST is not suitable to compare multimegabase DNA sequences. MegaBLAST is designed to compare closely related large sequences. Some tools to detect repeats in large sequences have already been developed such as MUMmer or REPuter. They also have time or space restrictions. Moreover, in terms of applications, REPuter only computes repeats and MUMmer works better with related genomes.   RESULTS We present a heuristic method, named FORRepeats, which is based on a novel data structure called factor oracle. In the first step it detects exact repeats in large sequences. Then, in the second step, it computes approximate repeats and performs pairwise comparison. We compared its computational characteristics with BLAST and REPuter. Results demonstrate that it is fast and space economical. We show FORRepeats ability to perform intra-genomic comparison and to detect repeated DNA sequences in the complete genome of the model plant Arabidopsis thaliana.	approximation algorithm;blast;chromosomes;computation;dna-directed dna polymerase;data structure;factor oracle;genome;heuristic;mummer;name	Arnaud Lefebvre;Thierry Lecroq;Hélène Dauchel;Joël Alexandre	2003	Bioinformatics	10.1093/bioinformatics/btf843	biology;dna sequencing;data structure;computer science;bioinformatics;genetics	Comp.	-0.669903587364649	-52.93368816273092	141075
326ab1010de8112f76f22c84f63c10c759f6287b	plot2groups: an r package to plot scatter points for two groups of values	scatter point;biological patents;biomedical journals;text mining;europe pubmed central;plot2groups;citation search;citation networks;computational biology bioinformatics;r package;research articles;abstracts;open access;life sciences;clinical guidelines;full text;computer appl in life sciences;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Researchers usually employ bar graphs to show two groups of data, which can be easily manipulated to yield false impressions. To some extent, scatterplot can retain the real data values and the spread of the data. However, for groups of numeric data, scatterplot may cause over-plotting problems. As a result, many values all stack on top of each other. We recently implemented an R package, plot2groups, to plot scatter points for two groups values, jittering the adjacent points side by side to avoid overlapping in the plot. The functions simultaneously calculate a P value of two group t- or rank-test and incorporated the P value into the plot. plot2groups is a simple and flexible software package which can be used to visualize two groups of values within the statistical programming environment R.	integrated development environment;level of measurement;p (complexity);r language	Yong Xu;Fuquan Zhang;Guoqiang Wang;Hongbao Cao;Yin Yao Shugart;Zaohuo Cheng	2014		10.1186/1751-0473-9-23	scatter plot;text mining;medical research;computer science;bioinformatics;data science;data mining;plot;carpet plot	HCI	-2.847603513795761	-56.943514542285	142027
9e3067ed19153e0267fe0ec407c713dc482ac81c	bs-seeker3: ultrafast pipeline for bisulfite sequencing		DNA methylation is an important epigenetic modification critical in regulation and transgenerational inheritance. The methylation level can be estimated at single-nucleotide resolution by whole-genome bisulfite sequencing (BS-seq; WGBS). Current bisulfite aligners provide pipelines for processing the reads by WGBS; however, few are able to analyze the BS-seqs in a reasonable timeframe that meets the needs of the rapid expansion of epigenome sequencing in biomedical research. We introduce BS-Seeker3, an extensively improved and optimized implementation of BS-Seeker2 that leverages the available computational power of a standard bioinformatics lab. BS-Seeker3 adopts all alignment features of BS-Seeker2. It performs ultrafast alignments and achieves both high accuracy and high mappability, more than twice that of the other aligners that we evaluated. Moreover, BS Seeker 3 is well linked with downstream analyzer MethGo for up to 9 types of genomic and epigenomic analyses. BS-Seeker3 is an accurate, versatile, ultra-fast pipeline for processing bisulfite-converted reads. It also helps the user better visualize the methylation data.	alignment;analyzer device component;bs 7799;bioinformatics;biopolymer sequencing;bisulfite sequencing;downstream (software development);epigenetic process;epigenomics;greater than;methylation;nucleotides;pipeline (computing);pipeline pilot;reading (activity);sequence number;hydrogen sulfite;study of epigenetics	Kevin Yu Yuan Huang;Yan-Jiun Huang;Pao-Yang Chen	2018		10.1186/s12859-018-2120-7	genetics;dna methylation;dna microarray;bisulfite;bisulfite sequencing;transgenerational epigenetics;biology;bioinformatics;methylation;epigenome;epigenetics	Comp.	-0.5196808581572667	-55.43066328681829	142312
79ea6fa30ea3f7e2f2d6b099972c1257ac48c701	topal 2.0: improved detection of mosaic sequences within multiple alignments	dna;software;computer program;genetic variability;parametric bootstrap;alignement sequence;recombinaison;logiciel;deteccion;statistical test;secuencia nucleotido;detection;alineacion secuencia;genetics;nucleotide sequence;sequence nucleotide;variabilidad genetica;algorithme;algorithm;topal;logicial;recombination;recombinacion;sequence alignment;variabilite genetique;programa computador;programme ordinateur;multiple alignment;algoritmo	MOTIVATION The Dss statistic was proposed by McGuire et al. (Mol. Biol. Evol., 14, 1125-1131, 1997) for scanning data sets for the presence of recombination, an important step in some phylogenetic analyses. The statistic, however, could not distinguish well between among-site rate variation and recombination, and had no statistical test for significant values. This paper addresses these shortfalls.   RESULTS A modification to the Dss statistic is proposed which accounts for rate variation to a large extent. A statistical test, based on parametric bootstrapping, is also suggested.   AVAILABILITY The TOPAL package (version 2) may be accessed from http:/ /www.bioss.sari.ac.uk/frank/Genetics and by anonymous ftp from typ://ftp.bioss.sari.ac.uk in the directory pub/phylogeny/topal.   CONTACT frank@bioss.sari.ac.uk	addresses (publication format);bootstrapping (compilers);bootstrapping (statistics);crossover (genetic algorithm);dss brand of docusate sodium;directory (computing);mosaic organism;phylogenetics;published directory;statistic (data);statistical test;topal	Gráinne McGuire;Frank Wright	2000	Bioinformatics	10.1093/bioinformatics/16.2.130	genetic variability;biology;statistical hypothesis testing;multiple sequence alignment;nucleic acid sequence;bioinformatics;sequence alignment;genetics;dna;recombination	Comp.	-3.139219862682466	-55.598112201972306	142776
d76c5a01b8e93ec4e9b3b3322ecce43cd86a1b05	rvboost: rna-seq variants prioritization using a boosting method	software;perfilacao da expressao genica;high throughput nucleotide sequencing;analise de sequencia de rna;genetic variation;exome;exoma;sequence analysis rna;gene expression profiling;exons;sequenciamento de nucleotideos em larga escala;variacao genetica	MOTIVATION RNA-seq has become the method of choice to quantify genes and exons, discover novel transcripts and detect fusion genes. However, reliable variant identification from RNA-seq data remains challenging because of the complexities of the transcriptome, the challenges of accurately mapping exon boundary spanning reads and the bias introduced during the sequencing library preparation.   METHOD We developed RVboost, a novel method specific for RNA variant prioritization. RVboost uses several attributes unique in the process of RNA library preparation, sequencing and RNA-seq data analyses. It uses a boosting method to train a model of 'good quality' variants using common variants from HapMap, and prioritizes and calls the RNA variants based on the trained model. We packaged RVboost in a comprehensive workflow, which integrates tools of variant calling, annotation and filtering.   RESULTS RVboost consistently outperforms the variant quality score recalibration from the Genome Analysis Tool Kit and the RNA-seq variant-calling pipeline SNPiR in 12 RNA-seq samples using ground-truth variants from paired exome sequencing data. Several RNA-seq-specific attributes were identified as critical to differentiate true and false variants, including the distance of the variant positions to exon boundaries, and the percent of the reads supporting the variant in the first six base pairs. The latter identifies false variants introduced by the random hexamer priming during the library construction.   AVAILABILITY AND IMPLEMENTATION The RVboost package is implemented to readily run in Mac or Linux environments. The software and user manual are available at http://bioinformaticstools.mayo.edu/research/rvboost/.	annotation;base pairing;base sequence;biopolymer sequencing;boosting (machine learning);breast carcinoma;exons;file spanning;ground truth;hoc (programming language);international hapmap project;jane (software);laboratory sample manual;linux;marathon;mast/stem cell growth factor receptor kit, human;medicine, east asian traditional;p50 evoked potentials;rna;reading (activity);requirement prioritization;sequence number;transcript;transcriptome;united states national institutes of health;whole exome sequencing;cdna library construction	Chen Wang;Jaime Davila;Saurabh Baheti;Aditya Bhagwate;Xue Wang;Jean-Pierre A. Kocher;Susan L. Slager;Andrew L. Feldman;Anne J. Novak;James R. Cerhan;E. Aubrey Thompson;Yan W. Asmann	2014		10.1093/bioinformatics/btu577	biology;exon;bioinformatics;genetic variation;data mining;gene expression profiling;exome;genetics	Comp.	1.5680640952626295	-54.89508690791812	143086
16b34a01c549bc1b12b6c5a17e85e1db117f26e3	socrates: identification of genomic rearrangements in tumour genomes by re-aligning soft clipped reads	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	MOTIVATION Methods for detecting somatic genome rearrangements in tumours using next-generation sequencing are vital in cancer genomics. Available algorithms use one or more sources of evidence, such as read depth, paired-end reads or split reads to predict structural variants. However, the problem remains challenging due to the significant computational burden and high false-positive or false-negative rates.   RESULTS In this article, we present Socrates (SOft Clip re-alignment To idEntify Structural variants), a highly efficient and effective method for detecting genomic rearrangements in tumours that uses only split-read data. Socrates has single-nucleotide resolution, identifies micro-homologies and untemplated sequence at break points, has high sensitivity and high specificity and takes advantage of parallelism for efficient use of resources. We demonstrate using simulated and real data that Socrates performs well compared with a number of existing structural variant detection tools.   AVAILABILITY AND IMPLEMENTATION Socrates is released as open source and available from http://bioinf.wehi.edu.au/socrates CONTACT: papenfuss@wehi.edu.au Supplementary information: Supplementary data are available at Bioinformatics online.	bioinformatics;biopolymer sequencing;computation;dna sequence rearrangement;diploid cell;effective method;genome;geographic information systems;homology (biology);massively-parallel sequencing;neoplasms;nucleotides;open-source software;parallel computing;reading (activity);sensitivity and specificity;sensor;algorithm	Jan Schröder;Arthur Hsu;Samantha E. Boyle;Geoff Macintyre;Marek Cmero;Richard W. Tothill;Ricky W. Johnstone;Mark Shackleton;Anthony T. Papenfuss	2014		10.1093/bioinformatics/btt767	biology;medical research;bioinformatics;data mining;genetics	Comp.	0.7960981914357396	-54.89297502344432	144071
ce8a9291fda3362131fff4109acf6dc9d50fa8f5	a simple algorithm for detecting circular permutations in proteins	evolution moleculaire;modelizacion;relation structure fonction;n terminal;secuencia aminoacido;genetic operator;estructura 3 dimensiones;alignement sequence;proteine;sequence aminoacide;aminoacid sequence;deteccion;efficient algorithm;dynamic programming algorithm;edit distance;relacion estructura funcion;detection;alineacion secuencia;structure function relationship;structure 3 dimensions;permutation;algorithme;modelisation;algorithm;large scale;evolucion molecular;molecular evolution;permutacion;proteina;sequence alignment;fortran;three dimensional structure;protein;circular configuration;modeling;configuration circulaire;configuracion circular;algoritmo	MOTIVATION Circular permutation of a protein is a genetic operation in which part of the C-terminal of the protein is moved to its N-terminal. Recently, it has been shown that proteins that undergo engineered circular permutations generally maintain their three dimensional structure and biological function. This observation raises the possibility that circular permutation has occurred in Nature during evolution. In this scenario a protein underwent circular permutation into another protein, thereafter both proteins further diverged by standard genetic operations. To study this possibility one needs an efficient algorithm that for a given pair of proteins can detect the underlying event of circular permutations. A possible formal description of the question is: given two sequences, find a circular permutation of one of them under which the edit distance between the proteins is minimal. A naive algorithm might take time proportional to N3 or even N4, which is prohibitively slow for a large-scale survey. A sophisticated algorithm that runs in asymptotic time of N2 was recently suggested, but it is not practical for a large-scale survey.   RESULTS A simple and efficient algorithm that runs in time N2 is presented. The algorithm is based on duplicating one of the two sequences, and then performing a modified version of the standard dynamic programming algorithm. While the algorithm is not guaranteed to find the optimal results, we present data that indicate that in practice the algorithm performs very well.   AVAILABILITY A Fortran program that calculates the optimal edit distance under circular permutation is available upon request from the authors.   CONTACT ron@biocom1.ls.biu.ac.il.	algorithm;cyclic permutation;dynamic programming;edit distance;fortran;function (biology);hl7publishingsubsection <operations>;sensor;staphylococcal protein a;newton	S. Uliel;A. Fliess;Amihood Amir;Ron Unger	1999	Bioinformatics	10.1093/bioinformatics/15.11.930	biology;combinatorics;systems modeling;edit distance;molecular evolution;computer science;circular shift;genetic operator;dynamic programming;n-terminus;sequence alignment;mathematics;permutation;algorithm	Comp.	-3.79549474002405	-54.96192661383689	144214
148b3a99a9c73043da587af6b8c5fe7b49532e34	clu: a new algorithm for est clustering	cluster algorithm;regulation of gene expression;low complexity;data mining;nucleotide sequence;computational biology bioinformatics;short tandem repeat;cluster analysis;genomic dna;continuous flow;detection algorithm;algorithms;combinatorial libraries;computer appl in life sciences;sequence analysis protein;expressed sequence tags;open source;microarrays;bioinformatics	The continuous flow of EST data remains one of the richest sources for discoveries in modern biology. The first step in EST data mining is usually associated with EST clustering, the process of grouping of original fragments according to their annotation, similarity to known genomic DNA or each other. Clustered EST data, accumulated in databases such as UniGene, STACK and TIGR Gene Indices have proven to be crucial in research areas from gene discovery to regulation of gene expression. We have developed a new nucleotide sequence matching algorithm and its implementation for clustering EST sequences. The program is based on the original CLU match detection algorithm, which has improved performance over the widely used d2_cluster. The CLU algorithm automatically ignores low-complexity regions like poly-tracts and short tandem repeats. CLU represents a new generation of EST clustering algorithm with improved performance over current approaches. An early implementation can be applied in small and medium-size projects. The CLU program is available on an open source basis free of charge. It can be downloaded from http://compbio.pbrc.edu/pti	annotation;base sequence;clu gene;candidate gene identification;cluster analysis;data mining;database;databases;gene expression regulation;matching;nucleotides;open-source software;short tandem repeat;tandem repeat sequences;unigene (experimental system);algorithm;statistical cluster	Andrey A. Ptitsyn;Winston A Hide	2005	BMC Bioinformatics	10.1186/1471-2105-6-S2-S3	biology;regulation of gene expression;dna microarray;nucleic acid sequence;computer science;bioinformatics;genomic dna;data mining;microsatellite;cluster analysis;genetics;expressed sequence tag	Comp.	-0.5326556374149067	-56.193857020228236	144444
8389647878e441aa736e4da5b72d4bbba7a78188	automatic annotation of bacterial community sequences and application to infections diagnostic		To annotate bacterial sequences from an environmental sample, we have developed an automatic annotation pipeline Fgenesb_annotator that includes self-training of gene-finding parameters, prediction of CDS, RNA genes, operons, promoters and terminators. New version of pipeline includes frame shift corrections and special module with improved prediction accuracy of ribosomal proteins. To analyze next-generation sequencing data we have developed OligiZip assembler and Transomics pipeline that provide solutions to the following tasks: 1) de novo reconstruction of genomic sequence; 2) reconstruction of sequence with a reference genome; 3) SNP discovery; 4) mapping RNA-Seq data to a reference genome, assemble them into alternative transcripts and quantify the abundance of these transcripts. Using the OligoZip assembler and gene Fgenesb pipeline we have developed a novel computational approach of identification toxic and nontoxic bacterial serotypes using next-generation sequencing data. It can be used for detection of bacterial infections in wounds, water or food contamination.	assembly language;de novo transcriptome assembly;gene prediction	Victor V. Solovyev;Asaf A. Salamov;Igor A. Seledtsov;Denis Vorobyev;Alexander G. Bachinsky	2011			bioinformatics;data mining	Comp.	-0.24076815248818587	-56.889379254865474	145796
e2d3908f1259479d47f2a3bfd253fd6538eac3af	hicdat: a fast and easy-to-use hi-c data analysis tool	software;genomics;computational biology bioinformatics;rna;statistics as topic;chromosomes;algorithms;humans;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	The study of nuclear architecture using Chromosome Conformation Capture (3C) technologies is a novel frontier in biology. With further reduction in sequencing costs, the potential of Hi-C in describing nuclear architecture as a phenotype is only about to unfold. To use Hi-C for phenotypic comparisons among different cell types, conditions, or genetic backgrounds, Hi-C data processing needs to be more accessible to biologists. HiCdat provides a simple graphical user interface for data pre-processing and a collection of higher-level data analysis tools implemented in R. Data pre-processing also supports a wide range of additional data types required for in-depth analysis of the Hi-C data (e.g. RNA-Seq, ChIP-Seq, and BS-Seq). HiCdat is easy-to-use and provides solutions starting from aligned reads up to in-depth analyses. Importantly, HiCdat is focussed on the analysis of larger structural features of chromosomes, their correlation to genomic and epigenomic features, and on comparative studies. It uses simple input and output formats and can therefore easily be integrated into existing workflows or combined with alternative tools.	alignment;biopolymer sequencing;chromosomes;data pre-processing;epigenomics;genetic background;graphical user interface;hl7 data type;hi performance filesystem;hi-tech crime enquiry cell;input/output;large;preprocessor;user interface device component;cell type;format	Marc W. Schmid;Stefan Grob;Ueli Grossniklaus	2015		10.1186/s12859-015-0678-x	computational biology;biology;genomics;rna;dna microarray;bioinformatics;chromosome;genetics	Comp.	-2.0990316288910997	-57.6907086676477	146351
ae4d050465ec2a2875f8573f703367261f98f9b6	magi: a node.js web service for fast microrna-seq analysis in a gpu infrastructure	software;computer graphics;internet;sequence analysis rna;computational biology;micrornas;programming languages	SUMMARY MAGI is a web service for fast MicroRNA-Seq data analysis in a graphics processing unit (GPU) infrastructure. Using just a browser, users have access to results as web reports in just a few hours->600% end-to-end performance improvement over state of the art. MAGI's salient features are (i) transfer of large input files in native FASTA with Qualities (FASTQ) format through drag-and-drop operations, (ii) rapid prediction of microRNA target genes leveraging parallel computing with GPU devices, (iii) all-in-one analytics with novel feature extraction, statistical test for differential expression and diagnostic plot generation for quality control and (iv) interactive visualization and exploration of results in web reports that are readily available for publication.   AVAILABILITY AND IMPLEMENTATION MAGI relies on the Node.js JavaScript framework, along with NVIDIA CUDA C, PHP: Hypertext Preprocessor (PHP), Perl and R. It is freely available at http://magi.ucsd.edu.	cuda protein, staphylococcus xylosus;computation (action);computer graphics;drag and drop;end-to-end principle;environmental infrastructure;fasta;fastq format;feature extraction;graphics processing unit;hl7publishingsubsection <operations>;hypertext;interactive visualization;javascript framework;multi-function printer;node.js;php;parallel computing;perl;preprocessor;sequence number;web service	Jihoon Kim;Eric Levy;Alex Ferbrache;Petra Stepanowsky;Claudiu Farcas;Shuang Wang;Stefan Brunner;Tyler Bath;Yuan Wu;Lucila Ohno-Machado	2014		10.1093/bioinformatics/btu377	biology;the internet;computer science;bioinformatics;database;computer graphics;world wide web;genetics;microrna;computer graphics (images)	Web+IR	-2.546485183495049	-57.96090379243822	146499
83010bdcf89e1d6f87c3d5337f69036b46aa5800	detection and prediction of alternative splicing within acceptor/donor sites in pre-mrna of arabidopsis thaliana	splicing probes libraries proteins gene expression genomics bioinformatics dna sequences data mining;chromosome alternative splicing acceptor sites donor sites pre mrna arabidopsis thaliana gene expression expressed sequence tags decision tree algorithm codons nucleotides;molecular biophysics biology computing cellular biophysics decision trees genetics;chromosome;biology computing;alternative splicing;acceptor sites;decision tree;nucleotides;pre mrna;genetics;arabidopsis thaliana;gene expression;donor sites;molecular biophysics;decision tree algorithm;codons;decision trees;cellular biophysics;expressed sequence tag;expressed sequence tags	Alternative splicing is an important process in gene expression. Presently, most studies aimed at detecting alternatively spliced genes use ESTs (expressed sequence tags). However, reliance on ESTs might have some weaknesses in predicting alternative splicing. It is difficult to predict whether or not alternative splicing exists for those genes where ESTs are not available. In addition, since the EST libraries are often not clearly organized and annotated, we can pick erroneous ESTs. To address these issues and to improve the quality of detection and prediction for alternative splicing, we propose a method that primarily uses pre-mRNAs. It is achieved by a decision tree algorithm using codons, three nucleotides, as attributes for each chromosome in Arabidopsis thaliana. Each decision tree shows that alternative and normal splicing have different splicing patterns according to triplet nucleotides. Based on the patterns, alternative splicing of unlabeled genes can also be predicted.	acceptor (semiconductors);algorithm;decision tree;gene expression profiling;library (computing);list of algorithms;sensor;triplet state	Minseo Park;Deane L. Falcone;Kil-Young Yun;Karen M. Daniels	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375562	biology;molecular biology;bioinformatics;decision tree;genetics;expressed sequence tag;molecular biophysics	Comp.	0.5793362194590986	-55.16119185586937	146516
75ea9b3f074b8bc8f3a38b426bd8291841d2834f	calculating and scoring high quality multiple flexible protein structure alignments		MOTIVATION Calculating multiple protein structure alignments (MSAs) is important for understanding functional and evolutionary relationships between protein families, and for modeling protein structures by homology. While incorporating backbone flexibility promises to circumvent many of the limitations of rigid MSA algorithms, very few flexible MSA algorithms exist today. This article describes several novel improvements to the Kpax algorithm which allow high quality flexible MSAs to be calculated. This article also introduces a new Gaussian-based MSA quality measure called 'M-score', which circumvents the pitfalls of RMSD-based quality measures.   RESULTS As well as calculating flexible MSAs, the new version of Kpax can also score MSAs from other aligners and from previously aligned reference datasets. Results are presented for a large-scale evaluation of the Homstrad, SABmark and SISY benchmark sets using Kpax and Matt as examples of state-of-the-art flexible aligners and 3DCOMB as an example of a state-of-the-art rigid aligner. These results demonstrate the utility of the M-score as a measure of MSA quality and show that high quality MSAs may be achieved when structural flexibility is properly taken into account.   AVAILABILITY AND IMPLEMENTATION Kpax 5.0 may be downloaded for academic use at http://kpax.loria.fr/   CONTACT dave.ritchie@inria.fr   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	arterial pulse quality:type:pt:xxx:nom:palpation;benchmark (computing);bioinformatics;display resolution;distance;elfacos ow 100;homologous gene;homology (biology);internet backbone;linear algebra;mesna;multiple sequence alignment;muscle rigidity;normal statistical distribution;pivot;position weight matrix;protein family;protein, organized by structure;recursion;reference implementation;score;sense of identity (observable entity);tpo wt allele;vertebral column;algorithm;hemoglobin osler	David W. Ritchie	2016	Bioinformatics	10.1093/bioinformatics/btw300	computer science;data mining;protein structure;bioinformatics;multiple sequence alignment;protein family;gaussian	Comp.	1.3811052124879704	-54.319900746005466	146564
c105d91ff9016a59af5236d138ee85ac14a46197	a cooperative co-evolutionary genetic algorithm for tree scoring and ancestral genome inference	dna;evolution molecular;genomics;phylogeny;genome rearrangement;sorting;phylogenetic reconstruction;chromosome mapping;vegetation;sorting based approach cooperative coevolutionary genetic algorithm ancestral genome inference deep evolutionary history genomic structures computing tree scores heuristic methods;models genetic;ancestor inference;statistics;algorithms;genetic algorithm;genetic algorithms;molecular sequence data;genomics biology computing genetics;ancestor inference genome rearrangement phylogenetic reconstruction genetic algorithm;pedigree;base sequence;computer simulation;sociology;mutation;bioinformatics genomics sociology statistics genetic algorithms sorting;bioinformatics	Recent advances of technology have made it easy to obtain and compare whole genomes. Rearrangements of genomes through operations such as reversals and transpositions are rare events that enable researchers to reconstruct deep evolutionary history among species. Some of the popular methods need to search a large tree space for the best scored tree, thus it is desirable to have a fast and accurate method that can score a given tree efficiently. During the tree scoring procedure, the genomic structures of internal tree nodes are also provided, which provide important information for inferring ancestral genomes and for modeling the evolutionary processes. However, computing tree scores and ancestral genomes are very difficult and a lot of researchers have to rely on heuristic methods which have various disadvantages. In this paper, we describe the first genetic algorithm for tree scoring and ancestor inference, which uses a fitness function considering co-evolution, adopts different initial seeding methods to initialize the first population pool, and utilizes a sorting-based approach to realize evolution. Our extensive experiments show that compared with other existing algorithms, this new method is more accurate and can infer ancestral genomes that are much closer to the true ancestors.	anatomy, regional;computation (action);converge;cooperative coevolution;dna sequence rearrangement;deletion mutation;entity name part qualifier - adopted;experiment;fitness function;gene duplication abnormality;genetic algorithm;genome;hl7publishingsubsection <operations>;heuristic;inference;insertion mutation;large;phylogenetics;rare events;score;seeds (cellular automaton);software release life cycle;sorting;tree network	Nan Gao;Yan Zhang;Bing Feng;Jijun Tang	2015	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2430860	computer simulation;biology;genomics;phylogenetic tree;genetic algorithm;computer science;bioinformatics;tree rearrangement;genetics;evolutionary biology	Comp.	1.1730224316929505	-52.649564301093044	146699
d08c8a2534692734791b5e4f22b22e2b4875b9f8	a new algorithm for localized motif detection in long dna sequences		The evolution in genome sequencing has known a spectacular growth during the last decade. One of the main challenges for the researchers is to understand the evolution of the genome and in particular to identify the DNA segments that have a biological significance. In this study we present a new algorithm – ADMSL – optimized for finding motifs in long DNA sequences and we emphasize some experiments done in order to evaluate the performance of the proposed algorithm in comparison with other motifs finding algorithms.	algorithm;experiment;motif;whole genome sequencing	Alin G. Voina;Petre G. Pop;Mircea-Florin Vaida	2014	The Computer Science Journal of Moldova		bioinformatics	Comp.	0.06280008685775376	-53.26619999319894	146969
5b16352005cfbeb9120225c636a82fccbd115649	freeibis: an efficient basecaller with calibrated quality scores for illumina sequencers	software;support vector machines;sequence analysis dna;genotyping techniques;reproducibility of results;calibration	MOTIVATION The conversion of the raw intensities obtained from next-generation sequencing platforms into nucleotide sequences with well-calibrated quality scores is a critical step in the generation of good sequence data. While recent model-based approaches can yield highly accurate calls, they require a substantial amount of processing time and/or computational resources. We previously introduced Ibis, a fast and accurate basecaller for the Illumina platform. We have continued active development of Ibis to take into account developments in the Illumina technology, as well as to make Ibis fully open source.   RESULTS We introduce here freeIbis, which offers significant improvements in sequence accuracy owing to the use of a novel multiclass support vector machine (SVM) algorithm. Sequence quality scores are now calibrated based on empirically observed scores, thus providing a high correlation to their respective error rates. These improvements result in downstream advantages including improved genotyping accuracy.   AVAILABILITY AND IMPLEMENTATION FreeIbis is freely available for use under the GPL (http://bioinf.eva.mpg.de/freeibis/). It requires a Python interpreter and a C++ compiler. Tailored versions of LIBOCAS and LIBLINEAR are distributed along with the package.	a library for support vector machines;algorithm;biopolymer sequencing;c++;compiler;computation;computational resource;downstream (software development);genotype determination;il31ra gene;ibis (organism);massively-parallel sequencing;nucleotides;open-source software;python;support vector machine;version	Gabriel Renaud;Martin Kircher;Udo Stenzel;Janet Kelso	2013		10.1093/bioinformatics/btt117	support vector machine;real-time computing;calibration;computer science;bioinformatics;data mining	Comp.	-0.2554300536316618	-55.203513088852816	147058
b3516990fbb5e43dcb0b886a8fa14589f55851c5	crumble: reference free lossy compression of sequence quality values		Motivation The bulk of space taken up by NGS sequencing CRAM files consists of per-base quality values. Most of these are unnecessary for variant calling, offering an opportunity for space saving.   Results On the CHM1+CHM13 test set, a 17 fold reduction in the quality storage portion of a CRAM file can be achieved while maintaining variant calling accuracy. The size reduction of an entire CRAM file varied from 2.2 to 7.4 fold, depending on the non-quality content of the original file. See Supplementary Data section 6 for details.   Availability Crumble is OpenSource and can be obtained from https://github.com/jkbonfield/crumble.   Supplementary information Supplementary data are available at Bioinformatics.		James K. Bonfield;Shane A. McCarthy;Richard Durbin	2018		10.1093/bioinformatics/bty608	data mining;lossy compression;computer science;massive parallel sequencing;text mining;test set	Comp.	-0.9846295111988991	-54.18040211958178	148193
12c6bfd32bc2d4d283683e27555acd0ab310cb6c	status quo of annotation of human disease variants	high throughput nucleotide sequencing;molecular sequence annotation;databases genetic;genetic variation;computational biology bioinformatics;conserved sequence;article letter to editor;cluster analysis;proteins;genome human;exome;sequence homology amino acid;artificial intelligence;algorithms;humans;sequence alignment;combinatorial libraries;computational biology;computer appl in life sciences;polymorphism single nucleotide;mutation;microarrays;bioinformatics	The ever on-going technical developments in Next Generation Sequencing have led to an increase in detected disease related mutations. Many bioinformatics approaches exist to analyse these variants, and of those the methods that use 3D structure information generally outperform those that do not use this information. 3D structure information today is available for about twenty percent of the human exome, and homology modelling can double that fraction. This percentage is rapidly increasing so that we can expect to analyse the majority of all human exome variants in the near future using protein structure information. We collected a test dataset of well-described mutations in proteins for which 3D-structure information is available. This test dataset was used to analyse the possibilities and the limitations of methods based on sequence information alone, hybrid methods, machine learning based methods, and structure based methods. Our analysis shows that the use of structural features improves the classification of mutations. This study suggests strategies for future analyses of disease causing mutations, and it suggests which bioinformatics approaches should be developed to make progress in this field.	annotation;bioinformatics;exome;homology modeling;machine learning;massively-parallel sequencing;mutation;next-generation network;silo (dataset)	Hanka Venselaar;Franscesca Camilli;Shima Gholizadeh;Marlou Snelleman;Han G. Brunner;Gert Vriend	2012		10.1186/1471-2105-14-352	mutation;biology;dna microarray;computer science;bioinformatics;genetic variation;sequence alignment;exome;cluster analysis;conserved sequence;genetics	Comp.	1.1105468000941485	-54.974042660451765	148521
b78c2f4ab127de2f138f38d001431389e9889756	fast and accurate protein substructure searching with simulated annealing and gpus	paper;computer graphics;nvidia geforce gtx 285;molecular dynamics;structural biology;biology;simulated annealing;tesla c1060;computational biology bioinformatics;cuda;protein structure;proteins;protein structure tertiary;package;nvidia;graphic processing unit;algorithms;biological physics;parallel implementation;combinatorial libraries;computer appl in life sciences;computer simulation;structural similarity;databases protein;microarrays;bioinformatics	Searching a database of protein structures for matches to a query structure, or occurrences of a structural motif, is an important task in structural biology and bioinformatics. While there are many existing methods for structural similarity searching, faster and more accurate approaches are still required, and few current methods are capable of substructure (motif) searching. We developed an improved heuristic for tableau-based protein structure and substructure searching using simulated annealing, that is as fast or faster and comparable in accuracy, with some widely used existing methods. Furthermore, we created a parallel implementation on a modern graphics processing unit (GPU). The GPU implementation achieves up to 34 times speedup over the CPU implementation of tableau-based structure search with simulated annealing, making it one of the fastest available methods. To the best of our knowledge, this is the first application of a GPU to the protein structural search problem.	bioinformatics;cpu (central processing unit of computer system);central processing unit;computer graphics;fastest;graphics processing unit;heuristic;long division;method of analytic tableaux;motif;query language;question (inquiry);search problem;simulated annealing;speedup;structural similarity	Alex D. Stivala;Peter J. Stuckey;Anthony Wirth	2010		10.1186/1471-2105-11-446	computer simulation;biology;computational science;protein structure;molecular dynamics;dna microarray;simulated annealing;computer science;bioinformatics;theoretical computer science;structural similarity;structural biology;computer graphics;package	Comp.	-2.1126638564484237	-52.32822314562972	148886
c1575c355fe8bf5559581333ae73db35af2e8199	an architecture and application for integrating curation data at the residue level for proteins	protein family;amino acid;storage management;system design;multiple sequence alignment	Understanding protein families requires the bringing together of many different kinds of data. These families are typically derived from multiple sequence alignments. Directed mutagenesis is one of the most common means of inferring which specific amino acid or set of amino acids are important in the function of a protein. Although there are a large number publicly available, protein specific repositories, e.g., PROSITE, UniProt, and Pfam, no tools exist for experimental biologists that provide a means for managing and visualizing the curation data of the protein families they study at the individual residue level. We present the development of a novel system designed for experimental biologists, called the Curation Alignment Tool for Protein Analysis (CATPA), that allows for the efficient and effective creation, storage, management, and querying of experimentally curated protein families.	digital curation	Mehmet M. Dalkilic	2005		10.1007/11530084_34	amino acid;multiple sequence alignment;computer science;bioinformatics;data science;data mining;protein family;systems design	ML	-1.7101994097087179	-59.00065930259754	149000
3ee30310565b19d138e200abc3fbd355d48b4423	optimized design and assessment of whole genome tiling arrays	article letter to editor;oligonucleotide probe;functional genomics;optimal design;greedy algorithm;source code;data quality;quality measures;false positive;repetitive sequence;high throughput;melting temperature;open source	MOTIVATION Recent advances in microarray technologies have made it feasible to interrogate whole genomes with tiling arrays and this technique is rapidly becoming one of the most important high-throughput functional genomics assays. For large mammalian genomes, analyzing oligonucleotide tiling array data is complicated by the presence of non-unique sequences on the array, which increases the overall noise in the data and may lead to false positive results due to cross-hybridization. The ability to create custom microarrays using maskless array synthesis has led us to consider ways to optimize array design characteristics for improving data quality and analysis. We have identified a number of design parameters to be optimized including uniqueness of the probe sequences within the whole genome, melting temperature and self-hybridization potential.   RESULTS We introduce the uniqueness score, U, a novel quality measure for oligonucleotide probes and present a method to quickly compute it. We show that U is equivalent to the number of shortest unique substrings in the probe and describe an efficient greedy algorithm to design mammalian whole genome tiling arrays using probes that maximize U. Using the mouse genome, we demonstrate how several optimizations influence the tiling array design characteristics. With a sensible set of parameters, our designs cover 78% of the mouse genome including many regions previously considered 'untilable' due to the presence of repetitive sequence. Finally, we compare our whole genome tiling array designs with commercially available designs.   AVAILABILITY Source code is available under an open source license from http://www.ebi.ac.uk/~graef/arraydesign/.	dna microarray;data quality;functional genomics;genome;greedy algorithm;high-throughput computing;mammals;nucleic acid hybridization;open-source license;open-source software;repetitive region;short;source code;substring;throughput;tiling array;tiling window manager	Stefan Gräf;Fiona G. G. Nielsen;Stefan Kurtz;Martijn A. Huynen;Ewan Birney;Henk Stunnenberg;Paul Flicek	2007		10.1093/bioinformatics/btm200	functional genomics;high-throughput screening;biology;greedy algorithm;data quality;type i and type ii errors;computer science;bioinformatics;optimal design;oligomer restriction;theoretical computer science;data mining;statistics;source code	Comp.	0.5828049151619678	-56.434293259487575	149112
cadcfd72f13ed3a755870a043b100b2750313324	a priori assessment of data quality in molecular phylogenetics	physiological cellular and medical topics;computational biology bioinformatics;algorithms;bioinformatics	Sets of sequence data used in phylogenetic analysis are often plagued by both random noise and systematic biases. Since the commonly used methods of phylogenetic reconstruction are designed to produce trees it is an important task to evaluate these trees a posteriori. Preferably, however, one would like to assess the suitability of the input data for phylogenetic analysis a priori and, if possible, obtain information on how to prune the data sets to improve the quality of phylogenetic reconstruction without introducing unwarranted biases. In the last few years several different approaches, algorithms, and software tools have been proposed for this purpose. Here we provide an overview of the state of the art and briefly discuss the most pressing open problems.	algorithm;data quality;graph coloring;list of code lyoko episodes;molecular phylogenetics;noise (electronics)	Bernhard Misof;Karen Meusemann;Björn Marcus von Reumont;Patrick Kück;Sonja J. Prohaska;Peter F. Stadler	2014	Algorithms for Molecular Biology	10.1186/s13015-014-0022-4	biology;computer science;bioinformatics;data science;data mining	Comp.	2.0366906696157887	-53.79903157782189	149168
bd58b203074dbe5d5ae4d1d69b810523f22eba52	a noise-aware method for building radiation hybrid maps	bioin formatics;high resolution maps;radiation hybrid mapping;data mining;noisy datasets;clustering;bioinformatics	The large numbers of markers in high-resolution radiation hybrid (RH) maps, increasingly necessitates the use of data mining techniques for reducing both the computational complexity and the impact of noise of the original data. Traditionally, the RH mapping process has been treated as equivalent to the traveling salesman problem, with the correspondingly high computational complexity. These techniques are also susceptible to noise, and unreliable marker can result in major disruptions of the overall order. In this paper, we propose a new approach that recognizes that the focus on nearest-neighbor distances that characterizes the traveling-salesman model, is no longer appropriate for the large number of markers in modern high-resolution mapping experiments. The proposed approach splits the mapping process into two levels, where the higher level only operates on the most stable markers of the lower level. A divide and conquer strategy, which is applied at the lower level, removes much of the impact of noise. Because of the high density of markers, only the most stable representatives from the lower level are then used at the higher level. The groupings within the lower level are so small that exhaustive search can be used. Markers are then mapped iteratively, while excluding problematic markers. The results for RH mapping dataset of the human genome show that the proposed approach can construct high-resolution maps with high agreement with the physical maps in a comparatively very short time.	brute-force search;computation;computational complexity theory;data mining;experiment;image noise;image resolution;map;travelling salesman problem	Raed I. Seetan;Anne M. Denton;Omar Al Azzam;Ajay Kumar;Muhammad Javed Iqbal;Shahryar F. Kianian	2014		10.1145/2649387.2649443	computer science;bioinformatics;artificial intelligence;cluster analysis	Comp.	1.1901225957996215	-52.435003517281686	149485
358f7c78870799f03352c7385f9817783e5310b5	fish: a guide to protein-coding dna sequences in the genbank database	microordenador;software;secuencia aminoacido;basic;base donnee;proteine;sequence aminoacide;aminoacid sequence;logiciel;computerized processing;tratamiento informatico;information retrieval;implementation;estudio comparativo;database;gen;base dato;secuencia nucleotido;microordinateur;microcomputer;nucleotide sequence;sequence nucleotide;algorithme;etude comparative;algorithm;ejecucion;proteins;recherche information;comparative study;gene;genbank;logicial;proteina;recuperacion informacion;dna sequence;traitement informatique;algoritmo	FISH (Fast Index Search for Homologous coding sequences) consists of a database and associated software and is intended to function as a directory of protein-coding gene sequences. The FISH index contains descriptions of 22,361 DNA sequences from release 69.0 of the GenBank genetic sequence database. Complete coding sequences are represented numerically with counts of nucleotides and synonymous codons, and with GenBank LOCUS names and short descriptions. The software permits the database to be queried by GenBank LOCUS name, sequence length (expressed as total number of codons), or by comparison with a DNA sequence. In the latter case, the numerical descriptions are compared with simple distance measures in place of actual DNA sequences. The FISH package can be used to rapidly assemble lists of similar coding sequences, without regard to functional annotation or sequence alignments. Typical search times are well under a minute on widely available IBM-compatible microcomputers.	annotation;codon (nucleotide sequence);dna sequence;description;directory (computing);genbank;ibm pc compatible;inclusion body myositis (disorder);locus;license;microcomputer;microcomputers;name;nucleotides;numerical analysis;published directory;sequence alignment;sequence database	D. W. Collins	1993	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/9.3.337	biology;genbank;dna sequencing;nucleic acid sequence;computer science;bioinformatics;comparative research;gene;sequence database;microcomputer;refseq;implementation;genetics	Comp.	-4.377380818987702	-56.17609409008231	150056
e6c1f33014aebd70cd7baa6b0e236565ccd8234d	"""""""genotypecolour™"""": colour visualisation of snps and cnvs"""	software;high density;genotype;data interpretation;computer graphics;genetic variation;computational biology bioinformatics;chip;copy number;gene dosage;algorithms;user computer interface;combinatorial libraries;computer appl in life sciences;polymorphism single nucleotide;single nucleotide polymorphism;microarrays;bioinformatics	"""The volume of data available on genetic variations has increased considerably with the recent development of high-density, single-nucleotide polymorphism (SNP) arrays. Several software programs have been developed to assist researchers in the analysis of this huge amount of data, but few can rely upon a whole genome variability visualisation system that could help data interpretation. We have developed GenotypeColour™ as a rapid user-friendly tool able to upload, visualise and compare the huge amounts of data produced by Affymetrix Human Mapping GeneChips without losing the overall view of the data. Some features of GenotypeColour™ include visualising the entire genome variability in a single screenshot for one or more samples, the simultaneous display of the genotype and Copy Number state for thousands of SNPs, and the comparison of large amounts of samples by producing """"consensus"""" images displaying regions of complete or partial identity. The software is also useful for genotype analysis of trios and to show regions of potential uniparental disomy (UPD). All information can then be exported in a tabular format for analysis with dedicated software. At present, the software can handle data from 10 K, 100 K, 250 K, 5.0 and 6.0 Affymetrix chips. We have created a software that offers a new way of displaying and comparing SNP and CNV genomic data. The software is available free at http://www.med.unibs.it/~barlati/GenotypeColour and is especially useful for the analysis of multiple samples."""	academia (organization);affymetrix;algorithm;bio-informatics;bioinformatics;british informatics olympiad;cardiomyopathies;cone (formal languages);copy number polymorphism;hp universal print driver;hereditary diseases;home page;interface device component;manuscripts;microsoft access;microsoft jet database engine;microsoft windows;nitroprusside;nucleotides;operating system;owner's manual;programming languages;programming language;requirement;sandy bridge;screenshot;single nucleotide polymorphism;single-chain antibodies;software development;spatial variability;table (information);uniparental disomy;upload;usability;visual basic;explanation	Sergio Barlati;Sergio Chiesa;Chiara Magri	2008	BMC Bioinformatics	10.1186/1471-2105-10-49	single-nucleotide polymorphism;chip;biology;gene dosage;dna microarray;bioinformatics;theoretical computer science;genetic variation;genotype;copy-number variation;data analysis;computer graphics;genetics	Comp.	-2.896651558033075	-58.24816934534938	150200
9c811477193d1779309ec8f7a6620604f346e247	cigenotyper: a machine learning approach for genotyping complex indel calls		Complex insertion and deletion (complex indel) is a rare category of genomic structural variations. A complex indel presents as one or multiple DNA fragments inserted into the genomic location where a deletion occurs. Several studies emphasize the importance of complex indels, and some state-of-the-art approaches are proposed to detect them from sequencing data. However, genotyping complex indel calls is another challenged computational problem because some commonly used features for genotyping indel calls from the sequencing data could be invalid due to the components of complex indels. Thus, in this article, we propose a machine learning approach, CIGenotyper to estimate genotypes of complex indel calls. CIGenotyper adopts a relevance vector machine (RVM) framework. For each candidate call, it first extracts a set of features from the candidate region, which usually includes the read depth, the variant allelic frequency for aligned contigs, the numbers of the splitting and discordant paired-end reads, etc. For a complex indel call, given its features to a trained RVM, the model outputs the genotype with highest likelihood. An algorithm is also proposed to train the RVM. We compare our approach to two popular approaches, Gindel and Pindel, on multiple groups of artificial datasets. The results of our model outperforms them on average success rates in most of the cases when vary the coverages of the given data, the read lengths and the distributions of the lengths of the pre-set complex indels.	machine learning	Tian Zheng;Yang Li;Yu Geng;Zhongmeng Zhao;Xuanping Zhang;Xiao Xiao;Jiayin Wang	2018		10.1007/978-3-319-78723-7_41	computational problem;engineering;genotyping;allele frequency;machine learning;genotype;relevance vector machine;indel;contig;artificial intelligence	NLP	0.30131727005441383	-53.47918453253511	150338
1c64427bee3b0d7913591d3c1d762a09443f5125	stamp: statistical analysis of taxonomic and functional profiles	software;cianobacterias;data interpretation statistical;confidence intervals;bacterias;classification;humanos;genoma bacteriano;genome bacterial;humans;interpretacao estatistica de dados;bacteria;intervalos de confianca;classificacao;cyanobacteria	UNLABELLED STAMP is a graphical software package that provides statistical hypothesis tests and exploratory plots for analysing taxonomic and functional profiles. It supports tests for comparing pairs of samples or samples organized into two or more treatment groups. Effect sizes and confidence intervals are provided to allow critical assessment of the biological relevancy of test results. A user-friendly graphical interface permits easy exploration of statistical results and generation of publication-quality plots.   AVAILABILITY AND IMPLEMENTATION STAMP is licensed under the GNU GPL. Python source code and binaries are available from our website at: http://kiwi.cs.dal.ca/Software/STAMP.		Donovan H. Parks;Gene W. Tyson;Philip Hugenholtz;Robert G. Beiko	2014	Bioinformatics	10.1093/bioinformatics/btu494	biology;confidence interval;bacteria;biological classification;computer science;bioinformatics;data mining;genetics;statistics	Comp.	-2.780169668222205	-56.812631500519345	150503
1ad16bed71897132fdc020d2d96ab1485ee4c34e	mixture models for analysis of the taxonomic composition of metagenomes	phylogeny;composition;composicion;sequence analysis dna;analyse;metagenome;modelo;models genetic;metagenomics;algorithms;teoria mezcla;analysis;modele;mixture theory;theorie melange;models;analisis	MOTIVATION Inferring the taxonomic profile of a microbial community from a large collection of anonymous DNA sequencing reads is a challenging task in metagenomics. Because existing methods for taxonomic profiling of metagenomes are all based on the assignment of fragmentary sequences to phylogenetic categories, the accuracy of results largely depends on fragment length. This dependence complicates comparative analysis of data originating from different sequencing platforms or resulting from different preprocessing pipelines.   RESULTS We here introduce a new method for taxonomic profiling based on mixture modeling of the overall oligonucleotide distribution of a sample. Our results indicate that the mixture-based profiles compare well with taxonomic profiles obtained with other methods. However, in contrast to the existing methods, our approach shows a nearly constant profiling accuracy across all kinds of read lengths and it operates at an unrivaled speed.   AVAILABILITY A platform-independent implementation of the mixture modeling approach is available in terms of a MATLAB/Octave toolbox at http://gobics.de/peter/taxy. In addition, a prototypical implementation within an easy-to-use interactive tool for Windows can be downloaded.	categories;gnu octave;matlab;metagenome;microsoft windows;mixture model;phylogenetics;pipeline (computing);preprocessor;profiling (computer programming);qualitative comparative analysis	Peter Meinicke;Kathrin Petra Aßhauer;Thomas Lingner	2011		10.1093/bioinformatics/btr266	biology;bioinformatics;analysis;data mining;metagenomics	Comp.	-2.9427774873768024	-55.85420810386151	150988
a3a7322f097f119d44f09fa10d53a3ba6beb4191	analysis of evolutionary patterns from huge dna sequence data	dna sequence			Albert J. Vilella;Angel Blanco-Garcia;Stephan Hutter;Julio Rozas	2004			sequencing by hybridization;consensus sequence;data mining;sequence assembly;alignment-free sequence analysis;sequence analysis;computer science;dna sequencing;bioinformatics	ML	-0.510858367334621	-56.04616517724111	151652
44aade3e19c484d000df66fb5907280376c0744f	scan for motifs: a webserver for the analysis of post-transcriptional regulatory elements in the 3′ untranslated regions (3′ utrs) of mrnas	software;animals;rna messenger;computational biology bioinformatics;rna stability;3 untranslated regions;algorithms;regulatory elements transcriptional;humans;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Gene expression in vertebrate cells may be controlled post-transcriptionally through regulatory elements in mRNAs. These are usually located in the untranslated regions (UTRs) of mRNA sequences, particularly the 3′UTRs. Scan for Motifs (SFM) simplifies the process of identifying a wide range of regulatory elements on alignments of vertebrate 3′UTRs. SFM includes identification of both RNA Binding Protein (RBP) sites and targets of miRNAs. In addition to searching pre-computed alignments, the tool provides users the flexibility to search their own sequences or alignments. The regulatory elements may be filtered by expected value cutoffs and are cross-referenced back to their respective sources and literature. The output is an interactive graphical representation, highlighting potential regulatory elements and overlaps between them. The output also provides simple statistics and links to related resources for complementary analyses. The overall process is intuitive and fast. As SFM is a free web-application, the user does not need to install any software or databases. Visualisation of the binding sites of different classes of effectors that bind to 3′UTRs will facilitate the study of regulatory elements in 3′ UTRs.	3' untranslated regions;5' untranslated regions;binding sites;class;dna binding site;database;gene expression;graphical user interface;micrornas;precomputation;regulatory elements, transcriptional;scanning probe microscopes (device);transcription, genetic;web application;web server	Ambarish Biswas;Chris M. Brown	2014		10.1186/1471-2105-15-174	computational biology;biology;au-rich element;dna microarray;computer science;bioinformatics;three prime untranslated region	Comp.	-1.2078578118971726	-58.59304405717933	151927
5cd6cfeed9df309a30e7ffb51506d8d23f23a4c5	compareads: comparing huge metagenomic experiments	bloom filter;computational biology bioinformatics;metagenomics;algorithms;ngs;combinatorial libraries;computer appl in life sciences;information storage and retrieval;microarrays;bioinformatics	"""Nowadays, metagenomic sample analyses are mainly achieved by comparing them with a priori knowledge stored in data banks. While powerful, such approaches do not allow to exploit unknown and/or """"unculturable"""" species, for instance estimated at 99% for Bacteria. This work introduces Compareads, a de novo comparative metagenomic approach that returns the reads that are similar between two possibly metagenomic datasets generated by High Throughput Sequencers. One originality of this work consists in its ability to deal with huge datasets. The second main contribution presented in this paper is the design of a probabilistic data structure based on Bloom filters enabling to index millions of reads with a limited memory footprint and a controlled error rate. We show that Compareads enables to retrieve biological information while being able to scale to huge datasets. Its time and memory features make Compareads usable on read sets each composed of more than 100 million Illumina reads in a few hours and consuming 4 GB of memory, and thus usable on today's personal computers. Using a new data structure, Compareads is a practical solution for comparing de novo huge metagenomic samples. Compareads is released under the CeCILL license and can be freely downloaded from http://alcovna.genouest.org/compareads/ ."""	bank (environment);bloom filter;data structure;de novo transcriptome assembly;experiment;memory footprint;metagenomics;ninety nine;personal computers;personal computer;reading (activity);throughput	Nicolas Maillet;Claire Lemaitre;Rayan Chikhi;Dominique Lavenier;Pierre Peterlongo	2012		10.1186/1471-2105-13-S19-S10	biology;dna microarray;computer science;bioinformatics;data science;bloom filter;data mining;metagenomics	Comp.	-0.9940016867836113	-54.5486629413482	152326
9e99f45cf9607d20fde0db2c2e35de2b8d2626da	oqtans: the rna-seq workbench in the cloud for complete and reproducible quantitative transcriptome analysis	software;internet;rna;transcriptome;sequence analysis rna;base sequence	We present Oqtans, an open-source workbench for quantitative transcriptome analysis, that is integrated in Galaxy. Its distinguishing features include customizable computational workflows and a modular pipeline architecture that facilitates comparative assessment of tool and data quality. Oqtans integrates an assortment of machine learning-powered tools into Galaxy, which show superior or equal performance to state-of-the-art tools. Implemented tools comprise a complete transcriptome analysis workflow: short-read alignment, transcript identification/quantification and differential expression analysis. Oqtans and Galaxy facilitate persistent storage, data exchange and documentation of intermediate results and analysis workflows. We illustrate how Oqtans aids the interpretation of data from different experiments in easy to understand use cases. Users can easily create their own workflows and extend Oqtans by integrating specific tools. Oqtans is available as (i) a cloud machine image with a demo instance at cloud.oqtans.org, (ii) a public Galaxy instance at galaxy.cbio.mskcc.org, (iii) a git repository containing all installed software (oqtans.org/git); most of which is also available from (iv) the Galaxy Toolshed and (v) a share string to use along with Galaxy CloudMan.	base sequence;cloud computing;data quality;documentation;emoticon;experiment;galaxy;gene expression profiling;machine learning;open-source software;persistence (computer science);pipeline (computing);power (psychology);quantitation;rna;transcript;transcriptome;workbench	Vipin T. Sreedharan;Sebastian J. Schultheiß;Géraldine Jean;André Kahles;Regina Bohnert;Philipp Drewe;Pramod Mudrakarta;Nico Görnitz;Georg Zeller;Gunnar Rätsch	2014		10.1093/bioinformatics/btt731	biology;the internet;rna;transcriptome;computer science;bioinformatics;data mining;world wide web;genetics	HPC	-2.194398019644594	-58.24528424184863	153626
fc5ac6b69832d93de40d35071ba43e2dc5d9728d	bayesian inference of phylogenetic networks from bi-allelic genetic markers		Phylogenetic networks are rooted, directed, acyclic graphs that model reticulate evolutionary histories. Recently, statistical methods were devised for inferring such networks from either gene tree estimates or the sequence alignments of multiple unlinked loci. Bi-allelic markers, most notably single nucleotide polymorphisms (SNPs) and amplified fragment length polymorphisms (AFLPs), provide a powerful source of genome-wide data. In a recent paper, a method called SNAPP was introduced for statistical inference of species trees from unlinked bi-allelic markers. The generative process assumed by the method combined both a model of evolution for the bi-allelic markers, as well as the multispecies coalescent. A novel component of the method was a polynomial-time algorithm for exact computation of the likelihood of a fixed species tree via integration over all possible gene trees for a given marker. Here we report on a method for Bayesian inference of phylogenetic networks from bi-allelic markers. Our method significantly extends the algorithm for exact computation of phylogenetic network likelihood via integration over all possible gene trees. Unlike the case of species trees, the algorithm is no longer polynomial-time on all instances of phylogenetic networks. Furthermore, the method utilizes a reversible-jump MCMC technique to sample the posterior of phylogenetic networks given bi-allelic marker data. Our method has a very good performance in terms of accuracy and robustness as we demonstrate on simulated data, as well as a data set of multiple New Zealand species of the plant genus Ourisia (Plantaginaceae). We implemented the method in the publicly available, open-source PhyloNet software package.	assumed;computation (action);directed acyclic graph;directed graph;estimated;genetic markers;genetic polymorphism;genus;graph - visual representation;inference;models of dna evolution;nucleotides;open-source software;phylogenetic network;phylogenetics;plantaginaceae;polynomial;reversible-jump markov chain monte carlo;sequence alignment;single nucleotide polymorphism;time complexity;trees (plant);algorithm	Jiafan Zhu;Dingqiao Wen;Yun Yu;Heidi M. Meudt;Luay Nakhleh	2018		10.1371/journal.pcbi.1005932	biology;bioinformatics;computational phylogenetics;phylogenetic network;genetics;statistics	Comp.	2.364781717099577	-52.22653183780962	153953
7f31c3a27dec800ee46b7918058354892c2283a2	a dna-based memory with in vitro learning and associative recall	sequence similarity;storage capacity;biological data;dna sequence	  A DNA-based memory was implemented with in vitro learning and associative recall. The learning protocol stores the sequences to which it is exposed, and memories are recalled  by sequence content through DNA-to-DNA template annealing reactions. Experiments demonstrated that biological DNA could be  learned, that sequences similar to the training DNA were recalled correctly, and that unlike sequences were differentiated.  Theoretical estimates indicate that the memory has a pattern separation capability that is very large, and that it can learn  long DNA sequences. The learning and recall protocols are massively parallel, as well as simple, inexpensive, and quick. The  memory has several potential applications in detection and classification of biological sequences, as well as a massive storage  capacity for non-biological data.    		Junghuei Chen;Russell J. Deaton;Yu-Zhen Wang	2003		10.1007/978-3-540-24628-2_14	natural language processing;machine learning;communication	ML	-0.4940743893555902	-54.95667159894933	154077
3d32baa6a364b84ad5dbc307c650a87ace640810	learning from evolution to predict protein structure	prediction method;genome analysis;protein structure;molecular biology;secondary structure;structure determination;solvent accessibility;data flow;neural network	In the wake of the genome data flow, we need more urgently than ever accurate tools to predict protein structure. The probl em of predicting protein structure from sequence remains fundamentally unsolved despite more than three decades of intensive research effo t. However, the wealth of evolutionary information deposited in current databases enabled a significant improvement for methods predicting prote n structure in 1D: secondary structure, transmembrane helices, and solvent accessibility. In particular, the combination of evolutionary infor mation with neural networks proved extremely successful. The new generation of prediction methods proved to be accurate and reliable enough to be useful in genome analysis, and in experimental structure determination. Moreover, the new generation of theoretical methods is increasingly infl uencing experiments in molecular biology.	accessibility;artificial neural network;database;dataflow;em (typography);experiment;protein structure prediction	Burkhard Rost	1997			biology;bioinformatics;data mining	Comp.	2.0815747772018622	-58.320672096349966	154386
5118a2dba2db5aafe337da248a37f2b5955f2f27	gnu mcsim: bayesian statistical inference for sbml-coded systems biology models	bayes estimation;bayesian statistics;systems biology;langage sbml;biologia de sistemas;modelo;estimacion bayes;inferencia;system biology;modele;biologie systemique;models;inference;estimation bayes	SUMMARY Statistical inference about the parameter values of complex models, such as the ones routinely developed in systems biology, is efficiently performed through Bayesian numerical techniques. In that framework, prior information and multiple levels of uncertainty can be seamlessly integrated. GNU MCSim was precisely developed to achieve those aims, in a general non-linear differential context. Starting with version 5.3.0, GNU MCSim reads in and simulates Systems Biology Markup Language models. Markov chain Monte Carlo simulations can be used to generate samples from the joint posterior distribution of the model parameters, given a dataset and prior distributions. Hierarchical statistical models can be used. Optimal design of experiments can also be investigated.   AVAILABILITY AND IMPLEMENTATION The GNU GPL source is available at (http://savannah.gnu.org/projects/mcsim). A distribution package is at (http://www.gnu.org/software/mcsim). GNU MCSim is written in standard C and runs on any platform supporting a C compiler. Supplementary Material is available online at (http://www.gnu.org/software/mcsim).	ansi c;cellular material:mcnt:pt:calculus:qn:estimated;compiler;design of experiments;experiment;gnu;il31ra gene;inference;language model;mcsim;markov chain monte carlo;markup language;monte carlo method;nonlinear system;numerical analysis;optimal design;population parameter;sbml;silo (dataset);simulation;statistical model;systems biology	Frédéric Y. Bois	2009	Bioinformatics	10.1093/bioinformatics/btp162	biology;econometrics;computer science;bioinformatics;data mining;bayesian statistics;systems biology;statistics	Comp.	-3.970938522554686	-54.46567371006248	154600
fa16494de8f26371f9da9cf2984508476b2df4a7	zomit: biological data visualization and browsing	software;base donnee;navegacion informacion;red www;visualizacion;logiciel;computerized processing;tratamiento informatico;recoleccion dato;data gathering;navigation information;aplicacion cliente servidor;information browsing;biologia molecular;database;base dato;application program interface;very large database;semantic zooming;visualization;visualisation;molecular biology;application client serveur;data transformation;world wide web;logicial;reseau www;biological data;genome mapping;biological database;collecte donnee;traitement informatique;client server application;biologie moleculaire	MOTIVATION The problems caused by the difficulty in visualizing and browsing biological databases have become crucial. Scientists can no longer interact directly with the huge amount of available data. However, future breakthroughs in biology depend on this interaction. We propose a new metaphor for biological data visualization and browsing that allows navigation in very large databases in an intuitive way. The concepts underlying our approach are based on navigation and visualization with zooming, semantic zooming and portals; and on data transformation via magic lenses. We think that these new visualization and navigation techniques should be applied globally to a federation of biological databases.   RESULTS We have implemented a generic tool, called Zomit, that provides an application programming interface for developing servers for such navigation and visualization, and a generic architecture-independent client (Javatrade mark applet) that queries such servers. As an illustration of the capabilities of our approach, we have developed ZoomMap, a prototype browser for the HuGeMap human genome map database.   AVAILABILITY Zomit and ZoomMap are available at the URL http://www.infobiogen.fr/services/zomit.	applet;application programming interface;biological data visualization;biological database;generic drugs;imagery;interface device component;lens (device);navigation;portals;prototype;published database;uniform resource locator;zooming user interface	Stuart Pook;Guy Vaysseix;Emmanuel Barillot	1998	Bioinformatics	10.1093/bioinformatics/14.9.807	visualization;computer science;bioinformatics;data mining;database;world wide web	Visualization	-4.470467040478261	-57.73631939431274	155356
5d67894c4f2d89d0ae2d3e653bb26db95951d821	interactive computer programs for the graphic analysis of nucleotide sequence data	dna;computers;software;graphic method;computer program;basic;proteine;palindromic sequence;methods;structure secondaire;computerized processing;tratamiento informatico;sequence palindromique;amino acid sequence;plasmids;microordinateur;microcomputer;nucleotide sequence;systeme conversationnel;sequence nucleotide;primary structure;estructura primaria;methode graphique;proteins;interactive system;secondary structure;hewlett packard;nucleic acid conformation;langage hpl;proteina;acido nucleico;base sequence;acide nucleique;description;nucleic acid;traitement informatique;microcomputers;programme ordinateur;structure primaire	A group of interactive computer programs have been developed which aid in the collection and graphical analysis of nucleotide and protein sequence data. The programs perform the following basic functions: a) enter, edit, list, and rearrange sequence data; b) permit automatic entry of nucleotide sequence data directly from an autoradiograph into the computer; c) search for restriction sites or other specified patterns and plot a linear or circular restriction map, or print their locations; d) plot base composition; e) analyze homology between sequences by plotting a two-dimensional graphic matrix; and f) aid in plotting predicted secondary structures of RNA molecules.	autoradiographic image;base composition;base sequence;computer program;emoticon;homologous gene;homology (biology);nucleotides;restriction mapping	V. A. Luckow;R. K. Littlewood;R. H. Rownd	1984	Nucleic acids research	10.1093/nar/12.1Part2.665	biology;bioinformatics;microcomputer;genetics	ML	-4.5026877028290615	-56.64995289540281	155364
9d0bb7c0a614be2a7f444d5eed4f682dd92a7217	ribostral: an rna 3d alignment analyzer and viewer based on basepair isostericities	evolution molecular;software;base pairing;imaging three dimensional;computer graphics;my publications;base pair mismatch;models chemical;models molecular;internet;rna;basepair isostericities;3d alignment;nucleic acid conformation;algorithms;molecular sequence data;sequence alignment;user computer interface;sequence analysis rna;base sequence;computer simulation;atomic resolution;structure alignment	UNLABELLED RNA atomic resolution structures have revealed the existance of different families of basepair interactions, each of which with its own isosteric sub-families. Ribostral (Ribonucleic Structural Aligner) is a user-friendly framework for analyzing, evaluating and viewing RNA sequence alignments with at least one available atomic resolution structure. It is the first of its kind that makes direct and easy- to-understand superposition of the isostericity matrices of basepairs observed in the structure onto sequence alignments, easily indicating allowed and unallowed substitutions at each BP position. Potential mistakes in the alignments can then be corrected using other sequence editing software. Ribostral has been developed and tested under Windows XP, and is capable of running on any PC or MAC platform with MATLAB 7.1 (SP3) or higher installed version. A stand-alone version is also available for the PC platform.   AVAILABILITY http://rna.bgsu.edu/ribostral.	analyzer device component;interaction;matlab;microsoft windows;personal computer;rna;sequence alignment;usability	Ali Mokdad;Neocles B Leontis	2006	Bioinformatics	10.1093/bioinformatics/btl360	computer simulation;biology;structural alignment;the internet;rna;base pair;computer science;bioinformatics;theoretical computer science;sequence alignment;computer graphics;world wide web;genetics	Comp.	-4.515962984830744	-57.85153729363819	155526
8bd38f27d5aee704cb20faec2f05d8d0434f7927	celltracker (not only) for dummies	qh301 biology biologia	MOTIVATION Time-lapse experiments play a key role in studying the dynamic behavior of cells. Single-cell tracking is one of the fundamental tools for such analyses. The vast majority of the recently introduced cell tracking methods are limited to fluorescently labeled cells. An equally important limitation is that most software cannot be effectively used by biologists without reasonable expertise in image processing. Here we present CellTracker, a user-friendly open-source software tool for tracking cells imaged with various imaging modalities, including fluorescent, phase contrast and differential interference contrast (DIC) techniques.   AVAILABILITY AND IMPLEMENTATION CellTracker is written in MATLAB (The MathWorks, Inc., USA). It works with Windows, Macintosh and UNIX-based systems. Source code and graphical user interface (GUI) are freely available at: http://celltracker.website/   CONTACT horvath.peter@brc.mta.hu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Filippo Piccinini;Alexa Kiss;Péter Horváth	2016	Bioinformatics	10.1093/bioinformatics/btv686	biology;simulation;computer science;bioinformatics;computer graphics (images)	Comp.	-4.477532630248491	-57.76961954824145	156024
5e6d554882cd7f21116eaa98db7a52c7dc5be043	idna4mc: identifying dna n4-methylcytosine sites based on nucleotide chemical properties		Motivation DNA N4-methylcytosine (4mC) is an epigenetic modification. The knowledge about the distribution of 4mC is helpful for understanding its biological functions. Although experimental methods have been proposed to detect 4mC sites, they are expensive for performing genome-wide detections. Thus, it is necessary to develop computational methods for predicting 4mC sites.   Results In this work, we developed iDNA4mC, the first webserver to identify 4mC sites, in which DNA sequences are encoded with both nucleotide chemical properties and nucleotide frequency. The predictive results of the rigorous jackknife test and cross species test demonstrated that the performance of iDNA4mC is quite promising and holds high potential to become a useful tool for identifying 4mC sites.   Availability and implementation The user-friendly web-server, iDNA4mC, is freely accessible at http://lin.uestc.edu.cn/server/iDNA4mC.   Contact chenweiimu@gmail.com or hlin@uestc.edu.cn.		Wei Chen;Hui Yang;Pengmian Feng;Hui Ding;Hao Lin	2017	Bioinformatics	10.1093/bioinformatics/btx479	dna;nucleotide;computer science;bioinformatics	Comp.	0.6221496779008912	-58.331689789917114	156245
3124e774a346355d5da4de64e8d2b86227e0bf37	gene family identification network design for protein sequence analysis	network design;counter propagation;protein family database;artificial neural networks;protein classification;gene family;protein sequence analysis;sequence motifs;back propagation	With the exponential accumulation of sequence data, continued progress in the Human Genome Project will depend increasingly on advanced computational tools to manage and analyze the data. Utilizing information embedded within families of homologous sequences, a gene family identification approach may facilitate the understanding of gene functions. We have developed a GeneFIND (Gene Family Identification Network Design) system for database searching against gene families. It provides rapid and accurate protein family identification by combining global and motif sequence similarities and incorporating ProClass family information. Multi-level filters are used, starting with the MOTIFIND neural networks and BLAST search, followed by SSEARCH alignment, motif pattern match, hidden Markov modeling of motifs and ClustalW motif alignment. GeneFIND has been implemented as a full-scale system for the classification of more than 1200 ProSite and 6000 PIR families. It has been used to identify thousands of new family members and is well suited for genomic sequence analysis. The system is available for on-line family identification from our WWW server ().		Cathy H. Wu;Hongzhan Huang;Jerry McLarty	1999	International Journal on Artificial Intelligence Tools	10.1142/S0218213099000282	network planning and design;computer science;bioinformatics;backpropagation;gene family;machine learning;data mining;artificial neural network;alignment-free sequence analysis;sequence motif	EDA	1.8595347960068607	-58.51433829917583	156564
4b05a04bf6a9534ea7e9ca18f24029b0b49751a0	towards efficient searching on the secondary structure of protein sequences	and approximate searching;protein sequence;secondary structure of proteins;indexing method;secondary structure;approximate searching	Approximate searching on the primary structure (i.e., amin o acid arrangement) of protein sequences is an essential part in predicting the functions a nd evolutionary histories of proteins. However, because proteins distant in an evolutionary history d not conserve amino acid residue arrangements, approximate searching on proteins’ secondary struc ture is quite important in finding out distant homology. In this paper, we propose an indexing scheme f or fficient approximate searching on the secondary structure of protein sequences which can be ea sily implemented in RDBMS. Exploiting the concept of clusteringandlookahead, the proposed indexing scheme processes three types of secondary structure queries (i.e., exact match, range matc h, nd wildcard match) very quickly. To evaluate the performance of the proposed method, we conduct ed ex ensive experiments using a set of actual protein sequences. According to the experimental results, the proposed method was proved to be faster than the existing indexing methods up to 6.3 time s in exact match, 3.3 times in range match, and 1.5 times in wildcard match, respectively.	approximation algorithm;experiment;homology (biology);match report;peptide sequence	Minkoo Seo;Sanghyun Park;Jung-Im Won	2007	Fundam. Inform.		combinatorics;bioinformatics;theoretical computer science;mathematics;protein secondary structure	Comp.	-2.4588037440563393	-52.36967101621076	157006
192f01ea382237b2a059d6cdee2cdd74e05b292a	poster: an ultra-fast computing pipeline for metagenome analysis with next-generation dna sequencers	metagenomics;metagenome analysis;large-scale computing pipeline;sensitive sequence homology search;symbiotic system;environment pollution;ultra-fast computing pipeline;next-generation dna sequencers;current metagenome analysis;gpgpu;large computation time;latest dna;next-generation sequencer;genomics;dna	Metagenome analysis is useful for not only understanding symbiotic systems but also watching environment pollutions. However, metagenome analysis requires sensitive sequence homology searches which require large computation time and it is thus a bottleneck in current metagenome analysis based on the data from the latest DNA sequencers generally called a next-generation sequencer. To solve the problem, we developed a large-scale computing pipeline for metagenome analysis on TSUBAME2.	metagenomics	Shuji Suzuki	2012		10.1109/SC.Companion.2012.321	computational biology;genomics;bioinformatics;metagenomics;dna;general-purpose computing on graphics processing units	HPC	-1.9314929077875493	-54.09467172421496	157150
73d3006f7e41ea084b464f868091564432c46e28	balibase: a benchmark alignment database for the evaluation of multiple alignment programs	secuencia aminoacido;base donnee;alignement sequence;proteine;sequence aminoacide;aminoacid sequence;computerized processing;tratamiento informatico;database;base dato;methode;alineacion secuencia;proteina;sequence alignment;protein;metodo;traitement informatique;method;multiple alignment	SUMMARY BAliBASE is a database of manually refined multiple sequence alignments categorized by core blocks of conservation sequence length, similarity, and the presence of insertions and N/C-terminal extensions.   AVAILABILITY From http://www-igbmc. u-strasbg.fr/BioInfo/BAliBASE/index.html	benchmark (computing);categorization;clinical act of insertion;multiple sequence alignment;newton	Julie Dawn Thompson;Frédéric Plewniak;Olivier Poch	1999	Bioinformatics	10.1093/bioinformatics/15.1.87	method;multiple sequence alignment;computer science;bioinformatics;sequence alignment;algorithm	Comp.	-4.177050625693162	-55.98474924801142	157241
1f5db5753084609d6ad15b4716d905ab6cd9fc0e	srmapper: a fast and sensitive genome-hashing alignment tool	computer processor day;default number;outputs alignment;alignment quality;higher number;large number;base pair;new alignment tool;base space fastq format;current program	UNLABELLED Modern sequencing instruments have the capability to produce millions of short reads every day. The large number of reads produced in conjunction with variations between reads and reference genomic sequences caused both by legitimate differences, such as single-nucleotide polymorphisms and insertions/deletions (indels), and by sequencer errors make alignment a difficult and computationally expensive task, and many reads cannot be aligned. Here, we introduce a new alignment tool, SRmapper, which in tests using real data can align 10s of billions of base pairs from short reads to the human genome per computer processor day. SRmapper tolerates a higher number of mismatches than current programs based on Burrows-Wheeler transform and finds about the same number of alignments in 2-8× less time depending on read length (with higher performance gain for longer read length). The current version of SRmapper aligns both single and pair-end reads in base space fastq format and outputs alignments in Sequence Alignment/Map format. SRmapper uses a probabilistic approach to set a default number of mismatches allowed and determines alignment quality. SRmapper's memory footprint (∼2.5 GB) is small enough that it can be run on a computer with 4 GB of random access memory for a genome the size of a human. Finally, SRmapper is designed so that its function can be extended to finding small indels as well as long deletions and chromosomal translocations in future versions.   AVAILABILITY http://www.umsl.edu/∼wongch/software.html.	align (company);analysis of algorithms;base pairing;biopolymer sequencing;burrows–wheeler transform;central processing unit;chromosomal translocation;clinical act of insertion;default;fastq format;genetic polymorphism;hash function;instrument - device;memory disorders;memory footprint;microsequencer;nucleotides;random access memory device component;random-access memory;reading (activity);samtools;sequence alignment;supernumerary maxillary left lateral incisor;tympanostomy tube insertion;version	Paul M. Gontarz;Jennifer Berger;Chung F. Wong	2013	Bioinformatics	10.1093/bioinformatics/bts712	computer science;bioinformatics;data mining;world wide web	Comp.	-0.9931549535159913	-54.39275154690514	157644
47733b1a0719d93b38386dd0fb48304c156603c9	an accurate and rapid continuous wavelet dynamic time warping algorithm for end-to-end mapping in ultra-long nanopore sequencing		Motivation Long-reads, point-of-care and polymerase chain reaction-free are the promises brought by nanopore sequencing. Among various steps in nanopore data analysis, the end-to-end mapping between the raw electrical current signal sequence and the reference expected signal sequence serves as the key building block to signal labeling, and the following signal visualization, variant identification and methylation detection. One of the classic algorithms to solve the signal mapping problem is the dynamic time warping (DTW). However, the ultra-long nanopore sequencing and an order of magnitude difference in the sampling speed complexify the scenario and make the classical DTW infeasible to solve the problem.   Results Here, we propose a novel multi-level DTW algorithm, continuous wavelet DTW (cwDTW), based on continuous wavelet transforms with different scales of the two signal sequences. Our algorithm starts from low-resolution wavelet transforms of the two sequences, such that the transformed sequences are short and have similar sampling rates. Then the peaks and nadirs of the transformed sequences are extracted to form feature sequences with similar lengths, which can be easily mapped by the original DTW. Our algorithm then recursively projects the warping path from a lower-resolution level to a higher-resolution one by building a context-dependent boundary and enabling a constrained search for the warping path in the latter. Comprehensive experiments on two real nanopore datasets on human and on Pandoraea pnomenusa demonstrate the efficiency and effectiveness of the proposed algorithm. In particular, cwDTW can gain remarkable acceleration with tiny loss of the alignment accuracy. On the real nanopore datasets, cwDTW can finish an alignment task in few seconds, which is about 3000 times faster than the original DTW. By successfully applying cwDTW on the tasks of signal labeling and ultra-long sequence comparison, we further demonstrate the power and applicability of cwDTW.   Availability and implementation Our program is available at https://github.com/realbigws/cwDTW.   Supplementary information Supplementary data are available at Bioinformatics online.	alignment;bioinformatics;biopolymer sequencing;context-sensitive language;continuous wavelet;dynamic time warping;end-to-end principle;experiment;extraction;geographic information systems;imagery;numerous;polymerase chain reaction;recursion;sampling (signal processing);sampling - surgical action;signal peptides;wavelet transform;algorithm;mapped	Renmin Han;Yu Li;Xin Gao;Sheng Wang	2018	Bioinformatics	10.1093/bioinformatics/bty555	computer vision;nanopore sequencing;computer science;continuous wavelet;dynamic time warping;end-to-end principle;artificial intelligence;bioinformatics	Comp.	-0.9420053518592492	-53.815173930935885	158412
2c1d7b43968e452729564b356c9a2ead68641921	perf: an exhaustive algorithm for ultra-fast and efficient identification of microsatellites from large dna sequences		Motivation Microsatellites or Simple Sequence Repeats (SSRs) are short tandem repeats of DNA motifs present in all genomes. They have long been used for a variety of purposes in the areas of population genetics, genotyping, marker-assisted selection and forensics. Numerous studies have highlighted their functional roles in genome organization and gene regulation. Though several tools are currently available to identify SSRs from genomic sequences, they have significant limitations.   Results We present a novel algorithm called PERF for extremely fast and comprehensive identification of microsatellites from DNA sequences of any size. PERF is several fold faster than existing algorithms and uses up to 5-fold lesser memory. It provides a clean and flexible command-line interface to change the default settings, and produces output in an easily-parseable tab-separated format. In addition, PERF generates an interactive and stand-alone HTML report with charts and tables for easy downstream analysis.   Availability and implementation PERF is implemented in the Python programming language. It is freely available on PyPI under the package name perf_ssr, and can be installed directly using pip or easy_install. The documentation of PERF is available at https://github.com/rkmlab/perf. The source code of PERF is deposited in GitHub at https://github.com/rkmlab/perf under an MIT license.   Contact tej@ccmb.res.in.   Supplementary information Supplementary data are available at Bioinformatics online.		Akshay Kumar Avvaru;Divya Tej Sowpati;Rakesh Kumar Mishra	2018	Bioinformatics	10.1093/bioinformatics/btx721	computer science;microsatellite;bioinformatics;dna sequencing	Comp.	-1.7804861723884613	-56.904399087328706	158572
45ba2746ceff1542150ebe468bc28a1b7c4e9d9a	union: an efficient mapping tool using unimark with non-overlapping interval indexing strategy		NGS has become a popular research field in biologists because it was able to produce inexpensive and accuracy short biology sequences very fast. NGS technique has been improved to produce long length sequences, more than 100bp, recently with the same quality, accuracy and speed. Thus, tools for short sequences may be not suitable for long length sequences. We propose a new tool called UNION for re-sequencing applications by mapping long length sequences to a reference genome. UNION uses the UniMarker with a non- overlapping interval indexing strategy and a tool, CORAL, to do sequence alignments. For the experiments we randomly cut ten thousands sequences with a length of 512bp from the genome of Trichomonas and also produce mutations/sequence errors for these sequences to simulate different similarities. UNION has been compared with GMAP in terms of speed and accuracy and achieves better performance than that of GMAP.		Che-Lun Hung;Chun-Yuan Lin;Yu-Chen Hu	2011		10.1007/978-3-642-27157-1_21	biology;bioinformatics;data mining;algorithm	EDA	-0.821030048824802	-53.37136012503064	159330
b7eec4fa66c0d3cd4dc19b854c2140b088fc517b	dynamic programming algorithms for two statistical problems in computational biology	multinomial distribution;position specific scoring matrix;dynamic programming algorithm;transcription factor binding site;statistical test;dynamic program;power function;position weight matrix;branch and bound method;efficient implementation;non parametric method;computational biology;gaussian distribution	  We present dynamic programming algorithms for two exact statistical tests that frequently arise in computational biology.  The first test concerns the decision whether an observed sequence stems from a given profile (also known as position specific  score matrix or position weight matrix), or from an assumed background distribution. We show that the common assumption that  the log-odds score has a Gaussian distribution is false for many short profiles, such as transcription factor binding sites  or splice sites. We present an efficient implementation of a non-parametric method (first mentioned by Staden) to compute  the exact score distribution. The second test concerns the decision whether observed category counts stem from a specified  Multinomial distribution. A branch-and-bound method for computing exact p-values for this test was presented by Bejerano at  a recent RECOMB conference. Our contribution is a dynamic programming approach to compute the entire distribution of the test statistic, allowing not only the computation of exact p-values for all values of the test statistic simultaneously,  but also of the power function of the test. As one of several applications, we introduce p-value based sequence logos, which  provide a more meaningful visual description of probabilistic sequences than conventional sequence logos do.    	algorithm;computation;computational biology;dynamic programming	Sven Rahmann	2003		10.1007/978-3-540-39763-2_12	normal distribution;mathematical optimization;statistical hypothesis testing;power function;computer science;theoretical computer science;machine learning;dynamic programming;position weight matrix;mathematics;multinomial distribution;dna binding site;statistics	Theory	1.37886985283334	-53.52859367645159	160072
899993a90c133f51692c844e7419656e46197450	chemsar: an online pipelining platform for molecular sar modeling	computer applications in chemistry;theoretical and computational chemistry;computational biology bioinformatics;documentation and information in chemistry	BACKGROUND In recent years, predictive models based on machine learning techniques have proven to be feasible and effective in drug discovery. However, to develop such a model, researchers usually have to combine multiple tools and undergo several different steps (e.g., RDKit or ChemoPy package for molecular descriptor calculation, ChemAxon Standardizer for structure preprocessing, scikit-learn package for model building, and ggplot2 package for statistical analysis and visualization, etc.). In addition, it may require strong programming skills to accomplish these jobs, which poses severe challenges for users without advanced training in computer programming. Therefore, an online pipelining platform that integrates a number of selected tools is a valuable and efficient solution that can meet the needs of related researchers.   RESULTS This work presents a web-based pipelining platform, called ChemSAR, for generating SAR classification models of small molecules. The capabilities of ChemSAR include the validation and standardization of chemical structure representation, the computation of 783 1D/2D molecular descriptors and ten types of widely-used fingerprints for small molecules, the filtering methods for feature selection, the generation of predictive models via a step-by-step job submission process, model interpretation in terms of feature importance and tree visualization, as well as a helpful report generation system. The results can be visualized as high-quality plots and downloaded as local files.   CONCLUSION ChemSAR provides an integrated web-based platform for generating SAR classification models that will benefit cheminformatics and other biomedical users. It is freely available at: http://chemsar.scbdd.com . Graphical abstract .	chemical structure;cheminformatics;computation (action);computer graphics;computer programming;drug discovery;feature selection;fingerprint;imagery;job stream;machine learning;molecular descriptor;occupations;pipeline (computing);predictive modelling;preprocessor;regulatory submission;small molecule;web application;ggplot2;scikit-learn	Jie Dong;Zhi-Jiang Yao;Min-Feng Zhu;Ning-Ning Wang;Ben Lu;Alex F. Chen;Aiping Lu;Hongyu Miao;Wen-Bin Zeng;Dong-Sheng Cao	2017		10.1186/s13321-017-0215-1	cheminformatics;theoretical computer science;chemical structure;data mining;standardization;bioinformatics;computer science;visualization;feature selection;preprocessor;ggplot2;computer programming	Comp.	-3.105886838390708	-58.341599554634925	160803
0556b4a3bb5a2d33b80f0840d58aa17696b1d4f5	betaware: a machine-learning tool to detect and predict transmembrane beta-barrel proteins in prokaryotes	important problem;gram-negative bacterium;bioinformatics online;in-home massive protein annotation;protein structure;outer membrane;betaware program;standalone program;membrane protein;computational biology	SUMMARY The annotation of membrane proteins in proteomes is an important problem of Computational Biology, especially after the development of high-throughput techniques that allow fast and efficient genome sequencing. Among membrane proteins, transmembrane β-barrels (TMBBs) are poorly represented in the database of protein structures (PDB) and difficult to identify with experimental approaches. They are, however, extremely important, playing key roles in several cell functions and bacterial pathogenicity. TMBBs are included in the lipid bilayer with a β-barrel structure and are presently found in the outer membranes of Gram-negative bacteria, mitochondria and chloroplasts. Recently, we developed two top-performing methods based on machine-learning approaches to tackle both the detection of TMBBs in sets of proteins and the prediction of their topology. Here, we present our BETAWARE program that includes both approaches and can run as a standalone program on a linux-based computer to easily address in-home massive protein annotation or filtering.   AVAILABILITY AND IMPLEMENTATION http://www.biocomp.unibo.it/∼savojard/betawarecl .	anatomy, regional;cell physiology;computation;computational biology;gram-negative bacteria;high-throughput computing;linux;lipid bilayers;machine learning;membrane potentials;membrane proteins;prokaryote;protein annotation;protein data bank;software release life cycle;standalone program;throughput;tissue membrane;whole genome sequencing;gram	Castrense Savojardo;Piero Fariselli;Rita Casadio	2013	Bioinformatics	10.1093/bioinformatics/bts728	biology;cell biology;bioinformatics	Comp.	0.22624465235117536	-58.10540250647536	160811
2f8439aa31101082b8809860614c972c285a440f	adegenet: a r package for the multivariate analysis of genetic markers	genetique;analyse multivariable;multivariate analysis;genetica;bioinformatique;genetics;genetic marker;marqueur genetique;analisis multivariable;bioinformatica;marcador genetico;bioinformatics	UNLABELLED The package adegenet for the R software is dedicated to the multivariate analysis of genetic markers. It extends the ade4 package of multivariate methods by implementing formal classes and functions to manipulate and analyse genetic markers. Data can be imported from common population genetics software and exported to other software and R packages. adegenet also implements standard population genetics tools along with more original approaches for spatial genetics and hybridization.   AVAILABILITY Stable version is available from CRAN: http://cran.r-project.org/mirrors.html. Development version is available from adegenet website: http://adegenet.r-forge.r-project.org/. Both versions can be installed directly from R. adegenet is distributed under the GNU General Public Licence (v.2).	class;forge;gnu;genetic markers;genetics, population;genotype;interoperability;nucleic acid hybridization;r language;simulation;version;web site	Thibaut Jombart	2008	Bioinformatics	10.1093/bioinformatics/btn129	biology;computer science;bioinformatics;genetic marker;data mining;multivariate analysis;genetics;algorithm	Comp.	-3.2302580746581278	-56.564927613315966	160947
7200b025337ed6e3b741f451966d231e8bc5a93a	object-oriented parsing of biological databases with python	embl;base donnee;interrogation base donnee;biologia molecular;database;interrogacion base datos;base dato;data processing program;object oriented;molecular biology;estructura datos;programme traitement donnee;analizador sintaxico;oriente objet;structure donnee;parser;sequence analysis;biological database;orientado objeto;analyseur syntaxique;data structure;database query;programa tratamiento datos;biologie moleculaire	MOTIVATION While database activities in the biological area are increasing rapidly, rather little is done in the area of parsing them in a simple and object-oriented way.   RESULTS We present here an elegant, simple yet powerful way of parsing biological flat-file databases. We have taken EMBL, SWISSPROT and GENBANK as examples. EMBL and SWISS-PROT do not differ much in the format structure. GENBANK has a very different format structure than EMBL and SWISS-PROT. Extracting the desired fields in an entry (for example a sub-sequence with an associated feature) for later analysis is a constant need in the biological sequence-analysis community: this is illustrated with tools to make new splice-site databases. The interface to the parser is abstract in the sense that the access to all the databases is independent from their different formats, since parsing instructions are hidden.	biological database;cheese swiss ab.igg:acnc:pt:ser:qn;clinical trial protocol document;flat file database;genbank;parsing;published database;python;swiss-model;splice (system call);switzerland;uniprot;format	Chenna Ramu;Christine Gemünd;Toby J. Gibson	2000	Bioinformatics	10.1093/bioinformatics/16.7.628	biological database;data structure;computer science;sequence analysis;data mining;database;object-oriented programming;world wide web	NLP	-4.538880437739424	-57.452178199469465	161066
0e06434e2cd9b78ed8ea1b6e1f62936c631155ba	detection of protein similarities using nucleotide sequence databases	animals;invertebrata;escherichia coli;secuencia aminoacido;base donnee;information systems;proteine;sequence aminoacide;aminoacid sequence;arthropoda;bacterie;amino acid sequence;database;gen;base dato;alpha glucosidases;methode;secuencia nucleotido;ibm pc;drosophilidae;nucleotide sequence;enterobacteriaceae;sequence nucleotide;comparacion interespecifica;proteins;escherichieae;homology;insecta;alcohol dehydrogenase;diptera;drosophila melanogaster;gene;proteina;molecular sequence data;bacteria;base sequence;homologia;metodo;method;interspecific comparison;bacterial proteins;comparaison interspecifique;homologie	A simple procedure is described for finding similarities between proteins using nucleotide sequence databases. The approach is illustrated by several examples of previously unknown correspondences with important biological implications: Drosophila elongation factor Tu is shown to be encoded by two genes that are differently expressed during development; a cluster of three Drosophila genes likely encode maltases; a flesh-fly fat body protein resembles the hypothesized Drosophila alcohol dehydrogenase ancestral protein; an unknown protein encoded at the multifunctional E. coli hisT locus resembles aspartate beta-semialdehyde dehydrogenase; and the E. coli tyrR protein is related to nitrogen regulatory proteins. These and other matches were discovered using a personal computer of the type available in most laboratories collecting DNA sequence data. As relatively few sequences were sampled to find these matches, it is likely that much of the existing data has not been adequately examined.	alcohol dehydrogenase;alpha-glucosidase;aspartic acid;base sequence;databases;encode;ethanol;f factor;locus;laboratory;multi-function printer;nucleotides;personal computer;sampling - surgical action;sequence database	Steven Henikoff;J. C. Wallace	1988	Nucleic acids research	10.1093/nar/16.13.6191	biology;homology;method;bacteria;nucleic acid sequence;bioinformatics;gene;peptide sequence;escherichia coli;ibm pc compatible;genetics;information system	Comp.	-4.184881954434559	-56.08446257035293	161098
c3ff632edeedc8b40559d35d897640fb4a0a71a2	kaboom! a new suffix array based algorithm for clustering expression data	large datasets;supplementary data;larger datasets;bioinformatics online;clustering expression data;new filter;new suffix array;all-versus-all comparison;macos x;expression data;source code;new expression	MOTIVATION Second-generation sequencing technology has reinvigorated research using expression data, and clustering such data remains a significant challenge, with much larger datasets and with different error profiles. Algorithms that rely on all-versus-all comparison of sequences are not practical for large datasets.   RESULTS We introduce a new filter for string similarity which has the potential to eliminate the need for all-versus-all comparison in clustering of expression data and other similar tasks. Our filter is based on multiple long exact matches between the two strings, with the additional constraint that these matches must be sufficiently far apart. We give details of its efficient implementation using modified suffix arrays. We demonstrate its efficiency by presenting our new expression clustering tool, wcd-express, which uses this heuristic. We compare it to other current tools and show that it is very competitive both with respect to quality and run time.   AVAILABILITY Source code and binaries available under GPL at http://code.google.com/p/wcdest. Runs on Linux and MacOS X.   CONTACT scott.hazelhurst@wits.ac.za; zsuzsa@cebitec.uni-bielefeld.de   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	aoc2 gene;algorithm;approximation algorithm;automatic parallelization;binary file;bioinformatics;bioinformatics;biopolymer sequencing;cache;cluster analysis;curie;elegant degradation;european union;experiment;gene expression;heuristic;il31ra gene;large;linux;linux;message passing interface;posix threads;parallel computing;polymer;preprocessor;run time (program lifecycle phase);source code;string metric;suffix array;zellweger syndrome;statistical cluster	Scott Hazelhurst;Zsuzsanna Lipták	2011	Bioinformatics	10.1093/bioinformatics/btr560	computer science;bioinformatics;theoretical computer science;machine learning;data mining	Comp.	-1.3014858878113988	-53.303188495493586	161397
76cbf450199e8b633c66a18c5df803f3662f8a98	interactive data input into the genenet database.	site web;interfase usuario;interfaz grafica;base donnee;computerized processing;tratamiento informatico;graphical interface;user interface;saisie donnee;sistema adquisicion dato;biologia molecular;database;gen;base dato;data acquisition system;toma dato;molecular biology;gene;interface utilisateur;sitio web;systeme acquisition donnee;traitement informatique;data acquisition;interface graphique;web site;biologie moleculaire	SUMMARY The GeneNet database has been developed for a formalized hierarchical description of the gene networks. To provide rapid data accumulation in the database, the Java graphical interface for data input through the Internet by independent experts equipped with convenient visual tools is developed.   AVAILABILITY http://wwwmgs. bionet.nsc.ru/systems/MGL/GeneNet/	clec10a wt allele;gene regulatory network;graphical user interface;interface device component;internet;java programming language;multiple granularity locking;tree accumulation	Fedor A. Kolpakov;Elena A. Ananko	1999	Bioinformatics	10.1093/bioinformatics/15.7.713	biology;computer science;operating system;database;data acquisition;world wide web;database design	DB	-4.282696930090307	-57.27437440448471	161553
ee8bfd2fad02ccf928ec89060f15f6388ee8966d	simulation of the usage of gaussian mixture models for the purpose of modelling virtual mass spectrometry data	expectation maximization algorithm;gmm;. mass spectrometry;gaussian mixture model;mass spectrometry;mass spectra;spectrum	This article presents the method of the processing of mass spectrometry data. Mass spectra are modelled with Gaussian Mixture Models. Every peak of the spectrum is represented by a single Gaussian. Its parameters describe the location, height and width of the corresponding peak of the spectrum. An authorial version of the Expectation Maximisation Algorithm was used to perform all calculations. Errors were estimated with a virtual mass spectrometer. The discussed tool was originally designed to generate a set of spectra within defined parameters.	computation;dna microarray;expectation–maximization algorithm;mass spectrometers (device);maxima and minima;maximum;mixture model;normal statistical distribution;simulation;spectrometry;time complexity;width	Malgorzata Plechawska-Wójcik;Joanna Polanska	2009	Studies in health technology and informatics	10.3233/978-1-60750-044-5-804	mass spectrum;spectral line;mixture model;expectation–maximization algorithm;statistics;gaussian;metre (music);mass spectrometry;medicine	Comp.	-1.7712997077158563	-55.275086684189425	161967
1c2425b80894d11c65efd572957bf99705fe6459	rhat: fast alignment of noisy long reads with regional hashing		MOTIVATION Single Molecule Real-Time (SMRT) sequencing has been widely applied in cutting-edge genomic studies. However, it is still an expensive task to align the noisy long SMRT reads to reference genome by state-of-the-art aligners, which is becoming a bottleneck in applications with SMRT sequencing. Novel approach is on demand for improving the efficiency and effectiveness of SMRT read alignment.   RESULTS We propose Regional Hashing-based Alignment Tool (rHAT), a seed-and-extension-based read alignment approach specifically designed for noisy long reads. rHAT indexes reference genome by regional hash table (RHT), a hash table-based index which describes the short tokens within local windows of reference genome. In the seeding phase, rHAT utilizes RHT for efficiently calculating the occurrences of short token matches between partial read and local genomic windows to find highly possible candidate sites. In the extension phase, a sparse dynamic programming-based heuristic approach is used for reducing the cost of aligning read to the candidate sites. By benchmarking on the real and simulated datasets from various prokaryote and eukaryote genomes, we demonstrated that rHAT can effectively align SMRT reads with outstanding throughput.   AVAILABILITY AND IMPLEMENTATION rHAT is implemented in C++; the source code is available at https://github.com/HIT-Bioinformatics/rHAT CONTACT: ydwang@hit.edu.cn   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Bo Liu;Dengfeng Guan;Mingxiang Teng;Yadong Wang	2016	Bioinformatics	10.1093/bioinformatics/btv662	hash table;genomics;computer science;dynamic programming;single molecule real time sequencing;bioinformatics;hash function;source code;reference genome;sequence alignment	Comp.	-0.9338713460365712	-54.15996145718262	162176
7edcb54493c5b86bdc03795724acd51de63d49a4	bioinformatics applications in genomics	genomics;cancer;pathogen detection;pan asian data analysis bioinformatics applications genomics computational pipelines genome wide data;genome assembly;chip seq;chip;data analysis;pathogen resequencing computing in asia next generation sequencing genome assembly chip seq microarrays pathogen detection;biological cells;computational complexity;informatics;pathogen resequencing;genomics bioinformatics data analysis;next generation sequencing;data consistency;computing in asia;microarrays;bioinformatics;bioinformatics genomics informatics biological cells cancer computational complexity	Instead of analyzing one gene at a time, researchers are using computational pipelines to evaluate genome-wide data consisting of hundreds of billions of bits of raw data to assist in pan-Asian data analysis.	bioinformatics;pipeline (computing);pipeline (software)	Wing-Kin Sung	2012	Computer	10.1109/MC.2012.151	computational biology;chip;dna sequencing;genomics;dna microarray;computer science;bioinformatics;data analysis;data consistency;computational complexity theory;informatics;sequence assembly;cancer;chip-sequencing	Comp.	-1.6348766111081172	-53.788099558774796	162424
8072deb6718a5535045f548ae49ce2a7347a051f	design and validation issues in rna-seq experiments	experimental design;sample size;validacion;tamano muestra;rna seq;plan experiencia;taille echantillon;conception;blocking;plan experience;replicates;high throughput sequencing;diseno;design;validation;sequencage a haut debit;next generation sequencing	The next-generation sequencing technologies are being rapidly applied in biological research. Tens of millions of short sequences generated in a single experiment provide us enormous information on genome composition, genetic variants, gene expression levels and protein binding sites depending on the applications. Various methods are being developed for analyzing the data generated by these technologies. However, the relevant experimental design issues have rarely been discussed. In this review, we use RNA-seq as an example to bring this topic into focus and to discuss experimental design and validation issues pertaining to next-generation sequencing in the quantification of transcripts.	binding sites;biopolymer sequencing;dna binding site;design of experiments;experiment;gene expression programming;massively-parallel sequencing;neuritis, autoimmune, experimental;quantitation;sequence number;transcript	Zhide Fang;Xiangqin Cui	2011	Briefings in bioinformatics	10.1093/bib/bbr004	biology;dna sequencing;simulation;bioinformatics;statistics	Comp.	0.5322088201628716	-57.027382711382984	162434
8867698a4671138dfda8bb5ad48ea7122dc368d6	bayesian unsupervised learning of dna regulatory binding regions	unsupervised learning;computer and information science;engineering and technology;teknik och teknologier;data och informationsvetenskap	Identification of regulatory binding motifs, that is, short specific words, within DNA sequences is a commonly occurring problem in computational bioinformatics. A wide variety of probabilistic approaches have been proposed in the literature to either scan for previously known motif types or to attempt de novo identification of a fixed number (typically one) of putative motifs. Most approaches assume the existence of reliable biodatabase information to build probabilistic a priori description of the motif classes. Examples of attempts to do probabilistic unsupervised learning about the number of putative de novo motif types and their positions within a set of DNA sequences are very rare in the literature. Here we show how such a learning problem can be formulated using a Bayesian model that targets to simultaneously maximize the marginal likelihood of sequence data arising under multiple motif types as well as under the background DNA model, which equals a variable length Markov chain. It is demonstrated how the adopted Bayesian modelling strategy combined with recently introduced nonstandard stochastic computation tools yields a more tractable learning procedure than is possible with the standard Monte Carlo approaches. Improvements and extensions of the proposed approach are also discussed.	bayesian network;bioinformatics;cobham's thesis;computation;dna binding site;de novo transcriptome assembly;de-identification;marginal model;markov chain;monte carlo method;motif;unsupervised learning	Jukka Corander;Magnus Ekdahl;Timo Koski	2009	Adv. Artificial Intellegence	10.1155/2009/219743	unsupervised learning;computer science;bioinformatics;artificial intelligence;machine learning;data mining;information and computer science	Comp.	2.6994651285850755	-54.45825782463388	162514
1c5c4e14e9701173bfbd5dcc852c18c130eb3550	the tomato expression atlas		Summary With the development of new high-throughput DNA sequencing technologies and decreasing costs, large gene expression datasets are being generated at an accelerating rate, but can be complex to visualize. New, more interactive and intuitive tools are needed to visualize the spatiotemporal context of expression data and help elucidate gene function. Using tomato fruit as a model, we have developed the Tomato Expression Atlas to facilitate effective data analysis, allowing the simultaneous visualization of groups of genes at a cell/tissue level of resolution within an organ, enhancing hypothesis development and testing in addition to candidate gene identification. This atlas can be adapted to different types of expression data from diverse multicellular species.   Availability and Implementation The Tomato Expression Atlas is available at http://tea.solgenomics.net/ . Source code is available at https://github.com/solgenomics/Tea .   Contact jr286@cornell.edu or lam87@cornell.edu.   Supplementary information Supplementary data are available at Bioinformatics online.	atlases;bioinformatics;candidate gene identification;ephrin type-b receptor 1, human;expression atlas;gene expression;geographic information systems;high-throughput computing;imagery;source code;throughput	Noé Fernández-Pozo;Yi Zheng;Stephen I. Snyder;Philippe Nicolas;Yoshihito Shinozaki;Zhangjun Fei;Carmen Catalá;James J. Giovannoni;Jocelyn K. C. Rose;Lukas A. Mueller	2017		10.1093/bioinformatics/btx190	computer science;atlas (anatomy);bioinformatics	Comp.	-2.083719285081128	-58.46684871849764	162743
5a48a8fd6edbeb209570ea802b19245506323504	gecont: gene context analysis	genome annotation;web interface;common property	SUMMARY The fact that adjacent genes in bacteria are often functionally related is widely known. GeConT (Gene Context Tool) is a web interface designed to visualize genome context of a gene or a group of genes and their orthologs in all the completely sequenced genomes. The graphical information of GeConT can be used to analyze genome annotation, functional ortholog identification or to verify the genomic context congruence of any set of genes that share a common property.   AVAILABILITY http://www.ibt.unam.mx/biocomputo/gecont.html	annotation;application program interface;congruence of squares;genes, vif;genome;graphical user interface;homology (biology);orthologous gene	Ricardo Ciria;Cei Abreu-Goodger;Enrique Morett;Enrique Merino	2004	Bioinformatics	10.1093/bioinformatics/bth216	biology;computer science;bioinformatics;user interface;genome project;world wide web;genetics	Comp.	-1.2824460285482937	-58.82198781863528	162845
4c7de7ec837ba829640ae59121e6a815748f5c40	using substitution probabilities to improve position-specific scoring matrices	computer program;secuencia aminoacido;base donnee;alignement sequence;proteine;sequence aminoacide;etude theorique;aminoacid sequence;computerized processing;tratamiento informatico;amino acid;protein sequence;database;base dato;amino acid substitution;alineacion secuencia;general solution;proteins;estudio teorico;proteina;sequence alignment;multiple sequence alignment;theoretical study;database search;programa computador;traitement informatique;programme ordinateur;multiple alignment	Each column of amino acids in a multiple alignment of protein sequences can be represented as a vector of 20 amino acid counts. For alignment and searching applications, the count vector is an imperfect representation of a position, because the observed sequences are an incomplete sample of the full set of related sequences. One general solution to this problem is to model unobserved sequences by adding artificial 'pseudo-counts' to the observed counts. We introduce a simple method for computing pseudo-counts that combines the diversity observed in each alignment position with amino acid substitution probabilities. In extensive empirical tests, this position-based method out-performed other pseudo-count methods and was a substantial improvement over the traditional average score method used for constructing profiles.	amino acid sequence;amino acid substitution;amino acids;computation (action);multiple sequence alignment;partial;peptide sequence;probability;pseudo brand of pseudoephedrine;score	Jorja G. Henikoff;Steven Henikoff	1996	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/12.2.135	biology;multiple sequence alignment;computer science;bioinformatics;artificial intelligence;algorithm	Comp.	-2.753807697033663	-54.500994305955565	163023
de0a75d6e7cb36c6969c7c83441536170cb75b4c	treevis: a matlab-based tool for tree visualization	software;tree;computer graphics;visualization;methylation;genome human;cell line tumor;algorithms;humans;user computer interface;neoplasms;computational biology;matlab	Network-based analyses of high-dimensional biological data often produce results in the form of tree structures. Generating easily interpretable layouts to visualize these tree structures is a non-trivial task. We present a new visualization algorithm to generate two-dimensional layouts for complex tree structures. Implementations in both MATLAB and R are provided.		Peng Qiu;Sylvia K. Plevritis	2013	Computer methods and programs in biomedicine	10.1016/j.cmpb.2012.08.008	computational science;visualization;computer science;bioinformatics;methylation;theoretical computer science;tree;computer graphics	Visualization	-3.9644811122686416	-58.514230315334416	163092
5e29c3efc47dac202d7b3e74ecf64aad4f52f088	translation initiation site prediction on a genomic scale: beauty in simplicity	genes;sequences;computational techniques;translation initiation site;web interface;computational method;recognition;false positive rate;gene prediction;biology and life sciences;false positive	MOTIVATION The correct identification of translation initiation sites (TIS) remains a challenging problem for computational methods that automatically try to solve this problem. Furthermore, the lion's share of these computational techniques focuses on the identification of TIS in transcript data. However, in the gene prediction context the identification of TIS occurs on the genomic level, which makes things even harder because at the genome level many more pseudo-TIS occur, resulting in models that achieve a higher number of false positive predictions.   RESULTS In this article, we evaluate the performance of several 'simple' TIS recognition methods at the genomic level, and compare them to state-of-the-art models for TIS prediction in transcript data. We conclude that the simple methods largely outperform the complex ones at the genomic scale, and we propose a new model for TIS recognition at the genome level that combines the strengths of these simple models. The new model obtains a false positive rate of 0.125 at a sensitivity of 0.80 on a well annotated human chromosome (chromosome 21). Detailed analyses show that the model is useful, both on its own and in a simple gene prediction setting.   AVAILABILITY Datafiles and a web interface for the StartScan program are available at http://bioinformatics.psb.ugent.be/supplementary_data/.	bioinformatics;chromosomes, human, pair 21;computational chemistry;gene prediction;pseudo brand of pseudoephedrine;pulse-width modulation;toxic shock syndrome;transcript;transcription initiation;translation initiation;user interface	Yvan Saeys;Thomas Abeel;Sven Degroeve;Yves Van de Peer	2007	Bioinformatics	10.1093/bioinformatics/btm177	biology;type i and type ii errors;false positive rate;computer science;bioinformatics;artificial intelligence;gene;data mining;sequence;user interface;genetics;gene prediction;statistics	Comp.	1.9085228527831732	-57.651922421575655	163300
2c5ce77cac649940f852b181191771a2fcd30090	improving de novo sequence assembly using machine learning and comparative genomics for overlap correction	genomics;complete genome;de novo sequencing;genome fungal;databases nucleic acid;comparative genomics;efficient algorithm;sequence analysis dna;computational biology bioinformatics;machine learning;genome bacterial;artificial intelligence;algorithms;combinatorial libraries;base sequence;dna sequence;computer appl in life sciences;genome sequence;microarrays;bioinformatics	With the rapid expansion of DNA sequencing databases, it is now feasible to identify relevant information from prior sequencing projects and completed genomes and apply it to de novo sequencing of new organisms. As an example, this paper demonstrates how such extra information can be used to improve de novo assemblies by augmenting the overlapping step. Finding all pairs of overlapping reads is a key task in many genome assemblers, and to this end, highly efficient algorithms have been developed to find alignments in large collections of sequences. It is well known that due to repeated sequences, many aligned pairs of reads nevertheless do not overlap. But no overlapping algorithm to date takes a rigorous approach to separating aligned but non-overlapping read pairs from true overlaps. We present an approach that extends the Minimus assembler by a data driven step to classify overlaps as true or false prior to contig construction. We trained several different classification models within the Weka framework using various statistics derived from overlaps of reads available from prior sequencing projects. These statistics included percent mismatch and k-mer frequencies within the overlaps as well as a comparative genomics score derived from mapping reads to multiple reference genomes. We show that in real whole-genome sequencing data from the E. coli and S. aureus genomes, by providing a curated set of overlaps to the contigging phase of the assembler, we nearly doubled the median contig length (N50) without sacrificing coverage of the genome or increasing the number of mis-assemblies. Machine learning methods that use comparative and non-comparative features to classify overlaps as true or false can be used to improve the quality of a sequence assembly.	algorithm;alignment;assembly language;collections (publication);database;de novo transcriptome assembly;genomics;k-mer;machine learning;mer;reading (activity);sequence assembly;staphylococcus aureus;weka;whole genome sequencing	Lance E. Palmer;Mathäus Dejori;Randall A. Bolanos;Daniel P. Fasulo	2009		10.1186/1471-2105-11-33	biology;dna sequencing;genomics;whole genome sequencing;dna microarray;computer science;bioinformatics;comparative genomics;genetics;hybrid genome assembly	Comp.	-0.33670356390303174	-54.608942669188295	163334
f3ee2c32c73be25cfea0b4b85ef93edc925cb809	mlegp: statistical analysis for computer models of biological systems using r	software;biological system;data interpretation statistical;analisis estadistico;biological model;normal distribution;computer model;bioinformatique;models biological;modelo biologico;statistical model;systeme biologique;statistical analysis;modele biologique;analyse statistique;modele statistique;biological systems;models statistical;algorithms;modelo estadistico;bioinformatica;computer simulation;sistema biologico;programming languages;bioinformatics	UNLABELLED Gaussian processes (GPs) are flexible statistical models commonly used for predicting output from complex computer codes. As such, GPs are well suited for the analysis of computer models of biological systems, which have been traditionally difficult to analyze due to their high-dimensional, non-linear and resource-intensive nature. We describe an R package, mlegp, that fits GPs to computer model outputs and performs sensitivity analysis to identify and characterize the effects of important model inputs.   AVAILABILITY http://www.biomath.org/mlegp	biological system;code;computer simulation;fits;gaussian process;nonlinear system;normal statistical distribution;numerical weather prediction;r language;statistical model	Garrett M. Dancik;Karin S. Dorman	2008	Bioinformatics	10.1093/bioinformatics/btn329	normal distribution;computer simulation;statistical model;computer science;artificial intelligence;models of abnormality;operations research;statistics	ML	-4.025901733379802	-54.454899469592185	163625
253a62ec1b56f02da3742172ef854ff60b96b48d	direct maximum parsimony phylogeny reconstruction from genotype data	evolution molecular;animals;genomics;dna mitochondrial;data interpretation statistical;generic algorithm;phylogeny;phylogeny reconstruction;genotype;genome analysis;databases genetic;sequence analysis dna;dna chloroplast;genetics;genetic variation;computational biology bioinformatics;conserved sequence;factor analysis statistical;models genetic;phylogenetic tree;likelihood functions;programming linear;sequence homology amino acid;models statistical;algorithms;humans;sequence alignment;combinatorial libraries;base sequence;computer appl in life sciences;haplotypes;mutation;lower bound;maximum parsimony;microarrays;bioinformatics;population genetics	Maximum parsimony phylogenetic tree reconstruction from genetic variation data is a fundamental problem in computational genetics with many practical applications in population genetics, whole genome analysis, and the search for genetic predictors of disease. Efficient methods are available for reconstruction of maximum parsimony trees from haplotype data, but such data are difficult to determine directly for autosomal DNA. Data more commonly is available in the form of genotypes, which consist of conflated combinations of pairs of haplotypes from homologous chromosomes. Currently, there are no general algorithms for the direct reconstruction of maximum parsimony phylogenies from genotype data. Hence phylogenetic applications for autosomal data must therefore rely on other methods for first computationally inferring haplotypes from genotypes. In this work, we develop the first practical method for computing maximum parsimony phylogenies directly from genotype data. We show that the standard practice of first inferring haplotypes from genotypes and then reconstructing a phylogeny on the haplotypes often substantially overestimates phylogeny size. As an immediate application, our method can be used to determine the minimum number of mutations required to explain a given set of observed genotypes. Phylogeny reconstruction directly from unphased data is computationally feasible for moderate-sized problem instances and can lead to substantially more accurate tree size inferences than the standard practice of treating phasing and phylogeny construction as two separate analysis stages. The difference between the approaches is particularly important for downstream applications that require a lower-bound on the number of mutations that the genetic region has undergone.	chromosomes;computation (action);computational phylogenetics;downstream (software development);genetics, population;genotype;haplotypes;homology (biology);maximum parsimony (phylogenetics);mutation;phylogenetic tree;trees (plant);variation (genetics);algorithm	Srinath Sridhar;Fumei Lam;Guy E. Blelloch;R. Ravi;Russell Schwartz	2007	BMC Bioinformatics	10.1186/1471-2105-8-472	mutation;biology;genomics;phylogenetic tree;genetic algorithm;dna microarray;haplotype;bioinformatics;genetic variation;tree rearrangement;genotype;sequence alignment;conserved sequence;upper and lower bounds;maximum parsimony;population genetics;genetics	Comp.	1.7882927580225663	-52.10171585464485	163785
d069d5d0a9ce7c98f1b3cb31ba75bf8fa99ffbba	gene model detection using mass spectrometry		The utility of a genome sequence in biological research depends entirely on the comprehensive description of all of its functional elements. Analysis of genome sequences is still predominantly gene-centric (i.e., identifying gene models/open reading frames). In this article, we describe a proteomics-based method for identifying open reading frames that are missed by computational algorithms. Mass spectrometry-based identification of peptides and proteins from biological samples provide evidence for the expression of the genome sequence at the protein level. This proteogenomic annotation method combines computationally predicted ORFs and the genome sequence with proteomics to identify novel gene models. We also describe our proteogenomic mapping pipeline - a set of computational tools that automate the proteogenomic annotation work flow. This pipeline is available for download at www.agbase.msstate.edu/tools/ .		Bindu Nanduri;Nan Wang;Mark L. Lawrence;Susan M. Bridges;Shane C. Burgess	2010	Methods in molecular biology	10.1007/978-1-60761-444-9_10	protein mass spectrometry	Comp.	-0.35323029685715324	-58.443532372054094	163908
52d8404d26672467cbe70cba70930000b8e2e769	effect of multi-k contig merging in de novo dna assembly	dna;genomics;memory management;molecular configurations bioinformatics dna genomics merging molecular biophysics;contig merging;assembly;multi k assembly;assembly genomics merging dna memory management bioinformatics educational institutions;de novo dna assembly;merging;bacterial genomes multik contig merging de novo dna assembly bioinformatics human genome next generation sequencing technology multiple k mer length;contig merging de novo dna assembly multi k assembly;bioinformatics	DNA Assembly is among the most fundamental and challenging problems in bioinformatics. Near optimal solutions are available for bacterial and small genomes. However assembling large and complex genomes including the human genome using Next-Generation-Sequencing (NGS) technologies is shown to be very difficult. This paper presents an algorithm for creating contigs from NGS short read data that is capable of working with multiple k-mer lengths and introduces a technique to combine contigs generated from different k runs with results from other assemblers in order to obtain significantly better assemblies. Experimental results from 9 real datasets show an increase in N50 value by a factor of 3, when combining newly created contigs with results from other assemblers.	algorithm;bioinformatics;communications satellite;de novo transcriptome assembly;k-mer;mer;sequence assembly	Mohammad Goodarzi;Sheridan K. Houghten;Ping Liang	2014	2014 IEEE International Conference on Bioinformatics and Bioengineering	10.1109/BIBE.2014.49	biology;contig;genomics;molecular biology;computer science;bioinformatics;assembly;genetics;dna;memory management	Visualization	-0.5175936763951144	-53.838412508827695	163917
370cb90d9eb63b1d0c5f27174483d0af3aeaafca	integrating image analysis algorithms in a web interface for the quantification of microtubule dynamics	microtubule dynamics;web interface;fluorescence microscopy;image analysis	We present improvements to a web interface and an integrated computational tracking algorithm for quantitative analysis of microtubule dynamics in live-cell microscopy images. Based on a previously implemented system, more new functionalities have been added to the interface. The system also integrates a computational tracking algorithm to aid the analysis. The analysis workflow of the proposed interface is made similar to the current manual analysis workflow in order to make the interface intuitive to use. We show the workflow of the computer analysis algorithm and how it is used to aid the existing analysis workflow. We also demonstrate how to re-evaluate existing data in a case study using real imaging data. Lastly, we show the added functionalities of the interface including how to share image data and analysis results.		Koon Yin Kong;Adam I. Marcus;Paraskevi Giannakakou;May D. Wang	2012	International journal of computational biology and drug design	10.1504/IJCBDD.2012.049211	fluorescence microscope;image analysis;simulation;computer science;bioinformatics;user interface;world wide web	Visualization	-4.024775175242162	-58.84335758019299	163920
9c00b27e8f7c9741fcadebfacd3a03a1fbb8d81c	fishingcnv: a graphical software package for detecting rare copy number variations in exome-sequencing data	supplementary data;pipeline implementation;exome-sequencing data;control sample;bioinformatics online;available online;batch effect;background distribution;heterozygous deletion;heterozygous duplication	SUMMARY Rare copy number variations (CNVs) are frequent causes of genetic diseases. We developed a graphical software package based on a novel approach that can consistently identify CNVs of all types (homozygous deletions, heterozygous deletions, heterozygous duplications) from exome-sequencing data without the need of a paired control. The algorithm compares coverage depth in a test sample against a background distribution of control samples and uses principal component analysis to remove batch effects. It is user friendly and can be run on a personal computer.   AVAILABILITY AND IMPLEMENTATION The main scripts are implemented in R (2.15), and the GUI is created using Java 1.6. It can be run on all major operating systems. A non-GUI version for pipeline implementation is also available. The program is freely available online: https://sourceforge.net/projects/fishingcnv/   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.		Yuhao Shi;Jacek Majewski	2013	Bioinformatics	10.1093/bioinformatics/btt151	computer science;bioinformatics;data mining;world wide web	Comp.	-1.7486124869825592	-56.584223278922806	164258
20197b2cb40fbeb281b86e8c8beee5b2a409f43d	predicting protein function: a versatile tool for the apple macintosh	outil logiciel;prediccion;relation structure fonction;computer program;secuencia aminoacido;software tool;protein function;genetical translation;proteine;sequence aminoacide;langage c;aminoacid sequence;computerized processing;tratamiento informatico;implementation;apple macintosh;relacion estructura funcion;secuencia nucleotido;structure function relationship;traduccion genetica;nucleotide sequence;sequence nucleotide;algorithme;algorithm;ejecucion;c language;traduction genetique;proteins;herramienta controlada por logicial;proteina;programa computador;traitement informatique;prediction;programme ordinateur;lenguaje c;algoritmo	A tool is presented that helps to find biological functions for new protein sequences. Running on any Macintosh computer system, MacPattern provides a unique combination of different algorithms, speed and user-friendliness. It supports searches for protein patterns using the PROSITE database, protein block searches with the BLOCKS database, and the identification of statistically significant protein segments. MacPattern allows batch processing of sequences and automatic translations of nucleotide sequence data. It is particularly suited for genome analysis or cDNA sequencing projects.	amino acid sequence;base sequence;batch processing;computer systems;dna, complementary;databases, protein;ethanol 0.62 ml/ml topical gel;language translations;nucleotides;prosite;peptide sequence;protein function prediction;usability;algorithm	Rainer Fuchs	1994	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/10.2.171	biology;prediction;nucleic acid sequence;computer science;bioinformatics;artificial intelligence;implementation;genetics;algorithm	Comp.	-4.156228291334642	-55.92432922732742	164269
02a160c45b0d63938e02aa786e44e106dd9cca3f	an ultra-fast approach to align longer short reads onto human genome		With the advent of second-generation sequencing (SG S) technologies, deoxyribonucleic acid (DNA) sequencing machines have started to produce reads, named as “longer short reads”, which are much longer than previous generation reads, the so calle d “short reads”. Unfortunately, most of the existin g read aligners do not scale well for those second-ge neration longer short reads. Moreover, many of the existing aligners are limited only to the short rea ds of previous generation. In this paper, we have proposed a new approach to solve this essential rea d alignment problem for current generation longer short reads. Our ultra-fast approach uses a hash-ba sed indexing and searching scheme to find exact matching for second-generation longer short reads w ithin reference genome. The experimental study shows that the proposed ultra-fast approach can acc urately find matching of millions of reads against human genome within few seconds and it is an order of magnitude faster than Burrows-Wheeler Transform (BWT) based methods such as BowTie and Burrows-Wheeler Aligner (BWA) for a wide range of read length.	acid;align (company);bowtie (sequence analysis);burrows–wheeler transform;elegant degradation;experiment;list of sequence alignment software;resources, events, agents (accounting model);sed;suicidegirls	Arup Ghosh;Gi-Nam Wang;Satchidananda Dehuri	2013	Informatica (Slovenia)		artificial intelligence;parallel computing;machine learning;computer science;human genome;hash function;search engine indexing;reference genome	Comp.	-1.52537601277322	-52.895358016746684	164390
2877ef2fbf4d089f8ae3761bc819891c33fb680d	dexsi: a new tool for the rapid quantitation of 13c-labelled metabolites detected by gc-ms		Summary Stable isotope directed metabolomics is increasingly being used to measure metabolic fluxes in microbial, plant and animal cells. Incorporation of 13C/15N isotopes into a wide range of metabolites is typically determined using gas chromatography-mass spectrometry (GC/MS) or other hyphenated mass spectrometry approaches. The DExSI (Data Extraction for Stable Isotope-labelled metabolites) pipeline is an interactive graphical software package which can be used to rapidly quantitate isotopologues for a wide variety of metabolites detected by GC/MS. DExSI performs automated metabolite annotation, mass and positional isotopomer abundance determination and natural isotope abundance correction. It provides a range of output options and is suitable for high throughput analyses.   Availability and implementation DExSI is available for non-commercial use from: https://github.com/DExSI/DExSI/. For Microsoft Windows 7 or higher (64-bit).   Contact malcolmm@unimelb.edu.au or michael.dagley@unimelb.edu.au.   Supplementary information Supplementary data are available at Bioinformatics online.	64-bit computing;annotation;bioinformatics;gas chromatography-mass spectrometry;gases;geographic information systems;graphical user interface;isotopes;metabolic process, cellular;metabolite;metabolomics;microsoft windows;quantitation;stable angina;throughput	Michael J. Dagley;Malcolm J. McConville	2018		10.1093/bioinformatics/bty025	computer science;gas chromatography–mass spectrometry;analytical chemistry;bioinformatics	Comp.	-2.2436730097042408	-57.69299770804756	165092
e8c6241a748d244a90f62f35d04a0bc04f6d464d	nail-network analysis interface for linking hmmer results	embl;outil logiciel;secuencia aminoacido;software tool;base donnee;affichage;navegacion informacion;alignement sequence;proteine;red www;sequence aminoacide;search result;visualizacion;aminoacid sequence;navigation information;information browsing;database;base dato;produit recherche;alineacion secuencia;resultado busqueda;network analysis;display;homology;herramienta controlada por logicial;world wide web;reseau www;proteina;sequence alignment;protein;database search;homologia;homologie	SUMMARY Network Analysis Interface for Linking HMMER results (NAIL) is a web-based tool for the analysis of results from a HMMER protein database-search. NAIL facilitates the selection of protein hits and the creation of an alignment, which can be used for a new sequence similarity search.	databases, protein;hmmer;pierre robin syndrome;sequence alignment;similarity search;web application	Luis Sánchez-Pulido;Yan P. Yuan;Miguel A. Andrade-Navarro;Peer Bork	2000	Bioinformatics	10.1093/bioinformatics/16.7.656	homology;database search engine;network analysis;computer science;bioinformatics;sequence alignment;world wide web	Comp.	-4.2728102195096485	-57.11347930146014	166082
f033a0f565746417884705f10b39aa0e58075b8f	data reduction of isotope-resolved lc-ms spectra	retention time;variable selection;general public license;data reduction;noise removal	MOTIVATION Data reduction of liquid chromatography-mass spectrometry (LC-MS) spectra can be a challenge due to the inherent complexity of biological samples, noise and non-flat baseline. We present a new algorithm, LCMS-2D, for reliable data reduction of LC-MS proteomics data.   RESULTS LCMS-2D can reliably reduce LC-MS spectra with multiple scans to a list of elution peaks, and subsequently to a list of peptide masses. It is capable of noise removal, and deconvoluting peaks that overlap in m/z, in retention time, or both, by using a novel iterative peak-picking step, a 'rescue' step, and a modified variable selection method. LCMS-2D performs well with three sets of annotated LC-MS spectra, yielding results that are better than those from PepList, msInspect and the vendor software BioAnalyst.   AVAILABILITY The software LCMS-2D is available under the GNU general public license from http://www.bioc.aecom.yu.edu/labs/angellab/as a standalone C program running on LINUX.	baseline (configuration management);chamaecyparis lawsoniana;feature selection;gnu;isotopes;iteration;iterative method;linux;liquid chromatography mass spectrometry;lysate;manuscripts;mass spectrometry data format;million book project;myelin basic proteins;proteomics;selection (genetic algorithm);united states national institutes of health;algorithm;mixture	Peicheng Du;Rajagopalan Sudha;Michael B. Prystowsky;Ruth Hogue Angeletti	2007	Bioinformatics	10.1093/bioinformatics/btm083	data reduction;simulation;computer science;data mining;feature selection;world wide web;statistics	Comp.	-0.1497770006669431	-56.01351466004998	166686
0204a19329b0d301188933800474404261cdc42d	a computer program for translating dna sequences into protein	dna;computadora;computers;software;programa;computer program;program;proteine;computerized processing;tratamiento informatico;ordinateur;amino acid sequence;computer;traduction;primary structure;estructura primaria;proteins;translation;programme;traduccion;proteina;base sequence;dna sequence;traitement informatique;protein biosynthesis;structure primaire	This paper describes a comprehensive program for translating one or two DNA sequences into amino acid sequences. Written in FORTRAN, it was designed for maximum flexibility of use and easy maintenance, modification and portability. It has full comments throughout.	amino acid sequence;amino acids;computer program;fortran	P. Taylor	1986	Nucleic acids research	10.1093/nar/14.1.437	translation;biology;dna sequencing;bioinformatics;protein primary structure;peptide sequence;genetics;dna;protein biosynthesis	Comp.	-4.447300818842627	-56.25217430708241	167122
81c5f4e45b58ded8d4827feecfbc712e5f3bbf59	3d visualization of haplotype risk maps		Traditionally, genetic risk maps consider genotypic differences in a small number of single markers. However, a more recent approach considers a very large set of input variables some of them with very little effect and haplotypes with several consecutive markers instead of genotypes. While a bidimensional map can only show the first of the two approaches, a 3D map together with a powerful visualization tool of virtual reality may combine both approaches, so that the molecular biologist can get immerse and explore every genetic risk factor represented in the map. Maps enriched with information from different annotation sources may fully benefit of this 3D immersive feature.	apple maps;chromosome (genetic algorithm);complexity;gnu;map;open-source software;user interface;virtual reality;world wide web	Sergio Torres-Sánchez;Manuel García Sánchez;Germán Arroyo;Nuria Medina-Medina;Rosana Montes-Soldado;Francisco Soler-Martínez;María M. Abad-Grau	2012			bioinformatics;genetics	Comp.	-3.2186433390165075	-57.97946990853056	167330
89da825240eb0ff09df0e2101b32ad2a02f89805	qmsim: a large-scale genome simulator for livestock	domestic animals;genome analysis;simulator;genomes;genoma a;large scale;livestock;simulador;genome a;mutations;genotypes;population structure;simulateur;ganado;a genome;escala grande;pedigree;betail;echelle grande	SUMMARY QMSim was designed to simulate large-scale genotyping data in multiple and complex livestock pedigrees. The simulation is basically carried out in two steps. In the first step, a historical population is simulated to establish mutation-drift equilibrium, and in the second step, recent population structures are generated, which can be very complex. A wide variety of genome architectures, ranging from infinitesimal model to single-locus model, can be simulated. The program is efficient in terms of computing time and memory requirements.   AVAILABILITY Executable versions of QMSim for Windows and Linux are freely available at http://www.aps.uoguelph.ca/~msargol/qmsim/.	architecture as topic;computation (action);data structure;executable;genotype determination;locus;linux;livestock;microsoft windows;mutation;natural science disciplines;population;requirement;simulation;time complexity;version;very helpful;algorithm;genetic pedigree	Mehdi Sargolzaei;Flavio S. Schenkel	2009	Bioinformatics	10.1093/bioinformatics/btp045	biology;bioinformatics;genotype;livestock;genetics;genome	Comp.	-3.265969260969517	-55.29320619099255	167423
ea900dd85604230b2574ce8b9c0cb5e0903b6835	goseek: a gene ontology search engine using enhanced keywords	biology computing;genomics;search engines;gene prediction tool goseek gene ontology search engine gene similarity tools lowest common ancestors lca annotation;genetics;search engines biology computing genetics genomics;semantics databases ontologies bioinformatics genomics search engines biological information theory	We propose in this paper a biological search engine called GOseek, which overcomes the limitation of current gene similarity tools. Given a set of genes, GOseek returns the most significant genes that are semantically related to the given genes. These returned genes are usually annotated to one of the Lowest Common Ancestors (LCA) of the Gene Ontology (GO) terms annotating the given genes. Most genes have several annotation GO terms. Therefore, there may be more than one LCA for the GO terms annotating the given genes. The LCA annotating the genes that are most semantically related to the given gene is the one that receives the most aggregate semantic contribution from the GO terms annotating the given genes. To identify this LCA, GOseek quantifies the contribution of the GO terms annotating the given genes to the semantics of their LCAs. That is, it encodes the semantic contribution into a numeric format. GOseek uses microarray experiment data to rank result genes based on their significance. We evaluated GOseek experimentally and compared it with a comparable gene prediction tool. Results showed marked improvement over the tool.	aggregate data;annotation;experiment;gene ontology;gene prediction;microarray;numbers;web search engine	Kamal Taha	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6609797	biology;genomics;bioinformatics;data mining;information retrieval;search engine	Comp.	2.048703291841272	-56.56200141061422	167662
0c87b7fc2d2d66446179e00324b96ad3583f50a1	an efficient dynamic programming algorithm for phosphorylation site assignment of large-scale mass spectrometry data	health research;dynamic programming;uk clinical guidelines;biological patents;peptides heuristic algorithms ions dynamic programming mass spectroscopy algorithm design and analysis redundancy;europe pubmed central;citation search;proteomics biochemistry bioinformatics dynamic programming mass spectroscopy proteins;bioinformatics efficient dynamic programming algorithm phosphorylation site assignment large scale mass spectrometry data high throughput tandem mass spectrometry data lc ms ms data phosphoproteomics phosphorylated residue biological context linear space dynamic programming strategy linear time dynamic programming strategy quality control signal to noise properties fragmentation spectra fragmentation strategy silac itraq;proteins;uk phd theses thesis;life sciences;mass spectroscopy;proteomics;uk research reports;medical journals;biochemistry;europe pmc;biomedical research;bioinformatics	Phosphorylation site assignment of large-scale data from high throughput tandem mass spectrometry (LC-MS/MS) data is an important aspect of phosphoproteomics. Correct assignment of phosphorylated residue(s) is important for functional interpretation of the data within a biological context. Common search algorithms (Sequest etc.) for mass spectrometry data are not designed for accurate site assignment; thus, additional algorithms are needed. In this paper, we propose a linear-time and linear-space dynamic programming strategy for phosphorylation site assignment. The algorithm, referred to as PhosSA, optimizes the objective function defined as the summation of peak intensities that are associated with theoretical phosphopeptide fragmentation ions. Quality control is achieved through the use of a post-processing criteria whose value is indicative of the signal-to-noise (S/N) properties and redundancy of the fragmentation spectra. The algorithm is tested using experimentally generated data sets of peptides with known phosphorylation sites while varying the fragmentation strategy (CID or HCD) and molar amounts of the peptides. The algorithm is also compatible with various peptide labeling strategies including SILAC and iTRAQ. PhosSA is shown to achieve > 99% accuracy with a high degree of sensitivity. The algorithm is extremely fast and scalable (able to process up to 0.5 million peptides in an hour). The implemented algorithm is freely available at http://helixweb.nih.gov/ESBL/PhosSA/ for academic purposes.	collision-induced dissociation;configuration interaction;dynamic programming;experiment;fragmentation (computing);ions;loss function;ninety nine;optimization problem;phosphorylation site;scalability;search algorithm;sequest scoring engine;signal-to-noise ratio;tandem mass spectrometry;throughput;time complexity;video post-processing	Fahad Saeed;Trairak Pisitkun;Jason D. Hoffert;Guanghui Wang;Marjan Gucek;Mark A. Knepper	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops	10.1109/BIBMW.2012.6470210	biology;computer science;bioinformatics;dynamic programming;proteomics	Visualization	-0.39254178286487507	-55.398113391009446	167783
6f6b643f1592c3ee1b2bbcfe8dd1a0b0a5e66550	gene analogue finder: a grid solution for finding functionally analogous gene products	software;animals;computational grid;limiting factor;sequence similarity;database management systems;databases genetic;computational biology bioinformatics;single machine;gene family;algorithms;humans;combinatorial libraries;potential function;computational biology;similarity function;computer appl in life sciences;biological process;microarrays;bioinformatics;gene ontology	To date more than 2,1 million gene products from more than 100000 different species have been described specifying their function, the processes they are involved in and their cellular localization using a very well defined and structured vocabulary, the gene ontology (GO). Such vast, well defined knowledge opens the possibility of compare gene products at the level of functionality, finding gene products which have a similar function or are involved in similar biological processes without relying on the conventional sequence similarity approach. Comparisons within such a large space of knowledge are highly data and computing intensive. For this reason this project was based upon the use of the computational GRID, a technology offering large computing and storage resources. We have developed a tool, GEN e AnaloG ue FIN dE r (ENGINE) that parallelizes the search process and distributes the calculation and data over the computational GRID, splitting the process into many sub-processes and joining the calculation and the data on the same machine and therefore completing the whole search in about 3 days instead of occupying one single machine for more than 5 CPU years. The results of the functional comparison contain potential functional analogues for more than 79000 gene products from the most important species. 46% of the analyzed gene products are well enough described for such an analysis to individuate functional analogues, such as well-known members of the same gene family, or gene products with similar functions which would never have been associated by standard methods. ENGINE has produced a list of potential functionally analogous relations between gene products within and between species using, in place of the sequence, the gene description of the GO, thus demonstrating the potential of the GO. However, the current limiting factor is the quality of the associations of many gene products from non-model organisms that often have electronic associations, since experimental information is missing. With future improvements of the GO, this limit will be reduced. ENGINE will manifest its power when it is applied to the whole GODB of more than 2,1 million gene products from more than 100000 organisms. The data produced by this search is planed to be available as a supplement to the GO database as soon as we are able to provide regular updates.	analog;cpu (central processing unit of computer system);central processing unit;computation (action);current limiting;gene ontology;gene family;greater than;grid computing;mental association;parallel computing;pierre robin syndrome;sequence alignment;vocabulary;cellular localization	Angelica Tulipano;Giacinto Donvito;Flavio Licciulli;Giorgio Pietro Maggi;Andreas Gisel	2007	BMC Bioinformatics	10.1186/1471-2105-8-329	biology;limiting factor;dna microarray;computer science;bioinformatics;theoretical computer science;gene family;data mining;biological process;genetics;algorithm	Comp.	0.3206121308348193	-56.91459274109261	167898
ab1d226e5b57f3b9e061e0dfb50418db745def22	recountdb: a database of mapped and count corrected transcribed sequences	databases nucleic acid;chromosome mapping;sequence alignment;sequence analysis rna;gene expression profiling;transcription initiation site	The field of gene expression analysis continues to benefit from next-generation sequencing generated data, which enables transcripts to be measured with unmatched accuracy and resolution. But the high-throughput reads from these technologies also contain many errors, which can compromise the ability to accurately detect and quantify rare transcripts. Fortunately, techniques exist to ameliorate the affects of sequencer error. We present RecountDB, a secondary database derived from primary data in NCBI's short read archive. RecountDB holds sequence counts from RNA-seq and 5' capped transcription start site experiments, corrected and mapped to the relevant genome. Via a searchable and browseable interface users can obtain corrected data in formats useful for transcriptomic analysis. The database is currently populated with 2265 entries from 45 organisms and continuously growing. RecountDB is publicly available at: http://recountdb.cbrc.jp.	base sequence;biopolymer sequencing;digital archive;experiment;high-throughput computing;interface device component;massively-parallel sequencing;microsequencer;national center for biotechnology information;population;rna;sequence read archive;throughput;transcript;transcription (software);transcription initiation site;format;mapped	Edward Wijaya;Martin C. Frith;Kiyoshi Asai;Paul Horton	2012		10.1093/nar/gkr1172	biology;molecular biology;bioinformatics;sequence alignment;gene expression profiling;genetics	Comp.	-0.43189099159016725	-56.385578295567356	168394
3b6887705b74fa55360ecb8cd12f67e7d9884e13	netprophet 2.0: mapping transcription factor networks by exploiting scalable data resources		"""Motivation Cells process information, in part, through transcription factor (TF) networks, which control the rates at which individual genes produce their products. A TF network map is a graph that indicates which TFs bind and directly regulate each gene. Previous work has described network mapping algorithms that rely exclusively on gene expression data and """"integrative"""" algorithms that exploit a wide range of data sources including chromatin immunoprecipitation sequencing (ChIP-seq) of many TFs, genome-wide chromatin marks, and binding specificities for many TFs determined in vitro . However, such resources are available only for a few major model systems and cannot be easily replicated for new organisms or cell types.   Results We present NetProphet 2.0, a """"data light"""" algorithm for TF network mapping, and show that it is more accurate at identifying direct targets of TFs than other, similarly data light algorithms. In particular, it improves on the accuracy of NetProphet 1.0, which used only gene expression data, by exploiting three principles. First, combining multiple approaches to network mapping from expression data can improve accuracy relative to the constituent approaches. Second, TFs with similar DNA binding domains bind similar sets of target genes. Third, even a noisy, preliminary network map can be used to infer DNA binding specificities from promoter sequences and these inferred specificities can be used to further improve the accuracy of the network map.   Availability Source code and comprehensive documentation are freely available at https://github.com/yiming-kang/NetProphet_2.0 .   Supplementary information Supplementary data are available at Bioinformatics online."""	base sequence;bioinformatics;chip-seq;dna binding site;data sources;documentation;gene expression;geographic information systems;graph (discrete mathematics);graph - visual representation;inference;network mapping;scalability;specimen source cvx/vag cyto;transcription factor;transcription (software);algorithm;chromatin immunoprecipitation	Yiming Kang;Hien-Haw Liow;Ezekiel J. Maier;Michael R. Brent	2017		10.1093/bioinformatics/btx563	computer science;scalability;transcription factor;bioinformatics	Comp.	2.48708802071281	-57.99172404753691	168598
2dbb0c1b7279d9112ebb57e25c8f456c2070da5b	normalization of circulating microrna expression data obtained by quantitative real-time rt-pcr	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	The high-throughput analysis of microRNAs (miRNAs) circulating within the blood of healthy and diseased individuals is an active area of biomarker research. Whereas quantitative real-time reverse transcription polymerase chain reaction (qPCR)-based methods are widely used, it is yet unresolved how the data should be normalized. Here, we show that a combination of different algorithms results in the identification of candidate reference miRNAs that can be exploited as normalizers, in both discovery and validation phases. Using the methodology considered here, we identify normalizers that are able to reduce nonbiological variation in the data and we present several case studies, to illustrate the relevance in the context of physiological or pathological scenarios. In conclusion, the discovery of stable reference miRNAs from high-throughput studies allows appropriate normalization of focused qPCR assays.	biological markers;circulating microrna;high-throughput computing;micrornas;normalize;real-time polymerase chain reaction;real-time clock;real-time transcription;relevance;reverse transcriptase polymerase chain reaction;reverse transcription;throughput;transcription (software);windows rt;algorithm	Francesco Marabita;Paola de Candia;Anna Torri;Jesper Tegnér;Sergio Abrignani;Riccardo L. Rossi	2016		10.1093/bib/bbv056	text mining;medical research;computer science;bioinformatics;data science;data mining	Visualization	2.66273895905929	-56.93718365190641	168702
69da16d1c7f9b1a93dc775065b4888a63f2b6da1	using convex hulls to extract interaction interfaces from known structures	convergent evolution;search space;distributed computing;multi domain;classification system;protein interaction;convex hull;protein data bank	MOTIVATION Protein interactions provide an important context for the understanding of function. Experimental approaches have been complemented with computational ones, such as PSIMAP, which computes domain-domain interactions for all multi-domain and multi-chain proteins in the Protein Data Bank (PDB). PSIMAP has been used to determine that superfamilies occurring in many species have many interaction partners, to show examples of convergent evolution through shared interaction partners and to uncover complexes in the interaction map. To determine an interaction, the original PSIMAP algorithm checks all residue pairs of any domain pair defined by classification systems such as SCOP. The computation takes several days for the PDB. The computation of PSIMAP has two shortcomings: first, the original PSIMAP algorithm considers only interactions of residue pairs rather than atom pairs losing information for detailed analysis of contact patterns. At the atomic level the original algorithm would take months. Second, with the superlinear growth of PDB, PSIMAP is not sustainable.   RESULTS We address these two shortcomings by developing a family of new algorithms for the computation of domain-domain interactions based on the idea of bounding shapes, which are used to prune the search space. The best of the algorithms improves on the old PSIMAP algorithm by a factor of 60 on the PDB. Additionally, the algorithms allow a distributed computation, which we carry out on a farm of 80 Linux PCs. Overall, the new algorithms reduce the computation at atomic level from months to 20 min. The combination of pruning and distribution makes the new algorithm scalable and sustainable even with the superlinear growth in PDB.	algorithm;classification;computation (action);distributed computing;eighty;interaction;linux;maxima and minima;numerous;protein data bank;scalability;scop	Panos Dafas;Dan M. Bolser;Jacek Gomoluch;Jong Park;Michael Schroeder	2004	Bioinformatics	10.1093/bioinformatics/bth106	biology;protein data bank;bioinformatics;theoretical computer science;convex hull;machine learning;mathematics;convergent evolution	Comp.	0.9765795166237476	-58.78196187029115	169573
1c173dc03fb1fd3ccd3991a70fa165641603b28d	graphalignment: bayesian pairwise alignment of biological networks	simulation and modeling;computer graphics;systems biology;bayes theorem;physiological cellular and medical topics;computational biology bioinformatics;algorithms;computational biology;bioinformatics	With increased experimental availability and accuracy of bio-molecular networks, tools for their comparative and evolutionary analysis are needed. A key component for such studies is the alignment of networks. We introduce the Bioconductor package GraphAlignment for pairwise alignment of bio-molecular networks. The alignment incorporates information both from network vertices and network edges and is based on an explicit evolutionary model, allowing inference of all scoring parameters directly from empirical data. We compare the performance of our algorithm to an alternative algorithm, Græmlin 2.0. On simulated data, GraphAlignment outperforms Græmlin 2.0 in several benchmarks except for computational complexity. When there is little or no noise in the data, GraphAlignment is slower than Græmlin 2.0. It is faster than Græmlin 2.0 when processing noisy data containing spurious vertex associations. Its typical case complexity grows approximately as O ( N 2 . 6 ) . On empirical bacterial protein-protein interaction networks (PIN) and gene co-expression networks, GraphAlignment outperforms Græmlin 2.0 with respect to coverage and specificity, albeit by a small margin. On large eukaryotic PIN, Græmlin 2.0 outperforms GraphAlignment. The GraphAlignment algorithm is robust to spurious vertex associations, correctly resolves paralogs, and shows very good performance in identification of homologous vertices defined by high vertex and/or interaction similarity. The simplicity and generality of GraphAlignment edge scoring makes the algorithm an appropriate choice for global alignment of networks.	algorithm;artificial neural network;bacterial proteins;bioconductor;biological network;british informatics olympiad;computational complexity theory;emoticon;gene co-expression network;homology (biology);inference;mental association;models of dna evolution;score;sensitivity and specificity;sequence alignment;signal-to-noise ratio;vertex (geometry)	Michal Kolář;Jörn Meier;Ville Mustonen;Michael Lässig;Johannes Berg	2012		10.1186/1752-0509-6-144	computational biology;biology;biological computation;computer science;bioinformatics;theoretical computer science;bayes' theorem;computer graphics;systems biology;computational genomics	ML	2.0084867319133304	-52.590940425388375	169761
3009215b6b6dbf01a9ea3abbf8d8acb4ddf700d5	dna repeats detection using a dedicated dot-plot analysis	dna distances;numerical similarities;dot plot analysis;dna numerical representations;numerical similarities dna repeats dna numerical representations dna distances dot plot analysis;dna feature extraction algorithm design and analysis bioinformatics satellites genomics digital signal processing;image processing techniques dna repeat detection genome evolution diseases digital signal processing field dsp field numerical representation mapping algorithm custom dot plot analysis;dna repeats;numerical analysis dna genomics image processing molecular biophysics	DNA repeats are believed to play significant roles in genome evolution and diseases. Many of the methods for finding repeated sequences are part of the digital signal processing (DSP) field and most of these methods use distances, similarities and consensus sequences to generate candidate sequences. This paper presents results obtained using a dedicated numerical representation with a mapping algorithm (using DNA distances and consensus types) and a custom dot-plot analysis (using similarities to represent DNA patterns) combined with image processing techniques, to isolate the position of DNA patterns with different lengths.	algorithm;consensus sequence;dna patterns;digital signal processing;dot plot (bioinformatics);image processing;numerical analysis	Petre G. Pop	2015	2015 38th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2015.7296434	bioinformatics	Robotics	0.8361972077216017	-56.57909606706857	169978
b958cccfaf524099c8d58ad184d054f7e72e9583	fluorescent automated dna sequencers: a file exchange program	dna;file transfer;sequencer;computer program;instrumentation;instrumentacion;transferencia fichero;appareillage;secuencia nucleotido;nucleotide sequence;sequence nucleotide;transfert fichier;dna sequence;programa computador;sequenceur;programme ordinateur;secuenciador			G. De Bellis;I. R. Consani;M. Manoni;R. Pergolizzi;M. Luzzana	1992	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/8.2.195	biology;dna sequencing;molecular biology;nucleic acid sequence;bioinformatics;genetics;dna;instrumentation	SE	-4.452511441840417	-56.49046649251822	170377
442813dd569da4f7868bbf09bf284cf2d7ef3b70	de-novo discovery of differentially abundant transcription factor binding sites including their positional preference	microarray data;indoleacetic acids;biological patents;animals;learning process;biomedical journals;motif discovery;text mining;europe pubmed central;gene regulation;citation search;transcription factor binding site;databases genetic;transcription factors;binding site;binding sites;citation networks;arabidopsis thaliana;chip;gene expression;models genetic;distributed learning;research articles;promoter regions genetic;transcription factor;abstracts;open access;indexation;arabidopsis;life sciences;clinical guidelines;models statistical;dna plant;arabidopsis proteins;genes plant;humans;transcription start site;full text;computational biology;rest apis;orcids;europe pmc;biomedical research;open source;bioinformatics;literature search;gene ontology	Transcription factors are a main component of gene regulation as they activate or repress gene expression by binding to specific binding sites in promoters. The de-novo discovery of transcription factor binding sites in target regions obtained by wet-lab experiments is a challenging problem in computational biology, which has not been fully solved yet. Here, we present a de-novo motif discovery tool called Dispom for finding differentially abundant transcription factor binding sites that models existing positional preferences of binding sites and adjusts the length of the motif in the learning process. Evaluating Dispom, we find that its prediction performance is superior to existing tools for de-novo motif discovery for 18 benchmark data sets with planted binding sites, and for a metazoan compendium based on experimental data from micro-array, ChIP-chip, ChIP-DSL, and DamID as well as Gene Ontology data. Finally, we apply Dispom to find binding sites differentially abundant in promoters of auxin-responsive genes extracted from Arabidopsis thaliana microarray data, and we find a motif that can be interpreted as a refined auxin responsive element predominately positioned in the 250-bp region upstream of the transcription start site. Using an independent data set of auxin-responsive genes, we find in genome-wide predictions that the refined motif is more specific for auxin-responsive genes than the canonical auxin-responsive element. In general, Dispom can be used to find differentially abundant motifs in sequences of any origin. However, the positional distribution learned by Dispom is especially beneficial if all sequences are aligned to some anchor point like the transcription start site in case of promoter sequences. We demonstrate that the combination of searching for differentially abundant motifs and inferring a position distribution from the data is beneficial for de-novo motif discovery. Hence, we make the tool freely available as a component of the open-source Java framework Jstacs and as a stand-alone application at http://www.jstacs.de/index.php/Dispom.	activation action;alignment;aquaporin 1;benchmark (computing);binding sites;chip-on-chip;compendium;computation;computational biology;consensus sequence;dvb-s2;digital subscriber line;discrimination learning;experiment;gene expression regulation;gene ontology;geographic information systems;heuristic;heuristics;hyperactive behavior;java programming language;manuscripts;medical transcription;megabyte;microarray;normal statistical distribution;nucleotides;open-source software;portable document format;precision and recall;promoter regions, genetic;randomness;sequence motif;silo (dataset);simulation;strand (programming language);test data;transcription (software);transcription initiation site;zip code;promoter;transcription factor binding	Jens Keilwagen;Jan Grau;Ivan A. Paponov;Stefan Posch;Marc Strickert;Ivo Grosse	2011		10.1371/journal.pcbi.1001070	biology;text mining;bioinformatics;binding site;data mining;world wide web;genetics;dna binding site;sequence motif;transcription factor	Comp.	1.3980496435946432	-56.703415216300364	170595
4dd62478e96ae0f2878b195edff618d66ec14f8c	discovery by minimal length encoding: a case study in molecular evolution	repetitive element;data compression;alu sequences;repetitive dna;good practice;molecular evolution;human genome;repetitive elements;machine discovery	We apply the Minimal Length Encoding Principle to formalize inference about the evolution of macromolecular sequences. The Principle is shown to imply a combination of Weighted Parsimony and Compatibility methods that have long been used by biologists because of their good practical performance. The background assumptions are expressed as an encoding scheme for the observed data and as heuristic rules for selection of diagnostic positions in the sequences. The Principle was applied to discover new subfamilies of Alu sequences, the most numerous family of repetitive DNA sequences in the human genome.	heuristic;line code;occam's razor;run-length encoding	Aleksandar Milosavljevic;Jerzy Jurka	1993	Machine Learning	10.1007/BF00993061	data compression;human genome;repeated sequence;molecular evolution;bioinformatics	Comp.	0.7407064534045451	-52.78794176414294	170901
c644b9cf791dea703d0d4a17bebd9a934a77a360	snp-search: simple processing, manipulation and searching of snps from high-throughput sequencing	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;microarrays;bioinformatics	A typical bacterial pathogen genome mapping project can identify thousands of single nucleotide polymorphisms (SNP). Interpreting SNP data is complex and it is difficult to conceptualise the data contained within the large flat files that are the typical output from most SNP calling algorithms. One solution to this problem is to construct a database that can be queried using simple commands so that SNP interrogation and output is both easy and comprehensible. Here we present snp-search, a tool that manages SNP data and allows for manipulation and searching of SNP data. After creation of a SNP database from a VCF file, snp-search can be used to convert the selected SNP data into FASTA sequences, construct phylogenies, look for unique SNPs, and output contextual information about each SNP. The FASTA output from snp-search is particularly useful for the generation of robust phylogenetic trees that are based on SNP differences across the conserved positions in whole genomes. Queries can be designed to answer critical genomic questions such as the association of SNPs with particular phenotypes. snp-search is a tool that manages SNP data and outputs useful information which can be used to test important biological hypotheses.	biopolymer sequencing;contain (action);fasta;genetic polymorphism;genome;high-throughput computing;nitroprusside;pathogenic organism;phenotype;phylogenetic tree;phylogenetics;snp array;single nucleotide polymorphism;throughput;trees (plant);variant call format;algorithm	Ali Al-Shahib;Anthony Underwood	2013		10.1186/1471-2105-14-326	biology;dna microarray;computer science;bioinformatics;data science;data mining;genetics	Comp.	-1.8696152701734527	-58.86014739167172	170961
492c962d39a1e0470a533a228a3e9a9a76581387	finding motifs from all sequences with and without binding sites	binding site;transcription factor;molecular biology;article;heuristic algorithm	MOTIVATION Finding common patterns, motifs, from a set of promoter regions of coregulated genes is an important problem in molecular biology. Most existing motif-finding algorithms consider a set of sequences bound by the transcription factor as the only input. However, we can get better results by considering sequences that are not bound by the transcription factor as an additional input.   RESULTS First, instead of using the simple hyper-geometric analysis, we propose to calculate the likelihood based on a more precise probabilistic analysis which considers motif length, sequence length and number of binding sites as input parameters for testing whether motif is found. Second, we adopt an heuristic algorithm bases on our analysis to find motifs. For the simulated and real datasets, our algorithm ALSE compares favorably against common motif-finding programs such as SeedSearch and MEME in all cases and performs very well, especially when each input sequence contains more than one binding site.   AVAILABILITY ALSE is available for download at the homepage http://alse.cs.hku.hk   CONTACT cmleung2@cs.hku.hk.	base;binding sites;download;geometric analysis;greater than;heuristic (computer science);home page;hyperactive behavior;meme;molecular biology;motif;probabilistic analysis of algorithms;promoter regions, genetic;transcription factor;transcription (software);algorithm	Henry C. M. Leung;Francis Y. L. Chin	2006	Bioinformatics	10.1093/bioinformatics/btl371	heuristic;biology;bioinformatics;binding site;data mining;world wide web;transcription factor	Comp.	1.0496489291453233	-53.84182372728854	172043
49212a20d9eda436ef3925b92868e8364ba03ed3	a novel approach for motif identification in unaligned molecular sequences	dna;biology computing;genomics;sequences;motif identification algorithm;unaligned molecular sequences;fourier transform;decoding;gibbs sampler;molecular biophysics biology computing dna fourier transforms;dna sequences;prediction algorithms;motif identification;repetitive dna;transcription factor unaligned molecular sequences repetitive dna sequences fourier transform motif identification algorithm;accuracy;repetitive dna sequences;proteins;transcription factor;satellites;fourier transforms;repeats;molecular biophysics;diseases;computer science;probabilistic logic;repetitive sequence;dna sequence;pulse width modulation;transcription factor motif identification dna sequences repeats;sequences dna satellites bioinformatics computer science pulse width modulation genomics diseases proteins decoding;bioinformatics	Repetitive sequences of DNA are important to a number of regulatory functions; hence, to find repetitive DNA sequences is of considerable importance and worth to study. The purpose of this paper is to look for all possible repetitive sequences of DNA in a great deal and then to provide the biologists the repetitive sequences to do further work. Our approach first predicts the possible lengths of repetitive sequences based on the Fourier transform, and then proposes a so-called motif-identification algorithm (MIA) to find the repetitive patterns of the corresponding motif lengths. Our method compares favorably against some common motif-finding approaches, such as Spectral Repeat Finder and Gibbs sampler in all cases, and performs very well for the synthetic and real datasets.	algorithm;gibbs sampling;motif;sampling (signal processing);synthetic intelligence	Rong-Ming Chen;M. T. Hou;Jeffrey J. P. Tsai	2009	2009 Ninth IEEE International Conference on Bioinformatics and BioEngineering	10.1109/BIBE.2009.43	biology;fourier transform;dna sequencing;genomics;molecular biology;computer science;bioinformatics;genetics;molecular biophysics	Visualization	0.12240138757245951	-52.813657849310026	172181
6fb04133f804bff359a494979ae4ce7bd2db72a5	focusstack and stimserver: a new open source matlab toolchain for visual stimulation and analysis of two-photon calcium neuronal imaging data	small memory footprint;biological patents;biomedical journals;text mining;europe pubmed central;citation search;neuronal responses;citation networks;two photon calcium imaging;research articles;analysis toolbox;abstracts;open access;life sciences;clinical guidelines;visual stimulus generation;full text;matlab;rest apis;orcids;europe pmc;biomedical research;open source;bioinformatics;literature search	Two-photon calcium imaging of neuronal responses is an increasingly accessible technology for probing population responses in cortex at single cell resolution, and with reasonable and improving temporal resolution. However, analysis of two-photon data is usually performed using ad-hoc solutions. To date, no publicly available software exists for straightforward analysis of stimulus-triggered two-photon imaging experiments. In addition, the increasing data rates of two-photon acquisition systems imply increasing cost of computing hardware required for in-memory analysis. Here we present a Matlab toolbox, FocusStack, for simple and efficient analysis of two-photon calcium imaging stacks on consumer-level hardware, with minimal memory footprint. We also present a Matlab toolbox, StimServer, for generation and sequencing of visual stimuli, designed to be triggered over a network link from a two-photon acquisition system. FocusStack is compatible out of the box with several existing two-photon acquisition systems, and is simple to adapt to arbitrary binary file formats. Analysis tools such as stack alignment for movement correction, automated cell detection and peri-stimulus time histograms are already provided, and further tools can be easily incorporated. Both packages are available as publicly-accessible source-code repositories.	binary file;biopolymer sequencing;calcium;computation (action);computer hardware;experiment;hoc (programming language);in-memory database;matlab;memory footprint;open-source software;out of the box (feature);photic stimulation;photons;repository (version control);thinking outside the box;toolchain;format	Dylan Richard Muir;Björn M Kampa	2014		10.3389/fninf.2014.00085	text mining;neuroscience;medical research;computer science;bioinformatics;data science;data mining	Visualization	-2.390371991510944	-57.39016445358251	173670
4cae0d1d10db8c84aedcf7ef7ef872ba604f4845	sssnper: identifying statistically similar snps to aid interpretation of genetic association studies	genetic association;follow up study	UNLABELLED ssSNPer is a novel user-friendly web interface that provides easy determination of the number and location of untested HapMap SNPs, in the region surrounding a tested HapMap SNP, which are statistically similar and would thus produce comparable and perhaps more significant association results. Identification of ssSNPs can have crucial implications for the interpretation of the initial association results and the design of follow-up studies.   AVAILABILITY http://fraser.qimr.edu.au/general/daleN/ssSNPer/	genetic association studies;international hapmap project;nitroprusside;single nucleotide polymorphism;usability;user interface	Dale R. Nyholt	2006	Bioinformatics	10.1093/bioinformatics/btl518	biology;bioinformatics;genetic association;genetics	Comp.	0.893058030394073	-58.117378407828106	173776
08b26a5512d2efae73800d0f5a40ad55efd88241	coxpresdb in 2015: coexpression database for animal species by dna-microarray and rnaseq-based expression data with multiple quality assessment systems	animals;mice;data interpretation statistical;databases genetic;humans;sequence analysis rna;gene expression profiling;oligonucleotide array sequence analysis	The COXPRESdb (http://coxpresdb.jp) provides gene coexpression relationships for animal species. Here, we report the updates of the database, mainly focusing on the following two points. For the first point, we added RNAseq-based gene coexpression data for three species (human, mouse and fly), and largely increased the number of microarray experiments to nine species. The increase of the number of expression data with multiple platforms could enhance the reliability of coexpression data. For the second point, we refined the data assessment procedures, for each coexpressed gene list and for the total performance of a platform. The assessment of coexpressed gene list now uses more reasonable P-values derived from platform-specific null distribution. These developments greatly reduced pseudo-predictions for directly associated genes, thus expanding the reliability of coexpression data to design new experiments and to discuss experimental results.	dna microarray;experiment;null value;platform-specific model;pseudo brand of pseudoephedrine	Yasunobu Okamura;Yuichi Aoki;Takeshi Obayashi;Shu Tadaka;Satoshi Ito;Takafumi Narise;Kengo Kinoshita	2015		10.1093/nar/gku1163	biology;molecular biology;bioinformatics;gene expression profiling;genetics	DB	1.7769326998038009	-56.256516414329894	173961
0c03dbc8dae953a21603ecc7cdf84d22d57b4359	algorithms to model single gene, single chromosome, and whole genome copy number changes jointly in tumor phylogenetics	breast neoplasms;female;disease progression;databases genetic;dna copy number variations;models genetic;uterine cervical neoplasms;algorithms;humans;neoplasms;computational biology;computer simulation	We present methods to construct phylogenetic models of tumor progression at the cellular level that include copy number changes at the scale of single genes, entire chromosomes, and the whole genome. The methods are designed for data collected by fluorescence in situ hybridization (FISH), an experimental technique especially well suited to characterizing intratumor heterogeneity using counts of probes to genetic regions frequently gained or lost in tumor development. Here, we develop new provably optimal methods for computing an edit distance between the copy number states of two cells given evolution by copy number changes of single probes, all probes on a chromosome, or all probes in the genome. We then apply this theory to develop a practical heuristic algorithm, implemented in publicly available software, for inferring tumor phylogenies on data from potentially hundreds of single cells by this evolutionary model. We demonstrate and validate the methods on simulated data and published FISH data from cervical cancers and breast cancers. Our computational experiments show that the new model and algorithm lead to more parsimonious trees than prior methods for single-tumor phylogenetics and to improved performance on various classification tasks, such as distinguishing primary tumors from metastases obtained from the same patient population.	chromosomes;color gradient;computation;copy number;edit distance;experiment;fluorescence;fluorescent in situ hybridization;gain;heuristic (computer science);mammary neoplasms;models of dna evolution;neck;nucleic acid hybridization;occam's razor;patients;phylogenetics;scientific publication;semantic heterogeneity;trees (plant);tumor progression;algorithm;cervical cancer;primary tumor	Salim Akhter Chowdhury;Stanley Shackney;Kerstin Heselmeyer-Haddad;Thomas Ried;Alejandro A. Schäffer;Russell Schwartz	2014		10.1371/journal.pcbi.1003740	computer simulation;computational biology;biology;copy number analysis;bioinformatics;genetics	Comp.	2.1190794480200674	-52.67780332389254	174194
862cf962788fab4e2b2dac65c0c6dad05512f12b	selecting pcr designed mismatch primers to create diagnostic restriction sites	microordenador;computer program;primer;basic;amorce;reaction chaine polymerase;mixto;microordinateur;microcomputer;polymerase chain reaction;reaccion cadena polimerasa;site restriction;oligonucleotide;ibm pc compatible;oligonucleotido;programa computador;programme ordinateur			Lance S Davidow	1992	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/8.2.193	biology;molecular biology;real-time computing;computer science;operating system;polymerase chain reaction;microcomputer;ibm pc compatible;genetics;primer;oligonucleotide	HCI	-4.468190338330012	-56.353587090183744	174235
20d3107eb8343941646c794d8549e0c32bbecc0a	invariant based quartet puzzling	health research;uk clinical guidelines;biological patents;maximum likelihood;europe pubmed central;citation search;physiological cellular and medical topics;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;uk research reports;medical journals;maximum likelihood method;europe pmc;biomedical research;bioinformatics	First proposed by Cavender and Felsenstein, and Lake, invariant based algorithms for phylogenetic reconstruction were widely dismissed by practicing biologists because invariants were perceived to have limited accuracy in constructing trees based on DNA sequences of reasonable length. Recent developments by algebraic geometers have led to the construction of lists of invariants which have been demonstrated to be more accurate on small sequences, but were limited in that they could only be used for trees with small numbers of taxa. We have developed and tested an invariant based quartet puzzling algorithm which is accurate and efficient for biologically reasonable data sets. We found that our algorithm outperforms Maximum Likelihood based quartet puzzling on data sets simulated with low to medium evolutionary rates. For faster rates of evolution, invariant based quartet puzzling is reasonable but less effective than maximum likelihood based puzzling. This is a proof of concept algorithm which is not intended to replace existing reconstruction algorithms. Rather, the conclusion is that when seeking solutions to a new wave of phylogenetic problems (super tree algorithms, gene vs. species tree, mixture models), invariant based methods should be considered. This article demonstrates that invariants are a practical, reasonable and flexible source for reconstruction techniques.	algorithm;dismiss;invariant (computer science);linear algebra;mixture model;phylogenetics;trees (plant)	Joseph P. Rusinko;Brian Hipp	2012		10.1186/1748-7188-7-35	biology;computer science;bioinformatics;data science;data mining;mathematics;maximum likelihood;algorithm	Comp.	2.4229687030281153	-52.90176802162165	174425
67744879f1e407e6a0604c7ffd79661da921b17a	local weighting schemes for protein multiple sequence alignment	score function;profile;dynamic program;multiple sequence alignment;sequence motif;weighting schemes;multiple alignment	This paper describes three weighting schemes for improving the accuracy of progressive multiple sequence alignment methods: (1) global profile pre-processing, to capture for each sequence information about other sequences in a profile before the actual multiple alignment takes place; (2) local pre-processing; which incorporates a new protocol to only use non-overlapping local sequence regions to construct the pre-processed profiles; and (3) local-global alignment, a weighting scheme based on the double dynamic programming (DDP) technique to softly bias global alignment to local sequence motifs. The first two schemes allow the compilation of residue-specific multiple alignment reliability indices, which can be used in an iterative fashion. The schemes have been implemented with associated iterative modes in the PRALINE multiple sequence alignment method, and have been evaluated using the BAliBASE benchmark alignment database. These tests indicate that PRALINE is a toolbox able to build alignments with very high quality. We found that local profile pre-processing raises the alignment quality by 5.5% compared to PRALINE alignments generated under default conditions. Iteration enhances the quality by a further percentage point. The implications of multiple alignment scoring functions and iteration in relation to alignment quality and benchmarking are discussed.	benchmark (computing);compiler;dna breaks, double-stranded;data-directed programming;default;display resolution;dynamic programming;iteration;list of sequence alignment software;multiple sequence alignment;preprocessor;score;scoring functions for docking;sequence motif	Jaap Heringa	2002	Computers & chemistry	10.1016/S0097-8485(02)00008-6	biology;multiple sequence alignment;computer science;bioinformatics;data mining	Comp.	0.07683643201630085	-55.06137599440923	174500
7dedf4bc7f99b2995e001b1b2f5f7bd87e3380bf	enhancing protein homology batch search algorithm with sequence compression and clustering	databases;acceleration	Homology search is a tremendous application of bioinformatics in the field of molecular biology, protein function analysis and drug development. To perform batch search in the growing database, the basic approach is to run Blast on each of the original queries or concatenate queries by grouping them together. This paper proposes an enhanced protein homology batch search algorithm with sequence compression and clustering (C2-BLASTP), which takes advantage of the joint information among the query sequences as well as the database. In C2-BLASTP, the queries and database are firstly compressed by redundancy analysis. And then the database is clustered according to subsequence similarity. Following this, hits finding can be implemented in the clustered database. Furthermore, a final execution database is reconstructed based on potential hits to mitigate the increasing scale of the sequence database. Finally, homology batch search is performed in execution database. Experiments on NCBI NR database demonstrate the effectiveness of the C2-BLASTP for homology batch search in terms of homology accuracy, search speed and memory usage.	blast;bioinformatics;cluster analysis;concatenation;homology (biology);homology modeling;noise reduction;search algorithm;sequence database;xslt/muenchian grouping	Hong-Wei Ge;Liang Sun;Jinghong Yu	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822809	acceleration;computer science;bioinformatics;data mining;database	DB	-1.9086565728000644	-53.0875409749708	175028
58bca4db032fc22c413a457251801b7bde1d6290	sequence optimization as an alternative to de novo analysis of tandem mass spectrometry data	de novo sequencing;amino acid sequence;spectrum;genetics;tandem mass spectrometry;peptides environmental molecular sciences laboratory;heuristic optimization;algorithms;genetic algorithm;optimization;mass spectroscopy;basic biological sciences	MOTIVATION Peptide identification following tandem mass spectrometry (MS/MS) is usually achieved by searching for the best match between the mass spectrum of an unidentified peptide and model spectra generated from peptides in a sequence database. This methodology will be successful only if the peptide under investigation belongs to an available database. Our objective is to develop and test the performance of a heuristic optimization algorithm capable of dealing with some features commonly found in actual MS/MS spectra that tend to stop simpler deterministic solution approaches.   RESULTS We present the implementation of a Genetic Algorithm (GA) in the reconstruction of amino acid sequences using only spectral features, discuss some of the problems associated with this approach and compare its performance to a de novo sequencing method. The GA can potentially overcome some of the most problematic aspects associated with de novo analysis of real MS/MS data such as missing or unclearly defined peaks and may prove to be a valuable tool in the proteomics field. We assess the performance of our algorithm under conditions of perfect spectral information, in situations where key spectral features are missing, and using real MS/MS spectral data.	amino acid sequence;amino acids;biopolymer sequencing;de novo transcriptome assembly;genetic algorithm;heuristic;mathematical optimization;proteomics;sequence database;software release life cycle;tandem mass spectrometry	Alejandro Heredia-Langner;William R. Cannon;Kenneth D. Jarman;Kristin H. Jarman	2004	Bioinformatics	10.1093/bioinformatics/bth242	tandem mass spectrometry;biology;spectrum;genetic algorithm;mass spectrometry;computer science;bioinformatics;peptide sequence;genetics	Comp.	0.8605233020247964	-53.300997051897575	175110
973a45ee5fe67a7c2486ed5fa99aa918cba23919	img2net: automated network-based analysis of imaged phenotypes		SUMMARY Automated analysis of imaged phenotypes enables fast and reproducible quantification of biologically relevant features. Despite recent developments, recordings of complex networked structures, such as leaf venation patterns, cytoskeletal structures or traffic networks, remain challenging to analyze. Here we illustrate the applicability of img2net to automatedly analyze such structures by reconstructing the underlying network, computing relevant network properties and statistically comparing networks of different types or under different conditions. The software can be readily used for analyzing image data of arbitrary 2D and 3D network-like structures.   AVAILABILITY AND IMPLEMENTATION img2net is open-source software under the GPL and can be downloaded from http://mathbiol.mpimp-golm.mpg.de/img2net/, where supplementary information and datasets for testing are provided.   CONTACT breuer@mpimp-golm.mpg.de.	computation (action);cytoskeleton;electronic supplementary materials;il31ra gene;open-source software;phenotype;quantitation	David Breuer;Zoran Nikoloski	2014	Bioinformatics	10.1093/bioinformatics/btu503	computer science;bioinformatics;theoretical computer science;data mining	Comp.	-0.33828956319506065	-59.095900111772224	175252
34db5716f6177c9ad73360bdc93a909df10fd20d	modbase, a database of annotated comparative protein structure models	forecasting;animals;escherichia coli;caenorhabditis elegans;complete genome;saccharomyces cerevisiae;homo sapiens;model assessment;database management systems;computer graphics;protein sequence;amino acid sequence;web interface;relational database;arabidopsis thaliana;protein structure;models molecular;target selection;3d model;comparative modeling;internet;proteins;protein conformation;interprofessional education;protein structure tertiary;structural genomics;ribosomal proteins;structure determination;drosophila melanogaster;genome;human;sequence homology amino acid;relational database management system;humans;molecular sequence data;sequence alignment;user computer interface;databases factual;information storage and retrieval;basic local alignment search tool;databases protein	ModBase is a queryable database of annotated comparative protein structure models. The models are derived by ModPipe, an automated modeling pipeline relying on the programs PSI-BLAST and Modeller. The database currently contains 3D models for substantial portions of approximately 17 000 proteins from 10 complete genomes, including those of Caenorhabditis elegans, Saccharomyces cerevisiae and Escherichia coli, as well as all the available sequences from Arabidopsis thaliana and Homo sapiens. The database also includes fold assignments and alignments on which the models were based. In addition, special care is taken to assess the quality of the models. ModBase is accessible through a web interface at http://guitar.rockefeller.edu/modbase/	homology modeling;modbase	Roberto Sánchez;Ursula Pieper;Nebojsa Mirkovic;Paul I. W. de Bakker;Edward Wittenstein;Andrej Sali	2000	Nucleic Acids Research	10.1093/nar/28.1.250	biology;protein structure;bioinformatics;genetics	Vision	-0.8728417208252507	-59.0878530192708	175319
64ef683283a13a2b7ad443f7cf07d1a9ac98df53	dass: efficient discovery and p-value calculation of substructures in unordered data	protein complex;protein domains;statistical significance;multi domain;transcription factor;data mining algorithm;biological data;protein interaction	MOTIVATION Pattern identification in biological sequence data is one of the main objectives of bioinformatics research. However, few methods are available for detecting patterns (substructures) in unordered datasets. Data mining algorithms mainly developed outside the realm of bioinformatics have been adapted for that purpose, but typically do not determine the statistical significance of the identified patterns. Moreover, these algorithms do not exploit the often modular structure of biological data.   RESULTS We present the algorithm DASS (Discovery of All Significant Substructures) that first identifies all substructures in unordered data (DASS(Sub)) in a manner that is especially efficient for modular data. In addition, DASS calculates the statistical significance of the identified substructures, for sets with at most one element of each type (DASS(P(set))), or for sets with multiple occurrence of elements (DASS(P(mset))). The power and versatility of DASS is demonstrated by four examples: combinations of protein domains in multi-domain proteins, combinations of proteins in protein complexes (protein subcomplexes), combinations of transcription factor target sites in promoter regions and evolutionarily conserved protein interaction subnetworks.   AVAILABILITY The program code and additional data are available at http://www.fli-leibniz.de/tsb/DASS	approximation algorithm;bioinformatics;bioinformatics;conserved sequence;data mining;geographic information systems;molecular biology;numerous;p-value;promoter regions, genetic;protein domain;sensor;small;transcription factor;transcription (software);verification of theories;verloes bourguignon syndrome;algorithm	Jens Hollunder;Maik Friedel;Andreas Beyer;Christopher T. Workman;Thomas Wilhelm-Stein	2007	Bioinformatics	10.1093/bioinformatics/btl511	biology;biological data;bioinformatics;machine learning;data mining;multiprotein complex;statistical significance;protein domain;genetics;transcription factor	Comp.	1.8742468897501707	-58.281272498985764	175582
2e16a3e7d02d86f95096daa7c9113e89ad754e04	fastmg: a simple, fast, and accurate maximum likelihood procedure to estimate amino acid replacement rate matrices from large data sets	evolution molecular;animals;probability;phylogeny;large data sets;amino acid substitution;computational biology bioinformatics;protein alignments;proteins;likelihood functions;amino acid replacement rate matrices;phylogenetic trees;maximum likelihood methods;algorithms;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Amino acid replacement rate matrices are a crucial component of many protein analysis systems such as sequence similarity search, sequence alignment, and phylogenetic inference. Ideally, the rate matrix reflects the mutational behavior of the actual data under study; however, estimating amino acid replacement rate matrices requires large protein alignments and is computationally expensive and complex. As a compromise, sub-optimal pre-calculated generic matrices are typically used for protein-based phylogeny. Sequence availability has now grown to a point where problem-specific rate matrices can often be calculated if the computational cost can be controlled. The most time consuming step in estimating rate matrices by maximum likelihood is building maximum likelihood phylogenetic trees from protein alignments. We propose a new procedure, called FastMG, to overcome this obstacle. The key innovation is the alignment-splitting algorithm that splits alignments with many sequences into non-overlapping sub-alignments prior to estimating amino acid replacement rates. Experiments with different large data sets showed that the FastMG procedure was an order of magnitude faster than without splitting. Importantly, there was no apparent loss in matrix quality if an appropriate splitting procedure is used. FastMG is a simple, fast and accurate procedure to estimate amino acid replacement rate matrices from large data sets. It enables researchers to study the evolutionary relationships for specific groups of proteins or taxa with optimized, data-specific amino acid replacement rate matrices. The programs, data sets, and the new mammalian mitochondrial protein rate matrix are available at http://fastmg.codeplex.com .	algorithm;algorithmic efficiency;amino acids;analysis of algorithms;computation;computational phylogenetics;estimated;experiment;inference;mammals;mitochondrial proteins;phylogenetic tree;protein analysis;sequence alignment;similarity search;trees (plant)	Cuong Cao Dang;Le Sy Vinh;Olivier Gascuel;Bart Hazes;Si Quang Le	2014		10.1186/1471-2105-15-341	biology;phylogenetic tree;dna microarray;bioinformatics;probability;phylogenetics	Comp.	0.20778764641034242	-53.440735214030056	176253
e4385f40a371b0bf0078a7ce2facaa71ea898658	database clustering with a combination of fingerprint and maximum common substructure methods		We present an efficient method to cluster large chemical databases in a stepwise manner. Databases are first clustered with an extended exclusion sphere algorithm based on Tanimoto coefficients calculated from Daylight fingerprints. Substructures are then extracted from clusters by iterative application of a maximum common substructure algorithm. Clusters with common substructures are merged through a second application of an exclusion sphere algorithm. In a separate step, singletons are compared to cluster substructures and added to a cluster if similarity is sufficiently high. The method identifies tight clusters with conserved substructures and generates singletons only if structures are truly distinct from all other library members. The method has successfully been applied to identify the most frequently occurring scaffolds in databases, for the selection of analogues of screening hits and in the prioritization of chemical libraries offered by commercial vendors.		Martin Stahl;Harald Mauser	2005	Journal of chemical information and modeling	10.1021/ci050011h	substructure;bioinformatics;database;cluster (physics);prioritization;cluster analysis;chemical database;mathematics	HPC	1.5549447033679136	-57.11745752198823	176287
2900aec97184ed081a19afbb9812af0c58e8aca1	glycopattern: a web platform for glycan array mining		UNLABELLED GlycoPattern is Web-based bioinformatics resource to support the analysis of glycan array data for the Consortium for Functional Glycomics. This resource includes algorithms and tools to discover structural motifs, a heatmap visualization to compare multiple experiments, hierarchical clustering of Glycan Binding Proteins with respect to their binding motifs and a structural search feature on the experimental data.   AVAILABILITY AND IMPLEMENTATION GlycoPattern is freely available on the Web at http://glycopattern.emory.edu with all major browsers supported.	algorithm;bioinformatics;cluster analysis;consortium;experiment;glycomics;heatmap;hierarchical clustering;polysaccharides;web search engine;webplatform;world wide web;statistical cluster	Sanjay Agravat;Joel H. Saltz;Richard D. Cummings;David F. Smith	2014	Bioinformatics	10.1093/bioinformatics/btu559	data mining;software;experimental data;visualization;microarray analysis techniques;structural motif;hierarchical clustering;computer science;bioinformatics;glycan;glycomics	Comp.	-1.5464674399938696	-58.94377083660694	176293
2317339f221cf550c89a789a61d3b676ae096156	nextgenmap: fast and accurate read mapping in highly polymorphic genomes		SUMMARY When choosing a read mapper, one faces the trade off between speed and the ability to map reads in highly polymorphic regions. Here, we report NextGenMap, a fast and accurate read mapper, which reduces this dilemma. NextGenMap aligns reads reliably to a reference genome even when the sequence difference between target and reference genome is large, i.e. highly polymorphic genome. At the same time, NextGenMap outperforms current mapping methods with respect to runtime and to the number of correctly mapped reads. NextGenMap efficiently uses the available hardware by exploiting multi-core CPUs as well as graphic cards (GPUs), if available. In addition, NextGenMap handles automatically any read data independent of read length and sequencing technology.   AVAILABILITY NextGenMap source code and documentation are available at: http://cibiv.github.io/NextGenMap/.   CONTACT fritz.sedlazeck@univie.ac.at.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioinformatics;central processing unit;choose (action);documentation;genome;graphics processing unit;mapper;multi-core processor;numerous;reading (activity);source code;video card	Fritz J. Sedlazeck;Philipp Rescheneder;Arndt von Haeseler	2013	Bioinformatics	10.1093/bioinformatics/btt468	computer science;bioinformatics;theoretical computer science;world wide web	Comp.	-1.1406746340881897	-54.820229306906015	177193
08c91cfc880aac97215db5e7daa5006c4713fdfd	a novel algorithm for automatic species identification using principal component analysis	dna;genetique;frecuencia aparicion;frecuencia secuencia;count data;frequence apparition;analisis componente principal;metodo diagramatico;genetica;nucleotides;random sampling;diagrammatic method;selective sampling;sequence frequency;genetics;identificacion sistema;methode diagrammatique;frequency of occurrence;occurrence frequency;system identification;frequence sequence;species identification;principal component analysis;genome;analyse composante principale;genoma;identification systeme;genome sequence	This paper describes a novel scheme for automatic identification of a species from its genomic data. Random samples of a given length (10,000 elements) are taken from a genome sequence of a particular species. A set of 64 keywords is generated using all possible 3-tuple combinations of the 4 letters: A (for Adenine), T (for Thymine), C (for Cytosine) and G (for Guanine) representing the four types of nucleotide bases in a DNA strand. These 43= 64 keywords are searched in a sample of the genome sequence and their corresponding frequencies of occurrence are determined. Upon repeating this process for N randomly selected samples taken from the genome sequence, an N × 64 matrix of frequency count data is obtained. Then Principal Component Analysis is employed on this data to obtain a Feature Descriptor of reduced dimension (1 × 64). On determining the feature descriptors of different species and also by taking different samples from the same species, it is found that they are unique for a particular species while wide differences exist between those of different species. The variance of the descriptors for a given genome sequence being negligible, the proposed scheme finds extensive applications in automatic species identification.	algorithm;principal component analysis	Shreyas Sen;Seetharam Narasimhan;Amit Konar	2005		10.1007/11590316_96	sampling;nucleotide;whole genome sequencing;system identification;bioinformatics;automated species identification;count data;dna;genome;principal component analysis	NLP	-3.2324378852374807	-55.104088353994726	177277
b0419bbfbd026fbe9a0a2519e4d7b56a3d1e8545	protbud: a database of biological unit structures of protein families and superfamilies	protein family;pdb entry;related protein;biological unit structure;pqs bus;pdbs bus;protein data bank;asymmetric unit;position-specific iterated blast;protein quaternary server;related complex;scop designation;structural classification of proteins	ABSTRACT Motivation: Modeling,of protein interactions is often possible from known,structures of related complexes.,It is often time-consuming,to find the most,appropriate,template. Hypothesized,biological units (BUs) often differ from the asymmetric,units and it is usually preferable to model,from the BUs. Results: ProtBuD is a database,of BUs for all structures in the Protein Data Bank (PDB). We use both the PDBs BUs and,those from the Protein Quaternary Server. ProtBuD is searchable,by PDB entry, the Structural Classification of Proteins (SCOP) designation or pairs of SCOP designations. The database,provides,the asymmetric and BU contents of related proteins in the PDB as identified in SCOP and Position-Specific Iterated BLAST (PSI–BLAST). The asymmetric unit is different from PDB and/or Protein Quaternary,Server (PQS) BUs for 52% of X-ray structures, and the PDB and PQS BUs disagree on 18% of entries. Availability: The database,is provided,as a standalone,program and a web,server from http://dunbrack.fccc.edu/ProtBuD.php. Contact: Roland.Dunbrack@fccc.edu	protein family	Qifang Xu;Adrian A. Canutescu;Zoran Obradovic;Roland L. Dunbrack	2007	Bioinformatics	10.1093/bioinformatics/btm273	biology;protein data bank;bioinformatics;data mining;protein family;world wide web	Comp.	0.41231306238802806	-58.85050862497922	177448
4c78b1ad43648cff937b35964e8ce335e175a92f	loess correction for length variation in gene set-based genomic sequence analysis	dna;software;animals;transcription factors;sequence analysis dna;protein structure tertiary;drosophila melanogaster;algorithms	MOTIVATION Sequence analysis algorithms are often applied to sets of DNA, RNA or protein sequences to identify common or distinguishing features. Controlling for sequence length variation is critical to properly score sequence features and identify true biological signals rather than length-dependent artifacts.   RESULTS Several cis-regulatory module discovery algorithms exhibit a substantial dependence between DNA sequence score and sequence length. Our newly developed LOESS method is flexible in capturing diverse score-length relationships and is more effective in correcting DNA sequence scores for length-dependent artifacts, compared with four other approaches. Application of this method to genes co-expressed during Drosophila melanogaster embryonic mesoderm development or neural development scored by the Lever motif analysis algorithm resulted in successful recovery of their biologically validated cis-regulatory codes. The LOESS length-correction method is broadly applicable, and may be useful not only for more accurate inference of cis-regulatory codes, but also for detection of other types of patterns in biological sequences.   AVAILABILITY Source code and compiled code are available from http://thebrain.bwh.harvard.edu/LM_LOESS/	amino acid sequence;compiler;morphologic artifacts;motif;neurogenesis;peptide sequence;rna;score;sequence analysis;source code;algorithm;mesoderm development	Anton Aboukhalil;Martha L. Bulyk	2012	Bioinformatics	10.1093/bioinformatics/bts155	consensus sequence;biology;bioinformatics;sequence analysis;genetics;dna;alignment-free sequence analysis;transcription factor	Comp.	1.733498003670557	-55.251448963696404	177586
a85f01e56739a4094874b839285ddbf8e598482b	couplet supertree based species tree estimation		Inference of a species tree from multi-locus gene trees having topological incongruence due to incomplete lineage sorting (ILS), is currently performed by either consensus (supertree), parsimony analysis (minimizing deep coalescence), or statistical methods. However, statistical approaches involve huge computational complexity. Accuracy of approximation heuristics used in either consensus or parsimony analysis, also varies considerably. We propose COSPEDSpec, a novel two stage species tree estimation method, combining both consensus and parsimony approaches. First stage uses our earlier proposed couplet supertree technique COSPEDTree [2][3], whereas the second stage proposes a greedy heuristic to refine a non-binary (unresolved) supertree into a binary species tree. During each iteration, it reduces the number of extra lineages between the current species tree and the input gene trees, thus modeling ILS as the cause of gene tree / species tree incongruence. COSPEDSpec incurs time and space complexity lower or equal to the reference methods. For large scale datasets having hundreds of taxa and thousands of gene trees, COSPEDSpec produces species trees with lower branch dissimilarities and much less computation.	supertree	Sourya Bhattacharyya;Jayanta Mukhopadhyay	2015		10.1007/978-3-319-19048-8_5	supertree	Robotics	1.2909996614337846	-52.19238843548276	177618
a83f02c7545a0d015c77e421d0930b6ff9a7ccdf	ebardenovo: highly accurate de novo assembly of rna-seq with efficient chimera-detection	aberrant chimeric;assembly algorithm;supplementary data;rna-seq data;rna-seq technology;rna-seq assembly;aberrant chimeric amplicons;assembly experiment;short sequencing;sequencing error	MOTIVATION High-accuracy de novo assembly of the short sequencing reads from RNA-Seq technology is very challenging. We introduce a de novo assembly algorithm, EBARDenovo, which stands for Extension, Bridging And Repeat-sensing Denovo. This algorithm uses an efficient chimera-detection function to abrogate the effect of aberrant chimeric reads in RNA-Seq data.   RESULTS EBARDenovo resolves the complications of RNA-Seq assembly arising from sequencing errors, repetitive sequences and aberrant chimeric amplicons. In a series of assembly experiments, our algorithm is the most accurate among the examined programs, including de Bruijn graph assemblers, Trinity and Oases.   AVAILABILITY AND IMPLEMENTATION EBARDenovo is available at http://ebardenovo.sourceforge.net/. This software package (with patent pending) is free of charge for academic use only.   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	base sequence;bioinformatics;biopolymer sequencing;bridging (networking);chimera organism;de bruijn graph;de novo transcriptome assembly;ephrin type-b receptor 1, human;experiment;rna;reading (activity);repetitive region;sequence number;sourceforge;trinity;algorithm	Hsueh-Ting Chu;William W. L. Hsiao;Jen-Chih Chen;Tze-Jung Yeh;Mong-Hsun Tsai;Han Lin;Yen-Wenn Liu;Sheng-An Lee;Chaur-Chin Chen;Theresa Tsao;Cheng-Yan Kao	2013	Bioinformatics	10.1093/bioinformatics/btt092	computer science;bioinformatics;genetics;algorithm	Comp.	0.2908564399986663	-55.48176062094409	177726
1065a0c51028883a86ef1dc7c645c1adaa7ff0b6	reconstruction of strings past	dna;evolucion biologica;alignement sequence;evolution biologique;langage c;computerized processing;tratamiento informatico;estudio comparativo;secuencia nucleotido;alineacion secuencia;nucleotide sequence;mutacion;sequence nucleotide;algorithme;etude comparative;algorithm;c language;iteraccion;biological evolution;homology;divergent evolution;evolution divergente;comparative study;iteration;evolucion divergente;sequence alignment;homologia;traitement informatique;mutation;lenguaje c;algoritmo;homologie	A major use of string-alignment algorithms is to compare macromolecules that are thought to have evolved from a common ancestor to estimate the duration of, or the amount of mutation in, their separate evolution and to infer as much as possible about their most recent common ancestor. Minimum message length encoding, a method of inductive inference, is applied to the string-alignment problem. It leads to an alignment method that averages over all alignments in a weighted fashion. Experiments indicates that this method can recover the actual parameters of evolution with high accuracy and over a wide range of values, whereas the use of a single optimal alignment gives biased results.	algorithm;alignment;experiment;inductive reasoning;inference;minimum message length;most recent common ancestor;mutation;run-length encoding;string (computer science)	Chut N. Yee;Lloyd Allison	1993	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/9.1.1	mutation;biology;homology;divergent evolution;iteration;nucleic acid sequence;comparative research;sequence alignment;genetics;dna;algorithm	Comp.	-3.2235856376721146	-54.55056180568531	178180
9f44aa75bf8d4bd691ad30a31d4ae836774d7b3d	fselector: a ruby gem for feature selection	software;artificial intelligence;algorithms;computational biology;programming languages	SUMMARY The FSelector package contains a comprehensive list of feature selection algorithms for supporting bioinformatics and machine learning research. FSelector primarily collects and implements the filter type of feature selection techniques, which are computationally efficient for mining large datasets. In particular, FSelector allows ensemble feature selection that takes advantage of multiple feature selection algorithms to yield more robust results. FSelector also provides many useful auxiliary tools, including normalization, discretization and missing data imputation.   AVAILABILITY FSelector, written in the Ruby programming language, is free and open-source software that runs on all Ruby supporting platforms, including Windows, Linux and Mac OS X. FSelector is available from https://rubygems.org/gems/fselector and can be installed like a breeze via the command gem install fselector. The source code is available (https://github.com/need47/fselector) and is fully documented (http://rubydoc.info/gems/fselector/frames).	algorithm;algorithmic efficiency;bioinformatics;discretization;document completion status - documented;feature selection;geo-imputation;linux;machine learning;microsoft windows;missing data;open-source software;operating system;programming language;ruby;rubygems;source code;statistical imputation;system 7;gemcitabine	Tiejun Cheng;Yanli Wang;Stephen H. Bryant	2012	Bioinformatics	10.1093/bioinformatics/bts528	computer science;bioinformatics;theoretical computer science;data mining;database;statistics	Security	-2.8500721996466787	-57.11408042066842	179383
18901e2da4d325f198424939ffa6e1c6c2d02435	gold-graphical overview of linkage disequilibrium	dna;software;genetic variability;representation graphique;complex disease;analisis datos;logiciel;representacion grafica;liaison genetique;linkage analysis;genetic map;gene mapping;genetic distance;distance genetique;variabilidad genetica;data analysis;gold;distancia genetica;human genome;polymorphism;software package;linkage;logicial;analyse donnee;polymorphisme;polimorfismo;variabilite genetique;human genetics;high throughput;linkage disequilibrium;graphics;single nucleotide polymorphism;positional cloning;ligamiento genetico	SUMMARY We describe a software package that provides a graphical summary of linkage disequilibrium in human genetic data. It allows for the analysis of family data and is well suited to the analysis of dense genetic maps.   AVAILABILITY http://www.well.ox.ac.uk/asthma/GOLD CONTACT: goncalo@well.ox.ac.uk	graphical user interface;linkage (software);map;heparan sulfate proteoglycan biosynthetic process, linkage to polypeptide	Gonçalo R. Abecasis;W. O. C. Cookson	2000	Bioinformatics	10.1093/bioinformatics/16.2.182	genetic variability;gold;single-nucleotide polymorphism;tag snp;linkage disequilibrium;high-throughput screening;biology;polymorphism;human genome;gene mapping;genetic distance;genetic linkage;bioinformatics;graphics;linkage;data analysis;genetics;dna;human genetics	Comp.	-3.326884037751924	-56.26584738834053	179390
cb8089764c719ae85a158e70db683ad0e239700d	scram: a pipeline for fast index-free small rna read alignment and visualization		Summary Small RNAs play key roles in gene regulation, defense against viral pathogens and maintenance of genome stability, though many aspects of their biogenesis and function remain to be elucidated. SCRAM (Small Complementary RNA Mapper) is a novel, simple-to-use short read aligner and visualization suite that enhances exploration of small RNA datasets.   Availability and implementation The SCRAM pipeline is implemented in Go and Python, and is freely available under MIT license. Source code, multiplatform binaries and a Docker image can be accessed via https://sfletc.github.io/scram/.   Supplementary information Supplementary data are available at Bioinformatics online.		Stephen J. Fletcher;Mikael Bodén;Neena Mitter;Bernard J. Carroll	2018	Bioinformatics	10.1093/bioinformatics/bty161	data mining;operating system;visualization;scram;rna;genome;source code;computer science;python (programming language);mit license;small rna	Comp.	-2.4144717129962983	-58.40312074276799	179687
3c3191884b032af136b610ae5f67602f0e00508b	gene prediction with a hidden markov model and a new intron submodel	parameter estimation;genome sequence;hidden markov model;dna sequence;gene prediction;gene finding;gc content	MOTIVATION The problem of finding the genes in eukaryotic DNA sequences by computational methods is still not satisfactorily solved. Gene finding programs have achieved relatively high accuracy on short genomic sequences but do not perform well on longer sequences with an unknown number of genes in them. Here existing programs tend to predict many false exons.   RESULTS We have developed a new program, AUGUSTUS, for the ab initio prediction of protein coding genes in eukaryotic genomes. The program is based on a Hidden Markov Model and integrates a number of known methods and submodels. It employs a new way of modeling intron lengths. We use a new donor splice site model, a new model for a short region directly upstream of the donor splice site model that takes the reading frame into account and apply a method that allows better GC-content dependent parameter estimation. AUGUSTUS predicts on longer sequences far more human and drosophila genes accurately than the ab initio gene prediction programs we compared it with, while at the same time being more specific.   AVAILABILITY A web interface for AUGUSTUS and the executable program are located at http://augustus.gobics.de.	de novo protein structure prediction;estimation theory;executable;gene prediction;genome;hidden markov model;interface device component;interferon alfa-2b 10000000 unt injection [intron a];introns;markov chain;population parameter;reading frames (nucleotide sequence);splice (system call);user interface	Mario Stanke;Stephan Waack	2003	Bioinformatics		computational gene;rna splicing;genetics;intron;computer science;gc-content;gene prediction;bioinformatics;genome;executable;hidden markov model	Comp.	0.6927654897700459	-57.40912506950144	179705
95de263bd905c9ef0911b3f18eb3cdbaed328a70	statistical confidence measures for genome maps: application to the validation of genome assemblies	confiance;aplicacion;validacion;confidence measure;confidence;confianza;genome;validation;genoma;genome mapping;application	MOTIVATION Genome maps are imperative to address the genetic basis of the biology of an organism. While a growing number of genomes are being sequenced providing the ultimate genome maps-this being done at an even faster pace now using new generation sequencers-the process of constructing intermediate maps to build and validate a genome assembly remains an important component for producing complete genome sequences. However, current mapping approach lack statistical confidence measures necessary to identify precisely relevant inconsistencies between a genome map and an assembly.   RESULTS We propose new methods to derive statistical measures of confidence on genome maps using a comparative model for radiation hybrid data. We describe algorithms allowing to (i) sample from a distribution of maps and (ii) exploit this distribution to construct robust maps. We provide an example of application of these methods on a dog dataset that demonstrates the interest of our approach.   AVAILABILITY Methods are implemented in two freely available softwares: Carthagene (http://www.inra.fr/mia/T/CarthaGene/) and a companion software (metamap, available at: http://snp.toulouse.inra.fr/~servin/index.cgi/Metamap).	assembly language;genome assembly sequence;imperative programming;map;silo (dataset);algorithm	Bertrand Servin;Simon de Givry;Thomas Faraut	2010	Bioinformatics	10.1093/bioinformatics/btq598	biology;gene mapping;bioinformatics;data mining;confidence;genetics;statistics;genome	Comp.	1.55739948204757	-55.12920016124598	179757
a41c760ed73dc850f826c0398968171a822f29b6	andromeda: a matlab automated cdna microarray data analysis platform	gene expression profile;relational database;data mining;data format;data analysis;microarray data analysis;cdna microarray;non parametric statistics;differentially expressed gene;hierarchical cluster analysis;dna microarray	DNA microarrays constitute a relatively new biological technology which allows gene expression profiling at a global level by measuring mRNA abundance. However, the grand complexity characterizing a microarray experiment entails the development of computationally powerful tools apt for probing the biological problem studied. ANDROMEDA (Automated aND RObust Microarray Experiment Data Analysis) is a MATLAB implemented program which performs all steps of typical microarray data analysis including noise filtering processes, background correction, data normalization, statistical selection of differentially expressed genes based on parametric or non parametric statistics and hierarchical cluster analysis resulting in detailed lists of differentially expressed genes and formed clusters through a strictly defined automated workflow. Along with the completely automated procedure, ANDROMEDA offers a variety of visualization options (MA plots, boxplots, clustering images etc). Emphasis is given to the output data format which contains a substantial amount of useful information and can be easily imported in a spreadsheet supporting software or incorporated in a relational database for fiirther processing and data mining.	cluster analysis;dna microarray;data mining;gene expression profiling;hierarchical clustering;matlab;relational database;spreadsheet	Aristotelis A. Chatziioannou;Panagiotis Moulos	2007		10.1007/978-0-387-74161-1_14	nonparametric statistics;microarray analysis techniques;gene chip analysis;dna microarray;relational database;computer science;bioinformatics;data science;data mining;hierarchical clustering;data analysis;microarray databases	ML	-2.1384640000827795	-57.22722482455954	179777
2d3e469b8c694e116b62525c796d5e2db3520644	strap: editor for structural alignments of proteins	computer program;secuencia aminoacido;representation graphique;edicion;alignement sequence;proteine;sequence aminoacide;aminoacid sequence;representacion grafica;edition;publishing;bioinformatique;alineacion secuencia;proteomique;proteina;sequence alignment;proteomics;protein;multiple;programa computador;strap;graphics;programme ordinateur;bioinformatics;structure alignment	STRAP is a comfortable and extensible tool for the generation and refinement of multiple alignments of protein sequences. Various sequence ordered input file formats are supported. These are the SwissProt-,GenBank-, EMBL-, DSSP- PDB-, MSF-, and plain ASCII text format. The special feature of STRAP is the simple visualization of spatial distances C(alpha)-atoms within the alignment. Thus structural information can easily be incorporated into the sequence alignment and can guide the alignment process in cases of low sequence similarities. Further STRAP is able to manage huge alignments comprising a lot of sequences. The protein viewers and modeling programs INSIGHT, RASMOL and WEBMOL are embedded into STRAP. STRAP is written in JAVA: The well-documented source code can be adapted easily to special requirements. STRAP may become the basis for complex alignment tools in the future.	amino acid sequence;distance;document completion status - documented;embedded system;embedding;imagery;java;peptide sequence;rasmol;refinement (computing);requirement;sequence alignment;source code;whole earth 'lectronic link;format	Christoph Gille;Cornelius Frömmel	2001	Bioinformatics	10.1093/bioinformatics/17.4.377	biology;structural alignment;computer science;bioinformatics;graphics;sequence alignment;publishing;proteomics;world wide web;multiple	Comp.	-4.30086666081522	-56.971854629672094	180213
57db4bac7d880528bdf362dbb9828df8e200022c	efficient alignment of pyrosequencing reads for re-sequencing applications	software;animals;software tool;high throughput nucleotide sequencing;sequence analysis dna;computational biology bioinformatics;indexation;algorithms;humans;sequence alignment;combinatorial libraries;base sequence;dna sequence;computer appl in life sciences;local alignment;microarrays;bioinformatics	Over the past few years, new massively parallel DNA sequencing technologies have emerged. These platforms generate massive amounts of data per run, greatly reducing the cost of DNA sequencing. However, these techniques also raise important computational difficulties mostly due to the huge volume of data produced, but also because of some of their specific characteristics such as read length and sequencing errors. Among the most critical problems is that of efficiently and accurately mapping reads to a reference genome in the context of re-sequencing projects. We present an efficient method for the local alignment of pyrosequencing reads produced by the GS FLX (454) system against a reference sequence. Our approach explores the characteristics of the data in these re-sequencing applications and uses state of the art indexing techniques combined with a flexible seed-based approach, leading to a fast and accurate algorithm which needs very little user parameterization. An evaluation performed using real and simulated data shows that our proposed method outperforms a number of mainstream tools on the quantity and quality of successful alignments, as well as on the execution time. The proposed methodology was implemented in a software tool called TAPyR--Tool for the Alignment of Pyrosequencing Reads--which is publicly available from http://www.tapyr.net .	alignment;biopolymer sequencing;computation;ephrin type-b receptor 1, human;indexes;programming tool;pyrosequencing;reading (activity);roland gs;run time (program lifecycle phase);smith–waterman algorithm	Francisco Fernandes;Paulo G. S. da Fonseca;Luís M. S. Russo;Arlindo L. Oliveira;Ana T. Freitas	2010		10.1186/1471-2105-12-163	biology;dna sequencing;molecular biology;dna microarray;computer science;bioinformatics;smith–waterman algorithm;sequence alignment;genetics;hybrid genome assembly	Comp.	-1.1847072091902224	-53.92388979973261	180640
9401378b94ee8d9161e73b64093bf9c579d3a07c	possumsearch: fast and sensitive matching of position specific scoring matrices using enhanced suffix arrays	complete genome;search space;nucleotides;amino acid;protein sequence;lazy evaluation;dynamic program;suffix array;sequence analysis;sequence motif;heuristic algorithm	In biological sequence analysis, position specific scoring matrices (PSSMs) are widely used to represent sequence motifs. In this paper, we present a new nonheuristic algorithm, called ESAsearch, to efficiently find matches of such matrices in large databases. Our approach preprocesses the search space, e.g. a complete genome or a set of protein sequences, and builds an enhanced suffix array which is stored on file. The enhanced suffix array only requires 9 bytes per input symbol, and allows to search a database with a PSSM in sublinear expected time. We also address the problem of non-comparable PSSM-scores by developing a method which allows to efficiently compute a matrix similarity threshold for a PSSM, given an E-value or a p-value. Our method is based on dynamic programming. In contrast to other methods it employs lazy evaluation of the dynamic programming matrix: it only evaluates those matrix entries that are necessary to derive the sought similarity threshold. We tested algorithm ESAsearch with nucleotide PSSMs and with amino acid PSSMs. Compared to the best previous methods, ESAsearch show speedups of a factor between 4 and 50 for nucleotide PSSMs, and speedups up to a factor 1.8 for amino acid PSSMs. Comparisons with the most widely used programs even show speedups by a factor of at least 10. The lazy evaluation method is also much faster than previous methods, with speedups by a factor of at least 10.	algorithm;alphabet (formal languages);approximation;average-case complexity;byte;database;dynamic programming;heuristic (computer science);lazy evaluation;peptide sequence;position weight matrix;sequence analysis;sequence motif;suffix array;time complexity	Michael Beckstette;Dirk Strothmann;Robert Homann;Robert Giegerich;Stefan Kurtz	2004			heuristic;biology;biochemistry;nucleotide;amino acid;bioinformatics;theoretical computer science;protein sequencing;sequence analysis;lazy evaluation;mathematics;algorithm;sequence motif	ML	-1.937224120068689	-52.36838126085552	180769
5e410ad025100890560ed7623559470db36a9eba	phyutility: a phyloinformatics tool for trees, alignments and molecular data	tree;large dataset;arbol;bioinformatique;phylogenetic tree;command line interface;arbre;source code;molecular data;bioinformatica;bioinformatics	SUMMARY Phyutility provides a set of phyloinformatics tools for summarizing and manipulating phylogenetic trees, manipulating molecular data and retrieving data from NCBI. Its simple command-line interface allows for easy integration into scripted analyses, and is able to handle large datasets with an integrated database.   AVAILABILITY Phyutility, including source code, documentation, examples, and executables, is available at http://code.google.com/p/phyutility.	command-line interface;executable;interface device component;national center for biotechnology information;phylogenetic tree;phylogenetics;software documentation;source code;trees (plant)	Stephen A. Smith;Casey W. Dunn	2008	Bioinformatics	10.1093/bioinformatics/btm619	command-line interface;phylogenetic tree;computer science;bioinformatics;data mining;database;tree;source code	Comp.	-3.558303811868215	-58.42387439159564	181861
61de250d35a306862a11dfc8f85f21196e2b8f5a	probfind: a computer program for selecting oligonucleotide probes from peptide sequences	dna;computadora;computers;software;programa;investigation method;computer program;peptides;program;methode etude;computerized processing;tratamiento informatico;ordinateur;amino acid sequence;computer;peptido;traduction;primary structure;oligonucleotide probe;estructura primaria;metodo estudio;oligonucleotides;oligonucleotide;translation;peptide;oligonucleotido;programme;traduccion;base sequence;traitement informatique;microcomputers;structure primaire;programme probfind	Synthetic oligonucleotides have proven to be extremely useful probes for screening cDNA and genomic libraries. Selection of the appropriate probe can be more easily and accurately achieved with the use of the computer program PROBFIND. The user enters the amino acid sequence from a file or from the keyboard, selects the minimum length allowed for the probe and the maximum allowable degeneracy. The computer prints a list of the sequences of potential probes which meet these minimum specifications and the location of the corresponding sequence in the protein to the screen and to a file. The user may modify the specifications for length and degeneracy at any time during the output of data, which allows for rapid selection of the desired probe. The program is interactive, accepts any file format with only a single modification of the file, is written in BASIC, and requires less than 6 kbytes of memory. This makes the program easy to use and adaptable even to unsophisticated microcomputers.	accepting of extremity;amino acids;basic;computer program;dna, complementary;degeneracy (graph theory);dhrystone;genetic selection;genomic library;libraries;microcomputer;microcomputers;oligonucleotide probes;oligonucleotides;peptide sequence;specification;keyboard	R. M. Lewis	1986	Nucleic acids research	10.1093/nar/14.1.567	biology;bioinformatics;genetics;oligonucleotide	Comp.	-3.99039626447468	-56.288628417123874	182058
5a0d55af0a69050d90efd841b88c01a66984804b	sparseassembler: de novo assembly with the sparse de bruijn graph	genome analysis;large scale;polymorphism;error rate;de bruijn graph;breadth first search;data structure;base pair	de Bruijn graph-based algorithms are one of the two most widely used approaches for de novo genome assembly. A major limitation of this approach is the large computational memory space requirement to construct the de Bruijn graph, which scales with k-mer length and total diversity (N) of unique k-mers in the genome expressed in base pairs or roughly (2k+8)N bits. This limitation is particularly important with large-scale genome analysis and for sequencing centers that simultaneously process multiple genomes. We present a sparse de Bruijn graph structure, based on which we developed SparseAssembler that greatly reduces memory space requirements. The structure also allows us to introduce a novel method for the removal of substitution errors introduced during sequencing. The sparse de Bruijn graph structure skips g intermediate k-mers, therefore reducing the theoretical memory space requirement to ~(2k/g+8)N. We have found that a practical value of g=16 consumes approximately 10% of the memory required by standard de Bruijn graph-based algorithms but yields comparable results. A high error rate could potentially derail the SparseAssembler. Therefore, we developed a sparse de Bruijn graph-based denoising algorithm that can remove more than 99% of substitution errors from datasets with a ≤ 2% error rate. Given that substitution error rates for the current generation of sequencers is lower than 1%, our denoising procedure is sufficiently effective to safeguard the performance of our algorithm. Finally, we also introduce a novel Dijkstra-like breadth-first search algorithm for the sparse de Bruijn graph structure to circumvent residual errors and resolve polymorphisms.	breadth-first search;dspace;de bruijn graph;de novo transcriptome assembly;k-mer;mer;noise reduction;requirement;search algorithm;sparse matrix	Chengxi Ye;Zhanshan Sam Ma;Charles H. Cannon;Mihai Pop;Douglas W. Yu	2011	CoRR		polymorphism;de bruijn graph;combinatorics;base pair;graph bandwidth;data structure;breadth-first search;word error rate;computer science;bioinformatics;mathematics;programming language;algorithm	Comp.	-1.317479667548929	-52.361467357974426	182137
4dcc497cbfd49eae363f3222ac6e3a88a897fbeb	robust inference of positive selection from recombining coding sequences	positive darwinian selection;positive selection;maximum likelihood;synonymous substitution rate;false positive rate;robust inference;molecular evolution;virus diseases pathogenesis;uct;high power;genetic recombination;genetic recombination research;peer reviewed journal article	MOTIVATION Accurate detection of positive Darwinian selection can provide important insights to researchers investigating the evolution of pathogens. However, many pathogens (particularly viruses) undergo frequent recombination and the phylogenetic methods commonly applied to detect positive selection have been shown to give misleading results when applied to recombining sequences. We propose a method that makes maximum likelihood inference of positive selection robust to the presence of recombination. This is achieved by allowing tree topologies and branch lengths to change across detected recombination breakpoints. Further improvements are obtained by allowing synonymous substitution rates to vary across sites.   RESULTS Using simulation we show that, even for extreme cases where recombination causes standard methods to reach false positive rates >90%, the proposed method decreases the false positive rate to acceptable levels while retaining high power. We applied the method to two HIV-1 datasets for which we have previously found that inference of positive selection is invalid owing to high rates of recombination. In one of these (env gene) we still detected positive selection using the proposed method, while in the other (gag gene) we found no significant evidence of positive selection.   AVAILABILITY A HyPhy batch language implementation of the proposed methods and the HIV-1 datasets analysed are available at http://www.cbio.uct.ac.za/pub_support/bioinf06. The HyPhy package is available at http://www.hyphy.org, and it is planned that the proposed methods will be included in the next distribution. RDP2 is available at http://darwin.uvigo.es/rdp/rdp.html	batch file;breakpoint;crossover (genetic algorithm);env;hyphy;inference;phylogenetics;positive selection;robustness (computer science);simulation	Konrad Scheffler;Darren P. Martin;Cathal Seoighe	2006	Bioinformatics	10.1093/bioinformatics/btl427	biology;molecular evolution;false positive rate;bioinformatics;maximum likelihood;genetic recombination;genetics;directional selection	Comp.	2.134604247055571	-53.883713430956796	182293
08043cc2baf23ba2c0e2f61842b3fb3d739b0444	iterative sequential monte carlo algorithm for motif discovery	ismc algorithm;nucleotide level performance coefficient;gene regulatory relationship;transcription factor binding sites;nucleotide sequence;site level positive prediction value;position weight matrix;nucleotide level correlation coefficient;computational complexity;iterative sequential monte carlo algorithm;motif discovery algorithms;evolution history;gibbs sampling model	The discovery of motifs in transcription factor binding sites is important in the transcription process, and is crucial for understanding the gene regulatory relationship and evolution history. Identifying weak motifs and reducing the effect of local optima, error propagation and computational complexity are still important, but challenging tasks for motif discovery. This study proposes an iterative sequential Monte Carlo (ISMC) motif discovery algorithm based on the position weight matrix and the Gibbs sampling model to locate conserved motifs in a given set of nucleotide sequences. Three sub-algorithms have been proposed. Algorithm 1 (see Fig. 1) deals with the case of one motif instance of fixed length in each nucleotide sequence. Furthermore, the proposed ISMC algorithm is extended to deal with more complex situations including unique motif of unknown length in Algorithm 2, unique motif with unknown abundance in Algorithm 3 (see Fig. 2) and multiple motifs. Experimental results over both synthetic and real datasets show that the proposed ISMC algorithm outperforms five other widely used motif discovery algorithms in terms of nucleotide and site-level sensitivity, nucleotide and site-level positive prediction value, nucleotide-level performance coefficient, nucleotide-level correlation coefficient and site-level average site performance.	iterative method;monte carlo algorithm;monte carlo method;particle filter;sequence motif	Mohammad Al Bataineh;Zouhair Al-qudah;Awad Al-Zaben	2016	IET Signal Processing	10.1049/iet-spr.2014.0356	nucleic acid sequence;computer science;bioinformatics;machine learning;position weight matrix;data mining;computational complexity theory;dna binding site	ML	1.2047588827810007	-53.672306355603844	182525
e114c26857a816becbcd8765ca0bd58f0d5e0636	functional performance of acgh design for clinical cytogenetics	functional performance;acgh;design optimization;custom array design	Array-comparative genomic hybridization (aCGH) technology enables rapid, high-resolution analysis of genomic rearrangements. With the use of it, genome copy number changes and rearrangement breakpoints can be detected and analyzed at resolutions down to a few kilobases. An exon array CGH approach proposed recently accurately measures copy-number changes of individual exons in the human genome. The crucial and highly non-trivial starting task is the design of an array, i.e. the choice of appropriate (multi)set of oligos. The success of the whole high-level analysis depends on the quality of the design. Also, the comparison of several alternative designs of array CGH constitutes an important step in development of new diagnostic chip. In this paper, we deal with these two often neglected issues. We propose a new approach to measure the quality of array CGH designs. Our measures reflect the robustness of rearrangements detection to the noise (mostly experimental measurement error). The method is parametrized by the segmentation algorithm used to identify aberrations. We implemented the efficient Monte Carlo method for testing noise robustness within DNAcopy procedure. Developed framework has been applied to evaluation of functional quality of several optimized array designs.		Tomasz Gambin;Paweł Stankiewicz;Maciej Sykulski;Anna Gambin	2013	Computers in biology and medicine	10.1016/j.compbiomed.2013.02.008	multidisciplinary design optimization;bioinformatics;mathematics;genetics	EDA	2.057342815265254	-55.095745496335354	182576
c40e6323ccfd48f854470d1367bbe203ab836fdc	evaluation of combined genome assemblies: a case study with fungal genomes		The rapid advances in genome sequencing leads to the generation of huge amount of data in a single sequencing experiment. Several genome assem- blers with different objectives were developed to process these genomic data. Obviously, the output assemblies produced by these assemblers have different qualities due to their diverse nature. Recent research efforts concluded that com- bining the assemblies from different assemblers would enhance the quality of the output assembly. Based on this, our study combines the five best assemblies of three fungal genomes and evaluates the quality of the output assembly as com- pared to that produced by individual assemblers. The results conclude that the output assembly quality is influenced by the increase of the number of gaps in the input assemblies more than the increase in N50 size. Based on this conclu- sion, we propose a set of guidelines to get better output assemblies.		Mostafa M. Abbas;Ponnuraman Balakrishnan;Qutaibah M. Malluhi	2015		10.1007/978-3-319-16480-9_33	biology;biotechnology;bioinformatics;nanotechnology	NLP	2.07321514877529	-58.40896241203126	183002
b7c5fdffec7aaee8cde031e0942810a005da4a71	graph2tab, a library to convert experimental workflow graphs into tabular formats	computer graphics;workflow;databases factual;computational biology;oligonucleotide array sequence analysis;programming languages	MOTIVATIONS Spreadsheet-like tabular formats are ever more popular in the biomedical field as a mean for experimental reporting. The problem of converting the graph of an experimental workflow into a table-based representation occurs in many such formats and is not easy to solve.   RESULTS We describe graph2tab, a library that implements methods to realise such a conversion in a size-optimised way. Our solution is generic and can be adapted to specific cases of data exporters or data converters that need to be implemented.   AVAILABILITY AND IMPLEMENTATION The library source code and documentation are available at http://github.com/ISA-tools/graph2tab.	documentation;exporter (computing);generic drugs;graph - visual representation;lambda calculus;source code;spreadsheet;table (information);format	Marco Brandizi;Natalja Kurbatova;Ugis Sarkans;Philippe Rocca-Serra	2012		10.1093/bioinformatics/bts258	workflow;computer science;bioinformatics;theoretical computer science;database;computer graphics;workflow technology	Comp.	-4.025894075783105	-58.69046141602909	183068
a1658aba0366f43d804ddf94e6abd0f4d1fcc8df	visualizing the repeat structure of genomic sequences		Repeats are a common feature of genomic sequences and much remains to be understood of their origin and structure. The identification of repeated strings in genomic sequences is therefore of importance for a variety of applications in biology. In this paper a new method for finding all repeats and visualising them in a two dimensional plot is presented. The method is first applied to a set of constructed sequences in order to develop a comparative framework. Several complete genomes are then analysed, including the whole human genome. The technique reveals the complex repeat structure of genomic sequences. In particular, interesting differences in the repeat character of the coding and non-coding regions of bacterial genomes are noted. The method allows fast identification of all repeats and easy intergenome comparison. In doing this the plot effectively creates a signature of a sequence which allows some classes of repeat present in a sequence to be identified by simple visual inspection. To our knowledge this is the first time all exact repeats have been visualised in a single plot that highlights the degree to which repeats occur within a genomic sequence, giving an indication of the important	visual inspection	Nava Whiteford;Niall J. Haslam;Gerald Weber;Adam Prügel-Bennett;Jonathan W. Essex;Cameron Neylon	2008	Complex Systems		bioinformatics	Comp.	0.8693114863563769	-57.465910367379266	183084
d72d32ee6dd2cb56fb5c5989c68ad5f65843b4e7	genetic mappings in artificial genomes	genotype;simulation;genetic map;genetic mapping;simulated evolution;genetics;visualization;translation;phenotype;computer simulation;artificial life	This paper concerns processing of genomes of artificial (computer-simulated) organisms. Of special interest is the process of translation of genotypes into phenotypes, and utilizing the mapping information obtained during such translation. If there exists more than one genetic encoding in a single artificial life model, then the translation may also occur between different encodings. The obtained mapping information allows to present genes-phenes relationships visually and interactively to a person, in order to increase understanding of the genotype-tophenotype translation process and genetic encoding properties. As the mapping associates parts of the source sequence with the translated destination, it may be also used to trace genes, phenes, and their relationships during simulated evolution. A mappings composition procedure is formally described, and a simple method of visual mapping presentation is established. Finally, advanced visualizations of gene-phene relationships are demonstrated as practical examples of introduced techniques. These visualizations concern genotypes expressed in various encodings, including an encoding which exhibits polygenic and pleiotropic properties.	artificial life;computer simulation;exhibits as topic;gene ontology;genetic translation process;genome;genotype;greater than;interactivity;phenes;phenotype	Maciej Komosinski;Szymon Ulatowski	2004	Theory in Biosciences	10.1016/j.thbio.2004.04.002	computer simulation;translation;biology;gene mapping;visualization;bioinformatics;phenotype;genotype;genetics;artificial life	Comp.	-1.626446301395222	-55.824341605194235	183656
5f39cd6d9944325a298e64a5603201d7e69a4186	dna sequences compression algorithms based on the two bits codation method	horizontal compression;extended ascii code;hexadecimal compression;molecular biophysics biological techniques dna;dna sequences;2 bits compression method;dna databases chlorine;vertical compression dna sequences extended ascii code hexadecimal compression 2 bits compression method horizontal compression;vertical compression;dna sequences compression algorithms genome compression rle technique hexadecimal representation extended ascii representation lossless algorithms online databases two bits codation method	Nowadays a large number of DNA sequences is being stored on online databases. To reduce this quantity of information, researchers have been trying to implement new DNA sequences compression techniques based on the LOSSLESS algorithms. In this article we will compare two algorithms using the binary representation of DNA sequences. Those two algorithms are characterized by their ease of implementation. The first one transforms the DNA sequence into extended-ASCII representation while the second algorithm transforms it into a hexadecimal representation. Thereafter, we will apply the RLE technique to further enhance the compression of entire genomes.	algorithm;binary number;data compression;database;hexadecimal;run-length encoding	Bacem Saada;Jing Zhang	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359928	data compression;biology;dna sequencing;computer science;bioinformatics;theoretical computer science;lossless compression;genetics;algorithm	Visualization	-3.05635867452951	-52.532177057187326	183732
84da59a555185d130f8864848909518d44e523d9	short read dna fragment anchoring algorithm	dna;databases genetic;sequence analysis dna;computational biology bioinformatics;dna fragmentation;genome human;algorithms;humans;sequence alignment;combinatorial libraries;next generation sequencing;base sequence;computational biology;computer appl in life sciences;genome sequence;microarrays;bioinformatics	The emerging next-generation sequencing method based on PCR technology boosts genome sequencing speed considerably, the expense is also get decreased. It has been utilized to address a broad range of bioinformatics problems. Limited by reliable output sequence length of next-generation sequencing technologies, we are confined to study gene fragments with 30~50 bps in general and it is relatively shorter than traditional gene fragment length. Anchoring gene fragments in long reference sequence is an essential and prerequisite step for further assembly and analysis works. Due to the sheer number of fragments produced by next-generation sequencing technologies and the huge size of reference sequences, anchoring would rapidly becoming a computational bottleneck. We compared algorithm efficiency on BLAT, SOAP and EMBF. The efficiency is defined as the count of total output results divided by time consumed to retrieve them. The data show that our algorithm EMBF have 3~4 times efficiency advantage over SOAP, and at least 150 times over BLAT. Moreover, when the reference sequence size is increased, the efficiency of SOAP will get degraded as far as 30%, while EMBF have preferable increasing tendency. In conclusion, we deem that EMBF is more suitable for short fragment anchoring problem where result completeness and accuracy is predominant and the reference sequences are relatively large.	algorithm;algorithmic efficiency;blat;bmc bioinformatics;bioinformatics;biopolymer sequencing;blocking (computing);centralized computing;compression;computation;enlargement procedure;euler;experiment;fastest;hl7publishingsubsection <query>;hash table;inverted index;manuscripts;marijuana abuse;massively-parallel sequencing;popliteal pterygium syndrome, lethal type;pacific asia conference on information systems;parallel computing;question (inquiry);revision procedure;soap;sampling (signal processing);sampling - surgical action;scalability;scientific publication;sea nettle, west coast;shattered world;small;speedup;triclosan 3 mg/ml medicated liquid soap;whole genome sequencing;window function;algorithm;citation;contents - htmllinktype;executing - querystatuscode;interest;juzentaihoto;mecarzole	Wendi Wang;Peiheng Zhang;Xinchun Liu	2009	BMC Bioinformatics	10.1186/1471-2105-10-S1-S17	biology;dna fragmentation;paired-end tag;dna sequencing;molecular biology;whole genome sequencing;dna microarray;bioinformatics;sequence alignment;sequence assembly;genetics;dna;hybrid genome assembly	Comp.	-1.3458176032061744	-53.723446076926784	183884
1d8f3781b62878b6997c41abe362a1cf97ca0b0e	chromhome: a rich internet application for accessing comparative chromosome homology maps	software;rich internet application;comparative genomics;chromosome mapping;sequence analysis dna;computational biology bioinformatics;internet;comparative mapping;sequence homology nucleic acid;user experience;algorithms;chromosome painting;molecular sequence data;user computer interface;combinatorial libraries;phylogenetic relationship;base sequence;computer appl in life sciences;article;open source;microarrays;bioinformatics	Comparative genomics has become a significant research area in recent years, following the availability of a number of sequenced genomes. The comparison of genomes is of great importance in the analysis of functionally important genome regions. It can also be used to understand the phylogenetic relationships of species and the mechanisms leading to rearrangement of karyotypes during evolution. Many species have been studied at the cytogenetic level by cross species chromosome painting. With the large amount of such information, it has become vital to computerize the data and make them accessible worldwide. Chromhome http://www.chromhome.org is a comprehensive web application that is designed to provide cytogenetic comparisons among species and to fulfil this need. The Chromhome application architecture is multi-tiered with an interactive client layer, business logic and database layers. Enterprise java platform with open source framework OpenLaszlo is used to implement the Rich Internet Chromhome Application. Cross species comparative mapping raw data are collected and the processed information is stored into MySQL Chromhome database. Chromhome Release 1.0 contains 109 homology maps from 51 species. The data cover species from 14 orders and 30 families. The homology map displays all the chromosomes of the compared species as one image, making comparisons among species easier. Inferred data also provides maps of homologous regions that could serve as a guideline for researchers involved in phylogenetic or evolution based studies. Chromhome provides a useful resource for comparative genomics, holding graphical homology maps of a wide range of species. It brings together cytogenetic data of many genomes under one roof. Inferred painting can often determine the chromosomal homologous regions between two species, if each has been compared with a common third species. Inferred painting greatly reduces the need to map entire genomes and helps focus only on relevant regions of the chromosomes of the species under study. Future releases of Chromhome will accommodate more species and their respective gene and BAC maps, in addition to chromosome painting data. Chromhome application provides a single-page interface (SPI) with desktop style layout, delivering a better and richer user experience.	applications architecture;bacterial artificial chromosomes;batman: arkham city;business logic;chromosome painting;dna sequence rearrangement;desktop computer;genome;graphical user interface;homologous gene;homology (biology);inference;interface device component;java platform, enterprise edition;java programming language;karyotype;map;mysql;numerous;open-source software;openlaszlo;phylogenetics;rich internet application;tomography, emission-computed, single-photon;user experience;web application;anatomical layer	Sridevi Nagarajan;Willem Rens;James Stalker;Antony Cox;Malcolm A. Ferguson-Smith	2007	BMC Bioinformatics	10.1186/1471-2105-9-168	biology;user experience design;the internet;rich internet application;dna microarray;computer science;bioinformatics;comparative genomics;genetics	Comp.	-2.6134019056819073	-58.75333038578319	184742
75a25d23e98aad7bf7ab241204b92b14a3e531e4	improving the consistency of domain annotation within the conserved domain database	molecular sequence annotation;protein structure tertiary;algorithms;sequence analysis protein;databases protein	When annotating protein sequences with the footprints of evolutionarily conserved domains, conservative score or E-value thresholds need to be applied for RPS-BLAST hits, to avoid many false positives. We notice that manual inspection and classification of hits gathered at a higher threshold can add a significant amount of valuable domain annotation. We report an automated algorithm that 'rescues' valuable borderline-scoring domain hits that are well-supported by domain architecture (DA, the sequential order of conserved domains in a protein query), including tandem repeats of domain hits reported at a more conservative threshold. This algorithm is now available as a selectable option on the public conserved domain search (CD-Search) pages. We also report on the possibility to 'suppress' domain hits close to the threshold based on a lack of well-supported DA and to implement this conservatively as an option in live conserved domain searches and for pre-computed results. Improving domain annotation consistency will in turn reduce the fraction of NR sequences with incomplete DAs.	algorithm;amino acid sequence;annotation;blast;cdd;conserved sequence;noise reduction;peptide sequence;precomputation;question (inquiry);score;staphylococcal protein a;tandem repeat sequences;web server	Myra K. Derbyshire;Noreen R. Gonzales;Shennan Lu;Jane He;Gabriele H. Marchler;Zhouxi Wang;Aron Marchler-Bauer	2015		10.1093/database/bav012	biology;computer science;bioinformatics;data mining;conserved domain database;world wide web	Comp.	-0.3066282039796196	-56.446808055393724	184942
7ebacfc6c8ee7dcce0e5a21150c3ee8ddf8f607d	the negatome database: a reference set of non-interacting protein pairs	software;animals;algoritmos;mapeamento de interacao de proteinas;protein complex;saccharomyces cerevisiae;genome fungal;databases nucleic acid;protein domains;databases genetic;genoma fungico;bases de dados geneticas;humanos;bases de dados de acidos nucleicos;animais;biologia computacional;internet;proteins;protein structure tertiary;proteinas;bases de dados de proteinas;algorithms;humans;protein interaction mapping;high throughput;computational biology;3d structure;information storage and retrieval;estrutura terciaria de proteina;physical interaction;structured data;databases protein;armazenamento e recuperacao da informacao	The Negatome is a collection of protein and domain pairs that are unlikely to be engaged in direct physical interactions. The database currently contains experimentally supported non-interacting protein pairs derived from two distinct sources: by manual curation of literature and by analyzing protein complexes with known 3D structure. More stringent lists of non-interacting pairs were derived from these two datasets by excluding interactions detected by high-throughput approaches. Additionally, non-interacting protein domains have been derived from the stringent manual and structural data, respectively. The Negatome is much less biased toward functionally dissimilar proteins than the negative data derived by randomly selecting proteins from different cellular locations. It can be used to evaluate protein and domain interactions from new experiments and improve the training of interaction prediction algorithms. The Negatome database is available at http://mips.helmholtz-muenchen.de/proj/ppi/negatome.	algorithm;digital curation;experiment;fundamental interaction;high-throughput computing;protein domain;randomness;throughput	Pawel Smialowski;Philipp Pagel;Philip Wong;Barbara Brauner;Irmtraud Dunger;Gisela Fobo;Goar Frishman;Corinna Montrone;Thomas Rattei;Dmitrij Frishman;Andreas Ruepp	2010		10.1093/nar/gkp1026	high-throughput screening;biology;the internet;data model;bioinformatics;multiprotein complex;protein domain	Comp.	1.1930386353559543	-56.89212438372903	185223
5ea0dacbcf544531e99c16dc8f6b89f1c33d21bc	rsw-seq: algorithm for detection of copy number alterations in deep sequencing data	tumor cells cultured;performance test;copy number alterations;sequence analysis dna;journal article;computational biology bioinformatics;copy number;dna copy number variations;large scale;genome human;human genome;dna neoplasm;algorithms;humans;combinatorial libraries;base sequence;computer appl in life sciences;article;dna copy number;computer simulation;oligonucleotide array sequence analysis;genome sequence;microarrays;bioinformatics	Recent advances in sequencing technologies have enabled generation of large-scale genome sequencing data. These data can be used to characterize a variety of genomic features, including the DNA copy number profile of a cancer genome. A robust and reliable method for screening chromosomal alterations would allow a detailed characterization of the cancer genome with unprecedented accuracy. We develop a method for identification of copy number alterations in a tumor genome compared to its matched control, based on application of Smith-Waterman algorithm to single-end sequencing data. In a performance test with simulated data, our algorithm shows >90% sensitivity and >90% precision in detecting a single copy number change that contains approximately 500 reads for the normal sample. With 100-bp reads, this corresponds to a ~50 kb region for 1X genome coverage of the human genome. We further refine the algorithm to develop rSW-seq, (recursive Smith-Waterman-seq) to identify alterations in a complex configuration, which are commonly observed in the human cancer genome. To validate our approach, we compare our algorithm with an existing algorithm using simulated and publicly available datasets. We also compare the sequencing-based profiles to microarray-based results. We propose rSW-seq as an efficient method for detecting copy number changes in the tumor genome.	biopolymer sequencing;chromosome aberrations;copy number;deep sequencing;microarray;neoplasms;reading (activity);recursion;sensor;sequence number;smith–waterman algorithm;virtual screening;whole genome sequencing	Tae-Min Kim;Lovelace J. Luquette;Ruibin Xi;Peter J. Park	2009		10.1186/1471-2105-11-432	computer simulation;biology;human genome;molecular biology;whole genome sequencing;dna microarray;copy number analysis;bioinformatics;copy-number variation;genetics;hybrid genome assembly	Comp.	1.2417742470639737	-54.967445981588085	185303
7de80045adbc124e8396cc24ef55cd03f7f02a21	identifying regulatory signals in dna-sequences with a non-statistical approximation approach	dna;biology computing;approximate algorithm;genome sequences identifying regulatory signals dna sequences nonstatistical approximation bioinformatics gene profiling technology gene expression transcription control dna binding motifs complex eukaryotic organism mathematical model;dna binding;genetics;gene expression;approximation theory;transcription factor;signal processing bioinformatics genomics gene expression organisms biology computing mathematical model mathematics computer science approximation algorithms;mathematical model;cellular biophysics genetics biology computing dna approximation theory microorganisms;dna sequence;microorganisms;cellular biophysics;genome sequence	The identi ̄cation of regulatory signals is one of the most di±cult and challenging tasks in bioinformatics. With the development of microarray technology, biologists can now reveal gigantic gene sequences, which contain parts of the genome believed to be responsible for most transcription ¤	approximation;bioinformatics;microarray;transcription (software)	Cun-Quan Zhang;Yunkai Liu;Elaine M. Eschen;Konstantinos E Vlachonasios	2003		10.1109/CSB.2003.1227417	biology;dna sequencing;whole genome sequencing;gene expression;bioinformatics;mathematical model;microorganism;genetics;dna;transcription factor;approximation theory	Comp.	0.04733909170919133	-52.24472490005515	185475
7de44cb71daa7583fd681b9ceb6c1f2659f4f9cd	sammate: a gui tool for processing short read alignments in sam/bam format	health research;uk clinical guidelines;biological patents;software tool;europe pubmed central;citation search;computational biology bioinformatics;gene expression;data analysis;uk phd theses thesis;life sciences;general public license;graphic user interface;source code;sequence alignment;next generation sequencing;uk research reports;ucsc;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	Next Generation Sequencing (NGS) technology generates tens of millions of short reads for each DNA/RNA sample. A key step in NGS data analysis is the short read alignment of the generated sequences to a reference genome. Although storing alignment information in the Sequence Alignment/Map (SAM) or Binary SAM (BAM) format is now standard, biomedical researchers still have difficulty accessing this information. We have developed a Graphical User Interface (GUI) software tool named SAMMate. SAMMate allows biomedical researchers to quickly process SAM/BAM files and is compatible with both single-end and paired-end sequencing technologies. SAMMate also automates some standard procedures in DNA-seq and RNA-seq data analysis. Using either standard or customized annotation files, SAMMate allows users to accurately calculate the short read coverage of genomic intervals. In particular, for RNA-seq data SAMMate can accurately calculate the gene expression abundance scores for customized genomic intervals using short reads originating from both exons and exon-exon junctions. Furthermore, SAMMate can quickly calculate a whole-genome signal map at base-wise resolution allowing researchers to solve an array of bioinformatics problems. Finally, SAMMate can export both a wiggle file for alignment visualization in the UCSC genome browser and an alignment statistics report. The biological impact of these features is demonstrated via several case studies that predict miRNA targets using short read alignment information files. With just a few mouse clicks, SAMMate will provide biomedical researchers easy access to important alignment information stored in SAM/BAM files. Our software is constantly updated and will greatly facilitate the downstream analysis of NGS data. Both the source code and the GUI executable are freely available under the GNU General Public License at http://sammate.sourceforge.net .	accessibility;annotation;base sequence;bioinformatics;communications satellite;customize;downstream (software development);ephrin type-b receptor 1, human;executable;exons;gnu;gene expression;graphical user interface;massively-parallel sequencing;name;portable software apps;programming tool;rna;reading (activity);samsn1 gene;samtools;sequence alignment;source code;sourceforge;standard operating procedure;the ucsc genome browser;wiggle stereoscopy;doxorubicin/semustine/streptozocin protocol;negative regulation of reactive oxygen species biosynthetic process	Guorong Xu;Nan Deng;Zhiyu Zhao;Thair Judeh;Erik K. Flemington;Dongxiao Zhu	2010		10.1186/1751-0473-6-2	biology;dna sequencing;gene expression;computer science;bioinformatics;data science;sequence alignment;graphical user interface;data analysis;world wide web;source code	Comp.	-2.331633079265886	-57.609836546162086	186341
f30d145c268d980593c8f08a073bd3c52d8b1e87	discovery and assembly of repeat family pseudomolecules from sparse genomic sequence data using the assisted automated assembler of repeat families (aaarf) algorithm	software;long terminal repeat;maize;raw materials;databases genetic;sequence analysis dna;repetitive sequences nucleic acid;journal article;repetitive dna;computational biology bioinformatics;zea mays;gene dosage;genome;dna plant;algorithms;sequence alignment;genome evolution;combinatorial libraries;transposable element;computer appl in life sciences;multigene family;genome sequence;microarrays;bioinformatics	Higher eukaryotic genomes are typically large, complex and filled with both genes and multiple classes of repetitive DNA. The repetitive DNAs, primarily transposable elements, are a rapidly evolving genome component that can provide the raw material for novel selected functions and also indicate the mechanisms and history of genome evolution in any ancestral lineage. Despite their abundance, universality and significance, studies of genomic repeat content have been largely limited to analyses of the repeats in fully sequenced genomes. In order to facilitate a broader range of repeat analyses, the Assisted Automated Assembler of Repeat Families algorithm has been developed. This program, written in PERL and with numerous adjustable parameters, identifies sequence overlaps in small shotgun sequence datasets and walks them out to create long pseudomolecules representing the most abundant repeats in any genome. Testing of this program in maize indicated that it found and assembled all of the major repeats in one or more pseudomolecules, including coverage of the major Long Terminal Repeat retrotransposon families. Both Sanger sequence and 454 datasets were appropriate. These results now indicate that hundreds of higher eukaryotic genomes can be efficiently characterized for the nature, abundance and evolution of their major repetitive DNA components.	algorithm;assembly language;blast;bioperl;cns disorder;class;clustalw/clustalx;dna transposable elements;ephrin type-b receptor 1, human;fill;gnu;home page;lineage (evolution);manuscripts;mike lesser;ncbi taxonomy;name;operating system;perl;programming languages;programming language;requirement;sourceforge;sparse matrix;system 7;test engineer;trial elements domain;universality probability;whole genome sequencing	Jeremy D. DeBarry;Renyi Liu;Jeffrey Bennetzen	2007	BMC Bioinformatics	10.1186/1471-2105-9-235	biology;gene dosage;cot analysis;molecular biology;whole genome sequencing;repeated sequence;transposable element;dna microarray;bioinformatics;genome evolution;raw material;sequence alignment;long terminal repeat;genetics;genome	Comp.	0.6794708581329096	-56.06797224593486	186467
badff70fe282b493868ac63414d51005dff96aa4	a quantitative determination of multi-protein interactions by the analysis of confocal images using a pixel-by-pixel assessment algorithm	analisis imagen;tissu;software;animals;mice;microscopy confocal;microscopia confocal;interaction moleculaire;molecular interaction;morbillivirus;analisis cuantitativo;hippocampus;paramyxovirinae;virus;mononegavirales;algorithme;tejido;algorithm;interaccion molecular;image interpretation computer assisted;analyse quantitative;virus rougeole;tissue;measles;quantitative analysis;determinacion;algorithms;image analysis;chemokine ccl5;measles virus;determination;microscopie confocale;paramyxoviridae;neurons;protein interaction;protein interaction mapping;confocal microscopy;multiple;analyse image;algoritmo	MOTIVATION Recent advances in confocal microscopy have allowed scientists to assess the expression, and to some extent, the interaction/colocalization of multiple molecules within cells and tissues. In some instances, accurately quantifying the colocalization of two or more proteins may be critical. This can require the acquisition of multiple Z plane images (Z stacks) throughout a specimen and, as such, we report here the successful development of a freeware, open-source image analysis tool, IMAJIN_COLOC, developed in PERL (v. 5.8, build 806), using the PERLMagick libraries (ImageMagick). Using a pixel-by-pixel analysis algorithm, IMAJIN_COLOC can analyze images for antigen expression (any number of colors) and can measure all possible combinations of colocalization for up to three colors by analyzing a Z stack gallery acquired for each sample. The simultaneous (i.e. in a single pass) analysis of three-color colocalization, and batch analysis capabilities are distinctive features of this program.   RESULTS A control image, containing known individual and colocalized pixel counts, was used to validate the accuracy of IMAJIN_COLOC. As further validation, pixel counts and colocalization values from the control image were compared to those obtained with the software packaged with the Zeiss laser-scanning microscope (LSM AIM, version 3.2). The values from both programs were found to be identical. To demonstrate the applicability of this program in addressing novel biological questions, we examined the role of neurons in eliciting an immune reaction in response to viral infection. Specifically, we successfully examined expression of the chemokine RANTES in measles virus (MV) infected hippocampal neurons and quantified changes in RANTES production throughout the disease period. The resultant quantitative data were also evaluated visually, using a gif image created during the analysis.   AVAILABILITY PERL (ActivePerl, version 5.8) is available at activestate.com; the PERLMagick libraries are available at imagemagick.org, and IMAJIN_COLOC, the source code and user documentation can be downloaded from http://www.fda.gov/cber/research/imaging/imageanalysis.htm.	activeperl;algorithm;biological specimen;body tissue;cdisc send biospecimens terminology;color;gif;image analysis;imagemagick;interaction;libraries;microscope device component;microscopy, confocal;murine sarcoma viruses;numerous;open-source software;perl;pixel;resultant;software documentation;source code;virus diseases	D. R. Goucher;S. M. Wincovitch;S. H. Garfield;K. M. Carbone;T. H. Malik	2005	Bioinformatics	10.1093/bioinformatics/bti531	biology;image analysis;virus;computer science;bioinformatics;quantitative analysis;confocal laser scanning microscopy;hippocampus;multiple	Comp.	-4.140554994293881	-56.3614297707657	187259
8a6982adb10b00bbbbd6d59f3623d12b74af5bdc	vindel: a simple pipeline for checking indel redundancy	software;high throughput nucleotide sequencing;sequence analysis dna;indel mutation;journal article;computational biology bioinformatics;genome;algorithms;humans;sequence alignment;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	With the advance of next generation sequencing (NGS) technologies, a large number of insertion and deletion (indel) variants have been identified in human populations. Despite much research into variant calling, it has been found that a non-negligible proportion of the identified indel variants might be false positives due to sequencing errors, artifacts caused by ambiguous alignments, and annotation errors. In this paper, we examine indel redundancy in dbSNP, one of the central databases for indel variants, and develop a standalone computational pipeline, dubbed Vindel, to detect redundant indels. The pipeline first applies indel position information to form candidate redundant groups, then performs indel mutations to the reference genome to generate corresponding indel variant substrings. Finally the indel variant substrings in the same candidate redundant groups are compared in a pairwise fashion to identify redundant indels. We applied our pipeline to check for redundancy in the human indels in dbSNP. Our pipeline identified approximately 8% redundancy in insertion type indels, 12% in deletion type indels, and overall 10% for insertions and deletions combined. These numbers are largely consistent across all human autosomes. We also investigated indel size distribution and adjacent indel distance distribution for a better understanding of the mechanisms generating indel variants. Vindel, a simple yet effective computational pipeline, can be used to check whether a set of indels are redundant with respect to those already in the database of interest such as NCBI’s dbSNP. Of the approximately 5.9 million indels we examined, nearly 0.6 million are redundant, revealing a serious limitation in the current indel annotation. Statistics results prove the consistency of the pipeline on indel redundancy detection for all 22 chromosomes. Apart from the standalone Vindel pipeline, the indel redundancy check algorithm is also implemented in the web server http://bioinformatics.cs.vt.edu/zhanglab/indelRedundant.php .	autosome;bioinformatics;checking (action);chromosomes;chromosomes, human, pair 22;clinical act of insertion;communications satellite;databases;deletion mutation;error detection and correction;indel mutation;insertion mutation;massively-parallel sequencing;morphologic artifacts;ncbi taxonomy;population;snp annotation;server (computing);substring;web server;algorithm;dbsnp	Zhiyi Li;Xiaowei Wu;Bin He;Liqing Zhang	2014		10.1186/s12859-014-0359-1	biology;molecular biology;dna microarray;bioinformatics;sequence alignment;genetics;genome	HPC	0.7533954362199515	-55.02869352682222	187489
89206f21d8991368cf779b02c4c5624d708f3ee0	metasv: an accurate and integrative structural-variant caller for next generation sequencing		UNLABELLED Structural variations (SVs) are large genomic rearrangements that vary significantly in size, making them challenging to detect with the relatively short reads from next-generation sequencing (NGS). Different SV detection methods have been developed; however, each is limited to specific kinds of SVs with varying accuracy and resolution. Previous works have attempted to combine different methods, but they still suffer from poor accuracy particularly for insertions. We propose MetaSV, an integrated SV caller which leverages multiple orthogonal SV signals for high accuracy and resolution. MetaSV proceeds by merging SVs from multiple tools for all types of SVs. It also analyzes soft-clipped reads from alignment to detect insertions accurately since existing tools underestimate insertion SVs. Local assembly in combination with dynamic programming is used to improve breakpoint resolution. Paired-end and coverage information is used to predict SV genotypes. Using simulation and experimental data, we demonstrate the effectiveness of MetaSV across various SV types and sizes.   AVAILABILITY AND IMPLEMENTATION Code in Python is at http://bioinform.github.io/metasv/.   CONTACT rd@bina.com   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	attempt;bina48;bioinformatics;biopolymer sequencing;breakpoint;clinical act of insertion;communications satellite;conflict (psychology);dna sequence rearrangement;dynamic programming;genotype;insertion mutation;massively-parallel sequencing;python;simulation;systemverilog;funding grant	Marghoob Mohiyuddin;John C. Mu;Narges Bani Asadi;Mark Gerstein;Alexej Abyzov;Wing H. Wong;Hugo Y. K. Lam	2015		10.1093/bioinformatics/btv204	bioinformatics;data mining;world wide web	Comp.	1.2358550788434624	-54.65524802315381	187860
87d43682ad468f33a2f98e7ef685139c3fbd8679	pergola: fast and deterministic linkage mapping of polyploids	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	A large share of agriculturally and horticulturally important plant species are polyploid. Linkage maps are used to locate associations between genes and traits by breeders and geneticists. Linkage map creation for polyploid species is not supported by standard tools. We want to overcome this limitation and validate our results with simulation studies. We developed PERGOLA, a deterministic and heuristic method that addresses this problem. We show that it creates correct linkage groups, marker orders and distances for simulated and real datasets. We compare it to existing tools and demonstrate that it overcomes limitations in ploidy and outperforms them in computational time and mapping accuracy. We represent linkage maps as dendrograms and show that this has advantages in the comparison of different maps. PERGOLA can be used successfully to calculate linkage maps for diploid and polyploid species and outperforms existing tools.	addresses (publication format);chromosome mapping;dendrogram;diploidy;distance;heuristic;linkage (software);map;mental association;polyploidy;simulation;time complexity;trait;genetic linkage;orders - hl7publishingdomain	Fabian Grandke;Soumya Ranganathan;Nikkie van Bers;Jorn R. de Haan;Dirk Metzler	2016		10.1186/s12859-016-1416-8	biology;dna microarray;computer science;bioinformatics;genetics	Comp.	1.6507020413583124	-53.44830352851314	188287
5a172f280df5aff228cdf4cb93a055e040498d65	fusionhunter: identifying fusion transcripts in cancer using paired-end rna-seq	rna mensajero;cancer;tumor maligno;rna messager;high throughput sequencing;messenger rna;tumeur maligne;sequencage a haut debit;malignant tumor	MOTIVATION Fusion transcripts can be created as a result of genome rearrangement in cancer. Some of them play important roles in carcinogenesis, and can serve as diagnostic and therapeutic targets. With more and more cancer genomes being sequenced by next-generation sequencing technologies, we believe an efficient tool for reliably identifying fusion transcripts will be desirable for many groups.   RESULTS We designed and implemented an open-source software tool, called FusionHunter, which reliably identifies fusion transcripts from transcriptional analysis of paired-end RNA-seq. We show that FusionHunter can accurately detect fusions that were previously confirmed by RT-PCR in a publicly available dataset. The purpose of FusionHunter is to identify potential fusions with high sensitivity and specificity and to guide further functional validation in the laboratory.   AVAILABILITY http://bioen-compbio.bioen.illinois.edu/FusionHunter/.	biopolymer sequencing;carcinogenesis;dna sequence rearrangement;genome;massively-parallel sequencing;open-source software;programming tool;rna;sensitivity and specificity;sequence number;silo (dataset);transcript;transcription, genetic	Yang Li;Jeremey Chien;David I. Smith;Jian Ma	2011	Bioinformatics	10.1093/bioinformatics/btr265	biology;bioinformatics;genetics;cancer	Comp.	0.09083281944096981	-57.25069198946845	189023
f3ca36238de5ddadde1f06bccc1291eda0814f50	ploidyngs: visually exploring ploidy with next generation sequencing data		Summary ploidyNGS is a model-free, open source tool to visualize and explore ploidy levels in a newly sequenced genome, exploiting short read data. We tested ploidyNGS using both simulated and real NGS data of the model yeast Saccharomyces cerevisiae. ploidyNGS allows the identification of the ploidy level of a newly sequenced genome in a visual way.   Availability and Implementation ploidyNGS is available under the GNU General Public License (GPL) at https://github.com/diriano/ploidyNGS. ploidyNGS is implemented in Python and R.   Contact diriano@gmail.com.	communications satellite;gnu;massively-parallel sequencing;open-source software;ploidies;python;saccharomyces cerevisiae	Renato A. C. dos Santos;Gustavo Henrique Goldman;Diego Mauricio Riaño-Pachón	2017	Bioinformatics	10.1093/bioinformatics/btx204	ploidy;computer science;genome;dna sequencing;bioinformatics;python (programming language);saccharomyces cerevisiae	Comp.	-2.2131400843623807	-57.41765116671374	189179
e5d55a6aef915523597c5999aa9dab2114c50cee	index-based similarity search for protein structure databases	feature vectors;index structure;statistical model;dataset join;feature vector;protein structure;protein structures;indexing;secondary structure;indexation;similarity search	We propose two methods for finding similarities in protein structure databases. Our techniques extract feature vectors on triplets of SSEs (Secondary Structure Elements) of proteins. These feature vectors are then indexed using a multidimensional index structure. Our first technique considers the problem of finding proteins similar to a given query protein in a protein dataset. This technique quickly finds promising proteins using the index structure. These proteins are then aligned to the query protein using a popular pairwise alignment tool such as VAST. We also develop a novel statistical model to estimate the goodness of a match using the SSEs. Our second technique considers the problem of joining two protein datasets to find an all-to-all similarity. Experimental results show that our techniques improve the pruning time of VAST 3 to 3.5 times while keeping the sensitivity similar.		Orhan Çamoglu;Tamer Kahveci;Ambuj K. Singh	2003	Proceedings. IEEE Computer Society Bioinformatics Conference	10.1142/S0219720004000491	biochemistry;protein structure;feature vector;computer science;bioinformatics;data science;machine learning;data mining;mathematics;protein structure database	Comp.	-2.5868822965231746	-52.19937654828898	189207
a4dcf000c6b73e9ac41332b63239b0bbb21fcfdd	empirical bayes conditional independence graphs for regulatory network recovery	software;gene regulatory networks;bayes theorem;respiratory mucosa;algorithms;humans;computational biology;computer simulation;gene expression profiling	MOTIVATION Computational inference methods that make use of graphical models to extract regulatory networks from gene expression data can have difficulty reconstructing dense regions of a network, a consequence of both computational complexity and unreliable parameter estimation when sample size is small. As a result, identification of hub genes is of special difficulty for these methods.   METHODS We present a new algorithm, Empirical Light Mutual Min (ELMM), for large network reconstruction that has properties well suited for recovery of graphs with high-degree nodes. ELMM reconstructs the undirected graph of a regulatory network using empirical Bayes conditional independence testing with a heuristic relaxation of independence constraints in dense areas of the graph. This relaxation allows only one gene of a pair with a putative relation to be aware of the network connection, an approach that is aimed at easing multiple testing problems associated with recovering densely connected structures.   RESULTS Using in silico data, we show that ELMM has better performance than commonly used network inference algorithms including GeneNet, ARACNE, FOCI, GENIE3 and GLASSO. We also apply ELMM to reconstruct a network among 5492 genes expressed in human lung airway epithelium of healthy non-smokers, healthy smokers and individuals with chronic obstructive pulmonary disease assayed using microarrays. The analysis identifies dense sub-networks that are consistent with known regulatory relationships in the lung airway and also suggests novel hub regulatory relationships among a number of genes that play roles in oxidative stress and secretion.   AVAILABILITY AND IMPLEMENTATION Software for running ELMM is made available at http://mezeylab.cb.bscb.cornell.edu/Software.aspx.   CONTACT ramimahdi@yahoo.com or jgm45@cornell.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithm;bioinformatics;causal filter;chronic obstructive airway disease;computational complexity theory;control system;direct mode;estimation theory;gw-basic;gene expression;graph - visual representation;graphical model;heuristic;inference;linear programming relaxation;lung diseases, obstructive;lung diseases;microarray;oxidative stress;population parameter;role playing disorder;scalability;structure of parenchyma of lung;usb hub;undirected graph;word lists by frequency;funding grant	Rami Mahdi;Abishek S. Madduri;Guoqing Wang;Yael Strulovici-Barel;Jacqueline Salit;Neil R. Hackett;Ronald G. Crystal;Jason G. Mezey	2012	Bioinformatics	10.1093/bioinformatics/bts312	computer simulation;biology;gene regulatory network;computer science;bioinformatics;theoretical computer science;machine learning;gene expression profiling;bayes' theorem;statistics	Comp.	2.5667961827172388	-52.10981651293973	189314
0c4ee089c91ca8c8f5a4c3fd050df0f70b81775a	asian: a web server for inferring a regulatory network framework from gene expression profiles	gene expression profile;software;hierarchical clustering;graphical gaussian model;expression profile;regulatory network;computer graphics;stopping rule;cluster analysis;internet;gene expression regulation;models statistical;user computer interface;correlation coefficient;computational biology;gene function;gene regulatory network;gene expression profiling	The standard workflow in gene expression profile analysis to identify gene function is the clustering by various metrics and techniques, and the following analyses, such as sequence analyses of upstream regions. A further challenging analysis is the inference of a gene regulatory network, and some computational methods have been intensively developed to deduce the gene regulatory network. Here, we describe our web server for inferring a framework of regulatory networks from a large number of gene expression profiles, based on graphical Gaussian modeling (GGM) in combination with hierarchical clustering (http://eureka.ims.u-tokyo.ac.jp/asian). GGM is based on a simple mathematical structure, which is the calculation of the inverse of the correlation coefficient matrix between variables, and therefore, our server can analyze a wide variety of data within a reasonable computational time. The server allows users to input the expression profiles, and it outputs the dendrogram of genes by several hierarchical clustering techniques, the cluster number estimated by a stopping rule for hierarchical clustering and the network between the clusters by GGM, with the respective graphical presentations. Thus, the ASIAN (Automatic System for Inferring A Network) web server provides an initial basis for inferring regulatory relationships, in that the clustering serves as the first step toward identifying the gene function.	cluster analysis;coefficient;dendrogram;dividend discount model;gene expression profiling;gene regulatory network;graphical user interface;hierarchical clustering;inference;information science;mathematical structure;mathematics;normal statistical distribution;pierre robin syndrome;sequence analysis;server (computer);server (computing);sports;time complexity;web server;statistical cluster	Sachiyo Aburatani;Kousuke Goto;Shigeru Saito;Hiroyuki Toh;Katsuhisa Horimoto	2005	Nucleic Acids Research	10.1093/nar/gki446	biology;gene regulatory network;the internet;regulation of gene expression;bioinformatics;hierarchical clustering;gene expression profiling;cluster analysis;computer graphics;genetics;hierarchical clustering of networks	ML	-2.0295771610586093	-56.48028674116051	189416
4f1a4d8338d550594babf915115ec8fd6188b6f3	infotrim: a dna read quality trimmer using entropy		Biological DNA reads are often trimmed before mapping, genome assembly, and other tasks to improve the quality of the results. Biological sequence complexity relates to alignment quality as low complexity regions can align poorly. There are many read trimmers, but many do not use sequence complexity for trimming. Alignment of reads generated from whole genome bisulfite sequencing is especially challenging since bisulfite treated reads tend to reduce sequence complexity. InfoTrim, a new read trimmer, was created to explore these issues. It is evaluated against five other trimmers using four read mappers on real and simulated bisulfite treated DNA data. InfoTrim has reasonable results.	entropy (information theory)	Jacob Porter;Liqing Zhang	2017		10.1109/ICCABS.2017.8114288	genetics;trimming;trimmer;bisulfite;dna;sequence assembly;whole genome bisulfite sequencing;bioinformatics;a-dna;biology	Logic	0.2890345096523475	-53.47314383758056	189746
9578c1bb304fc4a6f8bb147aea80957aed9ebfad	an automated approach of designing multiplex pcr primers for the amplification of exons		This paper presents a new program to design specific and reliable primers for amplification of exons in a multiplex PCR. The program is composed of two components: a data acquisition component and a data processing component. The data acquisition component automates time–consuming steps of preparing data for the primers design algorithms. It provides up-to-date information about selected genes including coding sequences and locations of SNPs. The data processing component automates the process of grouping candidate primers into an optimal set for the multiplex PCR by checking their specificity. It is an original and unconventional method that combines the hierarchical clustering and the separate-and-conquer strategy. The results obtained for various genes and presented in this paper prove that the proposed method is an effective way to design an optimal set of primers for the multiplex PCR.	multiplexing	Adam Skowron;Rafal Pokrzywa	2012		10.1007/978-3-642-32741-4_22	in silico pcr	NLP	-0.053671280759716423	-57.29496404573546	190205
0618e3c65b5c81dcb90a4e5fea5982eb6e636b60	finishing bacterial genome assemblies with mix	u10 methodes mathematiques et statistiques;high throughput nucleotide sequencing;sequence analysis dna;mycoplasma;computational biology bioinformatics;genome bacterial;algorithms;combinatorial libraries;software design;computer appl in life sciences;h20 maladies des plantes;microarrays;bioinformatics	Among challenges that hamper reaping the benefits of genome assembly are both unfinished assemblies and the ensuing experimental costs. First, numerous software solutions for genome de novo assembly are available, each having its advantages and drawbacks, without clear guidelines as to how to choose among them. Second, these solutions produce draft assemblies that often require a resource intensive finishing phase. In this paper we address these two aspects by developing Mix , a tool that mixes two or more draft assemblies, without relying on a reference genome and having the goal to reduce contig fragmentation and thus speed-up genome finishing. The proposed algorithm builds an extension graph where vertices represent extremities of contigs and edges represent existing alignments between these extremities. These alignment edges are used for contig extension. The resulting output assembly corresponds to a set of paths in the extension graph that maximizes the cumulative contig length. We evaluate the performance of Mix on bacterial NGS data from the GAGE-B study and apply it to newly sequenced Mycoplasma genomes. Resulting final assemblies demonstrate a significant improvement in the overall assembly quality. In particular, Mix is consistent by providing better overall quality results even when the choice is guided solely by standard assembly statistics, as is the case for de novo projects. Mix is implemented in Python and is available at https://github.com/cbib/MIX , novel data for our Mycoplasma study is available at http://services.cbib.u-bordeaux2.fr/mix/ .	communications satellite;de novo transcriptome assembly;fragmentation (computing);gage;genome;genome, bacterial;graph - visual representation;limb structure;mix;mycoplasma laboratorium;python;solutions;algorithm;benefit	Hayssam Soueidan;Florence Maurier;Alexis Groppi;Pascal Sirand-Pugnet;Florence Tardy;Christine Citti;Virginie Dupuy;Macha Nikolski	2013		10.1186/1471-2105-14-S15-S16	biology;dna microarray;computer science;bioinformatics;software design;genetics	Comp.	-0.20686408956564847	-54.53444823467564	190309
a22cd75cf1af7447668e775cd391df080fb67172	haystack, a web-based tool for metabolomics research	software;metabolomics;mass spectrometry;computational biology bioinformatics;cluster analysis;internet;chromatography liquid;algorithms;combinatorial libraries;computer appl in life sciences;microarrays;bioinformatics	Liquid chromatography coupled to mass spectrometry (LCMS) has become a widely used technique in metabolomics research for differential profiling, the broad screening of biomolecular constituents across multiple samples to diagnose phenotypic differences and elucidate relevant features. However, a significant limitation in LCMS-based metabolomics is the high-throughput data processing required for robust statistical analysis and data modeling for large numbers of samples with hundreds of unique chemical species. To address this problem, we developed Haystack, a web-based tool designed to visualize, parse, filter, and extract significant features from LCMS datasets rapidly and efficiently. Haystack runs in a browser environment with an intuitive graphical user interface that provides both display and data processing options. Total ion chromatograms (TICs) and base peak chromatograms (BPCs) are automatically displayed, along with time-resolved mass spectra and extracted ion chromatograms (EICs) over any mass range. Output files in the common .csv format can be saved for further statistical analysis or customized graphing. Haystack's core function is a flexible binning procedure that converts the mass dimension of the chromatogram into a set of interval variables that can uniquely identify a sample. Binned mass data can be analyzed by exploratory methods such as principal component analysis (PCA) to model class assignment and identify discriminatory features. The validity of this approach is demonstrated by comparison of a dataset from plants grown at two light conditions with manual and automated peak detection methods. Haystack successfully predicted class assignment based on PCA and cluster analysis, and identified discriminatory features based on analysis of EICs of significant bins. Haystack, a new online tool for rapid processing and analysis of LCMS-based metabolomics data is described. It offers users a range of data visualization options and supports non-biased differential profiling studies through a unique and flexible binning function that provides an alternative to conventional peak deconvolution analysis methods.	cluster analysis;customize;data modeling;data visualization;deconvolution;extraction;graphical user interface;high-throughput computing;imagery;ions;liquid chromatography mass spectrometry;little cms;metabolomics;parsing;principal component analysis;product binning;silo (dataset);throughput;user interface device component;web application	Stephen C. Grace;Stephen Embry;Heng Luo	2014		10.1186/1471-2105-15-S11-S12	biology;the internet;dna microarray;mass spectrometry;computer science;bioinformatics;metabolomics;cluster analysis	Comp.	-2.3508925771728983	-57.95701912734217	190508
1bdb87a49f02d2316899b60d728fc01e9658cddd	tisdb: a database for alternative translation initiation in mammalian cells	animals;mice;peptide chain initiation translational;databases genetic;codon initiator;internet;genome;humans;open reading frames	Proper selection of the translation initiation site (TIS) on mRNAs is crucial for the production of desired protein products. Recent studies using ribosome profiling technology uncovered a surprising variety of potential TIS sites in addition to the annotated start codon. The prevailing alternative translation reshapes the landscape of the proteome in terms of diversity and complexity. To identify the hidden coding potential of the transcriptome in mammalian cells, we developed global translation initiation sequencing (GTI-Seq) that maps genome-wide TIS positions at nearly a single nucleotide resolution. To facilitate studies of alternative translation, we created a database of alternative TIS sites identified from human and mouse cell lines based on multiple GTI-Seq replicates. The TISdb, available at http://tisdb.human.cornell.edu, includes 6991 TIS sites from 4961 human genes and 9973 TIS sites from 5668 mouse genes. The TISdb website provides a simple browser interface for query of high-confidence TIS sites and their associated open reading frames. The output of search results provides a user-friendly visualization of TIS information in the context of transcript isoforms. Together, the information in the database provides an easy reference for alternative translation in mammalian cells and will support future investigation of novel translational products.	biopolymer sequencing;codon, initiator;frame (physical object);genetic translation process;grid-tie inverter;interface device component;machine translation;mammals;nucleotides;open reading frames;open reading frame;protein isoforms;question (inquiry);reading frames (nucleotide sequence);ribosomes;transcript;transcription initiation;transcriptome;translation initiation;usability;web site	Ji Wan;Shu-Bing Qian	2014		10.1093/nar/gkt1085	open reading frame;biology;the internet;bioinformatics;genetics;genome	Comp.	-0.1760088113836538	-58.37543207437613	190871
81d104d175a7c2648c37cb4861fab9028202829b	manipulating multiple sequence alignments via mam and webmam	software;genomics;software tool;phylogeny;web interface;repetitive sequences nucleic acid;large scale;quality assessment;internet;genomic dna;source code;sequence analysis;sequence alignment;multiple sequence alignment;exons;genome sequence;phylogenetic analysis;multiple alignment	MaM is a software tool that processes and manipulates multiple alignments of genomic sequence. MaM computes the exact location of common repeat elements, exons and unique regions within aligned genomics sequences using a variety of user identified programs, databases and/or tables. The program can extract subalignments, corresponding to these various regions of DNA to be analyzed independently or in conjunction with other elements of genomic DNA. Graphical displays further allow an assessment of sequence variation throughout these different regions of the aligned sequence, providing separate displays for their repeat, non-repeat and coding portions of genomic DNA. The program should facilitate the phylogenetic analysis and processing of different portions of genomic sequence as part of large-scale sequencing efforts. MaM source code is freely available for non-commercial use at http://compbio.cs.sfu.ca/MAM.htm; and the web interface WebMaM is hosted at http://atgc.lirmm.fr/mam.	computer graphics;data table;exons;genomics;interface device component;lymphoma, large-cell, follicular;mammography;multiple sequence alignment;phylogenetics;programming tool;published database;source code;user interface;variation (genetics)	Can Alkan;Eray Tüzün;Jerome Buard;Franck Lethiec;Evan E. Eichler;Jeffrey A. Bailey;Süleyman Cenk Sahinalp	2005	Nucleic Acids Research	10.1093/nar/gki406	biology;genomics;molecular biology;multiple sequence alignment;bioinformatics;genetics	Comp.	-2.7102335119579175	-58.07847527110573	191068
897aec4342f43a6e0d75ea666ab9f23c49ee8ff9	knowledge-based analysis of microarrays for the discovery of transcriptional regulation relationships	transcription genetic;microarray data;sensitivity and specificity;saccharomyces cerevisiae;knowledge bases;proteome;gene regulatory networks;performance comparison;gene expression data;computational biology bioinformatics;chip;transcription regulation;transcription factor;algorithms;combinatorial libraries;high throughput;computer appl in life sciences;oligonucleotide array sequence analysis;databases protein;microarrays;bioinformatics;knowledge base	The large amount of high-throughput genomic data has facilitated the discovery of the regulatory relationships between transcription factors and their target genes. While early methods for discovery of transcriptional regulation relationships from microarray data often focused on the high-throughput experimental data alone, more recent approaches have explored the integration of external knowledge bases of gene interactions. In this work, we develop an algorithm that provides improved performance in the prediction of transcriptional regulatory relationships by supplementing the analysis of microarray data with a new method of integrating information from an existing knowledge base. Using a well-known dataset of yeast microarrays and the Yeast Proteome Database, a comprehensive collection of known information of yeast genes, we show that knowledge-based predictions demonstrate better sensitivity and specificity in inferring new transcriptional interactions than predictions from microarray data alone. We also show that comprehensive, direct and high-quality knowledge bases provide better prediction performance. Comparison of our results with ChIP-chip data and growth fitness data suggests that our predicted genome-wide regulatory pairs in yeast are reasonable candidates for follow-up biological verification. High quality, comprehensive, and direct knowledge bases, when combined with appropriate bioinformatic algorithms, can significantly improve the discovery of gene regulatory relationships from high throughput gene expression data.	algorithm;aquaporin 1;bmc bioinformatics;bio-informatics;bioinformatics;chip-on-chip;computation;database;display resolution;gene expression;high-throughput computing;interaction;javascript;kegg;kidney diseases;knowledge bases;knowledge base;knowledge-based systems;machine learning;microarray;proteome;revision procedure;ruby document format;scientific publication;sensitivity and specificity;silo (dataset);transcription factor;throughput;transcription (software);transcription, genetic;transcriptional regulation;verification of theories;algorithm;chromatin silencing;contents - htmllinktype;funding grant;interest;mecarzole;wxwidgets	Junhee Seok;Amit Kaushal;Ronald W. Davis;Wenzhong Xiao	2010		10.1186/1471-2105-11-S1-S8	chip;high-throughput screening;biology;microarray analysis techniques;gene regulatory network;knowledge base;molecular biology;dna microarray;computer science;bioinformatics;proteome;genetics;transcriptional regulation;transcription factor	Comp.	1.876472167623571	-55.346013199684144	191192
49dd5e9b5f5ea48aef932b7a45498735b35386f4	reusable, extensible, and modifiable r scripts and kepler workflows for comprehensive single set chip-seq analysis	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	There has been an enormous expansion of use of chromatin immunoprecipitation followed by sequencing (ChIP-seq) technologies. Analysis of large-scale ChIP-seq datasets involves a complex series of steps and production of several specialized graphical outputs. A number of systems have emphasized custom development of ChIP-seq pipelines. These systems are primarily based on custom programming of a single, complex pipeline or supply libraries of modules and do not produce the full range of outputs commonly produced for ChIP-seq datasets. It is desirable to have more comprehensive pipelines, in particular ones addressing common metadata tasks, such as pathway analysis, and pipelines producing standard complex graphical outputs. It is advantageous if these are highly modular systems, available as both turnkey pipelines and individual modules, that are easily comprehensible, modifiable and extensible to allow rapid alteration in response to new analysis developments in this growing area. Furthermore, it is advantageous if these pipelines allow data provenance tracking. We present a set of 20 ChIP-seq analysis software modules implemented in the Kepler workflow system; most (18/20) were also implemented as standalone, fully functional R scripts. The set consists of four full turnkey pipelines and 16 component modules. The turnkey pipelines in Kepler allow data provenance tracking. Implementation emphasized use of common R packages and widely-used external tools (e.g., MACS for peak finding), along with custom programming. This software presents comprehensive solutions and easily repurposed code blocks for ChIP-seq analysis and pipeline creation. Tasks include mapping raw reads, peakfinding via MACS, summary statistics, peak location statistics, summary plots centered on the transcription start site (TSS), gene ontology, pathway analysis, and de novo motif finding, among others. These pipelines range from those performing a single task to those performing full analyses of ChIP-seq data. The pipelines are supplied as both Kepler workflows, which allow data provenance tracking, and, in the majority of cases, as standalone R scripts. These pipelines are designed for ease of modification and repurposing.	base sequence;biopolymer sequencing;code::blocks;de novo transcriptome assembly;gene ontology;gene regulatory network;graphical user interface;kepler;lab-on-a-chip devices;libraries;microchip analytical devices;motif;pathway analysis;pipeline (computing);providing (action);sequence number;solutions;toxic shock syndrome;transcription (software);transcription initiation site;turnkey;chromatin immunoprecipitation;magnetic cell separation system	Nathan Cormier;Tyler Kolisnik;Mark Bieda	2016		10.1186/s12859-016-1125-3	biology;dna microarray;computer science;bioinformatics;theoretical computer science;data mining;algorithm	Graphics	-1.7121441792495928	-57.9616252003984	191945
c3f2de2c5844e14fb7722eca11ad5d2f6936f129	computational assessment of synthetic procedures	conformational analysis;computational method;data analysis;computer analysis	Synthetic chemistry is hard because some reasonable looking molecules cannot be made, because there are errors in the chemical literature, because it is easy to miss reaction possibilities and because even the shape of molecules is very difficult to determine. We propose an approach to the computational analysis of reactions that tries to circumvent these difficulties, by restricting the analysis to simple rules for reactivity that can generate a large number of competing pathways. This huge ensemble is filtered using computational methods to pick out the most likely pathways, and to suggest possible products.	computation;rule (guideline);synthetic biology;synthetic intelligence	Jonathan M. Goodman;Ingrid M. Socorro	2007	Journal of computer-aided molecular design	10.1007/s10822-007-9120-4	computer science;combinatorial chemistry;nanotechnology;data analysis;statistics	Comp.	0.6819257414515308	-53.39554277723213	192451
0c3411e652dc036239ec3103ed9ace2b4165783f	automated extraction of information on protein-protein interactions from the biological literature	traitement automatise;extraction information;computer program;escherichia coli;yeast;saccharomyces cerevisiae;proteine;thallophyta;information extraction;interaction moleculaire;bacterie;indexation automatique;information retrieval;molecular interaction;base donnee bibliographique;levure;bioinformatique;litterature scientifique;enterobacteriaceae;base datos bibliografica;levadura;fungi;interaccion molecular;literatura cientifica;proteomique;recherche information;tratamiento automatizado;part of speech;protein protein interaction;automatic indexing;bibliographical batabase;proteina;bacteria;ascomycetes;recuperacion informacion;protein interaction;proteomics;protein;programa computador;scientific literature;programme ordinateur;indizacion automatica;automated processing;biological process;bioinformatics;extraction informacion	MOTIVATION To understand biological process, we must clarify how proteins interact with each other. However, since information about protein-protein interactions still exists primarily in the scientific literature, it is not accessible in a computer-readable format. Efficient processing of large amounts of interactions therefore needs an intelligent information extraction method. Our aim is to develop an efficient method for extracting information on protein-protein interaction from scientific literature.   RESULTS We present a method for extracting information on protein-protein interactions from the scientific literature. This method, which employs only a protein name dictionary, surface clues on word patterns and simple part-of-speech rules, achieved high recall and precision rates for yeast (recall = 86.8% and precision = 94.3%) and Escherichia coli (recall = 82.5% and precision = 93.5%). The result of extraction suggests that our method should be applicable to any species for which a protein name dictionary is constructed.   AVAILABILITY The program is available on request from the authors.	biological processes;dictionary [publication type];human-readable medium;information extraction;ninl gene;name;natural language processing;precision and recall;rule (guideline);scientific literature;staphylococcal protein a;protein protein interaction	Toshihide Ono;Haretsugu Hishigaki;Akira Tanigami;Toshihisa Takagi	2001	Bioinformatics	10.1093/bioinformatics/17.2.155	protein–protein interaction;thallophyte;biology;bacteria;part of speech;bioinformatics;artificial intelligence;proteomics;escherichia coli;biological process;genetics;information extraction	Comp.	-4.431694896501035	-56.406373704437456	192714
e55f8b9175f50ae894fec68f30e2f8ee05a615f0	prot-2s: a new python web tool for protein secondary structure studies	electronic learning;protein chain;amino acid patterns;2s motifs;online learning;protein secondary structures;python web tools;protein secondary structure;amino acid propensity;e learning;prot 2s;python online application;bioinformatics	Prot-2S is a bioinformatics web application devised to analyse the protein chain secondary structures (2S) (http:/ /www.requimte.pt:8080/Prot-2S/). The tool is built on the RCSB Protein Data Bank PDB and DSSP application/files and includes calculation/graphical display of amino acid propensities in 2S motifs based on any user amino acid classification/code (for any particular protein chain list). The interface can calculate the 2S composition, display the 2S subsequences and search for DSSP non-standard residues and for pairs/triplets/quadruplets (amino acid patterns in 2S motifs). This work presents some Prot-2S applications showing its usefulness in protein research and as an e-learning tool as well.		Cristian R. Munteanu;Alexandre Magalhães	2009	International journal of bioinformatics research and applications	10.1504/IJBRA.2009.027513	biology;computer science;bioinformatics;protein structure prediction;data mining;world wide web;protein secondary structure	Comp.	-1.6147584051001491	-59.000169489020095	192776
dcc52ac374b6b5b05e8de3a399f89feda3e77764	modbase, a database of annotated comparative protein structure models, and associated resources		MODBASE (http://salilab.org/modbase) is a database of annotated comparative protein structure models for all available protein sequences that can be matched to at least one known protein structure. The models are calculated by MODPIPE, an automated modeling pipeline that relies on MODELLER for fold assignment, sequence-structure alignment, model building and model assessment (http:/salilab.org/modeller). MODBASE is updated regularly to reflect the growth in protein sequence and structure databases, and improvements in the software for calculating the models. MODBASE currently contains 3 094 524 reliable models for domains in 1 094 750 out of 1 817 889 unique protein sequences in the UniProt database (July 5, 2005); only models based on statistically significant alignments and models assessed to have the correct fold despite insignificant alignments are included. MODBASE also allows users to generate comparative models for proteins of interest with the automated modeling server MODWEB (http://salilab.org/modweb). Our other resources integrated with MODBASE include comprehensive databases of multiple protein structure alignments (DBAli, http://salilab.org/dbali), structurally defined ligand binding sites and structurally defined binary domain interfaces (PIBASE, http://salilab.org/pibase) as well as predictions of ligand binding sites, interactions between yeast proteins, and functional consequences of human nsSNPs (LS-SNP, http://salilab.org/LS-SNP).		Ursula Pieper;Ben M. Webb;David T. Barkan;Dina Schneidman-Duhovny;Avner Schlessinger;Hannes Braberg;Zheng Yang;Elaine C. Meng;Eric F. Pettersen;Conrad C. Huang;Ruchira S. Datta;Parthasarathy Sampathkumar;Mallur S. Madhusudhan;Kimmen Sjölander;Thomas E. Ferrin;Stephen K. Burley;Andrej Sali	2006	Nucleic acids research	10.1093/nar/gkq1091	biology;protein structure;genomics;molecular biology;bioinformatics;binding site;genetics	Comp.	-0.4759160062712073	-59.064521917795226	193081
5e69505239693d8b8f267f798616a18461ca0427	an intelligent primer design system for multiplex reverse transcription polymerase chain reaction and complementary dna microarray	drug discovery;clinical application;polymerase chain reaction;messenger rna;intelligent system;reverse transcription polymerase chain reaction;mathematical model;cdna microarray;cost effectiveness;microarray;primer design;dna microarray;gene function	Various biological and clinical applications require the quantification of the messenger RNA (mRNA) abundance of the biological materials under investigation. Reverse transcription-polymerase chain reaction (RT-PCR) is a popular technique for mRNA quantification. However, in these applications, multiple quantifications of mRNA are required from the same sample. Multiplex RT-PCR uses multiple primer pairs to amplify the specific targets simultaneously from a single sample. Therefore, multiplex RT-PCR is suitable for such applications. Complementary DNA (cDNA) microarray is a robust and versatile tool that allows researchers to examine the expression of hundreds or thousands of genes for the analysis of gene function, disease diagnosis, and drug discovery. Multiplex RT-PCR is usually performed for the cDNA preparation during the cDNA microarray experiments. Primer design is the most crucial step in multiplex RT-PCR. Therefore, the goal of this study was to implement an intelligent primer design system. The core of the system was based on mathematical techniques. The system can guide users to design cost effective primer collection. The system had also been successfully applied to two real world examples with biological and clinical significance. q 2005 Elsevier Ltd. All rights reserved.	analysis of algorithms;automatic variable;dna microarray;data structure;experiment;garbage collection (computer science);homology (biology);integer programming;linear programming;multiplexing;primer;sensitivity and specificity;transcription (software);universal quantification;window function;windows rt	Ming-Hua Hsieh;Ray Tsaih;Chiung-Yu Huang	2006	Expert Syst. Appl.	10.1016/j.eswa.2005.09.055	multiplex polymerase chain reaction;reverse transcription polymerase chain reaction;cost-effectiveness analysis;dna microarray;bioinformatics;polymerase chain reaction;microarray;mathematical model;messenger rna;drug discovery	Comp.	0.12866831575487092	-57.29918461827645	193098
14b5cad0820cf3e5f9eb24ace94fa805d30deb32	genepublisher: automated analysis of dna microarray data	software;farmaceutisk vetenskap;naturvetenskap;signal transduction;web interface;databases genetic;signal transduction pathway;natural sciences;cluster analysis;internet;statistical analysis;promoter regions genetic;pharmaceutical sciences;dna microarray data;metabolic pathway;dna microarray;gene expression profiling;oligonucleotide array sequence analysis;metabolism;regulatory sequences nucleic acid;automation	GenePublisher, a system for automatic analysis of data from DNA microarray experiments, has been implemented with a web interface at http://www.cbs.dtu.dk/services/GenePublisher. Raw data are uploaded to the server together with a specification of the data. The server performs normalization, statistical analysis and visualization of the data. The results are run against databases of signal transduction pathways, metabolic pathways and promoter sequences in order to extract more information. The results of the entire analysis are summarized in report form and returned to the user.	dna microarray format;database normalization;databases;experiment;imagery;interface device component;server (computer);server (computing);signal transduction pathways;specification;transduction (machine learning);user interface	Steen Knudsen;Christopher T. Workman;Thomas Sicheritz-Pontén;Carsten Friis	2003	Nucleic acids research	10.1093/nar/gkg629	biology;molecular biology;gene chip analysis;bioinformatics;pharmaceutical sciences;microarray databases;genetics;signal transduction	Comp.	-2.3996481436026613	-58.11361279259305	193214
5d752878b28d3504ac8fc24100fddd92c56d0fe7	easy-an expert analysis system for interpreting database search outputs	relation structure fonction;computer program;secuencia aminoacido;alignement sequence;proteine;sequence aminoacide;analisis datos;aminoacid sequence;easy;interrogation base donnee;relacion estructura funcion;motif structural;interrogacion base datos;bioinformatique;alineacion secuencia;structure function relationship;interpretacion informacion;motivo estructural;interpretation information;data analysis;proteomique;comparacion multiple;comparaison multiple;structural unit;analyse donnee;proteina;sequence alignment;proteomics;multiple comparison;protein;database search;programa computador;information interpretation;database query;programme ordinateur;bioinformatics	With the ever-increasing need to handle large volumes of sequence data efficiently and reliably, we have developed the EASY system for performing combined protein sequence and pattern database searches. EASY runs searches simultaneously and distils results into a concise 1-line diagnosis. By bringing together results of several different analyses, EASY provides a rapid means of evaluating biological significance, minimising the risk of inferring false relationships, for example from relying exclusively on top BLAST hits. The program has been tested using a variety of protein families and was instrumental in resolving family assignments in a major update of the PRINTS database.	amino acid sequence;blast;prints;protein family	J. N. Selley;J. Swift;Terri K. Attwood	2001	Bioinformatics	10.1093/bioinformatics/17.1.105	biology;database search engine;computer science;bioinformatics;structural unit;sequence alignment;data mining;proteomics;data analysis;multiple comparisons problem;algorithm	Comp.	-4.2127027991766735	-56.08978133598091	193295
1cf734c386f3182ea6462012758bc0a903b0ff4b	hidden markov models for sequence analysis: extension and analysis of the basic method	modelizacion;computer program;secuencia aminoacido;alignement sequence;sequence aminoacide;modelo markov;aminoacid sequence;computerized processing;tratamiento informatico;hidden markov model;dynamic model;modele lineaire;secuencia nucleotido;modelo lineal;alineacion secuencia;sh2 domain;nucleotide sequence;sequence nucleotide;modelisation;markov model;expectation maximization;linear model;sequence analysis;sequence alignment;modele markov;programa computador;modeling;traitement informatique;programme ordinateur;multiple alignment	Hidden Markov models (HMMs) are a highly effective means of modeling a family of unaligned sequences or a common motif within a set of unaligned sequences. The trained HMM can then be used for discrimination or multiple alignment. The basic mathematical description of an HMM and its expectation-maximization training procedure is relatively straightforward. In this paper, we review the mathematical extensions and heuristics that move the method from the theoretical to the practical. We then experimentally analyze the effectiveness of model regularization, dynamic model modification and optimization strategies. Finally it is demonstrated on the SH2 domain how a domain can be found from unaligned sequences using a special model type. The experimental work was completed with the aid of the Sequence Alignment and Modeling software suite.	expectation–maximization algorithm;experiment;heuristics;hidden markov model;markov chain;mathematical model;mathematical optimization;mathematics;motif;multiple sequence alignment;sequence analysis;software suite	Richard Hughey;Anders Krogh	1996	Computer applications in the biosciences : CABIOS	10.1093/bioinformatics/12.2.95	biology;sh2 domain;systems modeling;expectation–maximization algorithm;multiple sequence alignment;nucleic acid sequence;computer science;bioinformatics;machine learning;sequence analysis;linear model;sequence alignment;mathematics;markov model;algorithm;hidden markov model	AI	-3.941144383465681	-54.89235470155847	193324
2a90103abd95ad3bbe0c28f6e8af6c78aadf3305	computer programs to analyze dna and amino acid sequence data	dna;genes;computers;software;computer program;base composition;methods;amino acid sequence;proteins;dna restriction enzymes;base sequence	Extensive modifications have been incorporated into many of the computer programs written by Staden (1-4) to make them easier to cope with DNA and amino acid sequence data. These programs can be easily used by persons with minimal knowledge of computers.	amino acid sequence;amino acids;computer program	K. Isono	1982	Nucleic acids research	10.1093/nar/10.1.85	biology;biochemistry;bioinformatics;gene;peptide sequence;genetics;dna	DB	-3.7008546740948214	-56.79817323846654	193767
1392ca1d4b0e9e241284d6ea100d1f6c9856ae7c	a parallel gibbs sampling algorithm for motif finding on gpu	dna;paper;graphical user interfaces bioinformatics computational complexity coprocessors;gibbs sampling;gpu;moti;biology;coprocessors;motif finding algorithm;cuda;gpgpu;gpgpu moti gibbs sampling cuda;graphical user interfaces;gibbs sampling method parallel gibbs sampling algorithm gpu graphics processing unit bioinformatics computational complexity gene transcription data motif finding algorithm;gibbs sampling method;computational complexity;sampling methods graphics sequences bioinformatics acceleration yarn computer science computational complexity biology computing central processing unit;classification algorithms;gene transcription data;nvidia;graphic processing unit;biological data;parallel gibbs sampling algorithm;sequence matching;graphics processing unit;algorithm design and analysis;gene transcription;bioinformatics	Motif is overrepresented pattern in biological sequence and Motif finding is an important problem in bioinformatics. Due to high computational complexity of motif finding, more and more computational capabilities are required as the rapid growth of available biological data, such as gene transcription data. Among many motif finding algorithms, Gibbs sampling is an effective method for long motif finding. In this paper we present an improved Gibbs sampling method on graphics processing units (GPU) to accelerate motif finding. Experimental data support that, compared to traditional programs on CPU, our program running on GPU provides an effective and low-cost solution for motif finding problem, especially for long Motif finding.	algorithm;bioinformatics;central processing unit;computational complexity theory;computer graphics;effective method;gibbs sampling;graphics processing unit;motif;sampling (signal processing);transcription (software)	Linbin Yu;Yun Xu	2009	2009 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2009.88	statistical classification;algorithm design;gibbs sampling;biological data;computer science;bioinformatics;theoretical computer science;machine learning;graphical user interface;computational complexity theory;transcription;dna;general-purpose computing on graphics processing units;coprocessor	Visualization	-1.6359969804408676	-53.33321135128705	193957
cd374e6c9729c1583ad096f38d348f34dee8872d	the transcriptome analysis and comparison explorer—t-ace	sequence information;de supplementary information;comparison explorer;different ngs platform;non-model organism;transcriptome analysis;transcriptome analysis;assigned information;transcriptome information;transcriptome analysis module;external analysis;transcriptome project	MOTIVATION Next generation sequencing (NGS) technologies allow a rapid and cost-effective compilation of large RNA sequence datasets in model and non-model organisms. However, the storage and analysis of transcriptome information from different NGS platforms is still a significant bottleneck, leading to a delay in data dissemination and subsequent biological understanding. Especially database interfaces with transcriptome analysis modules going beyond mere read counts are missing. Here, we present the Transcriptome Analysis and Comparison Explorer (T-ACE), a tool designed for the organization and analysis of large sequence datasets, and especially suited for transcriptome projects of non-model organisms with little or no a priori sequence information. T-ACE offers a TCL-based interface, which accesses a PostgreSQL database via a php-script. Within T-ACE, information belonging to single sequences or contigs, such as annotation or read coverage, is linked to the respective sequence and immediately accessible. Sequences and assigned information can be searched via keyword- or BLAST-search. Additionally, T-ACE provides within and between transcriptome analysis modules on the level of expression, GO terms, KEGG pathways and protein domains. Results are visualized and can be easily exported for external analysis. We developed T-ACE for laboratory environments, which have only a limited amount of bioinformatics support, and for collaborative projects in which different partners work on the same dataset from different locations or platforms (Windows/Linux/MacOS). For laboratories with some experience in bioinformatics and programming, the low complexity of the database structure and open-source code provides a framework that can be customized according to the different needs of the user and transcriptome project.		E. E. R. Philipp;L. Kraemer;D. Mountfort;M. Schilhabel;Stefan Schreiber;Philip Rosenstiel	2012	Bioinformatics	10.1093/bioinformatics/bts056	computer science;bioinformatics;data mining;world wide web	Comp.	-1.3401714573289694	-58.11347661955606	194771
80c0f26bc6f94546d6a2f1542d39e44c0d799994	visant 3.5: multi-scale network visualization, analysis and inference based on the gene ontology	software;functional annotation;software platform;computer graphics;network visualization;auxiliary information;gene regulatory networks;network analysis;network topology;models genetic;internet;levels of abstraction;algorithms;cell cycle;humans;biological database;systems integration;article;biological network;gene ontology	Despite its wide usage in biological databases and applications, the role of the gene ontology (GO) in network analysis is usually limited to functional annotation of genes or gene sets with auxiliary information on correlations ignored. Here, we report on new capabilities of VisANT--an integrative software platform for the visualization, mining, analysis and modeling of the biological networks--which extend the application of GO in network visualization, analysis and inference. The new VisANT functions can be classified into three categories. (i) Visualization: a new tree-based browser allows visualization of GO hierarchies. GO terms can be easily dropped into the network to group genes annotated under the term, thereby integrating the hierarchical ontology with the network. This facilitates multi-scale visualization and analysis. (ii) Flexible annotation schema: in addition to conventional methods for annotating network nodes with the most specific functional descriptions available, VisANT also provides functions to annotate genes at any customized level of abstraction. (iii) Finding over-represented GO terms and expression-enriched GO modules: two new algorithms have been implemented as VisANT plugins. One detects over-represented GO annotations in any given sub-network and the other finds the GO categories that are enriched in a specified phenotype or perturbed dataset. Both algorithms take account of network topology (i.e. correlations between genes based on various sources of evidence). VisANT is freely available at http://visant.bu.edu.	access network;anatomic node;anatomy, regional;annotation;biological database;biological network;categories;classification;customize;databases;description;dropping;gene ontology;graph drawing;imagery;inference;laboratory sample manual;nar 2;network topology;numerous;plug-in (computing);scalable vector graphics;scientific publication;silo (dataset);structure of articular surface of bone;subnetwork;tree (data structure);xml;algorithm;format	Zhenjun Hu;Jui-Hung Hung;Yan Wang;Yi-Chien Chang;Chia-Ling Huang;Matt Huyck;Charles DeLisi	2009		10.1093/nar/gkp406	biology;gene regulatory network;biological network;the internet;biological database;network analysis;bioinformatics;cell cycle;graph drawing;computer graphics;genetics;network topology;system integration	Comp.	-0.28848125751432757	-58.595547607791865	194975
8ad2678a938cb6d14a23796954465fadf71d76e6	lncipedia: a database for annotated human lncrna transcript sequences and structures	genes;maps;long noncoding rna;software;complex;reveals;protein structure secondary;databases nucleic acid;mass spectrometry;molecular sequence annotation;datasets;binding sites;tandem mass spectra;differentiation;internet;rna;proteins;micro rna;chromatin;gene;biology and life sciences;gene organization;transcriptome;humans;sequence analysis rna;gene expression profiling;rna long untranslated	Here, we present LNCipedia (http://www.lncipedia.org), a novel database for human long non-coding RNA (lncRNA) transcripts and genes. LncRNAs constitute a large and diverse class of non-coding RNA genes. Although several lncRNAs have been functionally annotated, the majority remains to be characterized. Different high-throughput methods to identify new lncRNAs (including RNA sequencing and annotation of chromatin-state maps) have been applied in various studies resulting in multiple unrelated lncRNA data sets. LNCipedia offers 21 488 annotated human lncRNA transcripts obtained from different sources. In addition to basic transcript information and gene structure, several statistics are determined for each entry in the database, such as secondary structure information, protein coding potential and microRNA binding sites. Our analyses suggest that, much like microRNAs, many lncRNAs have a significant secondary structure, in-line with their presumed association with proteins or protein complexes. Available literature on specific lncRNAs is linked, and users or authors can submit articles through a web interface. Protein coding potential is assessed by two different prediction algorithms: Coding Potential Calculator and HMMER. In addition, a novel strategy has been integrated for detecting potentially coding lncRNAs by automatically re-analysing the large body of publicly available mass spectrometry data in the PRIDE database. LNCipedia is publicly available and allows users to query and download lncRNA sequences and structures based on different search criteria. The database may serve as a resource to initiate small- and large-scale lncRNA studies. As an example, the LNCipedia content was used to develop a custom microarray for expression profiling of all available lncRNAs.	access network;algorithm;annotation;binding sites;bioinformatics;download;gene expression profiling;hmmer;high-throughput computing;long intergenic non-protein coding rna;maintain healthcare records;map;micrornas;microarray;molecular profiling;nar 2;nomenclature;nucleotides;occupational health services;question (inquiry);rna, long untranslated;rna, untranslated;sensor;throughput;transcript;upload;user interface;web search engine;algorithm;non-t, non-b childhood acute lymphoblastic leukemia	Pieter-Jan Volders;Kenny Helsens;Xiaowei Wang;Björn Menten;Lennart Martens;Kris Gevaert;Jo Vandesompele;Pieter Mestdagh	2013		10.1093/nar/gks915	biology;molecular biology;mass spectrometry;bioinformatics;gene;genetics	Comp.	-0.6850098996515714	-58.867521444193486	195607
49a1000fba8d089d28eb5339620e5564023cedf9	arbor 3d: an interactive environment for examining phylogenetic and taxonomic trees in multiple dimensions	arbre phylogenetique;computer program;representation graphique;arbor 3d;visualizacion;phylogeny;phylogenese;representacion grafica;real time;espacio 3 dimensiones;espacio 2 dimensiones;hierarchical data;arbol filogenetico;three dimensional;systeme conversationnel;visualization;3d environment;visualisation;phylogenetic tree;interactive system;two dimensional space;espace 3 dimensions;filogenesis;three dimensional space;sistema conversacional;interactive environment;espace 2 dimensions;source code;biological data;virtual environment;3d input device;programa computador;tree of life;graphics;programme ordinateur;industrial design;virtual worlds	"""UNLABELLED This paper examines a new technique for the visualization of and the interaction with trees, objects frequently used to convey hierarchical relationships in biological data. Motivated by the quality of 2D tree interaction, we adapt the planar tree-of-life metaphor to a virtual, semi-immersive 3D environment. A 3D environment extends the utility of this metaphor by allowing the user to view an entire data set in a single screen. Interrogation of the tree is implemented using 3D input devices. This real-time interrogation of the tree itself provides a quick means by which to qualitatively analyse the hierarchical data. In this paper, we describe the techniques underlying the implementation of such an environment. We conclude by considering the utility of tree metaphors as a basis for the representation of highly dimensional data sets.   AVAILABILITY Arbor3D (source code, a binary executable for SGI IRIX 6.4, Perl parsers, and sample Newick data files) are available via the Internet (http://xian.tamu.edu/Arbor3D/). Arbor3D can be displayed in """"CAVE simulator"""" mode on an SGI workstation screen, or as an interactive virtual environment on a projection workbench.   CONTACT druths@rice.edu; echen@cs.rice.edu; leland@xian.tamu.edu"""	acclimatization;computation;data structure;dimensions;executable;graphical user interface;hierarchical database model;ibm notes;irix;imagery;input device;linear programming relaxation;manuscripts;newick format;numerous;nut hypersensitivity;parsing;perl;phylogenetic tree;phylogenetics;physical object;pierre robin syndrome;prototype;real-time clock;sgi onyx2;semiconductor industry;sequence alignment;simulators;source code;tree structure;trees (plant);virtual reality;window function;workbench;workstation;benefit	Derek A. Ruths;Edward S. Chen;Leland Ellis	2000	Bioinformatics	10.1093/bioinformatics/16.11.1003	three-dimensional space;simulation;industrial design;visualization;computer science;bioinformatics;theoretical computer science;mathematics;computer graphics (images)	Visualization	-4.207696982332996	-57.190780322881906	196432
4b28ac39da5af16f8f72582223783aae0333ec42	pbsim: pacbio reads simulator - toward accurate genome assembly	low error rate;pacbio sequencers;coverage depth;extensive assembly result;pacbio library;available simulator;hybrid error correction;pacbio datasets;assembly test;high error rate	MOTIVATION PacBio sequencers produce two types of characteristic reads (continuous long reads: long and high error rate and circular consensus sequencing: short and low error rate), both of which could be useful for de novo assembly of genomes. Currently, there is no available simulator that targets the specific generation of PacBio libraries.   RESULTS Our analysis of 13 PacBio datasets showed characteristic features of PacBio reads (e.g. the read length of PacBio reads follows a log-normal distribution). We have developed a read simulator, PBSIM, that captures these features using either a model-based or sampling-based method. Using PBSIM, we conducted several hybrid error correction and assembly tests for PacBio reads, suggesting that a continuous long reads coverage depth of at least 15 in combination with a circular consensus sequencing coverage depth of at least 30 achieved extensive assembly results.   AVAILABILITY PBSIM is freely available from the web under the GNU GPL v2 license (http://code.google.com/p/pbsim/).	biopolymer sequencing;de novo transcriptome assembly;error detection and correction;gnu;genome assembly sequence;libraries;reading (activity);sampling (signal processing);simulation;simulators;single molecule real time sequencing	Yukiteru Ono;Kiyoshi Asai;Michiaki Hamada	2013	Bioinformatics	10.1093/bioinformatics/bts649	computer science;bioinformatics;data mining;world wide web;hybrid genome assembly	Comp.	-0.3293727110491064	-55.32695431815171	196665
1529ca3a37f285f51fcd4846b046a71e99cbd533	arrayqcplot: software for checking the quality of microarray data	microarray data;exploratory analysis;snu	UNLABELLED arrayQCplot is a software for the exploratory analysis of microarray data. This software focuses on quality control and generates newly developed plots for quality and reproducibility checks. It is developed using R and provides a user-friendly graphical interface for graphics and statistical analysis. Therefore, novice users will find arrayQCplot as an easy-to-use software for checking the quality of their data by a simple mouse click.   AVAILABILITY arrayQCplot software is available from Bioconductor at http://www.bioconductor.org. A more detailed manual is available at http://bibs.snu.ac.kr/software/arrayQCplot   CONTACT tspark@stats.snu.ac.kr.	bioconductor;checking (action);event (computing);graphical user interface;graphics;interface device component;microarray;r language;usability	Eun-Kyung Lee;Sung-Gon Yi;Taesung Park	2006	Bioinformatics	10.1093/bioinformatics/btl367	biology;microarray analysis techniques;gene chip analysis;computer science;bioinformatics;data mining;database;world wide web	SE	-2.8276363898154258	-57.37659660457788	196804
16a71ae017fef3d109b78d95cb0f08b83e5f6e8c	compression of next-generation sequencing reads aided by highly efficient de novo assembly	software;probability;data compression;high throughput nucleotide sequencing;massively parallel genome sequencing;algorithms;compression	UNLABELLED We present Quip, a lossless compression algorithm for next-generation sequencing data in the FASTQ and SAM/BAM formats. In addition to implementing reference-based compression, we have developed, to our knowledge, the first assembly-based compressor, using a novel de novo assembly algorithm. A probabilistic data structure is used to dramatically reduce the memory required by traditional de Bruijn graph assemblers, allowing millions of reads to be assembled very efficiently. Read sequences are then stored as positions within the assembled contigs. This is combined with statistical compression of read identifiers, quality scores, alignment information and sequences, effectively collapsing very large data sets to <15% of their original size with no loss of information.   AVAILABILITY Quip is freely available under the 3-clause BSD license from http://cs.washington.edu/homes/dcjones/quip.	alignment;bsd;bsd/os;biopolymer sequencing;data structure;de bruijn graph;de novo transcriptome assembly;fastq format;graph - visual representation;identifier;lossless compression;massively-parallel sequencing;reading (activity);smc3 gene;algorithm;doxorubicin/semustine/streptozocin protocol	Daniel C. Jones;Walter L. Ruzzo;Xinxia Peng;Michael G. Katze	2012		10.1093/nar/gks754	data compression;bioinformatics;probability;compression	Comp.	-1.061161627173029	-54.21483664869593	196812
ace57ab6080e9b7538c7d519d848ca4cbd32c546	fast and snp-tolerant detection of complex variants and splicing in short reads	splicing;genomics;rna splicing;deteccion;empalme;polimorfismo mononucleotido;detection;genetic variation;polymorphisme mononucleotide;epissage;dna recombinant;base sequence;polymorphism single nucleotide;single nucleotide polymorphism	MOTIVATION Next-generation sequencing captures sequence differences in reads relative to a reference genome or transcriptome, including splicing events and complex variants involving multiple mismatches and long indels. We present computational methods for fast detection of complex variants and splicing in short reads, based on a successively constrained search process of merging and filtering position lists from a genomic index. Our methods are implemented in GSNAP (Genomic Short-read Nucleotide Alignment Program), which can align both single- and paired-end reads as short as 14 nt and of arbitrarily long length. It can detect short- and long-distance splicing, including interchromosomal splicing, in individual reads, using probabilistic models or a database of known splice sites. Our program also permits SNP-tolerant alignment to a reference space of all possible combinations of major and minor alleles, and can align reads from bisulfite-treated DNA for the study of methylation state.   RESULTS In comparison testing, GSNAP has speeds comparable to existing programs, especially in reads of > or=70 nt and is fastest in detecting complex variants with four or more mismatches or insertions of 1-9 nt and deletions of 1-30 nt. Although SNP tolerance does not increase alignment yield substantially, it affects alignment results in 7-8% of transcriptional reads, typically by revealing alternate genomic mappings for a read. Simulations of bisulfite-converted DNA show a decrease in identifying genomic positions uniquely in 6% of 36 nt reads and 3% of 70 nt reads.   AVAILABILITY Source code in C and utility programs in Perl are freely available for download as part of the GMAP package at http://share.gene.com/gmap.	align (company);alignment;biopolymer sequencing;clinical act of insertion;computer simulation;download;fastest;gal gene;license;massively-parallel sequencing;nitroprusside;perl;rna splicing;reading (activity);sensor;source code;splice (system call);transcription, genetic;transcriptome;hydrogen sulfite	Thomas D. Wu;Serban Nacu	2010		10.1093/bioinformatics/btq057	biology;genomics;molecular biology;bioinformatics;genetics;rna splicing	Comp.	-0.3832073473704839	-55.41112744160418	196849
de27baadbb74fb0f48faa8e562a9503867421647	cogi: towards compressing genomes as an image	genomics;image coding;sequence matrixization;entropy coding;rectangular partition coding;reference based compression;reviews biological techniques dna genomics molecular biophysics probability;genomics image coding bioinformatics computational biology encoding partitioning algorithms entropy;state of the art reference free genome compressor cogi sequencing technology genomic data storage genomic data transferring data distribution compressing genomic sequences probability models one dimensional text strings two dimensional binary imaging rectangular partition coding algorithm entropy based algorithms performance evaluation state of the art reference based genome compressors green rlz opt high compression efficiency;entropy;computational biology;encoding;genomes compression;partitioning algorithms;bioinformatics	Genomic science is now facing an explosive increase of data thanks to the fast development of sequencing technology. This situation poses serious challenges to genomic data storage and transferring. It is desirable to compress data to reduce storage and transferring cost, and thus to boost data distribution and utilization efficiency. Up to now, a number of algorithms / tools have been developed for compressing genomic sequences. Unlike the existing algorithms, most of which treat genomes as one-dimensional text strings and compress them based on dictionaries or probability models, this paper proposes a novel approach called CoGI (the abbreviation of Co mpressing Genomes as an I mage) for genome compression, which transforms the genomic sequences to a two-dimensional binary image (or bitmap), then applies a rectangular partition coding algorithm to compress the binary image. CoGI can be used as either a reference-based compressor or a reference-free compressor. For the former, we develop two entropy-based algorithms to select a proper reference genome. Performance evaluation is conducted on various genomes. Experimental results show that the reference-based CoGI significantly outperforms two state-of-the-art reference-based genome compressors GReEn and RLZ-opt in both compression ratio and compression efficiency. It also achieves comparable compression ratio but two orders of magnitude higher compression efficiency in comparison with XM—one state-of-the-art reference-free genome compressor. Furthermore, our approach performs much better than Gzip—a general-purpose and widely-used compressor, in both compression speed and compression ratio. So, CoGI can serve as an effective and practical genome compressor. The source code and other related documents of CoGI are available at: http://admis.fudan.edu.cn/projects/cogi.htm.	abbreviations;binary image;biopolymer sequencing;bitmap;compresses (device);compression;computer data storage;dictionary [publication type];general-purpose modeling;genome;google chrome;netbsd gzip / freebsd gzip;performance evaluation;source code;algorithm;orders - hl7publishingdomain	Xiaojing Xie;Shuigeng Zhou;Jihong Guan	2015	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2430331	data compression;biology;entropy;genomics;computer science;bioinformatics;entropy encoding;theoretical computer science;data mining;lossless compression;context-adaptive binary arithmetic coding;encoding	Comp.	-3.106356342079876	-52.484923446280035	198093
05579e3f675d75cb233888641d255ec2babf1a57	identifying structural variants using linked-read sequencing data		Motivation Structural variation, including large deletions, duplications, inversions, translocations, and other rearrangements, is common in human and cancer genomes. A number of methods have been developed to identify structural variants from Illumina short-read sequencing data. However, reliable identification of structural variants remains challenging because many variants have breakpoints in repetitive regions of the genome and thus are difficult to identify with short reads. The recently developed linked-read sequencing technology from 10X Genomics combines a novel barcoding strategy with Illumina sequencing. This technology labels all reads that originate from a small number (~5-10) DNA molecules ~50Kbp in length with the same molecular barcode. These barcoded reads contain long-range sequence information that is advantageous for identification of structural variants.   Results We present Novel Adjacency Identification with Barcoded Reads (NAIBR), an algorithm to identify structural variants in linked-read sequencing data. NAIBR predicts novel adjacencies in a individual genome resulting from structural variants using a probabilistic model that combines multiple signals in barcoded reads. We show that NAIBR outperforms several existing methods for structural variant identification - including two recent methods that also analyze linked-reads - on simulated sequencing data and 10X whole-genome sequencing data from the NA12878 human genome and the HCC1954 breast cancer cell line. Several of the novel somatic structural variants identified in HCC1954 overlap known cancer genes.   Availability Software is available at compbio.cs.brown.edu/software.   Contact braphael@princeton.edu.   Supplementary information Supplementary data are available at Bioinformatics online.	barcode;bioinformatics;biopolymer sequencing;breakpoint;chromosomal translocation;chromosome inversion;dna sequence rearrangement;diploid cell;geographic information systems;inversion (discrete mathematics);mammary neoplasms;reading (activity);repetitive region;statistical model;whole genome sequencing;algorithm;cancer cell	Rebecca Elyanow;Hsin-Ta Wu;Benjamin J. Raphael	2017	Bioinformatics	10.1093/bioinformatics/btx712	personal genomics;exome sequencing;genomics;structural variation;bioinformatics;cancer genome sequencing;hybrid genome assembly;dna sequencing theory;deep sequencing;biology	Comp.	1.5694548069370284	-54.882657888461914	198108
6a135a1b05453d29bb330b7163ab692aa43c8336	tripnet: a method for constructing phylogenetic networks from triplets	phylogenetic network;multi locus sequence typing;new zealand;quantitative method	We present TripNet, a method for constructing phylogenetic networks from triplets. We will present the motivations behind our approach and its theoretical and empirical justi fica on. To demonstrate the accuracy and efficiency of TripN et, we performed two simulations and also applied the method to fi ve published data sets: Kreitman’s data, a set of triplets from real yeast data obtained from the Fungal Biodiversity C enter in Utrecht, a collection of 110 highly recombinant Salmonella multi-locus sequence typing sequences, and nrD NA ITS and cpDNA JSA sequence data of New Zealand alpine buttercups of Ranunculus sect. Pseudadonis. Finall y, we compare our results with those already obtained by other authors using alternative methods. TripNet, data set s, and supplementary files are freely available for download at (www.bioinf.cs.ipm.ir/softwares/tripnet).	alpine;download;empirical risk minimization;locus;phylogenetic network;phylogenetics;recombinant dna;simulation	Ruzbeh Tusserkani;Changiz Eslahchi;Hadi Poormohammadi;Azin Azadi	2011	CoRR		quantitative research;bioinformatics;phylogenetic network	Comp.	1.7581558491265685	-53.6816458001085	198197
24fd3520fedd8824d74f80db26b71ec3de302b8e	constax: a tool for improved taxonomic resolution of environmental fungal its sequences	its;rdp;sintax;unoise;uparse;fungal microbiome;mycobiome;taxonomy classifiers	One of the most crucial steps in high-throughput sequence-based microbiome studies is the taxonomic assignment of sequences belonging to operational taxonomic units (OTUs). Without taxonomic classification, functional and biological information of microbial communities cannot be inferred or interpreted. The internal transcribed spacer (ITS) region of the ribosomal DNA is the conventional marker region for fungal community studies. While bioinformatics pipelines that cluster reads into OTUs have received much attention in the literature, less attention has been given to the taxonomic classification of these sequences, upon which biological inference is dependent. Here we compare how three common fungal OTU taxonomic assignment tools (RDP Classifier, UTAX, and SINTAX) handle ITS fungal sequence data. The classification power, defined as the proportion of assigned OTUs at a given taxonomic rank, varied among the classifiers. Classifiers were generally consistent (assignment of the same taxonomy to a given OTU) across datasets and ranks; a small number of OTUs were assigned unique classifications across programs. We developed CONSTAX (CONSensus TAXonomy), a Python tool that compares taxonomic classifications of the three programs and merges them into an improved consensus taxonomy. This tool also produces summary classification outputs that are useful for downstream analyses. Our results demonstrate that independent taxonomy assignment tools classify unique members of the fungal community, and greater classification power is realized by generating consensus taxonomy of available classifiers with CONSTAX.	bioinformatics;classification;community;dna barcoding, taxonomic;downstream (software development);high-throughput computing;inference;internal transcribed spacer;microbiome;personnameuse - assigned;pipeline (computing);python;remote desktop protocol;spacer device component;taxonomy;throughput;unit	Kristi Gdanetz;Gian Maria Niccolò Benucci;Natalie Vande Pol;Gregory M. Bonito	2017		10.1186/s12859-017-1952-x	dna microarray;genetics;bioinformatics;microbiome;biology;ribosomal dna;biological classification;mycobiome;internal transcribed spacer	Comp.	1.4992576645590134	-56.393627266072755	198306
1229589945b98e320c9832075422419fb59a19f9	a method to improve structural modeling based on conserved domain clusters	biology computing;solid modelling biology computing proteins pattern clustering;pattern clustering;structural model;protein sequence;protein domains;three dimensional;structure anchored alignment protocol structural modeling conserved domain cluster homology modeling query sequence protein chain protein data bank domain based template library multiple structural alignment;proteins;proteins libraries predictive models genomics bioinformatics crystallography nuclear magnetic resonance spectroscopy computers computational modeling;cluster system;homology modeling;sequence alignment;protein data bank;solid modelling;structure alignment	Homology modeling requires an accurate alignment between a query sequence and its homologs with known three-dimensional (3D) information. Current structural modeling techniques largely use entire protein chains as templates, which are selected based only on their sequence alignments with the queries. Protein can be largely described as combinations of conserved domains, and already more than two-third of the known protein domains can be found in the protein data bank (PDB). We presented a method to improve structural modeling based on conserved domain clusters. First, we searched and mapped all the inter pro domains in the entire PDB, partitioned and clustered homologous domains into the domain-based template library. For each of the resulting clusters created, a multiple structural alignment was generated based only on the 3D coordinates of all the residues involved. Then we used the structural alignments as anchors to increase the alignment accuracy between a query and its templates, and consequently improve the quality of predicted structure for query protein. We implemented the method on DAWNING 4000A cluster system. The preliminary results show that our domain-based template library and the structure-anchored alignment protocol can be used for the partial prediction for a majority of known protein sequences with better qualities.	academy;class diagram;dawning information industry;html element;homology (biology);homology modeling;interpro;peptide sequence;protein data bank;sequence alignment;spatial variability;station hypo	Fa Zhang;Lin Xu;Bo Yuan	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639540	three-dimensional space;structural alignment;homology modeling;protein data bank;multiple sequence alignment;computer science;bioinformatics;protein sequencing;loop modeling;sequence alignment;data mining;conserved domain database;protein domain	Comp.	1.3327437289937638	-58.451277232134736	198985
67e0db6f5e2055885c2ab6bb0b7d8be762b89cc1	information theory of dna shotgun sequencing	dna;information theory dna sequencing de novo assembly;noise impact dna shotgun sequencing information theory statistics renyi entropy rate;dna sequential analysis greedy algorithms assembly genomics bioinformatics algorithm design and analysis;statistics biological techniques dna entropy information theory molecular biophysics;molecular biophysics;statistics;entropy;biological techniques;information theory	DNA sequencing is the basic workhorse of modern day biology and medicine. Shotgun sequencing is the dominant technique used: many randomly located short fragments called reads are extracted from the DNA sequence, and these reads are assembled to reconstruct the original sequence. A basic question is: given a sequencing technology and the statistics of the DNA sequence, what is the minimum number of reads required for reliable reconstruction? This number provides a fundamental limit to the performance of any assembly algorithm. For a simple statistical model of the DNA sequence and the read process, we show that the answer admits a critical phenomenon in the asymptotic limit of long DNA sequences: if the read length is below a threshold, reconstruction is impossible no matter how many reads are observed, and if the read length is above the threshold, having enough reads to cover the DNA sequence is sufficient to reconstruct. The threshold is computed in terms of the Renyi entropy rate of the DNA sequence. We also study the impact of noise in the read process on the performance.	algorithm;asymptote;entropy rate;image noise;information theory;randomness;rényi entropy;statistical model	Abolfazl Seyed Motahari;Guy Bresler;David Tse	2013	IEEE Transactions on Information Theory	10.1109/TIT.2013.2270273	entropy;information theory;bioinformatics;k-mer;dna sequencing theory;mathematics;dna;hybrid genome assembly;statistics;molecular biophysics	Comp.	0.3563336614866564	-53.05635409684099	199007
42753f33cfc68b1eca798a0e93cab26c1779006e	path2ppi: an r package to predict protein–protein interaction networks for a set of proteins		UNLABELLED : We introduce Path2PPI, a new R package to identify protein-protein interaction (PPI) networks for fully sequenced organisms for which nearly none PPI are known. Path2PPI predicts PPI networks based on sets of proteins from well-established model organisms, providing an intuitive visualization and usability. It can be used to combine and transfer information of a certain pathway or biological process from several reference organisms to one target organism.   AVAILABILITY AND IMPLEMENTATION Path2PPI is an open-source tool implemented in R. It can be obtained from the Bioconductor project: http://bioconductor.org/packages/Path2PPI/ CONTACT: : ina.koch@bioinformatik.uni-frankfurt.de   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	bioconductor;bioinformatics;biological processes;gene regulatory network;open-source software;pixel density;proton pump inhibitors;usability;protein protein interaction	Oliver Philipp;Heinz D. Osiewacz;Ina Koch	2016		10.1093/bioinformatics/btv765	bioinformatics;protein–protein interaction prediction	Comp.	-0.5415649040370312	-58.3524578153642	199316
50374e63bca8795e940792792e3d06650e0ef6db	missing genes in the annotation of prokaryotic genomes	genomics;genome annotation;databases genetic;journal article;computational biology bioinformatics;prokaryotic cells;genome bacterial;high performance computer;gene family;algorithms;combinatorial libraries;genes bacterial;computer appl in life sciences;lower bound;candidate gene;microarrays;bioinformatics;open reading frames	Protein-coding gene detection in prokaryotic genomes is considered a much simpler problem than in intron-containing eukaryotic genomes. However there have been reports that prokaryotic gene finder programs have problems with small genes (either over-predicting or under-predicting). Therefore the question arises as to whether current genome annotations have systematically missing, small genes. We have developed a high-performance computing methodology to investigate this problem. In this methodology we compare all ORFs larger than or equal to 33 aa from all fully-sequenced prokaryotic replicons. Based on that comparison, and using conservative criteria requiring a minimum taxonomic diversity between conserved ORFs in different genomes, we have discovered 1,153 candidate genes that are missing from current genome annotations. These missing genes are similar only to each other and do not have any strong similarity to gene sequences in public databases, with the implication that these ORFs belong to missing gene families. We also uncovered 38,895 intergenic ORFs, readily identified as putative genes by similarity to currently annotated genes (we call these absent annotations). The vast majority of the missing genes found are small (less than 100 aa). A comparison of select examples with GeneMark, EasyGene and Glimmer predictions yields evidence that some of these genes are escaping detection by these programs. Prokaryotic gene finders and prokaryotic genome annotations require improvement for accurate prediction of small genes. The number of missing gene families found is likely a lower bound on the actual number, due to the conservative criteria used to determine whether an ORF corresponds to a real gene.	annotation;candidate disease gene;computation (action);computing methodologies;database;databases;glimmer;gene family;gene prediction;genemark;genome;introns;large;open reading frames;open reading frame;replicon;supercomputer	Andrew S. Warren;Jeremy S. Archuleta;Wu-chun Feng;João Carlos Setubal	2009		10.1186/1471-2105-11-131	open reading frame;biology;genomics;dna microarray;bioinformatics;gene family;upper and lower bounds;genome project;candidate gene;genetics	Comp.	1.7320804481303764	-56.30743455349576	199826
b8107002c6b92865055c2b34be79fa8002bc5d23	a collection of programs for nucleic acid and protein analysis, written in fortran 77 for ibm-pc compatible microcomputers	computers;software;programa;investigation method;program;proteine;methode etude;computerized processing;tratamiento informatico;amino acid sequence;nucleic acids;microordinateur;microcomputer;primary structure;estructura primaria;proteins;microcomputadora;metodo estudio;tecnica;programme;proteina;fortran;acido nucleico;base sequence;acide nucleique;nucleic acid;traitement informatique;technique;microcomputers;structure primaire	We have developed a collection of programs for manipulation and analysis of nucleotide and protein sequences. The package was written in Fortran 77 on a Sirius1/Victor microcomputer which can be easily implemented on a large variety of other computers. Some of the programs have already been adapted for use on a Vax 11. Our aim was to develop programs consisting of small, comprehensible and well documented units that have very fast execution times and are comfortably interactive. The package is therefore suitable for individual modifications, even with little understanding of computer languages.	amino acid sequence;cell nucleus;computer language;computers;document completion status - documented;fortran;ibm pc compatible;ibm personal computer;inclusion body myositis (disorder);microcomputer;nucleic acids;nucleotides;peptide sequence;programming languages;protein analysis;vax	B. Franz Lang;Gertraud Burger	1986	Nucleic acids research	10.1093/nar/14.1.455	biology;nucleic acid;bioinformatics;microcomputer;genetics	HPC	-4.483501819419297	-56.349824325594234	199877
