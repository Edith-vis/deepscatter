id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
dc712d78ac514bcce967580bbf088ee9af6f27a1	exploiting configuration dependencies for rapid area-efficient customization of soft-core processors	fpga;design space pruning;performance area trade off;area constraints;csoc;application specific customization;soft core processors	The large number of possible configurations in modern soft-core processors make it tedious and time consuming to select the optimal configuration for a given application. In this paper, we propose a framework for rapid area-efficient customization of soft-core processors that exploits the dependencies between the various configuration options to prune the design space. Additionally, the proposed technique relies on rapid and accurate estimation models instead of the time consuming synthesis and execution techniques proposed in the existing work. Experimental results based on hand-coded applications and applications from the popular CHStone benchmark suite show that the proposed framework can rapidly and reliably select the best processor configuration for a given application and save an average of 47.58% area over the processor with all the configuration options enabled while achieving similar performance.	benchmark (computing);central processing unit	Deshya Wijesundera;Alok Prakash;Siew Kei Lam;Thambipillai Srikanthan	2016		10.1145/2906363.2906385	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	1.1975479748948337	51.582887987285325	163482
c889abf559040a6c7bfb92a2baf16f52ef0f2de7	a queuing model for cpu functional unit and issue queue configuration		In a superscalar processor, instructions of various types flow through an execution pipeline, traversing hardware resources which are mostly shared among many different instruction types. A notable exception to shared pipeline resources is the collection of functional units, the hardware that performs specific computations. In a trade-off of cost versus performance, a pipeline designer must decide how many of each type of functional unit to place in a processor’s pipeline. In this paper, we model a superscalar processor’s issue queue and functional units as a novel queuing network. We treat the issue queue as a finite-sized waiting area and the functional units as servers. In addition to common queuing problems, customers of the network share the queue but wait for specific servers to become ready (e.g., addition instructions wait for adders). Furthermore, the customers in this queue are not necessary ready for service, since instructions may be waiting for operands. In this paper we model a novel queuing network that provides a solution to the expected queue length of each type of instruction. This network and its solution can also be generalized to other problems, notably other resource-allocation issues that arise in superscalar pipelines.	central processing unit;computation;execution unit;mathematical optimization;operand;optimization problem;pipeline (computing);queueing theory;register renaming;server (computing);superscalar processor	S. Carroll;W. Lin	2018	Simulation Modelling Practice and Theory	10.1016/j.simpat.2018.07.004	real-time computing;operand;computer science;traverse;queueing theory;computation;computer network;queue;pipeline transport;adder;server	Arch	-1.3333095172723872	52.913110154230544	164703
e2d5ec5eb624fe35bfd92a8c41b4c42f55017f35	developing embedded kernel for system-on-a-chip platform of heterogeneous multiprocessor architecture	software metrics;embedded system product;system on a chip platform;operating system software;kernel system on a chip computer architecture embedded system embedded software operating systems costs software performance software systems software development management;hardware software codesign;software complexity;gpp;tms320dsc25 texas instrument;special purpose processor;microkernel architecture;system on a chip;embedded system;chip;embedded software development;heterogeneous multiprocessor architecture;general purpose processor;computer architecture;embedded systems;message passing interface;operating system;system on chip;embedded kernel development;multiprocessor architecture;application program interfaces;c5409 dsp core embedded kernel development system on a chip platform heterogeneous multiprocessor architecture embedded system product embedded software development operating system software general purpose processor special purpose processor microkernel architecture message passing interface software complexity tms320dsc25 texas instrument heterogeneous multiprocessor soc gpp arm7tdmi core spp;heterogeneous multiprocessor soc;message passing;system on chip application program interfaces computer architecture embedded systems hardware software codesign message passing microprocessor chips multiprocessing systems operating system kernels software metrics;multiprocessing systems;arm7tdmi core;operating system kernels;spp;high performance;embedded software;microprocessor chips;c5409 dsp core	It has been common that modern embedded system products are built on platforms with system-on-a-chip (SOC) in which two or more different processor cores are put into one single chip and form the architecture of heterogeneous multiprocessor. Although providing high performance at low cost, such architecture brings new design challenges as well as increased complexity in developing embedded software especially at the level of kernel or operating system software. This paper presents our experience in developing an embedded microkernel that runs on embedded system of heterogeneous multiprocessor architecture composed of one general purpose processor (GPP) and one special purpose processor (SPP). Aside from following the traditional approach of monolithic operating system, the option of dual kernels based on microkernel architecture with uniform message-passing mechanism was taken to develop a symmetric embedded microkernel which can be compiled to run separately on each of the different processor cores in the system. The design and the approach not only reduce the software complexity in developing a kernel to manage different processors in a system but also enable a symmetric view from processors of different architectures. A prototype kernel was implemented on a reference design of Texas Instrument's TMS320DSC25 which is a heterogeneous multiprocessor SOC with a GPP of ARM7TDMI core and an SPP of C5409 DSP core	arm7;blocking (computing);central processing unit;compiler;embedded operating system;embedded software;embedded system;graph partition;kernel (operating system);message passing interface;message passing;microkernel;monolithic kernel;multiprocessing;non-blocking algorithm;operating system;programming complexity;prototype;reference design;scheduling (computing);self-propelled particles;symmetric multiprocessing;system on a chip	Jing Chen;Jian-Horng Liu	2006	12th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA'06)	10.1109/RTCSA.2006.27	system on a chip;embedded system;hybrid kernel;computer architecture;parallel computing;real-time computing;computer science;operating system;symmetric multiprocessor system	Embedded	-1.5221205781359193	48.75509376720508	164818
620eac63b314969fa88a0e3be1d3d682e5266f3d	udp: a programmable accelerator for extract-transform-load workloads and more		Big data analytic applications give rise to large-scale extract-transformload (ETL) as a fundamental step to transform new data into a native representation. ETL workloads pose significant performance challenges on conventional architectures, so we propose the design of the unstructured data processor (UDP), a software programmable accelerator that includes multi-way dispatch, variable-size symbol support, flexible-source dispatch (stream buffer and scalar registers), and memory addressing to accelerate ETL kernels both for current and novel future encoding and compression. Specifically, UDP excels at branch-intensive and symbol and pattern-oriented workloads, and can offload them from CPUs. To evaluate UDP, we use a broad set of data processing workloads inspired by ETL, but broad enough to also apply to query execution, stream processing, and intrusion detection/monitoring. A single UDP accelerates these data processing tasks 20-fold (geometric mean, largest increase from 0.4 GB/s to 40 GB/s) and performance per watt by a geomean of 1,900-fold. UDP ASIC implementation in 28nm CMOS shows UDP logic area of 3.82mm 2 (8.69mm 2 with 1MB local memory), and logic power of 0.149W (0.864W with 1MB local memory); both much smaller than a single core. CCS CONCEPTS • Information systems → Extraction, transformation and loading; • Computer systems organization → Parallel architectures; • Hardware → Application specific processors; • Theory of computation → Pattern matching;	ampersand;application-specific integrated circuit;big data;cmos;central processing unit;dynamic dispatch;gigabyte;intrusion detection system;memory address;performance per watt;stream processing	Yuanwei Fang;Chen Zou;Aaron J. Elmore;Andrew A. Chien	2017		10.1145/3123939.3123983	application-specific integrated circuit;computer science;stream processing;single-core;parallel computing;real-time computing;memory address;extract, transform, load;encoding (memory);performance per watt;intrusion detection system	Arch	-3.6777738317622686	47.857384582659805	164934
d34ea3bd170d48e8ed1aea580045bab814077cc7	design space exploration of deeply nested loop 2d filtering and 6 level fsbm algorithm mapped onto systolic array	heuristic search;systolic array;design space exploration;computational effort;computational task;computationally intensive nested loop;n-d problem space;pe array;computational expression;n-d problem;level fsbm algorithm;parallel array architecture;design space trade-off analysis	The high integration density in today’s VLSI chips offers enormous computing power to be utilized by the design of parallel computing hardware. The implementation of computationally intensive algorithms represented by n-dimensional (n-D) nested loop algorithms, onto parallel array architecture is termed as mapping. The methodologies adopted for mapping these algorithms onto parallel hardware often use heuristic search that requires a lot of computational effort to obtain near optimal solutions. We propose a new mapping procedure wherein a lower dimensional subspace (of the n-D problem space) of inner loop is identified, in which lies the computational expression that generates the output or outputs of the n-D problem. The processing elements (PE array) are assigned to the identified sub-space and the reuse of the PE array is through the assignment of the PE array to the successive sub-spaces in consecutive clock cycles/periods (CPs) to complete the computational tasks of the n-D problem. The above is used to develop our proposed modified heuristic search to arrive at optimal design and the complexity comparisons are given. The MATLAB results of the new search and the design space trade-off analysis using the high-level synthesis tool are presented for two typical computationally intensive nested loop algorithms—the 6D FSBM and the 4D edge detection alternatively known as the 2D filtering algorithm.	algorithm;clock signal;computation;computer hardware;control flow;design space exploration;discontinuous galerkin method;edge detection;field-programmable gate array;heuristic;heuristic (computer science);high- and low-level;high-level synthesis;inner loop;k-edge-connected graph;light field;matlab;optimal design;parallel array;parallel computing;problem domain;resultant;scheduling (computing);simulation;static timing analysis;systolic array;transformation matrix;very-large-scale integration	B. Bala Tripura Sundari	2012	VLSI Design	10.1155/2012/268402	embedded system;electronic engineering;real-time computing;computer science;theoretical computer science;operating system;distributed computing;algorithm	EDA	0.5029296609152075	51.36128185439616	165186
6ccb970de68fc2006bb2302dba8793a5016ac5d4	from soda to scotch: the evolution of a wireless baseband processor	processing element;software;software radio 3g mobile communication microprocessor chips protocols;protocols;application specific hardware accelerators soda scotch wireless baseband processor wireless standards hardware only baseband processing wireless communication software defined radio 3g wireless protocols programmable multicore architecture arm control processor single instruction multiple data processing ardbeg architectural evolution long instruction word support;software defined radio;research design;real time;hardware accelerator;software radio;single instruction multiple data;wireless communication;computer architecture;3g mobile communication;cost effectiveness;baseband wireless application protocol hardware prototypes communication standards wireless communication software radio costs multicore processing computer architecture;microprocessor chips;multiaccess communication;hardware	With the multitude of existing and upcoming wireless standards, it is becoming increasingly difficult for hardware-only baseband processing solutions to adapt to the rapidly changing wireless communication landscape. Software Defined Radio (SDR) promises to deliver a cost effective and flexible solution by implementing a wide variety of wireless protocols in software. In previous work, a fully programmable multicore architecture, SODA, was proposed that was able to meet the real-time requirements of 3G wireless protocols. SODA consists of one ARM control processor and four wide single instruction multiple data (SIMD) processing elements. Each processing element consists of a scalar and a wide 512-bit 32-lane SIMD datapath. A commercial prototype based on the SODA architecture, Ardbeg (named after a brand of Scotch Whisky), has been developed. In this paper, we present the architectural evolution of going from a research design to a commercial prototype, including the goals, tradeoffs, and final design choices. Ardbeg’s redesign process can be grouped into the following three major areas: optimizing the wide SIMD datapath, providing long instruction word (LIW) support for SIMD operations, and adding application-specific hardware accelerators. Because SODA was originally designed with 180nm technology, the wide SIMD datapath is re-optimized in Ardbeg for 90nm technology. This includes re-evaluating the most efficient SIMD width, designing a wider SIMD shuffle network, and implementing faster SIMD arithmetic units. Ardbeg also provides modest LIW support by allowing two SIMD operations to issue in the same cycle. This LIW execution supports SDR algorithms’ most common parallel SIMD execution patterns with minimal hardware overhead. A viable commercial SDR solution must be competitive with existing ASIC solutions. Therefore, algorithm-specific hardware is added for performance bottleneck algorithms while still maintaining enough flexibility to support multiple wireless protocols. The combination of these architectural improvements allows Ardbeg to achieve 1.5–7x speedup over SODA across multiple wireless algorithms while consuming less power.	arm architecture;algorithm;application-specific integrated circuit;baseband processor;datapath;etsi satellite digital radio;execution pattern;hardware acceleration;multi-core processor;opcode;overhead (computing);prototype;real-time clock;requirement;simd;speedup;very long instruction word	Mark Woh;Yuan Lin;Sangwon Seo;Scott A. Mahlke;Trevor N. Mudge;Chaitali Chakrabarti;Richard Bruce;Danny Kershaw;Alastair David Reid;Mladen Wilder;Krisztián Flautner	2008	2008 41st IEEE/ACM International Symposium on Microarchitecture	10.1109/MICRO.2008.4771787	embedded system;parallel computing;real-time computing;computer science;operating system;software-defined radio	Arch	2.5265217628189136	48.15273507922954	165332
c1d803b4188f2aa846c776ec8a782cbc9fdd1e69	global optimization of compositional systems	global optimization;common subexpression elimination;image processing;static analysis;embedded system;design optimization;field programmable gate arrays;hardware;compiler optimization;constraint optimization;embedded computing;entire function;flow analysis	Embedded systems typically consist of a composition of a set of hardware and software IP modules. Each module is heavily optimized by itself. However, when these modules are composed together, significant additional opportunities for optimizations are introduced because only a subset of the entire functionality is actually used. We propose COSE-a technique to jointly optimize such designs. We use symbolic execution to compute invariants in each component of the design. We propagate these invariants as constraints to other modules using global flow analysis of the composition of the design. This captures optimizations that go beyond, and are qualitatively different than, those achievable by compiler optimization techniques such as common subexpression elimination, which are localized. We again employ static analysis techniques to perform optimizations subject to these constraints. We implemented COSE in the Metropolis platform and achieved significant optimizations using reasonable computational resources.	adobe flash lite;common subexpression elimination;computation;computational resource;data-flow analysis;embedded system;global optimization;inline expansion;java;mathematical optimization;metropolis;observable;optimization mechanism;optimizing compiler;prototype;recurrence relation;solver;static program analysis;symbolic execution	Fadi A. Zaraket;John Pape;Adnan Aziz;Margarida F. Jacome;Sarfraz Khurshid	2007	Formal Methods in Computer Aided Design (FMCAD'07)	10.1109/FAMCAD.2007.17	parallel computing;real-time computing;multidisciplinary design optimization;common subexpression elimination;image processing;computer science;theoretical computer science;data-flow analysis;entire function;optimizing compiler;programming language;static analysis;field-programmable gate array;global optimization	EDA	-1.214303270842079	51.172050684012405	165536
2fd286bc45cb1d5396e508e4d94d6fda90d92dd4	embedded computing: new directions in architecture and automation	eficacia sistema;processor scheduling;sistema informatico;performance systeme;computer system;system performance;computer architecture;architecture ordinateur;parallel architectures;architecture parallele;hierarchie memoire;systeme informatique;arquitectura ordenador;memory hierarchy;ordonnancement processeur;jerarquia memoria;embedded computing	embedded computing, special-purpose architectures, customization, custom architectures, off-theshelf customizable systems, FPGA, automation, architecture synthesis, hardwaresoftware co-design, processor-compiler codesign, frameworks, constructors, constructors, design space exploration, PICO, system synthesis, VLIW synthesis, nonprogrammable accelerator synthesis, cache hierarchy synthesis With the advent of system level integration (SLI) and system-on-chip (SOC), the center of gravity of the computer industry is moving from personal computing into embedded computing. The resulting upheaval is only just beginning to be widely appreciated. The opportunities, needs and constraints of this next generation of computing are somewhat different from those to which we have got accustomed in general-purpose computing. In turn, we believe that this will lead to significantly different computer architectures, at both the system and the processor levels, and a rich diversity of off-the-shelf and custom designs. Furthermore, we predict that embedded computing will introduce a new theme into computer architecture: automation of computer architecture. In this report, we elaborate on these claims and provide, as an example, an overview of PICO, the architecture synthesis system that the authors and their colleagues have been developing over the past five years.	automation;compiler;computer architecture;constructor (object-oriented programming);design space exploration;embedded system;field-programmable gate array;general-purpose markup language;general-purpose modeling;next-generation network;personal computer;pico;renaissance;requirement;smart products;software framework;system on a chip;very-large-scale integration	B. Ramakrishna Rau;Mike Schlansker	2000		10.1007/3-540-44467-X_21	embedded system;parallel computing;real-time computing;computer science;cellular architecture;operating system;end-user computing;computer performance;utility computing	EDA	0.735715266743001	48.80762629269775	165788
36e5fb3896a5b94d6e91aadbbbc6674686131c81	writing portable applications that dynamically bind at run time to reconfigurable hardware	observability;hardware design languages;field programmable gate arrays hardware in the loop simulation system fpga jumble hardware description language hardware emulation xilinx readback technology;xilinx readback technology;clocks;controllability;circuit simulation field programmable gate arrays emulation hardware design languages observability controllability circuit synthesis clocks circuit testing computational modeling;hardware description languages;emulation;fpga;program verification;circuit simulation;computational modeling;program verification field programmable gate arrays hardware description languages logic cad program debugging;hardware in the loop simulation;jumble hardware description language;circuit testing;program debugging;field programmable gate arrays;hardware description language;logic cad;hardware in the loop simulation system;simulation environment;circuit synthesis;hardware emulation	Powerful multicomputer platforms that combine FPGAs and programmable processors promise tremendous performance benefits for applications that take advantage of these rapidly emerging architectures. Portable applications are desirable because they can be easily adapted to take advantage of different reconfigurable computing platforms. raditional practices, however, intertwine application code with hardware specific code such that porting entails a significant rewrite of the application and reuse is difficult. Vforce, based on the VSIPL++ standard, is an exten sible framework we created that allows the same application code to run on different reconfigurable computing platforms. Vforce offers application-level portability, framework-level extensibility to new hardware, and system-level run time resource management. In particular, Vforce supports very late binding of the application to a specific hardware platform such that binding does not occur until run time. This paper describes Vforce with a focus on late run time binding to a specific hardware platform. Results using Vforce to implement an FFT and a time domain adaptive beam- former are presented.	central processing unit;extensibility;fast fourier transform;field-programmable gate array;late binding;parallel computing;reconfigurable computing;rewrite (programming);run time (program lifecycle phase);software portability	Nicholas Moore;Albert Conti;Miriam Leeser;Laurie A. Smith King	2007	15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)	10.1109/FCCM.2007.39	hardware compatibility list;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;hardware description language;field-programmable gate array;hardware emulation;hardware-in-the-loop simulation	Arch	-0.5507202386642791	48.71848065429309	166267
29bf52ada11eae912886be10861e1d17c5c50590	hartes: hardware-software codesign for heterogeneous multicore platforms	digital signal processing;hartes toolchain;inf;hartes toolchain hardware software codesign heterogeneous multicore platforms;hardware software codesign;compiler;multicore processing;xml;heterogeneous multicore platforms;multiprocessing systems hardware software codesign microprocessor chips;multiprocessing systems;hartes;field programmable gate arrays hardware digital signal processing multicore processing programming xml;field programmable gate arrays;heterogeneous multicore platforms reconfigurable hardware hardware software interface compiler tool chain hartes;development time;tool chain;programming;hardware software interface;reconfigurable hardware;microprocessor chips;hardware	Developing heterogeneous multicore platforms requires choosing the best hardware configuration for mapping the application, and modifying that application so that different parts execute on the most appropriate hardware component. The hArtes toolchain provides the option of automatic or semi-automatic support for this mapping. During test and validation on several computation-intensive applications, hArtes achieved substantial speedups and drastically reduced development times.	computation;multi-core processor;semiconductor industry;toolchain	Koen Bertels;Vlad Mihai Sima;Yana Yankova;Georgi Kuzmanov;Wayne Luk;José Gabriel F. Coutinho;Fabrizio Ferrandi;Christian Pilato;Marco Lattuada;Donatella Sciuto;Andrea Michelotti	2010	IEEE Micro	10.1109/MM.2010.91	multi-core processor;programming;computer architecture;compiler;parallel computing;real-time computing;xml;reconfigurable computing;computer science;digital signal processing;programming language;field-programmable gate array	Arch	2.1851163639058635	50.127307088485296	166698
203a3114b41e6154d7c149e462637b57c228ef3c	introduction to the special section on estimedia'08	engineering and technology;teknik och teknologier	"""Today, the design process for high-end multimedia embedded systems has become a crucial bottleneck due to the increasing complexity of both the software and the underlying hardware, coupled with shortened time-to-market pressures. While there has been a notable growth in the use and applications of multimedia systems and in the evolution of system-on-chip design technology, there are still enormous opportunities for improving design productivity in this domain. Given this backdrop, there are serious challenges (e.g., in terms of software, architectures, real-time systems, design methodologies, DSP, compilers, multimedia applications) that need to be faced during the design of next-generation embedded multimedia systems. This special section on Embedded Systems for Real-Time Multimedia contains five articles, of which four are extended versions of papers that were presented at the 6th IEEE Workshop on Embedded Systems for Real-Time Multimedia (ESTIMedia 2008). This workshop is part of the Embedded Systems Week and is intended to be a forum for specialists from academia and industry for defining design methodologies, archi-tectures, and circuits for future embedded multimedia systems. In addition to an open call for papers, a collection of top papers from this workshop were invited to submit their extended versions to this special section. The four articles from this workshop that are included here were among those papers that were most highly ranked by the distinguished international panel of ESTIMedia 2008 technical program committee members. The range of the articles featured provides a glimpse of the current state-of-the-art in embedded systems for real-time multimedia. They cover a variety of topics spanning across performance analysis, design space exploration, and programming of such systems. In the first article, """" Performance Analysis of Reconfigurations in Adaptive Real-Time Streaming Applications """" , Zhu et al. propose a performance analysis framework for adaptive real-time synchronous data flow streaming applications on runtime re-configurable FPGAs. The article presents a constraint-based approach to capture both streaming application execution semantics and the varying design concerns during reconfigurations. The second article, """" Parallelization of Belief Propagation on Cell Processors for Stereo Vision """" , by Hsieh et al. shows how the belief propagation algorithm can be imported and effectively mapped onto the IBM cell multicore processor such that it fits real-time constraints. This gives insight into the parallelization of many applications, since a larger group of problems can be solved by using belief propagation algorithms. The third article """" Balancing Programmability and Silicon Efficiency of Heterogeneous …"""	algorithm;archi;automatic parallelization;backdrop cms;belief propagation;cell (microprocessor);compiler;dataflow;design space exploration;digital signal processor;embedded system;fits;field-programmable gate array;file spanning;load balancing (computing);multi-core processor;parallel computing;profiling (computer programming);programming language;real-time clock;real-time computing;real-time transcription;software propagation;stereopsis;synchronous data flow;system on a chip	Mladen Berekovic;Samarjit Chakraborty;Petru Eles;Andy D. Pimentel	2012	ACM Trans. Embedded Comput. Syst.	10.1145/2180887.2180891	computer science	Embedded	-1.7919627625185903	46.4536934347701	167507
9e7ecb29f4a81112af2cd843dd3f55e90a458fd9	a framework for dynamic parallelization of fpga-accelerated applications	dependence analysis;fpgas;parallelization;dynamic optimization	High-level synthesis and compiler studies have introduced many compile-time techniques for parallelizing applications. However, one fundamental limitation of compile-time optimization is the requirement for pessimistic dependence assumptions that can significantly restrict parallelism. To avoid this limitation, many compilers require a restrictive coding style that is not practical for many designers. We present a more transparent approach that aggressively parallelizes applications by dynamically analyzing actual runtime dependencies and scheduling functions onto multiple devices when dependencies allow. In addition, the approach applies FPGA-specific pipelining optimizations to exploit deep parallelism in chains of dependent functions. Experimental results show a speedup of 4.9x for a video-processing application compared to sequential software execution, a speedup of 5.6x compared to traditional FPGA execution, with a framework overhead of only 4%.	compile time;compiler;field-programmable gate array;high-level synthesis;mathematical optimization;overhead (computing);parallel computing;pipeline (computing);programming style;scheduling (computing);speedup	Jeremy Fowers;Jianye Liu;Greg Stitt	2014		10.1145/2609248.2609256	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;programming language;field-programmable gate array;dependence analysis	EDA	-2.832612512969245	50.13626248131471	167539
abe60669478e261f24d72424dba97a5cd7026aa4	iterational retiming with partitioning: loop scheduling with complete memory	optimal solution;hardware software codesign;memory management;cell processor;memory performance;loop scheduling;codesign;loop transformation;hardware software;memory latency;memory latency hiding	The widening gap between processor and memory performance is the main bottleneck for modern computer systems to achieve high processor utilization. To hide memory latency, a variety of techniques have been proposed—from intermediate fast memories (caches) to various prefetching and memory management techniques. In this article, we propose a new loop scheduling with memory management technique, Iterational Retiming with Partitioning (IRP), that can completely hide memory latencies for applications with multidimensional loops on architectures like CELL processor. In IRP, the iteration space is first partitioned carefully. Then a two-part schedule, consisting of processor and memory parts, is produced such that the execution time of the memory part never exceeds the execution time of the processor part. These two parts are executed simultaneously and complete memory latency hiding is reached. In this article, we prove that such optimal two-part schedule can always be achieved given the right partition size and shape. Experiments on DSP benchmarks show that IRP consistently produces optimal solutions as well as significant improvement over previous techniques.	algorithm;cas latency;cpu cache;cell (microprocessor);digital signal processor;experiment;i/o request packet;iteration;loop scheduling;memory management;multi-core processor;retiming;run time (program lifecycle phase);scheduling (computing)	Chun Jason Xue;Jingtong Hu;Zili Shao;Edwin Hsing-Mean Sha	2010	ACM Trans. Embedded Comput. Syst.	10.1145/1698772.1698780	uniform memory access;distributed shared memory;co-design;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;cas latency;memory refresh;computer science;operating system;memory protection;overlay;conventional memory;extended memory;flat memory model;registered memory;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-2.3957212316177428	50.924186500524236	167579
9bde366e1897ad3b370b3bc473b0de456c75f078	fathom: reference workloads for modern deep learning methods	libraries;analytical models;training;computer architecture;computational modeling;machine learning;hardware	Deep learning has been popularized by its recent successes on challenging artificial intelligence problems. One of the reasons for its dominance is also an ongoing challenge: the need for immense amounts of computational power. Hardware architects have responded by proposing a wide array of promising ideas, but to date, the majority of the work has focused on specific algorithms in somewhat narrow application domains. While their specificity does not diminish these approaches, there is a clear need for more flexible solutions. We believe the first step is to examine the characteristics of cutting edge models from across the deep learning community. Consequently, we have assembled Fathom: a collection of eight archetypal deep learning workloads for study. Each of these models comes from a seminal work in the deep learning community, ranging from the familiar deep convolutional neural network of Krizhevsky et al., to the more exotic memory networks from Facebook's AI research group. Fathom has been released online, and this paper focuses on understanding the fundamental performance characteristics of each model. We use a set of application-level modeling tools built around the TensorFlow deep learning framework in order to analyze the behavior of the Fathom workloads. We present a breakdown of where time is spent, the similarities between the performance profiles of our models, an analysis of behavior in inference and training, and the effects of parallelism on scaling.	algorithm;artificial intelligence;artificial neural network;computation;computer hardware;convolutional neural network;deep learning;fathom;parallel computing;semiconductor research corporation;sensitivity and specificity;tensorflow	Robert Adolf;Saketh Rama;Brandon Reagen;Gu-Yeon Wei;David M. Brooks	2016	2016 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2016.7581275	computer architecture;simulation;computer science;artificial intelligence;machine learning;computational model	Metrics	-3.8645177482531743	46.49454567110316	167844
6d8f862faca27673f54b4682756eb1e6e4848b24	document classification systems in heterogeneous computing environments		Datacenter workloads demand high throughput, low cost and power efficient solutions. In most data centers the operating costs dominates the infrastructure cost. The ever growing amounts of data and the critical need for higher throughput, more energy efficient document classification solutions motivated us to investigate alternatives to the traditional homogeneous CPU based implementations of document classification systems. Several heterogeneous systems were investigated in the past where CPUs were combined with GPUs and FPGAs as system accelerators. The increasing complexity of FPGAs made them an interesting device in the heterogeneous computing environments and on the other hand difficult to program using Hardware Description languages. We explore the trade-offs when using high level synthesis and low level synthesis when programming FPGAs. Using low level synthesis results in less hardware resource usage on FPGAs and also offers the higher throughput compared to using HLS tool. While using HLS tool different heterogeneous computing devices such as multicore CPU and GPU targeted. Through our implementation experience and empirical results for data centric applications, we conclude that we can achieve power efficient results for these set of applications by either using low level synthesis or high level synthesis for programming FPGAs.	algorithm;central processing unit;clock rate;data center;document classification;experiment;field-programmable gate array;graphics processing unit;hardware description language;heterogeneous computing;high-level programming language;high-level synthesis;machine learning;multi-core processor;performance per watt;throughput;verilog	Nasibeh Nasiri;Philip Colangelo;Oren Segal;Martin Margala;Wim Vanderbauwhede	2016	2016 26th International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)	10.1109/PATMOS.2016.7833702	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system	Arch	-1.1009377946471341	47.250144328032675	169264
06e119f0890740ef11b00506b34cfbf79b7af119	veriintel2c: abstracting rtl to c to maximize high-level synthesis design space exploration		Abstract The design of integrated circuits (ICs) is typically done using low level Hardware Description Languages (HDLs) like Verilog or VHDL (Register Transfer Level). These enable the full controllability of the generated hardware design as they allow to specify the detailed behaviour and structure of the architecture, at every single clock cycle. The main drawback of using these low level HDLs is that takes very long time to create and verify large ICs with them. Moreover, it is hard to re-use HDL code for future projects that require changes in the micro-architecture. Thus, the industry is moving the level of abstraction to C-based VLSI design where designers only have to specify the functionality of the program and High-Level Synthesis (HLS) tools generate the HDL automatically. One additional benefit of C-based VLSI design is that it enables to explore the search space of possible micro-architectures from a single behavioral description. The result of a Design Space Exploration (DSE) is a trade-off curve of Pareto-optimal designs with unique area vs. performance metrics. Most VLSI design companies have large amounts of legacy HDL code. Thus, it makes sense to have an automatic flow to convert HDL designs into behavioral descriptions (e.g. C, C++ or SystemC) optimized for HLS DSE. This implies that the generation of explorable constructs, e.g. loops and arrays, which upon exploration, lead to very different micro-architectures (e.g. loops can be unrolled or folded, arrays can be mapped to RAMs or registers). In this paper, we propose a robust RTL to C translation method called VeriIntel2C to abstract RTL descriptions (written in Verilog) into ANSI-C descriptions optimized for HLS DSE by generating a large number of loops and arrays. Our method is able to generate these explorable constructs with the use of extended Hardware Petri Nets to extract the behaviour of the Verilog designs and to generate a Control Data Flow Graph (CDFG) that allows the easy identification of these constructs. From the experimental results, we are able to demonstrate that VeriIntel2C expands the design space considerably and also improves the quality of design space by 55% on average compared to previous work, on a wide range of designs.		Anushree Mahapatra;Benjamin Carrión Schäfer	2019	Integration	10.1016/j.vlsi.2018.03.011	real-time computing;systemc;verilog;computer science;computer architecture;high-level synthesis;architecture;register-transfer level;design space exploration;hardware description language;vhdl	EDA	2.431880154284384	51.10159560402984	169451
8a27eebe31f4d30bf8413cd9d87e71058035ede4	global hardware synthesis from behavioral dataflow descriptions	bottom up logic synthesis;topology;program path probabilities;hardware high level synthesis registers costs delay estimation digital systems multiplexing scheduling algorithm topology wiring;bottom up;design space exploration global hardware synthesis program path probabilities behavioral dataflow descriptions bottom up logic synthesis straight line code synthesis block structured dataflow graphs block level parallelism high level synthesis;hardware synthesis;block level parallelism;behavioral dataflow descriptions;multiplexing;high level synthesis;scheduling algorithm;logic synthesis;registers;digital systems;global hardware synthesis;block structured dataflow graphs;design space exploration;wiring;parallel programs;logic cad;delay estimation;hardware;straight line code synthesis	This paper reports on a new bottom-up synthesis technique for general behavioral descriptions. Our technique extends traditional straight-line code synthesis by allowing hierarchical, block-structured dataflow graphs with block-level parallelism. Program path probabilities are taken into account; and both high-level synthesis and design-space exploration are addressed.	algorithm;basic block;cluster analysis;cycle count;dataflow;datapath;hierarchical clustering;high- and low-level;high-level synthesis;line code;parallel computing;pipeline (computing);program transformation;scheduling (computing);stepwise regression	Josef Scheichenzuber;Werner Grass;Ulrich Lauther;Sabine März	1990		10.1145/123186.123337	parallel computing;logic synthesis;real-time computing;computer science;theoretical computer science;operating system;top-down and bottom-up design;processor register;high-level synthesis;programming language;scheduling;multiplexing	EDA	1.3649443200943985	52.388437813925044	169920
72e2cd2badd89389903d7ce945ef7f26b35bc407	component selection, scheduling and control schemes for high level synthesis	data flow graph high level synthesis performance optimization problem automatically synthesized circuits area constraints scheduling behavioral specification component selection design space exploration user specified library synchronous control scheme control step adjusted control asics;high level synthesis;parallel processing scheduling logic cad application specific integrated circuits;application specific integrated circuits;scheduling;high level synthesis automatic control delay circuit synthesis libraries job shop scheduling control system synthesis robotics and automation scheduling algorithm algorithm design and analysis;design space exploration;logic cad;performance optimization;parallel processing	This paper emphasizes on the performance optimization problem of automatically synthesized circuits while respecting area constraints. This optimization is performed at the earliest step of synthesis, i.e. during the scheduling of the behavioral specification of the circuit. It is mainly based on an adequate determination of the type of component which implements each operation in the specification. An algorithm performing concurrently component selection and scheduling is presented. It gives the designer facilities for design-space exploration, i.e. time versus area, since components are taken from a user specified library. The special features of this algorithm are: several component types can implement an operation and one component type can implement several operations; the delays are associated to pairs (component type, operation type); both the usual synchronous control scheme and a control scheme allowing to tune independently the delay of every control step (referred as adjusted control) are discussed. >	high-level synthesis;scheduling (computing)	Bruno Rouzeyre;D. Dupont;Georges Sagnes	1994		10.1109/EDTC.1994.326832	control engineering;fair-share scheduling;real-time computing;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;gain scheduling	EDA	1.4035997832752387	52.59103185737648	170249
475513fec753bbedd27c3009ab3349b99d9a2c6e	towards generic embedded multiprocessing for rvc-cal dataflow programs	design automation;data flow computing;multiprocessor interconnection	Dataflow languages enable describing signal processing applications in a platform independent fashion, which makes them attractive in today's multiprocessing era. RVC-CAL is a dynamic dataflow language that enables describing complex data-dependent programs such as video decoders. To this date, design automation toolchains for RVC-CAL have enabled creating workstation software, dedicated hardware and embedded application specific multiprocessor implementations out of RVC-CAL programs. However, no solution has been presented for executing RVC-CAL applications on generic embedded multiprocessing platforms. This paper presents a dataflow-based multiprocessor communication model, an architecture prototype that uses it and an automated toolchain for instantiating such a platform and the software for it. The complexity of the platform increases linearly as the number of processors is increased. The experiments in this paper use several instances of the proposed platform, with different numbers of processors. An MPEG-4 video decoder is mapped to the platform and executed on it. Benchmarks are performed on an FPGA board.	cal;dataflow programming;multiprocessing	Jani Boutellier;Olli Silvén	2013	Signal Processing Systems	10.1007/s11265-013-0737-3	embedded system;computer architecture;parallel computing;real-time computing;electronic design automation;computer science;operating system;programming language	Embedded	2.277712999092125	49.61328300741012	170265
e5c8c9deea76da7414e226be7ad6582bc58a76e8	hardware/software partitioning of vhdl system specifications		This paper presents an approach for system level specification and hardware/software partitioning with VHDL. The implications of using VHDL as a specification language are discussed and a message passing mechanism is proposed for process interaction. We define the metric values for partitioning and develop a cost function that guides our heuristics towards performance optimization under hardware and software cost constraints. Experimental results are presented.	heuristic (computer science);loss function;mathematical optimization;message passing;performance tuning;specification language;vhdl	Petru Eles;Krzysztof Kuchcinski;Zebo Peng;Alex Doboli	1996			embedded system;systems analysis;computer architecture;parallel computing;message passing;real-time computing;simulated annealing;specification language;vhdl;computer science;operating system;hardware description language;high-level synthesis;programming language;coprocessor	EDA	1.6266700516157493	52.607511285081166	170888
7c13f893341dda6c0d4d5b28cc6889c1543f3769	epicure: a partitioning and co-design framework for reconfigurable computing	design tool;reconfigurable computing;performance estimation;programming model;reconfigurable architecture;parallelism exhibition and functional estimation;hardware software partitioning;design framework;smart hw sw interface;formal programming model;interface model;design productivity;design space exploration;design methodology	This paper presents a new design methodology able to bridge the gap between an abstract specification and a heterogeneous reconfigurable architecture. The EPICURE contribution is the result of a joint study on abstraction/refinement methods and a smart reconfigurable architecture within the formal Esterel design tools suite. The original points of this work are: i) a generic HW/SW interface model, ii) a specification methodology that handles the control, and includes efficient verification and HW/SW synthesis capabilities, iii) a method for parallelism exploration based on abstract resources/performance estimation expressed in terms of area/delay tradeoffs, iv) a HW/SW partitioning approach that refines the specification into explicit HW configurations and the associated SW control. The EPICURE framework shows how a cooperation of complementary methodologies and CAD tools associated with a relevant architecture can significantly improve the designer productivity, especially in the context of reconfigurable architectures.	computer-aided design;esterel;parallel computing;reconfigurable computing;refinement (computing);shattered world	Jean-Philippe Diguet;Guy Gogniat;Jean Luc Philippe;Yannick Le Moullec;Sébastien Bilavarn;Christian Gamrat;Karim Ben Chehida;Michel Auguin;Xavier Fornari;Philippe Kajfasz	2006	Microprocessors and Microsystems	10.1016/j.micpro.2006.02.015	embedded system;computer architecture;parallel computing;real-time computing;design methods;reconfigurable computing;computer science;operating system;programming paradigm	EDA	0.8496664944768174	50.95137838164879	171047
3930a50a02afa1ef2f128483bf0c02a8c1641116	an efficient framework for dynamic reconfiguration of instruction-set customization	dynamic reconfiguration;instruction set extensions;customizable processors;temporal partitioning;dynamic program;iterative algorithm;instruction set extension;graph partitioning;spatial partitioning	We present an efficient framework for dynamic reconfiguration of application-specific instruction-set customization. A key component of this framework is an iterative algorithm for temporal and spatial partitioning of the loop kernels. Our algorithm maximizes performance gain of an application while taking into consideration the dynamic reconfiguration cost. It selects the appropriate custom instruction-sets for the loops and maps them into appropriate configurations. We model the temporal partitioning problem as a k-way graph partitioning problem. A dynamic programming based solution is used for the spatial partitioning. Comprehensive experimental results indicate that our iterative algorithm is highly scalable while producing optimal or near-optimal (99% of the optimal) performance gain.	algorithm;binary space partitioning;dynamic programming;graph partition;iterative method;map;partition problem;scalability	Huynh Phung Huynh;Joon Edward Sim;Tulika Mitra	2007		10.1145/1289881.1289906	computer architecture;parallel computing;real-time computing;computer science;graph partition;space partitioning;iterative method;programming language;set partitioning in hierarchical trees	ML	-0.17916094433752025	51.98594752782078	171280
f0f17a7847c1b52bc2591758c67edea367ebbdb9	quantum accelerators for high-performance computing systems		We define some of the programming and system-level challenges facing the application of quantum processing to high-performance computing. Alongside barriers to physical integration, prominent differences in the execution of quantum and conventional programs challenges the intersection of these computational models. Following a brief overview of the state of the art, we discuss recent advances in programming and execution models for hybrid quantum-classical computing. We discuss a novel quantum-accelerator framework that uses specialized kernels to offload select workloads while integrating with existing computing infrastructure. We elaborate on the role of the host operating system to manage these unique accelerator resources, the prospects for deploying quantum modules, and the requirements placed on the language hierarchy connecting these different system components. We draw on recent advances in the modeling and simulation of quantum computing systems with the development of architectures for hybrid high-performance computing systems and the realization of software stacks for controlling quantum devices. Finally, we present simulation results that describe the expected system-level behavior of high-performance computing systems composed from compute nodes with quantum processing units. We describe performance for these hybrid systems in terms of time-to-solution, accuracy, and energy consumption, and we use simple application examples to estimate the performance advantage of quantum acceleration.	computation;computational model;final doom;hybrid system;operating system;parallel computing;quantum computing;quantum information science;requirement;simulation;supercomputer	Keith A. Britt;Fahd A. Mohiyaddin;Travis S. Humble	2017	2017 IEEE International Conference on Rebooting Computing (ICRC)	10.1109/ICRC.2017.8123664	discrete mathematics;theoretical computer science;computational model;software;quantum computer;hybrid system;mathematics;modeling and simulation;quantum information science;quantum;supercomputer	HPC	-3.1431692514647	47.16593333109025	171464
08758bd90b332d846b31a8849ef986003568bb78	modelling cryptonite - on the design of a programmable high-performance crypto processor		Cryptographic algorithms – even when designed for easy implementability on general purpose architectures – still show a huge performance gap between implementations in software and those using dedicated hardware. Such hardware is usually only able to deal with one single algorithm or a very narrowly defined set of algorithms. The tradeoff between speed/throughput and flexibility can be eased by programmable crypto architectures. These can be existing general purpose architectures enhanced by specialized functional units which fulfill the requirements of typical cryptographic algorithms. Alternatively, a fully custom architecture can be designed. In this paper we describe the methods used to design a programmable crypto architecture from scratch. We will introduce a set of typical cryptographic algorithms, investigate their requirements, and finally show the weighted result leading to our Cryptonite architecture.	algorithm;analysis of algorithms;cryptography;requirement;throughput	Rainer Buchty	2004			throughput;implementation;embedded system;scratch;cryptography;software;architecture;performance gap;computer science	Arch	2.442073938272932	48.68716194012126	171485
a3bce1a471306a4db83ec7f50b179a0889048f8a	using round-robin tracepoints to debug multithreaded hls circuits on fpgas	debugging;round robin tracepoints trace based debugging architecture multithreaded source code high level synthesis fpga multithreaded hls circuits;system on chip;field programmable gate arrays;instruction sets debugging hardware field programmable gate arrays computer bugs system on chip;computer bugs;high level synthesis field programmable gate arrays;instruction sets;hardware	High-level synthesis (HLS) for FPGA designs has gained significant traction in recent years. A key component in its adoption is allowing users to debug their hardware systems in the context of the original source code. This is becoming even more challenging as modern HLS tools enable the user to provide multithreaded source code for synthesis to hardware. Although recent work has begun to tackle source-level debugging of HLS circuits, none have addressed doing this in multithreaded circuits. In such systems it may be necessary to observe the behaviour of multiple threads for long run times in order to locate obscure or non-deterministic bugs and performance issues. In this paper we present a trace-based debugging architecture which records values from user-selected tracepoints into on-chip memories during circuit execution. The recorded values can be provided to the user as a cycle-accurate timeline of events to aid them in debugging multithreaded HLS circuits. We present a novel technique to allow multiple hardware threads to share trace buffers, effectively increasing the execution trace that can be recorded. This is accomplished by analyzing the control and data flow graph to determine the maximum rates at which each thread can encounter tracepoints, using this information to select which threads can share trace buffers, and automatically generating round-robin circuitry to arbitrate access to the buffers. Using this technique we are able to obtain an average of 4X improvement in trace length for an 8 thread system. This provides users with a longer timeline of execution and greater visibility into the execution of multithreaded HLS circuits.	dataflow;debugging;electronic circuit;field-programmable gate array;high-level synthesis;multithreading (computer architecture);round-robin scheduling;software bug;thread (computing);timeline;traction teampage	Jeffrey B. Goeders;Steven J. E. Wilton	2015	2015 International Conference on Field Programmable Technology (FPT)	10.1109/FPT.2015.7393128	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;software bug;computer science;operating system;instruction set;debugging;field-programmable gate array	EDA	-3.8016967797194297	50.64779143716168	171655
bf6663dc019782a983f96168365239c4a0d354a3	virtualrc: a virtual fpga platform for applications and tools portability	field programmable gate array;design reuse;virtual architectures;fpga;portability;high level synthesis;middleware	Numerous studies have shown significant performance and power benefits of field-programmable gate arrays (FPGAs). Despite these benefits, FPGA usage has been limited by application design complexity caused largely by the lack of code and tool portability across different FPGA platforms, which prevents design reuse. This paper addresses the portability challenge by introducing a framework of architecture and middleware for virtualization of FPGA platforms, collectively named VirtualRC. Experiments show modest overhead of 5-6% in performance and 1% in area, while enabling portability of 11 applications and two high-level synthesis tools across three physical platforms.	field-programmability;field-programmable gate array;high- and low-level;high-level synthesis;middleware;overhead (computing);software portability	Robert Kirchgessner;Greg Stitt;Alan D. George;Herman Lam	2012		10.1145/2145694.2145728	software portability;embedded system;computer architecture;parallel computing;computer science;operating system;portability testing;field-programmable gate array	EDA	0.16111979097106777	49.276829534085316	172059
168f1f10f25a50916c161ed870e9d58e23cffa14	design and analysis of an apu for exascale computing	computer architecture;three dimensional displays;graphics processing units;bandwidth;architecture;supercomputers;central processing unit	The challenges to push computing to exaflop levels are difficult given desired targets for memory capacity, memory bandwidth, power efficiency, reliability, and cost. This paper presents a vision for an architecture that can be used to construct exascale systems. We describe a conceptual Exascale Node Architecture (ENA), which is the computational building block for an exascale supercomputer. The ENA consists of an Exascale Heterogeneous Processor (EHP) coupled with an advanced memory system. The EHP provides a high-performance accelerated processing unit (CPU+GPU), in-package high-bandwidth 3D memory, and aggressive use of die-stacking and chiplet technologies to meet the requirements for exascale computing in a balanced manner. We present initial experimental analysis to demonstrate the promise of our approach, and we discuss remaining open research challenges for the community.	amd accelerated processing unit;apu nahasapeemapetilon;computer architecture;flops;memory bandwidth;open research;performance per watt;requirement;stacking;supercomputer	Thiruvengadam Vijayaraghavan;Yasuko Eckert;Gabriel H. Loh;Michael J. Schulte;Mike Ignatowski;Bradford M. Beckmann;William C. Brantley;Joseph L. Greathouse;Wei Huang;Arun Karunanithi;Onur Kayiran;Mitesh R. Meswani;Indrani Paul;Matthew Poremba;Steven Raasch;Steven K. Reinhardt;Greg Sadowski	2017	2017 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2017.42	computer architecture;parallel computing;computer hardware;computer science;architecture;operating system;central processing unit;bandwidth	Arch	-2.659352948045931	47.36517336798379	172306
1915572f8d11be80cf5914cd10e0a09a3c2c6d03	compilation approach for coarse-grained reconfigurable architectures	microarchitecture;customized architectural configurations;coarse grained reconfigurable architectures;reconfigurable architectures;space exploration;architecture description languages;computer architecture;reconfigurable architecture;compilation techniques;critical loops;generic reconfigurable architecture;registers;memory bottleneck;program compilers reconfigurable architectures;memory bottleneck coarse grained reconfigurable architectures critical loops computation intensive functions compilation techniques algorithm mapping customized architectural configurations generic reconfigurable architecture;coarse grained;program compilers;algorithm mapping;reconfigurable architectures microarchitecture delay registers computer architecture design methodology hardware space exploration architecture description languages timing;article;computation intensive functions;hardware;design methodology;timing	Coarse-grained reconfigurable architectures can enhance the performance of critical loops and computation-intensive functions. Such architectures need efficient compilation techniques to map algorithms onto customized architectural configurations. A new compilation approach uses a generic reconfigurable architecture to tackle the memory bottleneck that typically limits the performance of many applications.	algorithm;compiler;computation;reconfigurable computing;von neumann architecture	Jongeun Lee;Kiyoung Choi;Nikil D. Dutt	2003	IEEE Design & Test of Computers	10.1109/MDT.2003.1173050	computer architecture;parallel computing;real-time computing;design methods;microarchitecture;computer science;space exploration;operating system;processor register	EDA	1.1279571540105318	50.74776400980242	172933
3422fa0a148d8e11e5ebf4cd0eda841d2baebe1e	minimizing bank selection instructions for partitioned memory architecture	optimal solution;microcontrollers;pbqp;quadratic program;optimization technique;real time;partitioned memory architecture;chip;ram allocation;low power;memory architecture;compiler optimization;code size;bank switching	Bank switching is a technique that increases the code and data memory in microcontrollers without extending the address buses. Given a program in which variables have been assigned to data banks, we present a novel optimization technique that minimizes the overhead of bank switching through cost-effective placement of bank selection instructions. The optimal placement is controlled by a variety of different objectives, such as runtime, low power, small code size or a combination of these parameters. We have formulated the problem as a form of Partitioned Boolean Quadratic Programming (PBQP).We implemented the optimization as part of a PIC Micro-chip backend and evaluated the approach for several optimization objectives. Our benchmark suite comprises programs from MiBench and DSPStone plus a microcontroller real-time kernel and drivers for microcontroller hardware devices. Our optimization achieved a reduction of program memory space between 2.7% and 18.2%, and an overall improvement with respect to instruction cycles between 5.1% and 28.8%. Our optimization achieved an optimal solution for all benchmark programs.	bank switching;benchmark (computing);dspace;instruction cycle;mathematical optimization;microcontroller;overhead (computing);quadratic programming;read-only memory;real-time clock;run time (program lifecycle phase)	Bernhard Scholz;Bernd Burgstaller;Jingling Xue	2006		10.1145/1176760.1176786	chip;bank switching;microcontroller;embedded system;parallel computing;real-time computing;memory bank;profile-guided optimization;computer hardware;telecommunications;computer science;operating system;optimizing compiler;programming language;quadratic programming	EDA	-2.056876380149054	52.80382677492628	173010
35eafe36bb025d35ed7e02f0a5105412d2a055bd	design methodology for pipelined heterogeneous multiprocessor system	digital signal processing;pipelined heterogeneous multiprocessor system;image coding;hardware software codesign;application specific architectures;application software;multiprocessor systems;performance;extensible processor;design methodology multiprocessing systems application specific processors digital audio players pipelines hardware coprocessors application software computer architecture digital signal processing;conventional processing entity;coprocessor;design space;coprocessors;heterogeneous multiprocessor methodology;system on chip coprocessors hardware software codesign image coding pipeline processing;computer architecture;performance improvement;jpeg;system on chip;hardware software partitioning;pipelines;application specific processors;design;homogeneous processor;multiprocessor soc systems;multiprocessing systems;parallel hardware;multiprocessor design;pipeline configuration;digital audio players;experimentation;single processor design design methodology pipelined heterogeneous multiprocessor system multiprocessor soc systems parallel hardware coprocessor homogeneous processor application specific architectures asip conventional processing entity pipeline configuration heterogeneous multiprocessor methodology extensible processor multiprocessor design jpeg mp3;architecture;mp3;asip design experimentation performance hardware software partitioning architecture;asip;pipeline processing;single processor design;hardware;design methodology	Multiprocessor SoC systems have led to the increasing use of parallel hardware along with the associated software. These approaches have included coprocessor, homogeneous processor (e.g. SMP) and application specific architectures (i.e. DSP, ASIC). ASIPs have emerged as a viable alternative to conventional processing entities (PEs) due to its configurability and programmability. In this work, we introduce a heterogeneous multi-processor system using ASIPs as processing entities in a pipeline configuration. A streaming application is taken and manually broken into a series of algorithmic stages (each of which make up a stage in a pipeline). We formulate the problem of mapping each algorithmic stage in the system to an ASIP configuration, and propose a heuristic to efficiently search the design space for a pipeline-based multi ASIP system.  We have implemented the proposed heterogeneous multiprocessor methodology using a commercial extensible processor (Xtensa LX from Tensilica Inc.). We have evaluated our system by creating two benchmarks (MP3 and JPEG encoders) which are mapped to our proposed design platform. Our multiprocessor design provided a performance improvement of at least 4.11X (JPEG) and 3.36X (MP3) compared to the single processor design. The minimum cost obtained through our heuristic was within 5.47% and 5.74% of the best possible values for JPEG and MP3 benchmarks respectively.	application-specific instruction set processor;application-specific integrated circuit;benchmark (computing);coprocessor;digital signal processor;encoder;entity;heuristic;jpeg;mp3;pipeline (computing);processor design;symmetric multiprocessing	Seng Lin Shee;Sri Parameswaran	2007	2007 44th ACM/IEEE Design Automation Conference	10.1145/1278480.1278682	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;coprocessor	EDA	1.4874329455286273	49.71284696100883	173265
b6af791e161b99721a0c59a71050a3115942328b	hardware implementation based on fpga of semaphore management in μc/os-ii real-time operating system	system calls;simulation;fpgas;semaphore management;real time operating systems;hardware design;event control block;field programmable gate arrays;software hardening	Semaphore is a kind of mechanism used in a multithreaded environment to ensure that two or more key code segments are not concurrently invoked. In order to enhance the response capability of real-time operating systems, a hardware design scheme to implement semaphore management based on field programmable gate array is put forward in this paper. We take the μC/OS-II real-time operating system as an example to design hardware logical circuits of semaphore management function module according to its parallel characteristics, and simulation tests under Xilinx ISE software environment are performed. The simulation results show that implementing semaphore management by hardware can obviously improve the execution time of creating/deleting a semaphore, applying for/releasing a semaphore and P/V operations; therefore, the whole reliability of the real-time operating system is greatly improved.	code segment;field-programmable gate array;multithreading (computer architecture);real-time clock;real-time operating system;run time (program lifecycle phase);simulation;thread (computing);xilinx ise	Shi-hai Zhu	2015	IJGUC	10.1504/IJGUC.2015.070677	embedded system;parallel computing;real-time computing;real-time operating system;computer science;operating system;field-programmable gate array	Embedded	-3.5599616501947264	51.52422968261892	173726
b5ca84b61668d3b9e9929ed26177ba78880d80ac	a code-based analytical approach for using separate device coprocessors in computing systems	paper;cost function;profiling;performance;gpu;hardware accelerator;fpga;analysis;source code;computer science	Special hardware accelerators like FPGAs and GPUs are commonly introduced into a computing system as a separate device. Consequently, the accelerator and the host system do not share a common memory. Sourcing out the data to the additional hardware thus introduces a communication penalty. Based on a combination of a program's source code and execution profiling we perform an analysis which evaluates the arithmetic intensity as a cost function to identify those parts most reasonable to source out to the accelerating hardware. The basic principles of this analysis are introduced and tested with a sample application. Its concrete results are discussed and evaluated based on the performance of a FPGA-based and a GPU-based implementation.	coprocessor	Volker Hampel;Grigori Goronzy;Erik Maehle	2011		10.1007/978-3-642-19137-4_1	embedded system;parallel computing;hardware acceleration;computer hardware;performance;computer science;operating system;analysis;profiling;programming language;field-programmable gate array;source code	HCI	0.2819839396208079	48.26198450142804	173933
a2cb4523d91be0d99400fb8ec28f6b042690818c	the effect of multi-core communication architecture on system performance		— MPSoCs are gaining popularity because of its potential to solve computationally expensive applications. A multi-core processor combines two or more independent cores (normally a CPU) into a single package composed of a single integrated circuit (Chip). However, as the number of components on a single chip and their performance continue to increase, a shift from computation-based to communication-based design becomes mandatory. As a result, the communication architecture plays a major role in the area, performance, and energy consumption of the overall system. In this paper, multiple soft-cores (IPs) such as Micro Blaze in an FPGA is used to study the effect of different connection topologies on the performance of a parallel program.	analysis of algorithms;central processing unit;computation;field-programmable gate array;integrated circuit;multi-core processor	Bilal Habib;Ahmed Anber;Sultan Daud Khan	2016	CoRR		embedded system;parallel computing;real-time computing;operating system;distributed computing	Arch	-2.8789258352411524	48.96644099771668	174209
39a11d39ecaf05a9c8a843228721718323aeba5b	power efficiency in a partially reconfigurable multiprocessor system	benchmarking;partial reconfiguration;fpga	This paper describes the benchmarking of an FPGA-based computing system that uses partially reconfigurable tiles for real-time allocation of hardware resources. This system was developed for use in the aerospace industry in order to provide redundancy for fault mitigation and real-time hardware reallocation to reduce mass associated with separate functional systems. In this paper, we present the results of performance studies on our Xilinx Virtex-6 platform using the Dhrystone, LINPACK and NAS-Kernel benchmarks. We further present the impact on power consumption as processors and hardware accelerators are brought online to increase performance.	central processing unit;dhrystone;field-programmable gate array;hardware acceleration;lunpack;multiprocessing;real-time clock;real-time transcription	Raymond J. Weber;Justin A. Hogan;Brock J. LaMeres;Todd Kaiser	2013		10.1145/2464996.2467284	embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;field-programmable gate array;benchmarking	EDA	-1.8264240540489574	50.365408805653516	174483
44f3caab0fa91591bc5a6b53e714255e490e904d	advanced compiler optimization for calm risc8 low-end embedded processor	extensive use;advanced compiler optimization;architectural consideration;integer promotion elimination;8-bit embedded processor;link-time level;assembly level;intermediate code level;code size;calm risc8 low-end;instruction scheduling;embedded processor;register allocation;compiler optimization	CalmRISC8 is an 8-bit embedded processor, in which architectural considerations for compiler are ignored to reduce power consumption. To over- come these constrains, new techniques are presented at an intermediate code level, an assembly level, and a link-time level. Techniques include register allo- cation, integer promotion elimination, extensive use of library functions, in- struction scheduling for bank collects, and various optimizations at link-time. Experimental results show that 56.7 % reduction in code size can be achieved.	calm technology;embedded system;optimizing compiler	Dae Hwan Kim	2000		10.1007/3-540-46423-9_12	computer architecture;parallel computing;real-time computing;computer science;dead code elimination	EDA	-2.194432447888467	52.77219002411397	174683
85815016c620a10aed3e8ef62625794fa28b5ffb	multi-level loop fusion with minimal code size	loop fusion			Meilin Liu;Zili Shao;Chun Jason Xue;Kevin F. Chen;Edwin Hsing-Mean Sha	2005			control theory;loop fusion;loop fission;mathematics	EDA	-1.0054127898057315	47.6938469612781	175503
50ea61fc68b0a656bdcf534cd3c672c041c7a41f	understanding performance differences of fpgas and gpus: (abtract only)		The notorious power wall has significantly limited the scaling for general-purpose processors. To address this issue, various accelerators, such as GPUs and FPGAs, emerged to achieve better performance and energy-efficiency. Between these two programmable accelerators, a natural question arises: which applications are better suited for FPGAs, which for GPUs, and why?  In this paper, our goal is to better understand the performance differences between FPGAs and GPUs and provide more insights to the community. We intentionally start with a widely used GPU-friendly benchmark suite Rodinia, and port 11 of the benchmarks (15 kernels) onto FPGAs using the more portable and programmable high-level synthesis C. We provide a simple five-step strategy for FPGA accelerator designs that can be easily understood and mastered by software programmers, and present a quantitative performance breakdown of each step. Then we propose a set of performance metrics, including normalized operations per cycle (OPC_norm) for each pipeline, and effective parallel factor (effective_para_factor), to compare the performance of GPU and FPGA accelerator designs. We find that for 6 out of the 15 kernels, today's FPGAs can provide comparable performance or even achieve better performance, while only consume about 1/10 of GPUs' power (both on the same technology node). We observe that FPGAs usually have higher OPC_norm in most kernels in light of their customized deep pipeline but lower effective_para_factor due to far lower memory bandwidth than GPUs. Future FPGAs should increase their off-chip bandwidth and clock frequency to catch up with GPUs.	benchmark (computing);central processing unit;clock rate;field-programmable gate array;general-purpose modeling;graphics processing unit;high- and low-level;high-level synthesis;image scaling;memory bandwidth;programmer;semiconductor device fabrication	Jason Cong;Zhenman Fang;Michael Lo;Hanrui Wang;Jingxian Xu;Shaochong Zhang	2018		10.1145/3174243.3174970	field-programmable gate array;parallel computing;real-time computing;computer science;clock rate;scaling;memory bandwidth;software;bandwidth (signal processing)	Arch	-3.0881424340990034	46.981308124769185	175575
695e4a0794c29286ac62ad10026bb266cb67007a	code density and energy efficiency of exposed datapath architectures	tta;software bypassing;vliw;low power computing;exposed datapath architectures;processor architectures	Exposing details of the processor datapath to the programmer is motivated by improvements in the energy efficiency and the simplification of the microarchitecture. However, an instruction format that can control the data path in a more explicit manner requires more expressiveness when compared to an instruction format that implements more of the control logic in the processor hardware and presents conventional general purpose register based instructions to the programmer. That is, programs for exposed datapath processors might require additional instruction memory bits to be fetched, which consumes additional energy. With the interest in energy and power efficiency rising in the past decade, exposed datapath architectures have received renewed attention. Several variations of the additional details to expose to the programmer have been proposed in the academy, and some exposed datapath features have also appeared in commercial architectures. The different variations of proposed exposed datapath architectures and their effects to the energy efficiency, however, have not so far been analyzed in a systematic manner in public. This article provides a review of exposed datapath approaches and highlights their differences. In addition, a set of interesting exposed datapath design choices is evaluated in a closer study. Due to the fact that memories P. Jääskeläinen ( ) · H. Kultala · T. Viitanen · J. Takala Tampere University of Technology, Tampere, Finland e-mail: pekka.jaaskelainen@tut.fi H. Kultala e-mail: heikki.kultala@tut.fi T. Viitanen e-mail: timo.2.viitanen@tut.fi J. Takala e-mail: jarmo.takala@tut.fi constitute a major component of power consumption in contemporary processors, we analyze instruction encodings for different exposed datapath variations and consider the energy required to fetch the additional instruction bits in comparison to the register file access savings achieved with the exposed datapath.	academy;addressing mode;baseline (configuration management);bus encoding;central processing unit;clock rate;compiler;context switch;control point (mathematics);datapath;email;frequency scaling;image scaling;input/output;instruction-level parallelism;interrupt;microarchitecture;network switch;operand;operating system;parallel computing;performance per watt;preemption (computing);processor design;processor register;programmer;radio frequency;register file;requirement;scheduling (computing);text simplification;very long instruction word	Pekka Jääskeläinen;Heikki Kultala;Timo Viitanen;Jarmo Takala	2015	Signal Processing Systems	10.1007/s11265-014-0924-x	embedded system;computer architecture;finite state machine with datapath;parallel computing;real-time computing;computer science;very long instruction word;operating system	Arch	-1.054052851428198	53.06654338288367	176345
aac881310bb1eb7aa170876da7544d503cf0049f	programming highly parallel reconfigurable architectures for public-key cryptographic applications	public key cryptography;instruction level parallel;low level programming parallel reconfigurable architecture public key cryptographic application tiled architecture instruction level parallelism programming model;programming language;identity based encryption;application software;reconfigurable architectures;reconfigurable architectures cryptography parallel architectures programming;parallel programming;functional programming;coprocessors;programming model;domain knowledge;public key cryptographic application;computer architecture;reconfigurable architecture;cryptographic algorithm;public key;tiled architecture;parallel architectures;low level programming;cryptography;parallel programming reconfigurable architectures public key cryptography identity based encryption coprocessors functional programming parallel processing program processors computer architecture application software;parallel reconfigurable architecture;identity based cryptography;time to market;parallel programming model;functional unit;instruction level parallelism;development time;programming;program processors;parallel processing;domain specificity;generic programming	Tiled architectures are emerging as an architectural platform that allows high levels of instruction level parallelism. Traditional compiler parallelization techniques are usually employed to generate programs for these architectures. However, for specific application domains, the compiler is not able to effectively exploit the domain knowledge. In this paper, we propose a new programming model that, by means of the definition of software function units, allows domain-specific features to be explicitly modeled, achieving good performances while reducing development times with respect to low-level programming. Identity-based cryptographic algorithms are known to be computationally intensive and difficult to parallelize automatically. Recent advances have led to the adoption of embedded cryptographic coprocessors to speed up both traditional and identity-based public key algorithms. Custom-designed coprocessors have high development costs and times with respect to general purpose or DSP coprocessors. Therefore, the proposed methodology can be effectively employed to reduce time to market while preserving performances. It also represents a starting point for the definition of cryptography-oriented programming languages. We prove that tiled architecture well compare w.r.t. competitors implementations such as StrongARM and FPGAs	algorithm;application domain;compiler;coprocessor;digital signal processor;embedded system;field-programmable gate array;high- and low-level;instruction-level parallelism;low-level programming language;parallel computing;performance;programming model;public-key cryptography;strongarm	Giovanni Agosta;Luca Breveglieri;Gerardo Pelosi;Martino Sykora	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.160	parallel processing;computer architecture;parallel computing;computer science;theoretical computer science;operating system;public-key cryptography;programming language;functional programming	EDA	0.7781533725629954	48.61158664702292	176990
a708864766cf4f9086fe75ab17da4de0807894c2	a parallelizing matlab compiler framework and run time for heterogeneous systems	compiler matlab heterogeneous computing;kernel;paper;matlab kernel processor scheduling field programmable gate arrays data transfer scheduling message systems;heterogeneous systems;heterogeneous computing;processor scheduling;code generation;fpga;compiler;message systems;scheduling;computer science;field programmable gate arrays;matlab;data transfer	Compute-intensive applications incorporate ever increasing data processing requirements on hardware systems. Many of these applications have only recently become feasible thanks to the increasing computing power of modern processors. The Matlab language is uniquely situated to support the description of these compute-intensive scientific applications, and consequently has been continuously improved to provide increasing computational support in the form of multithreading for CPUs and utilizing accelerators such as GPUs and FPGAs. Moreover, to take advantage of the computational support in these heterogeneous systems from the problem domain to the computer architecture necessitates a wide breadth of knowledge and understanding. In this work, we present a framework for the development of compute-intensive scientific applications in Matlab using heterogeneous processor systems. We investigate systems containing CPUs, GPUs, and FPGAs. We leverage the capabilities of Matlab and supplement them by automating the mapping, scheduling, and parallel code generation. Our experimental results on a set of benchmarks achieved from 20x to 60x speedups compared to the standard Matlab CPU environment with minimal effort required on the part of the user.	best, worst and average case;central processing unit;code generation (compiler);compiler;computation;computer architecture;field-programmable gate array;graphics processing unit;matlab;map;medical imaging;memory management;multithreading (computer architecture);parallel computing;powerpc 600;problem domain;requirement;run time (program lifecycle phase);runtime system;scheduling (computing);situated;synthetic intelligence;thread (computing)	Sam Skalicky;Sonia López;Marcin Lukowiak;Andrew G. Schmidt	2015	2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems	10.1109/HPCC-CSS-ICESS.2015.51	computer architecture;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language;field-programmable gate array	Arch	-0.6414130805158201	49.13956808795733	177115
ff0e9bf39958a13d1d1ab160e3c163125a4acb26	an automated design approach to map applications on cgras	cgra;scheduling;mapping;binding	Coarse-Grained Reconfigurable Architectures (CGRAs) are promising high-performance and power-efficient platforms. However, their uses are still limited by the capability of mapping tools. This abstract paper outlines a new automated design flow to map applications on CGRAs. The interest of our method is shown through comparison with state of the art approaches.	reconfigurable computing	Thomas Peyret;Gwenolé Corre;Mathieu Thevenin;Kevin J. M. Martin;Philippe Coussy	2014		10.1145/2591513.2591552	real-time computing;computer science;bioinformatics;theoretical computer science;operating system;scheduling	EDA	-0.5376789718493877	48.88707914317323	177331
776099764d85a05af27e4613a705967664bd39f2	a reconfigurable distributed computing fabric exploiting multilevel parallelism	hardware design languages;reconfigurable architectures distributed processing field programmable gate arrays parallel architectures;microprocessors;reconfigurable architectures;distributed processing;logic;distributed computing;code portability;computer networks;distributed computing fabrics parallel processing field programmable gate arrays computer architecture computer networks hardware design languages microprocessors costs logic;computer architecture;reconfigurable architecture;parallel architectures;routing policies;architectural evolution;data flow processing;fabrics;reconfigurable distributed computing;field programmable gate arrays;data flow;multilevel parallelism;high performance;code portability reconfigurable distributed computing multilevel parallelism field programmable gate arrays reconfigurable architecture data flow processing crossbar technology architectural evolution;parallel processing;crossbar technology	This paper presents a novel reconfigurable data flow processing architecture that promises high performance by explicitly targeting both fine- and course-grained parallelism. This architecture is based on multiple FPGAs organized in a scalable direct network that is substantially more interconnect-efficient than currently used crossbar technology. In addition, we discuss several ancillary issues and propose solutions required to support this architecture and achieve maximal performance for general-purpose applications; these include supporting IP, mapping techniques, and routing policies that enable greater flexibility for architectural evolution and code portability	crossbar switch;dataflow;distributed computing;field-programmable gate array;general-purpose modeling;maximal set;parallel computing;routing;scalability;software portability	Charles L. Cathey;Jason D. Bakos;Duncan A. Buell	2006	2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2006.15	parallel processing;computer architecture;parallel computing;computer science;logic	Arch	-0.39302473687021217	48.91533998452747	177421
060e8f75afc43831dd62a33b30e28b8f3b2a63c6	crown scheduling: energy-efficient resource allocation, mapping and discrete frequency scaling for collections of malleable streaming tasks	optimisation;processor cores crown scheduling energy efficient resource allocation mapping malleable streaming tasks energy optimal code many core processor dynamic discrete frequency scaling streaming task collections data flows pipelined task graph optimization discrete voltage frequency scaling integer linear programming ilp dynamic rescaling power consumption;resource allocation;resource management schedules radio spectrum management dynamic scheduling optimization processor scheduling;scheduling;scaling circuits;datavetenskap datalogi;cores;computer science;power consumption;scheduling cores microprocessor chips optimisation power consumption resource allocation scaling circuits;microprocessor chips	We investigate the problem of generating energy-optimal code for a collection of streaming tasks that include parallelizable or malleable tasks on a generic many-core processor with dynamic discrete frequency scaling. Streaming task collections differ from classical task sets in that all tasks are running concurrently, so that cores typically run several tasks that are scheduled round-robin at user level in a data driven way. A stream of data flows through the tasks and intermediate results are forwarded to other tasks like in a pipelined task graph. In this paper we present crown scheduling, a novel technique for the combined optimization of resource allocation, mapping and discrete voltage/frequency scaling for malleable streaming task sets in order to optimize energy efficiency given a throughput constraint. We present optimal off-line algorithms for separate and integrated crown scheduling based on integer linear programming (ILP). We also propose extensions for dynamic rescaling to automatically adapt a given crown schedule in situations where not all tasks are data ready. Our energy model considers both static idle power and dynamic power consumption of the processor cores. Our experimental evaluation of the ILP models for a generic manycore architecture shows that at least for small and medium sized task sets even the integrated variant of crown scheduling can be solved to optimality by a state-of-the-art ILP solver within a few seconds.	algorithm;best, worst and average case;central processing unit;crown group;discrete frequency domain;frequency scaling;heuristic (computer science);image scaling;integer programming;linear programming;manycore processor;massively parallel processor array;mathematical optimization;microsoft outlook for mac;multi-core processor;online and offline;optimization problem;pipeline (computing);power domains;reduction (complexity);regular language description for xml;round-robin scheduling;run time (program lifecycle phase);scheduling (computing);solver;source code control system;synthetic intelligence;throughput	Christoph W. Kessler;Nicolas Melot;Patrick Eitschberger;Jörg Keller	2013	2013 23rd International Workshop on Power and Timing Modeling, Optimization and Simulation (PATMOS)	10.1109/PATMOS.2013.6662176	multi-core processor;fixed-priority pre-emptive scheduling;embedded system;parallel computing;real-time computing;resource allocation;computer science;operating system;distributed computing;scheduling	Embedded	-0.4955396167814908	52.42133835418173	177453
e45fcfab39dc68be469474e641b2e8043baadc8e	an algorithm for mapping loops onto coarse-grained reconfigurable architectures	algorithm design;place and route;memory bandwidth;design process	With the increasing demand for flexible yet highly efficient architecture platforms for media applications, there is a growing interest in the Coarse-grained Reconfigurable Architectures (CRAs). While many CRAs have demonstrated impressive performance improvement, the lack of compilation technology for such architectures causes a bottleneck in the current design process. In this paper, we present a novel mapping algorithm designed to support Reconfigurable ALU Array (RAA) architectures, that represent a significant class of CRAs. More specifically we present a core mapping algorithm that addresses the problem of placing and routing the operations of a loop body onto the ALU array, to be executed in a loop pipelined fashion. Experimental results using our mapping algorithm on a typical RAA show that our algorithm not only has very fast compilation time but can also generate quality mappings exhibiting high memory bandwidth utilization and low global interconnection requirements. Comparison with manual mapping also indicates that our algorithm can generate near-optimal mappings for several loops.	algorithm;arithmetic logic unit;compiler;high memory;interconnection;memory bandwidth;reconfigurable computing;requirement;routing	Jongeun Lee;Kiyoung Choi;Nikil D. Dutt	2003		10.1145/780732.780758	embedded system;algorithm design;routing;parallel computing;real-time computing;design process;computer science;place and route;memory bandwidth;bandwidth	EDA	0.3429381737887061	50.47160569849433	177850
f9025cbdc697f4edf01d8dc82f61f2ac8b4d6db3	exposing implementation details of embedded dram memory controllers through latency-based analysis		We explore techniques to reverse-engineer DRAM embedded memory controllers (MCs), including page policies, address mapping, and command arbitration. There are several benefits to knowing this information: They allow tightening worst-case bounds of embedded systems and platform-aware optimizations at the operating system, source-code, and compiler levels. We develop a latency-based analysis, which we use to devise algorithms and C programs to extract MC properties. We show the effectiveness of the proposed approach by reverse-engineering the MC details in the XUPV5-LX110T Xilinx platform. Furthermore, to cover a breadth of policies, we use a simulation framework and document our findings.	algorithm;apothecaries dram mass unit;best, worst and average case;compiler;controllers;dynamic random-access memory;edram;embedded system;inference;memory controller;memory timings;operating system;policy;programming tool;reverse engineering;rule (guideline);simulation;benefit	Mouyed Khudhair Hassan;Anirudh Kaushik;Hiren Patel	2018	ACM Trans. Embedded Comput. Syst.	10.1145/3274281	embedded system;latency (engineering);compiler;parallel computing;reverse engineering;inference;arbitration;computer science;dram	Embedded	-1.6905968445068456	52.740392059735946	178641
b1494e66bc5f1af62e5059dda66abbcba412676c	automatic synthesis and scheduling of multirate dsp algorithms	computational unit latency;automatic scheduling;timing problem;retiming technique;scheduling;multirate dsp ip core;dsp ip core;multirate dsp;polyphase iir idct;high-level synthesis system;multirate dsp algorithm;digital signal processing chips;automatic synthesis;multirate dsp architecture;folding transformation;physical implementation;high level synthesis;automatic synthesis methodology;process variation;reliability	To date, most high-level synthesis systems do not automatically solve present design problems, such as those related to timing associated with the physical implementation of multirate DSP architectures. Whilst others do not trade off area/speed of algorithm efficiently for such architectures. An automatic synthesis methodology based on both retiming techniques together with folding transformations is presented in this paper in order to solve timing problems associated with the implementation of multirate DSP algorithms. We demonstrate that techniques for modeling computational unit latencies, which can influence parameterisations of a multirate DSP IP core, can lead to highly efficient solutions. This is illustrated using a polyphase IIR IDCT example. Using the folding transformation, the control circuit for a hardware sharing multirate DSP is also presented.	algorithm;digital signal processor;scheduling (computing)	Ying Yi;Mark Milward;Sami Khawam;Ioannis Nousias;Tughrul Arslan	2005		10.1109/ASPDAC.2005.1466241	embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system;reliability;high-level synthesis;process variation;scheduling	EDA	1.8483902051206493	52.67530707221911	178798
7f75a3620680664b05c8ba3c94c201fd572c2e68	hpc benchmarking: problem size matters	libraries;software;bandwidth;benchmark testing;multiprocessor interconnection;hardware	In order to compare and rank the worlds fastest computers, benchmarks evaluating their performance are required. A single execution of HPL is used for the most widely recognized ranking: the TOP500. Lately, two benchmarks, arguably more representative of typical modern workloads, have been proposed: HPCG and HPGMG. Currently, all three benchmarks use the highest observed performance from a single problem size for ranking. In this paper we report benchmarking result for all three benchmarks with a wide range of problem sizes on six distinct hardware architectures, covering the full range of machines present on the TOP500 list. We find that the data holds significantly more information on the performance of the underlying hardware as compared to just the maximum performance observed. We therefore argue that an aggregate value derived from a whole range of problem sizes can significantly improve the sensitivity of a given benchmark to relevant hardware properties and thus be more representative. However, we refrain from proposing the specific way to best compose such an aggregate and invite the community to open the discussion on the topic.	ace;aggregate data;analysis of algorithms;benchmark (computing);central processing unit;computer;data center;fastest;general-purpose computing on graphics processing units;hpcg benchmark;in the beginning... was the command line;information;interconnection;linpack benchmarks;multi-core processor;nec sx-ace;obfuscation (software);parallax sx;relevance;rendering (computer graphics);supercomputer;top500;vector processor;xeon phi	Vladimir Marjanovic;José Gracia;Colin W. Glass	2016	2016 7th International Workshop on Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)	10.1109/PMBS.2016.6	benchmark;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;bandwidth	Arch	-4.211419675702499	46.51270713761943	179975
47f214a0ef6b6b5d629554405d9576340a002c95	an overview of high-level synthesis of multiprocessors for logic programming	machine learning;high level synthesis;logic design;field programmable gate arrays;application development;logic programming;cognitive robotics;multiprocessing	This paper introduces a language and framework for designing multiprocessor architectures in the logic programming domain. Our goal is to enable application developers in areas such as machine learning and cognitive robotics to produce high-performance designs without detailed knowledge of hardware development. This framework provides a high level of abstraction, enabling rapid system generation and design space exploration while supporting high performance. We present an overview of the Archlog language and its library-based compilation framework, which makes use of a customisable logic programming processor. An example implementation of a multiprocessor for the machine learning system Progol on a 35MHz XC2V6000 FPGA achieves 43 times faster execution than a 2GHz Pentium 4 processor	cognitive robotics;compiler;design space exploration;field-programmable gate array;high- and low-level;high-level programming language;high-level synthesis;logic programming;machine learning;multiprocessing;progol;pentium 4;programming domain;system generation	Andreas Fidjeland;Wayne Luk	2005	Proceedings. 2005 IEEE International Conference on Field-Programmable Technology, 2005.		embedded system;computer architecture;parallel computing;logic synthesis;real-time computing;multiprocessing;programming domain;computer science;operating system;functional logic programming;high-level synthesis;rapid application development;logic programming;field-programmable gate array;cognitive robotics	EDA	2.112712487167721	49.658805196135084	180163
d85400dd240107317dad433afab03113f99486ef	using the multi-threaded computation model as a unifying framework for hardware-software co-design and implementation	multi threading;hardware software codesign;hardware software co design;fpga components;fpga based component threads;field programmable gate arrays instruction sets hardware programming biological system modeling computational modeling object oriented modeling;computer model;biological system modeling;chip;programming model;hybrid cpu fpga chips;hybrid chips;embedded systems;computational modeling;distributed real time and embedded system;multithreaded computation model;system design;multithreaded programming model;time to market;field programmable gate arrays;economies of scale;performance ratio;programming;cpu components;integrated interface;object oriented modeling;cpu based component threads;instruction sets;hardware;dre system	The range of distributed real-time and embedded (DRE) system applications has continued to expand at a vigorous rate. Designers of DRE systems are constantly challenged to provide new capabilities to meet expanding requirements and increased computational needs, while under pressure to provide constantly improving price/performance ratios and shorter times to market. Recently emerging hybrid chips containing both CPUs and FPGA components have the potential to enjoy significant economies of scale, while enabling system designers to include a significant amount of specialization within the FPGA component. However, realizing the promise of these new hybrid CPU/FPGA chips will require programming models supporting a far more integrated view of CPU and FPGA based application components than that provided by current methods. This paper describes methods we are developing for supporting a multithreaded programming model providing strongly integrated interface to CPU and FPGA based component threads.	application programming interface;central processing unit;component-based software engineering;embedded system;field-programmable gate array;integrated development environment;model of computation;partial template specialization;programming model;real-time clock;requirement;systems design;thread (computing)	Douglas Niehaus;David L. Andrews	2003	2003 The Ninth IEEE International Workshop on Object-Oriented Real-Time Dependable Systems	10.1109/WORDS.2003.1267546	computer architecture;parallel computing;real-time computing;computer science	Embedded	-1.4012322326206605	48.63146116119045	181307
6c419848bfb94f466fc8f9b558a8f65994031e21	predicate-aware scheduling: a technique for reducing resource constraints	predicate-aware scheduling;predicated execution;predicate aware scheduling;distinct resource;overall effect;productive use;central idea;resource constraint;conditional operation;side effect;instruction scheduling;registers;parallel processing;computer architecture;vliw;parallel programming;multiplication operator;resource management;software pipelining;compiler;resource utilization	Predicated execution enables the removal of branches wherein segments of branching code are converted into straight-line segments of conditional operations. An important, but generally ignored side effect of this transformation is that the compiler must assign distinct resources to all the predicated operations at a given time to ensure that those resources are available at run-time. However, a resource is only put to productive use when the predicates associated with its operations evaluate to True. We propose predicate-aware scheduling to reduce the superfluous commitment of resources to operations whose predicates evaluate to False at run-time. The central idea is to assign multiple operations to the same resource at the same time, thereby oversubscribing its use. This assignment is intelligently performed to ensure that no two operations simultaneously assigned to the same resource will have both of their predicates evaluate to True. Thus, no resource is dynamically oversubscribed. The overall effect of predicate aware scheduling is to use resources more efficiently, thereby increasing performance when resource constraints are a bottleneck.	best, worst and average case;central processing unit;compiler;control flow;directed acyclic graph;modulo operation;overselling;predicate (mathematical logic);scheduling (computing);side effect (computer science);very long instruction word	Mikhail Smelyanskiy;Scott A. Mahlke;Edward S. Davidson;Hsien-Hsin S. Lee	2003			software pipelining;multiplication operator;parallel processing;computer architecture;in situ resource utilization;compiler;parallel computing;real-time computing;computer science;very long instruction word;resource management;operating system;processor register;instruction scheduling;programming language;side effect	Arch	-4.163671439177264	52.140576045621266	181654
5c9723865e1845ee054a1b497ddf8ae802c2f29f	generating high-performance custom floating-point pipelines	digital signal processing;generators;logic design;learning curve;logic design field programmable gate arrays floating point arithmetic hardware description languages;automatic testing;pipelines field programmable gate arrays open source software computer architecture frequency synthesizers automatic testing detectors digital signal processing logic delay;hardware description languages;floating point pipeline;fpga;computer arithmetic;frequency directed pipeline;adders;vhdl;pipelines;automatic test bench generation;high performance computer;architecture generator;floating point;open source architecture generator;floating point arithmetic;field programmable gate arrays;magnetic cores;high performance;off the shelf;automatic test bench generation floating point pipeline fpga open source architecture generator vhdl frequency directed pipeline;pipeline;pipeline processing;open source	Custom operators, working at custom precisions, are a key ingredient to fully exploit the FPGA flexibility advantage for high-performance computing. Unfortunately, such operators are costly to design, and application designers tend to rely on less efficient off-the-shelf operators. To address this issue, an open-source architecture generator framework is introduced. Its salient features are an easy learning curve from VHDL, the ability to embed arbitrary synthesizable VHDL code, portability to mainstream FPGA targets from Xilinx and Altera, automatic management of complex pipelines with support for frequency-directed pipeline, and automatic test-bench generation. This generator is presented around the simple example of a collision detector, which it significantly improves in accuracy, DSP count, logic usage, frequency and latency with respect to an implementation using standard floating-point operators.	field-programmable gate array;open-source software;pipeline (computing);software portability;supercomputer;vhdl	Florent de Dinechin;Cristian Klein;Bogdan Pasca	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272553	embedded system;computer architecture;parallel computing;computer hardware;computer science;floating point;operating system;field-programmable gate array	EDA	2.1683318251458332	48.92610112943126	181725
69e78f14b893f50d6850db10a63f395ac9042206	physical 2d morphware and power reduction methods for everyone	processing element;004;partial reconfiguration;2d online placement and routing reconfigurable computing;reconfigurable architecture;power reduction;power consumption;parallel processing	Dynamic and partial reconfiguration discovers more and more the focus in academic and industrial research. Modern systems in e.g. avionic and automotive applications exploit the parallelism of hardware in order to reduce power consumption and to increase performance. State of the art reconfigurable FPGA devices allows reconfiguring parts of their architecture while the other configured architecture stays undisturbed in operation. This dynamic and partial reconfiguration allows therefore adapting the architecture to the requirements of the application while run-time. The difference to the traditional term of software and its related sequential architecture is the possibility to change the paradigm of brining the data to the respective processing elements. Dynamic and partial reconfiguration enables to bring the processing elements to the data and is therefore a new paradigm. The shift from the traditional microprocessor approaches with sequential processing of data to parallel processing reconfigurable architectures forces to introduce new paradigms with the focus on computing in time and space.	field-programmable gate array;microprocessor;parallel computing;programming paradigm;requirement	Jürgen Becker;Michael Hübner;Katarina Paulsson	2006			embedded system;parallel computing;real-time computing;computer science	Arch	-1.3082275654006095	49.735116611905404	181748
0d174b5974811bf4a87f16c33aaa8a4bd9dfe24a	reuse-aware modulo scheduling for stream processors	reuse equation;scheduling;reuse-aware modulo scheduling;representative stream application;concurrency improvement;stream reuse maximization;multiprocessing systems;stream reuse;new representation;stream processor;multiprocessing programs;performance objective;media streaming;stream level loop;stream-level loop;64-bit ft64 stream processor;software-pipelined stream-level loop;simultaneous optimization;software pipelined stream level loop;reuse aware modulo scheduling;memetic algorithm;monte carlo;distributed processing;software pipelining;concurrent computing;schedules;kernel;process variation;strontium	This paper presents reuse-aware modulo scheduling to maximizing stream reuse and improving concurrency for stream-level loops running on stream processors. The novelty lies in the development of a new representation for an unrolled and software-pipelined stream-level loop using a set of reuse equations, resulting in simultaneous optimization of two performance objectives for the loop, reuse and concurrency, in a unified framework. We have implemented this work in the compiler developed for our 64-bit FT64 stream processor. Our experimental results obtained on FT64 and by simulation using nine representative stream applications demonstrate the effectiveness of the proposed approach.	64-bit computing;cell (microprocessor);central processing unit;compiler;concurrency (computer science);imagine (3d modeling software);loop unrolling;mathematical optimization;modulo operation;rams;scheduling (computing);simulation;stream processing;unified framework	Li Wang;Jingling Xue;Xuejun Yang	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		software pipelining;computer architecture;parallel computing;kernel;real-time computing;stream processing;strontium;concurrent computing;schedule;computer science;operating system;process variation;scheduling;memetic algorithm;monte carlo method	EDA	-1.2148335218771225	51.68794598779436	181966
adf401fd9fc065c9a8afc2cb095a5187bf22c70f	a framework for fast and fair evaluation of automata processing hardware		Programming Micron's Automata Processor (AP) requires expertise in both automata theory and the AP architecture, as programmers have to manually manipulate state transition elements (STEs) and their transitions with a low-level Automata Network Markup Language (ANML). When the required STEs of an application exceed the hardware capacity, multiple reconfigurations are needed. However, most previous AP-based designs limit the dataset size to fit into a single AP board and simply neglect the costly overhead of reconfiguration. This results in unfair performance comparisons between the AP and other processors. To address this issue, we propose a framework for the fast and fair evaluation of AP devices. Our framework provides a hierarchical approach that automatically generates automata for large datasets through user-defined paradigms and allows the use of cascadable macros to achieve highly optimized reconfigurations. We highlight the importance of counting the configuration time in the overall AP performance, which in turn, can provide better insight into identifying essential hardware features, specifically for large-scale problem sizes. Our framework shows that the AP can achieve up to 461x overall speedup fairly compared to CPU counterparts.	amdahl's law;automata theory;central processing unit;high- and low-level;markup language;overhead (computing);programmer;speedup;state transition table	Xiaodong Yu;Kaixi Hou;Hao Wang;Wu-chun Feng	2017	2017 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2017.8167767	control reconfiguration;architecture;real-time computing;speedup;macro;computer science;pattern matching;automata theory;computer hardware	Arch	-4.160550217260933	46.46322650752477	182436
4e1611b35bc5b6e7a9aa5e185ba9281af1ec5143	high-level modeling and simulation of single-chip programmable heterogeneous multiprocessors	computer aided design;system modeling;modeling and simulation;schedulers;network processor;design space;chip;instruction set simulator;heterogeneous multiprocessors;performance model;hardware design;performance modeling;model simulation;discrete event simulation	Heterogeneous multiprocessing is the future of chip design with the potential for tens to hundreds of programmable elements on single chips within the next several years. These chips will have heterogeneous, programmable hardware elements that lead to different execution times for the same software executing on different resources as well as a mix of desktop-style and embedded-style software. They will also have a layer of programming across multiple programmable elements forming the basis of a new kind of programmable system which we refer to as a Programmable Heterogeneous Multiprocessor (PHM). Current modeling approaches use instruction set simulation for performance modeling, but this will become far too prohibitive in terms of simulation time for these larger designs. The fundamental question is what the next higher level of design will be. The high-level modeling, simulation and design required for these programmable systems poses unique challenges, representing a break from traditional hardware design. Programmable systems, including layered concurrent software executing via schedulers on concurrent hardware, are not characterizable with traditional component-based hierarchical composition approaches, including discrete event simulation. We describe the foundations of our layered approach to modeling and performance simulation of PHMs, showing an example design space of a network processor explored using our simulation approach.	complex systems;component-based software engineering;defense in depth (computing);desktop computer;electronic design automation;embedded system;hlh orion;high- and low-level;instruction set simulator;interaction;multiprocessing;network processor;performance prediction;programmable logic device;scheduling (computing);simulation;software design;system on a chip	JoAnn M. Paul;Donald E. Thomas;Andrew S. Cassidy	2005	ACM Trans. Design Autom. Electr. Syst.	10.1145/1080334.1080335	chip;embedded system;computer architecture;parallel computing;real-time computing;systems modeling;computer science;discrete event simulation;operating system;modeling and simulation;network processor	EDA	-1.6087856294436969	48.26584744585348	182816
3057f539eef10a6f4ca4d214407b58e5ad600abd	high-level synthesis of hw tasks targeting run-time reconfigurable fpgas	libraries;optimisation;reconfigurable system;resource allocation;reconfigurable architectures;data flow graphs;hardware tasks;resource management;run time reconfigurable fpga architecture;control flow graph;runtime;design optimization;flow graphs;data flow graph;computer architecture;high level synthesis;run time reconfigurable;levels of abstraction;resource sharing;fpga architecture;ubiquitous computing;optimization;high level synthesis runtime field programmable gate arrays libraries flow graphs resource management design optimization computer architecture ubiquitous computing mobile computing;field programmable gate arrays;resource allocation data flow graphs field programmable gate arrays high level synthesis optimisation reconfigurable architectures;field programmable gate arrays high level synthesis hardware tasks run time reconfigurable fpga architecture optimization control flow graph data flow graph resource sharing register transfer level;mobile computing;register transfer level	This paper presents a novel high-level synthesis (HLS) and optimization approach targeting FPGA architectures that are reconfigurable at run-time. To model a reconfigurable system on a high level of abstraction, we use a hierarchical operation (control and data) flow graph. In order to reduce the overhead for reconfiguring the system, we apply resource sharing to our model to deduce reusable design parts for the implementation. A case study compares our HLS approach with a reference design which was manually coded on register-transfer-level (RTL).	field-programmable gate array;high- and low-level;high-level programming language;high-level synthesis;mathematical optimization;overhead (computing);reconfigurable computing;reference design;register-transfer level	Maik Boden;Thomas Fiebig;Torsten Meibner;Steffen Rülke;Jürgen Becker	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370390	shared resource;computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;resource allocation;computer science;resource management;operating system;data-flow analysis;high-level synthesis;programming language;mobile computing;ubiquitous computing;register-transfer level;field-programmable gate array;control flow graph	EDA	1.2807908763388571	52.159265400926536	182962
6ad7d7329c35d42a33a8cea2840400d15a848cbb	combining multicore and reconfigurable instruction set extensions	multi core processor;reconfigurable instruction set extensions;instruction set extension;efficient implementation;embedded;extensible microprocessors;multi core	The shift to multi-core processors presents a number of opportunities and challenges to different research fields, including the field of FPGA applications. This paper investigates the advantages of combining multi-core processors and reconfigurable instruction set extensions. Both our analysis and the experimental results show that these two approaches exploit different levels of parallelism. Using a case study on the Floyd-Warshall algorithm, we demonstrate that the multi-core architecture and the reconfigurable instruction set extensions complement each other. By combining these two methods together we find a win-win solution, which gives us a more efficient implementation with higher performance.	central processing unit;field-programmable gate array;floyd–warshall algorithm;intel core (microarchitecture);multi-core processor;parallel computing	Zhimin Chen;Richard Neil Pittman;Alessandro Forin	2010		10.1145/1723112.1723119	multi-core processor;embedded system;computer architecture;parallel computing;real-time computing;application-specific instruction-set processor;computer science;operating system;instruction set	EDA	0.042855411888045286	49.15889753834878	183445
40364061280208050573e25c48b7e12e5af40752	the hf-risc processor: performance assessment	reduced instruction set computing embedded systems instruction sets multiprocessing systems performance evaluation pipeline processing;performance evaluation;clocks;embedded design hf risc processor performance assessment 32 bit risc processor programming toolchain instruction set architecture mips i pipeline stages coremark industry standard performance evaluation benchmark processor performance reconfigurable hardware organization arm processors;computer architecture;registers;pipelines;pipelines hardware organizations registers clocks computer architecture performance evaluation;organizations;hardware;embedded systems risc mips fd soi design space exploration	This paper presents HF-RISC, a 32-bit RISC processor, along with its associated programming toolchain. The instruction set architecture of the processor is based on MIPS I and its hardware organization comprises three pipeline stages. The processor was synthesized in four different technology nodes for maximum frequency and simulated using CoreMark, an industry-standard performance evaluation benchmark. Using data obtained from synthesis and benchmarking we analyze the processor performance and compare it to similar commercial products. Obtained results indicate that HF-RISC is a good option for embedded design, as it presents performance figures similar to state-of-the-art ARM processors. Furthermore, its partially reconfigurable hardware organization allows the designer to explore performance and area trade offs.	32-bit;arm architecture;benchmark (computing);central processing unit;coremark;embedded system;field-programmable gate array;helicon filter;performance evaluation;reconfigurable computing;toolchain	Sergio Johann Filho;Matheus T. Moreira;Ney Laert Vilar Calazans;Fabiano Hessel	2016	2016 IEEE 7th Latin American Symposium on Circuits & Systems (LASCAS)	10.1109/LASCAS.2016.7451018	classic risc pipeline;reduced instruction set computing;computer architecture;parallel computing;real-time computing;application-specific instruction-set processor;processor design;computer science;instruction set;processor register	Arch	2.519122701537357	50.02341497072385	183509
4a4befc4ddae74ae2761399ca18b8e0c2d25a31e	architecture and compiler tradeoffs for a long instruction word microprocessor	vliw processor;optimizing compiler;multiplication operator;chip;compiler optimization;very long instruction word;floating point	A very long instruction word (VLIW) processor exploits parallelism by controlling multiple operations in a single instruction word. This paper describes the architecture and compiler tradeoffs in the design of iWarp, a VLIW single-chip microprocessor developed in a joint project with Intel Corp. The iWarp processor is capable of specifying up to nine operations in an instruction word and has a peak performance of 20 million floating-point operations and 20 million integer operations per second. An optimizing compiler has been constructed and used as a tool to evaluate the different architectural proposals in the development of iWarp. We present here the analysis and compiler optimizations for those architectural features that address two key issues in the design of a VLIW microprocessor: code density and a streamlined execution cycle. We support the results of our analysis with performance data for the Livermore Loops and a selection of programs from the LINPACK library.	flops;livermore loops;microprocessor;parallel computing;very long instruction word	Robert S. Cohn;Thomas R. Gross;Monica S. Lam;P. S. Tseng	1989		10.1145/70082.68183	computer architecture;parallel computing;real-time computing;compiler correctness;computer science;operating system;optimizing compiler;programming language;functional compiler	Arch	-0.5899717779107347	51.08225377037836	183643
10106c50b44167222f11cc6e30d9088c1294cd9f	openhmc - a configurable open-source hybrid memory cube controller	protocols;random access memory;clocks;rtl fpga hybrid memory cube openhmc;protocols clocks radio frequency open source software computer architecture field programmable gate arrays random access memory;computer architecture;radio frequency;dram openhmc configurable open source hybrid memory cube controller processor parallel buses computer systems memory wall ddrx memory interfaces serial hmc host interface packetized hmc host interface vendor agnostic open source implementation hmc host controller datapath widths hmc link variations in system verification xilinx ultrascale vu095 fpga field programmable gate array;formal verification dram chips field programmable gate arrays;field programmable gate arrays;open source software	The link between the processor and memory is one of the last remaining parallel buses and a major performance bottleneck in computer systems. The Hybrid Memory Cube (HMC) was developed with the goal of helping overcome this 'memory wall'. In contrast to DDRx memory interfaces, the HMC host interface is serial and packetized. This paper presents a vendor-agnostic, open-source implementation of an HMC host controller that can be configured for different datapath widths and HMC link variations. Due to its modular design, the controller can be integrated in many different system environments. In-system verification was performed using a Xilinx Ultrascale VU095 FPGA. Overall, the presented controller is a mature, freely available solution for experimenting with the HMC and evaluating its capabilities.	datapath;download;experiment;field-programmable gate array;host adapter;hybrid memory cube;mathematical optimization;modular design;open-source software;pointer (computer programming);random-access memory;retry;user interface;vii	Juri Schmidt;Ulrich Brüning	2015	2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2015.7393331	embedded system;communications protocol;interleaved memory;parallel computing;computer hardware;computer science;operating system;computer memory;memory controller;registered memory;radio frequency;field-programmable gate array	EDA	0.9462660685992763	48.37008762271561	183903
09b8ae3812aecfd3892da13cdfcb3122300713d9	speedups in embedded systems with a high-performance coprocessor datapath	kernels;design flow;chaining;coprocessor datapath;embedded system;synthesis;chip;performance improvement;performance improvements;high performance	This article presents the speedups achieved in a generic single-chip microprocessor system by employing a high-performance datapath. The datapath acts as a coprocessor that accelerates computational-intensive kernel sections thereby increasing the overall performance. We have previously introduced the datapath which is composed of Flexible Computational Components (FCCs). These components can realize any two-level template of primitive operations. The automated coprocessor synthesis method from high-level software description and its integration to a design flow for executing applications on the system is presented. For evaluating the effectiveness of our coprocessor approach, analytical study in respect to the type of the custom datapath and to the microprocessor architecture is performed. The overall application speedups of several real-life applications relative to the software execution on the microprocessor are estimated using the design flow. These speedups range from 1.75 to 5.84, with an average value of 3.04, while the overhead in circuit area is small. The design flow achieved the acceleration of the applications near to theoretical speedup bounds. A comparison with another high-performance datapath showed that the proposed coprocessor achieves smaller area-time products by an average of 23% for the generated datapaths. Additionally, the FCC coprocessor achieves better performance in accelerating kernels relative to software-programmable DSP cores.	computation;coprocessor;datapath;design flow (eda);digital signal processor;embedded system;high- and low-level;microprocessor;overhead (computing);real life;speedup	Michalis D. Galanis;Grigoris Dimitroulakos;Spyros Tragoudas;Constantinos E. Goutis	2007	ACM Trans. Design Autom. Electr. Syst.	10.1145/1255456.1255472	chip;embedded system;computer architecture;finite state machine with datapath;parallel computing;real-time computing;telecommunications;computer science;design flow;chaining;operating system	EDA	2.372433252438845	49.96527477155318	184289
026e029ff46ea0ebbeb745a89a25afdeba0ac02b	accuracy constraint determination in fixed-point system design	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices;fixed point;system design	Most of digital signal processing applications are specified and designed with floatingpoint arithmetic but are finally implemented using fixed-point architectures. Thus, the design flow requires a floating-point to fixed-point conversion stage which optimizes the implementation cost under execution time and accuracy constraints. This accuracy constraint is linked to the application performances and the determination of this constraint is one of the key issues of the conversion process. In this paper, a method is proposed to determine the accuracy constraint from the application performance. The fixed-point system is modeled with an infinite precision version of the system and a single noise source located at the system output. Then, an iterative approach for optimizing the fixed-point specification under the application performance constraint is defined and detailed. Finally the efficiency of our approach is demonstrated by experiments on an MP3 encoder.	design flow (eda);digital signal processing;encoder;experiment;fixed point (mathematics);fixed-point arithmetic;iterative method;mp3;noise generator;performance;run time (program lifecycle phase)	Daniel Ménard;Romain Serizel;Romuald Rocher;Olivier Sentieys	2008	EURASIP J. Emb. Sys.	10.1155/2008/242584	embedded system;real-time computing;computer science;fixed point;systems design	EDA	2.740101390089077	52.77582985868064	184551
ebf7cbcc225730f1d3314871f35dd6956d19e631	ideal: image denoising accelerator		Computational imaging pipelines (CIPs) convert the raw output of imaging sensors into the high-quality images that are used for further processing. This work studies how Block-Matching and 3D filtering (BM3D), a state-of-the-art denoising algorithm can be implemented to meet the demands of user-interactive (UI) applications. Denoising is the most computationally demanding stage of a CIP taking more than 95% of time on a highly-optimized software implementation [29]. We analyze the performance and energy consumption of optimized software implementations on three commodity platforms and find that their performance is inadequate.Accordingly, we consider two alternatives: a dedicated accelerator, and running recently proposed Neural Network (NN) based approximations of BM3D [9, 27] on an NN accelerator. We develop Image DEnoising AcceLerator(IDEAL), a hardware BM3D accelerator which incorporates the following techniques: 1) a novel software-hardware optimization, Matches Reuse (MR), that exploits typical image content to reduce the computations needed by BM3D, 2) pre-fetching and judicious use of on-chip buffering to minimize execution stalls and off-chip bandwidth consumption, 3) a careful arrangement of specialized computing blocks, and 4) data type precision tuning. Over a dataset of images with resolutions ranging from 8 megapixel (MP) and up to 42MP, IDEAL is 11, 352× and 591× faster than high-end general-purpose (CPU) and graphics processor (GPU) software implementations with orders of magnitude better energy effciency. Even when the NN approximations of BM3D are run on the DaDianNao [14] high-end hardware NN accelerator, IDEAL is 5.4× faster and 3.95× more energy effcient. CCS CONCEPTS • Computer systems organization → Special purpose systems; Neural networks; Real-time system architecture; Single instruction, multiple data;	algorithm;approximation;artificial neural network;cpu cache;central processing unit;computation;general-purpose macro processor;graphics processing unit;mathematical optimization;noise reduction;pipeline (computing);pixel;sensor;user interface	Mostafa Mahmoud;Bojian Zheng;Alberto Delmas Lascorz;Felix Heide;Jonathan Assouline;Paul Boucher;Emmanuel Onzon;Andreas Moshovos	2017		10.1145/3123939.3123941	data type;real-time computing;computer science;filter (signal processing);parallel computing;artificial neural network;computational photography;software;graphics;energy consumption;systems architecture	Arch	2.596120855915043	46.471306473672065	184729
1d866b553b20f88a6fe5e0d306a4a90d57d17f87	ibm power8 performance features and evaluation	servers monitoring radiation detectors program processors high performance computing system on chip encryption	The IBM POWER8™ processor was designed for high performance on traditional server workloads as well as big data, analytics, and cloud workloads. In this paper, we describe key performance features of the IBM POWER8 processor. These include hardware assists that allow the POWER8 processor to automatically adapt to changing workloads by dynamically monitoring and tuning itself, enhancements to hardware instrumentation for performance monitoring, and performance improvements for encryption, virtualization, and I/O. We also describe the performance characteristics of a wide variety of applications, and we present the results of these applications running on POWER8 processor-based systems compared with previous generations of IBM Power Systems™.		Alex E. Mericas;N. Peleg;Lorena Pesantez;S. B. Purushotham;P. Oehler;Carl A Anderson;B. A. King-Smith;M. Anand;J. A. Arnold;B. Rogers;L. Maurice;K. Vu	2015	IBM Journal of Research and Development	10.1147/JRD.2014.2380197	embedded system;parallel computing;computer science;operating system;ibm san volume controller	HPC	-4.527283799127019	53.18894777745829	184973
50791056abad404fb53cba4190caf1174ae60ec9	evaluation of the stretch s6 hybrid reconfigurable embedded cpu architecture for power-efficient scientific computing	reconfigurable hardware;scientific computing;computer architecture;floating point arithmetic	Embedded CPUs typically use much less power than desktop or server CPUs but provide limited or no support for floating-point arithmetic. Hybrid reconfigurable CPUs combine fixed and reconfigurable computing fabrics to balance better execution performance and power consumption. We show how a Stretch S6 hybrid reconfigurable CPU (S6) can be extended to natively support double precision floating-point arithmetic. For lower precision number formats, multiple parallel arithmetic units can be implemented. We evaluate if the superlinear performance improvement of floating-point multiplication on reconfigurable fabrics can be exploited in the framework of a hybrid reconfigurable CPU. We provide an in-depth investigation of data paths to and from the S6 reconfigurable fabric and present peak and sustained throughput as a function of wide registers used and total operand size. We demonstrate the effect of the given interface when using a floating-point fused multiply-accumulate (FMA) SIMD unit to accelerate the LINPACK benchmark. We identify a mismatch between the size of the S6’s reconfigurable fabric and the available interface bandwidth as the major bottleneck limiting performance which makes it a poor choice for scientific workloads relying on native support for floating-point arithmetic.	128-bit;benchmark (computing);central processing unit;clock rate;clock signal;computational science;coupling (computer programming);desktop computer;double-precision floating-point format;embedded system;fma instruction set;hybrid kernel;ibm 7030 stretch;input/output;lunpack;microsoft outlook for mac;multiply–accumulate operation;operand;parallel computing;processor design;reconfigurable computing;routing;server (computing);signal processing;streaming simd extensions;throughput	Thang Viet Huynh;Manfred Mücke;Wilfried N. Gansterer	2012		10.1016/j.procs.2012.04.021	computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-2.724916200568027	47.22759946194234	185131
3e5737f1a3362618fbf82cd968797cbd873a469e	fpga-based multi-core reconfigurable system for sar imaging		With the development of the very large scale integration circuit (VLSI), multi-core processors have been developing fast and provide a novel approach to meet the requirements of modern complex computing. Real-time SAR imaging is always a hard problem to deal with since the huge amount of data and complex processing. Therefore, we design a schedulable and scalable multi-core parallel architecture based on FPGA and map the fundamental Chirp Scaling algorithm to the system. The design of the master control core makes the system highly extensible and the dedicated computing cores are designed to accelerate the process. In addition, the system can meet the real-time requirements, and it performs well in the resource utilization and the FPGA chip takes up a small space as well.	algorithm;central processing unit;chirp;field-programmable gate array;integrated circuit;master control;multi-core processor;parallel computing;real-time clock;real-time transcription;reconfigurable computing;requirement;scalability;very-large-scale integration	Wei Di;Changlin Chen;Yongxiang Liu	2018	IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2018.8518256	field-programmable gate array;real-time computing;very-large-scale integration;computer vision;chip;process control;artificial intelligence;scalability;architecture;synthetic aperture radar;multi-core processor;computer science	Embedded	2.152699693770198	47.3012144932257	185914
36fa031fc06af12ee1692cef2727fd1200106aa5	hardware description language based on message passing and implicit pipelining	software;optimisation;generators;high level languages;hardware description languages;semantics;high level software language;optimization hardware description language message passing implicit pipelining high level software language vhdl hascol;implicit pipelining;hascol;hardware software registers field programmable gate arrays generators semantics message passing;registers;vhdl;message passing;pipeline processing hardware description languages high level languages message passing optimisation;place and route;optimization;field programmable gate arrays;hardware description language;pipeline processing;hardware	We present a hardware description language (currently called “HaSCoL”) which is based on both reliable and unreliable message passing and implicit pipelining of message handlers. The language consists of a small core and a number of extensions, which cover many features of high level software languages as well as high level hardware description languages (HDLs). These extensions have simple projections into the core language and allow compact and concise description of complex algorithms. The core language in turn can be converted into efficient VHDL. We discuss place-and-route results for some benchmarks implemented both in HaSCoL and VHDL and suggest an optimization which should improve the results significantly and make them close to those for hand-coded VHDL.	algorithm;hardware description language;high-level programming language;mathematical optimization;message passing;pipeline (computing);place and route;vhdl	Dmitri Boulytchev;Oleg Medvedev	2010	2010 East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2010.5742095	computer architecture;parallel computing;vhdl;computer science;programming language	Arch	1.7788747257195556	50.45537453852724	186073
9735947d45813a5d92e953cc8a4f1e37ebf6fd0d	a different view: hardware synthesis from systemc is a maturing technology	hardware synthesis;hardware description languages;hardware synthesis systemc;high level synthesis;c language;c like language;rtl code;high level synthesis c language hardware description languages;c like language hardware synthesis systemc rtl code;systemc;hardware	Commercial SystemC synthesis tools routinely produce more efficient hardware than handwritten RTL code typically produces. We argue that properties of C-like languages make this synthesis process computationally hard and time-consuming. Although some of the properties has cited do make synthesis more difficult, those problems have largely been solved. Fundamentally, the complexity imposed on these synthesis products results from starting at a higher abstraction level, not from the language	abstraction layer;systemc	John Sanguinetti	2006	IEEE Design & Test of Computers	10.1109/MDT.2006.111	computer architecture;parallel computing;computer science;hardware description language;high-level synthesis;programming language	EDA	2.368824650533153	50.39501704026732	187191
78cafefc830ac2f4c7f380329a461ea05f1442dd	fast placement and routing by extending coarse-grained reconfigurable arrays with omega networks	processing element;reconfigurable computing;multistage interconnection networks;dynamic compilation;multistage interconnection network;coarse grained reconfigurable arrays;placement and routing;embedded system;fpgas;artigo publicado em periodico;place and route;coarse grained;energy saving	Reconfigurable computing architectures are commonly used for accelerating applications and/or for achieving energy savings. However, most reconfigurable computing architectures suffer from computationally demanding placement and routing (P&R) steps. This problem may disable their use in systems requiring dynamic compilation (e.g., to guarantee application portability in embedded systems). Bearing in mind the simplification of P&R steps, this paper presents and analyzes a coarse-grained reconfigurable array (CGRA) extended with global multistage interconnect networks, specifically Omega Networks. We show that integrating one or two Omega Networks in a CGRA permits to simplify the P&R stage resulting in both low hardware resource overhead and low performance degradation (18% for an 8 8 array). We compare the proposed CGRA, which integrates one or two Omega Networks, with a CGRA based on a grid of processing elements with reach neighbor interconnections and with a torus topology. The execution time needed to perform the P&R stage for the two array architectures shows that the array using two Omega Networks needs a far simpler and faster P&R. The P&R stage in our approach completed on average in about 16 less time for the 17 benchmarks used. Similar fast approaches needed CGRAs with more complex interconnect resources in order to allow most of the benchmarks used to be successfully placed and routed. 2011 Elsevier B.V. All rights reserved.	benchmark (computing);bilateral filter;compiler;critical path method;data-flow analysis;dataflow;dynamic compilation;elegant degradation;embedded system;experiment;extended euclidean algorithm;hardware acceleration;international symposium on fundamentals of computation theory;joão pavão martins;level of detail;multistage amplifier;multistage interconnection networks;omega network;overhead (computing);pipeline (computing);place and route;polynomial;reconfigurable computing;retiming;routing;run time (program lifecycle phase);software portability	Ricardo S. Ferreira;João M. P. Cardoso;Alex Damiany;Julio C. Goldner Vendramini;Tiago Teixeira	2011	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2011.03.006	embedded system;parallel computing;real-time computing;dynamic compilation;reconfigurable computing;computer science;operating system;place and route;distributed computing;field-programmable gate array;computer network	EDA	-0.38460288491975636	50.11264350274312	187567
2b1099a61dbd42744fa5586ee6536174be5338de	performance analysis and design methodology for a scalable superscalar architecture	out of order;design method;performance analysis;hardware design;functional unit;instruction scheduling;dynamic scheduling;design methodology	This paper presents a scalable superscalar design method by using the Scheduling Pattern Algorithm. Out-of-order instruction scheduling is determined in advance by a set of scheduling patterns that are stored in a PLA. The scheduling pattern method is a new approach to dynamic scheduling in order to simplify hardware design. Also, it is a scalable design method which can resolve Functional Unit (FU) conflicts for any kind of FU configurations within the size (Ws) of the instruction window.	algorithm;central processing unit;execution unit;hardware random number generator;instruction scheduling;instruction window;kde applications;profiling (computer programming);programmable logic array;scalability;scheduling (computing);superscalar processor;very-large-scale integration;word-sense disambiguation	Takaaki Kato;Toshihisa Ono;Nader Bagherzadeh	1992		10.1145/144953.145820	fair-share scheduling;design methods;two-level scheduling	Arch	-0.8169278445008878	50.78395043479334	188080
91904e779123882f42986c1c76793b76b1bfebfa	modeling and performance evaluation of tso-preserving binary optimization	optimisation;memory management;performance evaluation;formal model;storage management;tso preserving optimization total store ordering binary optimization;memory consistency;program optimization;memory optimization;optimization instruction sets load modeling memory management heuristic algorithms;binary optimization;heuristic algorithms;tso preserving optimization;total store ordering;storage management multiprocessing systems optimisation performance evaluation;optimization;total store ordering tso preserving binary optimization program optimization multicore system formal tso memory model dynamic binary optimization;multiprocessing systems;load modeling;heuristic algorithm;instruction sets;memory model	Program optimization on multi-core systems must preserve the program memory consistency. This paper studies TSO-preserving binary optimization. We introduce a novel approach to formally model TSO-preserving binary optimization based on the formal TSO memory model. The major contribution of the modeling is a sound and complete algorithm to verify TSO-preserving binary optimization with O(N2) complexity. We also developed a dynamic binary optimization system to evaluate the performance impact of TSO-preserving optimization. We show in our experiments that, dynamic binary optimization without memory optimizations can improve performance by 8.1%. TSO-preserving optimizations can further improve the performance by 4.8% to a total 12.9%. Without considering the restriction for TSO-preserving optimizations, the dynamic binary optimization can improve the overall performance to 20.4%.	algorithm;consistency model;experiment;memory model (programming);multi-core processor;multiprocessing;object code optimizer;observable;optimization problem;performance evaluation;program optimization;read-only memory;x86	Cheng Wang;Youfeng Wu	2011	2011 International Conference on Parallel Architectures and Compilation Techniques	10.1109/PACT.2011.69	heuristic;probabilistic-based design optimization;memory model;multi-swarm optimization;computer architecture;parallel computing;real-time computing;test functions for optimization;profile-guided optimization;computer science;operating system;program optimization;instruction set;programming language;memory management	EDA	-3.0488245697783425	52.23116709693135	188238
f3ee42cd0d7c9a5901844d13f54eb83fe6d7b516	storage optimization through offset assignment with variable coalescing	assignment problem;variable coalescing;general offset assignment;digital signal processor;dsp;simple offset assignment;agu	Most modern digital signal processors (DSPs) provide multiple address registers and a dedicated address generation unit (AGU) which performs address generation in parallel to instruction execution. There is no address computation overhead if the next address is within the auto-modify range. A careful placement of variables in memory is utilized to decrease the number of address arithmetic instructions and thus to generate compact and efficient code. The simple offset assignment (SOA) problem concerns the layout of variables for machines with one address register and the general offset assignment (GOA) deals with multiple address registers. Both these problems assume that each variable needs to be allocated for the entire duration of a program. Both SOA and GOA are NP-complete. In this article, we present effective heuristics for the simple and the general offset assignment problems with variable coalescing where two or more non-interfering variables can be mapped into the same memory location. Results on several benchmarks show the significant improvement of our proposed heuristics compared to other heuristics in the literature.	address generation unit;algorithm;assignment problem;benchmark (computing);central processing unit;computation;digital signal processor;heuristic (computer science);memory address;np-completeness;overhead (computing);processor register	Hassan Salamy;J. Ramanujam	2012	ACM Trans. Embedded Comput. Syst.	10.1145/2180887.2180893	augmented assignment;embedded system;digital signal processor;parallel computing;real-time computing;computer science;operating system;digital signal processing;assignment problem;algorithm	Arch	-0.7254346592779206	52.03247090210558	188545
c8c266327db328c5489ed6d769f619b5bf09a9de	compact generic intermediate representation (cgir) to enable late binding in coarse grained reconfigurable architectures	memory management;elektroteknik och elektronik;fast fourier transform compact generic intermediate representation late binding coarse grained reconfigurable architecture inter application communication concurrency pattern static compile time decision making multiple configuration bitstream runtime resource manager system memory requirement dynamically reconfigurable resource array;dynamic reconfiguration;resource allocation;reconfigurable architectures;electrical engineering electronic engineering information engineering;storage management;resource manager;runtime;delay equation;fast fourier transform;reconfigurable architecture;parallel architectures;synchronization;concurrency control;fast fourier transforms;coarse grained;delay equations parallel processing runtime memory management synchronization;storage management concurrency control decision making fast fourier transforms parallel architectures reconfigurable architectures resource allocation;parallel processing;intermediate representation	In the era of platforms hosting multiple applications, where inter-application communication and concurrency patterns are arbitrary, static compile time decision making is neither optimal nor desirable. As a part of solving this problem, we present a novel method for compactly representing multiple configuration bitstreams of a single application, with varying parallelisms, as a unique, compact, and customizable representation, called CGIR. The representation thus stored is unraveled at runtime to configure the device with optimal (e.g. in terms of energy) implementation. Our goal was to provide optimal decision making capability to the runtime resource manager (RTM) without compromising the runtime behavior or the memory requirements of the system. The presence of multiple binaries enhance optimality by providing the RTM with multiple implementations to choose from. The CGIR ensures minimal increase in memory requirements with the addition of each binary. The low-cost unraveling of CGIR guarantees the runtime behavior. We have chosen the dynamically reconfigurable resource array (DRRA) as a vehicle to study the feasibility of our approach. Simulation results using 16 point decimation in time fast Fourier transform (FFT) has showed massive (up to 18% for 2 versions, 33% for 3 versions) memory savings compared to state of the art. Formal evaluation shows that the savings increase with the increase in the number of implementations stored.	binary file;compile time;compiler;concurrency (computer science);concurrency pattern;decimation (signal processing);directory information tree;fast fourier transform;inter-process communication;intermediate representation;late binding;power management system;reconfigurability;requirement;run time (program lifecycle phase);simulation	Syed M. A. H. Jafri;Ahmed Hemani;Kolin Paul;Juha Plosila;Hannu Tenhunen	2011	2011 International Conference on Field-Programmable Technology	10.1109/FPT.2011.6132719	embedded system;parallel processing;fast fourier transform;parallel computing;real-time computing;computer science;operating system;distributed computing	EDA	-0.591906733778016	49.60099361532897	188575
6789915ec7bd31d83d2c15dd7b795f1e258717f5	a hardware/software partitioner using a dynamically determined granularity	integer linear programming;convergence;hardware software co design;cost function;application software;adaptive optimization;simulated annealing;computational modeling;permission;hardware software partitioning;time use;national electric code;covering problems;dynamic adaptation;computer simulation;hardware permission application software laboratories national electric code computational modeling computer simulation simulated annealing convergence cost function;hardware	Computer aided hardware/software partitioning is one of the keychallenges in hardware/software co-design. While previous approacheshave used a fixed granularity, i.e. the size of the partitioningobjects was fixed, we present a partitioning approach thatdynamically determines the partitioning granularity to adapt optimizationsteps to application properties and to intermediate optimizationresults. Experiments with simulated annealing optimizationshow a faster convergence and far better adaptability to costfunction variations than in previous experiments with fixed granularity.	experiment;simulated annealing	Jörg Henkel;Rolf Ernst	1997		10.1145/266021.266323	computer simulation;adaptive optimization;mathematical optimization;national electrical code;application software;parallel computing;real-time computing;convergence;simulated annealing;covering problems;computer science;theoretical computer science;operating system;computational model	EDA	1.3265337060813784	53.255690921667046	189183
5c457bda2b2c7d164e666c40a1604ede9b65f9df	a component selection algorithm for high-performance pipelines	data flow graph;cost optimization;critical path;high performance	The use of a realistic component library with multiple implementations of operators, results in cost efcient designs; slow components can then be used on non-critical paths and the more expensive components on only the critical paths. This paper presents a cost-optimized algorithm for selecting components and pipelining a data ow graph, given a multipleimplementation library, and throughput and latency constraints. Experiments on a few benchmarks indicate that our algorithm gives results that are within 0.7% of the optimal result.	benchmark (computing);pipeline (computing);polynomial;selection algorithm;throughput;time complexity	Smita Bakshi;Daniel Gajski	1994			mathematical optimization;computer science;theoretical computer science;machine learning	EDA	-0.9500498918323581	51.87170612512661	190264
45468347f4d055f60c96ac36b6631daaf914a6fa	compilation, architectural support, and evaluation of simd graphics pipeline programs on a general-purpose cpu	optimising compilers;load imbalance;clustered processors;coprocessors;graphical programming;memory access;mask analysis media processing workload computing programmable graphics processor graphics pipeline general purpose cpu simd graphics program code generation compiler optimization instruction set;graphics pipelines optimizing compilers registers program processors hardware microprocessors process design performance analysis coprocessors;inter pe communication;instruction replication;parallel architectures;media processing;parallel architectures optimising compilers pipeline processing coprocessors instruction sets;compiler optimization;graphics processors;instruction distribution;pipeline processing;instructions per cycle;instruction sets	Graphics and media processing is quickly emerging to become one of the key computing workloads. Programmable graphics processors give designers extra flexibility by running a small program for each fragment in the graphics pipeline. This paper investigates low-cost mechanisms to obtain good performance for modern graphics programs on a general purpose CPU.This paper presents a compiler that compiles SIMD graphics program and generates efficient code on a general purpose CPU. The generated code can process between 25-0.3 million vertices per second on a 2.2 GHz Intel Pentium¯ 4 processor for a group of typical graphics programs.This paper also evaluates the impact of three changes in the architecture and compiler. Adding support for new specialized instructions improves the performance of the programs by 27.4 %. on average. A novel compiler optimization called mask analysis improves the performance of the programs by 19.5 % on average. Increasing the number of architectural SIMD registers from 8 to 16 registers significantly reduces the number of memory accesses due to register spills.	central processing unit;general-purpose markup language;graphics pipeline;graphics processing unit;graphics software;mathematical optimization;optimizing compiler;register file;simd;shader;speedup	Mauricio Breternitz;Herbert H. J. Hum;Sanjeev Kumar	2003		10.1109/PACT.2003.1238010	computer architecture;graphics pipeline;parallel computing;computer hardware;computer science;operating system;instruction set;optimizing compiler;visual programming language;programming language;graphics address remapping table;graphics hardware;general-purpose computing on graphics processing units;coprocessor;instructions per cycle	Arch	-0.9242320276587053	46.937248638059486	190268
f06e0a5d517aa102e5a2dded26b84e4247263ec6	finite state machine datapath design, optimization, and implementation	flowgraph;verilog;datapath;fpga;design optimization;scheduling;latency;pipelining;memories;finite state machine;throughput;timing	Abstract Finite State Machine Datapath Design, Optimization, and Implementation explores the design space of combined FSM/Datapath implementations. The lecture starts by examining performance issues in digital systems such as clock skew and its effect on setup and hold time constraints, and the use of pipelining for increasing system clock frequency. This is followed by definitions for latency and throughput, with associated resource tradeoffs explored in detail through the use of dataflow graphs and scheduling tables applied to examples taken from digital signal processing applications. Also, design issues relating to functionality, interfacing, and performance for different types of memories commonly found in ASICs and FPGAs such as FIFOs, single-ports, and dual-ports are examined. Selected design examples are presented in implementation-neutral Verilog code and block diagrams, with associated design files available as downloads for both Altera Quartus and Xilinx Virtex FPGA platforms. A working knowled...	datapath;finite-state machine;program optimization	Justin Davis;Robert B. Reese	2007		10.2200/S00087ED1V01Y200702DCS014	embedded system;throughput;computer architecture;electronic engineering;latency;finite state machine with datapath;parallel computing;real-time computing;multidisciplinary design optimization;computer science;operating system;finite-state machine;memory;scheduling;pipeline;field-programmable gate array	EDA	0.9112320427285996	52.2210001126312	190475
fe2bfa40d763d8c0cac3dc6ada3aef6b743bfb9d	applications of pipelining to firmware	direct control;brief description;firmware level;essential concept;system level pipeline;machine organization;different amount;floating point normalization;firmware pipeline approach;certain type	It has been found that pipelining at the firmware level of machine organization can provide significant execution time benefits for certain types of instructions. The essential concept involved with this approach is the pipelining of operations within the hardware under direct control of the firmware, rather than the pipelining of microinstructions. Specifically, multiple data values exist within the machine's datapaths which have undergone different amounts of processing.  This paper describes the application of these techniques to a specific pipelined computer. A brief description of the system level pipeline is provided with two examples of the firmware pipeline approach. The operations of floating point normalization and memory transfers have been described in detail.	datapath;firmware;microcode;pipeline (computing);run time (program lifecycle phase)	David M. Proulx	1984		10.1145/800016.808211	software pipelining;computer architecture;parallel computing;operating system	Arch	-1.855959476597741	49.072957160332685	190932
96e40ea18a474fe7b34b722694c1558b70f1bf40	the crns framework and its application to programmable and reconfigurable cryptography	general purpose graphical processing units gpgpu;cryptography;modular arithmetic;residue number system rns;framework	This article proposes the Computing with the ResidueNumber System (CRNS) framework, which aims at the design automation of accelerators for Modular Arithmetic (MA). The framework provides a comprehensive set of tools ranging from a programming language and respective compiler to back-ends targeting parallel computation platforms such as Graphical Processing Units (GPUs) and reconfigurable hardware. Given an input algorithm described with a high-level programming language, the CRNS can be used to obtain in a few seconds the corresponding optimized Parallel Thread Execution (PTX) program ready to be run on GPUs or the Hardware Description Language (HDL) specification of a fully functional accelerator suitable for reconfigurable hardware and embedded systems. The resulting framework's implementations benefit from the Residue Number System (RNS) arithmetic's parallelization properties in a fully automated way. Designers do not need to be familiar with the mathematical details concerning the employed arithmetic, namely the RNS representation. In order to thoroughly describe and evaluate the proposed framework, experimental results obtained for the supported back-ends (GPU and HDL) are presented targeting the implementation of the modular exponentiation used in the Rivest-Shamir-Adleman (RSA) algorithm and Elliptic Curve (EC) point multiplication. Results suggest competitive latency and throughput with minimum design effort and overcoming all the development issues that arise in the specification and verification of dedicated solutions.	apl;algorithm;compiler;computation;cryptography;embedded system;field-programmable gate array;graphical user interface;graphics processing unit;hardware description language;high- and low-level;high-level programming language;modular exponentiation;parallel computing;rsa (cryptosystem);reconfigurable computing;residue number system;throughput	Samuel Antão;Leonel Sousa	2013	TACO	10.1145/2400682.2400692	modular arithmetic;computer architecture;parallel computing;computer science;cryptography;theoretical computer science;operating system;programming language	EDA	2.4156838077569946	49.17688912240223	191499
1b16c8aa4f40bd5a597e29b189dfc01df4df52c8	leveraging latency-insensitivity to ease multiple fpga design	cycle time;design automation;programming language;switch architecture;high performance networks;fpga;compiler;high level synthesis;hardware design;programming languages;dsp	Traditionally, hardware designs partitioned across multiple FPGAs have had low performance due to the inefficiency of maintaining cycle-by-cycle timing among discrete FPGAs. In this paper, we present a mechanism by which complex designs may be efficiently and automatically partitioned among multiple FPGAs using explicitly programmed latency-insensitive links. We describe the automatic synthesis of an area efficient, high performance network for routing these inter-FPGA links. By mapping a diverse set of large research prototypes onto a multiple FPGA platform, we demonstrate that our tool obtains significant gains in design feasibility, compilation time, and even wall-clock performance.	compiler;field-programmable gate array;routing	Kermin Fleming;Michael Adler;Michael Pellauer;Angshuman Parashar;Arvind;Joel S. Emer	2012		10.1145/2145694.2145725	embedded system;computer architecture;compiler;parallel computing;real-time computing;electronic design automation;reconfigurable computing;cycle time variation;computer science;operating system;digital signal processing;high-level synthesis;field-programmable gate array	EDA	1.0568676357110065	51.01350898797177	191709
44166f89bf50e9902191c1e4509d7a25c3184baf	single instruction dual-execution model processor architecture	hardware design languages;processor architecture;reduced bit produced order queue computation;computer model;grouped ilp;instruction set architecture;dynamic switching single instruction dual execution model processor architecture reduced bit produced order queue computation instruction set architecture queue execution model stack execution models;computer architecture;computational modeling;registers;dual mode;pipelines;single instruction dual execution model processor architecture;design;optimization;dynamic switching;stack execution models;computer architecture hardware computer aided instruction power system modeling computational modeling internet java switches program processors ubiquitous computing;queue execution model;grouped ilp dual mode queue computing design;program processors;instruction sets;queue computing;hardware;program processors computer architecture instruction sets	We present in this paper architecture and preliminary evaluation results of a novel dual-mode processor architecture which supports queue and stack computation models in a single core. The core is highly adaptable in both functionality and configuration. It is based on a reduced bit produced order queue computation instruction set architecture and functions into Queue or Stack execution models. This is achieved via a so called dynamic switching mechanism implemented in hardware. The current design focuses on the ability to execute Queue programs and also to support Stack based programs without considerable increase in hardware to the base architecture. We present the architecture description and design results in a fair amount of details.	computation;microarchitecture	Taichi Maekawa;Ben A. Abderazek;Kenichi Kuroda	2008	2008 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing	10.1109/EUC.2008.116	computer simulation;reference architecture;embedded system;space-based architecture;computer architecture;parallel computing;real-time computing;computer science;cellular architecture;operating system;instruction set;transport triggered architecture;distributed computing;priority queue;data architecture	Arch	-3.3334729385313135	49.54727463664226	191885
3827e7713798b7c433c3928f131379c0324828eb	memory-aware platform description and framework for source-level embedded mpsoc software optimization			embedded system;mpsoc;mathematical optimization;program optimization	Robert Pyka	2017			program optimization;system on a chip;mpsoc;computer architecture;parallel computing;computer science	EDA	0.4145048465454678	51.379121550883454	192328
0ec8efb46f2591cc504e7f15263a12335d4b668f	transport-triggered soft cores		"""Soft cores are used as flexible software programmable components in FPGA designs. Transport-Triggered Architecture (TTA) is interesting for this use due to its scalability, modularity, simplified register files (RF) and fine-grained compiler control, but has the drawback of wider instructions and additional multiplexing due to extensive RF port sharing. In this paper we evaluate the trade-offs of TTA in soft core use in comparison to its closest multi-issue relative, the traditional """"operation triggered"""" VLIW architecture, as well as the Xilinx MicroBlaze, a popular single-issue soft core. For the compared alternatives running CHStone benchmarks, the dual-issue TTA with a monolithic RF provides the best performance/area trade-off. Its program size increase varies from 21% to 49% in comparison to the VLIW programming model. However, synthesis results on a Xilinx Zynq Z7020 device show that the dual-issue TTA requires 67% of the resources while providing up to 88% improvement in execution time compared to VLIW. Partitioning the RF is found beneficial for both VLIW and TTA programming models, resulting in a very similar FPGA resource usage, but with TTA model improving execution time up to 77%. As a single-issue soft core, we measured execution time improvements up to 173% when comparing with the TTA approach against the performance-optimized MicroBlaze with similar datapath resources."""	compiler;control unit;datapath;field-programmable gate array;integrated circuit design;manycore processor;memory hierarchy;multi-core processor;multiplexing;programming model;radio frequency;register file;run time (program lifecycle phase);simd;scalability;soft core (synthesis);transport triggered architecture;very long instruction word	Pekka Jääskeläinen;Aleksi Tervo;Guillermo Payá Vayá;Timo Viitanen;Nicolai Behmann;Jarmo Takala;Holger Blume	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00022	parallel computing;embedded system;computer science;field-programmable gate array;compiler;very long instruction word;programming paradigm;datapath;scalability;microblaze;multiplexing	Arch	0.22527115593439107	49.25607078150683	192636
3c9f8d4d60007d26ac6f6f8547a0166820c63295	a recursive computer architecture for vlsi	individual computer system;recursive computer architecture;larger computer system;machine organisation;lego-like computer system;data flow;program organisation;vlsi computer system;generation computer;general-purpose computer architecture;computer architecture;control flow;electronic circuits;architecture;integrated circuit;integrated circuits	We present a general-purpose computer architecture based on the concept of recursion, suitable for VLSI computer systems built from replicated (LEGO-like) computing elements. The recursive computer architecture is defined by presenting a program organisation, a machine organisation and an experimental machine implementation oriented to VLSI. The program organisation, called recursive control flow, attempts a synthesis of the concepts underlying traditional control flow, data flow and reduction, to exploit the individual strengths of each organisation. The machine organisation is based on replicated general-purpose computing elements, as well as special-purpose computing elements that allow the function of individual computer systems to be specialised. These elements are interconnected to form a larger computer system and cooperate in the concurrent execution of a program. The experimental implementation is being restricted to simple, identical microcomputers each containing a memory, a processor and a communications capability. This future generation of LEGO-like computer systems are termed fifth generation computers [1] by the Japanese.	computer architecture;control flow;dataflow;fifth generation computer;general-purpose markup language;microcomputer;recursion;very-large-scale integration	Philip C. Treleaven;Richard P. Hopkins	1982		10.1145/800048.801731	computer architecture;parallel computing;computer science;theoretical computer science;architecture;operating system;integrated circuit;programming language	Arch	1.2990055844328208	47.57348195361368	193130
dbac7de9ff7eac1bea140c2dc242a5b69e2d744c	computing in-memory, revisited		"""The Von Neumann's architecture has been the dominant computing paradigm ever since its inception in the mid-forties. It revolves around the concept of a """"stored program"""" in memory, and a central processing unit that executes the program. As an alternative, Processing-In-Memory (PIM) ideas have been around for at least two decades, however with very limited adoption. Today, three trends are creating a compelling motivation to take a second look. Novel devices such as memristor blur the boundary between memory and compute, effectively providing both in the same element. Power efficiency has become very important, both in the datacenter and at the edge. Machine learning applications driven by a data-flow model have become ubiquitous. In this paper, we sketch our Computing-In-Memory (CIM) vision, and its substantial performance and power improvement potential. Compared to PIM models, CIM more clearly separates computing from memory. We then discuss the programming model, which we consider the biggest challenge. We close by describing how CIM impacts non-functional characteristics, such as reliability, scale, and configurability."""	central processing unit;computer-integrated manufacturing;data center;data flow diagram;dataflow;machine learning;memristor;programming model;stored-program computer	Dejan S. Milojicic;Kirk Bresniker;Gary Campbell;Paolo Faraboschi;John Paul Strachan;Stan Williams	2018	2018 IEEE 38th International Conference on Distributed Computing Systems (ICDCS)	10.1109/ICDCS.2018.00130	distributed computing;von neumann architecture;memristor;programming paradigm;architecture;central processing unit;electrical efficiency;sketch;computer science	HPC	-3.4755829658645987	48.01118914847707	193921
369702e8b28a410f7cf4347a1c26e2ea2e0a6a79	implications of emerging 3d gpu architecture on the scan primitive		Analytic database workloads are growing in data size and query complexity. At the same time, computer architects are struggling to continue the meteoric increase in performance enabled by Moore's Law. We explore the impact of two emerging architectural trends which may help continue the Moore's Law performance trend for analytic database workloads, namely 3D die-stacking and tight accelerator-CPU integration, specifically GPUs. GPUs have evolved from fixed-function units, to programmable discrete chips, and now are integrated with CPUs in most manufactured chips. Past efforts to use GPUs for analytic query processing have not had widespread practical impact, but it is time to re-examine and re-optimize database algorithms for massively data-parallel architectures. We argue that high-throughput data-parallel accelerators are likely to play a big role in future systems as they can be easily exploited by database systems and are becoming ubiquitous. Using the simple scan primitive as an example, we create a starting point for this discussion. We project the performance of both CPUs and GPUs in emerging 3D systems and show that the high-throughput data-parallel architecture of GPUs is more efficient in these future systems. We show that if database designers embrace emerging 3D architectures, there is possibly an order of magnitude performance and energy efficiency gain.	algorithm;central processing unit;data parallelism;database;decision tree model;fixed-function;graphics processing unit;high-throughput computing;moore's law;parallel computing;sane;stacking;throughput	Jason Power;Yinan Li;Mark D. Hill;Jignesh M. Patel;David A. Wood	2015	SIGMOD Record	10.1145/2783888.2783896	parallel computing;real-time computing;computer science;theoretical computer science;database	Arch	-3.51986765342053	47.733288313398	193982
2bdde656e322cc6de44361178465abc0386b92bb	simple offset assignment in presence of subword data	storage assignment;soa;code size;swoa;embedded processor;subword data	Many embedded architectures support indirect addressing mode with autoincrement/autodecrement. By maximizing the use of this mode, generation of explicit instructions for performing address arithmetic can be avoided and thus reductions in code size and improvements in performance are achieved. Bartley [2] and Liao et al. [16] developed a method for finding a storage layout for program variables so that the use of autoincrement/autodecrement could be maximized. They introduced the Simple Offset Assignment (SOA) problem and solved it using a Path Cover (PC) formulation.We observe that many media and network processing applications make extensive use of subword data. Therefore, for such applications, by packing multiple subword variables into a single word, we can generate storage layouts that further reduce the cost of address arithmetic in two ways. First the need for address arithmetic is reduced as variables that are packed together share the same address. Second opportunities for using autoincrement andautodecrement instructions are increased as layouts are now possible which place a variable adjacent to more than two variables. This approach has become feasible because of the recent trend in embedded processor design which allows subword variables that are packed together to be accessed and manipulated without incurring performance penalty. We introduce the SubWord Offset Assignment (SWOA) problem and solve it using a Path Cover with Node Coalescing (PCwNC) formulation. Node coalescing corresponds to packing of multiple subword variables into a single word while path covering corresponds to placement of variables in adjacent memory locations to enable the use of autoincrement/autodecrement. We present three heuristics to solve the PCwNC problem. Experiments show that when the program is optimized for code size, the three proposed algorithms achieve 26%, 26.9% and 32% reduction in the number of static explicit address arithmetic instructions over Liao et al.'s algorithm. The algorithms also achieve 14.5%, 22.1%and 22.7% reduction in stack frame size. If the program is optimized for performance, the algorithms achieve 24.3%, 24.7% and 30.2% reduction in the dynamic instruction count of explicit address arithmetic instructions.	addressing mode;algorithm;call stack;embedded system;heuristic (computer science);network processor;path cover;processor design;set packing;substring	Bengu Li;Rajiv Gupta	2003		10.1145/951710.951715	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;service-oriented architecture;programming language	PL	-1.3388794717601267	52.20307859834192	193984
33e2a97bf7f64d108fec521b95b4062e42cc062e	sat-based compilation to a non-vonneumann processor		This paper describes a compilation technique used to accelerate dataflow computations, common in deep neural network computing, onto Coarse Grained Reconfigurable Array (CGRA) architectures. This technique has been demonstrated to automatically compile dataflow programs onto a commercial massively parallel CGRA-based dataflow processor (DPU) containing 16000 processing elements. The DPU architecture overcomes the von Neumann bottleneck by spatially flowing and reusing data from local memories, and provides higher computation efficiency compared to temporal parallel architectures such as GPUs and multi-core CPUs. However, existing software development tools for CGRAs are limited to compiling domain specific programs to processing elements with uniform structures, and are not effective on complex micro architectures where latencies of memory access vary in a nontrivial fashion depending on data locality. A primary contribution of this paper is to provide a general algorithm that can compile general dataflow graphs, and can efficiently utilize processing elements with rich micro-architectural features such as complex instructions, multi-precision data paths, local memories, register files, switches etc. Another contribution is a uniquely innovative application of Boolean Satisfiability to formally solve this complex, and irregular optimization problem and produce high-quality results comparable to hand-written assembly code produced by human experts. A third contribution is an adaptive windowing algorithm that harnesses the complexity of the SAT-based approach and delivers a scalable and robust solution.	algorithm;arbitrary-precision arithmetic;artificial neural network;assembly language;boolean satisfiability problem;central processing unit;compiler;computation;dataflow programming;deep learning;display resolution;experiment;graphics processing unit;locality of reference;mathematical optimization;multi-core processor;network switch;optimization problem;programmer;programming productivity;programming tool;reconfigurable computing;register file;run time (program lifecycle phase);scalability;software development;solver;vii;von neumann architecture	Samit Chaudhuri;Asmus Hetzel	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203842	von neumann architecture;computer science;real-time computing;architecture;parallel computing;massively parallel;assembly language;software development;scalability;dataflow;boolean satisfiability problem	EDA	-0.6054742792681427	49.56727562098115	194031
ec358b64ab6de7b66252166d44ae09a4df31f5da	towards a dynamic and reconfigurable multicore heterogeneous system	heterogeneous reconfigurable systems dynamic multicore heterogeneous system reconfigurable multicore heterogeneous system reconfigurable architectures superscalar organizations instruction level parallelism ilp multicore organizations thread level parallelism tlp creams transparent homogeneous multicore reconfigurable system chip area;reconfigurable system;heterogeneous systems;multiprocessor;systems engineering and theory;embedded systems;reconfigurable architectures microprocessor chips multiprocessing systems parallel architectures;heterogeneous systems reconfigurable system multiprocessor embedded systems	Reconfigurable architectures are an alternative for classic superscalar organizations to the exploration of instruction level parallelism (ILP), while the multicore organizations are the most commonly used strategy to exploit thread level parallelism (TLP). This work extends a dynamic and transparent homogeneous multicore reconfigurable system (CReAMS) that explores both TLP and ILP, by making it heterogeneous, featuring cores with distinct resources. We will show that, for applications with low TLP, a heterogeneous configuration of CReAMS outperforms its homogeneous counterpart with the same chip area. The performed simulations prove the potential of using heterogeneous reconfigurable systems, showing speedups of up to 90%.	instruction-level parallelism;multi-core processor;parallel computing;reconfigurable computing;simulation;superscalar processor;task parallelism	Jeckson Dellagostin Souza;Luigi Carro;Mateus B. Rutzig;Antonio Carlos Schneider Beck	2014	2014 Brazilian Symposium on Computing Systems Engineering	10.1109/SBESC.2014.21	computer architecture;parallel computing;real-time computing;computer science	Arch	-0.5773027379973696	49.93309712075961	194146
150ccf2886825532c5e786b17337e4c4ea127b39	a parallel computer system using distributed associative processing	cognitive modeling with heuristics;machine learning;learning strategy;parallel computer;heuristics	"""There is an insatiable demand for computing power which underscores the need for continued progress in the design of computer architectures. An obvious method of attaining significantly higher computing power is to employ parallel processing. Rapid developments in process technology are bringing the speed and size of electronic components close to their principal physical limits. Although VLSI has increased the computational power possible in a single device, the fundamental difficulties in constructing high performance computer systems are associated with architectural issues related to the organizational complexity of parallel processing. The concept of parallel processing leads to a number of very difficult problems. Parallel processing requires changing the philosophy of computer organization , especially in the arrangement of system control. In parallel computer systems, control can be decentralized and distributed over the system's elements so that each element can be provided with its own set of instructions and pursue its tasks autonomously. Systems with distributed control have Uan inherent potential for increasing performance, availability, and extensibility, compared to conventional centralized systems"""" [1]. However, conventional architectures do not possess the appropriate flexibility for fully realizing the anticipated advantages of parallel processing. The most apparent gain from the use of paralld processing, an increase in computing speed, is limited by: (1) communication bottlenecks, (2) lack of inherent paral-lelism of many algorithms, and (3) the overhead to control concurrent processes. The control of parallel processing is associated with several difficulties, from mapping of algorithms onto the structure of the computer system to redistribution of information during computations. In particular , there are difficulties in realizing common properties of ordinary computers, such as I / O operations and interrupts, in parallel systems. At Allied-Signal, we have developed a distributed parallel computer system which implements the principle of control-level parallelism. This system is a general-purpose computer with a wider range of applications than systems which only exploit data-level parallelism. The system is based on a novel method for organizing computer communications, leading to th 9 concept of distributed associative memory. Our research indicates that this approach has an excellent potential to overcome the basic difficulties in organizing a distributed parallel computer system. A r c h i t e c t u r e C I T O C o m m u n i c a t i o n s An architecture for a general-purpose multiprocessor computer system has been designed using methods of …"""	algorithm;centralized computing;complexity theory and organizations;computation;computer architecture;content-addressable memory;data parallelism;distributed control system;electronic component;extensibility;general-purpose modeling;higher computing;input/output;interrupt;linc;microarchitecture;multiplexing;multiprocessing;organizing (structure);overhead (computing);parallel computing;parallel processing (dsp implementation);real-time clock;supercomputer;very-large-scale integration	C. Walter;Y. Park;H. Yee;T. Roden;M. Stalker;Simon Y. Berkovich	1989		10.1145/75427.75447	natural language processing;computer science;theoretical computer science;heuristics;machine learning	Arch	1.2045282526650918	47.27732129449495	195000
fd04d6aa6ca28a174593b6e2fad503f060d5dc05	evaluating energy efficiency of floating point matrix multiplication on fpgas	energy conservation;field programmable gate arrays sdram arrays algorithm design and analysis system on chip;power aware computing;external dram energy efficiency floating point matrix multiplication fpga modular design onchip storage onchip matrix multiplication core single precision arithmetic double precision arithmetic;matrix multiplication;floating point arithmetic;field programmable gate arrays;power aware computing energy conservation field programmable gate arrays floating point arithmetic matrix multiplication	Energy efficiency has emerged as one of the key performance metrics in scientific computing. In this work, we evaluate the energy efficiency of floating point matrix multiplication on the state-of-the-art FPGAs. We implement a modular design parameterized with the problem size and the type of on-chip storage. To understand the efficiency of our implementations, we estimate the peak energy efficiency of any matrix multiplication implementation. Our on-chip matrix multiplication core achieves up to 7.07 and 2.28 GFlops/Joule for single and double precision arithmetic, respectively. Our implementations sustain up to 73% and 84% of the peak energy efficiency for single and double precision arithmetic, respectively. Using an optimal on-chip matrix multiplication core, we also model and estimate the energy efficiency of large-scale matrix multiplication using external DRAM. Our designs for large-scale matrix multiplication achieve energy efficiency of 5.21 and 1.60 GFlops/Joule for single and double precision, respectively.	analysis of algorithms;computational science;double-precision floating-point format;dynamic random-access memory;field-programmable gate array;joule;matrix multiplication;modular design;scalability	Kiran Kumar Matam;Ming Hsieh	2013	2013 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2013.6670345	electronic engineering;parallel computing;multiplication algorithm;arbitrary-precision arithmetic;kochanski multiplication;theoretical computer science;mathematics;extended precision	HPC	-2.5526938469983396	47.158688064472315	195281
f15e911d6e6becf07a15e6eb124dcdff1d513b15	equalizing data-path for processing speed determination in block level pipelining	pipeline processing communication system control signal processing algorithms control systems hardware centralized control frequency synchronization distributed control mobile computing laboratories;data flow graphs;buffer storage;local buffer controllers block level pipelined data flows processing block speed optimization low power consumption buffer usage minimization data path equalization processing speed determination block level pipelining hardware mapped data flow graphs buffer pipelining elements data path node execution time equalization;data flow graph;buffer storage pipeline processing data flow graphs circuit optimisation;signal processing;power dissipation;circuit optimisation;data flow;processing speed;pipeline processing	Signal processing algorithms represented by data flow graphs can be efficiently mapped to hardware using a block level pipelining architecture. In this scheme, nodes of data flow are mapped to processing blocks and buffers are inserted in between as pipelining elements. We present, in this paper, a methodology for equalizing execution times of various nodes in the data path. The method is used to minimize the power dissipation and buffer usage by judiciously selecting the execution speed of hardware units. The block level pipelining allows for simple local controllers for each buffer which are generated by the global controller based on data flow specifications. The evaluation of the methodology on a practical example is presented.	algorithm;dataflow;pipeline (computing);requirement;signal processing	Xiaoyao Liang;Akshay Athalye;Sangjin Hong	2005	2005 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2005.1464920	software pipelining;embedded system;data flow diagram;computer vision;parallel computing;real-time computing;computer science;dissipation;theoretical computer science;operating system;data-flow analysis;signal processing	Arch	1.3901782700659393	52.98922736280074	196395
862082d02a8618cfa1814ce778ec2a0e6332dc97	implementing the new ada 2005 timing event and execution time control features on the avr32 architecture	real time;gnat;execution time control;ada 2005;atmel avr32	This paper describes how the new Ada 2005 timing event and execution time control features were implemented for the GNAT bare-board Ravenscar run-time environment on the Atmel AVR32 architecture. High accuracy for execution time measurement was achieved by accounting for the effects of interrupts and executing entries by proxy. The implementation of timing events was streamlined by using a single alarm mechanism both for timing events and waking up tasks. Test results on the overhead and accuracy of the implemented features are presented. While the implementation is for the AVR32, it may serve as a blueprint for implementations on other architectures. It is also discussed how the presented design could be transferred to other systems such as C/POSIX and RTSJ.	avr32;ada;run time (program lifecycle phase)	Kristoffer Nyborg Gregertsen;Amund Skavhaug	2010	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2010.08.001	parallel computing;real-time computing;computer science;operating system	Embedded	-3.7343174935084362	51.51400202266277	196566
5f58eeb2d5483cdc266a8a6740736f4d558c7165	a system-level framework for evaluating area/performance/power trade-offs of vliw-based embedded systems	internet;application specific integrated circuits;embedded systems;instruction sets;microprocessor chips;optimisation;program compilers;internet;pareto-optimal configurations;design space exploration;embedded systems;estimation models;instruction level parallelism;memory hierarchy subsystem;multi-objective dse strategies;multimedia electronic appliances;parameterized vliw microprocessor;power consumption;retargetable compilers;simulation tools;specific instruction-set processor;very long instruction word	Architectures based on very long instruction word (VLIW) have found fertile ground in multimedia electronic appliances thanks to their ability to exploit high degrees of instruction level parallelism (ILP) with a reasonable trade-off in complexity and silicon costs. In this case application specific instruction-set processor (ASIP) specialization may require not only manipulation of the instruction-set but also tuning of the architectural parameters of the processor (e.g. the number and type of functional units, register files, etc.) and the memory subsystem (cache size, associativity, etc.). Setting the parameters so as to optimize certain metrics requires the use of efficient design space exploration (DSE) strategies and also simulation tools (retargetable compilers and simulators) and accurate estimation models operating at a high level of abstraction. In this paper we present a framework for evaluation, in terms of performance, cost and power consumption, of a system based on a parameterized VLIW microprocessor together with the memory hierarchy subsystem following execution of a specific application. The framework, which can be freely downloaded from the Internet, implements a number of multi-objective DSE strategies to obtain Pareto-optimal configurations for the system.	embedded system;very long instruction word	Giuseppe Ascia;Vincenzo Catania;Maurizio Palesi;Davide Patti	2005		10.1109/ASPDAC.2005.1466494	embedded system;computer architecture;parallel computing;real-time computing;the internet;computer science;very long instruction word;operating system;instruction set;application-specific integrated circuit;programming language;register file;field-programmable gate array	EDA	1.504092142472561	51.190749072677455	196967
1028b42e8a84fc01662a862c96414bc8bae5970b	module assignment for low power	high level synthesis;logic cad;optimisation;pipeline processing;power consumption;processor scheduling;average power savings;conditional branching;data intensive applications;functional pipelining;functional units;functionally pipelined data path;high level synthesis algorithms;max cost multi commodity flow problem;module assignment;pipelined designs;post processing step;power consumption;power optimization problem;scheduled data path;total power consumption minimisation	In this paper, we investigate the problem of minimizing the total power consumption during the binding of operations to functional units in a scheduled data path with functional pipelining and conditional branching for data intensive applications. We first present a technique to estimate the power consumption in a functionally pipelined data path and then formulate the power optimization problem as a max-cost multi-commodity flow problem and solve it optimally. Our proposed method can augment most high-level synthesis algorithms as a post-processing step for reducing power after the optimizations for area or speed have been completed. An average power savings of 28% has been observed after we apply our method to pipelined designs that have been optimized using conventional techniques.	algorithm;branch (computer science);data-intensive computing;flow network;functional derivative;high- and low-level;high-level synthesis;mathematical optimization;multi-commodity flow problem;optimization problem;pipeline (computing);video post-processing	Massoud Pedram;Jui-Ming Chang	1996			mathematical optimization;parallel computing;real-time computing;computer science;high-level synthesis;power optimization	EDA	0.2871698967406286	52.89340867276771	197059
35d8a6cbe3f26eb58b840061b5944c2fb5b71870	a library and platform for fpga bitstream manipulation	sparse matrices field programmable gate arrays floating point arithmetic logic design matrix multiplication;microprocessors;performance fpga sparse matrix multiplication;logic design;high performance computing;control logic;performance;logic circuits;innovative accumulation circuit;fpga;data format;design optimization;system performance;sparse matrix multiplication;system design;adders;pipelined floating point adders;pipelined floating point adders sparse matrix vector multiplication design fpga ssf control logic data flow innovative accumulation circuit;floating point;matrix multiplication;scalability;sparse matrix;floating point arithmetic;field programmable gate arrays;high throughput;data flow;ssf;sparse matrix vector multiplication design;sparse matrices;sparse matrices field programmable gate arrays throughput high performance computing system performance scalability logic circuits adders design optimization microprocessors;throughput	Since 1998, no commercially available FPGA has been accompanied by public documentation of its native machine code (or bitstream) format. Consequently, research in reconfigurable hardware has been confined to areas which are specifically supported by manufacturer-supplied tools. Recently, detailed documentation of the bitstream format for the Atmel FPSLIC series of FPGAs appeared on the usenet group comp.arch.fpga. This information has been used to create abits, a Java library for direct manipulation of FPSLIC bitstreams and partial reconfiguration. The abits library is accompanied by the slipway reference design, a low-cost USB bus-powered board carrying an FPSLIC. This paper describes the abits library and slipway platform, as well as a few applications which they make possible. Both the abits source code and slipway board layout are publicly available under the terms of the BSD license. It is our hope that these tools will enable further research in reconfigurable hardware which would not otherwise be possible.	bsd;bitstream format;direct manipulation interface;documentation;emergence;evolvable hardware;experiment;field-programmable gate array;java;library;machine code;printed circuit board;reconfigurable computing;reference design;software bug;usb;usenet	Adam Megacz	2007	15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)	10.1109/FCCM.2007.60	embedded system;parallel computing;sparse matrix;computer science;floating point;theoretical computer science;computer performance;field-programmable gate array	Logic	-0.05570162860519424	47.160533502069754	197601
08ff03255a27dbfb135b9f58f3386c626713a3a0	custom wide counterflow pipelines for high-performance embedded applications	automatic control;instruction level parallel;vliw architecture;microarchitecture;automatic architectural synthesis;printers;embedded computing application;counterflow pipelines;pipeline architecture;digital camera;cfp;instruction sets pipeline processing embedded systems parallel architectures;wide counterflow pipeline;wcfp;process design;digital cameras;computer architecture;embedded systems;automatic architectural synthesis application specific instruction set processor asip embedded computing application digital camera color printer cellular phone processor organization counterflow pipeline cfp wide counterflow pipeline wcfp parallel processor microarchitecture pipeline architecture vliw architecture;parallel architectures;cellular phone;color printer;pipelines;pipeline processing parallel architectures computer instructions;application specific processors;processor organization;counterflow pipeline;high performance;cost model;asip;application specific instruction set processor;embedded computing;pipeline processing;cellular phones;parallel processor;instruction sets	Application-specific instruction set processor (ASIP) design is a promising technique to meet the performance and cost goals of high-performance systems. ASIPs are especially valuable for embedded computing (e.g., digital cameras, color printers, cellular phones, etc.) where a small increase in performance and decrease in cost can have a large impact on a product's viability. Sutherland, Sproull, and Molnar have proposed a processor organization called the counterflow pipeline (CFP) that is appropriate for ASIP design due to its simple and regular structure, local control and communication, and high degree of modularity. This paper describes a new CFP architecture, called the wide counterflow pipeline (WCFP) that extends the original proposal to be better suited for custom embedded instruction-level parallel processors. This work presents a novel and practical application of the CFP to automatic and quick turn-around design of ASIPs. The paper introduces the WCFP architecture and describes several microarchitecture enhancements needed to get good performance from custom WCFPs. We demonstrate that custom WCFPs have performance that is up to four times better than that of ASIPs based on the original CFP.	analysis of algorithms;application-specific instruction set processor;cache (computing);central processing unit;computers, freedom and privacy conference;dataflow;digital camera;dynamic dispatch;elegant degradation;embedded system;instruction-level parallelism;interconnection;microarchitecture;mobile phone;parallel computing;passthrough;pipeline (computing);pipeline (software);register file;requirement;set packing;wide-issue	Bruce R. Childers;Jack W. Davidson	2000	IEEE Transactions on Computers	10.1109/TC.2004.1261825	process design;embedded system;parallel processing;computer architecture;parallel computing;microarchitecture;computer science;operating system;automatic control;instruction set;pipeline transport	Arch	-1.0459482637142201	53.01443734002373	197728
a535f6dbc4ad530e3f485ba470757cee5badc932	a qualitative comparison of mpsoc mobile and embedded virtualization techniques		Virtualization is generally adopted in server and desktop environments to provide for fault tolerance, resource management, and energy efficiency. Virtualization enables parallel execution of multiple operating systems (OSs) while sharing the hardware resources. Virtualization was previously not deemed as feasible technology for mobile and embedded devices due to their limited processing and memory resource. However, the enterprises are advocating Bring Your Own Device (BYOD) applications that enable co-existence of heterogeneous OSs on a single mobile device. Moreover, embedded device require virtualization for logical isolation of secure and general purpose OSs on single device. In this paper we investigate the processor architectures in the mobile and embedded space while examining their formal virtualizabilty. We also compare the virtualization solutions enabling coexistence of multiple OSs in Multicore Processor System-on-Chip (MPSoC) mobile and embedded systems. We advocate that virtualization is necessary to manage resource in MPSoC designs and to enable BYOD, security, and logical isolation use cases.	bring your own device;coexist (image);desktop computer;embedded system;fault tolerance;full virtualization;mpsoc;mobile device;multi-core processor;operating system;server (computing);smartphone;system on a chip;tablet computer	Junaid Shuja;Abdullah Gani;Sajjad Ahmad Madani	2016	CoRR		embedded system;full virtualization;real-time computing;virtualization;computer science;operating system;storage virtualization	Mobile	-1.96406085238158	49.732334208278935	197864
c28129e3463d18dca0f0e26833fff9efa1745268	exploring the vlsi scalability of stream processors	floating point unit;performance evaluation;energy efficient;energy dissipation;chip;parallel architectures;cost efficiency;vlsi;parallel architectures vlsi performance evaluation floating point arithmetic power consumption microprocessor chips;floating point arithmetic;power consumption;high performance;intercluster scaling vlsi scalability stream processors high performance programmable processors stream architectures floating point units alu kernel speedup application speedup energy dissipation intracluster scaling;very large scale integration scalability bandwidth arithmetic streaming media kernel application software laboratories computer architecture degradation;microprocessor chips	Stream processors are high-performance programmable processors optimized to run media applications. Recent work has shown these processors to be more areaand energy-efficient than conventional programmable architectures. This paper explores the scalability of stream architectures to future VLSI technologies where over a thousand floating-point units on a single chip will be feasible. Two techniques for increasing the number of ALUs in a stream processor are presented: intracluster and intercluster scaling. These scaling techniques are shown to be cost-efficient to tens of ALUs per cluster and to hundreds of arithmetic clusters. A 640-ALU stream processor with 128 clusters and 5 ALUs per cluster is shown to be feasible in 45 nanometer technology, sustaining over 300 GOPS on kernels and providing 15.3x of kernel speedup and 8.0x of application speedup over a 40-ALU stream processor with a 2% degradation in area per ALU and a 7% degradation in energy dissipated per ALU operation.	arithmetic logic unit;central processing unit;cost efficiency;elegant degradation;flops;image scaling;microprocessor;network switch;scalability;speedup;stream processing;very-large-scale integration;watts humphrey	Brucek Khailany;William J. Dally;Scott Rixner;Ujval J. Kapasi;John D. Owens;Brian Towles	2003		10.1109/HPCA.2003.1183534	chip;floating-point unit;computer architecture;parallel computing;real-time computing;computer science;floating point;dissipation;operating system;efficient energy use;very-large-scale integration;cost efficiency	Arch	-2.773789555094021	47.50341665009795	199483
87e904bdba7f862b06cfd4c06ac81706cb527353	fast prototyping and refinement of complex dynamic data types in multimedia applications for consumer embedded devices	libraries;complex dynamics;optimisation;memory management;dynamic data types;multimedia applications;software prototyping;storage management;prototypes;low power embedded devices;consumer electronics;multimedia application;c programming fast prototyping complex dynamic data types consumer embedded devices multimedia algorithms dynamic memory optimization memory management low power embedded devices high speed embedded devices abstract derived classes;refinement;prototypes multimedia systems energy consumption libraries algorithm design and analysis memory management costs delta modulation workstations embedded system;abstract data type;data type;fast prototyping;delta modulation;multimedia systems;embedded system;multimedia computing;dynamic memory optimization;embedded systems;prototyping;c language;complex data;low power;memory optimization;energy consumption;data structures;consumer embedded devices;workstations;storage management c language consumer electronics embedded systems multimedia computing optimisation programming software prototyping;abstract derived classes;high speed embedded devices;multimedia algorithms;c programming;c;complex dynamic data types;programming;high speed;algorithm design and analysis;embedded device;optimization methods	Portable consumer devices are increasing their capabilities more and more and can now implement new multimedia algorithms that were reserved only for powerful workstations a few years ago. Unfortunately, the original design characteristics of such algorithms do not often allow them to be ported directly to current embedded devices. These algorithms share complex and intensive dynamic memory use and actual embedded systems cannot provide efficient general-purpose memory management as it is needed. As a result, dynamic memory optimizations are a requirement when porting these applications. Within these optimizations, the refinement of the dynamically (de)allocated abstract data type implementations in the complex multimedia applications involved is one of the most important and difficult parts for an efficient mapping of the algorithms on low-power and high-speed embedded consumer devices. We describe a high-level approach for modeling and refining complex data types using abstract derived classes in C++. This approach enables the multimedia developer to compose, evaluate and refine complex data types in a conceptually straightforward way, without a time-consuming programming effort	abstract data type;algorithm;c++;dynamic data;embedded system;general-purpose modeling;high- and low-level;low-power broadcasting;memory management;refinement (computing);requirement;workstation	David Atienza;Marc Leeman;Francky Catthoor;Geert Deconinck;Jose Manuel Mendias;Vincenzo De Florio;Rudy Lauwereins	2004	2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763)	10.1109/ICME.2004.1394322	embedded system;parallel computing;real-time computing;data structure;computer science;operating system;prototype;programming language	Embedded	2.2747867234385932	52.489213151892336	199927
