id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
6efeca92853ea979f5ba323011cddb088f0ca270	rocsearch: an roc-guided search strategy for subgroup discovery	technology and engineering	Subgroup Discovery (SD) aims to find coherent, easy-to-interpret subsets of the dataset at hand, where something exceptional is going on. Since the resulting subgroups are defined in terms of conditions on attributes of the dataset, this data mining task is ideally suited to be used by non-expert analysts. The typical SD approach uses a heuristic beam search, involving parameters that strongly influence the outcome. Unfortunately, these parameters are often hard to set properly for someone who is not a data mining expert; correct settings depend on properties of the dataset, and on the resulting search landscape. To remove this potential obstacle for casual SD users, we introduce ROCsearch [1], a new ROC-based beam search variant for Subgroup Discovery. On each search level of the beam search, ROCsearch analyzes the intermediate results in ROC space to automatically determine a sensible search width for the next search level. Thus, beam search parameter setting is taken out of the domain expert’s hands, lowering the threshold for using Subgroup Discovery. Also, ROCsearch automatically adapts its search behavior to the properties and resulting search landscape of the dataset at hand. Aside from these advantages, we also show that ROCsearch is an order of magnitude more efficient than traditional beam search, while its results are equivalent and on large datasets even better than traditional beam search results.	beam search;coherence (physics);data mining;heuristic;subject-matter expert	Marvin Meeng;Wouter Duivesteijn;Arno J. Knobbe	2014		10.1137/1.9781611973440.81	interpolation search;beam search;computer science;artificial intelligence;phrase search;jump search;data mining;mathematics;incremental heuristic search;iterative deepening depth-first search;algorithm	ML	-1.599492151128891	-34.270145696948376	192822
f3684f6066c2755f233ddbb2f44ef5f0ac77e6d7	don't pay for validation: detecting drifts from unlabeled data using margin density		Validating online stream classifiers has traditionally assumed the availability of labeled samples, which can be monitored over time, to detect concept drift. However, labeling in streaming domains is expensive, time consuming and in certain applications, such as land mine detection, not a possibility at all. In this paper, the Margin Density Drift Detection (MD3) approach is proposed, which can signal change using unlabeled samples and requires labeling only for retraining, in the event of a drift. The MD3 approach when evaluated on 5 synthetic and 5 real world drifting data streams, produced statistically equivalent classification accuracy to that of a fully labeled accuracy tracking drift detector, and required only a third of the samples to be labeled, on average.		Tegjyot Singh Sethi;Mehmed M. Kantardzic	2015		10.1016/j.procs.2015.07.284	simulation;machine learning;data mining	ML	0.3448681050922111	-35.091249135147734	193009
986355378caf7738b39efc404fc695888e2ccdf8	comparison of detection and classification algorithms using boolean and fuzzy techniques	classification system;hypothetical target classification scenario;available data;related signal data;logic diagram;fuzzy logic system;fuzzy technique;approximate target classification;conventional boolean;data stream;multi-quantization boolean	Modern military ranging, tracking, and classification systems are capable of generating large quantities of data. Conventional “brute-force” computational techniques, even with Moore’s law for processors, present a prohibitive computational challenge, and often, the system either fails to “lock onto” a target of interest within the available duty cycle, or the data stream is simply discarded because the system runs out of processing power or time. In searching for high-fidelity convergence, researchers have experimented with various reduction techniques, often using logic diagrams to make inferences from related signal data. Conventional Boolean and fuzzy logic systems generate a very large number of rules, which often are difficult to handle due to limitations in the processors. Published research has shown that reasonable approximations of the target are preferred over incomplete computations. This paper gives a figure of merit for comparing various logic analysis methods and presents results for a hypothetical target classification scenario. Novel multiquantization Boolean approaches also reduce the complexity of these multivariate analyses, making it possible to better use the available data to approximate target classification. This paper shows how such preprocessing can reasonably preserve result confidence and compares the results between Boolean, multi-quantization Boolean, and fuzzy techniques.		Rahul Dixit;Harpreet Singh	2012	Adv. Fuzzy Systems	10.1155/2012/406204	circuit minimization for boolean functions;standard boolean model;computer science;artificial intelligence;fuzzy number;theoretical computer science;machine learning;data mining;algorithm	SE	-1.5271500335314327	-31.109235691390168	194199
c3540b754b3f9f4cd5dbc492b38d2709f3d2d502	concept semilattice: construction and complexity	lattice theory computational complexity data analysis;hasse diagram concept semilattice data analysis;lattice theory;incremental formation;pediatrics;formal concept analysis fca;complexity theory;lattices;approximation algorithms;construction industry;concept semilattice;join semilattice complexity concept semilattice formal concept analysis fca incremental formation;complexity;data analysis;hasse diagram;concept lattice;computational complexity;lattices upper bound hydrogen sun data analysis algorithm design and analysis psychology sociology biology computing mathematics;join semilattice;algorithm design and analysis;buildings;formal concept analysis	Concept lattice is widely used in data analysis. This paper aims to reduce redundant nodes from concept lattice by replacing lattice with join-semilattice. We develop an algorithm based on gradual insertion of concepts into partially hasse diagram. An example is proposed to illustrate the constructing procedure of concept semilattice. Experimental results show effectiveness and efficiency of the proposed algorithm.	algorithm;best, worst and average case;formal concept analysis;hasse diagram;time complexity	Chengming Qi;Yingjie Tian;Shoumei Cui;Yunchuan Sun	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.157	combinatorics;discrete mathematics;computer science;lattice;mathematics;approximation algorithm;algorithm	Robotics	-3.9068977096986024	-36.29976748299163	195287
abb2431a8f68b646cd1fd9062cd56879946eece3	arm-amo: an efficient association rule mining algorithm based on animal migration optimization		Abstract Association rule mining (ARM) aims to find out association rules that satisfy predefined minimum support and confidence from a given database. However, in many cases ARM generates extremely large number of association rules, which are impossible for end users to comprehend or validate, thereby limiting the usefulness of data mining results. In this paper, we propose a new mining algorithm based on animal migration optimization (AMO), called ARM–AMO, to reduce the number of association rules. It is based on the idea that rules which are not of high support and unnecessary are deleted from the data. Firstly, Apriori algorithm is applied to generate frequent itemsets and association rules. Then, AMO is used to reduce the number of association rules with a new fitness function that incorporates frequent rules. It is observed from the experiments that, in comparison with the other relevant techniques, ARM–AMO greatly reduces the computational time for frequent item set generation, memory for association rule generation, and the number of rules generated.	algorithm;association rule learning;mathematical optimization	Le Hoang Son;Francisco Chiclana;Raghavendra Kumar;Mamta Mittal;Manju Khari;Jyotir Moy Chatterjee;Sung Wook Baik	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.04.038	end user;data mining;apriori algorithm;computer science;limiting;algorithm;association rule learning;fitness function	DB	-2.32033375858305	-34.870638952618066	195644
885e756807b001d293f73bf84cafe0e56400f2dc	efficient mining of dissociation rules	extraction information;association statistique;correlacion;analisis datos;information extraction;systeme aide decision;efficient algorithm;statistical association;almacen dato;sistema ayuda decision;data mining;negative association;association rule mining;data analysis;decision support system;asociacion estadistica;association rule;fouille donnee;decouverte connaissance;utilisabilite;representacion parsimoniosa;descubrimiento conocimiento;analyse donnee;entrepot donnee;correlation;usabilidad;data warehouse;usability;sparse representation;busca dato;extraccion informacion;representation parcimonieuse;knowledge discovery	Association rule mining is one of the most popular data mining techniques. Significant work has been done to extend the basic association rule framework to allow for mining rules with negation. Negative association rules indicate the presence of negative correlation between items and can reveal valuable knowledge about examined dataset. Unfortunately, the sparsity of the input data significantly reduces practical usability of negative association rules, even if additional pruning of discovered rules is performed. In this paper we introduce the concept of dissociation rules. Dissociation rules present a significant simplification over sophisticated negative association rule framework, while keeping the set of returned patterns concise and actionable. A new formulation of the problem allows us to present an efficient algorithm for mining dissociation rules. Experiments conducted on synthetic datasets prove the effectiveness of the proposed solution.	algorithm;association rule learning;data mining;level of detail;sparse matrix;synthetic intelligence;usability	Mikolaj Morzy	2006		10.1007/11823728_22	association rule learning;decision support system;computer science;artificial intelligence;machine learning;data warehouse;data mining;information extraction	ML	-3.230459366984615	-33.20953746157374	197345
48f4fe290a21251fd0b5ccf816a0eeec10805cad	large-scale detection of non-technical losses in imbalanced data sets	imbalanced classification;non technical losses;electricity theft detection;fuzzy logic;support vector machine	Non-technical losses (NTL) such as electricity theft cause significant harm to our economies, as in some countries they may range up to 40% of the total electricity distributed. Detecting NTLs requires costly on-site inspections. Accurate prediction of NTLs for customers using machine learning is therefore crucial. To date, related research largely ignore that the two classes of regular and non-regular customers are highly imbalanced, that NTL proportions may change and mostly consider small data sets, often not allowing to deploy the results in production. In this paper, we present a comprehensive approach to assess three NTL detection models for different NTL proportions in large real world data sets of 100Ks of customers: Boolean rules, fuzzy logic and Support Vector Machine. This work has resulted in appreciable results that are about to be deployed in a leading industry solution. We believe that the considerations and observations made in this contribution are necessary for future smart meter research in order to report their effectiveness on imbalanced and large real world data sets.	apache spark;boolean expression;deep learning;fuzzy logic;machine learning;mathematical optimization;ntl;scalability;smart meter;support vector machine;unsupervised learning	Patrick O. Glauner;Andre Boechat;Lautaro Dolberg;Radu State;Franck Bettinger;Yves Rangoni;Diogo Duarte	2016	2016 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)	10.1109/ISGT.2016.7781159	fuzzy logic;support vector machine;computer science;artificial intelligence;machine learning;data mining	ML	1.5526052827327645	-33.66675474329981	197611
4255f9447c0302a843c7dcd59bdaed1b31d15477	mining outliers with faster cutoff update and space utilization	disk based algorithms;traitement signal;detection erreur;evaluation performance;distance function;optimisation;deteccion error;mise a jour;algorithm performance;surface equivalente radar;high dimensionality;performance evaluation;optimizacion;efficient algorithm;evaluacion prestacion;modelo hibrido;outlier;modele hybride;hybrid model;outlier detection;algorithme;actualizacion;observacion aberrante;algorithm;detection objet;radar cross section;hybrid approach;memory optimization;resultado algoritmo;distance based outliers;signal processing;linear time;superficie equivalente radar;performance algorithme;observation aberrante;optimization;error detection;procesamiento senal;updating;object detection;algoritmo	It is desirable to find unusual data objects by Ramaswamy et al’s distance-based outlier definition because only a metric distance function between two objects is required. It does not need any neighborhood distance threshold required by many existing algorithms based on the definition of Knorr and Ng. Bay and Schwabacher proposed an efficient algorithm ORCA, which can give near linear time performance, for this task. To further reduce the running time, we propose in this paper two algorithms RC and RS using the following two techniques respectively: (i) faster cutoff update, and (ii) space utilization after pruning. We tested RC, RS and RCS (a hybrid approach combining both RC and RS) on several large and high-dimensional real data sets with millions of objects. The experiments show that the speed of RCS is as fast as 1.4 to 2.3 times that of ORCA, and the improvement of RCS is relatively insensitive to the increase in the data size.	algorithm;experiment;orca;reed–solomon error correction;revision control system;time complexity	Chi-Cheong Szeto;Edward Hung	2010	Pattern Recognition Letters	10.1016/j.patrec.2010.04.002	time complexity;outlier;error detection and correction;metric;computer science;artificial intelligence;signal processing;mathematics;radar cross-section;algorithm;statistics	ML	-3.4862721651612683	-33.966691401453005	197854
275fe30db6477b40e1894794fe0f1702a8110f85	ga-based item partition for data mining	itemsets;insert;association rules;styling;style;data mining;formatting;arrays;biological cells;insert formatting style styling;biological cells association rules itemsets partitioning algorithms arrays;genetic algorithms;transformation scheme ga based item partition data mining very large databases branch and bound search strategy encoding representation;very large databases;tree searching;very large databases data mining genetic algorithms tree searching;partitioning algorithms	When a mining procedure is directly executed on very large databases, the computer memory may not allow the processing in memory. In the past, we adopted a branch-and-bound search strategy to divide the domain items as a set of groups. Although it works well in partitions the items, the time is quite time consuming. In this paper, we thus propose a GA-based approach to speed up the partition process. A new encoding representation and a transformation scheme are designed to help the search process. Experimental results also show that the algorithm can get a proper partition with good efficiency.	algorithm;association rule learning;branch and bound;computer memory;data mining;database;eisenstein's criterion;software release life cycle	Tzung-Pei Hong;Jheng-Nan Huang;Wen-Yang Lin;Ming-Chao Chiang	2011	2011 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/ICSMC.2011.6084010	genetic algorithm;association rule learning;disk formatting;computer science;theoretical computer science;machine learning;data mining;database;insert	DB	-4.298742412687621	-37.17505919077534	199027
