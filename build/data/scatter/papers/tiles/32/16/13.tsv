id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e8f806c9411f6ac60553cbb18eaf169009558c65	eshopper modeling and simulation	consumidor;intervalo tiempo;customer behavior;commerce electronique;reponse temporelle;consumption;base donnee;inventory management;comercio electronico;direct product;red www;generic model;modeling and simulation;consommateur;e commerce;real time;simulation;comercializacion;database;base dato;real time processing;simulacion;mode conversationnel;interactive mode;prise decision;customer retention;data mining;time interval;processing time;consumo;commercialisation;feedback;transaction data;internet;time response;fouille donnee;marketing;consumer;modo conversacional;consommation;temps reel;preferencia;tiempo real;temps traitement;world wide web;preference;reseau www;stochastic model;toma decision;direct marketing;respuesta temporal;modeling;tiempo proceso;electronic trade;intervalle temps	The advent of e-commerce gives an opportunity to shift the paradigm of customer communication into a highly interactive mode. The new generation of commercial Web servers, such as the Blue Martini’s server, combines the collection of data on a customer behavior with real-time processing and dynamic tailoring of a feedback page. The new opportunities for direct product marketing and cross selling are arriving. The key problem is what kind of information do we need to achieve these goals, or in other words, how do we model the customer? The paper is devoted to customer modeling and simulation. The focus is on modeling an individual customer. The model is based on the customer’s transaction data, click stream data, and demographics. The model includes the hierarchical profile of a customer’s preferences to different types of products and brands; “consumption” models for the different types of products; the current focus, trends, and stochastic models for time intervals between purchases; product affinity models; and some generalized features, such as purchasing power, sensitivity to advertising, price sensitivity, etc. This type of model is used for predicting the date of the next visit, overall spending, and spending for different types of products and brands. For some type of stores (for example, a supermarket) and stable customers, it is possible to forecast the shopping lists rather accurately. The forecasting techniques are discussed. The forecasting results can be used for on-line direct marketing, customer retention, and inventory management. The customer model can also be used as a generative model for simulating the customer’s purchasing behavior in different situations and for estimating customer’s features.	affinity analysis;blue gene;clickstream;consistency model;e-commerce;generative model;inventory;martini;online and offline;programming paradigm;purchasing;real-time transcription;server (computing);simulation;stochastic process;transaction data;web server	Valery A. Petrushin	2001		10.1117/12.421061	customer to customer;voice of the customer;customer;the internet;systems modeling;customer lifetime value;consumption;consumer;telecommunications;loyalty business model;stochastic modelling;house of quality;attitudinal analytics;transaction data;customer reference program;direct marketing;modeling and simulation;feedback;customer intelligence;customer satisfaction;customer retention;direct product;conversion marketing	AI	-1.8476915682441752	-10.716934843276796	159994
4e76e116dae5c4faf6a149dc5c81790fd1fd84fb	the relation between ordinal problem space sizes and the maximum number of ordinal classification rules	modelizacion;regle inference;monotony;sistema experto;redundancia;intelligence artificielle;prise decision;raisonnement;classification;consistencia;monotonie;inference rule;modelisation;redundancy;ordinal reasoning;classification rules;monotonicity;consistance;razonamiento;artificial intelligence;maximum number of classification rules;monotonia;inteligencia artificial;rule base size;systeme expert;reasoning;toma decision;modeling;clasificacion;consistency;redondance;regla inferencia;expert system	A method is presented of establishing bounds on the number of classification rules in such applications as credit worthiness assessment, investment decisions, premium determination, consumer choices, employee selection, and editorial preferences, to name just a few. A function that relates the maximum number of classification rules to the problem space size of such application domains is established. It is shown that in this important class of ordinal classification problems, the maximum possible number of rules is significantly lower than the relative problem space sizes. The approach grants the ability to a priori estimate worst case response time and memory requirements, and to better predict the effectiveness of knowledge acquisition efforts.	level of measurement;ordinal data	Arie Ben-David	1993	Computational Intelligence	10.1111/j.1467-8640.1993.tb00299.x	ordinal regression;systems modeling;monotonic function;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;redundancy;ordinal data;consistency;expert system;reason;rule of inference	AI	0.46601551965430443	-15.779738549162085	161042
da1ee52abe7d56077bdcea3fce8eba2b2d06c96b	investment using technical analysis and fuzzy logic	finanza;application signal;finance;analyse technique;inversion;fuzzy relation;evaluation method;investment;human behavior;comportement humain;fuzzy logic;price formation;technical indicators;stock price;investissement;fuzzy logic system;signal mapping;fuzzy relations;technical indicator;relation floue;technical analysis;indicateur technique;relacion difusa	Deploy fuzzy logic engineering tools in the 0nance arena, speci0cally in the technical analysis 0eld. Since technical analysis theory consists of indicators used by experts to evaluate stock prices, the new proposed method maps these indicators into new inputs that can be fed into a fuzzy logic system. The only required inputs to these indicators are past sequence of stock prices. This method relies on fuzzy logic to formulate a decision making when certain price movements or certain price formations occur. The success of the system is measured by comparing system output versus stock price movement. The new stock evaluation method is proven to exceed market performance and it can be an excellent tool in the technical analysis 0eld. The 5exibility of the system is also demonstrated. c © 2002 Elsevier Science B.V. All rights reserved.	best, worst and average case;formal system;fuzzy logic;matlab;map;simulation	Hussein Dourra;Pepe Siy	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00169-5	inversion;fuzzy logic;investment;artificial intelligence;human behavior;operations research;technical analysis	Logic	0.8836385298914504	-12.764704322087224	161686
0c14c219a8d1163b552cbb2afcc3f708e1d61a3a	endogeneity and exogeneity in sales response functions	production function;stochastic derivative constraints;market model;pharma marketing;model choice;marketing strategy;response function;sales response functions;mcmc;simultaneous estimation	Endogeneity and exogeneity are topics that are mainly discussed in macroeconomics. We show that sales response function (SRF) are exposed to the same problem if we assume that the control variables in a SRF reflect behavioral reactions of the supply side. The supply side actions are covering a flexible marketing component which could interact with the sales responses if sales managers decide to react fast according to new market situations. A recent article of [3] suggested to use a class of production functions under constraints to estimate the sales responses that are subject to marketing strategies. In this paper we demonstrate this approach with a simple SRF(1) model that contains one endogenous variable. Such models can be extended by further exogenous variables leading to SRF-X models. The new modeling approach leads to a multivariate equation system and will be demonstrated using data from a pharma-marketing survey in German regions.	endogeneity (econometrics);frequency response;markov chain monte carlo;model selection	Wolfgang Polasek	2010		10.1007/978-3-642-24466-7_52	financial economics;economics;marketing;operations management	ML	1.1503373492259357	-10.038834600351638	162174
28ebf94d45deb9e7a4f56971c8710230188e044a	the effects of anchoring in interactive mcdm solution methods	experimental design;multicriteria analysis;eleccion;factor humano;test statistique;anchorage effect;test estadistico;plan experiencia;statistical test;search method;interactive method;prise decision;efecto anclaje;decision problem;multi dimensional;effet ancrage;plan experience;scheduling;human factor;decision process;not significant;ordonamiento;analisis multicriterio;analyse multicritere;choix;toma decision;facteur humain;ordonnancement;choice	-In this paper we describe an experiment that examines the effects of the anchoring and adjustment bias in two different interactive MCDM solution methods. Anchoring is a well documented psychological bias in decision making, where the starting point of a decision process often greatly affects the final outcome. In this experiment we consider a three objective, continuous decision problem with multi-dimensional outcomes and solved using a free search interactive method and the Zionts and Wallenius method. Results suggest that subjects are anchored by the starting point in the Zionts and Wallenius method, but the effect of anchoring is not significant in the free search method. We hypothesize that the anchoring effect is significandy affected by the structure of the solution method, and offer suggestions for using and designing interactive MCDM solution methods. © 1997 Elsevier Science Ltd	decision problem;decision theory;zionts–wallenius method	John T. Buchanan;James Corner	1997	Computers & OR	10.1016/S0305-0548(97)00014-2	statistical hypothesis testing;computer science;artificial intelligence;human factors and ergonomics;decision problem;mathematics;design of experiments;operations research;scheduling;statistics	AI	-0.8363509993913534	-15.546916168017903	162966
2dd3986b14c5eab4ac4cebb2bbde58ef39e8a622	organizational risk assessment using adaptive neuro-fuzzy inference system	small deviation;strategic management;emergency management;risk assessment;contingency management;adaptive neuro fuzzy inference system;fuzzy model;historical data	In this paper a fuzzy model based on Adaptive Neuro-Fuzzy Inference System (ANFIS) is introduced for calculating the level of risk in managerial problems. In this model, affecting factors on the level of risk are considered as inputs and the level of risk as the output. Using the fuzzy model, the risky condition changes smoothly in a fuzzy environment as it is the case in the real world; while in classic models, we may have some stepwise changes in the state of the system caused by an infinity small deviations in input parameters. The main advantage of the introduced model is that for continuous values of input factors, the counters of risk surface represent a more realistic behavior for different systems. The model is designed for a system of three inputs (probability, impact and ability to react) and one output (risky situation). The ability of using historical data as well as experts’ knowledge and flexibility of adaptation to unusual risky situations are some benefits of the introduced model. This model which is originally used in strategic management system to analyze the external environment and the level of threats can also be used in contingency management for incidents (CMI) or as a tool for Comprehensive Emergency Management Program (CEMP). The strategic Risk of Roche is considered as the bench mark for comparing the difference between fuzzy and classic systems. Keywords— Risk assessment, Adaptive Neuro-Fuzzy Inference System, Risk Counters, Impact, Ability to react,	adaptive neuro fuzzy inference system;computer memories inc.;emoticon;fuzzy concept;linear model;neuro-fuzzy;nonlinear system;real life;risk assessment;serial ata;smoothing;stepwise regression;strategic management	Javad J. Jassbi;Sohrab Khanmohammadi	2009			simulation;adaptive neuro fuzzy inference system;engineering;operations management;management science	AI	-3.5367615169525957	-15.030567646091798	163271
777ad7a0d6db6168fd84debea12eb3f3ef4e4859	financial portfolio management through the goal programming model: current state-of-the-art	settore secs s 06 metodi mat dell economia e scienze attuariali e finanziarie;multi attribute portfolio management;settore secs p 08 economia e gestione delle imprese;typology;goal programming;article	Since Markowitz (1952) formulated the portfolio selection problem, many researchers have developed models aggregating simultaneously several conflicting attributes such as: the return on investment, risk and liquidity. The portfolio manager generally seeks the best combination of stocks/assets that meets his/ her investment objectives. The Goal Programming (GP) model is widely applied to finance and portfolio management. The aim of this paper is to present the different variants of the GP model that have been applied to the financial portfolio selection problem from the 1970s to nowadays. 2013 Elsevier B.V. All rights reserved.	goal programming;programming model;selection algorithm	Belaïd Aouni;Cinzia Colapinto;Davide La Torre	2014	European Journal of Operational Research	10.1016/j.ejor.2013.09.040	financial economics;mathematical optimization;typology;economics;marketing;operations management;goal programming;mathematics;application portfolio management;black–litterman model;commerce	AI	-3.792690309316935	-14.990644342834958	163952
014bde06bbb3acc580528f36ab833666516c6ebc	behavioral finance has come of age				Philip Maymin	2011	Risk and Decision Analysis	10.3233/RDA-2011-0035	financial economics;actuarial science;economics;finance	NLP	-0.5167294492303732	-10.316904305370707	164319
773cb47a25155a5f60479b5e00c19110de6c1ae4	multifractional processes in finance	behavioral finance;efficient markets;stylized facts	There is a growing consensus that fundamental financial theory based on the assumption that markets are complete is not sustainable when financial markets become increasingly complex. Traditional models fail to capture many of the stylized facts and biases identified by recent financial developments that have sought to explain financial prices when markets are incomplete. These approaches, and in particular behavioral finance, in turn, do not formalize and quantify the assumptions their approaches are based on, which are required for financial calculations and assets pricing. One of the open questions is therefore whether a model exists that is able to deduce the overall equilibrium stated by the current paradigm as a sequence of balancing disequilibria. To this aim, a class of stochastic models - the multifractional processes - is suggested and presented in this paper. Multifractional processes are defined as a generalization of fractional Brownian motion, providing a parsimonious mechanisms for modeling real financial markets. This approach includes temporary departures from equilibrium triggered by investorsu0027 biases.		Sergio Bianchi;Augusto Pianese	2014	Risk and Decision Analysis	10.3233/RDA-130097	financial economics;financial modeling;actuarial science;economics;mathematical economics;statistical finance	Logic	-3.7292120438281837	-10.797466549352889	164521
acb52d191ec40911d1893a8171db1c086af30b63	a genetic algorithm system for product line exploration and optimization	product line management genetic algorithms conjoint analysis portfolio optimization marketing;cbc data genetic algorithm near optimal product portfolio marketing management product line exploration consumer preference information individual level bootstrapping model level bootstrapping personal computer accessory product line;personal computer;product line;product line management;consumer preference;portfolio optimization;conjoint analysis;analytical models genetics;marketing;consumer behaviour;industrial application;genetic algorithm;genetic algorithms;marketing consumer behaviour genetic algorithms;optimal portfolio;discrete choice model	We report development and industrial application of a genetic algorithm (GA) model to find near-optimal product portfolios for marketing management. This GA model uses consumer preference information that is typically available from marketing studies such as choice-based conjoint (CBC) analysis and other discrete choice model projects. Because a single result might capitalize on chance, the system does not simply find one optimal portfolio but instead allows individual-level and model-level bootstrapping of results. Examination of the resulting distribution of near-optimal portfolios is informative for strategic insight and generation of market hypotheses. We describe application of the GA model in a personal computer accessory product line for a major manufacturer using CBC data from N=716 respondents. The distribution of portfolio results suggested that the manufacturer's actual product line was potentially much larger than optimal and was missing two products that might be highly desired by consumers. Finally, we review the underlying computer code and its options. The model provides multiple methods of determining individual preference for the GA model along with various adjustable parameters.	choice modelling;discrete choice;evolutionary algorithm;genetic algorithm;information;mathematical optimization;personal computer;software release life cycle;yahoo! answers	Christopher N. Chapman;James L. Alford	2010	2010 Second World Congress on Nature and Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2010.5716267	genetic algorithm;computer science;consumer behaviour	ECom	-2.3091715473506493	-11.130327412215877	164863
8373b8a30f0db81adbeef32ad95730541846cfd0	an empirical assestment of fuzzy black and scholes pricing option model in spanish stock option market		The main objective of this paper is assessing the empirical performance of fuzzy extension to Black-Scholes option pricing formula (FBS). Concretely we evaluate the goodness of the FBS predictions for traded prices of options on the Spanish stock index IBEX35 during March 2017. We firstly propose a procedure to fit, from real data, the fuzzy parameters to implement FBS in stock options: price of the subjacent asset, free discount rate and stock volatility. Subsequently we evaluate the capability of FBS to include actual traded prices and whether this capability depends on option moneyness and expiration date. We find that FBS fits quite well actual traded prices. However, generally most representative market prices (closing and medium) are not better fitted than those more extreme (minimum and maximum). We have also check that the goodness of the FBS predictions often depends on the moneyness grade and the expiration date of options.	black–scholes model;closing (morphology);fits;function-behaviour-structure ontology;volatility	Jorge de Andrés Sánchez	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-17719	artificial intelligence;fuzzy logic;mathematics;machine learning;financial economics;binary option;black–scholes model;asian option	AI	1.037249570767869	-13.2072958959186	165752
da54e02d356b1527dac9d11a25f8153c13b0b79a	new perspectives on a more-or-less familiar poverty index	indexation;poverty measurement;scale invariance	A particular scale-invariant index of poverty is subjected to careful analysis. This leads to a new perspective, not seen before, on the family of subgroup-consistent and scaleinvariant poverty indices. Parametric families of new poverty indices are presented which offer the analyst a degree of flexibility in the choice of transfer sensitivity and distribution sensitivity which has not been available before now.		Kristof Bosmans;Lucio Esposito;Peter Lambert	2011	Social Choice and Welfare	10.1007/s00355-010-0473-6	economics;public economics;scale invariance;welfare economics;economic growth	ECom	-0.8880749295091064	-11.722341307752572	167792
8eea8baace7a539e6bbc5dc60b98224e36f0fd6b	slacks-based measures of efficiency in imprecise data envelopment analysis: an approach based on data envelopment analysis with double frontiers	overall efficiency interval;imprecise data;optimistic efficiency interval;ranking;data envelopment analysis;pessimistic efficiency interval	Article history: Received 17 April 2013 Received in revised form 30 August 2014 Accepted 26 October 2014 Available online 4 November 2014	data envelopment analysis	Hossein Azizi;Sohrab Kordrostami;Alireza Amirteimoori	2015	Computers & Industrial Engineering	10.1016/j.cie.2014.10.019	econometrics;ranking;computer science;data mining;data envelopment analysis;mathematics;statistics	SE	2.0908744134936392	-15.343523832155908	167925
b9d8b0108424e495df65919c3cd3bbec83a71e76	greeks analysis of currency option under fuzzy environment	foreign interest rates;mathematics;spot exchange rate;fuzzy set;finance;fuzzy numbers;garman kohlhagen model;fluctuations;fuzzy number;pricing;input variables;currency options;risk management;biological system modeling;stock markets economic indicators fuzzy set theory;exchange rates;greeks;fuzzy set theory;fuzzy sets;stock markets;interest rate;fuzzy techniques;fuzzy environment;put call parity;statistical analysis;stochastic processes;currency options markets;pricing fuzzy set theory economic indicators exchange rates fluctuations input variables risk management arithmetic mathematics statistical analysis;fuzzy g k models;financial market;financial markets;arithmetic;exchange rate;european currency option prices;greeks fuzzy sets finance currency options put call parity;currency option;fuzzy numbers greeks analysis currency options markets fuzzy environment financial markets spot exchange rate garman kohlhagen model fuzzy techniques fuzzy g k models fuzzy set theory foreign interest rates domestic interest rates european currency option prices;europe;greeks analysis;domestic interest rates;economic indicators	Owing to the fluctuations in the financial markets from time to time, some input variables such as the interest rate, spot exchange rate and volatility in the Garman-Kohlhagen model can not be expected in a precise sense. Therefore, it is nature to start from the fuzzy environments of currency options markets. In this paper, we introduce fuzzy techniques and obtain the fuzzy versions of the Garman-Kohlhagen model. The calculation formulas of the Greek letters according to the fuzzy G-K models are obtained based on fuzzy set theory. By assuming the foreign and domestic interest rates, spot exchange rate and volatility as fuzzy numbers, the European currency option prices and the Greek letters turn into fuzzy numbers. Then the financial investors can pick any European currency option price and Greek letters with an acceptable belief degree for his later use. The empirical results indicate that the Greeks calculated under fuzzy environment can be considered as a useful tool for managing option risk for an option writer.	fuzzy number;fuzzy set;set theory;volatility	Jinhong Xu;Weidong Xu;Weijun Xu	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.18	financial economics;actuarial science;economics;risk management;finance;fuzzy set	Theory	0.9675996049962018	-12.363166051859617	167960
a5b06d07c0750e3839ddbbeda2f9326ef3dde9a0	empirical laws of a stock price index and a stochastic model	econophysics;returns;exponential distribution;volatility;a stochastic model of stock markets;stock market;interaction strength;satisfiability;stock price;indexation;stochastic model;power law;power law distribution;nikkei 225 index	Recent works by econo-physicists [5,8,15,19] have shown that the probability function of the share returns and the volatility satisfies a power law with an exponent close to 4. On the other hand, we investigated quantitatively the return and the volatility of the daily data of the Nikkei 225 index from 1990 to 2003, and we found that the distributions of the returns and the volatility can be accurately described by the exponential distributions [11]. We then propose a stochastic model of stock markets that can reproduce these empirical laws. In our model the fluctuations of stock prices are caused by interactions among traders. We indicate that the model can reproduce the empirical facts mentioned above. In particular, we show that the interaction strengths among traders are a key variable that can distinguish the emergence of the exponential distribution or the power-law distribution.		Taisei Kaizoji;Michiyo Kaizoji	2003	Advances in Complex Systems	10.1142/S0219525903000906	financial economics;exponential distribution;power law;volatility;economics;stochastic modelling;pareto distribution;finance;macroeconomics;econophysics;mathematics;stochastic volatility;sabr volatility model;statistics;satisfiability	ML	2.225053976518151	-12.172521286904539	168119
47cff60660d6a1945aad932d800ace49105d8aac	a compromise solution for the multiobjective stochastic linear programming under partial uncertainty	multiobjective stochastic programming compromise programming chance constrained approach modified l shaped method;multiobjective programming;programmation multiobjectif;programmation stochastique;partial information;probabilistic approach;imperfect information;multiobjective stochastic programming;systeme incertain;programacion lineal;chance constrained approach;enfoque probabilista;approche probabiliste;probability distribution;compromise programming;linear programming;informacion imperfecta;programmation lineaire;linear program;modified l shaped method;stochastic programming;sistema incierto;uncertain system;programacion estocastica;information imparfaite;programacion multiobjetivo	This paper solves the multiobjective stochastic linear program with partially known probability. We address the case where the probability distribution is defined by crisp inequalities. We propose a chance constrained approach and a compromise programming approach to transform the multiobjective stochastic linear program with linear partial information on probability distribution into its equivalent uniobjective problem. The resulting program is then solved using the modified L-shaped method. We illustrate our results by an example. 2009 Elsevier B.V. All rights reserved.	linear partial information;linear programming;stochastic programming	Fouad Ben Abdelaziz;Hatem Masri	2010	European Journal of Operational Research	10.1016/j.ejor.2009.05.019	stochastic programming;probability distribution;mathematical optimization;second-order cone programming;linear programming;perfect information;mathematics;mathematical economics;algorithm	AI	0.6004875223412178	-16.61000911176021	170017
09dfbaf2e6e3b62ed5ee877258244841164cb4dc	reservoir operations optimization via fuzzy criterion decision processes	reservoir operation;dynamic programming;optimal solution;decision models;open inventory networks;multi reservoir networks joint operation;development tool;modeling and optimization;decision process;network dynamics;fuzzy criterion decision making	In this paper, we propose the treatment of complex reservoir operation problems via our newly developed tool of fuzzy criterion decision processes. This novel approach has been shown to be a more flexible and useful analysis tool especially when it is desirable to incorporate an expert’s knowledge into the decision models. Additionally, it has been demonstrated that this form of decision models will usually result in an optimal solution, which guarantees the highest satisfactory degree. We provide a practical exemplification procedure for the models presented as well as an application example.	exemplification;fuzzy concept;fuzzy set;mathematical optimization	Augustine O. Esogbue;Baoding Liu	2006	FO & DM	10.1007/s10700-006-0015-y	mathematical optimization;decision model;optimal decision;network dynamics;decision tree;dynamic programming;mathematics;management science	AI	-2.954051639558334	-16.731961412865065	170110
347718323a9e38596569dce3b76edb107f6790a7	things change: dynamic resource constraints and system-dependent selection in the evolution of organizational populations	oscillations;resource constraint;grupo de excelencia;satisfiability;system dependent selection;administracion de empresas;organizational form;organizations and environments;organizational ecology;resource availability;economia y empresa;grupo a;density dependence;microstructures	An extensive empirical literature has demonstrated the existence of density-dependent selection in organizational vital rates. This research has also shown that historical trajectories followed by organizational populations only partly conform to the predictions of the original model. Inconsistencies with the model's predictions prompt a series of questions: Why do organizational populations suddenly collapse after reaching a peak? Why do organizational populations oscillate after collapsing? What causes extinction of organizational forms? To address these questions, scholars have proposed a variety of modifications to the original model of density dependence. All have merit, but none is completely satisfying. The main objective of this study is to narrow the gap between theories, models, and observed historical trajectories by identifying a unitary analytical framework that can account for the variety of empirical trajectories typically followed by mature organizational populations. The model that we present is based on the hypothesis of system-dependent selection, according to which patterns of resource availability are produced by processes that are partly endogenous to organizational populations. The main analytical insight of the study is that under conditions of dynamic resource constraints introduced by system-dependent selection, the presence of population-level inertia leads to a rich variety of historical trajectories during population maturity. We show that this result holds in the absence of any particular assumption about the microstructure of organizational populations. Possible trajectories include sustained oscillations, resurgence, and extinction.	population	Alessandro Lomi;Erik R. Larsen;John H. Freeman	2005	Management Science	10.1287/mnsc.1050.0364	resource dependence theory;mathematical optimization;simulation;microstructure;organizational ecology;operations management;oscillation;management;density dependence;satisfiability	AI	0.23676601617729012	-10.736081760439944	170501
5dd2f664734e82836f0384677c259bbffd39f3d7	an integrated approach for enhancing the quality of the product by combining robust design and customer requirements	non linear programming;quality function deployment;response surface methodology rsm;customer;robust design	Enhancing the quality of the product has always been one considerable concern of production process management, and this subject gave way to implementing so many methods including robust design. In this paper, robust design utilizes response surface methodology (RSM) considering the mean and variance of the response variable regarding system design, parameter design, and tolerance design. In this paper, customer requirements and robust design are regarded simultaneously to achieve enriched quality. Subsequently, with a non-linear programming, a novel method for integrating RSM and quality function deployment has been proposed to achieve robustness in design. The customer requirements are regarded in every stage of product development process meaning system design, parameter design and tolerance design. To validate the applicability of the proposed approach, it has been implemented in a real case of chemical industry. Research findings show that the proposed method is much better than other existing methods including MSE and dual response methods. According to this method, the resulted mean is better than MSE method, and more importantly, the variance of the process is approximately 14% and 10% lesser than dual response and MSE method. Copyright © 2013 John Wiley & Sons, Ltd.	fuzzy set;industrial engineering;industry standard architecture;john d. wiley;linear programming;mike lesser;new product development;nonlinear programming;nonlinear system;quality function deployment;requirement;response surface methodology;reza iravani;set theory;software deployment;systems design	Hamid Shahriari;M. Janizade Haji;Reza Eslamipoor	2014	Quality and Reliability Eng. Int.	10.1002/qre.1548	reliability engineering;customer;quality function deployment;probabilistic design;nonlinear programming;systems engineering;engineering;operations management	EDA	-4.403503123053587	-15.862707924798546	170711
78d4456e14dc4923b8bc46b701c9b013b2e5e161	utilization of case-based reasoning in auditing - determining the audit fee	657 racunovodstvo;case base reasoning;audit fees;330 47 ekonomska informatika	Case-based reasoning represents a method for solving problems and decision making support which is based on the previous business experience. It uses cases from the past to solve new problems. Case can be defined as conceptualized piece of knowledge representing the experience that teaches a lesson fundamental to achieving the goals of the decision maker and it usually incorporate input (situation part of the case) and output features (solution part of the case). Many studies tried to explain types and impact of different factors that determine audit fees. Mostly all authors concentrate their research on the impact of following determinants: auditee size, auditee complexity, auditee profitability, ownership control, timing variables, auditor location and auditor size. In paper all mentioned factors are described except auditor size and location since these factors are not significant in Croatian audit service market. All significant audit fee determinants will be appropriately quantified in order to build a case-based reasoning model for determining audit fee for smaller and mid sized auditing firms in Croatia but also for the same firms in the other, particularly transition, countries too.	case-based reasoning;decision theory;norm (social)	Robert Zenzerovic	2006			case-based reasoning;computer science;artificial intelligence;auditor's report;data mining;joint audit;management;audit plan	AI	-3.143120947041223	-12.964029911336302	172353
c8d170a6cf0b773a1db726492793c0426c7bc10a	valuation of it investments using real options theory	project valuation;binomial model;information technology;black scholes model;investment valuation;real options	Real Options Theory is often applied to the valuation of IT investments. The application of Real Options Theory is generally accompanied by a monetary valuation of real options through option pricing models which in turn are based on restrictive assumptions and thus subject to criticism. Therefore, this paper analyzes the application of option pricing models for the valuation of IT investments. A structured literature review reveals the types of IT investments which are valuated with Real Options Theory in scientific literature. These types of IT investments are further investigated and their main characteristics are compared to the restrictive assumptions of traditional option pricing models. This analysis serves as a basis for further discussion on how the identified papers address these assumptions. The results show that a lot of papers do not account for critical assumptions, although it is known that the assumptions are not fulfilled. Moreover, the type of IT investment determines the criticality of the assumptions. Additionally, several extensions or adaptions of traditional option pricing models can be found which provide the possibility of relaxing critical assumptions. Researchers can profit from the results derived in this paper in two ways: First, which assumptions can be critical for various types of IT investments are demonstrated. Second, extensions of option pricing models that relax critical assumptions are introduced.	scientific literature;self-organized criticality;systematic review;value (ethics)	Christian Ullrich	2013	Business & Information Systems Engineering	10.1007/s12599-013-0286-0	financial economics;datar–mathews method for real option valuation;actuarial science;economics;binomial distribution;public economics;marketing;black–scholes model;finite difference methods for option pricing;rational pricing;binomial options pricing model;information technology;statistics	AI	-2.8648836662724486	-12.083782895992853	173687
e29bca5e57cb7d4edd00eaa6667794663984c77f	decision support for cybersecurity risk planning	decision support;confidencialidad;fuzzy set;algoritmo busqueda;agresion;contre mesure electronique;bolsa valores;systeme aide decision;availability;disponibilidad;algorithme recherche;data collection;securite informatique;logique floue;search algorithm;conjunto difuso;logica difusa;integrite;ensemble flou;integridad;sistema ayuda decision;attaque informatique;algoritmo genetico;fuzzy sets;bourse valeurs;confidentiality;stock exchange;fuzzy logic;computer security;confidentialite;support system;systeme incertain;planificacion;decision support system;it security;integrity;contra medida electronica;seguridad informatica;coste;portfolio management;computer attack;algorithme genetique;ataque informatica;genetic algorithm;planning;genetic algorithms;gestion cartera;information system;planification;electronic countermeasure;aggression;gestion portefeuille;sistema incierto;disponibilite;uncertain system;systeme information;agression;cout;sistema informacion	"""a r t i c l e i n f o Security countermeasures help ensure the confidentiality, availability, and integrity of information systems by preventing or mitigating asset losses from Cybersecurity attacks. Due to uncertainty, the financial impact of threats attacking assets is often difficult to measure quantitatively, and thus it is difficult to prescribe which countermeasures to employ. In this research, we describe a decision support system for calculating the uncertain risk faced by an organization under cyber attack as a function of uncertain threat rates, countermeasure costs, and impacts on its assets. The system uses a genetic algorithm to search for the best combination of countermeasures, allowing the user to determine the preferred tradeoff between the cost of the portfolio and resulting risk. Data collected from manufacturing firms provide an example of results under realistic input conditions. Assuring a secure information technology (IT) environment for the transaction of commerce is a major concern. The magnitude of the task is growing yearly, as attackers become more knowledgeable, more determined, and bolder in their efforts. According to a lead security expert at International Data Corporation, a global provider of market intelligence and advisory services for the IT community, """" Emerging new attack vectors, more targeted campaigns, compliance, and the layering of new technologies on the corporate infrastructure are all factors having a tremendous impact on the overall risk to an organization """" [21]. Given that corporate expenditures are usually a good indicator of the level of concern about an issue, security is obviously near the top of many IT executives' lists. According to IDC, the global market for providers of information security services is projected to exceed $32.5 billion in 2010 [21]. The New York Times recently noted that """" the intrusion into Google's computers and related attacks from within China on some thirty other companies point to the rising sophistication of such assaults and the vulnerability of even the best defenses """" [25]. The Times further states that, according to the Computer Security Institute, malware infections are up from one-half to almost two-thirds of companies surveyed last year, at an average cost of $235,000 for each organization. Finally, the newspaper observes, malware is being exploited in new terrains with malicious code that turns on cellphone microphones and cameras for the purpose of industrial spying [25]. Because of the variety of methods attackers use to try to infiltrate and disrupt …"""	artificial intelligence;computer security institute;confidentiality;countermeasure (computer);data center;data security;decision support system;decision theory;genetic algorithm;information security;information system;malware;microphone;mobile phone;norm (social);the new york times;the times;threat (computer)	Loren Paul Rees;Jason Deane;Terry R. Rakes;Wade H. Baker	2011	Decision Support Systems	10.1016/j.dss.2011.02.013	genetic algorithm;decision support system;asset;computer science;artificial intelligence;operations management;fuzzy set;operations research;computer security	Security	-4.044672300228793	-14.713747772843094	174409
e80ac00bae58bce4b9ee68edc0b0bb82d10f4935	global variation of outputs with respect to the variation of inputs in performance analysis; generalized rts	perturbation method;analisis envolvimiento datos;rendement echelle;retorno de escala;efficiency;estimation non parametrique;partial information;classification;return to scale;imperfect information;non parametric estimation;analyse parametrique;programacion lineal;data envelopment analysis;metodo perturbacion;performance analysis;linear programming;informacion imperfecta;programmation lineaire;linear program;contraccion;parametric analysis;methode perturbation;estimacion no parametrica;data envelope analysis;clasificacion;analyse enveloppement donnee;information imparfaite;returns to scale;contraction	Returns to scale (RTS) is an important topic in performance analysis, which helps managers to make decisions about the expansion or contraction of the operation of the DMU under assessment. But the RTS classification of DMUs gives only partial information, because it is a local notion. In this paper we extend the concept of RTS and we seek the precise relation between the proportional variation of outputs and the proportional variation of inputs. An approach is provided, which is able to determine this relation based on the parametric analysis and perturbation in linear programming.	profiling (computer programming)	M. Zarepisheh;Majid Soleimani-Damaneh	2008	European Journal of Operational Research	10.1016/j.ejor.2007.01.046	returns to scale;econometrics;mathematical optimization;economics;linear programming;data envelopment analysis;mathematics;operations research;statistics	Robotics	0.14096759314094154	-14.69229902800276	174411
b8c86e9cfa2e03f37a85b13ec6a6bcb21180a4bb	chance constrained programming with fuzzy parameters	fuzzy simulation;fuzzy programming;numero difuso;fuzzy number;algoritmo borroso;nombre flou;programmation stochastique;algoritmo genetico;resolucion problema;fuzzy algorithm;algorithme genetique;algorithme flou;genetic algorithm;programmation floue;stochastic programming;programacion estocastica;programacion difusa;problem solving;resolution probleme;chance constraint	This paper extends chance constrained programming from stochastic to fuzzy environments. Analogous to stochastic programming, some crisp equivalents of chance constraints in fuzzy environments are presented. We also propose a technique of fuzzy simulation for the chance constraints which are usually hard to be converted to their crisp equivalents. Finally, a fuzzy simulation based genetic algorithm is designed for solving this kind of problems and some numerical examples are discussed.		Baoding Liu;Kakuzo Iwamura	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00236-9	stochastic programming;mathematical optimization;genetic algorithm;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set operations;algorithm	Robotics	0.4663261439341562	-16.741075809468363	175630
f902bfa72b882cb1184def84dd4d3d852feee2dc	complex system analysis of market return percolation model on sierpinski carpet lattice fractal	dong yanfang wang jun sierpinski地毯 多重分形分析 a股市场 复杂系统分析 渗流模型 自相关分析 价格变化 金融市场 complex system analysis of market return percolation model on sierpinski carpet lattice fractal	This paper investigates the statistical behaviors of fluctuations of price changes in a stock market. The Sierpinski carpet lattice fractal and the percolation system are applied to develop a new random stock price for the financial market. The Sierpinski carpet is an infinitely ramified fractal and the percolation theory is usually used to describe the behavior of connected clusters in a random graph. The authors investigate and analyze the statistical behaviors of returns of the price model by some analysis methods, including multifractal analysis, autocorrelation analysis, scaled return interval analysis. Moreover, the authors consider the daily returns of Shanghai Stock Exchange Composite Index, and the comparisons of return behaviors between the actual data and the simulation data are exhibited.	complex system;fractal;percolation;sierpinski carpet;system analysis	Yanfang Dong	2014	J. Systems Science & Complexity	10.1007/s11424-014-2073-5	combinatorics;mathematics;mathematical economics	Logic	2.7337029448790946	-12.575921538033892	177031
3765f353bef5379683e8c6568a5c5180a92f9b9b	estimating demand heterogeneity using aggregated data: an application to the frozen pizza category	generalized method of moments;sample size;random coefficients logit model;market share;monte carlo study;price elasticity;demand estimation;time series data;logit model;new products;generalized method of moment;random coefficients	T paper combines different aggregate-level data sets to identify new product demand in consumer packaged goods (CPG) categories. Our approach augments market-level time-series data with widely available summaries of household purchase behavior, i.e., brand penetration and purchase set size data. We show that this augmentation is helpful in the estimation of consumer heterogeneity. For instance, observing a brand with relatively large shares and low penetration typically indicates that preferences are dispersed, with relatively few customers liking the brand a lot. Whereas the combination of share and penetration is informative about heterogeneity with realistic sample sizes, in isolation neither variable may lead to precise estimates of heterogeneity. In addition, other widely available data, e.g., category penetration, is helpful in estimating the size of the total market. Using a large Monte Carlo study, the paper demonstrates the benefits of the proposed approach in estimating model parameters, price elasticities, and brand switching. Empirically, the approach is used to evaluate the launch of a new national brand, DiGiorno, in the frozen pizza category. The new brand is inferred to be very successful at expanding the category, while avoiding cannibalization of existing company share. Using only standard information, i.e., market shares, to estimate the demand model leads, in our data, to poor estimates of the degree of consumer taste variation and of switching to a new brand.	aggregate data;central pattern generator;information;monte carlo method;pizza;time series	Paulo Albuquerque;Bart J. Bronnenberg	2009	Marketing Science	10.1287/mksc.1080.0403	sample size determination;price elasticity of demand;market share;economics;generalized method of moments;marketing;operations management;time series;logistic regression;advertising;statistics	AI	-0.8009983690511449	-10.509263178250071	177243
3f83102e6a6a2db1236b9f90978dad7757a0aedb	negative data in data envelopment analysis: efficiency analysis and estimating returns to scale	efficiency;data envelopment analysis;negative data;returns to scale	We address the efficiency measure in the presence of negative data.We suggest a method to discover the state of returns to scale in the presence of negative data.The main results are elaborated by some illustrative examples. The classic Data Envelopment Analysis (DEA) models developed with the assumption that all inputs and outputs are non-negative, whereas, we may face a case with negative data in the actual business world. So, the need to adapt the DEA models so that they are applicable to cases includes inputs and outputs which can take both negative and non-negative values has been an issue. It can be readily demonstrated that the assumption of constant returns to scale (CRS) is not possible in technologies under negative data. So, one of the interesting and challenge questions is how to determine the state of RTS in the presence of negative data under variable returns to scale (VRS) technology. Accordingly, in this contribution, we first address the efficiency measure and then suggest a method to discover the state of returns to scale (RTS) in the presence of negative input and output values which has not been discussed much enough so far in DEA literature. Finally, the main results are elaborated by some illustrative examples.	data envelopment analysis	Maryam Allahyar;Mohsen Rostamy-Malkhalifeh	2015	Computers & Industrial Engineering	10.1016/j.cie.2015.01.022	returns to scale;econometrics;economics;operations management;data mining;data envelopment analysis;efficiency	SE	-0.38995602664319584	-12.469196399830391	177253
16209a1a02fde8a00159c446f02eff42f51a2761	"""commentary - discussion of """"alleviating the constant stochastic variance assumption in decision research: theory, measurement, and experimental test"""""""	experimental tests;stochastic utility;choice models;discrete choice model;variance	We discuss the Salisbury and Feinberg paper [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. Marketing Sci.29(1) 1--17], setting their contribution in the historical context of the wider literature on the role of error variability in discrete choice models. We discuss the seminal nature of their contribution and suggest that the paper should be required reading for current and future Ph.D. students.		Jordan Louviere;Joffre Swait	2010	Marketing Science	10.1287/mksc.1080.0474	econometrics;discrete choice;mathematics;variance;mathematical economics;statistics	ML	-1.0274337928399626	-12.967311991578088	177620
44b206d0f109d8344ff8535e5fc0e2aca5ceb554	application of fuzzy logic controller for machine load balancing in discrete manufacturing system		Abstract. The paper presents a concept of control of discrete manufacturing system with the use of fuzzy logic. A controller based on the concept of Mam-dani was developed. The primary function realized by the controller was the balancing of machine tool loads taking into account the criteria of minimisation of machining times and costs. Two models of analogous manufacturing systems were developed, differing in the manner of assignment of production tasks to machine tools. Simulation experiments were conducted on both models and the results obtained were compared. In effect of the comparison of the results of both experiments it was demonstrated that better results were obtained in the system utilising the fuzzy inference system. 1 Introduction Most issues related with the organisation of manufacturing processes are character-ised by a high level of complexity [1,2,3]. One of the main reasons for that situation is the large number of factors that have an effect on the decisions which have to be made in that area of business [4]. Moreover, those factors are often hard to define. As the description of manufacturing processes by means of classic mathematical func-tions requires precise determination of both the variables and the manner of transfor-mations, under the conditions of uncertainty better performance is obtained with methods based on artificial intelligence, including also operations based on fuzzy sets. Fuzzy logic control is used successfully in various aspects of management, espe-cially where decision-making is necessary, as well as process control and optimisa-tion. Examples of application of fuzzy logic can be cases of combining standard tech-niques of control and fuzzy logic in compound control systems [5]. Another kind of application of fuzzy logic can be exemplified by the selection of a manipulator based on the criteria of adaptation to the kind of object [6]. In practice one can also encoun-ter applications of fuzzy logic control in discrete manufacturing systems. An example here can be the use of that solution in dynamic organisation of dispatching of parts	discrete manufacturing;fuzzy control system;fuzzy logic;load balancing (computing)	Grzegorz Klosowski;Arkadiusz Gola;Antoni Swic	2015		10.1007/978-3-319-24834-9_31	fuzzy electronics;defuzzification;adaptive neuro fuzzy inference system;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;control theory;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	-2.1967168376754134	-16.685973751913977	177711
8dfb170b9250afdfeed775b0b4aaa8edd760d734	the contribution of improved joint survival conditions to living standards: an equivalent consumption approach	measurement;mortality;standards of living;joint survival;own survival;coexistence	Individuals care not only about their own survival, but also about the survival of other persons. However, little attention has been paid so far to measuring the contribution of longer coexistence time to living standards. For that purpose, we develop a measure of coexistence time the joint life expectancy -, which quantifies the average duration of existence for a group of persons. Then, using a lifecycle model with risky lifetime, we construct an equivalent consumption measure incorporating gains in single and joint life expectancies. An empirical application to France (1820-2010) shows that, assuming independent individual mortality risks, the rise in joint life expectancies contributed to improve standards of living significantly. We examine the robustness of that result to the introduction of dependent mortality risks using copulas, and we show that equivalent consumption patterns are robust to introducing risk dependence.	coexist (image)	Gregory Ponthiere	2016	Social Choice and Welfare	10.1007/s00355-015-0919-y	standard of living;actuarial science;economics;operations management;welfare economics;measurement	AI	-3.140520772771632	-10.505592762556306	178741
17b755cbc38ad1ec6f8382c38d7f9ddbf58d56b0	parallelization of a modified firefly algorithm using gpu for variable selection in a multivariate calibration problem		This paper proposes an efficient approach for optimizing the multiple quality characteristics (QCHs) in manufacturing applications on the Taguchi method using the super efficiency technique in data envelopment analysis (DEA). Each experiment in Taguchi’s orthogonal array (OA) is treated as a decision making unit (DMU) with multiple QCHs set as inputs or outputs. DMU’s efficiency is evaluated then adopted as a performance measure to identify the combination of optimal factor levels. Three real case studies were employed for illustration in which the proposed approach provided the largest total anticipated improvements in multiple QCHs among other techniques such as principal component analysis (PCA) and DEA based ranking (DEAR) approach. Analysis of variance is finally employed to decide significant factor effects and to predict performance. (NTB), and the larger-the-better (LTB) type QCHs. In the Taguchi method, an orthogonal array (OA) is utilized to reduce the number of experiments under permissive reliability. Signal-to-noise (S/N) ratio is then adopted to optimize performance. Although this method has been widely applied, it is usually efficient for only optimizing a single QCH. Recently, optimization of multiple QCHs in the Taguchi method was faced by several studies (Tong et al., 1997; Lin & Lin, 2002; Jean & Wang, 2006; Al-Refaie et al., 2008; Al-Refaie et al., 2009). Data envelopment analysis (DEA) developed by Charnes et al. (1978) is a fractional mathematical programming technique widely DOI: 10.4018/jalr.2010040105 International Journal of Artificial Life Research, 1(2), 58-71, April-June 2010 59 Copyright © 2010, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited. used for measuring the performance of homogeneous decision making units (DMUs) with multiple inputs and multiple outputs at organizational level; such as cities, hospitals, and universities The most popular DEA technique is the CCR model developed by Charnes, Cooper, and Rhodes (1978). Assuming there are n DMUs each with m inputs and s outputs to be evaluated, then the CCR model measures the relative efficiency of each DMU once by comparing it to a group of the other DMUs that have the same set of inputs and outputs. Hence, there are n optimizations needed. The relative efficiency, Eo, of DMUo with inputs of Xio (i = 1, ..., m) and outputs of Yro (r = 1, ..., s) is evaluated by CCR model as (Charnes et al., 1994)	artificial life;automatic parallelization;data envelopment analysis;experiment;feature selection;firefly algorithm;graphics processing unit;jean;local interconnect network;mathematical optimization;parallel computing;principal component analysis;signal-to-noise ratio;taguchi methods;xio	Lauro Cássio Martins de Paula;Anderson da Silva Soares;Telma W. L. Soares;Alexandre C. B. Delbem;Clarimar José Coelho;Arlindo Rodrigues Galvão Filho	2014	IJNCR	10.4018/ijncr.2014010103	mathematical optimization;computer science;theoretical computer science;machine learning;algorithm	AI	-2.1333639052291553	-15.527265019219561	179172
1c9e8af6a7e78b70da370f2ef032e155f06fce5d	a discussion on extent analysis method and applications of fuzzy ahp	analytic hierarchy process;sequencage;aplicacion;processus hierarchie analytique;numero difuso;fuzzy number;chine;nombre flou;petroleo;sequencing;petroleum;asie;triangular fuzzy number;petrole;proceso jerarquia analitico;china;application;extent analysis method;asia	This paper proves the basic theory of the triangular fuzzy number and improves the formulation of comparing the triangular fuzzy number's size. On this basis, a practical example on petroleum prospecting is introduced.		Ke-Jun Zhu;Yu Jing;Da-Yong Chang	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00331-2	analytic hierarchy process;defuzzification;computer science;artificial intelligence;fuzzy number;operations management;calculus;sequencing;chine;petroleum;fuzzy set operations;china	SE	-0.1261824911894213	-15.973699381471606	180581
9df1c27d30781bea4414952feccc707b22b4bb7f	stability of the classification of returns to scale in fdh models	fdh;metodo polinomial;evaluation performance;analisis envolvimiento datos;entrada salida;rendement echelle;performance evaluation;temps polynomial;retorno de escala;evaluacion prestacion;estimation non parametrique;dea;classification;return to scale;stability;input output;polynomial time algorithm;non parametric estimation;envoltura libre disposicion;data envelopment analysis;mathematical programming;polynomial method;dea fdh returns to scale polynomial time algorithms stability;polynomial time;enveloppe libre disposition;estimacion no parametrica;methode polynomiale;programmation mathematique;programacion matematica;polynomial time algorithms;clasificacion;free disposal hull;entree sortie;analyse enveloppement donnee;returns to scale;tiempo polinomial	This paper deals with the estimation of returns to scale (RTS) in free disposal hull (FDH) models and provides some stability intervals for preserving the RTS classification. It has been shown that the proposed stability intervals can be obtained via a polynomial-time algorithm based on the calculation of certain ratios of inputs and outputs, without solving any mathematical programming problem. The results of the study have been proved via some lemmas and theorems and have been illustrated by numerical examples and a real application.	full domain hash	Majid Soleimani-Damaneh;A. Mostafaee	2009	European Journal of Operational Research	10.1016/j.ejor.2008.05.020	returns to scale;econometrics;economics;computer science;operations management;mathematics;statistics	Vision	0.19156564835172757	-14.703784624658972	182390
d15e0f27a08e48f18e7d257bd4a8ca34ad98085e	study on venture investment model base on signaling game	signaling game;analytical models;game theory;investments game theory pricing mathematical model decision making information analysis uncertainty signal processing technology management signal analysis;uncertainty;pricing;venture investment corporation;investment decision;bayes methods;venture capital;venture investment;biological system modeling;bayesian equilibrium;option pricing;bayesian equilibrium venture investment model signaling game venture investment decision making option value model venture enterprise venture investment corporation;venture investment decision making;venture enterprise;asymmetric information;incomplete information;games;venture investment model;mathematical model;perfect bayesian equilibrium;option value;option game;venture investment signaling game option pricing option game;option value model;venture capital bayes methods game theory;marketing and sales	This paper present a new method and model to analysis the venture investment decision based on the signaling game. We analyze and describe the venture investment decision making of two-period using mathematics models under the uncertainty to establish our option value model of venture investment. We analyze the asymmetric information between venture enterprises and venture investment corporations. Boil down the game between venture enterprises and venture investment corporations to signaling game under the incompleteness information. Get a solution of combination perfect Bayesian equilibrium.	bayesian programming	Xubo Zhang;Zigang Zhang	2008	2008 International Symposiums on Information Processing	10.1109/ISIP.2008.109	pre-money valuation;financial economics;social venture capital;finance;business	Metrics	0.7863297451267677	-12.049473173912615	182461
7edebe67cf49a8d085cc3b061e579e3748ce724d	rough set based rule induction from two decision tables	regle inference;sistema experto;rule induction;systeme aide decision;operating conditions;social decision;rugosidad;teoria conjunto;theorie ensemble;sistema ayuda decision;set theory;satisfiability;inference rule;regle decision;decision matrix;decision support system;condition operatoire;roughness;table decision;decision rules;tabla decision;rugosite;rough sets;regla decision;systeme expert;decision colectiva;point of view;rough set;decision collective;condicion operatoria;group decision;decision table;decision rule;regla inferencia;expert system;group decisions and negotiations	We study rule induction from two decision tables as a basis of rough set analysis of more than one decision tables. We regard the rule induction process as enumerating minimal conditions satisfied with positive examples but unsatisfied with negative examples and/or with negative decision rules. From this point of view, we show that seven kinds of rule induction are conceivable for a single decision table. We point out that the set of all decision rules from two decision tables can be split in two levels: a first level decision rule is positively supported by a decision table and does not have any conflict with the other decision table and a second level decision rule is positively supported by both decision tables. To each level, we propose rule induction methods based on decision matrices. Through the discussions, we demonstrate that many kinds of rule induction are conceivable.	decision table;rough set;rule induction	Masahiro Inuiguchi;Takuya Miyajima	2007	European Journal of Operational Research	10.1016/j.ejor.2005.11.054	decision table;rough set;optimal decision;influence diagram;decision support system;weighted product model;computer science;artificial intelligence;machine learning;decision tree;data mining;decision rule;mathematics;admissible decision rule;evidential decision theory;expert system;weighted sum model;statistics;dominance-based rough set approach;decision matrix	ML	0.41698818553897105	-16.170942621773246	183936
034134eee6408751e22ded5e730ea3b67c446c34	efficiency bounds and efficiency classifications in dea with imprecise data	efficiency classifications;analisis envolvimiento datos;entrada salida;systeme aide decision;imprecise data;prension;estimation non parametrique;sistema ayuda decision;prise decision;dea;gripping;classification;efficiency bounds;input output;upper bound;plan classement;non parametric estimation;decision support system;particion;data envelopment analysis;relative efficiency;borne inferieure;partition;eficacia relativa;prehension;plan clasificacion;estimacion no parametrica;borne superieure;toma decision;clasificacion;efficacite relative;lower bound;classification scheme;cota superior;cota inferior;entree sortie;analyse enveloppement donnee	This paper is concerned with the use of imprecise data in data envelopment analysis (DEA). Imprecise data means that some data are known only to the extent that the true values lie within prescribed bounds while other data are known only in terms of ordinal relations. Imprecise data envelopment analysis (IDEA) has been developed to measure the relative efficiency of decision-making units (DMUs) whose input and/or output data are imprecise. In this paper, we show two distinct strategies to arrive at an upper and lower bound of efficiency that the evaluated DMU can have within the given imprecise data. The optimistic strategy pursues the best score among various possible scores of efficiency and the conservative strategy seeks the worst score. In doing so, we do not limit our attention to the treatment of special forms of imprecise data only, as done in some of the studies associated with IDEA. We target how to deal with imprecise data in a more general form and, under this circumstance, we make it possible to grasp an upper and lower bound of efficiency. The generalized method we develop in this paper also gives rise to a new scheme of efficiency classifications that is more detailed and informative than the standard efficient and inefficient partition.		K. S. Park	2007	JORS	10.1057/palgrave.jors.2602178	econometrics;decision support system;computer science;data mining;mathematics;upper and lower bounds;operations research;statistics	DB	0.3218174789619998	-15.201591446314922	184351
8761c54ab0676b35c2d3972cf9165d24d4644d4d	interactive group decision-making procedure using weak strength of preference	forecasting;multicriteria analysis;reliability;project management;information systems;mano de obra;maintenance;systeme aide decision;social decision;informacion incompleta;soft or;information technology;analisis decision;packing;sistema ayuda decision;prise decision;operations research;location;investment;systeme aide decision groupe;journal;decision analysis;journal of the operational research society;inventory;purchasing;incomplete information;decision support system;history of or;logistics;group decision support systems;mathematical programming;weak strength of preference;marketing;scheduling;information incomplete;preferencia;production;communications technology;preference;group decision making;analisis multicriterio;computer science;operational research;analyse multicritere;decision colectiva;decision collective;toma decision;main d oeuvre;programmation mathematique;analyse decision;programacion matematica;applications of operational research;or society;jors;management science;manpower;infrastructure	In the paper, we present an interactive decision procedure for aggregating group members’ preferences which are specified in incomplete ways. A group consensus under incomplete information is not usually reached at a single step since less-specific preferences on attribute weights and performance scores make a clear selection of best alternative more difficult. To circumvent these difficulties, a measure, indicating the strength of preference between alternatives, is derived to help each of decision makers change his/her preference structure. To make preference changes based on the measure effective, we present a solution method to convert an intractable nonlinear programme into a linear one.		C. H. Han;B. S. Ahn	2005	JORS	10.1057/palgrave.jors.2601946	project management;logistics;preference learning;inventory;decision support system;economics;decision analysis;forecasting;investment;artificial intelligence;marketing;operations management;reliability;location;management;operations research;information technology;scheduling	HCI	-0.551279720510989	-15.595792944874473	184461
032209f0f2e965ac7fcbb3c76ba78afda18268ca	sampling size and efficiency bias in data envelopment analysis	sample size;data envelope analysis	In Data Envelopment Analysis when the number of decision making units is small the number of units of the dominant or e cient set is relatively large and the average e ciency is generally high The high average e ciency is the result of assuming that the units in the e cient set are e cient If this assumption is not valid this results in an overestimation of the e ciencies which will be larger for a smaller number of units Samples of various sizes are used to nd the related bias in the e ciency estimation The samples are drawn from a large scale application of DEA to bank branch e ciency The e ects of di erent assumptions as to returns to scale and the number of inputs and outputs are investigated	data envelopment analysis	Mohammad R. Alirezaee;Murray Howland;Cornelis van de Panne	1998	JAMDS	10.1155/S1173912698000030	sample size determination;econometrics;operations management;data envelopment analysis;mathematics;statistics	Vision	2.578453906027896	-10.268938557463574	184884
96272e48077ac0e5966f8582246c812334fca542	assessing the performance of biogas plants with multi-criteria and data envelopment analysis	multicriteria analysis;energy conversion;evaluation performance;analisis envolvimiento datos;renewable energy;entrada salida;electre tri;performance evaluation;economic sciences;evaluacion prestacion;estimation non parametrique;multi criteria decision analysis;prise de decision;dea;input output;non parametric estimation;ciencias economicas;energia renovable;data envelopment analysis;evaluation criteria;energie renouvelable;relative efficiency;preferencia;eficacia relativa;preference;sciences economiques;analisis multicriterio;estimacion no parametrica;biogas;analyse multicritere;conversion energie;conversion energetica;toma decision;data envelope analysis;multi criteria decision analysis dea renewable energy biogas electre tri;article;efficacite relative;entree sortie;analyse enveloppement donnee	This paper performs an assessment of 41 agricultural biogas plants located in Austria to determine their relative performance in terms of economic, environmental, and social criteria and corresponding indicators. The comparison of these renewable energy conversion plants is based on two complementary analyses. Data envelopment analysis (DEA) was conducted to provide measures of radial efficiency relative to the observed frontier of production possibilities. Multi-criteria decision analysis (MCDA), using the IRIS/ ELECTRE TRI methodology, was conducted to obtain a different perspective on the results, and as a tool that would enable to incorporate managerial preferences easily. To be able to use IRIS while keeping the spirit behind DEA, the evaluation criteria were defined as different output/input efficiency ratios, and no information about criteria weights was introduced at the outset. The results suggest that MCDA, and the use of IRIS in particular, constitutes a useful approach that can be applied in a complementary way to	data envelopment analysis;decision analysis;radial (radio)	Reinhard Madlener;Carlos Henggeler Antunes;Luis C. Dias	2009	European Journal of Operational Research	10.1016/j.ejor.2007.12.051	computer science;operations management;data envelopment analysis;mathematics;operations research;welfare economics	AI	-1.131607312109477	-14.697474093658991	185924
a0261ce72073e63fd2c05ede03656c9f3c9718a9	certainty equivalent citation: generalized classes of citation indexes	w index;citation index;h index;decision maker;scientific production;certainty equivalence;a29 other;indexation;i29 other;inequality measurement	Drawing from the existing literature on risk and inequality measurement, we implement the notion of “certainty equivalent citation” in order (i) to generalize most of the h-type citation indexes (h-, g-, $$\tilde{g},$$ t-, f-, w-index), and (ii) to highlight the centrality of the decision-maker’s preferences on distributive aspects (concentration aversion) for the ranking of citation profiles. In order to highlight the sensitivity of citation orderings with respect to concentration aversion, an application to both simulated and real citation profiles is presented.	centrality;citation index;risk aversion;social inequality	Antonio Abatemarco;Roberto Dell'Anno	2012	Scientometrics	10.1007/s11192-012-0758-x	decision-making;data mining;mathematics;statistics	AI	-1.4499464841516285	-12.54806287900379	186285
a7f5ec69d9f8f7b0a0df069a13ae7902780c3271	hospital ownership and operating efficiency: evidence from taiwan	intensive care unit;statistical test;empirical evidence;data envelopment analysis;hospital ownership;relative efficiency;cross section;national health insurance;health services;data envelope analysis;operating efficiency;health care	This paper employs the non-parametric data envelopment analysis to document empirical evidence on the relationship between hospital ownership and operating efficiency using annual cross-sectional data on Taiwan hospitals over the period 1996–1997. Hospitals within the same category are compared on the basis of their relative efficiency. Conventional and data-envelopment-analysis-based test procedures are employed to test for efficiency differences between public and private hospitals. The statistical test results indicate that, in general, public hospitals are less efficient than private hospitals for both regional and district hospitals. Specifically, we provide evidence that private hospitals without intensive-care units outperform their public counterparts.		Hsihui Chang;Mei-Ai Cheng;Somnath Das	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00412-0	actuarial science;operations management;data envelopment analysis;mathematics;statistics	Theory	-1.700013835863005	-13.64627772746176	186900
6aad8e55d63bd38f74bd3d43b97108b343a3908f	using the transferable belief model and a qualitative possibility theory approach on an illustrative example: the assessment of the value of a candidate	transferable belief model;multicriteria analysis;multiagent system;intervalo confianza;value analysis;analyse valeur;confidence interval;intervalle confiance;multievaluation;analisis valor;possibility theory;evaluation candidat;analisis multicriterio;analyse multicritere;sistema multiagente;teoria posibilidad;systeme multiagent;theorie possibilite	The problem of assessing the value of a candidate is viewed here as a multiple combination problem. On the one hand a candidate can be evaluated according to different criteria, and on the other hand several experts are supposed to assess the value of candidates according to each criterion. Criteria are not equally important, experts are not equally competent or reliable. Moreover levels of satisfaction of criteria, or levels of confidence are only assumed to take their values in linearly ordered scales, whose nature is rather qualitative. The problem is discussed within two frameworks, the transferable belief model and the qualitative possibility theory. They respectively offer a quantitative and a qualitative setting for handling the problem, providing thus a way to emphasize what are the underlying assumptions in each approach.	eisenstein's criterion;ordered pair;possibility theory	Didier Dubois;Michel Grabisch;Henri Prade;Philippe Smets	2001	Int. J. Intell. Syst.	10.1002/int.1058	possibility theory;confidence interval;computer science;artificial intelligence;operations research	AI	-1.2089703019503681	-16.16666816920704	186976
b46ccda61517032561aa2e6bd1326e7af1f99cf9	measuring service quality in macau luxury hotels using the qfd method: a case study	quality assurance;quality function deployment;qfd;quality measurement;hospitality industry;customer satisfaction;tourism development;macau;economic development;luxury hotels;service quality	With the vigorous development of its gaming industry, Macau has become a very popular tourist destination and many well-known brands in the hospitality industry have entered this highly competitive market. In order to improve service quality and maximise customer satisfaction, it is critical for hotels in this region to clearly understand the true needs of their customers. This study applied the quality function deployment (QFD) method to empirical data collected from 280 Macau customers at three hotels, four Macau hospitality managers, and one academic expert. The resulting customer requirement weights and improvement priorities can be used by hotel administrators to assess their current service offerings and to make adjustments as needed to achieve service quality assurance. Further, the study shows how the QFD method, with some modifications, can be applied more generally by tourism and economic development agencies in creating programs which support improvement priorities across an entire region.	quality function deployment	Chen-Kuo Pai;Shun-Hsing Chen;David Hinds	2016	IJSTM	10.1504/IJSTM.2016.077668	quality assurance;quality function deployment;economics;marketing;operations management;advertising;service quality	SE	-2.710901097052823	-13.482864108674168	187176
bcf846dc2e95a9277f6dd24b1cab1afe9238eacf	discussing landscape compositional scenarios generated with maximization of non-expected utility decision models based on weighted entropies	decision models;precautionary approach;system manifold;landscape services;landscape diversity;weighted shannon entropy;weighted gini simpson index;non expected utility methods;economic values	The search for hypothetical optimal solutions of landscape composition is a major issue in landscape planning and it can be outlined in a two-dimensional decision space involving economic value and landscape diversity, the latter being considered as a potential safeguard to the provision of services and externalities not accounted in the economic value. In this paper, we use decision models with different utility valuations combined with weighted entropies respectively incorporating rarity factors associated to Gini-Simpson and Shannon measures. A small example of this framework is provided and discussed for landscape compositional scenarios in the region of Nisa, Portugal. The optimal solutions relative to the different cases considered are assessed in the two-dimensional decision space using a benchmark indicator. The results indicate that the likely best combination is achieved by the solution using Shannon weighted entropy and a square root utility function, corresponding to a risk-averse behavior associated to the precautionary principle linked to safeguarding landscape diversity, anchoring for ecosystem services provision and other externalities. Further developments are suggested, mainly those relative to the hypothesis that the decision models here outlined could be used to revisit the stability-complexity debate in the field of ecological studies.	benchmark (computing);ecosystem services;entropy (information theory);expectation–maximization algorithm;expected utility hypothesis;risk aversion;shannon (unit);simpson's rule	José Pinto Casquilho;Francisco Castro Rego	2017	Entropy	10.3390/e19020066	decision model;optimal decision;actuarial science;mathematical economics;statistics	ML	-1.6551011422008894	-12.169212117349014	187620
70eb2204c118f789a212f166140af2a8c0d32929	efficiency analysis with ratio measures	technology;efficiency;production assumptions;data envelopment analysis;ratio measures;article	In applications of data envelopment analysis (DEA) data about some inputs and outputs is often available only in the form of ratios such as averages and percentages. In this paper we provide a positive answer to the long-standing debate as to whether such data could be used in DEA. The problem arises from the fact that ratio measures generally do not satisfy the standard production assumptions, e.g., that the technology is a convex set. Our approach is based on the formulation of new production assumptions that explicitly account for ratio measures. This leads to the estimation of production technologies under variable and constant returns-to-scale assumptions in which both volume and ratio measures are native types of data. The resulting DEA models allow the use of ratio measures “as is”, without any transformation or use of the underlying volume measures. This provides theoretical foundations for the use of DEA in applications where important data is reported in the form of ratios.		Ole Bent Olesen;Niels Christian Petersen;Victor V. Podinovski	2015	European Journal of Operational Research	10.1016/j.ejor.2015.03.013	econometrics;economics;data envelopment analysis;mathematics;efficiency;welfare economics;statistics;technology	ML	-0.24567861530610913	-12.390088548041284	187773
262241f8c14941416459f880a4ed993bae275df5	normalization procedures on multicriteria decision making - an example on environmental problems	multicriteria decision making;environmental problem	In Multicriteria Decision Making, a normalization procedure is required to conduct the aggregation process. Even though this methodology is widely applied in strategic decision support systems, scarce published papers detail this specific question. In this paper, we analyze the results of the influence of normalization procedures in the weight sum aggregation in Multicriteria Decision problems devoted to sustainable development.	database normalization;decision support system;decision theory;eisenstein's criterion;relevance;subject reduction	Ethel Mokotoff;Estefanía García;Joaquín Pérez Ortega	2010			computer science;business decision mapping	ML	-3.8397701553366037	-16.68844494710526	187939
6fcf36cf628933582b90d490b98ef332aef458c8	mining criminal databases to finding investigation clues—by example of stolen automobiles database	extraction information;police;association statistique;gestion de calidad;seguro;decision support;gestion memoire;linguistique;steganographie;automovil;analisis datos;secteur public;information extraction;systeme aide decision;storage management;securite informatique;sector publico;data management;database;base dato;statistical association;resumen;intelligence artificielle;justice;almacen dato;sistema ayuda decision;data mining;classification;public sector;computer security;vida privada;data analysis;steganography;gestion memoria;decision support system;esteganografia;asociacion estadistica;linguistica;assurance;private life;association rule;fouille donnee;automobile;resume;law enforcement;seguridad informatica;motor car;decouverte connaissance;base de donnees;policia;gestion de la qualite;artificial intelligence;vie privee;descubrimiento conocimiento;analyse donnee;inteligencia artificial;entrepot donnee;data warehouse;private information;abstract;busca dato;clasificacion;extraccion informacion;insurance;quality management;justicia;knowledge discovery;linguistics	While businesses have been extensively using data mining to pursue everlasting prosperity, we seldom consider this technique in public affairs. The government holds a large quantity of data that are records of official operations or private information of the people. These data can be used for increasing benefits of the people or enhancing the efficiency of governmental operations. In this paper we will apply this technique to the data of stolen automobiles to explore the unknown knowledge hidden in the data and provide this knowledge to transportation, insurance as well as law enforcement for decision supports. The data we use are abstracted from 378 thousand records of stolen automobiles in the past eleven years in Taiwan. After constructing a data warehouse, we apply the technique of classification, association rule, prediction, data generalization and summarization-based characterization to discover new knowledge. Our results include the understanding of automotive theft, possibility of finding stolen automobiles, intrigue in theft claims, etc. The knowledge we acquired is useful in decision support, showing the applicability of data mining in public affairs. The experience we gathered in this study would help the use of this technique in other public sectors. Along with the research results, we suggest the law enforcement to consider data mining as a new means to investigate criminal cases, to set up a team of criminal data analysis, to launch a new program to crack down automotive thefts, and to improve the quality of criminal data management.		Patrick S. Chen;Kimberlee Chestnut Chang;Tai-Ping Hsing;Shihchieh Chou	2006		10.1007/11734628_12	decision support system;insurance;computer science;artificial intelligence;justice;data mining;database;public sector;steganography;data analysis;operations research;computer security	DB	2.7154193162673046	-16.21356433786755	188041
11a842c1924e9496e952d8bfeaab115e5cf3fd0c	rough approximation of a preference relation by dominance relations	business and management;modelizacion;eleccion;relation dominance;rough set theory;rangement;teoria conjunto;theorie ensemble;set theory;computing;decision problem;modelisation;regle decision;information preferentiel;ranking;multi criteria choice and ranking problems;approximation rugueuse;rough sets theory;decision rules;relation preference;regla decision;preference modeling;water supply system;choix;rough set;business information systems;ordenamiento;modeling;choice;decision rule;pairwise comparison table	An original methodology for using rough sets to preference modeling in multi-criteria decision problems is presented. This methodology operates on a pairwise comparison table (PCT), including pairs of actions described by graded preference relations on particular criteria and by a comprehensive preference relation. It builds up a rough approximation of a preference relation by graded dominance relations. Decision rules derived from the rough approximation of a preference relation can be used to obtain a recommendation in multi-criteria choice and ranking problems. The methodology is illustrated by an example of multi-criteria programming of water supply systems.	approximation	Salvatore Greco;Benedetto Matarazzo;Roman Slowinski	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00127-1	rough set;computer science;artificial intelligence;machine learning;management information systems;data mining;decision rule;mathematics;statistics;dominance-based rough set approach	NLP	-1.0613430416465823	-16.833763165375238	189102
3a7139bdcbd8b289d056d8e994649d1020d53597	observational data-driven modeling and optimization of manufacturing processes		Abstract The dramatic increase of observational data across industries provides unparalleled opportunities for data-driven decision making and management, including the manufacturing industry. In the context of production, data-driven approaches can exploit observational data to model, control and improve process performance. When supplied by observational data with adequate coverage to inform the true process performance dynamics, they can overcome the cost associated with intrusive controlled designed experiments and can be applied for both process monitoring and improvement. We propose a novel integrated approach that uses observational data for identifying significant control variables while simultaneously facilitating process parameter design. We evaluate our method using data from synthetic experiments and also apply it to a real-world case setting from a tire manufacturing company.	mathematical optimization	Najibesadat Sadati;Ratna Babu Chinnam;Milad Zafar Nezhad	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.10.028	data mining;management science;process variable;tire manufacturing;observational study;exploit;computer science;manufacturing	ML	-0.02391425449918119	-13.118677147793928	189212
ca6f84e85f266f54effc8e563dd1df6a932194fe	a marketing category management system: a decision support system using scanner data	forecasting;prevision;management system;consumer good;marketing management;bvar forecasts;systeme aide decision;analyse tendance;decision bayes;comercializacion;autoregression;trend analysis;scanneur;comportement consommateur;sistema ayuda decision;vecteur;decision maker;bayes decision;code barre;scanner;commercialisation;decision support system;bar code;marketing;scanner data;decision support systems;comportamiento consumidor;codigo barra;autoregresion;barredor;consumer behavior;vector;point of sale;bien consumacion;bien consommation;analisis tendencia	Point-of-sale scanner data provides a unique opportunity for analyzing consumer package goods (CPG) trends and patterns. Decisions of ever-increasing complexity are made possible by the amount of data available. Unfortunately, the analysis of such voluminous data requires complex techniques and processing requirements not available to many marketing decision makers. In this paper, we describe a prototype system which allows users to manage the complex models and scanner data to make forecasts in an interactive fashion. A limited test of the prototype allowed users not familiar with the underlying models to develop product forecasts.	decision support system	James J. Jiang;Gary Klein;Roger Alan Pick	1998	Decision Support Systems	10.1016/S0167-9236(98)00053-0	decision-making;decision support system;trend analysis;vector;forecasting;computer science;artificial intelligence;marketing;operations management;data mining;management system;autoregressive model;point of sale;statistics	Robotics	-1.3609300346431406	-11.544207676672244	190286
6c6e29186afd71abe286105c337064e615c8848f	portfolio resampling in malaysian equity market		Since the work of Markowitz (1959) and Sharpe (1964), Mean-Variance (MV) analysis has been a central focus of financial economics. Problems involving quadratic objective functions generally incorporate a MV analysis. However, estimation error is known to have huge impact on MV optimized portfolios, which is one of the primary reasons to make standard Markowitz optimization unfeasible in practice. In these studies we focus on a relatively new approach introduced by Michaud (1998), resampled efficiency. Michaud argues that the limitations of MV efficiency in practice generally derive from a lack of statistical understanding of MV optimization. He advocates a statistical view of MV optimization that leads to new procedures that can reduce estimation error. Optimal portfolio based on MV efficiency and resampled efficiency is compared in an empirical out-of sample study in term of their performances using Malaysian stock market. We divided the data to three groups, daily, weekly and monthly. We found that, resampled efficiency performed well and group of daily and weekly data have the least estimation error.	equity crowdfunding	Siti Nurleena Abu Mansor;Adam Baharum;Anton Abdulbasah Kamil	2006	Monte Carlo Meth. and Appl.	10.1515/156939606778705146	econometrics	Robotics	0.5118446495740835	-13.622918496192582	191029
78f6087db4c342044c535cc4b92a037c916c7e79	returns to scale of two-stage production process	data envelopment analysis;network dea;two stage production process;returns to scale	Network data envelopment analysis (network DEA) is a non-parametric approach for measuring the relative efficiencies of decision making units (DMUs) with multi-stage structures. With the application of network DEA techniques, this research studies two-stage production process, and proposes definitions of returns to scale (RTS) as well as the necessary and sufficient conditions for distinguishing the status of RTS of each stage under the framework of network DEA models in two cases. The first case is that the initial input takes changes only to maximize the ratio between the intermediate output and the initial input, and the final output changes following the corresponding change of intermediate product, i.e., Stage 1 is ‘‘leader” and Stage 2 is ‘‘follower”; the second case is that the initial input changes so as to obtain the maximal sum of ratio between the output and the input of the two stages. In these two cases, a new method of calculating the changing quantity of the initial input is proposed, and the relationships of RTS between the sub-stage and the entire production process are determined consequently. Numerical examples illustrate the theoretical results. 2015 Elsevier Ltd. All rights reserved.	data envelopment analysis;maximal set;numerical method	Qianwei Zhang;Zhihua Yang	2015	Computers & Industrial Engineering	10.1016/j.cie.2015.09.009	returns to scale;econometrics;economics;computer science;operations management;data envelopment analysis;operations research	AI	-2.1874973065462058	-14.426464718426587	191323
cf3cd6b9e89ad9e41d2dc8cbc9c0b127328b2a9c	a practical framework for automatic food products classification using computer vision and inductive characterization	computer vision;food products;inductive characterization;feature selection;framework	With the increasingly international regulatory demands for food products import and export, as 9 well as with the increased awareness and sophistication of consumers, the food industry needs accurate, 10 fast and efficient quality inspection means. Each producer seeks to ensure that their products satisfy all 11 consumer’s expectations and that the appropriate quality level of each product is offered and sold to each 12 different socio-economic consumer group. This paper presents a framework that uses computer vision and 13 inductive characterization with a reduced set of features, along with three cases where this framework has 14 been successfully applied to improve the quality inspection process. Three different basic food products 15 are studied: Hass Avocado, Manila Mango and Corn Tortillas. These products are very important in 16 economical terms for the sheer volume of their production and marketing. Each product has particular 17 characteristics that involve different ways of handling the quality inspection process, but this framework 18 allows addressing common key points that allow automatizing this process. Experimental results of each 19 case shows that the proposed technique is competitive with existing systems and has significantly lower 20 costs in terms of the number of features required for classification. 21	computer vision;inductive reasoning;mango;unknotting problem	Hiram Calvo;Marco A. Moreno-Armendáriz;Salvador Godoy-Calderón	2016	Neurocomputing	10.1016/j.neucom.2015.06.095	simulation;computer science;software framework;machine learning;feature selection	Web+IR	-2.662354888980242	-11.82294067664245	192642
8db578487fa6a0f83bb0421a6cfc1207548b962e	gestalt system of holistic graphics: new management support view of mcdm	finanza;finance;decision aid;perfil;agregat;profile;ayuda decision;prise decision;multiple criteria;teoria decision;informacion;agregado;theorie decision;decision theory;multiple critere;aide decision;systeme holistique;toma decision;information;profil;aggregate	Abstract   We present a holistic system of unaggregated multidimensional “profiles”, avoiding the information-destroying reduction of multicriterion dimensionality to a single “number”. Visualization of system information is proposed through transforming numerical data into graphical images. This enhances human ability (and preference) to reason directly on the basis of a graphical  Gestalt .  The system is driven by attainable and non-attainable ideals, aspiration levels or referential structures and their purposeful redefinition under changing circumstances.  Management support systems, of which any MCDM technology must be an integral part, both impose and reflect system-organizational requirements and conditions that should not be ignored by methodological developments or applications. Referential or aspirational structures are powerful strategic tools which enhance the flexibility, responsiveness and competitiveness of a modern enterprise.	gestalt psychology;graphics;holism	Eero Kasanen;Ralf Östermark;Milan Zeleny	1991	Computers & OR	10.1016/0305-0548(91)90093-7	aggregate;information;decision theory;computer science;artificial intelligence;mathematics;operations research;statistics	Graphics	-1.3182450935291912	-14.74943460465575	193199
8a95a309bf9fd51597860791dbbaec88d5a402ad	the conceptual link between measurements, evaluations, preferences and indicators, according to the representational theory	representation theory;judgment;indicator;methode empirique;measurement;metodo empirico;empirical method;classification;plan classement;dictation;jugement;preferencia;evaluation;plan clasificacion;preference;juicio;representational theory of measurement;clasificacion;classification scheme	This paper presents a classification of the different ways by which observable or non-directly observable properties of an object or an event can be judged/described. The reference framework is the representational theory. The definition of measurement as an empirical and objective operation is the starting point. The concept of representational measurement is compared with those of evaluation, preference and indicator. Evaluation maintains the empiricity but not the objectivity of measurement: there is no unanimously acknowledged reference for the description of latent constructs. Preference is neither empirical nor objective: every subject has his/her own relation to express the judgment and this relation is not exogenously known. This article shows how all these operations can be considered as separate subsets of the concept of indicator. In the end a further operation, the dictation, is introduced. According to this scheme of classification, dictation is objective but not empirical. Finally, we trace possible research paths to be undertaken for further analysis on the argument. 2006 Elsevier B.V. All rights reserved.	enigma machine;latent variable;objectivity/db;observable;pareto efficiency;relevance;semantic interpretation;system of measurement	P. Cecconi;Fiorenzo Franceschini;Maurizio Galetto	2007	European Journal of Operational Research	10.1016/j.ejor.2006.03.018	judgment;representation theory;biological classification;classification scheme;artificial intelligence;evaluation;mathematics;empirical research;algorithm;measurement;statistics	AI	-1.2470450386043153	-15.841692281763043	193285
f24b9094f5014f314da2a04a768b9248b1142194	recognizing strong and weak congestion slack based in data envelopment analysis	returns to scale rts;efficient;slack;data envelopment analysis dea;congestion	The common concept of congestion is that a decrease (increase) in one or more inputs of a decision making unit (DMU) causes an increase (decrease) in one or more outputs (Cooper, Gu, & Li, 2001a). So far several congestion approaches have been proposed in DEA (data envelopment analysis) literature by many authors, such as Färe’s et al. (FGL), Brockett’s et al. (BCSW), and Tone and Sahoo’s congestion approaches (Färe, Grosskopf, & Lovell, 1985, 1994; Brockett, Cooper, Shin, & Wang, 1998; Tone & Sahoo, 2004). Tone and Sahoo’s approach (Tone & Sahoo, 2004) is one of the most robust congestion approaches in DEA literature. Moreover, Tone and Sahoo’s approach has some advantages with respect to FGL and BSCW congestion approaches. However, the proposed approaches have many difficulties to treat congestion. For instance, in the presence of alternative optimal solutions, the approach proposed by Tone and Sahoo is unable to detect congestion (strong and weak). Moreover, in Tone and Sahoo’s approach, all inputs and outputs of decision making units (DMUs) have been considered positive, while in real world, data is often non-negative. In this research, a slack-based DEA approach is proposed to recognize congestion (strong and weak) for the target DMUs. One of the advantages of our proposed approach is capable of detecting congestion (strong and weak) for evaluating the DMUs in the presence of alternative optimal solutions. Other advantage of our research is capable of identifying congesting (strong and weak) DMUs with non-negative inputs and outputs. However in these situations, Tone and Sahoo’s congestion approach is incapable of identifying congestion. Lastly, we apply the approach to the data sets for making comparisons between the proposed approach and Tone and Sahoo’s approach then some conclusions are drawn and directions for future research are suggested. 2012 Elsevier Ltd. All rights reserved.	data envelopment analysis;fifth-generation programming language;network congestion;sensor;slack variable;tone mapping;wang tile	Mohammad Khoveyni;Robabeh Eslami;Mohammad Khodabakhshi;Gholam Reza Jahanshahloo;F. Hosseinzadeh Lotfi	2013	Computers & Industrial Engineering	10.1016/j.cie.2012.11.014	simulation;slack bus;operations management;operations research	AI	-2.519275659082251	-15.559781641383912	193786
d04ea947176d5b11b252ff3e6ac15c3227a9e7a9	application of facts-set regression in bid evaluation	contractor selection facts set regression analysis bid evaluation procurement equivalent benefit curve indexes related evaluation system linear correlation nonlinear correlation;decision support;application software;procurement;evaluation method;regression analysis procurement purchasing;bid evaluation;purchasing;procurement regression analysis concrete arithmetic machine vision man machine systems application software computer science cost accounting forward contracts;cost accounting;forward contracts;machine vision;facts set;indexation;arithmetic;regression analysis;computer science;facts set regression analysis procurement bid evaluation decision support;man machine systems;concrete	Customarily modern procurement puts to use a multi-purpose way, so the bid valuation and contractor selection turn to be a great problem. The purpose of this paper is to provide a new method to resolve the problem by using regression analysis based on facts-set. This method can evaluation all bids with some cheap computer software. By proving the scientific of this new method, I thinks that it can avoid the defects of the past evaluation methods efficiently. In virtue of the regression analysis to carry on the bid evaluation, this paper puts forward three kinds of different concrete ways: comparison of the residuals, using equivalent-benefit curve, setting up indexes-related evaluation system. The method can be used separately to trade off two or several factors, and can handle indexes whatever linear correlation or nonlinear correlation. This method can also be used in conjunction with other methods in the procession of bid evaluation, to better solve problems in our life.	multi-purpose viewer;nonlinear system;procurement;usability;value (ethics)	Zhigang Zhou	2010	2010 International Conference on Machine Vision and Human-machine Interface	10.1109/MVHI.2010.163	application software;concrete;procurement;machine vision;computer science;artificial intelligence;forward contract;regression analysis;cost accounting	Robotics	-2.690833557794675	-14.805357678244388	194161
c60e4ff8278f70e597717998a588db6c4925233f	evaluation of disaster level caused by power quality problems	power quality;evaluation method;power systems;decision making power quality problems disaster level evaluation customers care economical considerations;power supply quality;power quality power generation economics steady state decision making fuzzy neural networks power system harmonics voltage fluctuations consumer electronics power engineering and energy power markets;indexes;monitoring;indexation;economics;power supply quality decision making disasters;disasters;harmonic analysis	It is difficult to find a reasonable and accepted index to evaluate the comprehensive level of power quality because of the great difference in characteristics and effects among all the individual problems. A new evaluation method is proposed which responses to what the customers care, based on the economical considerations. Example is given to show the advantages of the proposed method, including straightforward meaning, practicability in decision-making for preferential improvement of power quality problems.	electric power quality	Yongqiang Zhu;Yingying Liu;Yonghai Xu;Xiangning Xiao	2008	APCCAS 2008 - 2008 IEEE Asia Pacific Conference on Circuits and Systems	10.1109/APCCAS.2008.4746003	database index;disaster;actuarial science;computer science;engineering;electrical engineering;harmonic analysis;management science;electric power system	EDA	-3.176660640524912	-14.011318021056823	194538
c0265002a1bc02a875cdddb7b4783358d629aaa2	portfolio selection with skewness: a comparison of methods and a generalized one fund result	shortage function;mean variance;efficient frontier;mean variance skewness;pgp	This contribution compares existing and newly developed techniques for geometrically representing mean–variance–skewness portfolio frontiers based on the rather widely adapted methodology of polynomial goal programming (PGP) on the one hand and the more recent approach based on the shortage function on the other hand. Moreover, we explain the working of these different methodologies in detail and provide graphical illustrations in relation to the goal programming literature in operations research. Inspired by these illustrations, we prove two new results: a formal relation between both approaches and a generalization of the well-known one fund separation theorem from traditional mean–variance portfolio theory. 2013 Elsevier B.V. All rights reserved.	2d computer graphics;gabbay's separation theorem;goal programming;graphical user interface;maximal set;modern portfolio theory;operations research;polynomial;pretty good privacy;situated;utility;whole earth 'lectronic link;wrapping (graphics)	Walter Briec;Kristiaan Kerstens;Ignace Van de Woestyne	2013	European Journal of Operational Research	10.1016/j.ejor.2013.04.021	financial economics;efficient frontier;econometrics;mathematical optimization;economics;marketing;mathematics;statistics	AI	1.8998159918294453	-11.236410294811014	194724
23e682af363581e7c494a34db052ea030dd0a82b	online pricing dynamics in internet retailing: the case of the dvd market	rate of change;regression model;cross sectional heterogeneity;price dispersion;price level;price competition;online branch of multichannel retailer;internet marketing;price dynamics;cross section;dotcom;random coefficients	The explosive growth of Internet retailing offers an excellent opportunity to collect online prices at a disaggregate (e.g., individual store and/or individual product) level over time and to investigate the evolution of Internet markets. In this paper, we generalize the results obtained in existing static analyses and develop two random coefficient regression models to capture the dynamics of prices in the US online DVD market. On the basis of the models, we test hypotheses to compare the rates of change in price levels and in price dispersion at both pure dotcoms and online branches of multichannel retailers in the DVD market. The results, based on the analysis of 6759 price quotes over a 12-month period, suggest that multichannel retailers effectively differentiated themselves from pure dotcoms on nonprice dimensions so that they charged higher prices and maintained the difference in price levels throughout the time period of the study. Head-to-head price competition within pure dotcoms tended to be more severe. Our results also suggest that there is a sign of maturity in the current US online DVD market. 2010 Elsevier B.V. All rights reserved.	capability maturity model;coefficient;dot-com company;omnichannel;static program analysis	Baibing Li;Fang-Fang Tang	2011	Electronic Commerce Research and Applications	10.1016/j.elerap.2010.06.004	digital marketing;economics;limit price;marketing;mid price;cross section;advertising;price level;regression analysis;commerce	ECom	2.680844589920236	-12.89017328294067	194831
61eda171fb81f8e41842ac1ae5e4bc65a9148029	on constructing a composite indicator with multiplicative aggregation and the avoidance of zero weights in dea	efficiency;data envelopment analysis	Recently there has been interest in combining the use of multiplicative aggregation together with data envelopment analysis (DEA). For example Blancas et al (2013), Cook and Zhu (2013), Giambona and Vassallo (2013); the first two of these are JORS papers. The purpose of this note is to highlight differences in the way multiplicative DEA is being applied and to draw attention to the fact that a units-invariant (i.e. scale-invariant) form is available. Moreover, this model avoids the ‘zero weight problem’ in DEA (where criteria are effectively ignored).	data envelopment analysis;word lists by frequency	Chris Tofallis	2014	JORS	10.1057/jors.2013.137	econometrics;economics;operations management;data envelopment analysis;mathematics;efficiency;operations research;statistics	AI	-1.0134704950327083	-12.50420247251715	195418
c51afa130a13d8d57b67225cfd5e840784b8b95f	behavioral and procedural consequences of structural variation in value trees	splitting bias;multiattribute value theory;multi attribute value theory splitting bias value tree weight elicitation;conjoint analysis;value tree;multiattribute value theory splitting bias value tree weight elicitation;multi attribute value theory;weight change;weight elicitation	Our experiment shows that the division of attributes in value trees can either increase or decrease the weight of an attribute. The structural variation of value trees may also change the rank of attributes. We propose that our new ®ndings related to the splitting bias, some other phenomena appearing with attribute weighting in value trees, and the number-of-attribute-levels eect in conjoint analysis may have the same origins. One origin for these phenomena is that decision makers' responses mainly re¯ect the rank of attributes and not to the full extent the strength of their preferences as the value theory assumes. We call this the unadjustment phenomenon. A procedural source of biases is the normalization of attribute weights. One consequence of these two factors is that attribute weights change if attributes are divided in a value tree. We also discuss how the biases in attribute weighting could be avoided in practice.	value (computer science)	Mari Pöyhönen;Hans Vrolijk;Raimo P. Hämäläinen	2001	European Journal of Operational Research	10.1016/S0377-2217(00)00255-1	variable and attribute;economics;marketing;data mining;conjoint analysis;mathematics;welfare economics	ML	-3.628404156164903	-11.377219877100265	197337
49cd27a45c1f9f2a4ea6efb2464e90d94dcff062	a data-driven method for in-game decision making in mlb: when to pull a starting pitcher	mlb;predictive modeling	Professional sports is a roughly $500 billion dollar industry that is increasingly data-driven. In this paper we show how machine learning can be applied to generate a model that could lead to better on-field decisions by managers of professional baseball teams. Specifically we show how to use regularized linear regression to learn pitcher-specific predictive models that can be used to help decide when a starting pitcher should be replaced. A key step in the process is our method of converting categorical variables (e.g., the venue in which a game is played) into continuous variables suitable for the regression. Another key step is dealing with situations in which there is an insufficient amount of data to compute measures such as the effectiveness of a pitcher against specific batters.   For each season we trained on the first 80% of the games, and tested on the rest. The results suggest that using our model could have led to better decisions than those made by major league managers. Applying our model would have led to a different decision 48% of the time. For those games in which a manager left a pitcher in that our model would have removed, the pitcher ended up performing poorly 60% of the time.	data-driven programming;machine learning;predictive modelling;quantum key distribution;venue (sound system);virtual world	Gartheeban Ganeshapillai;John V. Guttag	2013		10.1145/2487575.2487660	predictive analytics;simulation;computer science;machine learning;data mining;operations research	ML	1.651655720914599	-13.507505520270891	197534
2bae5f25edd57bff28bef0b9b83bb6a0a551beb5	performance evaluation of distribution service efficiency based on supply chain	performance evaluation;distribution costs;supply chain;distribution center	To save the cost for customers and distribution center is the key issue to survive for distribution centers. The low service efficiency will certainly lead to increased distribution cost. It is a universal concern problem for distribution industry even for logistics industry to improve distribution service efficiency. On the basis of analyzing the characteristics of customer needs and the characteristics of distribution service efficiency based on the supply chain, this paper discusses the challenges of distribution service, puts forward the principles of distribution service to put up the distribution service efficiency, and sets up the evaluation system of distribution service efficiency for distribution center. Then, this article proposes one appraisal method to appraise distribution service efficiency for distribution centers. The method is presented that appraise distribution service efficiency for distribution centers, and has carried on the real diagnosis analysis.	logistics;performance evaluation	Hongzhi Liu;Qilan Zhao	2007		10.1007/978-0-387-76312-5_77	operations management;distribution center;supply chain;valuation (finance);business	Metrics	-3.6320979919427754	-14.082839438470762	197674
d34cf541304f1a1bf260c14170cb08da3e5cf602	"""a note on """"ranking generalized exponential trapezoidal fuzzy numbers based on variance"""""""		It is well known that the ranking of generalized fuzzy numbers depend upon the height of fuzzy numbers. In this note, it is shown that the method, proposed by Rezvani [Applied Mathematics and Computation 262 (2015) 191-198] for ranking of generalized exponential trapezoidal fuzzy numbers, is independent from height of fuzzy numbers. Hence, it is not genuine to use this method for ranking of generalized exponential trapezoidal fuzzy numbers.	computation;fuzzy number;time complexity	Gourav Gupta;Amit Kumar;S. S. Appadoo	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-162134	econometrics;mathematical optimization;mathematics;statistics	AI	1.9891425792193265	-15.524088981135703	199277
92e203d513ef53ab83458cc9b6b72abcba46e875	utility of income as a random function: behavioral characterization and empirical evidence	scientific article;jel classification d19;stated preference;jel classification b21;theoretical framework;empirical analysis;functional form;consumer demand;utility of income;journal article;invariance principle;empirical evidence;peer reviewed;random function;empirical model;model fitting;working paper;random utility;likelihood function;invariance principles	The paper proposes a particular approach to model the utility of income. We develop a theoretical framework that restricts the class of admissible functional forms and distributions of the random components of the model. The theoretical approach is based on theories of probabilistic choice and ideas that are used in modern psychophysical research. From our theoretical framework, we obtain the empirical model and the corresponding likelihood function. The empirical analysis is based on a “Stated Preference” survey. The model fits the data quite well. Finally, we discuss the concept of cardinality and the implications for consumer demand relations.	fits;marginal model;stochastic process;theory;utility	John K. Dagsvik;Steinar Strøm;Zhiyang Jia	2006	Mathematical Social Sciences	10.1016/j.mathsocsci.2005.07.006	econometrics;peer review;empirical evidence;economics;random function;mathematics;likelihood function;mathematical economics;higher-order function;invariance principle;welfare economics;empirical modelling;statistics	ML	-0.9849067444413	-12.403283674365063	199742
