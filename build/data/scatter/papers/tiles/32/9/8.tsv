id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
668d3b2597cc15ffa846bd29e5f274eb52c1a81c	fantasy, curiosity and challenge as adaptation indicators in multimodal dialogue systems for preschoolers	speech interfaces;dialogue system;multimodal interface;children computer interaction;graphical user interfaces;user experience;graphic user interface;evaluation;multimodal interfaces;computer game;preschool children	In this paper, we investigate how fantasy, curiosity and challenge contribute to the user experience in multimodal dialogue computer games for preschool children. For this purpose, an on-line multimodal platform has been designed, implemented and used as a starting point to develop five task oriented games suitable for preschoolers, with varying levels of fantasy and curiosity elements, as well as, variable difficulty levels. Nine preschool children were asked to play these games in different configurations and choose the application setup that they enjoyed most. Results show that fantasy and curiosity are correlated with children's entertainment, while the level of difficulty seems to depend on each child's individual preferences and capabilities. In addition, a variety of objective metrics (task completion, interaction time, wrong answers), audio features and emotional state have been investigated as potential features that can predict optimal levels of fantasy, curiosity and difficulty for each child. Emotional state recognition results are also reported.	dialog system;multimodal interaction;online and offline;pc game;user experience	Theofanis Kannetis;Alexandros Potamianos;Georgios N. Yannakakis	2009		10.1145/1640377.1640378	psychology;simulation;multimedia;communication	HCI	-49.77003596388373	-46.9502705632971	150231
36e218c4585d54388c19c885bc496e05739b1844	co-embodied interaction between two persons in different situation	marine vehicles human robot interaction informatics engines power supplies power engineering and energy conducting materials navigation;multi agent systems avatars;multi agent systems;multiagent system co embodied interaction avatar;avatars;embodied interaction	In this paper, we focus on co-embodied interaction between two persons who occupy a differentiated situation in the same environment. An experiment is conducted to examine how two persons orally coordinate their intention and action in order to achieve a given experimental task. As a result, it is suggested that the co-embodied interaction efficiently works, even when the participants are grounded in different situations or have a different view toward the target.		Yugo Takeuchi;Naoki Tsuchiya	2007	RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2007.4415123	simulation;human–computer interaction;computer science;artificial intelligence;multi-agent system	Robotics	-50.77365008150415	-50.29964834669635	150692
2ae8a49d5ef38bc7dc022e086eabcf94249e7e2b	expectations and feedback in user-system communication	interfase usuario;user interface;relacion hombre maquina;man machine relation;interface utilisateur;relation homme machine;communication;comunicacion	"""Abstract   In terms of speed and accuracy of intention transfer, normal human conversation proves to be very efficient: exchanged messages carry only sufficient information relative to contextual knowledge assumed to be present at the receiver's end. Furthermore, by receiving layered feedback from the recipient, the speaker is able to verify at an early stage of communication whether his intentions are being accurately perceived. Finally, in divergencies from the expected messages, the listener may ask for clarification at an early stage of message interpretation.  For user-system communication to become similarly more efficient, machine interfaces should display both early layered (I)-feedback about partial message interpretations as well as layered expectations (E-feedback) about the message components still to be received. Examples of interfaces are given which already possess these desirable characteristics in part.  The """"layered-protocol model"""", proposed by Taylor (1988, Layered protocols for computer-human dialogue. I. Principles,  International Journal of Man-Machine Studies ,  28 , 175-218) as a framework for user-system interface design, details the use of layered I-feedback and related repair messages in user-system communication. In this paper we suggest that the model can be improved by providing it with layered E-feedback, as derived from assumed intentions and layered knowledge of the interaction history."""		Frits L. Engel;Reinder Haakma	1993	International Journal of Man-Machine Studies	10.1006/imms.1993.1067	simulation;human–computer interaction;computer science;artificial intelligence;multimedia;user interface	Arch	-51.146261771895865	-47.31008362999622	151266
c57075ae5d26771324fbac2fc3fafbd83d521f9d	development of an embodied interaction system with interactor by speech and hand motion input	human interaction;speech processing;indexing terms;human interface;sensory evaluation;speech input hand motion input embodied interaction system embodied interaction character interrobot technology interpuppet natural communicative motions data glove mobile keyboard;robots;speech humans character generation mobile communication prototypes data gloves keyboards displays computer science systems engineering and theory;data gloves;user interfaces;robots speech processing data gloves user interfaces;embodied interaction	InterActor is a CG embodied interaction character driven by iRT which is the InterRobot Technology to make communicative motions and actions based on only speech input for entrained interaction. In this paper, the concept of an embodied entrainment character InterPuppet is proposed by using both iRT and hand motion input like a puppet, so that humans can communicate effectively by introducing intentional body actions as well as natural communicative motions and actions. Two prototypes of the system with data glove and that with mobile keyboard are developed for the new embodied interaction and communication. The sensory evaluation demonstrates that the proposed systems have great advantage for communication support compared with conventional systems. Some actual and extensive applications to human interface are also demonstrated.	brainwave entrainment;cg (programming language);embodied cognition;user interface;wired glove	Michiya Yamamoto;Tomio Watanabe;Koji Osaki	2005	ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.	10.1109/ROMAN.2005.1513799	robot;computer vision;interpersonal relationship;simulation;index term;embodied agent;computer science;speech processing;user interface;human interface device	Robotics	-50.074489740504276	-49.60124176488937	151274
cf7d113a14be46b289a275cffec22f2c132127ad	quadrotor or blimp? noise and appearance considerations in designing social aerial robot	social robot;social factors;better platform;interaction;functional point;psychology studies;social aerial robot design;noise;novel hri platform;interaction experiment;social factor;human-robot interaction;social aerial robot;appearance consideration;appearance issue;autonomous aerial vehicles;aerial robot;appearance;unmanned aerial vehicle;physiology studies;slower speed;safety;hri platform;animal-like;quadrotor;human factor;blimp;higher speed;psychology;human robot interaction;robots;physiology	Aerial robots offer a novel HRI platform thanks to their flying capabilities. However, existing aerial robots are designed from functional point of view and do not take social factors such as noise and appearance issues into serious consideration. Compared to quadrotor (noisier but higher speed), we investigated whether blimp (quieter but slower speed) is a better platform for social aerial robot. We formed hypotheses based on findings from physiology and psychology studies and examined our ideas with responses collected from online survey and interaction experiment.	aerial photography;aerobot;human–robot interaction;robot	Chun Fui Liew;Takehisa Yairi	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)			Robotics	-50.921925325233765	-51.45438675213359	151495
5a7c7025c85b01aab55689ec4ee2f6493f3ac24b	assessing agreement in human-robot dialogue strategies: a tale of two wizards		The Wizard-of-Oz (WOz) method is a common experimental technique in virtual agent and human-robot dialogue research for eliciting natural communicative behavior from human partners when full autonomy is not yet possible. For the first phase of our research reported here, wizards play the role of dialogue manager, acting as a robot’s dialogue processing. We describe a novel step within WOz methodology that incorporates two wizards and control sessions: the wizards function much like corpus annotators, being asked to make independent judgments on how the robot should respond when receiving the same verbal commands in separate trials. We show that inter-wizard discussion after the control sessions and the resolution with a reconciled protocol for the followon pilot sessions successfully impacts wizard behaviors and significantly aligns their strategies. We conclude that, without control sessions, we would have been unlikely to achieve both the natural diversity of expression that comes with multiple wizards and a better protocol for modeling an automated system.	dialog system;robot	Matthew Marge;Claire Bonial;Kimberly A. Pollard;Ron Artstein;Brendan Byrne;Susan G. Hill;Clare R. Voss;David R. Traum	2016		10.1007/978-3-319-47665-0_60	multimedia;robot;human–computer interaction;autonomy;wizard;computer science;human–robot interaction	NLP	-52.40207939309604	-48.56390850017764	151672
7699de7a086b1d2bbf8c9b5b3f882e74d66e897c	discourse cues to deception in the case of multiple receivers	discourse feature;discourse of interaction;deception behavior;discourse analysis;deception detection	Behavioral cues to deception are instrumental in detecting deception. As one of the primary sources of deception behavior, text has been analyzed at the level of sub-sentence or message but not the discourse of interaction. Additionally, empirical studies on cues to deception in the case of multiple receivers remain nonexistent. To fill these voids, we propose a discourse framework and six hypotheses about deception behaviors in a multi-receiver environment. The deception behaviors are operationalized by discourse features based on an analysis of real-world data. The results of statistical analysis validate the efficacy of discourse features in discriminating deceivers from truth-tellers.		Lina Zhou;Jiang Wu;Dongsong Zhang	2014	Information & Management	10.1016/j.im.2014.05.011	psychology;developmental psychology;discourse analysis;communication;social psychology	AI	-53.998342414081876	-49.42121391357441	152137
cb30b9741531c0ef56e7e183d669c2712dcee6ae	mood fern: exploring shape transformations in reactive environments	tangible interface;organic user interface;paper computing;kinetic user interface;interactive art	We present Mood Fern: digital flora which responds to touch. Depending on the length and intensity of the touch a subset of leaves physically react. The leaves respond on a spectrum of slight oscillation, imitating the effects of swaying in a slight breeze, to complete deformation, as if they were physically trying to respond in a similar manner. Mood Fern's reference to nature highlights its appeal to calm computing. Painted capacitive sensors mimic the appearance of leaf veins and Flexinol SMA wire is used to actuate The Mood Fern's paper structures.	barnsley fern;sensor	Bernard Cheng;Antonio Gomes;Paul Strohmeier;Roel Vertegaal	2014		10.1145/2663806.2663818	simulation;engineering;multimedia;communication	HCI	-48.48941602602683	-49.54451381369159	153233
d767240564eb347c685eb5610b4972eafe2f87c2	learning robot speech models to predict speech acts in hri			human–robot interaction;robot	Ankuj Arora;Humbert Fiorino;Damien Pellier;Sylvie Pesty	2018	Paladyn	10.1515/pjbr-2018-0015		NLP	-52.90794905873372	-48.64101026399775	153589
97d1fd0fbb4509d9b1dd8af8803eceb4ec0c464c	exploring passive user interaction for adaptive narratives	meetings and proceedings;entertainment computing;interactive storytelling;physiological;user interaction;passive interaction;physiological response;multi modal interaction	"""Previous Interactive Storytelling systems have been designed to allow active user intervention in an unfolding story, using established multi-modal interactive techniques to influence narrative development. In this paper we instead explore the use of a form of passive interaction where users' affective responses, measured by physiological proxies, drive a process of narrative adaptation. We introduce a system that implements a passive interaction loop as part of narrative generation, monitoring users' physiological responses to an on-going narrative visualization and using these to adapt the subsequent development of character relationships, narrative focus and pacing. Idiomatic cinematographic techniques applied to the visualization utilize existing theories of establishing characteristic emotional tone and viewer expectations to foster additional user response. Experimental results support the applicability of filmic emotional theories in a non-film visual realization, demonstrating significant appropriate user physiological response to narrative events and """"emotional cues"""". The subsequent narrative adaptation provides a variation of viewing experience with no loss of narrative comprehension."""	3d computer graphics;baseline (configuration management);catherine;display resolution;dixon's factorization method;experiment;feedback;field electron emission;interactive storytelling;interactivity;list comprehension;modal logic;natural language processing;theory;unfolding (dsp implementation);while	Stephen W. Gilroy;Julie Porteous;Fred Charles;Marc Cavazza	2012		10.1145/2166966.2166990	simulation;human–computer interaction;multimedia;world wide web	HCI	-53.06508917653627	-45.45677465938697	153998
05606646bb52b4e0a7bbbf0a5af8c29e6689e8e9	building up child-robot relationship for therapeutic purposes: from initial attraction towards long-term social engagement	control engineering education;quality of life;long term interaction;pediatrics;therapeutic purposes;interdependence theory;data collection;dyadic interaction;long term social engagement;distinctive perceptions;human robot interaction;positron emission tomography;social assistive robots;robot based programs;role attribution human robot interaction social assistive robots long term interaction interdependence theory;robots educational institutions pediatrics conferences interviews positron emission tomography context;robots;child robot relationship;interviews;behavioural sciences;patient treatment;laboratory experiment;robot dynamics behavioural sciences control engineering education health care human robot interaction patient treatment;elementary school;robot dynamics;social assistance;context;field study;conferences;interactive behavior;robot based programs child robot relationship therapeutic purposes long term social engagement laboratory experiment dyadic interaction interactive behavior distinctive perceptions;role attribution;health care	This work explores the dynamics of the emergence of the social bonds with robots. A field study with 49 sixth grade scholars (aged 11–12 years) and 4 different robots was carried out at an elementary school. A subsequent laboratory experiment with 4 of the participants was completed. For the first experience, at school children's preferences, expectancies on functionality and communication, and interaction behavior were studied. Using the data collected in the laboratory, recognition, the selection of partner, and dyadic interaction were explored. Both at school and in the lab, data from videotaped direct observation, questionnaires and interviews were gathered. The results showed that different appearance and performance of robots elicit in children distinctive perceptions and interactive behavior, and affect social processes, such as role attribution and attachment. This work presents a preliminary field study to explore the introduction of robot-based programs to improve the quality of life of hospitalized children1	attachments;dyadic transformation;emergence;field research;human–robot interaction;interdependence;nao (robot);pleo;robot;the quality of life	Marta Díaz;Neus Nuno;Joan Saez-Pons;Diego E. Pardo;Cecilio Angulo	2011	Face and Gesture 2011	10.1109/FG.2011.5771375	psychology;simulation;communication;social psychology	HCI	-54.8582149051106	-50.65323232057897	154320
d29eaf05cfbc0ac293e53566a0957ccc3faffe05	an interactive wall game as an evolution of proto language		"""A new interactive """"wall game"""" is proposed in which two human players alternatively configure a pattern to communicate. A pattern consists of 3x3 sites, on which a player can place one of three symbols. The two major findings in this paper are i) the subjects mainly communicated in two modes. Either the subjects changed the pattern by watching the pattern as it is (dynamical mode) or by having narrative reflection (metaphorical mode). ii) Subjects switched between these two modes. Most of the experiments in evolutionary linguistics are based on “taskoriented communication” and they observe the emergence of lexical items. In contrast, our experiment explores whether “communication without purpose” leads to the emergence of complex rules such as linguistic grammar. We argue that the switching between the two modes observed in our experiment can be seen as a grammatical process in the sense that it is a procedure to take an internal state outside using the media (i.e., patterns in the wall game). Under this hypothesis, the players’ exploration of the media becomes a crucial step in the emergence of language and grammar."""	emergence;experiment	Ryoko Uno;Keisuke Suzuki;Takashi Ikegami	2011			artificial intelligence;natural language processing;evolutionary linguistics;proto-language;computer science;multimedia;narrative;grammar;lexical item	AI	-53.19057049829743	-48.10327878486405	155878
3c3808c1238623af98f5782f04c76501d807bb4d	apports perceptivo-cognitifs des avatars dans des environnements virtuels familier, semi-familier et non familier	virtual reality;cognition;test series;perception;avatar	"""The fundamental studies of perceptivo-cognitive type on the contributions of the avatars in the virtual scenes are generally carried out in situations of laboratory. For our own projects, we needed to evaluate these contributions in complex industrial scenes. This document is the synthesis of this applicative study.The study presented treats of the appreciation of the avatars carrying qualitative information ; they do not carry the principal information but populate virtual environments. This study aims to better know information than these avatars convey by their presence in order to reinforce the messages contained in the application and to avoid the misinterpretations or ambiguity they could bring if this factor is not controlled.The principal results of the experimentation are the following: • In any type of virtual environment, the avatars bring realism and """"life"""". In a nonfamiliar environment, they also allow to provide indications on the work and the activity in environnement, in particular with their working clothes and actions. • Nevertheless, to be appreciated and effective, these avatars will be sufficiently realistic and their behaviors must be relevant compared to environnement. Mobility alone is not enough, their actions must be coordinated, fluid and be centered on an identifiable objective of task. The clothes of the civil avatars must also be considered because it conveys a significant psychosociological dimension."""	applicative programming language;avatar (computing);experiment;population;semiconductor industry;virtual reality	Sandrine Tonnoir;Catherine Berenblit	2002		10.1145/777005.777028	simulation;engineering;multimedia;communication	Visualization	-54.213964024900505	-47.090876111971	156216
35b7e8ad60cb357cd9457ca2687eb2ba37068bd6	how visualization layout relates to locus of control and other personality factors	electronic mail;individual differences;locus of control;layout;age factors computer graphics extraversion psychology humans internal external control introversion psychology personality personality assessment sex factors visual perception;data visualisation;visualization;data visualization;locus of control visualization individual differences;visualization externalization theory visualization layout locus of control personality factors individual personality difference complex visualization systems personality traits loc color interaction labeling list metaphor containment metaphor adaptive visual analytics visualization evaluation;correlation;visual analytics;problem solving;layout data visualization problem solving correlation visual analytics electronic mail	"""Existing research suggests that individual personality differences are correlated with a user's speed and accuracy in solving problems with different types of complex visualization systems. We extend this research by isolating factors in personality traits as well as in the visualizations that could have contributed to the observed correlation. We focus on a personality trait known as """"locus of control” (LOC), which represents a person's tendency to see themselves as controlled by or in control of external events. To isolate variables of the visualization design, we control extraneous factors such as color, interaction, and labeling. We conduct a user study with four visualizations that gradually shift from a list metaphor to a containment metaphor and compare the participants' speed, accuracy, and preference with their locus of control and other personality factors. Our findings demonstrate that there is indeed a correlation between the two: participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor, while those with an external locus of control perform well with such visualizations. These results provide evidence for the externalization theory of visualization. Finally, we propose applications of these findings to adaptive visual analytics and visualization evaluation."""	altered level of consciousness;cdisc sdtm anatomical location terminology;comefrom;color;imagery;isolate - microorganism;locus;response time (technology);theory of visualization;trait;type a personality;usability testing;visual analytics	Caroline Ziemkiewicz;Alvitta Ottley;R. Jordan Crouser;Ashley Rye Yauilla;Sara L. Su;William Ribarsky;Remco Chang	2013	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2012.180	layout;computer vision;visual analytics;visualization;computer science;locus of control;multimedia;correlation;data visualization	Visualization	-48.85226233122407	-50.14262845973104	156351
d6600153b6464d303853b77b8bb35899534a408f	group judgment processes and outcomes in video-conferencing versus face-to-face groups	credible interval;video conferencing;interactive media;group decision;face to face;group interaction	Two hundred and eighty-two participants formed 94 groups of size three and completed an estimation task by interacting either face-to-face or via a video-conferencing system. Results showed significant differences across conditions with regard to the confidence attached by groups to their decisions, the degree to which groups were able to improve upon the best individually arrived at decision, and the self-reported enjoyment of group members. Compared to face-to-face groups, video-conferencing groups showed lower levels of confidence in their decisions, especially if they were instructed to discuss their beliefs and assumptions underlying their estimates and not the estimates themselves. However, this lower level of confidence was more appropriate than that of the face-to-face groups. Groups interacting face-to-face were more likely to improve upon the best individual solution, and, on average, improved to a greater degree. Further, video-conferencing groups reported modifying more of their beliefs during discussion. However, there were no significant differences between the two interaction media on the following outcome dimensions: accuracy; overconfidence; commitment to the group decision; size of credible intervals; improvement over average initial individual estimates; and the number of beliefs discussed or learned. Implications for the design and application of advanced systems for decision-making support and research are discussed.		Marcus Crede;Janet A. Sniezek	2003	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2003.07.001	simulation;computer science;interactive media;videoconferencing;credible interval	Logic	-53.13654116264294	-51.75161481461908	157195
b4794100330fd0da64c80e66558ba3ac72633b2d	question strategy and interculturality in human-robot interaction	language group;human participant;multi party;human-robot interaction;question strategy;quiz robot;grammar;speech;head;english;japanese;robot kinematics;human robot interaction	This paper demonstrates the ways in which multi party human participants in 2 language groups, Japanese and English, engage with a quiz robot when they are asked a question. We focus on both speech and bodily conducts where we discovered both universalities and differences.	human–robot interaction;robot	Mihoko Fukushima;Rio Fujita;Miyuki Kurihara;Tomoyuki Suzuki;Keiichi Yamazaki;Akiko Yamazaki;Keiko Ikeda;Yoshinori Kuno;Yoshinori Kobayashi;Takaya Ohyama;Eri Yoshida	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		natural language processing;human–robot interaction;computer science;artificial intelligence;grammar	Robotics	-52.33056301331143	-49.90330091466509	157537
384749d8852f398f58d38a582a68c393890f6063	a phase-entrained particle filter for audio-locomotion synchronization	sonification;syncrhonization;particle filter;gait analysis	Synchronized, audio feedback capitalizes on the innate human ability to entrain motion to a rhythmic, audio stimulus, and has been used in training athletes and treating gait disorders. For such feedback to be effective, it is necessary to close the loop to achieve mutual, bi-directional synchronization between human and machine. Although we know we can do the synchronization within an expensive, static motion capture facility, we seek low-cost, portable methods suitable for individual and outpatient use. This paper presents a phase-entrained particle filter that synchronizes to the periodic silhouettes of a gait, using only a simple camera as a sensor. Tests with eight participants show reliable synchronization, and timing errors in sonic stimulus generation less than 60ms, or a 1/32 note in a 120bpm march. While these errors are just audible, they do not confound mutual human-machine synchronization.	audio feedback;motion capture;particle filter	Peyman Manikashani;Jeffrey E. Boyd	2016	2016 13th Conference on Computer and Robot Vision (CRV)	10.1109/CRV.2016.24	real-time computing;simulation;gait analysis;sonification;particle filter;computer science;statistics	Vision	-48.82285017837747	-51.09022496024629	157820
587e9ad1090143026d17afc69768b5feba0dd7e3	individual differences in work load while doing multitasking with a computer	tactile;context awareness;individual differences;multimodal information;cognitive task load;audio;multitasking	In the present study we examined the individual differences in work load while doing high and low cognitive load multimodal (audive + tactile or visual + tactile) tasks. We found among other things that participants that are characterized as having high information capacity had lower levels of work load and shorter reactions times during easy and difficult multitask as compared to participant that are characterized as having lower level of information processing capacity.	computer multitasking	Kari Kallinen;Inger Ekman;Niklas Ravaja	2011		10.1007/978-3-642-21602-2_38	differential psychology;simulation;human multitasking;computer science;cognitive load	HCI	-48.8733367293879	-47.87298187654789	157879
acf847da0e0c78c1ec1b51f13d3da5db3eee6444	experimental evaluation of bi-directional multimodal interaction with conversational agents	conversational agent;multimodal interaction	In the field of intuitive HCI, Embodied Conversational Agents (ECAs) are being developed mostly with speech input. In this paper, we study whether another input modality leads to a more effective and pleasant “bi-directional” multimodal communication. In a Wizard-of-Oz experiment, adults and children were videotaped while interacting with 2D animated agents within a game application. Each subject carried out a multimodal scenario (speech and/or pen input) and a speech-only scenario. The results confirm the usefulness of multimodal input, which yielded shorter scenarios, higher and more homogeneous ratings of easiness. Additional results underlined the importance of gesture interaction for children, and showed a modality specialization for certain actions. Finally, multidimensional analyses revealed links between behavioral and subjective data, such as an association of pen use and pleasantness for children. These results can be used for both developing the functional prototype and in the general framework of ECA-systems evaluation and specification.	computer animation;dialog system;embodied agent;human–computer interaction;modality (human–computer interaction);multimodal interaction;partial template specialization;prototype;wizard of oz experiment	Stéphanie Buisine;Jean-Claude Martin	2003			multimedia;human–computer interaction;embodied agent;computer science;natural language processing;multimodal interaction;homogeneous;embodied cognition;gesture;dialog system;artificial intelligence	HCI	-50.38010787832969	-47.708182131450435	158449
3059ea4ab2f127dc866baa5c6d7565ff2fc68a8f	techsportiv: constructing objects-to-think-with for physical education	constructivism;construction kits;physical education;constructionism;sports;children	In this paper we present a computational construction kit that allows young people to create devices that measure body movement and provide feedback about it. We also present an environment in which young people are empowered to become creators and inventors of their own body movement device. A workshop where this kit was used is described in detail to show how TechSportiv - the kit and the environment - should be implemented.  We present results from working with this kit and show that it is possible to create diverse personally meaningful devices which have been used to quantify movements, to indicate the quality of a movement, to assist the performance, or to translate body movement into an artistic representation. We describe the construction process of one project in detail and show how the kit acts as an object-to-think-with. We argue, that TechSportiv is a way to connect aspects of body and mind in human movement.	feedback	Nadine Dittert	2014		10.1145/2639189.2639202	physical education;simulation;human–computer interaction;computer science;multimedia;constructionism;management;constructivism	HCI	-55.443254833538234	-46.866302337575426	158741
2d5c22d9338d95af8b7dd5ef3b9920c8ab04e3b5	the media inequality: comparing the initial human-human and human-ai social interactions	the computers are social actors paradigm;social interaction;human machine communication;the cognitive affective processing system;artificial intelligence;chatbot	As human-machine communication has yet to become pr evalent, the rules of interactions between human and intelligent machines need to be e xplor d. This study aims to investigate a specific question: During human users’ initial inte ractions with artificial intelligence, would they reveal their personality traits and communicat ive attributes differently from human-human interactions? A sample of 245 participa nts was recruited to view six targets’ twelve conversation transcripts on a social media p latform: Half with a chatbot Microsoft’s Little Ice, and half with human friends. The findin gs suggested that when the targets interacted with Little Ice, they demonstrated diffe rent personality traits and communication attributes from interacting with humans. Specifical ly, users tended to be more open, more agreeable, more extroverted, more conscientious and self-disclosing when interacting with humans than with AI. The findings not only echo Mis chel’s cognitive-affective processing system model but also complement the Computers Are Social Actors Paradigm. Theoretical implications were discussed.	artificial intelligence;interaction;programming paradigm;social inequality;social media	Yi Mou;Kun Xu	2017	Computers in Human Behavior	10.1016/j.chb.2017.02.067	psychology;social relation;social science;simulation;sociology;communication;social psychology;world wide web	AI	-53.27249817312516	-48.98175174040895	159074
4d5ed84cf1149689922ce31a35442044ffe9d586	style-phase adaptation of human and humanoid biped walking patterns in real systems	legged locomotion;exoskeletons;monitoring;synchronization;medical robotics adaptive control feedback handicapped aids humanoid robots;legged locomotion robot kinematics synchronization exoskeletons monitoring adaptation models;adaptation models;visual feedback humanoid biped walking patterns human walking assistance walking assist exoskeleton robots predicted user motions temporal synchronization style phase adaptive pattern generation;robot kinematics	Several studies have been attempted on human walking assistance using exoskeleton robots. To achieve the effective walking assistance with a variety of user motions, the robot behaviors need to be coordinated with both predicted user motions and the environment spatiotemporally. In this paper, we study how movement prediction and temporal synchronization can be beneficial for walking assist exoskeletons using the framework of style-phase adaptive pattern generation [1]. In particular, we empirically investigate the following two issues: i) mutual synchronization between a human subject and a humanoid model through style-phase adaptation, and ii) using style-phase adaptation for walking assistance. We developed two experimental platforms for the two investigations and conducted subjective experiments. The experimental results suggest that visual feedback of the state of the humanoid model can enhance the mutual synchronization through style-phase adaptation, and the estimated style and phase can be useful to assist walking movement of the human subject.	experiment;mcgurk effect;robot;sensor	Takamitsu Matsubara;Daisuke Uto;Tomoyuki Noda;Tatsuya Teramae;Jun Morimoto	2014	2014 IEEE-RAS International Conference on Humanoid Robots	10.1109/HUMANOIDS.2014.7041348	synchronization;simulation;exoskeleton;computer science;humanoid robot;artificial intelligence;robot kinematics	Robotics	-49.823266064089765	-50.72708750038125	160182
3b50814d76e398de6d9005af26ab430771ee5657	gender differences in the impact of presentational factors in human character animation on decisions in ethical dilemmas	gender difference;character animation	Simulated humans in computer interfaces are increasingly taking on roles that were once reserved for real humans. The presentation of simulated humans is affected by their appearance, motion quality, and interactivity. These presentational factors can influence the decisions of those who interact with them. This is of concern to interface designers and users alike, because these decisions often have moral and ethical consequences. However, the impact of presentational factors on decisions in ethical dilemmas has not been explored. This study is intended as a first effort toward filling this gap. In a between-groups experiment, a female character presented participants with an ethical dilemma. The character's human photorealism and motion quality were varied to generate four stimulus conditions: real human versus computer-generated character fluid versus jerky movement. The results indicate that the stimulus condition had no significant effect on female participants, while male participants were significantly more likely to rule against the character when her visual appearance was computer generated and her movements were jerky.	artificial intelligence;computer-generated holography;domain-specific language;human–computer interaction;interactivity;kelly criterion;user interface design;word lists by frequency	Karl F. MacDorman;Joseph A. Coram;Chin-Chang Ho;Himalaya Patel	2010	PRESENCE: Teleoperators and Virtual Environments	10.1162/pres.19.3.213	character animation;simulation;computer science	HCI	-49.63848620736113	-49.96103240375253	160916
9afa323353d9a78671166070fc406419b62b6483	interactions in perceived quality of auditory-visual displays	programming language;sampling frequency;gaussian white noise;auditory display;virtual environment;article;auditory visual	The quality of realism in virtual environments (VEs) is typically considered to be a function of visual and audio fidelity mutually exclusive of each other. However, the VE participant, being human, is multimodal by nature. Therefore, in order to validate more accurately the levels of auditory and visual fidelity that are required in a virtual environment, a better understanding is needed of the intersensory or crossmodal effects between the auditory and visual sense modalities. To identify whether any pertinent auditory-visual cross-modal perception phenomena exist, 108 subjects participated in three experiments which were completely automated using HTML, Java, and JavaScript programming languages. Visual and auditory display quality perceptions were measured intraand intermodally by manipulating the pixel resolution of the visual display and Gaussian white noise level, and by manipulating the sampling frequency of the auditory display and Gaussian white noise level. Statistically significant results indicate that high-quality auditory displays coupled with highquality visual displays increase the quality perception of the visual displays relative to the evaluation of the visual display alone, and that low-quality auditory displays coupled with high-quality visual displays decrease the quality perception of the auditory displays relative to the evaluation of the auditory display alone. These findings strongly suggest that the quality of realism in VEs must be a function of both auditory and visual display fidelities inclusive of each other.	auditory display;experiment;html;java;javascript;modal logic;multimodal interaction;noise (electronics);pixel;programming language;relevance;sampling (signal processing);virtual reality;white noise	Russell L. Storms;Michael Zyda	2000	Presence: Teleoperators & Virtual Environments	10.1162/105474600300040385	computer vision;computer science;virtual machine;operating system;auditory display;multimedia;white noise;auditory scene analysis;sampling	HCI	-49.16320233652981	-46.26602268794663	161104
3481121bef8cdcd3e0d9bc08e4411dc28c799136	influence of error rate on frustration of bci users	fatigue;bci;fake feedback;ssvep;frustration;motivation	Brain-Computer Interfaces (BCIs) are still much less reliable than other input devices. The error rates of BCIs range from 5% up to 60%. In this paper, we assess the subjective frustration, motivation, and fatigue of BCI users, when confronted to different levels of error rate. We conducted a BCI experiment in which the error rate was artificially controlled. Our results first show that a prolonged use of BCI significantly increases the perceived fatigue, and induces a drop in motivation. We also found that user frustration increases with the error rate of the system but this increase does not seem critical for small differences of error rate. Thus, for future BCIs, we would advise to favor user comfort over accuracy when the potential gain of accuracy remains small.	bit error rate;brain–computer interface;input device	Andéol Évain;Ferran Argelaguet;Anthony Strock;Nicolas Roussel;Géry Casiez;Anatole Lécuyer	2016		10.1145/2909132.2909278	brain–computer interface;simulation;motivation;frustration	HCI	-48.51793539160638	-46.176828398914886	161108
ffc68e8211959abafc9d7c2d0c56b4b7f3a93bbc	conveying browsing context through audio on digital talking books	auditory icons;digital talking books;audio interfaces;speech;application sharing;evaluation;earcons;audio interface;computer science;article	This paper presents the results of a study comparing the use of auditory icons, earcons and speech in an audio only interface for a digital talking book player. The different techniques were evaluated according to the identification errors made, and subjective measures of understandability, intrusiveness and pleasurability. Results suggest the use of auditory icons combined with speech whenever necessary, in detriment to the use of earcons, for applications sharing the characteristics of digital talking book players.	audiobook;browsing;daisy digital talking book;earcon	Carlos Duarte;Luís Carriço	2007		10.1007/978-3-540-73283-9_30	speech recognition;computer science;multimedia;communication	HCI	-48.889514024740144	-46.01711601323379	161173
0948ecc0570836319093f5e61a16cbee3ee6d817	a novel virtual reality driving environment for autism intervention	virtual reality;autism intervention;physiological signals;adaptive task;eye gaze	Individuals with autism spectrum disorders (ASD) often have difficulty functioning independently and display impairments related to important tasks related to adaptive independence such as driving. Ability to drive is believed to be an important factor of quality of life for individuals with ASD. The presented work describes a novel driving simulator based on a virtual city environment that will be used in the future to impart driving skills to teenagers with ASD as a part of intervention. A physiological data acquisition system, which was used to acquire and process participant’s physiological signals, and an eye tracker, which was utilized to detect eye gaze signals, were each integrated into the driving simulator. These physiological and eye gaze indices were recorded and computed to infer the affective states of the participant in real-time when he/she was driving. Based on the affective states of the participant together with his/her performance, the driving simulator adaptively changes the difficulty level of the task. This VR-based driving simulator will be capable of manipulating the driving task difficulty in response to the physiological and eye gaze indices recorded during the task. The design of this novel driving simulator system and testing data to validate its functionalities are presented in this paper.	adaptive behavior;data acquisition;design of experiments;driving simulator;eye tracking;real-time clock;simulation;systems design;usability testing;virtual reality;virtual world	Dayi Bian;Joshua W. Wade;Lian Zhang;Esubalew Bekele;Amy Swanson;Julie Ana Crittendon;Medha Shukla Sarkar;Zachary Warren;Nilanjan Sarkar	2013		10.1007/978-3-642-39191-0_52	psychology;computer vision;simulation;communication	HCI	-50.19804983935301	-47.946227166891674	161992
f51518ea66315bba4120c1088b3416eea7509cd6	method and improvisation: theatre arts performance techniques to further hri in social and affective robots		Theatre Arts methodologies of improvisational humor and method performance techniques are combined for a social robot study to further the affective, relational, and social interaction between humans and robots for positive effect. The complexity of performance methodology develops authentic human social interaction and is thereby well-suited to a positive human-robot model. The analysis of this pilot research study incorporating the proposed theatre methodology approach allows innovative solutions in affective communication to be studied from a multimodal sensory perspective.	humans;human–robot interaction;multimodal interaction;social robot	Julienne A. Greer	2017	2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2017.8172465	simulation;the arts;human–computer interaction;educational robotics;improvisation;social robot;robot;computer science;social relation;human–robot interaction;affect (psychology)	Robotics	-55.10035099277472	-47.16492857040055	162001
e3996ca0d432ba4f8ecce01655f4b24ee97b8828	catalyst personality for fostering communication among groups with opposing preference	prior understanding;important basis;new person;independent segment;catalyst personality;art piece	The activity of an organization is excited by introducing new persons. Understanding such catalyst personality is an important basis for fostering communication among groups with opposing preference. In the prior understanding, the groups are independent segments. Cognition, resulted from seeing the overlaps revealed between the segments, is not the same as the prior understanding. This gap is a clue. We demonstrate an experiment using questionnaire on the preference of art pieces.		Yoshiharu Maeno;Yukio Ohsawa;Takaichi Ito	2007		10.1007/978-3-540-73325-6_80	social psychology;personality;cognition;psychology	HCI	-54.12130255995573	-47.74012611211476	162049
93f9fd22df7bf248c4181c8c4a77b33558246ae4	basic study of evoking emotion through extending one's body image by integration of internal sense and external sense				Sho Sakurai;Takuji Narumi;Toki Katsumura;Tomohiro Tanikawa;Michitaka Hirose	2015		10.1007/978-3-319-20612-7_42	psychology;developmental psychology;communication;social psychology	NLP	-55.285526153489734	-51.07120874289735	162137
3529ea8639afda196a6925b4acfd2c802525cc26	scoringtalk: a tablet system scoring and visualizing conversation for balancing of participation	interaction;multi party conversation;communication support;gamification	We present a tablet system providing real-time feedback to participants of face-to-face multi-party conversation on the scores of talking. In multi-party conversation, sometimes one speaks too much or too little and is a problem because we cannot obtain enough information. Our system has a game element to break through such a conversation. The system measures and visualizes the statistical profiles of conversation: who speaks to whom, who watches whom, and cumulative times of them by using front and back cameras of a tablet which each participant has. The system also calculates the participants' scores based on the rules of the game; the participant gets points when speaking and listening, and loses points when too much speaking. Therefore, the participants talk while trying maximize their scores, and it makes a balanced conversation. We evaluated the system in a within-subjects experiment, where five minutes of three-person conversation about a topic of software development and the system recorded the utterance of the participants. The results of experiments suggest that our system adequately balances the participants' utterance amounts.	experiment;real-time transcription;software development;tablet computer	Hiroyuki Adachi;Seiko Myojin;Nobutaka Shimada	2015		10.1145/2818427.2818454	interaction;simulation;computer science;artificial intelligence;multimedia	HCI	-52.7046239090877	-47.010071528610986	162682
da449f511b81af7de313d75247df0e098484ec42	investigating the contributing factors to make users react toward an on-screen agent as if they are reacting toward a robotic agent	cognitive science human robot interaction personal digital assistants cellular phones tv computer displays textile technology mobile robots animation personal communication networks;virtual reality;human robot interaction;data mining;educational robots;virtual reality human robot interaction;media;robotic agent;games;computer displays;analysis of variance;shiritori game;on screen agent;human robot interaction on screen agent robotic agent shiritori game;user acceptance	Our former study showed that users tended not to react to an on-screen agent's invitation of a Shiritori game (a last and first game) but that they did to a robotic agent. Our purpose in this study was to investigate the contributing factors that could make users react toward an on-screen agent as if they were reacting toward a robotic agent. We focused on the following two contributing factors: whether users accepted an invitation from a robotic agent and whether an on-screen agent was an attractive personality or character for the users. The results showed that the participants who first accepted the invitation of the robotic agent that was assigned an attractive character reacted toward the on-screen agent as if they were reacting to the robotic one; that is, these two factors played a significant role in making the participants react toward the on-screen agent as if they were reacting toward the robotic agent.	robot	Takanori Komatsu;Nozomi Kuki	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326350	human–robot interaction;games;simulation;media;analysis of variance;human–computer interaction;computer science;artificial intelligence;virtual reality;multimedia;educational robotics	Robotics	-52.32058263941412	-50.85047560857187	163107
c8985992f017f1a544922e4fb7399a692db1f50d	detecting posture mirroring in social interactions with wearable sensors	silicon;body worn motion sensors;atmospheric measurements;social interaction;nonverbal behavior;object detection image sensors;particle measurements;wearable sensors;psychology;image sensors;psychology lead silicon atmospheric measurements motion measurement wearable sensors particle measurements;wearable social behavioral assistants;posture mirroring detection;lead;social behavior;leadership style;body worn motion sensors posture mirroring detection social interactions wearable sensors wearable social behavioral assistants;social interactions;motion measurement;object detection	We envision wearable social-behavioral assistants which measure the nonverbal behavior of their users during social interaction. Research in psychology has linked posture mirroring, a key element of nonverbal behavior, to rapport and empathy and has been found to support communication. In this paper, we present a method to measure posture mirroring in social interaction with body-worn motion sensors. Our method is based on the detection of basic posture classes and the comparison of displayed postures across group members. We apply our method in a group discussion scenario involving 42 groups consisting of three subjects each in which group leaders express different leaderships styles. Our results show that we can measure differences in posture mirroring across groups of different leadership styles.	disk mirroring;interaction;poor posture;sensor;wearable computer;wearable technology	Sebastian Feese;Bert Arnrich;Gerhard Tröster;Bertolt Meyer;Klaus Jonas	2011	2011 15th Annual International Symposium on Wearable Computers	10.1109/ISWC.2011.31	social relation;computer vision;lead;simulation;leadership style;social behavior;image sensor;silicon	HCI	-50.050330356699455	-49.044305918587426	163176
85424195bfcc2372b6085a8336ac6e64f3783ffa	towards enhancing human experience by affective robots: experiment and discussion	robots motion pictures games psychology face human robot interaction computers	Many studies have addressed the affective robot, a robot that can express emotion, in the field of human-robot interaction. Really useful applications, however, can only be designed if the effect of such expressions on the user are completely elucidated. In this paper, we propose a new useful application scenario for the affective robot that shares the user's experience and describe an experiment in which the user's experience is altered by the presence of the affective robot. As the stimulus, we use movie scenes to evoke 4 types of emotion: excitement, fright, depression, and relaxation. Twenty four participants watch different movies under three conditions: no robot present, with robot that offers appropriate emotional expression, and with robot that has random emotional expression. The results show that the participants watching with the appropriate emotion robot experienced stronger emotion with exciting and relaxing movies and weaker emotion with scary movies than is true without the robot. These changes in the viewer's experience did not occur when watching with the random emotion robot. From the results, we extract design points of affective robot behavior for enhancing user experience. This research is novel in terms of examining the impact of robot emotion, seen as appropriate by the viewer, on the viewer's experience.	human–robot interaction;linear programming relaxation;robot;user experience	Takahiro Matsumoto;Shunichi Seko;Ryosuke Aoki;Akihiro Miyata;Tomoki Watanabe;Tomohiro Yamada	2015	2015 24th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2015.7333591	simulation;computer science;artificial intelligence;social robot;multimedia	Robotics	-52.11020023358683	-49.68314272374838	163362
6e9e10b250492d44b807b0d0298cc7a8161bea61	attitude modeling for virtual character based on temporal sequence mining: extraction and evaluation		Virtual agents are increasingly being integrated in our everyday life thanks to their communicative skills and abilities to express social affects like emotions and attitudes. The goal of this work is to evaluate the perception of agents expressing interpersonal attitudes through non-verbal behaviors. The interpretation of these behaviors depends on how they are sequenced and coordinated over time. To encompass the sequentiality and the dynamics of non-verbal signals, we rely on temporal sequence mining. From a multimodal corpus, this algorithm produces meaningful sequences resulting in more adapted expression of social attitudes of the agent.	algorithm;experiment;multimodal interaction;sequential pattern mining	Soumia Dermouche;Catherine Pelachaud	2018		10.1145/3212721.3212806	communication;perception;sequential pattern mining;computer science;everyday life;interpersonal communication	AI	-52.36146899068009	-48.0035547980881	163420
5281e72996f922f8f572ca8d8224727f13a3e680	the sociocognitive psychology of computer-mediated communication: the present and future of technology-based interactions	computer mediated communication	"""The increased diffusion of the Internet has made computer-mediated communication (CMC) very popular. However, a difficult question arises for psychologists and communication researchers: """"What are the communicative characteristics of CMC?"""" According to the """"cues-filtered-out"""" approach, CMC lacks the specifically relational features (social cues), which enable the interlocutors to identify correctly the kind of interpersonal situations they find themselves in. This paper counters this vision by integrating in its theoretical frame the different psycho-social approaches available in current literature. In particular, the paper describes the characteristics of the socio-cognitive processes-emotional expression, context definition, and identity creation-used by the interlocutors to make order and create relationships out of the miscommunication processes typical of CMC. Moreover, it presents the emerging forms of CMC-instant messaging, shared hypermedia, weblogs, and graphical chats-and their possible social and communicative effects."""	acclimatization;action theory (philosophy);blog;body position;computer keyboard;computer monitor;computer-mediated communication;cyberspace;deindividuation;graphical user interface;hypermedia;identity creation;instant messaging;interaction;item unique identification;mediator brand of benfluorex hydrochloride;muscle rigidity;poor posture;prosthesis;rule (guideline);sense of identity (observable entity);situated;social information processing;socio-cognitive;meeting	Giuseppe Riva	2002	Cyberpsychology & behavior : the impact of the Internet, multimedia and virtual reality on behavior and society	10.1089/109493102321018222	psychology;communication accommodation theory;medicine;human–computer interaction;computer science;engineering;artificial intelligence;multimedia;sociology;psychotherapist;communication;social psychology	HCI	-55.2852724275054	-50.93383418198274	163852
8e455c9a967921ed14a2d04d3f481f295e6f80d1	learning about harmony with harmony space: an overview	harmony space;pointing device;interface design;human computer interaction	Recent developments are presented in the evolution of Harmony Space, a highly enabling interface designed to encourage and facilitate rapid learning, especially by beginners, about the practical use and theory of tonal harmony, especially as applied to composition and analysis. The interface exploits both cognitive theories of tonal harmony and principles of human-computer interaction. In particular, the design of the interface draws on Balzano's and Longuet-Higgins' theories of tonal harmony. The interface allows entities of interest (notes, chords, chord progressions, key areas, modulations) to be manipulated via direct manipulation techniques using a single principled spatial metaphor. This makes it possible for novices to learn to perform a wide range of musical tasks involving harmony relatively quickly. The interface can also be used by experienced musicians to gain new insights and perform certain tasks much more easily than with conventional tools and notations. The interface is highly interactive and multimodal, using two pointing devices and spatial, aural and kinaesthetic cues that all map uniformly into the underlying representation. Some recent implementations of Harmony Space are discussed, together with some of the musical tasks which they make tractable for beginners and experienced musicians. Aspects of the simple, consistent, principled framework behind the interface are outlined.	cobham's thesis;computational model;direct manipulation interface;entity;higgins;human–computer interaction;mark steedman;morgan;multimodal interaction;pointing device;stanford university centers and institutes;theory	Simon D Holland	1993			simulation;computer science;artificial intelligence;interface design;communication;pointing device	HCI	-51.45500195805934	-46.30515501144426	163855
daf6a45835601c5fd4538de34d12d1e6139a7336	collaborative planning for mixed-autonomy lane merging		Driving is a social activity: drivers often indicate their intent to change lanes via motion cues. We consider mixed-autonomy traffic where a Human-driven Vehicle (HV) and an Autonomous Vehicle (AV) drive together. We propose a planning framework where the degree to which the AV considers the other agent's reward is controlled by a selfishness factor. We test our approach on a simulated two-lane highway where the AV and HV merge into each other's lanes. In a user study with 21 subjects and 6 different selfishness factors, we found that our planning approach was sound and that both agents had less merging times when a factor that balances the rewards for the two agents was chosen. Our results on double lane merging suggest it to be a non-zero-sum game and encourage further investigation on collaborative decision making algorithms for mixed-autonomy traffic.		Shray Bansal;Akansel Cosgun;Alireza Nakhaei;Kikuo Fujimura	2018	2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)	10.1109/IROS.2018.8594197	machine learning;artificial intelligence;computer science;robot;group decision-making;merge (version control);human–computer interaction;selfishness;autonomy	Robotics	-49.44430348345031	-51.469814521261355	164058
dea746574717d3e139edb2e8e57e126f8836ffc3	a multi-category theory of intention		People excel at attributing intentionality to other agents, whether in simple scenarios such as shapes moving in two dimensions or complex scenarios such as people interacting. We note that intentionality attributions seem to fall into two categories: low-level intentionality in which an observer has a theory of mind about an agent, and high-level intentionality in which an observer believes the agent has a theory of mind about something else. We introduce the terms L-intentionality and H-intentionality to refer to these attributions, respectively, and describe this division by using examples from previous research. Social robots provide a particularly good platform for evaluating the presence of different types of intentionality, and we discuss how robots can help distinguish the relationship between Hand L-intentionality, based on a number of possible models that we enumerate. We conclude by highlighting some interesting questions about intentionality in general and the interplay between Hand L-intentionality in particular.	categorization;category theory;enumerated type;experiment;high- and low-level;intentionality;interaction;robotics;social robot;top-down and bottom-up design;vocabulary	Henny Admoni;Brian Scassellati	2012			cognitive psychology;intentionality;animacy;cued speech;social psychology;psychology;psychological research;social robot;theory of mind;attribution;perception	HCI	-51.444036120183824	-50.795536435260026	164064
6930dbafab2c87ef297e77fe50c7baadb1571795	scrutinizing natural scenes: controlling the gaze of an embodied conversational agent	embodied conversational agents;visual acuity;data collection;visual scene analysis;physical environment;talking face;embodied conversational agent;visual attention;face to face;eye gaze;natural scenes;scene analysis;face to face interaction	We present here a system for controlling the eye gaze of a virtual embodied conversational agent able to perceive the physical environment in which it interacts. This system is inspired by known components of human visual attention system and reproduces its limitations in terms of visual acuity, sensitivity to movement, limitations of short-memory and object pursuit. The aim of this coupling between animation and visual scene analysis is to provide sense of presence and mutual attention to human interlocutors. After a brief introduction to this research project and a focused state of the art, we detail the components of our system and confront simulation results to eye gaze data collected from viewers observing the same natural scenes.	anomalous experiences;dialog system;embodied agent;embodied cognition;simulation	Antoine Picot;Gérard Bailly;Frédéric Elisei;Stephan Raidt	2007		10.1007/978-3-540-74997-4_25	psychology;computer vision;face-to-face interaction;eye tracking;embodied agent;multimedia;communication;data collection	HCI	-51.13357305175832	-47.990659819056056	164738
ac8031793d1909950bb51f1dbfa68ffdd3a5c3ee	inspire, guide, and entertain: designing a mobile assistant for runners	mobile sports;user interface;real time;mobile ui;training assistant;auditory design	The paper presents the design of a mobile assistant for runners. We propose visual and auditory user interface for a mobile assistant, called Mobota. The system supports navigation on a new track, provides competition against a virtual rival, monitors real time user performance, entertains and encourages runners. We introduce entertaining and inspiring community notes that convey emotional messages from other sportsmen who exercise on the same track. The design and evaluation of Mobota provides an insight into the specifics of visual and auditory design of running assistants.	user interface	Ekaterina Kurdyukova	2009		10.1145/1613858.1613947	simulation;human–computer interaction;computer science;operating system;multimedia;user interface	HCI	-50.35368438841407	-46.60015859288527	165109
35c904cbd0c50042e32f78e9d1191ce3400c297e	the importance of interface agent visual presence: voice alone is less effective in impacting young women's attitudes toward engineering	computer based social modeling;anthropomorphic interface agents;attitude change;interface agent;self efficacy;persuasion;agency theory	Anchored in social agency theory, recent research has emphasized the importance of anthropomorphic interface agents' voice to impact learning-related outcomes. Nevertheless, literature on human social models suggests that the appearance of an interface agent may have important implications for its ability to influence attitudes and self-efficacy. Therefore, we hypothesized that visual presence of the interface agent would result in more positive attitudes toward engineering and greater self-efficacy than the presence of a human voice alone. In accordance to our hypothesis, results revealed that participants who interacted with the visible agents reported significantly greater utility for engineering, greater self-efficacy, and greater interest in engineering related fields than those who interacted with a human voice. Thus, the current work indicates the importance of anthropomorphic agent's visibility in changing attitudes and beliefs.	intelligent user interface	Rinat B. Rosenberg-Kima;Amy L. Baylor;E. Ashby Plant;Celeste E. Doerr	2007		10.1007/978-3-540-77006-0_27	psychology;self-efficacy;principal–agent problem;simulation;communication;social psychology;attitude change	HCI	-52.7747514767574	-51.26832148586998	165895
7529359f30c3b83c1d350fec182f948ae128d5e9	real-time visual prosody for interactive virtual agents		Speakers accompany their speech with incessant, subtle head movements. It is important to implement such “visual prosody” in virtual agents, not only to make their behavior more natural, but also because it has been shown to help listeners understand speech. We contribute a visual prosody model for interactive virtual agents that shall be capable of having live, non-scripted interactions with humans and thus have to use Text-To-Speech rather than recorded speech. We present our method for creating visual prosody online from continuous TTS output, and we report results from three crowdsourcing experiments carried out to see if and to what extent it can help in enhancing the interaction experience with an agent.	crowdsourcing;experiment;intelligent agent;interaction;netware file system;real-time transcription;semantic prosody	Herwin van Welbergen;Yu Ding;Kai Sattler;Catherine Pelachaud;Stefan Kopp	2015		10.1007/978-3-319-21996-7_16	natural language processing;speech recognition;computer science;communication	HCI	-52.36461054452487	-46.98613263596415	166077
55b9a8d6dd98de106231983273140218081f753a	designing motivational robot: how robots might motivate children to eat fruits and vegetables	robot persuasiveness motivational robot children social robots personal assistants public assistants humanoid robot verbal features bodily features healthier lifestyles motivational cues;pediatrics;niobium;niobium pediatrics games modulation humanoid robots educational institutions;humanoid robots;games;service robots humanoid robots human robot interaction;modulation	This study contribute toward the creation of social robots as personal and public assistants. The ability of the robot to persuade and motivate people to follow a given behavior is of particular relevance in several cases, especially those related to people's health recover and maintenance (e.g., personal trainer, diet coach, etc.). In this paper, we evaluated the effect of a humanoid robot's use of verbal and bodily features and behaviors in motivating 80 children (age: 8-9 years old) in following healthier lifestyles (namely eat more fruit and vegetables). The results confirmed the hypothesis that the use of such motivational cues significantly improves the persuasiveness of the robot. Moreover the results highlighted a higher impact of the verbal cues implementations, which is in contrast with previous studies.	experiment;humanoid robot;mcgurk effect;modulation;relevance;social robot	Ilaria Baroni;Marco Nalin;Mattia Coti Zelati;Elettra Oleari;Alberto Sanna	2014	The 23rd IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2014.6926350	games;niobium;simulation;computer science;humanoid robot;artificial intelligence;multimedia;modulation	Robotics	-52.763095468808245	-51.073398994956285	166098
a7ed90afc5aebe81a08d84e27b5f9a300e384873	perceptual analysis of haptic icons: an investigation into the validity of cluster sorted mds	haptic interfaces multidimensional systems data analysis algorithm design and analysis displays state feedback frequency actuators information analysis machine intelligence;haptic icons;data collection;state feedback;actuators;data analysis;tactile feedback tactile display haptic icons handheld device multidimensional scaling tactile perception;tactile feedback;tactile display;tactile perception;displays;machine intelligence;multidimensional scaling;handheld device;haptic interfaces;frequency;information analysis;algorithm design and analysis;multidimensional systems	The design of usable haptic icons (brief informational signals delivered through the sense of touch) requires a tool for measuring perceptual distances between icons that will be used together as a set. Our experiences with one potentially powerful approach, Multidimensional Scaling (MDS) analysis of perceptual data acquired using an efficient cluster sorting technique, raised questions relating to the methodology for data collection. In this paper, we review key issues relating to perceptual data collection method, describe an example data set and present its initial MDS analysis, and then examine the impact of collection method on MDS outcome through a secondary analysis of the data and the inherent structure of the algorithm components. Our analysis suggests that an understanding of these issues is important for the method’s effective use, but has not exposed any major flaws with the process.	algorithm;distance matrix;distortion;dual-flashlight plot;experiment;haptic technology;multidimensional scaling;ordinal data;self-replication;sorting;sorting algorithm;usability	Jérôme Pasquero;Joseph Luk;Shannon Little;Karon E. MacLean	2006	2006 14th Symposium on Haptic Interfaces for Virtual Environment and Teleoperator Systems	10.1109/HAPTICS.2006.171	computer vision;human–computer interaction;computer science;communication	SE	-49.656249237728154	-47.02938916725764	166360
773b8ec81f162fc237211221f780a0fe2821d800	mkultra (demo)		MKULTRA is an experimental game that explores novel AIbased game mechanics. Similar in some ways to text-based interactive fiction, the player controls a character who interacts with other characters through dialog. Unlike traditional IF, MKULTRA characters have simple natural language understanding and generation capabilities, sufficient to answer questions and carry out simple tasks. The game explores a novel game mechanic, belief injection, in which players can manipulate the behavior of NPCs by injecting false beliefs into their knowledge bases. This allows for an unusual form of puzzle-based gameplay, in which the player must understand the beliefs and motivational structure of the characters well enough to understand what beliefs to inject.	game mechanics;knowledge base;natural language processing;natural language understanding;text-based (computing);dialog	Ian D. Horswill	2015				NLP	-53.41632003219341	-47.99034568437868	166611
b54c86d7c8871e2b61f9095fa405677c77d23e51	measuring physical exertion in virtual reality exercise games		We demonstrate a novel method of applying the capabilities of mobile virtual reality technology to the health sciences by measuring physical exertion in a VR exercise game. By measuring changes in heart rate in a thirteen person user study, we find evidence to suggest that virtual reality exercise is able to induce a moderate to high level of physical exertion and produce an immersive an intriguing experience.	high-level programming language;usability testing;virtual reality	Nikesh Mishra;Eelke Folmer	2018		10.1145/3281505.3281611	simulation;computer science;human–computer interaction;immersion (virtual reality);virtual reality;exertion	HCI	-54.678877201941724	-49.75891082571226	167180
89a626bc522154d5bb56901a99f84e6c819cf889	instrumental access to natural multimodal discourse	instruments;multimodal communication;torso;speech analysis;psychology;drives;computer vision;hidden markov models;humans;labeling;timing;instruments psychology humans hidden markov models speech analysis torso drives computer vision timing labeling	Human multimodal communicative behaviors form a tightly integrated whole. We have present a paradigm multimodal analysis in natural discourse based on a feature decompositive psycholinguistically derived model that permits us to access the underlying structure and intent of multimodal communicative discourse. We outline the psycholinguistics that drive our paradigm, the Catchment concept that facilitates our getting a computational handle on discourse entities, and summarize some approaches and results that realize the vision. We show examples of such discoursestructuring features as handedness, types of symmetry, gaze-atinterlocutor, and hand ‘origos’. Such analysis is an alternative to the ‘recognition of one discrete gesture out of stylized whole gesture models’ paradigm.	circular polarization;computation;entity;gesture recognition;multimodal interaction;programming paradigm	Francis K. H. Quek	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237779	computer vision;labeling theory;speech recognition;torso;computer science;artificial intelligence;machine learning;hidden markov model	Robotics	-51.72657932295021	-47.01573336242015	167865
d0c3960dc69ec858f8e455fd7c4e082b707d2308	detecting betrayers in online environments using active indicators		Research into betrayal ranges from case studies of real-world betrayers to controlled laboratory experiments. However, the capability of reliably detecting individuals who have previously betrayed through an analysis of their ongoing behavior (after the act of betrayal) has not been studied. To this aim, we propose a novel method composed of a game and several manipulations to stimulate and heighten emotions related to betrayal. We discuss the results of using this game and the manipulations as a mechanism to spot betrayers, with the goal of identifying important manipulations that can be used in future studies to detect betrayers in real-world contexts. In this paper, we discuss the methods and results of modeling the collected game data, which include behavioral logs, to identify betrayers. We used several analysis methods based both on psychology-based hypotheses as well as machine learning techniques. Results show that stimuli that target engagement, persistence, feedback to teammates, and team trust produce behaviors that can contribute to distinguishing betrayers from non-betrayers.	sensor	Paola Rizzo;Chaima Jemmali;Alice Leung;Karen Haigh;Magy Seif El-Nasr	2018		10.1007/978-3-319-93372-6_2	data mining;social psychology;deception;betrayal;persistence (computer science);computer science	Robotics	-53.620216436754205	-49.58212930481928	168861
3ec7459b13147c9b62b8b4ee67fec203387c5eeb	investigating the effects of physical and virtual embodiment in task-oriented and conversational contexts	virtual character;embodiment;evaluation;comparison study;robot	Studies comparing physically embodied robots with virtually embodied screen characters (e.g. Powers et al., 2007. Jung and Lee, 2004.) have resulted in unsimilar findings with respect to subjective (users' evaluations) as well as objective (e.g. task performance of the users) measurements. The comparability of these results is mainly impeded by the use of different robots, a variety of virtual embodiments (video recording, computer simulation, animated characters, etc.) and different interaction scenarios. To overcome this problem, an experimental study was conducted in which the embodiment of an artificial entity was varied systematically as well as the type of interaction using a 2x2 between subjects design (N=83). Participants interacted with either a robot or a virtual representation of this robot (on a screen) in a task-oriented or a persuasive-conversational scenario. The results revealed that participants perceived the robot as more competent than the virtual character in the task-oriented scenario, but the opposite was true for the persuasive-conversational scenario. Furthermore, participants in the task-oriented scenario felt better after the interaction than participants who had a persuasive-conversational interaction with the artificial entity, regardless of its embodiment. No statistically significant differences between the experimental conditions emerged with respect to objective measures (persuasion and task performance). Various explanations for these findings are discussed and implications for the application of robots and virtual characters are derived.		Laura Hoffmann;Nicole C. Krämer	2013	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2013.04.007	robot;simulation;computer science;artificial intelligence;evaluation	HCI	-51.09656330698452	-50.50674918685375	169736
2bedf18031a79205273a2ebf0047610a691176ce	social snacking with a virtual agent - on the interrelation of need to belong and effects of social responsiveness when interacting with artificial entities		Abstract Based on considerations that people´s need to belong can be temporarily satisfied by “social snacking” (Gardner et al., 2005) in the sense that in absence of social interactions which adequately satisfy belongingness needs surrogates can bridge lonely times, it was tested whether the interaction with a virtual agent can serve to ease the need for social contact. In a between subjects experimental setting, 79 participants interacted with a virtual agent who either displayed socially responsive nonverbal behavior or not. Results demonstrate that although there was no main effect of socially responsive behavior on participants´ subjective experience of rapport and on connectedness with the agent, those people with a high need to belong reported less willingness to engage in social activities after the interaction with a virtual agent – but only if the agent displayed socially responsive behavior.	artificial intelligence;entity;interaction;responsiveness	Nicole C. Krämer;Gale M. Lucas;Lea Schmitt;Jonathan Gratch	2018	Int. J. Hum.-Comput. Stud.	10.1016/j.ijhcs.2017.09.001	social responsibility;social connectedness;social psychology;nonverbal communication;human–computer interaction;belongingness;main effect;snacking;computer science	AI	-52.88974689956612	-50.52119248219906	170583
831ea299d0d71bb56510dd0654c3432678e63fc1	group attention control for communication robots with wizard of oz approach	microphones;humanoid robot;social sciences;telerobotics human factors humanoid robots human robot interaction mobile robots museums social sciences;mobile robots;human robot interaction;field trial;science museum robot;museums;wizard of oz;attentional control;human factors;humanoid robots;abstracts;robots;telerobotics;commutation robot;group attention control;interaction effect;interactive humanoid robots group attention control communication robots oz approach social situations control semiautonomous gac system science museum free play interactions two week experiment;science museum robot commutation robot group attention control field trial human robot interaction;cameras;robots abstracts cameras microphones	This paper describes a group attention control (GAC) system that enables a communication robot to simultaneously interact with many people. GAC is based on controlling social situations and indicating explicit control to unify all purposes of attention. We implemented a semi-autonomous GAC system into a communication robot that guides visitors to exhibits in a science museum and engages in free-play interactions with them. The GAC system's effectiveness was demonstrated in a two-week experiment in the museum. We believe these results will allow us to develope interactive humanoid robots that can interact effectively with groups of people.	autonomous robot;humanoid robot;interaction;semiconductor industry;smoothing	Masahiro Shiomi;Takayuki Kanda;Satoshi Koizumi;Hiroshi Ishiguro;Norihiro Hagita	2007	2007 2nd ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1228716.1228733	human–robot interaction;simulation;computer science;humanoid robot;artificial intelligence;human factors and ergonomics	Robotics	-51.56167793715227	-50.21588143802014	172144
12bb40d11a4245b688d9cbc3b49428ec469007f4	bridging music using sound-effect insertion	sound effects;rhythm;ism 2014;mashups;multimedia;international symposium on multimedia;bridges;user interfaces music;media;visualization;concatenating music;videos visualization mashups media rhythm bridges;international symposium on multimedia sound effects concatenating music multimedia ism 2014;music recomposition sound effect insertion audio music concatenation music clips user preference;videos	In this article, the authors offer a general overview of audio music concatenation, then deliberate on how to connect not-so-coherent music clips in response to the rising awareness of user-preference in music recomposition. In particular, they introduce sound-effect insertion into the proposed bridging process to make the transition natural and euphonious. To systematically verify the feasibility of the proposed music concatenation methods, they conducted specifically designed experiments to collect subjective opinions and reduce the cognitive loads of the participants. The results indicate that using suitable sound effects greatly enhances the listening experiences among various subjects.	bridging (networking);coherence (physics);concatenation;experience;experiment;insertion sort	Yin-Tzu Lin;Chuan-Lung Lee;Jyh-Shing Roger Jang;Ja-Ling Wu	2015	IEEE MultiMedia	10.1109/MMUL.2015.71	speech recognition;media;visualization;telecommunications;computer science;rhythm;music visualization;multimedia;pop music automation;world wide web;mashup	HCI	-53.19303450359226	-45.84313470236454	172190
719a22cd8062d68dbdfc62d2d626bc2965e5dada	prosodic synchrony: dynamic social coordination in conversation		A key goal for participants in language communication is to bring about a mutually shared experience of ideas, event narratives, and emotional responses. This goal is achieved not only through the exchange of lexical meaning, but also through interactive signaling to coordinate information status, expressed in both verbal and nonverbal forms. This study presents our results on prosodic interactive alignment in spoken dialogues, drawing from extended conversational data in English and Chinese. Our results show that global prosodic synchrony (alignment)and dissynchrony both occur in conversation, and that synchrony is achieved gradually as participants interactively cooperate to build up a shared information and involvement state. Detailed analysis of our data further indicates that participant feedback responses are a critical component of cooperative adaptation to new information, and that the complementary distribution of feedback responses helps to bring about the synchronous prosodic patterns associated with convergent speaker states.	interactivity	Li-chiung Yang	2014			psychology;cognitive psychology;conversation;complementary distribution;narrative;nonverbal communication;communication;lexical definition	HCI	-53.18368563725556	-48.23162586770865	172923
38bfa875d8cdee294b092e878b646cec913b8596	a receptionist robot for brazilian people: study on interaction involving illiterates	uncanny valley;service robotics;human robot interaction;socially assistive robotics;illiteracy;receptionist;anthropomorphism		robot	Gabriele Trovato;Josué Jr. Guimarães Ramos;Helio Azevedo;Artemis Moroni;Silvia Magossi;Reid G. Simmons;Hiroyuki Ishii;Atsuo Takanishi	2017	Paladyn	10.1515/pjbr-2017-0001	human–robot interaction;functional illiteracy;simulation;computer science;artificial intelligence;uncanny valley	HCI	-53.09026459691902	-50.47422979597497	173317
acee6857252f3d1d409c758d921171e94ec51a70	talking with sentiment: adaptive expression generation behavior for social robots		This paper presents a neural-based approach for generating natural gesticulation movements for a humanoid robot enriched with other relevant social signals depending on sentiment processing. In particular, we take into account some simple head postures, voice parameters, and eyes colors as expressiveness enhancing elements. A Generative Adversarial Network (GAN) allows the proposed system to extend the variability of basic gesticulation movements while avoiding repetitive and monotonous behavior. Using sentiment analysis on the text that will be pronounced by the robot, we derive a value for emotion valence and coherently choose suitable parameters for the expressive elements. In this way, the robot has an adaptive expression generation during talking. Experiments validate the proposed approach by analyzing the contribution of all the factors to understand the naturalness perception of the robot behavior.	social robot	Igor Rodriguez Rodriguez;Adriano Manfré;Filippo Vella;Ignazio Infantino;Elena Lazkano	2018		10.1007/978-3-319-99885-5_15	humanoid robot;social robot;human–computer interaction;sentiment analysis;robot;behavior-based robotics;naturalness;cognitive robotics;computer science;human–robot interaction	AI	-50.5704432051422	-49.51462471614222	173342
a5a0e2bcfe21819d2c8a02611a54cd80c6948807	the relevance of social cues in assistive training with a social robot		This paper examines whether social cues, such as facial expressions, can be used to adapt and tailor a robot-assisted training in order to maximize performance and comfort. Specifically, this paper serves as a basis in determining whether key facial signals, including emotions and facial actions, are common among participants during a physical and cognitive training scenario. In the experiment, participants performed basic arm exercises with a social robot as a guide. We extracted facial features from video recordings of participants and applied a recursive feature elimination algorithm to select a subset of discriminating facial features. These features are correlated with the performance of the user and the level of difficulty of the exercises. The long-term aim of this work, building upon the work presented here, is to develop an algorithm that can eventually be used in robot-assisted training to allow a robot to tailor a training program based on the physical capabilities as well as the social cues of the users.	relevance;social robot	Neziha Akalin;Andrey Kiselev;Annica Kristoffersson;Amy Loutfi	2018		10.1007/978-3-030-05204-1_45	social robot;cognitive training;social cue;facial expression;machine learning;artificial intelligence;computer science	ML	-50.434633449143966	-48.99854025984921	173360
b49c7bf70db710c4275ac9bd799ca08183b8e161	the effects of favor and apology on compliance	apology;favor;compliance;persuasion;indirect effect;reciprocity	This study was designed to test the effects of favor and apology on compliance and to explain any potential effect via indebtedness, gratitude, and liking. Two experiments were devised to accomplish these ends. In the first experiment favor and apology were varied in the absence of a transgression to see if apologizing for not providing a favor can be used proactively to increase compliance. In the second experiment favor and apology were varied in a more common scenario, following a transgression. Results show that favor has a positive effect on compliance mediated by gratitude when using a general prosocial request and by liking when using a more altruistic request. Results also suggest that apology has a positive effect on liking and that apology has an indirect effect on compliance under certain conditions.		Ryan Goei;Anthony Roberto;Gary Meyer;Kellie Carlyle	2007	Communication Research	10.1177/0093650207307896	psychology;communication;reciprocity;social psychology	Robotics	-52.14536071588248	-51.96233510490893	173578
8032461ba8944c584d3c1655603c8c2f03c9be52	as time goes by: long-term evaluation of social presence in robotic companions	human robot interaction intelligent robots robot sensing systems intelligent sensors particle measurements virtual reality scholarships heart intelligent agent medical services;long period;pediatrics;human robot interactions;synthetic character;social robot robotic companions social presence human robot interactions;intelligent robots;human robot interaction;data mining;social relation;socio economic effects human robot interaction intelligent robots;games;animation;robotic companions;mood;socio economic effects;social presence;social robot;robot kinematics	Given the recent advances in robot and synthetic character technology, many researchers are now focused on ways of establishing social relations between these agents and humans over long periods of time. Early studies have shown that the novelty effect of robots and agents quickly wears out and that people change their attitudes and preferences towards them over time. In this paper, we study the role of social presence in long-term human-robot interactions. We conducted a study where children played chess exercises with a social robot over a five week period. With this experiment, we identified possible key issues that should be considered when designing social robots for long-term interactions.	humans;interaction;social presence theory;social robot;synthetic intelligence	Iolanda Leite;Carlos Martinho;André Pereira;Ana Paiva	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326256	human–robot interaction;anime;games;social relation;simulation;computer science;artificial intelligence;social robot;robot kinematics	Robotics	-53.35241447771892	-50.72871177714272	173754
afa63438d948ed07a2448b4ce5a447ad0830bce7	initial expectations, interactions, and beyond with social robots		Abstract The current study builds upon our previous research on the human-to-human interaction script, which suggests that people expect to interact with other humans when encountering an initial interaction. This expectation impacts the initial impressions people have when interacting with social robots. The current experiment examined how short interactions might change those initial impressions. Participants were less uncertain and perceived greater social presence after a single brief interaction with a humanoid social robot. Social presence decreased after a short interaction with a human. These data suggest that initial impressions based on the human-to-human interaction script may impact actual interaction and the impressions that result from it. Open-ended responses suggest the potential for hyperpersonal communication in human-robot interaction. These findings are further discussed, as are limitations and directions for future research. The paper concludes with an agenda for applying constructivist interpersonal communication frameworks to human-robot interaction studies.	interaction;social robot	Autumn Edwards;Chad Edwards;David Westerman;Patric R. Spence	2019	Computers in Human Behavior	10.1016/j.chb.2018.08.042	social robot;psychology;social psychology;constructivism (philosophy of education);hyperpersonal model;interpersonal communication	Robotics	-53.39055665055151	-50.42668036860786	173891
8cada0a104fe5ae149700d8c383eb1be4172e9b4	“errare humanum est”: simulation of communication error among a virtual team in crisis situation	stress;training;biological system modeling;virtual environments;receivers;computational modeling;context	In the context of medical team leaders training, we present a multiagent communication model that can introduce errors in a team of agents. This model is built from existing work from the literature in multiagents systems and information science, but also from a corpus of dialogues collected during actual field training for medical teams. Our model supports four types of communication errors (misunderstanding, misinterpretation, non-understanding and absence of answer) that appear at different stages of the communication process.	agent-based model;ambiguous name resolution;distortion;domain generation algorithm;estdomains;experiment;information science;modality (human–computer interaction);procurement;simulation	Lauriane Huguet;Nicolas Sabouret;Domitile Lourdeaux	2016	2016 IEEE 15th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)	10.1109/ICCI-CC.2016.7862058	simulation;engineering;knowledge management;communication	Robotics	-53.730633848392856	-47.762048249312315	174025
116610b360f629854993161c2c24d49f07fed22d	adaptive automated storytelling based on audience response		To tell a story, the storyteller uses all his/her skills to entertain an audience. This task not only relies on the act of telling a story, but also on the ability to understand reactions of the audience during the telling of the story. A well-trained storyteller knows whether the audience is bored or enjoying the show just by observing the spectators and adapts the story to please the audience. In this work, we propose a methodology to create tailored stories to an audience based on personality traits and preferences of each individual. As an audience may be composed of individuals with similar or mixed preferences, it is necessary to consider a middle ground solution based on the individual options. In addition, individuals may have some kind of relationship with others that influence their decisions. The proposed model addresses all steps in the quest to please the audience. It infers what the preferences are, computes the scenes reward for all individuals, estimates their choices independently and in group, and allows Interactive Storytelling systems to find the story that maximizes the expected audience reward.		Augusto Baffa;Marcus Poggi de Aragão;Bruno Feijó	2015		10.1007/978-3-319-24589-8_4	multimedia;world wide web;computer graphics (images)	HCI	-54.24972658440746	-48.18791605498854	174300
7eb06f0aa53984b6a8dfe2621d238b3c15aae252	lessons from teachers on performing hri studies with young children in schools	teachers;field work;learning;longitudinal studies hri studies human robot interaction autonomous social robotic learning companion preschool classrooms american public school young children teachers teaching assistants preschool education educational tool child robot interaction studies microgenetic studies;teaching educational robots human robot interaction;social assistive robotics;testing;user studies;monitoring;robots education interviews games headphones monitoring testing;games;robots;interviews;headphones;robotic learning companions;field work education user studies learning social assistive robotics robotic learning companions teachers	We deployed an autonomous social robotic learning companion in three preschool classrooms at an American public school for two months. Before and after this deployment, we asked the teachers and teaching assistants who worked in the classrooms about their views on the use of social robots in preschool education. We found that teachers' expectations about the experience of having a robot in their classrooms often did not match up with their actual experience. These teachers generally expected the robot to be disruptive, but found that it was not, and furthermore, had numerous positive ideas about the robot's potential as a new educational tool for their classrooms. Based on these interviews, we provide a summary of lessons we learned about running child-robot interaction studies in preschools. We share some advice for future researchers who may wish to engage teachers and schools in the course of their own human-robot interaction work. Understanding the teachers, the classroom environment, and the constraints involved is especially important for microgenetic and longitudinal studies, which require more of the school's time-as well as more of the researchers' time-and is a greater opportunity investment for everyone involved.	autonomous robot;human–robot interaction;information source;social robot;software deployment	Jacqueline Kory Westlund;Goren Gordon;Samuel Spaulding;Jin Joo Lee;L. Kenyon Plummer;Marayna Martinez;Madhurima Das;Cynthia Breazeal	2016	2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1109/HRI.2016.7451776	robot;games;simulation;interview;computer science;artificial intelligence;multimedia;software testing;field research	Robotics	-53.98948691967972	-51.57225224605957	175249
b50f33393b82f2d624be7595e453ac77ed559d0c	intelligent agents - conversations from human-agent imitation games		What do humans say/ask beyond initial greetings? Are humans always the best at conversation? How easy is it to distinguish an intelligent human from an ‘intelligent agent’ just from their responses to unrestricted questions during a conversation? This paper presents an insight into the nature of human communications, including behaviours and interactions, from a type of interaction stranger-to-stranger discourse realised from implementing Turing’s question-answer imitation games at Bletchley Park UK in 2012 as part of the Turing centenary commemorations. The authors contend that the effects of lying, misunderstanding, humour and lack of shared knowledge during human-machine and human-human interactions can provide an impetus to building better conversational agents increasingly deployed as virtual customer service agents. Applying the findings could improve human-robot interaction, for example as conversational companions for the elderly or unwell. But do we always want these agents to talk like humans do? Suggestions to advance intelligent agent conversation are provided.	dialog system;e-commerce;emoticon;humans;human–robot interaction;intelligent agent;turing;turing test	Kevin Warwick;Huma Shah	2015				AI	-52.25942229148955	-48.50497474749253	175768
ccbed85a2ec6bc3a46270a745589e671a97f8934	exploring feedback strategies to improve public speaking: an interactive virtual audience framework	public speaking;training;virtual reality;multimodal interfaces	Good public speaking skills convey strong and effective communication, which is critical in many professions and used in everyday life. The ability to speak publicly requires a lot of training and practice. Recent technological developments enable new approaches for public speaking training that allow users to practice in a safe and engaging environment. We explore feedback strategies for public speaking training that are based on an interactive virtual audience paradigm. We investigate three study conditions: (1) a non-interactive virtual audience (control condition), (2) direct visual feedback, and (3) nonverbal feedback from an interactive virtual audience. We perform a threefold evaluation based on self-assessment questionnaires, expert assessments, and two objectively annotated measures of eye-contact and avoidance of pause fillers. Our experiments show that the interactive virtual audience brings together the best of both worlds: increased engagement and challenge as well as improved public speaking skills as judged by experts.	experiment;interactivity;programming paradigm	Mathieu Chollet;Torsten Wörtwein;Louis-Philippe Morency;Ari Shapiro;Stefan Scherer	2015		10.1145/2750858.2806060	simulation;human–computer interaction;public speaking;computer science;virtual reality;multimedia	HCI	-54.67871455370582	-47.70718475794832	175984
5bb43bc283d816ecaeec4c7e44031a7b309e7e1d	analysing emotional video using consumer eeg hardware		Low-cost, easy to use EEG hardware produced for the consumermarket provide interesting possibilities for human-computer interaction in a wide variety of applications. Recent years have produced numerous papers discussing the use of these types of devices in various ways, but only some of this work looks into what these devices can actually measure. In this paper, data is used that has been collected using a Myndplay Brainband, while 30 participants viewed emotional videos eliciting different mental states. This data is analyzed by looking at average power in multiple frequency bands and eSense values, as well as peaks in the measurements detected throughout the videos. Although average values do not differentiate well between the mental states, peak detection provides some promising results worthy of future research.	electroencephalography;frequency band;human–computer interaction;mental state;myndplay	Jeroen de Man	2014		10.1007/978-3-319-07230-2_69	human–computer interaction;multimedia;computer science;electroencephalography;computer hardware	HCI	-52.518869216143116	-45.230759078435426	176184
3ab28303bf4c8cc432f763de8f10ba366d9c4383	constructing awareness through speech, gesture, gaze and movement during a time-critical medical task		We conducted a video-based study to examine how medical teams construct and maintain awareness of what is going on in the environment during a time-critical, collaborative task—endotracheal intubation. Drawing on a theme that characterizes work practices in collaborative work settings—reading a scene—we examine both vocal and non-vocal actions (e.g., speech, body movement, gesture, gaze) of team members participating in this task to understand how these actions are used to display status of one’s work or to acquire information about the work status of others. While each action modality was helpful in constructing awareness to some extent, it posed different challenges, requiring team members to combine both vocal and non-vocal actions to achieve awareness about each other’s activities and their temporal order. We conclude by discussing different types of non-vocal actions, their purpose, and the need for computational support in this dynamic work setting.	computation;modality (human–computer interaction);window of opportunity	Zhan Zhang;Aleksandra Sarcevic	2015		10.1007/978-3-319-20499-4_9	computer vision;speech recognition;communication	HCI	-51.28651268874311	-48.608096606943995	176735
6a2af91df88f67d8b4992dc70db5678f4ab7c582	improving bci usability as hci in ambient assisted living system control		"""Brain Computer Interface (BCI) technology is an alternative/ augmentative communication channel, based on the interpretation of the user's brain activity, who can then interact with the environment without relying on neuromuscular pathways. Such technologies can act as alternative HCI devices towards AAL (Ambient Assisted Living) systems, thus opening their services to people for whom interacting with conventional interfaces could be troublesome, or even not viable. We present here a complete solution for BCI-enabled home automation. The implemented solution is, nonetheless, more general in the approach, since both the realized hardware module and the software infrastructure can handle general bio-potentials. We demonstrate the effectiveness of the solution by restricting the focus to a SSVEP-based, self-paced BCI, featuring calibration-less operation and a subject-independent, """"plugp at the same time, the signal processing chain will be presented, introducing a novel method for improving accuracy and immunity to false positives. The results achieved, especially in terms of false positive rate containment (0.26 min −1 ) significantly improve over the literature."""	human–computer interaction;usability	Niccolò Mora;Ilaria De Munari;Paolo Ciampolini	2015		10.1007/978-3-319-20816-9_28	simulation;human–computer interaction;computer science;communication	HCI	-49.84172217609131	-45.61982979892563	177304
3ea2889b72cb9e2a021672c0a107a824261a2c7a	modeling facial expression of uncertainty in conversational animation	embodied conversational agents;uncertainty;conversational agent;talking head;face to face conversation;facial expression;face to face;facial displays	Building animated conversational agents requires developing a fine-grained analysis of the motions and meanings available to interlocutors in face-to-face conversation and implementing strategies for using these motions and meanings to communicate effectively. In this paper, we sketch our efforts to characterize people’s facial displays of uncertainty in face-to-face conversation. We analyze empirical data from human–human conversation and extend our platform for conversational animation, including RUTH (the Rutgers University Talking Head), to simulate what we find. This methodology leads to a range of new insights into the structure, timing, expressive content and communicative function of facial actions.	avatar (computing);bcs-facs;coherence (physics);computer animation;dialog system;prototype;requirement;semantic prosody;simulation	Matthew Stone;Insuk Oh	2006		10.1007/978-3-540-79037-2_4	speech recognition;computer science;multimedia;communication	HCI	-52.63408432087037	-47.158634925718324	177380
633d56f583ef7d66c0ef4f2258fef74fe4e29709	validating test chambers to study cooperative communication mechanics in portal 2	game analysis;cooperation;experimentation;communication	Cooperative communication mechanics, such as avatar gestures or in-game visual pointers, enable player collaboration directly through gameplay. There remain open questions about how players use cooperative communication mechanics, and whether they can effectively supplement or even supplant traditional voice and chat communication. This paper describes a future study to investigate player communication in Portal 2, and chronicles the design and validation of test chambers for the study.	portal 2;virtual world	Deepika Vaddi;Rina R. Wehbe;Zachary O. Toups;Samantha N. Stahlke;Rylan Koroluk;Lennart E. Nacke	2015		10.1145/2793107.2810302	simulation;human–computer interaction;computer science;multimedia	HCI	-54.764712653197556	-46.0768862512567	177950
2eab4ada99157b68a90cf3cf354f139d266fcab5	a method for semi-automatic explicitation of agent's behavior: application to the study of an immersive driving simulator	multi agent simulation;credibility evaluation;behavior clustering and explicitation;objective and subjective approach	This paper presents a method for evaluating the credibility of agents’ behaviors in immersive multi-agent simulations. It combines two approaches. The first one is based on a qualitative analysis of questionnaires filled by the users and annotations filled by others participants to draw categories of users (related to their behavior in the context of the simulation or in real life). The second one carries out a quantitative behavior data collection during simulations in order to automatically extract behavior clusters. We then study the similarities between user categories, participants’ annotations and behavior clusters. Afterward, relying on user categories and annotations, we compare human behaviors to agent ones in order to evaluate the agents’ credibility and make their behaviors explicit. We illustrate our method with an immersive driving simulator	algorithm;cluster analysis;domain-specific language;driving simulator;high- and low-level;immersion (virtual reality);multi-agent system;real life;relevance;robustness (computer science);semiconductor industry;simulation;virtual reality;weka	Kévin Darty;Julien Saunier;Nicolas Sabouret	2014		10.5220/0004821400810091	simulation;computer science;multimedia;world wide web	HCI	-53.473601450054474	-46.866567621583194	178212
4d5954476527b069330608312dc36074840ed276	the effects of virtual agent humor and gaze behavior on human-virtual agent proxemics	natural dialogue management;proxemics;culture;humor;social influence;persuasion	We study whether a virtual agent that delivers humor through verbal behavior can affect an individual's proxemic behavior towards the agent. Participants interacted with a virtual agent through natural language and, in a separate task, performed an embodied interpersonal interaction task in a virtual environment. The study used minimum distance as the dependent measure. Humor generated by the virtual agent through a text chat did not have any significant effects on the proxemic task. This is likely due to the experimental constraint of only allowing participants to interact with a disembodied agent through a textual chat dialogue.	natural language;online chat;virtual reality	Peter Khooshabeh;Sudeep Gandhe;Cade McCall;Jonathan Gratch;Jim Blascovich;David R. Traum	2011		10.1007/978-3-642-23974-8_61	psychology;proxemics;social influence;multimedia;communication;social psychology;culture	HCI	-52.6320877639764	-49.069487003050114	178364
06e5eaea07947bce08b26ff1795318989c46dc8b	the influence of users' personality on the perception of intelligent virtual agents' personality and the trust within a collaborative context	macquarie university institutional repository;researchonline;digital repository;macquarie university	As Intelligent Virtual Agents (IVAs) have been widely used for applications that require human interaction and collaboration, modeling an IVA that can exhibit personalities is becoming increasingly important. A large body of research has studied variant verbal and nonverbal aspects that are used to deduce an IVAu0027s personality; however, research falls short in showing whether humansu0027 personality influences their perception of the IVAu0027s personality. This paper presents an empirical study that investigated whether human users can perceive the intended personality of an IVA through verbal and/or non-verbal communication, on one hand, and the influence of the useru0027s own personality on their perception, on the other hand. Furthermore, we investigated whether the perceived personality had an impact on the humanu0027s level of trust in the IVA teammate. The results showed that similarity in personalities between humans and IVAs tended to significantly influence the humansu0027 correct perception of the IVAu0027s personality and that different perceived personalities influenced the humanu0027s level of trust.	intelligent agent	Nader Hanna;Deborah Richards	2015		10.1007/978-3-319-24804-2_3	psychology;human–computer interaction;knowledge management;world wide web	HCI	-52.711413138144614	-50.68983005787848	179004
cdf8a434060919dd7e9c95ce6248387a2e05b85b	building rapport between human and eca: a pilot study	nonverbal communication;familiarity;paralinguistic;embodied conversational agent;rapport	This study is part of a longer-term project to provide embodied conversational agents (ECAs) with behaviors that enable them to build and maintain rapport with their human partners. We focus on paralinguistic behaviors, and especially nonverbal behaviors, and their role in communicating rapport. Using an ECA that guides its players through a speech-controlled game, we attempt to measure the familiarity built between humans and ECAs across several interactions based on paralinguistic behaviors. In particular, we studied the effect of differences in the amplitude of nonverbal behaviors by an ECA interacting with a human across two conversational sessions. Our results suggest that increasing amplitude of nonverbal paralinguistic behaviors may lead to an increased perception of physical connectedness between humans and ECAs.	dialog system;embodied agent;interaction;speech synthesis;voice stress analysis	David G. Novick;Iván Gris	2014		10.1007/978-3-319-07230-2_45	nonverbal communication;multimedia;paralanguage	HCI	-52.73090813719653	-48.7867729875868	179966
f85f8559657124e3c9651877367be090a361b51a	comparative evaluation of virtual and real humanoid with robot-oriented psychology scale	virtual reality control engineering computing humanoid robots;humanoid robot;legged locomotion psychology humans controllability service robots robot motion;robot design;legged locomotion;psychological evaluation;controllability;virtual reality;service robots;psychology;participant observation;humanoid robots;service robot;robot motion;control engineering computing;humans;objective hardness virtual reality real humanoid robot robot oriented psychology scale vr robot psychological evaluation utility clumsiness of motion possibility of communication controllability vulnerability	The aim of this study was to compare a robot designed using a virtual reality (VR) system (termed VR robot) with a real robot by using a psychological evaluation to investigate whether the VR robot can be used in the same manner as a real robot. To make the direct comparison between a VR and real robots possible, the same designed robots in both VR and real are used in the experiment. For evaluating the robots on a psychological basis, we focused on six basic dimensions (Utility, Clumsiness of motion, Possibility of communication, Controllability, Vulnerability, and Objective hardness) that ordinary people generally use to perceive robots. Sixty-one participants observed and evaluated a real and VR humanoid robots using a psychological scale. Results show that the real robot was evaluated to have higher scores for Utility, Possibility of communication, and Objective hardness and lower ones for Controllability as compared to a VR robot. The Vulnerability scores of the real robot and the VR robot were not significantly different. The usability of a VR robot is discussed in the paper.	computational hardness assumption;humanoid robot;human–robot interaction;usability;virtual reality	Hiroko Kamide;Mika Yasumoto;Yasushi Mae;Tomohito Takubo;Kenichi Ohara;Tatsuo Arai	2011	2011 IEEE International Conference on Robotics and Automation	10.1109/ICRA.2011.5979893	computer vision;simulation;computer science;engineering;humanoid robot;artificial intelligence;social robot;virtual reality;robot control	Robotics	-52.15759937704201	-50.93719788377179	180112
d38acfcdc63a5447c2c506dc1b49d28c906009a8	improving the human-robot interaction through emotive movements a special case: walking	gait analysis;human-robot interaction;humanoid robots;mobile robots;emotional clues;emotive movements;gait patterns;human-robot interaction;humanoid robots;professional actress;social context;walking;emotion;gait analysis;humanoid robot;walking	Walking is one of the most common activities that we perform every day. If the main goal of walking is to go from a point A to a point B, walking can also convey emotional clues in social context. Those clues can be used to improve interactions or any messages we want to express. We observed a professional actress perform emotive walking and analyzed the recorded data. For each emotion, we found characteristic features which can be used to model gait patterns for humanoid robots. The findings were assessed by subjects who were asked to recognize the emotions displayed in the acts of walking.	humanoid robot;human–robot interaction	Matthieu Destephe;Takayaki Maruyama;Massimiliano Zecca;Kenji Hashimoto;Atsuo Takanishi	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)			Robotics	-51.15469362608805	-50.059124149866705	180613
9fb1d498446b64b19b0b79ed06df43f22a62a009	a system for practicing formations in dance performance supported by self-propelled screen	mobile robot;formation dance training;projector	Collapsed formation in a group dance will greatly reduce the quality of the performance even if the dance in the group is synchronized with music. Therefore, learning the formation of a dance in a group is as important as learning its choreography. However, if someone cannot participate in practice, it is difficult for the rest of the members to gain a sense of the proper formation in practice. We propose a practice-support system for performing the formation smoothly using a self-propelled screen even if there is no dance partner. We developed a prototype of the system and investigated whether a sense of presence provided by both methods of practicing formations was close to the sense we really obtain when we dance with humans. The result verified that the sense of dancing with a projected video was closest to the sense of dancing with a dancer, and the trajectory information from dancing with a self-propelled robot was close to the trajectory information from dancing with a dancer. Practicing in situations similar to real ones is able to be done by combining these two methods. Furthermore, we investigated whether the self-propelled screen obtained the advantages of dancing with both methods and found that it only obtained advantages of dancing with projected video.	anomalous experiences;humans;prototype;self-propelled particles;smoothing	Shuhei Tsuchida;Tsutomu Terada;Masahiko Tsukamoto	2013		10.1145/2459236.2459266	simulation;engineering;multimedia;communication	AI	-53.80493634691026	-45.85797575178677	180693
de1bcd808e289927371c68860b00952e30329e05	stereotypes or golden rules? exploring likable voice traits of social robots as active aging companions for tech-savvy baby boomers in taiwan	social robots;baby boomers;human computer interface;voice user interface;voice personality;computers are social actors	Social robots are foreseen to be a growing category of assistive devices for baby boomers in helping them live independently at home and in good health for longer. Artificial intelligence and speech recognition are making natural language user interfaces possible. Yet voices of social robots could influence users' preferences and perceptions in ways that have not been widely explored yet, let alone in Chinese-speaking context. A quantitative study was conducted to probe what voice characteristics of social robots baby boomers may prefer and what personality traits they would associate with various pitch, speech rate, and intonation in Mandarin. While the findings coincide with previous research in that female, extroverted voices were preferred by most participants, many individual differences remain, which could be attributed to differences in demographics, lifestyles, and personality traits. This study contributes to design decisions that would take individual differences into consideration. Social robots that could learn and adapt to people's preferences, perceptions, as well as the tasks at hand may fare better than a one-size-fits-all approach. We believe that voice user interfaces will play a key role in elderly people's initial acceptance and continuing engagement of social robots.	social robot;stereotype (uml)	Rebecca Cherng-Shiow Chang;Hsi-Peng Lu;Peishan Yang	2018	Computers in Human Behavior	10.1016/j.chb.2018.02.025	psychology;social robot;social psychology;perception;natural language user interface;big five personality traits;demographics;user interface	Robotics	-53.33410658436448	-51.10367555656778	180946
ccd22aca681a51bca7fe03dbf996cdc0bcd0e141	communious mouse: a mouse interface to experience emotions in remarks on the web by extending and modulating one's body image	jumping device;click beetle;interaction	In this paper, we propose a mouse interface for giving users others' emotional experiences as own one by extending the body image to the mouse device through active controlling the mouse device of the users and modifying the extended body image that involve with others' emotion.		Sho Sakurai;Yuki Ban;Toki Katsumura;Takuji Narumi;Tomohiro Tanikawa;Michitaka Hirose	2014		10.1145/2669047.2669057	interaction;simulation;computer science;multimedia	HCI	-48.70312127966137	-46.76154445552997	181076
7ab360c3cdae6a6ce0c95d0226ebef1bdf825429	affectcam: arousal- augmented sensecam for richer recall of episodic memories	sra informations och kommunikationsteknik;emotions;memory characteristics;cued recall;sra ict;galvanic skin response;engineering and technology;teknik och teknologier;sensecam;arousal;episodic memories	This paper describes the design and evaluation of AffectCam, a wearable system integrating SenseCam and BodyMedia SenseWear for capturing galvanic skin response as a measure of bodily arousal. AffectCam's algorithms use arousal as a filtering mechanism for selecting the most personally relevant photos captured during people's ordinary daily life, i.e. high arousal photos. We discuss initial findings showing that emotional arousal does improve the quality of memory recall associated with emotionally arousing events. In particular, the high arousal photos support richer recall of episodic memories than low arousal ones, i.e. over 50% improvement. We also consider how various phenomenological characteristics of autobiographical memories such as event, emotions, thoughts, place and time are differently cued by the AffectCam.	algorithm;galvanic isolation;microsoft sensecam;wearable computer	Corina Sas;Tomasz Fratczak;Matthew Rees;Hans-Werner Gellersen;Vaiva Kalnikaité;Alina Coman;Kristina Höök	2013		10.1145/2468356.2468542	emotion;arousal;skin conductance;episodic memory	HCI	-54.42525276426757	-48.75420978930878	181301
32907fa25b4e7df4cb6ccaad5c6af6e7b6c1379a	adapting an immersive virtual reality system for rehabilitation	virtual reality;immersion;neurological rehabilitation;immersive virtual reality;presence	The purpose of this paper is to present an overview of the adaptations that have been done to VividGroup’s Gesture Xtreme projected VR scenarios in order to facilitate their use in neurological rehabilitation. First, the scenarios and the adaptation (control of the type, speed, location and direction of all stimuli) are briefly described. The advantages and limitations of the adapted VR system as a rehabilitation tool are presented. Next, initial results and two examples of case studies, one a patient following stroke and one who requires balance training as a result of complete spinal cord injury, are used to illustrate applications of this VR system to rehabilitation. Copyright # 2003 John Wiley & Sons, Ltd.	immersion (virtual reality);john d. wiley;population;relevance;usability;virtual reality	Rachel Kizony;Noomi Katz;Patrice L. Weiss	2003	Journal of Visualization and Computer Animation	10.1002/vis.323	simulation;human–computer interaction;computer science;artificial intelligence;virtual reality;multimedia;immersion	Visualization	-50.31290010917645	-45.95306894172967	181492
c68624b9256889a8a4c1170643180004ec147140	user engagement study with virtual agents under different cultural contexts		Human communication literature states that people with different culture backgrounds act differently in conversations. Currently most virtual agents are designed for a single targeted popular culture. We implemented two versions of a virtual agent targeting American and Chinese cultures. We found that users from different culture context express engagement differently.	intelligent agent	Zhou Yu;Xinrui He;Alan W. Black;Alexander I. Rudnicky	2016		10.1007/978-3-319-47665-0_34	human–computer interaction;knowledge management;multimedia	HCI	-53.03699390723548	-48.379560308326596	181906
9290886a114d70e983202993b153d8a9fa9c1a9d	synestouch: haptic + audio affective design for wearable devices	multisensory;haptic interfaces vibrations phantoms prototypes actuators distance measurement usability;stimulus;vibrations;phantoms;prototypes;wearable;actuators;haptics;distance measurement;sound;affective interactions synestouch haptic plus audio affective design wearable devices multisensory stimuli sensory modalities wrist worn wearable prototype emotional responses mismatching emotions;vibration;wearable computers audio user interfaces emotion recognition haptic interfaces;multimodal;affective haptics vibration vibrotactile multimodal multisensory sound auditory stimulus wearable;affective;vibrotactile;haptic interfaces;usability;auditory	"""Little is known about the affective expressivity of multisensory stimuli in wearable devices. While the theory of emotion has referenced single stimulus and multisensory experiments, it does not go further to explain the potential effects of sensorial stimuli when utilized in combination. In this paper, we present an analysis of the combinations of two sensory modalities - haptic (more specifically, vibrotactile) stimuli and auditory stimuli. We present the design of a wrist-worn wearable prototype and empirical data from a controlled experiment (N=40) and analyze emotional responses from a dimensional (arousal + valence) perspective. Differences are exposed between “matching” the emotions expressed through each modality, versus """"mixing"""" auditory and haptic stimuli each expressing different emotions. We compare the effects of each condition to determine, for example, if the matching of two negative stimuli emotions will render a higher negative effect than the mixing of two mismatching emotions. The main research question that we study is: When haptic and auditory stimuli are combined, is there an interaction effect between the emotional type and the modality of the stimuli? We present quantitative and qualitative data to support our hypotheses, and complement it with a usability study to investigate the potential uses of the different modes. We conclude by discussing the implications for the design of affective interactions for wearable devices."""	affective computing;affective design;experiment;haptic technology;interaction;matching (graph theory);modality (human–computer interaction);prototype;usability testing;wearable computer;wearable technology	Pablo Paredes;Ryuka Ko;Arezu Aghaseyedjavadi;John C.-I. Chuang;John F. Canny;Linda Babler	2015	2015 International Conference on Affective Computing and Intelligent Interaction (ACII)	10.1109/ACII.2015.7344630	computer vision;computer science;vibration;multimedia;communication	HCI	-49.6077430619868	-48.00722293974947	182258
2d0125295031db2b197e054193de34207710930b	have you ever lied?: the impacts of gaze avoidance on people's perception of a robot	normal situation;human-robot interactions;sociability;gaze;human-robot interaction;intention;human-human interaction;within-participants experiment;people perception;embarrassing situation;gaze avoidance;question type;indexes;human robot interaction;analysis of variance;psychology	In human-human interaction, gaze avoidance is usually interpreted as having intention to escape from an embarrassing situation. This study explores whether gaze avoidance by a robot can be delivered as an intention, and whether this intention can make a robot perceived as having sociability and intelligence. We executed a 2 (question type: normal vs. embarrassing) x 2 (gaze type: gaze vs. gaze avoidance) within-participants experiment (N=24). In an embarrassing situation, a robot with gaze avoidance is perceived as more sociable and intelligent than a robot that holds its gaze, while the robot that holds its gaze in a normal situation is perceived as more sociable and intelligent than a robot with gaze avoidance. Implications for the design of human-robot interactions are discussed.	human–robot interaction;robot	Jung-Ju Choi;Yunkyung Kim;Sonya S. Kwak	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		human–robot interaction;computer vision;computer science;artificial intelligence	Robotics	-51.74358653405559	-50.568994447228256	182311
b57fd107a1c1cbe2e52ba6634e86e4bdf6ff6060	an educational talking toy based on an enhanced chatbot	educational talk;autonomous toy;children;educational patterns;memory;chatbot	Children are often motivated in their communication behaviour by pets or toys. Our aim is to investigate,#R##N##R##N#how communication with âintelligentâ systems affects interaction of children. Enhanced chatbot technology#R##N##R##N#â hidden in toys - is used to talk to children. In the Hablame-project (started as part of the EU-funded#R##N##R##N#Gaviota project) a first prototype talking German is available. We outline the technical solution, and discuss#R##N##R##N#further steps.		Eberhard Grötsch;Alfredo Pina;Roman Balandin;Andre Barthel;Maximilian Hegwein	2015		10.5220/0005452403600363	simulation;multimedia;memory;communication	EDA	-55.33155726857042	-46.620365844454376	182755
a560818d824bff9bc2f422ac46f32e32488f26c4	emotional expression online: the impact of task, relationship and personality perception on emoticon usage in instant messenger	computer mediated communication;emoticons;person perception;instant messenger;emotional expression	Emoticons have been widely used in Computer mediated communication (CMC) such as Instant Messenger (IM). This study investigates the effect of communication task, interpersonal perception, and relationship intimacy on the use of emoticons in IM. The results show that task formality and two aspects of interpersonal perception affect the emoticon usage. Detailed discussions and implications are addressed.	computer-mediated communication;emoticon;instant messaging	Lingling Xu;Cheng Yi;Yunjie Calvin Xu	2007			emotional expression;multimedia;social psychology;computer-mediated communication;social perception	HCI	-54.68176670346184	-49.10715774703677	182811
54328a13bbe6dd59e0eab4252c2ca4a061385b89	a speech driven embodied interaction robots system for human communication support	human interaction;speech human robot interaction biological system modeling robot sensing systems arm motion estimation prototypes computer graphics computer science systems engineering and theory;human robot interaction;speech based user interfaces;sensory evaluation;moving average;human factors;remote communication speech driven embodied interaction robots human communication support human interaction support expressive actions speech input interrobots speaker listener action model nodding blinking head motions hierarchical ma model moving average model burst pause speech sensory evaluation behavioral analysis human robot interaction;robots;moving average processes;moving average processes robots interactive systems speech based user interfaces human factors;interactive systems;behavior analysis;embodied interaction	The concept of a speech driven embodied interaction robots system is introduced for supporting human interaction by generating the expressive actions or motions of robots coherently related to speech input. The prototype of the system which consists of 2 interaction robots called InterRobots with both functions of speaker and listener is developed in which two models are incorporated: one is a listener's action model where nodding, blinking and the motions of head, arms and body are estimated by the hierarchical MA (moving-average) model of the burst-pause (ON-OFF) of speech to the nodding; the other is a speaker's action model in which the motions of head, arms and body are estimated by its own MA model of the ON-OFF of speech to the head motion. By sensory evaluation and behavioral analysis in human-robot interaction, the effectiveness of InterRobot's interaction is demonstrated. The effectiveness of the embodied interaction robot system is also confirmed in remote communication.	robot	Tomio Watanabe;Masashi Okubo;Hiroki Ogawa	2000		10.1109/ICSMC.2000.885956	human–robot interaction;robot;sensory analysis;interpersonal relationship;simulation;speech recognition;computer science;artificial intelligence;human factors and ergonomics;moving average	Robotics	-50.12278886309322	-50.08795271883883	183059
6b65fbaff0227fb3981c76fb86400e183240fa92	towards modelling exploratory learning in the context of direct manipulation interfaces	human interaction;human computer interaction;direct manipulation;production system;context of use;user behaviour;problem solving;production rule	The characteristics of direct manipulation interfaces (DMIs) are examined. The main purpose of this examination is to provide ideas for future research on modelling exploratory learning in the context of using DMIs. Four topics are discussed: the perceptual characteristics of DMIs, exploratory learning and display-based problem-solving in general, modelling human-computer interaction in the context of DMIs, and the consequences of DMIs for modelling the interaction by means of a production system. Specifically, the questions that are discussed are: first, how do DMIs afford, encourage and support exploratory learning, and how can typical DMI characteristics such as the objects on the screen be included in models of user behaviour? Second, what are the characteristics of problem-solving and exploratory learning in the context of visual displays? Third, how is novice behaviour and, more generally, problem-solving modelled in the context of human-computer interaction? In the final section, suggestions are made based on the topics discussed, with the aim of presenting some steps towards developing a model consisting of production rules that can simulate human interaction with DMIs more adequately than has been the case thus far. Two important consequences of DMIs for modelling human interaction are discussed. First, the external display of DMIs allows recognition instead of recaI1. Consequently, production ruIes can be more recognition-based. Second, with regard to the structure of production systems, the mechanism of partial matching is proposed to account for errors during performance. Constraints and affordances can be accounted for by proposing production rules to fire contextdependently, and by assuming that production rules can be meaningfully grouped and actively scanned for a match.	direct manipulation interface;exploratory testing;human–computer interaction;problem solving;production (computer science);production system (computer science);simulation	Herre van Oostendorp;Benjamin J. Walbeehm	1995	Interacting with Computers	10.1016/0953-5438(95)90817-5	interpersonal relationship;simulation;human–computer interaction;computer science;artificial intelligence;production system	HCI	-54.69295456764295	-45.07476461125135	183636
eb02492c690e7f9f2f10b830a4d749ff849347b8	evaluation of an intelligent assistive technology for voice navigation of spreadsheets	human computer interaction;repetitive strain injury;voice recognition;assistive technology;navigation system;computer science	An integral part of spreadsheet auditing is navigation. For sufferers of Repetitive Strain Injury who need to use voice recognition technology this navigation can be highly problematic. To counter this the authors have developed an intelligent voice navigation system, iVoice, which replicates common spreadsheet auditing behaviours through simple voice commands. This paper outlines the iVoice system and summarizes the results of a study to evaluate iVoice when compared to a leading voice recognition technology.	assistive technology;dragon naturallyspeaking;floor and ceiling functions;repetitive strain;speech recognition;spreadsheet;usability	Derek Flood;Kevin McDaid;Fergal McCaffery;Brian Bishop	2008	CoRR		simulation;speech recognition;human–computer interaction;computer science	Robotics	-48.31953966017393	-45.886055470972515	184621
4bb42dd72e3c3428a62c698398e0514485771fa3	opinions and attitudes toward humanoid robots in the middle east		Robotics is expected to boom in the near future, moving massively beyond traditional application areas, and extending to all parts of the globe. Thus, in order to enable effective international customization of robot designs, and in order to facilitate their smoother harmonious introduction to everyday life, it is important to study the opinions and attitudes toward robots in different regions of the world. Although there exists a small body of research covering the US, EU, and Asia, there is almost no research regarding attitudes toward robots in the Middle East, a region with its own marked cultural idiosyncrasies. Therefore, we brought Ibn Sina, an Arabic-language conversational android robot to Dubai’s Gitex, one of the most important exhibitions in the region, and performed a questionnaire-based empirical study with 355 subjects from 38 countries, which had seen the robot interacting, and most of which had also interacted directly with it. Many interesting findings are presented: First, a statistically significant ordering of preferred application areas for robots overall was found, as well as strong effects of the region of origin on the preferred applications. Furthermore, strong religion, age, and education effects were observed. Overall, the results together with a theoretical discussion of possible causes provide interesting insights on cultural acceptance of robots in this richly complex region, which potentially have strong implications to their wider deployment in the future in specific settings.	android (robot);gulf of evaluation;han unification;humanoid robot;humans;interaction;mechatronics;plasma cleaning;robotics;software deployment	Nikolaos Mavridis;Marina-Selini Katsaiti;Silvia Naef;Abdullah Falasi;Abdulrahman Nuaimi;Hamdan Araifi;Ahmed Kitbi	2011	AI & SOCIETY	10.1007/s00146-011-0370-2	simulation;artificial intelligence	Robotics	-54.07886690504835	-50.83932854945167	185408
a259300491cf21386a555ce2fbf9088ec43a76b5	can humanoid service robots perform better than service employees? a comparison of innovative behavior cues		This research compares human-robot interaction with human-human interaction. More specifically, it compares potential customer responses to a humanoid service robot’s (HSR’s) behavioral cues during service encounters with those expressed by a human service employee. The behavioral cues tested in this study include innovative service behavior, defined as the extent to which a service representative creates new ideas and solutions for the customer. Based on role theory and the expectancy disconfirmation paradigm, we propose that customers generally respond positively toward an HSR’s artificial innovative service behavior cues. The experimental laboratory study with 132 student participants and an HSR of the Pepper type, shows positive responses to an HSR’s artificial innovative service behavior, but that those responses are weaker compared to human-human interactions within a similar setting. Furthermore, innovative service behavior cues exceed customer expectations and therefore, lead to customer satisfaction and delight with the HSR.	hierarchical state routing;human–robot interaction;programming paradigm;service robot	Ruth Maria Stock;Moritz Merkle	2018			knowledge management;management science;computer science;robot;role theory	HCI	-53.53076051742956	-50.69274025126226	185925
afee97fd733504a6759a00b34faffb2cf4238f05	are you looking at me? perception of robot attention is mediated by gaze type and group size	human-robot interaction;hri;mykeepon toys;gaze behaviors;gaze type;group size;programmable research platform;programmable robots;robot attention perception;robot gaze;gaze;group dynamics;human-robot interaction;social robotics	Studies in HRI have shown that people follow and understand robot gaze. However, only a few studies to date have examined the time-course of a meaningful robot gaze, and none have directly investigated what type of gaze is best for eliciting the perception of attention. This paper investigates two types of gaze behaviors---short, frequent glances and long, less frequent stares---to find which behavior is better at conveying a robot's visual attention. We describe the development of a programmable research platform from MyKeepon toys, and the use of these programmable robots to examine the effects of gaze type and group size on the perception of attention. In our experiment, participants viewed a group of MyKeepon robots executing random motions, occasionally fixating on various points in the room or directly on the participant. We varied type of gaze fixations within participants and group size between participants. Results show that people are more accurate at recognizing shorter, more frequent fixations than longer, less frequent ones, and that their performance improves as group size decreases. From these results, we conclude that multiple short gazes are preferable for indicating attention over one long gaze, and that the visual search for robot attention is susceptible to group size effects.	human–robot interaction;robot;toys	Henny Admoni;Bradley Hayes;David Feil-Seifer;Daniel Ullman;Brian Scassellati	2013	2013 8th ACM/IEEE International Conference on Human-Robot Interaction (HRI)			HCI	-50.41041582738372	-51.29016568416409	186096
67dbf8f8a8014068482484a4c988b278093fe9d9	assessing product design using photos and real products		Many studies examining user responses to the product design have been implemented using product photos instead of real products due to practical limitations. In this study, we investigate the validity of such an approach for a particular case with car design evaluation. We compare users' perceptual responses for photos and real cars. In particular, we employ both explicit and implicit response channels, i.e., subjective rating, electroencephalography (EEG), and visual attention. The results show that, although largely similar perceptual responses are obtained in the two cases, significant differences are also observed, which need to be considered in evaluation of product design and user experience.	electroencephalography;user experience	Seong-Eun Moon;Jun-Hyuk Kim;Sun-Wook Kim;Jong-Seok Lee	2017		10.1145/3027063.3053336	multimedia;simulation;eye tracking;user experience design;perception;product design;computer science	HCI	-48.746587196223366	-47.33942790914709	186101
95e4867b8a65a0fe0e136d29ba6397236a314751	personality perception of robot avatar tele-operators		Nowadays a significant part of human-human interaction takes place over distance. Tele-operated robot avatars, in which an operator's behaviours are portrayed by a robot proxy, have the potential to improve distance interaction, e.g., improving social presence and trust. However, having communication mediated by a robot changes the perception of the operator's appearance and behaviour, which have been shown to be used alongside vocal cues in judging personality. In this paper we present a study that investigates how robot mediation affects the way the personality of the operator is perceived. More specifically, we aim to investigate if judges of personality can be consistent in assessing personality traits, can agree with one another, can agree with operators' self-assessed personality, and shift their perceptions to incorporate characteristics associated with the robot's appearance. Our experiments show that (i) judges utilise robot appearance cues along with operator vocal cues to make their judgements, (ii) operators' arm gestures reproduced on the robot aid personality judgements, and (iii) how personality cues are perceived and evaluated through speech, gesture and robot appearance is highly operator-dependent. We discuss the implications of these results for both tele-operated and autonomous robots that aim to portray personality.	autonomous robot;avatar (computing);expectation propagation;experiment;human–robot interaction;motion capture;nao (robot);operating system;social presence theory;television	Paul Bremner;Oya Çeliktutan;Hatice Gunes	2016	2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)		robot learning;computer vision;simulation;interview;computer science;humanoid robot;artificial intelligence;social robot;mediation;personality;personal robot;robot kinematics	Robotics	-51.663082638800795	-50.52634148053759	186867
04cf30e04025d173da7f7f251dd4a92c6db67989	icoobook: when the picture book for children encounters aesthetics of interaction		In this work, we propose a novel PCA (Perception & Cognition & Affection) model from the prospective of aesthetics in interaction. Based on PCA, we establish a new electronic interactive picture book for children, named IcooBook. At the first level of perception, the proposed IcooBook provides interfaces of multi-sensory interaction; at the second level of cognition, IcooBook builds immersive interactive scenes; at the third level of affection, IcooBook creates high-level interaction modes based on automatic emotion recognition. The research on user study had proved the effectiveness of IcooBook in helping children being focusing on reading, getting better understanding about the context, and further encouraging children to appreciate the beauty of deep affective interaction.	cognition;emotion recognition;high- and low-level;prospective search;usability testing	Yaohua Bu;Jia Jia;Xiang Li;Suping Zhou;Xiaobo Lu	2018		10.1145/3240508.3241397	beauty;emotion recognition;perception;affect (psychology);affection;aesthetics;computer science;cognition	HCI	-53.166955588687266	-46.11333087860114	186911
4bd749c65588b7d2e2d17316b385019aafaf490d	fake empathy and human-robot interaction (hri): a preliminary study		Empathy﻿is﻿a﻿basic﻿emotion﻿trigger﻿for﻿human﻿beings,﻿especially﻿while﻿regulating﻿social﻿relationships﻿ and﻿behaviour.﻿The﻿main﻿challenge﻿of﻿this﻿paper﻿is﻿study﻿whether﻿people’s﻿empathic﻿reactions﻿towards﻿ robots﻿change﻿depending﻿on﻿previous﻿information﻿given﻿to﻿human﻿about﻿the﻿robot﻿before﻿the﻿interaction.﻿ The﻿use﻿of﻿false﻿data﻿about﻿robot﻿skills﻿creates﻿different﻿levels﻿of﻿what﻿we﻿call﻿‘fake﻿empathy’.﻿This﻿ study﻿performs﻿an﻿experiment﻿in﻿WOZ﻿environment﻿in﻿which﻿different﻿subjects﻿(n=17)﻿interacting﻿ with﻿the﻿same﻿robot﻿while﻿they﻿believe﻿that﻿the﻿robot﻿is﻿a﻿different﻿robot,﻿up﻿to﻿three﻿versions.﻿Each﻿ robot﻿ scenario﻿ provides﻿ a﻿ different﻿ ‘humanoid’﻿ description,﻿ and﻿ out﻿ hypothesis﻿ is﻿ that﻿ the﻿ more﻿ human-like﻿looks﻿the﻿robot,﻿the﻿more﻿empathically﻿can﻿be﻿the﻿human﻿responses.﻿Results﻿were﻿obtained﻿ from﻿questionnaires﻿and﻿multi-﻿angle﻿video﻿recordings.﻿Positive﻿results﻿reinforce﻿the﻿strength﻿of﻿our﻿ hypothesis,﻿although﻿we﻿recommend﻿a﻿new﻿and﻿bigger﻿and﻿then﻿more﻿robust﻿experiment. KEywoRdS Emotions, Empathy, Fake, HRI, Human-Robot Interaction, Robots, WOZ		Jordi Vallverdú;Toyoaki Nishida;Yoshimasa Ohmoto;Stuart Moran;Sarah Lázare	2018	IJTHI	10.4018/IJTHI.2018010103	knowledge management;empathy;human–computer interaction;computer science;social psychology;human–robot interaction	Robotics	-53.08539406975157	-49.45708859769909	187199
22c064856861e574c2492d5249c414c45a3c77de	toward collaboration sensing: multimodal detection of the chameleon effect in collaborative learning settings		In this paper, I describe part of my doctoral dissertation in which I have attempted to automatically detect a phenomenon known as the chameleon effect in collaborative learning settings. The chameleon effect refers to non-conscious mimicry of other comportments (e.g. postures, mannerisms, facial expressions), such that one's behavior passively and unintentionally changes to match a partner’s behaviors. As described below, social mimicry is associated with more productive collaborations and potentially higher learning gains in classroom settings. I describe several studies where I was able to show that visual synchronization (i.e. joint attention), and verbal synchronization (i.e. discourse coherence) were associated with higher learning gains and better collaboration in groups of students, while body synchronization and grammatical mimicry did not predict any of those outcomes. I conclude by discussing implications for educational data mining and describe future work using additional measures such as voice synchronization (e.g. variations in pitch and volume) and arousal synchronization (i.e. variations in heart beat rhythms).	educational data mining;mcgurk effect;multimodal interaction	Bertrand Schneider	2014			simulation	HCI	-53.796763748067995	-49.46824771724245	188110
d05cc77b4d1c08c996dd1c49d7082f938ef07e97	collaborating with an autonomous agent to generate affective music	musical metacreation;algorithmic composition;music and emotions;interactive installations	Multidisciplinary research recently has been investigating solutions to offer new experiences of music making to musically untrained users. Our approach proposes to distribute the process of music making between the user and an autonomous agent by encoding this collaboration in the emotional domain. In this framework, users communicate the emotions they wish to express to Robin, the autonomous agent, which interprets this information to generate music with matching affective flavor. Robin is taught a series of basic compositional rules of tonal music, which are used to create original compositions in Western classical-like music. Associations between alterations to musical factors and changes in the communicated emotions are operationalized on the basis of recent outcomes that have emerged from research in the field of psychology of music. At each new bar, a number of stochastic processes determine the values of seven musical factors, whose combinations best match the intended emotion. The ability of Robin to validly communicate emotions was tested in an experimental study (N = 33). Results indicated that listeners correctly identified the intended emotions. Robin was employed for the purposes of two interactive artworks, which are also discussed in the article, showing the potential of the algorithm to be employed in interactive installations.	algorithm;autonomous agent;autonomous robot;experience;experiment;stochastic process	Fabio Morreale;Antonella De Angeli	2016	Computers in Entertainment	10.1145/2967508	music psychology;artificial intelligence;multimedia;music and emotion;musicality	AI	-50.75437955542578	-47.00266484963572	188242
76e3dab28a81b2e52276fbd062144d1fefe1ad0f	measuring player experience on runtime dynamic difficulty scaling in an rts game	computers;runtime dynamic difficulty scaling;runtime vehicle dynamics artificial intelligence military computing force control vehicles buildings humans springs state estimation;realtime strategy game;data mining;static computer opponents;lead;games;social networking online;datavetenskap datalogi;social networking online computer games;datavetenskap;artificial intelligence;humans;realtime strategy game player experience measurement runtime dynamic difficulty scaling static computer opponents;computer science;computer games;player experience measurement;real time systems	Do players find it more enjoyable to win, than to play even matches? We have made a study of what a number of players expressed after playing against computer opponents of different kinds in an RTS game. There were two static computer opponents, one that was easily beaten, and one that was hard to beat, and three dynamic ones that adapted their strength to that of the player. One of these three latter ones intentionally drops its performance in the end of the game to make it easy for the player to win. Our results indicate that the players found it more enjoyable to play an even game against an opponent that adapts to the performance of the player, than playing against an opponent with static difficulty. The results also show that when the computer player that dropped its performance to let the player win was the least enjoyable opponent of them all.	image scaling	Johan Hagelbäck;Stefan J. Johansson	2009	2009 IEEE Symposium on Computational Intelligence and Games	10.1109/CIG.2009.5286494	bayesian game;games;lead;simulation;computer science;artificial intelligence;strategic dominance;information set;strategy;multimedia;fictitious play	Arch	-49.769485209337944	-51.75026201268723	188526
b08c1a64f3c243a0a309d972708e30ea18a1fe49	gestural attributions as semantics in user interface sound design	human cognition;theoretical framework;user interface;semantics;sound design;gestures;literature review;action understanding;user interfaces	Sound-based communication within different kinds of media have a long tradition. Sound design of radio-plays as early as 1920s have defined the basis for communicative use of sound effects which is still relevant in the tradition of film sound design. One of the common demands for sound designers have been the creation of sound effects with various emotional or intentional qualities. The suitable door knock for narration, for example, can be described as urging, gentle or angry in the manuscript [1]. This kind of agency perspective for sound design is widely adopted in the tradition of film sound design exploiting the natural perceptual bias towards looking for cues of agency in actions. But when it comes to the sound design for human-computer interaction (HCI), this kind of expressiveness is seldom used. In this paper we argue that the design of non-speech audio feedback for the HCI context would vastly benefit from such agency approach, which aims at utilising the gestures of interpersonal interaction in the sonic interaction design. In the social interaction, we naturally express our mental states with bodily involvement which can be called either facial, vocal, hand or other kind of gestural action. As a communicative acts, gestures usually posses derived intentionality which is caused by the mental states of an acting person [2]. The involvement of gestural imagery in music perception[3], for example, suggests that similar motor-mimetic gestural attuning to the sound would be central element also in both the perception and the design of UI sounds. Therefore we suggest that gestural attributions of derived intentionality with sounds would be considered as a form of semantics and thus as means to design communicative sounds for interaction.	audio feedback;gentle;human–computer interaction;intentionality;mental state;sonic interaction design;sound blaster;user interface design	Kai Tuuri	2009		10.1007/978-3-642-12553-9_23	psychology;multimedia;communication;social psychology	HCI	-52.788459465248124	-45.339051164839525	188581
ab355793bd3e87407b8344019fc22f14a95dfedc	evaluating an intuitive teleoperation platform explored in a long-distance interview	user study;head motion;humanoid robot proxy;social robotics;previous experience with robots;teleoperation	SWoOZ is an intuitive teleoperation platform using a humanoid robot as a proxy between two humans: a remote user teleoperating the robot and a local user interacting directly with it. NAO (Aldebaran) is the proxy used in this study. The remote user controls its head motion with his own movements (live) while his real voice is transmitted to the local user with an unnoticeable lag time. This paper presents a user study of the platform in the context of a long-distance survey and investigates the possible effect of the remote user's previous experience with robots on the local users' evaluation of the proxy. Although found useful, likable and satisfying by all the local users, only the ones interviewed by the non-naive user find it averagely credible. Results fail to validate an effect of the remote users' previous experience with robots on the local users.		Ritta Baddoura;Gentiane Venture;Guillaume Gibert	2014		10.1145/2658861.2658930	simulation;human–computer interaction;engineering;multimedia	HCI	-50.6341962230316	-49.536596822914525	188603
6ec63cf79378f479da63ee1cd85f098447da1710	migration between two embodiments of an artificial pet	artificial pets;robotics;human robot interaction;robotteknik och automation;agent migration	Characters that cross dimensions have elicited an avid interest in literature and cinema. In analogy to these characters, we explore the concept of migration: process by which an agent moves between embodiments, being active in only one at a time. We developed an autonomous artificial pet with two embodiments: a virtual within a smartphone and a physical robotic embodiment. Considering that owners’ interactions with real pets lead to emotional attachment and potentially related health benefits, we conducted a user study with elementary school students to assess their attachment to the prototype and how natural they felt the interaction was. By the end of the experiment children felt closer to the artificial pet and 43.3% considered the two embodiments to correspond to the same entity, although migration was never explained to them. As a result, this paper presents a novel generic methodology that allows the evaluation of other implemented prototypes that support migration. Furthermore, we created a set of design guidelines for migrating agents. ∗pfontain@ucsc.edu †jose.alberto.sardinha@tecnico.ulisboa.pt ‡elena@sics.se §henriette@mobilelifecentre.org ¶ana.paiva@inesc-id.pt 1Preprint of an article published in International Journal of Humanoid Robotics (IJHR), Volume 11, Issue 1, 2014, DOI 10.1142/S0219843614500017, c ©World Scientific Publishing Company, Journal URL http://www.worldscientific.com/worldscinet/ijhr	android;attachments;autonomous robot;cinema 4d;digital pet;entity;interaction;mobile phone;obedience (human behavior);outline of object recognition;prototype;responsiveness;robotic pet;robotics;sensor;smartphone;soul;stepping level;system migration;usability testing;user experience;virtual world	Paulo Fontaínha Gomes;Alberto Sardinha;Elena Márquez Segura;Henriette Cramer;Ana Paiva	2014	I. J. Humanoid Robotics	10.1142/S0219843614500017	human–robot interaction;simulation;computer science;artificial intelligence;robotics	HCI	-55.4918168312209	-45.92942255934007	188617
bda3290863e351036043fa3d36bf742a7bc265b0	how robots persuasion based on personality traits may affect human decisions	human robot interaction;social robotics;persuasion;personality traits;smart interactions	The use of autonomous agents, such as robots, to perform tasks such as cleaning the floor or washing the car are more frequent in our daily life. Moreover, with the advance of technology the society is in front of a new trend, your devices are connected to each other, for example, the robot that cleans the floor is connected to the cellphone. This trend was named as Smart Interaction. Those connections may be used to persuade the person to do some task in his/her own benefits, e.g., the refrigerator could recommend healthy foods if the bathroom scale informs that the person is overweight. However, the communications and the persuasions made by these autonomous agents should not be performed in an invasive way. In this sense, our motivation is to create a model were the autonomous agents may perform persuasion based on the person's personality traits.	autonomous agent;autonomous robot;institute for operations research and the management sciences;mobile phone;sputter cleaning	Raul Benites Paradeda;Maria José Ferreira;João Dias;Ana Paiva	2017		10.1145/3029798.3038348	human–robot interaction;simulation;computer science;artificial intelligence;social robot;big five personality traits	AI	-53.35132000441501	-50.675718422526	188658
3ec4f36dcc23dd58901074950d3fdb6f69277cac	strategy for the development of a walk-in-place interface for virtual reality		Many features of a Virtual Reality system can influence the immer- sion and the sense of presence. Navigation is one of those features, since pro- prioceptive and vestibular cues can have a positive impact on immersion and sense of presence. This is especially important for studies about human beha- vior, where behavioral responses should be as close as in the real world. Differ- ent types of interfaces are been developed to be more natural and closer to mov- ing in a real environment. A Walk-In-Place (WIP) interface can be used in small rooms and gives some proprioceptive and vestibular cues. A participant walks in the same place and a device captures that movement and translates it to movement inside the Virtual Environment. This paper presents a strategy for implementing a WIP interface using only one inertial orientation sensor, placed above the knee, mainly about the calibration and real-time detection phases and the approach taken on direction changing.	virtual reality	Luís Teixeira;Elisângela Vilar;Emília Duarte;Paulo Noriega;Francisco Rebelo;Fernando Moreira da Silva	2013		10.1007/978-3-642-39238-2_46	computer vision;simulation;engineering;communication	Visualization	-49.544039145219216	-48.907167882274564	188805
1b781faee797beff41ef67703dd80bd6da3c8b23	a survey of affect recognition methods: audio, visual, and spontaneous expressions	multimodal user interfaces;human computing;emotion recognition;multimodal human computer interaction;affect recognition;affective computing	Automated analysis of human affective behavior has attracted increasing attention from researchers in psychology, computer science, linguistics, neuroscience, and related disciplines. Promising approaches have been reported, including automatic methods for facial and vocal affect recognition. However, the existing methods typically handle only deliberately displayed and exaggerated expressions of prototypical emotions-despite the fact that deliberate behavior differs in visual and audio expressions from spontaneously occurring behavior. Recently efforts to develop algorithms that can process naturally occurring human affective behavior have emerged. This paper surveys these efforts. We first discuss human emotion perception from a psychological perspective. Next, we examine the available approaches to solving the problem of machine understanding of human affective behavior occurring in real-world settings. We finally outline some scientific and engineering challenges for advancing human affect sensing technology.	affective disorders, psychotic;artificial intelligence;audio media;emotion markup language;emotions;linguistics;neuroscience discipline;spontaneous order;algorithm;computer science	Zhihong Zeng;Maja Pantic;Glenn I. Roisman;Thomas S. Huang	2007	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1145/1322192.1322216	computer vision;human–computer interaction;computer science;artificial intelligence;machine learning;affective computing;multimedia	AI	-52.584650591823376	-46.395416718422226	188819
7c187a42cdc6a21206679dc4e9cde54d1ba4e28b	exploring home robot capabilities by medium fidelity prototyping		In order for autonomous robots to be able to support people’s wellbeing in homes and everyday environments, new interactive capabilities will be required, as exemplified by the soft design used for Disney’s recent robot character Baymax in popular fiction. Home robots will be required to be easy to interact with and intelligent–adaptive, fun, unobtrusive and involving little effort to power and maintain–and capable of carrying out useful tasks both on an everyday level and during emergencies. The current article adopts an exploratory medium fidelity prototyping approach for testing some new robotic capabilities in regard to recognizing people’s activities and intentions and behaving in a way which is transparent to people. Results are discussed with the aim of informing next designs.	autonomous robot;domestic robot;prototype	Martin Cooney;Sepideh Pashami;Yuantao Fan;Anita Sant'Anna;Yinrong Ma;Tianyi Zhang;Yuwei Zhao;Wolfgang Hotze;Jeremy Heyne;Cristofer Englund;Achim J. Lilienthal;Tom Ziemke	2017	CoRR		simulation;engineering;fidelity;robot	Robotics	-50.49668512163359	-47.71440924822912	189138
4a332b37a16b5668c7345776bc485880756a2a15	computer-mediated communication: can we make it better?	computer mediated communication	Computer-mediated communication (CMC) is fundamentally different to face-to-face (FTF) communication. The low bandwidth of CMC filters out important social and contextual cues, which results in behaviour in users that is different to FTF communication. There are a number of social psychological theories that attempt to explain why reduced social cues in CMC affect behaviour. This work attempts to apply this knowledge to the design of the user interface (UI) of CMC applications. Key aspects of communication are identified and electronically represented in the UI of a prototype CMC application. Two evaluation studies are then carried out, using the Tropical Island Dilemma and the Map Task, to determine if the identified features have an effect on one-to-one distributed communication. Finally, a set of guidelines is presented for the design of CMC applications, which are derived from the findings of this work.	computer-mediated communication;one-to-one (data model);prisoner's dilemma;prototype;social engineering (security);social presence theory;theory;user interface	Sanjeet Bhachu;D. Ramanee Peiris;Annalu Waller	2004			dilemma;social cue;human–computer interaction;user interface;computer-mediated communication;computer science	HCI	-54.022902318346944	-48.912693310095456	189449
1736017452010ad003650e423eb3ee1c51ea5667	towards the synthetic self: making others perceive me as an other			synthetic intelligence	Stéphane Lallée;Vasiliki Vouloutsi;Maria Blancas;Klaudia Grechuta;Jordi-Ysard Puigbo;Marina Sardà Gou;Paul F. M. J. Verschure	2015	Paladyn		computer science;human–computer interaction;simulation;social robot;human–robot interaction;cognitive architecture	HCI	-52.8117597034281	-48.87713249288509	189486
4757fda9083c69c2352aab8864504843f15ad014	safety-critical personality aspects in human-machine teams of aviation	luft und raumfahrtpsychologie	Working safely and successfully in highly automated human-machine interfaces of future aviation is not only a matter of performance, but also of personality. This study examines which personality aspects correlate with safety-critical performance in human-machine teams. The research tools HTQ (Hybrid Team Questionnaire) and HINT (Hybrid Interaction Scenario) were combined for a comprehensive exploratory study. The HTQ includes personality scales measuring broad factors of personality (Big Five) as well as more specific scales and was added with objective personality assessments to measure risk taking. The simulation tool HINT simulates relevant processes in future human-machine team interaction in aviation. In a study with 156 applicants for aviation careers, safetycritical relations of some facets of general personality as well as risk taking were found. Especially personality aspects concerning disinhibiting, spontaneous behaviour and sensation seeking show correlations with poorer performance in the HINT simulation.		Solveig C. S. Eschen;Doris Keye-Ehing;Katja Gayraud	2016	i-com	10.1515/icom-2016-0032	psychology;simulation;social psychology	HCI	-53.987611357269216	-50.72412879965643	189548
153bbe2d956228edf1c91004debca4f058df05b1	measuring immersion and affect in a brain-computer interface game	brain computer interface;positive affect;immersion;user experience;games;medical application;brain computer interfaces;computer game;affective computing	As the scope of the brain-computer interface (BCI) applications is extending from medical to recreational use, the expectations of BCIs are also changing. Although the performance of BCIs is still important, finding suitable BCI modalities and investigating their influence on user experience demand more and more attention. As BCIs have widely been used in medical applications, to facilitate making selections, it would be imaginable to apply similar BCIs to recreational applications. However, whether they are suitable for recreational applications is unclear as they have rarely been evaluated for user experience. In this study two BCI selection methods and a comparable non-BCI selection method were integrated into a computer game to evaluate user experience in terms of immersion and affect. The BCI selection methods were based on the P300 and steady-state visually evoked potential (SSVEP) paradigms. An experiment with fourteen participants showed that the SSVEP selection method was capable of enriching the user experience in terms of immersion and affect. Participants were significantly more immersed and the SSVEP selection method was found more positively affective. Although it was expected that P300 would enrich user experience, it did not.	ccir system a;computer multitasking;epoc (operating system);emotiv systems;futures studies;immersion (virtual reality);input device;pc game;user experience;virtual world	Gido Hakvoort;Hayrettin Gürkök;Danny Plass-Oude Bos;Michel Obbink;Mannes Poel	2011		10.1007/978-3-642-23774-4_12	brain–computer interface;user experience design;simulation;human–computer interaction;computer science;affective computing;multimedia	HCI	-49.46941212746419	-46.28961890953011	189868
a83a1dabb5be4be02e8addb0fce65a8b7f44c4ca	a method for autonomous positioning avatars in a group		In this paper, we describe a method to position a group of avatars in a virtual environment. The method aims at a group setting that seems natural for a group of people attending a guided tour and was developed in particular to assist participants by autonomously positioning their avatars on each stop of a virtual tour. The geometry of the virtual environment is key input, but also engagement of participants and possible social networks are taken into account. Consequently, it may serve to position avatars in similar type of situations.	avatar (computing);global positioning system;social network;virtual reality;virtual tour	Fons Kuijk	2017	2017 International Conference on Cyberworlds (CW)	10.1109/CW.2017.40	multimedia;virtual tour;software;computer science;virtual machine;social group;social network	Visualization	-51.587328919908764	-48.76252696909084	191062
2ca992afd858bcc5a12e3fe058c372a0c92e8ca7	on natural language dialogue with assistive robots	decision support;natural language dialogue;human robot interaction;assistive technology;assistive robotics	This paper examines the appropriateness of natural language dialogue (NLD) with assistive robots. Assistive robots are defined in terms of an existing human-robot interaction taxonomy. A decision support procedure is outlined for assistive technology researchers and practitioners to evaluate the appropriateness of NLD in assistive robots. Several conjectures are made on when NLD may be appropriate as a human-robot interaction mode.	decision support system;human–robot interaction;natural language;norm (social);robot	Vladimir A. Kulyukin	2006		10.1145/1121241.1121270	human–robot interaction;decision support system;human–computer interaction;computer science;artificial intelligence	Robotics	-53.87902074783423	-50.26118994561663	191643
c35aed04279ad07ef28d731eb25b8e45d35b5816	understanding social robots: a user study on anthropomorphism	social aspects of automation game theory man machine systems robots;game theory;classical prisoners dilemma game;user study;human partner;interaction study;social aspects of automation;computer;human subjects;anthropomorphic robot;social robots;computer social robots anthropomorphism interaction study classical prisoners dilemma game human partner anthropomorphic robot functional robot;robots;functional robot;anthropomorphism;man machine systems;social robot	Anthropomorphism is one of the keys to understand the expectations people have about social robots. In this paper we address the question of how a robotpsilas actions are perceived and represented in a human subject interacting with the robot and how this perception is influenced only by the appearance of the robot. We present results of an interaction-study in which participants had to play a version of the classical Prisonerspsila Dilemma Game (PDG) against four opponents: a human partner (HP), an anthropomorphic robot (AR), a functional robot (FR), and a computer (CP). As the responses of each game partner were randomized unknowingly to the participants, the attribution of intention or will to an opponent (i.e. HP, AR, FR or CP) was based purely on differences in the perception of shape and embodiment. We hypothesize that the degree of human-likeness of the game partner will modulate what the people attribute to the opponents - the more human like the robot looks the more people attribute human-like qualities to the robot.	computer;interaction;program dependence graph;randomized algorithm;social robot;usability testing	Frank Hegel;Soeren Krach;Tilo Kircher;Britta Wrede;Gerhard Sagerer	2008	RO-MAN 2008 - The 17th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2008.4600728	game theory;simulation;computer science;artificial intelligence;social robot	Robotics	-52.10632870588909	-50.45780566995818	192286
0a7c5094b2dac38126ae0e1a0bb76c1a2b1f12d7	long-term interactions with empathic robots: evaluating perceived support in children	long term interaction;empathy;social support	In this paper, we present an empathic model for social robots that aim to interact with children for extended periods of time. The application of this model to a scenario in which a social robot plays chess with children is described. To evaluate the proposed model, we ran a long-term study in an elementary school and measured children's perception of social support. Our results suggest that children felt supported by the robot in a similar extent to what, in general, children feel supported by their peers. Another interesting finding was that the most valued form of social support was esteem support (reinforcing the other person's sense of competence and self-esteem).	interaction;robot	Iolanda Leite;Ginevra Castellano;André Pereira;Carlos Martinho;Ana Paiva	2012		10.1007/978-3-642-34103-8_30	psychology;developmental psychology;communication;social psychology	HCI	-53.29586908166006	-50.64162659588682	192504
98f2040a6332f134aacd82841ca2816ffbc61d69	effects of echoic mimicry using hummed sounds on human-computer interaction	echoic mimicry;human computer interaction;psychological evaluation;interpersonal relations;interactive system;hummed sounds;animated character	Our research goal is to investigate interpersonal relations involving empathy in human–computer interaction. We focus on mimicry behavior and its ability to elicit intentional stance of a partner in interaction. In this study, we conducted a psychological experiment to examine how prosodic mimicry by computers affects people. An interactive system in this experiment uses an animated character that mimics the prosodic features in a human s voice echoicly by synthesizing the hummed sounds. The sounds consist only of prosodic components similar to continuous humming of the open vowel /a/ or /o/ without any language information. The subjects completed a questionnaire to evaluate the character at different mimicry ratio. The results indicated the following possibilities: First, people favorably interpret a computer s simple response such as echoic mimicry using hummed sounds mixed with a slightly constant prosody response. Second, people may establish an interpersonal relations with a computer through such facilitated interac-	answer to reset;autonomous robot;computer;effective method;experiment;human–computer interaction;interactivity;semantic prosody;signal-to-noise ratio	Noriko Suzuki;Yugo Takeuchi;Kazuo Ishii;Michio Okada	2003	Speech Communication	10.1016/S0167-6393(02)00180-2	psychological evaluation;interpersonal relationship;speech recognition	HCI	-52.198757981771024	-49.37031786469699	192569
4121708d3502f68e5ad5098d4004296bdeab4ade	visual information as a conversational resource in collaborative physical tasks	human computer interaction;theoretical framework;visual information;social impact;wearable computer;collaborative physical tasks;face to face;conversational resource	In this article we consider the ways in which visual information is used as a conversational resource in the accomplishment of collaborative physical tasks. We focus on the role of visual information in maintaining task awareness and in achieving mutual understanding in conversation. We first describe the theoretical framework we use to analyze the role of visual information in physical collaboration. Then, we present two experiments that vary the amount and quality of the visual information available to participants during a collaborative bicycle repair task. We examine the effects of this visual information on performance and on conversational strategies. We conclude with a general discussion of how situational awareness and conversational grounding are achieved in collaborative repair and with some design considerations for systems to support remote collaborative repair. HUMAN-COMPUTER INTERACTION, 2003, Volume 18, pp. 13–49 Copyright © 2003, Lawrence Erlbaum Associates, Inc. Robert Kraut is a social psychologist with an interest in communications and the social impact of computing; he is a professor in the Human Computer Interaction Institute at Carnegie Mellon University. Susan Fussell is a social and cognitive psychologist with interests in face-to-face and computer-mediated communication; she is a system scientist in the Human Computer Interaction Institute at Carnegie Mellon University. Jane Siegel is a psychologist with interests in interpersonal communication and wearable computers; she is a senior system scientist in the Human Computer Interaction Institute at Carnegie Mellon University.	computer-mediated communication;experiment;human computer;human–computer interaction;jane (software);susan landau;wearable computer	Robert E. Kraut;Susan R. Fussell;Jane Siegel	2003	Human-Computer Interaction	10.1207/S15327051HCI1812_2	wearable computer;human–computer interaction;computer science;knowledge management;multimedia;communication	HCI	-54.52222286671736	-46.27593550121782	192593
6444a2c1874a1088a7d375f66e745fbf588f9146	puppet narrator: utilizing motion sensing technology in storytelling for young children	cognition puppet narrator motion sensing technology storytelling young children avatars human computer interaction techniques hci digital puppetry storytelling system virtual puppet virtual items virtual environment motor coordination;avatars animation training motion control human computer interaction virtual environments sensors;image motion analysis avatars cognition computer aided instruction human computer interaction	Using avatars in storytelling to assist narration has proved to be beneficial on promoting creativity, collaboration and intimacy among young children. Development of novel Human Computer Interaction (HCI) techniques provides us with new possibilities to explore the training aspects of storytelling by creating new ways of interaction. In this paper, we design and develop a novel digital puppetry storytelling system - Puppet Narrator for young children, utilizing depth motion sensing technology as the HCI method. More than merely allowing children to narrate orally, our system allows them to use hand gestures to play with a virtual puppet and manipulate it to interact with virtual items in virtual environment to assist narration. Under this novel pattern of interaction, children's narrative ability can be trained and the competencies of cognition and motor coordination can also be nourished.	avatar (computing);cognition;computer animation;digital puppetry;human computer;human–computer interaction;supervised learning;usability;virtual reality	Hui Liang;Jian Chang;Ismail Khalid Kazmi;Jian-Jun Zhang;Peifeng Jiao	2015	2015 7th International Conference on Games and Virtual Worlds for Serious Applications (VS-Games)	10.1109/VS-GAMES.2015.7295784	simulation;engineering;multimedia;communication	HCI	-55.498582271662364	-47.45513481457732	192814
4dcf15af142dfe630c16dea0dabe661d45261bea	a disclosure intimacy rating scale for child-agent interaction	human operational modelling;pcs perceptual and cognitive systems;healthy for life;pal project;robotics;intimacy;elss earth life and social sciences;long term;healthy living;chriself disclosure	Reciprocal self-disclosure is an integral part of social bonding between humans that has received little attention in the field of humanagent interaction. To study how children react to self-disclosures of a virtual agent, we developed a disclosure intimacy rating scale that can be used to assess both the intimacy level of agent disclosures and that of child disclosures. To this end, 72 disclosures were derived from a biography created for the agent and rated by 10 university students for intimacy. A principal component analysis and subsequent k-means clustering of the rated statements resulted in four distinct levels of intimacy based on the risk of a negative appraisal and the impact of betrayal by the listener. This validated rating scale can be readily used with other agents or interfaces.	cluster analysis;floor and ceiling functions;k-means clustering;principal component analysis;rating scale	Franziska Burger;Joost Broekens;Mark A. Neerincx	2016		10.1007/978-3-319-47665-0_40	psychology;computer science;artificial intelligence;term;robotics;social psychology;clinical psychology	HCI	-54.7744054596276	-51.955206455869984	192900
2f2db26a6fa7f8867d9d2055649fddbae7cb662c	caregiving intervention for children with autism spectrum disorders using an animal robot	animals;robots animals variable speed drives educational institutions autism frequency measurement abstracts;autism spectrum disorder;pleo;autism;social skills development;frequency measurement;caregiving behavior;pdd caregiving intervention autism spectrum disorders animal robot social behavior teaching caregiving behavior asd pervasive development disorder;animal robot;robots handicapped aids health care medical disorders;social skill;variable speed drives;social behavior;abstracts;robots;pleo autism animal robot caregiving behavior social skills development;autistic spectrum disorder	In this paper, we explore the possibility of using animal robot for teaching social behaviors, especially caregiving behavior to children with Autistic Spectrum Disorders (ASD) and Pervasive Development Disorder (PDD).	robot	Kwangsu Cho;Christine Shin	2011	2011 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/1957656.1957801	social skills;social behavior;autism;computer science;artificial intelligence	Robotics	-55.536635071190425	-50.747805305825075	193131
32abb3c7dd359b6f42d144a6ea8e7143a6ec6ace	methodological issues using a comfort level device in human-robot interactions	social interaction human robot interactions comfort level device data analysis social robot;text;social interaction;human robot interaction;indexing terms;proof of concept;social sciences computing;robots;computer science;social sciences computing user interfaces robots;article;user interfaces;social robot;human robot interaction biosensors biomedical monitoring adaptive systems computer science educational institutions data analysis video recording testing performance analysis	This paper introduces a handheld comfort level device to measure subjects' comfort levels in human-robot interaction experiments. We discuss methodological issues of using the device in an exploratory HRI study where subjects were asked to use the device to indicate their subjective comfort level throughout the experiment. The recorded comfort data were time stamped for synchronization and analysis purposes in conjunction with the video footage to help identify certain situations in the HRI trials where subjects felt uncomfortable. In order to provide a proof-of-concept for the suitability of the handheld comfort level device for HRI studies we analyzed the data for seven selected subjects. These examples show that our method helped identifying robot behaviors that subjects felt uncomfortable with. We demonstrate that the device revealed certain uncomfortable states that are visually hidden. Limitations of the device and possible implications for future work conclude the paper.	experiment;handheld game console;human–robot interaction;robot	Kheng Lee Koay;Michael L. Walters;Kerstin Dautenhahn	2005	ROMAN 2005. IEEE International Workshop on Robot and Human Interactive Communication, 2005.	10.1109/ROMAN.2005.1513805	human–robot interaction;robot;social relation;simulation;index term;human–computer interaction;computer science;artificial intelligence;social robot;multimedia;user interface;proof of concept	HCI	-50.13195774481552	-47.803185645967154	194036
3abdcc2d0d0be072cf46b0b426061b041c74a658	comparative analysis of verbal alignment in human-human and human-agent interactions		Engagement is an important feature in human-human and human-agent interaction. In this paper, we investigate lexical alignment as a cue of engagement, relying on two different corpora : CID and SEMAINE. Our final goal is to build a virtual conversational character that could use alignment strategies to maintain user’s engagement. To do so, we investigate two alignment processes : shared vocabulary and other-repetitions. A quantitative and qualitative approach is proposed to characterize these aspects in human-human (CID) and human-operator (SEMAINE) interactions. Our results show that these processes are observable in both corpora, indicating a stable pattern that can be further modelled in conversational agents.	configuration interaction;dialog system;observable;text corpus;vocabulary	Sabrina Campano;Jessica Durand;Chloé Clavel	2014			natural language processing;artificial intelligence;computer science;vocabulary	AI	-52.7923942399263	-48.23767323338151	194318
9d69cf6e5b7dcd614fe604245dc1f8017993087e	social coordination in toddler's word learning: interacting systems of perception and action	health research;uk clinical guidelines;biological patents;motion tracking analysis;social interaction;europe pubmed central;parent child dyad;citation search;motion tracking;social cognition;embodied social cognition;uk phd theses thesis;interactive system;life sciences;word learning;quality measures;perception and action;dynamic time warping;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	We measured turn-taking in terms of hand and head movements and asked if the global rhythm of the participants' body activity relates to word learning. Six dyads composed of parents and toddlers (M = 18 months) interacted in a tabletop task wearing motion-tracking sensors on their hands and head. Parents were instructed to teach the labels of 10 novel objects and the child was later tested on a name-comprehension task. Using dynamic time warping, we compared the motion data of all body-part pairs, within and between partners. For every dyad, we also computed an overall measure of the quality of the interaction, that takes into consideration the state of interaction when the parent uttered an object label and the overall smoothness of the turn-taking. The overall interaction quality measure was correlated with the total number of words learned.In particular, head movements were inversely related to other partner's hand movements, and the degree of bodily coupling of parent and toddler predicted the words that children learned during the interaction. The implications of joint body dynamics to understanding joint coordination of activity in a social interaction, its scaffolding effect on the child's learning and its use in the development of artificial systems are discussed.	apela gene;dynamic time warping;head movements;interaction;learning disorders;movement;physical object;toddler (age group);sensor (device)	Alfredo F. Pereira;Linda B. Smith;Chen Yu	2008	Connection science	10.1080/09540090802091891	social cognition;social relation;computer science;artificial intelligence;machine learning;dynamic time warping;cognitive science	HCI	-50.666382087076634	-50.787209793962575	196115
68ba4e9edd052792ba981b9e7192e97e954cfae3	"""a """"somatic alphabet"""" approach to """"sensitive skin"""""""	humanoid robot;somatosensory cortex;touch physiological;human computer interaction;temperature sensors;human robot interaction;tactile sensing;humanoid robots;human skin;tactile sensors;sensory system;cortical neurons	"""The sense of touch is one of the most important sensory system in humans. This paper describes an initial step toward the realization of a fully """"sensitive skin"""" for robots in which somatic sensors of varying modalities such as touch, temperature, pain, and proprioception combine, as if letters in an alphabet, to create a more vivid depiction of the world and foster richer human robot interactions. We have developed a new """"sensitive"""" hand, covered in a lifelike silicone """"skin"""" to explore the importance of touch and the formation of the somatic alphabet in the context of our humanoid robot, Leonardo. From initial tests the populations of these sensors show the potential for similar performance to both the mechanoreceptors in human skin and the cortical neurons in the somatosensory cortex."""	humanoid robot;interaction;population;sensor	Walter Dan Stiehl;Levi Lalla;Cynthia Breazeal	2004	IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004	10.1109/ROBOT.2004.1307495	human–robot interaction;computer vision;computer science;humanoid robot;artificial intelligence	Robotics	-49.52854879638308	-48.612929704549984	196407
4de67d1fcc9cafe952ddf93942188847a32b6766	human evaluation of affective body motions expressed by a small-sized humanoid robot: comparison between elder people and university students	users demographic factors;humanoid robots human robot interaction demography psychology motion measurement velocity measurement recruitment service robots senior citizens informatics;small sized humanoid robot;emotions identification;elder people;humanoid robot;age difference;motion speed;manipulators;psychological experiment;university student;senior citizens;legged locomotion;age;emotion recognition;human robot interaction;data mining;gender;humanoid robots;human robot interaction emotion recognition humanoid robots;university students;human evaluation;motion speed affective body motions small sized humanoid robot human evaluation elder people university students users demographic factors gender age psychological experiment emotions identification;affective body motions;correlation;leg	Body motion expression is one of the useful methods that robots present their emotional states toward users. Since emotions themselves have cultural dependency, however, it is estimated that effects of affective body motions depend on users' demographic factors such as gender and age. In order to clarify this dependence, a psychological experiment was conducted to investigate age differences on evaluation of affective body motions which were implemented into a small-sized humanoid robot. The results showed differences between younger and elder people on identification of emotions, body parts paid attention to, and impression of motion speed and magnitude for the affective body motions of the robot. The paper also discusses about the implications.	humanoid robot	Tatsuya Nomura;Akira Nakao	2009	RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication	10.1109/ROMAN.2009.5326270	human–robot interaction;simulation;computer science;humanoid robot;artificial intelligence	Robotics	-51.189085226672866	-51.25729692402128	196478
0d21b7786ada9fec94faa8f6a030d35e448199b9	social responses to conversational tv vui: apology and voice	vui;apology;social response to computer;voice gender;synthetic voice	The study investigated whether apologetic synthetic gendered voices affect users’ perception of an error-prone VUI. In a TV viewing task, participants interacted with the conversational TV, and executed eight menus in a 2 (apologetic error message: yes vs. no) by 2 (voice gender) by 2 (subject gender) gender balanced, between participants experiment. When participants encountered errors, the TV provided verbal error messages, with or without an apology. The results revealed significant two-way interaction effects of apology (yes) and voice gender (male) on perception of the TV, and the voice. Irrespective of gender, participants responded to a male voice more, when it offered apologies for errors. It is interpreted that the context in which genuineness of apology was regarded as important made participants perceive a male voice as being more trustworthy than a female voice. The participants seem to have applied gender stereotypical perceptions to gendered VUI, as they do to other humans. Social Responses to Conversational TV VUI: Apology and Voice		Eun Kyung Park;Kwan Min Lee;Dong Hee Shin	2015	IJTHI	10.4018/ijthi.2015010102	psychology;multimedia;communication;social psychology	HCI	-52.398288260293654	-51.55996819340213	196859
820512fdc1e53fcf29a3b137867017d11c29a27d	manipulating the perception of virtual audiences using crowdsourced behaviors		Virtual audiences are used for training public speaking and mitigating anxiety related to it. However, research has been scarce on studying how virtual audiences are perceived and which non-verbal behaviors should be used to make such an audience appear in particular states, such as boredom or engagement. Recently, crowdsourcing methods have been proposed for collecting data for building virtual agents’ behavior models. In this paper, we use crowdsourcing for creating and evaluating a nonverbal behaviors generation model for virtual audiences. We show that our model successfully expresses relevant audience states (i.e. low to high arousal, negative to positive valence), and that the overall impression exhibited by the virtual audience can be controlled my manipulating the amount of individual audience members that display a	crowdsourcing	Mathieu Chollet;Nithin Chandrashekhar;Ari Shapiro;Louis-Philippe Morency;Stefan Scherer	2016		10.1007/978-3-319-47665-0_15	social psychology;multimedia;perception;anxiety;nonverbal communication;impression;computer science;public speaking;boredom;crowdsourcing	HCI	-53.31392030675791	-49.881095495087635	196890
270bdce6053a143b32915d56808063adf8007d5a	real-time game adaptation for optimizing player satisfaction	player satisfaction;controllable game parameter;game survey experiment;neural nets;computational intelligence;real time;real time adaptation;adaptive control;programmable control;gradient ascent;testing;artificial neural networks augmented reality testing optimization methods programmable control adaptive control statistics accuracy computational intelligence artificial intelligence;user modeling;artificial neural network user model;player satisfaction optimisation;accuracy;artificial neural networks;controllable game parameter real time game adaptation player satisfaction optimisation playware physical interactive platform artificial neural network user model augmented reality game player game survey experiment;augmented reality games;augmented reality game player;user modeling augmented reality games gradient ascent neuro evolution player satisfaction real time adaptation;statistics;artificial intelligence;playware physical interactive platform;neuro evolution;augmented reality;computer games;neural nets artificial intelligence augmented reality computer games;real time game adaptation;physical interaction;user model;artificial neural network;optimization methods	"""A methodology for optimizing player satisfaction in games on the """"playware"""" physical interactive platform is demonstrated in this paper. Previously constructed artificial neural network user models, reported in the literature, map individual playing characteristics to reported entertainment preferences for augmented-reality game players. An adaptive mechanism then adjusts controllable game parameters in real time in order to improve the entertainment value of the game for the player. The basic approach presented here applies gradient ascent to the user model to suggest the direction of parameter adjustment that leads toward games of higher entertainment value. A simple rule set exploits the derivative information to adjust specific game parameters to augment the entertainment value. Those adjustments take place frequently during the game with interadjustment intervals that maintain the user model's accuracy. Performance of the adaptation mechanism is evaluated using a game survey experiment. Results indicate the efficacy and robustness of the mechanism in adapting the game according to a user's individual playing features and enhancing the gameplay experience. The limitations and the use of the methodology as an effective adaptive mechanism for entertainment capture and augmentation are discussed."""	algorithm;artificial neural network;augmented reality;baseline (configuration management);cross-validation (statistics);feature selection;game controller;gradient descent;initial condition;interaction;mathematical optimization;online and offline;optimizing compiler;playware;preference learning;real-time clock;real-time locating system;real-time transcription;response time (technology);robustness (computer science);times ascent;user modeling	Georgios N. Yannakakis;John Hallam	2009	IEEE Transactions on Computational Intelligence and AI in Games	10.1109/TCIAIG.2009.2024533	non-cooperative game;game design;bayesian game;simulation;user modeling;simultaneous game;computer science;artificial intelligence;game mechanics;machine learning;strategy;multimedia;screening game;simulations and games in economics education;sequential game;artificial neural network	HCI	-48.795758889313724	-51.489338665876474	196910
2ac39e733bc76cb246ac4c191794f119434ab5d7	investigation into a mixed hybrid using ssvep and eye gaze for optimising user interaction within a virtual environment	brain;computer;hybrid;interface;eye gaze	Brain Computer Interface (BCI) technology has been used successfully in neurophysiological research laboratories, but has had less success when used outside the laboratory and particularly for people with disability. The hybrid BCI approach offers the potential for a more robust solution, with potential better usability to promote greater acceptance. The emphasis on improving human computer interaction may facilitate more widespread deployment, particularly where BCI alone has proved unsuccessful. This paper adapts an existing modular BCI architecture to support a ‘mixed hybrid’, by combining a BCI with a commercial eye tracker, and suggests graphical user interfaces to facilitate operation and control of a virtual environment.	brain–computer interface;eye tracking;graphical user interface;human computer;human–computer interaction;software deployment;usability;virtual reality	Paul J. McCullagh;Leo Galway;Gaye Lightbody	2013		10.1007/978-3-642-39188-0_57	computer vision;simulation;human–computer interaction;engineering	HCI	-50.55808378066787	-45.70002552344919	197264
365d31447366592ff12d0becfd8312f6e2cbc916	evaluation of emotional influence in communicating with a robot	robots;correlation;labeling	Communicative robots have studied as a mediator to obtain the information in Human-robot interaction research field. The study in this paper presents that people have emotional feelings during interacting with robot by the comparison experiment conducted as two assignment, ifbot-assignment and web-assignment. 62 subjects are participated in both assignments. Ifbot assignment is to get a particular information by verbal communication with a robot. Web assignment is to search the same information by using a limited search engine. The emotions of participants are assessed by the questionnaire regarding eight different emotional status. As a result, the participants experienced more strongly emotional feelings, such as “Depression”, “Boredom”, “Liveliness”, “Friendliness”, and “Startlement”, in the ifbot assignment than web assignment. Moreover, participants are divided into five groups based on the statistical tests.	human–robot interaction;robot;web search engine	Ryo Suzuki;Jaeryoung Lee;Goro Obinata;Takenori Suzuki;Kotaro Matsumoto	2015	2015 International Symposium on Micro-NanoMechatronics and Human Science (MHS)	10.1109/MHS.2015.7438263	psychology;simulation;communication;social psychology	Robotics	-52.81043967761884	-50.01402133159658	197589
1788b1b52a056b4d3f0174ac4d0d781ab354f86d	reverbalize: a crowdsourced reverberation controller	reverberation;interfaces;human factors;audio;human computation;audio engineering;crowdsourcing	"""One of the most commonly-used audio production tools is the reverberator. Reverberators apply subtle or large echo effects to sound and are typically used in commercial audio recordings. Current reverberator interfaces are often complex and hard-to-understand. In this work, we describe Reverbalize, a novel and easy-to-use interface for a reverberator. Reverbalize uses crowdsourced data to create a 2-dimensional map of adjectives used to describe reverberation (e.g. """"underwater'). Adjacent words describe similar reverberation effects. Word size correlates with agreement for the definition of a word. To use Reverbalize, the user simply clicks on the descriptive adjective that best describes the desired effect. The tool modifies the sound accordingly. A text search box also lets the user type in the desired word."""	crowdsourcing;word lens	Prem Seetharaman;Bryan Pardo	2014		10.1145/2647868.2654876	speech recognition;reverberation;computer science;human factors and ergonomics;interface;crowdsourcing	HCI	-49.38223759640744	-45.53130000819805	198302
dd5e8b4376759f12d85d7f98257a17aad09678c3	robot gossip: effects of mode of robot communication on human perceptions of robots	robots in society;robot communication style;groups of robots;social robotics;human attitudes toward robots;group effects	With robots becoming more prevalent, it is important to understand human attitudes toward robots not only when humans directly interact with the robots as most research examines, but when robots are performing nonsocial tasks (e.g., cleaning) within sight and hearing of humans. This study examined how presumed robot communication style in such situations of human-robot co-location affects human perceptions of a group of robots. Results suggest that communication style of robots did not affect perceptions of robots, but further studies should use different techniques to manipulate supposed communication style.	plasma cleaning;robot	Marlena R. Fraune;Selma Sabanovic	2014	2014 9th ACM/IEEE International Conference on Human-Robot Interaction (HRI)	10.1145/2559636.2559832	simulation;media lab europe's social robots;computer science;artificial intelligence;social robot;robot control;personal robot	Robotics	-52.194300537771845	-50.81687339620613	198355
a334b9ce04cbee6d047d7aecc4cc29e7aa62294e	gaze patterns during face-to-face interaction	cognitive state;gaze patterns;eye tracker;multimodal data;experimental setup;realistic face-to-face interaction;social role;social interaction;face-to-face interaction;control model;target speaker;data gathering;user interfaces	We present here the analysis of multimodal data gathered during realistic face-to-face interaction of a target speaker with a number of interlocutors. Videos and gaze of both interlocutors were monitored with an experimental setup using coupled cameras and screens equipped with eye trackers. With the aim to understand the functions of gaze in social interaction and to develop a gaze control model for our talking heads we investigate the influence of cognitive state and social role on the observed gaze behaviour.	eye tracking;multimodal interaction	Stephan Raidt;Gérard Bailly;Frédéric Elisei	2007	2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology - Workshops		social relation;computer vision;computer science;multimedia;user interface;statistics;data collection	HCI	-51.49610127671972	-48.29768646620846	199816
