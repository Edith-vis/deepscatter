id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
02d5a1be04d3cb05b11c1eeeee8755f044342e2f	maximum likelihood estimation of a reverberation model for robust distant-talking speech recognition	reverberation;transient response hidden markov models maximum likelihood estimation reverberation speech recognition;training;speech;maximum likelihood estimation;room impulse response;automatic speech recognition;maximum likelihood estimate;hidden markov models;speech recognition;hidden markov models reverberation speech speech recognition maximum likelihood estimation training data models;room impulse responses maximum likelihood estimation reverberation model robust distant talking speech recognition reverberation modelfor asr hmm training;data models	We propose a novel approach for estimating a reverberation model for a robust recognizer according to [1], which is designed to allow distant-talking automatic speech recognition (ASR) in reverberant environments. Based on a few calibration utterances with known transcriptions recorded in the target environment, a maximum likelihood estimator is used to find the means and variances of the reverberation model. In contrast to [1] and to HMM training on artificially reverberated training data (e. g. [2]), measurements of room impulse responses become unnecessary, and the effort for training is greatly reduced. Simulations of a connected digit recognition task show that, in highly reverberant environments, the reverberation models estimated by the proposed approach achieve significantly higher recognition rates than HMMs trained on reverberant data.	finite-state machine;hidden markov model;simulation;speech recognition	Armin Sehr;Yuanhang Zheng;Elmar Nöth;Walter Kellermann	2007	2007 15th European Signal Processing Conference		speech recognition;acoustics;engineering;pattern recognition	Vision	-15.42180550693654	-92.33987271290898	182783
eb9717dadd36d876f0578eae65e70f3ff1347fc8	new machine scores and their combinations for automatic mandarin phonetic pronunciation quality assessment	gabor feature;formant;time frequency;posterior probability;quality assessment;pattern classification;speech recognition;correlation coefficient;computer assisted language learning;neural network	This paper discusses Mandarin vowel pronunciation quality assessment. The phonetic pronunciation quality is traditionally evaluated under the speech recognition framework by the phonetic posterior probability score, which may be computed by normalizing the frame-based posterior probability or be calculated on the phone segment directly. By the first method, we can achieve a human-machine scoring correlation coefficient (CC) of 0.832 for vowel; and by the second, the CC can be up to 0.847. This paper proposes a novel kind of formant feature and applies the feature to the evaluation of vowel: we transform the formant plots on the time-frequency plane to a bitmap and extract its Gabor feature for pattern classification; when use the classification probability for pronunciation assessment, we can get a CC of 0.842. Finally we combine the three scores with various linear or nonlinear methods; the best CC of 0.913 is gotten by using neural network.	super robot monkey team hyperforce go!	Fuping Pan;Qingwei Zhao;Yonghong Yan	2007		10.1007/978-3-540-74819-9_101	natural language processing;speech recognition;computer science;pattern recognition	NLP	-13.824537636231396	-87.41627750718858	182857
eaf66bf3fe052b7235f4484c33b044717d43c9d4	handwriting recognition using neural networks and hidden markov models			artificial neural network;handwriting recognition;hidden markov model;markov chain	Markus Schenkel	1995			recurrent neural network;signature recognition	ML	-15.606835568704867	-87.55107216092344	183361
67f1dd47296df538e42c9c811df263978e1da599	an autoencoder neural-network based low-dimensionality approach to excitation modeling for hmm-based text-to-speech	hmm based text to speech;perceptual quality;parametric model;quality attributes;convergence;neural networks;highly intelligible speech;speech synthesis;neural nets;hidden markov model;speech processing;training;speech processing neural nets;vocal tract;speech;excitation modeling speech synthesis hidden markov models neural networks autoencoders;runtime;autoencoder neural network;excitation modeling;artificial neural networks;hidden markov models;hidden markov models speech synthesis signal synthesis signal generators speech analysis runtime neural networks signal analysis cepstral analysis matched filters;text to speech;highly intelligible speech autoencoder neural network low dimensionality approach excitation modeling hmm based text to speech data driven systems;autoencoders;data driven systems;neural network;low dimensionality approach	HMM-TTS synthesis is a popular approach toward flexible, low-footprint, data driven systems that produce highly intelligible speech. In spite of these strengths, speech generated by these systems exhibit some degradation in quality, attributable to an inadequacy in modeling the excitation signal that drives the parametric models of the vocal tract. This paper proposes a novel method for modeling the excitation as a low-dimensional set of coefficients, based on a non-linear map learned through an autoencoder. Through analysis-and-resynthesis experiments, and a formal listening test, we show that this model produces speech of higher perceptual quality compared to conventional pulse-excited speech signals at the p ≪ 0.01 significance level.	artificial neural network;autoencoder;coefficient;elegant degradation;experiment;hidden markov model;nonlinear system;speech synthesis;tract (literature)	Srikanth Vishnubhotla;Raul Fernandez;Bhuvana Ramabhadran	2010	2010 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2010.5495546	natural language processing;vocal tract;speech recognition;parametric model;convergence;computer science;speech;machine learning;artificial neural network	Robotics	-16.87192816747021	-90.58529780037343	183447
3cde7526b26e02fefdd8148ffe276f21222642fb	censrec-3: an evaluation framework for japanese speech recognition in real car-driving environments	article publisher;tecnologia electronica telecomunicaciones;base donnee;sistema mano libre;common evaluation framework;in car speech database;japonais;speech processing;hand free system;database;tratamiento palabra;traitement parole;base dato;censrec 3;microfono;metodo secuencial;sequential method;programacion lineal;reconocimiento voz;systeme mains libres;linear programming;mot isole;palabra aislada;methode sequentielle;programmation lineaire;speech recognition;reconnaissance parole;incar speech database;tecnologias;isolated word;grupo a;japones;noisy speech recognition;microphone;evaluation framework;japanese	This paper introduces a common database, an evaluation framework, and its baseline recognition results for in-car speech recognition, CENSREC-3, as an outcome of the IPSJ-SIG SLP Noisy Speech Recognition Evaluation Working Group. CENSREC-3, which is a sequel to AURORA-2J, has been designed as the evaluation framework of isolated word recognition in real car-driving environments. Speech data were collected using two microphones, a close-talking microphone and a hands-free microphone, under 16 carefully controlled driving conditions, i.e., combinations of three car speeds and six car conditions. CENSREC-3 provides six evaluation environments designed using speech data collected in these conditions. key words: noisy speech recognition, common evaluation framework, incar speech database, CENSREC-3	baseline (configuration management);microphone;speech recognition;superword level parallelism	Masakiyo Fujimoto;Kazuya Takeda;Satoshi Nakamura	2006	IEICE Transactions	10.1093/ietisy/e89-d.11.2783	japanese;speech recognition;telecommunications;computer science;linear programming;artificial intelligence;speech processing	NLP	-14.42930934683822	-88.22442082626353	183577
b6caa83e513bf597577ad4312d1a8266a730d182	comparison the performance of hybrid hmm/mlp and rbf/lvq ann models - application for speech and medical pattern classification		In the last several years, the hybrid models have become increasingly popular. We use involves multinetwork RBF/LVQ structure and hybrid HMM/MLP model for speech recognition and medical diagnosis.	hidden markov model;learning vector quantization;linear approximation;memory-level parallelism;radial basis function;speech recognition	Lilia Lazli;Mounir Boukadoum;Abdennasser Chebira;Kurosh Madani	2012			speech recognition;machine learning;pattern recognition	NLP	-15.47781312855379	-87.57635541942432	184725
e01f974178df81782f336b1ed97f1ca7152e6476	learning corpus-invariant discriminant feature representations for speech emotion recognition		As a hot topic of speech signal processing, speech emotion recognition methods have been developed rapidly in recent years. Some satisfactory results have been achieved. However, it should be noted that most of these methods are trained and evaluated on the same corpus. In reality, the training data and testing data are often collected from different corpora, and the feature distributions of different datasets often follow different distributions. These discrepancies will greatly affect the recognition performance. To tackle this problem, a novel corpus-invariant discriminant feature representation algorithm, called transfer discriminant analysis (TDA), is presented for speech emotion recognition. The basic idea of TDA is to integrate the kernel LDA algorithm and the similarity measurement of distributions into one objective function. Experimental results under the cross-corpus conditions show that our proposed method can significantly improve the recognition rates. key words: speech emotion recognition, transfer learning, dimensionality reduction	algorithm;dimensionality reduction;emotion recognition;linear discriminant analysis;local-density approximation;loss function;optimization problem;signal processing;speech processing;text corpus;topological data analysis	Peng Song;Shifeng Ou;Zhenbin Du;Yanyan Guo;Wenming Ma;Jinglei Liu;Wenming Zheng	2017	IEICE Transactions		speech recognition;feature;pattern recognition	AI	-16.293157855925532	-91.59844639854518	184732
4a1a38dbf941b3604a943d057c992474b5d62c6b	dialog context language modeling with recurrent neural networks		In this work, we propose contextual language models that incorporate dialog level discourse information into language modeling. Previous works on contextual language model treat preceding utterances as a sequence of inputs, without considering dialog interactions. We design recurrent neural network (RNN) based contextual language models that specially track the interactions between speakers in a dialog. Experiment results on Switchboard Dialog Act Corpus show that the proposed model outperforms conventional single turn based RNN language model by 3.3% on perplexity. The proposed models also demonstrate advantageous performance over other competitive contextual language models.	artificial neural network;interaction;language model;perplexity;random neural network;recurrent neural network;telephone switchboard;dialog	Bing Liu;Ian Lane	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7953251	natural language processing;speech recognition;computer science;machine learning	NLP	-18.96318321261246	-87.33836815407078	184745
4366755dd65c68f3d268d8b159a2f3696d279538	"""""""eigenlips"""" for robust speech recognition"""	background noise;front end;robust speech recognition;image segmentation;image processing;acoustic distortion;crosstalk;acoustic information;visual front end;multilayer perceptrons;additive noise;acoustic signal processing;deformable models;eigenlips;robust speech recognition system;visual information;acoustic noise;lip movements;speech recognition equipment;visual features;speech recognition;robustness;feedforward neural nets;computer science;cocktail party effect;multi speaker connected letters recognizer;hybrid connectionist speech recognition	In this study we improve the performance of a hybrid connectionist speech recognition system by incorporating visual information about the corresponding lip movements. Specifically, we investigate the benefits of adding visual features in the presence of additive noise and crosstalk (cocktail party effect). Our study extends our previous experiments [3] by using a new visual front end, and an alternative architecture for combining the visual and acoustic information. Furthermore, we have extended our recognizer to a multi-speaker, connected letters recognizer. Our results show a significant improvement for the combined architecture (acoustic and visual information) over just the acoustic system in the presence of additive noise and crosstalk.	acoustic cryptanalysis;additive white gaussian noise;connectionism;crosstalk;experiment;finite-state machine;front and back ends;speech recognition;utility functions on indivisible goods	Christoph Bregler;Yochai Konig	1994		10.1109/ICASSP.1994.389567	crosstalk;speech recognition;image processing;computer science;front and back ends;noise;background noise;image segmentation;robustness;cocktail party effect	HCI	-15.23242776385347	-89.61574265344832	184798
11c05370470435a316acdb578d8bc607fc57fa92	factor analysis for audio-based video genre classification	error reduction;indexing terms;gaussian mixture model;factor analysis;index terms video genre identification;universal background model;support vector machine;genre classification;automatic classification;factor analy sis	Statistical classifiers operate on features that generally include both useful and useless information. These two types of information are difficult to separate in the feature domain. Recently, a new paradigm based on a Latent Factor Analysis (LFA) proposed a model decomposition into usefull and useless components. This method was successfully applied to speaker and language recognition tasks. In this paper, we study the use of LFA for video genre classification by using only the audio channel. We propose a classification method based on short-term cepstral features and Gaussian Mixture Models (GMM) or Support Vector Machine (SVM) classifiers, that are combined with Factor Analysis (FA). Experiments are conducted on a corpus composed of 5 types of video (musics, commercials, cartoons, movies and news). The relative classification error reduction obtained by using the best factor analysis configuration with respect to the baseline system, Gaussian Mixture Model Universal Background Model (GMM-UBM), is about 56%, corresponding to a correct identification rate of about 90%.	baseline (configuration management);cepstrum;experiment;factor analysis;google map maker;mixture model;programming paradigm;statistical classification;support vector machine;text corpus;utility	Mickael Rouvier;Driss Matrouf;Georges Linarès	2009			support vector machine;speech recognition;index term;computer science;machine learning;pattern recognition;mixture model;factor analysis	ML	-12.417891082492357	-88.77748697544973	185034
ccbe3c0510765a02761f88d253ce4e86c27b291a	an unsupervised approach to glottal inverse filtering	filtering;speech processing;training;speech;estimation;dictionaries;adaptation models	The extraction of the glottal volume velocity waveform from voiced speech is a well-known example of a sparse signal recovery problem. Prior approaches have mostly used well-engineered speech processing or convex L1-optimization methods to solve the inverse filtering problem. In this paper, we describe a novel approach to modeling the human vocal tract using an unsupervised dictionary learning framework. We make the assumption of an all-pole model of the vocal tract, and derive an L1 regularized least squares loss function for the all-pole approximation. To evaluate the quality of the extracted glottal volume velocity waveform, we conduct experiments on real-life speech datasets, which include vowels and multi-speaker phonetically balanced utterances. We find that the the unsupervised model learns meaningful dictionaries of vocal tracts, and the proposed data-driven unsupervised framework achieves a performance comparable to the IAIF (Iterative Adaptive Inverse Filtering) glottal flow extraction approach.	approximation;artificial neural network;database;detection theory;dictionary;distortion;experiment;genetic algorithm;inverse filter;iterative method;least squares;loss function;machine learning;real life;sparse matrix;speech processing;tract (literature);unsupervised learning;velocity (software development);waveform	Sayan Ghosh;Eugene Laksana;Louis-Philippe Morency;Stefan Scherer	2016	2016 24th European Signal Processing Conference (EUSIPCO)	10.1109/EUSIPCO.2016.7760242	speech recognition;computer science;machine learning;pattern recognition	ML	-16.133302654360136	-93.65882048832037	185094
0f6abadeb4396b5a18f2b8d065fb264d83eecc58	investigating deep neural network adaptation for generating exclamatory and interrogative speech in mandarin	pragmatics;neural networks;acoustics;speech;training data;hidden markov models;adaptation models	Currently, most speech synthesis systems generate speech only in a reading style, which greatly affects the expressiveness of synthetized speech. To improve the expressiveness of synthetized speech, this paper focuses on the generation of exclamatory and interrogative speech for Mandarin spoken language. We propose a multi-style (exclamatory and interrogative) deep neural network-based acoustic model with a style-specific layer (can have multiple layers) while other layers are shared. The style-specific layer is used to model the distinct style specific patterns. The shared layers allow maximum knowledge sharing between the declarative and multi-style speech. Such method is also validated on different neural networks. Besides, models that adapted on both spectral and F0 parameter is compared with models while only F0 parameter is adapted. Both subjective and objective evaluations show that this method is superior to prior work (which is trained by the combination of constrained Maximum likelihood linear regression (CMLLR) and structural maximum a posterior (SMAP)), and the BLSTM where top 1 layer is style-specific layer that adapted on both spectral and F0 parameter can achieve the best results.	acoustic cryptanalysis;acoustic model;artificial neural network;declarative programming;deep learning;speech synthesis;super robot monkey team hyperforce go!	Yibin Zheng;Ya Feng Li;Zhengqi Wen;Bin Liu;Jianhua Tao	2016	2016 10th International Symposium on Chinese Spoken Language Processing (ISCSLP)	10.1109/ISCSLP.2016.7918426	natural language processing;training set;speech recognition;computer science;speech;machine learning;linguistics;hidden markov model;pragmatics	NLP	-17.903799173499436	-87.37011625944254	185172
520c9fb1746d6cc2447c21892e8d5d2a78750572	a novel model for phoneme recognition using phonetically derived features	databases;standards;speech;hidden markov models speech recognition computational modeling speech databases cepstral analysis standards;computational modeling;cepstral analysis;hidden markov models;hidden markov model phoneme recognition phonetically derived features segmental modelling phoneme based speech recognition hmm based recognition hybrid phoneme model timit database classification task segmentation hypothesis approach iid assumption independent and identically distributed assumption;speech recognition;speech recognition feature extraction hidden markov models signal classification speech processing	This paper presents work on the use of segmental modelling and phonetic features for phoneme based speech recognition. The motivation for the work is to lessen the effects of the IID assumption in HMM based recognition. The use of phonetic features which are derived across the duration of a phonetic segment is discussed. In conjunction with the use of these features, a hybrid phoneme model is introduced. In a classification task on the TIMIT database, these features are capable of outperforming standard HMM. The extension of the work to recognition is presented in detail. The challenges are identified and a novel algorithm presented for recognition based on phonetic features and the hybrid phoneme model. The approach is built around a segmentation hypothesis approach employing pruning at a number of levels.	algorithm;hidden markov model;speech recognition;synapomorphy;timit	Naomi Harte;Saeed Vaseghi;Paul M. McCourt	1998	9th European Signal Processing Conference (EUSIPCO 1998)		natural language processing;speaker recognition;speech recognition;computer science;pattern recognition	Vision	-18.316150600930204	-90.96490113799885	185207
74ab2a9419f5356d77bad207ccb1b2caaf196a4f	context-dependent classes in a hybrid recurrent network-hmm speech recognition system	word error rate;recurrent network;speech recognition;context dependent	A method for incorporating context-dependent phone classes in a connectionist-HMM hybrid speech recognition system is introduced. A modular approach is adopted, where single-layer networks discriminate between different context classes given the phone class and the acoustic data. The context networks are combined with a context-independent (CI) network to generate context-dependent (CD) phone probability estimates. Experiments show an average reduction in word error rate of 16% and 13% from the CI system on ARPA 5,000 word and SQALE 20,000 word tasks respectively. Due to improved modelling, the decoding speed of the CD system is more than twice as fast as the CI system.	acoustic cryptanalysis;connectionism;context-sensitive language;hidden markov model;recurrent neural network;sqale;speech recognition;word error rate	Dan J. Kershaw;Anthony J. Robinson;Mike Hochberg	1995			natural language processing;speech recognition;word error rate;computer science;machine learning;context-dependent memory	ML	-17.72551106381158	-87.83643429970597	185391
c777adb679742e9edd25345c865118103d0010a2	lattice-based risk minimization training for unsupervised language model adaptation		This paper describes a lattice-based risk minimization training method for unsupervised language model (LM) adaptation. In a broadcast archiving system, unsupervised LM adaptation using transcriptions generated by speech recognition is considered to be useful for improving the performance. However, conventional linear interpolation methods occasionally degrade the performance because of incorrect words in the training transcriptions. Accordingly, we propose a new adaptation method aiming to reflect error information among training lattices. The method minimizes the whole risk of training lattices to yield a log-linear model, which consists of a set of linguistic features. The advantage of the method is that the model parameters can be obtained efficiently in an unsupervised manner. Experimental results obtained in transcribing Japanese broadcast news showed significant word error rate reduction for those of conventional mixture LMs.	archive;language model;linear interpolation;linear model;log-linear model;speech recognition;teaching method;unsupervised learning;word error rate	Akio Kobayashi;Takahiro Oku;Shinichi Homma;Toru Imai;Seiichi Nakagawa	2011			speech recognition;artificial intelligence;transcription (linguistics);word error rate;pattern recognition;linear interpolation;unsupervised learning;minification;lattice (order);bayes' theorem;language model;machine learning;computer science	NLP	-18.882355055639284	-91.72695138108126	185577
581353ac732ee59c4db03ee121e1d98cca6f485d	weakly supervised keyword learning using sparse representations of speech	sparseness;histograms;vocabulary acquisition;learning performance weakly supervised keyword learning sparse speech representations nonnegative matrix factorization small scale speech applications speech impaired assistive technology exemplar based sparse representations;psi_speech;acoustics;speech processing;vocabulary;exemplars;speech;exemplars vocabulary acquisition nonnegative matrix factorization sparseness lasso;handicapped aids;vectors;matrix decomposition;nonnegative matrix factorization;speech recognition;speech recognition handicapped aids learning artificial intelligence matrix decomposition speech processing;learning artificial intelligence;adaptation models;speech vectors speech recognition acoustics histograms vocabulary adaptation models;lasso	When applied to speech, Non-negative Matrix Factorization is capable of learning a small vocabulary of words, foregoing any prior linguistic knowledge. This makes it adequate for small-scale speech applications where flexibility is of the utmost importance, e.g. assistive technology for the speech impaired. However, its performance depends on the way its inputs are represented. We propose the use of exemplar-based sparse representations of speech, and explore the influence of some of these representation's basic parameters, such as the total number of exemplars considered and the sparseness imposed on them. We show that the resulting learning performance compares favorably with those of previously proposed approaches.	assistive technology;neural coding;non-negative matrix factorization;sparse matrix;supervised learning;vocabulary	Joris Driesen;Jort F. Gemmeke;Hugo Van hamme	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6287950	natural language processing;speech recognition;computer science;speech;machine learning;lasso;pattern recognition;speech processing;histogram;matrix decomposition;non-negative matrix factorization	Robotics	-17.266483636536716	-92.05210690344376	185642
20f63127eae7e7838fe19d838f59955a58699171	variable-size gaussian mixture models for music similarity measures.		An algorithm to efficiently determine an appropriate number of components for a Gaussian mixture model is presented. For determining the optimal model complexity we do not use a classical iterative procedure, but use the strong correlation between a simple clustering method (BSAS [13]) and an MDL-based method [6]. This approach is computationally efficient and prevents the model from representing statistically irrelevant data. The performance of these variable size mixture models is evaluated with respect to hub occurrences, genre classification and computational complexity. Our variable size modelling approach marginally reduces the number of hubs, yields 3-4% better genre classification precision and is approximately 40% less computationally expensive.	algorithm;algorithmic efficiency;analysis of algorithms;cluster analysis;computation;computational complexity theory;database;distance matrix;iterative method;mdl (programming language);mixture model;relevance;usb hub	Wietse Balkema	2007			speech recognition;pattern recognition;statistics	ML	-17.84630014970664	-94.22574923823777	185677
3adef1ab6f7f7d56c501a54b2e2d36e8cc19b8dd	a vowel based approach for acted emotion recognition	emotion recognition;vowel detection;automatic speech segmentation	This paper is devoted to the description of a new approach for emotion recognition. Our contribution is based on both the extraction and the characterization of phonemic units such as vowels and consonants, which are provided by a pseudophonetic speech segmentation phase combined with a vowel detector. Concerning the emotion recognition task, we explore acoustic and prosodic features from these pseudo-phonetic segments (vowels and consonants), and we compare this approach with traditional voiced and unvoiced segments. The classification is realized by the well-known k-nn classifier (k nearest neighbors) from two different emotional speech databases: Berlin (German) and Aholab (Basque).	acoustic cryptanalysis;database;emotion recognition;k-nearest neighbors algorithm;liburuklik;speech segmentation;whole earth 'lectronic link	Fabien Ringeval;Mohamed Chetouani	2007		10.1007/978-3-540-70872-8_19	psychology;natural language processing;speech recognition;communication	NLP	-12.683515157405614	-87.4782387100724	185727
aedc0c5bbc591953f6125cb101f5c2625236be0e	on the use of spectro-temporal features for the ieee aasp challenge ‘detection and classification of acoustic scenes and events’	gabor filters;acoustics hidden markov models feature extraction speech event detection noise frequency modulation;speech enhancement;speech recognition gabor filters hidden markov models signal classification speech enhancement;d case challenge spectro temporal features acoustic event detection system two layer hidden markov model ieee aasp challenge detection and classification of acoustic scenes and events noise reduction log spectral amplitude estimator noise power density estimation signal enhancement gabor filterbank features mel frequency cepstral coefficients automatic speech recognition office live recordings;ieee aasp d case challenge acoustic event detection gabor filterbank amplitude modulation spectrogram;hidden markov models;signal classification;speech recognition	In this contribution, an acoustic event detection system based on spectro-temporal features and a two-layer hidden Markov model as back-end is proposed within the framework of the IEEE AASP challenge `Detection and Classification of Acoustic Scenes and Events' (D-CASE). Noise reduction based on the log-spectral amplitude estimator by [1] and noise power density estimation by [2] is used for signal enhancement. Performance based on three different kinds of features is compared, i.e. for amplitude modulation spectrogram, Gabor filterbank-features and conventional Mel-frequency cepstral coefficients (MFCCs), all of them known from automatic speech recognition (ASR). The evaluation is based on the office live recordings provided within the D-CASE challenge. The influence of the signal enhancement is investigated and the increase in recognition rate by the proposed features in comparison to MFCC-features is shown. It is demonstrated that the proposed spectro-temporal features achieve a better recognition accuracy than MFCCs.	acoustic cryptanalysis;coefficient;filter bank;hidden markov model;markov chain;mel-frequency cepstrum;modulation;noise power;noise reduction;noise spectral density;spectrogram;speech recognition	Jens Schröder;Niko Moritz;Marc René Schädler;Benjamin Cauchi;Kamil Adiloglu;Jörn Anemüller;Simon Doclo;Birger Kollmeier;Stefan Goetze	2013	2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics	10.1109/WASPAA.2013.6701868	speech recognition;acoustics;computer science;pattern recognition;hidden markov model	Vision	-12.953246380972134	-90.82652616999226	185819
8ca3d900d8589d8d7771ae18d73bd19ba3b86de6	spectral envelope transformation using dfw and amplitude scaling for voice conversion with parallel or nonparallel corpora		Dynamic Frequency Warping (DFW) offers an appealing alternative to GMM-based voice conversion, which suffers from ”over-smoothing” that hinders speech quality. However, to adjust spectral power after DFW, previous work returns to GMMtransformation. This paper proposes a more effective DFWwith amplitude scaling (DFWA) that functions on the acoustic class level and is independent of GMM-transformation. The amplitude scaling compares average target and warped source log amplitude spectra for each class. DFWA outperforms the GMM in terms of both speech quality and timbre conversion, as confirmed in objective and subjective testing. Moreover, DFWA performance is equivalent using parallel or nonparallel corpora.	acoustic cryptanalysis;bilinear transform;google map maker;image scaling;parallel computing;smoothing;text corpus	Elizabeth Godoy;Olivier Rosec;Thierry Chonavel	2011			speech recognition;pattern recognition;artificial intelligence;amplitude;spectral envelope;computer science;scaling	NLP	-13.242540145690848	-90.49279850087117	185962
2e815e27dd2424fa33ffa5d2514b61a9910a4ad2	t-norm for text-dependent commercial speaker verification applications: effect of lexical mismatch	databases;speaker recognition testing robustness error analysis convergence speech databases security;background modeling;convergence;test time score normalization technique;normative system;text dependent commercial speaker verification applications;speech;controlled experiment;testing;cohort speaker models;hybrid scoring scheme;speaker verification;speaker recognition;error analysis;dependable systems;text dependent speaker verification;relative error;hybrid scoring scheme t norm text dependent commercial speaker verification applications lexical mismatch test time score normalization technique text dependent speaker verification target speaker model cohort speaker models relative error rate reduction;lexical mismatch;target speaker model;error statistics;robustness;t norm;relative error rate reduction;security;error statistics speaker recognition	We describe a test-time score normalization technique (T-Norm) for text-dependent speaker verification that is robust to lexical mismatch. The main challenge to the deployment of T-Norm in a text-dependent task is the mismatch between the lexicon of the target speaker model in the application and that of the cohort speaker models. We show the negative effect of that mismatch in controlled experiments and propose a hybrid scoring scheme (T-Norm and background model) to remedy it. In a lexically mismatched scenario, which is inherent to the deployment of T-Norm in a text-dependent system, we show a 31% relative error rate reduction using the hybrid scoring over T-Norm alone. A 22% relative error rate reduction is measured over the baseline (no T-Norm) system.	approximation error;baseline (configuration management);database normalization;experiment;lexicon;software deployment;speaker recognition;t-norm	Matthieu Hébert;Daniel Boies	2005	Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.	10.1109/ICASSP.2005.1415217	natural language processing;speaker recognition;approximation error;speech recognition;convergence;computer science;speech;pattern recognition;software testing;t-norm;robustness	Robotics	-15.36075042644747	-92.49421162808859	186092
04d5c624dffcf82974fae06c831cebc040bfcbba	low-resource language recognition using a fusion of phoneme posteriorgram counts, acoustic and glottal-based i-vectors	glottal source low resource language recognition phoneme posteriorgram counts acoustic i vectors glottal based i vectors albayzin 2012 lre competition online videos noisy files i vector based systems acoustic system mfcc phonotactic system trigrams rplp;i vectors lid system noise robustness scarce data posteriorgram counts;acoustic signal processing;speech training speech recognition acoustics noise vectors training data;speech recognition acoustic signal processing;speech recognition	This paper presents a description of our system for the Albayzin 2012 LRE competition. One of the main characteristics of this evaluation was the reduced number of available files for training the system, especially for the empty condition where no training data set was provided but only a development set. In addition, the whole database was created from online videos and around one third of the training data was labeled as noisy files. Our primary system was the fusion of three different i-vector based systems: one acoustic system based on MFCCs, a phonotactic system using trigrams of phone-posteriorgram counts, and another acoustic system based on RPLPs that improved robustness against noise. A contrastive system that included new features based on the glottal source was also presented. Official and post-evaluation results for all the conditions using the proposed metrics for the evaluation and the Cavg metric are presented in the paper.	acoustic cryptanalysis;language identification;software metric;test set;trigram;video clip	Luis Fernando D'Haro;Ricardo de Córdoba;Miguel Ánguel Caraballo;José Manuel Pardo	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6638989	speech recognition;computer science	Robotics	-16.10965051890816	-90.92950936474479	186190
45eaee3a3dbfafad35e1121c1b616c206b3ad1e6	support vector machine with dynamic time-alignment kernel for speech recognition	hidden markov model;linear time;speaker dependent;speech recognition;support vector machine;sequential pattern;frame synchronization	A new class of Support Vector Machine (SVM) which is applicable to sequential-pattern recognition is developed by incorporating an idea of non-linear time alignment into the kernel. Since time-alignment operation of sequential pattern is embedded in the kernel evaluation, same algorithms with the original SVM for training and classification can be employed without modifications. Furthermore, frame-wise evaluation of kernel in the proposed SVM (DTAK-SVM) enables frame-synchronous recognition of sequential pattern, which is suitable for continuous speech recognition. Preliminary experiments of speakerdependent 6 voiced-consonants recognition demonstrated excellent recognition performance of more than 98% in correct classification rate, whereas 93% by hidden Markov models (HMMs).	algorithm;embedded system;experiment;hidden markov model;kernel (operating system);loudspeaker time alignment;markov chain;nonlinear system;pattern recognition;speech recognition;support vector machine;time complexity	Hiroshi Shimodaira;Ken-ichi Noma;Mitsuru Nakai;Shigeki Sagayama	2001			time complexity;speaker recognition;support vector machine;kernel method;speech recognition;feature vector;feature;frame synchronization;computer science;machine learning;pattern recognition;relevance vector machine;structured support vector machine;hidden markov model;signature recognition	ML	-16.552137774072623	-90.81037225682205	186697
b94b74fa36bdad71501a32309b371a891ab1a996	a pashtu speakers database using accent and dialect approach		A small scale Pashtu speakersu0027 database with multiple accents and dialects has been developed to use in Pashtu Speaker Identification Systems (SIS) and accents and dialect identification systems. Pashtu is a major spoken language of Pakistan and Afghanistan. At present, it has become very prominent worldwide due to its regional importance. The regions of Pakistan and Afghanistan where Pashtu is spoken are mostly occupied by the extremists who use Pashtu for their communication. In order to design Pashtu voice-based systems for security and other applications, a database has been designed in which the voice data is collected from 32 native Pashtu speakers of different regions of Pakistan and Afghanistan. Finally, using a subset of the data, Multi-Layer Perceptron (MLP)-based SIS has been designed. The designed system achieved overall 87.5% identification accuracy and outperformed the recently proposed i-vector and GMM-based accent identification systems by showing 3.8% and 12.0% relative improvement respec...		Shahid Munir Shah;Shahzad Ahmed Memon;Khalil-ur-Rehman Khoumbati;Muhammad Moinuddin	2017	IJAPR	10.1504/IJAPR.2017.10010248	perceptron;pashtu language;database;mel-frequency cepstrum;spoken language;computer science	Vision	-15.785337598299563	-88.78220165795027	186968
03edd8d878db7c87b15c33a66936acd1fd16d085	robust speech analysis by lag-weighted linear prediction	lag weighted linear prediction method;time domain analysis speech processing stability;spectrum analysis;signal analysis;speech processing;speech;autocorrelation based weighting scheme lag weighted linear prediction method linear predictive robust spectrum analysis time domain properties signal analysis stabilization operation euclidean mfcc distortion additive noise conditions robustness;spectrum analysis linear prediction;noise measurement;time domain analysis;stability;mel frequency cepstral coefficient;additive noise conditions;time domain properties;speech recognition;robustness;mel frequency cepstral coefficient speech robustness noise noise measurement spectral analysis speech recognition;linear predictive robust spectrum analysis;linear prediction;spectral analysis;euclidean mfcc distortion;autocorrelation based weighting scheme;stabilization operation;noise	This study introduces an approach for linear predictive spectrum analysis based on emphasizing selected time-domain properties in the analyzed signal in combination with a stabilization operation. A stable weighted linear predictive method based on a novel autocorrelation-based weighting scheme is described and its spectral properties are demonstrated. The robustness of the proposed method is compared with conventional techniques in terms of an Euclidean MFCC distortion measure in different additive noise conditions. In the experimental evaluation, the novel speech analysis technique outperforms the other evaluated methods.	additive white gaussian noise;autocorrelation;distortion;utility functions on indivisible goods;voice analysis	Jouni Pohjalainen;Paavo Alku	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288908	spectrum analyzer;speech recognition;stability;linear prediction;computer science;noise measurement;noise;speech;signal processing;pattern recognition;speech processing;mathematics;statistics;robustness	Robotics	-12.23120997828004	-91.53942414792346	187431
0489864a8177cdef6af574c672bd27a68ed260c2	learning model-based f0 production through goal-directed babbling	learning regimens learning model based f0 production surface acoustics articulatory commands speech acquisition stochastic optimization pitch targets quantitative target approximation model qta acoustic to articulatory inverse model inverse kinematics learning developmental robotics traditionally separated babbling speech acquisition imitation stages unified acoustic goal directed babbling process multilayer perceptron mlp articulatory command space self generated examples;production speech adaptation models acoustics space exploration robot sensing systems stochastic processes;f0 contour modeling;acoustic to articulatory inversion;online learning;sensorimotor coordination;speech acquisition;speech processing learning artificial intelligence multilayer perceptrons optimisation;online learning f0 contour modeling target approximation speech production speech acquisition acoustic to articulatory inversion sensorimotor coordination;speech production;target approximation	How surface acoustics can be mapped to underlying articulatory commands is a central yet unsolved issue about speech acquisition. Previously, stochastic optimization has been shown to be proficient in learning underlying pitch targets of the quantitative Target Approximation (qTA) model. The present study tested whether it is possible to develop an acoustic-to-articulatory inverse model for qTA by taking advantages of a recent advance in inverse kinematics learning in the field of developmental robotics, known as goal babbling. By treating traditionally separated babbling and imitation stages of speech acquisition as a unified acoustic goal-directed babbling process, the inverse model implemented by a multilayer perceptron (MLP) can be bootstrapped rapidly without the necessity of exploring the whole articulatory command space. The MLP was trained in online mode with self-generated examples obtained after every production of the host learner. The results show that with this novel learning paradigm the inverse model can be improved in a progressive manner and underlying pitch targets can be obtained by querying the mature inverse model. Our findings also demonstrate that qTA is an intrinsically robust F0 production model that can be operated by various learning regimens.	acoustic cryptanalysis;approximation;developmental robotics;inverse kinematics;mathematical optimization;memory-level parallelism;multilayer perceptron;pitch (music);programming paradigm;speech acquisition;stochastic optimization	Hao Liu;Yi Xu	2014	The 9th International Symposium on Chinese Spoken Language Processing	10.1109/ISCSLP.2014.6936720	natural language processing;speech production;speech recognition;computer science;machine learning;linguistics	Robotics	-18.433449745742926	-93.82037204477282	187499
00b8e6387b466bca7ecb5c8fa4e86627f1bfd407	improved semi-parametric mean trajectory model using discriminatively trained centroids	semi parametric mean trajectory model;conditional independence;time varying;discriminatively trained centroids;maximum likelihood;hidden markov model;minimum phone error;acoustics;acoustic modeling;training;speech;indexing terms;maximum likelihood estimation;maximum likelihood estimation semi parametric mean trajectory model discriminatively trained centroids hidden markov models speech recognition time varying information;maximum likelihood estimate;hidden markov models;trajectory;error rate;speech recognition;discriminative training;time varying information;speech recognition hidden markov models maximum likelihood estimation;hidden markov models gaussian processes speech recognition maximum likelihood estimation error analysis radio access networks laboratories acoustics vocabulary mutual information	"""In order to alleviate the limitation of """"state output probability conditional independence"""" assumption held by Hidden Markov models (HMMs) in speech recognition, a discriminative semi-parametric trajectory model was proposed in recent years, in which both means and variances in the acoustic models are modeled as time-varying variables. The time- varying information is modeled as a weighted contribution from all the """"centroids"""", which can be viewed as the representation of the acoustic space. In previous literatures, such centroids are often obtained by clustering the Gaussians in the baseline acoustic models to some reasonable number or by training a baseline model with fewer Gaussian components. The centroids obtained in this way are maximum likelihood estimation of the acoustic space, which are relatively weak in discriminability compared to the discriminatively trained acoustic models. In this paper, we proposed an improved semi-parametric mean trajectory model training framework, in which the centroids are first discriminatively trained by minimum phone error criterion to provide a more discriminative representation of the acoustic space. This method was evaluated on the Mandarin digit string recognition task. The experimental result shows that our proposed method improves the recognition performance by a relative string error rate reduction of 7.5% compared to the traditional discriminative semi-parametric trajectory model, and it outperforms the baseline acoustic model trained with maximum likelihood criterion by a relative string error rate reduction of 28.6%."""	acoustic cryptanalysis;acoustic model;baseline (configuration management);cluster analysis;discriminative model;hidden markov model;markov chain;semiconductor industry;speech recognition;super robot monkey team hyperforce go!	Ran Xu;Jielin Pan;Yonghong Yan	2008	2008 6th International Symposium on Chinese Spoken Language Processing	10.1109/CHINSL.2008.ECP.63	speech recognition;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;hidden markov model;statistics	ML	-18.998181898763452	-91.33635797692415	188074
96492cf200e337a8c6515e30162af13d566b7914	eigen-mllr coefficients as new feature parameters for speaker identification			coefficient;eigen (c++ library);speaker recognition	Nick Jui-Chang Wang;Wei-Ho Tsai;Lin-Shan Lee	2001			artificial intelligence;speech recognition;pattern recognition;speaker recognition;speaker diarisation;computer science	NLP	-14.010443796650394	-87.74696650538786	188517
73ebb34a17d112f8e735e895ae0ffcd3f94a6685	improving the performance of acoustic event classification by selecting and combining information sources using the fuzzy integral	information sources;fuzzy measure;vocal tract;edge extraction;fuzzy integral;limit set;classification system;feature selection;support vector machine;non speech sound	Acoustic events produced in meeting-room-like environments may carry information useful for perceptually aware interfaces. In this paper, we focus on the problem of combining different information sources at different structural levels for classifying human vocal-tract non-speech sounds. The Fuzzy Integral (FI) approach is used to fuse outputs of several classification systems, and feature selection and ranking are carried out based on the knowledge extracted from the Fuzzy Measure (FM). In the experiments with a limited set of training data, the FI-based decision-level fusion showed a classification performance which is much higher than the one from the best single classifier and can surpass the performance resulting from the integration at the featurelevel by Support Vector Machines. Although only fusion of audio information sources is considered in this work, the conclusions may be extensible to the multi-modal case.	acoustic cryptanalysis;baseline (configuration management);experiment;fm broadcasting;feature selection;formal system;fuzzy measure theory;information source;modal logic;oracle fusion architecture;statistical classification;support vector machine;tract (literature)	Andrey Temko;Dusan Macho;Climent Nadeu	2005		10.1007/11677482_31	vocal tract;limit set;support vector machine;speech recognition;computer science;machine learning;pattern recognition;mathematics;feature selection	AI	-12.801543877780329	-88.28060205754774	189729
1625d88945cfc8e37d61357f2294a2f5df84d7d6	differentiable pooling for unsupervised acoustic model adaptation	acoustics;training;speech;hidden markov models;neural networks adaptation differentiable pooling;transforms;word error rates reductions unsupervised acoustic model adaptation differentiable pooling deep neural network dnn acoustic model differentiable pooling operators decision boundaries pooling parametrisations weighted gaussian pooling vocabulary speech recognition corpora switchboard conversational telephone speech;adaptation models;unsupervised learning gaussian processes neural nets speech recognition;adaptation models acoustics hidden markov models speech data models training transforms;data models	We present a deep neural network DNN acoustic model that includes parametrised and differentiable pooling operators. Unsupervised acoustic model adaptation is cast as the problem of updating the decision boundaries implemented by each pooling operator. In particular, we experiment with two types of pooling parametrisations: learned Lp-norm pooling and weighted Gaussian pooling, in which the weights of both operators are treated as speaker-dependent. We perform investigations using three different large vocabulary speech recognition corpora: AMI meetings, TED talks, and Switchboard conversational telephone speech. We demonstrate that differentiable pooling operators provide a robust and relatively low-dimensional way to adapt acoustic models, with relative word error rates reductions ranging from 5-20% with respect to unadapted systems, which themselves are better than the baseline fully-connected DNN-based acoustic models. We also investigate how the proposed techniques work under various adaptation conditions including the quality of adaptation data and complementarity to other feature- and model-space adaptation methods, as well as providing an analysis of the characteristics of each of the proposed approaches.	acoustic cryptanalysis;acoustic model;artificial neural network;baseline (configuration management);complementarity theory;deep learning;speech recognition;telephone switchboard;text corpus;vocabulary	Pawel Swietojanski;Steve Renals	2016	IEEE/ACM Transactions on Audio, Speech, and Language Processing	10.1109/TASLP.2016.2584700	data modeling;speech recognition;acoustics;computer science;speech;machine learning;pattern recognition;linguistics;hidden markov model	ML	-17.881329089398342	-89.20558658388211	190242
cd55a960686f721b5deb752798a8ff15a7b69b77	gender identification using mfcc for telephone applications - a comparative study		— Gender recognition is an essential component of automatic speech recognition and interactive voice response systems. Determining gender of the speaker reduces the computational burden of such systems for any further processing. Typical methods for gender recognition from speech largely depend on features extraction and classification processes. The purpose of this study is to evaluate the performance of various state-of-the-art classification methods along with tuning their parameters for helping selection of the optimal classification methods for gender recognition tasks. Five classification schemes including k-nearest neighbor, naïve Bayes, multilayer perceptron, random forest, and support vector machine are comprehensively evaluated for determination of gender from telephonic speech using the Mel-frequency cepstral coefficients. Different experiments were performed to determine the effects of training data sizes, length of the speech streams, and parameter tuning on classification performance. Results suggest that SVM is the best classifier among all the five schemes for gender recognition.	coefficient;experiment;interactive voice response;k-nearest neighbors algorithm;mel-frequency cepstrum;multilayer perceptron;naive bayes classifier;naivety;random forest;speech recognition;support vector machine	Jamil Ahmad;Mustansar Fiaz;Soon-il Kwon;Maleerat Sodanil;Bay Vo;Sung Wook Baik	2016	CoRR		speaker recognition;speech recognition;computer science;machine learning;pattern recognition	NLP	-12.470237171850806	-88.6320396079873	190526
3ae72e6bffd2b79943ee5fedf82e47699dfbb088	soft margin feature extraction for automatic speech recognition	hidden markov model;model generation;acoustic modeling;indexing terms;automatic speech recognition;discriminative feature extraction;discrimination learning;feature extraction;soft margin estimation	We propose a new discriminative learning framework, called soft margin feature extraction (SMFE), for jointly optimizing the parameters of transformation matrix for feature extraction and of hidden Markov models (HMMs) for acoustic modeling. SMFE extends our previous work of soft margin estimation (SME) to feature extraction. Tested on the TIDIGITS connected digit recognition task, the proposed approach achieves a string accuracy of 99.61%, much better than our previously reported SME results. To our knowledge, this is the first study on applying the margin-based method in joint optimization of feature extraction and acoustic modeling. The excellent performance of SMFE demonstrates the success of soft margin based method, which targets to obtain both high accuracy and good model generalization.	acoustic cryptanalysis;acoustic model;feature extraction;hidden markov model;markov chain;mathematical optimization;speech recognition;transformation matrix	Jinyu Li;Chin-Hui Lee	2007			speech recognition;index term;feature extraction;computer science;machine learning;pattern recognition;hidden markov model;discrimination learning	ML	-19.072508065547957	-91.457354474062	190664
58e7e2b930b00f8904157d71cd799348d72586c8	plda in the i-supervector space for text-independent speaker verification	signal image and speech processing;acoustics;i supervector;mathematics in music;i vector;speaker verification;engineering acoustics;probabilistic linear discriminant analysis	In this paper, we advocate the use of the uncompressed form of i-vector and depend on subspace modeling using probabilistic linear discriminant analysis (PLDA) in handling the speaker and session (or channel) variability. An i-vector is a low-dimensional vector containing both speaker and channel information acquired from a speech segment. When PLDA is used on an i-vector, dimension reduction is performed twice: first in the i-vector extraction process and second in the PLDA model. Keeping the full dimensionality of the i-vector in the i-supervector space for PLDA modeling and scoring would avoid unnecessary loss of information. We refer to the uncompressed i-vector as the i-supervector. The drawback in using the i-supervector with PLDA is the inversion of large matrices in the estimation of the full posterior distribution, which we show can be solved rather efficiently by portioning large matrices into smaller blocks. We also introduce the Gaussianized rank-norm, as an alternative to whitening, for feature normalization prior to PLDA modeling. We found that the i-supervector performs better during normalization. A better performance is obtained by combining the i-supervector and i-vector at the score level. Furthermore, we also analyze the computational complexity of the i-supervector system, compared with that of the i-vector, at four different stages of loading matrix estimation, posterior extraction, PLDA modeling, and PLDA scoring.	computational complexity theory;dimensionality reduction;heart rate variability;linear discriminant analysis;speaker recognition;whitening transformation	Ye Jiang;Kong-Aik Lee;Longbiao Wang	2014	EURASIP J. Audio, Speech and Music Processing	10.1186/s13636-014-0029-2	speech recognition;acoustics;computer science;machine learning;pattern recognition;physics	ML	-16.819154099604024	-92.26882475640646	190760
67efaba1be4c0462a5fc2ce9762f7edf9719c6a0	speech emotion recognition using hidden markov models	human communication;hidden markov model;voice quality;performance of systems;log frequency power coefficients;emotion recognition;system performance;mel frequency cepstral coefficient;recognition of emotion;speech recognition;linear predictive cepstral coefficients;fundamental frequency;emotional speech	In emotion classification of speech signals, the popular features employed are statistics of fundamental frequency, energy contour, duration of silence and voice quality. However, the performance of systems employing these features degrades substantially when more than two categories of emotion are to be classified. In this paper, a text independent method of emotion classification of speech is proposed. The proposed method makes use of short time log frequency power coefficients (LFPC) to represent the speech signals and a discrete hidden Markov model (HMM) as the classifier. The emotions are classified into six categories. The category labels used are, the archetypal emotions of Anger, Disgust, Fear, Joy, Sadness and Surprise. Adatabase consisting of 60 emotional utterances, each from twelve speakers is constructed and used to train and test the proposed system. Performance of the LFPC feature parameters is compared with that of the linear prediction Cepstral coefficients (LPCC) and mel-frequency Cepstral coefficients (MFCC) feature parameters commonly used in speech recognition systems. Results show that the proposed system yields an average accuracy of 78% and the best accuracy of 96% in the classification of six emotions. This is beyond the 17% chances by a random hit for a sample set of 6 categories. Results also reveal that LFPC is a better choice as feature parameters for emotion classification than the traditional feature parameters. 2003 Elsevier B.V. All rights reserved.	coefficient;emotion recognition;hidden markov model;markov chain;mel-frequency cepstrum;sadness;speech recognition	Tin Lay Nwe;Say Wei Foo;Liyanage C. De Silva	2003	Speech Communication	10.1016/S0167-6393(03)00099-2	speech recognition;phonation;computer science;pattern recognition;linguistics;fundamental frequency;human communication;hidden markov model	AI	-12.473319774016977	-88.39870203068334	190787
cc4237cdbf62df52d570ded5100e6ff0e3b33eb5	a statistical sample-based approach to gmm-based voice conversion using tied-covariance acoustic models		This paper presents a novel statistical sample-based approach for Gaussian Mixture Model (GMM)-based Voice Conversion (VC). Although GMM-based VC has the promising flexibility of model adaptation, quality in converted speech is significantly worse than that of natural speech. This paper addresses the problem of inaccurate modeling, which is one of the main reasons causing the quality degradation. Recently, we have proposed statistical sample-based speech synthesis using rich context models for high-quality and flexible Hidden Markov Model (HMM)-based Text-To-Speech (TTS) synthesis. This method makes it possible not only to produce high-quality speech by introducing ideas from unit selection synthesis, but also to preserve flexibility of the original HMM-based TTS. In this paper, we apply this idea to GMM-based VC. The rich context models are first trained for individual joint speech feature vectors, and then we gather them mixture by mixture to form a Rich context-GMM (R-GMM). In conversion, an iterative generation algorithm using R-GMMs is used to convert speech parameters, after initialization using over-trained probability distributions. Because the proposed method utilizes individual speech features, and its formulation is the same as that of conventional GMMbased VC, it makes it possible to produce high-quality speech while keeping flexibility of the original GMM-based VC. The experimental results demonstrate that the proposed method yields significant improvements in term of speech quality and speaker individuality in converted speech. key words: GMM-based voice conversion, sample-based speech synthesis, speech parameter conversion, rich context model	acoustic cryptanalysis;acoustic model;algorithm;elegant degradation;google map maker;hidden markov model;iterative method;javaserver pages;markov chain;mixture model;modulation;natural language;netware file system;speech synthesis	Shinnosuke Takamichi;Tomoki Toda;Graham Neubig;Sakriani Sakti;Satoshi Nakamura	2016	IEICE Transactions		speech recognition	NLP	-17.606540642586097	-91.86938019239246	191313
491d6df12dc6766bed1e1f7bde5745d92a8bd25a	acoustic vector resampling for gmmsvm-based speaker verification		This paper proposes a resampling technique to mitigate the data imbalance problem in GMMSVM-based speaker verification. The sequence order of acoustic vectors in an enrollment utterance is first randomized; then the randomized sequence is partitioned into a number of segments. Each of these segments is then used to produce a GMM-supervector. A desirable number of speaker-class supervectors can be produced by repeating this randomization and partitioning process a number of times. Evaluations suggest that the method can reduce the EER of GMM-SVM systems by 10%. Results	acoustic cryptanalysis;enhanced entity–relationship model;google map maker;randomized algorithm;resampling (statistics);speaker recognition	Man-Wai Mak;Wei Rao	2010			concatenation;artificial intelligence;resampling;randomization;pattern recognition;speaker diarisation;support vector machine;computer science;utterance	Logic	-16.35694330567039	-91.60910712939864	191347
953e69c66f2bdfcc65e4d677fa429571cdec2a60	emotion recognition in speech using neural networks	base donnee;independance;learning;reconocimiento palabra;speech processing;database;tratamiento palabra;traitement parole;base dato;neural network architecture;emotion recognition;aprendizaje;independence;apprentissage;speaker independent;independencia;feature extraction;contexto;architecture reseau neuronal;contexte;arquitectura;speech recognition;reconnaissance parole;extraction caracteristique;reseau neuronal;speaker;locutor;architecture;red neuronal;context;reconnaissance emotion;locuteur;neural network	Emotion recognition in speech is a topic on which little research has been done to-date. In this paper, we discuss why emotion recognition in speech is a significant and applicable research topic, and present a system for emotion recognition using oneclass-in-one neural networks. By using a large database of phoneme balanced words, our system is speakerand context-independent. We achieve a recognition rate of approximately 50% when testing eight emotions.	alexandre oliva;answer to reset;automatic target recognition;consciousness;emotion recognition;neural networks	Joy Nicholson;K. Takahashi;Ryohei Nakatsu	2000	Neural Computing & Applications	10.1007/s005210070006	independence;speaker recognition;speech recognition;feature extraction;computer science;artificial intelligence;architecture;machine learning;speech processing;artificial neural network	NLP	-15.683292908585926	-87.28659397402177	191430
4595f387f69ac057b44881aa4d7787620c9d7ae3	noise suppression based onwavelet packet decomposition and quantile noise estimation for robust automatic speech recognition	background noise;noise estimation;robust automatic speech recognition;human perception noise suppression wavelet packet decomposition quantile noise estimation robust automatic speech recognition denoising algorithm machine recognition;snow;working environment noise;noise suppression;speech enhancement;noise robustness;wavelet transforms signal denoising speech recognition;wavelet transforms;automatic speech recognition;noise robustness automatic speech recognition working environment noise signal processing algorithms noise reduction speech recognition background noise snow speech enhancement acoustic noise;denoising algorithm;acoustic noise;noise reduction;wavelet packet decomposition;speech recognition;quantile noise estimation;machine recognition;signal processing algorithms;human perception;signal denoising	In this paper we address the application of a denoising algorithm based on wavelet packet decomposition and quantile noise estimation to noise suppression for automatic speech recognition. The denoising algorithm is adapted to suit the different requirements in machine recognition, as compared to human perception, and is tested in combination with state-of-the-art speech recognition systems. The results show, that, if the proposed algorithm is integrated with the recognition system - including the training process - a performance comparable to recent high-quality noise suppression methods is achieved	algorithm;network packet;noise reduction;requirement;speech recognition;wavelet packet decomposition;zero suppression	Erhard Rank;Tuan Van Pham;Gernot Kubin	2006	2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings	10.1109/ICASSP.2006.1660061	snow;speech recognition;computer science;noise measurement;noise;pattern recognition;noise reduction;background noise;wavelet packet decomposition;perception;wavelet transform	Robotics	-12.918352651738347	-90.77658123167095	191486
ac0b6d44202e44a212f0c29d08cea1d66192fee7	multi-source neural networks for speech recognition	error reduction;probability;hidden markov model;multilayer perceptrons;neural net architecture;state estimation;neural networks speech recognition artificial neural networks cepstral analysis hidden markov models stochastic processes speech processing signal processing feature extraction mel frequency cepstral coefficient;hidden markov models;feature extraction;speech recognition;telephone quality test set multi source neural networks heterogeneous input features mel based cepstral coefficients rasta plp cepstral coefficients;artificial neural network;neural network;probability speech recognition feature extraction neural net architecture hidden markov models multilayer perceptrons state estimation	"""In speech recognition the most diffused technology (Hidden Markov Models) is constrained by the condition of stochastic independence of its input features. That limits the simultaneous use of features derived fiom the speech signal with dyerent processing algorithms. On the contrary Artificial Neural Networks (A"""") are capable of incorporating multiple heterogeneous input features, which do not need to be treated as independent, finding the optimal combination of these features for classification. The purpose of this work is the exploitation of this characteristic of AhW to improve the speech recognition accuraq through the combined use of input features coming fiom dyerent sources (different feature extraction algorithms). In this work we integrate two input sources: the Me1 based Cepstral Coeflcients (MFCC) derived fiom FFT and fhe RASTA-PLP Cepstral Coeflcients. The results show that this integration leads to an error reduction of 26% on a telephone quality test set."""	algorithm;cepstrum;fast fourier transform;feature extraction;hidden markov model;markov chain;neural networks;pl/p;speech recognition;test set	Roberto Gemello;Dario Albesano;Franco Mana	1999		10.1109/IJCNN.1999.835942	speech recognition;feature extraction;computer science;machine learning;pattern recognition;probability;time delay neural network;artificial neural network;hidden markov model	ML	-16.771334036345394	-89.75800894591875	191897
4b54c8d1f5010849e26b6cd5499e069b2245c811	speaker recognition with region-constrained mllr transforms	support vector machines;transforms speech speaker recognition cepstral analysis speech recognition feature extraction adaptation models;speech;region constrained speaker modeling speaker recognition mllr svm;indexing terms;maximum likelihood estimation;speaker verification;support vector machines feature extraction maximum likelihood estimation regression analysis speaker recognition;speaker recognition;automatic speech recognition;cepstral analysis;feature extraction;support vector machine region constrained mllr transform speaker recognition model speaker adaptation transform region constrained feature extraction phonetic criteria prosodic criteria automatic speech recognition sre2010 speaker verification maximum likelihood linear regression;transforms;speech recognition;regression analysis;adaptation models;speaker adaptation;region constrained speaker modeling;mllr svm	It has been shown that standard cepstral speaker recognition models can be enhanced by region-constrained models, where features are extracted only from certain speech regions defined by linguistic or prosodic criteria. Such region-constrained models can capture features that are more stable, highly idiosyncratic, or simply complementary to the baseline system. In this paper we ask if another major class of speaker recognition models, those based on MLLR speaker adaptation transforms, can also benefit from region-constrained feature extraction. In our approach, we define regions based on phonetic and prosodic criteria, based on automatic speech recognition output, and perform MLLR estimation using only frames selected by these criteria. The resulting transform features are appended to those of a state-of-the-art MLLR speaker recognition system and jointly modeled by SVMs. Multiple regions can be added in this fashion. We find consistent gains over the baseline system in the SRE2010 speaker verification task.	baseline (configuration management);cepstrum;feature extraction;speaker recognition;speech recognition	Andreas Stolcke;Arindam Mandal;Elizabeth Shriberg	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6288894	natural language processing;speaker recognition;support vector machine;speaker diarisation;speech recognition;index term;feature extraction;computer science;speech;pattern recognition;maximum likelihood;regression analysis	Vision	-17.962200921414304	-91.61781793473993	192111
44e03a6907f124545f780f4a5105614993622258	discriminative training of gmm using a modified em algorithm for speaker recognition	em algorithm;speaker recognition	In this paper, we present a new discriminative training method for Gaussian Mixture Models (GMM) and its application for the text-independent speaker recognition. The objective of this method is to maximize the frame level normalized likelihoods of the training data. That is why we call it the Maximum Normalized Likelihood Estimation (MNLE). In contrast to other discriminative algorithms, the objective function is optimized using a modi ed Expectation-Maximization (EM) algorithm which greatly simpli es the training procedure. The evaluation experiments using both clean and telephone speech showed improvement of the recognition rates compared to the Maximum Likelihood Estimation (MLE) trained speaker models, especially when the mismatch between the training and testing conditions is signi cant.	discriminative model;expectation–maximization algorithm;experiment;google map maker;linuxmce;loss function;mixture model;optimization problem;speaker recognition;statistical classification;teaching method;whole earth 'lectronic link	Konstantin Markov;Seiichi Nakagawa	1998			speech recognition;discriminative model;artificial intelligence;pattern recognition;speaker recognition;expectation–maximization algorithm;computer science	NLP	-18.349489199945282	-91.97391804720553	192215
9bdd51b41b73665aa548ca1ff4164f258eb31ed7	improved feature vector normalization for noise robust connected speech recognition	feature vector;speech recognition	Feature vector normalization has been successfully used to improve the noise robustness of speech recognizers. Unfortunately, it may cause additional insertion errors in connected digit recognition in clean environments. We propose two methods to reduce the number of insertions. Based on estimated instantaneous signal-to-noise ratio we form a reliability measure for the recognized digits. We discard unreliable digits from the beginning and the end of the recognized digit sequence. Since the proposed reliability hypotheses are independent of the likelihoods produced by an HMM classifier, we are capable of bringing new useful information into the classification process. In addition, we constrain the normalization process on the basis of statistics obtained from the training data. Experimental results show that we are capable of achieving an average 32% string level error rate reduction in simulations of a noisy car environment.	feature vector;finite-state machine;hidden markov model;signal-to-noise ratio;simulation;speech recognition;speech synthesis	Juha Häkkinen;Janne Suontausta;Ramalingam Hariharan;Marcel Vasilache;Kari Laurila	1999			robustness (computer science);word error rate;feature (computer vision);speech recognition;feature (machine learning);artificial intelligence;pattern recognition;normalization (statistics);hidden markov model;feature vector;computer science;classifier (linguistics)	ML	-14.757209437375378	-92.03237843600311	193092
efc4aefa7577760a1563bd7b7bbe8912fe3a1e05	a rank based metric of anchor models for speaker verification	speaker recognition audio databases indexing;speaker verification;speaker recognition;testing indexing audio databases robustness educational institutions computer science large scale systems euclidean distance principal component analysis linear discriminant analysis;indexing;indexation;audio databases;yoho database anchor speaker model speaker verification speaker indexing audio database rank based metric	In this paper, we present an improved method of anchor models for speaker verification. Anchor model is the method that represent a speaker by his relativity of a set of other speakers, called anchor speakers. It was firstly introduced for speaker indexing in large audio database. We suggest a rank based metric for the measurement of speaker character vectors in anchor model. Different from conventional metric methods which consider each anchor speaker equally and compare the log likelihood scores directly, in our method the relative order of anchor speakers is exploited to characterize target speaker. We have taken experiments on the YOHO database. The results show that EER of our method is 13.29% lower than that of conventional metric. Also, our method is more robust against the mismatching between test set and anchor set	enhanced entity–relationship model;experiment;speaker recognition;test set	Yingchun Yang;Min Yang;Zhaohui Wu	2006	2006 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2006.262726	speaker recognition;search engine indexing;speaker diarisation;speech recognition;computer science;machine learning;pattern recognition	Robotics	-15.638713666157983	-93.10449713176344	193207
86a3259d3bbb41b06ccc54a643ae7028177ea312	consistent dnn uncertainty training and decoding for robust asr		We consider the problem of robust automatic speech recognition (ASR) in noisy conditions. The performance improvement brought by speech enhancement is often limited by residual distortions of the enhanced features, which can be seen as a form of statistical uncertainty. Uncertainty estimation and propagation methods have recently been proposed to improve the ASR performance with deep neural network (DNN) acoustic models. However, the performance is still limited due to the use of uncertainty only during decoding. In this paper, we propose a consistent approach to account for uncertainty in the enhanced features during both training and decoding. We estimate the variance of the distortions using a DNN uncertainty estimator that operates directly in the feature maximum likelihood linear regression (fMLLR) domain and we then sample the uncertain features using the unscented transform (UT). We report the resulting ASR performance on the CHiME-2 and CHiME-3 datasets for different uncertainty estimation/propagation techniques. The proposed DNN uncertainty training method brings 4% and 8% relative improvement on these two datasets, respectively, compared to a competitive fMLLR-domain DNN acoustic modeling baseline.	acoustic cryptanalysis;acoustic model;artificial neural network;automated system recovery;baseline (configuration management);decoding methods;deep learning;distortion;software propagation;speech enhancement;speech recognition;teaching method	Karan Nathwani;Emmanuel Vincent;Irina Illina	2017	2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)	10.1109/ASRU.2017.8268934	unscented transform;residual;estimator;artificial neural network;linear regression;artificial intelligence;decoding methods;machine learning;speech enhancement;engineering;pattern recognition;fmllr	AI	-15.14655863713821	-90.8864768802713	194077
1e70d9f45f915c275c8a86e999e9fa09b718a40a	comparison of single-model and multiple-model prediction-based audiovisual fusion		Prediction-based fusion is a recently proposed audiovisual fusion approach which outperforms feature-level fusion on laughter-vs-speech discrimination. One set of predictive models is trained per class which learns the audio-to-visual and visual-to-audio feature mapping together with the time evolution of audio and visual features. Classification of a new input is performed via prediction. All the class predictors produce a prediction of the expected audio / visual features and their prediction errors are combined for each class. The model which best describes the audiovisual feature relationship, i.e., results in the lowest prediction error, provides its label to the input. In all the previous works, a single set of predictors was trained on the entire training set for each class. In this work, we investigate the use of multiple sets of predictors per class. The main idea is that since models are trained on clusters of data, they will be more specialised and they will produce lower prediction errors which can in turn enhance the classification performance. We experimented with subject-based clustering and clustering based on different types of laughter, voiced and unvoiced. Results are presented on laughter-vs-speech discrimination on a cross-database experiment using the AMI and MAHNOB databases. The use of multiple sets of models results in a significant performance increase with the latter clustering approach achieving the best performance. Overall, an increase of over 4% and 10% is observed for F1 speech and laughter, respectively, for both datasets.	cluster analysis;database;predictive modelling;test set	Stavros Petridis;Varun Rajgarhia;Maja Pantic	2015			speech recognition;computer science;machine learning;pattern recognition	ML	-12.640838168476748	-88.93753660851436	194167
b2e0aba8599bcdb0c147f83f435f0c800ac68f9c	confidence- and margin-based mmi/mpe discriminative training for off-line handwriting recognition	maximum mutual information;minimum phone error;margin;model adaptation;hidden markov models;arabic;discriminative training;confidences	We present a novel confidence- and margin-based discriminative training approach for model adaptation of a hidden Markov model (HMM)-based handwriting recognition system to handle different handwriting styles and their variations. Most current approaches are maximum-likelihood (ML) trained HMM systems and try to adapt their models to different writing styles using writer adaptive training, unsupervised clustering, or additional writer-specific data. Here, discriminative training based on the maximum mutual information (MMI) and minimum phone error (MPE) criteria are used to train writer-independent handwriting models. For model adaptation during decoding, an unsupervised confidence-based discriminative training on a word and frame level within a two-pass decoding process is proposed. The proposed methods are evaluated for closed-vocabulary isolated handwritten word recognition on the IFN/ENIT Arabic handwriting database, where the word error rate is decreased by 33% relative compared to a ML trained baseline system. On the large-vocabulary line recognition task of the IAM English handwriting database, the word error rate is decreased by 25% relative.	artificial neural network;baseline (configuration management);cluster analysis;discriminative model;feature extraction;handwriting recognition;hidden markov model;identity management;iterative method;loss function;margin classifier;markov chain;mathematical optimization;memory-level parallelism;mutual information;nonlinear system;online and offline;robustness (computer science);transcription (software);vocabulary;word error rate	Philippe Dreuw;Georg Heigold;Hermann Ney	2011	International Journal on Document Analysis and Recognition (IJDAR)	10.1007/s10032-011-0160-x	margin;speech recognition;computer science;machine learning;arabic;pattern recognition;hidden markov model	NLP	-19.04936836103594	-91.6969640604418	194202
b92e3d7a83816e10cd933ab1bfc9ba19b49d254c	adaptive voice-quality control based on one-to-many eigenvoice conversion	voice quality;voice conversion;unsupervised adaptation;eigenvoices;voice quality control	This paper presents adaptive voice-quality con仕01 methods based on one-to-many eigenvoice conversion. To intuitively con汀01 the converted voice quality by manipulating a small number of con仕01 parameters, a multiple regression Gaussian mixture l110del (MR-GMM) has been proposed. The MR-GMM also allows us to estimate the optimum conむ01 parameters if target speech samples are available. However, its adaptation performance is Iimited because the nUl11ber of control paral11e ters is too small to widely model voice quality of various target speakers. To improve the adaptation perfomlance while keep ing capability of voice-quality control, this paper proposes an extended MR-GMM (EMR-GMM) with additional adaptive pa­ rameters to extend a subspace modeling target voice司uality. Experimental results demonstrate that the EMR-GMM yields signifì.cant improvements of the adaptation perfomlance while allowing us to intuitively control the conveJted voice quality.	excalibur: morgana's revenge;google map maker;mixture model;one-to-many (data model)	Kumi Ohta;Tomoki Toda;Yamato Ohtani;Hiroshi Saruwatari;Kiyohiro Shikano	2010			speech recognition;phonation;telecommunications;linguistics	Robotics	-17.57026602365861	-92.84200077861749	194608
6a98cd45bb90d22c3b28632dad1aa3fff73fadd3	a speech enhancement method based on multi-task bayesian compressive sensing			compressed sensing;speech enhancement	Hanxu You;Zhixian Ma;Wei Li;Jie Zhu	2017	IEICE Transactions		speech recognition;computer science;machine learning;pattern recognition;sparse approximation;compressed sensing	Mobile	-13.971187631267453	-88.07846868308123	194616
ade2ef2f484f256912868ea6edf53eeda73cf066	automatic analysis of audiostreams in the concept drift environment	computational modeling;hidden markov models;feature extraction;signal processing;dictionaries;pattern recognition;adaptation models	Computational Auditory Scene Analysis (CASA) is typically achieved by statistical models trained offline on available data. Their performance relies heavily on the assumption that the process generating the data along with the recording conditions are stationary over time. Nowadays, there is a high demand for methodologies and tools dealing with a series of problems tightly coupled with non-stationary conditions, such as changes in the recording conditions, appearance of unknown audio classes, reverberant effects, etc. This paper unifies these obstacles under a concept drift framework and explores the passive adaptation approach. The overall aim is to learn online the statistical properties of the evolving data distribution and incorporate them into the recognition mechanism for boosting its performance. The proposed CASA system encompasses: a) an approach to discriminate abrupt and gradual concept drifts b) an online adaptation module to both kinds of drifts, and c) a mechanism which automatically updates the dictionary of the audio classes if needed. The proposed framework was evaluated in the auditory analysis of a home environment, based on a combination of professional sound event collections, while we report encouraging experimental results.	computational auditory scene analysis;concept drift;dictionary;emoticon;online and offline;stationary process;statistical model	Stavros Ntalampiras	2016	2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)	10.1109/MLSP.2016.7738905	speech recognition;computer science;artificial intelligence;machine learning	ML	-11.99116858190778	-93.39932993585994	194790
783017e7dad4091873d4f3e842a541d934c088e5	cell phone verification from speech recordings using sparse representation	measurement;vectors audio recording cepstral analysis digital forensics error statistics gaussian processes signal representation smart phones speech recognition;eer source cell phone verification speech recording device recognition digital media forensic recording device identification problem sparse representation gaussian supervectors mel frequency cepstral coefficients mfcc exemplar based dictionary dictionary learning k svd algorithm equal error rate;speech;dictionaries cellular phones speech measurement speech recognition forensics feature extraction;feature extraction;dictionaries;speech recognition;sparse representation digital audio forensic source cell phone verification gaussian supervector;forensics;cellular phones	Source recording device recognition is an important emerging research field of digital media forensic. Most of the prior literature focus on the recording device identification problem. In this study we propose a source cell phone verification scheme based on sparse representation. We employed Gaussian supervectors (GSVs) based on Mel-frequency cepstral coefficients (MFCCs) extracted from the speech recordings to characterize the intrinsic fingerprint of the cell phone. For the sparse representation, both exemplar based dictionary and dictionary learned by K-SVD algorithm were examined to this problem. Evaluation experiments were conducted on a corpus consists of speech recording recorded by 14 cell phones. The achieved equal error rate (EER) demonstrated the feasibility of the proposed scheme.	algorithm;coefficient;dictionary;digital media;enhanced entity–relationship model;experiment;fingerprint;k-svd;mel-frequency cepstrum;mobile phone;singular value decomposition;sparse approximation;sparse matrix;text corpus	Ling Zou;Qianhua He;Xiaohui Feng	2015	2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2015.7178278	natural language processing;speech recognition;feature extraction;computer science;speech;pattern recognition;speech processing;forensic science;measurement	Robotics	-15.406801816607663	-92.62636945709009	194985
8ce80cd64f410deb1ee16619ce3eef0615f1dd5e	robust submodular data partitioning for distributed speech recognition	partitioning algorithms acoustics approximation algorithms robustness distributed databases neural networks training;speech recognition approximation theory learning artificial intelligence neural nets;acoustic modeling distributed deep neural networks automatic speech recognition systems asr systems robust submodular partitioning approach training data disjoint data subsets greedi max minorization maximization submodular data partition problem timit database multi class adaboost;deep neural network;speech recognition;greedi max;robust submodular data partitioning;deep neural network robust submodular data partitioning greedi max minorization maximization speech recognition;minorization maximization	Distributed deep neural networks are commonly employed for building automatic speech recognition (ASR) systems. In this work, we employ the robust submodular partitioning approach, which aims to split the training data into small disjoint data subsets and use each of these subsets to train a particular deep neural network. Two efficient algorithms are used as robust submodular functions [1], namely `Greedi-Max' and `Minorization-Maximization' [2], which are guaranteed to provide tight approximations to the submodular data partition problem. Experiments on TIMIT database show that each of the distributed neural networks trained by the submodular data subset obtains better results than that trained on any subset of data partitioned in a random way., In addition, multi-class adaboost is effectively used to fuse the outputs of the deep neural networks and provides competitive ASR results compared with the traditional ASR system. Besides, the time incurred by acoustic modeling is significantly reduced, which delivers us further benefits.	acoustic cryptanalysis;acoustic model;adaboost;approximation;artificial neural network;automated system recovery;deep learning;distributed database;expectation–maximization algorithm;partition problem;speech recognition;submodular set function;timit	Jun Qi;Javier Tejedor	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472078	speech recognition;computer science;submodular set function;machine learning;pattern recognition	Robotics	-16.492407177090353	-90.77539256766707	195047
4656855f40e6538434ec2aed97773459a35a8e93	texture classification based on multiple gauss mixture vector quantizers	image retrieval texture classification multiple gaussian mixture multiple gauss mixture vector quantizers lloyd algorithm isolated utterance speech recognizer brodatz texture database content based retrieval;lloyd algorithm;image databases;gaussian processes;texture classification;multiple gaussian mixture;image classification;gaussian processes image databases distortion measurement vector quantization speech recognition image retrieval content based retrieval spatial databases frequency measurement sun;low complexity;distortion measurement;frequency measurement;multiple gauss mixture;image texture;generalized lloyd algorithm;isolated utterance speech recognizer;vector quantization;computational complexity;spatial databases;vector quantizers;sun;speech recognition;vector quantizer;learning artificial intelligence;vector quantisation;content based image retrieval;image retrieval image classification image texture vector quantisation learning artificial intelligence visual databases computational complexity content based retrieval;brodatz texture database;content based retrieval;visual databases;image retrieval	We propose a texture classification method using multiple Gauss mixture vector quantizers. We designed a separate model codebook or Gauss mixture for each texture using the generalized Lloyd algorithm with a minimum discrimination information (MDI) distortion based on a training set of data. The multi-codebook structure of GMVQ classifier is an extension to images of the isolated utterance speech recognizer of Shore and Burton. We applied the algorithm to the Brodatz texture database and showed it to be competitive in performance in comparison to other texture classifiers. Its low complexity implementation and real-time operation make the approach suitable for content-based image retrieval. that produces the smallest average distortion when used to code the signal segment. For speech, Shore and Burton used the Itakura-Saito distortion, which is a special case of an MDI distortion [3]. We here consider a 2-D MDI distortion for a similar application recognizing distinct textures by multiple codebook vector quantizers. The MDI distortion is non-Euclidean and tries to match second-order or spectral characteristics of observed data with stored codewords and the chief motivation for the work reported here was the conjecture that such second-order matching should work as well for images as it does for speech. Clustering a training set using the MDI distortion proves to be equivalent to finding an optimum robust classified vector quantizer for a source, which in turn can be interpreted as using clustering to fit a Gauss mixture model to the source [4,5].	algorithm;cluster analysis;code word;codebook;content-based image retrieval;distortion;finite-state machine;itakura–saito distance;kullback–leibler divergence;medium-dependent interface;mixture model;multiple document interface;quantization (signal processing);rate–distortion theory;real-time transcription;speech recognition;statistical classification;test set	Kyungsuk Pyun;Chee Sun Won;Johan Lim;Robert M. Gray	2002		10.1109/ICME.2002.1035657	image texture;computer vision;contextual image classification;speech recognition;image retrieval;computer science;machine learning;pattern recognition;gaussian process;computational complexity theory;vector quantization	Vision	-15.555225697326351	-93.83276032799375	195210
b0ff908b8a71798c16d0ebaa253bd7373a13fb73	reverberant speech segregation based on multipitch tracking and classification	computational auditory scene analysis;reverberation;speech separation;likelihood ratio test reverberant speech segregation multipitch tracking multipitch classification room reverberation computational auditory scene analysis monaural segregation reverberant voiced speech grouping cue;supervised learning;hidden markov model;speech processing;time frequency;supervised classification;speech;interference;system performance;speech labeling interference feature extraction harmonic analysis hidden markov models reverberation;hidden markov models;monaural segregation;supervised learning computational auditory scene analysis casa monaural segregation room reverberation speech separation;feature extraction;computational auditory scene analysis casa;likelihood ratio test;room reverberation;time frequency analysis reverberation speech processing;time frequency analysis;labeling;harmonic analysis	Room reverberation creates a major challenge to speech segregation. We propose a computational auditory scene analysis approach to monaural segregation of reverberant voiced speech, which performs multipitch tracking of reverberant mixtures and supervised classification. Speech and nonspeech models are separately trained, and each learns to map from a set of pitch-based features to a grouping cue which encodes the posterior probability of a time-frequency (T-F) unit being dominated by the source with the given pitch estimate. Because interference may be either speech or nonspeech, a likelihood ratio test selects the correct model for labeling corresponding T-F units. Experimental results show that the proposed system performs robustly in different types of interference and various reverberant conditions, and has a significant advantage over existing systems.	computational auditory scene analysis;interference (communication);machine learning;supervised learning	Zhichao Jin;Dong Wang	2011	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2011.2134086	speech recognition;time–frequency analysis;acoustics;computer science;machine learning;hidden markov model	ML	-11.883703323146326	-91.9936840365276	195259
6611472d5fd4b6503192a8fd06d17c52073249c7	multimodal speaker verification based on electroglottograph signal and glottal activity detection	signal image and speech processing;speaker verification;quantum information technology spintronics	To achieve robust speaker verification, we propose a multimodal method which includes additional nonaudio features and glottal activity detector. As a nonaudio sensor an electroglottograph (EGG) is applied. Parameters of EGG signal are used to augment conventional audio feature vector. Algorithm for EGG parameterization is based on the shape of the idealized waveform and glottal activity detector. We compare our algorithm with conventional one in the term of verification accuracy in high noise environment. All experiments are performed using Gaussian Mixture Model recognition system. Obtained results show a significant improvement of the text-independent speaker verification in high noise environment and opportunity for further improvements in this area.	activity tracker;algorithm;cepstrum;electroglottograph;experiment;feature vector;information;mixture model;modality (human–computer interaction);multimodal interaction;noise (electronics);pc speaker;population;signal-to-noise ratio;speaker recognition;waveform	Zoran Cirovic;Milan M. Milosavljevic;Zoran Dj Banjac	2010	EURASIP J. Adv. Sig. Proc.	10.1155/2010/930376	speech recognition;computer science	EDA	-13.327495269597275	-90.95649798685359	196025
b181e199dab39406c8eb00ff2de3cc11cbb9f658	bayesian speech synthesis framework integrating training and synthesis processes.		This paper proposes a speech synthesis technique integrating training and synthesis processes based on the Bayesian framework. In the Bayesian speech synthesis, all processes are derived from one single predictive distribution which represents the problem of speech synthesis directly. However, it typically assumes that the posterior distribution of model parameters is independent of synthesis data, and this separates the system into training and synthesis parts. This paper removes the approximation and derives an algorithm that the posterior distributions, decision trees and synthesis data are iteratively updated. Experimental results show that the proposed method improves the quality of synthesized speech.	algorithm;approximation;decision tree;speech synthesis	Kei Hashimoto;Yoshihiko Nankaku;Keiichi Tokuda	2010			natural language processing;speech recognition;machine learning	ML	-18.313356052633672	-92.35409870350249	196170
b10fecb214695eebf8546d831fc8755b6de87381	a study of applying subspace based pronunciation modeling in verifying pronunciation accuracy	state level projection vectors;phoneme level mispronunciations;neuromuscular disorders;eer subspace based pronunciation modeling pronunciation accuracy phoneme level mispronunciations utterance impaired children neuromuscular disorders pronunciation verification pv approach subspace based gaussian mixture model sgmm state level projection vectors equal error rate;gaussian processes;speech;utterance;pronunciation accuracy;pronunciation verification;handicapped aids;hidden markov models;vectors;subspace based pronunciation modeling;speech recognition gaussian processes handicapped aids;pv approach;equal error rate;statistics;mathematical model;sgmm;eer;speech recognition;automated speech recognition;medical treatment;subspace based gaussian mixture model;vectors speech hidden markov models mathematical model equations statistics medical treatment;impaired children	This paper investigates a new approach for detecting phoneme level mispronunciations from utterances obtained from impaired children with neuromuscular disorders. This new pronunciation verification (PV) approach is obtained from the subspace based Gaussian mixture model (SGMM) based pronunciation model, where a set of state level projection vectors is applied for representing phonetic variability. SGMM models are trained from disabled speakers' utterances and PV scores are computed directly from distances between disabled and reference speaker projection vectors. An experimental study was performed to evaluate the performance of the SGMM based approach with respect to an approach based on the lattice posterior probabilities. A reduction in equal error rate (EER) of approximately 15% was obtained when the SGMM based scores were combined with lattice posterior probabilities.	enhanced entity–relationship model;experiment;sensor;spatial variability;subspace gaussian mixture model;verification and validation	Shou-Chun Yin;Richard C. Rose;Yun Tang	2012	2012 11th International Conference on Information Science, Signal Processing and their Applications (ISSPA)	10.1109/ISSPA.2012.6310622	natural language processing;speech recognition;computer science;speech;pattern recognition;mathematical model;gaussian process;mathematics;hidden markov model;statistics	Robotics	-18.963415692128006	-89.95072303415517	196430
18a838ea6402db87ad388b38766f5c24c16eaca7	compact acoustic model for embedded implementation	acoustic modeling	An acoustic model for an embedded speech recognition system must exhibit two desirable features; ability to minimize performance degradation in recognition while solving the memory problem under limited system resources. To cope with the challenges, we introduce the state-clustered tied-mixture (SCTM) HMM as an acoustic model optimization. The proposed SCTM modeling shows a significant improvement in recognition performance as well as a solution to sparse training data problem. Moreover, the state weight quantizing method achieves a drastic reduction in model size. In this paper, we describe the acoustic model optimization procedure for embedded speech recognition system and corresponding performance evaluation results.	acoustic cryptanalysis;acoustic model;elegant degradation;embedded system;hidden markov model;mathematical optimization;performance evaluation;sparse matrix;speech recognition	Kyoungjun Park;Hanseok Ko	2004			computer science	Robotics	-17.510893065532397	-90.94645404631663	196570
9433696d100b205426381561c2851fa93336c4ab	the segmentation of multi-channel meeting recordings for automatic speech recognition	system configuration;multiple channels;rich transcription;automatic segmentation;speech segmentation;speech;automatic generation;automatic speech recognition	One major research challenge in the domain of the analysis of meeting room data is the automatic transcription of what is spoken during meetings, a task which has gained considerable attention within the ASR research community through the NIST rich transcription evaluations conducted over the last three years. One of the major difficulties in carrying out automatic speech recognition (ASR) on this data is dealing with the challenging recording environment, which has instigated the development of novel audio pre-processing approaches. In this paper we present a system for the automatic segmentation of multiple-channel individual headset microphone (IHM) meeting recordings for automatic speech recognition. The system relies on an MLP classifier trained from several meeting room corpora to identify speech/non-speech segments of the recordings. We give a detailed analysis of the segmentation performance for a number of system configurations, with our best system achieving ASR performance on automatically generated segments within 1.3% (3.7% relative) of a manual segmentation of the data.	automated system recovery;automatic system recovery;crosstalk;elegant degradation;headset (audio);information management;memory-level parallelism;microphone;modal logic;mutual information;nist hash function competition;naive bayes classifier;preprocessor;programming paradigm;speech recognition;statistical classification;switzerland;text corpus;transcription (software);word error rate	John Dines;Jithendra Vepa;Thomas Hain	2006			natural language processing;audio mining;speech recognition;computer science;speech;acoustic model;linguistics;speech segmentation;speech analytics	NLP	-14.54987719896639	-88.16577235212593	197661
0cdb2308a6cc02e20d25e3469f713351b0f2a035	a generative model for score normalization in speaker recognition		We propose a theoretical framework for thinking about score normalization, which confirms that normalization is not needed under (admittedly fragile) ideal conditions. If, however, these conditions are not met, e.g. under data-set shift between training and runtime, our theory reveals dependencies between scores that could be exploited by strategies such as score normalization. Indeed, it has been demonstrated over and over experimentally, that various ad-hoc score normalization recipes do work. We present a first attempt at using probability theory to design a generative score-space normalization model which gives similar improvements to ZT-norm on the text-dependent RSR 2015 database.	database normalization;experiment;generative model;hoc (programming language);speaker recognition	Albert Swart;Niko Brümmer	2017			machine learning;normalization model;generative grammar;speaker recognition;normalization (statistics);generative model;pattern recognition;probability theory;artificial intelligence;computer science	Vision	-17.792004602456643	-91.58641128523257	197952
c8898c29e315cd046d12be8bc9114edf89ec89f0	training log-linear acoustic models in higher-order polynomial feature space for speech recognition	article in monograph or in proceedings	The use of higher-order polynomial acoustic features can improve the performance of automatic speech recognition. However, the dimensionality of the polynomial representation can be prohibitively large, making the training of acoustic models using polynomial features for large vocabulary ASR systems infeasible. This paper presents an iterative polynomial training framework for acoustic modeling, which recursively expands the current acoustic features into their second-order polynomial feature space. In each recursion the dimensionality is reduced by a linear projection, such that increasingly higher order polynomial information is incorporated while keeping the dimensionality of the acoustic models constant. Experimental results obtained for a large-vocabulary continuous speech recognition task show that the proposed method outperforms conventional mixture models.	acoustic cryptanalysis;acoustic model;feature vector;iterative method;log-linear model;mixture model;polynomial;recursion;speech recognition;vocabulary	Muhammad Ali Tahir;Heyun Huang;Ralf Schlüter;Hermann Ney;Louis ten Bosch;Bert Cranen;Lou Boves	2013			speech recognition;computer science;artificial intelligence;data science	NLP	-17.69367311801705	-92.09626892586888	198035
8623df6795eae8f5988d414dc062ed23cbc911b8	voicing state determination of co-channel speech	bayesian classifier;decision tree;supervised learning;bayes methods;signal to interference ratio;feature vector;decision theory;speech state estimation supervised learning bayesian methods binary trees vectors decision trees spatial databases testing interference;pattern classification;fisher linear discriminant;speech recognition;learning artificial intelligence;classification accuracy;speech recognition learning artificial intelligence pattern classification bayes methods decision theory;female female voices voicing state determination co channel speech voicing state determination algorithm vsda supervised learning bayesian classifier silence voiced voiced state voiced unvoiced state unvoiced voiced state unvoiced unvoiced state binary tree decision structure 37 dimensional feature vector decision tree fisher linear discriminant classification accuracy male female voices male male voices;binary tree	This paper presents a voicing state determination algorithm (VSDA) that is used to simultaneously estimate the voicing state of two speakers present in a segment of co-channel speech. Supervised learning trains a Bayesian classifier to predict the voicing states. The possible voicing states are silence, voiced/voiced, voiced/unvoiced, unvoiced/voiced and unvoiced/unvoiced. We have assumed the silent state as a subset of the unvoiced class, except when both speakers are silent. We have chosen a binary tree decision structure. Our feature set is a projection of a 37 dimensional feature vector onto a single dimension applied at each branch of the decision tree, using the Fisher linear discriminant. We have produced co-channel speech from the TIMIT database which is used for training and testing. Preliminary results, at signal to interference ratio of 0 dB, have produced classification accuracy of 82.6%, 73.45%, and 68.24% on male/female, male/male and female/female mixtures respectively		Daniel S. Benincasa;Michael I. Savic	1998		10.1109/ICASSP.1998.675441	naive bayes classifier;speech recognition;signal-to-interference ratio;feature vector;decision theory;binary tree;computer science;machine learning;decision tree;pattern recognition;mathematics;supervised learning	ML	-16.364724382201175	-89.99501100314812	198458
8852b9eacd631493705ee15c3b07ed2a8bc8fbf3	robust feature extraction based on teager-entropy and half power spectrum estimation for speech recognition		In this paper, we present a robust feature extraction scheme for speech recognition. Compared to standard mel-frequency cepstral coefficients (MFCC), it incorporates perceptual information into half parameter spectrum not into the whole classical spectrum, and combines with Teager-Entropy to construct a new feature vector. Its performance is compared with several techniques, and detailed comparative performance analysis with various types of noise and a wide range of SNR values is presented. The results suggest that our feature achieves superior robustness with HMM-based recognizer on an English digit task. The 8.87 % reduction of average error rate is obtained in comparison to ordinary MFCC. Furthermore, the results also uncover that the half power spectrum-based method leads to superior performance over the whole power spectrum-based method in most given environment.	feature extraction;spectral density estimation;speech recognition	Jing Dong;Dongsheng Zhou;Qiang Zhang	2015		10.1007/978-3-319-26181-2_9	speech recognition;machine learning;pattern recognition	EDA	-12.44182965833465	-90.56900355453058	198495
b2f85e9dfbab6aac1f48e149f06a0d3314d10689	mce estimation of vq parameters for mvqhmm speech recognition	speech recognition		linuxmce;speech recognition;vector quantization	Antonio M. Peinado;Antonio J. Rubio;José C. Segura;Victoria E. Sánchez;Jesús E. Díaz-Verdejo	1995			speech recognition;artificial intelligence;pattern recognition;computer science	ML	-14.58663642267768	-87.51478936561391	198540
2676c9d2889ee42835c419c6d84cc05d9f55fd91	unit selection and emotional speech	speech synthesis;signal processing;quality improvement	Unit SelectionSynthesis,whereappropriateunits areselected from large databasesof naturalspeech,hasgreatly improved the quality of speechsynthesis.But the quality improvement hascomeat a cost. The quality of the synthesisrelieson the fact that little or no signalprocessingis doneon the selected units,thusthestyleof therecordingis maintainedin thequality of thesynthesis.Thesynthesisstyleis implicitly thestyleof the database. If we wantmoregeneralflexibility we have to record moredataof the desiredstyle. Which meansthat our already largeunit databases mustbemadeevenlarger. This papergivesexamplesof how to producevariedstyle andemotionusingexisting unit selectionsynthesistechniques andalsohighlights the limitations of generatingtruly flexible syntheticvoices.	database	Alan W. Black	2003			speech recognition;voice activity detection;artificial intelligence;quality management;signal processing;pattern recognition;speech processing;computer science;speech synthesis	DB	-13.832136025538272	-87.92836963585859	198980
78bf7741af63339b22d93701b97265041e99c127	session variability contrasts in the marp corpus		Intra-session and inter-session variability in the Multi-session Audio Research Project (MARP) corpus are contrasted in two experiments that exploit the long-term nature of the corpus. In the first experiment, Gaussian Mixture Models (GMMs) model 30-second session chunks, clustering chunks using the Kullback-Leibler (KL) divergence. Cross-session relationships are found to dominate the clusters. Secondly, session detection with 3 variations in training subsets is performed. Results showed that small changes in long-term characteristics are observed throughout the sessions. These results enhance understanding of the relationship between long-term and short-term variability in speech and will find application in speaker and speech recognition systems.	cluster analysis;computer cluster;experiment;heart rate variability;kullback–leibler divergence;mixture model;spatial variability;speech recognition;text corpus	Keith W. Godin;John H. L. Hansen	2010			natural language processing;speech recognition;computer science	NLP	-12.279055404815939	-88.83636209808247	199010
90737eb69ae34cd7b5b701f0c7eb145f106aa3ec	efficient tone classification of speaker independent continuous chinese speech using anchoring based discriminating features		Anchoring based discriminating features were proposed efficient for tone discrimination of Chinese continuous speech, and have been successfully applied before to tone classification of speaker dependent experiment. This paper presents its application to speaker independent tone classification experiments. Furthermore, we made detailed comparison experiments on the efficiencies of three groups of features: the left context dependent, the right context dependent anchoring F0 features, and the conventional F0 features. Experimental results showed that a combination of all three groups achieved a significant improvement of absolute 6.4% from 82.6% by the baseline system to 89.0%. When the three groups of features are used individually, both groups of the anchoring features led to better results than the conventional features, and the left context dependent anchoring features led to the highest performance.	baseline (configuration management);experiment;statistical classification	Jinsong Zhang;Satoshi Nakamura;Keikichi Hirose	2004			speech recognition;artificial intelligence;anchoring;speaker diarisation;pattern recognition;computer science;speaker recognition	NLP	-13.296034396209764	-89.71019239116745	199130
1419bb2cb4b181165986dde078553b4da13c9328	improving automatic speech recognition containing additive noise using deep denoising autoencoders of lstm networks		Automatic speech recognition systems (ASR) suffer from performance degradation under noisy conditions. Recent work, using deep neural networks to denoise spectral input features for robust ASR, have proved to be successful. In particular, Long Short-Term Memory (LSTM) autoencoders have outperformed other state of the art denoising systems when applied to the mfcc’s of a speech signal. In this paper we also consider denoising LSTM autoencoders (DLSTMA), but instead use three different DLSTMAs and apply each to the mfcc’s, fundamental frequency, and energy features, respectively. Results are given using several kinds of additive noise at different intensity levels, and show how this collection of DLSTMA’s improves the performance of the ASR in comparison with the LSTM autoencoder.	additive model;additive white gaussian noise;long short-term memory;noise reduction;speech recognition	Marvin Coto-Jiménez;John Goddard Close;Fabiola Martínez Licona	2016		10.1007/978-3-319-43958-7_42	speech recognition;acoustics;machine learning	NLP	-14.641197642771328	-90.88563361757078	199791
f9448b7a3e1b3de53449114bfae454898b4f1a4d	divergence estimation based on deep neural networks and its use for language identification	statistical divergence;mathematical model gaussian distribution estimation acoustics neural networks adaptation models feature extraction;structural feature;structural feature language identification deep neural network i vector statistical divergence;i vector;statistical distributions bayes methods learning artificial intelligence natural language processing neural nets signal sampling speech processing;deep neural network;speech structure extraction deep neural network training language identification statistical divergence estimation probability distribution dnn based discriminative approach probability density function bayes theorem divergence function integral feature space model adaptation method sampling approach;language identification	In this paper, we propose a method to estimate statistical divergence between probability distributions by a DNN-based discriminative approach and its use for language identification tasks. Since statistical divergence is generally defined as a functional of two probability density functions, these density functions are usually represented in a parametric form. Then, if a mismatch exists between the assumed distribution and its true one, the obtained divergence becomes erroneous. In our proposed method, by using Bayes' theorem, the statistical divergence is estimated by using DNN as discriminative estimation model. In our method, the divergence between two distributions is able to be estimated without assuming a specific form for these distributions. When the amount of data available for estimation is small, however, it becomes intractable to calculate the integral of the divergence function over all the feature space and to train neural networks. To mitigate this problem, two solutions are introduced; a model adaptation method for DNN and a sampling approach for integration. We apply this approach to language identification tasks, where the obtained divergences are used to extract a speech structure. Experimental results show that our approach can improve the performance of language identification by 10.85% relative compared to the conventional approach based on i-vector.	artificial neural network;deep learning;feature vector;language identification;sampling (signal processing);vergence	Yosuke Kashiwagi;Congying Zhang	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472716	language identification;speech recognition;computer science;machine learning;pattern recognition;statistics	Robotics	-17.140211209276135	-91.65581325974628	199923
