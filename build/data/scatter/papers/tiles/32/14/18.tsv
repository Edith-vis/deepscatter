id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
998a67e66cf8af9701a13937fd27222ce7495238	spartacus: a tableau prover for hybrid logic	optimization technique;automated reasoning;modal logic;decision procedure;hybrid logic;tableau algorithms;decision procedures	Spartacus is a tableau prover for hybrid multimodal logic with global modalities and reflexive and transitive relations. Spartacus is the first system to use pattern-based blocking for termination. To achieve a competitive performance, Spartacus implements a number of optimization techniques, including a new technique that we call lazy branching. We evaluate the practical impact of pattern-based blocking and lazy branching for the basic modal logic K and observe high effectiveness of both techniques.	approximation algorithm;backtracking;benchmark (computing);blocking (computing);elegant degradation;hybrid logic;lattice boltzmann methods;lazy evaluation;mathematical optimization;method of analytic tableaux;modal depth;modal logic;modality (human–computer interaction);multimodal interaction;multimodal logic;parallel building blocks;termination analysis	Daniel Götzmann;Mark Kaminski;Gert Smolka	2010	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2010.04.010	modal logic;dynamic logic;discrete mathematics;computer science;theoretical computer science;mathematics;automated reasoning;multimodal logic;algorithm	Logic	-14.901507664557881	23.331270458799786	155665
45b7a2343d67dc8c540fe0d6a215b942342fe6f4	logspace hierarchies, polynomial time and the complexity of fairness problems concerning omega-machines	general and miscellaneous mathematics computing and information science;mathematics;logspace oracle hierarchy;p codes;temporal logic;simulation;nonemptiness problem;concurrent finite state programs;mathematical logic;functional programming;polynomials;complete problems;model checking;logspace alternation hierarchy;computational complexity;complexity of fairness problems;68c25;computer calculations;supercomputers 1987 1989;computerized simulation;finite automata;polynomial time;computer codes;validation;n codes;space;omega automata;programming;functions;finite state machine;parallel processing;68d05;testing 990230 mathematics mathematical models 1987 1989;mathematical space	In this paper, the authors define a restricted logspace oracle hierarchy which turns out to be equivalent to the logspace alternation hierarchy and thus is contained within the second level of the logspace oracle hierarchy. They then examine problems concerning various types of ''fair'' computations with respect to epsilon-Finite State Machines (epsilon-FSM's) and epsilon-One Counter Machines (epsilon-1CM's). For example, the authors consider the nonemptiness problem for epsilon-FSM's and epsilon-1CM's where acceptance is defined in the usual fashion, but with a fairness constraint imposed on accepting computations. The results yield problems that are complete not only for NLOGSPACE and PTIME but the second and third levels of the restricted logspace oracle hierarchy as well. As far as it is known, these are the first natural problems shown to be complete for various levels of the logspace alternation hierarchy. The problems are also of independent interest. In fact, the nonemptiness problem (with fairness constraints) for epsilon-machines has been shown to have immediate applications to the verification of concurrent finite state programs. Furthermore, the results can be used to strengthen known results concerning some related fairness problems that involve temporal logic (e.g. model checking).	fairness measure;l (complexity);lisp machine;omega;p (complexity);polynomial;time complexity	Louis E. Rosier;Hsu-Chun Yen	1987	SIAM J. Comput.	10.1137/0216052	time complexity;model checking;programming;mathematical optimization;combinatorics;mathematical logic;discrete mathematics;temporal logic;computer science;space;mathematics;finite-state machine;programming language;computational complexity theory;function;algorithm;polynomial;algebra	Theory	-11.92208376769855	20.818322705374726	155864
b51c6b06af1c589107b1446b68b3b4d4b9662d85	logic + control: an example	004;program correctness program completeness specification declarative programming declarative diagnosis;datavetenskap datalogi;computer science	We present a Prolog program – the SAT solver of Howe and King – as a (pure) logic program with added control. The control consists of a selection rule (delays of Prolog) and pruning the search space. We construct the logic program together with proofs of its correctness and completeness, with respect to a formal specification. Correctness and termination of the logic program are inherited by the Prolog program; the change of selection rule preserves completeness. We prove that completeness is also preserved by one case of pruning; for the other an informal justification is presented. For proving correctness we use a method, which should be well known but is often neglected. For proving program completeness we employ a new, simpler variant of a method published previously. We point out usefulness of approximate specifications. We argue that the proof methods correspond to natural declarative thinking about programs, and that they can be used, formally or informally, in every-day programming. 1998 ACM Subject Classification D.1.6 Logic Programming, F.3.1 Specifying and Verifying and Reasoning about Programs, D.2.4 Software/Program Verification, D.2.5 Testing and Debugging	approximation algorithm;boolean satisfiability problem;correctness (computer science);debugging;formal specification;formal verification;logic control;logic programming;prolog;selection rule;solver	Wlodzimierz Drabent	2012		10.4230/LIPIcs.ICLP.2012.301	program analysis;correctness;horn clause;computer science;theoretical computer science;programming language;logic programming;program derivation;algorithm	PL	-17.414583839783667	23.17374641053442	156358
bfdb93275ab7854bb0052bf953230835b940b03a	efficient method extraction for automatic elimination of type-3 clones	time complexity;semantics;cloning;optimization;context;benchmark testing;automation	A semantics-preserving transformation by Komondoor and Horwitz has been shown to be most effective in the elimination of type-3 clones. The two original algorithms for realizing this transformation, however, are not as efficient as the related (slice-based) transformations. We present an asymptotically-faster algorithm that implements the same transformation via bidirectional reachability on a program dependence graph, and we prove its equivalence to the original formulation.	algorithm;bidirectional search;control flow;mathematical optimization;optimization problem;program dependence graph;reachability;turing completeness	Ran Ettinger;Shmuel S. Tyszberowicz;Shay Menaia	2017	2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)	10.1109/SANER.2017.7884633	discrete mathematics;computer science;theoretical computer science;algorithm	SE	-16.501234043023118	24.464948838288638	156549
9848d1d56488c97ed6540b41299577cf549b87a9	strictness, totality, and non-standard-type inference	langage fonctionnel;programming language;totality type;logic;lenguaje funcional;operational semantics;totality analysis;semantics;analyse totalite;interseccion;semantica;semantique;functional programming;strictness;analisis programa;higher order;totality;inferencia;non standard type inference;program analysis;analyse programme;intersection;functional language;type inference;intersection types;type totalite;inference;logique;logica	In this paper we present two non-standard-type inference systems for conjunctive strictness and totality analyses of higher-order-typed functional programs and prove completeness results for both the strictness and the totality-type entailment relations. We also study the interactions between strictness and totality analyses, showing that the information obtainable by a system that combines the two analyses, even though more re.ned than the information given by the two separate systems, cannot be e/ectively used. A main feature of our approach is that all the results are proved by relying directly on the operational semantics of the programming language considered. This leads to a rather direct presentation which involves relatively little mathematical overhead. c © 2002 Elsevier Science B.V. All rights reserved.	interaction;operational semantics;overhead (computing);programming language;schedule (computer science);type inference	Mario Coppo;Ferruccio Damiani;Paola Giannini	2002	Theor. Comput. Sci.	10.1016/S0304-3975(00)00348-0	program analysis;computer science;artificial intelligence;type inference;intersection;mathematics;programming language;strictness analysis;functional programming;operational semantics;logic;algorithm	PL	-18.194613203587974	21.194825751683098	156708
12c2989210eca593f41f994b5503941c6fbc9ed8	mind the gap! abstract versus concrete models of specifications	standard ml;program development;semantics of programming languages	In the theory of algebraic specifications, many-sorted algebras are used to model programs: the representation of data is arbitrary and operations are modelled as ordinary functions. The theory that underlies the formal development of programs from specifications takes advantage of the many useful properties that these models enjoy. The models that underlie the semantics of programming languages are different. For example, the semantics of Standard ML uses rather concrete models, where data values are represented as closed constructor terms and functions are represented as “closures”. The properties of these models are quite different from those of many-sorted algebras. This discrepancy brings into question the applicability of the theory of specification and formal program development in the context of a concrete programming language, as has been attempted in the Extended ML framework for the formal development of Standard ML programs. This paper is a preliminary study of the difference between abstract and concrete models of specifications, inspired by the kind of concrete models used in the semantics of Standard ML, in an attempt to determine the consequences of the discrepancy.	discrepancy function;extended ml;formal methods;programming language;semantics (computer science);standard ml	Donald Sannella;Andrzej Tarlecki	1996		10.1007/3-540-61550-4_143	computer science;theoretical computer science;programming language;operational semantics;second-generation programming language;denotational semantics;algorithm	PL	-13.89581111581313	19.27606390145756	156768
9e0b2c1f1e3e4543ee812621dffe5c43ca410219	higher order deforestation	efficiency;functional programming;higher order;first order;termination;transformation;higher order functions;data structure;deforestation	Deforestation is a well known transformation algorithm which can eliminate intermediate structures from functional programs. In previous work, we have shown how the deforestation algorithm can be extended to handle higher order programs. A higher order treeless form of expression was defined to ensure the termination of this algorithm. Our higher order algorithm was further extended by Seidl and Sørensen, and this extension was shown to remove some intermediate structures not removed by our algorithm (although our original algorithm can also remove some intermediate structures not removed by their technique). In this paper, we show how our original definition of higher order treeless form can be extended to allow the intermediate structures in the examples given by Seidl and Sørensen to be removed. We argue that, because our extended algorithm uses an easy to recognise treeless form, there is more transparency for the programmer in terms of the improvements which will be made. We prove that our new algorithm terminates, and we conjecture that it ensures that there is no efficiency loss, which we argue is essential for any optimisation.	algorithm;mathematical optimization;programmer	Geoff W. Hamilton	1996	Fundam. Inform.	10.1007/3-540-61756-6_87	transformation;higher-order logic;data structure;computer science;deforestation;first-order logic;mathematics;efficiency;programming language;functional programming;higher-order function;algorithm	DB	-16.857591668474075	22.33566075832518	156945
f54544b1b8737b2c54c67f44d812e136860e5b55	vm lambda: a functional calculusfor scientific discovery	discovery science;corresponding domain science;non-trivial extension;knowledge discovery;functional language;original vml;real implementation;programming language;vm lambda;discovery scientist;functional calculusfor scientific discovery;hypothetical view	We present VMλ, a formalization and implementation of the functional language VML. VML is a programming language proposed by discovery scientists for the purpose of assisting the process of knowledge discovery. It is a non-trivial extension of ML with hypothetical views. Operationally, a hypothetical view is a value with a representation that indicates how the value was created. The notion of hypothetical views has already been successful in the domain of genome analysis, and known to be useful in the process of knowledge discovery. However, VML as a programming language was only informally defined in English prose, and indeed found problematic both in theory and in practice. Thus, a proper definition and imple- mentation of VML with formal foundations would be of great help to discovery science and hence corresponding domain sciences. This paper gives a solid foundation of VML by extending the standard simply typed call-by-value λ-calculus. Although this extension, VMλ ,i s simple and clear, its design required much care to find and fix problems of the original VML. We also present a real implementation of VMλ, written in Camlp4 as a conservative translator into OCaml. This implementation makes extensive use of labeled arguments and polymorphic variants - two advanced features of OCaml that originate in OLabl.		Eijiro Sumii;Hideo Bannai	2001		10.1007/3-540-45788-7_18	computer science;artificial intelligence;programming language;algorithm	HPC	-15.805858377012623	18.696510954635194	157063
34989c6c7c8e8573ad82a44c07508aef68ae4cd1	improving real analysis in coq: a user-friendly approach to integrals and derivatives	improving real analysis;user-friendly approach	Verification of numerical analysis programs requires dealing with derivatives and integrals. High confidence in this process can be achieved using a formal proof checker, such as Coq. Its standard library provides an axiomatization of real numbers and various lemmas about real analysis, which may be used for this purpose. Unfortunately, its definitions of derivative and integral are unpractical as they are partial functions that demand a proof term. This proof term makes the handling of mathematical formulas cumbersome and does not conform to traditional analysis. Other proof assistants usually do not suffer from this issue; for instance, they may rely on Hilbert’s epsilon to get total operators. In this paper, we propose a way to define total operators for derivative and integral without having to extend Coq’s standard axiomatization of real numbers. We proved the compatibility of our definitions with the standard library’s in order to leverage existing results. We also greatly improved automation for real analysis proofs that use Coq standard definitions. We exercised our approach on lemmas involving iterated partial derivatives and differentiation under the integral sign, that were missing from the formal proof of a numerical program solving the wave equation.	automated proof checking;axiomatic system;coq (software);formal proof;iteration;list of numerical analysis software;proof assistant;standard library	Sylvie Boldo;Catherine Lelay;Guillaume Melquiond	2012		10.1007/978-3-642-35308-6_22	discrete mathematics;mathematics;algorithm	PL	-17.957974379813088	18.683960726972156	158895
1d4309e5ecc67f5430183253047ef539bbbd59b2	weighted pushdown systems and their application to interprocedural dataflow analysis	dataflow analysis;pushdown system;interprocedural dataflow-analysis problem;weighted pushdown system;new algorithm;certain criterion;certain class;meet-over-all-paths value;weighted pdss	Recently, pushdown systems (PDSs) have been extended to weighted PDSs, in which each transition is labeled with a value, and the goal is to determine the meet-over-allpaths value (for paths that meet a certain criterion). This paper shows how weighted PDSs yield new algorithms for certain classes of interprocedural dataflow-analysis problems.	algorithm;data-flow analysis;dataflow;stack (abstract data type)	Thomas W. Reps;Stefan Schwoon;Somesh Jha	2003		10.1007/3-540-44898-5_11	data flow diagram;yield;flowchart;computer science;theoretical computer science;transfer function;programming language;specification;algorithm	PL	-12.40056529212798	23.801250908144986	159191
1986fbf172c6932a254ca0f26ae5027fa2f0c415	experiments with associative-commutative discrimination nets	efficient algorithm;theorem proving;theorem prover;data structure;associative commutative	We recently proposed a data structure, called associative-commutative discrimination nets, that supports efficient algorithms for (manyto-one) term matching in the presence of associative-commutative functions. In this paper we discuss the integration of such discrimination nets into an actual equational theorem prover and report on corresponding experiments. The general associativecommutative matching problem is known to be NP-complete, but can be solved in polynomial time if the given terms are linear, i.e., do not contain multiple occurrences of the same variable. We therefore have implemented a two-stage matching procedure. First we check whether a match exists for the linearized versions of the given terms (where different occurrences of the same variable are replaced by different new variables). If a match for the linearized terms does exist, we then determine whether there is also a match for the original, non-linear terms (i.e., whether the proposed substitutions for different occurrences of the same variable are consistent). Our experimental results indicate that this approach works very well in theorem proving, where most matching attempts actually fail and are filtered out during the first stage, so that the second, more expensive stage of the algorithm is only needed in comparatively few cases.	algorithm;automated reasoning;automated theorem proving;data structure;experiment;np-completeness;nonlinear system;polynomial;time complexity	Leo Bachmair;Ta Chen;I. V. Ramakrishnan;Siva Anantharaman;Jacques Chabin	1995			discrete mathematics;data structure;computer science;artificial intelligence;machine learning;mathematics;automated theorem proving;programming language;algorithm;statistics	AI	-13.979523838005159	20.69946173079347	159323
80f747e603471eafa2214aea4b5d2b52d7aafc2a	project presentation: algorithmic structuring and compression of proofs (ascop)	new connection;formal language theory;computer-generated proof;formal grammar;algorithmic structure;recent groundbreaking result;mathematical proof;project presentation;efficient algorithm;proof theory;abbreviate analytic proof;compress proof	Computer-generated proofs are typically analytic, i.e. they essentially consist only of formulas which are present in the theorem that is shown. In contrast, mathematical proofs written by humans almost never are: they are highly structured due to the use of lemmas. The ASCOP-project aims at developing algorithms and software which structure and abbreviate analytic proofs by computing useful lemmas. These algorithms will be based on recent groundbreaking results establishing a new connection between proof theory and formal language theory. This connection allows the application of efficient algorithms based on formal grammars to structure and compress proofs.	algorithm;computer-generated holography;formal grammar;formal language	Stefan Hetzl	2012		10.1007/978-3-642-31374-5_32	proofs involving the addition of natural numbers;discrete mathematics;mathematics;proof assistant;algorithm	Theory	-13.29486362582985	21.97778692645255	159394
5937c17afbc02e571c42b8e4d9f47096492ef638	exact heap summaries for symbolic execution	constraint based reasoning;symbolic references;symbolic execution	A recent trend in the analysis of object-oriented programs is the modeling of references as sets of guarded values, enabling multiple heap shapes to be represented in a single state. A fundamental problem with using these guarded value sets is the inability to generate test inputs in a manner similar to symbolic execution based analyses. Although several solutions have been proposed, none have been proven to be sound and complete with respect to the heap properties provable by generalized symbolic execution (GSE). This work presents a method for initializing input references in a symbolic input heap using guarded value sets that exactly preserves GSE semantics. A correctness proof for the initialization scheme is provided with a proof-of-concept implementation. Results from an empirical evaluation on a command set of GSE data structure benchmarks show an increase in the size and number of analyzed heaps over existing GSE representations. The initialization technique can be used to ensure that guarded value set based symbolic execution engines operate in a provably correct manner with regards to symbolic references as well as provide the ability to generate concrete heaps that serve as test inputs to the program.	algorithm;correctness (computer science);data structure;generic stream encapsulation;provable security;symbolic execution	Benjamin Hillery;Eric Mercer;Neha Rungta;Suzette Person	2016		10.1007/978-3-662-49122-5_10	binomial heap;computer science;theoretical computer science;symbolic data analysis;programming language;symbolic trajectory evaluation;algorithm	PL	-17.623101517668808	23.731610356357137	159731
4f92d2e1eeedda9940de8319fee37c7e6a01beb8	formalization of incremental simplex algorithm by stepwise refinement		We present an Isabelle/HOL formalization and total correctness proof for incremental version of Simplex algorithm which is used in most state-of-the-art SMT solvers. Formalization relies on stepwise program and data refinement, starting from a simple specification, going trough a number of fine refinement steps, and ending up in a fully executable functional implementation. Symmetries present in the algorithm are handled with special care.	correctness (computer science);cut, copy, and paste;definition;executable;function model;hol (proof assistant);imperative programming;isabelle;maximal set;refinement (computing);simplex algorithm;stepwise regression;termination analysis;top-down and bottom-up design;turing completeness	Mirko Spasic;Filip Maric	2012		10.1007/978-3-642-32759-9_35	mathematical optimization;machine learning;algorithm	PL	-16.800920324079847	23.63884823045738	160671
5e637d7f061fcb8995c1d38dd20234d479f46b2f	formalizing java's two's-complement integral type in isabelle/hol	integer arithmetic;theorem proving;tools;java card;formal methods;integer arithmetic.;java;formal semantics;formal method;source code	We present a formal model of the Java two’s-complement integral arithmetics. The formalization is based on a direct analysis of the Java Language Specification (JLS) [GJSB00] and led to the discovery of several underspecifications and ambiguities. Underspecifications are highly undesirable since even compliant Java compilers may interpret the same program differently, which leads to unportable code. The Java integral types are finite datatypes that possess a surprisingly rich theory (they form a ring, for example) which comprises a number of highly non-standard and tricky laws with nonintuitive and subtle preconditions. With respect to their formalization, we followed the so-called wrap-around approach: integers are defined on [−2n−1 .. 2n−1 − 1], where in case of overflow the results of the arithmetic operations are mapped back into this interval through modulo calculations. These numbers can be equivalently realized by bitstrings of length n in the widely-used two’scomplement representation system [Gol02]. The advantage of this approach is that it closely follows the definition of the Java type int in the JLS, for which certain surprising properties like “Maxint + 1 = Minint” or crucial laws like the associativity law “a + (b + c) = (a + b) + c” hold. The formal model should reflect these properties. A “partial approach” (i.e. a modelling that does not allow overflows) is not able to prove them. The wrap-around approach therefore gives better support for automated reasoning. The theory is formally analyzed in Isabelle/HOL [Pau94], that is, machine-checked proofs for the ring properties, divisor/remainder theorems etc. are provided. This work is a necessary prerequisite for machine-supported reasoning over arithmetic formulae in the context of Java sourcecode verification, especially of efficient arithmetic Java programs such as encryption algorithms, in particular in tools like Jive [MPH00] that generate verification conditions over arithmetic formulae from such programs. In the future, we strongly suggest to supplement informal language definitions by machinechecked specifications like the one referred to in this abstract as a part of the normative basis of a programming language.	apl;algorithm;automated reasoning;compiler;emoticon;encryption;formal language;hol (proof assistant);integer (computer science);isabelle;java;jive;modulo operation;precondition;programming language specification;two's complement	Nicole Rauch;Burkhart Wolff	2003	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80808-9	discrete mathematics;java modeling language;mathematics;programming language;algorithm	PL	-14.913285942961622	19.132684632656378	160945
088baece6d14fbcd873465325b02c03d8d9b28a2	termination of algorithms over non-freely generated data types	software verification;data type;theorem proving	"""Termination proofs for recursively deened operations serve several purposes: On the one hand, of course, they ensure the termination of the respective algorithms which is an essential topic in software veri-cation. On the other hand, a successful termination proof allows to use the termination ordering as an induction ordering for future inductive proofs. So far, in the area of explicit inductive theorem proving only data types were admitted whose objects possess a unique syntactical representation. These data types include nat 1 , lists, and trees. However, there are data types that do not possess this property, as, for instance, nite sets and nite arrays, which are frequently used for speciications in software veriication. In this paper we are concerned with these data types. We admit them to explicit inductive theorem proving and, furthermore, we present an approach for an automated termination analysis of recursively deened algorithms over these data types. 1 Motivation In Computer Science the question of terminating programs is very important, especially when it comes to the veriication of software. Total correctness is given only if the software meets its speciication and if the software always terminates. To perform these correctness and termination proofs inductive theorem provers are frequently used. Another aspect of termination is also very important: having proved the termination of a recursively deened algorithm, the termination ordering can then be used to formulate induction schemes for future inductive proofs. This relationship between termination and induction is, of course, not very surprising since there is a very close relationship between recursion and induction. Hence, proving the termination of recursively deened algorithms is a key technique in inductive theorem proving. Therefore, improvements in the automation of termination proofs at the same time mean progress in the automation of induction in general. So far, within the area of explicit induction only speciic data types were allowed, namely those which possess the Unique Factorization Property (UFP), 1 Throughout this paper we use the notion \nat"""" for the data type specifying natural numbers in order to distinguish from the semantic notion of natural numbers which we also use. i.e. all objects can be uniquely represented by a single constructor ground term. For example, in Nqthm 2] they are enforced by the shell-principle, as by similar mechanisms in 1], Clam 3], and INKA 9]. Take e.g. the data type lists of nat. They can be deened using two constructor functions, nil …"""	algorithm;automated theorem proving;computer science;correctness (computer science);explicit substitution;inductive reasoning;mathematical induction;network address translation;nqthm;recursion;rewriting;termination analysis;tree (data structure)	Claus Sengler	1996		10.1007/3-540-61511-3_73	discrete mathematics;data type;software verification;computer science;theoretical computer science;automated theorem proving;programming language;algorithm	Logic	-15.367668184051638	18.60974735992815	161247
19e3cd74c2897a75e25fb17e55224209b0e6f895	interprocedural program analysis: herbrand equalities and local solvers		This thesis covers two aspects of interprocedural program analysis. In the first part we concentrate on pure syntactical relations between program variables and introduce a novel analysis in order to compute all interprocedural valid Herbrand equalities for programs where right-hand sides contain at most one program variable (which can occur several times). The novel analysis is based on procedure summaries representing the weakest preconditions for finitely many generic postconditions. In order to arrive at effective representations for all occurring weakest preconditions, we show for almost all run-time values possibly computed by the program, that they can be uniquely factorized into tree patterns and a terminating ground term. Moreover, we introduce an approximate notion of subsumption which is effectively decidable and ensures that finite conjunctions of equalities may not grow infinitely. Based on these technical results, we realize an effective fixed point iteration. Finally we show that a two-variable invariant candidate can be verified in time polynomial in the size of the program and in the size of the invariant candidate. Furthermore, we show that a multi-variable invariant candidate can be verified in time polynomial in the size of the program and in the size of the invariant candidate, and only exponentially in the number of variables of the invariant candidate. In the second part of this thesis we perform interprocedural program analysis by means of partial tabulation of procedure summaries. Such an analysis might not terminate if a procedure is analyzed for infinitely many calling contexts or when the domain has infinite strictly ascending chains. As a remedy we provide two local solvers for general equation systems, be they monotone or not, and prove that these solvers only fail to terminate, if infinitely many variables are encountered during a run. We show that interprocedural analysis performed by these local solvers is guaranteed to terminate for all non-recursive programs. Moreover, for recursive programs we are still able to provide termination guarantees by over-approximating calling contexts such that the number of contexts for each procedure is kept finite.	approximation algorithm;fixed point (mathematics);fixed-point iteration;interprocedural optimization;newman's lemma;polynomial;postcondition;precondition;predicate transformer semantics;program analysis;recursion;subsumption architecture;table (information);terminate (software);variable (computer science);monotone	Stefan Schulze Frielinghaus	2018				PL	-15.47188544909263	22.793787072866365	161502
8e7cff5c1f51a27d505ef884a1c3ede3eb7d91f0	contextualized abstraction for assertion-level theorem proving	swinburne;theorem proving;levels of abstraction;proof search	In this paperwe proposea context-basedapproachto abstracttheorem proving. The challengesstemfrom the needto identify an abstractlevel for theoremproving where(lessimportant)information can be temporarily ignoredso that a (plan for a) proof of the abstractedproblemcanbedevisedto guidethe(re)constructionof the object-level proof. Contextualization is realizedby preservingthe logicalstructuresof theformulasof theoriginal representationwhile pushingthelessimportantsubformulas,accordingto a relevancerelation, into the hierarchicalsubcontexts. This representationallows the problemto be graduallyunfoldedduring the proof searchprocessby hierarchicallyexploring thesubcontexts requiredto provide supportfor thehypothesesusedin theproof plan. Theunderlyinginferencemachineryis alsoequippedwith anassertion applicationmodule which allows mathematicalassertions suchasaxioms,definitions,theorems,andevenglobalandlocal assumptionsto be applieddirectly to a proof situationto obtaintheir logicalconsequences (from theappliedproofsituation)andfill in the gapsopenedup by anabstract-level proof step.This guaranteesthat ourachievementis two-fold: ontheonehand,weareableto carryout effectivetechniquesto searchfor andconstructproofsfor aproblem; on the otherhand,the constructedproof is readily at a sufficiently high level of abstractionso that it canbe communicateddirectly to humanmathematicianswithout undergoing a proof transformation processasrequiredby mostmachine-generated proofs.	high-level programming language	Quoc Bao Vo	2004			computer science;artificial intelligence;automated theorem proving;proof complexity;algorithm	PL	-17.898756414523586	22.102812424180225	161569
41c52fd45b209a1a72f45fe7b99afddd653c5a12	a top-down compiler for sentential decision diagrams		The sentential decision diagram (SDD) has been recently proposed as a new tractable representation of Boolean functions that generalizes the influential ordered binary decision diagram (OBDD). Empirically, compiling CNFs into SDDs has yielded significant improvements in both time and space over compiling them into OBDDs, using a bottomup compilation approach. In this work, we present a top-down CNF to SDD compiler that is based on techniques from the SAT literature. We compare the presented compiler empirically to the state-ofthe-art, bottom-up SDD compiler, showing ordersof-magnitude improvements in compilation time.	algorithm;binary decision diagram;bottom-up parsing;cobham's thesis;compiler;conjunctive normal form;influence diagram;top-down and bottom-up design	Umut Oztok;Adnan Darwiche	2015			compiler correctness;computer science;theoretical computer science;programming language;algorithm	AI	-15.432516760565205	25.199182760046817	161816
1e82b84a4782d7f3b8de3f0515cae76708709fcc	formal synthesis of a unification algorithm by the deductive-tableau method	unification algorithm;formal synthesis;deductive-tableau method	We present the formal derivation of a unification algorithm using the deductive-tableau method for program synthesis. The methodology is briefly described, with emphasis on the deduction rules used in the derivation. Starting from an input-output specification expressed in first-order logic, a unification algorithm is synthesized by proving the validity of the specification. The termination of the synthesized program is also proved.	algorithm;method of analytic tableaux;newton's method;unification (computer science)	Daniele Nardi	1989	J. Log. Program.	10.1016/0743-1066(89)90008-3	discrete mathematics;mathematics;program derivation;algorithm	Logic	-17.024530291987663	19.09618322574672	162895
6185e9ce6a13182595dcfddf8e5b67c9cc70dbaa	computer science logic	higher order logic;proof theory;complexity;semantics;formal methods;mathematical logic;resolution;logic	We describe an extension of Hoare’s logic for reasoning about programs that alter data structures. We consider a low-level storage model based on a heap with associated lookup, update, allocation and deallocation operations, and unrestricted address arithmetic. The assertion language is based on a possible worlds model of the logic of bunched implications, and includes spatial conjunction and implication connectives alongside those of classical logic. Heap operations are axiomatized using what we call the “small axioms”, each of which mentions only those cells accessed by a particular command. Through these and a number of examples we show that the formalism supports local reasoning: A specification and proof can concentrate on only those cells in memory that a program accesses. This paper builds on earlier work by Burstall, Reynolds, Ishtiaq and O’Hearn on reasoning about data structures.	assertion (software development);bunched logic;computer science;data structure;high- and low-level;hoare logic;logical connective;lookup table;memory management;possible world;semantics (computer science);storage model	Jan van Leeuwen	2001		10.1007/3-540-44802-0	logic synthesis;computer science;computational logic;automated reasoning;logic control	PL	-17.12229937943157	20.19914430443314	162948
0084f7786438ef7fa9e77dba39ab45e0a04d4b7a	introducing symbolic problem solving techniques in the dependence testing phases of a vectorizer	computer arithmetic;problem solving;semantic analysis	The purpose of a vectorizer is to perform program restructuring in order to exhibit the most efficiently exploitable forms of vector loops. This is guided by a suitable form of semantic analysis, Dependence Testing, which must be precise in order to fully exploit the architecture. Most studies reduce this phase to the application of a series of explicitly computable arithmetic criteria. In many cases, this will fail if the criteria cannot be computed numerically, and contain symbols which cannot be evaluated. This also makes the use of other information pertaining to these symbols difficult. It is shown here that it is possible to extract symbolic equations from some of the classical criteria, merge them with other symbolic knowledge about the program, and use the global system to decide the non-existence of a dependence. In the context of the VATIL vectorizer [17, 16], it is also shown possible to control the computation cost, and obtain a good overall efficiency.	comparison of raster-to-vector conversion software;computable function;numerical analysis;problem solving;semantic analysis (compilers);symbolic computation	Alain Lichnewsky;François Thomasset	1988		10.1145/55364.55403	discrete mathematics;parallel computing;computer science;theoretical computer science;mathematics;programming language;algorithm	SE	-17.03577876343725	24.815062655826427	163020
4893f778f16e778e478df0fe97b6683fb90c2625	programmed strategies for program verification	verification;reduction;logic;data type;program verification;theorem proving;stratego;type checking;decision procedure;design and implementation;normal form;exponential growth;decision procedures;rewriting;normal forms;strategies;haskell;type system	Plover is an automated property-verifier for Haskell programs that has been under development for the past three years as a component of the Programatica project. In Programatica, predicate definitions and property assertions written in P-logic, a programming logic for Haskell, can be embedded in the text of a Haskell program module. Properties refine the type system of Haskell but cannot be verified by type-checking alone; a more powerful logical verifier is needed. Plover codes the proof rules of P-logic, and additionally, embeds strategies and decision procedures for their application and discharge. It integrates a reduction system that implements a rewriting semantics for Haskell terms with a congruence-closure algorithm that supports reasoning with equality. It employs strategies such as structure splitting and case analysis to explore alternative valuations of expressions of type Bool or other finite data types, but these strategies can lead to exponential growth of terms and must be employed cautiously. Plover itself is written in Stratego, which has proven to be a powerful language tool for implementating a verifier. We discuss the design and implementation of some strategies that enable Plover to comprehend Haskell and verify many valid property assertions.	algorithm;code;congruence of squares;discharger;embedded system;formal verification;haskell;plover;rewriting;time complexity;type system	Richard B. Kieburtz	2007	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2007.02.045	exponential growth;verification;type system;reduction;data type;rewriting;computer science;theoretical computer science;mathematics;programming language;logic;algorithm	PL	-17.27921798266874	22.349282915379863	163102
219ce96f6e3f107cd8f6bc9041a7347c5ea0c3b2	bonsai: synthesis-based reasoning for type systems		When designing a type system, we may want to mechanically check the design to guide its further development. We describe algorithms that perform symbolic reasoning about executable models of type systems. The algorithms support three queries. First, they check type soundness and synthesize a counterexample program if such a soundness bug is found. Second, they compare two versions of a type system, synthesizing a program accepted by one but rejected by the other. Third, they minimize the size of synthesized counterexample programs.   These algorithms symbolically evaluate typecheckers and interpreters, producing formulas that characterize the set of programs that fail or succeed in the typechecker and the interpreter. However, symbolically evaluating interpreters poses efficiency challenges, which are caused by having to merge execution paths of the various possible input programs. Our main contribution is the bonsai tree, a novel symbolic representation of programs and program states that addresses these challenges. Bonsai trees encode complex syntactic information in terms of logical constraints, enabling more efficient merging.   We implement these algorithms in the Bonsai tool, an assistant for type system designers. We perform case studies on how Bonsai helps test and explore a variety of type systems. Bonsai efficiently synthesizes counterexamples for soundness bugs previously inaccessible to automatic tools and is the first automated tool to find a counterexample for the recently discovered Scala soundness bug SI-9633.	abstract syntax tree;algorithm;automated proof checking;bonsai;constraint programming;data structure;encode;executable;interactivity;interpreter (computing);scala;software bug;solver;symbolic computation;symbolic execution;systems design;type safety;type system	Kartik Chandra;Rastislav Bodík	2017	PACMPL	10.1145/3158150	theoretical computer science;programming language;counterexample;computer science;soundness;executable	PL	-18.93612035947468	25.208857055775304	163147
ca2291f737f511a06632022689580588d4108073	black-box/white-box simplification and applications to quantifier elimination	quantifier elimination;simplification;factor structure;tarski formulas;cylindrical algebraic decomposition	"""This paper describes a new method for simplifying Tarski formulas. The method combines simplifications based purely on the factor structure of inequalities (""""black-box"""" simplification) with simplifications that require reasoning about the factors themselves. The goal is to produce a simplification procedure that is very fast, so that it can be applied --- perhaps many, many times --- within other algorithms that compute with Tarski formulas without ever slowing them down significantly, but which also produces useful simplification in a substantial number of cases. The method has been implemented and integrated into implementations of two important algorithms: quantifier elimination by virtual term substitution, and quantifier elimination by cylindrical algebraic decomposition. The paper reports on how the simplification method has been integrated with these two algorithms, and reports experimental results that demonstrate how their performance is improved."""	algorithm;black box;level of detail;linear algebra;quantifier (logic);symbolic computation;text simplification	Christopher W. Brown;Adam W. Strzebonski	2010		10.1145/1837934.1837953	discrete mathematics;quantifier elimination;cylindrical algebraic decomposition;mathematics;simplification;algorithm;algebra	Graphics	-15.978802746827823	24.661084343374025	163178
d88fe55f9956a90c53b72eab134c8ee7184db639	set-based failure analysis for logic programs and concurrent constraint programs	lenguaje programacion;programmation concurrente par contrainte;programmation logique avec contrainte;concurrent constraint programming;programming language;approximation method;programacion logica con restriccion;analisis programa;failure analysis;analyse syntaxique;analisis sintaxico;syntactic analysis;langage programmation;constraint logic programming;program analysis;logic programs;analyse programme;analisis semantico;abstract interpretation;analyse semantique;semantic analysis	This paper presents the rst approximation method of the nite failure set of a logic program by set based analysis In a dual view the method yields a type analysis for programs with ongoing behaviors perpetual processes Our technical contributions are the semantical characterization of nite failure of logic programs over in nite trees and the design and soundness proof of the rst set based analysis of logic programs with the greatest model semantics Finally we exhibit the con nection between nite failure and the inevitability of the inconsistent store error in fair executions of concurrent constraint programs where no process suspends forever This indicates a potential application to error diagnosis for concurrent constraint programs	approximation;concurrent constraint logic programming;failure analysis;naruto shippuden: clash of ninja revolution 3	Andreas Podelski;Witold Charatonik;Martin Müller	1999		10.1007/3-540-49099-X_12	program analysis;constraint logic programming;concurrent constraint logic programming;failure analysis;discrete mathematics;computer science;parsing;mathematics;programming language;algorithm	PL	-18.570976848519738	21.833665415539553	164348
093e9e83b306297092491b2bbb6352448f182b62	reducing expression size using rule-based integration	ssreflect;rule based;coq;formal verification;regularity of interval matrices;interval analysis	This paper describes continuing progress on the development of a repository of transformation rules relevant to indefinite integration. The methodology, however, is not restricted to integration. Several optimization goals are being pursued, including achieving the best form for the output, reducing the size of the repository while retaining its scope, and minimizing the number of steps required for the evaluation process. New optimizations for expression size are presented.	mathematical optimization;optimization problem	David J. Jeffrey;Albert D. Rich	2010		10.1007/978-3-642-14128-7_20	rule-based system;discrete mathematics;formal verification;computer science;artificial intelligence;mathematics;programming language;algorithm	PL	-16.709941379326672	18.76999721577043	164941
51edbefe970fb4d09e74a0c55ae18744cc8ea0e4	advanced theorem proving techniques in pvs and applications		The Prototype Verification System (PVS) is an interactive verification environment that combines a strongly typed specification language with a classical higher-order logic theorem prover. The PVS type system supports: predicate subtypes, dependent types, abstract data types, compound types such as records, unions, and tuples, and basic types such as numbers, Boolean values, and strings. The PVS theorem prover includes decision procedures for a variety of theories such as linear arithmetic, propositional logic, and temporal logic. This paper surveys advanced PVS features, including: types for specifications, implicit induction, iterations, rapid prototyping, strategy writing, and computational reflection. These features are illustrated with simple examples taken from NASA PVS developments.	abstract data type;automated theorem proving;computation;dependent type;iteration;propositional calculus;prototype verification system;rapid prototyping;reflection (computer programming);specification language;temporal logic;type system	César A. Muñoz;Ramiro Demasi	2011		10.1007/978-3-642-35746-6_4	discrete mathematics;theoretical computer science;mathematics;algorithm	Logic	-19.096782811450417	20.30657369117486	164947
298e37ceebd50c59ae472f077d180cc2ad5765ae	abstraction in fixpoint logic	simulation;parameterised boolean equation systems	We present a theory of abstraction for the framework of parameterised Boolean equation systems, a first-order fixpoint logic. Parameterised Boolean equation systems can be used to solve a variety of problems in verification. We study the capabilities of the abstraction theory by comparing it to an abstraction theory for Generalised Kripke modal Transition Systems (GTSs). We show that for model checking the modal μ-calculus, our abstractions can be exponentially more succinct than GTSs and our theory is as complete as the GTS framework for abstraction. Furthermore, we investigate the completeness of our theory irrespective of the encoded decision problem. We illustrate the potential of our theory through case studies using the first-order modal μ-calculus and a real-time extension thereof, conducted using a prototype implementation of a new syntactic transformation for parameterised Boolean equation systems.	boolean algebra;decision problem;first-order logic;first-order predicate;fixed point (mathematics);modal logic;modal μ-calculus;model checking;prototype;real-time clock;theory	Sjoerd Cranen;Maciej Gazda;Wieger Wesselink;Tim A. C. Willemse	2015	ACM Trans. Comput. Log.	10.1145/2740964	discrete mathematics;computer science;theoretical computer science;mathematics;algorithm	Logic	-12.001569481829748	22.355932535612943	165231
61fa4d78f81cee37544167b617bdc969137e15b6	kripke open bisimulation - a marriage of game semantics and operational techniques		Proving that two programs are contextually equivalent is notoriously hard, particularly for functional languages with references (i.e., local states). Many operational techniques have been designed to prove such equivalences, and fully abstract denotational model, using game semantics, have been built for such languages. In this work, we marry ideas coming from trace semantics, an operational variant of game semantics, and from Kripke logical relations, notably the notion of worlds as transition systems of invariants, to define a new operational technique: Kripke open bisimulations. It is the first framework whose completeness does not rely on any closure by contexts.	bisimulation;denotational semantics;functional programming;game semantics;logical relations	Guilhem Jaber;Nicolas Tabareau	2015		10.1007/978-3-319-26529-2_15	kripke semantics	Logic	-16.342282505071374	19.899006022678417	165461
24867d97a952009cb9195e40322e3b76cc2f99fc	a clp proof method for timed automata	constraint logic programs;theorem proving constraint handling automata theory inference mechanisms;automata logic programming safety inference mechanisms upper bound inference algorithms real time systems calculus computer languages;assertion proving clp proof method constraint logic programming timed safety automata tsa clp inference method inference mechanism coinduction principle;inference mechanisms;safety properties;theorem proving;upper bound;transition systems;automata theory;constraint handling;timed automata;logic programs;high light	Constraint logic programming (CLP) has been used to model programs and transition systems for the purpose of verification problems. In particular, it has been used to model timed safety automata (TSA). In this paper, we start with a systematic translation of TSA into CLP. The main contribution is an expressive assertion language and a CLP inference method for proving assertions. A distinction of the assertion language is that it can specify important properties beyond traditional safety properties. We highlight one important property: that a system of processes is symmetric. The inference mechanism is based upon the well-known method of tabling in logic programming. It is distinguished by its ability to use assertions that are not yet proven, using a principle of coinduction. Apart from given assertions, the proof mechanism can also prove implicit assertions such as discovering a lower or upper bound of a variable. Finally, we demonstrate significant improvements over state-of-the-art systems using standard TSA benchmark examples.	assertion (software development);automata theory;benchmark (computing);coinduction;constraint logic programming;memoization;timed automaton;whole earth 'lectronic link	Joxan Jaffar;Andrew E. Santosa;Razvan Voicu	2004	25th IEEE International Real-Time Systems Symposium	10.1109/REAL.2004.5	computer science;theoretical computer science;automata theory;proof calculus;automated theorem proving;upper and lower bounds;algorithm	Embedded	-13.793982544103283	25.232740939545355	165568
9bf06ae90833c801fcd974d4a201968ea5961c7d	guarded commands, nondeterminacy and formal derivation of programs	sequencing primitives;programming language semantics;repetition;programming language;building block;case construction;program semantics;programming methodology;termination;repetitive sequence;nondeterminancy;correctness proof;derivation of programs;programming languages	So-called “guarded commands” are introduced as a building block for alternative and repetitive constructs that allow nondeterministic program components for which at least the activity evoked, but possibly even the final state, is not necessarily uniquely determined by the initial state. For the formal derivation of programs expressed in terms of these constructs, a calculus will be be shown.	formal proof;guarded command language;nondeterministic programming	Edsger W. Dijkstra	1975	Commun. ACM	10.1145/360933.360975	first-generation programming language;programming domain;computer science;theoretical computer science;predicate transformer semantics;programming language;program derivation;software development process;algorithm	PL	-16.953072489968058	23.540701548527746	165575
ecfad40f25e1b22dc3d952b5a97d95be5e0125d7	on the simplification and equivalence problems for straight-line programs	hilbert s tenth problem	The sunphficaUon and equivalence problems are examined for several classes of straightline programs. It is shown that the problems are unsolvable for all nontrivlal classes. For example, it is proved that there is no algorithm to determine if an arbRrary program using only the constructs x ,,-1, r , x + y , x ,,-x / y , where x / y ~s integer diwsion with truncation, computes the ftinction which has value l for all integer inputs. The result holds even if one considers only programs with three input variables which compute total 0/l-functions. Thus, under any criteria of smaplification, no algorithm exists for simphfymg such programs When x ,-x + y is replaced by x ~-x y , the number of input variables can be reduced to two This ~s the best possible, since equivalence Is decidable for {x ~1, x ~ x + y, x , , x y , x ~ x * y , x ~-x / y } p r o g r a m s wtth one input variable All the results translate directly to simdar results concermng arithmetic expressions. Categortes and Subject Descriptors: D.3.1 [Programming Languages]: Formal Definitions and Theory; F.2 1 [Analysis of Algorithms and Problem Complexity]: Numerical Algorithms and Problems; F 2 2 [Analysis of Algorithms and Problem Complexity]Nonnumencal Algorithms and Problems; F.3.3 [Logics and Meanings of Programs]: Studies of Program Constructs; F.4.1 [Mathematical I.~gic and Formal Languages]. Mathematical Logic General Terms: Algorithms, Theory AddRional	algorithm;analysis of algorithms;binary logarithm;level of detail;numerical analysis;operand;text simplification;truncation;turing completeness;undecidable problem	Oscar H. Ibarra;Brian S. Leininger	1983	J. ACM	10.1145/2402.322396	computer science;mathematics;hilbert's tenth problem	Theory	-19.02302618623658	19.089443149239212	165621
76c2859734d725a4f8d7e94f33a4bdad32581fc5	clp* and constraint abstraction	constraint logic programs;efficient implementation	CLP*(D) is a class of constraint logic programming languages which incorporates the notion of abstraction. Predicates in CLP*(D) are (potentially) infinite rational trees which represent abstractions of constraint expressions. This view of predicates as constraint abstractions was motivated by the language Scheme, where closures are viewed as abstractions of functional expressions. A semantics and an efficient implementation of the language are provided, along with several examples of the novel programming techniques provided by this class of languages.	constraint logic programming;programming language;scheme	Timothy J. Hickey	1989		10.1145/75277.75288	constraint logic programming;concurrent constraint logic programming;constraint programming;constraint satisfaction;computer science;theoretical computer science;fifth-generation programming language;programming language;algorithm	PL	-16.689718384395853	20.235993863324598	165637
1dc7b4c794f6cc78e19fd63c96f18c9d873ff9f0	safe folding/unfolding with conditional narrowing	semantica operacional;operational semantics;program transformation;logical programming;transformation programme;functional programming;transformacion programa;semantique operationnelle;programmation logique;informatique theorique;partial evaluation;programmation fonctionnelle;functional logic programming;term rewriting;programacion logica;programacion funcional;computer theory;informatica teorica	Functional logic languages with a complete operational semantics are based on narrowing, a generalization of term rewriting where unification replaces matching. In this paper, we study the semantic properties of a general transformation technique called unfolding in the context of functional logic languages. Unfolding a program is defined as the application of narrowing steps to the calls in the program rules in some appropriate form. We show that, unlike the case of pure logic or pure functional programs, where unfolding is correct w.r.t. practically all available semantics, unrestricted unfolding using narrowing does not preserve program meaning, even when we consider the weakest notion of semantics the program can be given. We single out the conditions which guarantee that an equivalent program w.r.t. the semantics of computed answers is produced. Then, we study the combination of this technique with a folding transformation rule in the case of innermost conditional narrowing, and prove that the resulting transformation still preserves the computed answer semantics of the initial program, under the usual conditions for the completeness of innermost conditional narrowing. We also discuss a relationship between unfold/fold transformations and partial evaluation of functional logic programs.	correctness (computer science);lazy evaluation;logic programming;operational semantics;partial evaluation;program transformation;rewriting;unfolding (dsp implementation);unification (computer science)	María Alpuente;Moreno Falaschi;Ginés Moreno;Germán Vidal	1997		10.1007/BFb0026999	discrete mathematics;computer science;artificial intelligence;functional logic programming;database;mathematics;programming language;functional programming;operational semantics;partial evaluation;algorithm	PL	-17.26186074410438	21.9109197205299	165904
aca97e62a98818069580896b5c2cf32c63038d25	challenges and applications of assembly level software model checking		ion functions φ map states S = (S1, . . . , Sk) to patterns φ(S) = (φ(S1), . . . , φ(Sk)). Pattern databases [CS98] are hash tables for fully explored abstract state spaces, storing with each abstract state the shortest path distance in the abstract space to the abstract goal. They are constructed in a complete traversal of the inverse abstract search space graph. Each distance value stored in the hash table is a lower bound on the solution cost in original space and serves as a heuristic estimate. Different pattern databases can be combined either by adding or maximizing the individual entries for a state. Pattern databases work, if the abstraction function is a homomorphism, so that each path in the original state space has a corresponding one in the abstract state space. In difference to the search in original space, the entire abstract space has to be looked at. As pattern databases are themselves hash tables we apply incremental hashing, too. If we restrict the exploration in STRIPS planning to some certain subset of propositions R ⊆ AP , we generate a planning state space homomorphism φ and an abstract planning state space [Ede01] with states SA ⊆ R. Abstractions of operators o = (P,A, D) are defined as φ(o) = (P ∩ R, A ∩ R,D ∩ R). Multiple pattern databases are composed based on a partition AP = R1 ∪ . . . ∪ Rl and induce abstractions φ1, . . . , φl as well as lookup hash tables PDB1,. . . ,PDBl. Two pattern databases are additive, if the sum of the retrieved values is admissible. One sufficient criterion is the following. For every pair of non-trivial operators o1 and o2 in the abstract spaces according to φ1 and φ2, we have that preimage φ−1 1 (o1) differs from φ −1 2 (o2). For pattern database addressing we use a multivariate variable encoding, namely, SAS+ [Hel04]. 6.7 Hashing Dynamic State Vectors In the previous section, we devised an incremental hashing scheme for static state vectors. This is not directly applicable for program model checkers, as they operate on dynamic and structured states. Dynamic means, that the size of a vector may change. For example, a program can dynamically allocate new memory regions. Structured means, that the state is separated in several subvectors rather than a single big vector. In StEAM for example, the stacks, machines, variable sections and the lock/memory pools constitute subvectors which together form a global state vector. In the following, we extend the incremental hashing scheme from the last section to be applicable for dynamic and distributed states. For dynamic vectors, components may be inserted at arbitrary positions. We will regard dynamic vectors as the equivalent of strings over an alphabet Σ. In the following, for two vectors a and b, let a, b denote the concatenation of a and b. For 100 CHAPTER 6. HASHING example, for a = (0, 8) and b = (15), we define a, b = (0, 8, 15). We define four general lemmas for the hash function h as used in Rabin-Karp hashing (cf. Section 6.5.1). Lemmas 1 and 2 relate to the insertion-, lemmas 3 and 4 to the deletion of components. Afterwards, we apply the lemmas to different types of data structures, such as stacks and queues. We use |a| to denote the size of a vector a. Lemma 1. For all a, b, c ∈ Σ∗ we have h(a, b, c) = h(a, c)− h(c) · |Σ||a| + h(b) · |Σ||a| + h(c) · |Σ||a|+|b| mod q.	admissible heuristic;concatenation;cryptographic hash function;data structure;database;eisenstein's criterion;emoticon;hash table;incremental compiler;lookup table;memory pool;model checking;queue (abstract data type);rabin–karp algorithm;strips;shortest path problem;stack (abstract data type);state space;steam;tree traversal;utility functions on indivisible goods;vector graphics	Tilman Mehler	2005	KI		model checking;computer architecture;verification and validation;computer science;theoretical computer science;software development;software construction;programming language;abstraction model checking	Logic	-12.700094568504522	24.18026879466546	166066
5c9099246ca10ec5a3322da7ad2f0ec2f58a7b58	towards smart proof search for isabelle		Despite the recent progress in automatic theorem provers, proof engineers are still suffering from the lack of powerful proof automation. In this position paper we first report our proof strategy language based on a meta-tool approach. Then, we propose an AI-based approach to drastically improve proof automation for Isabelle, while identifying three major challenges we plan to address for this objective. 1 PSL and Meta-Tool Approach In the last decade, we have seen the successful application of automated theorem provers to assist interactive theorem proving. Despite the popularity of these so-called “hammer-style” tools, their performance is still suffering from the gap between underlying logics [1]. To circumvent this problem, we introduced a proof strategy language, PSL [4], to Isabelle/HOL [5], taking a meta-tool approach. A proof strategy is an abstract description of how to attack proof obligations. Users write strategies in PSL based on their intuitions on a conjecture. Using a strategy, PSL’s runtime system generates many invocations of Isabelle’s native proof tools, called tactics. The runtime tries to find out the appropriate combination of tactics for each goal by trial-and-error. This way, PSL reduces the domain specific procedure of interactive theorem proving to the well-known dynamic tree search problem. The meta-tool language approach brought the following advantages: • Domain specific procedures can be added as new sub-tools. • Sub-tools (including Sledgehammer) can be improved independently of PSL. • Generated efficient-proof scripts are native Isabelle proofs. We provided a default strategy, try hard. Our evaluation shows that PSL based on try hard significantly outperforms Sledgehammer for many use cases [4]. However, PSL’s proof search procedure is still mostly brute-force. For example, when PSL generates many tactics, even though each of these is tailored for the proof goal utilizing the runtime information, it is still the statically written strategy, try hard, that decides: • what kind of tactics to generate, • how to combine them, and • in which order to apply generated tactics. 2 Meta-Tool Based Smart Proof Search On a fundamental level, this lack of flexibility stems from the procedural nature of Isabelle’s tactic language, which describes how to prove conjectures in a step-by-step manner rather than what to do with conjectures. However, a conventional declarative language would not be a good solution either, since in many cases we cannot determine what to do with a given proof goal with certainty. Our guess may or may not be right: given a proof goal, we cannot tell which tactics to use until we complete the goal. Probabilistic Declarative Proof Strategy Language. We propose the development of a probabilistic declarative proof strategy language (PDPSL), which allows for the description of feasible tactics for arbitrary proof obligation. For example, when we apply a proof strategy, str, written in PDPSL, to a proof search graph, pgraph, PDPSL’s runtime: (1) picks up the most promising node in pgraph, (2) applies the most promising tactic that has not been applied to that node yet, and (3) adds the resultant node to pgraph, connecting them with a labelled edge representing the tactic application and weight indicating how promising that tactic application is. The runtime keeps applying this procedure until it reaches a solved state, in which the proof is complete. This way, PDPSL executes a best-first search at runtime. The major challenge is the design of PDPSL. A strategy written in PDPSL is applied repeatedly during a best-first search against emerging proof obligations, and we cannot predict how these intermediate goals look like prior to the search; therefore, weighting of tactics against concrete proof goals is not good enough. PDPSL should only describe the meta-information on proof goals and information in Isabelle’s standard library. Such meta-information includes: • Which ITP mechanism was used to define constants that appear in the proof obligation? • Does the conjecture involve recursive functions? This may sound too restrictive; however, when proof engineers use ITPs, we often reason on the meta level. For instance, Isabelle’s tutorial [5] introduces the following heuristics: “Theorems about recursive functions are proved by induction.” “The right-hand side of an equation should (in some sense) be simpler than the left-hand side.” These are independent of user-defined formalisation. Posterior Proof Attempt Evaluation. Evaluating a proof goal to write promising tactics is only half the story of typical interactive proof development. The other half is about evaluating the results of tactic applications. We presume that posterior evaluations on tactic application will improve proof search when combined with prior expectations on tactics described in PDPSL. The main challenge is to find a useful measure by: • discovering (possibly multiple) useful measures that indicate progress in proof search, and • investigating how to integrate these measures into the best-first search of PDPSL. Of course it is not possible to come up with a heuristic that is guaranteed to work for all kinds of proof obligations. But when focusing on a particular problem domain such as systems verification, it is plausible to find a useful measure to narrow the search space at runtime. Reinforcement Learning of PDPSL using Large Proof Corpora. PDPSL is primarily for engineers to encode their intuitions. But at the next step, we plan to improve this handwritten heuristics using reinforcement learning on the existing proof corpora. The Isabelle community has a repository of formal proofs, called the Archive of Formal Proofs (AFP) [3]. As of 2015, the size of the AFP is larger than one million lines of code. Additionally, the seL4 project open-sourced roughly 400,000 lines of Isabelle proof script [2]. We speculate that these large proof corpora will work as the basis for reinforcement learning of strategies written in PDPSL. The lack of concrete information on proof obligations can be an advantage at this stage: since PDPSL cannot describe concrete proof obligations, it is less likely to cause over-fitting.	archive;automated theorem proving;best-first search;brute-force search;declarative programming;domain-specific language;encode;formal proof;formal system;formal verification;hol (proof assistant);heuristic (computer science);interactive proof system;isabelle;l4 microkernel family;link/cut tree;mathematical induction;overfitting;principle of good enough;problem domain;proof assistant;recursion (computer science);reinforcement learning;resultant;run time (program lifecycle phase);runtime system;search problem;source lines of code;standard library;text corpus;whole earth 'lectronic link	Yutaka Nagashima	2017	CoRR		automation;position paper;computer science;artificial intelligence;proof assistant	PL	-16.137175030761437	24.252050294351502	166087
0d434399114b6d00371032fa781bf30f4ed82cc9	a dataflow semantics for constraint logic programs	constraint logic programs;transition systems;logic programs;article in monograph or in proceedings	This paper introduces an alternative operational model for constraint logic programs First a transition system is introduced which is used to de ne a trace semantics T Next an equivalent xpoint se mantics F is de ned a data ow graph is assigned to a program and a consequence operator on tuples of sets of constraints is given whose least xpoint determines one set of constraints for each node of the data ow graph To prove that F and T are equivalent an intermediate semantics O is used which propagates a given set of constraints through the paths of the data ow graph Possible applications of F and O are discussed in particular its incrementality is used to de ne a parallel execution model for clp s based on asynchronous processors assigned to the nodes of the program graph Moreover O is used to formalize the Intermittent Assertion Method of Burstall Bur for constraint logic programs	3d xpoint;central processing unit;dataflow;logic programming;transition system	Livio Colussi;Elena Marchiori;Massimo Marchiori	1995		10.1007/BFb0026834	dynamic logic;concurrent constraint logic programming;constraint programming;classical logic;description logic;horn clause;stable model semantics;computer science;theoretical computer science;computational logic;programming language;axiomatic semantics;prolog;logic programming;multimodal logic;algorithm;philosophy of logic	DB	-16.01878061080426	20.79135388395748	166116
9137dd66bdc93f14084531f903fbbee911fd26f7	automating induction for solving horn clauses		Verification problems of programs in various paradigms can be reduced to problems of solving Horn clause constraints on predicate variables that represent unknown inductive invariants. This paper presents a novel Horn constraint solving method based on inductive theorem proving: the method reduces Horn constraint solving to validity checking of first-order formulas with inductively defined predicates, which are then checked by induction on the derivation of the predicates. To automate inductive proofs, we introduce a novel proof system tailored to Horn constraint solving, and use a PDR-based Horn constraint solver as well as an SMT solver to discharge proof obligations arising in the proof search. We prove that our proof system satisfies the soundness and relative completeness with respect to ordinary Horn constraint solving schemes. The two main advantages of the proposed method are that (1) it can deal with constraints over any background theories supported by the underlying SMT solver, including nonlinear arithmetic and algebraic data structures, and (2) the method can verify relational specifications across programs in various paradigms where multiple function calls need to be analyzed simultaneously. The class of specifications includes practically important ones such as functional equivalence, associativity, commutativity, distributivity, monotonicity, idempotency, and non-interference. Our novel combination of Horn clause constraints with inductive theorem proving enables us to naturally and automatically axiomatize recursive functions that are possibly non-terminating, non-deterministic, higher-order, exception-raising, and over non-inductively defined data types. We have implemented a relational verification tool for the OCaml functional language based on the proposed method and obtained promising results in preliminary experiments.	automated theorem proving;concurrent computing;constraint satisfaction problem;discharger;divergence (computer science);experiment;first-order predicate;functional programming;horn clause;idempotence;imperative logic;imperative programming;interference (communication);mathematical induction;newman's lemma;non-interference (security);ontology components;proof calculus;recursion;recursive definition;solver	Hiroshi Unno;Sho Torii	2017		10.1007/978-3-319-63390-9_30	programming language;algorithm;functional programming;associative property;mathematical proof;horn clause;soundness;automated theorem proving;satisfiability modulo theories;constraint satisfaction problem;computer science	AI	-16.79207408108697	20.547455131375052	166244
3a486c41d8992589cbca036ce30e5adb70e27d30	programs as partial graphs i: flow equivalence and correctness		Abstract   A common feature of most theoretical investigations on semantics, correctness and termination is the strict distinction between the descriptional tool used for the flow of program control and the completely different tool describing single program steps. Since these different methods are difficult to handle when it is necessary that they be used together this paper presents a unified approach to the presentation of these concepts in terms of Tarski's and Riguet's relational algebra. Partial graphs and programs are introduced to formally manipulate relational notions of semantics, correctness and termination. The second part of this paper will extend these notions to systems of recursive programs.	correctness (computer science);turing completeness	Gunther Schmidt	1981	Theor. Comput. Sci.	10.1016/0304-3975(81)90060-8	combinatorics;discrete mathematics;theoretical computer science;mathematics;programming language;algorithm	Theory	-13.549637077139046	21.545273921061522	166248
58a0055ffe5071a1a9105b5050a0998484e69167	a purely logical approach to the termination of imperative loops	verification;programming language semantics;generators;predicate logic purely logical approach imperative loop symbolic execution first order logic program semantic metalevel function;nested loops;semantics;semantics syntactics generators arrays safety cognition programming;theorema system;theorema system program analysis verification symbolic execution semantics induction termination;arrays;symbolic execution;first order;induction;syntactics;cognition;safety;fixed point theory;termination;program analysis;existence and uniqueness;programming;first order logic	We present and illustrate a method for the generation of the termination conditions for nested loops with abrupt termination statements. The conditions are (first-order) formulae obtained by certain transformations of the program text. The loops are treated similarly to calls of recursively defined functions. The program text is analyzed on all possible execution paths by forward symbolic execution using certain meta-level functions which define the syntax, the semantics, the verification conditions for the partial correctness, and the termination conditions. The termination conditions are expressed as induction principles, however, still in first-order logic. Our approach is simpler than others because we use neither an additional model for program execution, nor a fix point theory for the definition of program semantics. Because the meta-level functions are fully formalized in predicate logic, it is possible to prove in a purely logical way and at object level that the verification conditions are necessary and sufficient for the existence and uniqueness of the function implemented by the program.	control flow;correctness (computer science);first-order logic;imperative programming;mathematical induction;mutual recursion;recursion (computer science);recursive definition;rewriting;semantics (computer science);symbolic execution	Madalina Erascu;Tudor Jebelean	2010	2010 12th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2010.64	computer science;theoretical computer science;first-order logic;semantics;programming language;algorithm	PL	-16.865797640838878	21.242487390143445	166874
6af7bcd26801d3885153ff38b6e0725005e1cde5	automated verification = graphs, automata, and logic	graph theory;automated verification;automata theory	"""In automated verification one uses algorithmic techniques to establish the correctness of the design with respect to a given property. Automated verification is based on a small number of key algorithmic ideas, tying together graph theory, automata theory, and logic. In this self-contained talk I will describe how this """"holy trinity"""" gave rise to automated-verification tools."""	automaton;formal verification;rewriting	Moshe Y. Vardi	2000		10.1007/3-540-44622-2_8	discrete mathematics;computer science;graph theory;theoretical computer science;automata theory;intelligent verification;algorithm;functional verification	Logic	-13.850990697678236	25.223335436236784	168148
1c510c8548b0de299de3fce426c4450bcddeb4b1	a decision algorithm for full propositional temporal logic	automatic verification;linear time temporal logic;efficient algorithm;temporal logic;satisfiability;programming model;support system;model checking;proof of correctness	"""The paper presents an eecient algorithm for checking the satissability of a propo-sitional linear time temporal logic formula, which may have past as well as future operators. This algorithm can be used to check validity of such formulas over all models as well as over computations of a nite-state program (model checking). Unlike previous theoretical presentations of a decision method for checking satiss-ability or validity, whose rst step is to construct the full set of all possible atoms of a tableau (satisfaction graph) and immediately pay the worst case exponential complexity price, the algorithm presented here builds the tableau incrementally. This means that the algorithm constructs only those atoms that are reachable from a possible initial atom, satisfying the formula to be checked. While incremental tableau construction for the future fragment of linear time temporal logic can be done in a single pass, the presence of past operators requires multiple passes that successively construct augmented versions of existing atoms, while still maintaining consistency and reachability. The proof of correctness of the algorithm is based on showing that any model of the considered formula is embedded as a path in the tableau at all the construction stages, and can be delineated when the construction terminates. The paper also describes an implementation of the algorithm with further attention to eeciency. This implementation is available as a support system for the book 8] under the name \temporal prover"""". It has been used to verify all the propositional temporal formulas and to model-check all the nite-state programs appearing in the book."""	algorithm;atom;best, worst and average case;computation;correctness (computer science);embedded system;linear temporal logic;long division;method of analytic tableaux;model checking;reachability;time complexity	Yonit Kesten;Zohar Manna;Hugh McGuire;Amir Pnueli	1993		10.1007/3-540-56922-7_9	dynamic logic;zeroth-order logic;model checking;modal μ-calculus;correctness;resolution;linear temporal logic;description logic;logic optimization;horn clause;temporal logic;interval temporal logic;computation tree logic;computer science;theoretical computer science;programming paradigm;programming language;logic programming;multimodal logic;algorithm;temporal logic of actions;autoepistemic logic;satisfiability	Logic	-12.675298262153074	24.038779453124263	168348
151e7a33bd0c56f950ecf11cd4d853157af6fd4b	bmc via on-the-fly determinization	verification;labeled transition system;asynchronous system;bounded model checking;parallel systems;on the fly	This paper develops novel bounded model checking (BMC) techniques for asynchronous parallel systems. The aim is to increase the ef£ciency of BMC by exploiting the inherent concurrency in such systems. This added ef£ciency is gained by covering more reachable states within a given bound using two techniques. Firstly, a non-standard execution model, step executions, where multiple actions can take place simultaneously is applied. Secondly, the number of executions the system can have is reduced by modeling the execution of the system components as if they were determinized. This determinization technique also enables the removal of the internal transitions of the components. Step executions can be further restricted to a subclass called process executions without losing any reachable states. The paper presents a translation scheme for bounded model checking of reachability properties. The translation is from an asynchronous system where the components are modeled as labeled transition systems (LTSs) to a propositional formula. The models of the formula correspond to the step executions of the original system where each component is replaced with its determinized counterpart. The formula for step executions can be easily extended in such a way that its models correspond to the process executions of the system. The translation scheme has been implemented and some experimental comparisons performed. The results show that the bound needed to detect a violation of a reachability property is for step and process executions in most cases lower than in interleaving ? The £nancial support from Nokia Foundation, Helsinki Graduate School of Computer Science and Engineering and the Academy of Finland (Project 53695) is gratefully acknowledged. ?? The £nancial support from Academy of Finland (Project 53695, grant for research work abroad, research fellow post) is gratefully acknowledged. This work has also been £nancially supported by FET project ADVANCE contract No IST-1999-29082 and EPSRC grant 93346/01 (An AutomataTheoretic Approach to Software Model Checking) while the author was af£liated with University of Stuttgart, Institute for Formal Methods in Computer Science. ??? The £nancial support from Academy of Finland (Project 53695) is gratefully acknowledged. executions and that the running time of the model checker using process executions is smaller than using steps. Moreover, the performance compares favorably to a state-of-the-art interleaving BMC implementation in the NuSMV system.	academy;asynchronous system;cardinality (data modeling);computer science;concurrency (computer science);formal methods;forward error correction;intelligent platform management interface;logic programming;model checking;nusmv;petri net;powerset construction;propositional calculus;reachability;semantics (computer science);state (computer science);state space;syntax-directed translation;test case;time complexity	Toni Jussila;Keijo Heljanko;Ilkka Niemelä	2003	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(05)82543-5	asynchronous system;parallel computing;real-time computing;verification;computer science;distributed computing	Logic	-12.733194312462594	24.919468002700317	168699
de0f3df4a2c061173536f3292156dca633d16773	termination analysis of logic programs: extended abstract	termination analysis	  Termination is well-known to be one of the most intriguing aspects of program verification. Since logic programs are Turing-complete,  it follows by the undecidability of the halting problem that there exists no algorithm which, given an arbitrary logic program,  decides whether the program terminates. However, one can propose both conditions that are equivalent to termination and their  approximations that imply termination and can be verified automatically. This paper briefly discusses these kinds of conditions  that were studied in [2].    	logic programming;termination analysis	Alexander Serebrenik	2003		10.1007/978-3-540-24599-5_45	program analysis;computer science;termination analysis;programming language;algorithm	Theory	-14.274794153163617	21.08576582869814	169229
4d7e3212079cf0b05e92c10957de034bd07e022d	from proofs to focused proofs: a modular proof of focalization in linear logic	automated deduction;cut elimination;game semantics;inference rule;first order;proof search;logic programs;linear logic	Probably the most significant result concerning cut-free sequent calculus proofs in linear logic is the completeness of focused proofs. This completeness theorem has a number of proof theoretic applications — e.g. in game semantics, Ludics, and proof search — and more computer science applications — e.g. logic programming, call-by-name/value evaluation. Andreoli proved this theorem for first-order linear logic 15 years ago. In the present paper, we give a new proof of the completeness of focused proofs in terms of proof transformation. The proof of this theorem is simple and modular: it is first proved for MALL and then is extended to full linear logic. Given its modular structure, we show how the proof can be extended to larger systems, such as logics with induction. Our analysis of focused proofs will employ a proof transformation method that leads us to study how focusing and cut elimination interact. A key component of our proof is the construction of a focalization graph which provides an abstraction over how focusing can be organized within a given cut-free proof. Using this graph abstraction allows us to provide a detailed study of atomic bias assignment in a way more refined that is given in Andreoli’s original proof. Permitting more flexible assignment of bias will allow this completeness theorem to help establish the completeness of a number of other automated deduction procedures. Focalization graphs can be used to justify the introduction of an inference rule for multifocus derivation: a rule that should help us better understand the relations between sequentiality and concurrency in linear logic.	automated theorem proving;centrality;computer science;concurrency (computer science);directed acyclic graph;first-order logic;first-order predicate;fixed point (mathematics);game semantics;lightweight java;linear logic;logic programming;mathematical induction;natural deduction;proof calculus;sequent calculus;theory	Dale Miller;Alexis Saurin	2007		10.1007/978-3-540-74915-8_31	formal proof;direct proof;proof by contradiction;linear logic;combinatorial proof;discrete mathematics;computer-assisted proof;probabilistically checkable proof;cut-elimination theorem;geometry of interaction;computer science;game semantics;bunched logic;automated proof checking;analytic proof;proof theory;first-order logic;mathematics;proof calculus;mathematical proof;proof assistant;programming language;structural proof theory;proof complexity;algorithm;rule of inference;original proof of gödel's completeness theorem;statistical proof;gödel's completeness theorem	Logic	-15.204258566419204	18.938772128684523	169318
f727acc57d2465b1038315a948f695f8a6ddfa86	sequential algorithms on concrete data structures	data structure	We provide a sequential denotational semantics for sequential programming languages, based on a new notion of sequential algorithm on the Kahn-Plotkin concrete data structures. Intuitively an algorithm may be seen either as a concrete object—a “program” in a simple output-driven language — or as an abstract object — the pair of a sequential function and of a computation strategy for it. The concrete and abstract presentations are equivalent, as shown by a representation theorem. The algorithms form a cartesian closed category with straightforward solutions to recursive domain equations. Hence they may replace functions in the denotational semantics of any sequential language. An applicative programming language based on sequential algorithms is presented in a companion paper.	algorithm;data structure	G. Berry;Pierre-Louis Curien	1982	Theor. Comput. Sci.	10.1016/S0304-3975(82)80002-9	combinatorics;discrete mathematics;data structure;computer science;theoretical computer science;mathematics;programming language;algorithm	Theory	-12.27330973926714	18.987933141363772	169607
2ace8e58fbb67168c06a364ff4a2b62b53318567	an implementation kernel for theorem proving with equality clauses	theorem proving	keywords: Automated theorem proving, rst-order clauses with equality. Abstract We provide a standard abstract architecture around which high-performance theorem provers for full clausal logic with equality can be built. A WAM-like heap structure for storing terms (as DAG's, with structure sharing) and several substitution trees Gra95b] are central in the architecture. These two data structures turn out to be surprisingly well combinable due to conceptual similarities. Indexing techniques based on substitution trees outperform previous methods, and are integrated in such a way that e.g. no writing on the heap is needed during (many-to-one) term uniication. Static clause (sub)sets can be compiled in this framework into eecient abstract machine code for inference computation and redundancy proving. Finally, as an example, a toy equational completion system based on the framework is described.	abstract machine;automated theorem proving;compiler;computation;data structure;heap (data structure);kernel (operating system);machine code;one-to-many (data model)	Robert Nieuwenhuis;José Miguel Rivero;Miguel Ángel Vallejo	1996			kernel (linear algebra);mathematical analysis;brouwer fixed-point theorem;factor theorem;unit propagation;compactness theorem;fundamental theorem;automated theorem proving;mathematics	Logic	-17.19728211241326	20.012993367331724	169634
32fd6a3d86a3007854705fdd69d4fd1ad60e0e70	on the computational complexity of dynamic slicing problems for program schemas	herbrand domain;computacion informatica;linear schemas;np completeness;ciencias basicas y experimentales;program schemas;matematicas;program slicing;grupo a;article	Given a program, a quotient can be obtained from it by deleting zero or more statements. The field of program slicing is concerned with computing a quotient of a program which preserves part of the behaviour of the original program. All program slicing algorithms take account of the structural properties of a program such as control dependence and data dependence rather than the semantics of its functions and predicates, and thus work, in effect, with program schemas. The dynamic slicing criterion of Korel and Laski requires only that program behaviour is preserved in cases where the original program follows a particular path, and that the slice/quotient follows this path. In this paper we formalise Korel and Laski’s definition of a dynamic slice as applied to linear schemas, and also formulate a less restrictive definition in which the path through the original program need not be preserved by the slice. The less restrictive definition has the benefit of leading to smaller slices. For both definitions, we compute complexity bounds for the problems of establishing whether a given slice of a linear schema is a dynamic slice and whether a linear schema has a non-trivial dynamic slice and prove that the latter problem is NP-hard in both cases. We also give an example to prove that minimal dynamic slices (whether or not they preserve the original path) need not be unique.	algorithm;co-np;co-np-complete;computational complexity theory;data dependency;linear logic;np-completeness;np-hardness;polynomial;procedural programming;program slicing;time complexity	Sebastian Danicic;Robert Mark Hierons;Michael R. Laurence	2011	Mathematical Structures in Computer Science	10.1017/S0960129511000223	program slicing;discrete mathematics;np-complete;computer science;theoretical computer science;mathematics;programming language;algorithm	PL	-15.619037254334497	22.34437301574222	170321
3b31fb4dba679bda0f3bf2656430cfbb9a8561f3	practical stutter-invariance checks for ω-regular languages		An ω-regular language is stutter-invariant if it is closed by the operation that duplicates some letter in a word or that removes some duplicate letter. Model checkers can use powerful reduction techniques when the specification is stutter-invariant. We propose several automata-based constructions that check whether a specification is stutter-invariant. These constructions assume that a specification and its negation can be translated into Büchi automata, but aside from that, they are independent of the specification formalism. These transformations were inspired by a construction due to Holzmann and Kupferman, but that we broke down into two operations that can have different realizations, and that can be combined in different ways. As it turns out, implementing only one of these operations is needed to obtain a functional stutter-invariant check. Finally we have implemented these techniques in a tool so that users can easily check whether an LTL or PSL formula is stutter-invariant.	automata theory;benchmark (computing);büchi automaton;decision problem;list of model checking tools;omega;omega-regular language;semantics (computer science);stutter bisimulation;ω-automaton	Thierry Michaud;Alexandre Duret-Lutz	2015		10.1007/978-3-319-23404-5_7	arithmetic;computer science;theoretical computer science;algorithm	PL	-13.1608597700139	24.48990639285962	170966
624b6c791a778fc6a750f6e1d7f767dfe0d649e5	a categorical approach to structuring and promoting z specifications		In this paper, we study a formalisation of specification structuring mechanisms used in Z. These mechanisms are traditionally understood as syntactic transformations. In contrast, we present a characterisation of Z structuring mechanisms which takes into account the semantic counterpart of their typical syntactic descriptions, based on category theory. Our formal foundation for Z employs well established abstract notions of logical systems. This setting has a degree of abstraction that enables us to understand what is the precise semantic relationship between schemas obtained from a schema operator and the schemas it is applied to, in particular with respect to property preservation. Our formalisation is a powerful setting for capturing structuring mechanisms, even enabling us to formalise promotion. Also, its abstract nature provides the rigour and flexibility needed to characterise extensions of Z and related languages, in particular the heterogeneous ones.	category theory;database schema;subject reduction;z notation	Pablo F. Castro;Nazareno Aguirre;Carlos López Pombo;T. S. E. Maibaum	2012		10.1007/978-3-642-35861-6_5	discrete mathematics;operator (computer programming);structuring;category theory;categorical variable;syntax;schema (psychology);abstraction;mathematics	SE	-12.902493892509062	18.31574337271545	170996
cd0e7a5289f1df2c8ab07ea3cc21705def25ec04	relating full abstraction results for different programming languages	programming language;full abstraction;folk theorem	We prove that Plotkin's model of bottomless cpos and partial continuous functions [19] is fully abstract for PCFv, a call-by-yalue version of the language PCF [17]. This settles a folk theorem which has occasionally been misunderstood. We then show that the (known) full abstraction results for lazy PCF [2] and PCF with control [22] can be derived as corollaries from this theorem. Such a connection is particularly surprising, because observational congruence—the central notion in the definition of full abstraction—is usually not preserved by many known translations between different programming languages. We expect that new full abstraction theorems for some extensions of PCFv can be derived in the same way.	denotational semantics	Kurt Sieber	1990		10.1007/3-540-53487-3_58	natural language processing;fourth-generation programming language;intentional programming;first-generation programming language;declarative programming;very high-level programming language;programming domain;reactive programming;computer science;folk theorem;third-generation programming language;functional logic programming;programming paradigm;low-level programming language;inductive programming;fifth-generation programming language;programming language theory;programming language;abstraction principle;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm	PL	-15.982342452078385	19.635053787532694	171065
e23d5b923ba4e779f7f3a73f30264f060220e735	delay analysis in synchronous programs	synchronous programming;programming language;logic programs;static analysis;abstract interpretation	"""Linear relation analysis [CH78, Hal79] has been proposed a long time ago as an abstract interpretation which permits to discover linear relations invarianfly satisfied by the variables of a program. Here, we propose to apply this general method to variables used to count delays in synchronous programs. The """"regular"""" behavior of these counters makes the results of the analysis especially precise. These results can be applied to code optimization and to the verification of real-tlme properties of programs."""	abstract interpretation;mathematical optimization;program optimization	Nicolas Halbwachs	1993		10.1007/3-540-56922-7_28	real-time computing;declarative programming;programming domain;reactive programming;computer science;programming language implementation;extensible programming;functional logic programming;signal programming;computer programming;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;static analysis;algorithm;control flow analysis	Logic	-17.821101381914918	24.82301048706224	171300
079034a44a3a03b964f60dc6f1b282aa19fb1b1c	syntactic theories in practice	evaluation function;time complexity;continuation passing style;transitive closure	The evaluation function of a syntactic theory is canonically defined as the transitive closure of (1) decomposing a program into an evaluation context and a redex, (2) contracting this redex, and (3) plugging the result in the context. Directly implementing this evaluation function therefore yields an interpreter with a quadratic time factor over its input. We present sufficient conditions over a syntactic theory to circumvent this quadratic factor, and we illustrate the method with two programminglanguage interpreters and a transformation into continuation-passing style (CPS). As a byproduct, the time complexity of this CPS transformation is mechanically changed from quadratic to linear. We also flesh out a new connection between continuations and evaluation contexts, using Reynolds’s defunctionalization. ∗Extended version of an article to appear in the informal proceedings of the Second International Workshop on Rule-Based Programming (RULE 2001), Firenze, Italy, September 4, 2001. †Basic Research in Computer Science (www.brics.dk), funded by the Danish National Research Foundation. ‡Ny Munkegade, Building 540, DK-8000 Aarhus C, Denmark E-mail: {danvy,lrn}@brics.dk	computer science;continuation;continuation-passing style;defunctionalization;denotational semantics;evaluation function;interpreter (computing);legendre transformation;model transformation;operational semantics;programming language;reduction strategy (code optimization);serializability;structural induction;time complexity;transitive closure	Olivier Danvy;Lasse R. Nielsen	2001	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)00297-X	time complexity;computer science;continuation-passing style;evaluation function;mathematics;programming language;transitive closure;algorithm	PL	-13.233167735367266	22.428463588022996	171784
92354a3350e26fef78dcd6588213a1efefcf0e66	controlled use of clausal lemmas in connection tableau calculi	computacion informatica;search space;model elimination;ciencias basicas y experimentales;partial evaluation;grupo a	Proof procedures based on model elimination or the connection tableau calculus have become more and more successful. But these procedures still suffer from long proof lengths as well as from a rather high degree of redundant search effort in comparison with resolution-style search procedures. In order to overcome these problems we investigate the use of clausal lemmas. We develop a method to augment a given set of clauses by a lemma set in a preprocessing phase and discuss the ability of this technique to reduce proof lengths and depths and to provide an appropriate reordering of the search space. We deal with the basic connection tableau calculus as well as with several calculus refinements and extensions. In order to control the use of lemmas we develop techniques for selecting relevant lemmas based on partial evaluation techniques. Experiments with the prover Setheo performed in several domains demonstrate the high potential of our approach. c © 2000 Academic Press	method of analytic tableaux;model elimination;partial evaluation;preprocessor	Marc Fuchs	2000	J. Symb. Comput.	10.1006/jsco.1999.0363	discrete mathematics;mathematics;partial evaluation;algorithm;algebra	AI	-16.01635249991971	23.293523449211765	171825
3a2e729edf5eadf8743f326645240d201dd25434	contributions to the theory of logic programming	programming language;theorem prover;first order;logic programs	Hom clauses of first-order predicate logic can be regarded as a high-level programming language when SLD-resolution, a special-purpose resolution theorem prover, is used as interpreter. Consequently, the semantics of Hom clauses can be studied both by model-theoretic and fixpoint methods (in the sense of Scott). This possibility is exploited here by identifying the least (greatest) fixpoint with a least (greatest) model. Successful termination of SLD-resolution is characterized by least fupoints. A semantic characterization of finite failure of SLD-resolution is given, which coincides with the greatest fixpoint only for a special case of clauses. It is shown that nondeterministic flowchart schemata of bounded nondeterminacy are modeled by this special case; the connection between finite failure and greatest fixpoint is then used to give a semantic characterization of termination, blocking, and nontermination of such flowchart schemata.	automated theorem proving;blocking (computing);divergence (computer science);first-order logic;first-order predicate;fixed point (mathematics);flowchart;high- and low-level;high-level programming language;least fixed point;logic programming;sld resolution;theory	Krzysztof R. Apt;M. H. van Emden	1982	J. ACM	10.1145/322326.322339	dynamic logic;concurrent constraint logic programming;first-generation programming language;description logic;declarative programming;horn clause;programming domain;computer science;theoretical computer science;functional logic programming;first-order logic;computational logic;mathematics;signature;automated theorem proving;programming paradigm;procedural programming;inductive programming;fifth-generation programming language;programming language;prolog;logic programming;second-order logic;algorithm;philosophy of logic	Theory	-16.01918161848229	18.86645323457199	172040
c1cf5e77127c5f36c0926ddf1614d4c312158483	equivalence checking of arithmetic expressions using fast evaluation	computability theory;building block;software systems;general techniques;limit set;mutual exclusion;decision problem;expression equivalence;partial evaluation;high level language;equivalence checking;interval analysis	Arithmetic expressions are the fundamental building blocks of hardware and software systems. An important problem in computational theory is to decide if two arithmetic expressions are equivalent. However, the general problem of equivalence checking, in digital computers, belongs to the NP Hard class of problems. Moreover, existing general techniques for solving this decision problem are applicable to very simple expressions and impractical when applied to more complex expressions found in programs written in high-level languages. In this paper we propose a method for solving the arithmetic expression equivalence problem using partial evaluation. In particular, our technique is specifically designed to solve the problem of equivalence checking of arithmetic expressions obtained from high-level language descriptions of hardware/software systems, which consists of regular arithmetic operators (+, -, x) and logical operators (and, or, not). In our method, we use interval analysis to substantially prune the domain space of arithmetic expressions and limit the evaluation effort to a sufficiently limited set of subspaces. Our results show that the proposed method is fast enough to be of use in practice.	arithmetic logic unit;computer;decision problem;emoticon;formal equivalence checking;high- and low-level;high-level programming language;interval arithmetic;logical connective;np-hardness;partial evaluation;software system;theory of computation;turing completeness	Mohammad Ali Ghodrat;Tony Givargis;Alexandru Nicolau	2005		10.1145/1086297.1086317	logical equivalence;limit set;arbitrary-precision arithmetic;computability theory;mutual exclusion;computer science;decision problem;formal equivalence checking;programming language;partial evaluation;high-level programming language;algorithm;software system	PL	-15.4738036604072	23.084600616624133	172806
3f4dd8f3e6e0b5e9657ebbdcfdf2915815775664	circ: a behavioral verification tool based on circular coinduction	theorem proving;data structure	CIRC is a tool for automated inductive and coinductive theorem proving. It includes an engine based on circular coinduction, which makes CIRC particularly well-suited for proving behavioral properties of infinite data-structures. This paper presents the current status of the coinductive features of the CIRC prover, focusing on new features added over the last two years. The presentation is by examples, showing how CIRC can automatically prove behavioral properties.	automated theorem proving;binary tree;coinduction;formal proof;inductive reasoning;proof calculus;whole earth 'lectronic link	Dorel Lucanu;Eugen-Ioan Goriac;Georgiana Caltais;Grigore Rosu	2009		10.1007/978-3-642-03741-2_30	discrete mathematics;coinduction;data structure;computer science;mathematics;automated theorem proving;programming language;algorithm	Logic	-13.794104586535891	22.384540533343817	173402
8fcc476131fded5bc14ff8ef82f5268f5f87b5bb	advanced automata-based algorithms for program termination checking		"""In 2014, Heizmann et al. proposed a novel framework for program termination analysis. The analysis starts with a termination proof of a sample path. The path is generalized to a Büchi automaton (BA) whose language (by construction) represents a set of terminating paths. All these paths can be safely removed from the program. The removal of paths is done using automata difference, implemented via BA complementation and intersection. The analysis constructs in this way a set of BAs that jointly """"cover"""" the behavior of the program, thus proving its termination. An implementation of the approach in Ultimate Automizer won the 1st place in the Termination category of SV-COMP 2017.   In this paper, we exploit advanced automata-based algorithms and propose several non-trivial improvements of the framework. To alleviate the complementation computation for BAs---one of the most expensive operations in the framework---, we propose a multi-stage generalization construction. We start with generalizations producing subclasses of BAs (such as deterministic BAs) for which efficient complementation algorithms are known, and proceed to more general classes only if necessary. Particularly, we focus on the quite expressive subclass of semideterministic BAs and provide an improved complementation algorithm for this class. Our experimental evaluation shows that the proposed approach significantly improves the power of termination checking within the Ultimate Automizer framework."""	algorithm;business architecture;büchi automaton;computation;rewriting;systemverilog;termination analysis	Yu-Fang Chen;Matthias Heizmann;Ondrej Lengál;Yong Li;Ming-Hsien Tsai;Andrea Turrini;Lijun Zhang	2018		10.1145/3192366.3192405	computer science;subclass;computation;normalization property;exploit;if and only if;büchi automaton;generalization;algorithm;termination analysis	Logic	-12.303050051982241	24.49971571320596	173706
8eac47917c148e2b8cc36cfa4d3bdca813e11aa6	on qe algorithms over an algebraically closed field based on comprehensive gröbner systems		We introduce new algorithms for quantifier eliminations (QE) in the domain of an algebraically closed field. Our algorithms are based on the computation of comprehensive Gröbner systems (CGS). We study Suzuki– Sato’s CGS computation algorithm and its successors in more detail and modify them into an optimal form for applying to QE. Based on this modified algorithm, we introduce two QE algorithms. One is pursuing the simplest output quantifier free formula which employs only the computations of CGS. The other consists of parallel computations of CGS and GCD of parametric unary polynomials. It achieves faster computation time with reasonably simple output quantifier free formulas. Our implementation shows that in many examples our algorithms are superior to other existing algorithms such as a QE algorithm adopted in the Mathematica package Reduce and Resolve or a QE algorithm adopted in the Maple package Projection which are the most efficient existing implementations as far as we know.	algorithm;approximation algorithm;computation;experiment;for loop;level of detail;maple;polynomial;quadratic equation;quantifier (logic);query expansion;recursion (computer science);regular chain;terminate (software);time complexity;unary operation;wolfram mathematica	Ryoya Fukasaku;Shutaro Inoue;Yosuke Sato	2015	Mathematics in Computer Science	10.1007/s11786-015-0237-x	combinatorics;mathematical analysis;discrete mathematics;computer science;theoretical computer science;mathematics;geometry;algorithm;algebra	AI	-15.81948298481897	24.525908126055686	174553
a6cae7a3b615d23612f9e98d0682780bd0cf9bad	algorithmic analysis of array-accessing programs	algorithm analysis;software model checking;nested loops;program verication	For programs whose data variables range over Boolean or finite domains, program verification is decidable, and this forms the basis of recent tools for software model checking. In this paper, we consider algorithmic verification of programs that use Boolean variables, and in addition, access a single array whose length is potentially unbounded, and whose elements range over pairs from Σ × D, where Σ is a finite alphabet and D is a potentially unbounded data domain. We show that the reachability problem, while undecidable in general, is (1) Pspace-complete for programs in which the array-accessing for-loops are not nested, (2) solvable in Expspace for programs with arbitrarily nested loops if array elements range over a finite data domain, and (3) decidable for a restricted class of programs with doubly-nested loops. The third result establishes connections to automata and logics defining languages over data words. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-08-35. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/894 Algorithmic Analysis of Array-Accessing	abstract interpretation;automata theory;automaton;complexity class;data domain;data structure;decision problem;expspace;formal verification;information and computer science;information science;java platform, micro edition;linked list;mobile device;model checking;pspace-complete;reachability problem;software verification;tree (data structure)	Rajeev Alur;Pavol Cerný;Scott Weinstein	2009		10.1007/978-3-642-04027-6_9	discrete mathematics;nested loop join;computer science;theoretical computer science;mathematics;programming language;algorithm	Logic	-12.69537799158766	24.371251908980852	174865
1c8f350811227f5582f8b49f07e6d4d0dd146c8a	parallelism and synchronization in an infinitary context		We study multitoken interaction machines in the context of a very expressive linear logical system with exponentials, fix points and synchronization. The advantage of such machines is to provide models in the style of the Geometry of Interaction, i.e., An interactive semantics which is close to low-level implementation. On the one hand, we prove that despite the inherent complexity of the framework, interaction is guaranteed to be deadlock-free. On the other hand, the resulting logical system is powerful enough to embed PCF and to adequately model its behaviour, both when call-by-name and when call-by-value evaluation are considered. This is not the case for single-token stateless interactive machines.	a-normal form;beta normal form;computation;database normalization;deadlock;fixed point (mathematics);formal system;geometry of interaction;high- and low-level;lambda calculus;plotkin bound;programming computable functions;programming paradigm;quantum computing;quantum entanglement;recursion;rewriting;simpson's rule;stateless protocol;time complexity;well-formed formula	Ugo Dal Lago;Claudia Faggian;Benoît Valiron;Akira Yoshimizu	2015	2015 30th Annual ACM/IEEE Symposium on Logic in Computer Science		linear logic;geometry of interaction;computer science;theoretical computer science;distributed computing;abstract machine;programming language;algorithm	Logic	-12.096168494605015	20.530968894987545	176775
25e2daa45459809256f8b36769accbcc4240b8e5	on the implementation of construction functions for non-free concrete data types	programming language;data type;satisfiability;pattern matching;logic in computer science	Many algorithms use concrete data types with some additional invariants. The set of values satisfying the invariants is often a set of representatives for the equivalence classes of some equational theory. For instance, a sorted list is a particular representative wrt commutativity. Theories like associativity, neutral element, idempotence, etc. are also very common. Now, when one wants to combine various invariants, it may be difficult to find the suitable representatives and to efficiently implement the invariants. The preservation of invariants throughout the whole program is even more difficult and error prone. Classically, the programmer solves this problem using a combination of two techniques: the definition of appropriate construction functions for the representatives and the consistent usage of these functions ensured via compiler verifications. The common way of ensuring consistency is to use an abstract data type for the representatives; unfortunately, pattern matching on representatives is lost. A more appealing alternative is to define a concrete data type with private constructors so that both compiler verification and pattern matching on representatives are granted. In this paper, we detail the notion of private data type and study the existence of construction functions. We also describe a prototype, called Moca, that addresses the entire problem of defining concrete data types with invariants: it generates efficient construction functions for the combination of common invariants and builds representatives that belong to a concrete data type with private constructors.		Frédéric Blanqui;Thérèse Hardin;Pierre Weis	2007		10.1007/978-3-540-71316-6_8	discrete mathematics;data type;computer science;artificial intelligence;pattern matching;mathematics;programming language;algorithm;satisfiability	Logic	-14.863258038657028	19.150007132561644	177333
86f1063f73ded85a6de913bd7308ed0dd5064563	an exercise in the transformational derivation of an efficient program by joing development of control and data structure	data structure	Abstract   Formal program development by transformations comprises not only transitions between equivalent control constructs but also suitable changes of data structures. In principle these two (conceptually different) kinds of derivation steps can be done rather independent of each other. If, however, efficient programs are aimed at, it is vitally important to suitably intertwine them in order to benefit from their mutual influence. For this way of ‘joint development’ the paper aims at elaborating a kind of guide-line the practical use of which is illustrated by means of a non-trivial example.	data structure	Helmuth Partsch	1983	Sci. Comput. Program.	10.1016/0167-6423(83)90002-3	data structure;computer science;programming language;algorithm	SE	-14.29959285793986	19.432111015610808	177349
711083dd09bc72c56bea93e4c09dc2664e3d6ea3	efficient generation of craig interpolants in satisfiability modulo theories	decision procedure;smt;experimental evaluation;decision procedures;satisfiability modulo theories;first order logic;craig interpolation	The problem of computing Craig interpolants has recently received a lot of interest. In this article, we address the problem of efficient generation of interpolants for some important fragments of first-order logic, which are amenable for effective decision procedures, called satisfiability modulo theory (SMT) solvers.  We make the following contributions. First, we provide interpolation procedures for several basic theories of interest: the theories of linear arithmetic over the rationals, difference logic over rationals and integers, and UTVPI over rationals and integers. Second, we define a novel approach to interpolate combinations of theories that applies to the delayed theory combination approach.  Efficiency is ensured by the fact that the proposed interpolation algorithms extend state-of-the-art algorithms for satisfiability modulo theories. Our experimental evaluation shows that the MathSAT SMT solver can produce interpolants with minor overhead in search, and much more efficiently than other competitor solvers.	algorithm;first-order logic;first-order predicate;interpolation;modulo operation;overhead (computing);satisfiability modulo theories;solver;theory;ut-vpn	Alessandro Cimatti;Alberto Griggio;Roberto Sebastiani	2010	ACM Trans. Comput. Log.	10.1145/1838552.1838559	discrete mathematics;computer science;first-order logic;mathematics;programming language;satisfiability modulo theories;algorithm	Logic	-16.00251247082942	24.7503451818701	177429
1b3f696f0eb8a16285432ac6a2c7df2d87bd7b95	fm 2014: formal methods		A bidirectional transformation consists of pairs of transformations —a forward transformation get produces a target view from a source, while a putback transformation put puts back modifications on the view to the source— satisfying sensible roundtrip properties. Existing bidirectional approaches are get-based in that one writes (an artifact resembling) a forward transformation and a corresponding backward transformation can be automatically derived. However, the unavoidable ambiguity that stems from the underspecification of put often leads to unpredictable bidirectional behavior, making it hard to solve nontrivial practical synchronization problems with existing bidirectional transformation approaches. Theoretically, this ambiguity problem could be solved by writing put directly and deriving get , but differently from programming with get it is easy to write invalid put functions. An open challenge is how to check whether the definition of a putback transformation is valid, while guaranteeing that the corresponding unique get exists. In this paper, we propose, as far as we are aware, the first safe language for supporting putback-based bidirectional programming. The key to our approach is a simple but powerful language for describing primitive putback transformations. We show that validity of putback transformations in this language is decidable and can be automatically checked. A particularly elegant and strong aspect of our design is that we can simply reuse and apply standard results for treeless functions and tree transducers in the specification of our checking algorithms.	algorithm;bidirectional transformation;fm broadcasting;formal methods;program transformation;transducer	Cliff B. Jones;Pekka Pihlajasaari;Jun Sun	2014		10.1007/978-3-319-06410-9	natural language processing;formal methods;artificial intelligence;computer science	PL	-17.657780748154376	22.335574473158438	177706
4d4385bf8c97fa89f68648b5eb377f609ac34ddc	input-output model programs	abstract state machine;theorem proving;input output;software development	Model programs are used as high-level behavioral specifications typically representing abstract state machines. For modeling reactive systems, one uses input-output model programs, where the action vocabulary is divided between two conceptual players: the input player and the output player. The players share the action vocabulary and make moves that are labeled by actions according to their respective model programs. Conformance between the two model programs means that the output (input) player only makes output (input) moves that are allowed by the input (output) players model program. In a bounded game, the total number of moves is fixed. Here model programs use a background theory T containing linear arithmetic, sets, and tuples. We formulate the bounded game conformance checking problem, or BGC, as a theorem proving problem modulo T and analyze its complexity.	abstract state machines;automated theorem proving;conformance testing;high- and low-level;modulo operation;vocabulary	Margus Veanes;Nikolaj Bjørner	2009		10.1007/978-3-642-03466-4_21	input/output;discrete mathematics;computer science;theoretical computer science;software development;mathematics;automated theorem proving;programming language;algorithm;abstract state machines	Logic	-12.442613790577253	24.672978640667374	178083
5bb66955dd186e5d46a54751034fe905f0707d8d	an investigation of hilbert's implicit reasoning through proof discovery in idle-time	proof space;tedious lemma;prose argument;new discovery tool;readable formalised proof-scripts;incomplete proof;incidence reasoning;interactive proof assistant;inferring fact;proof discovery;implicit reasoning;alternative proof	In this paper, we describe how we captured and investigated incidence reasoning in Hilbert's Foundations of Geometry by using a new discovery tool integrated into an interactive proof assistant. Our tool exploits concurrency, inferring facts independently of the user with the incomplete proof as a guide. It explores the proof space, contributes tedious lemmas and discovers alternative proofs. We show how this tool allowed us to write readable formalised proof-scripts that correspond very closely to Hilbert's prose arguments.		Phil Scott;Jacques D. Fleuriot	2010		10.1007/978-3-642-25070-5_11	computer science;artificial intelligence;theoretical computer science;mathematics;proof assistant;algorithm	Logic	-17.8126290308652	21.77560992126403	178254
48ccd0185337386cc07f6faba0921ef3829ecb3a	zres: the old davis-putman procedure meets zbdd	automatic proving;deduction automatique;demostracion automatica;logical programming;specification programme;theorem proving;demonstration automatique;demonstration theoreme;performance programme;programmation logique;eficacia programa;pigeon;program performance;demostracion teorema;program specification;programacion logica;automatic deduction;especificacion programa	ZRES is a propositional prover based on the original procedure of Davis and Putnam, as opposed to its modified version of Davis, Logeman and Loveland, on which most of the current efficient SAT provers are based. On some highly structured SAT instances, such as the well known Pigeon Hole and Urquhart problems, both proved hard for resolution, ZRES performs very well and surpasses all classical SAT provers by an order of magnitude.	zero-suppressed decision diagram	Philippe Chatalic;Laurent Simon	2000		10.1007/10721959_35	computer science;artificial intelligence;mathematics;automated theorem proving;programming language;algorithm	Theory	-18.281849307614596	20.789885218308505	178334
f2e192bde173b8ceb7abd2307392adb21fc2a402	non-deterministic planning with conditional effects	strong cyclic planning;fond;conditional effects;non deterministic planning;state relevance	Recent advances in fully observable non-deterministic (FOND) planning have enabled new techniques for various applications, such as behaviour composition, among others. One key limitation of modern FOND planners is their lack of native support for conditional effects. In this paper we describe an extension to PRP, the current state of the art in FOND planning, that supports the generation of policies for domains with conditional effects and non-determinism. We present core modifications to the PRP planner for this enhanced functionality without sacrificing soundness and completeness. Additionally, we demonstrate the planner’s capabilities on a variety of benchmarks that include actions with both conditional effects and non-deterministic outcomes. The resulting planner opens the door to models of greater expressivity, and does so without affecting PRP’s efficiency.	benchmark (computing);nondeterministic algorithm;observable;parallel redundancy protocol;soundness (interactive proof)	Christian J. Muise;Sheila A. McIlraith;Vaishak Belle	2014			computer science;artificial intelligence;algorithm	AI	-14.541970424408126	23.607792542654135	178858
ccd2d45c0daca902634c9f90aabd3884db1d6c7e	deducing fairness properties in unity logic—a new completeness result	verification;regle inference;logica temporal;temporal logic;specification;unity;ingenieria logiciel;program verification;raisonnement;software engineering;inference rule;verificacion programa;specification and verification;logica algoritmica;especificacion;fairness properties;transition systems;razonamiento;genie logiciel;completitud;algorithmic logic;completeness;reasoning;verification programme;completude;logique algorithmique;logique temporelle;regla inferencia	We explore the use of UNITY logic in specifying and verifying fairness properties of UNITY and UNITY-like programs whose semantics can be modeled by weakly fair transition systems. For such programs, strong fairness properties in the form of “if <?Pub Fmt italic>p<?Pub Fmt /italic> holds infinitely often then <?Pub Fmt italic>q<?Pub Fmt /italic> also holds infinitely often <inline-equation> <f> □◊p⇒□◊q</f> </inline-equation>, can be expressed as conditional UNITY properties of the form of “Hypothesis: <inline-equation> <f> true→p Conclusion:true→q</f> </inline-equation>”. We show that UNITY logic is relatively complete for proving such properties; in the process, a simple inference rule is derived. Specification and  verification of weak fairness properties are also discussed.	fairness measure;formal specification;unity;verification and validation	Yih-Kuen Tsay;Rajive L. Bagrodia	1995	ACM Trans. Program. Lang. Syst.	10.1145/200994.200997	verification;temporal logic;completeness;computer science;theoretical computer science;specification;reason;algorithm;rule of inference	PL	-13.909442306719935	22.00508669681218	179358
022ebe35bf0ca2e38173955783627e9b5167e4f1	discovering properties about arrays in simple programs	verification;lenguaje programacion;dependence analysis;analyse statique;programming language;sorting;semantics;paralelisacion;interpretacion abstracta;tria;program verification;semantica;semantique;analisis estatica;sentinel;arrays;verificacion programa;invariant synthesis;parallelisation;theory;indexation;triage;sorting algorithms;parallelization;langage programmation;invariante;interpretation abstraite;static analysis;abstract interpretation;verification programme;sorting algorithm;invariant	"""Array bound checking and array dependency analysis (for parallelization) have been widely studied. However, there are much less results about analyzing properties of array <i>contents</i>. In this paper, we propose a way of using abstract interpretation for <i>discovering</i> properties about array contents in some restricted cases: one-dimensional arrays, traversed by simple """"for"""" loops. The basic idea, borrowed from [GRS05], consists in partitioning arrays into symbolic intervals (e.g., [1,<i>i</i> -- 1], [<i>i</i>,<i>i</i>], [<i>i</i> + 1,<i>n</i>]), and in associating with each such interval <i>I</i> and each array <i>A</i> an abstract variable <i>A</i><sub><i>I</i></sub>; the new idea is to consider <i>relational</i> abstract properties ψ(<i>A</i><sub><i>I</i></sub>, <i>B</i><sub><i>I</i></sub>, ...) about these abstract variables, and to interpret such a property pointwise on the interval <i>I</i>: ∀<i>l</i> ∈ <i>I</i>, ψ(<i>A</i>[<i>l</i>], <i>B</i>[<i>l</i>],...). The abstract semantics of our simple programs according to these abstract properties has been defined and implemented in a prototype tool. The method is able, for instance, to discover that the result of an insertion sort is a sorted array, or that, in an array traversal guarded by a """"sentinel"""", the index stays within the bounds."""	abstract interpretation;array data structure;dependence analysis;insertion sort;parallel computing;prototype;sorted array;tree traversal	Nicolas Halbwachs;Mathias Péron	2008		10.1145/1375581.1375623	parallel computing;computer science;theoretical computer science;sorting algorithm;semantics;programming language;algorithm	Logic	-18.5186428860313	24.69710179795528	179880
8ae9b7dcaf18d273ca31ebcd786aa066a66f2525	the z-polyhedral model	computer program;program transformation;program verification;programming model;polyhedral model;equational programming;models of computations;loop optimization	The polyhedral model is a well developed formalism and has been extensively used in a variety of contexts viz. the automatic parallelization of loop programs, program verification, locality, hardware generationand more recently, in the automatic reduction of asymptotic program complexity. Such analyses and transformations rely on certain closure properties. However, the model is limited in expressivity and the need for a more general class of programs is widely known.  We provide the extension to ⁰-polyhedra which are the intersection of polyhedra and lattices. We prove the required closure properties using a novel representation and interpretation of ⁰-polyhedra. In addition, we also prove closure in the ⁰-polyhedral model under images by dependence functions---thereby proving that unions of LBLs, widely assumedto be a richer class of sets, is equal to unions of ⁰-polyhedra. Another corollary of this result is the equivalence of the unions of ⁰-polyhedraand Presburger sets. Our representation and closure properties constitute the foundations of the ⁰-polyhedral model. As an example, we presentthe transformation for automatic reduction of complexity in the ⁰-polyhedral model.	automatic parallelization;code generation (compiler);expressive power (computer science);formal verification;functional programming;fuzzy set;interpretation (logic);locality of reference;parallel computing;polyhedron;polytope model;regular expression;scheduling (computing);semantics (computer science);theory;turing completeness;viz: the computer game	Gautam Gupta;Sanjay V. Rajopadhye	2007		10.1145/1229428.1229478	parallel computing;computer science;loop optimization;theoretical computer science;programming paradigm;programming language;algorithm	PL	-11.97186563699898	20.520874398816904	179961
0b860d48b0704638162a06b6c32a0d90c822f6d1	a logic of agent programs	cognitive agents;operational semantics;complete axiomatization;theorem prover;agent programming;agent programming language	We present a sound and complete logic for reasoning about SimpleAPL programs. SimpleAPL is a fragment of the agent programming language 3APL designed for the implementation of cognitive agents with beliefs, goals and plans. Our logic is a variant of PDL, and allows the specification of safety and liveness properties of agent programs. We prove a correspondence between the operational semantics of SimpleAPL and the models of the logic for two example program execution strategies. We show how to translate agent programs written in SimpleAPL into expressions of the logic, and give an example in which we show how to verify correctness properties for a simple agent program.	3apl;correctness (computer science);liveness;operational semantics;programming language	Natasha Alechina;Mehdi Dastani;Brian Logan;John-Jules Ch. Meyer	2007			computer science;theoretical computer science;belief–desire–intention software model;automated theorem proving;programming language;logic programming;operational semantics;algorithm	AI	-17.10906834157065	21.66371811416777	180183
2a80da9e4324e33f876505d273919bce7ed49921	subrecursive program schemata i & ii: i. undecidable equivalence problems; ii. decidable equivalence problems	programming language;analysis of algorithm;fortran	The study of program schemata and the study of subrecursive programming languages are both concerned with limiting program structure in order to permit a more complete analysis of algorithms while retaining sufficiently rich computing power to allow interesting algorithms. In this paper we combine these approaches by defining classes of subrecursive program schemata and investigating their equivalence problems. Since the languages are all subrecursive, any scheme written in any one of them must halt (as long as we assume the basic functions and predicates are all total). Hence equivalence of schemes is the first question of interest we can ask about these languages.  We consider schematic versions of various subrecursive programming languages similar to the Loop language. We distinguish between Pre-Loop and Post-Loop languages on the basis of whether the exit condition in an iteration loop is tested before iteration, as in Algol (Pre-), or after iteration, as in FORTRAN (Post-). We show that at the program level all these languages have the same computing power (the primitive recursive functions) and all have unsolvable equivalence problems (of arithmetic degree π01). But at the level of schemes, Pre-Loop has an unsolvable equivalence problem, while at least one formulation of Post-Loop has a solvable equivalence problem.	algorithm;analysis of algorithms;decision problem;fortran;halting problem;iteration;primitive recursive function;programme level;programming language;recursion;schematic;structured programming;turing completeness;undecidable problem	Robert L. Constable;Steven S. Muchnick	1972		10.1145/800152.804892	combinatorics;discrete mathematics;computer science;mathematics;programming language;algorithm	Theory	-15.69942621151925	21.58324727466997	180578
a1a774793ed679bd4cdb14b602f84d7314438a55	timing and causality in process algebra	simultaneidad informatica;concurrency;causalite;transition systems;algebre processus;process algebra;simultaneite informatique;causality;causalidad	There has been considerable controversy in concurrency theory between the ‘interleaving’ and ‘true concurrency’ schools. The former school advocates associating a transition system with a process which captures concurrent execution via the interleaving of occurrences; the latter adopts more complex semantic structures to avoid reducing concurrency to interleaving. In this paper we show that the two approaches are not irreconcilable. We define a timed process algebra where occurrences are associated with intervals of time, and give it a transition system semantics. This semantics has many of the advantages of the interleaving approach; the algebra admits an expansion theorem, and bisimulation semantics can be used as usual. Our transition systems, however, incorporate timing information, and this enables us to express concurrency: merely adding timing appropriately generalises transition systems to asynchronous transition systems, showing that time gives a link between true concurrency and interleaving. Moreover, we can provide a complete axiomatisation of bisimulation for our algebra; a result that is often problematic in a timed setting. Another advantage of incorporating timing information into the calculus is that it allows a particularly simple definition of action refinement; this we present. The paper concludes with a comparison of the equivalence we present with those in the literature, and an example system specification in our formalism.	axiomatic system;bisimulation;causality;concurrency (computer science);forward error correction;norm (social);process calculus;refinement (computing);semantics (computer science);transition system;turing completeness	Luca Aceto;David Murphy	1996	Acta Informatica	10.1007/s002360050047	process calculus;real-time computing;causality;concurrency;computer science;theoretical computer science;programming language;algorithm	Logic	-12.50844227275631	21.045048344889352	180723
cb5d1a3700483909543bdb562d3190a920c79070	incremental compilation-to-sat procedures	compilacion;verificacion modelo;sintesis mecanismo;satisfiabilite;verification modele;logique propositionnelle;program verification;synthese mecanisme;satisfiability;constraint satisfaction;public domain;bounded model checking;satisfaction contrainte;verificacion programa;model checking;decision procedure;propositional logic;compilation;satisfaccion restriccion;mechanism synthesis;logica proposicional;sat solver;verification programme;satisfactibilidad	We focus onincremental compilation-to-SAT procedures (iCTS), a promising way to push the standard CTS approaches beyond their limits. We propose the first comprehensive framework that encompasses all the aspects of an incremental decision procedure , from the encoding to the incremental solver. We apply our guidelines to a real-world CTS approach ( Bounded Model Checking ) and show how to modify both the generation mechanism of a real BMC tool ( NuSMV) and the solving engine of a public-domain SAT solver ( SIM). Related approaches and experimental results are discussed as well.	boolean satisfiability problem;carpal tunnel syndrome;decision problem;explicit substitution;incremental compiler;model checking;natural deduction;nusmv;solver	Marco Benedetti;Sara Bernardini	2004		10.1007/11527695_4	model checking;mathematical optimization;public domain;constraint satisfaction;computer science;artificial intelligence;theoretical computer science;mathematics;linguistics;propositional calculus;boolean satisfiability problem;algorithm;satisfiability	AI	-18.92351262804438	20.487526018837695	180731
1364694215712fdf12dbae769aabd7a9ea014a5e	free theorems for functional logic programs	logical relation;parametric polymorphism;relational parametricity;curry;functional programming;polymorphism;functional logic programming;haskell	Type-based reasoning is popular in functional programming. In particular, parametric polymorphism constrains functions in such a way that statements about their behavior can be derived without consulting function definitions. Is the same possible in a strongly, and polymorphically, typed functional logic language? This is the question we study in this paper. Logical features like nondeterminism and free variables cause interesting effects, which we examine based on examples and address by identifying appropriate conditions that guarantee standard free theorems or inequational versions thereof to hold. We see this case study as a stepping stone for a general theory, not provided here, involving the definition of a logical relation and other machinery required for parametricity arguments appropriate to functional logic languages.	crazy stone (software);free variables and bound variables;functional logic programming;functional programming;nondeterministic algorithm;parametric polymorphism;parametricity;stepping level	Jan Christiansen;Daniel Seidel;Janis Voigtländer	2010		10.1145/1707790.1707797	polymorphism;parametric polymorphism;declarative programming;combinatory logic;computer science;functional logic programming;programming language;functional programming;algorithm	PL	-16.04282238433493	19.989868353179933	181047
ff6ba491ea2e93ab6bda10a5318a6197cd7f7fd7	proving correctness of graph programs relative to recursively nested conditions		We propose a new specification language for the proof-based approach to verification of graph programs by introducing μ-conditions as an alternative to existing formalisms which can express many non-local properties of interest. The contributions of this paper are the lifting of constructions from nested conditions to the new, more expressive conditions and a proof calculus for partial correctness relative to μ-conditions. Most importantly, we prove the correctness of a new construction to compute weakest preconditions with respect to finite graph programs.		Nils Erik Flick	2015	ECEASST	10.14279/tuj.eceasst.73.1037	correctness;null graph;theoretical computer science;programming language;algorithm	PL	-12.804930545698616	21.201438724676244	182269
3ab47baf9ce71e5a47c56d198738202e0681b040	alternating fixed points in boolean equation systems as preferred stable models	logica booleana;fixed point theorem;boolean equation;stable models;logique propositionnelle;logical programming;program verification;theoreme point fixe;teorema punto fijo;ecuacion booliana;fixed point;verificacion programa;model checking;programmation logique;computational complexity;propositional logic;linear time;equation booleenne;logique booleenne;verification enumerative;logic programs;logica proposicional;verification programme;boolean logic;programacion logica	We formally characterize alternating fixed points of boolean equation systems as models of (propositional) normal logic programs. To this end, we introduce the notion of a preferred stable model of a logic program, and define a mapping that associates a normal logic program with a boolean equation system such that the solution to the equation system can be “read off” the preferred stable model of the logic program. We also show that the preferred model cannot be calculated a-posteriori (i.e. compute stable models and choose the preferred one) but rather must be computed in an intertwined fashion with the stable model itself. The mapping reveals a natural relationship between the evaluation of alternating fixed points in boolean equation systems and the GelfondLifschitz transformation used in stable-model computation. For alternation-free boolean equation systems, we show that the logic programs we derive are stratified, while for formulas with alternation, the corresponding programs are non-stratified. Consequently, our mapping of boolean equation systems to logic programs preserves the computational complexity of evaluating the solutions of special classes of equation systems (e.g., linear-time for the alternation-free systems, exponential for systems with alternating fixed points).	boolean algebra;computation;computational complexity theory;fixed point (mathematics);logic programming;stable model semantics;time complexity	K. Narayan Kumar;C. R. Ramakrishnan;Scott A. Smolka	2001		10.1007/3-540-45635-X_23	boolean algebra;boolean circuit;and-inverter graph;combinatorics;circuit minimization for boolean functions;discrete mathematics;boolean domain;boolean expression;product term;standard boolean model;computer science;maximum satisfiability problem;mathematics;boolean function;algorithm;two-element boolean algebra;parity function	Logic	-14.002132749450348	21.22040607541782	182319
607500bbe59eef41b2b29402b6ce5bf8cefc8628	sufficient criteria for applicability and non-applicability of rule sequences	modeling technique;rule based;satisfiability;graph transformation	In several rule-based applications using graph transformation as underlying modeling technique the following questions arise: How can one be sure that a specific sequence of rules is applicable (resp. not applicable) on a given graph? Of course, it is possible to use a trial and error strategy to find out the answer to these questions. In this paper however, we will formulate suitable sufficient criteria for applicability and other ones for non-applicability. These criteria can be checked in a static way i.e. without trying to apply the whole rule sequence explicitly. Moreover if a certain criterion is not satisfied, then this is an indication for reasons why rule sequences may or may not be applicable. Consequently it is easier to rephrase critical rule sequences. The results are formulated within the framework of double pushout (DPO) graph transformations with negative application conditions (NACs).	anti-grain geometry;common criteria;graph rewriting;logic programming;mathematical optimization;non-functional requirement	Leen Lambers;Hartmut Ehrig;Gabriele Taentzer	2008	ECEASST	10.14279/tuj.eceasst.10.139	rule-based system;computer science;algorithm;satisfiability	ML	-13.777595600253576	20.01514441074633	182603
14bdb9eacc45b0cafe35119db4ec02b13e1758cb	sat solving for termination analysis with polynomial interpretations	termination analysis;sat solver	Polynomial interpretations are one of the most popular techniques for automated termination analysis and the search for such interpretations is a main bottleneck in most termination provers. We show that one can obtain speedups in orders of magnitude by encoding this task as a SAT problem and by applying modern SAT solvers.	boolean satisfiability problem;polynomial;termination analysis	Carsten Fuhs;Jürgen Giesl;Aart Middeldorp;Peter Schneider-Kamp;René Thiemann;Harald Zankl	2007		10.1007/978-3-540-72788-0_33	mathematical optimization;discrete mathematics;computer science;theoretical computer science;termination analysis;mathematics;boolean satisfiability problem;algorithm	Logic	-15.058668428999066	24.706972229721508	182898
0e11a8162f40efe704ba8231ce3e338111d71300	creating büchi automata for multi-valued model checking		In explicit state model checking of linear temporal logic properties, a Büchi automaton encodes a temporal property. It interleaves with a Kripke model to form a state space, which is searched for counterexamples. Multi-valued model checking considers additional truth values beyond the Boolean true and false; these values add extra information to the model, e.g. for the purpose of abstraction or execution steering. This paper presents a method to create Büchi automata for multi-valued model checking using quasi-Boolean logics. It allows for multi-valued propositions as well as multi-valued transitions. A logic for the purpose of execution steering and abstraction is presented as an application.	automata theory;büchi automaton;kripke semantics;linear temporal logic;model checking;state space	Stefan J. J. Vijzelaar;Wan Fokkink	2017		10.1007/978-3-319-60225-7_15	discrete mathematics;theoretical computer science;linear temporal logic;counterexample;model checking;ω-automaton;computer science;state space;büchi automaton;truth value;abstraction	Logic	-12.610417550406169	23.766914449673635	182934
359a85af5ec45eda02bc23edd7b6d9efa0637eb1	a novel derivation framework for definite logic program	definite logic program;termination prediction;abstract and refinement;logic programs	Is a closed atom derivable from a definite logic progam? This derivation problem is undecidable. Focused on this problem there exist two categories approaches: the accurate approach that does not guarantee termination, and the terminated abstract approaches. Both approaches have its advantages and disadvantages. We present a novel derivation framework for the definite logic program. A dynamic approach to characterizing termination of fixpoint is presented, then which is used to approximately predict termination of fixpoint in advance.If the fixpoint is predicted termination, we use the non-terminational approach to the derivation problem, otherwise,the terminated abstract approach is used. With this termination predicting approach, we combine the non-termination accurate approaches and the termination abstract approaches together for solving the derivation problem more efficiently. And the experiment results demonstrates the effectiveness of our approach.	algorithm;divergence (computer science);existential quantification;fixed point (mathematics);logic programming;termination analysis;undecidable problem	Mengjun Li;Zhoujun Li;Huowang Chen;Ti Zhou	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.04.054	discrete mathematics;mathematics;algorithm	Logic	-13.926417449952314	21.038133149986756	183047
4d78ba90213b1e3b68378a047d4fb057ac372c67	succinct minimal generators: theoretical foundations and applications	frequent itemset;data mining application;association rule;closure operator;experimental evaluation;theoretical foundation	In data mining applications, highly sized contexts are handl ed what usually results in a considerably large set ofrequentitemsets, even for high values of the minimum support threshold . An interesting solution consists then in applying an appropri ate closure operator that structures frequent itemsets intoequivalence classes , uch that two itemsets belong to the same class if they appear i n the same sets of objects. Among equivalent itemsets, minimal eleme nts (w.r.t. the number of items) are calledminimal generators(MGs), while their associated closure is called closed itemset (CI), and is the largest one within the corresponding equivalence cla ss. Thus, the pairs composed by MGs and their associated CIs make easier localizing each itemset since it is necessarily encompassed by an MG and an CI. In addition, they offer informative implicat ion/association rules, with minimal premises and maximal conclusions, which losslessly represent th entire rule set. These important concepts MG and CI were hence at the origin of various works . Nevertheless, the inherent absence of a unique MG associated to a given CI leads to an intra-class ombinatorial redundancy that leads an exhaustive storage and impractical use. This motivated an i n-depth study towards a lossless reduction of this redundancy. This study was started by Dong et al. who introduced the succinct system of minimal generators (SSMG) as an attempt to eliminate the redunda ncy within this set. In this paper, we give a thorough study of the SSMG as formerly defined by Dong et al. This system will be shown to suffer from some flaws. As a remedy, we introduce a new lossles s reduction of the MG set allowing to overcome its limitations. The new SSMG will then be incorpor ated into the framework of generic bases of association rules. This makes it possible to only main tain succinctand informativerules. After that, we give a thorough formal study of the related infe rence mechanisms allowing to derive all redundantassociation rules, starting from the maintained ones. Final ly, an experimental evaluation shows the utility of our approach towards eliminating importa nt r te of redundant information.	algorithm;association rule learning;cyclic redundancy check;data mining;disjunctive normal form;experiment;information;internationalization and localization;lossless compression;maximal set;mg (editor);super game boy;turing completeness	Tarek Hamrouni;Sadok Ben Yahia;Engelbert Mephu Nguifo	2008	Int. J. Found. Comput. Sci.	10.1142/S0129054108005681	combinatorics;discrete mathematics;association rule learning;computer science;data mining;mathematics;algorithm;closure operator	DB	-15.438561771345064	18.549699573318176	183637
c624efe61bbb965e4f54d3114e728db39d7d9cb5	verification of parameterized hierarchical state machines using action language verifier	boolean logic;hierarchical state machine;action language verifier;integer variable;explicit specification;parameterized hierarchical state machine;parameterized process;temporal logic property;parameterized integer constant;abstraction technique;parameterized instantiations;arithmetic;boolean functions;temporal logic;finite state machines;formal semantics;control systems;software systems;engines;formal specification;formal verification;action language;state machine;data structures;computer science	Action language verifier (ALV) is an infinite-state symbolic model checker. ALV can verify (or falsify, by generating counter-examples) temporal logic properties of systems that can be modeled using a combination of Boolean logic and linear arithmetic expressions on Boolean, enumerated and (possibly unbounded) integer variables and parameterized integer constants. In this paper, we apply ALV to the verification of parameterized hierarchical state machine specifications. We extend the standard notation for hierarchical state machines by introducing primitives for explicit specification of asynchronous processes and their finite and parameterized instantiations. We define the formal semantics of these primitives, where the states of the parameterized processes are mapped to integer variables using the counting abstraction technique. We apply the presented approach to the specification and analysis of an airport ground traffic controller and verify several correctness properties of this specification using ALV.	action language;boolean algebra;control system;correctness (computer science);finite-state machine;logical connective;model checking;semantics (computer science);temporal logic;uml state machine;universal instantiation	Tuba Yavuz-Kahveci;Tevfik Bultan	2005	Proceedings. Second ACM and IEEE International Conference on Formal Methods and Models for Co-Design, 2005. MEMOCODE '05.		temporal logic;formal verification;action language;computer science;theoretical computer science;formal semantics;formal specification;finite-state machine;boolean function;programming language;algorithm;software system	Logic	-12.686721478216397	25.07932607693553	183912
13275f9fd2cef7f40c60eceada9af38ddd738062	exploiting parallelism in the me calculus		We present some parallelization techniques for the Model Evolution (ME) calculus, an instantiation-based calculus that lifts the DPLL procedure to first-order clause logic. Specifically, we consider a restriction of ME to the EPR fragment of clause logic for which the calculus is a decision procedure. The main operations in ME’s proof procedures, namely clause instantiation and candidate literal generation, offer opportunities for MapReducestyle parallelization. This term/clause-level parallelization is largely orthogonal to the sort of search-level parallelization performed by portfolio approaches. We describe a hybrid parallel proof procedure for the restricted calculus that exploits parallelism at both levels to synergistic effect. The calculus and the proof procedure have been implemented in a new solver for EPR formulas. Our initial experimental results show that our term/clauselevel parallelization alone is effective in reducing runtime and can be combined with a portfolio-based approach to maximize performance.	automated theorem proving;dpll algorithm;decision problem;epr paradox;experiment;first-order predicate;interaction;literal (mathematical logic);mapreduce;parallel computing;runtime system;solver;synergy;universal instantiation	Tianyi Liang;Cesare Tinelli	2012			discrete mathematics;theoretical computer science;mathematics;proof calculus;algorithm	Logic	-14.912399505552	23.311087021772686	183984
25af0dd18210bb65d3cc77e17207b1d7a2cdb58b	proving the correctness of regular deterministic programs: a unifying survey using dynamic logic	dynamic logic	Abstract   The simple set WL of deterministic  while  programs is defined and a number of known methods for proving the correctness of these programs are surveyed. Emphasis is placed on the tradeoff existing between data-directed and syntax-directed methods, and on providing, especially for the latter, a uniform description enabling comparison and assessment. Among the works considered are the Floyd/Hoare invariant assertion method for partial correctness, Floyd's well-founded sets method for termination, Dijkstra's notion of weakest precondition, the Burstall/Manna and Waldinger intermittent assertion method and more. Also, a brief comparison is carried out between three logics of programs: dynamic logic, algorithmic logic and programming logic.	correctness (computer science)	David Harel	1980	Theor. Comput. Sci.	10.1016/0304-3975(80)90005-5	dynamic logic;discrete mathematics;separation logic;computer science;theoretical computer science;mathematics;hoare logic;programming language;axiomatic semantics;algorithm	Theory	-12.354660493631567	20.60329632470217	184793
54eeae546e871d875d7034453c80c6f4f11d1ac5	correctness of parallel programs: the church-rosser approach	parallel programs	For many purposes, asynchronous parallel programs may be viewed as sequential but nondeterministic programs. The direct translation to nondeterministic sequential form leads to a combinatorial explosion of program size before correctness proofs can even begin.	church–rosser theorem;correctness (computer science)	Barry K. Rosen	1976	Theor. Comput. Sci.	10.1016/0304-3975(76)90032-3	computer science;theoretical computer science;mathematics;distributed computing;algorithm	Logic	-12.138422631477686	24.722337673488706	184995
93e55da2563514cb6805faba393d7f7fd2974e55	equations for hereditary substitution in leivant's predicative system f: a case study		This paper presents a case study of formalizing a normalization proof for Leivant’s Predicative System F [6] using the EQUATIONS package. Leivant’s Predicative System F is a stratified version of System F, where type quantification is annotated with kinds representing universe levels. A weaker variant of this system was studied by Stump & Eades [5, 3], employing the hereditary substitution method to show normalization. We improve on this result by showing normalization for Leivant’s original system using hereditary substitutions and a novel multiset ordering on types. Our development is done in the COQ proof assistant using the EQUATIONS package, which provides an interface to define dependently-typed programs with well-founded recursion and full dependent patternmatching. EQUATIONS allows us to define explicitly the hereditary substitution function, clarifying its algorithmic behavior in presence of term and type substitutions. From this definition, consistency can easily be derived. The algorithmic nature of our development is crucial to reflect languages with type quantification, enlarging the class of languages on which reflection methods can be used in the proof assistant.	coq (software);decision stump;dependent type;dershowitz–manna ordering;hereditary property;impredicativity;proof assistant;recursion;substitution method;system f	Cyprien Mangin;Matthieu Sozeau	2015	CoRR	10.4204/EPTCS.185.5	discrete mathematics;computer science;artificial intelligence;mathematics;algorithm	PL	-13.693479881147471	19.24530674820578	185181
45b14eefd1352ddcd8fb33389717c53a9d88154b	parallel execution of logic programs in the framework of or-forest	automatic partition algorithm;or-forest-based execution system;parallel execution;or-forest-based process model;nondeterministic program;benchmark program;new framework;or-forest description;automatic partition;logic program	A new framework for parallel execution of logic programs is described in this paper. First, we present the OR-forest description for the execution of logic programs. Then, an algorithm for automatic partition of subgoals and an OR-forest-based process model are given and discussed. All the algorithms and schemes discussed in this paper have been implemented in PROLOG and their correctness and feasibility have been proved by testing a number of benchmark programs. Experiments show that the OR-forest-based execution system, incorporated with an automatic partition algorithm, can efficiently exploit parallelisms in both deterministic and nondeterministic programs, and nearly always achieve optimal AND-OR-parallelism.		Y. Tzu;Chao Sun	1987			real-time computing;computer science;theoretical computer science;programming language	Arch	-17.224724284280647	25.18331303072494	185239
e8d7d0d7c9b511fa3df9a0580b51df4fd9e14ec7	a completeness theorem for straight-line programs with structured variables	code optimization;program transformation;input output	A program scheme which models straight-line code admitting structured variables such as arrays, lists, and queues is considered. A set of expressions is associated with a program reflecting the input-output transformations. A basic set of axioms is given and program equivalence is defined in terms of expression equivalence. Program transformations are then defined such that two programs are equivalent if and only if one program can be transformed to the other via the transformations. An application of these results to code optimization is then discussed.	array data structure;line code;mathematical optimization;program optimization;program transformation;turing completeness	Christoph M. Hoffmann;Lawrence H. Landweber	1976	J. ACM	10.1145/321921.321940	input/output;combinatorics;discrete mathematics;computer science;program optimization;mathematics;programming language;algorithm	PL	-13.281472073714264	23.165265551065538	185241
1dd44accabd507c98dbcef3223306a7db75a8b64	polymorphic contracts	type soundness;manifest contract;defining fh;refinement types;omitting subtyping;dynamic checking;core system;input type;denotational model;logical relations;syn- tactic proof;subtyping.;abstract type;parametric polymorphism;postconditions;contracts;polymorphic contract;preconditions;natural combination;refining type;abstract datatypes	Manifest contracts track precise properties by refining types with predicates—e.g., {x :Int | x > 0} denotes the positive integers. Contracts and polymorphism make a natural combination: programmers can give strong contracts to abstract types, precisely stating preand post-conditions while hiding implementation details—for example, an abstract type of stacks might specify that the pop operation has input type {x :α Stack | not (empty x)}. We formalize this combination by defining FH, a polymorphic calculus with manifest contracts, and establishing fundamental properties including type soundness and relational parametricity. Our development relies on a significant technical improvement over earlier presentations of contracts: instead of introducing a denotational model to break a problematic circularity between typing, subtyping, and evaluation, we develop the metatheory of contracts in a completely syntactic fashion, omitting subtyping from the core system and recovering it post facto as a derived property.	abstract type;manifest (transportation);parametricity;predicate (mathematical logic);programmer;smart contract;type safety	João Filipe Belo;Michael Greenberg;Atsushi Igarashi;Benjamin C. Pierce	2011		10.1007/978-3-642-19718-5_2	computer science;programming language;algorithm	PL	-16.523852163896862	20.261950623496773	185481
5cf25eea1593b37233a4495dd6fe8c2ecf68900c	dynamic translucency with abstraction kinds and higher-order coercions	singleton kinds;coercions;higher order;parametricity;polymorphism;generativity;modules;abstract types;type system	When a module language is combined with forms of non-parametric type analysis, abstract types require an opaque dynamic representation in order to maintain abstraction safety. As an idealisation of such a module language, we present a foundational calculus that combines higher-order type generation, modelling type abstraction, with singleton kinds, modelling translucency. In this calculus, type analysis can dynamically exploit translucency, without breaking abstraction. Abstract types are classified by a novel notion of abstraction kinds. These are analogous to singletons, but instead of inducing equivalence they induce an isomorphism that is witnessed by explicit type coercions on the term level. To encompass higher-order forms of translucent abstraction, we give an account for higher-order coercions in a rich type system with higher-order polymorphism and dependent kinds. The latter necessitate the introduction of an analogous notion of kind coercions on the type level. Finally, we give an abstraction-safe encoding of ML-style module sealing in terms of higher-kinded type generation and higher-order coercion.	abstract type;kind (type theory);record sealing;turing completeness;type system	Andreas Rossberg	2008	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2008.10.019	polymorphism;higher-order logic;type system;generativity;computer science;modular programming;pure mathematics;parametricity;mathematics;programming language;algorithm	PL	-13.250222055222768	18.853071397384834	186308
580ce4cee1d9f9ca1b187305187a75bafb94b7b2	symbolic shape analysis	online katalog;recherche;deutsche nationalbibliothek	The goal of program verification is to ensure software reliability by establishing a mathematical proof which guarantees that the software behaves correctly. Program analysis tools assist the developer in the verification process. Ideally a program analysis should be applicable to a wide range of verification problems without imposing a high burden on its users, i.e., without requiring deep mathematical knowledge and experience in program verification. A big step forward towards this ideal has been achieved by combining abstract interpretation with techniques for automated reasoning. In abstract interpretation one transforms the concrete program into an abstract program. The abstract program enables the analysis to statically collect information over all possible executions of the concrete program. This information is used to automatically verify the correctness of the concrete program. Abstract interpretation increases the degree of automation in verification by shifting the burden of formally reasoning about programs from the developer to the designer of the program analysis. Automated reasoning pushes the degree of automation even further. It enables the automatic construction of the abstraction for a specific program and a specific correctness property and (if necessary) the automatic refinement of this abstraction. We refer to program analyses that combine abstract interpretation with automated reasoning as symbolic program analysis. A problem that has recently seen much attention in program verification is the question of how to effectively deal with linked heap-allocated data structures. Program analyses that target properties of these data structures are commonly referred to as shape analyses. A symbolic shape analysis promises to handle a spectrum of different linked heap-allocated data structures, and a spectrum of properties to verify, without requiring the user to manually adjust the analysis to the specific problem instance. It was open what a symbolic shape analysis would look like. In this thesis we are concerned with this question. We present domain predicate abstraction, which generalizes predicate abstraction to the point where it becomes effectively applicable for shape analysis. Domain predicate abstraction incorporates the key idea of three-valued shape analysis into predicate abstraction by replacing predicates on program states by predicates on objects in the heap of program states. We show how to automate the transformation of a heap-manipulating program into an abstract program using automated reasoning procedures. We further develop an abstraction refinement technique that complements domain predicate abstraction to a fully automated symbolic shape analysis. Finally, we present field constraint analysis, a new technique for reasoning about heap programs. Field constraint analysis enables the application of decision procedures for reasoning about specific data structures (such as trees) to arbitrary data structures. This technique makes our symbolic shape analysis applicable to the diverse data structures that occur in practice. All the techniques presented in this thesis have been implemented and evaluated in the Bohne Verifier. We used Bohne to verify complex user-specified properties of data structure implementations. For instance, we were able to verify preservation of data structure invariants for operations on threaded binary trees (including sortedness and the in-order traversal invariant) without manually adjusting the analysis to this specific problem or providing user assistance beyond stating the properties to verify. We are not aware of any other shape analysis that can verify such properties with a comparable degree of automation.	abstract interpretation;automated reasoning;binary tree;correctness (computer science);data structure;decision problem;designated verifier signature;formal verification;predicate abstraction;program analysis;refinement (computing);shape analysis (digital geometry);software reliability testing;tree traversal	Thomas Wies	2009			geometry;shape analysis (digital geometry);mathematics	PL	-17.58463827845558	25.255136396981236	186657
bc45ed94a3199c06ec571d483f3357978c24f451	committed-choice concurrent logic programming in linear logic	concurrent logic programming;linear logic	The paper deals with the relationship of committed-choice logic programming languages and their proof-theoretic semantics based on linear logic. Fragments of linear logic are used in order to express various aspects of guarded clause concurrent programming and behavior of the system. The outlined translation comprises structural properties of concurrent computations, providing a sound and complete model wrt. to the interleaving operational semantics based on transformation systems. In the presence of variables, just asynchronous properties are captured without resorting to special proof-generating strategies, so the model is only correct for deadlock-free programs.	concurrent logic programming;linear logic	Jirí Zlatuska	1993		10.1007/BFb0022581	dynamic logic;concurrent constraint logic programming;linear temporal logic;logic optimization;horn clause;logic family;functional logic programming;sequential logic;prolog;logic programming;substructural logic;multimodal logic;temporal logic of actions	Logic	-16.80036565424038	21.514662902586423	186781
82c121ce0a62d2d78071bca1e467a622610416a4	transforming and analyzing proofs in the ceres-system	input output;automated deduction	Cut-elimination is the most prominent form of proof transformation in logic. The elimination of cuts in formal proofs corresponds to the removal of intermediate statements (lemmas) in mathematical proofs. Cut-elimination can be applied to mine real mathematical proofs, i.e. for extracting explicit and algorithmic information. The system CERES (cut-elimination by resolution) is based on automated deduction and was successfully applied to the analysis of nontrivial mathematical proofs. In this paper we focus on the input-output environment of CERES, and show how users can interact with the system and extract new mathematical knowledge.	algorithmic information theory;automated theorem proving;formal proof;natural deduction	Stefan Hetzl;Alexander Leitsch;Daniel Weller;Bruno Woltzenlogel Paleo	2008			discrete mathematics;input/output;mathematical proof;lemma (mathematics);automated theorem proving;mathematics	Logic	-16.086831839113618	18.852784875309425	187151
5e609fa90c7be5e3a175f486ed186f4bbc35583b	stratified static analysis based on variable dependencies	widening operator;programming language;polyhedral domain;static analysis;abstract interpretation	In static analysis by abstract interpretation, one often uses widening operators in order to enforce convergence within finite time to an inductive invariant. Certain widening operators, including the classical one over finite polyhedra, exhibit an unintuitive behavior: analyzing the program over a subset of its variables may lead a more precise result than analyzing the original program! In this article, we present simple workarounds for such behavior.	abstract interpretation;polyhedron;static program analysis;workaround	David Monniaux;Julien Le Guen	2012	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2012.10.008	mathematical optimization;discrete mathematics;computer science;mathematics;programming language;static analysis;algorithm	Logic	-15.848724994448741	22.164124881493304	187287
9e7da2bb2ba456b2bdb423d62d25a0392b972e7e	model checking procedural programs		We consider the model-checking problem for sequential programs with procedure calls. We first present basic algorithms for solving the reachability problem and the fair computation problem. The algorithms are based on two techniques: summarization, that computes reachability information by solving a set of fixpoint equations, and saturation, that computes the set of all reachable program states (including call stacks) using automata. Then, we study formalisms to specify requirements of programs with procedure calls. We present an extension of the linear temporal logic allowing propagation of information across the hierarchical structure induced by procedure calls and matching returns. Finally, we show how modelchecking can be extended to this class of programs and properties.	algorithm;automata theory;computation;computational problem;fixed point (mathematics);linear temporal logic;model checking;reachability problem;requirement;software propagation	Rajeev Alur;Ahmed Bouajjani;Javier Esparza	2018		10.1007/978-3-319-10575-8_17	automatic summarization;model checking;discrete mathematics;linear temporal logic;computation;stack (abstract data type);reachability problem;fixed point;reachability;computer science	Logic	-12.789493711484244	23.88665855305252	187539
b3db67d7c6ec023e1f4d638b2c7e554c1f3971be	some characteristics of strong innermost normalization	algebraic specification;characteristic;normalisation;tipo dato;invarianza;data type;caracteristica;trs;term rewrite system;invariance;specification donnee;rewriting systems;specification algebrique;normalizacion;especificacion datos;caracteristique;logic programs;type donnee;systeme reecriture;standardization;data specification	A term rewriting system is strongly innermost normalizing if every innermost derivation of it is of finite length. This property is very important in the integration of functional and logic programming paradigms. Unlike termination, strong innermost normalization is not preserved under subsystems, i.e., every subsystem of a strongly innermost normalizing system need not be strongly innermost normalizing. Preservation of a property under subsystems is important in analyzing systems in a modular fashion. In this paper, we identify a few classes of TRSs which enjoy this property. These classes are of particular interest in studying modularity of composable and hierarchical combinations. It is also proved that the choice of the innermost redex to be reduced at any step has no bearing on termination (finiteness) of innermost derivations. It may be noted that such selection invariance does not hold for outermost derivations, The proof techniques used are novel and involve oracle based reasoning - which is very sparsely used in the rewriting literature.		M. R. K. Krishna Rao	2000	Theor. Comput. Sci.	10.1016/S0304-3975(99)00215-7	discrete mathematics;data type;computer science;invariant;mathematics;characteristic;programming language;standardization;algorithm;algebra	ECom	-12.173675886539044	18.594688878808064	187953
17d419d93c0800c84edbb2b7228ba42bc22b83e2	comparing ltl semantics for runtime verification	runtime verification;ltl property;execution trace;usu;temporal logic monitoring;keywords classical semantics;temporal logic;semantics;moore machines;satisfiability;journal article;infinite word;finite words;linear temporal logic;observed systems;four valued logic;runtimes;finite traces;run time verification	When monitoring a system w.r.t. a property defined in a temporal logic such as LTL, a major concern is to settle with an adequate interpretation of observable system events; that is, models of temporal logic formulae are usually infinite words of events, whereas at runtime only finite but incrementally expanding prefixes are available. In this work, we review LTL-derived logics for finite traces from a runtime-verification perspective. In doing so, we establish four maxims to be satisfied by any LTL-derived logic aimed at runtime verification. As no pre-existing logic readily satisfies all of them, we introduce a new four-valued logic Runtime Verification Linear Temporal Logic RV-LTL in accordance to these maxims. The semantics of Runtime Verification Linear Temporal Logic (RV-LTL) indicates whether a finite word describes a system behaviour which either (i) satisfies the monitored property, (ii) violates the property, (iii) will presumably violate the property, or (iv) will presumably conform to the property in the future, once the system has stabilized. Notably, (i) and (ii) correspond to the classical semantics of LTL, whereas (iii) and (iv) are chosen whenever an observed system behaviour has not yet lead to a violation or acceptance of the monitored property. Moreover, we present a monitor construction for RV-LTL properties in terms of Moore machines signalizing the semantics of the so far obtained execution trace w.r.t. the monitored property.	continuation;four-valued logic;journal of logic and computation;linear temporal logic;moore machine;observable;run time (program lifecycle phase);runtime verification;tracing (software);value (ethics);verification and validation	Andreas Bauer;Martin Leucker;Christian Schallhart	2010	J. Log. Comput.	10.1093/logcom/exn075	linear temporal logic;temporal logic;computation tree logic;computer science;artificial intelligence;theoretical computer science;mathematics;semantics;linguistics;runtime verification;programming language;algorithm;satisfiability	Logic	-13.151238361131231	23.753469794984404	187977
992b6f2e9ca502bb442f265ebb34cdbbaafb19b7	exploiting symbolic traversal techniques for efficient process algebra manipulation	efficient process algebra manipulation;exploiting symbolic traversal techniques;process algebra	Abstract Process Algebras are attracting people from the engineering community, as they are a suitable formalism for describing systems at very high abstraction levels and for proving equivalences and properties. In the past, the application of Process Algebras was hindered by the inefficiency of the tools. Recent developments in the field of boolean function representation with Binary Decision Diagrams and of symbolic traversal techniques allow us to bridge the gap. This paper presents an efficient tool for Process Algebra manipulation that extends and enhances the work of [EFTa91]. The system is open and flexible, as any Process Algebra operator can easily be integrated in it.	process calculus;tree traversal	Paolo Camurati;Fulvio Corno;Paolo Prinetto	1993			boolean algebra;discrete mathematics;theoretical computer science;mathematics;algorithm	EDA	-13.722225416150634	25.060848903084153	188682
8b149e376f22b26517ace0c917950320ac4ae8e3	on efficient consistency checks by robots	lts efficient consistency checks autonomous robotic agents logic inference model checking boolean evolution system synchronous semantics labelled transition system computation tree logic ctl linear temporal logic ltl;cost accounting robot sensing systems stability analysis model checking semantics data structures;temporal logic boolean functions mobile robots	Most autonomous robotic agents use logic inference to keep to safe and permitted behaviour. Given a set of rules, it is important that the robot is able to establish the consistency of its rules and its current perception-based beliefs. This paper investigates how a robotic agent can use model checking to examine the consistency of its rules and beliefs. A rule set is modelled by a Boolean evolution system with synchronous semantics which can be translated into a labelled transition system (LTS). It is proven that stability and consistency can be formulated as computation tree logic (CTL) and linear temporal logic (LTL) properties. Two new algorithms are presented to perform realtime consistency and stability checks respectively, which is crucial for efficient consistency checks by robots.	algorithm;autonomous robot;computation tree logic;control theory;iteration;iterative design;linear temporal logic;model checking;on-board data handling;programmer;transition system	Hongyang Qu;Sandor M. Veres	2014	2014 European Control Conference (ECC)	10.1109/ECC.2014.6862528	dynamic logic;boolean circuit;and-inverter graph;discrete mathematics;linear temporal logic;description logic;computation tree logic;theoretical computer science;mathematics;ctl*;multimodal logic;algorithm;temporal logic of actions	AI	-12.502902242524762	24.684335943341058	189270
38e5f7d1b9baed7974f803fb86b530bee22afdb0	logical approximation for program analysis	logical frameworks;substructural operational semantics;ordered linear logic;program analysis;abstract interpretation	The abstract interpretation of programs relates the exact semantics of a programming language to a finite approximation of those semantics. In this article, we describe an approach to abstract interpretation that is based in logic and logic programming. Our approach consists of faithfully representing a transition system within logic and then manipulating this initial specification to create a logical approximation of the original specification. The objective is to derive a logical approximation that can be interpreted as a terminating forward-chaining logic program; this ensures that the approximation is finite and that, furthermore, an appropriate logic programming interpreter can implement the derived approximation. We are particularly interested in the specification of the operational semantics of programming languages in ordered logic, a technique we call substructural operational semantics (SSOS). We show that manifestly sound control flow and alias analyses can be derived as logical approximations of the substructural operational semantics of relevant languages.	apl;abstract interpretation;approximation;control flow;forward chaining;logic programming;newman's lemma;operational semantics;program analysis;programming language;semantics (computer science);transition system	Robert J. Simmons;Frank Pfenning	2011	Higher-Order and Symbolic Computation	10.1007/s10990-011-9071-2	non-classical logic;linear logic;higher-order logic;horn clause;stable model semantics;theoretical computer science;bunched logic;proof-theoretic semantics;formal semantics;mathematics;programming language;axiomatic semantics;well-founded semantics;logic programming;operational semantics;substructural logic;multimodal logic;denotational semantics;algorithm	PL	-17.343309737910072	21.1011892441146	190042
728ed416054f72df909f72e125db25df06166adc	towards symbolic model checking for multi-agent systems via obdd's	translating;multi agent system;formalism;boolean algebra;model checking;mathematical models;algorithms;symbolic model checking	We present an algorithm for model checking temporal-epistemic properties of multi-agent systems, expressed in the formalism of interpreted systems. We first introduce a technique for the translation of interpreted systems into boolean formulae, and then present a modelchecking algorithm based on this translation. The algorithm is based on OBDD’S, as they offer a compact and efficient representation for boolean formulae.	algorithm;binary decision diagram;model checking;multi-agent system;semantics (computer science)	Franco Raimondi;Alessio Lomuscio	2004		10.1007/978-3-540-30960-4_14	model checking;and-inverter graph;discrete mathematics;theoretical computer science;mathematics;abstraction model checking;symbolic trajectory evaluation;algorithm	AI	-12.660846625229857	24.241115590753374	190878
7daafb9c6d3cec78ad6a25d6d106d1345f3c947b	on interpolation in decision procedures	equality sharing;program analysis;formulae decorate program location;decision procedure;interpolation system;intermediate formula;completeness proof;intermediate location;program analyzer;program state	Interpolation means finding intermediate formulae between given formulae. When formulae decorate program locations, and describe sets of program states, interpolation may enable a program analyzer to discover information about intermediate locations and states. This mechanism has an increasing number of applications, that are relevant to program analysis and synthesis. We study interpolation in theorem proving decision procedures based on the DPLL(T ) paradigm. We survey interpolation systems for DPLL, equality sharing and DPLL(T ), reconstructing from the literature their completeness proofs, and clarifying the requirements for interpolation in the presence of equality.	automated theorem proving;dpll algorithm;interpolation;program analysis;programming paradigm;requirement	Maria Paola Bonacina;Moa Johansson	2011		10.1007/978-3-642-22119-4_1	combinatorics;discrete mathematics;mathematics;algorithm	Logic	-16.07118954435164	18.69922744846967	191043
475e341134076fba9875a238e40a08d1fe076d37	improvement of recursive programs from a logic programming point of view	logic programs;point of view	This paper discusses within the setting of logic programming some techniques which were presented in [BUR 77] for the development of recursive programs. This approach reduces the mystery of some of the eureka in [BUR 77] by making them computationally feasible. Furthermore the idea to do folding and unfolding with the completed logic programs both facilitates and adds translucency to the procedure.	logic programming;point of view (computer hardware company);recursion	Bertram Fronhöfer;Ricardo Caferra;Philippe Jacquet	1984		10.1007/978-3-642-46546-8_18	dynamic logic;mathematical optimization;horn clause;theoretical computer science;bunched logic;functional logic programming;computational logic;mathematics;inductive programming;prolog;logic programming;multimodal logic;algorithm	Theory	-17.26525169764516	19.081323124326037	191448
06f8b41db78eb7dd0fa837f1ef72cd4361352b78	structure sharing for quantified terms: fundamentals	symbolic computation;quantified terms;theorem proving;theorem prover;quantifiers;logic programming;interactive proofs;structure sharing;logic programs;verification systems	Structure sharing is used in symbolic computation to share a common top level between terms with different lower levels. It is widely used in the implementation of Prolog interpreters and is of interest for the implementation of automatic theorem provers, interactive proof editors and verification systems. Previously, structure sharing has been applied only to free-variable terms. In this paper we extend the structure sharing technique to quantified terms. We give an efficient unification algorithm of our structure sharing representation of quantified terms, and we prove the correctness of the algorithm.	algorithm;automated theorem proving;correctness (computer science);free variables and bound variables;prolog;symbolic computation;unification (computer science)	John Staples;Peter J. Robinson	1990	Journal of Automated Reasoning	10.1007/BF00245815	discrete mathematics;symbolic computation;computer science;theoretical computer science;mathematics;automated theorem proving;programming language;algorithm	Logic	-16.140657557629584	20.375855299369995	192048
9fa26d6d8b04a5f9e8e2f5b1584729a596292aa0	tableau recycling	tableau recycling	In this paper we improve a model checking algorithm based on the tableau method of Stifling and Walker. The algorithm proves whether a property expressed in the modal mu-calculus holds for a state in a finite transition system. It makes subsequent use of subtableaux which were calculated earlier in the proof run. These aubtab|eaux are reduced to expressions. Examples show that both size of tableaux and execution time of the algorithm are reduced.	algorithm;amiga walker;long division;method of analytic tableaux;modal μ-calculus;model checking;reduction (complexity);run time (program lifecycle phase);transition system	Angelika Mader	1992		10.1007/3-540-56496-9_26		Logic	-11.867068840194207	24.23722695819777	192679
6ab286689b776d6757a2b85a67a87c610c529ffe	algebraic laws for true concurrency		We find the algebraic laws for true concurrency. Eventually, we establish a whole axiomatization for true concurrency called APTC (Algebra for Parallelism in True Concurrency). The theory APTC has four modules: BATC (Basic Algebra for True Concurrency), APTC (Algebra for Parallelism in True Concurrency), recursion and abstraction. And also, we show the applications and extensions of APTC.	algebraic equation;axiomatic system;concurrency (computer science);recursion	Yong Wang	2016	CoRR		discrete mathematics;theoretical computer science;mathematics;algorithm;concurrent object-oriented programming	Logic	-16.215568512068863	19.30862021256722	192797
358ed7d9d9720268ca002a1d6774e00109433179	behavior preservation in model refactoring using dpo transformations with borrowed contexts	observational equivalence;general techniques;graph transformation	Behavior preservation, namely the fact that the behavior of a model is not altered by the transformations, is a crucial property in refactoring. The most common approaches to behavior preservation rely basically on checking given models and their refactored versions. In this paper we introduce a more general technique for checking behavior preservation of refactorings defined by graph transformation rules. We use double pushout (DPO) rewriting with borrowed contexts, and, exploiting the fact that observational equivalence is a congruence, we show how to check refactoring rules for behavior preservation. When rules are behavior-preserving, their application will never change behavior, i.e., every model and its refactored version will have the same behavior. However, often there are refactoring rules describing intermediate steps of the transformation, which are not behavior-preserving, although the full refactoring does preserve the behavior. For these cases we present a procedure to combine refactoring rules to behavior-preserving concurrent productions in order to ensure behavior preservation. An example of refactoring for finite automata is given to illustrate the theory.	automata theory;case preservation;code refactoring;congruence of squares;finite-state machine;graph rewriting;observational equivalence;turing completeness	Guilherme Rangel;Leen Lambers;Barbara König;Hartmut Ehrig;Paolo Baldan	2008		10.1007/978-3-540-87405-8_17	combinatorics;discrete mathematics;computer science;mathematics;programming language;code refactoring;algorithm;graph rewriting	Logic	-13.53822397932819	20.406466254055484	192891
8393de17138a37b462c0dd5652fffb322cd203d5	two iteration theorems for the ll(k) languages		The structure of derivation trees over an LL( k ) grammar is explored and a property of these trees obtained which is shown to characterize the LL( k ) grammars. This characterization, called the LL( k ) Left Part Theorem, makes it possible to establish a pair of iteration theorems for the LL( k ) languages. These theorems provide a general and powerful method of showing that a language is not LL( k ) when that is the case. They thus provide for the first time a flexible tool with which to explore the structure of the LL( k ) languages and with which to discriminate between the LL( k ) and LR( k ) language classes. Examples are given of LR( k ) languages which, for various reasons, fail to be LL( k ). Easy and rigorous proofs to this effect are given using our LL( k ) iteration theorems. In particular, it is proven that the dangling-ELSE construct allowed in PL/I and Pascal cannot be generated by any LL( k ) grammar. We also give a new and straightforward proof based on the LL( k ) Left Part Theorem that every LL( k ) grammar is LR( k ).	iteration;ll parser	John C. Beatty	1980	Theor. Comput. Sci.	10.1016/0304-3975(80)90029-8	arithmetic;algorithm	ECom	-13.396298132355044	19.29299022124352	193417
b11312dbf62514c826308278428f1c13fea905fc	rewriting with a nondeterministic choice operator: from algebra to proofs	algebraic specification;concurrent language;term rewrite system;automatic theorem proving	The privileged field of classical algebra and term rewriting systems is that of strictly deterministic systems: the confluence property is generaly assumed to hold, which ensures determinism about the result of the computations, even if there exist several different computation paths. In this paper, we develop a new formalism introducing a bounded nondeterministic choice operator  into algebraic specifications and related term rewriting systems; nondeterminism about the result becomes allowed in this framework. We define the algebraic and the operational aspects of such systems, and investigate their relationship. Methods à la Knuth-Bendix are developed for automatic theorem proving in such theories. Several examples are considered, including a toy concurrent language, for which non-trivial properties may be automatically proved.	rewriting	Stéphane Kaplan	1986		10.1007/3-540-16442-1_27	programming language;algorithm;rippling	Logic	-13.277957229220018	20.15613949334263	193425
12f000052d3e168a41b2e1968b4488a982cf9fd8	an interpolating sequent calculus for quantifier-free presburger arithmetic	sequent calculus;presburger arithmetic;03b70;datavetenskap datalogi;computer science;craig interpolation	Craig interpolation has become a versatile tool in formal verification, used for instance to generate program assertions that serve as candidates for loop invariants. In this paper, we consider Craig interpolation for quantifier-free Presburger arithmetic (QFPA). Until recently, quantifier elimination was the only available interpolation method for this theory, which is, however, known to be potentially costly and inflexible. We introduce an interpolation approach based on a sequent calculus for QFPA that determines interpolants by annotating the steps of an unsatisfiability proof with partial interpolants. We prove our calculus to be sound and complete. We have extended the Princess theorem prover to generate interpolating proofs, and applied it to a large number of publicly available Presburger arithmetic benchmarks. The results document the robustness and efficiency of our interpolation procedure. Finally, we compare the procedure against alternative interpolation methods, both for QFPA and linear rational arithmetic.	assertion (software development);automated theorem proving;for loop;formal verification;interpolation;loop invariant;presburger arithmetic;quantifier (logic);sequent calculus	Angelo Brillout;Daniel Kroening;Philipp Rümmer;Thomas Wahl	2011	Journal of Automated Reasoning	10.1007/s10817-011-9237-y	discrete mathematics;cut-elimination theorem;computer science;presburger arithmetic;mathematics;programming language;sequent calculus;algorithm	Logic	-16.141542790294935	24.933114748463044	193467
6a11fbccdd01b0c8488142cd6c327a7ee075cbf7	logics and algorithms for verification of concurrent systems	datavetenskap datalogi;datavetenskap;computer science	Somla, R. 2012. Logics and Algorithms for Verification of Concurrent Systems. Uppsala University. Digital Comprehensive Summaries of Uppsala Dissertations from the Faculty of Science and Technology 964. 48 pp. Uppsala. ISBN 978-91-554-8447-7. In this thesis we investigate how the known framework of automatic formal verification by model checking can be extended in different directions. One extension is to go beyond the common limitation of the existing specification formalisms, that they can describe only regular properties of components. This can be achieved using logics capable of expressing nonregular properties, such as the Propositional Dynamic Logic of Context-free Programs (PDLCF), Fixpoint Logic with Chop (FLC) or the Higher-order Fixpoint Logic (HFL). Our main result in this area is proving that the problem of model checking HFL formulas of order bounded by k is k-EXPTIME complete. In the proofs we demonstrate two model checking algorithms for that logic. We also show that PDLCF is equivalent to a proper fragment of FLC. The standard model checking algorithms, which are run on a single computer, are severely limited by the amount of available computing resources. A way to overcome this limitation is to develop distributed algorithms, which can be run on a cluster of computers and use their joint resources. In this thesis we show how a distributed model checking algorithm for the alternationfree fragment of the modal μ-calculus can be extended to handle formulas with one level of alternation. This is an important extension, since Lμ formulas with one level of alternation can express the same properties as logics LTL and CTL commonly used in formal verification. Finally, we investigate stochastic games which can be used to model additional aspects of components, such as their interaction with environment and their quantitative properties. We describe new algorithms for finding optimal values and strategies in turn-based stochastic games with reachability winning conditions. We prove their correctness and report on experiments where we compare them against each other and against other known algorithms, such as value iteration and strategy improvement. Rafał Somla, Uppsala University, Department of Information Technology, Box 337, SE-751 05 Uppsala, Sweden.	computer;concurrency (computer science);correctness (computer science);distributed algorithm;dynamic logic (modal logic);exptime;experiment;fixed point (mathematics);formal verification;international standard book number;iteration;markov decision process;modal μ-calculus;model checking;r language;reachability	Rafal Somla	2012			computer science;theoretical computer science;runtime verification;programming language;algorithm;functional verification	Logic	-12.958696193116637	22.5233740271036	194278
5c2ff07fcece48025c3077f9b7da3867a4b14b22	grids: a domain for analyzing the distribution of numerical values	linear algebra;vector space;efficient implementation;static analysis	This paper explores the abstract domain of grids, a domain that is able to represent sets of equally spaced points and hyperplanes over an n-dimensional vector space. Such a domain is useful for the static analysis of the patterns of distribution of the values program variables can take. We present the domain, its representation and the basic operations on grids necessary to define the abstract semantics. We show how the definition of the domain and its operations exploit well-known techniques from linear algebra as well as a dual representation that allows, among other things, for a concise and efficient implementation.	algorithm;best, worst and average case;coefficient;congruence of squares;gaussian elimination;lambda lifting;linear algebra;polyhedron;static program analysis;worst-case complexity	Roberto Bagnara;Katy Louise Dobson;Patricia M. Hill;Matthew Mundell;Enea Zaffanella	2006		10.1007/978-3-540-71410-1_16	mathematical optimization;domain;vector space;linear algebra;static analysis	PL	-18.0571064744488	24.425994260195612	194573
441792f9b8562b68378a8686f26c263109cfb9ce	correctness and completeness of logic programs	declarative programming;declarative diagnosis algorithmic debugging;specifications;program completeness;logic programming;program correctness	We discuss proving correctness and completeness of definite clause logic programs. We propose a method for proving completeness, while for proving correctness we employ a method that should be well known but is often neglected. Also, we show how to prove completeness and correctness in the presence of SLD-tree pruning, and point out that approximate specifications simplify specifications and proofs.  We compare the proof methods to declarative diagnosis (algorithmic debugging), showing that approximate specifications eliminate a major drawback of the latter. We argue that our proof methods reflect natural declarative thinking about programs, and that they can be used, formally or informally, in everyday programming.	approximation algorithm;correctness (computer science);debugging;horn clause;logic programming	Wlodzimierz Drabent	2016	ACM Trans. Comput. Log.	10.1145/2898434	declarative programming;computer science;theoretical computer science;programming language;logic programming;algorithm	PL	-17.819819337832502	22.163337271059017	194821
a0a29018e22566ef2b8b32893ea324d45b0d0863	constraint-based program reasoning with heaps and separation	separation logic;constraint handling rules;symbolic execution;heap manipulating programs;satisfiability modulo theories	This paper introduces a constraint language H for finite partial maps (a.k.a. heaps) that incorporates the notion of separation from Separation Logic. We use H to build an extension of Hoare Logic for reasoning over heap manipulating programs using (constraint-based) symbolic execution. We present a sound and complete algorithm for solving quantifier-free (QF) H-formulae based on heap element propagation. An implementation of the H-solver has been integrated into a Satisfiability Modulo Theories (SMT) framework. We experimentally evaluate the implementation against Verification Conditions (VCs) generated from symbolic execution of large (heap manipulating) programs. In particular, we mitigate the path explosion problem using subsumption via interpolation – made possible by the constraint-based encoding.	algorithm;experiment;formal verification;heap (data structure);hoare logic;interpolation;modulo operation;quantifier (logic);recursion;satisfiability modulo theories;separation logic;simple set;software propagation;solver;subsumption architecture;symbolic execution	Gregory J. Duck;Joxan Jaffar;Nicolas C. H. Koh	2013		10.1007/978-3-642-40627-0_24	binomial heap;discrete mathematics;separation logic;computer science;theoretical computer science;mathematics;programming language;satisfiability modulo theories;algorithm	PL	-16.285689089627148	23.35451716773214	194909
a19b15b6ce868771233eff97715dde819ca69ccf	transformation systems and nondeclarative properties	program transformation;program synthesis;program optimization;logic programs	Program transformation systems are applied both in program synthesis and in program optimization. For logic programs the “logic” component makes transformations very natural and easy to be studied formally. But, when we move to Prolog programs, the “control” component cannot be ignored. In particular we need to cope with termination properties which are essential for ensuring the reachability of solutions for a given query. We give an overview of the main proposals in the field of transformation systems for logic programs and we emphasize how they cope with those properties of logic programs which are not strictly declarative. We focus in particular on how the transformation can affect the termination of a program.	declarative programming;logic programming;mathematical optimization;program optimization;program synthesis;program transformation;prolog;reachability	Annalisa Bossi;Nicoletta Cocco;Sandro Etalle	2002		10.1007/3-540-45628-7_8	program analysis;computer science;theoretical computer science;programming language;algorithm	AI	-18.377445909663518	22.39029310059498	194959
52b8cd7d5e8273628da61b863f4b49905382bfb0	synthesis of ml programs in the system coq	optimisation;optimizacion;specification;punto fijo;functional programming;induccion;ml;programming theory;induction;especificacion;point fixe;theorie programmation;optimization;programmation fonctionnelle;programacion funcional;fix point	The system Coq (Dowek et al., 1991) is an environment for proof development based on the Calculus of Constructions (Coquand, 1985) (Coquand and Huet, 1985) enhanced with inductive definitions (Coquand and Paulin-Mohring, 1990). From a constructive proof formalized in Coq, one extracts a functional program which can be compiled and executed in ML. This paper describes how to obtain ML programs from proofs in Coq. The methods are illustrated with the example of a propositional tautology checker. We study the specification of the problem, the development of the proof and the extraction of the executable ML program. Part of the example is the development of a normalization function for IF-expressions, whose termination has been studied in several formalisms (Leszczylowski, 1981) (Paulson, 1986) (Dybjer, 1990). We show that the total program using primitive recursive functionals obtained out of a structural proof of termination leads to an (at first) surprisingly efficient algorithm. We explain also how to introduce a fixpoint and get the usual recursive program. Optimizations which are necessary in order to obtain efficient programs from proofs will be explained. We also justify the properties of the final ML program with respect to the initial specification.	algorithm;calculus of constructions;compiler;coq (software);executable;fixed point (mathematics);functional programming;primitive recursive function;primitive recursive functional;recursion;thierry coquand	Christine Paulin-Mohring;Benjamin Werner	1993	J. Symb. Comput.	10.1016/S0747-7171(06)80007-6	calculus of constructions;discrete mathematics;mathematics;functional programming;specification;algorithm	PL	-17.331048880994686	22.33101100564123	195183
fdc27726b792416fbed65ccb58b801d1deb63c2b	simplified parsing expression derivatives		This paper presents a new derivative parsing algorithm for parsing expression grammars; this new algorithm is both simpler and faster than the existing parsing expression derivative algorithm presented by Moss[12]. This new algorithm improves on the worst-case space and runtime bounds of the previous algorithm by a linear factor, as well as decreasing runtime by about half in practice. A proof of correctness for the new algorithm is included in this paper, a result not present in earlier work.	algorithm;backtracking;best, worst and average case;correctness (computer science);level of detail;linear function;parse tree;parsing expression grammar	Aaron Moss	2018	CoRR		discrete mathematics;correctness;parsing;mathematics;rule-based machine translation	PL	-15.701283514743757	24.7542227545409	195529
f3380a1328188113480b784e5a378c8e38af8ba3	simplifying questions in maude declarative debugger by transforming proof trees	declarative debugging;maude;proof tree transformation	Declarative debugging is a debugging technique that abstracts the execution details that in general may be difficult to follow in declarative languages to focus on results. It relies on a data structure representing the wrong computation, the debugging tree, which is traversed by asking questions to the user about the correctness of the computation steps related to each node. Thus, the complexity of the questions is an important factor regarding the applicability of the technique. In this paper we present a transformation for debugging trees for Maude specifications that ensures that any subterm occurring in a question has been previously replaced by the most reduced form that it has taken during the computation, thus ensuring that questions become as simple as possible.	algorithmic program debugging;computation;correctness (computer science);data structure;debugger;declarative programming;maude system	Rafael Caballero;Adrián Riesco;Alberto Verdejo;Narciso Martí-Oliet	2011		10.1007/978-3-642-32211-2_6	computer science;theoretical computer science;algorithmic program debugging;programming language;algorithm	PL	-18.297140005782627	24.416399766153035	195629
18094fc32c04cad3419c119c5bcd9d4765fe2ce9	gradual certified programming in coq	gradual typing;subset types;coq;program extraction;certified programming;casts;refinements	"""Expressive static typing disciplines are a powerful way to achieve high-quality software. However, the adoption cost of such techniques should not be under-estimated. Just like gradual typing allows for a smooth transition from dynamically-typed to statically-typed programs, it seems desirable to support a gradual path to certified programming. We explore gradual certified programming in Coq, providing the possibility to postpone the proofs of selected properties, and to check """"at runtime"""" whether the properties actually hold. Casts can be integrated with the implicit coercion mechanism of Coq to support implicit cast insertion à la gradual typing. Additionally, when extracting Coq functions to mainstream languages, our encoding of casts supports lifting assumed properties into runtime checks. Much to our surprise, it is not necessary to extend Coq in any way to support gradual certified programming. A simple mix of type classes and axioms makes it possible to bring gradual certified programming to Coq in a straightforward manner."""	coq (software);gradual typing;lambda lifting;linear algebra;run time (program lifecycle phase);type class;type conversion;type system	Éric Tanter;Nicolas Tabareau	2015		10.1145/2816707.2816710	computer science;theoretical computer science;programming language;algorithm	PL	-18.4841968851293	23.718052480656365	195720
f3094cf563c3b80b86ea7d52600714ea2ebb1e6b	high-level programs and program conditions	program derivation;satisfiability;normal form;structural properties	High-level conditions are well-suited for expressing structural properties. They can describe the precondition and the postcondition for a high-level program, but they cannot describe the relationship between the input and the output of a program derivation. Therefore, we investigate program conditions, a generalized type of conditions expressing properties on program derivations. Program conditions look like nested rules with application conditions. We present a normal form result, a suitable graphical notation, and conditions under which a satisfying program can be constructed from a program condition. We define a sequential composition on program conditions and show that, for a suitable type of program conditions with a complete dependence relation we have that: Whenever the original programs satisfy the original program conditions, then the composed program satisfies the composed program condition.	a-normal form;graphical user interface;high- and low-level;postcondition;precondition;process calculus;program derivation	Karl Azab;Annegret Habel	2008		10.1007/978-3-540-87405-8_15	discrete mathematics;computer science;mathematics;programming language;program derivation;algorithm;satisfiability	PL	-13.320404227228144	20.06031265476782	195956
f721b2e36b18ef662e36dda2a09d1480ce5ec151	differential methods in logic program analysis	program analysis;logic programs;abstract interpretation	Program analysis based on abstract interpretation has proven very useful in compilation of constraint and logic programming languages. Unfortunately, the traditional goal-dependent framework is inherently imprecise. This is because it handles call and return in such a way that dataflow information may be re-asserted unnecessarily, leading to a loss of precision for many description domains. For a few specific domains, the literature contains proposals to overcome the problem, and some implementations use various unpublished tricks that sometimes avoid the precision loss. The purpose of this paper is to map the landscape of goal-dependent, goalindependent, and combined approaches to generic analysis of logic programs. This includes formalising existing methods and tricks in a way that is independent of specific description domains. Moreover, we suggest new methods for overcoming the loss of precision altogether eight different semantics are considered and compared. We provide theoretical results determining the relative accuracy of the approaches. These show that two of our new semantics are uniformly more accurate than existing approaches. Experiments that we have performed (for two description domains) with implementations of the eight different approaches enable a discussion of their relative runtime performances. We discuss the expected effect on other domains as well and conclude that our new methods can be trusted to yield significantly more accurate analysis for a small extra implementation effort, without compromising the efficiency of analysis. © 1998 Elsevier Science Inc. All rights reserved.	abstract interpretation;compiler;dataflow;logic programming;performance;program analysis;programming language	Maria Garcia de la Banda;Kim Marriott;Peter J. Stuckey;Harald Søndergaard	1998	J. Log. Program.	10.1016/S0743-1066(97)10002-4	program analysis;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;algorithm	PL	-18.424055288276392	22.33800039909216	196386
7272421b9230d4e22680c9e51913f3c6ae127a83	recursion in logics of programs	program proving;formal specifications;context free;program verification;dynamic logic;concurrency;message buffers;parallel programs	The problem of reasoning about recursive programs is considered. Utilizing a simple analogy between iterative and recursive programs viewed as unfinite unions of finite terms, we carry out an investigation analogous to that carried out recently for iterative programs. The main results are the arithmetical completeness of axiom systems for (1) context-free dynamic logic and (2) its extension for dealing with infinite computations. Having the power of expression of these logics in mind, these results can be seen to supply (as corollaries) complete proof methods for the various kinds of correctness of recursive programs.	computation;context-free language;correctness (computer science);iterative method;np-completeness;recursion;tagged union	David Harel	1979		10.1145/567752.567760	dynamic logic;concurrency;computer science;theoretical computer science;programming language;algorithm	PL	-12.38169790427916	20.134467685524918	196488
1265437367f43a105f4a33d8d1405b75b0c63584	recursive definitions of monadic functions	fixed point;logic in computer science	Using standard domain-theoretic fixed-points, we present an approach for defining recursive functions that are formulated in monadic style. The method works both in the simple option monad and the state-exception monad of Isabelle/HOL’s imperative programming extension, which results in a convenient definition principle for imperative programs, which were previously hard to define. For such monadic functions, the recursion equation can always be derived without preconditions, even if the function is partial. The construction is easy to automate, and convenient induction principles can be derived automatically.	domain theory;fixed-point theorem;hol (proof assistant);imperative programming;isabelle;iterative method;mathematical induction;monad (functional programming);overhead (computing);precondition;recursion (computer science);scott continuity	Alexander Krauss	2010		10.4204/EPTCS.43.1	discrete mathematics;computer science;mathematics;fixed point;monadic predicate calculus;programming language;algorithm	PL	-16.14510283706558	21.38003397506718	196495
e651c127860d3ebf480b7ed729b8536986629cf6	sequential calculus	sequential calculus	This paper presents an algebraic calculus like the relational calculus for reasoning about sequential phenomena. It provides a common foundation for several proposed models of concurrent or reactive systems. It is clearly diierentiated from the relational calculus by absence of a general converse operation. This permits the treatment of temporal logic within the sequential calculus.	communicating sequential processes;concurrent computing;interval temporal logic;linear algebra;norm (social);regular expression;relational calculus;temporal logic	Burghard von Karger;C. A. R. Hoare	1995	Inf. Process. Lett.	10.1016/0020-0190(94)00205-D	process calculus	AI	-12.24135778401281	20.717744497136177	196560
6dd8764170845a007ba819428fc8dcf5fe0e26bb	the anchored version of the temporal framework	temporal logic;computer model;finite automata;transition systems;reactive system;concurrent programs	In this survey paper we present some of the recent developments in the temporal formal system for the speciication, veriication and development of reactive programs. While the general methodology remains very much the one presented in some earlier works on the subject , such as MP83c, MP83a, Pnu86], there have been several technical improvements and gained insights in understanding the computational model, the logic itself, the proof system and its presentation , and connections with alternative formalisms, such as nite automata. In this paper we explicate some of these improvements and extensions. The main diierence between this and preceding versions is that here we consider a notion of validity for temporal formulae, which is anchored at the initial state of the computation. The paper discusses some of the consequences of this decision.	automata theory;computation;computational model;formal system;proof calculus	Zohar Manna;Amir Pnueli	1988		10.1007/BFb0013024	real-time computing;interval temporal logic;computer science;theoretical computer science;algorithm;temporal logic of actions	Logic	-12.13482347006488	21.893138594973806	196603
04fa2916ec4795149098f611c448a14c573e3bb2	string analysis as an abstract interpretation	context free language;pushdown automata;abstract interpretation	We formalize a string analysis within abstract interpretation framework. The abstraction of strings is given as a conjunction of predicates that describes the common configuration changes on the reference pushdown automaton while processing the strings. We also present a family of pushdown automata called bounded pushdown automata. This family covers all context-free languages, and by using this family of pushdown automata, we can prevent abstract values from becoming infinite conjunctions and guarantee that the operations required in the analyzer are computable.	abstract interpretation;automata theory;computable function;context-free language;pushdown automaton;stack (abstract data type);string (computer science)	Se-Won Kim;Kwang-Moo Choe	2011		10.1007/978-3-642-18275-4_21	deterministic context-free language;deterministic pushdown automaton;deterministic context-free grammar;computer science;nested word;context-free language;programming language;pushdown automaton;embedded pushdown automaton;algorithm	Logic	-12.376183667455152	24.056291161898745	197217
0d1b2969f6c9c7f16631dbe50d909d1fc2ffb093	the rewriting calculus - part ii	matching;lambda calculus;strategy;rule based;operational semantics;first order;rewriting	The ρ-calculus integrates in a uniform and simple setting first-order rewriting, λ-calculus and nondeterministic computations. Its abstraction mechanism is based on the rewrite rule formation and its main evaluation rule is based on matching modulo a theory T . We have seen in the first part of this work the motivations, definitions and basic properties of the ρ-calculus. This second part is first devoted to the use of an extension of the ρ-calculus for encoding a (conditional) rewrite relation. This extension is based on the first operator whose purpose is to detect rule application failure. It allows us to express recursively rule application and therefore to encode strategy based rewriting processes. We then use this extended calculus to give an operational semantics to ELAN programs. We conclude with an overview of ongoing and future works on ρ-calculus.	asf+sdf meta-environment;computation;confluence;encode;elan;emergence;explicit substitution;expressive power (computer science);first-order predicate;fixed-point combinator;lambda calculus;like button;linear algebra;maude system;modulo operation;newman's lemma;non-deterministic turing machine;nondeterministic algorithm;normalization property (abstract rewriting);operational semantics;recursion;rewrite (programming);rewrite order;rewriting;rho calculus;subject reduction;theory;time complexity;tree traversal;type system;unification (computer science)	Horatiu Cirstea;Claude Kirchner	2001	Logic Journal of the IGPL	10.1093/jigpal/9.3.377		Logic	-11.995439127495787	19.85089122794603	197596
a0a633b8aec0948695c3ed2d43cd364b6e8ff1e5	parametricity as subtyping	lazy functional languages;functional programming;binding time improvements;polymorphism;partial evaluation;intersection types;compiler generation	A polymorphic function is parametric if it has uniform behavior for all type parameters. This property is useful when writing, reasoning about, and compiling functional programs. We show how to syntactically define and reason about parametricity in a language with intersection types and bounded polymorphism. Within this framework, parametricity is subtyping, and reasoning about parametricity becomes reasoning about the well-typedness of terms. This work also demonstrates the expressiveness of languages that combine intersection types and bounded polymorphism.	compiler;parametricity;whole earth 'lectronic link	QingMing Ma	1992		10.1145/143165.143225	polymorphism;computer science;theoretical computer science;parametricity;programming language;functional programming;partial evaluation;algorithm	PL	-17.16170926405843	21.369420513321558	197693
ac0f4168856ff3a1552c90d0911f2fc11c285861	testing first-order logic axioms in program verification	declarative programming;computer model;random testing;program verification;property based testing;theorem prover;first order;test generation;model based testing;constraint solving;meta programming;automated theorem proving;off the shelf;first order logic;domain specificity	Program verification systems based on automated theorem provers rely on user-provided axioms in order to verify domain-specific properties of code. However, formulating axioms correctly (that is, formalizing properties of an intended mathematical interpretation) is non-trivial in practice, and avoiding or even detecting unsoundness can sometimes be difficult to achieve. Moreover, speculating soundness of axioms based on the output of the provers themselves is not easy since they do not typically give counterexamples. We adopt the idea of model-based testing to aid axiom authors in discovering errors in axiomatizations. To test the validity of axioms, users define a computational model of the axiomatized logic by giving interpretations to the function symbols and constants in a simple declarative programming language. We have developed an axiom testing framework that helps automate model definition and test generation using off-the-shelf tools for meta-programming, property-based random testing, and constraint solving. We have experimented with our tool to test the axioms used in AUtoCERT, a program verification system that has been applied to verify aerospace flight code using a first-order axiomatization of navigational concepts, and were able to find counterexamples for a number of axioms.	first-order logic;first-order predicate;formal verification	Ki Yung Ahn;Ewen Denney	2010		10.1007/978-3-642-13977-2_4	computer simulation;armstrong's axioms;computer science;reverse mathematics;first-order logic;automated theorem proving;programming language;theory;algorithm	Logic	-18.340235083581856	23.061525807310808	198143
9325caad871f00cd6574f0f5228ca8a79c3bbde8	on the complexity of standard and specialized dtd parsing	context free language;expressive power	"""We investigate an extension of standard DTDs named s-xschemes yielding more expressive power. We show that s-xschemes which are also known as specialized DTDs have more convenient complexity properties than context-free languages. We invent the notions of proper-ness and 1-unambiguity for a close characterization of s-xschemes, whereby \1-unambiguity"""" is a restriction concerning expressive power while \properness"""" is not. With these conditions we show an eecient algorithm for parsing in linear time and space. We propose a concrete syntax and provide an implementation inside a parsing system."""	algorithm;context-free language;earthbound;expressive power (computer science);parse tree;parsing;time complexity	Ralf Behrens	2000			expressive power;parsing;natural language processing;programming language;artificial intelligence;computer science;context-free language	NLP	-13.358574604089496	18.70122530122769	198406
e2c346015f5d876feb0a27b527be677e6dc6c800	unfolding - definition - folding, in this order, for avaoiding unnecessary variables in logic programs	transformacion matematica;mathematical transformation;logical programming;performance programme;programmation logique;transformation mathematique;eficacia programa;program performance;is success;logic programs;programacion logica;data structure	We present an approach to the automatic improvement of performances of logic programs by using the unflod/fold transformation technique. A cause of program inefficiency is often the presence of variables which are unnecessary, in the sense that they force computations of redundant values or multiple visits of data structures. We propose a strategy which automatically transforms initial program versions into new and more efficient versions by avoiding unnecessary variables. Our strategy is an extension of the one which was introduced in an earlier paper by Projetti-Pettorossi (1990). It is based on the syntactical characterization of the unnecessary variables and it uses a composite transformation rule made out of unfolding-definition-folding steps, in this order. The strategy consists in the repeated application of that composite rule to each clause with unnecessary variables. It avoids the search for eureka definitions which is often required by other techniques proposed in the literature. We define a class of programs for which our transformation strategy is successful and we propose a variant of that strategy which uses the so-called generalization rule. This variant is always terminating, but, in general, not all unnecessary variables are climinated. We finally present an enhancement of the proposed transformation techniques which exploits the functionality of some predicates	logic programming;net (polyhedron)	Maurizio Proietti;Alberto Pettorossi	1995	Theor. Comput. Sci.	10.1016/0304-3975(94)00227-A	data structure;computer science;artificial intelligence;mathematics;programming language;algorithm	AI	-17.897817837924926	22.755799517550773	199030
45f7e340225e673233203055d9f14ce278d9df04	the hanoi omega-automata format	verification;automata;infinite words	We propose a flexible exchange format for ω-automata, as typically used in formal verification, and implement support for it in a range of established tools. Our aim is to simplify the interaction of tools, helping the research community to build upon other people’s work. A key feature of the format is the use of very generic acceptance conditions, specified by Boolean combinations of acceptance primitives, rather than being limited to common cases such as Büchi, Streett, or Rabin. Such flexibility in the choice of acceptance conditions can be exploited in applications, for example in probabilistic model checking, and furthermore encourages the development of acceptance-agnostic tools for automata manipulations. The format allows acceptance conditions that are either state-based or transition-based, and also supports alternating automata.	alternating finite automaton;archive;automata theory;formal verification;issue tracking system;model checking;omega;prism (surveillance program);smoothing;statistical model;ω-automaton	Tomás Babiak;Frantisek Blahoudek;Alexandre Duret-Lutz;Joachim Klein;Jan Kretínský;David Müller;David Parker;Jan Strejcek	2015		10.1007/978-3-319-21690-4_31	verification;simulation;computer science;artificial intelligence;theoretical computer science;automaton;common power format;programming language;algorithm	Logic	-13.526453942314948	23.368188281229823	199098
36fbfd64604ca754df8c6ed1ca4e7e788eb36f99	programming and reasoning with infinite structures using copatterns and sized types		Inductive data such as lists and trees is modeled category-theoretically as algebra where construction is the primary concept and elimination is obtained by initiality. In a more practical setting, functions are programmed by pattern matching on inductive data. Dually, coinductive structures such as streams and processes are modeled as coalgebras where destruction (or transition) is primary and construction rests on finality [Hag87]. Due to the coincidence of least and greatest fixed-point types [SP82] in lazy languages such as Haskell, the distinction between inductive and coinductive types is blurred in partial functional programming. As a consequence, coinductive structures are treated just as infinitely deep (or, non-well-founded) trees, and pattern matching on coinductive data is the dominant programming style. In total functional programming, which is underlying the dependently-typed proof assistants Coq [INR12] and Agda [Nor07], the distinction between induction and coinduction is vital for the soundness, and pattern matching on coinductive data leads to the loss of subject reduction [Gim96]. Further, in terms of expressive power, the productivity checker for definitions by coinduction lacks behind the termination checker for inductively defined functions.	agda;coinduction;coq (software);dependent type;expressive power (computer science);haskell;lazy evaluation;least fixed point;pattern matching;programming style;proof assistant;subject reduction;total functional programming	Andreas Abel	2014			mathematics	PL	-15.123557780209639	19.88635119473237	199146
2ea348187ff34e2df3285b33823003a01c566fe0	principal typings for explicit substitutions calculi	principal typings;lambda calculus;satisfiability;strong normalization;explicit substitution;type inference;type system	Having principal typings (for short PT) is an important property of type systems. In simply typed systems, this property guarantees the possibility of a complete and terminating type inference mechanism. It is well-known that the simply typed λ-calculus has this property but recently J.B. Wells has introduced a system-independent definition of PT, which allows to prove that some type systems, e.g. the Hindley/Milner type system, do not satisfy PT. Explicit substitutions address a major computational drawback of the λ-calculus and allow the explicit treatment of the substitution operation to formally correspond to its implementation. Several extensions of the λ-calculus with explicit substitution have been given but some of which do not preserve basic properties such as the preservation of strong normalization. We consider two systems of explicit substitutions (λse and λσ) and show that they can be accommodated with an adequate notion of PT. Specifically, our results are as follows: • We introduce PT notions for the simply typed versions of the λseand the λσ-calculi and prove that they agree with Wells’ notion of PT. • We show that these versions satisfy PT by revisiting previously introduced type inference algorithms.	algorithm;correctness (computer science);explicit substitution;first-order predicate;newman's lemma;normalization property (abstract rewriting);simply typed lambda calculus;subject reduction;type inference;type system;unification (computer science)	Daniel Lima Ventura;Mauricio Ayala-Rincón;Fairouz Kamareddine	2008		10.1007/978-3-540-69407-6_60	discrete mathematics;type system;computer science;type inference;lambda calculus;simply typed lambda calculus;mathematics;programming language;type inhabitation;algorithm;satisfiability	Logic	-14.415867017658453	18.658398422396477	199317
31314dfe91ec83febf9b9fd6de2a1332cbf31294	a structure-preserving clause form translation	structure preservation	Most resolution theorem provers convert a theorem into clause form before attempting to find a proof. The conventional translation of a first-order formula into clause form often obscures the structure of the formula, and may increase the length of the formula by an exponential amount in the worst case. We present a non-standard clause form translation that preserves more of the structure of the formula than the conventional translation. This new translation also avoids the exponential increase in size which may occur with the standard translation. We show how this idea may be combined with the idea of replacing predicates by their definitions before converting to clause form. We give a method of lock resolution which is appropriate for the non-standard elause form translation, and which has yielded a spectacular reduction in search space and time for one example. These techniques should increase the attractiveness of resolution theorem provers for program verification applications, since the theorems that arise in program verification are often simple but tedious for humans to prove.	best, worst and average case;first-order predicate;formal verification;predicate (mathematical logic);resolution (logic);standard translation;time complexity	David A. Plaisted;Steven Greenbaum	1986	J. Symb. Comput.	10.1016/S0747-7171(86)80028-1	arithmetic;mathematics;algorithm	Logic	-15.000803390830848	21.413071791864454	199518
