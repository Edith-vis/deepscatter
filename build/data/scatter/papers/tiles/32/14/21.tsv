id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c95c84b08acfd15e96fd0f087700155f49e45dbc	calculation and use of peaking factors for remote terminal emulation	workload;evaluation performance;emulateur;performance evaluation;ensayo en el banco;evaluacion prestacion;sistema informatico;computer system;charge travail;systeme informatique;emulador;carga trabajo;bench test;essai banc;emulator	An important goal of the remote terminal emulator-driven tests described here was obtaining a representative test workload. Reaching this goal depended on (i) imposing the test workload in a representative manner, (ii) using representative types of user scenarios in that workload, and (iii) specifying a representative number of each type. How the first two subgoals were reached is briefly described. Achieving the third required definition and calculation of em peaking factors. A tool for calculating these quantities is described.	terminal emulator	William A. Ward;David D. Langan	2000		10.1007/3-540-46429-8_27	simulation;operations research	HCI	-18.507174585685156	46.19201952048383	145135
6299b639b91996a7210b44586ef7c09bf740edd4	das hicon-modell	parallelisme;communication process;topology;estacion trabajo;dynamic load balancing;workstation network;resource allocation;station travail;reseau ordinateur;topologie;prise decision;classification;topologia;computer network;proceso comunicacion;synchronisation;data intensive applications;processus communication;workstation;parallelism;paralelismo;dynamic allocation;synchronization;red ordenador;procesador;sincronizacion;systeme parallele;asignacion recurso;asignacion dinamica;parallel system;processeur;allocation ressource;allocation dynamique;toma decision;clasificacion;parallel applications;processor;sistema paralelo	Das hier vorgestellte Modell versucht im wesentlichen durch drei Ansätze, die Leistungssteigerungen durch dynamische Lastbalancierung für ein deutlich breiteres Feld von Anwendungen und Systemen anwendbar zu machen, als es derzeit möglich ist. Teilsysteme werden durch zentrale Balancierungsverfahren verwaltet; erst zwischen Teilsystemen werden dezentrale Verfahren eingesetzt, um hohe Skalierbarkeit zu gewährleisten. Das ermöglicht Lastverteilung im Zusammenspiel der Anwendungen und Ressourcen und vermeidet kontra-produktive Entscheidungen unabhängiger Balancierungskomponenten. Der zweite wichtige Ansatz ist die Berücksichtigung mehrerer Ressourcen für Lastbalancierungsentscheidungen. So können Verteilungsstrategien im HiCon-Modell beispielsweise sowohl Prozessorauslastungen als auch Datenaffinitäten der Anwendungen im Entscheidungsalgorithmus kombinieren. Das eröffnet der Lastbalancierung ein weiteres Spektrum unterschiedlicher Anwendungen auf heterogenen Systemen. Als dritter Schwerpunkt soll Lastbalancierung im HiCon-Modell adaptiv auf aktuelle Systemlast- und Anwendungsprofile reagieren können. Dazu werden für schwer vorabsehbare Größen und Effekte durch Beobachtung des realen Verhaltens Gewichtungen für Balancierungsentscheidungen dynamisch eingeregelt, und die Effizienz der Lastbalancierung wird durch angepaßte Reduktion des Zusatzaufwandes gewährleistet. Der Artikel stellt die Grundideen vor und validiert die Konzepte durch Messung verschiedener Anwendungen auf einem heterogenen Netz von Arbeitsplatzrechnern. Dazu werden mehrere verschiedene parallelisierte Anwendungen konkurrierend auf dem Rechnersystem balanciert. The HiCon model investigates three approaches to exploit performance improvement by dynamic load balancing for a wider range of applications and systems than it is possible nowadays. Centralized load balancing schemes manage cells of the whole system. Between cells decentralized strategies are employed to achieve high scalability. This enables harmonized load balancing of applications and resources and avoids contra-productive decisions of independent load balancing agents. The second approach is the consideration of multiple resources for load balancing decisions. For example, load balancing policies in the HiCon model may combine both processor utilization and data affinities of the applications for decision making. This opens a wide range of different application types for load balancing in heterogeneous environments. The third issue is the facility to dynamically adapt HiCon load balancing to current system load and application profiles. Therefore parameter values for load balancing decisions can be adjusted dynamically according to observations of the real behavior for quantities and effects that are hardly foreseeable. Efficiency of dynamic load balancing is guaranteed by situation-specific reduction of additional overhead. This article introduces the main ideas and validates the concepts through measurement of different applications on a heterogeneous workstation network. Therefore several different parallelized Applications are balanced concurrently within the computing system.	blitzkrieg;centralized computing;gesellschaft für informatik;load (computing);load balancing (computing);overhead (computing);parallel computing;scalability;unified model;v-model;vhf omnidirectional range;workstation	Wolfgang Becker	1995	Informatik Forschung und Entwicklung	10.1007/PL00009129	embedded system;synchronization;telecommunications;computer science;artificial intelligence	HPC	-17.80869018231647	44.506730139799494	145168
16fad85a98125f024fce94b11acc70c4486943a6	design of an os9 operating system extension for a message-passing multiprocessor	interprocess communication;virtual circuit;group communication;operating system;message passing	This paper describes an interprocess communication system built around a message-passing model which we have developed for a busbased multiprocessor. It is based on an OS9 operating system kernel in each processor, and comprises an integrated set of system modules. These manage message communication between processors and provide a global communication interface for application processes through which they can exchange messages in a transparent way. The implementation uses channels and virtual circuits and the communication primitives support synchronous and asynchronous message transfer, and group communication. © 1998 Elsevier Science B.V.	central processing unit;channel (programming);inter-process communication;kernel (operating system);message passing;multiprocessing;os-9;operating system;virtual circuit	Ernesto F. V. Martins;António Nunes da Cruz	1998	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(98)00046-5	mailslot;embedded system;parallel computing;message passing;real-time computing;communication in small groups;computer science;operating system;distributed computing;channel;virtual circuit;inter-process communication	Arch	-12.049053889871818	45.28400779206074	145708
8fccffa10b54c578d4e78c1039bbf8972405459b	distributed instruction set computer architecture	computadora;modelizacion;system model distributed instruction set computer fine grained multiprocessing computer architecture parallel instruction set distributed control mechanism;distributed system;distributed control mechanism;compilacion;architecture systeme;systeme reparti;enlace;system modeling;ordinateur;distributed processing;simulation;simulacion;computer;out of order;modelisation;fine grained multiprocessing;computer architecture;liaison;parallel instruction set;sistema repartido;parallel architectures;computer architecture distributed control parallel processing centralized control out of order logic computer aided instruction decoding hardware distributed computing;scheduling;system model;distributed instruction set computer;compilation;arquitectura sistema;ordonamiento;systeme parallele;parallel system;functional unit;system architecture;modeling;distributed control;binding;parallel architectures distributed processing instruction sets;parallel processing;ordonnancement;sistema paralelo;instruction sets	The Distributed Instruction Set Computer Architecture (DISC) is proposed as a fine-grained multiprocessing computer architecture. DISC uses a parallel instruction set and a distributed control mechanism to explore fine-grained, parallel processing in a multiple-functional-unit system. Multiple instructions are executed in parallel and/or out of order at the highest speed of n instructions/cycle, where n is the number of functional units. Based on this architecture, a hardware system is developed. Extensive studies were conducted on a behavioral DISC system model to investigate the performance level, the effect of program sizes, and the hardware utilization. Simulation showed that a DISC system incorporating 16 functional units can run 7.7 times faster than a single-functional-unit DISC system. >	computer architecture	Lingtao Wang;Chuan-lin Wu	1991	IEEE Trans. Computers	10.1109/12.83637	embedded system;parallel processing;reduced instruction set computing;computer architecture;parallel computing;real-time computing;systems modeling;computer science;operating system;instruction set;programming language;instructions per cycle	Arch	-15.244219818107277	43.76664476287025	145862
3054709ad30593e6e18c871c974c1ea16a6caf3f	concurrency and trie hashing	hachage;memory layout;range query;storage access;search strategy;estructura archivo;simultaneite;organizacion memoria;concurrency;hashing;simultaneidad;arbol binario;structure fichier;arbre binaire;file structure;acces memoire;organisation memoire;strategie recherche;acceso memoria;hash function;access method;data structure;estrategia investigacion;binary tree	The Trie Hashing (TH), defined by Litwin, is one of the fastest access methods for dynamic and ordered files. The hashing function is defined in terms of a trie, which is basically a binary tree where a character string is associated implicitly with each node. This string is compared with a prefix of the given key in the search process, and depending on the result either the left or the right child is chosen as the next node to visit. The leaf nodes point to buckets which contain the records. The buckets are on a disk, whereas the trie itself is in the core memory. In this paper we consider concurrent execution of the TH operations. In addition to the usual search, insertion and deletion operations, we also include range queries among the concurrent operations. Our algorithm locks only leaf nodes and at most two nodes need to be locked simultaneously by any operation regardless of the number of buckets being accessed. The modification required in the basic data structure in order to accommodate concurrent operations is very minor.	algorithm;binary tree;data structure;fastest;hash function;locality-sensitive hashing;lock (computer science);magnetic-core memory;range query (data structures);string (computer science);tree (data structure);trie	Witold Litwin;Yehoshua Sagiv;Krishnamurthy Vidyasankar	1989	Acta Informatica	10.1007/BF00288973	parallel computing;hash function;extendible hashing;data structure;computer science;theoretical computer science;database;programming language;x-fast trie;algorithm	DB	-17.282519804146556	45.1244095784963	146210
603d1fc1e30cceed6b604ed913fdc9dd7ead70b7	an and/or-parallel implementation of akl	parallelisme;lenguaje programacion;concurrent language;fijacion;verrouillage;shared memory;programming language;multiprocessor;programming paradigm;implementation;memoria compartida;prolog;langage concurrent;coaccion;locking;contrainte;simultaneidad informatica;logical programming;ejecucion;parallelism;concurrency;constraint;paralelismo;logic programming;programmation logique;langage programmation;distributed work;parallel implementation;logic programs;binding scheme;multiprocesador;programacion logica;simultaneite informatique;memoire partagee;shared memory multiprocessor;multiprocesseur	The Agents Kernel Language (AKL) is a general purpose concurrent constraint language. It combines the programming paradigms of search-oriented languages such as Prolog and process-oriented languages such as GHC. The paper is focused on three essential issues in the parallel implementation of AKL for shared-memory multiprocessors: how to maintain multiple binding environments, how to represent the execution state and how to distribute work among workers. A simple scheme is used for maintaining multiple binding environments. A worker will immediately see conditional bindings placed on variables, all workers will have a coherent view of the constraint stores. A locking scheme is used that entails little overhead for operations on local variables. The goals in a guard are represented in a way that allows them to be inserted and removed without any locking. Continuations are used to represent sequences of untried goals. The representation keeps the granularity of work more coarse. Available work is distributed among workers in such a way that hot-spots are avoided. And- and or-tasks are distributed and scheduled in a uniform way.	coherence (physics);conditional (computer programming);continuation;guard (information security);local variable;lock (computer science);overhead (computing);programming paradigm;prolog;shared memory;the glorious glasgow haskell compilation system	Johan Montelius;Khayri A. M. Ali	1996	New Generation Computing	10.1007/BF03037217	shared memory;computer architecture;parallel computing;multiprocessing;concurrency;computer science;programming paradigm;constraint;fifth-generation programming language;programming language;implementation;prolog;logic programming;second-generation programming language;comparison of multi-paradigm programming languages	PL	-16.983554307785344	40.8212405299428	146359
d499494971802387dff636abefc5a1834cac2b8c	architecture of the component collective messaging interface	programming language;programming paradigm;active messages;collective communication;reduce;non blocking collectives;broadcast;software architecture;blue gene;message passing;allreduce;mpi;c programming language;component architecture;barrier	Different programming paradigms utilize a variety of collective communication operations, often with different semantics. We present the component collective messaging interface (CCMI) that can support asynchronous non-blocking collectives and is extensible to different programming paradigms and architectures. CCMI is designed with components written in the C++ programming language, allowing it to be reusable and extendible. Collective algorithms are embodied in topological schedules and executors that execute them. Portability across architectures is enabled by the multisend data movement component. CCMI includes a programming language adaptor used to implement different APIs with different semantics for different paradigms. We study the effectiveness of CCMI on 16K nodes of Blue Gene/P machine and evaluate its performance for the barrier, broadcast, and allreduce collective operations and several application benchmarks. We also present the performance of the barrier collective on the Abe Infiniband cluster.		Sameer Kumar;Ahmad Faraj;Amith R. Mamidala;Brian E. Smith;Gábor Dózsa;Bob Cernohous;John A. Gunnels;Douglas Miller;Joe Ratterman;Philip Heidelberger	2010	IJHPCA	10.1177/1094342009359011	software architecture;parallel computing;message passing;real-time computing;computer science;message passing interface;operating system;distributed computing;programming paradigm;programming language;algorithm	HPC	-12.026300421757727	41.4628351097859	146664
7d14e4d3dd49748bcd65d3ecb02bdaba5e125f02	formal methods and algorithms for parallel real-time computing		Parallel computers are becoming increasingly popular as vehicles for speeding up the execution of programs, enabling more real-time processes to execute and to meet deadlines than would be possible otherwise. Formal methods are useful for exploiting parallelism in real-time systems and some techniques used to develop parallel algorithms are useful for developing predictable real-time systems.	algorithm;formal methods;real-time computing;real-time transcription	Lonnie R. Welch	1992		10.1007/978-3-642-88049-0_126	parallel computing;formal methods;formal verification;theoretical computer science;analysis of parallel algorithms;distributed computing;unconventional computing	AI	-11.89667962506395	42.30286496622024	146884
133388bc395fdc62ac41d94cb7999a0dc86682ee	a concurrent search structure	tratamiento paralelo;shared memory;storage access;traitement parallele;multiprocessor;memoria compartida;sistema informatico;computer system;simultaneite;concurrency;simultaneidad;estructura datos;acces memoire;acceso memoria;sibluig tries;structure donnee;systeme informatique;multiprocesador;data structure;parallel processing;memoire partagee;multiprocesseur	The sibling trie is a highly concurrent dynamic search structure. It supports search, update, insertion, and deletion. The sibling trie is designed to minimize “hot spots” in highly concurrent shared memory environments. The main novelty of this data structure is that searches can start from any node. As a result, many alternate routes are provided to each datum, and the bottleneck of a single root node is avoided. Search time is logarithmic (regardless of the starting point), and storage is proportional to the number of data items and independent of the number of processes that access the data structure.	data structure;geodetic datum;shared memory;tree (data structure);trie	Jeff D. Parker	1989	J. Parallel Distrib. Comput.	10.1016/0743-7315(89)90020-8	shared memory;parallel processing;parallel computing;multiprocessing;concurrency;data structure;computer science;operating system;database;programming language;algorithm	Theory	-17.200706269019623	44.97317894217022	147186
63089786ea2ae3b190c095fdd94965b402b3fb9c	visual, object-oriented development of parallel applications	software computer software;object oriented language;dr jim webber;eprints newcastle university;large scale;parallelism;vorlon;parallel programming language;object oriented;visual object oriented language;open access;object flow;architectures;parallel languages;parallel applications;parallel processing;professor pete lee	Parallelism is always going to be required to support the computational demands of some problem domains, and will continue to be exploited via `traditional? parallel processing methods and languages. However, this paper argues that parallelism will also become a requirement for the non-specialist user, but that the traditional parallelism languages and techniques do not have the right support for the engineering of large-scale, parallel applications. The paper discusses this issue, and presents a visual, object-oriented parallel programming language, Vorlon, which addresses the management of both problem domain complexity and implementation complexity, to support the development of general-purpose parallel applications by programmers who are non-specialists in parallelism.		James Webebr;Peter A. Lee	2001	J. Vis. Lang. Comput.	10.1006/jvlc.2000.0193	parallel processing;parallel computing;computer science;theoretical computer science;operating system;software engineering;database;data parallelism;programming language;object-oriented programming;instruction-level parallelism;implicit parallelism;task parallelism	HPC	-14.138919943292722	39.57852011871641	147329
00c2383da088f8e6d751233136c181732e12c5a9	parallel functional reactive programming	system modeling;program transformation;functional programming;interactive system;parallel systems;parallel implementation;parallel programs;functional language;functional reactive programming	In this paper , we demonstratehow FunctionalReacti ve Programming (FRP),a framework for the descriptionof interacti ve systems,canbe extended to encompassparallelsystems.FRPis basedonHaskell, a purelyfunctionalprogramminglanguage,and incorporatesthe conceptsof time variation andreactivity. ParallelFRPservesasadeclarati vesystemmodelthatmaybetransformedinto a parallelimplementationusingthestandardprogramtransformationtechniquesof functionalprogramming. Thesemanticsof parallelFRPincludenon-determinism, enhancingopportunitiesto introduceparallelism.We demonstratea variety of programtransformationsbasedonparallelFRPandshow how aFRPmodelmay be transformedinto explicitly parallelcode.ParallelFRPis implementedusing the Linda programmingsystemto handletheunderlyingparallelism.As an exampleof parallelFRP, we show how a specificationfor a web-basedonlineauctioning systemcanbetransformedinto a parallelimplementation.	functional programming;functional reactive programming;linda (coordination language)	John Peterson;Valery Trifonov;Andrei Serjantov	2000		10.1007/3-540-46584-7_2	parallel computing;systems modeling;reactive programming;functional reactive programming;computer science;functional logic programming;programming language;functional programming;algorithm;parallel programming model	Vision	-17.010252193538165	40.225906598484094	147898
800be594e4f5f3304f687763776a7ac106552216	flexible user-definable memory coherence scheme in distributed shared memory of galaxy	distributed system;distributed shared memory	In this paper we have described a user-definable memory coherence scheme for distributed shared memory that is flexible enough to meet the varying needs of a wide variety of user applications. We believe that the concepts presented in this paper will be useful for the design of other distributed systems.	distributed shared memory;memory coherence	Pradeep Kumar Sinha;Hyo Ashihara;Kentaro Shimizu;Mamoru Maekawa	1991		10.1007/BFb0032922	shared disk architecture;uniform memory access;distributed shared memory;shared memory;cache coherence;interleaved memory;parallel computing;real-time computing;distributed memory;distributed computing;memory coherence;flat memory model;data diffusion machine;cache-only memory architecture;memory map	HPC	-14.203496838406783	43.97268469049022	149084
b9e035edfe7f06c491c98d2e12f1ba13b16fdda5	associative processing in data base management.	air traffic control;macro processors;information retrieval;data independence;computer graphic;numerical analysis;data manipulation languages;associative memory;data management systems;large data	Associative memories and processors have been discussed in the literature for the past 15 years and a small number of hardware devices have actually been built (21). A number of applications have been considered and include air traffic control (22), computer graphics (24, 25), information retrieval (9), numerical analysis (15), networks (18, 19) and among others, data base management (2, 4-8, 10, 11, 13, 17).  A significant percentage of present computer resources are expended in the processing of large data bases. Of this, a great deal is wasted by processing largely non-sequential data by sequential means. With hardware costs going down; and software and personnel costs going up it is important to look at the impact of non-sequential computer hardware on data base management (DBM). Based upon previous research in the application of associative processing devices to DBM, it appears that the opportunity exists for increased efficiencies by utilizing these devices in conjunction with sequential computers. In this paper, some of the advantages and limitations of these devices are presented and some brief thoughts are given concerning this author's view of the future. It is assumed here that the reader has a working knowledge of associative processors and DBM.	central processing unit;computer graphics;computer hardware;database;dbm;information retrieval;numerical analysis	P. Bruce Berra	1974		10.1145/800296.811526	data independence;parallel computing;numerical analysis;computer science;theoretical computer science;air traffic control;content-addressable memory;database	DB	-12.894758121536073	45.86674385314817	149297
073001bcd0421496da6bc0c976a9f9f6bec99a22	incremental parallelization with migration	pivot computes parallel and distributed programming incremental parallelization migration feedback tiling;multi threading;migration;pivot computes;processor scheduling;software performance evaluation;tiles instruction sets distributed databases synchronization processor scheduling computer science;tiling;incremental parallelization;feedback;incremental performance improvement;parallel and distributed programming;computer science incremental parallelization with migration university of california irvine lubomir f bic;synchronization;distributed databases;tiles;computer science;data handling;michael b dillencourt zhang wenhui;data distribution incremental parallelization parallel distributed programs data partitioning migration minimization remote access statements executable code performance improvement;software performance evaluation data handling multi threading program compilers;program compilers;instruction sets;feeback	We present a new methodology for developing parallel distributed programs in a series of incremental steps. The methodology takes advantage of threads that are able to migrate through the network and thus are able to follow distributed data. This allows the data to be partitioned and distributed first, which guarantees that elements that are used together in a computation are collocated on the same node. Next, the loops in the code are tiled to minimize migration among nodes. After deciding on the location at which each loop is to execute, the necessary migration and remote access statements are inserted to make the code executable. This process is repeated based on feedback obtained from the execution, which may improve the overall performance by suggesting a different data distribution or a different coarseness of tiling. We illustrate the trade-offs and the performance using a well-known application with two different data distributions.	automatic parallelization;computation;executable;parallel computing;tiling window manager	Wenhui Zhang	2012	2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2012.37	synchronization;computer architecture;parallel computing;multithreading;telecommunications;computer science;human migration;theoretical computer science;operating system;group method of data handling;instruction set;feedback;database;distributed computing;programming language;distributed database	HPC	-13.766783102290262	45.41896658985144	149570
4abd60a08e017f0cf6d7d78eb80413a366048e9f	hands-on training for undergraduates in high-performance computing using java	informatica;tratamiento datos;extraction information;parallelisme;guide programme etude;evaluation performance;haute performance;performance evaluation;red www;analisis datos;information extraction;numerical method;evaluacion prestacion;data processing;traitement donnee;data analysis;large scale;codificacion;parallelism;internet;paralelismo;paradigm shift;metodo numerico;coding;object oriented approach;curriculum guides;high performance computer;parallel computer;alto rendimiento;world wide web;analyse donnee;informatique;reseau www;computer science;escala grande;high performance;methode numerique;large scale problem;codage;echelle grande;extraction informacion	In recent years, the object-oriented approach has emerged as a key technology for building highly complex scientific codes, as has the use of parallel computers for the solution of large-scale problems. We believe that the paradigm shift towards parallelism will continue and, therefore, principles and techniques of writing parallel programs should be taught to the students at an early stage of their education rather than as an advanced topic near the end of a curriculum. A certain understanding of the practical aspects of numerical modeling is also a useful facet in computer science education. The reason is that, in addition to their traditional prime role in computational science and engineering, numerical techniques are also increasingly employed in seemingly nonnumerical settings as large-scale data mining and web searching. This paper describes a practical training course for undergraduates, where carefully selected problems of high-performance computing are solved using the programming language Java.	java	Christian H. Bischof;H. Martin Bücker;Jörg Henrichs;Bruno Lang	2000		10.1007/3-540-70734-4_36	paradigm shift;parallel computing;the internet;simulation;data processing;numerical analysis;computer science;artificial intelligence;database;coding;data analysis;information extraction;algorithm;statistics	HPC	-15.864617168381509	41.17800522470832	150806
2734f7cdfcd5c50b9d28e33b332fd0e1fd542510	task scheduling facility for pvm	distributed system;virtual machine;eficacia sistema;cargamento;equilibrado;architecture systeme;systeme reparti;heterogenous computing;heterogeneous computing;etude experimentale;reseau ordinateur;performance systeme;loading;chargement;machine virtuelle;system performance;computer network;algorithme;algorithm;client server;sistema repartido;pvm;scheduling;balancing;red ordenador;load balancing;arquitectura sistema;ordonamiento;task scheduling;system architecture;maquina virtual;estudio experimental;equilibrage;ordonnancement;algoritmo	Whilst the concept of a virtual metacomputer over a networked collection of heterogeneous computer systems has slowly emerged. there are still some shortcomings of these systems. For example. some systems are still incapable of balancing the load amongst the workstations; the problem is accentuated by the heterogeneity of the computers. In this paper. we first discuss the design issue of our centralized task scheduler. and then present our implementation details. To use the task scheduler. some new library routines are provided. The task scheduler is layered above PVM; this has the advantage of retaining its portability. Since load-balancing is considered in task scheduling. our approach has been also proven to be more effective than the existing PVM round-robin task allocation scheme.	parallel virtual machine;schedule (project management);scheduling (computing)	Bu-Sung Lee;Alfred Heng;Wentong Cai;Tai-Ann Tan	1996	Parallel Processing Letters	10.1142/S0129626496000509	fixed-priority pre-emptive scheduling;parallel computing;real-time computing;computer science;virtual machine;load balancing;operating system;computer performance;scheduling;symmetric multiprocessor system;client–server model;systems architecture	HPC	-16.8416689789842	43.80090070400717	151065
e1bd4d45476525ff40268c8ef740dac3d9405098	automatic collapsing of non-rectangular loops		Loop collapsing is a well-known loop transformation which combines some loops that are perfectly nested into one single loop. It allows to take advantage of the whole amount of parallelism exhibited by the collapsed loops, and provides a perfect load balancing of iterations among the parallel threads. However, in the current implementations of this loop optimization, as the ones of the OpenMP language, automatic loop collapsing is limited to loops with constant loop bounds that define rectangular iteration spaces, although load imbalance is a particularly crucial issue with non-rectangular loops. The OpenMP language addresses load balance mostly through dynamic runtime scheduling of the parallel threads. Nevertheless, this runtime schedule introduces some unavoidable executiontime overhead, while preventing to exploit the entire parallelism of all the parallel loops. In this paper, we propose a technique to automatically collapse any perfectly nested loops defining non-rectangular iteration spaces, whose bounds are linear functions of the loop iterators. Such spaces may be triangular, tetrahedral, trapezoidal, rhomboidal or parallelepiped. Our solution is based on original mathematical results addressing the inversion of a multi-variate polynomial that defines a ranking of the integer points contained in a convex polyhedron. We show on a set of non-rectangular loop nests that our technique allows to generate parallel OpenMP codes that outperform the original parallel loop nests, parallelized either by using options “static” or “dynamic” of the OpenMPschedule clause.	code;computation;data parallelism;infinite loop;iteration;iterator;langton's loops;linear function;load balancing (computing);loop optimization;mathematical optimization;openmp;overhead (computing);parallel computing;polyhedron;polynomial;scheduling (computing)	Philippe Clauss;Ervin Altintas;Matthieu Kuhn	2017	2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2017.34	parallel computing;computer science;distributed computing;loop interchange;loop inversion;loop fusion;loop splitting;loop counter;conditional loop;loop fission;loop tiling	HPC	-13.68373503821849	45.993336911754334	151380
8d23d08fbb95a7ac6bc98551cbe136f01f81a8c2	evaluation of two programming paradigms for heterogeneous computing	program graph;agregacion;evaluation performance;paradigme programmation;heterogenous computing;performance evaluation;execution time;programming paradigm;heterogeneous computing;computability;evaluacion prestacion;instruction;abstraction;instruccion;ejecucion programa;calculo automatico;abstraccion;aggregation;program execution;computing;calcul automatique;synchronisation;synchronization;levels of abstraction;graphe programme;execution programme;calculabilite;agregation;hasc;temps execution;cluster m;sincronizacion;tiempo ejecucion;parallel programs;grafo programa;calculabilidad;heterogeneous network	In this paper, we evaluate two diierent programming paradigms, Cluster-M and Heterogeneous Associative Computing (HAsC) for heterogeneous computing. These paradigms can eeciently support heterogeneous networks by preserving a level of abstraction without containing any architectural details. The paradigms are architecturally independent and scalable for various network and problem sizes. Cluster-M can be applied to both coarse-grained and ne-grained networks. Cluster-M provides an environment for porting heterogeneous tasks onto the machines in a heterogeneous suite such that resource utilization is maximized and the overall execution time is minimized. HAsC models a heterogeneous network as a coarse-grained associative computer. It is designed to optimize the execution of problems where the program size is small compared with the amount of data processed. Unlike other existing heterogeneous orchestration tools which are MIMD based, HAsC is for data-parallel SIMD associative computing. Ease of programming and execution speed are the primary goals of HAsC. We evaluate how these two paradigms can be used together to provide an eecient scheme for heterogeneous programming. Finally, their scalability issues are discussed.	heterogeneous computing;mimd;programming paradigm;run time (program lifecycle phase);simd;scalability	Song Chen;Mary Mehrnoosh Eshaghian-Wilner;Richard F. Freund;Jerry L. Potter;Ying-Chieh Wu	1995	J. Parallel Distrib. Comput.	10.1006/jpdc.1995.1143	synchronization;parallel computing;real-time computing;telecommunications;computer science;operating system;distributed computing;programming language	HPC	-14.819897090993534	41.989110468553754	152059
00408c53be7a65c9d2e702c1810272d4e759701c	performance improvement using parallel simulation protocol and time warp for devs based applications	shared memory multiprocessor machine;time warp;protocols;dicrete event system specification;formal specification;time warp optimistic algorithm;software performance evaluation;continuous system;simultaneous scheduling;time warp simulation formal specification parallel processing scheduling shared memory systems software performance evaluation;performance improvement;computational modeling;adaptation model;shared memory systems;devsfire;scheduling;heuristic algorithms;time warp simulation protocols computational modeling discrete event simulation computer simulation power system modeling application software computer science processor scheduling testing;devsfire performance improvement parallel simulation protocol parallel discrete event system simulation protocol simultaneous scheduling time warp optimistic algorithm shared memory multiprocessor machine dicrete event system specification;time warp simulation;parallel discrete event system simulation protocol;parallel simulation protocol;program processors;algorithm design and analysis;parallel processing;discrete event;parallel simulation;data models;shared memory multiprocessor	DEVS is a formalism intended to model both discrete and continuous systems. The use of discrete events, rather than time steps, as the basis for simulation has been shown to reduce the computation time in many applications. Parallel DEVS is an extension to standard DEVS, which provides means to handle simultaneous scheduling. In this paper, we present an implementation of the parallel DEVS simulation protocol that uses a modified time warp optimistic algorithm for shared memory multiprocessor machine. This implementation is designed to execute the DEVS models in parallel and, at the same time to correctly simulate every model defined in terms of DEVS specification. Two test cases and the DEVSFIRE example are used to verify this algorithm. Preliminary experimental results are presented that show the implementation can speedup a DEVS simulation.	computation;devs;dynamic time warping;experiment;load balancing (computing);multiprocessing;overhead (computing);parallel algorithm;parallel computing;run time (program lifecycle phase);scheduling (computing);semantics (computer science);shared memory;simulation;speedup;test case;time complexity	Yi Sun;James J. Nutaro	2008	2008 12th IEEE/ACM International Symposium on Distributed Simulation and Real-Time Applications	10.1109/DS-RT.2008.24	computer architecture;parallel computing;real-time computing;computer science;devs	Embedded	-12.916263648726542	42.41739756333108	152116
e5700010e8b9a5010b8b2eeccdb4c80c6a5efd9d	task pool teams: a hybrid programming environment for irregular algorithms on smp clusters	dynamic change;hybrid programming;programming environment;hybrid parallel programming;irregular algorithms;programming model;application programmer interface;asynchronous communication;data access;hierarchical radiosity;message passing;communication protocol;load balance;smp clusters;parallel programs;smp cluster	Clusters of SMPs (symmetric multiprocessors) are popular platforms for parallel programming since they provide large computational power for a reasonable price. For irregular application programs with dynamically changing computation and data access behavior a flexible programming model is needed to achieve efficiency. In this paper we propose Task Pool Teams as a hybrid parallel programming environment to realize irregular algorithms on clusters of SMPs. Task Pool Teams combine task pools on single cluster nodes by an explicit message passing layer. They offer load balance together with multi-threaded, asynchronous communication. Appropriate communication protocols and task pool implementations are provided and accessible by an easy to use application programmer interface. As application examples we present a branch & bound algorithm and the hierarchical radiosity algorithm.	algorithm;branch and bound;computation;computer cluster;data access;data structure;data-intensive computing;integrated development environment;load balancing (computing);message passing interface;message passing;parallel computing;programmer;programming model;radiosity (computer graphics);symmetric multiprocessing;telecommunications network;thread (computing)	Judith Hippold;Gudula Rünger	2006	Concurrency and Computation: Practice and Experience	10.1002/cpe.1006	data access;communications protocol;parallel computing;message passing;real-time computing;computer science;load balancing;asynchronous communication;distributed computing;programming paradigm;programming language	HPC	-12.43585098434225	43.22596038362292	152122
94b9caf3dca0cd9d15d654d55dc5b5d181329d07	a quest for unified, global view parallel programming models for our future		Developing highly scalable programs on today's HPC machines is becoming ever more challenging, due to decreasing byte-flops ratio, deepening memory/network hierarchies, and heterogeneity. Programmers need to learn a distinct programming API for each layer of the hierarchy and overcome performance issues at all layers, one at a time, when the underlying high-level principle for performance is in fact fairly common across layers---locality. Future programming models must allow the programmer to express locality and parallelism in high level terms and their implementation should map exposed parallelism onto different layers of the machine (nodes, cores, and vector units) efficiently by concerted efforts of compilers and runtime systems. In this talk, I will argue that a global view task parallel programming model is a promising direction toward this goal that can reconcile generality, programmability, and performance at a high level. I will then talk about our ongoing research efforts with this prospect. They include: MassiveThreads, a lightweight user-level thread package for multicore systems; MassiveThreads/DM, its extension to distributed memory machines; DAGViz, a performance analyzer specifically designed for task parallel programs; and a task-vectorizing compiler that transforms task parallel programs into vectorized and parallelized instructions. I will end by sharing our prospects on how emerging hardware features and fruitful co-design efforts may help achieve the challenging goal.	application programming interface;byte;compiler;computer hardware;distributed memory;flops;high- and low-level;high-level programming language;locality of reference;multi-core processor;parallel computing;parallel programming model;performance analyzer;programmer;runtime system;scalability;user space	Kenjiro Taura	2016		10.1145/2931088.2931089	parallel computing;real-time computing;computer science;theoretical computer science	HPC	-13.462635587230519	40.13373369952426	152373
65cf61ff3e31089f37cdc26ce6ae8274d5c67c6d	geometrical interpretation for data partitioning on a grid architecture	clustering algorithms sorting load management partitioning algorithms context computer architecture storage area networks broadcasting communication system control ethernet networks;resource allocation;data partitioning;workstation clusters grid computing open systems resource allocation;load balancing;load balance;workstation clusters;sort algorithm data partitioning grid architecture load balancing;open systems;sorting algorithm;grid computing;grid architecture;sort algorithm	We study, in this work, the load balancing of sort algorithm executed on a two cluster grid. Our solution is based on data partitioning. We use mainly geometrical interpretations to find out the optimal partition that reduces both communication and computing times in an heterogeneous context	grid computing;load balancing (computing);sorting algorithm	Dominique Bernardi;Christophe Cérin;Hazem Fkaier;Mohamed Jemni;Michel Koskas	2006	2006 15th IEEE International Conference on High Performance Distributed Computing	10.1109/HPDC.2006.1652181	grid file;parallel computing;real-time computing;computer science;load balancing;sorting algorithm;distributed computing;grid computing	HPC	-12.333113532972787	43.639226673761485	152506
8b2469ae497cd3207fec1ae1454338c9c34dd846	proteus system architecture and organization	organization;large granularity tasks;image processing;fault tolerant;circuit switched enhanced hypercube serial interconnection network;circuit switched;partitioning;software systems;hierarchical reconfigurable interconnection network;interconnection network;computer vision;20 gflops;fault tolerant computing;20 gflops proteus system architecture organization highly parallel mimd multiple instruction multiple data large granularity tasks machine vision image processing hierarchical reconfigurable interconnection network circuit switched enhanced hypercube serial interconnection network read write allocating caches fault tolerance partitioning scheduling;parallel architectures;highly parallel mimd;parallel architectures computer vision computerised picture processing fault tolerant computing hypercube networks;scheduling;machine vision;fault tolerance;computerised picture processing;read write allocating caches;multiple instruction multiple data;multiprocessor interconnection networks machine vision image processing switching circuits hypercubes reduced instruction set computing electrical fault detection fault detection circuit faults hardware;system architecture;proteus system architecture;data transfer;hypercube networks	The Proteus architecture is a highly parallel MIMD, multiple instruction multiple data, machine, optimized for large granularity tasks such as machine vision and image processing. The system can achieve 20 G-flops (80 G-flops peak). It accepts data via multiple serial links at a rate of up to 640 megabytes/mnd. The system employs hierarchical reconfigurable interconnection network with the highest level being a circuit switched Enhanced Hypercube serial interconnection network for intemal data transfers. The system is designed to use 256 to 1,024 RISC processors. The processors use 1 M byte external R e d w r i t e Allocating Caches for reduced multiprocessor contention. The system detects, locates and replaces faulty subsystems using redundant hardware to facilitatefault tolerance. The parallelism is directly controllable through an advanced software system for partitioning, scheduling and development.	byte;central processing unit;circuit switching;flops;fault tolerance;image processing;interconnection;mimd;machine vision;megabyte;multiprocessing;parallel computing;proteus;scheduling (computing);software system;systems architecture	Arun K. Somani;Craig M. Wittenbrink;Robert M. Haralick;Linda G. Shapiro;Jenq-Neng Hwang;Chung-Ho Chen;Robert Johnson;Kenneth Cooper	1991		10.1109/IPPS.1991.153793	computer architecture;parallel computing;real-time computing;computer science	Arch	-12.03947994312375	44.55874635668488	152543
46990bd37b6a88eecc5b61686a8e86cd81a7f429	supporting dynamic parallel object arrays	parallel hashtable;object migration;indexation;parallel machines;load balance;parallel runtime;workstation cluster;dynamic behavior	"""ABSTRACT We present efficient support for generalized arrays of parallel data driven objects. Array elements are regular C++ objects, and are scattered across the parallel machine. An individual element is addressed by its """"index"""", which can be an arbitrary object rather than a simple integer. For example, an array index can be a series of numbers, supporting multidimensional sparse arrays; a bit vector, supporting collections of quadtree nodes; or a string. Methods can be invoked on any individual array element from any processor, and the elements can participate in reductions and broadcasts. Individual elements can be created or deleted dynamically at any time. Most importantly, the elements can migrate from processor to processor at any time. The paper discusses support for message delivery and collective operations in face of such dynamic behavior. The migration capabilities of array elements have proven extremely useful, for example, in implementing flexible load balancing strategies and for exploiting workstation clusters adaptively. Additional Keywords: parallel runtime, object migration, parallel hashtable."""		Orion Sky Lawlor;Laxmikant V. Kalé	2003	Concurrency and Computation: Practice and Experience	10.1002/cpe.665	parallel computing;real-time computing;computer science;load balancing;distributed computing;computer security	SE	-13.828459517054583	46.288882933793246	153118
6b4bbb1894afec283b34f1e57be9bda6cc7bba74	overlapping window algorithm for computing gvt in time warp	multiprocessor interconnection networks;time warp;global virtual time;frequency synchronization;clocks;processor scheduling;distributed processing;minimum local virtual time;transient message problem;interconnection network;bbn butterfly;computational modeling;computing gvt;hybrid partitioning;window algorithms overlapping;moon;interconnection networks hybrid partitioning window algorithms overlapping computing gvt time warp global virtual time minimum local virtual time transient message problem distributed shared memory machines bbn butterfly message passing machines;message passing;interconnection networks;distributed shared memory machines;computer science;time warp simulation;computational modeling discrete event simulation time warp simulation message passing frequency synchronization clocks moon computer science multiprocessor interconnection networks processor scheduling;message passing machines;circuit analysis computing;distributed processing circuit analysis computing;distributed shared memory;discrete event simulation	Techniques are proposed for computing a global virtual time (GVT), which is the minimum local virtual time of processes in Time Warp. The algorithm computes a conservative estimate of GVT using an approach which is considerably simpler than previous algorithms for computing GVT. This algorithm does not require a global synchronization of processors. An inherent problem is GVT computation relates to handling messages in transit. Several alternatives are proposed for solving the transient message problem. The algorithm is suitable for distributed shared memory machines such as the BBN Butterfly and message passing machines with a variety of interconnection networks. >	algorithm;dynamic time warping;x86 virtualization	Reid A. Baldwin;Moon-Jung Chung;Yunmo Chung	1991		10.1109/ICDCS.1991.148722	distributed shared memory;parallel computing;message passing;real-time computing;computer science;natural satellite;discrete event simulation;operating system;distributed computing;programming language;computational model	HPC	-13.183530542506109	42.99726765295103	153410
cb490a0d2ca89904326147126d1f07edf2f6eba2	assertions about past and future in highways: global flush broadcast and flush-vector-time	distributed system;informatique reparti;haute performance;systeme reparti;multimedia;high performance computing;distributed computing;radiodifusion;synchronisation;sistema repartido;synchronization;alto rendimiento;systeme reparti asynchrone;asynchronous distributed system;sincronizacion;broadcasting;high performance;radiodiffusion;calcul haute performance	Highways is a high-performance distributed-programming toolkit. The suite of broadcast primitives implemented in Highways, called Global-Flush Broadcast Primitives (GFBCASTS), is an alternative to the CBCAST primitive of ISIS. GFBCASTs permit making an assertion about messages broadcasted in the past of broadcasting m, in the future of broadcasting m, about both, or neither. In this paper, we define GFBCASTs. We also define flush-vector time, which is preferable to vector time because it has additional applications, e.g. in implementing GFBCASTs. Using flush-vector time, we give a simple implementation of GFBCASTs at costs comparable to the most lightweight implementation of CBCAST.	assertion (software development);cpu cache;distributed computing;isis;vector clock	Mohan Ahuja	1993	Inf. Process. Lett.	10.1016/0020-0190(93)90263-9	synchronization;supercomputer;atomic broadcast;simulation;telecommunications;computer science;distributed computing	Arch	-18.45808790538091	42.7289609943098	153539
1f6901ee32b102fdcf32a27bb9c6008ccbe0aae5	concurrent cs: preparing students for a multicore world	parallel programming;curriculum design	Current trends in microprocessor design are fundamentally changing the way that performance is extracted from computer systems. The previous programming model of sequential uniprocessor execution is being replaced quickly with a need to write software for tightly-coupled shared memory multiprocessor systems. Academicians and business leaders have both challenged programmers to update their skill sets to effectively tackle software development for these newer platforms [2].  At the University of Wisconsin - Eau Claire, we have taken steps early in our curriculum to introduce our students to concurrent programming. Our approach is not to add parallel programming as a separate class, but to integrate concurrency concepts into traditional material throughout a student's coursework, beginning in CS1. Our goal is for students to gain both familiarity and confidence in using parallelism to their advantage. This paper describes the programming process we seek to introduce to our students and provides example assignments that illustrate the ease of integrating this process into a typical curriculum.	claire;concurrency (computer science);concurrent computing;directive (programming);exploit (computer security);multi-core processor;multiprocessing;parallel computing;processor design;programmer;programming model;shared memory;software development;uniprocessor system	Daniel J. Ernst;Daniel E. Stevenson	2008		10.1145/1384271.1384333	computer architecture;parallel computing;real-time computing;reactive programming;computer science;software development;operating system;software engineering;programming paradigm;inductive programming;programming language;system programming;concurrent object-oriented programming	HCI	-14.756781879806622	39.766703860605325	154857
56d5fd03b3fbc88f9e312273e69ada6cb355c8e0	strategies optimization and integration in dsm	tratamiento paralelo;parallelisme;granularite;distributed memory;nudo estructura;eficacia sistema;hierarchical structure;evaluation performance;optimisation;nodes;release consistency;shared memory;performance evaluation;traitement parallele;optimizacion;flexibilidad;memoria compartida;evaluacion prestacion;performance systeme;estrategia;system performance;consistencia;parallel computation;strategy;consistency model;large scale;parallelism;calculo paralelo;paralelismo;granularity;consistance;grouped data;parallel computer;dsm;flexible structure;noeud structure;flexibilite;estructura flexible;optimization;conmutador;consistency maintenance;structure flexible;coarse grained;memoire repartie;strategie;calcul parallele;article;protocol engineering;consistency;parallel processing;group;protocol engine;flexibility;memoire partagee;commutateur;selector switch	In large-scale parallel computing that may contain many nodes, a computing task is often divided into several sub-tasks running on a large platform. We present a group-based DSM model named GDSM. In our solution, sub-tasks are to be mapped to groups, which applies different techniques based on the distinct characteristics of inter-group and intra-group data sharing. A series of alternatives are employed with respect to the intra-group and inter-group property of this mechanism: (1) Consistency model: Release Consistency vs. Scope Consistency; (2) Coherence protocol: multiple-writer and write-update protocol vs. single-writer and write-invalidate protocol; (3) Granularity: fine-grain vs. coarse-grain. Our strategy to combine grouping with consistency and granularity switch enhances flexibility, cuts down the overhead of consistency maintenance, increases the parallelism between groups and thus promotes system performance. We also put forward the architecture of Protocol Engine based on group implementing the GDSM scheme, the engine that achieves message-forwarding transparency, clear hierarchical structure and encapsulation of group.	cache coherence;consistency model;encapsulation (networking);overhead (computing);parallel computing;release consistency	Li Ji;Li Tianning;Guihai Chen;Xie Lie;C. L. Wang	2000	Operating Systems Review	10.1145/506117.506122	shared memory;parallel processing;parallel computing;real-time computing;granularity;distributed memory;strategy;computer science;consistency model;operating system;release consistency;distributed computing;computer performance;group;grouped data;node;consistency	HPC	-17.06967336007449	44.09406543794492	154978
94acd619520cfc17d87bb4b065a02f45571b9d60	twlinux : operating system support for optimistic parallel discrete event simulation	distributed system;sistema operativo;haute performance;systeme reparti;systeme evenement discret;simulation;distributed computing;simulacion;sistema acontecimiento discreto;discrete event system;sistema repartido;operating system;distributed computing system;parallel discrete event simulation;alto rendimiento;calculo repartido;systeme exploitation;distributed simulation;high performance;simulation model;calcul reparti;discrete event simulation	Parallel or Distributed Discrete Event Simulation (PDES) refers to the concurrent execution of a single discrete event simulation application on a parallel or distributed computing system. Most available PDES implementations provide user level library support for writing distributed simulation applications. We discuss how OS support can be designed to facilitate optimistic PDES of large, complex simulation models. TWLinuX is our implementation of these concepts through modification of Linux. Through TWLinuX, a simple, low cost network of machines becomes a high performance discrete event simulation platform.	circuit restoration;distributed computing;linux;operating system;parallel computing;requirement;scalability;simulation	Subramania Sharma;Matthew J. Thazhuthaveetil	2001		10.1007/3-540-45307-5_23	parallel computing;real-time computing;computer science;discrete event simulation;simulation modeling;distributed computing;simulation language	HPC	-18.45888453066703	42.8298094226046	155191
8bca2abaa842fdbcef23065f9969e521594263b2	loop-level parallelism in numeric and symbolic programs	parallelisme;dynamic behavior numeric programs loop level parallelism symbolic programs speed improvement parallel computer sequential program parallelism analyzer parallel execution parallel performance;parallelism analyzer;tracage programme;numeric programs;symbolic programs;parallel performance;concurrent computing;performance evaluation;program tracing;programcompilers;programacion paralela;sequential program;performance;simulation;parallelizing compilers;parallel programming;size measurement;simulacion;parallel compiler;tracing;loop level parallelism;indexing terms;parallelism;computational modeling;programming theory;paralelismo;parallel processing concurrent computing program processors size measurement computational modeling parallel programming writing data structures associate members information analysis;data structures;tracage;parallel computer;writing;parallelperformance;theorie programmation;speed improvement;associate members;program compilers parallel programming performance evaluation;rendimiento;index termsnumeric programs;compilateur parallele;program compilers;parallel programs;parallel execution;information analysis;program processors;parallel processing;automatic parallelization;trazado;programmation parallele;dynamic behavior	This paper describes a new technique for estimating and understanding the speed improvement that can result from executing a program on a parallel computer. The technique requires no additional programming and minimal effort by a program’s author. The analysis begins by tracing a sequential program. A parallelism analyzer (pp) uses information from the trace to simulate parallel execution of the program. In addition to predicting parallel performance, pp measures many aspects of a program’s dynamic behavior. This paper presents measurements of six substantial programs. These results indicate that the three symbolic (nonnumeric) programs differ substantially from the numeric programs and, as a consequence, cannot be automatically parallelized with the same compilation techniques.	compiler;loop-level parallelism;parallel computing;simulation	James R. Larus	1993	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.238302	parallel processing;computer architecture;parallel computing;index term;tracing;concurrent computing;performance;computer science;distributed computing;data parallelism;data analysis;programming language;computational model;writing;instruction-level parallelism;implicit parallelism;task parallelism;automatic parallelization	PL	-14.76863638793669	41.519214126469116	155302
51ffa58d45cb9065c39ff9db140a50f488f8c394	verifying very large industrial circuits using 100 processes and beyond	estensibilidad;modelizacion;distributed system;verificacion modelo;haute performance;systeme reparti;computational grid;tranchage;espace etat;distributed computing;verification modele;program verification;analisis automatico;analisis programa;checkpointing;grid;modelisation;verificacion programa;slicing;automatic analysis;sistema repartido;model checking;state space method;distributed environment;methode espace etat;rejilla;scheduling;state space;tâche controle;distributed models;punto reanudacion;chapeado;analyse automatique;alto rendimiento;grille;calculo repartido;checking task;tarea control;point reprise;extensibilite;scalability;program analysis;information system;analyse programme;espacio estado;verification programme;modeling;high performance;calcul reparti;systeme information;ordonnancement;hybrid algorithm;reglamento;metodo espacio estado;sistema informacion	Recent advances in scheduling and networking open the way to the efficient exploitation of largescale distributed computing platforms, such as computational grids and huge clusters. Such infrastructure holds a great promise for the highly resource-demanding task of verifying and checking large models, given that model checkers would be designed with a high degree of scalability and flexibility in mind. In this paper we focus on the mechanisms required to execute a high-performance, distributed, symbolic model checker, on top of a large-scale distributed environment. We develop a hybrid algorithm for slicing the state space and dynamically distribute the work among the worker processes. We show that the new approach is faster, more effective, and thus much more scalable than previous slicing algorithms. We then present a checkpoint-restart module that involves very low overhead. This module can be used to combat failures which become probable with the size of the computing platform. However, checkpoint-restart is even more handy for the scheduling system to avoid reservation of large numbers of workers, thus making the distributed computation work-efficient. Finally, we discuss for the first time the effect of reorder on the distributed model checker. We show how the distributed system performs more efficient reordering than the sequential one. We implemented our contributions on a network of 200 processors, using a distributed scalable scheme that employs a high-performance industrial model checker from Intel. Our results in testing execution on real-life examples, show that the system was able to verify models much larger than was previously possible.	application checkpointing;central processing unit;computation;distributed computing;handy board;hybrid algorithm;model checking;overhead (computing);real life;scalability;scheduling (computing);state space;transaction processing system;verification and validation	Limor Fix;Orna Grumberg;Amnon Heyman;Tamir Heyman;Assaf Schuster	2005		10.1007/11562948_4	program analysis;model checking;real-time computing;scalability;simulation;systems modeling;hybrid algorithm;computer science;state space;distributed computing;programming language;grid;scheduling;information system;algorithm;distributed computing environment	HPC	-18.907630382908035	42.8059875018764	156384
b642412ffa567b7c129e363464f9cb0aced9e1fc	improving generic non-contiguous file access for mpi-io	tratamiento datos;traitement liste;distributed system;largeur bande;entrada salida;systeme reparti;communicating process;data management;tipo dato;data processing;tratamiento lista;traitement donnee;data type;buffer system;sistema amortiguador;proceso comunicante;input output;sistema repartido;envoi message;processus communicant;anchura banda;message passing;bandwidth;type donnee;systeme tampon;list processing;entree sortie	We present a fundamental improvement of the generic techniques for non-contiguous file access in MPI-IO. The improvement consists in the replacement of the conventional data management algorithms based on a representation of the non-contiguous fileview as a list of 〈offset , length〉 tuples. The improvement is termed listless i/o as it instead makes use of spaceand time-efficient datatype handling functionality that is completely free of lists for processing non-contiguous data in the file or in memory. Listless i/o has been implemented for both independent and collective file accesses and improves access performance by increasing the data throughput between user buffers and file buffers. Additionally, it reduces the memory footprint of the process performing non-contiguous I/O. In this paper we give results for a synthetic benchmark on a PC cluster using different file systems. We demonstrate improvements in I/O bandwidth that exceed a factor of 10.	algorithm;asynchronous i/o;benchmark (computing);central processing unit;computer cluster;decade (log scale);dhrystone;input/output;memory footprint;microsoft outlook for mac;posix;socket.io;throughput	Joachim Worringen;Jesper Larsson Träff;Hubert Ritzdorf	2003		10.1007/978-3-540-39924-7_44	fork;self-certifying file system;input/output;embedded system;message passing;torrent file;memory-mapped file;device file;data processing;data type;data management;computer science;class implementation file;bicarbonate buffering system;stub file;versioning file system;operating system;unix file types;journaling file system;database;open;programming language;file system fragmentation;bandwidth;file control block	OS	-17.417918152738398	44.278422077023194	157078
28a96ec30ba78fb5b61bbfb6831d59a7be3309fe	hympi - a mpi implementation for heterogeneous high performance systems	distributed system;sistema operativo;evaluation performance;communication primitive;haute performance;systeme reparti;performance evaluation;multiprocessor;collective communication;heterogeneous cluster;point to point;pervasive computing;evaluacion prestacion;communicating process;distributed computing;compatibilidad;proceso comunicante;grid;informatica difusa;abstraction communication;imaging system;sistema repartido;operating system;informatique diffuse;rejilla;envoi message;processus communicant;compatibility;message passing;alto rendimiento;grille;calculo repartido;systeme exploitation;compatibilite;runtime system;multiprocesador;high performance;calcul reparti;abstraccion comunicacion;multiprocesseur	This paper presents the HyMPI, a runtime system to integrate several MPI implementations, used to develop Heterogeneous High Performance Applications. This means that a single image system can be composed by mono and multiprocessor nodes running several Operating Systems and MPI implementations, as well as, heterogeneous clusters as nodes of the system. HyMPI supports blocking and non-blocking point-to-point communication and collective communication primitive in order to increase the range of High Performance Applications that can use it and to keep compatibility with MPI Standard.		Francisco Isidro Massetto;Augusto Mendes Gomes Junior;Liria Matsumoto Sato	2006		10.1007/11745693_32	parallel computing;message passing;real-time computing;multiprocessing;point-to-point;computer science;operating system;distributed computing;compatibility;grid;ubiquitous computing	HPC	-17.814816518850396	42.769858904120206	157107
1576188766cd0eb1ca222a3ca9181d3e03ce3180	performance tuning on parallel systems: all problems solved?	scientific application;evaluation performance;architecture systeme;performance evaluation;programacion paralela;evaluacion prestacion;parallel programming;aplicacion cientifica;accord frequence;tuning;parallel systems;analyse performance;performance analysis;arquitectura sistema;application scientifique;sintonizacion frecuencia;systeme parallele;parallel system;system architecture;parallel programs;performance tuning;sistema paralelo;problem solving;programmation parallele;analisis eficacia	Performance tuning of parallel programs, considering the current status and future developments in parallel programming paradigms and parallel system architectures, remains an important topic even if the single CPU performance is doubling every 18 months. Based on a brief summary of state of the art parallel programming techniques, new performance tuning aspects will be identified. The main part of the paper concentrates on how to deal with these aspects by means of new performance analysis and tuning concepts. First tool developments are presented where part of these concepts are already implemented. Finally, an existing scientific parallel application will be presented with respect to its performance tuning stages which were carried out at our center.	performance tuning	Holger Brunst;Wolfgang E. Nagel;Stephan Seidl	2000		10.1007/3-540-70734-4_33	parallel computing;simulation;computer science;artificial intelligence;musical tuning;algorithm;systems architecture	HPC	-16.935455927755985	41.47670766600993	157403
c9e7976e3be3eed6cf843f1148f17059220c2ba4	performance characterization of the 64-bit x86 architecture from compiler optimizations' perspective	modelizacion;virtual memory;gestion memoire;64 bit processor;compilateur;multimedia;servicio gprs;processeur 64 bit;competitividad;storage management;tipo dato;inlining;inclusion par reference;optimizacion compiladora;data type;compiler;procesador 32 bits;performance characterization;modelisation;gestion memoria;marcador;pointer;service gprs;inclusion por referencia;processeur 32 bits;compiler optimization;memoire virtuelle;competitiveness;pointeur;fortran;information system;general packet radio service;type donnee;competitivite;procesador 64 bit;modeling;optimisation compilateur;systeme information;memoria virtual;compilador;32 bit processor;sistema informacion	Intel Extended Memory 64 Technology (EM64T) and AMD 64-bit architecture (AMD64) are emerging 64-bit x86 architectures that are fully x86 compatible. Compared with the 32-bit x86 architecture, the 64-bit x86 architectures cater some new features to applications. For instance, applications can address 64 bits of virtual memory space, perform operations on 64-bit-wide operands, get access to 16 general-purpose registers (GPRs) and 16 extended multi-media (XMM) registers, and use a register-based argument passing convention. In this paper, we investigate the performance impacts of these new features from compiler optimizations’ standpoint. Our research compiler is based on the Intel Fortran/C++ production compiler, and our experiments are conducted on the SPEC2000 benchmark suite. Results show that for 64-bit-wide pointer and long data types, several SPEC2000 C benchmarks are slowed down by more than 20%, which is mainly due to the enlarged memory footprint. To evaluate the performance potential of 64-bit x86 architectures, we designed and implemented the LP32 code model such that the sizes of pointer and long are 32 bits. Our experiments demonstrate that on average the LP32 code model speeds up the SPEC2000 C benchmarks by 13.4%. For the register-based argument passing convention, our experiments show that the performance gain is less than 1% because of the aggressive function inlining optimization. Finally, we observe that using 16 GPRs and 16 XMM registers significantly outperforms the scenario when only 8 GPRs and 8 XMM registers are used. However, our results also show that using 12 GPRs and 12 XMM registers can achieve as competitive performance as employing 16 GPRs and 16 XMM registers.	32-bit;64-bit computing;benchmark (computing);c++;cpu cache;central processing unit;dspace;database;experiment;extended memory;fortran;general-purpose markup language;ia-32;imperative programming;inline expansion;instruction scheduling;linux;mathematical optimization;memory footprint;operand;optimizing compiler;pointer (computer programming);processor register;register allocation;register file;scheduling (computing);x86;x86-64	Jack Liu;Youfeng Wu	2006		10.1007/11688839_14	embedded system;compiler;parallel computing;real-time computing;pointer;systems modeling;32-bit;data type;computer science;virtual memory;optimizing compiler;programming language;information system;algorithm;general packet radio service	Arch	-15.709637395232148	42.466879372192174	157576
26986f861ddf29e397b0b72b3091f87296cba138	a message ordering problem in parallel programs	parallelisme;distributed system;virtual machine;programa paralelo;systeme reparti;machine virtuelle;parallelism;sistema repartido;paralelismo;emotion emotionality;envoi message;protocolo reglamento;pc cluster;message passing;emotion emotivite;completitud;emocion emotividad;experimental evaluation;completeness;protocole ordonnancement;message ordering;parallel programs;completude;maquina virtual;parallel program;programme parallele	We consider a certain class of parallel program segments in which the order of messages sent affects the completion time. We give characterization of these parallel program segments and propose a solution to minimize the completion time. With a sample parallel program, we experimentally evaluate the effect of the solution on a PC cluster.	computation;computer cluster;experiment;parallel computing;sparse matrix	Bora Uçar;Cevdet Aykanat	2004		10.1007/978-3-540-30218-6_23	parallel computing;message passing;real-time computing;completeness;computer science;virtual machine;operating system;database;distributed computing;programming language;algorithm;statistics	HPC	-17.043748719194436	43.71333765697817	157675
d09a427a43c66ec88388315e2a5d187af6e2bfe0	architecture of a reduction-based parallel inference machine: pim-r	structure methods;parallel machines;logic programs;distributed shared memory;structured data	This paper presents a highly parallel machine architecture for logic programs. We propose a Reduction-Based Parallel Inference Machine: PIM-R and describe the parallel execution mechanisms for PIM-R to run Prolog and Concurrent Prolog programs and sofware simulation results. PIM-R uses the structure-copy method. It also uses the only reducible goal copy method, a unique process-structuring method, and the reverse compaction method to decrease the amount, of copying and various copyingrelated operations and the number of packets passing through the network. PIM-R architecture features include the distributed shared memory for Concurrent Prolog, network nodes for efficient packet distribution, and the structure memory to store a part of structured data for reducing the copying overhead.	data compaction;distributed shared memory;logic programming;network packet;overhead (computing);parallel computing;prolog;simulation	Rikio Onai;Moritoshi Aso;Hajime Shimizu;Kanae Masuda;Akira Matsumoto	1985	New Generation Computing	10.1007/BF03037069	distributed shared memory;computer architecture;parallel computing;data model;computer science;programming language	HPC	-13.666591204271873	45.7765001048078	157763
99d16e95f744860fbc36e952f934bd3e98bb1da3	evaluating scheduling policies for fine-grain communication protocols on a cluster of smps	symmetric configuration;distributed memory;scheduling communication;distributed system;systeme reparti;protocole transmission;multiprocessor;gollete estrangulamiento;processor scheduling;reconfigurable architectures;high speed networks;memoria compartida;configuration symetrique;memoria virtualmente compartida;machine parallele;protocol processing policies;memoire virtuellement partagee;general purpose processor;fine grain software protocols;protocolo transmision;goulot etranglement;configuracion simetrica;sistema repartido;parallel systems;scheduling;processeur symetrique;network of workstation;parallel computer;clusters of smps;communication protocol;parallel machines;ordonnancement processeur;multiprocesador;memoire repartie;distributed shared memory;bottleneck;architecture reconfigurable;effective bandwidth;ordonnancement;reglamento;multiprocesseur;transmission protocol	Distributed-memory parallel computers and networks of workstations (NOWs) both rely on efficient communication over increasingly high-speed networks. Software communication protocols are often the performance bottleneck. Several current and proposed parallel systems address this problem by dedicating one general-purpose processor in a symmetric multiprocessor (SMP) node specifically for protocol processing. This protocol processing convention reduces communication latency and increases effective bandwidth, but also reduces the peak performance since the dedicated processor no longer performs computation. In this paper, we study a parallel machine with SMP nodes and compare two protocol processing policies: the Fixed policy, which uses a dedicated protocol processor; and the Floating policy, where all processors perform both computation and protocol processing. The results from synthetic microbenchmarks and five macrobenchmarks show that: (i) a dedicated protocol processor benefits light-weight protocols much more than heavy-weight protocols, (ii) a dedicated protocol processor is generally advantageous when there are four or more processors per node, (iii) multiprocessor node performance is not as sensitive to interrupt overhead as uniprocessor node because a message arrival is likely to find an idle processor on a multiprocessor node, thereby eliminating interrupts, (iv) the system with the lowest cost-performance will include a dedicated protocol processor when interrupt overheads are much higher than protocol weight—as in light-weight protocols. © 2005 Elsevier Inc. All rights reserved.	cache coherence;central processing unit;communications protocol;computation;computer;distributed memory;general-purpose markup language;intel paragon;interrupt;overhead (computing);parallel computing;request–response;scheduling (computing);shared memory;simulation;symmetric multiprocessing;synthetic intelligence;uniprocessor system;user space;workstation	Babak Falsafi;David A. Wood	2005	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2004.11.011	distributed shared memory;communications protocol;universal composability;parallel computing;real-time computing;multiprocessing;distributed memory;msi protocol;computer science;operating system;distributed computing;scheduling;computer network	Arch	-15.729160635191233	43.94197926946928	158114
69e46f94b0f4e672ae09c0be08cc8cbb333a5c55	vizzscheduler - a framework for the visualization of scheduling algorithms	tratamiento paralelo;programa paralelo;traitement parallele;gestion labor;time measurement;chronometrie;testing and debugging;parallel computation;np hard problem;scheduling algorithm;calculo paralelo;gestion tâche;data dependence;scheduling;cronometria;parallel computer;communication cost;parallel machines;ordonamiento;experimental evaluation;task graphs;parallel architecture;task scheduling;parallel programs;calcul parallele;parallel program;parallel processing;ordonnancement;cost model;programme parallele	The computation of efficient schedules of task graphs for parallel machines is a major issue in parallel computing. Such algorithms are often hard to understand and hard to evaluate. We present a framework for the visualization of scheduling algorithms. Using the LogP cost model for parallel machines, we simulate the effects of scheduling algorithms for specific target machines and task graphs before performing time and resource consumptive measurements in the real world.	algorithm;analysis of algorithms;computation;debugging;experiment;logp machine;parallel computing;scheduling (computing);simulation	Welf Löwe;Alex Liebrich	2001		10.1007/3-540-44681-8_10	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel processing;parallel computing;real-time computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;operating system;two-level scheduling;distributed computing;round-robin scheduling;scheduling	HPC	-14.40639694660219	44.89490819572461	158198
60e9d7718c6687690c4e3efb6041581e21991e41	tiles: a new language mechanism for heterogeneous parallelism	parallel programming	This paper studies the essence of heterogeneity from the perspective of language mechanism design. The proposed mechanism, called tiles, is a program construct that bridges two relative levels of computation: an outer level of source data in larger, slower or more distributed memory and an inner level of data blocks in smaller, faster or more localized memory.	computation;distributed memory;openacc;openmp;parallel algorithm;parallel computing;partitioned global address space;programmer;programming style;source data;turing completeness	Yifeng Chen;Xiang Cui;Hong Mei	2015		10.1145/2688500.2688555	parallel computing;computer science;theoretical computer science;programming language;algorithm	HPC	-13.003234278612455	39.37881922428051	158226
d8ae1d599cededbaa7953de2bc7e6ea267a3fa36	revisiting communication code generation algorithms for message-passing systems	distributed memory;formal specification;parallelizing compiler;message passing system;parallelizing compilers;code generation;parallel computation;parallel computer;message passing;mapping	In this paper, we investigate algorithms for generating communication code to run on distributedmemory systems. We modify algorithms from previously published work and prove that the algorithms produce correct code. We then extend these algorithms to incorporate the mapping of virtual processors to physical processors and prove the correctness of this extension. This technique can reduce the number of interprocessor messages. In the examples that we show, the total number of messages was reduced from O(N 2) to O(P2), where N is the input size and P is the number of physical processors. The reason that it is important to revisit communication code generation and to introduce a formal speci cation of the incorporation of mapping in the communication code generation is so that we can make use of the many scheduling heuristics proposed in the literature. We need a generalized mapping function so that we can apply di erent mapping and scheduling heuristics proposed in the literature for each input program, therefore improving the average performance.	algorithm;best, worst and average case;central processing unit;code generation (compiler);correctness (computer science);heuristic (computer science);information;message passing;scheduling (computing)	Clayton S. Ferner	2006	IJPEDS	10.1080/17445760600568089	parallel computing;message passing;distributed memory;computer science;theoretical computer science;operating system;formal specification;distributed computing;programming language;algorithm;code generation	PL	-14.897337225626542	45.83295284492631	158798
455f38cc18a273b2f33e4c07788a1fad15326a0f	evaluating the impact of coherence protocols on parallel logic programming systems	col	In this paper we use execution-driven simulation of a scalable multiprocessor to evaluate the performance of the Andorra-I parallel logic programming system under invalidate and update-based protocols. We study a well-known invalidate protocol and two different update-based protocols. Our results show that for our sample logic programs the update-based protocols outperform their invalidatebased counterparts. The detailed analysis of these results shows that update-based protocols outperform invalidatebased protocols regardless of the type of parallelism exhibited by the benchmarks. The reasons for this behaviour are explained in detail. We conclude that parallel logic programming systems can benefit from update-based protocols and that multiprocessors designed for running these systems efficiently should adopt some form of update or hybrid protocol.	admissible numbering;benchmark (computing);logic programming;multiprocessing;parallel computing;scalability;simulation	Vítor Santos Costa;Ricardo Bianchini;Inês de Castro Dutra	1997			inductive programming;parallel computing;logic optimization;distributed computing;horn clause;functional logic programming;computer science;programming paradigm;logic programming;concurrent constraint logic programming;prolog	HPC	-14.727557205974561	46.31184928391895	158803
2962591fcfc1b177918f7af245ac649e1bd943e1	a dynamic task distribution and engine allocation strategy for distributed execution of logic programs	gestion labor;programacion paralela;program transformation;parallel programming;ejecucion programa;logical programming;transformation programme;program execution;distributed scheduling;performance programme;transformacion programa;gestion tâche;programmation logique;execution programme;communication cost;eficacia programa;program performance;logic programs;task scheduling;programacion logica;programmation parallele	Distributed execution of logic programs on heterogeneous pro cessors requires e cient task distribution and engine synchronization to exploit the potential for performance This paper presents a task driven scheduling technique to distribute tasks to engines e ectively It consists of a dynamic hierarchy of distributed scheduling components able to adapt to program characteristics and the platform con guration and to control the considerable communication costs while exploiting good degrees of parallelism It also incorporates an abort failure mechanism to reduce speculative work and keep engines as busy as possible Several experimen tal results illustrate the performance of the model	failure cause;logic programming;naruto shippuden: clash of ninja revolution 3;parallel computing;scheduling (computing);speculative execution	George Xirogiannis;Hamish Taylor	1998		10.1007/BFb0037156	parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-16.183598722811464	41.788573994432895	159418
5fbe2619c4d45ed8538db65e36306ad731962627	contention-free communication scheduling for irregular data redistribution in parallelizing compilers	distributed memory;algoritmo paralelo;haute performance;distribution donnee;communication scheduling;parallel algorithm;conflict points;gen_block;memoria compartida;parallelizing compilers;distributed computing;paralelisacion;satisfiability;data distribution;algorithme parallele;compilateur parallelisation;scheduling algorithm;irregular data redistribution;optimal scheduling;scheduling;high performance fortran;parallelisation;parallelization;alto rendimiento;calculo repartido;fortran;memoire repartie;algoritmo optimo;algorithme optimal;optimal algorithm;high performance;distribucion dato;calcul reparti;ordonnancement;reglamento	The data redistribution problems on multi-computers had been extensively studied. Irregular data redistribution has been paid attention recently since it can distribute different size of data segment of each processor to processors according to their own computation capability. High Performance Fortran Version 2 (HPF-2) provides GEN_BLOCK data distribution method for generating irregular data distribution. In this paper, we develop an efficient scheduling algorithm, Smallest Conflict Points Algorithm (SCPA), to schedule HPF2 irregular array redistribution. SCPA is a near optimal scheduling algorithm, which satisfies the minimal number of steps and minimal total messages size of steps for irregular data redistribution.	algorithm;automatic parallelization;central processing unit;computation;computer;data segment;high performance fortran;scheduling (computing);semiconductor chip protection act of 1984;verification and validation	Kun-Ming Yu;Chi-Hsiu Chen;Ching-Hsien Hsu;Chang Yu;Chiu Liang	2005		10.1007/11573937_13	parallel computing;computer science;theoretical computer science;operating system;distributed computing;scheduling	HPC	-15.185278027734185	42.90245766560763	159590
f8eb856e8a2bff5d6991db7335b30560601d558a	performance evaluation of suprenum for the linpack benchmark (short communication)	memoire vectorielle;eficacia sistema;memoire;evaluation performance;optimisation;architecture systeme;performance evaluation;optimizacion;benchmark;implementation;evaluacion prestacion;performance systeme;system performance;benchmark computers;ejecucion;memoria;processeur suprenum;arquitectura sistema;optimization;systeme parallele;parallel system;vector processor;system architecture;sistema paralelo;memory;processeur vectoriel	Abstract   We report on some performance measurements obtained on the SUPRENUM parallel supercomputer for the LINPACK benchmark. We shorthly describe some features of our implementation of the LINPACK routines suitable for the SUPRENUM architecture. Performance measurements are presented for various numbers of processors and different matrix sizes. For the 1000 by 1000 problem we obtain over 100 Mflops on 1 cluster which contains 16 processors.	benchmark (computing);lunpack;performance evaluation	Stephan Kohlhoff;Jörg Krone	1992	Parallel Computing	10.1016/0167-8191(92)90082-I	embedded system;vector processor;parallel computing;simulation;benchmark;computer science;computer performance;memory;implementation;systems architecture	HPC	-15.88976060788065	42.96827918102781	159650
332c6133e9720d1908cbc3ee3912e874a5375cb5	a practical data flow computer	computers;general and miscellaneous mathematics computing and information science;concurrent computing;prototypes;resource management;matching function;flow graphs;data flow computing flow graphs concurrent computing computer architecture parallel processing data structures environmental management resource management prototypes vector processors;computer architecture;dynamic data;data structures;array processors;data flow processing;programming 990200 mathematics computers;data flow computing;data flow;environmental management;architecture;vector processors;parallel processing	One of the more natural ways of viewing a parallel computation is to take a close look at a data flow graph. Now, although data flow techniques have been used, in various forms, to assist the mapping of programs onto multiprocessors, vector processors, and the like, it seems likely that a machine architecture which directly executes a data-driven model will eventually lead to a more efficient parallel computer structure. But in order to devise such a structure, one first has to examine the data flow model itself. In a simple data flow execution model, data tokens are assumed to move along the arcs of the graph to the computational nodes, the nodes executing when all of their input data are present. ' However, during token movement only one token is allowed to exist on an arc at any given time, otherwise it would not be clear which sets of tokens belong together at a node input. A machine architecture executing this type of model was described by Dennis and Misunas2 in 1975. Their work led other researchers to investigate ways in which such a machine could be programmed, and this is when several problems emerged. For example, the work of Arvind and Gostelow on the U-interpreter3 suggests that maximum parallelism can only be extracted from a program if an arc is allowed to carry more than a single token-a process achieved by carrying a label with each token that identifies the context of that particular tokep. This is usually referred to as a dynamic tagged data flow model. Another problem is that of the actual data structures themselves. Ackerman4 shows that a data structure must be treated as a single object rather than a collection of elements capable of being manipulated individually if the clean semantics of data flow are to be maintained. This suggests that an entire structure has to be copied whenever a change is made in the structure-even if only one element is to be altered. We feel that no serious machine architect would suggest that any parallel machine, however powerful, can afford such inefficiencies. Apart from the overhead involved in structure copying, the treatment of a structure as a single object can seriously limit runtime parallelism. A section of a computation requiring a structure that is still in the process of formation cannot start to run until the structure is complete, even though many of its …	arvind (computer scientist);central processing unit;computation;data flow diagram;data structure;dataflow architecture;overhead (computing);parallel computing;vector processor	Ian Watson;John R. Gurd	1982	Computer	10.1109/MC.1982.1653941	parallel processing;parallel computing;dynamic data;concurrent computing;computer science;resource management;theoretical computer science;architecture;operating system;distributed computing;programming language	PL	-13.707632131992748	40.46694455284942	159737
a457be46f868ccfeb113716aa43c81f90a94a017	heterogeneous mpi application interoperation and process management under pvmpi	virtual machine;data transmission;evaluation performance;general and miscellaneous mathematics computing and information science;performance evaluation;programacion paralela;p codes;evaluacion prestacion;sistema informatico;performance;parallel programming;computer system;machine virtuelle;data transmission systems;process management;parallel computation;computer networks;computer network;university of tennessee;calculo paralelo;software package;systeme informatique;maquina virtual;management;calcul parallele;programmation parallele	Presently, di erent vendors' MPI implementations cannot interoperate directly with each other. As a result, performance of distributed computing across di erent vendors' machines requires use of a single MPI implementation, such as MPICH. This solution may be sub-optimal since it cannot utilize the vendors' own optimized MPI implementations. PVMPI, a software package currently under development at the University of Tennessee, provides the needed interoperability between di erent vendors' optimized MPI implementations. As the name suggests PVMPI is a powerful combination of the proven and widely ported Parallel Virtual Machine (PVM) system and MPI. PVMPI is transparent to MPI applications thus allowing intercommunication via all the MPI point-to-point calls. Additionally, PVMPI allows exible control over MPI applications by providing access to all the process control and resource control functions available in the PVM virtual machine.	control function (econometrics);distributed computing;interoperability;interoperation;mpich;message passing interface;parallel virtual machine;point-to-point protocol	Graham E. Fagg;Jack J. Dongarra;Al Geist	1997		10.1007/3-540-63697-8_74	parallel computing;performance;computer science;virtual machine;operating system;database;distributed computing;programming language;algorithm;data transmission	HPC	-17.95044808119335	42.88479905780851	160860
8aa3afa0a410cfd3926a32e18d5e2c47d23237e4	debugging message passing programs using invisible message tags	tratamiento paralelo;outil logiciel;evaluation performance;interfase usuario;interfaz grafica;software tool;performance evaluation;traitement parallele;graphical interface;user interface;evaluacion prestacion;programa puesta a punto;herramienta controlada por logicial;interface utilisateur;interface graphique;programme debogage;parallel processing;debugging program	Source level debuggers for parallel PVM or MPI programs currently ooer good support for debugging multiple processes, however, they still lack adequate mechanisms for debugging message passing errors. In this paper, we present a new concept called message breakpoints, which allows to follow the information ow between processes. We also show how these breakpoints can be implemented very eeciently by using invisible message tags and will demonstrate other applications of this technique in the context of distributed event detection.	breakpoint;debugger;debugging;message passing;overhead (computing);parallel virtual machine;prototype;tag (metadata)	Roland Wismüller	1997		10.1007/3-540-63697-8_97	embedded system;parallel processing;message passing;real-time computing;computer science;operating system;graphical user interface;message broker;user interface	Visualization	-18.530557221382512	41.87569314938321	160996
35454bce7e1557feeeb82330f54dc04fc5f3ff0a	integrating path and timing analysis using instruction-level simulation techniques	systeme temps reel;execution time;processor scheduling;sistema informatico;ejecucion programa;computer system;program execution;computer architecture;worst case execution time;architecture ordinateur;simulation technique;execution programme;timing analysis;temps execution;real time system;systeme informatique;arquitectura ordenador;sistema tiempo real;memory hierarchy;ordonnancement processeur;tiempo ejecucion	Previously published methods for estimation of the worstcase execution time on contemporary processors with complex pipelines and multi-level memory hierarchies result in overestimations owing to insu cient path and/or timing analysis. This paper presents a new method that integrates path and timing analysis to address these limitations. First, it is based on instruction-level architecture simulation techniques and thus has a potential to perform arbitrarily detailed timing analysis of hardware platforms. Second, by extending the simulation technique with the capability of handling unknown input data values, it is possible to exclude infeasible (or false) program paths in many cases, and also calculate path information, such as bounds on number of loop iterations, without the need for annotating the programs. Finally, in order to keep the number of program paths to be analyzed at a manageable level, we have extended the simulator with a path-merging strategy. This paper presents the method and particularly evaluates its capability to exclude infeasible paths based on seven benchmark programs.	benchmark (computing);central processing unit;iteration;memory hierarchy;path (graph theory);path analysis (statistics);pipeline (computing);run time (program lifecycle phase);simulation;static timing analysis;worst-case execution time	Thomas Lundqvist;Per Stenström	1998		10.1007/BFb0057776	parallel computing;real-time computing;simulation;real-time operating system;computer science;operating system;distributed computing;static timing analysis;algorithm;worst-case execution time	EDA	-14.601047107882255	42.84755855765232	161070
8837e034d4e91329127a12515726a5f9332323cf	availability in parallel systems: automatic process restart	tolerancia falta;modelizacion;availability;disponibilidad;sistema informatico;computer system;service;algorithme;modelisation;algorithm;parallel systems;fault tolerance;software component;systeme informatique;systeme parallele;parallel system;modeling;disponibilite;tolerance faute;sistema paralelo;algoritmo;servicio	Parallel and clustered architectures are increasingly being used as a foundation for high-capacity servers. At the same time, the availability expectations are also rising rapidly, since the effects of down time become more apparent and have higher economic consequences for larger systems. The use of parallel structures generally implies more hardware and software components. The presence of more and larger components increases the chances that an individual component will fail, and that failure has the potential to hurt the overall availability of the system. This paper discusses the use of restart techniques as an important strategy in providing increased availability in a parallel structure. The paper covers a set of functions that have been developed for the S/390 Parallel Sysplex.		Nicholas S. Bowen;James Antognini;Richard D. Regan;Nicholas C. Matsakis	1997	IBM Systems Journal	10.1147/sj.362.0284	embedded system;availability;fault tolerance;real-time computing;service;systems modeling;computer science;component-based software engineering;operating system;algorithm	HPC	-18.75066116609672	43.88019734739977	161680
ae5d59c4a11f31f987c7c7a19a854c963e5acd58	a framework to develop symbolic performance models of parallel applications	computers;analytical models;incremental model construction;floating point operation cost;memory requirements;general and miscellaneous mathematics computing and information science;performance evaluation;mpi message volume parallel applications symbolic performance models performance modeling workload modeling modeling assertions incremental model construction model validation error bounding floating point operation cost memory requirements;high performance computing;procurement;performance;simulation;workload modeling;space exploration;mpi message volume;design evaluation;model validation;tuning;modeling assertions;character generation;performance analysis;benchmarks;performance model;error bounding;mathematical model;computer codes;exploration;symbolic performance models;design;validation;predictive models;evaluation;optimization;floating point;design space exploration;performance evaluation parallel processing;error bound;communication system control;high end computing;performance modeling;programming;construction;parallel applications;parallel processing;analytical model;mathematical model character generation communication system control high performance computing predictive models procurement analytical models costs space exploration performance analysis	Performance and workload modeling has numerous uses at every stage of the high-end computing lifecycle: design, integration, procurement, installation and tuning. Despite the tremendous usefulness of performance models, their construction remains largely a manual, complex, and time-consuming exercise. We propose a new approach to the model construction, called modeling assertions (MA), which borrows advantages from both the empirical and analytical modeling techniques. This strategy has many advantages over traditional methods: incremental construction of realistic performance models, straightforward model validation against empirical data, and intuitive error bounding on individual model terms. We demonstrate this new technique on the NAS parallel CG and SP benchmarks by constructing high fidelity models for the floating-point operation cost, memory requirements, and MPI message volume. These models are driven by a small number of key input parameters thereby allowing efficient design space exploration of future problem sizes and architectures	cg (programming language);computation;design space exploration;flops;free license;network-attached storage;performance prediction;procurement;requirement;ut-vpn	Sadaf R. Alam;Jeffrey S. Vetter	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639625	parallel processing;programming;design;parallel computing;real-time computing;simulation;construction;exploration;procurement;performance;computer science;floating point;theoretical computer science;evaluation;space exploration;operating system;mathematical model;distributed computing;programming language;algorithm	HPC	-13.878303728629259	41.84256499057637	161819
11983eac626ec712e30bd79d49234f78f87d9ce8	mpi collective algorithm selection and quadtree encoding	tratamiento datos;distributed system;virtual machine;raisonnement base sur cas;razonamiento fundado sobre caso;systeme reparti;quad tree;quad arbol;decision function;generacion automatica;fenomeno colectivo;communicating process;phenomene collectif;distributed computing;data processing;traitement donnee;fonction decision;machine virtuelle;automatic generation;proceso comunicante;generation fonction;generation automatique;sistema repartido;message passing interface;funcion decision;collective process;envoi message;processus communicant;message passing;quad arbre;calculo repartido;generacion funcion;mpi;case based reasoning;function generation;algoritmo optimo;experimental measurement;algorithme optimal;optimal algorithm;maquina virtual;calcul reparti	In this paper, we focus on MPI collective algorithm selection process and explore the applicability of the quadtree encoding method to this problem. During the algorithm selection process, a particular MPI collective algorithm is selected based on the collective operation parameters. We construct quadtrees with different properties from the measured algorithm performance data and analyze the quality and performance of decision functions generated from these trees. The experimental data indicates that in some cases, the decision function based on quadtree structure with a mean depth of 3 can incur as little as a 5% performance penalty on average. The exact, experimentally measured, decision function for all tested collectives could be fully represented using quadtrees with a maximum of 6 levels. These results indicate that quadtrees may be a feasible choice for both processing of the performance data and automatic decision function generation.	algorithm selection;collective intelligence;collective operation;experiment;quadtree	Jelena Pjesivac-Grbovic;Graham E. Fagg;Thara Angskun;George Bosilca;Jack J. Dongarra	2006		10.1007/11846802_14	simulation;data processing;computer science;artificial intelligence;message passing interface;distributed computing;programming language;algorithm	HPC	-16.70797688573881	42.93264329252468	162063
1a222cf236dabe8bb93e409a0b9fc2603c055983	analysis of the spatial and temporal locality in data accesses	optimisation;replacement;haute performance;remplacement;storage access;cache optimization;localite;grouping;algorithm analysis;optimizacion;gestion labor;multiprocessor;analisis espacial;optimization technique;secuencia repetida;base donnee temporelle;prefetching;chip multiprocessor;sequence repetee;prechargement donnee;distributed computing;program transformation;locality;cache memory;information access;transformation programme;spatial database;user assistance;antememoria;memory access;transformation donnee;antememoire;transformacion programa;transformacion dato;gestion tâche;assistance utilisateur;estructura datos;comportement utilisateur;asistencia usuario;acces memoire;data transformation;data access;acces information;alto rendimiento;acceso memoria;calculo repartido;spatial data structures;reemplazo;acceso informacion;optimization;analyse algorithme;structure donnee;temporal databases;base dato especial;user behavior;agrupamiento;spatial locality;base de donnees spatiale;spatial analysis;task scheduling;multiprocesador;precargamento dato;high performance;repeated sequence;data structure;calcul reparti;analyse spatiale;analisis algoritmo;comportamiento usuario;groupage;multiprocesseur;structure donnee spatiale	Cache optimization becomes increasingly important for achieving high computing performance, especially on current and future chip-multiprocessor (CMP) systems, which usually show a rather higher cache miss ratio than uni-processors. For such optimization, information about the access locality is needed in order to help the user in the tasks of data allocation, data transformation, and code transformation which are often used to enhance the utilization of cached data towards a better cache hit rate.#R##N##R##N#In this paper we demonstrate an analysis tool capable of detecting the spatial and temporal relationship between memory accesses and providing information, such as access pattern and access stride, which is required for applying some optimization techniques like address grouping, software prefetching, and code transformation. Based on the memory access trace generated by a code instrumentor, the analysis tool uses appropriate algorithms to detect repeated address sequences and the constant distance between accesses to the different elements of a data structure. This allows the users to pack data with spatial locality in the same cache block so that needed data can be loaded into the cache at the same time. In addition, the analysis tool computes the push back distance which shows how a cache miss can be avoided by reusing the data before replacement. This helps to reduce cache misses increasing therefore the temporal reusability of the working set.	locality of reference	Jie Tao;Siegfried Schloissnig;Wolfgang Karl	2006		10.1007/11758525_68	locality of reference;bus sniffing;data access;least frequently used;cache-oblivious algorithm;parallel computing;real-time computing;multiprocessing;cache coloring;repeated sequence;page cache;cpu cache;data structure;cache;computer science;write-once;cache invalidation;database;spatial analysis;temporal database;smart cache;data transformation;cache algorithms;cache pollution;spatial database;statistics;non-uniform memory access	PL	-17.356610634911355	45.24276494116602	162224
ee4ab9c8ccedc449e3379848e05aa8bf151fe662	d3dpr: a direct3d-based large-scale display parallel rendering system architecture for clusters	parallelisme;distributed system;parallel rendering;arquitectura red;representation graphique;reseau communication;accelerateur;haute performance;distribution donnee;architecture systeme;systeme reparti;high resolution;distribution network;reseau distribution;accelerator;architecture reseau;data distribution;interconnection network;red distribucion;type logique;synchronisation;computer architecture;large scale;haute resolution;parallelism;sistema repartido;paralelismo;architecture ordinateur;parallel systems;synchronization;alta resolucion;alto rendimiento;grafo curva;arquitectura sistema;network architecture;arquitectura ordenador;sincronizacion;system architecture;tipo logico;acelerador;red de comunicacion;high performance;distribucion dato;communication network;logical type;graphics	The current trend in hardware for parallel rendering is to use clusters instead of high-end super computer. We describe a novel parallel rendering system that allows application to render to a large-scale display. Our system, called D3DPR, uses a cluster of PCs with high-performance graphics accelerators to drive an array of projectors. D3DPR consists of two types of logical nodes, Geometry Distributing Node and Geometry Rendering Node. It allows existing Direct3D9 application to run on our parallel system without any modification. The advantage of high-resolution and high-performance can be obtained in our system, especially when the triangle number of the application becomes very large. Moreover, the details of interconnecting network architecture, data distribution, communication and synchronization, etc. are hidden from the users.	direct3d;parallel rendering	Zhen Liu;Jiaoying Shi;Haoyu Peng;Hua Xiong	2005		10.1007/11572961_44	embedded system;synchronization;tiled rendering;rendering;computer science;parallel rendering;real-time rendering;software rendering;systems architecture;computer graphics (images)	HPC	-17.4755754850409	42.6443970392453	163796
66911a1792915def802cabdf24a6d216fe1c4444	distributed versus parallel computing	process design;parallel computer;parallel machines	The elegant but simple von Neumann single processing design of computers has been challenged by new applications of databases, computer vision and speech where a multi-processing system seems more suited to such tasks. We look at three ways in which parallel machines may be used: for general purpose computing, for algorithms which are not well suited to von Neumann machines and for exploring forms of computation which cannot reasonably be dealt with on von Neumann machines.	algorithm;computation;computer vision;database;multiprocessing;parallel computing;von neumann architecture	Allan Ramsay	1986	Artificial Intelligence Review	10.1007/BF01988525	process design;computer science;theoretical computer science;distributed computing	HPC	-12.461969618757825	39.77631326416189	164931
5ab7f5c4be22729ab34e2e6c7e90fefa8ebcb38d	the data diffusion space for parallel computing in clusters	modelizacion;distributed memory;distributed system;sistema operativo;algoritmo paralelo;cluster computing;entrada salida;haute performance;systeme reparti;parallel algorithm;racimo calculadura;memoria compartida;communicating process;distributed computing;spmd;algorithme parallele;proceso comunicante;input output;modelisation;grappe calculateur;sistema repartido;message passing interface;operating system;envoi message;processus communicant;pc cluster;parallel computer;message passing;alto rendimiento;calculo repartido;systeme exploitation;mpi;memoire repartie;modeling;high performance;calcul reparti;parallel applications;single program multiple data;entree sortie	The data diffusion space (DDS) is an all-software shared address space for parallel computing on distributed memory platforms. It is an extra address space to that of each process running a parallel application under the SPMD (Single Program Multiple Data) model. The size of DDS can be up to 2 bytes, either on 32or on 64-bit architectures. Data laid on DDS diffuses, or migrates and replicates, in the memory of each processor using the data. This data is used through an interface similar to that used to access data in files. We have implemented DDS for PC clusters with Linux. However, being all-software, DDS should require little change to make it immediately usable in other distributed memory platforms and operating systems. We present experimental results on the performance of two applications both under DDS and under MPI (Message Passing Interface). DDS tends to perform better in larger processor counts, and is simpler to use than MPI for both in-core and out-of-core computation.	64-bit computing;address space;byte;central processing unit;clustered file system;compiler;computation;computer cluster;disk space;distributed memory;linux;message passing interface;operating system;out-of-core algorithm;parallel computing;programming model;quine (computing);spmd	Jorge Buenabad Chávez;Santiago Domínguez-Domínguez	2005		10.1007/11549468_10	parallel computing;computer science;message passing interface;operating system;database;distributed computing	HPC	-16.938942745433334	43.10140804424205	165032
b581ce145abdcb72ff6514aea99840914b092d31	experimental evaluation of automatic array alignment in parallelized matlab	tratamiento paralelo;alignement;array alignment;evaluation performance;compilateur;performance evaluation;traitement parallele;optimization technique;programming environment;etude experimentale;implementation;evaluacion prestacion;layout problem;parallelizing compilers;probleme agencement;paralelisacion;optimizacion compiladora;compiler;medio ambiente programacion;ejecucion;parallel architectures;architecture parallele;parallelisation;compiler optimization;alineamiento;parallelization;problema disposicion;optimization;experimental evaluation;parallel architecture;estudio experimental;optimisation compilateur;alignment;matlab;parallel processing;compilador;environnement programmation	Efficiency of matrix applications in parallel processing environments relies on two factors: speed of primitive matrix operations and layout of distributed arrays. Good array layout improves locality and reduces communication overheads. Array alignment is especially important, being a minimum requirement for locality. Existing matrix programming environments either require manual alignment, which compromises the simplicity of use, or resort to some default settings, which sacrifices performance. Techniques for automatic alignment have been proposed, but their use is not widespread, and their practical significance has not been sufficiently examined. We present an experimental evaluation of an alignment optimization technique implemented in a parallelizing compiler for Matlab scripts. We have measured the performance of five applications on two parallel architectures. The significance of alignment optimization is demonstrated by 430 average improvement in performance and doubling the speed in some realistic cases. This optimization technique enabled ordinary Matlab scripts to run at a similar speed to hand-coded PBLAS implementations. 2001 Academic Press	algorithm;artificial neural network;automatic differentiation;automatic parallelization;backpropagation;benchmark (computing);biconjugate gradient method;compiler;connection machine;convex conjugate;dec alpha;general-purpose macro processor;gradient descent;heuristic;interaction;locality of reference;matlab;manifold alignment;mathematical optimization;multilayer perceptron;pblas;parallel computing;period-doubling bifurcation;power iteration;principal component analysis;programmer;run time (program lifecycle phase);software propagation;speedup;supercomputer;workstation	Igor Z. Milosavljevic;Marwan A. Jabri	2001	J. Parallel Distrib. Comput.	10.1006/jpdc.2000.1665	parallel processing;computer architecture;compiler;parallel computing;computer science;theoretical computer science;operating system;optimizing compiler;distributed computing;programming language;implementation;algorithm	HPC	-15.98110174445232	42.05662844123698	165083
843b81d6f66ae71e384eae0592bc5101eb8ce4fd	transparent state management for optimistic synchronization in the high level architecture	distributed system;recouvrement arriere;evaluation performance;hla;systeme reparti;personal communication networks;performance evaluation;transparent checkpointing recovery;evaluacion prestacion;federated simulation systems;logicial personalizado;distributed computing;recubrimiento atras;telecommunication network;time management;checkpointing;intergiciel;synchronisation;software architecture;sistema repartido;reseau communication personnel;synchronization;red telecomunicacion;reseau telecommunication;punto reanudacion;rollback recovery;calculo repartido;middleware;point reprise;sincronizacion;gestion temps;high level architecture;calcul reparti;architecture logiciel	In this article, the authors present the design and implementation of a software architecture—namely, MAgic State Manager (MASM)—to be employed within a runtime infrastructure (RTI) in support of High Level Architecture (HLA) federations. MASM allows performing checkpointing/recovery of the federate state in a way completely transparent to the federate itself, thus providing the possibility of demanding to the RTI any task related to state management in optimistic synchronization. Different from existing proposals, through this approach, the federate programmer is required neither to supply modules for state management within the federate code nor to explicitly interface the federate code with existing, third-party checkpointing/recovery libraries. Hence, the federate programmer is completely relieved from the burden of facing state management issues. One major application of this proposal is the possibility to employ optimistic synchronization, even in case of federates originally designed for the conservative approach. This can provide a way of improving the simulation system performance in specific scenarios (e.g., in case of poor or zero lookahead within the federation). The authors elaborate on this issue by discussing on how to integrate MASM within the RTI to achieve such a synchronization objective. Some experimental results demonstrating limited runtime overhead introduced by MASM are also reported for two case studies—namely, an interconnection network simulation and a personal communication system simulation.	application checkpointing;federated identity;federation (information technology);interconnection;library (computing);microsoft macro assembler;overhead (computing);parsing;programmer;run-time infrastructure (simulation);simulation;software architecture;state management;synchronization (computer science)	Andrea Santoro;Francesco Quaglia	2006	Simulation	10.1177/0037549706065350	embedded system;synchronization;real-time computing;simulation;computer science;artificial intelligence;operating system	OS	-18.94181499586865	42.822129883879626	165117
27d51ede2fbf80bd2ac3e7162bc5d13937a2ef03	flexible load balancing software for parallel applications in a time-sharing environment	distributed memory;time sharing;network of workstation;load balance;parallel applications	Networks of workstations become more and more appropriate for parallel applications, as modern network technology enables high quality communication between powerful workstations. In this perspective, load balancing software must be extremely exible as the set of available nodes for a particular distributed memory application may change at run time. XENOOPS is an advanced environment for parallel software. The XENOOPS load balancing framework meets the requirements of the regular users of a time sharing distributed system, and of the HPCN users who prefer to exploit multiple processors as if these processors have been reserved for one particular application. On the one hand, a parallel application can dynamically obtain and eeectively exploit workstations as these turn idle. On the other hand, workstation owners transparently claim their device when accessing it on the console. In both cases, the parallel application transparently reorganises itself while maintaining a balanced work load distribution. Consequently, dynamic load balancing also becomes important for regular applications, which would not require dynamic reallocation in a space-sharing system. This paper treats the elements of XENOOPS that realise generic load balancing support. A key element is object migration combined with granularity control. We discuss the subsystem that integrates XENOOPS applications in a time sharing environment. We illustrate the resulting possibilities with a visualisation of dynamic load balancing behaviour in a PDE solver with adaptive grid reenement.	adaptive mesh refinement;central processing unit;display resolution;distributed computing;distributed memory;load balancing (computing);requirement;run time (program lifecycle phase);solver;time-sharing;tree traversal;workstation	Wouter Joosen;Stijn Bijnens;Bert Robben;Johan Van Oeyen;Pierre Verbaeten	1995		10.1007/BFb0046659	network load balancing services;computer architecture;parallel computing;distributed memory;computer science;load balancing;operating system;distributed computing;time-sharing	HPC	-13.922064352373194	43.69252119243971	165239
41e71c53ca2a7be0ba90919af8f3049d957e665e	ciel: a universal execution engine for distributed data-flow computing	distributed data;fault tolerant;programming language;distributed programs;iterative algorithm;data dependence;control flow;recursive algorithm;high performance;scripting language;cloud computing	This paper introduces CIEL, a universal execution engine for distributed data-flow programs. Like previous execution engines, CIEL masks the complexity of distributed programming. Unlike those systems, a CIEL job can make data-dependent control-flow decisions, which enables it to compute iterative and recursive algorithms. We have also developed Skywriting, a Turingcomplete scripting language that runs directly on CIEL. The execution engine provides transparent fault tolerance and distribution to Skywriting scripts and highperformance code written in other programming languages. We have deployed CIEL on a cloud computing platform, and demonstrate that it achieves scalable performance for both iterative and non-iterative algorithms.	algorithm;cloud computing;control flow;data dependency;dataflow;distributed computing;distributed data flow;fault tolerance;iteration;iterative method;mega man zero;programming language;recursion;scalability;scripting language	Derek Gordon Murray;Malte Schwarzkopf;Christopher Smowton;Steven A F Smith;Anil Madhavapeddy;Steven Hand	2011			fault tolerance;real-time computing;cloud computing;computer science;operating system;database;distributed computing;scripting language;iterative method;programming language;control flow;recursion	Networks	-14.051901988892297	45.237359901897705	165344
455fb5ef008219be5683c8b7dc8fa1dada0d9b3f	exploiting parallelism through directives on the nano-threads programming model	lenguaje programacion;programming language;program optimization;programming model;computer architecture;architecture ordinateur;parallel architectures;architecture parallele;langage programmation;optimisation programme;arquitectura ordenador;fortran;optimizacion programa	The ability of an application to eeciently use the resources shared with other applications is a key feature needed to eeciently exploit the potential parallelism ooered by nowadays parallel architectures. In these multiprogrammed environments, the application should be able to adapt the parallelism (kind and amount) that it is worth to be exploited to the global utilization of system resources. In this paper we present a programming model oriented to the hierarchical exploitation of unstructured parallelism in multiprogrammed multiprocessor systems. The model ooers a set of directives targetted to be used by Fortran programmers that allow them to express the parallelism of the application. The compiler is responsible for the generation of code that eeciently exploits and manages this parallelism at run time. The code generated runs on top of a user-level threads library that allow the program to decide and adapt himself to the parallelism that can be attained at any time.	compiler;fortran;library (computing);multiprocessing;nano-threads;parallel computing;programmer;programming model;run time (program lifecycle phase);thread (computing);user space	Eduard Ayguadé;Xavier Martorell;Jesús Labarta;Marc González;Nacho Navarro	1997		10.1007/BFb0032701	computer architecture;parallel computing;computer science;cellular architecture;artificial intelligence;operating system;program optimization;database;distributed computing;data parallelism;programming paradigm;programming language;instruction-level parallelism;implicit parallelism;algorithm;task parallelism	HPC	-14.823967797831404	41.830951756390895	165977
1bd413e01343374135bfc7a5be96307a53de8012	computing blas level-2 operations on workstation clusters using the divisible load paradigm,	linear algebra;optimal solution;level 2;15xx;eficacia sistema;front end;multicast communication;solution optimale;estacion trabajo;closed form solution;matematicas aplicadas;mathematiques appliquees;49j30;05bxx;producto matriz;divisible loads;station travail;condition necessaire suffisante;distributed computing;performance systeme;asymptotic analysis;system performance;matrix vector products;optimal load distribution;workstation;algebre lineaire;necessary and sufficient condition;solucion optima;analyse performance;basic linear algebra subprograms;performance analysis;parallel computer;load sharing;calculo repartido;communication delay;algebra lineal;load distribution;load partitioning;distributed computing environment;systeme parallele;blas level 2 operations;parallel system;modele amas;05c78;cluster model;49k30;applied mathematics;produit matrice;calcul reparti;condicion necesaria suficiente;matrix product;sistema paralelo;analisis eficacia;workstation cluster	The problem of executing large BLAS (basic linear algebra subprograms) Level-2 operations, such as matrix-vector products, in a network-based distributed computing environment composed of a bus-oriented workstation cluster is considered. Unlike previous contributions, we take into account the fact that workstations, as against mainframe computers, are not equipped with communication coprocessors or front-ends, precluding any possibility of communication off-loading. Communication delays, which are significant in workstation clusters due to limited bandwidth availability, are specifically accounted for. This aspect is generally ignored in most performance analysis of parallel computing systems. The important contribution of this study is to show that the optimal load partitioning, and the subsequent performance of the network, depends critically on network bandwidth, computing capacity, and load characteristics. We design load distribution strategies for three cases (no communication, broadcast communication, and multicast communication) based on closed-form solutions of the optimal load partitioning problem and also present extensive and complete asymptotic analysis with respect to several parameters of the load and the system. Necessary and sufficient conditions for feasible and optimal load sharing are also derived. A trade-off study between the optimal number of workstations and the bandwidth of the bus is also presented.	blas;programming paradigm;workstation	Debasish Ghose;Hyoung Joong Kim	2005	Mathematical and Computer Modelling	10.1016/j.mcm.2004.01.004	closed-form expression;parallel computing;real-time computing;asymptotic analysis;workstation;matrix multiplication;computer science;weight distribution;linear algebra;front and back ends;basic linear algebra subprograms;mathematics;distributed computing;algorithm;distributed computing environment;algebra	Theory	-14.937217176132009	44.14696484448921	167402
024391e969787a6fe8cdc981a37f7f9a3a89d61a	distributed hardwired barrier synchronization for scalable multiprocessor clusters	tratamiento paralelo;distributed memory;gestion memoire;programmability;barrier synchronization;distributed memory systems;massively parallel mimd systems;shared memory;traitement parallele;wired nor logic;multiprocessor;hardware scalability fuzzy logic buildings integrated circuit interconnections computer architecture wires application software timing memory architecture;application software;gollete estrangulamiento;hardwired barrier architecture;memoria compartida;storage management;wires;memoire repartie partagee;indexing terms;doacross loops;synchronisation distributed memory systems shared memory systems;hot spot;fuzzy barriers;fuzzy logic;hot spots;synchronisation;computer architecture;gestion memoria;goulot etranglement;partially ordered set;scalable multiprocessor clusters;shared memory systems;memory architecture;synchronization;partially ordered barriers;integrated circuit interconnections;concurrent processes synchronisation;distributed hardwired barrier synchronization;timing analysis;memory based barriers;sincronizacion;scalability;multiprocesador;distributed shared memory distributed hardwired barrier synchronization scalable multiprocessor clusters memory based barriers concurrent processes synchronisation hot spots hardwired barrier architecture scalability timing analysis fuzzy barriers programmability massively parallel mimd systems;memoire repartie;distributed shared memory;bottleneck;concurrent process;parallel processing;buildings;memoire partagee;hardware;doal loops;partial order;multiprocesseur;timing	Conventional multiprocessors mostly use centralized, memory-based barriers to synchronize concurrent processes created in multiple processors. These centralized barriers often become the bottleneck or hot spots in the shared memory. In this paper, we overcome the difficulty by presenting a distributed and hardwired barrier architecture, that is hierarchically constructed for fast synchronization in cluster-structured multiprocessors. The hierarchical architecture enables the scalability of cluster-structured multiprocessors. A special set of synchronization primitives is developed for explicit use of distributed barriers dynamically. To show the application of the hardwired barriers, we demonstrate how to synchronize Doall and Doacross loops using a limited number of hardwired barriers. Timing analysis shows an O( 10’) to O( lo5) reduction in synchronization overhead, compared with the use of software-controlled barriers implemented in a shared memory. The hardwired architecture is effective in implementing any partially ordered set of barriers or fuzzy barriers with extended synchronization regions. The versatility, scalability, programmability, and low overhead make the distributed barrier architecture attractive in constructing finegrain, massively parallel MIMD systems using multiprocessor clusters with distributed shared memory.	barrier (computer science);central processing unit;centralized computing;distributed shared memory;mimd;multiprocessing;overhead (computing);scalability	Shisheng Shang;Kai Hwang	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.388040	partially ordered set;parallel processing;synchronization;parallel computing;real-time computing;computer science;operating system;distributed computing	Arch	-12.562457840828698	45.66617578855175	167580
82910fc6a85d4105e4e2a0897b1ad6661010f481	a comparison of three programming models for adaptive applications on the origin2000	workload;algoritmo paralelo;metodo adaptativo;evaluation performance;dynamic load balancing;general and miscellaneous mathematics computing and information science;parallel algorithm;shared address space;performance evaluation;communications;programacion paralela;evaluacion prestacion;n body problem;performance;machine parallele;parallel programming;dynamic loads;transmission message;methode adaptative;message transmission;algorithme parallele;programming model;application software dynamic programming parallel programming concurrent computing postal services computer applications runtime load management parallel machines libraries;mesh adaptation;adaptive applications;envoi message;adaptive method;charge travail;cache coherence;message passing;algorithms;parallel machines;parallel implementation;spatial locality;parallel programming model;carga trabajo;parallel programs;programming;dynamic mesh adaptation;communication pattern;99 general and miscellaneous mathematics computing and information science;programmation parallele;transmision mensaje	Adaptive applications have computational workloads and communication patterns which change unpredictably at runtime, requiring load balancing to achieve scalable performance on parallel machines. Efficient parallel implementation of such adaptive application is therefore a challenging task. In this paper, we compare the performance of and the programming effort required for two major classes of adaptive applications under three leading parallel programming models on an SGI Origin 2000 system, a machine which supports all three models efficiently. Results indicate that the three models deliver comparable performance. However, the implementations differ significantly beyond merely using explicit messages versus implicit loads/stores even though the basic parallel algorithms are similar. Compared with the message-passing (using MPI) and SHMEM programming models, the cache-coherent shared address space (CC-SAS) model provides substantial ease of programming at both the conceptual level and program orchestration levels, often accompanied by performance gains. However, CC-SAS currently has portability limitations and may suffer from poor spatial locality of physically distributed shared data on large numbers of processors.	address space;cache coherence;central processing unit;coherence (physics);computer performance;load balancing (computing);locality of reference;message passing interface;parallel algorithm;parallel computing;principle of locality;run time (program lifecycle phase);sas;shmem;scalability;software portability	Hongzhang Shan;Jaswinder Pal Singh;Leonid Oliker;Rupak Biswas	2000	ACM/IEEE SC 2000 Conference (SC'00)	10.1006/jpdc.2001.1777	programming;cache coherence;mathematical optimization;parallel computing;message passing;performance;computer science;theoretical computer science;operating system;n-body problem;distributed computing;parallel algorithm;programming paradigm;programming language;algorithm;parallel programming model	HPC	-16.054117960704147	42.939280935340804	167715
75ae3af7e407ffb0c7b28d376644f9efd2085b9a	bsp performance analysis and prediction: tools and application	programacion paralela;sistema informatico;parallel programming;computer system;object oriented programming;performance programme;bulk synchronous parallel;performance analysis;eficacia programa;systeme informatique;load balance;program performance;information system;programmation orientee objet;systeme information;programmation parallele;sistema informacion	Load balance is one of the critical factors affecting the overall performance of the BSP (Bulk Synchronous Parallel) programs. Without sufficient performance profiling information generated by effective profiling tools, it is often difficult to find out what extent and where load imbalance has occurred in a BSP program. In this paper, we introduce a new parallel performance profiling system for the BSP model. The system traces and generates comprehensive information on timing and communication by each process in each superstep. Its aim is to assist in the improvement of BSP program performance by identifying load imbalance among processors. The profiling data is visualised via a series of performance profiling graphs, making it easier to identify overloaded processes in a superstep. The visualising component of the system is written in Java, thus runs on almost any type of computer systems.	profiling (computer programming)	Weiqun Zheng;Shamim Khan;Hong Xie	1999		10.1007/3-540-48387-X_33	embedded system;parallel computing;real-time computing;simulation;computer science;load balancing;operating system;distributed computing;programming language;object-oriented programming;information system;algorithm;bulk synchronous parallel	HPC	-17.85294947693129	41.46493121900692	168051
eb7bc0f07ea9f9f09fbc3ab6c02069a79e72cf11	multiprogramming coordination	architecture.;processes;and phrases: multiprogramming;operating systems;parallehsm;coordination;multiprogramming coordination;synchronization;multiproeessing	This is a tutorial paper on the coordination of parallel activities It commences with an overview of multiprogrammed operating systems that uncovers an architecture consisting of cooperating, but competing processes working in parallel This is followed by a formal treatment of processes, and an analysis of the fundamental coordination needs of concomitant processes. The analysm leads to a set of two coordination primitives originally defined by Dijkstra. In the rest of the paper, an evolutionary series of examples of increasing coordination complexity is formulated and solved. As the various examples are discussed, cumulative extenmons to the original set of coordination primitives are justified and formally defined.	computer multitasking;language primitive;operating system	Leon Presser	1975	ACM Comput. Surv.	10.1145/356643.356646	theoretical computer science;computer architecture;computer multitasking;computer science	Logic	-15.917717192635507	39.39646787706099	168078
ced38dc3b9621a683a9ef986f19123591467f430	interprocedural compilation of irregular applications for distributed memory machines	memory parallel machine;new interprocedural optimizations;interprocedural compilation;memory machine;irregular data access pattern;interprocedural partial redundancy elimination;fortran d compilation system;irregular application;architecture independent mode;optimized placement;collective communication;data parallel language;high performance fortran;computer science;partial redundancy elimination;program slicing;technical report;distributed memory;sonet;parallel programming;synchronous optical network;data access;gateway;networking;application software;computer architecture	Data parallel languages like High Performance Fortran (HPF) are emerging as the architecture independent mode of programming distributed memory parallel machines. In this paper, we present the interprocedural optimizations required for compiling applications having irregular data access patterns, when coded in such data parallel languages. We have developed an Interprocedural Partial Redundancy Elimination (IPRE) algorithm for optimized placement of runtime preprocessing routine and collective communication routines inserted for managing communication in such codes. We also present two new interprocedural optimizations, placement of scatter routines and use of coalescing and incremental routines. We then describe how program slicing can be used for further applying IPRE in more complex scenarios. We have done a preliminary implementation of the schemes presented here using the Fortran D compilation system as the necessary infrastructure. We present experimental results from two codes compiled using our system to demonstrate the efficacy of the presented schemes.	distributed memory	Gagan Agrawal;Joel H. Saltz	1995		10.1109/SUPERC.1995.40	computer architecture;parallel computing;computer science;operating system;synchronous optical networking;programming language;computer network	HPC	-12.73057101515441	41.47330632512337	168101
83ae7ee2d313fe16183cac2edad25a6779969609	modeling and parallel simulation of large atm switch fabrics	simulation ordinateur;modelizacion;architecture systeme;systeme multiprocesseur memoire repartie;reseau asynchrone;etude experimentale;reseau ordinateur;teleinformatica;discrete time;data acquisition system;interconnection network;computer network;algorithme;modelisation;algorithm;teleinformatique;atm switch;multi stage interconnec tion network;sistema multiprocesador memoria distribuida;analyse performance;performance analysis;red ordenador;arquitectura sistema;systeme parallele;distributed memory multiprocessor system;simulacion computadora;parallel system;tiempo discreto;system architecture;temps discret;atm;modeling;remote data processing;computer simulation;estudio experimental;red interconexion;sistema paralelo;parallel simulation;simulation modeling;algoritmo;discrete event simulation;analisis eficacia;reseau interconnexion;multi stage interconnection network	Simulation is an essential tool in the development, testing, and performance evaluation of communication systems. In this paper we discuss a practical case where large ATM switches are needed and develop a cell-level simulator to evaluate the performance of such switches. We exploit the conservative time-window algorithm for parallel simulation to reduce the simulation time and resolve the memory limitation problem with sequential machines. We simulate ATM switches of sizes up to 1 k x 1 k ports. A speed-up of 7 using ten processors on a shared-memory multiprocessor is achieved.	atm turbo;simulation	Fredrik Östman;Tawfik Lazraq;Rassul Ayani	1998	Simulation	10.1177/003754979807000104	computer simulation;embedded system;discrete time and continuous time;simulation;systems modeling;telecommunications;computer science;discrete event simulation;simulation modeling;atmosphere;data acquisition	HPC	-14.005915394430648	42.468189717242	168149
535558c6211a211647efe69442f09baeed6ab917	crossing the machine interface	processor line;generalized architecture;data processor;underlying circuitry implementation;basic architecture;hardware implementation;processor architecture;generalized data processor architecture;processor implementation;generalized processor architecture;machine interface;system performance;hardware accelerator;software systems	The concepts and theory behind a specific type of hardware accelerator, oar “assist”, are presented. Such accelerators are specific extensions to an existing, generalized data processor architecture. Both the software system and the hardware implementation are changed by the extensions. For the accelerators discussed here, the hardware implementation is accomplished solely in microcode.  The generalized architecture can be compatible across a line of data processors. The accelerators are defacto extensions to the basic architecture and are normally only defined on a subset of the processor line. These architectural extensions consist of functions migrated from “above” the generalized architecture (i.e. software functions) into the processor architecture and processor implementation.  The accelerators provide a method of improving system performance that is complementary to improving performance through modification of the generalized processor architecture itself or the underlying circuitry implementation.	central processing unit;electronic circuit;hardware acceleration;ibm system/370;microarchitecture;microcode;simulation;software system;virtual machine	Arthur G. Olbert	1982			reference architecture;computer architecture;parallel computing;real-time computing;hardware acceleration;microarchitecture;computer science;applications architecture;operating system;hardware architecture;computer performance;data architecture;software system	Arch	-12.549815123831996	46.3187222723044	168567
074f5fe7d36ce17ed7da4d7ac93870e337dbf939	design of hm2p&#8212;a hierarchical multimicroprocessor for general-purpose applications	computers;electronic circuits;microprocessors;general and miscellaneous mathematics computing and information science;communications;uses;microelectronic circuits 990200 mathematics computers;user interface;performance falloff;performance;data processing;closed queueing network;data distribution;synchronization;queueing model;synchronization hierarchical multiprocessor monitor performance falloff processing data distribution hierarchy queueing model;array processors;tree structure;hierarchical multiprocessor;design;queues;processing data distribution hierarchy;monitor	This paper presents a tree-structured multiprocessor called the hierarchical multimicroprocessor (HM2p), each node of which is composed of a cluster of processor modules (PM's), common memory, DMA interface, switches, communication lines, and a data processor associated with it. The HM2p consists of two different hierarchies, one for data processing and the other for data distribution, which provide clean, structured separation between processing components and user interface components.	direct memory access;general-purpose markup language;multiprocessing;network switch;user interface	Kang G. Shin;Yann-Hang Lee;J. Sasidhar	1982	IEEE Transactions on Computers	10.1109/TC.1982.1675921	embedded system;synchronization;design;monitor;parallel computing;real-time computing;data processing;performance;computer science;electrical engineering;operating system;tree structure;programming language;user interface;computer network	Arch	-12.340661993109327	45.03782025356939	168878
267235d171c9d15edcac9f93abe1a0bef402ec86	using hints in dune remote procedure calls	distributed system;interprocess communication;system performance;remote procedure call;general solution;col;communication protocol	Remote Procedure Calls (RPC) are an established mechanism for coordinating activity between multiple processors in a distributed system. Their efficient use in Interprocess Communication (IPC) is difficult because underlying network protocols can have a great effect on system performance, especially when the IPC is within the same processor. This problem is compounded if the distributed system must work simultaneously over several forms of interconnect. Designing an efficient solution for one medium may make accommodating another awkward or impractical, while designing a general solution for all may not take full advantage of any specifrc medium. This paper describes an enhanced remote procedure call model of interprocess communication that is generally efficient for many types of network, even nontraditional ones. It is based on optional hints supplied by the client of an operation, which enable medium-specifrc optimizations in a uniformly structured interface. The model supports dynamic rebinding of communications protocols, which is necessary for the efficient treatment of migratable system objects. @ Computing Systems, Vol. 3 . No. I . Winter 1990 47	central processing unit;communications protocol;dune;distributed computing;inter-process communication;remote procedure call;subroutine	Marc F. Pucci;J. L. Alberi	1989	Computing Systems		communications protocol;real-time computing;simulation;computer science;operating system;distributed computing;computer performance;programming language;remote procedure call;inter-process communication	HPC	-14.870994132248233	44.508781205429386	169477
70ad7036d1c05c4d6542ac990c82928f4d959d4d	"""on the usage of simulators to detect inefficiency of parallel programs caused by """"bad"""" schedulings: the simparc approach"""	algoritmo paralelo;inefficacite;optimisation;parallel algorithm;inefficiency;optimizacion;gollete estrangulamiento;deteccion;detection;simulator;algorithme parallele;goulot etranglement;simulador;scheduling;simulateur;ordonamiento;optimization;parallel programs;ineficacia;bottleneck;ordonnancement;parallel simulation	"""The problem of producing eecient parallel programs against diierent possible execution orders or schedulings is addressed. Our method emphasizes experimental modiications of the machine parameters, using a parallel simulator, so that by a proper choice of the simulator parameters, the user can detect potential harmful schedulings. Two types of time statistics named ideal and eeective times are deened. We show that the gap between them can be used to detect weather the current scheduling is a possible cause for a performance degradation. This allows the user to make a more \compact"""" search of the huge space of all possible schedulings. This search for \bad"""" schedulings is done by allowing the user to change the simulation parameters, in particular the context-switch delay and the scheduling policy. The paper includes a set of typical experiments with the simulator, covering the main factors for such bad schedulings. Altogether, the simulator and the experiments form a methodology for detecting possible bad schedulings. Consequently, we argue that optimal performances can be achieved only if the user is able to set both the context-switch delay and the scheduling policy, not only for simulation purposes but also before and during actual execution."""	context switch;elegant degradation;experiment;performance;scheduling (computing);sensor;simulation	Yosi Ben-Asher;Gadi Haber	1996	Journal of Systems and Software	10.1016/0164-1212(96)00028-3	real-time computing;simulation;computer science;operating system;distributed computing;parallel algorithm;scheduling	Metrics	-17.026526703702494	43.943677844794756	169831
1f07cd3b186a446dea299ce8f53b3a64c0e5a7f1	an integrated simulation environment for parallel and distributed system prototying	parallel and distributed system;prototipificacion rapida;tratamiento paralelo;ise;concepcion asistida;computer aided design;systeme evenement discret;conception ordinateur;realite virtuelle;traitement parallele;realidad virtual;parallel software evaluation;reseau ordinateur;simulation;virtual reality;performance comparison;simulacion;cluster of workstations;computer network;sistema acontecimiento discreto;concepcion ordenador;development tool;virtual prototyping;discrete event system;bones;rapid prototyping;operating system;community networks;parallel systems;execution driven simulation;red ordenador;conception assistee;parallel system simulation;mpi;computer design;rapid virtual prototyping;parallel architecture;power modeling;parallel applications;parallel processing;simulation environment;prototypage rapide;software evaluation;discrete event simulation	The process of designing parallel and distributed computer systems requires predicting performance in response to given workloads. The scope and interaction of applications, operating systems, communication networks, processors, and other hardware and software lead to substantial system complexity. Development of virtual prototypes in lieu of physical prototypes can result in tremendous savings, especially when created in concert with a powerful model development tool. When high-fidelity models of parallel architecture are coupled with workloads generated from real parallel application code in an execution-driven simulation, the result is a potent design and analysis tool for parallel hardware and software alike. This paper introduces the concepts, mechanisms, and results of an Integrated Simulation Environment (ISE) that makes possible the rapid virtual prototyping and profiling of legacy and prototype parallel processing algorithms, architectures, and systems using a networked cluster of workstations. Performance results of virtual prototypes in ISE are shown to faithfully represent those of an equivalent hardware configuration, and the benefits of ISE for predicted performance comparisons are illustrated by a case study.	algorithm;american and british english spelling differences;central processing unit;computational complexity theory;computer cluster;digital signal processor;distributed computing;integrated development environment;memory hierarchy;operating system;parallel computing;prototype;sonar (symantec);scalability;seamless3d;shared memory;simulation;sonar signal processing;telecommunications network;testbed;virtual reality;workstation;xilinx ise	Alan D. George;Ryan Fogarty;Jeff Markwell;Michael D. Miars	1999	Simulation	10.1177/003754979907200502	embedded system;parallel processing;real-time computing;simulation;computer science;message passing interface;discrete event simulation;operating system;virtual reality	HPC	-17.807959328660445	42.520229835576664	170330
d45e9386e808f105e917286ec55b357e17c14f57	distributed environments & distributed program design	buffer storage;distributed programs;parallel programming;software performance;parallel processing process control computer architecture parallel programming software design central processing unit buffer storage computer science operating systems software performance;computer architecture;operating system;distributed environment;comparative study;process control;computer science;point of view;software design;parc;parallel processing;central processing unit;operating systems	This paper presents an overall view about the most common methods for implementing task schemes for process control from the operating system point of view as well as from the Occam point of view. They are discussed and compared in a comparative study. In addition the best known development platforms He-lios, ParC, Inmos Toolsets and Parix are introduced. Their signiicant features are presented and compared in a detailed comparative study focussing the advantages and disadvantages of each development platform with respect to its usefulness in distributed program design. The presented study is addressed to all developers who intend to design distributed software for their transputer based architecture and who are on the outlook for a programming platform that almost suits their needs.	distributed computing;microsoft outlook for mac;operating system;point of view (computer hardware company);transputer;occam	Ralf Diekmann;Knut Menzel;Frank Stangenberg	1994		10.1109/EMPDP.1994.592465	computer architecture;parallel computing;computer science;distributed computing;distributed design patterns	SE	-15.21648309744465	39.89226628566086	170414
560a0e0ca31e3141d8e53cb3ab6d9a7798961998	application-centric parallel multimedia software	parallel programming;multimedia application;software engineering;engineering system;user oriented parallel computing software application centric parallel multimedia software engineering systems visual intensive multimedia applications data intensive multimedia applications parallel high performance computing software development parallel computing software languages system designs compilers tools construction;multimedia computing;software engineering multimedia computing parallel programming;parallel processing application software concurrent computing multimedia computing distributed computing reflection data engineering systems engineering and theory multimedia systems programming;system design;java applet;software development;high performance computer;parallel computer;high performance	Argy Krikelis Apsex Microsystems Ltd. Brunel University Uxbridge, Middlesex, UK argy.krikelis@aspex.co.uk Over the last 15 years, software advances have been the primary drive behind the rapid development of computer hardware. Most experts would agree that hardware is only as good as the software that uses it. Softwaredevelopment tools, as well as their users, constantly demand more speed and performance from the hardware. One significant exception to this drive is parallel computing. Despite the substantial material and intellectual efforts to realize parallel systems, parallel computing remains largely an exotic field. The average scientist and user are somewhat skeptical about its effectiveness. This skepticism has its roots in the failure of implemented parallel systems to achieve anything close to their anticipated peak performance. Parallel-computing software exacerbated this problem by focusing on program performance, not user productivity. The typical scapegoat for the limited acceptance of parallel computing is the generalpurpose parallel-computing model or, rather, the lack of one. Developers created languages and tools that were architecture-specific and could be used only in isolation. So, applications need reprogramming each time a new architecture emerges or an old architecture is modified. Attempts to emulate the universality of the von Neumann programming model used in sequential computing achieved very little. The few so-claimed successes soon revealed how tedious it was to write parallel programs in dialects that made the user responsible for creating and managing parallel computations and for the explicit communications between the processors.	central processing unit;computation;computer hardware;emergence;parallel computing;programming model;universal turing machine	Anargyros Krikelis	1997	IEEE Concurrency	10.1109/4434.641630	computing;parallel computing;computer science;theoretical computer science;software development;operating system;end-user computing;data-intensive computing;distributed computing;utility computing;programming language;grid computing;unconventional computing;java applet;software system;systems design	Arch	-13.185218033016021	40.14582425101632	170660
8b4392aab6a003a7f97cb54bba9bd8c158a0794f	hardware support for fast capability-based addressing	guardes pointers;architecture systeme;storage access;branch prediction;sistema informatico;computer system;segmentation;addressing;profile based optimization;branch target buffers;protection;trace scheduling;almacenamiento;acces memoire;stockage;adressage;memory systems;acceso memoria;proteccion;arquitectura sistema;systeme informatique;direccionamiento;system architecture;storage;segmentacion	Traditional methods of providing protection in memory systems do so at the cost of increased context switch time and/or increased storage to record access permissions for processes. With the advent of computers that supported cycle-by-cycle multithreading, protection schemes that increase the time to perform a context switch are unacceptable, but protecting unrelated processes from each other is still necessary if such machines are to be used in non-trusting environments. This paper examines guarded pointers, a hardware technique which uses tagged 64-bit pointer objects to implement capability-based addressing. Guarded pointers encode a segment descriptor into the upper bits of every pointer, eliminating the indirection and related performance penalties associated with traditional implementations of capabilities. All processes share a single 54-bit virtual address space, and access is limited to the data that can be referenced through the pointers that a process has been issued. Only one level of address translation is required to perform a memory reference. Sharing data between processes is efficient, and protection states are defined to allow fast protected subsystem calls and create unforgeable data keys.	64-bit computing;address space;capability-based addressing;computer;context switch;encode;indirection;multithreading (computer architecture);pointer (computer programming);segment descriptor;thread (computing);trust (emotion)	Nicholas P. Carter;Stephen W. Keckler;William J. Dally	1994		10.1145/195473.195579	embedded system;parallel computing;real-time computing;addressing mode;computer science;segmentation;branch predictor;systems architecture;trace scheduling	Arch	-16.91262797254415	45.9860542676644	171253
fe675b16571ff4068261e5fe8ba7d295bfcca8dd	compassion: a parallel i/o runtime system including chunking and compression for irregular applications	o runtime system;com- pression.;irregular applications;parallel i/o;runtime systems	In this paper we present two designs, namely, Collective I/O and Pipelined Collective I/O, of a runtime library for irregular applications based on the two-phase collective 1/O technique. We also present the optimization of both models by using chunking and compression mechanisms. In the first scheme, all processors participate in compressions and I/O at the same time, making scheduling of I/O requests simpler but creating a possibility of contention at the I/O nodes. In the second approach, processors are grouped into several groups, overlapping communication, compression, and I/O to reduce I/O contention dynamically. Finally, evaluation results are shown that demonstrates that we can obtain significantly high-performance for I/O above what has been possible so far.	input/output;parallel i/o;runtime system;shallow parsing	Jesús Carretero;Jaechun No;Sung-Soon Park;Alok N. Choudhary;Pang Chen	1998		10.1007/BFb0037194	embedded system;parallel computing;real-time computing;data processing;computer science;computer network	HPC	-13.970991960322426	46.2398581553587	171644
1857a8ae0269481c0950da20327d601cddc2c3d8	sorting large files on a backend multiprocessor	tratamiento paralelo;distributed system;fast packet bus;base donnee;processing 990210 supercomputers 1987 1989;systeme reparti;general and miscellaneous mathematics computing and information science;fichier;parallel sorting;traitement parallele;multiprocessor;sorting;database management systems;executive codes;sorting database management systems multiprocessing systems;database;implantation;base dato;data processing;fichero;mathematical logic;capacidad memoria;algorithme;distributed operating system;algorithm;capacite memoire;sistema repartido;memory capacity;file;design and implementation;sorting bandwidth operating systems power measurement database systems concurrent computing prototypes size measurement packaging aggregates;array processors;measuring methods;computer codes;cost effectiveness;algorithms;jasmin prototype;technical report;multiprocessing systems;computer science;backend multiprocessor;multiprocesador;configuration;management;high performance;off the shelf;streamlined distributed operating system backend multiprocessor parallel sort merge algorithm jasmin prototype fast packet bus;data base management;implantacion;streamlined distributed operating system;logical process;parallel processing;parallel sort merge algorithm;algoritmo;multiprocesseur	The authors investigate the feasibility and efficiency of a parallel sort-merge algorithm by considering its implementation of the JASMIN prototype, a backend multiprocessor built around a fast packet bus. They describe the design and implementation of a parallel sort utility and present and analyze the results of measurements corresponding to a range of file sizes and processor configurations. The results show that using current, off-the-shelf technology coupled with a streamlined distributed operating system, three- and five-microprocessor configurations, provide a very cost-effective sort of large files. The three-processor configuration sorts a 100-Mb file in 1 hr which compares well to commercial sort packages available on high-performance mainframes. In additional experiments, the authors investigate a model to tune their sort software and scale their results to higher processor and network capabilities. >	multiprocessing;sorting	Micah Beck;Dina Bitton;Kevin Wilkinson	1988	IEEE Trans. Computers	10.1109/12.2222	merge sort;embedded system;parallel processing;mathematical logic;parallel computing;multiprocessing;cost-effectiveness analysis;data processing;computer file;computer science;sorting;technical report;operating system;programming language;configuration;algorithm	HPC	-17.448266792726933	44.49277203610757	171741
6d3428c9dbf4d5032834fd04c148d75ddc07b97c	challenges and solutions to improve the scalability of an operational regional meteorological forecasting model	estensibilidad;modelizacion;forecasting;prevision;replacement;haute performance;meteorologie;remplacement;execution time;weather;tiempo meteorologico;etude experimentale;examination;control conocimientos;codigo tiempo;distributed computing;temps meteorologique;meteorologia;regional weather forecasting models;metodo secuencial;sequential method;effet dimensionnel;grid;parallel scalability;modelisation;controle connaissance;rejilla;size effect;code temps;alto rendimiento;methode sequentielle;grille;calculo repartido;temps execution;reemplazo;procesador;extensibilite;scalability;efecto dimensional;processeur;tiempo ejecucion;modeling;high performance;estudio experimental;time code;meteorology;calcul reparti;processor	This work investigates the parallel scalability of BRAMS, a limited area weather forecasting production code, from O(100) cores to O(1,000) cores on large grids (20 km and 10 km resolution runs over South America). Initial experiments show lack of scalability at modest core count. Execution time profiling and source code examination revealed the causes of the limited scalability: sequential algorithms and extensive memory requirements at scarcely used phases of the computation. As processor count increases, these 'secondary' phases dominate execution time. Algorithm replacement and memory reduction generate a new code version that possesses strong and weak scaling. The new version achieved a speed-up of 6 from 100 to 700 processors on the 20 km resolution grid and a speed-up of 6.9 on the same processor range on the 10 km resolution grid. Results were confirmed at another machine with a distinct architecture. Further experiments show that the scalability of the 20 km resolution case is limited by load unbalancing at the most demanding computational phase.	numerical weather prediction;scalability	Alvaro Luiz Fazenda;Jairo Panetta;Daniel M. Katsurayama;Luiz Flavio Rodrigues;Luis F. G. Motta;Philippe Olivier Alexandre Navaux	2011	IJHPSA	10.1504/IJHPSA.2011.040462	scalability;simulation;systems modeling;telecommunications;forecasting;computer science;grid	Robotics	-16.497667715748232	42.943566342490726	171922
65c4e4d9209eff471bf29b76158a2ac5d9a94e8c	using platform-specific performance counters for dynamic compilation	juste a temps;parallelisme;utilisation information;lenguaje programacion;compilacion;uso informacion;haute performance;compilateur;programming language;information use;distributed computing;dynamic compilation;cache memory;compiler;antememoria;antememoire;parallelism;paralelismo;hardware performance counters;alto rendimiento;langage programmation;compilation;calculo repartido;procesador oleoducto;just in time;block method;justo en tiempo;parallel architecture;processeur pipeline;high performance;calcul reparti;compilador;pipeline processor	Hardware performance counters provide information about events in the hardware platform (e.g., cache misses, pipeline stalls), in contrast to profiles that capture program properties (e.g., execution frequencies for basic blocks, methods, function calls). As platform architectures become more complex and also more diverse, it is important for a compiler to exploit platform-specific information. A dynamic (JIT) compiler is in the unique position to run on the same platform as the target application, but in practice, exploiting the wealth of information available through performance counters is far from easy. If a JIT compiler is to use performance counter information, this information must be fine-grained (e.g., attributing cache misses to a single load instruction) and must be obtainable without undue overhead. We present a runtime+compiler framework to tie hardware performance counter information to a dynamic compiler and argue that the overhead is low and fine-grained. As parallel architectures or multi-core architectures proliferate, performance issues will play a crucial role in all compilation engines, and our paper reports on a modular approach to make such counter information available to the compiler.	dynamic compilation	Florian T. Schneider;Thomas R. Gross	2005		10.1007/978-3-540-69330-7_23	computer architecture;compiler;parallel computing;real-time computing;dynamic compilation;cpu cache;profile-guided optimization;compiler correctness;computer science;loop optimization;programming language;functional compiler	HPC	-15.827175125227107	43.5642472069434	172119
bfb6b580b86c96da43597881c87fa0533deda919	distributing code in a parallel fine grain machine using the actor model	multiprocessor interconnection networks;message passing parallel machines parallel programming parallel architectures;parallel fine grain machine;grain size;massively parallel systems;electronic mail;execution time;routing;parallel programming;preferment communication unit code distribution parallel fine grain machine actor model massively parallel systems execution time i o operations code loading application growth;network servers;routing load management multiprocessor interconnection networks circuits electronic mail parallel machines delay grain size hardware network servers;parallel architectures;application growth;i o operations;load management;message passing;code distribution;parallel machines;circuits;code loading;preferment communication unit;actor model;hardware	One way to gain execution time on massively parallel systems is to minimise I/O operations by reducing code loading. A solution to this problem is to distribute code dynamically among all processors. The actor model is well suited to this execution model. Actors are modelled as processes which have to be duplicated at execution, at request and depending on the application growth. Our project aims to integrate a preferment communication unit that handles dynamic code distribution. >	actor model	Youssef Latrous;Guy Mazaré	1995		10.1109/EMPDP.1995.389147	dead code;parallel computing;real-time computing;computer science;distributed computing	NLP	-13.517718336865274	44.75461212654021	172367
fac7b05384a7605898db2f4e869e522167e400f5	mp/c: a multiprocessor/computer architecture	supersystems;multicomputers;switched bus;multiprocessors;tree structured computer;computer architecture;tree structured computer computer architecture multicomputers multiprocessors supersystems switched bus	A computer architecture for concurrent computing is proposed that has the shared memory aspect of tightly coupled multiprocessor systems and also the connection simplicity associated with message-connected, loosely coupled multicomputer systems. A large address space is dynamically partitioned into contiguous segments that can be accessed by a single processor. The partitioning is accomplished by switching the system buses, using semiconductor switches. The completion of a concurrent process is signaled by a processor's return to an idle state and the reattachment of its memory segment to the neighboring active processor. In effect, the assignment of an address sequence and the activation of a processor is a process FORK operation, and the processor deactivation and memory segment reattachment is a process JOIN. Following a description of the MP/C structure and basic operation, some additional enhancements of the system, which improve the applicability of MP/C to many classes of computations, are outlined. Applications include tree-structured multiprocessing, recursive and nondeterministic procedures, arbitrary concurrent computations, very high precision numeric calculations, and process-structured operating systems. The linear MP/C structure is extensible to higher dimensions. A two-dimensional system is described, and its usefulness for data base operations and array processing is discussed.	address space;array processing;computation;computer architecture;concurrent computing;database;fork (software development);fork (system call);loose coupling;memory segmentation;multiprocessing;network switch;nondeterministic algorithm;operating system;parallel computing;recursion;semiconductor;shared memory	Bruce W. Arden;Ran Ginosar	1981	IEEE Transactions on Computers	10.1109/TC.1982.1676022	embedded system;computer architecture;parallel computing;computer science;operating system;distributed computing	Arch	-12.573942782628968	45.29933882046367	172967
365420f02124444e32b02eb62e3dff8109881942	design algorithms for asynchronous write operations in disk-buffer-cache memory	evaluation performance;gestion memoire;storage access;performance evaluation;asynchrone;storage management;evaluacion prestacion;cache memory;antememoria;algorithme;algorithm;gestion memoria;antememoire;acces memoire;operation ecriture;acceso memoria;asincrono;asynchronous;algoritmo	This article introduces new algorithms for asynchronous operations in disk-buffer-cache memory. These algorithms allow for writing files into the buffer cache by the processes. The number of active processes in the system and the length of the queue to the disk-buffer cache are considered in the algorithm design. This information is obtained dynamically during the execution of the algorithms. The performance of the operations on the buffer cache is improved by using the algorithms, which allow for writing the contents of the buffer cache to the disk depending on the system load and the write activity. The elapsed time of writing a file into the buffer cache is calculated. The waiting time to start writing a file is also considered. It is shown that the elapsed time of writing a file decreases using the algorithms, which write the blocks to the disk depending on the rate of write operations and the number of active processes in the system. The time for a block to become available for update in the buffer cache is given. The number of blocks available for update in the buffer cacha is derived. The performance of the algorithms is compared. It is shown that the proposed algorithms allow for better performance than an algorithm that does not use information about the system load.	algorithm design;asynchronous i/o;load (computing);overhead (computing);page cache	Anna Hác	1991	Journal of Systems and Software	10.1016/0164-1212(91)90019-3	interleaved memory;parallel computing;cpu cache;computer science;artificial intelligence;asynchronous communication;write combining;algorithm	OS	-16.272139971642957	45.081619255016975	173409
53105ddd19a0d9e85222cec161ba74a83884edab	reduced interprocessor-communication architecture and its implementation on em-4	processor architecture;data parallel;design principle;systeme unix;architecture systeme;shared memory;programmation;implementation;unix system;calculo automatico;integration;parallel computation;computing;paralelismo masivo;programacion;programming model;calcul automatique;ejecucion;calculo paralelo;massively parallel computers;massively parallel computer;community structure;integracion;em 4 prototype;interprocessor communication;risc processor;message passing;cost effectiveness;arquitectura sistema;programming models;procesador;sistema unix;processeur risc;processeur;system architecture;communication;calcul parallele;comunicacion;programming;rica;parallelisme massif;series parallel;massive parallelism;processor	One of the most significant issues in building general purpose massively parallel computers is the integration of computation and communication in an efficient and cost-effective manner. This paper presents a way of integrating computation and communication from the viewpoint of the processor architecture. Two statements will be presented and examined: (1) computation and communication should be tightly coupled and their operation should be highly overlapped; and (2) communication structure should be efficient and simple, i.e. turnaround from data input to execution should be as short as possible using a simplified message handling mechanism. Briefly we can say ‘Fuse communication and computation, then reduce the fused structure as simple and efficient as possible’. The fused structure is called RICA, Reduced Interprocessor-Communication Architecture, in this paper. The word ‘Reduced’ here means the simplified structure of message handling, invocation of a new thread, computation and message generation. Based on the RICA design principles, the authors have developed the EM-series parallel computers, EM-4, EM-X and EM-5 This paper concentrates on the architecture and implementation of EM-4, whose first prototype has been fully operational since April 1990. The communication performance of EM-4 is comparable with its computation performance. These two primitives are efficiently and simply fused within the EM-4 architecture, i.e. a * Corresponding author. Email: sakai@trc.rwcp.or.jp 0167-8191/95/$09.50	a* search algorithm;computation;computer;email;expectation–maximization algorithm;inter-process communication;memory virtualization;message passing;parallel computing;parallel language;prototype;shared memory;software system;thread (computing)	Shuichi Sakai;Yuetsu Kodama;Mitsuhisa Sato;Andrew Shaw;Hiroshi Matsuoka;Hideo Hirono;Kazuaki Okamoto;Takashi Yokota	1995	Parallel Computing	10.1016/0167-8191(94)00109-N	computer architecture;parallel computing;real-time computing;computer science;operating system;programming paradigm;programming language;algorithm;systems architecture	HPC	-16.654229352951326	42.7746812492555	173445
ab6c27ff0c6d5ad44481dfe9e676a10810f11c17	a novel architecture for handling persistent objects		The storage and processing of persistent objects presents opportunities for add-on hardware support. Particularly, there are aspects of memory management and wholestructure processing which are seen to benefit from a novel kind of S/MD-parallel architecture. This paper discusses the top-down requirements for handlingpersistent objects and then describes an active memory unit called the IFS/2 which meets these requirements. Performance figures for prototype hardware are given, and its application to an object-oriented data mode/ is briefly described.	add-ons for firefox;memory management;parallel computing;prototype;requirement;top-down and bottom-up design	Simon H. Lavington	1993	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(93)90043-7	content-addressable memory;memory management;parallel computing;real-time computing;architecture;memory map;data model;theoretical computer science;computer science	Arch	-12.572554965549747	45.33505492216017	173597
119beb99e939a862e045b7fec964e87abbb054d5	a multiprogramming system for a microcomputer	microcomputer;operating system;multiprogramming nucleus	Abstract#R##N##R##N#A multiprogramming operating system nucleus has been constructed for a microcomputer. An account is given of the implementation, analysing the problems encountered, the magnitude of the effort required and the performance characteristics. The aim is to aid other implementors by drawing attention to those aspects of the work likely to be characteristic of microcomputer implementations in general.	computer multitasking;microcomputer	A. Smith;D. H. Bell	1982	Softw., Pract. Exper.	10.1002/spe.4380120503	computer architecture;computer hardware;computer science;operating system;microcomputer	NLP	-16.152283970230258	39.64861759145756	174297
9b464000291c4b04afda3d80141372eb55b72c23	parallel programmable architectures and compilation for multi-dimensional processing	tratamiento paralelo;algoritmo paralelo;compilacion;parallel algorithm;articulo sintesis;traitement parallele;multidimensional system;article synthese;algorithme parallele;multi dimensional;computer architecture;architecture ordinateur;compilation;sistema n dimensiones;architecture programmable;arquitectura ordenador;systeme n dimensions;systeme parallele;parallel system;multi dimensional processing;sista;review;parallel programmable architectures;parallel processing;sistema paralelo	In this introduction, we will summarize the main contributions of the papers collected in this special issue. Moreover, the topics addressed in these papers will be linked to the major research trends in the domain of parallel algorithms, architectures and compilation.	compiler	Francky Catthoor;Marc Moonen	1995	Microprocessing and Microprogramming	10.1016/0165-6074(95)00019-K	parallel processing;parallel computing;multidimensional systems;computer science;parallel algorithm;algorithm	Arch	-15.84687426229019	40.92825805786022	174950
02be7986b3420b01c5558ddb11d6b64654051afc	a distributed parallel algorithm to solve the 2d cutting stock problem	distributed memory;bin packing;parallel algorithm;resource allocation;synchronization scheme cutting stock problem load balancing;cutting stock problem;parallel algorithms concurrent computing algorithm design and analysis load management libraries data structures ethernet networks dynamic programming upper bound computer architecture;gigabit ethernet;load balancing;mpich gigabit ethernet 2d cutting stock problem distributed memory parallel algorithm time driven task intercommunication service load balancing mvapich infiniband;resource allocation bin packing local area networks parallel algorithms;load balance;synchronization scheme;local area networks;parallel algorithms	This work analyses the difficulties of parallelizing the best known sequential algorithm for the 2D cutting stock problem. All the approaches to parallelize the algorithm strive against its highly irregular computation structure and its sequential nature. A distributed-memory parallel algorithm has been designed through a time-driven task intercommunication service. The service allows to introduce a load balancing scheme that tries to hide the non-homogeneous work load nature of the involved single tasks. Experimental results obtained for MVAPICH infiniband and MPICH gigabit Ethernet implementations prove the efficiency of the communication and balancing schemes and show the almost linear speedup achieved by the parallel algorithm.	central processing unit;computation;cutting stock problem;distributed memory;gigabit;infiniband;load balancing (computing);mpich;mvapich;parallel algorithm;parallel computing;scalability;sequential algorithm;speedup;unbalanced circuit;whole earth 'lectronic link	Coromoto León;Gara Miranda;Casiano Rodríguez;Carlos Segura	2008	16th Euromicro Conference on Parallel, Distributed and Network-Based Processing (PDP 2008)	10.1109/PDP.2008.31	parallel computing;real-time computing;computer science;load balancing;distributed computing;parallel algorithm	HPC	-12.892459622774249	43.52126684802462	175720
86968a84a1bb664d04ae7878cc70751ef2ed820c	tuning mpi collectives by verifying performance guidelines		ABSTRACT MPI collective operations provide a standardized interface for performing data movements within a group of processes. The e ciency of collective communication operations depends on the actual algorithm, its implementation, and the speci c communication problem (type of communication, message size, number of processes). Many MPI libraries provide numerous algorithms for speci c collective operations. The strategy for selecting an e cient algorithm is often times prede ned (hard-coded) in MPI libraries, but some of them, such as Open MPI, allow users to change the algorithm manually. Finding the best algorithm for each case is a hard problem, and several approaches to tune these algorithmic parameters have been proposed. We use an orthogonal approach to the parameter-tuning of MPI collectives, that is, instead of testing individual algorithmic choices provided by an MPI library, we compare the latency of a speci c MPI collective operation to the latency of semantically equivalent functions, which we call the mock-up implementations. The structure of the mock-up implementations is de ned by selfconsistent performance guidelines. The advantage of this approach is that tuning using mock-up implementations is always possible, whether or not an MPI library allows users to select a speci c algorithm at run-time. We implement this concept in a library called PGMPITuneLib, which is layered between the user code and the actual MPI implementation. This library selects the best-performing algorithmic pattern of an MPI collective by intercepting MPI calls and redirecting them to our mock-up implementations. Experimental results show that PGMPITuneLib can signi cantly reduce the latency of MPI collectives, and also equally important, that it can help identifying the tuning potential of MPI libraries.	algorithm;baseline (configuration management);blocking (computing);collective operation;distributed memory;hard coding;interaction;library (computing);maxima and minima;message passing interface;mock object;open mpi;principle of good enough;tag (game)	Sascha Hunold;Alexandra Carpen-Amarie	2017	CoRR		parallel computing;latency (engineering);distributed computing;implementation;computer science;semantic equivalence	HPC	-12.138232266955306	41.620200343705896	176540
6b0a6abc774d5f23f70c4251953243465ca80ecd	a new kernel approach for modular real-time systems development	architectural design;protocols;kernel real time systems scheduling algorithm resource management protocols testing libraries round robin algorithm design and analysis dynamic scheduling;system configuration;software prototyping;processor scheduling;network operating systems;scheduling algorithm;posix compliant kernels kernel approach modular real time systems development dynamic configurable kernel architecture scheduling algorithms prototyping aperiodic servers concurrency control protocols posix 1003 13 pse52 specifications application code;network operating systems concurrency control unix protocols software prototyping processor scheduling real time systems;concurrency control;unix;real time systems;dynamic configuration	This paper presents a dynamic configurable kernel architecture designed for supporting a simple implementation, integration and evaluation of scheduling algorithms. The main goal of the proposed architecture is to provide a pla@orin for fast prototyping scheduling algorithms both for the CPU and for the devices. The kernel is fully modular in terms of scheduling policies, aperiodic servers, and concurrency control protocols, allowing applications to be developed independently from a particular system configuration. Finally, the system is compliant with the POSlX 1003.13 PSE52 specifications to sirriplifi porting of application code developed f o r other POSlX compliant kernels.	algorithm;central processing unit;concurrency (computer science);concurrency control;kernel (operating system);real-time transcription;scheduling (computing);system configuration	Paolo Gai;Luca Abeni;Massimiliano Giorgi;Giorgio C. Buttazzo	2001		10.1109/EMRTS.2001.934032	fair-share scheduling;unix architecture;fixed-priority pre-emptive scheduling;communications protocol;parallel computing;real-time computing;computer science;operating system;concurrency control;two-level scheduling;unix;round-robin scheduling;scheduling	Embedded	-13.236484716129695	45.194500106974274	176815
5efc89315522aeafa457d1b22c6101f658837534	parallel sequence mining on shared-memory machines	algorithme rapide;parallelisme;algoritmo paralelo;controle vibration;equilibrado;base donnee;parallel algorithm;shared memory;red www;search space;equilibrio de carga;memoria compartida;equilibrage charge;database;base dato;sequential patterns;data mining;sequence mining;algorithme parallele;synchronisation;parallelism;paralelismo;association rule;synchronization;fast algorithm;vibration control;decouverte connaissance;balancing;load balancing;world wide web;frequent sequences;reseau www;sincronizacion;sequential pattern;algoritmo rapido;equilibrage;temporal association rules;memoire partagee;control vibracion;knowledge discovery	We present pSPADE, a parallel algorithm for fast discovery of frequent sequences in large databases. pSPADE decomposes the original search space into smaller suffix-based classes. Each class can be solved in main-memory using efficient search techniques and simple join operations. Furthermore, each class can be solved independently on each processor requiring no synchronization. However, dynamic interclass and intraclass load balancing must be exploited to ensure that each processor gets an equal amount of work. Experiments on a 12 processor SGI Origin 2000 shared memory system show good speedup and excellent scaleup results. 2001 Academic Press	computer data storage;data parallelism;database;experiment;fits;gigabyte;load balancing (computing);locality of reference;message passing;microsoft windows;parallel algorithm;parallel computing;recursion;shared memory;speedup;symmetric multiprocessing	Mohammed J. Zaki	2001	J. Parallel Distrib. Comput.	10.1006/jpdc.2000.1695	synchronization;parallel computing;real-time computing;computer science;artificial intelligence;operating system;knowledge extraction;algorithm	HPC	-17.235163273093146	44.78512785233083	176818
a34dfe1f32224b035b3f5443de9a82497f169316	solving irregular inter-processor data dependency in image understanding tasks	tratamiento paralelo;analisis imagen;comprension imagen;vision ordenador;image processing;traitement parallele;efficient algorithm;image understanding;distributed memory machine;procesamiento imagen;satisfiability;traitement image;computer vision;data dependence;estructura datos;high performance computer;comprehension image;distributed search;image analysis;vision ordinateur;structure donnee;geometric constraints;image comprehension;analyse image;data structure;parallel processing	It is challenging to parallelize the problems with irregular computation and communication. In this paper, we proposed a scalable and efficient algorithm for solving irregular inter-processor data dependency in image understanding tasks on distributed memory machines. Depending on the input data, in the scatter phase, each processor distributes search requests to collect the remote data satisfying certain geometric constraints. In the gather phase, the requested remote data are collected by using a DataZone data structure and are returned to each processor. For demonstrating the usefulness of our algorithm, we conducted experiments on an IBM SP2. The experimental results were consistent with the theoretical analyses, and showed the scalability and efficiency of the proposed algorithm. Our code using C and MPI is portable onto other High Performance Computing(HPC) platforms.		Yongwha Chung;Jin-Won Park	1999		10.1007/3-540-49164-3_22	parallel processing;computer vision;image analysis;data structure;image processing;computer science;artificial intelligence;theoretical computer science;operating system;database;distributed computing;algorithm;satisfiability	ML	-15.891285784264277	42.560382656103336	177890
4b502fb74ad7be6cd7b2ce02d818683a7374f3e8	parallel programming and the poker programming environment	computer languages;programming environments;concurrent computing;programming environment;specifications;computer aided instruction;parallel programming programming environments concurrent computing programming profession parallel algorithms writing operating systems computer aided instruction software systems computer languages;software systems;parallel programming;front end processors;computer programming;programming profession;interactions;writing;algorithms;parallel programs;parallel processing;programming languages;operating systems;parallel algorithms	Since there are few parallel computers in existence, programmers who have written and run a parallel program are rare.' Yet, as parallel computers become more widely available in response to recently recognized critical needs, 1-2 the number of programmers developing parallel programs is sure to grow. Although parallel programming is quite different from the familiar sequential programming, it is nonetheless straightforward and understandable. To demonstrate this, we begin by establishing what the programmer must accomplish in parallel programming and then analyze how programs might be developed in a particular parallel programming environment. t This article presents an overview of the Poker parallel programming environment that has been developed to support the Configurable, Highly Parallel, or CHiP, computer. 3 The Poker environment runs on a front-end sequential computer (Vax 11/780) and serves as a comprehensive system for writing and running parallel programs. Poker is sufficiently general that, with minor modification , it could be a parallel programming environment for any of a half-dozen recently-proposed ensemble parallel computers. 4 The parallel programming activity Before building a parallel programming environment, one must analyze the programming activity to determine what can be included to simplify programming and what must be excluded to avoid making it hard. *In fact, it was possible to track down virtually everyone who ever programmed the Illiac IV. 5 t A parallel programming environment is the collection of all language and operating system facilities needed to support parallel programming integrated together into a single system. What do programmers do? Programming, either sequential or parallel, is the conversion of an abstract (machine independent) algorithm into a form called apro-gram, that can be run on a particular computer. The algorithm is an abstraction describing a process that could be implemented on many machines. The program is an implementation of the algorithm for a particular machine. Programming is a conversion activity and, as such, will be easy or difficult depending on whether the algorithmic form is similar or dissimilar to the desired program form. But what are the sources of dissimilarity between algorithm and program? First, algorithms are abstractions whose generality is intended to transcend the specifics of any implementation. As a result, when an algorithm is specified in the technical literature, many details are purposely omitted or, at best, merely implied because they have little or no bearing on the operation of the algorithm. The omitted details must be defined in the …	abstract machine;algorithm;computer;concurrent computing;illiac iv;integrated development environment;lambda calculus;microprocessor;n-gram;operating system;parallel computing;programmer;vax	L. Snyder	1984	Computer	10.1109/MC.1984.1659183	parallel processing;constraint programming;protocol;declarative programming;concurrent computing;programming domain;reactive programming;functional reactive programming;computer science;theoretical computer science;extensible programming;computer programming;distributed computing;programming paradigm;procedural programming;symbolic programming;inductive programming;fifth-generation programming language;programming language;writing;concurrent object-oriented programming;parallel programming model	PL	-13.222368164633487	39.37014021184465	178040
07bbf3a2de43595f5f7358d16011bb14289e2bd1	a collective i/o scheme based on compiler analysis	optimization technique;programacion paralela;programme entree sortie;parallel programming;optimizacion compiladora;specification programme;evaluation partielle compilateur;performance programme;input output program;programa entrada salida;pattern matching;compiler optimization;data access;eficacia programa;parallel i o;program performance;program specification;optimisation compilateur;especificacion programa;programmation parallele;partial evaluation compilers	Current approaches to parallel I/O demand extensive user effort to obtain acceptable performance. This is in part due to difficulties in understanding the characteristics of a wide variety of I/O devices and in part due to inherent complexity of I/O software. While parallel I/O systems provide users with environments where persistent datasets can be shared between parallel processors, the ultimate performance of I/O-intensive codes depends largely on the relation between data access patterns and storage patterns of data in files and on disks. In cases where access patterns and storage patterns match, we can exploit parallel I/O hardware by allowing each processor to perform independent parallel I/O. To handle the cases in which data access patterns and storage patterns do not match, several I/O optimization techniques have been developed in recent years. Collective I/O is such an optimization technique that enables each processor to do I/O on behalf of other processors if doing so improves the overall performance. While it is generally accepted that collective I/O and its variants can bring impressive improvements as far as the I/O performance is concerned, it is difficult for the programmer to use collective I/O in an optimal manner.#R##N##R##N#In this paper, we propose and evaluate a compiler-directed collective I/O approach which detects the opportunities for collective I/O and inserts the necessary I/O calls in the code automatically. An important characteristic of the approach is that instead of applying collective I/O indiscriminately, it uses collective I/O selectively, only in cases where independent parallel I/O would not be possible. We have implemented the necessary algorithms in a source-to-source translator and within a stand-alone tool. Our experimental results demonstrate that our compiler-directed collective I/O scheme performs very well on different setups built using nine applications from several scientific benchmarks.	input/output;optimizing compiler	Mahmut T. Kandemir	2000		10.1007/3-540-40889-4_1	data access;real-time computing;simulation;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;pattern matching;optimizing compiler;database;distributed computing;programming language;computer security;algorithm	HPC	-16.271094326516643	43.62930595135341	178509
0d97ab1dc60d387c2b25a788628912d89f89aa53	an evaluation of java's i/o capabilities for high-performance computing	general and miscellaneous mathematics computing and information science;bulk;performance;i o;high performance computer;evaluation;fortran;security;high performance;programming;asynchronous;java	ABSTRACT Java is quickly becoming the preferred language for writing distributed applications because of its inherent support for programming on distributed platforms. In particular, Java provides compile-time and run-time security, automatic garbage collection, inherent support for multithreading, support for persistent objects and object migration, and portability. Given these signi cant advantages of Java, there is a growing interest in using Java for high-performance computing applications. To be successful in the high-performance computing domain, however, Java must have the capability to e ciently handle the signi cant I/O requirements commonly found in high-performance computing applications. While there has been signi cant research in high-performance I/O using languages such as C, C++, and Fortran, there has been relatively little research into the I/O capabilities of Java. In this paper, we evaluate the I/O capabilities of Java for high-performance computing. We examine several approaches that attempt to provide high-performance I/O| many of which are not obvious at rst glance|and investigate their performance in both parallel and multithreaded environments. We also provide suggestions for expanding the I/O capabilities of Java to better support the needs of high-performance computing applications.	c++;compile time;compiler;distributed computing;fortran;garbage collection (computer science);input/output;java;multithreading (computer architecture);requirement;supercomputer;thread (computing)	Phillip M. Dickens;Rajeev Thakur	2000		10.1145/337449.337462	parallel computing;java concurrency;computer science;theoretical computer science;interface;strictfp;embedded java;real time java;javabeans;programming language;java;generics in java;scala;java annotation	HPC	-13.293131525559973	39.872912135344066	178727
d98f7fc175ee773d781d4de5ce2aa6b66cbce02a	the improved raid 5 with the disk cache using the load-balanced destage algorithm	memory layout;gestion memoire;algorithm performance;storage management;algorithme;algorithm;organizacion memoria;computer architecture;gestion memoria;architecture ordinateur;resultado algoritmo;organisation memoire;performance algorithme;load balance;arquitectura ordenador;disk array;algoritmo	Write requests are written from the disk cache to the disk array by the destage algorithm and the response time of host read request dominates the performance of the RAID 5. Since RAID is composed of multiple disk, the overal performance at the disk array level is more important than the performance at a disk level. However, previous destage algorithms do not take into consideration the overall performance of the disk array but the optimal performance at an each individual disk, and it may eventually decreases the RAID 5 performance. This paper proposes the load-balanced destage algorithm adopted in the RAID 5 at the disk array level, and shows that the proposed destage algorithm has higher performance than other previous destage algorithms.	algorithm;page cache;standard raid levels	Yun Seok Chang;Bo Yeon Kim	1998		10.1007/BFb0037246	embedded system;parallel computing;it8212;disk array;computer science;disk array controller;load balancing;operating system;disk mirroring;disk buffer;algorithm;nested raid levels;standard raid levels;raid processing unit	Robotics	-16.530651504787368	44.914016918064185	179436
bfa882bcb0052c05ce6cf2ebdf1e9f61da8a1ca5	instrumentation database for performance analysis of parallel scientific applications	scientific application;eficacia sistema;architecture systeme;programacion paralela;product code;sistema informatico;performance systeme;parallel programming;computer system;system performance;performance programme;performance analysis;arquitectura sistema;eficacia programa;systeme informatique;program performance;information system;sparse matrix;system architecture;systeme information;programmation parallele;sistema informacion	Parallel codes, especially scientific applications, have increased the need for performance analysis tools. In this paper we present an approach that addresses some of the difficulties inherent to the analysis of parallel codes. To this end we examine the three components that comprise performance analysis: instrumentation, visualization, and analysis. The essence of our approach is to use a database to capture instrumentation data. We discuss how this approach influences the design of performance analysis components. We further motivate our approach with an example using a sparse matrix vector product code. We conclude by discussing some of the key implementation issues and outlining future plans.	profiling (computer programming)	Jeffrey Nesheiwat;Boleslaw K. Szymanski	1998		10.1007/3-540-49530-4_17	simulation;sparse matrix;computer science;computer performance;universal product code;operations research;information system;systems architecture	HPC	-17.24800864564433	41.47744993596361	179674
c1b14e6f98ce1184407e33d9ef3bfbdd1c75a34b	expression et optimisation des réorganisations de données dans du parallélisme de flots. (expression and optimization of data reorganizations in dataflow parallelism)			dataflow;mathematical optimization;parallel computing	Pablo De Oliveira Castro Herrero	2010				EDA	-17.08910049088428	39.5178805737462	180593
890ad9d3689828c9b29406960f0bde88e7af454f	a combined virtual shared memory and network which schedules	auto- matic data distribution;cache only memory architecture;interconnections networks;automatic scheduling.;multi- threaded architecture	In this paper, we follow a new path to arrive at the idea of a COMA — a Cache Only Memory Architecture. We show how the evolution of another architecture (ADARC) leads quite naturally to the concept of a self-distributing data space, where the separate local memories, once augmented with special associative hardware, function as giant caches. Having come to the idea of COMA from an unusual route, we can cast a new perspective on this architectural concept, and show (1) how the same hardware that augments the memories can implement a high performance network from off-the-shelf components; (2) how this hardware can reduce false sharing by implementing variable size “cache lines”; and (3) most importantly, how a COMAbased self-distributingdata space, combined with structures from multithreaded architectures, leads naturally to automatic scheduling. The network thus provides a virtual shared memory where data and computation distribute themselves. This self-distribution is fully transparent to the processors and the programmer.	active message;cache (computing);cache-only memory architecture;central processing unit;computation;computer architecture;dataflow;dataspaces;distributed computing;emulator;false sharing;java;model of computation;parallel computing;programmer;programming paradigm;scheduling (computing);shared memory;telecommunications network;thread (computing)	Ronald Charles Moore;Bernd Klauer;Klaus Waldschmidt	1997			cache coloring;data diffusion machine;parallel computing;distributed shared memory;cache-only memory architecture;shared disk architecture;distributed memory;uniform memory access;computer science;shared memory	Arch	-11.96026320141902	46.09702414403954	180605
643ef1227ad2ccc34ecb3d1d82fe9776c2d550bb	the development of the data-parallel gpu programming language cgis	parallelisme;lenguaje programacion;algoritmo paralelo;data parallel;parallel algorithm;programming language;carte graphique;gpu programming;algorithme parallele;parallelism;paralelismo;design and implementation;langage programmation;graphic processing unit;langage parallele;unidad de proceso grafico;parallel languages	In this paper, we present the recent developments on the design and implementation of the data-parallel programming language CGiS. CGiS is devised to facilitate use of the data-parallel resources of current graphics processing units (GPUs) for scientific programming.	computer graphics;data parallelism;graphics processing unit;parallel computing;parallel programming model;programming language	Philipp Lucas;Nicholas Fritz;Reinhard Wilhelm	2006		10.1007/11758549_31	parallel computing;computer science;parallel algorithm;programming language;general-purpose computing on graphics processing units;algorithm	PL	-16.13146296119797	41.03104958340907	180667
448340ed2365222b42098cfd13a107ea2b4ec9e1	discussing hpf design issues	high performance fortran	As High Performance Fortran (HPF) is being both developed and redesigned by the HPF Forum, it is important to provide comprehensive criteria for analyzing HPF features. This paper presents such criteria related to three aspects: adequacy to applications, aesthetic and soundness in a language, and implementability. Some features already in HPF or being currently discussed are analyzed according to these criteria. They are shown as not balanced. Thus new or improved features are suggested to solve the outlined deeciencies: namely a scope provider, multiple mapping declarations and simpler remappings.	high performance fortran	Fabien Coelho	1996		10.1007/3-540-61626-8_77	computational science;computer architecture;parallel computing;computer science	HPC	-14.89178429949273	40.30856113529413	180824
56a22815870767f6c5d8d1d97f5fc4384fc12f58	a high-level interpreted mpi library for parallel computing in volunteer environments	libraries;parallel computing;volunteer computing;parallel programming;volatile resources volunteer computing parallel computing;receivers;large scale;servers;c language;redundancy;operating system;fortran language mpi library message passing interface parallel computing volunteer environment volpex tool parallel execution on volatile nodes process redundancy message logging python linux platform windows platform c language;parallel programming c language fortran linux message passing;process control;parallel computer;message passing;libraries parallel processing concurrent computing operating systems usability master slave large scale systems message passing security distributed computing;linux;fortran;parallel programs;benchmark testing;volatile resources;message logging	Idle desktops have been successfully used to run sequential and master-slave task parallel codes on a large scale in the context of volunteer computing. However, execution of message passing parallel programs in such environments is challenging because a pool of nodes to execute an application may have architectural and operating system heterogeneity, can include widely distributed nodes across security domains, and nodes may become unavailable for computation frequently and without warning. The VolPEx (Parallel Execution on Volatile Nodes) tool set is building MPI support in such environments based on selective use of process redundancy and message logging. However, addressing this challenge requires tradeoffs between performance, portability, and usability. The paper introduces a robust MPI library that is designed to be highly portable across heterogeneous architectures and operating systems. This VolpexPyMPI library is built with Python, works with Linux and Windows platforms and accepts user level MPI programs written in C or FORTRAN. The performance of VolpexPyMPI is compared with a traditional C based implementation of MPI. The paper examines in detail the tradeoffs of these usability focused and performance focused approaches.	algorithm;application checkpointing;benchmark (computing);code;computation;computer cluster;computer memory;data structure;desktop computer;fault tolerance;fortran;in-memory database;interpreted language;linux;linux;message passing interface;microsoft windows;nas parallel benchmarks;operating system;overhead (computing);parallel computing;python;requirement;seamless3d;software portability;transaction processing system;usability;vii;volunteer computing;xml;xml-rpc	Troy P. LeBlanc;Jaspal Subhlok;Edgar Gabriel	2010	2010 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing	10.1109/CCGRID.2010.85	benchmark;parallel computing;message passing;computer science;message passing interface;operating system;process control;database;distributed computing;redundancy;linux kernel;server	HPC	-12.511174988497727	44.62552088832078	181334
3c02ac7cf080b6e640940d063b9006349f300b88	distributed xml processing over various topologies - pipeline and parallel processing characterization		This paper characterizes distributed XML processing on networking nodes. XML documents are sent from a client node to a server node through relay nodes, which process the documents before arriving at the server. According as the node topology, the XML documents are processed in a pipelining manner or a parallel fashion. We evaluate distributed XML processing with synthetic and realistic XML documents on real and virtual environments. Characterization of well-formedness and grammar validation processing via pipelining and parallel models reveals inherent advantages of the parallel processing model.	central processing unit;distributed computing;document processing;intelligent network;mobile device;overhead (computing);parallel computing;pipeline (computing);relay;server (computing);stream (computing);streaming media;synthetic intelligence;virtual reality;web server;xml	Yoshiyuki Uratani;Hiroshi Koide;Dirceu Cavendish;Yuji Oie	2012			parallel computing;database;distributed computing	DB	-12.291774038732747	43.67468979430337	181505
bca2d744857af64f5387869bbe84304b432d1f9f	distributed file system for clusters and grids	file transfer;distributed memory;distributed system;data transmission;entrada salida;haute performance;systeme reparti;plagio;memoria compartida;pervasive computing;transferencia fichero;distributed storage;input output;grid;informatica difusa;software architecture;sistema repartido;transfert fichier;informatique diffuse;rejilla;transmission donnee;plagiat;alto rendimiento;grille;distributed file system;functionality;fonctionnalite;memoire repartie;high performance;funcionalidad;architecture logiciel;transmision datos;data transfer;copying;entree sortie	NFSG aims at providing a solution for file accesses within a cluster of clusters. Criteria of easiness (installation, administration, usage) but also efficiency as well as a minimal hardware and software intrusivity have led our developments. By using several facilities such as distributed file systems (NFSP) and a high-performance data transfer utility (GXfer), we hope to offer a software architecture fully compatible with the ubiquitous NFS protocol. Thanks to a distributed storage (especially multiple I/O servers provided by NFSP), several parallel streams may be used when copying a file from one cluster to another within a same grid. This technique improves data transfers by connecting distributed file system at both ends. The GXfer component implements this functionality. Thus, performances only reachable with dedicated and expensive hardware may be achieved.	clustered file system;computer cluster;dce distributed file system;grid computing;high performance file system;input/output;load balancing (computing);prototype;server (computing);software architecture	Olivier Valentin;Pierre Lombard;Adrien Lèbre;Christian Guinet;Yves Denneulin	2003		10.1007/978-3-540-24669-5_142	fork;self-certifying file system;embedded system;parallel computing;distributed data store;computer science;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;database;distributed computing;open;distributed file system;computer security;ubiquitous computing;data transmission	HPC	-19.104765141445423	44.49976575324165	182103
37497c27273bf5b602ea62893bf48c30e9b9a58a	performance evaluation of jpvm	tratamiento paralelo;juste a temps;bibliotheque;distributed memory;distributed system;virtual machine;evaluation performance;communication libraries;architecture systeme;systeme reparti;compilateur;performance evaluation;traitement parallele;heterogeneous systems;ordinateur parallele;programacion paralela;memoria compartida;evaluacion prestacion;parallel programming;cote nivellement;machine virtuelle;compiler;parallel computation;calculo paralelo;sistema repartido;pvm;parallel programming environment;bench mark;benchmarks;ordenador paralelo;parallel computer;message passing;arquitectura sistema;just in time;justo en tiempo;memoire repartie;system architecture;maquina virtual;biblioteca;calcul parallele;library;cota nivelacion;parallel processing;compilador;programmation parallele;java	PVM for Java (JPVM) is a parallel programming environment that provides a set of Parallel Virtual Machine (PVM)-like class libraries developed using Java. JPVM inherits the attributes of Java, especially Java support of heterogeneous system. However, it also has the disadvantage of poor performance when compared to native codes. Java performance has been considerably improved with the recent introduction of Just in Time (JIT) compilers. This paper evaluates JPVM, with and without JIT compilation, using some well-known parallel processing benchmarks. It will also highlight some of the areas that limit the use of Java in parallel processing on distributed systems. The objective of the paper is to provide some performance indications of using Java in network-based parallel computing environments.	performance evaluation	Bu-Sung Lee;Yan Gu;Wentong Cai;Alfred Heng	1999	Parallel Processing Letters	10.1142/S0129626499000372	parallel processing;compiler;parallel computing;message passing;real-time computing;distributed memory;benchmark;java concurrency;library;computer science;virtual machine;operating system;strictfp;embedded java;distributed computing;real time java;programming language;java;scala;java annotation	HPC	-16.651991219148712	42.5705222628855	182474
5174a86fc2ca88e8bb3882542d6146740f1b420c	hybrid simulation models of computer systems	performance evaluation;system modeling;simulation;model performance;hybrid model;queueing network model;model of computation;queueing network models;hybrid simulation;central server model;discrete event simulation	This paper describes the structure and operation of a hybrid simulation model in which both discrete-event simulation and analytic techniques are combined to produce efficient yet accurate system models. In an example based on a simple hypothetical computer system, discrete-event simulation is used to model the arrival and activation of jobs, and a central-server queueing network models the use of system processors. The accuracy and efficiency of the hybrid technique are demonstrated by comparing the result and computational costs of the hybrid model of the example with those of an equivalent simulation-only model.	central processing unit;computation;computer;queueing theory;server (computing);simulation	Herb Schwetman	1978	Commun. ACM	10.1145/359588.359594	model of computation;real-time computing;simulation;systems modeling;computer science;theoretical computer science;discrete event simulation;layered queueing network;hybrid system;network traffic simulation	Metrics	-13.600106901558393	42.265644320532616	182583
1075a266e3960470dbb32b08df114a4a303c0166	performance of a modular interactive data analysis system (midas)	processing element;communication system;general and miscellaneous mathematics computing and information science;multiprocessor systems;performance;data analysis;array processors;data flow processing;programming 990200 mathematics computers;data flow;architecture;parallel processing	A processor cluster, part of a multiprocessor system named MIDAS (modular interactive data analysis system), has been constructed and tested. The architecture permits considerable flexibility in organizing the processing elements for different applications. The current tests involved 8 general CPUs from commercial computers, 2 special purpose pipelined processors and a specially designed communications system. Results on a variety of programs indicated that the cluster performs from 8 to 16 times faster than a standard computer with an identical CPU. The range represents the effect of differing CPU and I/O requirements-ranging from CPU intensive to I/O intensive. A benchmark test indicated that the cluster performed at approximately 85percent the speed of the CDC 7600. Plans for further cluster enhancements and multicluster operation are discussed. 5 references.		Creve Maples;Daniel Weaver;Douglas Logan;William Rathbun	1983			parallel processing;computer architecture;parallel computing;real-time computing;performance;computer science;architecture;operating system;data analysis;programming language;cpu shielding	HCI	-11.946713278037375	44.80138390304931	182725
0a46c07da2f75fcd427691b8b8e18d239657359b	experiment of multithreading symbolic and algebraic computations with openmp	symbolic computation;gestion memoire;shared memory;memory management;multiprocessor;cout developpement;memoria compartida;storage management;development cost;program library;calculo simbolico;gestion memoria;funcion matematica;bibliotheque programme;multithread;mathematical function;fonction mathematique;multitâche;multiprocesador;calcul symbolique;multitarea;biblioteca programa;memoire partagee;multiprocesseur	This paper describes the current status of a project for multithreading algebraic computations, which aims at the utilization of today’s high-spec PCs with hyperthreading or dual-core technologies. Our effort is done by applying OpenXM with minimal cost of development, and includes memory management in multithreaded environment. Our empirical results show that the performance gain can be attained in numeric cases and in some cases of purely symbolic computations.	multithreading (computer architecture);openmp;thread (computing)	Hirokazu Murao	2006		10.1007/11832225_44	shared memory;parallel computing;symbolic computation;multiprocessing;computer science;theoretical computer science;operating system;mathematics;function;algorithm;memory management	HPC	-15.302751722145187	43.07424743161044	183236
74e01366bb6718712bb76a292bc365a14bcb6034	online querying of d-dimensional hierarchies	workload;hachage;distributed memory;distributed system;grain size;sobrecarga;systeme reparti;mise a jour;distributed hash table;on line;memoria compartida;par a par;en linea;interrogation base donnee;interrogacion base datos;distributed storage;multidimensional data;prior knowledge;almacen dato;systeme adaptatif;actualizacion;hashing;sistema repartido;poste a poste;indexing;grosor grano;surcharge;indexation;adaptive system;indizacion;charge travail;data warehousing;sistema adaptativo;concept hierarchy;en ligne;it management;experimental evaluation;entrepot donnee;data warehouse;memoire repartie;carga trabajo;overload;distributed hash tables;peer to peer;concept hierarchies;database query;updating;grosseur grain	In this paper we describe a distributed system designed to efficiently store, query and update multidimensional data organized into concept hierarchies and dispersed over a network. Our system employs an adaptive scheme that automatically adjusts the level of indexing according to the granularity of the incoming queries, without assuming any prior knowledge of the workload. Efficient roll-up and drill-down operations take place in order to maximize the performance by minimizing query flooding. Updates are performed on-line, with minimal communication overhead, depending on the level of consistency needed. Extensive experimental evaluation shows that, on top of the advantages that a distributed storage offers, our method answers the vast majority of incoming queries, both point and aggregate ones, without flooding the network and without causing significant storage or load imbalance. Our scheme proves to be especially efficient in cases of skewed workloads, even when these change dynamically with time. At the same time, it manages to preserve the hierarchical nature of data. To the best of our knowledge, this is the first attempt towards the support of concept hierarchies in DHTs. © 2010 Elsevier Inc. All rights reserved.	aggregate data;clustered file system;data drilling;distributed computing;keyboard technology;online and offline;overhead (computing);scheme	Katerina Doka;Dimitrios Tsoumakos;Nectarios Koziris	2011	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2010.10.005	search engine indexing;parallel computing;hash function;distributed memory;computer science;chord;theoretical computer science;operating system;data warehouse;data mining;database;distributed computing;algorithm;grain size	DB	-18.9628017027604	45.784933993842586	183269
0cba6ff7b26c9d0bd73f2b6cd99c1106e3543518	a simulation study of decoupled architecture computers	pipelined processors;computers;digital computers;scientific application;evaluation performance;time dependent;national organizations;processing;general and miscellaneous mathematics computing and information science;lawrence livermore laboratory;storage access;performance evaluation;execution time;memoria acceso;us organizations 990200 mathematics computers;estudio comparativo;queue length;evaluacion prestacion;sistema informatico;performance;simulation;data processing;decoupled architectures;simulacion;computer system;architecture decouplee;supercomputers decoupled architectures performance evaluation pipelined processors scientific computers;supercomputer;aplicacion cientifica;supercomputador;memory access;etude comparative;computer architecture;performance improvement;time dependence;us aec;data flow processing;computerized simulation;comparative study;acces memoire;delai d execution;us erda;simulation study;temps execution;procesador oleoducto;application scientifique;plazo ejecucion;optimization;systeme informatique;us doe;task scheduling;data flow;processeur pipeline;tiempo ejecucion;scientific computers;instructions per clock;architecture;programming;simulation model;memory devices;queues;supercomputers;cray computers;time allowed;pipeline processor;superordinateur;dynamic scheduling	Decoupled architectures achieve high scalar performance by cleanly splitting instruction processing into memory access and execution tasks. Several decoupled architectures have been proposed, and they all have two characteristics in common: 1) they have two separate sets of instructions, one for accessing memory and one for performing function execution. 2) The memory accessing task and the execution task communicate via architectural queues.	access time;cas latency;clock rate;clock signal;cray-1;out-of-order execution;simulation;unbounded nondeterminism	James E. Smith;Shlomo Weiss;Nicholas Y. Pang	1986	IEEE Transactions on Computers	10.1109/TC.1986.1676820	embedded system;programming;computer architecture;supercomputer;parallel computing;real-time computing;data processing;performance;computer science;processing;architecture;operating system;programming language;algorithm	Arch	-14.901412365389966	43.86852896930115	183586
b7172ae8ea0f604ef159707af7883981cbfe5f8e	parallel query execution in prisma/db		1 I n t r o d u c t i o n In the PRISMA-project, a large multi-processor system has been built, is be used to study the performance gains from parallelism. A parallel, main-memory relational database system (PItlSMA/DB) runs on this so-called POOMA-machine. This paper studies the possibilities of using parallelism to improve the performance of relational database management systems. Because the equi-join is an important, and time-consuming operation, queries consisting of a number of equi-joins are used to describe how different forms of parallelism can speed up the execution of such queries. This paper is organized as follows: First, a brief introduction into PRISMA and the DBMS running on it is given. After that, different forms of parallelism are described and the ways in which they can be used is identified. Using this knowledge, the possible parallelism in the execution of join-queries is discussed. Special attention is paid to pipelining. It is shown, that pipelining needs a new hash-join algorithm and that using this algorithm may yield effective parallelism over a pipeline of join operations. Finally, we discuss the implications of using pipelining as a source of parallelism for the optimization of join queries. The paper is concluded with our plans for future work. The PaRallel Inference and Storage MAchine PRISMA is a highly parallel machine for data and knowledge processing. The PRISMA-ma~hine contains 100 nodes that each contain a data processor, a communication processor and 16 Mbyte of local memory. 50 nodes have a disk a~d some nodes have an ethernet card that provides an interface with a host computer. Each communication processor connects a node to 4 other nodes. In this way a fast, high-bandwidth network is provided. This hardware can be classified as a shared-nothing multi-processor system. The maz~ine is designed to support a relational main memory database management system PRISMA/DB. An extensive introduction to this system can be found in [Kers87] and in [Wils89]. Here, only the features that are important for this paper are summarized. PRISMA/DB stores the entire database in main memory. Disks are used for backup only. To gain performance and to make storage in main memory feasible, the tuples belonging to one relation are fragmented over more than one node. A fragment is a set of tuples that belong to the same relation and that reside on the same node. A relation does not necessarily use all available nodes. …	algorithm;backup;computer data storage;decibel;floppy disk;host (network);in-memory database;join (sql);mathematical optimization;megabyte;multiprocessing;network interface controller;parallel computing;pipeline (computing);prisma;relation (database);relational database management system;shared nothing architecture	Annita N. Wilschut;Peter M. G. Apers;Jan Flokstra	1990		10.1007/3-540-54132-2_69	parallel computing;theoretical computer science;programming language	DB	-12.65407129157541	46.08606297633091	184493
c2f01a00829c926354429e5413b173a80cb3f69b	allocation wall: a limiting factor of java applications on emerging multi-core platforms	estensibilidad;correlacion;microprocessor;largeur bande;gestion memoire;multi core processor;haute performance;storage access;limiting factor;storage management;processeur multicoeur;disparity;processus leger logiciel;metric;langage java;procesador multinucleo;allocation;disparidad;proceso ligero logicial;gestion memoria;object oriented;anchura banda;acces memoire;thread software;alto rendimiento;acceso memoria;bandwidth;oriente objet;metrico;lenguaje java;multicore processor;microprocesseur;extensibilite;scalability;correlation;experimentation;orientado objeto;high performance;microprocesador;disparite;performance allocation;metrique;java language;java	"""Multi-core processors are widely used in computer systems. As the performance of microprocessors greatly exceeds that of memory, the memory wall becomes a limiting factor. It is important to understand how the large disparity of speed between processor and memory influences the performance and scalability of Java applications on emerging multi-core platforms.  In this paper, we studied two popular Java benchmarks, SPECjbb2005 and SPECjvm2008, on multi-core platforms including Intel Clovertown and AMD Phenom. We focus on the """"partially scalable"""" benchmark programs. With smaller number of CPU cores these programs scale perfectly, but when more cores and software threads are used, the slope of the scalability curve degrades dramatically.  We identified a strong correlation between scalability, object allocation rate and memory bus write traffic in our experiments with our partially scalable programs. We find that these applications allocate large amounts of memory and consume almost all the memory write bandwidth in our hardware platforms. Because the write bandwidth is so limited, we propose the following hypothesis: the scalability and performance is limited by the object allocation on emerging multi-core platforms for those objects-allocation intensive Java applications, as if these applications are running into an """"allocation wall"""".  In order to verify this hypothesis, several experiments are performed, including measuring key architecture level metrics, composing a micro-benchmark program, and studying the effect of modifying some of the """"partially scalable"""" programs. All the experiments strongly suggest the existence of the allocation wall."""	benchmark (computing);binocular disparity;central processing unit;experiment;intel core (microarchitecture);java;memory bus;microprocessor;multi-core processor;random-access memory;scalability;the wall street journal	Yi Zhao;Jin Shi;Kai Zheng;Haichuan Wang;Haibo Lin;Ling Shao	2009		10.1145/1640089.1640116	multi-core processor;parallel computing;real-time computing;computer science;operating system;programming language	Metrics	-16.435780636214403	43.005553214318596	184766
4f0c125ff0ac68e256989d5b7a3f0c997fa7614e	"""modeling and analysis of """"hot spots"""" in an asynchronous nxn crossbar switch"""	hot spot	• How is memory access controlled? – Exclusive Read, Exclusive Write (EREW) • only one processor can access a memory location at one time – Concurrent Read, Exclusive Write (CREW) • multiple processors can read a memory location, but only one at a time can write to it – Exclusive Read, Concurrent Write (ERCW) • multiple processors can write to a memory location but reads are exclusive – Concurrent Read, Concurrent Write (CRCW) • processors may freely read or write to memory locations without restriction	central processing unit;crossbar switch;exclusive or;memory address;parallel random-access machine	Eugene Pinsky;Paul A. Stirpe	1991			distributed computing;parallel computing;asynchronous communication;computer science;crossbar switch	Logic	-12.49998576195165	45.54111871346023	184810
43b99dd15dcb1d952c6b559c240b84022adb930a	access normalization: loop restructuring for numa compilers	distributed system;remote access;nonsingular loop transformation;lattice theory;systeme reparti;compilateur;storage access;multiprocessor;programacion paralela;generation code;data locality;performance;parallelizing compilers;generacion codigo;code generation;parallel programming;compiler;data distribution;algorithme;memory access;algorithm;computer architecture;sistema repartido;nonuniform memory access machines;architecture ordinateur;loop transformation;basic linear algebra subprograms;acces memoire;acceso memoria;parallel machines;arquitectura ordenador;fortran;remote memory access;rendimiento;multiprocesador;compilador;programmation parallele;algoritmo;multiprocesseur	In scalable parallel machines, processors can make local memory accesses much faster than they can make remote memory accesses. Additionally, when a number of remote accesses must be made, it is usually more efficient to use block transfers of data rather than to use many small messages. To run well on such machines, software must exploit these features. We believe it is too onerous for a programmer to do this by hand, so we have been exploring the use of restructuring compiler technology for this purpose. In this article, we start with a language like HPF-Fortran with user-specified data distribution and develop a systematic loop transformation strategy called access normalization that restructures loop nests to exploit locality and block transfers. We demonstrate the power of our techniques using routines from the BLAS (Basic Linear Algebra Subprograms) library. An important feature of our approach is that we model loop transformation using invertible matrices and integer lattice theory.	blas;central processing unit;compiler;high performance fortran;linear algebra;locality of reference;loop optimization;programmer;scalability;subroutine	Wei Li;Keshav Pingali	1993	ACM Trans. Comput. Syst.	10.1145/161541.159766	compiler;parallel computing;real-time computing;multiprocessing;performance;computer science;theoretical computer science;operating system;lattice;basic linear algebra subprograms;distributed computing;programming language;code generation	Arch	-16.15035713997431	42.25503077113882	185040
b43a9bca2681ef19490a77f971cec49a791cd6fa	étude de deux solutions pour le support matériel de la programmation parallèle dans les multiprocesseurs intégrés : vol de travail et mémoires transactionnelles. (study of two solutions for hardware support of parallel programming in integrated multiprocessors: work-stealing and transactional memory		The arrival of multiprocessor chips rises again some questions about the way of writing programs, which must then include a high degree of parallelism. We tackle this problem via two orthogonal approaches. First, via the work-stealing paradigm, for which we perform a study targeting on the first hand to seek for simple architectural characteristics giving the best performances for an implementation of this paradigm ; and on the second hand to show that the overhead compared to a static parallelization is low, while allowing performances improvement thanks to dynamic load balancing. This question is nevertheless especially tackled via the transaction based programming paradigm – sequence of instructions executing atomically from the other cores’ point of view. Supporting this abstraction requires the implementation of a system called TM, often complex, either software or hardware. The study focuses first on the comparison between two hardware TM systems based on different architecture choices (cache coherence protocol), and then on the impact on performances of several conflict resolution policies, in other words the actions to be taken when two or more transactions try to access the same pieces of data.		Quentin L. Meunier	2010				Arch	-14.250426362573902	45.10558163319889	186413
b609b537639b80736fe448c38fb7d0ba49c4b4ab	a common framework for inter-process communication on a cluster	distributed memory;distributed system;communication process;virtual memory;cluster;systeme reparti;shared memory;api;communication interprocessus;memoria compartida;memoria virtualmente compartida;interface programme application;scaling up;ipc;memoire virtuellement partagee;proceso comunicacion;processus communication;sistema repartido;application program interfaces;memoire virtuelle;dsm;memoire repartie;distributed shared memory;memoria virtual;memoire partagee;inter process communication	Many implementations of distributed shared memories (DSMs) have been developed to date. All of the implementations have taken into consideration, the performance issues of DSM systems more than other issues like the portability of programs and API. DSM systems have APIs that fit a specific system and require a programmer's adaptation to that specific API. A migration between systems might require a significant revision of a program. Moreover, programmers who write programs for SMPs need to rewrite large portions of their programs in order to scale up their programs from SMP to a cluster of computers. We are presenting a concept for a distributed system, in which an API for any IPC mechanism will be identical, or very close to the SMP's conventional API. This concept can accustom programmers to a distributed system with ease, as well as improving the migration of existing SMP programs to distributed systems.	application programming interface;computer;distributed computing;distributed shared memory;inter-process communication;programmer;rewrite (programming);symmetric multiprocessing;system migration	Moti Geva;Yair Wiseman	2004	Operating Systems Review	10.1145/1031154.1031157	embedded system;parallel computing;computer science;operating system;programming language;inter-process communication	OS	-16.447052952891127	42.58308752686896	187604
28c1de90bf7cad8c0ba7dbd4d8586398bacfbab5	mpi i/o analysis and error detection with marmot	parallelisme;distributed system;algoritmo paralelo;virtual machine;detection erreur;deteccion error;systeme reparti;parallel algorithm;communicating process;development process;machine virtuelle;algorithme parallele;proceso comunicante;parallelism;sistema repartido;estimation erreur;paralelismo;error estimation;envoi message;processus communicant;estimacion error;message passing;parallel i o;error detection;parallel programs;maquina virtual	The most frequently used part of MPI-2 is MPI I/O. Due to the complexity of parallel programming in general, and of handling parallel I/O in particular, there is a need for tools that support the application development process. There are many situations where incorrect usage of MPI by the application programmer can be automatically detected. In this paper we describe the MARMOT tool that uncovers some of these errors and we also analyze to what extent it is possible to do so for MPI I/O.	asynchronous i/o;error detection and correction;input/output;message passing interface;parallel i/o;parallel computing;programmer	Bettina Krammer;Matthias S. Müller;Michael M. Resch	2004		10.1007/978-3-540-30218-6_36	message passing;error detection and correction;computer science;virtual machine;theoretical computer science;operating system;distributed computing;parallel algorithm;programming language;software development process;algorithm	HPC	-18.005966012311823	41.280715036149054	187934
e2c48c18b19c395b4f7a16501c22f4a7d88c70f2	accélération de la simulation modulaire		Actual processors integrate many units (computing units, m emory, control, core), either duplicated or vectorized. In order to test new concepts , architects simulate them. The main quality for such simulators is their modularity. Without mo dularity, a simulator cannot follow the microarchitectural evolutions and rapidly becomes les and less representative. Modularity is today a more essential quality for simulators than speed. We present a simulator vectorization methodology replacing the multiplication of the units , which improves the simulation speed. The aim of this study is to : (1) present an extension of the UNI SIM (Augustet al., 2007) communication protocol, (2) introduce a simple and systematic developing methodology to build simulators with duplicated resources, (3) study the method ology impact on the simulator time speedup and more precisely, the time spent in the SystemC sch eduler. MOTS-CLÉS :Vectorisation, Simulation modulaire, protocole de commun ication modulaire, Accélération de la simulation.	automatic vectorization;central processing unit;communications protocol;linear algebra;microarchitecture;simulation;speedup;systemc	Mourad Bouache;David Parello;Bernard Goossens	2011	Technique et Science Informatiques	10.3166/tsi.30.1115-1134	computer science;database;performance art	EDA	-15.687422109420277	41.58517407013451	188107
4a3c52b1b48bcf8cb41d52f4b74741369ae7ed63	an automatic iteration/data distribution method based on access descriptors for dsmm	distributed memory systems;programacion entera;programacion paralela;data locality;parallel programming;optimizacion compiladora;programmation en nombres entiers;data distribution;computer architecture;systeme memoire repartie;shared memory systems;architecture ordinateur;integer programming;compiler optimization;arquitectura ordenador;integer program;optimisation compilateur;systeme memoire partagee;programmation parallele	Nowadays NUMA architectures are widely accepted. For such multiprocessors exploiting data locality is clearly a key issue. In this work, we present a method for automatically selecting the iteration/data distributions for a sequential F77 code, while minimizing the parallel execution overhead (communications and load unbalance). We formulate an integer programming problem to achieve that minimum parallel overhead. The constraints of the integer programming problem are derived directly from a graph known as the Locality-Communication Graph (LCG), which captures the memory locality, as well as the communication patterns, of a parallel program. The constraints derived from the LCG express the data locality requirements for each array and the aan-ity relations between diierent array references. In addition, our approach use the LCG to automatically schedule the communication operations required during the program execution, once the iteration/data distributions have been selected. The aggregation of messages in blocks is also dealt in our approach. The TFFT2 code, from NASA benchmarks, that includes non-aane access functions and non-aane index bounds, and repeated subroutine calls inside loops, has been correctly handled by our approach. The version of the code, parallelized with the iteration/data distributions derived from our method, achieves parallel eeciencies of over 69% for 16 processors, in a Cray T3E, an excellent performance for a complex real code.	central processing unit;code;compiler;cray t3e;experiment;fortran;integer programming;iteration;locality of reference;loss function;mathematical optimization;optimization problem;overhead (computing);parallel computing;requirement;sion's minimax theorem;subroutine;worldwide lhc computing grid	Angeles G. Navarro;Emilio L. Zapata	1999		10.1007/3-540-44905-1_9	parallel computing;integer programming;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;optimizing compiler;database;mathematics;distributed computing;programming language;algorithm	HPC	-15.1121823680815	41.84301621343082	188136
93c698e0b86aafd194d8ae2cb6124a5f92dd7473	an enhanced two-level adaptive multiple branch prediction for superscalar processors	evaluation performance;performance evaluation;branch prediction;evaluacion prestacion;sistema informatico;computer system;systeme adaptatif;computer architecture;architecture ordinateur;adaptive system;sistema adaptativo;superscalar processor;systeme informatique;arquitectura ordenador;systeme parallele;parallel system;sistema paralelo	We propose an enhanced multiple branch predictor using per-primary address branch histories. Using this scheme, the interferences among different branches are reduced, enhancing the average prediction accuracy. Also, since the branches predicted each cyde do not suffer from successive dependencies, predictions are generated in parallel. This scheme results in higher average branch prediction accuracy than the previous global history scheme under the same implementation cost. For two branch predictions, the prediction accuracy of integer benchmarks varies between 92.0 and 96.9 percent. For floating point benchmarks including nasa7, the accuracy is between 94.8 and 95.8 percent.	branch predictor;superscalar processor	Jongbok Lee;Wonyong Sung;Soo-Mook Moon	1997		10.1007/BFb0002852	embedded system;parallel computing;real-time computing;computer science;artificial intelligence;adaptive system;operating system;branch predictor	Arch	-16.416305312034396	44.74581229592487	188514
cc3dcd32be69e764edf72434cc564dd863876ac1	high performance message passing library for maestro2 cluster network	message passing library;high-performance network;cluster computing;performance evaluation;cost effectiveness;message passing	A cluster has become a popular platform for high performance computing. Currently, many PC clusters are organized with cost-effective conventional wide-area network technologies to connect each processing elements. Here, it is difficult to extract potential performance of the cluster, since there is the disparity between the wide-area network technologies and the performance characteristics required by cluster computing. We have developed a network to build high performance PC cluster, called Maestro2 cluster network. This paper describes about high performance communication library for Maestro2, named MMP. First, we describe the novel technique, design, and implementation of Maestro2. Then, we present a design and an implementation of MMP. We also show experimental results to evaluate the basic performance of MMP and compare it with raw performance of Maestro2. The experimental results show that MMP can extract maximum performance from Maestro2.	binocular disparity;computer cluster;mathematics-mechanization platform;message passing;supercomputer	Keiichi Aoki;Koichi Wada;Masaaki Ono;Shinichi Yamagiwa	2005			message passing;parallel computing;cluster (physics);computer cluster;wide area network;distributed computing;supercomputer;computer science	HPC	-11.906852208689322	44.435371863156675	188562
ba925eb3268f5005697aacafcfd42b56cf53de9f	the multiuser calculator: an operating system project	operating system	Anyone who has taught a first course on Operating Systems knows the frustration of trying to come up with a good course project, one which confronts the students with basic principles without overwhelming them with complicating, extraneous details. Such a project should allow the student to see the relationships between an operating system and (i) the interrupt mechanism which drives it (at the low level) and (ii) the work which the system manages for users (at the high level), i.e., the execution of their programs. In addition, the project should give the student some insight into the relationships among the basic components of an operating system (memory and process managers and the process scheduler). Projects which have been discussed in the literature [1,6,7,8,9] have used various approaches to this problem but have fallen short of the goal. As Wadland has pointed out [8] ‘Either you start at the middle (of a system) and work up or start at the bottom (interrupt level) and don’t get very far.’ In either case the student will get an incomplete experience. The multiuser calculator project described in sections 2 and 3 of this paper has evolved over the past two years in a first course (junior level) in operating systems. The background of the students in this course is courses in machine organization, programming languages and data structures. The project requires the students to design and implement a complete multitasking system, from interrupt mechanism straight through to a translator for user programs, the execution of which will be managed by the system. Though implemented on a network of 24 Apollo Computer workstations, the discussion of the project which follows is system independent. Section 3 below contains a discussion of the project environment provided by the Apollo network as well as a discussion of the suitability of other systems as hosts. The purpose of this paper is not just to present yet another operating system project, but to point to a new source	apollo computer;computer multitasking;data structure;high-level programming language;interrupt;multi-user;operating system;scheduling (computing);workstation;yet another	Jerud J. Mead	1988		10.1145/52964.52973	real-time computing;computer science;operating system	OS	-15.51836111865268	39.87613140392197	188758
c024198fec04a076123d04dac5806903bb3bcbab	concept of distributed processing system of images flow in terms of π-calculus	cluster computer distributed processing system image flow pi calculus automata data stream processing parallel processing systems pipeline parallelism;data processing parallel processing pipelines automata graphical user interfaces hardware data communication;pi calculus automata theory image processing parallel processing	The paper describes a concept of software tools for data stream processing. The tools can be used to implement parallel processing systems. Description of the task is presented in the first part of paper. The system is based on pipeline parallelism and was distributed for using on a cluster computer. The paper describes a base scheme and a main work algorithm of the system. Tasks are presented in π-calculus terms. The system process tasks as π-calculus automata. An actual application example is presented.	algorithm;automata theory;computer cluster;distributed computing;parallel computing;pipeline (computing);stream processing;π-calculus	Aleksey Kondratyev;Igor Tishchenko	2016	2016 18th Conference of Open Innovations Association and Seminar on Information Security and Protection of Information Technology (FRUCT-ISPIT)	10.1109/FRUCT-ISPIT.2016.7561518	parallel computing;computer science;theoretical computer science;digital image processing;distributed computing	HPC	-12.181636176071189	42.64022437745325	189229
f2085e5af71a19211390626f9ecd03fac9d350ab	process synchronization in the uts kernel		Any operating system kernel has some form of process synchronization, allowing a process to wait for a particular condition. The traditional choice for UNIX systems, the event-wait mechanism , leads to race conditions on multiprocessors.	kernel (operating system);operating system;race condition;synchronization (computer science);uts;unix	Lawrence M. Ruane	1990	Computing Systems		kernel (linear algebra);synchronization (computer science);distributed computing;computer science	OS	-13.023037374349247	45.01909124141954	189377
34a4bdc2acfed349f3f9d12d6671465f756da5df	contention resolution on a broadcast-based distributed shared memory multiprocessor	sistema fila espera;file attente;simultaneous multiprocessor optical exchange bus;distributed memory;systeme attente;tiempo espera;remote access;evaluation performance;interconnection;acceso remoto;algorithm performance;performance evaluation;systeme multiprocesseur memoire repartie;integrated circuit;execution time;canal bus;multiprocessor;bus optique;equilibrio de carga;memoria compartida;evaluacion prestacion;acces a distance;equilibrage charge;canal colector;radiodifusion;queue;punto caliente;barra colectora optica;circuito integrado;system buses distributed shared memory systems multiprocessing systems;temps attente;hot spot;algorithme;interconexion;radiation detectors heuristic algorithms protocols receivers computational modeling monitoring integrated circuit modeling;algorithm;codificacion;broadcast based distributed shared memory multiprocessor;resultado algoritmo;some bus;waiting time;some bus broadcast based distributed shared memory multiprocessor memory access contention simultaneous multiprocessor optical exchange bus dynamic page migration protocol;sistema multiprocesador memoria distribuida;queueing system;contention resolution;interconnexion;coding;performance algorithme;load balancing;point chaud;bus channel;dynamic page migration protocol;temps execution;procesador;distributed memory multiprocessor system;broadcasting;processeur;multiprocesador;memoire repartie;tiempo ejecucion;distributed shared memory;optical bus;fila espera;radiodiffusion;processor;circuit integre;codage;memory access contention;algoritmo;multiprocesseur	The issue of resolving remote memory access contention on hardware distributed shared memory multiprocessors and the performance impact of implementing a contention resolution algorithm are focused. After summarising a multiprocessor architecture called the simultaneous multiprocessor optical exchange-bus (SOME-Bus), a simple but effective contention resolution algorithm that relies on the information of the number of messages in the channel queue reported by each node is presented. The algorithm detects potential hot spots and resolves contention using dynamic page migration protocol, and balances remote memory accesses across the nodes of the system. Simulations with eight parallel codes on a 64-processor SOME-Bus show that the algorithm yields significant performance improvements such as balanced-memory load, reduction in the execution times, number of remote memory accesses, average channel waiting times and average network latencies.	algorithm;code;distributed shared memory;multiprocessing	Mehmet Fatih Akay;Constantine Katsinis	2008	IET Computers & Digital Techniques	10.1049/iet-cdt:20060189	distributed shared memory;shared memory;embedded system;parallel computing;real-time computing;multiprocessing;distributed memory;computer science;load balancing;operating system;integrated circuit;interconnection;coding;queue;hot spot;broadcasting	HPC	-15.505570739086172	44.80954951559142	190023
4d0557d60bab33a86db8b34f161dcfc97d142289	toward a map interface not inherently related to geography		quickly “get something running” using threads, only to spend days debugging subtle race conditions. In contrast, using MPI may take longer at first, but subsequent debugging and verification is more straightforward. The second issue with the metric is it does not address performance. Richards et al. said they did not measure performance. Why not? It would have been easy and provided insight into the trade-offs inherent in the choice of language. In parallel programming, a significant portion of the development effort goes to ensuring the program exhibits adequate runtime performance. Studies of productivity could take this time into account by, say, requiring the program to obtain some specified speedup using some specified (large) number of processors before it is deemed complete. (Richards et al. did mention X10 programs from another study that performed well, but presumably development of those programs did not end with their first successful two-core run.) Without doubt, there is a need for parallel programming languages that are more productive, as well as more studies measuring productivity. But, to be persuasive, the studies must use metrics that address all phases of a realistic development process. Stephen F. Siegel, Newark, DE	central processing unit;debugging;parallel computing;programming language;race condition;run time (program lifecycle phase);speedup;x10	CACM Staff	2014	Commun. ACM	10.1145/2693189	theoretical computer science;human–computer interaction;computer science	PL	-18.97007279593209	39.35091736885643	192373
fae93b92ef5966fd8f43afcd8f6cfd6044d10e1a	occam- and c-based multiprocessor environments for unix cluster	estacion trabajo;langage c;multiprocessor;programming environment;programacion paralela;station travail;parallel programming;transputer;medio ambiente programacion;workstation;c language;multiprocesador;transputers;unix;environnement programmation;lenguaje c;programmation parallele;multiprocesseur	"""Two new multiprocessor environments, OCCNIX and CNIX, are described in this paper. These are scalable modular software systems that enable parallel programs to run on clusters of UNIX workstations. The environments unite the paradigms of multiprocessing and multithreading into a single programming model. OCCNIX executes OCCAM programs, using linked binary level interpreters to form a virtual Transputer network. This is based around the TCP/IP client server model and, like JAVA, provides hardware-independent multiple platform access. CNIX is a derivative of OCCNIX that migrates the major architectural features of the Transputer, along with selected OCCAM and Transputer assembler (TASM) instructions, to C++ libraries. The CNIX libraries provide C++ with a multithreaded, multiprocessor environment and a set of simple OCCAM/TASM-based parallel processing commands. Results con""""rm that OCCNIX can execute OCCAM/Transputer binaries over groups of UNIX workstations and has the ability to run development tools such as the ISPY network debugger. Performance tests of CNIX show a multiprocessor speed-up, compared with the ideal case, of 93% and, using 16 workstations, have achieved execution times that are 15 times faster than a sequential C++ program."""	assembly language;binary classification;binary file;c++;central processing unit;computation;computer cluster;debugger;emulator;genetic algorithm;ibm pc compatible;integrated development environment;internet protocol suite;java;just-in-time compilation;library (computing);linux;ms-dos;machine code;microsoft windows;modular programming;multiprocessing;multithreading (computer architecture);operating system;optimizing compiler;parallel computing;programming model;run time (program lifecycle phase);sparc;scalability;scheduling (computing);server (computing);software system;sunos;the computer journal;thread (computing);transputer;turbo assembler;unix;workstation;occam	D. G. Patrick;P. R. Green;Trevor A. York	1997	Comput. J.	10.1093/comjnl/40.1.12	computer architecture;parallel computing;multiprocessing;workstation;computer science;operating system;unix;programming language	OS	-17.28541351001143	41.66137252460432	192517
7c72d0f9e2971f1832cb78cd3c60c3ddfea59d90	book review: systems programming in parallel logic languages by lan foster (prentice hall, 1990)		"""This book provides an elegant demonstration of parallel logic programming techniques. Although it's emphasis is on the implementation of operating systems, this book is also about how to build large applications using parallel logic languages. Throughout the book, the author repeatedly shows how the concepts underlying parallel logic languages-dynamic lightweight processes and logical variables-can be used to create abstractions for managing complexity in parallel programs. The book is composed of 10 chapters which can be roughly organized into three parts. The first part (Chapters 1, 2, and 3) provides introductions to the fundamental problems in operating system (OS) design and to Flat Parlog, which is the parallel logic programming language used throughout the book. The discussion on OS design problems is quite good, comparing many previous solution strategies. The introduction to Flat Parlog, however, is very brief and assumes a prior knowledge of parallel logic programming fundamentals. For those unfamiliar without such knowledge, a (slightly less brief) tutorial is presented in an appendix. The second part (Chapters 4, 5, and 6) discusses the design and implementation of a simple OS kernel. An operational model and abstract machine for Flat Parlog is presented , along with extensions which are necessary to support task management and processor scheduling. Chapter 6 is devoted to convincing the reader that the Flat Parlog abstract machine provides an adequate basis for studying performance aspects of an OS written in a parallel logic programming language, and then attempts to quantify the overhead associated with task management functions. Unfortunately, the efficiency comparisons are made with respect to other parallel logic languages and not more conventional implementation strategies; one does not get a feel for how the kernel performs """"in real life."""" The third part (Chapters 7, 8, and 9) extend the kernel into a fledgling OS. Chapter 7 creates an OS nucleus on top of the kernel, providing user programs with access to essential kernel services and protecting these services against malicious or erroneous access. Chapter 8 describes a programming environment for the creation of user programs. Chapter 9 discusses issues associated with multiprocessor OS design, such as global process scheduling and process migration. The book concludes in Chapter 10 with a brief review of the preceding chapters, a survey of related work, and suggestions for future research. On the whole, I think this book is a well written, coherent exposition on the use of parallel logic languages …"""	abstract machine;coherence (physics);integrated development environment;jim hall (programmer);kernel (operating system);logic programming;malware;multiprocessing;operating system;overhead (computing);parlog;process migration;programming language;real life;scheduling (computing);system programming;windows nt processor scheduling	Michael L. Hilton	1990		10.1145/121956.773548	computer science;artificial intelligence;cognitive science	PL	-15.526668391575024	39.6916904225778	192796
cba65a25fc18a721f5a2884d0aa48ef9325ad9cb	oregami: tools for mapping parallel computations to parallel architectures	outil logiciel;software tool;theoretical model;architecture systeme;gestion labor;concepcion sistema;programming environment;routing;programacion paralela;implementation;sistema informatico;parallel programming;transmission message;computer system;message transmission;medio ambiente programacion;ejecucion;gestion tâche;herramienta controlada por logicial;system design;parallel programming environment;parallel computer;message passing;arquitectura sistema;systeme informatique;task assignment;encaminamiento;oregami;systeme parallele;parallel system;parallel architecture;task scheduling;interactive graphics;system architecture;conception systeme;sistema paralelo;environnement programmation;acheminement;programmation parallele;transmision mensaje	The OREGAMI project involves the design, implementation, and testing of algorithms for mapping parallel computations to message-passing parallel architectures. OREGAMI addresses the mapping problem by exploiting regularity and by allowing the user to guide and evaluate mapping decisions made by OREGAMI's efficient combinatorial mapping algorithms. OREGAMI's approach to mapping is based on a new graph theoretic model of parallel computation called the Temporal Communication Graph. The OREGAMI software tools include three components: (1) LaRCS is a graph description language which allows the user to describe regularity in the communication topology as well as the temporal communication behavior (the pattern of message-passing over time). (2) MAPPER is our library of mapping algorithms which utilize information provided by LaRCS to perform contraction, embedding, and routing. (3) METRICS is an interactive graphics tool for display and analysis of mappings. This paper gives an overview of the OREGAMI project, the software tools, and OREGAMI's mapping algorithms.	algorithm;computation;computer architecture;graphics;mapper;message passing;parallel computing;routing;theory	Virginia Mary Lo;Sanjay V. Rajopadhye;Samik Gupta;David Keldsen;Moataz A. Mohamed;Bill Nitzberg;Jan Arne Telle;Xiaoxiong Zhong	1991	International Journal of Parallel Programming	10.1007/BF01379319	routing;parallel computing;message passing;simulation;computer science;theoretical computer science;operating system;distributed computing;programming language;implementation;systems architecture;systems design	HPC	-16.14530176224689	40.660674171285194	193618
422d6a789dcf0534ebc4fd57917f69ffd7204a61	a layered, codesign virtual machine approach to modeling computer systems	computer architecture;drams;software design;design methodology;virtual machine;dynamic scheduling;hardware;software performance;virtual machines	By using a macro/micro state model we show howassumptions on the resolution of logical and physical timingof computation in computer systems has resulted in designmethodologies such as component-based decomposition,where they are completely coupled, and function/architectureseparation, where they are completely independent. Wediscuss why these are inappropriate for emergingprogrammable, concurrent system design. By contrast,schedulers layered on hardware in concurrent systemsalready couple logical correctness with physicalperformance when they make effective resource sharingdecisions. This paper lays a foundation for understandinghow layered logical and physical sequencing will impact thedesign process, and provides insight into the problems thatmust be solved in such a design environment. Our layeredapproach is that of a virtual machine. We discuss our MESHresearch project in this context.	component-based software engineering;computation;computer;concurrency (computer science);correctness (computer science);interaction;scheduling (computing);systems design;virtual machine	JoAnn M. Paul;Donald E. Thomas	2002			embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;computer science;virtual machine;operating system;programming language	PL	-13.639330538666515	40.79130496600958	194374
184a2e37f933ed99a2cfd5affe622ea296e0c4e9	a space-efficient on-chip compressed cache organization for high performance computing	fine grained management;algoritmo paralelo;haute performance;parallel algorithm;metadata;mesa;distributed computing;cache memory;memory performance;fragmentacion;on chip compressed cache;simulator;algorithme parallele;chip;antememoria;organizacion memoria;internal fragmentation problem;antememoire;simulador;high performance computer;organisation memoire;metadonnee;table;parallel computer;simulateur;alto rendimiento;calculo repartido;metadatos;memory organization;processor memory performance gap;coarse grained;high performance;fragmentation;calcul reparti;parallel processing	In order to alleviate the ever-increasing processor-memory performance gap of high-end parallel computers, on-chip compressed caches have been developed that can reduce the cache miss count and off-chip memory traffic by storing and transferring cache lines in a compressed form. However, we observed that their performance gain is often limited due to their use of the coarse-grained compressed cache line management which incurs internally fragmented space. In this paper, we present the fine-grained compressed cache line management which addresses the fragmentation problem, while avoiding an increase in the metadata size such as tag field and VM page table. Based on the SimpleScalar simulator with the SPEC benchmark suite, we show that over an existing compressed cache system the proposed cache organization can reduce the memory traffic by 15%, as it delivers compressed cache lines in a fine-grained way, and the cache miss count by 23%, as it stores up to three compressed cache lines in a physical cache line.	benchmark (computing);best, worst and average case;cpu cache;cache (computing);computer memory;fragmentation (computing);page table;parallel computing;run time (program lifecycle phase);simulation	Keun Soo Yim;Jang-Soo Lee;Jihong Kim;Shin-Dug Kim;Kern Koh	2004		10.1007/978-3-540-30566-8_109	chip;bus sniffing;least frequently used;pipeline burst cache;parallel processing;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;table;database;distributed computing;fragmentation;adaptive replacement cache;parallel algorithm;smart cache;memory organisation;mesi protocol;metadata;cache algorithms;cache pollution;mesif protocol	HPC	-15.75674378977418	44.755540427827874	194421
c3f226098b346e8a9ab1e002024fd126726f9b3b	performance analysis of overheads for matrix - vector multiplication in cluster environment	calcul matriciel;modelizacion;distributed memory;distributed system;algoritmo paralelo;evaluation performance;haute performance;systeme reparti;parallel algorithm;performance evaluation;perforation;memoria compartida;evaluacion prestacion;communicating process;distributed computing;cost analysis;analisis costo;algorithme parallele;proceso comunicante;modelisation;analyse cout;sistema repartido;message passing interface;envoi message;processus communicant;performance analysis;message passing;alto rendimiento;communication cost;calculo repartido;mpi;parallel implementation;matrix calculus;memoire repartie;modeling;high performance;calcul reparti;calculo de matrices	This paper presents the basic parallel implementation and a variation for matrix vector multiplication. We evaluated and compared the performance of the two implementations on a cluster of workstations using Message Passing Interface (MPI) library. The experimental results demonstrate that the basic implementation achieves lower performance than the other variation. Further, we analyzed the several classes of overheads contribute to lowered performance of the basic implementation. These analyses have identified cost of reading of data from disk and communication cost as the primary factors affecting performance of the basic parallel matrix vector implementation. Finally, we present a performance model for estimating the performance of two proposed matrix vector implementations on a cluster of heterogeneous workstations.	computer cluster;elegant degradation;matrix multiplication;message passing interface;profiling (computer programming);workstation	Panagiotis D. Michailidis;Vasilis Stefanidis;Konstantinos G. Margaritis	2005		10.1007/11573036_23	parallel computing;computer science;message passing interface;distributed computing;algorithm	HPC	-16.779012817298835	43.08594063342184	194469
2992b8985e094c3943e29dffc550862791fae147	thread clustering: sharing-aware scheduling on smp-cmp-smt multiprocessors	symmetric configuration;sistema operativo;data sharing;cache locality;analyse amas;thread scheduling;systeme unix;cache behavior;performance monitoring;shared memory;systeme grande taille;storage access;affinity scheduling;systeme multiprocesseur memoire repartie;multiprocessor;multiprocessing;surveillance;resource allocation;memoria compartida;unix system;configuration symetrique;hardware performance monitors;chip multiprocessor;simultaneidad informatica;cache memory;large scale system;classification;detecting sharing;chip;antememoria;thread migration;systeme linux;antememoire;configuracion simetrica;performance improvement;concurrency;sharing;vigilancia;pattern detection;cluster analysis;operating system;monitoring;design and implementation;sistema linux;shared caches;scheduling;performance monitoring unit;sistema multiprocesador memoria distribuida;retard;acces memoire;hardware performance counters;multitraitement;acceso memoria;multithread;thread placement;systeme exploitation;analisis cluster;smt;smp;single chip multiprocessors;distributed memory multiprocessor system;multitâche;sistema unix;simultaneous multithreading;monitorage;multiprocesador;monitoreo;linux system;retraso;cmp;simultaneite informatique;multitarea;clasificacion;ordonnancement;reglamento;memoire partagee;sistema gran escala;multithreading;shared memory multiprocessor;multiprocesseur;multitratamiento	The major chip manufacturers have all introduced chip multiprocessing (CMP) and simultaneous multithreading (SMT) technology into their processing units. As a result, even low-end computing systems and game consoles have become shared memory multiprocessors with L1 and L2 cache sharing within a chip. Mid- and large-scale systems will have multiple processing chips and hence consist of an SMP-CMP-SMT configuration with non-uniform data sharing overheads. Current operating system schedulers are not aware of these new cache organizations, and as a result, distribute threads across processors in a way that causes many unnecessary, long-latency cross-chip cache accesses.  In this paper we describe the design and implementation of a scheme to schedule threads based on sharing patterns detected online using features of standard performance monitoring units (PMUs) available in today's processing units. The primary advantage of using the PMU infrastructure is that it is fine-grained (down to the cache line) and has relatively low overhead. We have implemented our scheme in Linux running on an 8-way Power5 SMP-CMP-SMT multi-processor. For commercial multithreaded server workloads (VolanoMark, SPECjbb, and RUBiS), we are able to demonstrate reductions in cross-chip cache accesses of up to 70%. These reductions lead to application-reported performance improvements of up to 7%.	cpu cache;central processing unit;cluster analysis;linux;multithreading (computer architecture);operating system;overhead (computing);power5;power management unit;scheduling (computing);server (computing);shared memory;simultaneous multithreading;symmetric multiprocessing;thread (computing)	David K. Tam;Reza Azimi;Michael Stumm	2007		10.1145/1272996.1273004	embedded system;parallel computing;multiprocessing;cache coloring;page cache;cache;computer science;cache invalidation;operating system;cache algorithms;cache pollution	OS	-16.598039595656118	44.103791345647096	194537
c49194ec1ed571de7e8fb8fa1c17c2127ff22874	horizontally divided signature files on a parallel machine architecture	multidisk system;hachage;distributed system;eficacia sistema;base donnee;systeme reparti;multimedia;performance evaluation;programacion paralela;etude experimentale;resource allocation;frame slicing technique;heuristic method;performance systeme;database;base dato;gestion fichier;parallel programming;metodo heuristico;file management;system performance;extendible hashing;signature file;algorithme;algorithm;parallel databases;hashing;sistema repartido;indexation;multimedia data;manejo archivos;systeme multidisque;parallel machines;asignacion recurso;methode heuristique;allocation ressource;estudio experimental;parallel signature files;programmation parallele;algoritmo	The signature file method has widely been advocated as an efficient index scheme to handle new applications demanding a large amount of textual databases. Moreover, it has been recently extended to support multimedia unformatted data. These multimedia data must be rapidly managed for synchronization and presentation, for which parallel database computers have been proposed. In order to achieve good performance, the signature file approach has been required to support parallel database processing. Therefore, in this paper we propose a horizontally divided parallel signature file method (HPSF) using extendible hashing and frame-slicing techniques. In addition, we propose a heuristic processor allocation method so that we may assign signatures into the given number of processors in a uniform way. To show the efficiency of HPSF, we evaluate the performance of HPSF in terms of retrieval time, storage overhead, and insertion time. Finally, we show from the performance results that HPSF outperforms the conventional parallel signature file methods on retrieval performance as well as a dynamic operating measure to combine both retrieval and insertion time.	digital signature;parallel computing	Jeong-Ki Kim;Jae-Woo Chang	1998	Journal of Systems Architecture	10.1016/S1383-7621(97)00015-5	embedded system;parallel computing;hash function;extendible hashing;resource allocation;computer science;operating system;database;distributed computing;computer performance;programming language;algorithm	Arch	-17.56743495740189	44.71007445665763	195213
e0dc879a0cb4a07e6b715c2c7cb098df61e7fe1f	speculative computation: overcoming communication delays	synchronous iterative algorithms n body simulation speculative computation communication latency masking;n body simulations;n body simulation;iterative algorithm;synchronous iterative algorithms;performance improvement;parallel computer;communication delay;communication latency masking;speculative computation	"""Communication latencies and delays are a major source of performance degradation in parallel computing systems. It is important to """"mask"""" these communication delays by overlapping them with useful computation in order to obtain good parallel performance. This article proposes speculative computation as a technique to mask communication latencies in synchronous iterative algorithms. Processors speculate the contents of messages that are not yet received and perform computation based on the speculated values. When the messages are received, they are compared with the speculated values and, if the error is unacceptable, the resulting computation is corrected or recomputed. If the error is small, the speculated value is accepted and the processor has masked the communication delay. The technique, applied to N-body simulations yielded a performance improvement of up to 34%."""	algorithm;central processing unit;computation;distributed computing;elegant degradation;iteration;iterative method;parallel virtual machine;parallel computing;sparc;simulation;speculative execution;speedup;workstation	Vasudha Govindan;Mark A. Franklin	1994	1994 International Conference on Parallel Processing Vol. 3	10.1109/ICPP.1994.183	parallel computing;real-time computing;computer science;theoretical computer science;n-body simulation;distributed computing;iterative method	HPC	-15.245239933129518	45.901253427125575	195321
0a5bb204cf8e5cce872573445f3cdb17c12203ff	informed prefetching and caching	workload;parallelisme;digital computers;microprocessors;access;execution time;stalling;buffers;multiprogrammation;dynamic file allocation;prefetching;resource manager;resource management;data management;programme entree sortie;buffer storage;computations;theses;time;relational database;data bases;multiprogramming;cost analysis;scientific visualization;arrays;gestion recursos;parallelism;performance improvement;input output program;paralelismo;operating system;time series analysis;interrogation;programa entrada salida;multiprogramacion;file system;chemistry;charge travail;input output processing;gestion ressources;speech recognition;temps execution;computer files;cashing;magnetic tape;benefits;tiempo ejecucion;magnetic disks;carga trabajo;data acquisition;cost benefit analysis;allocations;parallel processing;estimates;throughput	In this paper, we present aggressive, proactive mechanisms that tailor file system resource management to the needs of I/O-intensive applications. In particular, we show how to use applicationdisclosed access patterns (hints) to expose and exploit I/O parallelism, and to dynamically allocate file buffers among three competing demands: prefetching hinted blocks, caching hinted blocks for reuse, and caching recently used data for unhinted accesses. Our approach estimates the impact of alternative buffer allocations on application execution time and applies cost-benefit analysis to allocate buffers where they will have the greatest impact. We have implemented informed prefetching and caching in Digital’s OSF/1 operating system and measured its performance on a 150 MHz Alpha equipped with 15 disks running a range of applications. Informed prefetching reduces the execution time of text search, scientific visualization, relational database queries, speech recognition, and object linking by 20-83%. Informed caching reduces the execution time of computational physics by up to 42% and contributes to the performance improvement of the object linker and the database. Moreover, applied to multiprogrammed, I/Ointensive workloads, informed prefetching and caching increase overall throughput.	cpu cache;cache (computing);computational physics;dec alpha;input/output;link prefetching;operating system;parallel computing;proactive parallel suite;relational database;run time (program lifecycle phase);scientific visualization;speech recognition;throughput;tru64 unix	R. Hugo Patterson;Garth A. Gibson;Eka Ginting;Daniel Stodolsky;Jim Zelenka	1995		10.1145/224056.224064	parallel computing;real-time computing;computer science;cost–benefit analysis;resource management;operating system;programming language;computer security	OS	-15.508038117198426	45.0808923741239	196084
2dca770b3c7cd4a437074720f6a162293fe79198	distributed simulation model for computer integrated manufacturing	computer integrated manufacturing;distributed processing;local area networks;time warp simulation;computer integrated manufacturing;distributed computer system;distributed simulation model;ethernet;large complex manufacturing system;simulation tool;synchronization;time bucket method;time warp mechanism;virtual factory;workstations	In this paper, we firstly discuss the necessity of a simulation tool’which can handle h huge and complex manufacturing system under CIM environment, and then consider problems incurred by developing such a large system model. A distributed simulation model developed on a distributed computer system is then shown to be a promising method to cope with the problems. Lastly, a time bucket method with time warp mechanism is proposed for the synchronization of the distributed simulation and its computational features are investigated by modeling a virtual factory on six workstations connected by the ethernet.	computer;computer-integrated manufacturing;distributed computing;dynamic time warping;naruto shippuden: clash of ninja revolution 3;performance evaluation;simulation;workstation	Susumu Fujii;Haruhisa Tsunoda;Atsushi Ogita;Yasushi Kidani	1994			local area network;stochastic approximation;real-time computing;simulation;systems modeling;computer science;technical report;computer-integrated manufacturing;computer engineering	HPC	-13.866002910787635	42.3331631892028	196187
30236a66daebf0ce3f2501545365a8fccaf0464e	parallel code clone detection using mapreduce	software engineering;cloning educational institutions detection algorithms parallel processing distributed databases usa councils software engineering;program compilers;operating systems computers;parallel processing;source coding operating systems computers parallel processing program compilers software engineering;source coding;operating systems parallel code clone detection mapreduce software engineering inherent parallelism large scale code clone detection	Code clone detection is an established topic in software engineering research. Many detection algorithms have been proposed and refined but very few exploit the inherent parallelism present in the problem, making large scale code clone detection difficult. To alleviate this shortcoming, we present a new technique to efficiently perform clone detection using the popular MapReduce paradigm. Preliminary experimental results demonstrates speed-up and scale-up of the proposed approach.	algorithm;duplicate code;mapreduce;parallel computing;programming paradigm;software engineering	Hitesh Sajnani;Joel Ossher;Cristina V. Lopes	2012	2012 20th IEEE International Conference on Program Comprehension (ICPC)	10.1109/ICPC.2012.6240500	parallel processing;parallel computing;computer science;theoretical computer science;operating system;software engineering;distributed computing;programming language;source code	SE	-14.881913930601357	40.78623709063128	196527
e49cf376b9e997fcfaff8b1ee58b79c450b1d50b	noncommittal barrier synchronization	tratamiento paralelo;systeme grande taille;traitement parallele;simulation;parallel prefix;simulacion;large scale system;parallel computation;synchronisation;calculo paralelo;synchronization;parallel computer;sincronizacion;systeme parallele;parallel system;algoritmo optimo;algorithme optimal;optimal algorithm;optimistic computation;calcul parallele;parallel processing;sistema paralelo;parallel simulation;sistema gran escala	"""Barrier synchronization is a fundamental operation in parallel computation. In many contexts, at the point a process enters a barrier it knows that it has already processed all work required of it prior to the synchronization. It then commits to the barrier, in the sense that the process blocks until every other process has also committed to the barrier. This paper treats the alternative case, when a process cannot enter a barrier with the assurance that it has already performed all necessary pre-synchronization computation. The problem arises when the number of pre-synchronization messages to be received by a process is unknown, for example, in any computation that is largely driven by an unpredictable exchange of messages. We describe a O(log 2 P) time barrier algorithm for such problems, study its performance on a large-scale parallel system, and consider extensions to general associative reductions, as well as associative parallel preex computations. under the title \Global Synchronization for Optimistic Parallel Discrete Event Simulations"""", portions of which are reprinted with permission."""	algorithm;apple lisa;barrier (computer science);computation;computer simulation;consortium;experiment;floor and ceiling functions;intel paragon;parallel algorithm;parallel computing;speculative execution;supercomputer;synchronization (computer science);touchstone file	David M. Nicol	1995	Parallel Computing	10.1016/0167-8191(94)00101-F	parallel processing;synchronization;parallel computing;computer science;distributed computing;algorithm	HPC	-16.155976859792155	45.431111917453826	196700
0bc746ffb67dd9d18f71367d44d73faf9653ec04	a component-based implementation of multiple sequence alignment	persistent data caching;component based architecture;distributed computing;distributed memory machine;cluster of workstations;data cache;multiple sequence alignment	This paper addresses the efficient execution of a Multiple Sequence Alignment (MSA) method, in particular the progressive alignment-based CLUSTAL W algorithm, on a cluster of workstations. We describe a scalable component-based implementation of CLUSTAL W program targeting distributed memory machines and multiple query workloads. We look at the effect of data caching on the performance of the data server. We present a distributed, persistent cache approach for caching intermediate results for reuse in subsequent or concurrent queries. Our initial results show that the cache-enabled CLUSTAL W program scales well on a cluster of workstations.	algorithm;cache (computing);clustalw/clustalx;component-based software engineering;computer cluster;distributed memory;multiple sequence alignment;scalability;server (computing);workstation	Ümit V. Çatalyürek;Mike Gray;Tahsin M. Kurç;Joel H. Saltz;Eric Stahlberg;Renato A. C. Ferreira	2003		10.1145/952532.952559	parallel computing;real-time computing;multiple sequence alignment;computer science;component-based software engineering;operating system;database;programming language	HPC	-13.07483640960661	44.75231062456836	196864
0c7684af6552b9fee1b38234daaa7e275732a84f	a speculative execution scheme of macrotasks for parallel processing systems	tratamiento paralelo;parallelisme;programa paralelo;traitement parallele;multiprocessor;ejecucion programa;compiler;program execution;parallelism;paralelismo;grande vitesse;execution programme;speculative execution;eager evaluation;gran velocidad;multiprocesador;multiprocessor system;parallel program;high speed;parallel processing;programme parallele;multiprocesseur	A problem in the parallel execution of the FORTRAN program is that it is difficult to extract sufficiently the parallelism from the program, based on the execution order due to the control dependence (precedence constraint) [ 11. The control dependence is defined strictly in section 2.5, but a brief explanation is given by using an example. In the following example, whether or not S2 should be executed cannot be decided until the conditional branch of S1 is decided. Consequently, S2 is not executed until the con-	branch (computer science);fortran;macrotasking;parallel computing;speculative execution	Hayato Yamana;Toshiaki Yasue;Yoshihiko Ishii;Yoichi Muraoka	1995	Systems and Computers in Japan	10.1002/scj.4690260601	parallel processing;computer architecture;compiler;parallel computing;real-time computing;multiprocessing;computer science;operating system;foreground-background;eager evaluation;programming language;speculative multithreading;speculative execution	HPC	-16.318872199107943	40.77798170361508	196915
a6c08e748a3d9d557a3d0ed27ef85bb41a5912ce	distributed simulation of resource constrained project scheduling	distributed system;systeme reparti;project;proyecto;resource allocation;sistema informatico;simulation;simulacion;computer system;planificacion;sistema repartido;planning;systeme informatique;project scheduling;asignacion recurso;planification;projet;allocation ressource;distributed simulation	Algorithms for resource constrained project scheduling are available for implementation on centralized computer systems. Multiple runs to study the effect of changes in exogenous variables have to be simulated in a serial order on a centralized system. However, inherent parallelism in simulation can be beneficially exploited on a distributed computer system to drastically reduce simulation time. Distributed simulation on loosely coupled systems involves problem partitioning into components, allocation of components to computing systems, communication and coordination between components. Strategies for distributed simulation of time analysis of project network have been reported in our previous work. However, no previous work is reported on the distributed simulation of resource and cost analysis. In this paper, we discuss how resource constrained project scheduling can be simulated on a distributed computer system. The process involves partitioning of the project network into independent sub-project networks, and allocating sub-projects to independent computing systems connected through communication channels without shared variables. The total resource availability of the project is maintained at a central computer system. The central system receives activity time vector from individual sub-projects as and when an activity becomes eligible for scheduling and returns the activity schedule vector after the activity has been scheduled. The distributed algorithm proposed has been illustrated by an example.	centralized computing;distributed algorithm;distributed computing;loose coupling;optimal design;parallel computing;project network;project networks;schedule (computer science);scheduling (computing);shared variables;simulation;viz: the computer game	R. K. Arora;R. K. Sachdeva	1989	Computers & OR	10.1016/0305-0548(89)90001-4	planning;distributed algorithm;real-time computing;simulation;project;resource allocation;computer science;distributed computing;schedule	HPC	-18.23365572867932	43.51595925693857	197970
364f621d4c9e97870cab0ad71de0b81040bc5495	loop parallelization algorithms: from parallelism extraction to code generation	parallel algorithm;nested loops;code generation;parallelization algorithms;loop fusion;loop transformation;optimality criteria;automatic parallelization	In this paper we survey loop parallelization algorithms analyzing the depen dence representations they use the loop transformations they generate the code generation schemes they require and their ability to incorporate various optimiz ing criteria such as maximal parallelism detection permutable loops detection minimization of synchronizations easiness of code generation etc We complete the discussion by presenting new results related to code generation and loop fusion for a particular class of multi dimensional schedules called shifted lin ear schedules We demonstrate that algorithms based on such schedules while generally considered as too complex can indeed lead to simple codes	algorithm;automatic parallelization;code generation (compiler);local interconnect network;maximal set;parallel computing	Pierre Boulet;Alain Darte;Georges-André Silber;Frédéric Vivien	1998	Parallel Computing	10.1016/S0167-8191(98)00020-9	loop tiling;loop fusion;loop inversion;parallel computing;loop fission;nested loop join;loop interchange;loop dependence analysis;computer science;loop nest optimization;theoretical computer science;loop unrolling;distributed computing;parallel algorithm;code generation;automatic parallelization	PL	-14.290497715316553	42.37298779951921	198034
265dc2065146ca50f01cc26092e2b7fdd22338d9	a performance evaluation of the cray x1 for scientific applications	estensibilidad;engineering;computers;scientific application;microprocessors;microprocessor;evaluation performance;haute performance;performance evaluation;software libraries;paysage;efficiency;evaluacion prestacion;vector processing computers;performance;massively parallel processors;distributed computing;nuclear disarmament safeguards and physical protection;paisaje;cache memory;program library;42 engineering;supercomputer;software engineering;algorithme numerique;performance tests;paralelismo masivo;antememoria;supercomputador;software architecture;antememoire;performance improvement;applications programs computers;vectors;reingenieria;reingenierie;proliferation;numerical algorithm;computer systems programs;bibliotheque programme;alto rendimiento;parallel processing computers;calculo repartido;cost effectiveness;algoritmo numerico;evaluation;microprocesseur;extensibilite;scalability;landscape;98 nuclear disarmament safeguards and physical protection;bibliotheque logiciel;capacity;ibm personal computers;architecture computers;computational efficiency;computer systems performance;high performance;architecture;reengineering;microprocesador;parallelisme massif;calcul reparti;massive parallelism;cray computers;biblioteca programa;architecture logiciel;superordinateur	The last decade has witnessed a rapid proliferation of superscalar cache-based microprocessors to build high-end capability and capacity computers primarily because of their generality, scalability, and cost effectiveness. However, the recent development of massively parallel vector systems is having a significant effect on the supercomputing landscape. In this paper, we compare the performance of the recentlyreleased Cray X1 vector system with that of the cacheless NEC SX-6 vector machine, and the superscalar cache-based IBM Power3 and Power4 architectures for scientific applications. Overall results demonstrate that the X1 is quite promising, but performance improvements are expected as the hardware, systems software, and numerical libraries mature. Code reengineering to effectively utilize the complex architecture may also lead to significant efficiency enhancements.	automatic vectorization;block (data storage);blocking (computing);cpu cache;code refactoring;computer;cray x1;data access;dhrystone;directive (programming);elegant degradation;ibm power systems;library (computing);locality of reference;memory bandwidth;microprocessor;n-body problem;nec sx-6;numerical analysis;overhead (computing);power3;power4;performance evaluation;scalability;software development process;supercomputer;superscalar processor;test case;testbed	Leonid Oliker;Rupak Biswas;Julian Borrill;Andrew Canning;Jonathan Carter;M. Jahed Djomehri;Hongzhang Shan;David Skinner	2004		10.1007/11403937_5	software architecture;computer architecture;supercomputer;parallel computing;scalability;performance;computer science;architecture;evaluation;operating system;database;distributed computing;landscape;efficiency;programming language;algorithm	HPC	-16.223145051744446	43.35872690882597	198422
e23b962eb96587914fe10cc6978cf5a8a92d440b	virtual machine file system	estensibilidad;tolerancia falta;distributed memory;distributed system;resource scheduling;virtual machine;high availability;metodo adaptativo;storage virtualization;gestion memoire;fiabilidad;reliability;haute performance;systeme reparti;storage system;san;measurement;fault tolerant;metadata;virtualisacion proceso;availability;disponibilidad;processor scheduling;gestion archivos;memoria compartida;storage management;performance;storage hardware acceleration;gestion fichier;contextual information;inicializacion;methode adaptative;hardware accelerator;file management;machine virtuelle;gestion memoria;sistema repartido;planificacion del procesador;process virtualization;file system;fiabilite;systeme memoire;fault tolerance;adaptive method;metadonnee;alto rendimiento;procesador oleoducto;design;virtualisation processus;extensibilite;metadatos;scalability;ordonnancement processeur;memoire repartie;processeur pipeline;clustered file system;sistema memoria;maquina virtual;high performance;disponibilite;tolerance faute;initialization;initialisation;pipeline processor	The Virtual Machine File System (VMFS) is a scalable and high performance symmetric clustered file system for hosting virtual machines (VMs) on shared block storage. It implements a clustered locking protocol exclusively using storage links, and does not require network-based inter-node communication between hosts participating in a VMFS cluster. VMFS layout and IO algorithms are optimized towards providing raw device speed IO throughput to VMs. An adaptive IO mechanism masks errors on the physical fabric using contextual information from the fabric. The VMFS lock service forms the basis of VMware's clustered applications such as vMotion, Storage vMotion, Distributed Resource Scheduling, High Availability, and Fault Tolerance. Virtual machine metadata is serialized to files and VMFS provides a POSIX interface for cluster-safe virtual machine management operations. It also contains a pipelined data mover for bulk data initialization and movement. In recent years, VMFS has inspired changes to diskarray firmware and the SCSI protocol. These changes enable the file system to implement a hardware accelerated data mover and lock manager, among other things. In this paper, we present the VMFS architecture and its evolution over the years	algorithm;application programming interface;backup;blacklist (computing);block (data storage);cluster analysis;clustered file system;computer data storage;exabyte;fault tolerance;firmware;hardware acceleration;hardware virtualization;high availability;hypervisor;local interconnect network;lock (computer science);logical disk;os-tan;posix;peripheral;pipeline (computing);requirement;scsi;scalability;schedule (project management);scheduling (computing);storage efficiency;storage virtualization;throughput;virtual machine	Satyam B. Vaghani	2010	Operating Systems Review	10.1145/1899928.1899935	embedded system;fault tolerance;parallel computing;computer science;operating system;computer security	OS	-18.07573520073233	44.24635649053156	198936
6607b2232996c4d685129f3f6b53e81e9a8bebfa	automatic performance analysis of master/worker pvm applications with kpi	gollete estrangulamiento;programacion paralela;parallel programming;application directeur travailleur;analisis automatico;goulot etranglement;automatic analysis;automatic detection;envoi message;analyse performance;performance analysis;message passing;analyse automatique;systeme parallele;parallel system;parallel programming model;bottleneck;sistema paralelo;programmation parallele;analisis eficacia	PVM parallel programming model provides a convenient methodology of creating dynamic master/worker applications. In this paper, we introduce the benefits from the use of KappaPi tool for automatic analysis of master/worker applications. First, by the automatic detection of the master/worker paradigm in the application. And second, by the performance analysis of the application focusing on the performance bottlenecks and the limitations of this master/worker collaboration.	parallel virtual machine;profiling (computer programming)	Antonio Espinosa;Tomàs Margalef;Emilio Luque	2000		10.1007/3-540-45255-9_10	parallel computing;message passing;simulation;computer science;operating system;parallel programming model	HPC	-17.681565830885305	41.50387146439201	199853
d2c4a6e17f14d9364e14d64c05421fd5a0a7d10f	comparing communication in two languages employing buffered message-passing	lenguaje programacion;cospol;programming language;programacion paralela;estudio comparativo;parallel programming;transmission message;message transmission;etude comparative;modelo;comparative study;message passing;langage programmation;modele;models;kahn et macqueen;programmation parallele;transmision mensaje	Parallel programming fanguages are usually compared informally. Such informal comparisons have led to considerable disagreement and debate about the relationship between various parallel programming languages and features. This article presents a model that permits more precise comparisons for a particular class of parallel programming languages: those in which the communication between the parallel processes is via a first-in, first-out queue of messages. This model was developed to assist with the detailed comparisons between the communication aspect of various parallel programing languages, required as part of the design of a new parallel programming language. The model is illus~ated by describing interprocess communi~tin in two buffered message-passing languages: the language of Kahn and MacQueen, and the language Cospol.	fifo (computing and electronics);kahn process networks;message passing;parallel computing;parallel programming model;programming language;triangulated irregular network	Chris D. Marlin;Dennis Freidel	1990	Journal of Systems and Software	10.1016/0164-1212(90)90073-U	fourth-generation programming language;first-generation programming language;parallel computing;message passing;declarative programming;language primitive;programming domain;computer science;programming language generations;theoretical computer science;third-generation programming language;functional logic programming;comparative research;programming paradigm;low-level programming language;fifth-generation programming language;programming language theory;programming language;second-generation programming language;high-level programming language;comparison of multi-paradigm programming languages;algorithm;control flow analysis;parallel programming model	PL	-16.369919040500946	40.95133285717531	199873
