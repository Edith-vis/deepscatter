id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
09de0f5e88979ee7ba3a73942c44badbc4797bb9	voice pitch changing by linear predictive coding method to keep the singer's personal timbre			linear predictive coding	Yoshinari Sasahira;Shuji Hashimoto	1995			linear predictive coding;speech recognition;computer science;timbre	ML	-13.481841743027042	-86.4897318718413	183058
c923b5313bf94bc73ec519f5b6dd835e460b9b17	exploiting vocal-source features to improve asr accuracy for low-resource languages		A traditional framework in speech production describes the output speech as an interaction between a source excitation and a vocal-tract configured by the speaker to impart segmental characteristics. In general, this simplification has led to approaches where systems that focus on phonetic segment tasks (e.g. speech recognition) make use of a front-end that extracts features that aim to distinguish between different vocal-tract configurations. The excitation signal, on the other hand, has received more attention for speaker-characterization tasks. In this work we look at augmenting the front-end in a recognition system with vocal-source features, motivated by our work with languages that are low in resources and whose phonology and phonetics suggest the need for complementary approaches to classical ASR features. We demonstrate that the additional use of such features provides improvements over a state-of-the-art system for low-resource languages from the BABEL Program.	automated system recovery;level of detail;speech recognition;tract (literature)	Raul Fernandez;Jia Cui;Andrew Rosenberg;Bhuvana Ramabhadran;Xiaodong Cui	2014			speech recognition;artificial intelligence;pattern recognition;computer science	NLP	-18.6757363481681	-84.96770935973412	183106
a775413d821f0f65d7a9a6a3b8cb15b1955a25e9	probability distributions of grapheme frequencies in irish and manx		Abstract Drawing on the work of Grzybek and colleagues on Slavic and German, this study tests three hypotheses regarding the probability distribution of grapheme frequencies in Irish and Manx. First, is the 1-displaced negative hypergeometric distribution an appropriate theoretical distribution for grapheme frequencies in these languages? Second, does the language with the larger grapheme inventory exhibit larger values of the parameter K in this distribution? Third, are the parameters K and M of the 1-displaced negative hypergeometric distribution positively related in both languages? An analysis of 32 parallel texts (16 Irish + 16 Manx) suggests that the 1-displaced negative hypergeometric distribution is an appropriate model for both languages and that the parameters K and M are positively related. However, the analysis does not support the hypothesis that the language with the larger grapheme inventory also has higher values of K. Further work needs to clarify how far this latter finding extends beyon...		Andrew Wilson	2013	Journal of Quantitative Linguistics	10.1080/09296174.2013.799919	arithmetic;natural language processing;speech recognition;mathematics	NLP	-12.38710836595439	-80.55421621915872	183182
5e1f1b09b72dcf116f87807646be0c50bc42bd39	modèles de génération de trajectoires pour l'animation de visages parlants. (trajectories generation models for talking heads animation)		The work performed during this thesis concerns visual speech synthesis in the context of humanoid animation. Our study proposes and implements control models for facial animation that generate articulatory trajectories from text. We have used 2 audiovisual corpuses in our work. First of all, we compared objectively and subjectively the main state-of-the-art models. Then, we studied the spatial aspect of the articulatory targets generated by HMM-based synthesis and concatenation-based synthesis that combines the advantages of these methods. We have proposed a new synthesis model named TDA (Task Dynamics for Animation). The TDA system plans the geometric targets by HMM synthesis and executes the computed targets by concatenation of articulatory segments. Then, we have studied the temporal aspect of the speech synthesis and we have proposed a model named PHMM (Phased Hidden Markov Model). The PHMM manages the temporal relations between different modalities related to speech. This model calculates articulatory gestures boundaries as a function of the corresponding acoustic boundaries between allophons. It has been also applied to the automatic synthesis of Cued speech in French. Finally, a subjective evaluation of the different proposed systems (concatenation, HMM, PHMM and TDA) is presented. Key-words : audiovisual synthesis, coarticulation, facial animation, HMM, concatenation, evaluation.		Oxana Govokhina	2008				Graphics	-15.320622141479461	-81.84307529479133	183807
5e827545e01f930b53bdbfa9630a9bad2695b5cd	perceptual contract and stability in vowel systems: a 3-d simulation study.	simulation study			Jean-Luc Schwartz;Louis-Jean Boë;Pascal Perrier;Bernard Guérin;Pierre Escudier	1989			simulation;speech recognition;communication	ECom	-12.982110442028741	-86.17573369025895	183868
68adb6799d406f181b25fb727a4992976bdf48ec	design and recording of czech speech corpus for audio-visual continuous speech recognition	katedra kybernetiky;kybernetika;publikace design and recording of czech speech corpus for audio visual continuous speech recognition;informacni a řidici systemy;automaticke řizeni;uměla inteligence;publications design and recording of czech speech corpus for audio visual continuous speech recognition		speech corpus;speech recognition	Petr Císar;Milos Zelezný;Zdenek Krnoul;Jakub Kanis;Jan Zelinka;Ludek Müller	2005			natural language processing;speech recognition;speech corpus;acoustic model;linguistics	NLP	-15.35654834450614	-85.85268223063498	184403
651a2002124c4c5fb89b9680ce1a91a7d681299b	speech intelligibility tested by the pediatric matrix sentence test in 3-6 year old children	sentence matrix test;speech intelligibility in children;speech reception threshold	Objective: The present study was aimed at the development and application of the Polish Pediatric Matrix Sentence Test (PPMST) for testing speech intelligibility in normal-hearing (NH) and hearing-impaired (HI) children aged 3-6. Methods & Procedures: The test was based on sentences of the subject-verb-object pattern. Pictures illustrating PPMST utterances were prepared and the picture-point (PP) method was used for administering the 1-up/1-down adaptive procedure converging the signal to noise ratio (SNR) to the speech reception threshold (SRT). The correctness of verbal responses (VR), preceding PP responses, was also judged. Outcomes & Results: The normative SRT for the PP method was shown to decrease with age. The guessing rate (@c) turned out to be close to the theoretical value for forced-choice procedures, @c=1/n, where n=6 for the six-alternative PP method (@c~0.166) and n=4 for the four-alternative PP method (@c~0.25). Test optimization resulted in minimizing the lapse rate (@l) (ratio @c/@l~8.0 for n=4 and @c/@l~5.6 for n=6, both for NH and HI children). Significantly higher SRTs were observed for HI children than for the NH group. Conclusions & Implications: For children aged 3-6, tested by the developed PPMST, speech intelligibility performance, for both the VR and PP method, increases with age. For each age group, significantly worse intelligibility was observed for HI children than for NH children. The PPMST combined with the PP method is a reliable tool for pediatric speech intelligibility measurements.	intelligibility (philosophy)	Edward Ozimek;Dariusz Kutzner;Pawel Libiszewski	2012	Speech Communication	10.1016/j.specom.2012.06.001	speech recognition;computer science	NLP	-11.974022705324337	-82.27996750153262	186629
e09a4984c0d77a9d3cf2e39066b5504633fbe182	driving a speech synthesizer from conceptual input in the context of a voice dialogue system.	dialogue system		dialog system;dialog tree;speech synthesis	Nick J. Youd;Frank Fallside	1989			speech recognition;computer science	NLP	-15.617187734054744	-85.27349167710459	187060
65b2ae4b27f623391a81f30deccc3c1cf2a66666	on unit selection algorithms and their evaluation in non-uniform unit speech synthesis			algorithm;speech synthesis	Kazuya Takeda;Katsuo Abe;Yoshinori Sagisaka	1990			computer science;speech recognition;speech synthesis	NLP	-14.89850639830304	-86.03820098198726	187317
32708cc4188d980a52e86029f428aab059cc8cb7	developing component scores from natural language processing tools to assess human ratings of essay quality	automatic essay scoring;automatice writing evaluation;intelligent tutoring systems;natural language processing	This study explores correlations between human ratings of essay quality and component scores based on similar natural language processing indices and weighted through a principal component analysis. The results demonstrate that such component scores show small to large effects with human ratings and thus may be suitable to providing both summative and formative feedback in an automatic writing evaluation systems such as those found in Writing-Pal.	natural language processing	Scott A. Crossley;Danielle S. McNamara	2014			natural language processing;speech recognition;computer science;multimedia	NLP	-17.203117655340165	-81.24234515253363	187713
2d456997abc4a82aa64739b157566c4782f1b935	speech recognition using the atomic speech units constructed from overlapping articulatory features	speech recognition		speech recognition	Li Deng;Don X. Sun	1993			artificial intelligence;speech recognition;pattern recognition;computer science	NLP	-14.863659251289105	-86.44817929615284	187852
781cee8081ed13c30a2a9cf1f8c00401ad154615	speech recognition for an information kiosk	speech coding;real time systems;natural languages;face recognition;real time;reduced instruction set computing;speech processing;language model;speech recognition;decoding	In the context of the ESPRIT MASK project we face the problem of adapting a “state-of-the-art” laboratory speech recognizer for use in the real world with naive users. The speech recognizer is a softwareonly system that runs in real-time on a standard Risc processor. All aspects of the speech recognizer have been reconsidered from signal capture to adaptive acoustic models and language models. The resulting system includes such features as microphone selection, response cancellation, noise compensation, query rejection capability and decoding strategies for real-time recognition.	acoustic cryptanalysis;algorithm;finite-state machine;language model;microphone;real-time clock;real-time transcription;rejection sampling;signal-to-noise ratio;speech recognition;trigram;vocabulary	Jean-Luc Gauvain;J. J. Gangolf;Lori Lamel	1996			voice activity detection;natural language processing;speech recognition;computer science;speech processing;acoustic model;communication;speech analytics	Vision	-15.625320754310481	-86.6970113392676	187999
dda469ca136b99addff98176774642ca95a93855	hmm-based indonesian speech synthesis system with declarative and question sentences intonation	speech synthesis hidden markov models natural language processing speech intelligibility;hidden markov model indonesian speech synthesis system declarative sentence intonation question sentence intonation speech quality mcd method;indonesian;speech hidden markov models training data training speech synthesis degradation data models;declarative sentence;question sentence;f0 indonesian hmm based speech synthesis system declarative sentence question sentence;hmm based speech synthesis system	In this paper, we present a result of HMM-based speech synthesis system applied to Indonesian with prosody information in declarative and question sentences. The purpose is to observe the speech quality of synthesized speech from declarative to question sentences, conversely. Variation is given in kind of sentences and training data amount. We use 44, 72, 116, 450, 929 and 1379 training data. The result were evaluated by objective and subjective test. In objective test using MCD method earn the best score for question sentences with score 4.32 in 450 training data. Then for declarative sentences with score 5.13 in 929 training data. Subjective test with DMOS method obtain naturalness for declarative and question sentences with score 3.53 and 3.36 in 1379 training sentences respectively. The result shows that “degradation speech is slightly annoying”, it is possibly caused by poor F0 estimates.	algorithm;computation;declarative programming;elegant degradation;hidden markov model;interpolation;magnetic circular dichroism;semantic prosody;speech synthesis;time complexity	Elok Cahyaningtyas;Dhany Arifianto	2015	2015 International Symposium on Intelligent Signal Processing and Communication Systems (ISPACS)	10.1109/ISPACS.2015.7432756	natural language processing;speech recognition;computer science;linguistics	NLP	-18.9717412494402	-83.76766551322078	188004
6655a0de85465772484a2cf1ff21b50fd02328cb	interaction quality estimation in spoken dialogue systems using hybrid-hmms		Research trends on SDS evaluation are recently focusing on objective assessment methods. Most existing methods, which derive quality for each systemuser-exchange, do not consider temporal dependencies on the quality of previous exchanges. In this work, we investigate an approach for determining Interaction Quality for human-machine dialogue based on methods modeling the sequential characteristics using HMM modeling. Our approach significantly outperforms conventional approaches by up to 4.5% relative improvement based on Unweighted Average Recall metrics.	baseline (configuration management);experiment;hidden markov model;logic programming;markov chain;programming paradigm;rule induction;software quality	Stefan Ultes;Wolfgang Minker	2014			speech recognition;computer science;machine learning;data mining	NLP	-17.071576665368823	-86.93808311067485	188862
9e3accd46eaa8e48e7044a63b3d3ef4e6009c895	regression-based tempo recognition from chroma and energy accents for slow audio recordings				Thorsten Deinert;Igor Vatolkin;Günter Rudolph	2011			aesthetics;multimedia;communication	HCI	-13.026325638146073	-86.02380399005361	188981
61b0e53e97ba8c25818f9ce7319b5d425df377ee	is there an emotion signature in intonational patterns? and can it be used in synthesis?				Tanja Bänziger;Michel Morel;Klaus R. Scherer	2003			artificial intelligence;speech recognition;pattern recognition;computer science	Logic	-14.35552411145266	-85.7753756639986	189294
440c88b6fc292df8af07e8aebb322fb10ec74efc	analysis of intraperiodic formant modulations in spoken and sung vowels		Intraperiodic formant modulations in spoken and sung vowels from four Finnish male opera singers were analyzed by using analytic filtering method and modified covariance type of linear prediction. Both methods revealed clear formant modulations. Differences were seen between spoken and sung samples and between individuals.		Unto K. Laine;Anne-Maria Laukkanen;Timo Leino	1999			formant;speech recognition;modulation (music);voice analysis;computer science;speech production	Comp.	-12.503003165068467	-86.53955596017697	189906
d612213dd488f159dadc6acb46b82fe1cbd7bf13	drum transcription via classification of bar-level rhythmic patterns	conference proceeding	We propose a novel method for automatic drum transcription from audio that achieves the recognition of individual drums by classifying bar-level drum patterns. Automatic drum transcription has to date been tackled by recognising individual drums or drum combinations. In high-level tasks such as audio similarity, statistics of longer rhythmic patterns have been used, reflecting that musical rhythm emerges over time. We combine these two approaches by classifying bar-level drum patterns on sub-beat quantised timbre features using support vector machines. We train the classifier using synthesised audio and carry out a series of experiments to evaluate our approach. Using six different drum kits, we show that the classifier generalises to previously unseen drum kits when trained on the other five (80% accuracy). Measures of precision and recall show that even for incorrectly classified patterns many individual drum events are correctly transcribed. Tests on 14 acoustic performances from the ENST-Drums dataset indicate that the system generalises to real-world recordings. Limited by the set of learned patterns, performance is slightly below that of a comparable method. However, we show that for rock music, the proposed method performs as well as the other method and is substantially more robust to added polyphonic accompaniment.	acoustic cryptanalysis;drum memory;experiment;high- and low-level;medical transcription;performance;precision and recall;support vector machine;transcription (software)	Lucas Thompson;Simon Dixon;Matthias Mauch	2014			simulation;speech recognition;computer science	Web+IR	-18.347520888685253	-82.90074260217327	189976
fc6ea594dcd58aee390574157e0017ffd3c545bf	a study on pitch pattern generation using hmm-based statistical information	hidden markov model;speech synthesis	This paper describes a novel pitch pattern generation method for speech synthesis using Hidden Markov Models (HMMs). In the proposed method, the F0 contours of minor phrase are modeled by HMMs (pitch-HMMs). The pitch-HMMs are trained using F0 and 1F0 considering phonetic environments (e.g. accent type, mora count, mora position, phonemic category, etc.). To evaluate the pitch-HMMs, accent identication experiments are performed. The results indicate that the pitch-HMMs can capture the movement in F0 contours appropriately. In the F0 contour generation experiments, the proposed method yields an averaged root mean square error of 132cent (equivalent to 9.2Hz at 120Hz) between the original and the generated F0 contours. Furthermore, an application of the proposed method to text-to-speech system is also discussed.	contour line;experiment;hidden markov model;markov chain;mean squared error;norm (social);pitch (music);speech synthesis	Toshiaki Fukada;Yasuhiro Komori;Takashi Aso;Yasunori Ohora	1994			artificial intelligence;speech recognition;hidden markov model;pattern recognition;markov model;computer science;speech synthesis	Robotics	-16.981619487121662	-84.61831924536105	190207
6d831c02d0dba706f38cf740ff98dc9ab784a12b	unit selection using linguistic, prosodic and spectral distance for developing text-to-speech system in hindi	linguistic features;unit selection;prosodic features and spectral features;text to speech	In this paper we propose a new method for unit selection in developing text-to-speech (TTS) system for Hindi. In the proposed method, syllables are used as basic units for concatenation. Linguistic, positional and contextual features derived from the input text are used at the first level in the unit selection process. The unit selection process is further refined by incorporating the prosodic and spectral characteristics at the utterance and syllable levels. The speech corpora considered for this task is the broadcast Hindi news read by a male speaker. Synthesized speech from the developed TTS system using multi-level unit selection criterion is evaluated using listening tests. From the evaluation results, it is observed that the synthesized speech quality has improved by refining the unit selection process using spectral and prosodic features.		K. Sreenivasa Rao;Sudhamay Maity;Amol Taru;Shashidhar G. Koolagudi	2009		10.1007/978-3-642-11164-8_86	natural language processing;speech recognition;computer science;speech synthesis	NLP	-17.611374400215745	-84.31848687359778	190299
6540f5d076f3d242e383b1f3daaf3ec70a1a7bfc	evaluating synthesiser performance: is segmental intelligibility enough?			intelligibility (philosophy)	Kim E. A. Silverman;Sara Basson;Suzi Levas	1990			speech recognition	HPC	-14.246814672254231	-85.12749404309412	190782
edc7ce8e3f4ae36d6781aa1e8f469e43cc9d690b	singing voice detection across different music genres				Florian Scholz;Igor Vatolkin;Günter Rudolph	2017			vocal music;acoustics;art;singing	NLP	-13.414131863356499	-86.06459880276667	192297
2a5c397f6bc8354fb6a98e38ad98b6444ae19e25	automatic speech recognition for assistive writing in speech supplemented word prediction	automatic speech recognition;word prediction	This paper describes a system for assistive writing, the Speech Supplemented Word Prediction Program (SSWPP). This system uses the first letter of a word typed by the user as well as the user’s (possibly low-intelligibility) speech to predict the intended word. The ASR system, which is the focus of this paper, is a speaker-dependent isolated-word recognition system. Word-level results from a non-dysarthric speaker indicate that almost all errors could be corrected by the SSWPP language model. Results from five speakers with moderate to severe dysarthria (average intelligibility 61.7%) averaged 62% for word recognition and 65% for out-ofvocabulary identification.	intelligibility (philosophy);language model;speech recognition	John-Paul Hosom;Tom Jakobs;Allen Baker;Susan Fager	2010			voice activity detection;natural language processing;speaker recognition;audio mining;speech recognition;factored language model;word error rate;computer science;acoustic model;logogen model;speech analytics	NLP	-18.71060202046745	-84.24756690608912	193020
ba3e078e62208749e8d609323c939f76750ade70	a hierarchical perception-linked model for machine recognition of phonemes	phoneme;modelizacion;lenguaje natural;formant;reconocimiento palabra;bengali;langage naturel;fonema;phonem;percepcion;classification;modelisation;modelo;asie;telugu;natural language;speech recognition;modele;reconnaissance parole;perception;formante;modeling;models;clasificacion;india;asia;inde	Abstract   The present paper describes a hierarchical perception-linked model (HPLM) for recognition of all phonemes that have either characteristic formant structures or significant effects on the formant structures of the adjoining phonemes. The model essentially converts the multiclass multiparameter classification problem into a hierarchy of two-class single-parameter classification problems. The model is based on the perceptual phenomenon of substitution of  F  1  and  F  2  by a composite formant when their distance is less than a certain threshold and of comparison with stored prototypes otherwise. It is further shown that the above two perceptually different situations can also be viewed as the composite frequency lying between  F  1  and  F  2  in one situation and that lying below  F  1  in the other situation. The model is tested on adequate samples from Telugu vowels and laterals, Telugu unaspirated plosives, and Bengali vowels for many speakers. The recognition scores obtained for Telugu vowels, Telugu plosives, and Bengali vowels are 79%, 72%, and 83%, respectively.		A. K. Datta;N. R. Ganguli;Anuradha Ray;B. Mukherjee	1991	Inf. Sci.	10.1016/0020-0255(91)90017-O	speech recognition;formant;computer science;natural language;perception;bengali	AI	-12.46227085253045	-85.70587603944436	193631
6bcc3d360d25c904ab1d745c730e6987f8ef8924	prosodic annotation in a thai text-to-speech system	conference paper	This paper describes a preliminary work on prosody modeling aspect of a text-tospeech system for Thai. Specifically, the model is designed to predict symbolic markers from text (i.e., prosodic phrase boundaries, accent, and intonation boundaries), and then using these markers to generate pitch, intensity, and durational patterns for the synthesis module of the system. In this paper, a novel method for annotating the prosodic structure of Thai sentences based on dependency representation of syntax is presented. The goal of the annotation process is to predict from text the rhythm of the input sentence when spoken according to its intended meaning. The encoding of the prosodic structure is established by minimizing speech disrhythmy while maintaining the congruency with syntax. That is, each word in the sentence is assigned a prosodic feature called strength dynamic which is based on the dependency representation of syntax. The strength dynamics assigned are then used to obtain rhythmic groupings in terms of a phonological unit called foot. Finally, the foot structure is used to predict the durational pattern of the input sentence. The aforementioned process has been tested on a set of ambiguous sentences, which represents various structural ambiguities involving five types of compounds in Thai.	emoticon;heart rate variability;high-level programming language;intelligibility (philosophy);semantic prosody;speech synthesis;speech technology;spontaneous order;synthetic intelligence;text-based user interface	Siripong Potisuk	2007			rhythm;natural language processing;encoding (memory);prosody;syntax;artificial intelligence;annotation;computer science;sentence;speech synthesis;phrase	NLP	-12.333419593637366	-80.53587801711	194507
812526b00dccc3397a54d83048be13932dd34b6a	computer assisted chinese birthday couplets generation		We propose a method of generating birthday couplets fitting to gender, age and birthday time. The work of this paper are as follows: Using memory mapping file to load large number of files which can significantly reduce the initializing time; Using the TFIDF method to train key words related age and gender; Using a dynamic Bi-gram graph model to generate the more semantically coherent left roll which can conversely alter the weights between words of the model.		Shunting Wang;Shicheng Zhang;Zhigeng Pan	2016	T. Edutainment	10.1007/978-3-662-50544-1_17	machine learning;initialization;artificial intelligence;memory-mapped file;tf–idf;speech recognition;graph;computer science	NLP	-13.589638117857584	-80.40616996871515	195156
80c9234633f1f9e4e7341db2cd0d73fb0ee37e16	automatic melody transcription based on chord transcription		This paper focuses on automatic melody transcription in a situation where a chord transcription is already available. Given an excerpt of music in audio form and a chord transcription in symbolic form, the task is to create a symbolic melody transcription that consists of note onset times and pitches. We present an algorithm that divides the audio into segments based on the chord transcription, and then matches potential melody patterns to each segment. The algorithm uses chord information to favor melody patterns that are probable in the given harmony context. To evaluate the algorithm, we present a new ground truth dataset that consists of 1,5 hours of audio excerpts together with hand-made melody and chord transcriptions.	algorithm;ground truth;medical transcription;onset (audio);transcription (software)	Antti Laaksonen	2014			melody;speech recognition;computer science;world wide web	Comp.	-18.365492745180834	-82.66504281356093	195594
d3f02debe86927ded6e2d657d4aea968d16fba84	filtering wiktionary triangles by linear mbetween distributed word models				Márton Makrai	2016			speech recognition;natural language processing;artificial intelligence;filter (signal processing);computer science	NLP	-14.199910690587412	-86.51651873016364	196157
3b8ff30cbbbdde434755bc53871643f8f558e6fd	modeling topic coherence for speech recognition	base-line system;topic continuity;particular longer-range word dependency;speech recognizers;word error rate;current speech recognition system;topic coherence;possible word error improvement;base-line speech recognition system;word preference;speech recognition	"""St, atist,ical langmtge models play a major role in current spee~(:h re.cognition systems. Most of these models have ti)cussed on relatively local interactions between words. R(.'(:ently, however, |her('. have been sevcr;d a t tempts to incorporate other knowlcdg(; source.s, in particular long(x-range word (tet)(;nden(:ies, in order to improve. Sl)(.~ech r(;(:ognize.rs. We will 1)rcs(~.nt one. such m('.t;ho(l, which tries to autonmticatly utilize t)rolmri;ics of topic continuity. Whim a l)asc-linc. spee.ch re.(x)gnil;ion sysl;em gencra, l;('.s a.[ternativ(', hypothe.s(~s for a senl;enc( L we will ul~ilize the word prefercn(:(~s based on topic coherence to sele(:t tim b(;st hy~ pothesis. In our experiment, we achi(wed a 0.65% imI)rovenmn|; in the wor(1 eiror rat(', on top of t;h(; base-lin(! sysi;em. It corre.sponds to 10A0% (if tlm possit)le word error improvement. 1 I n t r o d u c t i o n Statistical language models play ~ major role. in current language i)rocessing applications. Most of these models have fbcussed on r('~lative.ly local interactions betwe.en words, in t~articular, large. vocabulary sl)eech recognition systems have. used primarily bi-gram and tri~grmn language mod('.ls. Recently, howev(;r, there have been several atte.mt)t,s to incorl)orate other knowl(~dge. SOlll'C(~,8~ and in pa.rticular longer-range word (tepe.nd(mcies, in order l;o improve speech recognizers, th'.rc, 'loi~ger-range d(~tienden(:ics' me,ms dependencies extending beyoiM several words or beyond senl;(mce boundmies. There have be.en several al;l;eml)l;s in the last few years to make use of these prot)erti('~s. One of them is the """"cache language mode.l"""" (Kulm, 1988) (aelinek et al., 1.991) (Kul, icc, 1989). This is a dynamic language model which ul,ilizes the partially dictated document (""""cache."""") in order to predict the next word. In essence, it is based on the ol)se.rvation l;ll&(; ;~ wor(t whi(:h has ah'cady al)1)care(1 in a (locum(.,nt has ;m incr(;ased i)rol)ability of r(;at)ticaring. Jelind¢ showed tim usefuhmss of this method in terms of spe(;ch re.(:ognii;ion quality. For sllorI; (locllIll(.~II~,s~ how('.v(w~ SllCh ~)~s lieWSt)3I)cr f~rl;icl(;s, th('. mnnl)cr of words whi(;h can 1)e a(:-cunml~t(;(t fi'om tim l/rior text will be small and accordingly the b(;nelit of |;tie niethod will gen('xally tie small. l~os('.nti'M proliosed t;he %rigger model"""" to try to ov('.r(:om(~' this limitation (RosentbM, 1992). lie used a large (:orpus to build a s('.t of """"trigger tmirs"""", each of which consists of it l)a.ir of words al)t)('.arbig in a single (to(:um(mt ot: a liirg(~ corpus. Th(~se pairs ar('. used as a (:omI)oncnl; in th('~ t)rot)nt)ilisti(: mo(M. If a particular word w apt)('.ars in the 1)receding t('~xt of do(:mncnt, the model will 1)red|(:( a. lmightened t)rot)al)ility not just for w t )u t also for all th(; wor(ls related to w through a trigger I/a.ir. Our apt)roa(:h can b(; briefly summarize(l as follows. The topic or sut)jecl; mat te r of ;m mtitle influcn(:(;s its linguistic l)rot)eri;i(;s, su(:h as word (;hoic(~ and (:o-oc(:m'ren(:e patt(~rns; in ctl'(~ct it giv('.s rise |,o a very Sl)ccializc(l """"sublmlguag(~"""" for th;tt topic. We try to find the sul)languag(~ to which t,h(', art|el(; 1Mongs based on the sentences already recognized. At a cert;fin stag(; of the st)ee(:h recognition processing of an art|el('., w o r d s in |;hi; pr( ;viot ls ll|;|;(2r~rlic(?s &ro s(~.le(;i;(Rt ~)~s keywords. Then, based on the keywords, similm' arti('.h.'s are retri('.ved from a large corpus t)y a nmthod similar to that used iil information retrieval. They are asseml)l('~d into a subla.nguag(', """"mini-cortms"""" for tim mti(:h',. Then wc analyze the mini-(:orlms in or(lcr to d(~tc, rminc word l)rcf(;rcn(:c whi(:h will l)e used in analyzing the following sent(mcc. The &;tails (if e, ach stet) will be described lat(~u'. Our work is similar to I;ll~I; using trigger t)~firs. ltowever, the triggea' |);fir approach does a. v(uy broad s('~a.rch, retrieving m'ti(:h;s which have a n y word in common wil;h the, 1)rior discourse'. ()llr appro~,ch, in contrast, makes a Inllch l n o r c fOCllSSe(t sear(:h, taking only a small set of articles most similar to the prior discourse. This may allow us to make, sharper t)redictions in the case of well-"""	finite-state machine;information retrieval;interaction;linc;language model;local interconnect network;off topic;scott continuity;speech recognition;tails;triangular function;vocabulary;ical	Satoshi Sekine	1996			natural language processing;cache language model;speech recognition;factored language model;word error rate;computer science;linguistics;logogen model	NLP	-18.62273347060684	-80.38687807424259	196191
6533369bfc28111de64e04a467898176ef5a485e	video-realistic expressive audio-visual speech synthesis for the greek language		High quality expressive speech synthesis has been a long-standing goal towards natural human-computer interaction. Generating a talking head which is both realistic and expressive appears to be a considerable challenge, due to both the high complexity in the acoustic and visual streams and the large non-discrete number of emotional states we would like the talking head to be able to express. In order to cover all the desired emotions, a significant amount of data is required, which poses an additional time-consuming data collection challenge. In this paper we attempt to address the aforementioned problems in an audio-visual context. Towards this goal, we propose two deep neural network (DNN) architectures for Video-realistic Expressive Audio-Visual Text-To-Speech synthesis (EAVTTS) and evaluate them by comparing them directly both to traditional hidden Markov model (HMM) based EAVTTS, as well as a concatenative unit selection EAVTTS approach, both on the realism and the expressiveness of the generated talking head. Next, we investigate adaptation and interpolation techniques to address the problem of covering the large emotional space. We use HMM interpolation in order to generate different levels of intensity for an emotion, as well as investigate whether it is possible to generate speech with intermediate speaking styles between two emotions. In addition, we employ HMM adaptation to adapt an HMM-based system to another emotion using only a limited amount of adaptation data from the target emotion. We performed an extensive experimental evaluation on a medium sized audio-visual corpus covering three emotions, namely anger, sadness and happiness, as well as neutral reading style. Our results show that DNN-based models outperform HMMs and unit selection on both the realism and expressiveness of the generated talking heads, while in terms of adaptation we can successfully adapt an audio-visual HMM set trained on a neutral speaking style database to a target emotion. Finally, we show that HMM interpolation can indeed generate different levels of intensity for EAVTTS by interpolating an emotion with the neutral reading style, as well as in some cases, generate audio-visual speech with intermediate expressions between two emotions.		Panagiotis Paraskevas Filntisis;Athanasios Katsamanis;Pirros Tsiakoulis;Petros Maragos	2017	Speech Communication	10.1016/j.specom.2017.08.011	data collection;speech recognition;streams;computer science;interpolation;pattern recognition;artificial neural network;expressivity;hidden markov model;machine learning;artificial intelligence;expression (mathematics);speech synthesis	NLP	-18.76681311888375	-84.23853857053095	196631
4661deca4f7cb20b65bc3ca547469a9bc70ad255	'speech-smile', 'speech-laugh', 'laughter' and their sequencing in dialogic interaction	no 1 2;vol 65;phonetique;acoustic analysis;statistical evaluation;238712;allemand;phonetics;german;rire;phonetica 2008	Laughing is examined auditorily and acoustico-graphically, on the basis of exemplary speech data from spontaneous German dialogues, as pulmonic air stream modulation for communicative functions, paying attention to fine phonetic detail in interactional context. These phonetic case descriptions of laughing phenomena in speaker interaction in a small corpus have as their goal to create an awareness of the phonetic and functional parameters that need to be considered in the future acquisition, acoustic analysis and statistical evaluation of large spontaneous databases.	acoustic cryptanalysis;body of uterus;database;description;modulation;speech disorders;spontaneous order;text corpus	Klaus J. Kohler	2008	Phonetica	10.1159/000130013	psychology;phonetics;speech recognition;philosophy;german;linguistics;sociology;communication	NLP	-13.079327559704646	-83.75506427144046	197058
5a4c33e8312b72ada12ab00a8e2a55504df8fd18	maximum-likelihood dynamic intonation model for concatenative text-to-speech system		In this work we present a Maximum Likelihood (ML) joint pitch curve modeling, inspired by HMM TTS synthesis concept. This model provides an optimal solution for the coarse target intonation curve (3 points per syllable) and incorporates both static and dynamic pitch values for better utterance intonation modeling. The coarse intonation curve may be optionally combined with the original pitch extracted from the concatenated units, by a technique, named microprosody preservation, which is also described. The latter is intended for reducing pitch modification ratio and improving sound naturalness for large-scale concatenative TTS systems. The proposed model was successfully applied on IBM’s trainable concatenative TTS system improving the subjective intonation quality.	concatenation;hidden markov model;netware file system;pitch shift;speech synthesis;syllable	Slava Shechtman	2007			syllable;concatenation;maximum likelihood;hidden markov model;naturalness;artificial intelligence;utterance;computer science;speech synthesis;pattern recognition	Robotics	-17.97754478345224	-84.7221282611592	197187
e726debe5e57009183478a19afbc9f03acdf2c74	flemish accent identification based on formant and duration features	psi_speech;electronic mail;speaker adaptation	In this paper, we describe a method to identify Flemish accents and examine the contribution of different features. Although Flanders is quite small, we found that, even when speakers are asked to read a text in standard Dutch, regional accents are still present in the speech. Our method uses formant and phoneme duration features in a text-dependent model. A relevant set of features is automatically selected and normalized in a way reminiscent of the eigenvoices approach for speaker adaptation. We found good discrimination results between the accent of Antwerp and Brabant versus the accents of other provinces. However, the confusion between the accent of Limburg versus the accent of East- and West-Flanders shows that the feature set is too restricted,		Pieter-Jan Ghesquiere;Dirk Van Compernolle	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5743826	speech recognition	Robotics	-11.956018208551967	-82.26425735940438	197399
849b573fa7c84449993dd58f411455d1b444b7d1	discriminative-transitional/steady units for spanish continuous speech recognition.	continuous speech recognition		speech recognition	Amparo Varona;M. Inés Torres;Francisco Casacuberta	1995			speech recognition;computer science;acoustic model	ML	-15.446911825172105	-86.07207369559231	197879
c7b2fb8d78fc71b028175f4ed15308dc7ac209dc	animating with style: defining expressive semantics of motion	motion synthesis;feature extraction;human motion;verbal description of motion style;feature selection;computer animation;motion style;style vector	Actions performed by a virtual character can be controlled with verbal commands such as ‘walk five steps forward’. Similar control of the motion style, meaning how the actions are performed, is complicated by the ambiguity of describing individual motions with phrases such as ‘aggressive walking’. In this paper, we present a method for controlling motion style with relative commands such as ‘do the same, but more sadly’. Based on acted example motions, comparative annotations, and a set of calculated motion features, relative styles can be defined as vectors in the feature space. We present a new method for creating these style vectors by finding out which features are essential for a style to be perceived and eliminating those that show only incidental correlations with the style. We show with a user study that our feature selection procedure is more accurate than earlier methods for creating style vectors, and that the style definitions generalize across different actors and annotators. We also present a tool enabling interactive control of parametric motion synthesis by verbal commands. As the control method is independent from the generation of motion, it can be applied to virtually any parametric synthesis method.	feature selection;feature vector;interpolation;numerical analysis;requirement;speech synthesis;usability testing;virtual actor	Klaus Förger;Tapio Takala	2015	The Visual Computer	10.1007/s00371-015-1064-4	computer vision;feature extraction;computer science;machine learning;computer animation;multimedia;feature selection	Graphics	-15.662360760052199	-81.80073744031763	199108
e20b2e0eb2ed1bbc6e6f8ac58b6dd38e47507ed4	fast automatic segmentation and labeling: results on timit and euromo			timit	A. Vorstermanst;Jean-Pierre Martens;Bert Van Coile	1995			artificial intelligence;timit;speech recognition;scale-space segmentation;pattern recognition;computer science;segmentation	Vision	-14.876827123805596	-87.1077885295176	199318
25417d1e942c59f93d13b36b4382d02a1af83ca2	the expressivity of turn-taking: understanding children pragmatics by hybrid classifiers	speech recognition signal classification;feature classifier turn taking expressivity children pragmatics hybrid classifier children age pragmatic skill conversation dynamics simple speech period silence period generative framework discriminative framework classification framework preschool conversation steady conversational period observed influence model feature extractor lasso regression feature selector;speech feature extraction vectors pragmatics training educational institutions accuracy;qa75 electronic computers computer science;signal classification;speech recognition	We analyze the effect of children age on pragmatic skills, i.e. on the way children manage the conversation dynamics. In particular, we focus exclusively on the turn-taking (who talks when and how much), reducing conversations as sequences of simple speech/silence periods. Employing a hybrid (generative + discriminative) classification framework, we demonstrate that such a simple signature is very informative, allowing to separate 22 “pre-School” conversations (between 3-4 years old children) and 24 “School” conversations (between 6-8 years old children) subjects, with 78% of accuracy. The framework exploits Steady Conversational Periods and Observed Influence Models as feature extractors, plus LASSO regression as feature selector and classifier. The generative nature of our method permits, as byproduct, to identify the pragmatic skills that better discriminate the two groups: no-tably, scholar children tend to have more frequent periods of sustained conversation, in a statistically significant way.	feature extraction;information;least squares	Cristina Segalin;Anna Pesarin;Alessandro Vinciarelli;Monja Tait;Marco Cristani	2013	2013 14th International Workshop on Image Analysis for Multimedia Interactive Services (WIAMIS)	10.1109/WIAMIS.2013.6616149	natural language processing;speech recognition;computer science;artificial intelligence;machine learning	Web+IR	-12.53685171060182	-82.77431145527999	199322
18834f10abe525c18db429ed7e3d6b67f0bc3e3a	affect-expressive hand gestures synthesis and animation	hidden semi markov models prosody analysis continuous affect gesture animation;speech;speech synthesis computer animation gesture recognition hidden markov models;joints;hidden markov models speech animation joints feature extraction correlation principal component analysis;hidden markov models;feature extraction;principal component analysis;animation;correlation;speech gesture relationship hand gesture synthesis speech gesture synthesis composite communicative signal speech prosody hidden semimarkov model hsmm multimodal analysis framework hand gesture animation	Speech and hand gestures form a composite communicative signal that boosts the naturalness and affectiveness of the communication. We present a multimodal framework for joint analysis of continuous affect, speech prosody and hand gestures towards automatic synthesis of realistic hand gestures from spontaneous speech using the hidden semi-Markov models (HSMMs). To the best of our knowledge, this is the first attempt for synthesizing hand gestures using continuous dimensional affect space, i.e., activation, valence, and dominance. We model relationships between acoustic features describing speech prosody and hand gestures with and without using the continuous affect information in speaker independent configurations and evaluate the multimodal analysis framework by generating hand gesture animations, also via objective evaluations. Our experimental studies are promising, conveying the role of affect for modeling the dynamics of speech-gesture relationship.	acoustic cryptanalysis;hidden markov model;markov chain;multimodal interaction;semantic prosody;semiconductor industry;speech synthesis;spontaneous order	Elif Bozkurt;Engin Erzin;Yücel Yemez	2015	2015 IEEE International Conference on Multimedia and Expo (ICME)	10.1109/ICME.2015.7177478	natural language processing;anime;speech recognition;feature extraction;computer science;speech;machine learning;gesture recognition;correlation;hidden markov model;principal component analysis	Robotics	-14.695470504077848	-83.44144122104147	199404
