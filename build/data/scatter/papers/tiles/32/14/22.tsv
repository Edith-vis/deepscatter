id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
426a74455881a81a964878eea08b7e6b94779165	the design of advanced communication to reduce memory usage for exa-scale systems		Current MPI (Message Passing Interface) communication libraries require larger memories in proportion of the number of processes, and can not be used for exa-scale systems. This paper proposes a global memory based communication design to reduce memory usage for exascale communication. To realize exa-scale communication, we propose true global memory based communication primitives called Advanced Communication Primitives (ACPs). ACPs provide global address, which is able to use remote atomic memory operations on the global memory, RDMA (Remote Direct Memory Access) based remote memory copy operation, global heap allocator and global data libraries. ACPs are different from the other communication libraries because ACPs are global memory based so that house keeping memories can be distributed to other processes and programmers explicitly consider memory usage by using ACPs. The preliminary result of memory usage by ACPs is 70MB on one million processes.	exa;library (computing);message passing interface;programmer;remote direct memory access	Shinji Sumimoto;Yuichiro Ajima;Kazushige Saga;Takafumi Nose;Naoyuki Shida;Takeshi Nanri	2016		10.1007/978-3-319-61982-8_15	remote memory;remote direct memory access;computer science;computer architecture;embedded system;distributed computing;message passing interface;allocator;communication design;heap (data structure)	HPC	-13.029669867110384	46.63223760034964	136729
5d3ab929a09498913dcd2b23980bb1751abc73c2	lessons learned from memory errors observed over the lifetime of cielo		Maintaining the performance of high-performance computing (HPC) applications as failures increase is a major challenge for next-generation extreme-scale systems. Recent work demonstrates that hardware failures are expected to become more common. Few existing studies, however, have examined failures in the context of the entire lifetime of a single platform. In this paper, we analyze a corpus of empirical failure data collected over the entire five-year lifetime of Cielo, a leadership-class HPC system. Our analysis reveals several important findings about failures on Cielo: (i) its memory (DRAM and SRAM) exhibited no aging effects; detectable, uncorrectable errors (DUE) showed no discernible increase over its five-year lifetime; (ii) contrary to popular belief, correctable DRAM faults are not predictive of future uncorrectable DRAM faults; (iii) the majority of system down events have no identifiable hardware root cause, highlighting the need for more comprehensive logging facilities to improve failure analysis on future systems; and (iv) continued advances will be needed in order for current failure mitigation techniques to be viable on future systems. Our analysis of this corpus of empirical data provides critical analysis of, and guidance for, the deployment of extreme-scale systems.	cielo (supercomputer)	Scott Levy;Kurt B. Ferreira;Nathan DeBardeleben;Taniya Siddiqua;Vilas Sridharan;Elisabeth Baseman	2018			memory management;software deployment;static random-access memory;reliability engineering;root cause;cielo;memory errors;dram;distributed computing;computer science;supercomputer	Logic	-17.61846730166165	49.50520476345076	137349
d617310c93888baa08aa9a898663d5baf698bc66	a comparison of logical and physical parallel i/o patterns	scientific application;operating system;parallel file system;parallel i o;device driver;parallel applications	Although there are several extant studies of parallel scientiic application request patterns, there is little experimental data on the correlation of physical input/output patterns with application input/output stimuli. To understand these correlations , we have instrumented the SCSI device drivers of the Intel Paragon OSF/1 operating system to record key physical input/output activities and have correlated this data with the input/output patterns of scientiic applications captured via the Pablo analysis toolkit. Our analysis shows that disk hardware features profoundly aaect the distribution of request delays and that current parallel le systems respond to parallel application input/output patterns in non-scalable ways.	device driver;input/output;intel paragon;operating system;parallel i/o;scsi;scalability;tru64 unix	Huseyin Simitci;Daniel A. Reed	1998	IJHPCA	10.1177/109434209801200305	parallel computing;computer hardware;computer science;operating system;programming language	Arch	-13.93099339799101	50.26458284739324	138612
1044d92478de59f78f28f3acf48deddbcbd716e0	optimizing fastquery performance on lustre file system	i o performance;fastquery;tuning	FastQuery is a parallel indexing and querying system we developed for accelerating analysis and visualization of scientific data. We have applied it to a wide variety of HPC applications and demonstrated its capability and scalability using a petascale trillion-particle simulation in our previous work. Yet, through our experience, we found that performance of reading and writing data with FastQuery, like many other HPC applications, could be significantly affected by various tunable parameters throughout the parallel I/O stack. In this paper, we describe our success in tuning the performance of FastQuery on a Lustre parallel file system. We study and analyze the impact of parameters and tunable settings at file system, MPI-IO library, and HDF5 library levels of the I/O stack. We demonstrate that a combined optimization strategy is able to improve performance and I/O bandwidth of FastQuery significantly. In our tests with a trillion-particle dataset, the time to index the dataset reduced by more than one half.	clustered file system;hierarchical data format;input/output;lustre;mathematical optimization;optimizing compiler;parallel i/o;petascale computing;scalability;simulation	Kuan-Wu Lin;Surendra Byna;Jerry Chi-Yuan Chou;Kesheng Wu	2013		10.1145/2484838.2484853	parallel computing;real-time computing;computer science;operating system;database;musical tuning	HPC	-14.04646459911074	52.88525881371089	138943
0b0c7febace2998ec9ed93e696d952a47df11be3	scalable and precise dynamic datarace detection for structured parallelism	parallelism;data races;program analysis;geometric mean;parallel programs	Existing dynamic race detectors suffer from at least one of the following three limitations:  (i)space overhead per memory location grows linearly with the number of parallel threads [13], severely limiting the parallelism that the algorithm can handle;  (ii)sequentialization: the parallel program must be processed in a sequential order, usually depth-first [12, 24]. This prevents the analysis from scaling with available hardware parallelism, inherently limiting its performance;  (iii) inefficiency: even though race detectors with good theoretical complexity exist, they do not admit efficient implem entations and are unsuitable for practical use [4, 18].  We present a new precise dynamic race detector that leverages structured parallelism in order to address these limitations. Our algorithm requires constant space per memory location, works in parallel, and is efficient in practice. We implemented and evaluated our algorithm on a set of 15 benchmarks. Our experimental results indicate an average (geometric mean) slowdown of 2.78x on a 16-core SMP system.	algorithm;benchmark (computing);data structure;depth-first search;dynamic data;heterojunction;image scaling;memory address;overhead (computing);parallel computing;program structure tree;race condition;sensor;shared memory;switch;symmetric multiprocessing	Raghavan Raman;Jisheng Zhao;Vivek Sarkar;Martin T. Vechev;Eran Yahav	2012		10.1145/2254064.2254127	program analysis;parallel computing;geometric mean;computer science;theoretical computer science;distributed computing;data parallelism;programming language;instruction-level parallelism;implicit parallelism;task parallelism	PL	-13.375536916731722	47.845871241526254	139592
25ebfdfcdb5a21414d6833c791ba98807d7ff718	software assisted transact cache to support efficient unbounded transactional memory	software;cache storage;protocols;transactional data buffering;kernel;tcache overflow exceptions unbounded transactional memory deadlock free parallel programming model multicore ubiquitous hardware platform hybridtcache operating system l1 cache transactional data buffering;deadlock free parallel programming model;hardware registers coherence kernel software computer architecture protocols;hybridtcache;tcache overflow exceptions;computer architecture;transaction data;l1 cache;operating system;multicore ubiquitous hardware platform;registers;ubiquitous computing cache storage operating systems computers transaction processing;coherence;ubiquitous computing;unbounded transactional memory;parallel programming model;transaction processing;transactional memory;operating systems computers;hardware	Transactional memory (TM) provides efficient, easy, deadlock-free parallel programming model for today's multicore-ubiquitous hardware platform. Implementation of TM needs to guarantee that the transaction is executed atomically and in isolation. Our paper proposes an efficient and unbounded hybrid-mode TM system with strong isolation guarantee, called HybridTCache. HybridTCache optimizes the common case by executing small transactions completely by hardware, and triggers operating system (OS) support with low overhead for the uncommon case when transaction size exceeds the hardware capacity. HybridTCache adds a new L1 cache, named TCache, to buffer transactional data for the active transaction executed by the processor. Compared with traditional log based approach, TCache provides fast bookkeeping which eliminates software logging overhead for the un-overflowed blocks, thus making both transaction commit and abort fast. A key design point of hardware TM is to support unbounded transactions. HybridTCache achieves this by introducing TCache overflow exceptions and resorting to OS to handle the overflowed blocks.	cpu cache;deadlock;dynamic data;interrupt;isolation (database systems);multi-core processor;operating system;overhead (computing);parallel computing;parallel programming model;transactional memory	Shaogang Wang;Dan Wu;Zhengbin Pang;Xiaodong Yang	2008	2008 10th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2008.24	communications protocol;transactional memory;parallel computing;kernel;real-time computing;coherence;cpu cache;transaction processing;computer science;operating system;transaction data;distributed computing;processor register;ubiquitous computing;parallel programming model	Arch	-12.957840944097564	49.90023586517929	139686
0507acce879b52415ca7e374619c2fcde1363391	prefetching over a network: early experience with ctip	early experience;cache replacement;cache management	We discuss CTIP, an implementation of a network filesystem extension of the successful TIP informed prefetching and cache management system. Using a modified version of TIP in NFS client machines (and unmodified NFS servers). CTIP takes advantage of application-supplied hints that disclose the application's future read accesses. CTIP uses these hints to aggressively prefetch file data from an NFS file server and to make better local cache replacement decisions. This prefetching hides disk latency and exposes storage parallelism. Preliminary measurements that show CTIP can reduce execution time by a ratio comparable to that obtained with local TIP over a suite of I/O-intensive hinting applications. (For four disks, the reductions in execution time range from 17% to 69%). If local TIP execution requires that data first be loaded from remote storage into a local scratch area, then CTIP execution is significantly faster than the aggregate time for loading the data and executing. Additionally, our measurements show that the benefit of CTIP for hinting applications improves in the face of competition from other clients for server resources. We conclude with an analysis of the remaining problems with using unmodified NFS servers.	aggregate data;cpu cache;clustered file system;file server;input/output;link prefetching;parallel computing;run time (program lifecycle phase);server (computing)	David Rochberg;Garth A. Gibson	1997	SIGMETRICS Performance Evaluation Review	10.1145/270900.270906	parallel computing;real-time computing;computer science;operating system	OS	-13.911733696519255	53.17299275143551	140327
eca4531319667ca9b28a1c677ce3c8e4d9f4ee85	evaluating new cluster setup on 10gbit/s network to support the superb computing model	computers;storage computing;superb project;bit rate 10 gbit s;site wide cluster setup;cluster file system;high energy physics instrumentation computing;computing model;cluster;file systems computational modeling servers physics scalability data models computers;multiprocessing programs;superb;gpu;storage resources;coprocessors;data distribution;storage area networks;physics;data analysis;network interfaces;servers;computational modeling;storage availability;jbod configuration;networking superb cluster computing model;graphic programming unit;networking;particle physics;workstation clusters coprocessors data analysis high energy physics instrumentation computing multiprocessing programs network interfaces storage area networks;computing resources;scalability;workstation clusters;bit rate 10 gbit s superb computing model particle physics data analysis data distribution storage availability superb project multicore programming unit graphic programming unit gpu site wide cluster setup tierl computer facilities storage resources computing resources high density storage solution cluster file system network interfaces storage computing jbod configuration;multicore programming unit;high density storage solution;tierl computer facilities;file systems;data models;superb computing model	The new era of particle physics poses strong constraints on computing and storage availability for data analysis and data distribution. The SuperB project plans to produce and analyzes bulk of dataset two times bigger than the actual HEP experiment. In this scenario one of the main issues is to create a new cluster setup, able to scale for the next ten years and to take advantage from the new fabric technologies, included multicore and graphic programming units (GPUs). In this paper we propose a new site-wide cluster setup for Tier1 computer facilities, aimed to integrate storage and computing resources through a mix of high density storage solutions, cluster file system and Nx10Gbit/s network interfaces. The main idea is overcome the bottleneck due to the storage-computing decoupling through a scalable model composed by nodes with many cores and several disks in JBOD configuration. Preliminary tests made on 10Gbit/s cluster with a real SuperB use case, show the validity of our approach.	clustered file system;computation;coupling (computer programming);experiment;graphics processing unit;heterogeneous element processor;image scaling;large hadron collider;multi-core processor;non-raid drive architectures;scalability	Domenico del Prete;Silvio Pardi;Guido Russo	2011	2011 First International Conference on Data Compression, Communications and Processing	10.1109/CCP.2011.33	parallel computing;computer hardware;computer science;operating system	DB	-16.16625490074322	51.9488880432313	140360
3705e42c3a2afc85f09e6b1a8c6f1b47240df5f9	implementation and performance evaluation of a parallel transitive closure algorithm on prisma/db	performance evaluation;transitive closure	This paper describes an experimental performance study of the parallel computation of transitive closure operations on a parallel database system. This work brings two research eeorts together. The rst is the development of an eecient execution strategy for the parallel computation of path problems, called the Disconnection Set Approach. The second is the development and implementation of a parallel, main-memory DBMS, called PRISMA/DB. Here, we report on the implementation of the dis-connection set approach on PRISMA/DB, showing how the latter's design allowed us to easily extend the functionality of the system. It is shown that the parallel implementation of the disconnection set approach yields good performance characteristics, and that linear speedup with respect to a special purpose single processor algorithm is achieved. Finally, we describe a number of experiments that show to what extent data fragmentation issues innuence the performance of the disconnection set approach.	algorithm;computation;computer data storage;decibel;experiment;fragmentation (computing);limbo;parallel computing;parallel database;performance evaluation;speedup;transitive closure	Maurice A. W. Houtsma;Annita N. Wilschut;Jan Flokstra	1993			computer science;theoretical computer science;transitive closure;algorithm	DB	-13.35142167154437	50.33241947196787	140380
51a72b3f7a41ab32fb33aaae856a5d0bc102a9d7	erratum to: onfs: a hierarchical hybrid file system based on memory, ssd, and hdd for high performance computers		In the original version of this article, the abbreviation ‘OWDM’ was incorrectly defined. The phrase ‘orthogonal wavelength division multiplexing’ should all be changed to ‘one-way wave depth migration’.	computer;hard disk drive;metafile;one-way function;solid-state drive;supercomputer;wavelength-division multiplexing	Xin Liu;Yutong Lu;Jie Yu;Peng-Fei Wang;Jieting Wu;Ying Lu	2018	Frontiers of Information Technology & Electronic Engineering	10.1631/FITEE.17e0626	parallel computing;control theory;computer science;file system	HPC	-18.949749520607554	51.424178160132456	140656
1973b8bd9d997cb9f4532d5f661806ce46d27a70	toward effective nic caching: a hierarchical data cache architecture for iscsi storage servers	cache storage;helper cache;peripheral interfaces;data cache architecture;storage area networks cache storage network interfaces memory architecture telecommunication traffic peripheral interfaces;hierarchical data;buffer storage;nic caching;cache storage network servers bandwidth telecommunication traffic storage area networks web server computer architecture throughput ethernet networks buffer storage;storage area networks;iscsi storage servers;computer architecture;pci bus;network interfaces;telecommunication traffic;network servers;memory architecture;bandwidth;state locality aware cache placement algorithm;state locality aware cache placement algorithm nic caching data cache architecture iscsi storage servers helper cache pci bus;web server;ethernet networks;reading and writing;throughput;open source	In this paper, we present a hierarchical data cache architecture called DCA to effectively slash local interconnect traffic and thus boost the storage server performance. DCA is composed of a read cache in NIC card called NIC cache and a read/write unify cache in host memory called Helper cache. NIC cache services most portion of read requests without fetching data via PCI bus, while Helper cache 1) supplies some portions of read requests given partial NIC cache hits; 2) directs cache placement for NIC cache and 3) absorbs most transient writes locally. We developed a novel State Locality Aware cache Placement algorithm called SLAP to improve NIC cache hit ratio for mixed read and write workloads. To demonstrate the effectiveness of DCA, we developed a DCA prototype system and evaluated it with open source iSCSI implementation under representative storage server workloads. Experimental results showed that DCA can boost iSCSI storage server throughput by up to 121% and slash the PCI traffic by up to 74% compared with an iSCSI target without DCA.	algorithm;cpu cache;cache (computing);clustered file system;conventional pci;experiment;file server;hierarchical database model;hit (internet);iscsi;locality of reference;network interface controller;open-source software;prototype;real life;server (computing);slash (cms);throughput	Xiaoyu Yao;Jun Wang	2005	2005 International Conference on Parallel Processing (ICPP'05)	10.1109/ICPP.2005.76	bus sniffing;embedded system;throughput;cache-oblivious algorithm;snoopy cache;parallel computing;storage area network;cache coloring;page cache;cache;computer science;write-once;network interface;cache invalidation;operating system;conventional pci;smart cache;cache acceleration software;mesi protocol;cache algorithms;cache pollution;bandwidth;web server;hierarchical database model;mesif protocol;computer network	HPC	-14.344811754467425	51.40186181905521	140751
83dd916fbce0b03c231d7e2ace661f2b6b4dc9f0	optimize performance of virtual machine checkpointing via memory exclusion	virtual machines checkpointing optimisation;virtual machine;optimisation;random access memory;memory management;transparence;memory exclusion;virtual machine checkpointing;virtual machining;xen environment optimization virtual machine checkpointing memory exclusion compatibility transparence flexibility simplicity ballooning mechanism;checkpointing;virtual machine monitors;ballooning;ballooning virtual machine checkpoint restart memory exclusion;virtual machines;ballooning mechanism;checkpoint;compatibility;driver circuits;linux;optimization;virtual machining checkpointing virtual manufacturing virtual machine monitors hardware operating systems containers encapsulation fault tolerant systems availability;xen environment;experimental measurement;restart;simplicity;flexibility	Virtual Machine (VM) level checkpoints bring several advantages which process-level checkpoint implementation can hardly provide: compatibility, transparence, flexibility, and simplicity. However, the size of VM-level checkpoint may be very large and even in the order of gigabytes. This disadvantage causes the VM checkpointing and restart time become very long. To reduce the size of VM checkpoint, this paper proposes a memory exclusion scheme using ballooning mechanism, which omits saving unnecessary free pages in the VM. We implement our prototype in Xen environment. Experimental measurements show our approach can significantly reduce the size of VM checkpoint with minimal runtime overhead, thereby greatly improve the checkpoint performance.	application checkpointing;gigabyte;overhead (computing);prototype;transaction processing system;virtual machine;z/vm	Haikun Liu;Hai Jin;Xiaofei Liao	2009	2009 Fourth ChinaGrid Annual Conference	10.1109/ChinaGrid.2009.42	parallel computing;real-time computing;computer science;operating system	HPC	-17.92788654155402	50.092324182702505	141022
c2052eec1b12f97cda27f7e771f0a7ed462484e3	monitoring large systems via statistical sampling	sample selection;computational grid;performance monitoring;large systems;parallel systems;statistical sampling;wide area network	As thetrendin parallelsystemsscalestowardpetaflopperformancetappedby advancesin circuit densityandby anincreasinglyavailablecomputational Grid, thedevelopmentof efficientmechanisms for monitoringlarge systemsbecomesimperati ve. Whencomputationalcomponentsarecoupledvia dynamicallyshifting connectionswith variousremoteresources, the numberof potentialfactorsaffectingsystembehavior is enormous.Yet theoverheadof monitoringcanbeprohibitive. This paper presentsanew techniquefor monitoringlargesystemsbasedonstatisticalsampling.Ratherthanmonitoring eachcomponent,we selecta statisticallyvalid sampleandmeasurethe behavior of sample members.We describethe formal requirementsof sampleselectionandverify the feasibility of our approachwith experimentson large parallelsystemsandwide-areanetworks. Our resultsshow that this techniquecanbea powerful tool to enableeffective monitoringwithout incurringthe largecosts typically associatedto exhausti ve checking.	national center for supercomputing applications;telecommuting	Celso L. Mendes;Daniel A. Reed	2004	IJHPCA	10.1177/1094342004038958	sampling;parallel computing;real-time computing;computer science;data mining;distributed computing	EDA	-16.495713877375234	47.926679946596785	141618
0487bb04b3cce6b0939315b2c12f37961fa835bb	prophesy: an infrastructure for analyzing and modeling the performance of parallel and distributed applications	distributed application;distributed system;data recording;program diagnostics;experimental analysis;program diagnostics software performance evaluation computer aided software engineering relational databases data recording distributed programming parallel programming;application details prophesy infrastructure parallel applications distributed applications application performance analysis application performance modelling efficient execution system features experience based insight acquisition relational database performance data recording;software performance evaluation;parallel programming;relational database;computer aided software engineering;distributed programming;performance model;relational databases;performance analysis data analysis application software analytical models instruments relational databases kernel scalability algorithm design and analysis laboratories	Efficient execution of applications requires insight into how the system features impact the performance of the application. For distributed systems, the task of gaining this insight is complicated by the complexity of the system features. This insight generally results from significant experimental analysis and possibly the development of performance models. This paper presents the Prophesy project, an infrastructure that aids in gaining this needed insight based upon experience. The core component of Prophesy is a relational database that allows for the recording of performance data, system features and application details.		Xingfu Wu;Valerie E. Taylor;Jonathan Geisler;Xin Li;Zhiling Lan;Rick L. Stevens;Mark Hereld;Ivan R. Judson	2000		10.1109/HPDC.2000.868668	parallel computing;relational database;computer science;theoretical computer science;operating system;data mining;database;distributed computing;programming language	HPC	-16.277156831851713	49.097564195977164	141973
692a014e5c92277f81b9f3c0ad97f900852fc16d	high-performance i/o and networking software in sequoia 2000	distributed application;high speed networks;memory access;performance improvement;design and implementation;high performance	We describe our experiences producing high-speed network and I/O software in the Sequoia 2000 Project. The ef ficient movement of very large objects, from tens to hundreds of megabytes in size, is a key requirement for Sequoia distributed applications. We present new designs and implementations of operating system I/O software. Our methods provide significant performance improvements for transfers among devices, processes, and between the two. We describe techniques that reduce or eliminate costly memory accesses, avoid unnecessary processing, and bypass system overheads to improve throughput while reducing latency.	distributed computing;ibm sequoia;ibm system i;input/output;megabyte;operating system;throughput	Joseph Pasquale;Eric W. Anderson;Kevin R. Fall;Jonathan S. Kay	1995	Digital Technical Journal		embedded system;parallel computing;computer science	OS	-15.084606306632306	51.3882994183932	142268
01d90c401538756ff7bad1a7097c3ab6d2be3d7a	a statistical failure/load relationship: results of a multicomputer study	performance reliability models;computer failure data;statistical failure models;statistical failure models computer failure data performance reliability models	In this correspondence we present a statistical model which relates mean computer failure rates to level of system activity. Our analysis reveals a strong statistical dependency of both hardware and software component failure rates on several common measures of utilization (specifically CPU utilization, I/O initiation, paging, and job-step initiation rates). We establish that this effect is not dominated by a specific component type, but exists across the board in the two systems studied. Our data covers three years of normal operation (including significant upgrades and reconfigurations) for two large Stanford University computer complexes. The complexes, which are composed of IBM mainframe equipment of differing models and vintage, run similar operating systems and provide the same interface and capability to their users. The empirical data comes from identically structured and maintained failure logs at the two sites along with IBM OS/VS2 operating system performance/load records.	central processing unit;component-based software engineering;failure cause;mainframe computer;operating system;paging;parallel computing;statistical model	Ravishankar K. Iyer;Steven E. Butner;Edward J. McCluskey	1982	IEEE Transactions on Computers	10.1109/TC.1982.1676070	embedded system;parallel computing;real-time computing;simulation;predictive failure analysis;computer science;operating system	OS	-17.367241900469843	50.27715465645151	142327
0c60a1f8a3efd037c59286af6af9778e6e2dc2d1	memory affinity management for numerical scientific applications over multi-core multiprocessors with hierarchical memory	automatic control;stress;scientific application;remote access;memory access pattern;nonuniform memory access management;memory management;memory affinity management;performance evaluation;explicit memory;high performance computing;resource allocation;load balancing issue;source code modification memory affinity management numerical scientific application multicore multiprocessor hierarchical memory nonuniform memory access management memory bank load balancing issue memory contention remote access memory access cost data placement memory access pattern architecture abstraction;storage management;non uniform memory access;memory bank;source code modification;memory access cost;numerical scientific application;memory access;memory contention;multi core numa platforms memory affinity;numa platforms;multicore processing;architecture abstraction;multicore multiprocessor;hierarchical memory;bandwidth;source code;load balance;multiprocessing systems;memory management stress multicore processing automatic control delay bandwidth performance evaluation high performance computing;storage management multiprocessing systems resource allocation;memory affinity;data placement;multi core	Nowadays, on Multi-core Multiprocessors with Hierarchical Memory (Non-Uniform Memory Access (NUMA) characteristics), the number of cores accessing memory banks is considerably high. Such accesses produce stress on the memory banks, generating load-balancing issues, memory contention and remote accesses. In this context, how to manage memory accesses in an efficient fashion remains an important concern. To reduce memory access costs, developers have to manage data placement on their application assuring memory affinity. The problem is: How to guarantee memory affinity for different applications/NUMA platforms and assure efficiency, portability, minimal or none source code changes (transparency) and fine control of memory access patterns? In this Thesis, our research have led to the proposal of Minas: an efficient and portable memory affinity management framework for NUMA platforms. Minas provides both explicit memory affinity management and automatic one with good performance, architecture abstraction, minimal or none application source code modifications and fine control. We have evaluated its efficiency and portability by performing some experiments with numerical scientific HPC applications on NUMA platforms. The results have been compared with other solutions to manage memory affinity.	benchmark (computing);cache (computing);central processing unit;compiler;data structure;dynamic data;dynamization;experiment;load balancing (computing);memory bank;memory management;multi-core processor;non-uniform memory access;numerical analysis;processor affinity;software portability;uniform memory access	Christiane Pousa Ribeiro;Jean-François Méhaut;Alexandre Carissimi	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470796	multi-core processor;uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;parallel computing;real-time computing;distributed memory;computer science;physical address;operating system;automatic control;overlay;extended memory;flat memory model;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	HPC	-16.84000006560968	53.1926911345231	142580
19ae1f1c4da69542fdba8068bffe66ea0b7fc0ef	lightweight memory management for high performance applications in consolidated environments	random access memory;kernel;memory management;high performance computing;resource management;hpc cluster lightweight memory management high performance application consolidated environment linux based operating system hpc system linux based os rs feature set developer familiarity system overhead hpc community lwk lightweight memory subsystem hpmmap lightweight memory performance linux memory management layer;storage management cloud computing linux operating system kernels parallel processing;runtime;linux memory management resource management kernel benchmark testing random access memory runtime;cloud computing lightweight kernels operating systems high performance computing;lightweight kernels;linux;benchmark testing;cloud computing;operating systems	Linux-based operating systems and runtimes (OS/Rs) have emerged as the environments of choice for the majority of HPC systems. While Linux-based OS/Rs have advantages such as extensive feature sets and developer familiarity, these features come at the cost of additional system overhead. In contrast to Linux, there is a substantial history of work in the HPC community focused on lightweight OS/Rs that provide scalable and consistent performance for HPC applications, but lack many of the features offered by commodity OS/Rs. In this paper, we propose to bridge the gap between LWKs and commodity OS/Rs by selectively providing a lightweight memory subsystem for HPC applications in a commodity OS/R where concurrently executing a diverse range of workloads is commonplace. Our system HPMMAP provides lightweight memory performance transparently to HPC applications by bypassing Linux's memory management layer. Using HPMMAP, HPC applications achieve consistent performance while the same local compute nodes execute competing workloads likely to be found in HPC clusters and “in-situ” workload deployments. Our approach is dynamically configurable at runtime, and requires no resources when not in use. We show that HPMMAP can decrease variance and reduce application runtime by up to 50 percent when executing a co-located competing commodity workload.	lightweight kernel operating system;linux;memory management;overhead (computing);run time (program lifecycle phase);runtime system;scalability	Brian Kocoloski;John R. Lange	2016	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2015.2397452	embedded system;benchmark;parallel computing;kernel;cloud computing;computer science;resource management;operating system;database;linux kernel;memory management	HPC	-15.839582697464483	51.670506924774756	143328
073373e2258268f04cd4c2149c60f33b86657ef2	application performance monitoring: trade-off between overhead reduction and maintainability		Monitoring of a software system provides insights into its runtime behavior, improving system analysis and comprehension. System-level monitoring approaches focus, e.g., on network monitoring, providing information on externally visible system behavior. Application-level performance monitoring frameworks, such as Kieker or Dapper, allow to observe the internal application behavior, but introduce runtime overhead depending on the number of instrumentation probes. We report on how we were able to significantly reduce the runtime overhead of the Kieker monitoring framework. For achieving this optimization, we employed microbenchmarks with a structured performance engineering approach. During optimization, we kept track of the impact on maintainability of the framework. In this paper, we discuss the emerged trade-off between performance and maintainability in this context. To the best of our knowledge, publications on monitoring frameworks provide none or only weak performance evaluations, making comparisons cumbersome. However, our micro-benchmark, presented in this paper, provides a basis for such comparisons. Our experiment code and data are available as open source software such that interested researchers may repeat or extend our experiments for comparison on other hardware platforms or with other monitoring frameworks.	benchmark (computing);dapper;experiment;mathematical optimization;open-source software;overhead (computing);performance engineering;software system;system analysis	Jan Waller;Florian Fittkau;Wilhelm Hasselbring	2014			embedded system;real-time computing;operating system	SE	-17.12376343282342	47.80755433101068	144547
fe71e39c3011218a452c088fb19710ae8efb84a8	linux i/o performance on vm			input/output;linux;z/vm	Thomas Beretvas;Barton Robinson	2002			operating system;input/output;computer science	OS	-19.058535176266783	51.62791248882997	144606
270e61ce5a56577275c119fe35f13b15d726da83	dual display of virtual machines for automotive infotainment systems	api remoting;dual display;gpu virtualization;embedded hypervisor;embedded virtualization	We introduce GPU sharing between virtual machines on an embedded hypervisor. To achieve GPU virtualization, we apply an API remoting method which requires low overhead to OpenGL ES. For multimedia application, we target in-vehicle infotainment system and configure two guest virtual machines. The digital cluster is implemented on a bare-metal virtual machine. Furthermore, for multimedia application, we implement a linux-compatible 3D navigation. The framework can display both guest virtual machines on a single display panel so that output could be allocated separately on the panel. With this work, we explored how to virtualize GPU on a hypervisor for automotive systems.	application programming interface;bare machine;bare-metal restore;embedded hypervisor;embedded system;graphics processing unit;linux;opengl es;overhead (computing);virtual machine;virtualize;x86 virtualization	Hyunwoo Joe;Dongwook Kang;Jin-Ah Shin;Vincent Dupre;Soo-Young Kim;Taeho Kim;Chaedeok Lim	2015	2015 IEEE 4th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2015.7398732	embedded system;real-time computing;storage hypervisor;computer science;virtual machine;operating system;hardware virtualization	Visualization	-17.680679441004283	51.468148380775844	144717
1b4f97d52fa7061c910b3ebd7ce1d93b49b5d138	time-sharing redux for large-scale hpc systems	discrete wavelet transforms;memory management;noise measurement;load modeling;programming;supercomputers;conferences	HPC facilities typically use batch scheduling to space-share jobs. In this paper we revisit time-sharing using a trace of over 2.4 million jobs obtained during 20 months of operation of a modern petascale supercomputer. Our simulations show that batch scheduling produces skewed distributions with much larger slowdowns for shorter-running, larger jobs, whereas time-sharing produces more uniform slowdowns. Consequently, for applications that strong scale, the turnaround time does not scale with batch scheduling, but it does with time-sharing, resulting in turnarounds that are orders of magnitude better at the largest scales. We also show that time-sharing can confer additional benefits in noisy systems and with modern programming practices. Future Exascale HPC systems are expected to exhibit billion-way heterogeneous parallelism and poor performance predictability. As many applications will run in strong scaling, how resource allocation policies affect the experience of supercomputer users has once again become a timely subject.	central processing unit;image scaling;intel edison;job scheduler;job stream;network processor;offset binary;parallel computing;petascale computing;spmd;scalability;scheduling (computing);simulation;supercomputer;time-sharing	Steven A. Hofmeyr;Costin Iancu;Juan A. Colmenares;Eric Roman;Brian Austin	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0051	programming;parallel computing;real-time computing;computer science;noise measurement;operating system;distributed computing;memory management	HPC	-16.677304169946165	50.691207150459235	144871
762f3b8329735f81032aa96592f5627e0c12e127	fault tolerant scheduling for parallel loops on shared memory systems	work stealing;loop scheduling;期刊论文;fault tolerance;multicore and multiprocessor;self scheduling	While multicore/multiprocessor systems achieve significant speedup for many applications by exploiting loop level parallelism, they also suffer from increased reliability problems as a result of ever scaling device size. This paper addresses the reliability of loop dominated applications, aiming to execute parallel loops efficiently in the presence of various types of hardware faults. In this paper, we present a fault tolerant work-stealing scheme which makes parallel loop execution resilient to hardware faults. A lightweight buffer-commit mechanism is applied in the proposed scheme to ensure the correctness of the re-execution of loop iterations. In addition, we split large failing chunks of loop iterations at runtime to improve load balancing, and a worker thread is discarded when faults occur frequently on it. We evaluated our techniques on a multi-socket multicore system, using a set of loop dominated benchmarks. The proposed scheme achieves the minimum overhead of supporting fault tolerance and optimal load balancing.	algorithm;benchmark (computing);correctness (computer science);data parallelism;emoticon;executable;failure;fault tolerance;image scaling;iteration;load balancing (computing);loop-level parallelism;multi-core processor;overhead (computing);parallel computing;run time (program lifecycle phase);scheduling (computing);shared memory;speedup;symmetric multiprocessing;transaction processing system;work stealing	Yizhuo Wang;Rosario Cammarota;Alexandru Nicolau	2015	J. Inf. Sci. Eng.		loop tiling;loop inversion;fault tolerance;parallel computing;real-time computing;loop fission;computer science;operating system;distributed computing	HPC	-15.291548043889888	49.00199957545628	145282
0724b04d473a3c1ab14855fd2d76ca2efb58d0dc	disk striping strategies for large video-on-demand servers	replication;demand systems;waiting time;video on demand;disk striping	The storage structure of videos on disks affects the number of concurrent users a videoon-demand system can support and hence the average waiting time. In this paper, we propose a phase-based striping method which has the desirable characteristics that it guarantees the maximum waiting time. We also develop a data replication scheme to further reduce the average waiting time of the phase-based method. These two schemes, together with the conventional sequential striping scheme, are then employed to optimize the waiting time of videos based on their access pattern. Simulations were conducted using a video server containing 36 videos under different loading and hardware configurations. The results show that such optimization can reduce the waiting time significantly. The use of replication method could further improve the server performance by over 25%. The overall results indicate that with limited resources, the use of phase-based striping method with replication is preferable under heavy loads.	acm-mm;algorithm;cave story;chua's circuit;computer simulation;data striping;database;disk storage;emoticon;entity–relationship model;fast forward;file server;institute of electronics, information and communication engineers;load balancing (computing);mathematical optimization;media controls;memory hierarchy;os-tan;ocean observatories initiative;raid;replication (computing);sequential access;server (computing);vldb;vehicle identification number;video server	Tat-Seng Chua;Jiandong Li;Beng Chin Ooi;Kian-Lee Tan	1996		10.1145/244130.244231	embedded system;replication;real-time computing;simulation;data striping;statistics	DB	-12.805012046391935	52.00294162580399	145310
53bdb7e223e2218e82880ba1c37c171235d6e481	a high performance cluster jvm presenting a pure single system image	single system image;persistence;java virtual machine;recovery;execution state;checkpointing;jvm;high performance;object model;java	cJVM is a Java Virtual Machine (JVM) which provides a single system image of a traditional JVM while executing in a distributed fashion on the nodes of a cluster, cJVM virtualizes the cluster, transparently distributing the objects and threads of any pure Java application. The aim of cJVM is to obtain improved scalability for Java Server Applications by distributing the application's work among the cluster's computing resources. cJVM's architecture, its unique object model, thread and memory models were described in [6]. In this article we focus on the optimization techniques employed in cJVM to achieve high scalability. In particular, we focus on the techniques used to enhance locality thereby reducing the amount of communication generated by cJVM. In addition, we describe how communication overhead can be reduced by taking advantage of Java semantics. Our optimization techniques are based on three principles. First, we employ a large number of mostly simple optimizations which address caching, locality of execution and object migration. Second, we take advantage of the Java semantics and of common usage patterns in implementing the optimizations. Third, we use speculative optimizations, taking advantage of the fact that the cJVM run-time environment can correct false speculations. We have demonstrated the usefulness of these techniques on a large (10Kloc) Java application, achieving 80% efficiency on a four-node cluster. This paper discusses the various techniques used and reports our results.	java virtual machine;locality of reference;mathematical optimization;overhead (computing);runtime system;scalability;single system image;speculative execution	Yariv Aridor;Michael Factor;Avi Teperman;Tamar Eilam;Assaf Schuster	2000		10.1145/337449.337543	parallel computing;real-time computing;java concurrency;computer science;operating system;strictfp;java	PL	-15.41854898098393	47.45059109775779	145949
6b52c469cb86cc81a2f37e3662ad985c225b5107	ec-frm: an erasure coding framework to speed up reads for erasure coded cloud storage systems	reliability;standards;performance evaluation;cloud storage system;erasure code;reed solomon codes;storage management cloud computing performance evaluation reed solomon codes reliability;会议论文;cloud computing reed solomon codes encoding performance evaluation standards reliability;normal read;degraded read;encoding;standard reed solomon code erasure coding framework erasure coded cloud storage system reliability device failure read performance improvement ec frm rs code ec frm lrc code;degraded read erasure code cloud storage system normal read;cloud computing	With the reliability requirements increasingly important, erasure codes have been widely used in today's cloud storage systems because they achieve both high reliability and low storage overhead. However, the performance for most existing erasure codes can be further improved on both normal reads to user's data without device failures and degraded reads under device failures, which are crucial in cloud storage systems. In this paper, we propose an erasure coding framework named EC-FRM to integrate existing codes in order to improve the read performance. The constructed code over EC-FRM named EC-FRM-Code, which keeps most of wonderful properties of the integrated code and achieves good performance on both normal reads and degraded reads. We transform Reed-Solomon code and LRC code to EC-FRM-RS and EC-FRM-LRC respectively, and then conduct a series of experiments to evaluate their read performance. The results show that EC-FRM-RS code gains 19.2% to 33.9% higher normal read speed and 9.1% to 9.9% higher degraded read speed than standard Reed-Solomon code, while EC-FRM-LRC code owns 23.5% to 46.9% higher normal read speed and 3.3% to 12.8% higher degraded read speed than standard LRC code.	cosmo-rs;cloud storage;erasure code;experiment;input/output;java api for restful web services (jax-rs);overhead (computing);reed–solomon error correction;requirement	Yingxun Fu;Jiwu Shu;Zhirong Shen	2015	2015 44th International Conference on Parallel Processing	10.1109/ICPP.2015.57	erasure code;parallel computing;real-time computing;constant-weight code;cloud computing;telecommunications;computer science;operating system;tornado code;code rate;reliability;encoding;statistics	OS	-15.222372263504777	52.4346111919505	146266
888ad52f69f7acd7606d401e2abf46cc5ce2f953	asynchronous checkpoint migration with mrnet in the scalable checkpoint / restart library	libraries;mainframes;thyristors;high performance computing;implementation;network operating systems;efficiency;overlay networks;trees mathematics;checkpointing;parallel databases;mathematical methods and computing;large scale;fault tolerant computing;redundancy;parallel file system;overlay network;writing;trees mathematics checkpointing fault tolerant computing mainframes network operating systems overlay networks parallel databases;scalability;scalability issues asynchronous checkpoint migration mrnet supercomputers failure tolerance file checkpointing stable storage scalable checkpoint restart library scr library tree based overlay network library checkpoint transfer parallel file system application efficiency;storage;supercomputers;thyristors libraries checkpointing redundancy scalability high performance computing writing	Applications running on today's supercomputers tolerate failures by periodically saving their state in checkpoint files on stable storage, such as a parallel file system. Although this approach is simple, the overhead of writing the checkpoints can be prohibitive, especially for large-scale jobs. In this paper, we present initial results of an enhancement to our Scalable Checkpoint / Restart Library (SCR). We employ MRNet, a tree-based overlay network library, to transfer checkpoints from the compute nodes to the parallel file system asynchronously. This enhancement increases application efficiency by removing the need for an application to block while checkpoints are transferred to the parallel file system. We show that the integration of SCR with MRNet can reduce the time spent in I/O operations by as much as 15×. However, our experiments exposed new scalability issues with our initial implementation. We discuss the sources of the scalability problems and our plans to address them.	application checkpointing;clustered file system;experiment;fault tolerance;input/output;overhead (computing);overlay network;scalability;stable storage;supercomputer	Kathryn Mohror;Adam Moody;Bronis R. de Supinski	2012	IEEE/IFIP International Conference on Dependable Systems and Networks Workshops (DSN 2012)	10.1109/DSNW.2012.6264668	supercomputer;parallel computing;real-time computing;overlay network;computer science;operating system;distributed computing;computer network	HPC	-18.011718785325098	49.87160471031944	146336
03c5b60d398526579a3140d69f8c8a2871c0f0f9	scalability of write-ahead logging on multicore and multisocket hardware	scaling to multisockets;flush pipelining;log manager;consolidation array;log buffer contention;early lock release;scaling to multisocket	The shift to multi-core and multi-socket hardware brings new challenges to database systems, as the software parallelism determines performance. Even though database systems traditionally accommodate simultaneous requests, a multitude of synchronization barriers serialize execution. Write-ahead logging is a fundamental, omnipresent component in ARIES-style concurrency and recovery, and one of the most important yet-to-be addressed potential bottlenecks, especially in OLTP workloads making frequent small changes to data. In this paper, we identify four logging-related impediments to database system scalability. Each issue challenges different level in the software architecture: (a) the high volume of small-sized I/O requests may saturate the disk, (b) transactions hold locks while waiting for the log flush, (c) extensive context switching overwhelms the OS scheduler with threads executing log I/Os, and (d) contention appears as transactions serialize accesses to in-memory log data structures. We demonstrate these problems and address them with techniques that, when combined, comprise a holistic, scalable approach to logging. Our solution achieves a 20–69% speedup over a modern database system when running log-intensive workloads, such as the TPC-B and TATP benchmarks, in a single-socket multiprocessor server. Moreover, it achieves log insert throughput over 2.2 GB/s for small log records on the single-socket server, roughly 20 times higher than the traditional way of accessing the log using a single mutex. Furthermore, we investigate techniques on scaling the performance of logging to multi-socket servers. We present a set of optimizations which partly ameliorate the latency penalty that comes with multi-socket hardware, and then we investigate the feasibility of applying a distributed log buffer design at the socket level.	allocate-on-flush;bottleneck (software);concurrency (computer science);context switch;data structure;data synchronization;emoticon;gigabyte;holism;ibm tivoli storage productivity center;image scaling;in-memory database;input/output;lock (computer science);multi-core processor;multiprocessing;mutual exclusion;online transaction processing;operating system;parallel computing;scalability;scheduling (computing);serialization;server (computing);software architecture;speedup;throughput;write-ahead logging	Ryan Johnson;Ippokratis Pandis;Radu Stoica;Manos Athanassoulis;Anastasia Ailamaki	2011	The VLDB Journal	10.1007/s00778-011-0260-8	parallel computing;real-time computing;computer science;operating system;database	DB	-12.371320635987827	51.508256057830025	146428
fee92972d2fec7c38504d92e498d6190de417be3	java card performance optimization of secure transaction atomicity based on increasing the class field locality	secure transaction atomicity;cache storage;persistent objects;smart card;random access memory;ram;java card;transaction processing cache storage embedded systems java persistent objects security of data smart cards;transaction mechanism;persistent memory;anti tearing;java optimization data security power system security random access memory embedded system smart cards banking electronic government hardware;java fields java card performance optimization secure transaction atomicity class field locality embedded systems smart cards security mechanisms transaction mechanism persistent memory caching mechanism ram buffer;virtual machining;java annotations;caching mechanism;embedded system;security mechanisms;arrays;java fields;java card performance optimization;embedded systems;smart cards;eprom;buffer;class field locality;transaction processing;java annotations java card anti tearing transaction mechanism;security;high performance;performance optimization;use case;security of data;java	Java Cards are embedded systems, very often implemented as smart cards. They are used in banking, e-government, telecommunication, and ticketing. Due to these use cases they have to provide a wide range of security mechanisms and a high performance in relation to the available hardware cost. One of these security features is the transaction mechanism. It ensures that data in persistent memory stays consistent in case of the execution of the application is interrupted unexpectedly by e.g. loss of power. Such transaction mechanisms are very time consuming. Therefore, we propose a caching mechanism for transactions. The mechanism uses a buffer located in RAM and reduces costly write cycles into persistent memory without any loss of security. In order to further increase the performance of this caching mechanism, we additionally introduce a concept to maximize the locality of selected Java fields which are written very often.	algorithm;atomicity (database systems);byte;cpu cache;cache (computing);database;e-government;embedded system;interrupt;java card;java annotation;locality of reference;mobile ticketing;overhead (computing);persistent memory;random-access memory;smart card;virtual machine	Johannes Loinig;Christian Steger;Reinhold Weiss;Ernst Haselsteiner	2009	2009 Third IEEE International Conference on Secure Software Integration and Reliability Improvement	10.1109/SSIRI.2009.39	smart card;parallel computing;real-time computing;distributed transaction;computer science;information security;operating system;programming language;computer security	Embedded	-13.84272393205794	51.12338916038764	146583
9f5a3799a66345e8d52f36bac7c5de376aba6904	privatizing transactions for lee’s algorithm in commercial hardware transactional memory		Lee’s algorithm solves the path-connection problems that arise in logical drawing, wiring diagramming or optimal route finding. Its parallel version has been widely used as a benchmark to test transactional memory systems. It exhibits transactions of large size and duration that stress these systems exposing their limitations. In fact, Lee’s algorithm has been proved to perform similar to sequential in commercial hardware transactional memory systems due to persistent capacity overflows. In this paper, we propose a novel approach to Lee’s algorithm in the context of commercial hardware transactional memory systems. We show how the majority of the computation of the largest transaction, i.e. grid privatization and path calculation, can be executed out of the boundaries of the transaction, thus reducing the size requirements. We leverage the correctness criteria of lazy subscription fallback locks to ensure a correct execution. This novel approach uses transactional memory extensions from commercial processors from a different point of view, not needing either early release or open-nested transaction features that are not yet implemented in these systems. We propose an application programming interface to facilitate the task of the programmer. Experiments are carried out with the Intel Core and IBM Power8 architectures, showing speedups around 3.5$$\times $$ × over both the standard transactional version of the algorithm and the sequential for certain grid inputs and four threads. We also compare our proposal with a software transactional memory LeeTM approach.	algorithm;application programming interface;benchmark (computing);central processing unit;code;computation;correctness (computer science);html;ibm personal computer;lazy evaluation;list of concept- and mind-mapping software;lock (computer science);nested transaction;printed circuit board;printing;programmer;release early, release often;requirement;software transactional memory;speedup;wiring	Ricardo Quislant;Eladio Gutiérrez;Emilio L. Zapata;Oscar G. Plata	2017	The Journal of Supercomputing	10.1007/s11227-017-2188-2	parallel computing;power8;computer science;software transactional memory;application programming interface;correctness;transactional leadership;algorithm;thread (computing);transactional memory;database transaction	HPC	-14.519157243701969	49.87614412528476	146733
28261c2c6965b29155a7dd3d133206f86da5919c	parallel query processing in dbs3	shared memory;query processing;search space;parallel programming;shared memory systems;timing optimization;distributed databases;parallel query processing;distributed memory architectures parallel query processing dbs3 compile time optimization parallelization shared memory parallel database system search space zigzag trees left deep right deep trees response time parallel dataflow execution model;shared memory systems database theory distributed databases parallel programming program compilers query processing;program compilers;database theory;query processing delay database systems measurement throughput space exploration load management program processors optimizing compilers automatic control	In this paper, we describe our approach to the compile-time optimization and parallelization of queries for execution in DBS3, a shared-memory parallel database system. Our approach enables exploring a search space large enough to include zigzag trees which are intermediate between left-deep and right-deep trees. Zigzag trees are shown to provide better response time than right-deep trees in the case of limited memory. Because DBS3 implements a parallel dataaow execution model, this approach applies to both shared-memory and distributed-memory architectures. Performance measurements run using the DBS3 prototype show the advantages of zigzag trees under various conditions.	central processing unit;compile time;compiler;distributed memory;experiment;mathematical optimization;naruto shippuden: clash of ninja revolution 3;parallel computing;parallel database;pipeline (computing);prototype;randomized algorithm;response time (technology);shared memory;spooling;time-sharing;tree traversal	Mikal Ziane;Mohamed Zaït;Pascale Borla-Salamet	1993		10.1109/PDIS.1993.253066	distributed shared memory;query optimization;parallel computing;computer science;theoretical computer science;programming language	DB	-12.825017051594664	48.15622841281402	146833
1a44b95e65604c25fe2dc4d9b6b5f51339fb1ec2	naming and binding in a vertical migration environment	naming;object oriented model;ligamiento;editor vinculo;microprogrammation;microprogramacion;linkage editing process;implementation;abstractions;editeur lien;program compilers data structures microprogramming;compilers;micro objects compilers vertical migration environment maximum performance microcode linkage editing process object oriented model naming binding abstractions;micro objects;ejecucion;particion;data structures;partition;maximum performance;microcode;linkage;couplings compaction timing microprogramming processor scheduling production large hadron collider dynamic scheduling degradation programmable logic arrays;partage;microprogramming;program compilers;link editor;vertical migration;binding;vertical migration environment	Achieving maximum performance through migration of functions from software to microcode requires rethinking the linkage editing process. An object-oriented model of naming and binding clarifies the alternative abstractions available in naming and linking across the macro-micro machine boundary. Alternative abstractions for sharing micro-objects and for dynamic use of micro-objects are presented and their implementations discussed. The conclusions are based on actual implementations. >		Robert I. Winner	1988	IEEE Trans. Software Eng.	10.1109/32.6138	parallel computing;real-time computing;data structure;computer science;operating system;software engineering;microcode;programming language	SE	-11.871091987526933	48.447841741832455	147780
7c102b326b56207b4092dc4491e9040fef0833b8	effectiveness of multiple pageable page sizes for commercial applications	kb page size;concurrent pageable page size;application developer;operating system;software application;system administrator;typical commercial application;typical modern e-commerce application;john wiley;computational characteristic;multiple pageable page size	This paper investigates the computational characteristics of the software applications that are developed to support typical modern e-commerce applications. These applications have a significant impact on global commerce, and their influence is expected to continue. We show that the micro-architectural features of the memory subsystem, assisted by the operating system (such as the use of concurrent pageable page sizes), can significantly improve the performance of these applications. In particular, we show that using 64 kB page size in addition to the default 4 kB page size provides substantial throughput improvements for a typical commercial application (Trade6) without adding any complexity for the application developer or the system administrator. Copyright © 2007 John Wiley & Sons, Ltd.	paging	Joefon Jann;Niteesh Dubey;Pratap Pattnaik;R. Sarma Burugula	2008	Softw., Pract. Exper.	10.1002/spe.866	parallel computing;real-time computing;computer science;operating system	Arch	-14.562352163228114	49.94852386174994	147939
e0d34e98728ee5e30dbfc12f7eb41190a0246b42	design and implementation of holistic scheduling and efficient storage for flexray	automotive engineering;protocols;storage system;telecommunication network reliability;data deduplication real time scheduling storage system;journal;approximation theory;dynamic scheduling schedules bandwidth real time systems vehicle dynamics fault tolerance fault tolerant systems;fault tolerant systems;computational complexity;scheduling;communication protocol flexray storage design next generation automotive communication holistic integrated scheduling scheme scalability hosa flexible ease of use dual channel communication slot pilfering technique approximate computation computational complexity storage management deduplication aware scheme ssd prototype;real time scheduling;fault tolerance;emerging technologies;schedules;bandwidth;data deduplication;ssd prototype;hosa;flexray storage design;next generation networks;network communications;approximate computation;vehicle dynamics;applications;telecommunication network reliability approximation theory automotive engineering computational complexity next generation networks protocols scheduling telecommunication network management;dynamic scheduling;telecommunication network management;real time systems	FlexRay is a new industry standard for next-generation communication in automotives. Though there are a few recent researches on performance analysis of FlexRay, two important aspects of the FlexRay design have been overlooked. The first is a holistic integrated scheduling scheme that can handle both static and dynamic segments in a FlexRay network. The second is cost-effective and scalable performance. To address these aspects, we propose a novel holistic scheduling scheme, called HOSA, which can provide scalable performance by using flexible and ease-of-use dual channel communication in FlexRay. HOSA is built upon a novel slot pilfering technique to schedule and optimize the available slots in both static and dynamic segments. Moreover, to achieve efficient implementation, we propose approximate computation, which can efficiently support cost-effective and holistic scheduling by judiciously obtaining the tradeoff between computation complexity and available pilfered slots. HOSA hence offers the salient feature of improving bandwidth utilization. Moreover, to deliver high performance and bridge the gap between real-time communications in networks and storage management in end devices, we implement a deduplication-aware scheme in a real SSD prototype that can reduce space overhead and extend SSD lifespan (by significantly reducing duplicate write operations). The deduplication scheme meets the needs of message updates in FlexRay. Extensive experiments based on synthetic test cases and real-world case studies demonstrate the efficiency and efficacy of HOSA.	approximation algorithm;computation;data deduplication;experiment;fault tolerance;flexray;holism;interrupt latency;multi-channel memory architecture;overhead (computing);prototype;real-time clock;scalability;scheduling (computing);solid-state drive;synthetic intelligence;technical standard;test case	Yu Hua;Xue Liu;Wenbo He;Dan Feng	2014	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2013.205	embedded system;communications protocol;fault tolerance;parallel computing;real-time computing;vehicle dynamics;next-generation network;data deduplication;dynamic priority scheduling;schedule;computer science;operating system;database;distributed computing;emerging technologies;computational complexity theory;information technology;scheduling;bandwidth;computer network;approximation theory	HPC	-12.47962796352604	51.6736577495467	148269
b5c9e42f21c1daaa16e4fbb1c73c7238513cd328	fast snapshottable concurrent braun heaps		This paper proposes a new concurrent heap algorithm, based on a stateless shape property, which efficiently maintains balance during insert and removeMin operations implemented with hand-over-hand locking. It also provides a O(1) linearizable snapshot operation based on lazy copy-on-write semantics. Such snapshots can be used to provide consistent views of the heap during iteration, as well as to make speculative updates (which can later be dropped). The simplicity of the algorithm allows it to be easily proven correct, and the choice of shape property provides priority queue performance which is competitive with highly optimized skiplist implementations (and has stronger bounds on worst-case time complexity). A Scala reference implementation is provided.	best, worst and average case;concurrent computing;copy-on-write;data structure;fairness measure;image scaling;iteration;iterator;lazy evaluation;linearizability;lock (computer science);non-blocking algorithm;priority queue;reference implementation;round-off error;rounding;scala;skip list;snapshot (computer storage);speculative execution;stateless protocol;time complexity	Thomas D. Dickerson	2017	CoRR		parallel computing;snapshot (computer storage);time complexity;discrete mathematics;stateless protocol;priority queue;mathematics;scala;skip list;reference implementation;heap (data structure)	PL	-15.124419186741987	46.833225584587396	148275
59f00dc14f55b1087667f92f9dc71f1036d6c583	user-level remote memory paging for multithreaded applications	libraries;suspensions;virtual memory;paged storage;multi threading;memory management;virtual memory memory server distributed shared memory memory paging page swap remote memory;user level remote memory paging systems page swap mechanism inconsistent page problem multithreaded applications;instruction sets memory management suspensions servers libraries bandwidth educational institutions;remote memory;memory server;memory paging;page swap;servers;paged storage multi threading;bandwidth;distributed shared memory;instruction sets	The new page swap mechanism is introduced to resolve an inconsistent page problem for multithreaded applications in user-level remote paging systems. According to the evaluations, its overhead is limited and it can be applicable to actual use for multithreaded applications.	multithreading (computer architecture);overhead (computing);paging;thread (computing);user space	Hiroko Midorikawa;Yuichiro J Suzuki;Masatoshi Iwaida	2013	2013 13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing	10.1109/CCGrid.2013.63	distributed shared memory;demand paging;parallel computing;real-time computing;multithreading;page replacement algorithm;computer science;virtual memory;operating system;instruction set;database;bandwidth;server;memory management	Arch	-13.697642990247978	51.10563459390883	149061
5ec18a87068639a2d7259311fa48f796de1fc3e7	scalea: a performance analysis tool for parallel programs	instrumentation;visualization;performance analysis;performance overheads;parallel programs	Abstract#R##N##R##N#Many existing performance analysis tools lack the flexibility to control instrumentation and performance measurement for code regions and performance metrics of interest. Performance analysis is commonly restricted to single experiments.#R##N##R##N##R##N##R##N#In this paper we present SCALEA, which is a performance instrumentation, measurement, analysis, and visualization tool for parallel programs that supports post-mortem performance analysis. SCALEA currently focuses on performance analysis for OpenMP, MPI, HPF, and mixed parallel programs. It computes a variety of performance metrics based on a novel classification of overhead. SCALEA also supports multi-experiment performance analysis that allows one to compare and to evaluate the performance outcome of several experiments. A highly flexible instrumentation and measurement system is provided which can be controlled by command-line options and program directives. SCALEA can be interfaced by external tools through the provision of a full Fortran90 OpenMP/MPI/HPF frontend that allows one to instrument an abstract syntax tree at a very high-level with C-function calls and to generate source code. A graphical user interface is provided to view a large variety of performance metrics at the level of arbitrary code regions, threads, processes, and computational nodes for single- and multi-experiment performance analysis. Copyright © 2003 John Wiley & Sons, Ltd.	profiling (computer programming)	Hong Linh Truong;Thomas Fahringer	2003	Concurrency and Computation: Practice and Experience	10.1002/cpe.778	computational science;parallel computing;real-time computing;visualization;computer science;operating system;database;distributed computing;programming language;instrumentation	SE	-16.622326909231568	47.488668756845314	149224
b54e86b81db3d6c054972cc5a9ded880f330a0df	the interaction of parallel programming constructs and coherence protocols	nested parallelism;language implementation;parallel programs;space efficiency;dynamic scheduling;multithreading	Some of the most common parallel programming idioms include locks, barriers, and reduction operations. The interaction of these programming idioms with the multiprocessor's coherence protocol has a significant impact on performance. In addition, the advent of machines that support multiple coherence protocols prompts the question of how to best implement such parallel constructs, i.e. what combination of implementation and coherence protocol yields the best performance. In this paper we study the running time and communication behavior of (1) centralized (ticket) and MCS spin locks, (2) centralized, dissemination, and tree-based barriers, and (3) parallel and sequential reductions, under pure and competitive update coherence protocols; results for write-invalidate protocol are presented mostly for comparison purposes. Our experiments indicate that parallel programming techniques that are well-established for write invalidate protocols, such as MCS locks and parallel reductions, are often inappropriate for update-based protocols. In contrast, techniques such as dissemination and tree barriers achieve superior performance under update-based protocols. Our results also show that the implementation of parallel programming idioms must take the coherence protocol into account, since update-based protocols often lead to different design decisions than write invalidate protocols. Our main conclusion is that protocol-conscious implementation of parallel programming structures can significantly improve application performance; for multiprocessors that can support more than one coherence protocol both the protocol and implementation should betaken into account when exploiting parallel constructs.	barrier (computer science);cache coherence;centralized computing;communications protocol;experiment;lock (computer science);multiprocessing;parallel computing;programming idiom;reduction (complexity);time complexity	Ricardo Bianchini;Enrique V. Carrera;Leonidas I. Kontothanassis	1997		10.1145/263764.263774	parallel computing;real-time computing;multithreading;dynamic priority scheduling;computer science;distributed computing;programming language	HPC	-13.489317148807038	48.915399200867796	149900
76c519140a24ddae89de265ef6ee23fb8a39c232	virtual time iii: unification of conservative and optimistic synchronization in parallel discrete event simulation		There has long been a divide in synchronization approaches for parallel discrete event simulation, between conservative methods requiring lookahead and optimistic methods requiring rollback. These are usually seen as dichotomous, so that a model writer must make an early, static design decision between them. An optimistic simulator does not need lookahead information but is unable to take advantage of it even if it were available, whereas a conservative simulator may perform poorly or even deadlock without good lookahead information. Here we introduce unified virtual time (UVT) synchronization which provides the advantages of both conservative and optimistic synchronization dynamically for all models. Conservative synchronization becomes an accelerator for optimistic synchronization. When lookahead information is available the simulation will execute conservatively. Otherwise it will execute optimistically. In this paper we present UVT, argue for its correctness, and show adaptations of Time Warp, YAWNS, and Null Messages which cooperatively synchronize a single simulation.	correctness (computer science);deadlock;dynamic time warping;han unification;optimistic concurrency control;parsing;rollback (data management);simulation;synchronization (computer science)	David R. Jefferson;Peter D. Barnes	2017	2017 Winter Simulation Conference (WSC)	10.1109/WSC.2017.8247832	discrete event simulation;rollback;simulation;correctness;real-time computing;deadlock;computer science;synchronization;unification	HPC	-14.528110911285793	48.50553838067639	150350
e5238683325771e87aa405f17a6feebd30904190	bandwidth of crossbar and multiple-bus connections for multiprocessors	general and miscellaneous mathematics computing and information science;shared memory;multiprocessors;shared memory bus arbitration memory bandwidth multiple buses multiprocessors;interconnection network;equipment interfaces;array processors;bus arbitration;memory devices 990200 mathematics computers;multiple buses;memory bandwidth;effective bandwidth	In this paper we compare the effective bandwidth in a multiprocessor with shared memory using as interconnection networks the crossbar or the multiple-bus. We consider a system with N processors and N memory modules, in which the processor requests to the memory modules are independent and uniformly distributed random variables. We consider two cases: in the first the processor makes another request immediately after a memory service, and in the second there is some internal processing time.	central processing unit;crossbar switch;dimm;interconnection;multiprocessing;shared memory	Tomás Lang;Mateo Valero;Ignacio Alegre	1982	IEEE Transactions on Computers	10.1109/TC.1982.1675947	uniform memory access;distributed shared memory;shared memory;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer science;physical address;operating system;computer memory;overlay;conventional memory;extended memory;flat memory model;registered memory;memory bandwidth;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-11.849281073519585	48.07688570421085	150889
8801d22c30198db5deb655e3bd6b697e64e45c83	extreme scale data management in high performance computing	gerald f;io;adaptive;hpc;dissertation;computer science extreme scale data management in high performance computing georgia institute of technology karsten schwan lofstead;storage;file systems;ii	Extreme scale data management in high performance computing requires consideration of the end-to-end scientific workflow process. Of particular importance for runtime performance, the write-read cycle must be addressed as a complete unit. Any optimization made to enhance writing performance must consider the subsequent impact on reading performance. Only by addressing the full write-read cycle can scientific productivity be enhanced. #R##N#The ADIOS middleware developed as part of this thesis provides an API nearly as simple as the standard POSIX interface, but with the flexibility to choose what transport mechanism(s) to employ at or during runtime. The accompanying BP file format is designed for high performance parallel output with limited coordination overheads while incorporating features to accelerate subsequent use of the output for reading operations. This pair of optimizations of the output mechanism and the output format are done such that they either do not negatively impact or greatly improve subsequent reading performance when compared to popular self-describing file formats. This end-to-end advantage of the ADIOS architecture is further enhanced through techniques to better enable asychronous data transports affording the incorporation of 'in flight' data processing operations and pseudo-transport mechanisms that can trigger workflows or other operations.	ibm websphere extreme scale;supercomputer	Gerald Fredrick Lofstead	2010			real-time computing;simulation;computer science;database	HPC	-14.595288395021157	52.735956206309574	151013
56cd3ff6758b55fd95df82cf877934167589b281	the read-copy-update mechanism for supporting real-time applications on shared-memory multiprocessor systems with linux	bepress selected works;parallel processing electronic computers electronic data processing distributed processing;real time application;shared memory multiprocessor	D. Guniguntala P. E. McKenney J. Triplett J. Walpole Read-copy update (RCU) is a synchronization mechanism in the Linux TM kernel that provides significant improvements in multiprocessor scalability by eliminating the writer-delay problem of readers-writer locking. RCU implementations to date, however, have had the side effect of expanding non-preemptible regions of code, thereby degrading real-time response. We present here a variant of RCU that allows preemption of read-side critical sections and thus is better suited for real-time applications. We summarize priority-inversion issues with locking, present an overview of the RCU mechanism, discuss our counter-based adaptation of RCU for real-time use, describe an additional adaptation of RCU that permits general blocking in readside critical sections, and present performance results. We also discuss an approach for replacing the readers-writer synchronization with RCU in existing implementations.	blocking (computing);critical section;linux;lock (computer science);multiprocessing;preemption (computing);priority inversion;read-copy-update;real-time clock;real-time transcription;scalability;shared memory	Dinakar Guniguntala;Paul E. McKenney;Josh Triplett;Jonathan Walpole	2008	IBM Systems Journal	10.1147/sj.472.0221	embedded system;parallel computing;real-time computing;computer science;operating system;database	OS	-13.709232192588049	49.95879352141309	152301
258d9e34477f2d1d8c402fe9b214ecd870103b68	towards a methodology for deliberate sample-based statistical performance analysis	performance measure;program diagnostics;statistical analysis program diagnostics software performance evaluation;software performance evaluation;profiles techniques;statistical analysis;high performance computer;performance analysis;perturbation effects statistical performance analysis dynamic performance analysis long running program analysis high performance computing statistical profiling technique systematic sampling rates deliberate sampling rate selection systematic sample rate selection performance measurement model measurement precision;time measurement performance analysis analytical models context instruments measurement errors;systematic sampling	Dynamic performance analysis of long-running programs in the high performance computing community increasingly relies on statistical profiling techniques to provide performance measurement results. Systematic sampling rates used to generate the statistical data are typically selected in an ad hoc manner with little formal regard for the context provided by the program being analyzed and the underlying system on which it is run. In an effort to provide a more effective statistical profiling process and additional rigor we argue in favor of the general principle of deliberate sampling rate selection. We present our idea for a methodology of systematic sample rate selection based on a performance measurement model incorporating the effect of sampling on both measurement precision and perturbation effects.	experiment;hoc (programming language);profiling (computer programming);sampling (signal processing);statistical model;supercomputer	Geoff Stoker;Jeffrey K. Hollingsworth	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.262	systematic sampling;simulation;computer science;statistics	Arch	-17.415975907409166	47.87578029787617	152569
1493d9a82704b37787d04e4afd7aabed5e2165ea	a load-sharing architecture for high performance optimistic simulations on multi-core machines	performance evaluation discrete event simulation distributed algorithms multiprocessing systems parallel architectures;open source c based simulation platform load sharing architecture high performance optimistic simulations multicore machines parallel discrete event simulation pdes logical processes lp multiprocessor machines rollback techniques kernel instance simulation distributed algorithm rome optimistic simulator root sim	In Parallel Discrete Event Simulation (PDES), the simulation model is partitioned into a set of distinct Logical Processes (LPs) which are allowed to concurrently execute simulation events. In this work we present an innovative approach to load-sharing on multi-core/multiprocessor machines, targeted at the optimistic PDES paradigm, where LPs are speculatively allowed to process simulation events with no preventive verification of causal consistency, and actual consistency violations (if any) are recovered via rollback techniques. In our approach, each simulation kernel instance, in charge of hosting and executing a specific set of LPs, runs a set of worker threads, which can be dynamically activated/deactivated on the basis of a distributed algorithm. The latter relies in turn on an analytical model that provides indications on how to reassign processor/core usage across the kernels in order to handle the simulation workload as efficiently as possible. We also present a real implementation of our load-sharing architecture within the ROme OpTimistic Simulator (ROOT-Sim), namely an open-source C-based simulation platform implemented according to the PDES paradigm and the optimistic synchronization approach. Experimental results for an assessment of the validity of our proposal are presented as well.	causal consistency;causal filter;distributed algorithm;multi-core processor;multiprocessing;open-source software;programming paradigm;root;simulation;synchronization (computer science);thread (computing)	Roberto Vitali;Alessandro Pellegrini;Francesco Quaglia	2012	2012 19th International Conference on High Performance Computing	10.1109/HiPC.2012.6507510	parallel computing;real-time computing;computer science;operating system;distributed computing;programming language	HPC	-15.083268364798622	47.91900398102337	153568
1393b0233321538385ed771255c16a244f7d7859	ycsb++: benchmarking and performance debugging advanced features in scalable table stores	benchmarking;ycsb;operating system;scalable table stores;nosql	Inspired by Google's BigTable, a variety of scalable, semi-structured, weak-semantic table stores have been developed and optimized for different priorities such as query speed, ingest speed, availability, and interactivity. As these systems mature, performance benchmarking will advance from measuring the rate of simple workloads to understanding and debugging the performance of advanced features such as ingest speed-up techniques and function shipping filters from client to servers. This paper describes YCSB++, a set of extensions to the Yahoo! Cloud Serving Benchmark (YCSB) to improve performance understanding and debugging of these advanced features. YCSB++ includes multi-tester coordination for increased load and eventual consistency measurement, multi-phase workloads to quantify the consequences of work deferment and the benefits of anticipatory configuration optimization such as B-tree pre-splitting or bulk loading, and abstract APIs for explicit incorporation of advanced features in benchmark tests. To enhance performance debugging, we customized an existing cluster monitoring tool to gather the internal statistics of YCSB++, table stores, system services like HDFS, and operating systems, and to offer easy post-test correlation and reporting of performance behaviors. YCSB++ features are illustrated in case studies of two BigTable-like table stores, Apache HBase and Accumulo, developed to emphasize high ingest rates and finegrained security.	apache accumulo;apache hbase;apache hadoop;b-tree;benchmark (computing);debugging;eventual consistency;google bigtable;interactivity;mathematical optimization;operating system;scalability;semiconductor industry;ycsb	Swapnil Patil;Milo Polte;Kai Ren;Wittawat Tantisiriroj;Lin Xiao;Julio López Hernandez;Garth A. Gibson;Adam Fuchs;Billie Rinaldi	2011		10.1145/2038916.2038925	embedded system;real-time computing;computer science;operating system;database;benchmarking	HPC	-14.405117223576795	53.17556484258447	153926
b461090b20490b5f41bf63155a943c6f47019c98	data placement and buffer management for concurrent mergesorts with parallel prefetching	databases;query processing;sorting;storage management;prefetching delay writing throughput sorting databases query processing;prefetching;buffer management;buffer storage;concurrent mergesorts;run placement policies buffer management data placement concurrent mergesorts parallel prefetching initial sorted runs multiple disks thrashing;parallel prefetching;writing;multiple disks;storage management sorting parallel algorithms buffer storage;thrashing;run placement policies;data placement;initial sorted runs;throughput;parallel algorithms	Various data placement policies are studied for the merge phase of concurrent mergesorts using parallel prefetching, where initial sorted runs (input) of a merge and its final sorted run (output) are stored on multiple disks but each run resides only on a single disk. Since the merge phase involves only sequential references, parallel prefetching can be attractive an reducing the average response time for concurrent merges. However, without careful buffer control, severe thrashing may develop under certain run placement policies, reducing the benefits of prefetching. The authors examine through detailed simulations three different run placement policies. The results show that even though buffer thrashing can be almost avoided by placing the output run of a job on the same disk with at least one of its input runs, this thrashing-avoiding run placement policy can be substantially outperformed by other policies that use buffer thrashing control. With buffer thrashing avoidance, the best performance as achieved by a run placement policy that uses a proper subset of disks dedicated for writing the output runs while the rest of the disks are used for prefetching the input runs in parallel. >	link prefetching	Kun-Lung Wu;Philip S. Yu;James Z. Teng	1994		10.1109/ICDE.1994.283063	throughput;parallel computing;real-time computing;thrashing;computer science;sorting;database;distributed computing;parallel algorithm;writing	DB	-12.933268103815745	52.15312691821829	155345
9705d73b7acf4dc4413bfa894d227c271a03ea75	parallel garbage collection without synchronization overhead	synchronization overhead;parallel garbage collection;parallel processing;garbage;reduction;virtual memory;algorithm design;efficiency;garbage collection;collection;switching;algorithms;consistency	Incremental garbage collection schemes incur substantial overhead which is directly translated as reduced execution efficiency for the user. Parallel garbage collection schemes implemented via time-slicing on a serial processor also incur this overhead, which might even be aggravated due to context switching. It is useful, therefore, to examine the possibility of implementing a parallel garbage collection algorithm using a separate processor operating asynchronously with the main list processor. The overhead in such a scheme arises from the synchronization necessary to manage the two processors, maintaining memory consistency. In this paper, we present an architecture and supporting parallel garbage collection algorithms designed for a virtual memory system with separate processors for list processing and for garbage collection. Each processor has its own primary memory; in addition, there is a small common memory which both processors may access. Individual memories swap off a common secondary memory, but no locking mechanism is required. In particular, a page may reside in both memories simultaneously, and indeed may be accessed and modified freely by each processor. A secondary memory controller ensures consistency without necessitating numerous lockouts on the pages.	algorithm;auxiliary memory;central processing unit;communications of the acm;computer architecture;computer data storage;consistency model;context switch;disk controller;garbage collection (computer science);lisp;locality of reference;lock (computer science);memory controller;openvms;overhead (computing);page fault;paging;the art of computer programming;time slicing (digital broadcasting)	Ashwin Ram;Janak H. Patel	1985		10.1145/327010.327134	manual memory management;parallel processing;algorithm design;computer architecture;garbage;parallel computing;real-time computing;collection;region-based memory management;distributed memory;reduction;computer hardware;computer science;virtual memory;operating system;garbage in, garbage out;efficiency;consistency;garbage collection;programming language;memory leak	Arch	-12.767052050906653	48.83413952131203	155698
e2a36759079cb1bf0d66fa759535c1e9c7f1a3c2	performance improvement with zero copy technique on fuse-based consumer devices	input output programs;buffer storage;buffer storage zero copy technique fuse based multimedia consumer devices user space fuse file system pc compatible fat file system emulation performance overhead data copy kernel space fuse file system performance improvement user data copy removal i o performance improvement kernel data copy removal;fuses kernel file systems performance evaluation multimedia communication benchmark testing;operating system kernels buffer storage input output programs;operating system kernels	FUSE is a framework to develop file systems in user space. Recently, multimedia consumer devices are adopting the FUSE file system to emulate PC-compatible FAT file system. However, FUSE file system has performance overhead due to the data copy between user space and kernel space. In this paper, we improve the performance of FUSE file system by removing the user/kernel data copy with a zero copy technique. Our experiments on a smartphone showed that the zero copy technique improves I/O performance by up to 25%.	design of the fat file system;experiment;file allocation table;input/output;overhead (computing);smartphone;user space;zero-copy	Junsup Song;Dongkun Shin	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776074	self-certifying file system;embedded system;real-time computing;device file;computer file;computer hardware;computer science;stub file;operating system;unix file types;journaling file system;file system fragmentation;configfs	Embedded	-13.23196610842489	52.63320140278008	156015
0ddd47eeb68100f13633f43f71086e606389115f	improving machine virtualization with 'hotplug memory'	virtual machine;memory management;paging activity machine virtualization hotplug memory server consolidation on demand server provisioning virtualization software virtual machine dynamic memory extension linux memory management;storage management;storage management virtual machines linux;virtual machines;linux;virtual environment;virtual machining linux operating systems laboratories hardware memory management virtual machine monitors software performance throughput platform virtualization;device driver	Machine virtualization has emerged as a key technology for server consolidation and on-demand server provisioning. To support this trend, it is essential to improve the performance of virtualization software and hence enable the efficient running of many virtual machines. We present a virtualization system that can dynamically extend the real memory of its guest virtual machines. We describe an implementation of dynamic memory extension for Linux guests running on the IBM zVM virtualization environment. Our implementation for the Linux extension is based on device drivers for accessing these dynamic memory extensions. Moreover, we show that this new capability can improve utilization and performance of the Linux guests in our virtualization environment. Specifically, memory management is improved and more virtual machines can run at a given moment. We study the utilization of dynamic memory extension of a Linux guest for a JVM heap. Running Specjbb2000 benchmark on a small virtual machine extended with dynamic memory to host the heap, we measured an improvement in transaction throughput and a 23.23% reduction in paging activity compared to an initially large machine. We further studied the implication of our experiments on the number of virtual machines that can run efficiently.	benchmark (computing);device driver;experiment;hardware virtualization;hot swapping;java virtual machine;linux;memory management;paging;provisioning;semiconductor consolidation;server (computing);throughput	Shlomit S. Pinter;Steven S. Shultz;Yariv Aridor;Sergey Guenender	2005	17th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD'05)	10.1109/CAHPC.2005.29	memory footprint;embedded system;full virtualization;parallel computing;virtualization;thin provisioning;temporal isolation among virtual machines;application virtualization;computer science;virtual machine;virtual memory;kernel virtual address space;operating system;hardware virtualization;page table;overlay;conventional memory;extended memory;hypervisor;data diffusion machine;memory management	Arch	-16.32467062333641	51.70876518068943	156019
1004152121cc1bae4c3485b49fb5d39d1fdccb22	collective caching: application-aware client-side file caching	libraries;cache data management;distributed system;cache storage;file servers;concurrent computing;performance evaluation;gpfs collective caching application aware client side file caching parallel file subsystems high performance computers i o optimization distributed systems i o request processing distributed environment parallel applications caching subsystem cache data management cache coherence message passing interface portability ibm sp;application software;application aware client side file caching;parallel file subsystems;gpfs;client server systems;i o request processing;portability;caching subsystem;col;message passing interface;distributed environment;data structures;collective caching;application program interfaces;parallel file system;high performance computer;cache coherence;client server systems cache storage parallel processing application program interfaces message passing;message passing;high performance computers;distributed systems;i o optimization;parallel applications;parallel processing;ibm sp;file systems;concurrent computing application software file servers file systems laboratories message passing parallel processing data structures libraries	"""Parallel file subsystems in today's high-performance computers adopt many I/O optimization strategies that were designed for distributed systems. These strategies, for instance client-side file caching, treat each I/O request process independently, due to the consideration that clients are unlikely related with each other in a distributed environment. However, it is inadequate to apply such strategies directly in the high-performance computers where most of the I/O requests come from the processes that work on the same parallel applications. We believe that client-side caching could perform more effectively if the caching subsystem is aware of the process scope of an application and regards all the application processes as a single client. In this paper, we propose the idea of """"collective caching"""" which coordinates the application processes to manage cache data and achieve cache coherence without involving the I/O servers. To demonstrate this idea, we implemented a collective caching subsystem at user space as a library, which can be incorporated into any message passing interface implementation to increase its portability. The performance evaluation is presented with three I/O benchmarks on an IBM SP using its native parallel file system, GPFS. Our results show significant performance enhancement obtained by collective caching over the traditional approaches."""	benchmark (computing);cache (computing);cache coherence;client-side;clustered file system;collective intelligence;computer;distributed computing;ibm gpfs;input/output;lock (computer science);mathematical optimization;message passing interface;performance evaluation;software portability;supercomputer;user space	Wei-keng Liao;Kenin Coloma;Alok N. Choudhary;Lee Ward;Eric Russell;Sonja Tideman	2005	HPDC-14. Proceedings. 14th IEEE International Symposium on High Performance Distributed Computing, 2005.	10.1109/HPDC.2005.1520940	file server;parallel processing;cache coherence;application software;parallel computing;message passing;false sharing;concurrent computing;computer science;inline caching;message passing interface;operating system;database;distributed computing;smart cache;programming language;distributed computing environment	HPC	-15.051640130585493	50.69211199852395	157250
4ce5d8853dc8bc06df447396c01c8d81ef7f0517	a wait-free multi-word atomic (1,n) register for large-scale data sharing on multi-core machines		We present a multi-word atomic (1,N) register for multi-core machines exploiting Read-Modify-Write (RMW) instructions to coordinate the writer and the readers in a wait-free manner. Our proposal, called Anonymous Readers Counting (ARC), enables large-scale data sharing by admitting up to 2^{32}-2 concurrent readers on off-the-shelf 64-bit machines, as opposed to the most advanced RMW-based approach which is limited to 58 readers. Further, ARC avoids multiple copies of the register content while accessing it—this affects classical register's algorithms based on atomic read/write operations on single words. Thus, ARC allows for higher scalability with respect to the register size.	64-bit computing;correctness (computer science);multi-core processor;non-blocking algorithm;read-modify-write;scalability;shared memory	Mauro Ianni;Alessandro Pellegrini;Francesco Quaglia	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.84	parallel computing;arc (geometry);computer science;stack register;real-time computing;scalability;multi-core processor;data sharing	HPC	-14.965202585435268	46.98112202762796	157419
8970c263b3cbd8408763c6aeec81beedaaec9b7b	a new network storage architecture based on nas and san	storage management adaptive control bandwidth allocation file organisation ip networks memory architecture storage area networks;high availability;protocols;storage system;bandwidth allocation;storage management;adaptive control;ac;network storage architecture nas san ip high performance storage network global multiprotocol file system iscsi module adaptive controller ultra high throughput block i o file i o services bandwidth allocation;iscsi;storage area networks;servers;hpsn;memory architecture;file system;gmpfs;ac hpsn gmpfs iscsi;bandwidth;ip networks;scalability;high throughput;storage area networks storage automation scalability network servers file servers protocols file systems programmable control adaptive control bandwidth;file systems;file organisation	NAS and SAN have their own advantages respectively and they are two primary network storage systems nowadays. But they also have their own limitations and can't meet the demand of the high-speedly increasing network application. This paper presents a new network storage architecture made by integrating NAS and SAN in IP: the high performance storage network (HPSN). Firstly, with the help of the Global Multi Protocol File System (GMPFS), HPSN implements the unification of NAS and SAN,and meet the requirements of a high scalability and large capacity. Secondly, HPSN can offer the block I/O and file I/O services simultaneously by iSCSI module, and it has the advantages of NAS and SAN. Thirdly, with the help of the adaptive controller (AC), HPSN can adaptively allocate bandwidth in accordance with various requirements of the performance and priority. As shown in the experiments concerned, HPSN has an ultra-high-throughput for the file I/O and block I/O, and may provide the great capacity network storage system with higher performance, scalability, compatibity,high- availability and quality/price, etc.	computer data storage;experiment;high availability;high-throughput computing;iscsi;input/output;multi-user;network-attached storage;prototype;quality of service;requirement;scalability;storage area network;throughput;unification (computer science)	Xiao-Gao Yu;Wei-Xing Li	2008	2008 10th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2008.4795877	high-throughput screening;communications protocol;real-time computing;storage area network;scalability;ac;adaptive control;computer science;operating system;high availability;bandwidth;server;computer network;bandwidth allocation	HPC	-17.82848270597531	52.29393197269763	157685
31aa79b78b13d5796473ce8c7ac7f40a35370f93	nomadic threads: a migrating multithreaded approach to remote memory accesses in multiprocessors	distributed memory;yarn program processors delay data structures hardware runtime workstations parallel processing programming profession pipelines;distributed memory systems;yarn;nomadic threads;data locality;abstract data types;multiprocessors;abstract multithreaded architecture;migrating multithreaded approach;remote memory accesses;runtime interface;runtime;compilers nomadic threads migrating multithreaded approach remote memory accesses multiprocessors abstract multithreaded architecture distributed memory multicomputers spatial locality runtime interface;compilers;parallel architectures;data structures;programming profession;pipelines;workstations;multithreaded architecture;network of workstation;distributed memory multicomputers;distributed memory multicomputer;runtime system;spatial locality;remote memory access;data structure;program processors;parallel processing;data transfer;hardware	This paper describes an abstract multithreaded ar chitecture for distributed memory multicomputers that signifi cantly reduces the number of message transfers when compared to conventional “remote memory access” approaches. Instead of statically executing on its assigned processor and fetching data from remote storage, a Nomadic Thread transfers itself to the processor which contains the data it needs. This enables Nomadic Thr eads to take advantage of spatial locality found in the usage of many data structures, because the migration of a thread to a node makes access to surrounding data local. By r educing the number of messages and taking advantage of locality, the Nomadic Threads approach allows programs to use fewer data transfers than conventional approaches while providing a simple runtime interface to compilers. The Nomadic Threads runtime system is currently implemented for the Thinking Machines Corp. Connection Machine 5 (CM5), but is portable to other distributed memory systems, including networks of workstations.	autonomous robot;benchmark (computing);cache (computing);central processing unit;compiler;connection machine;data access;data structure;distributed memory;locality of reference;multithreading (computer architecture);principle of locality;process migration;recursion;reflow soldering;runtime system;sisal;scheduling (computing);synchronization (computer science);thread (computing);workstation	Stephen Jenks;Jean-Luc Gaudiot	1996		10.1109/PACT.1996.554028	computer architecture;compiler;parallel computing;distributed memory;workstation;data structure;computer science;operating system;distributed computing;pipeline transport;programming language;abstract data type	Arch	-12.540643184772984	46.6099298521147	157934
b751cd98c3bc8959597570c5ec42577ed015c75e	automatic parameter assessment of logp-based communication models in mpi environments	network design;communication model;interconnection network;automatic detection;behavior change;software development;performance model;communication protocol	Communications in modern interconnection networks become a complex issue to deliver the highest performance to the user. In MPI environments, this complexity leads to different communication protocols for different message sizes. The characterization of these different behaviors is important and useful for software developers and network designers. This paper presents an automatic method to obtain a characterization of the communication behavior of a specific MPI environment using LogP-based models. This procedure automatically detects the message sizes where the communication behavior changes due to the influence of different architectural features and protocols. Thereby, the range of message sizes is split in different intervals, in a transparent way to the user. Then, using LogP-based models, each interval is characterized by its own set of parameters. The procedure is based on the parameterized roundtrip-time low-overhead microbenchmark. The detection of different communication behaviors and the assessment of the parameters of the LogP-based models were implemented in an easy to use statistical environment. Real MPI environments were characterized using our proposal.	logp machine;message passing interface	Diego Rodriguez Martínez;Vicente Blanco Pérez;José Carlos Cabaleiro;Tomás F. Pena;Francisco F. Rivera	2010		10.1016/j.procs.2010.04.241	communications protocol;network planning and design;real-time computing;models of communication;computer science;theoretical computer science;software development;operating system;behavior change;distributed computing	HPC	-16.534772400264348	47.652622887983185	158275
8b3235bbd59d3b85081d9c22cf1af494e2d1159a	diskless checkpointing	distributed processing;fault tolerant computing;system recovery	Diskless Checkpointing is a technique for checkpointing the state of a long-running computation on a distributed system without relying on stable storage. As such, it eliminates the performance bottleneck of traditional checkpointing on distributed systems. In this paper, we motivate diskless checkpointing and present the basic diskless checkpointing scheme along with several variants for improved performance. The performance of the basic scheme and its variants is evaluated on a high-performance network of workstations and compared to traditional disk-based checkpointing. We conclude that diskless checkpointing is a desirable alternative to disk-based checkpointing that can improve the performance of distributed applications in the face of failures.	application checkpointing;computation;computer cluster;diskless node;distributed computing;stable storage;workstation	James S. Plank;Kai Li;Michael A. Puening	1998	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.730527	parallel computing;real-time computing;computer science;distributed computing	HPC	-19.053618480433336	49.616608068076935	158935
b478612e2e5636f12a109366be3c4ee241d494ee	virtual memory constraints in 32-bit windows: an update	virtual memory	This paper discusses the signs that indicate a machine is suffering from a virtual memory constraint in 32-bit Windows. Machines configured with 2 GB or more of RAM installed are particularly vulnerable to this condition. It also discusses options to keep this from happening, including (1) changing the way 32-bit virtual address spaces are partitioned into private and shared ranges, (2) settings that govern the size of system memory pools, (3) hardware that supports 37-bit addressing, and (4) hardware that supports 64-bit addressing but can still run 32-bit applications in compatibility mode. Ultimately, the option of running Windows on 64-bit processors is the safest and surest way to relieve the virtual memory constraints associated with 32-bit Windows.	32-bit;64-bit computing;central processing unit;compatibility mode;gigabyte;memory pool;microsoft windows;random-access memory;update (sql)	Mark B. Friedman	2005			flat memory model;conventional memory;dos memory management;data diffusion machine;memory segmentation;real-time computing;extended memory;commit charge;virtual memory;computer science	Arch	-13.726108835836861	51.890172273836484	159205
5cc68f891d9e372ddb83117f4057f8aa4a970acd	virtual time ii: storage management in conservative and optimistic systems	time warp;shared memory;storage management	The main contribution of this paper is the Cancelback Protocol, an extension of the Time Warp mechanism that handles storage management. It includes both fossil collection, the recovery of storage for messages and states that can never again influence the computation, and cancelback, the recovery of storage assigned to messages and states at times so far in the future that their memory would be better used for more immediate purposes. It guarantees that Time Warp is optimal in its storage requirements when run in shared memory, i.e. Time Warp will successfully complete a simulation using no more space than it would take to execute the same simulation with the sequential event list algorithm. This is better by a factor of two than the only previously published result. Without this protocol (or equivalent) Time Warp’s behavior can be unstable; hence it should be considered an essential part of Time Warp mechanism, rather than simply a refinement. In addition we also prove that asynchronous conservative algorithms, including all of the Chandy-Misra-Bryant (CMB) mechanisms, are not optimal; they cannot necesPermission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission.	chandy-misra-haas algorithm resource model;computation;control theory;dynamic time warping;fossil;misra c;refinement (computing);requirement;shared memory;simulation	David Jefferson	1990		10.1145/93385.93403	shared memory;parallel computing;real-time computing;computer science;database;memory management	PL	-15.898415576735008	47.201311867562374	159975
59d079131c21613ecc025ce22eaa73e545b8ceaf	towards an efficient performance evaluation of communication systems described by message sequence charts	performance measure;developpement logiciel;distributed system;resource utilization;evaluation performance;consumption;metodo analitico;queueing network;communication system;systeme reparti;systeme evenement discret;performance evaluation;asynchrone;diagrama secuencia mensaje;interaction;evaluacion prestacion;metodo formal;methode formelle;red cola espera;system performance;formal method;sistema acontecimiento discreto;consumo;discrete event system;sistema repartido;message sequence chart;reseau file attente;desarrollo logicial;analytical method;consommation;envoi message;software development;retard;message passing;methode analytique;regime permanent;interaccion;regimen permanente;end to end delay;retraso;diagramme sequence message;asincrono;asynchronous;steady state;discrete event simulation	A message sequence chart (MSC) is a high-level description of the message interaction between system components and their environment. Communication between distributed instances can be described by MSCs and these descriptions can be extended by notions for time consumption and resources and afterwards included in a system performance model. Such models can be evaluated by discrete event simulation or under reasonable assumptions alternatively with analytical queueing network algorithms. In this way steady state performance measures like resource utilizations and end-to-end delays can be calculated with low effort. The simulation uses the same input like the analytical formulas and allows for the investigation of dynamic performance behaviour or for the study of models including features which can not be handled by analytical formulas.	algorithm;end-to-end principle;high- and low-level;message sequence chart;microsoft cluster server;performance evaluation;queueing theory;simulation;steady state	Hesham Kamal Arafat Mohamed;Bruno Müller-Clostermann	2003		10.1007/978-3-540-39979-7_27	in situ resource utilization;message passing;interaction;real-time computing;simulation;formal methods;consumption;telecommunications;computer science;software development;discrete event simulation;asynchronous communication;end-to-end delay;computer performance;programming language;steady state;message sequence chart;communications system	Metrics	-18.627577383424793	46.40500764039654	160702
b0d549bcd02d14b0f0d32735a6913a537953186e	competitive prefetching for concurrent sequential i/o	workload;busqueda informacion;politica optima;sistema operativo;sequential access;compilacion;grain size;system core;systeme unix;entrada salida;distribution donnee;competitive prefetching;performance evaluation;competitividad;information retrieval;unix system;prefetching;serveur informatique;prechargement donnee;nucleo sistema;i o;information access;optimal policy;noyau systeme;data distribution;input output;noyau systeme exploitation;systeme linux;acceso secuencial;operating system;sistema linux;grosor grano;recherche information;acceso aleatorio;indexation;retard;charge travail;data access;competitiveness;acces information;compilation;servidor informatico;a priori information;systeme exploitation;acceso informacion;sistema unix;operating system kernels;carga trabajo;precargamento dato;linux system;retraso;politique optimale;competitivite;acces sequentiel;distribucion dato;random access;oracle;disk array;entree sortie;computer server;grosseur grain;acces aleatoire	During concurrent I/O workloads, sequential access to one I/O stream can be interrupted by accesses to other streams in the system. Frequent switching between multiple sequential I/O streams may severely affect I/O efficiency due to long disk seek and rotational delays of disk-based storage devices. Aggressive prefetching can improve the granularity of sequential data access in such cases, but it comes with a higher risk of retrieving unneeded data. This paper proposes a competitive prefetching strategy that controls the prefetching depth so that the overhead of disk I/O switch and unnecessary prefetching are balanced. The proposed strategy does not require a-priori information on the data access pattern, and achieves at least half the performance (in terms of I/O throughput) of the optimal offline policy. We also provide analysis on the optimality of our competitiveness result and extend the competitiveness result to capture prefetching in the case of random-access workloads.  We have implemented the proposed competitive prefetching policy in Linux 2.6.10 and evaluated its performance on both standalone disks and a disk array using a variety of workloads (including two common file utilities, Linux kernel compilation, the TPC-H benchmark, the Apache web server, and index searching). Compared to the original Linux kernel, our competitive prefetching system improves performance by up to 53%. At the same time, it trails the performance of an oracle prefetching strategy by no more than 42%.	benchmark (computing);cpu cache;compiler;data access;disk array;hard disk drive performance characteristics;ibm tivoli storage productivity center;input/output;interrupt;linux;online and offline;oracle nosql db;overhead (computing);random access;sequential access;server (computing);throughput;web server	Chuanpeng Li;Kai Shen;Athanasios E. Papathanasiou	2007		10.1145/1272996.1273017	input/output;embedded system;real-time computing;computer science;operating system;computer security	OS	-13.924246203442628	53.2667590618162	161414
5a36f17e0560750a956064ff06b63bcd57c6145f	scalable in-memory transaction processing with htm		We propose a new HTM-assisted concurrency control protocol, called HTCC, that achieves high scalability and robustness when processing OLTP workloads. HTCC attains its goal using a two-pronged strategy that exploits the strengths of HTM. First, it distinguishes between hot and cold records, and deals with each type differently – while accesses to highly contended data are protected using conventional fine-grained locks, accesses to cold data are HTM-guarded. This remarkably reduces the database transaction abort rate and exploits HTM’s effectiveness in executing low-contention critical sections. Second, to minimize the overhead inherited from successive restarts of aborted database transactions, HTCC caches the internal execution states of a transaction for performing delta-restoration, which partially updates the maintained read/write set and bypasses redundant index lookups during transaction re-execution at best effort. This approach is greatly facilitated by HTM’s speedy hardware mechanism for ensuring atomicity and isolation. We evaluated HTCC in a main-memory database prototype running on a 4 socket machine (40 cores in total), and confirmed that HTCC can scale near-linearly, yielding high transaction rate even under highly contended workloads.	atomicity (database systems);best-effort delivery;circuit restoration;co-fired ceramic;concurrency (computer science);concurrency control;critical section;database transaction;html;in-memory database;isolation (database systems);lock (computer science);online transaction processing;overhead (computing);prototype;scalability	Yingjun Wu;Kian-Lee Tan	2016			parallel computing;real-time computing;distributed transaction;computer science;operating system;database;online transaction processing	OS	-14.542022697418094	50.10208612430011	162427
3c5474cdc4b5d9bafb5036b17e58d588b634dada	audit: new synchronization for the get/put protocol	protocols;parallel programming;synchronisation;application program interfaces;synchronization protocols radiation detectors libraries algorithms benchmark testing program processors;synchronisation application program interfaces parallel programming protocols;armci audit synchronization get put protocol api parallel computing get put memory regions natural parallel programing	"""The GET/PUT protocol is considered as an effective communication API for parallel computing. However, the one-sided nature of the GET/PUT protocol lacks the synchronization functionality on the target process. So far, several techniques have been proposed to tackle this problem. The APIs for the synchronization proposed so far have failed to hide the implementation details of the synchronization. In this paper, a new synchronization API for the GET/PUT protocol is proposed. The idea here is to associate the synchronization flags with the GET/PUT memory regions. By doing this, the synchronization flags are hidden from users, and users are free from managing the associations between the memory regions and synchronization flags. The proposed API, named """"Audit,"""" does not incur additional programing and thus enables natural parallel programing. The evaluations show that {it Audit} exhibits better performance than the Notify API proposed in ARMCI."""	application programming interface;benchmark (computing);counter (digital);message passing interface;mg (editor);open-source software;parallel computing;remote direct memory access;synchronization (computer science);universal product code	Atsushi Hori;Jinpil Lee;Mitsuhisa Sato	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.217	embedded system;communications protocol;synchronization;parallel computing;real-time computing;computer science;operating system;distributed computing;data synchronization;synchronization;programming language;algorithm;computer network	Embedded	-14.261893969146824	47.969426371270075	162808
cdb16c870e905e94a650aa97bb1689aeb2a92ed5	optimizing communication in time-warp simulators	time warp;performance evaluation;communication complexity;performance evaluation time warp simulation message passing communication complexity;time warp simulation computational modeling kernel costs frequency synchronization message passing computer networks physics computing propagation delay contracts;communication behavior message passing environments message send time overheads message size fine grained applications time warp simulators communication subsystem optimization dynamic message aggregation close temporal proximity dynamic aggregation single physical message aggregation strategies communication overhead message delay network of workstations smp;network of workstation;message passing;time warp simulation	In message passing environments, the message send time is dominated by overheads that are relatively independent of the message size. Therefore,Jine-grained applications (such as TimeWarp simulators) suffer high overheads because of frequent communication. In this paper; we investigate the optimization of the communication subsystem of TimeWarp simulators using dynamic message aggregation. Under this scheme, TimeWarp messages with the same destination Le occuring in close temporal proximity are dynamically aggregated and sent as a single physical message. Several aggregation strategies that attempt to minimize the communication overhead without harming the progress of the simulation (because of messages being delayed) are developed. The performance of the strategies is evaluated for a network of workstations, and an SMP, using a number of applications that have different communication behavior	computer cluster;mathematical optimization;message passing;optimizing compiler;overhead (computing);simulation;symmetric multiprocessing;workstation	Malolan Chetlur;Nael B. Abu-Ghazaleh;Radharamanan Radhakrishnan;Philip A. Wilsey	1998		10.1145/278008.278017	parallel computing;message passing;real-time computing;computer science;communication complexity;distributed computing;message broker;programming language	HPC	-14.31187523696513	47.59126278723689	163084
01aa440dd2d46ee79988b482f3e739952880ff14	file placing control for improving the i/o performance of hadoop in virtualized environment	computers;performance evaluation;virtual machining;layout;large scale systems;cloud computing;operating systems	Hadoop is a popular open-source MapReduce implementation and has been widely used in many large scale systems. For improving I/O performance of Hadoop, a method which controlled file storing location based on sequential I/O speed of the storage device was proposed. However, the method did not take account of virtualized environment. In this paper, we focus on virtualized environment with a fixed number of virtual machines and discuss a method for improving I/O performance of Hadoop in virtualized environment. First, we evaluate performance of the existing method in virtualized environment and point its ineffective behaviors out. Second, we propose a new method considering this issue. The method takes account of both of the sequential access performance and seek distance among virtual machine image files. Third, we evaluate the proposed method with virtualized environment wherein plural virtual machines are running on a physical machine and demonstrate that the method can improve I/O performance of Hadoop application.	apache hadoop;bare machine;data striping;input/output;mapreduce;mathematical optimization;open-source software;performance evaluation;sandbox (computer security);sequential access;virtual machine	Kenji Nakashima;Eita Fujishima;Saneyasu Yamaguchi	2016	2016 Fourth International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2016.0076	real-time computing;computer science;operating system;database	HPC	-14.729326691546653	52.98081725338181	163221
ca31372588e32747968a7e992beae0739b0e8f6d	dtranx: a seda-based distributed and transactional key value store with persistent memory log		Current distributed key value stores achieve scalability by trading off consistency. As persistent memory technologies evolve tremendously, it is not necessary to sacrifice consistency for performance. This paper proposes DTranx, a distributed key value store based on a persistent memory aware log. DTranx integrates a state transition based garbage collection mechanism in the log design to effectively and efficiently reclaim old logs. In addition, DTranx adopts the SEDA architecture to exploit higher concurrency in multicore environments and employs the optimal core binding strategy to minimize context switch overhead. Moreover, we customize a hybrid commit protocol that combines optimistic concurrency control and two-phase commit to reduce critical section of distributed locking and introduce a locking mechanism to avoid deadlocks and livelocks. In our evaluations, DTranx reaches 514.11k transactions per second with 36 servers and 95% read workloads. The persistent memory aware log is 30 times faster than the SSD based system. And, our state transition based garbage collection mechanism is efficient and effective. It does not affect normal transactions and log space usage is steadily low.	attribute–value pair;concurrency (computer science);context switch;critical section;deadlock;garbage collection (computer science);l (complexity);lock (computer science);multi-core processor;optimistic concurrency control;overhead (computing);persistent memory;scalability;solid-state drive;space–time tradeoff;state transition table;transactions per second;two-phase commit protocol	Ning Gao;Zhang Liu;Dirk Grunwald	2017	CoRR		optimistic concurrency control;scalability;distributed lock manager;deadlock;concurrency;computer science;commit;architecture;real-time computing;garbage collection;distributed computing	OS	-13.924867207739588	49.4853822750151	165327
4d79ccf4d5d8b614502af3056547ea1869ee1b1f	reducing overheads of local communications in fine-grain parallel computation	concurrent computing yarn computer architecture delay multithreading parallel processing communication switching context modeling data structures switches;benchmark programs;yarn;concurrent computing;performance evaluation;overheads reduction;optimization technique;fine grain parallel computation;software performance evaluation;software performance evaluation parallel architectures parallel algorithms performance evaluation;computer architecture;parallel architectures;local community;local communication;data structures;local subtasks;multithreaded architecture;parallel computer;benchmark programs overheads reduction local communications fine grain parallel computation optimization technique local subtasks network interface;communication switching;network interface;switches;local communications;context modeling;parallel processing;fine grain computation;multithreading;parallel algorithms	For fine-grain computation to be ejjective, the cost of communications between the large number of subtasks should be minimized. In this papec we present an optimization technique which reduces overheads of communications between local subtasks by bypassing the network inte$ace and transferring data directly from memory or registers to memory. On average, the optimization results in 35.6% improvement in total execution time on instruction-level simulations with six benchmark programs from I to 32 nodes.	benchmark (computing);computation;mathematical optimization;parallel computing;run time (program lifecycle phase);simulation	Jin-Soo Kim;Soonhoi Ha;Chu Shik Jhon	1997		10.1109/ICPP.1997.622648	parallel processing;computer architecture;parallel computing;multithreading;concurrent computing;network switch;computer science;network interface;operating system;distributed computing;parallel algorithm;context model	HPC	-12.490935257698627	46.67377821134584	165620
33431c50e91213b41319dd921a68e0e17aaecf35	implementing signatures for transactional memory	multi threading;conflict detection;k single hash function bloom filter;bloom filter;rodler cuckoo hashing;digital signatures;transactional memory systems;parallel bloom signature;tm systems;multiported sram;cryptography;concurrency control;hash function;pagh cuckoo hashing;concurrent transactions;cuckoo bloom signatures;false positive;transaction processing;transactional memory;formal analysis;multithreaded programming transactional memory systems tm systems conflict detection concurrent transactions tm signatures k single hash function bloom filter multiported sram parallel bloom signature pagh cuckoo hashing rodler cuckoo hashing cuckoo bloom signatures;transaction processing concurrency control cryptography digital signatures multi threading;filters hardware read write memory performance analysis hydrogen costs system testing degradation space exploration dynamic programming;tm signatures;multithreaded programming;reading and writing	Transactional Memory (TM) systems must track the read and write sets - items read and written during a transaction - to detect conflicts among concurrent transactions. Several TMs use signatures, which summarize unbounded read/write sets in bounded hardware at a performance cost of false positives (conflicts detected when none exists). This paper examines different organizations to achieve hardware-efficient and accurate TM signatures. First, we find that implementing each signature with a single k-hash- function Bloom filter (True Bloom signature) is inefficient, as it requires multi-ported SRAMs. Instead, we advocate using k single-hash-function Bloom filters in parallel (Parallel Bloom signature), using area-efficient single-ported SRAMs. Our formal analysis shows that both organizations perform equally well in theory and our simulation- based evaluation shows this to hold approximately in practice. We also show that by choosing high-quality hash functions we can achieve signature designs noticeably more accurate than the previously proposed implementations. Finally, we adapt Pagh and Rodler's cuckoo hashing to implement Cuckoo-Bloom signatures. While this representation does not support set intersection, it mitigates false positives for the common case of small read/write sets and performs like a Bloom filter for large sets.	antivirus software;bloom filter;cuckoo hashing;hash function;static random-access memory;transactional memory;type signature	Daniel Sánchez;Luke Yen;Mark D. Hill;Karthikeyan Sankaralingam	2007	40th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2007)	10.1109/MICRO.2007.24	transactional memory;parallel computing;real-time computing;hash function;transaction processing;computer science;cryptography;bloom filter;operating system;concurrency control;distributed computing;programming language	Arch	-14.834756866662593	49.11313447371189	165789
bab83564fb6f4517b260192e6d7f9a5bb11ad9b4	middleware support for rdma-based data transfer in cloud computing	protocols;protocol offloading;semantics;infiniband environment middleware support rdma based data transfer high speed data transfer data intensive application cloud computing system middleware layer high speed communication remote direct memory access http file copy sync file i o remote file i o end to end bandwidth high speed architecture resource abstraction task synchronization scheduling file transfer protocol rdma based middleware tcp based ftp tool gridftp cpu consumption line speed performance lan man ethernet;data center networks;transport protocols;computer architecture;remote direct memory access;servers;message systems;middleware protocols servers semantics message systems computer architecture;protocol offloading data center networks middleware remote direct memory access;access protocols;middleware;transport protocols access protocols cloud computing middleware;cloud computing	Providing high-speed data transfer is vital to various data-intensive applications in cloud computing systems. We design a middleware layer of high-speed communication based on Remote Direct Memory Access (RDMA) that serves as the common substrate to accelerate various data transfer applications in cloud computing, such as FTP, HTTP, file copy, sync and remote file I/O. This middleware offers higher end-to-end bandwidth than the traditional TCP-based alternatives, while it hides the heterogeneity of the underlying high-speed architecture. In this paper, we describe the design of this middleware, including resource abstraction, and task synchronization and scheduling. We provide a reference implementation of the file-transfer protocol over this RDMA based middleware. Our experimental results show that it outperforms several TCP-based FTP tools, such as GridFTP, while maintaining very low CPU consumption on a variety of platforms. Furthermore, these results confirm that our middleware achieves near line-speed performance in both LAN and MAN, and scales consistently from 10Gbps Ethernet to 40Gbps Ethernet and InfiniBand environments.	asynchronous i/o;central processing unit;cloud computing;data-intensive computing;end-to-end principle;experiment;gridftp;hypertext transfer protocol;infiniband;marginal model;middleware;reference implementation;remote direct memory access;scheduling (computing);test data	Yufei Ren;Tan Li;Dantong Yu;Shudong Jin;Thomas G. Robertazzi	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.133	communications protocol;middleware;parallel computing;rdma over converged ethernet;remote direct memory access;cloud computing;computer science;message oriented middleware;operating system;middleware;distributed computing;semantics;transport layer;server;computer network	HPC	-16.74241570992157	52.28857073083855	166088
ae97f978acaecdd1fe16982cca7d1bf1966b4605	an approach towards benchmarking of fault-tolerant commercial systems	benchmarking;reliability;performance evaluation;fault tolerant;system level fault tolerance;tmr based prototype machines;fault tolerant commercial systems;dependable systems;fault tolerant computing;computer testing;catastrophic incidents;system level fault tolerance benchmarking fault tolerant commercial systems dependable systems catastrophic incidents performance degradation synthetic workloads tmr based prototype machines duplex architecture;computer testing fault tolerant computing reliability performance evaluation;synthetic workloads;fault tolerant systems prototypes degradation fault tolerance high performance computing central processing unit aerospace safety time factors computer applications telecommunication computing;performance degradation;fault injection;duplex architecture	This paper presents a benchmark for dependable systems. The benchmark consists of two metrics, number of catastrophic incidents and performance degradation, which are obtained by a tool that (1) generates synthetic workloads that produce a high level of CPU, memory, and I/O activity and (2) injects CPU, memory, and I/O faults according to an	benchmark (computing);catastrophic interference;central processing unit;definition;dependability;dhrystone;duplex (telecommunications);elegant degradation;experiment;fairness measure;fault injection;fault model;fault tolerance;fault-tolerant computer system;high- and low-level;high-level programming language;input/output;prototype;software portability;system under test;triple modular redundancy;window of opportunity	Timothy K. Tsai;Ravishankar K. Iyer;Doug Jewitt	1996		10.1109/FTCS.1996.534616	reliability engineering;embedded system;real-time computing;engineering	HPC	-17.601860470286894	49.33557060015238	166570
a43f2375fc8ac9fadbab91d5c10e61ef88a0525d	high performance database logging using storage class memory	phase change materials random access memory databases computer crashes hardware writing data structures;databases;phase change materials;dbms logging;database system;disc drives;random access memory;computer crashes;database management systems;scm based approach;logging transaction actions;data format;partial write detection;storage class memory;input output;system design;data structures;hard disk drives;ibm soliddb;writing;phase change material;hole detection;telecommunication application transaction processing benchmark high performance database logging storage class memory hard disk drives main memory dbms logging logging transaction actions log records scm based approach hole detection partial write detection any point failure recovery ibm soliddb;log records;transaction processing;failure recovery;hard disk drive;high performance;data structure;high speed;telecommunication application transaction processing benchmark;hard discs;main memory;any point failure recovery;transaction processing database management systems disc drives hard discs;hardware;high performance database logging	Storage class memory (SCM), a new generation of memory technology, offers non-volatility, high-speed, and byte-addressability, which combines the best properties of current hard disk drives (HDD) and main memory. With these extraordinary features, current systems and software stacks need to be redesigned to get significantly improved performance by eliminating disk input/output (I/O) barriers; and simpler system designs by avoiding complicated data format transformations. In current DBMSs, logging and recovery are the most important components to enforce the atomicity and durability of a database. Traditionally, database systems rely on disks for logging transaction actions and log records are forced to disks when a transaction commits. Because of the slow disk I/O speed, logging becomes one of the major bottlenecks for a DBMS. Exploiting SCM as a persistent memory for transaction logging can significantly reduce logging overhead. In this paper, we present the detailed design of an SCM-based approach for DBMSs logging, which achieves high performance by simplified system design and better concurrency support. We also discuss solutions to tackle several major issues arising during system recovery, including hole detection, partial write detection, and any-point failure recovery. This new logging approach is used to replace the traditional disk based logging approach in DBMSs. To analyze the performance characteristics of our SCM-based logging approach, we implement the prototype on IBM SolidDB. In common circumstances, our experimental results show that the new SCM-based logging approach provides as much as 7 times throughput improvement over disk-based logging in the Telecommunication Application Transaction Processing (TATP) benchmark.	access time;algorithm;atomicity (database systems);benchmark (computing);byte;c syntax;computer data storage;concurrency (computer science);data logger;database;durability (database systems);dynamic random-access memory;hard disk drive;in-memory database;input/output;l (complexity);login;next-generation network;non-volatile memory;overhead (computing);persistent memory;prototype;software design;soliddb;systems design;tatp benchmark;throughput;transaction log;transaction processing;volatility	Ru Fang;Hui-I Hsiao;Bin He;C. Mohan;Yun Wang	2011	2011 IEEE 27th International Conference on Data Engineering	10.1109/ICDE.2011.5767918	input/output;real-time computing;data structure;transaction processing;computer hardware;computer science;database;writing;systems design	DB	-13.884475094515235	52.075527726013746	166866
9f53c1dbc257c536ee832e12238f6ab6451436fe	state-aware concurrency throttling		Reconfiguration of parallel applications has gained traction with the increasing emphasis on energy/performance trade-off. The ability to dynamically change the amount of resources used by an application allows reaction to changes in the environment, in the application behavior or in the user’s requirements. A popular technique consists in changing the number of threads used by the application (Dynamic Concurrency Throttling). Although this provides good control of application performance and power consumption, managing the technique can impose a significant burden on the application programmer, mainly due to state management and redistribution following the addition or removal of a thread. Nevertheless, some common state access patterns have been identified in some popular applications. By leveraging on this knowledge, we will describe how it is possible to simplify the state management procedures following a Concurrency Throttling operation.	programmer;requirement;state management;thread (computing);traction teampage	Daniele De Sensi;Peter Kilpatrick;Massimo Torquati	2017		10.3233/978-1-61499-843-3-201	bandwidth throttling;parallel computing;computer science;concurrency	SE	-12.484269520877021	50.63407959769649	167626
362620ae8b658dc266e95a06badf75c8ef02245a	performance analysis, design considerations, and applications of extreme-scale in situ infrastructures		A key trend facing extreme-scale computational science is the widening gap between computational and I/O rates, and the challenge that follows is how to best gain insight from simulation data when it is increasingly impractical to save it to persistent storage for subsequent visual exploration and analysis. One approach to this challenge is centered around the idea of in situ processing, where visualization and analysis processing is performed while data is still resident in memory. This paper examines several key design and performance issues related to the idea of in situ processing at extreme scale on modern platforms: scalability, overhead, performance measurement and analysis, comparison and contrast with a traditional post hoc approach, and interfacing with simulation codes. We illustrate these principles in practice with studies, conducted on large-scale HPC platforms, that include a miniapplication and multiple science application codes, one of which demonstrates in situ methods in use at greater than 1M-way concurrency.		Utkarsh Ayachit;Andrew C. Bauer;Earl P. N. Duque;Greg Eisenhauer;Nicola J. Ferrier;Junmin Gu;Kenneth E. Jansen;Burlen Loring;Zarija Lukic;Suresh Menon;Dmitriy Morozov;Patrick O'Leary;Reetesh Ranjan;Michel E. Rasquin;Christopher P. Stone;Venkatram Vishwanath;Gunther H. Weber;Brad Whitlock	2016	SC16: International Conference for High Performance Computing, Networking, Storage and Analysis		data modeling;parallel computing;simulation;concurrent computing;computer science;theoretical computer science;operating system;distributed computing;computer performance;programming language;computational model;data visualization;network topology;computer network	HPC	-15.941587633221195	49.583123213218606	167629
b868cece391af2247aa9130bc4f587efe2622e1f	the k computer and its failures	reliability;failure;the k computer	We report failure analysis for three and a half years on the K computer which is well known as one of the largest and fastest supercomputer in the world. The failures of the major components, such as CPU, DIMM, compute node, etc. as well as the system wide failures are analyzed and compared with the that of other systems. The results show that the K computer has higher reliability and stability than the other systems and file system failures give serious impacts on the operation of the K computer.	central processing unit;dimm;failure analysis;fastest;k computer;supercomputer;top500	Fumiyoshi Shoji	2016		10.1145/2909428.2909436	reliability engineering;embedded system;real-time computing;computer science	HPC	-17.422718772140893	50.228349625107285	167635
0c597849f9f1283a869e113cbb627ef8c8ede8bf	the ibm z systems coupling facility exploitation of storage-class memory	mainframes memory management aerospace electronics flash memories couplings resource management	This paper describes the new support in the IBM zEnterpriseA EC12 and BC12 mainframes, as well as the IBM z13i, which allows Coupling Facility partitions to use storage-class memory. This use of storage-class memory improves the scalability and availability of the implementation of the IBM WebSphereA MQ shared message queues in a Parallel SysplexA configuration. By providing a new level in the storage hierarchy in the Coupling Facility to which MQ messages may be offloaded and from which they may be later retrieved, storage-class memory augments capacity and scale within the MQ shared message queue environment. This in turn provides availability benefits for the Parallel Sysplex by allowing shared queue message content to remain available and grow as needed to accommodate interruptions in service on the part of one or more of the MQ queue managers.	flash memory;ibm parallel sysplex;ibm system z;ibm websphere mq;mainframe computer;memory hierarchy;message queue;scalability	David H. Surman	2015	IBM Journal of Research and Development	10.1147/JRD.2015.2430052	parallel computing;computer hardware;computer science;engineering;electrical engineering	HPC	-14.409763617083737	51.69604287525271	168922
f93c3b42513754f0302a9de01151aad952e4932f	proposal of container-based hpc structures and performance analysis				Chan Ho Yong;Ga-Won Lee;Eui-nam Huh	2018	JIPS			HPC	-18.105763781118767	47.05216126795748	169983
6fbd9834cb888b84db1f25756cb6173b3622e4a0	a practically constant-time mpi broadcast algorithm for large-scale infiniband clusters with multicast	data transmission;multicast communication;multicast algorithms;broadcasting clustering algorithms multicast algorithms large scale systems hardware broadcast technology computer science open systems laboratories chemical technology;hardware multicast operation;parallel scientific application;tuned openmpl collective;tuned openmpl collective mpi broadcast algorithm large scale infiniband cluster parallel scientific application hardware multicast operation optimal algorithm data transmission;large scale;large scale infiniband cluster;efficient implementation;application program interfaces;semantic gap;cluster system;high performance computer;message passing;broadcast technology;clustering algorithms;workstation clusters application program interfaces message passing multicast communication parallel processing;computer science;broadcasting;workstation clusters;mpi broadcast algorithm;open systems;optimal algorithm;parallel processing;large scale systems;chemical technology;hardware	"""An efficient implementation of the MPI_BCAST operation is crucial for many parallel scientific applications. The hardware multicast operation seems to be applicable to switch-based infiniband cluster systems. Several approaches have been implemented so far, however there has been no production-ready code available yet. This makes optimal algorithms to a subject of active research. Some problems still need to be solved in order to bridge the semantic gap between the unreliable multicast and MPI_BCAST. The biggest of those problems is to ensure the reliable data transmission in a scalable way. Acknowledgement-based methods that scale logarithmically with the number of participating MPI processes exist, but they do not meet the supernormal demand of high-performance computing. We propose a new algorithm that performs the MPI_BCAST operation in a practically constant time, independent of the communicator size. This method is well suited for large communicators and (especially) small messages due to its good scaling and its ability to prevent parallel process skew. We implemented our algorithm as a collective component for the Open MPI framework using native infiniband multicast and we show its scalability on a cluster with 116 compute nodes, where it saves up to 41% MPI_BCAST latency in comparison to the """"TUNED"""" OpenMPl collective."""	algorithm;clustered file system;image scaling;infiniband;multicast;open mpi;scalability;supercomputer;time complexity	Torsten Hoefler;Christian Siebert;Wolfgang Rehm	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370475	parallel processing;parallel computing;message passing;computer science;theoretical computer science;operating system;distributed computing;cluster analysis;open system;broadcasting;semantic gap;computer network;data transmission	HPC	-17.583953323692473	50.250508928971996	170106
c5041851468cbce080f8b1b2566aabfac532eb70	a validated performance model for distributed file system			clustered file system;dce distributed file system	Anna Hác	1986	SIGMETRICS Performance Evaluation Review			HPC	-19.037645940680402	52.09127983841418	170310
ba92f274df327d29932415d57295be05181773ec	efficient, strongly consistent implementations of shared memory (extended abstract)	extended abstract;consistent implementations;shared memory	 )Marios Mavronicolas?Dan Roth??Aiken Computation Laboratory, Harvard University, Cambridge, MA 02138, USAAbstract. We present linearizable implementations for two distributedorganizations of multiprocessor shared memory. For the full caching organization,where each process keeps a local copy of the whole memory,we present a linearizable implementations of read/write memory objectsthat achieves essentially optimal efficiency and allows quantitative degradationof the less frequently ... 	shared memory	Marios Mavronicolas;Dan Roth	1992		10.1007/3-540-56188-9_23	uniform memory access;distributed shared memory;distributed memory	Theory	-15.66541967427469	47.046745161560246	170484
5f27ce09ff3e3f8377ac7066cd7039b229a7b324	hash join algorithms on smps clusters: effects of netcaches on its scalability and performance	remote access;performance evaluation;relational database;smp cluster	We investigate the effect that caches, particularly caches for remote accesses, have on the performance of hash join algorithms. The join is a computationally intensive operation of relational databases and is used in many important applications. Thus, there are a considerable number of studies on the parallel hash join. However, most of the previous research does not show how cache affects the performance of these algorithms. In this paper, we show the impact and benefits of remote caches (Netcaches) on the overall performance of parallel hash join algorithms running on SMP clusters. Furthermore, we show the effects of these caches on speedup and scalability of these algorithms. Our simulation results leads us to conclude that the execution time of hash join algorithms on modern’s multiprocessors, with large local and remote caches, could be reduced up to 70%. Finally, we show results for verifying the big effects of netcaches on scalability of these algorithms.	algorithm;cpu cache;central processing unit;hash join;join (sql);relational database;run time (program lifecycle phase);scalability;simulation;speedup;symmetric multiprocessing	Edward D. Moreno	2002	J. Inf. Sci. Eng.		parallel computing;relational database;computer science;operating system;database;distributed computing	DB	-13.041703262706914	50.92443007465113	170771
47e853fa3b8e1d3c0fc981ce737a44ec10f08aad	an llvm instrumentation plug-in for score-p		Reducing application runtime, scaling parallel applications to higher numbers of processes/threads, and porting applications to new hardware architectures are tasks necessary in the software development process. Therefore, developers have to investigate and understand application runtime behavior. Tools such as monitoring infrastructures that capture performance relevant data during application execution assist in this task. The measured data forms the basis for identifying bottlenecks and optimizing the code.  Monitoring infrastructures need mechanisms to record application activities in order to conduct measurements. Automatic instrumentation of the source code is the preferred method in most application scenarios. We introduce a plug-in for the LLVM infrastructure that enables automatic source code instrumentation at compile-time. In contrast to available instrumentation mechanisms in LLVM/Clang, our plug-in can selectively include/exclude individual application functions. This enables developers to fine-tune the measurement to the required level of detail while avoiding large runtime overheads due to excessive instrumentation.	bottleneck (software);clang;compile time;compiler;image scaling;llvm;level of detail;plug-in (computing);software development process	Ronny Tschüter;Johannes Ziegenbalg;Bert Wesarg;Matthias Weber;Christian Herold;Sebastian Döbel;Ronny Brendel	2017		10.1145/3148173.3148187	porting;computer science;software development process;source code;real-time computing;plug-in;performance measurement;thread (computing);instrumentation;instrumentation (computer programming)	SE	-17.09524329911593	47.68501843437202	172211
7d9da3f5be055f36ac8294d44b688c356072c6cd	resilience analysis of top k selection algorithms		As the number of components in high-performance computing (HPC) systems continues to grow, the number of vehicles for soft errors will rise in parallel. Petascale research has shown that soft errors on supercomputers can occur as frequently as multiple times per day, and this rate will only increase with the exascale era. Due to this frequency, the resilience community has taken an interest in algorithmic resilience as a means for reliable computing in faulty environments. Probabilistic algorithms in particular have generated interest, due to their imprecise nature and ability to handle incorrect guesses. In this paper, we analyze the intrinsic resilience of a probabilistic Top K selection algorithm to silent data corruption in the event of a single event upset. We introduce a new paradigm of analytically quantifying an algorithm's resilience as a function of its inputs, which permits a precise comparison of the resilience of competing algorithms. In addition, we discuss the implications of our findings on the resilience of probabilistic algorithms as a whole in comparison to their deterministic counterparts.	deterministic algorithm;petascale computing;programming paradigm;randomized algorithm;secure digital container;selection algorithm;single event upset;smart data compression;supercomputer;terminate (software)	Ryan Slechta;Laura Monroe;Nathan DeBardeleben;Qiang Guan;Joanne Wendelberger;Sarah Ellen Michalak	2017	2017 13th European Dependable Computing Conference (EDCC)	10.1109/EDCC.2017.23	probabilistic analysis of algorithms;selection algorithm;psychological resilience;approximation algorithm;algorithm;algorithm design;computer science;data corruption;petascale computing;probabilistic logic	HPC	-18.12706708275627	50.113779763739466	172293
65d6deef56a6e38a207f5e9668266885521ab39c	nb-gclock: a non-blocking buffer management based on the generalized clock	database management systems;buffer management;buffer storage;hash table;microprocessor chips buffer storage database management systems;clocks scalability databases concurrent computing technology management engineering management synchronization knowledge management biology information science;database scalability nb gclock non blocking buffer management generalized clock gclock buffer management module;microprocessor chips	In this paper, we propose a non-blocking buffer management scheme based on a lock-free variant of the GCLOCK page replacement algorithm. Concurrent access to the buffer management module is a major factor that prevents database scalability to processors. Therefore, we propose a non-blocking scheme for bufferfix operations that fix buffer frames for requested pages without locks by combining Nb-GCLOCK and a non-blocking hash table. Our experimental results revealed that our scheme can obtain nearly linear scalability to processors up to 64 processors, although the existing locking-based schemes do not scale beyond 16 processors.	adaptive multi-rate audio codec;blocking (computing);central processing unit;hash table;lock (computer science);non-blocking algorithm;page replacement algorithm;scalability	Makoto Yui;Jun Miyazaki;Shunsuke Uemura;Hayato Yamana	2010	2010 IEEE 26th International Conference on Data Engineering (ICDE 2010)	10.1109/ICDE.2010.5447872	hash table;real-time computing;computer science;database;distributed computing;write buffer;programming language	DB	-12.639680194977457	50.40206962407092	172413
39f89cbb2c2951ec21ea53eacfa98186b78085b2	timestamp consistency and trace-driven analysis for distributed parallel systems	integrated approach;trace driven analysis tools;ibm spn systems;performance evaluation;parallel program execution progress;clocks;clocks synchronization registers switches message passing performance analysis kernel monitoring timing analytical models;separate streams;multiple processors;continuous event data stream;system monitoring;performance analysis techniques;trace driven analysis;local clock discrepancy;distributed parallel systems;ibm computers system monitoring parallel processing parallel machines message passing clocks timing performance evaluation;parallel systems;logical event order;performance analysis;ibm computers;message passing;nas kernel benchmarks timestamp consistency trace driven analysis distributed parallel systems continuous event data stream parallel program execution progress separate streams multiple processors logical event order local clock discrepancy integrated approach performance analysis techniques ibm spn systems message passing system events minimal trace overhead trace driven analysis tools;parallel machines;source code;system events;minimal trace overhead;nas kernel benchmarks;parallel programs;timestamp consistency;information analysis;parallel processing;timing	A continuous stream of event data describing the progress of parallel program execution is realized for trace-driven analysis. Unfortunately, it is often the case that separate streams are produced independently by multiple processors in the system, and the logical order of events cannot be guaranteed due to discrepancy among local clocks. We present an integrated approach for timestamp consistency and performance analysis techniques for IBM SPn systems. The trace facility requires no source code modification, and can generate message passing and system events with minimal trace overhead. Trace-driven analysis tools are developed to extract useful information. Analysis results for NAS kernel benchmarks are reported. >		Ching-Farn Eric Wu;Yew-Huey Liu;Yarsun Hsu	1995		10.1109/IPPS.1995.395875	parallel computing;real-time computing;computer science;distributed computing	HPC	-16.572768220371273	48.45432197754873	173102
013d32c193c884fd92bf790564ad1c255dc6e534	implementation and evaluation of prefetching in the intel paragon parallel file system	databases;degradation;prefetching file systems prototypes operating systems concurrent computing production systems measurement degradation;concurrent computing;performance evaluation;measurement;multiprocessor;software prototyping;prototypes;input output programs;performance;prefetching;software performance evaluation;performance improvement;supercomputers prefetching intel paragon parallel file system input output system compute processors bottleneck performance disk access multiprocessor prototype operating system intel paragon operating system performance degradation;operating system;design and implementation;parallel systems;file system;input output system;disk access;parallel file system;production systems;parallel machines;input output programs software performance evaluation parallel machines data handling file organisation operating systems computers;compute processors;data handling;performance degradation;bottleneck;prototype;intel paragon;operating systems computers;supercomputers;central processing unit;file systems;large scale systems;operating systems;computational intelligence society;intel paragon operating system;file organisation	The significant difference between the speeds of the I/O system (e.g. disks) and compute processors in parallel systems creates a bottleneck that lowers the performance of an application that does a considerable amount of disk accesses. A major portion of the compute processors’ time is wasted on waiting for I/O to complete. This problem can be addressed to a certain extent, if the necessary data can be fetched from the disk before the I/O call to the disk is issued. Fetching data ahead of time, known as prefetching in a multiprocessor environment depends a great deal on the application’s access pattern. The subject of this paper is implementation and performance evaluation of a prefetching prototype in a production parallel file system on the Intel Paragon. Specifically, this paper presents a) design and implementation of a prefetching strategy in the parallel file system and b) performance measurements and evaluation of the file system with and without prefetching. The prototype is designed at the operating system level for the PFS. It is implemented in the PFS subsystem of the Intel Paragon Operating System. It is observed that in many cases prefetching provides considerable performance improvements. In some other cases no improvements or some performance degradation is observed due to the overheads incurred in prefetching.	block cipher mode of operation;cpu cache;central processing unit;clustered file system;computation;elegant degradation;emoticon;file sharing;forward secrecy;input/output;intel paragon;link prefetching;multiprocessing;normal mode;operating system;performance evaluation;prototype;scalability;scanline rendering	Meenakshi Arunachalam;Alok N. Choudhary;Brad Rullman	1996		10.1109/IPPS.1996.508111	computer architecture;parallel computing;concurrent computing;computer science;operating system;prototype	OS	-13.77364531177406	51.387362761228935	173255
1ffe67801491e2d8ba6a1b8e57a82b18d84f763e	toward resilient algorithms and applications	scalable computing;resilience;fault tolerance	Large-scale computing platforms have always dealt with unreliability coming from many sources. In contrast applications for large-scale systems have generally assumed a fairly simplistic failure model: The computer is a reliable digital machine, with consistent execution time and infrequent failures that can be handled by occasionally storing a checkpoint of application state and restarting from that saved state if the system fails. Many computing experts, and several key technology trends indicate that the current simplistic application view of a high-end system is no longer feasible. Instead, algorithms and application developers must adopt more complex models for system reliability and adapt algorithms and implementation to be more resilient in the presence of failures and increased failure detection and correction.  In this talk we present motivation for moving away from a checkpoint-restart-only model and discuss several new models for resilience, including latency tolerance, local recovery from local failure and selective reliability. We also discuss strategies for designing new algorithms and applications, and some of the required system and programming environment features.	algorithm;application checkpointing;end system;integrated development environment;run time (program lifecycle phase);state (computer science);transaction processing system	Michael A. Heroux	2013		10.1145/2465813.2465814	fault tolerance;real-time computing;simulation;operating system;distributed computing;algorithm;psychological resilience	Arch	-19.083675254584513	49.00564504429948	173429
91ad588507d3ec349906a063324eafb1ae4fdd6b	a new design of rdma-based small message channels for infiniband clusters	storage management;workstation clusters message passing storage management;message passing;remote direct memory access mpi program smp node memory management buffer association mvapich design infiniband clusters small message channels rdma;workstation clusters;memory management receivers libraries scalability benchmark testing organizations data structures	We propose a novel design for RDMA-based small message channels that significantly improves the MVAPICH design. First, we develop a technique that eliminates persistent buffer association, a scheme used in MVAPICH that not only results in significant memory requirement, but also imposes restrictions in memory management. Building upon this technique, we propose a novel shared RDMA-based small message channel design that allows MPI processes on the same SMP node to share small message channels, which greatly reduces the number of small message channels needed for an MPI program on clusters with SMP nodes. Our techniques considerably improve the scalability and reduce memory requirement in comparison to MVAPICH, allowing RDMA-based small message channels to be used by a much larger number of MPI processes. The experimental results demonstrate that our techniques achieve the improvements without adding noticeable overheads or sacrificing the performance benefits of RDMA in practice.	infiniband;mvapich;memory management;remote direct memory access;scalability;symmetric multiprocessing	Matthew Small;Xin Yuan	2013	2013 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2013.6702671	parallel computing;message passing;computer science;operating system;distributed computing;programming language	HPC	-13.412066403672236	49.658604400142046	174160
a92bf27d6018beda7e6fafa2b297c31874c6f89e	performance of the vesta parallel file system	parallel processing file organisation performance evaluation;concurrent computing;performance evaluation;system software;application software;parallel access;prototypes;performance;prefetching;bridges;single node performance;vesta parallel file system;computer architecture;data analysis;prefetching vesta parallel file system parallel file system performance single node performance parallel access bandwidth scales orthogonal partitioning;parallel file system;file systems application software system software prefetching concurrent computing computer architecture computational intelligence society bridges prototypes data analysis;orthogonal partitioning;bandwidth scales;parallel processing;file systems;computational intelligence society;experience design;file organisation	Vesta is an experimental parallel file system implemented on the IBM SPI. Its main features are support for parallel access from multiple application processes to file, and the ability to partition and re-partition the file data among these processes. This paper reports on a set of experiments designed to evaluate Vesta's performance. This includes basic single-node performance, and performance using parallel access with different file partitioning schemes. Results are that bandwidth scales with the number of I/O nodes accessed, and that orthogonal partitioning schemes achieve essentially the same performance. In many cases performance equals the disk hardware limit. This is often attributed to prefetching and write-behind in the I/O nodes. >	clustered file system;vesta (software configuration management)	Dror G. Feitelson;Peter F. Corbett;Jean-Pierre Prost	1995		10.1109/IPPS.1995.395926	self-certifying file system;parallel computing;real-time computing;device file;computer science;operating system;file system fragmentation	HPC	-14.82184609368683	50.23993573663309	174328
fae44b7600a187783be6b9e81ca09005b706a182	pcftl: a plane-centric flash translation layer utilizing copy-back operations	merge operations;random access memory;memory management;performance evaluation;solid state disk;garbage collection flash translation layer copy back merge operations solid state disk;garbage collection;parallel processing servers random access memory memory management performance evaluation time factors solid state circuits;solid state circuits;storage management disc drives flash memories hard discs;servers;time factors;flash translation layer;copy back;parallel processing;write distribution plane centric flash translation layer pcftl copy back operation software module flash ssd solid state disk flash memory block storage device plane level parallelism	A software module named flash translation layer (FTL) running in the controller of a flash SSD exposes the linear flash memory to the system as a block storage device. The effectiveness of an FTL significantly impacts the performance and durability of a flash SSD. In this research, we propose a new FTL called PCFTL (Plane-Centric FTL), which fully exploits plane-level parallelism supported by modern flash SSDs. Its basic idea is to allocate updates onto the same plane where their associated original data resides on so that the write distribution among planes is balanced. Furthermore, it utilizes fast intra-plane copy-back operations to transfer valid pages of a victim block when a garbage collection occurs. We largely extend a validated simulation environment called SSDsim to implement PCFTL. Comprehensive experiments using realistic enterprise-scale workloads are performed to evaluate its performance with respect to mean response time and durability in terms of standard deviation of writes per plane. Experimental results demonstrate that compared with the well-known DFTL, PCFTL improves performance and durability by up to 47 and 80 percent, respectively. Compared with its earlier version (called DLOOP), PCFTL enhances durability by up to 74 percent while delivering a similar I/O performance.	algorithm;block (data storage);cpu cache;durability (database systems);dynamic dispatch;experiment;ftl: faster than light;field-programmable gate array;flash file system;flash memory controller;garbage collection (computer science);input/output;locality of reference;parallel computing;response time (technology);round-robin dns;simulation;solid-state drive;whole earth 'lectronic link	Wei Wang;Tao Xie	2015	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2014.2371022	flash file system;parallel processing;parallel computing;real-time computing;computer hardware;computer science;operating system;database;distributed computing;garbage collection;server;memory management	OS	-12.84201510161493	52.896854106459436	174392
261f93c671207b2da5a29c9bbed271654197b43c	non-intrusive coscheduling for general purpose operating systems	similar performance;general purpose operating system;dynamic energy budget distribution;specialized commercial solution;multicore era;coscheduling design;non-intrusive coscheduling;certain problem;multicore system;linux kernel;viable solution;non-intrusive solution	Coscheduling, invented originally on early parallel computer systems 30 years ago, provided the possibility to improve the resource utilization of these systems substantially by coordinating the execution of processes across multiple processors in time. Almost forgotten in the multicore era, recent research addressing certain problems on multicore systems, such as performance of virtual machines, contention of processor resources, or dynamic energy budget distribution, concludes that coscheduling is a viable solution.#R##N##R##N#In this paper, we do not focus on a specific problem or application of coscheduling, but on coscheduling itself. We present a coscheduling design that is able to cover most of the identified use cases on multicore systems and can be seamlessly integrated into currently used general purpose operating systems. We have applied this design to the Linux kernel and show that this approach allows a non-intrusive solution that fulfills the promises of coscheduling and is able to achieve a similar performance as a specialized commercial solution.	coscheduling	Jan Hendrik Schönherr;Bianca Lutz;Jan Richling	2012		10.1007/978-3-642-31202-1_7	parallel computing;real-time computing;computer science;distributed computing	Theory	-15.884329178298906	51.18155380280029	174752
011b9e6f11b4bb48d9e5ec9d2e4b2c595cfec15a	localtiy and false sharing in coherent-cache parallel graph reduction	shared memory;cache coherence protocol;simulation study;parallel programs	Parallel graph reduction is a model for parallel program execution in which shared-memory is used under a strict access regime with single assignment and blocking reads. We outline the design of an ee-cient and accurate multiprocessor simulation scheme and the results of a simulation study of the performance of a suite of benchmark programs operating under a cache coherency protocol that is representative of protocols used in commercial shared-memory machines and in more scalable distributed shared-memory systems. We analyse the innuence of cache line size on performance and expose the relative contributions of spatial, temporal and processor locality and false sharing to overall performance. 1 Background Parallel graph reduction (PGR) is a well-known technique which uses a shared graph structure to manage and synchronise the parallel execution of a functional program. It provides a simple model of communication and synchronisation between processors with distinctive properties of importance in the design and op-timisation of multicache implementations of shared-memory. The performance of parallel programs under this regime depends critically on the provision of access to the shared heap with very high average performance. The most successful implementations of PGR (e.g. Goldberg's Buckwheat system 8] and Augustsson and Johnsson's h; Gi-machine 3]) have used general-purpose shared-memory multiprocessors based on well-known bus-based snooping cache coherency protocols. For modest numbers of processors these systems implement a shared heap with near-ideal performance, but their size is limited by contention for the snooping bus. Shared-bus multiprocessors now have relatively high-latency communication (i.e. the latency of a memory reference that requires use of the bus is now many times greater than one that does not), and the number of processors that can be supported by the bus is very small. A solution is to use a more complex network, but such networks favour the transmission of larger messages, and require directories to store copy sets because broadcast is no longer available. There are three important issues to be addressed: 1. How does cache line size aaect the performance of parallel graph reduction operating with invalidation based protocols?	assignment (computer science);benchmark (computing);best, worst and average case;blocking (computing);cpu cache;cache coherence;central processing unit;coherent;complex network;data dredging;directory (computing);distributed shared memory;false sharing;functional programming;general-purpose markup language;graph reduction;lennart augustsson;locality of reference;multiprocessing;scalability;simulation	Andrew J. Bennett;Paul H. J. Kelly	1993		10.1007/3-540-56891-3_26	bus sniffing;uniform memory access;distributed shared memory;shared memory;cache coherence;parallel computing;computer science;write-once;theoretical computer science;cache invalidation;distributed computing;mesi protocol;mesif protocol	Arch	-12.215281151621785	49.21048807649507	175263
bd80556653a915f53b932ad13189b9fa10453436	the case for ramcloud	data intensive application	With scalable high-performance storage entirely in DRAM, RAMCloud will enable a new breed of data-intensive applications.	data-intensive computing;dynamic random-access memory;scalability	John K. Ousterhout;Parag Agrawal;David Erickson;Christoforos E. Kozyrakis;Jacob Leverich;David Mazières;Subhasish Mitra;Aravind Narayanan;Diego Ongaro;Guru M. Parulkar;Mendel Rosenblum;Stephen M. Rumble;Eric Stratmann;Ryan Stutsman	2011	Commun. ACM	10.1145/1965724.1965751	parallel computing;real-time computing;computer science;operating system	OS	-12.095395635568405	51.00009937560621	175295
4ad7e1a7a07a811c118023744ff21b2bedcb37d5	the apparatus and enabling of a code balanced system	flash memory;download architecture code balanced system execute in place architecture nor flash memory memory architecture store architecture;embedded systems;nor flash memory;execute in place architecture;operating system;memory architecture;random access memory application software read write memory bills of materials cellular phones delay flash memory costs memory architecture hardware;random access storage;code balanced system;cost effectiveness;store architecture;random access storage embedded systems flash memories memory architecture;download architecture;flash memories	Success in the embedded world revolves around two key concepts: cost effectiveness and performance. The ability for an operating system to boot quickly combined with speedy application usage at runtime is important with regards to consumer unit adoption. The most common memory sub-system setup in cellular phone architectures today is what is called an eXecute-In-Place architecture. This type of memory sub-system defines the execution of code and data directly from NOR flash memory. An additional memory architecture of choice is called a Store and Download architecture. This is a memory subsystem where the compressed code gets copied to RAM at boot time and executes out of the RAM. This paper explores the addition of a new memory usage model called a code balanced system. The result is a system that combines a small RAM memory requirement with a performance increase for improved targeted application and boot time execution	booting;download;embedded system;execute in place;flash memory;launch time;memory management;mobile phone;nand gate;overhead (computing);random-access memory;real-time operating system;run time (program lifecycle phase);software system	Tony Benavides;Justin Treon;Weide Chang	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.191	uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;cost-effectiveness analysis;memory refresh;computer hardware;computer science;operating system;flash memory emulator;computer data storage;computer memory;overlay;non-volatile random-access memory;conventional memory;extended memory;flat memory model;registered memory;cache-only memory architecture;memory map;memory management	EDA	-15.495365343288412	52.045756999935065	175597
9c1a063105bc1a6c831c887f1081b2c8ad0a04f5	the qrqw pram: accounting for contention in parallel algorithms	parallel algorithm;efficient algorithm;directed hypergraphs;edit distance;evolutionary trees;leader election;recombination;bottleneck optimality;computational biology;algorithm design;lower bound;reading and writing	We introduce the queue-read, queue-write (QRQW) PRAM model, which permits concurrent reading and writing, but at a cost proportional to the number of readers/writers to a memory location in a given step. Previously, there were no formal complexity models that accounted for the contention to memory locations, despite the large impact of such contentions on the performance of parallel prdgrams. The QRQW PRAM is strictly more powerful than the EREW PRAM. We show a separation of fi between the two models, and present faster and more efficient QRQW algorithms for many basic problems. Nevertheless, we show that the QRQW can be efficiently emulated with only logarithmic slowdown on Valiant’s BSP model, and hence on hypercube-type non-combining networks, even when latency, synchronization, and memory granularity overheads are taken into account. In contrast, efficient emulations for the CRCW PRAM on such networks are only known with polynomial slowdown. Most existing machines obey the queue-read, queue-write rule. We study the impact of the QRQW rule on algorithm design, devising new techniques for low-contention algorithms. Our results include fast and efficient algorithms for computing the OR, leader election, linear compaction, multiple compaction, integer sorting, and CRCW simulation, as well as several lower bounds. Some of the results presented are quite involved and use several novel ideas and techniques that are interesting on their own.	algorithm design;data compaction;emulator;leader election;memory address;parallel algorithm;parallel random-access machine;polynomial;simulation	Phillip B. Gibbons;Yossi Matias;Vijaya Ramachandran	1994			algorithm design;phylogenetic tree;edit distance;computer science;theoretical computer science;leader election;distributed computing;parallel algorithm;upper and lower bounds;algorithm;recombination	Theory	-15.6447344601754	46.44040772643229	176538
3b90c6cf8700ca1f06f24e44ab1c9ce19a004d7d	research on algorithm of parallel garbage collection based on lisp 2 for multi-core system	garbage collection	As the multi-core architecture prevails, garbage collection is becoming an essential feature to support high performance systems. In this paper, we have described a parallel copying garbage collector and presented experimental results that demonstrate good scalability. The collector is designed for shared-memory multi-core systems or multiprocessor systems in the base of classical LISP2 algorithm. The earlier parallel copying collector can not compact the live data into a single contiguous region at one end of the heap, but leaves multiple object groups, one for every two neighboring partitions. Our novel algorithm overcomes this drawback. The average collector speedup is 1.65 at two-core system and 2.4 at four-core system.	algorithm;garbage collection (computer science);lisp 2;multi-core processor;on lisp	Congpin Zhang;Changmao Wu;Lili Zhao	2010		10.1007/978-3-642-14831-6_61	manual memory management;garbage;parallel computing;computer science;operating system;garbage collection;programming language;mark-compact algorithm	EDA	-13.224429578119786	47.2784867947493	176745
8964497eef0b88462213f152a776d260388cff36	on the scalability, performance isolation and device driver transparency of the ihk/mckernel hybrid lightweight kernel	kernel;performance evaluation;system call offloading;lightweight kernels;software reusability application program interfaces linux parallel processing software performance evaluation;hybrid kernels;linux;scalability;scalability operating systems hybrid kernels lightweight kernels system call offloading;parallel processing;operating systems;hardware;linux device driver reusability scalability performance isolation device driver transparency ihk mckernel hybrid lightweight kernel lwk operating system parallel application posix api system service offloading;linux kernel scalability performance evaluation hardware parallel processing	Extreme degree of parallelism in high-end computing requires low operating system noise so that large scale, bulk-synchronous parallel applications can be run efficiently. Noiseless execution has been historically achieved by deploying lightweight kernels (LWK), which, on the other hand, can provide only a restricted set of the POSIX API in exchange for scalability. However, the increasing prevalence of more complex application constructs, such as in-situ analysis and workflow composition, dictates the need for the rich programming APIs of POSIX/Linux. In order to comply with these seemingly contradictory requirements, hybrid kernels, where Linux and a lightweight kernel (LWK) are run side-by-side on compute nodes, have been recently recognized as a promising approach. Although multiple research projects are now pursuing this direction, the questions of how node resources are shared between the two types of kernels, how exactly the two kernels interact with each other and to what extent they are integrated, remain subjects of ongoing debate. In this paper, we describe IHK/McKernel, a hybrid software stack that seamlessly blends an LWK with Linux by selectively offloading system services from the lightweight kernel to Linux. Specifically, we are focusing on transparent reuse of Linux device drivers and detail the design of our framework that enables the LWK to naturally leverage the Linux driver codebase without sacrificing scalability or the POSIX API. Through rigorous evaluation on a medium size cluster we demonstrate how McKernel provides consistent, isolated performance for simulations even in face of competing, in-situ workloads.	application programming interface;bulk synchronous parallel;degree of parallelism;device driver;hybrid kernel;interference (communication);lightweight kernel operating system;linux;posix;parallel computing;remote direct memory access;requirement;scalability;simulation	Balazs Gerofi;Masamichi Takagi;Atsushi Hori;Gou Nakamura;Tomoki Shirasawa;Yutaka Ishikawa	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.80	embedded system;parallel processing;parallel computing;kernel;real-time computing;scalability;computer science;operating system;futex;distributed computing;epoll;linux kernel;configfs	Arch	-16.161339925563194	51.65388792845862	177269
abbf8134aa59c532e83b7045992a2303a22f1a5b	life span strategy - a compiler-based approach to cache coherence	simple invalidation;fast selective invalidation;multiprocessor systems;parallel task execution;doacross loop;compiler based cache coherence;inter task level temporal locality;large scale;cache coherence;life span strategy;version control;cache management;life span;time stamp approach	In this paper, a cache coherence strategy with a combined software and hardware approach is proposed for large-scale multiprocessor systems. The new strategy has the scalability advantages of existing software strategies and does not rely on shared hardware resources to maintain coherence. It exploits as much intra-task temporal locality as previously proposed low-cost, compiler-based strategies such as Simple Invalidation and Fast Selective Invalidation. With a small amount of additional hardware and a small set of cache management instructions, the new strategy preserves more inter-task-level temporal locality than these strategies. It is an economical alternative and has potential performance close to that of more elaborate strategies such as Version Control and Time Stamp. Also, the new strategy is easily extendable to include Doacross loops.	basic stamp;cache coherence;compiler;computer program;extensibility;locality of reference;multiprocessing;scalability;version control	Hoichi Cheong	1992		10.1145/143369.143402	life expectancy;cache coherence;parallel computing;real-time computing;cache;computer science;revision control;cache invalidation;operating system;distributed computing;mesi protocol;cache algorithms	Arch	-14.379994974326753	48.59731692881294	177980
653aca2181dcbea18fe9c0c121bbb6c61d3b8405	highly scalable self-healing algorithms for high performance scientific computing	parallel and distributed system;computers;preconditioned conjugate gradient equation solver;reed solomon erasure codes;fault tolerant;high performance computing;reed solomon codes application program interfaces checkpointing fault tolerant computing message passing parallel machines;erasure code;reed solomon codes;mean time to failure;ft mpi;message passing interface self healing diskless checkpointing fault tolerance pipeline parallel and distributed systems high performance computing;program processors encoding checkpointing fault tolerance fault tolerant systems computers pipeline processing;checkpointing;diskless checkpointing;fault tolerant computing;message passing interface;fault tolerant systems;preconditioned conjugate gradient equation solver highly scalable self healing algorithm high performance scientific computing mean time to failure ft mpi diskless checkpointing reed solomon erasure codes floating point numbers;floating point numbers;numerical computation;application program interfaces;link failure;fault tolerance;parallel and distributed systems;high performance computer;reed solomon;scientific computing;message passing;parallel machines;floating point;high performance scientific computing;preconditioned conjugate gradient;encoding;high performance;highly scalable self healing algorithm;program processors;self healing;pipeline;pipeline processing	As the number of processors in today's high-performance computers continues to grow, the mean-time-to-failure of these computers is becoming significantly shorter than the execution time of many current high-performance computing applications. Although today's architectures are usually robust enough to survive node failures without suffering complete system failure, most of today's high-performance computing applications cannot survive node failures. Therefore, whenever a node fails, all surviving processes on surviving nodes usually have to be aborted and the whole application has to be restarted. In this paper, we present a framework for building self-healing high-performance numerical computing applications so that they can adapt to node or link failures without aborting themselves. The framework is based on FT-MPI and diskless checkpointing. Our diskless checkpointing uses weighted checksum schemes, a variation of Reed-Solomon erasure codes over floating-point numbers. We introduce several scalable encoding strategies into the existing diskless checkpointing and reduce the overhead to survive k failures in p processes from 2[log p]. k ((beta + 2gamma) m + alpha) to (1 + O (radic(p)/radic(m)))2. k (beta + 2gamma)m, where alpha is the communication latency, 1/beta is the network bandwidth between processes, {1\over \gamma } is the rate to perform calculations, and m is the size of local checkpoint per process. When additional checkpoint processors are used, the overhead can be reduced to (1 + O (1/radic(m))). k (beta + 2gamma)m, which is independent of the total number of computational processors. The introduced self-healing algorithms are scalable in the sense that the overhead to survive k failures in p processes does not increase as the number of processes p increases. We evaluate the performance overhead of our self-healing approach by using a preconditioned conjugate gradient equation solver as an example. Experimental results demonstrate that our self-healing scheme can survive multiple simultaneous process failures with low-performance overhead and little numerical impact.	algorithm;application checkpointing;central processing unit;checksum;computational science;computer;conjugate gradient method;diskless node;erasure code;fault tolerance;mean time between failures;message passing interface;numerical analysis;overhead (computing);reed–solomon error correction;run time (program lifecycle phase);scalability;solver;supercomputer;transaction processing system	Zizhong Chen;Jack J. Dongarra	2009	IEEE Transactions on Computers	10.1109/TC.2009.42	embedded system;fault tolerance;supercomputer;parallel computing;real-time computing;computer science;floating point;operating system;distributed computing	HPC	-17.860920969620803	49.79463742616118	177995
e2bff82b8f5328220c66daa2695a2aa70488bd76	barrier synchronization over multistage interconnection networks	multiprocessor interconnection networks;dissemination barrier;nodes;barrier synchronization;bbn gp 1000 multistage interconnection networks barrier synchronization nonuniform memory architectures dissemination barrier progressive skip ring flags reflected tree barrier software combining trees barrier stages remote network operations algorithm completion probabilistic simulations nodes;multiprocessor interconnection networks computer architecture concurrent computing computer science appropriate technology computational modeling programming profession bandwidth contracts cloning;multistage interconnection networks;multistage interconnection network;software combining trees;parallel algorithms multiprocessor interconnection networks operating systems computers;memory architecture;progressive skip ring;bbn gp 1000;probabilistic simulations;remote network operations;barrier stages;operating systems computers;flags;algorithm completion;reflected tree barrier;nonuniform memory architectures;parallel algorithms	This paper identifies two barrier syiichronizatioii techniques that are appropriate for nonuniform memory architectures. The first of these two, the disseniination barrier, is based on a progressive skip-ring of flags. The second, called the reflected-tree barrier. is based on software combining trees. The dissemination barrier is optimal in terms of the number of barrier stages required by the algorithm. The reflectedtree barrier, however, is optimal in the number of remote network operations required for algorithm completion. These two are compared by means of probabilistic simulations up to 4096 nodes and actual execution results up to 90 nodes on a BBN GP-1000. These results show that the reflected-tree barrier is significantly better on 30 nodes or more.	algorithm;barrier (computer science);memory barrier;multistage interconnection networks;parallax barrier;simulation	C. A. Lee	1990		10.1109/SPDP.1990.143520	parallel computing;real-time computing;computer science;operating system;distributed computing;parallel algorithm;node;computer network	Arch	-16.210034798202567	47.55819211791698	178306
e4610b8c1e1360e25d9a678f07a0da9cf9cce2c2	cyclic: a locality-preserving load-balancing algorithm for pdes on shared memory multiprocessors	shared memory systems;multicore;pdes;vhdl;load balancing;locality of references;parallel algorithms	This paper presents a new load-balancing algorithm for shared memory multiprocessors that is currently being applied to the parallel simulation of logic circuits, specifically VHDL simulations. The main idea of this load-balancing algorithm is based on the exploitation of the usual characteristics of these simulations, that is, cyclicity and predictability, to obtain a good load balance while preserving the locality of references. This algorithm is useful not only in the area of logic circuit simulation but also in systems presenting a cyclic execution pattern, that is, repetition over time, making the future behavior of the tasks predictable. An example of this is Parallel Discrete Event Simulation (PDES), where several tasks are repeatedly executed in response to certain events. A comparison between the proposed algorithm and other load-balancing algorithms found in the literature reveals consistently better execution times with improvements in both load-balancing and locality of references that can be of help on current multicore desktop computers.	algorithm;desktop computer;electronic circuit simulation;execution pattern;load balancing (computing);locality of reference;logic gate;multi-core processor;shared memory;vhdl	Antonio García Dopico;Antonio Pérez;Santiago Rodríguez;Maria Isabel García	2012	Computing and Informatics		multi-core processor;parallel computing;real-time computing;vhdl;computer science;load balancing;operating system;database;distributed computing;parallel algorithm;programming language	HPC	-13.734546898288295	47.60763257295355	178882
4c66d671456a1efa42fdae2991e1533e723eb664	cooperative client-side file caching for mpi applications	distributed system;i o thread;coherent control;performance evaluation;distributed environment;file system;client side file caching;parallel file system;high performance computer;cache coherence;parallel i o;mpi i o;parallel applications	Client-side file caching is one of many I/O strategies adopted by today’s parallel file systems that were initially designed for distributed systems. Most of these implementations treat each client independently because clients’ computations are seldom related to each other in a distributed environment. However, it is misguided to apply the same assumption directly to high-performance computers where many parallel I/O operations come from a group of processes working within the same parallel application. Thus, file caching could perform more effectively if the scope of processes sharing the same file is known. In this paper, we propose a client-side file caching system for MPI applications that perform parallel I/O operations on shared files. In our design, an I/O thread is created and runs concurrently with the main thread in each MPI process. The MPI processes that collectively open a shared file use the I/O threads to cooperate with each other to handle file requests, cache page access, and coherence control. By bringing the caching subsystem closer to the applications as a user space library, it can be incorporated into an MPI I/O implementation to increase its portability. Performance evaluations using three I/O benchmarks demonstrate a significant improvement over traditional methods that use either byte-range file locking or rely on coherent I/O provided by the file system.	byte;cache (computing);client-side;coherence (physics);computation;computer;distributed computing;input/output;lock (computer science);message passing interface;parallel i/o;software portability;supercomputer;user space	Wei-keng Liao;Kenin Coloma;Alok N. Choudhary;Lee Ward	2007	IJHPCA	10.1177/1094342007077857	coherent control;fork;self-certifying file system;cache coherence;parallel computing;torrent file;memory-mapped file;device file;computer file;computer science;class implementation file;stub file;versioning file system;operating system;unix file types;ssh file transfer protocol;journaling file system;distributed computing;open;distributed file system;file system fragmentation;distributed computing environment;file control block;virtual file system	HPC	-15.200533064678439	50.4995364996898	179194
43b9fc3dabc4f9cf6550f64e50b92bbe58dd3893	practical condition synchronization for transactional memory	condition synchronization;computer engineering practical condition synchronization for transactional memory lehigh university michael spear wang;retry;atomicity;semaphore;chao;transactional memory	Few transactional memory implementations allow for condition synchronization among transactions. The problems are many, most notably the lack of consensus about a single appropriate linguistic construct, and the lack of mechanisms that are compatible with hardware transactional memory. In this paper, we introduce a broadly useful mechanism for supporting condition synchronization among transactions. Our mechanism supports a number of linguistic constructs for coordinating transactions, and does so without introducing overhead on in-flight hardware transactions. Experiments show that our mechanisms work well, and that the diversity of linguistic constructs allows programmers to chose the technique that is best suited to a particular application.	atomicity (database systems);c++;computation;contingency (philosophy);high- and low-level;lock (computer science);monitor (synchronization);nested transaction;overhead (computing);parsec;programmer;retry;stress testing;transactional memory	Chao Wang;Michael F. Spear	2016		10.1145/2901318.2901342	transactional memory;parallel computing;real-time computing;semaphore;computer science;operating system;software transactional memory;database;distributed computing;programming language;atomicity	OS	-14.810917618449578	48.31754823730397	179263
20a0e11fa2eeab85eee993cb27d1d038b85488f2	system ten: a new approach to multiprogramming	resource scheduling;input output	Historically, the creation of a multiprogramming computer system, one capable of concurrently operating a number of independent programs, has been viewed largely as a programming task, the task being that of creating an executive program which allocates resources, schedules tasks, and manages input/output. The problems involved in the implementation and operation of such programs have been well documented in the literature.	acknowledgement (data networks);computer multitasking;digitally controlled impedance;etx (form factor);equivalent oxide thickness;input/output;lattice boltzmann methods;null character;object linking and embedding;opcode;roland gs;singer system 10;state of health	Robert V. Dickinson;W. K. Orr	1970		10.1145/1478462.1478488	real-time computing;computer science;operations management;distributed computing	PL	-12.053469317609242	47.71262654947616	179484
d5bf05a67a31a892829d8e81d240421871370a94	hashing strategies for the cray xmt	distribution;shared memory parallel hashing;general and miscellaneous mathematics computing and information science;string data types;concurrent computing;memory management;cost function;magnetic heads;frequency laboratories data structures dictionaries parallel machines concurrent computing peer to peer computing scalability magnetic heads cost function;hashing with chaining hashing strategies cray xmt linear probing memory contention power law distribution shared memory parallel hashing 128 processors power law datasets integer data types string data types scalability;performance;resource management;hashing strategies;integer data types;arrays;parallel databases;memory contention;mathematical methods and computing;shared memory systems;128 processors;synchronization;data structures;dictionaries;linear probing;shared memory systems cray computers data structures parallel databases program processors;parallel machines;cray xmt;scalability;power law datasets;hashing with chaining;peer to peer computing;frequency;power law distribution;program processors;supercomputers;parallel processing;cray computers	Two of the most commonly used hashing strategies-linear probing and hashing with chaining-are adapted for efficient execution on a Cray XMT. These strategies are designed to minimize memory contention. Datasets that follow a power law distribution cause significant performance challenges to shared memory parallel hashing implementations. Experimental results show good scalability up to 128 processors on two power law datasets with different data types: integer and string. These implementations can be used in a wide range of applications.	central processing unit;cray xmt;experiment;hash function;hash table;image scaling;java hotspot virtual machine;linear probing;population;scalability;shared memory;thread-local storage	Eric L. Goodman;David J. Haglin;Chad Scherrer;Daniel G. Chavarría-Miranda;Jace Mogill;John Feo	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470688	hash table;parallel computing;dynamic perfect hashing;computer science;theoretical computer science;distributed computing	HPC	-11.998688324500604	49.502115874273365	179945
7482847f6dbe71f48423d91a6eb84f0df8c36109	improving raid-5 performance by un-striping moderate-sized files	disk drives;interleaved codes;computational modeling;delay computational modeling file systems parallel processing interleaved codes mathematical model disk drives tracking tiles;mathematical model;tiles;parallel processing;tracking;file systems	A method to reduce the average operation time, and increase the transfer rate of RAID-5 disk arrays is presented, along with quantitative models for these paramaers as a funcrion of load. The modified controller allocates moderate sized files (1-20 sectors), and parity information for groups of consecutive sectors, entirely on one disk, avoiding the compounding of latency and queuing delaysfor multiple disks.	data striping;disk array;operation time;standard raid levels	Ronald K. McMurdy;Badrinath Roysam	1993	1993 International Conference on Parallel Processing - ICPP'93	10.1109/ICPP.1993.103	parallel processing;parallel computing;real-time computing;disk sector;computer hardware;computer science;operating system;mathematical model;tracking;computational model	HPC	-12.73497280898794	52.064307061691	180106
f8c6ff024149ebde2f0e8af2afb8b09d6acb73fa	predicting robustness against transient faults of mpi based programs	reliability;program execution;pas2p;message passing interface;transient faults;robustness prediction;parallel application signature;performance prediction;mpi;soft errors;parallel programs	The evaluation of a program’s behavior in the presence of transient faults is often a very time consuming work. In order to achieve significant data, thousands of executions are normally required and each execution will have the significant overhead of the fault injection environment. Our previously published methodology reduced significantly the time needed to evaluate the robustness of a program execution by exhaustively analyzing its basic blocks trace instead of using fault injection. In this paper we present an even forward improvement in the evaluation time of parallel programs robustness against transient faults by combining our methodology with PAS2P – a method that strives to describe an application based on its messagepassing activity. The combination of our approach and PAS2P allowed us to predict the robustness of larger parallel programs, reducing in some cases in more than 20 times the time needed to calculate the robustness while obtaining a robustness prediction error of less than 4%. Transient faults, robustness, soft errors, reliability, PAS2P	ace;basic block;fault injection;message passing interface;overhead (computing);robustness (computer science);run time (program lifecycle phase);secure digital container;software bug;type signature	Joao Gramacho;Alvaro Wong;Dolores Rexachs;Emilio Luque	2016	IJCSE	10.1504/IJCSE.2016.076218	parallel computing;real-time computing;computer science;message passing interface;operating system;distributed computing;robustness	HPC	-17.53051076016094	48.619926629237796	181063
b32bd446e1e44cbba6f8993fa7eb747f811a89a2	freeloader: scavenging desktop storage resources for scientific data	distributed storage;parallel i/o;scientific data management;serverless storage system;storage cache;storage scavenging;striped storage;distributed storage;parallel i/o;scientific data management;serverless storage system;storage cache;storage scavenging;striped storage	High-end computing is suffering a data deluge from experiments, simulations, and apparatus that creates overwhelming application dataset sizes. End-user workstations-despite more processing power than ever before-are ill-equipped to cope with such data demands due to insufficient secondary storage space and I/O rates. Meanwhile, a large portion of desktop storage is unused. We present the FreeLoader framework, which aggregates unused desktop storage space and I/O bandwidth into a shared cache/scratch space, for hosting large, immutable datasets and exploiting data access locality. Our experiments show that FreeLoader is an appealing low-cost solution to storing massive datasets, by delivering higher data access rates than traditional storage facilities. In particular, we present novel data striping techniques that allow FreeLoader to efficiently aggregate a workstation’s network communication bandwidth and local I/O bandwidth. In addition, the performance impact on the native workload of donor machines is small and can be effectively controlled.	aggregate data;auxiliary memory;computer data storage;data access;data striping;desktop computer;distributed computing;experiment;immutable object;information explosion;input/output;lisp machine;locality of reference;simulation;throughput;workstation	Sudharshan S. Vazhkudai;Xiaosong Ma;Vincent W. Freeh;Jonathan W. Strickland;Nandan Tammineedi;Stephen L. Scott	2005	ACM/IEEE SC 2005 Conference (SC'05)		data access;parallel computing;distributed data store;computer hardware;performance;computer science;technical report;operating system;database;data;data transmission	HPC	-15.502861103157688	53.20796004021146	181565
0276440f721b17ff77165f2b1ed24e029b9a2432	memcached design on high performance rdma capable interconnects	cache storage;distributed memory systems;transport protocols application program interfaces cache storage computer centres distributed memory systems local area networks public domain software;socket direct protocol high performance rdma capable interconnect distributed memory object caching system data center environment database call api call bsd socket interface byte stream oriented semantics memcached memory object semantics socket byte stream semantics remote direct memory access rdma api open source memcached software ethernet network hardware accelerated tcp ip connectx infiniband qdr adapter;infiniband;computer centres;public domain software;transport protocols;application program interfaces;memcached;rdma;ucr memcached rdma infiniband incr;incr;high performance;ucr;local area networks;sockets software servers ip networks databases protocols runtime	Memcached is a key-value distributed memory object caching system. It is used widely in the data-center environment for caching results of database calls, API calls or any other data. Using Memcached, spare memory in data-center servers can be aggregated to speed up lookups of frequently accessed information. The performance of Memcached is directly related to the underlying networking technology, as workloads are often latency sensitive. The existing Memcached implementation is built upon BSD Sockets interface. Sockets offers byte-stream oriented semantics. Therefore, using Sockets, there is a conversion between Memcached's memory-object semantics and Socket's byte-stream semantics, imposing an overhead. This is in addition to any extra memory copies in the Sockets implementation within the OS. Over the past decade, high performance interconnects have employed Remote Direct Memory Access (RDMA) technology to provide excellent performance for the scientific computation domain. In addition to its high raw performance, the memory-based semantics of RDMA fits very well with Memcached's memory-object model. While the Sockets interface can be ported to use RDMA, it is not very efficient when compared with low-level RDMA APIs. In this paper, we describe a novel design of Memcached for RDMA capable networks. Our design extends the existing open-source Memcached software and makes it RDMA capable. We provide a detailed performance comparison of our Memcached design compared to unmodified Memcached using Sockets over RDMA and 10Gigabit Ethernet network with hardware-accelerated TCP/IP. Our performance evaluation reveals that latency of Memcached Get of 4KB size can be brought down to 12 µs using ConnectX InfiniBand QDR adapters. Latency of the same operation using older generation DDR adapters is about 20µs. These numbers are about a factor of four better than the performance obtained by using 10GigE with TCP Offload. In addition, these latencies of Get requests over a range of message sizes are better by a factor of five to ten compared to IP over InfiniBand and Sockets Direct Protocol over InfiniBand. Further, throughput of small Get operations can be improved by a factor of six when compared to Sockets over 10 Gigabit Ethernet network. Similar factor of six improvement in throughput is observed over Sockets Direct Protocol using ConnectX QDR adapters. To the best of our knowledge, this is the first such memcached design on high performance RDMA capable interconnects.	application programming interface;attribute–value pair;bsd;berkeley sockets;bitstream;byte;cache (computing);computation;computational science;data center;distributed memory;electrical connection;fits;gigabit;hardware acceleration;high- and low-level;infiniband;interrupt latency;lambda calculus;memcached;open-source software;operating system;overhead (computing);performance evaluation;quad data rate sram;remote direct memory access;sockets direct protocol;tcp offload engine;throughput;unix domain socket	Jithin Jose;Hari Subramoni;Miao Luo;Minjia Zhang;Jian Huang;Md. Wasi-ur-Rahman;Nusrat S. Islam;Xiangyong Ouyang;Hao Wang;Sayantan Sur;Dhabaleswar K. Panda	2011	2011 International Conference on Parallel Processing	10.1109/ICPP.2011.37	local area network;parallel computing;real-time computing;remote direct memory access;computer science;operating system;public domain software;transport layer;computer network	HPC	-16.626931538808805	52.18327619893213	181770
a814087dff7977b3ac4af367a79db655259c75ae	pipelined hash joins using network of workstations		Drrnands for h ish transaction procrssing rates from databasc systsms Ird to the use of parallel processing techniqiiçs for transxtions and queries. Most commercial parallel database systems suffer from sxceptionally high hardware and software costs. Availùbility of public domain software (such as PVM and MPI) that hnmcsses several workstations on a LXN into a vinual pardlel machine provides a welcome alternative for parallel query processing. This thesis exploits such a Network of Workstations for intra-qurry (pipe l i n d ) and intrn-operation (partitionrd) parallelism usin_o the Hash Join algorithm in a crntralizrd qurry processing architectiire. A comprehensive performance evaluation siigsests that Pipelined Hash Joins rxhibit significant improvemrnt in response timr for complex queries and for qurries consiiming very large input relations.	algorithm;computer cluster;hash join;join (sql);message passing interface;parallel virtual machine;parallel computing;parallel database;performance evaluation;public-domain software;workstation	Susheel Jalali;Sivarama P. Dandamudi	2001			parallel computing;polyethylene terephthalate;thermoplastic;hash function;workstation;joins;computer science;molding (process)	DB	-15.178116108001753	51.28540524480834	181910
8157d814fb3462973a62bbed2a05c73aaece4dda	partial differential equations preconditioner resilient to soft and hard faults	regression analysis client server systems elliptic equations fault tolerant computing mathematics computing partial differential equations;high performance computing;boundary conditions;partial differential equations preconditioner 2d elliptic pde asynchronous server client framework boundary conditions regression step domain decomposition based preconditioner hard fault resilience soft fault resilience;distributed computing;linear approximation;parallel programming;client server systems;software engineering;partial d;servers computational modeling data models hardware boundary conditions linear approximation;servers;computational modeling;resilience;fault tolerant systems;fault tolerance;scientific computing;message passing;partial d resilience scientific computing supercomputers parallel programming distributed computing client server systems high performance computing parallel algorithms software engineering fault tolerance message passing fault tolerant systems;supercomputers;data models;hardware;parallel algorithms	We present a domain-decomposition-based pre-conditioner for the solution of partial differential equations (PDEs) that is resilient to both soft and hard faults. The algorithm is based on the following steps: first, the computational domain is split into overlapping subdomains, second, the target PDE is solved on each subdomain for sampled values of the local current boundary conditions, third, the subdomain solution samples are collected and fed into a regression step to build maps between the subdomains' boundary conditions, finally, the intersection of these maps yields the updated state at the subdomain boundaries. This reformulation allows us to recast the problem as a set of independent tasks. The implementation relies on an asynchronous server-client framework, where one or more reliable servers hold the data, while the clients ask for tasks and execute them. This framework provides resiliency to hard faults such that if a client crashes, it stops asking for work, and the servers simply distribute the work among all the other clients alive. Erroneous subdomain solves (e.g. due to soft faults) appear as corrupted data, which is either rejected if that causes a task to fail, or is seamlessly filtered out during the regression stage through a suitable noise model. Three different types of faults are modeled: hard faults modeling nodes (or clients) crashing, soft faults occurring during the communication of the tasks between server and clients, and soft faults occurring during task execution. We demonstrate the resiliency of the approach for a 2D elliptic PDE, and explore the effect of the faults at various failure rates.		Francesco Rizzi;Karla Morris;Khachik Sargsyan;Paul Mycek;Cosmin Safta;Olivier P. Le Maître;Omar M. Knio;Bert J. Debusschere	2015		10.1109/CLUSTER.2015.103	data modeling;fault tolerance;parallel computing;message passing;real-time computing;boundary value problem;computer science;operating system;distributed computing;parallel algorithm;computational model;psychological resilience;server;linear approximation	Crypto	-18.961707969530405	48.63970192660198	182936
02f76b5601951347a4ff9f1b35edefb1ab45fea2	easy and efficient disk i/o workload characterization in vmware esx server	virtual machine;file servers;peripheral interface disk i o workload characterization vmware esx server online histogram virtual machine hypervisor operating system virtual scsi command tracing framework file system cpu solaris;workload characterization;peripheral interface;peripheral interfaces;vmware esx server;disc storage;input output programs;virtual scsi command tracing framework;performance metric;online histogram;statistical analysis;efficient implementation;operating system;virtual machines;file system;virtual machine hypervisor;histograms application software virtual machining operating systems delay instruments virtual machine monitors measurement filters hardware;spatial locality;cpu;virtual machines disc storage file servers input output programs peripheral interfaces statistical analysis;solaris;disk i o workload characterization	Collection of detailed characteristics of disk I/O for workloads is the first step in tuning disk subsystem performance. This paper presents an efficient implementation of disk I/O workload characterization using online histograms in a virtual machine hypervisor VMware ESX Server. This technique allows transparent and online collection of essential workload characteristics for arbitrary, unmodified operating system instances running in virtual machines. For analysis that cannot be done efficiently online, we provide a virtual SCSI command tracing framework. Our online histograms encompass essential disk I/O performance metrics including I/O block size, latency, spatial locality, I/O interarrival period and active queue depth. We demonstrate our technique on workloads of Filebench, DBT-2 and large file copy running in virtual machines and provide an analysis of the differences between ZFS and UFS filesystems on Solaris. We show that our implementation introduces negligible overheads in CPU, memory and latency and yet is able to capture essential workload characteristics.	block size (cryptography);central processing unit;hypervisor;input/output;locality of reference;operating system;principle of locality;scsi command;universal flash storage;virtual machine;zfs	Irfan Ahmad	2007	2007 IEEE 10th International Symposium on Workload Characterization	10.1109/IISWC.2007.4362191	computer architecture;parallel computing;real-time computing;computer hardware;computer science;virtual machine;operating system	Arch	-13.60574366676166	50.35464841941592	183480
c6b040bef0135f44f2cb8c69813e40c6f6f66a05	cuda powered user-defined types and aggregates	databases;relational databases matrix multiplication parallel architectures;sql server;user defined types;net framework;cuda;arrays;graphics processing units servers databases aggregates arrays;servers;parallel architectures;aggregates;graphics processing units;user defined types distributed systems parallel processing cuda sql server net framework matrix multiplication;parallel processing cuda powered user defined data type rdbms heavyweight algebraic calculation programming model complex user data type custom aggregate function multiplication matrix mathematical operation microsoft sql server net platform;matrix multiplication;relational databases;distributed systems;parallel processing	This paper illustrates how CUDA can be successfully integrated into a RDBMS. It describes how heavyweight algebraic calculations can be efficiently performed by RDBMS by utilizing CUDA's programming model. We focus on the implementationof the complex user data type which is stored and managed by the RDBMS. In addition, we examine practicality of the custom aggregate functions and their performance when applied to CUDA powered user defined data types. We demonstrate this concept by using matrices and their multiplication as an example of a mathematical operation that requires great computational power which can be delivered by CUDA. This presented solution employs Microsoft SQL Server and the .NET platform.	.net framework;aggregate data;aggregate function;algorithm;cuda;computation;computer data storage;data structure;elegant degradation;graphics processing unit;inter-process communication;microsoft sql server;programming model;random-access memory;relational database management system;shared memory	Marcin Gorawski;Michal Lorek;Anna Gorawska	2013	2013 27th International Conference on Advanced Information Networking and Applications Workshops	10.1109/WAINA.2013.223	parallel processing;parallel computing;matrix multiplication;relational database;computer science;theoretical computer science;operating system;database;server	DB	-14.75875086055813	50.77365290456066	183498
185867aa71a003c80cb12ae98d72a22c0aba41cd	seekable sockets: a mechanism to reduce copy overheads in tcp-based messaging	libraries;transport protocols linux message passing operating system kernels software libraries;kernel;linux kernel;software libraries;tcpip;linux kernel seekable socket interface copy overhead tcp based messaging tcp ip communication copy avoidance library buffer;tcp ip communication;sockets;sockets libraries tcpip computer interfaces linux kernel operating systems ethernet networks frequency;out of order;copy overhead;library buffer;transport protocols;copy avoidance;message passing;seekable socket interface;linux;tcp based messaging;operating system kernels;frequency;computer interfaces;ethernet networks;operating systems	This paper extends the traditional socket interface to TCP/IP communication with the ability to seek rather than simply receive data in order. Seeking on a TCP socket allows a user program to receive data without first receiving all previous data on the connection. Through repeated use of seeking, a messaging application or library can treat a TCP socket as a list of messages with the potential to receive and remove data from any arbitrary point rather than simply the head of the socket buffer. Seeking facilitates copy-avoidance between a messaging library and user code by eliminating the need to first copy unwanted data into a library buffer before receiving desired data that appears later in the socket buffer. The seekable sockets interface is implemented in the Linux 2.6.13 kernel. Experimental results are gathered using a simple microbenchmark that receives data out-of-order from a given socket, yielding up to a 40% reduction in processing time. The code for seekable sockets is now available for patching into existing Linux kernels and for further development into messaging libraries	benchmark (computing);c file input/output;download;internet protocol suite;library (computing);linux;random access	Chase Douglas;Vijay S. Pai	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639559	embedded system;parallel computing;raw socket;computer science;operating system;distributed computing;network socket;linux kernel;computer network	HPC	-12.042840476684948	46.6883836586347	183604
1261412aa7bfd0978dead4f8ce5b17ad763e0c18	mass storage requirements in the intelligence community	loosely coupled computers;data mining;intelligence community;generate and test;distributed computing system;process mapping;sequential selection;mass storage system;user requirements;of research and development;humans;heuristics;intelligent structures;algorithm design and analysis;supercomputers;time constraints;file systems	The major requirements established among a significant portion of mass storage system workloads in the intelligence community are large numbers (10/sup 7/-10/sup 9/) of objects and a range of small (10 bits) to large-sized (10/sup 8/-10/sup 10/ bits) data objects. Two very different but representative workloads are presented, one centralized and the other distributed. u003e	mass storage;requirement	Tom Myers;Elizabeth Williams	1991	Proceedings of the 1991 ACM/IEEE Conference on Supercomputing (Supercomputing '91)	10.1145/125826.126717	algorithm design;parallel computing;simulation;computer science;user requirements document;operating system;heuristics;data mining;database;programming language	HPC	-17.761445911885236	51.329817912107075	183768
87e218a318f1329f4a8e9e459ad14af503d0718a	optimization of a multilevel checkpoint model with uncertain execution scales	equations computational modeling optimization analytical models mathematical model heating approximation algorithms;heterogeneous support;optimisation checkpointing multiprocessing systems numerical analysis;programming model;load balancing;processes cores optimization multilevel checkpoint model uncertain execution scales extreme scale systems failure scales transient uncorrectable memory errors massive system outages optimal checkpoint intervals checkpoint levels extreme scale numerical simulation wall clock length	Future extreme-scale systems are expected to experience different types of failures affecting applications with different failure scales, from transient uncorrectable memory errors in processes to massive system outages. In this paper, we propose a multilevel checkpoint model by taking into account uncertain execution scales (different numbers of processes/cores). The contribution is threefold: (1) we provide an in-depth analysis on why it is difficult to derive the optimal checkpoint intervals for different checkpoint levels and optimize the number of cores simultaneously; (2) we devise a novel method that can quickly obtain an optimized solution---the first successful attempt in multilevel checkpoint models with uncertain scales; and (3) we perform both large-scale real experiments and extreme-scale numerical simulation to validate the effectiveness of our design. The experiments confirm that our optimized solution outperforms other state-of-the-art solutions by 4.3--88% on wall-clock length.	application checkpointing;computer simulation;experiment;failure cause;ibm websphere extreme scale;transaction processing system	Sheng Di;Leonardo Arturo Bautista-Gomez;Franck Cappello	2014	SC14: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1109/SC.2014.79	parallel computing;real-time computing;simulation;computer science;load balancing;distributed computing;programming paradigm	HPC	-18.20207091523889	49.524735294215006	183874
6784f317ed2d2a0439b7c8442fe96e94c8213298	comparing the memory system performance of dss workloads on the hp v-class and sgi origin 2000	shared memory multiprocessors memory system performance evaluation decision support systems hp v class query process data cache context switches sequential query;performance evaluation;query processing;storage management;shared memory systems;data cache;storage management decision support systems performance evaluation query processing shared memory systems;decision support systems;memory systems;system performance decision support systems query processing computer science computer architecture performance analysis context switches communication switching databases	In this paper, we present an in-depth analysis of the memory system performance of the DSS commercial workloads on two state-of-the-art multiprocessors: the SGI Origin 2000 and the HP V-Class. Our results show that a single query process takes almost the same amount of cycles in both machines. However, when multiple query processes run simultaneously on the system, the execution time tends to increase more in SGI Origin 2000 than in HP V-Class due to the more expensive communication overhead in SGI Origin 2000. We also show how the rate at which number of data cache misses, context switches and the overall execution time increases when more query processes run simultaneously.	benchmark (computing);cpu cache;context switch;hp 3000;ibm tivoli storage productivity center;index (publishing);locality of reference;network switch;overhead (computing);run time (program lifecycle phase)	Laxmi N. Bhuyan;Ravi R. Iyer	2002		10.1109/IPDPS.2002.1015507	shared memory;sargable;query optimization;parallel computing;real-time computing;decision support system;computer science;operating system;database;cache-only memory architecture;non-uniform memory access	Metrics	-12.2481281988812	51.47320823761936	184164
b6e27cac4056ba280c443770689a1977985ddb3b	on transactional memory, spinlocks, and database transactions		Currently, hardware trends include a move toward multicore processors, cheap and persistent variants of memory, and even sophisticated hardware support for mutual exclusion in the form of transactional memory. These trends, coupled with a growing desire for extremely high performance on short database transactions, raise the question of whether the hardware primitives developed for mutual exclusion can be exploited to run database transactions. In this paper, we present a preliminary exploration of this question. We conduct a set of experiments on both a hardware prototype and a simulator of a multi-core processor with Transactional Memory (TM.) Our results show that TM is attractive under low contention workloads, while spinlocks can tolerate high contention workloads well, and that in some cases these approaches can beat a simple implementation of a traditional database lock manager by an order of magnitude.	central processing unit;database transaction;experiment;multi-core processor;mutual exclusion;prototype;spinlock;transactional memory	Khai Q. Tran;Spyros Blanas;Jeffrey F. Naughton	2010			transactional memory;parallel computing;real-time computing;computer science;software transactional memory;database	DB	-13.772656484972119	49.7167760734718	184365
89314e73c549591f41c2f5f6ac29d9adf3c82a5d	stream sockets on shrimp	buffer management;public domain;high performance;data transfer	This paper describes an implementation of stream sockets for the SHRIMP multicomputer. SHRIMP supports protected, user-level data transfer, allows user-level code to perform its own bu er management, and separates data transfers from control transfers so that data transfers can be done without the interrupting the receiving node's CPU. Our sockets implementation exploits all of these features to provide high performance. End-to-end latency for 8 byte transfers is 11 microseconds, which is considerably lower than all previous implementations of the sockets interface. For large transfers, we obtain a bandwidth of 13.5 MBytes/sec, which is close to the hardware limit when the receiver must perform a copy. Further experiments with the public-domain benchmarks ttcp and netperf con rm the performance of our implementation.	bandwidth (signal processing);byte;central processing unit;direct memory access;experiment;input/output;interrupt;memory bus;message passing;naruto shippuden: clash of ninja revolution 3;netperf;parallel computing;stream socket;ttcp;user space;zero-copy	Stefanos N. Damianakis;Cezary Dubnicki;Edward W. Felten	1997		10.1007/3-540-62573-9_2	parallel computing;public domain;real-time computing;computer science;operating system	HPC	-11.923190987955932	46.587450862415864	185055
627a32d5f5c0f848e8d6b522fa101f82f856d7b8	algorithm-based fault tolerance for fail-stop failures	checksum relationship;kernel;parallel algorithm;fault tolerant;checksum matrix;reliability and robustness;algorithm based fault tolerance;distributed processing;parallel matrix matrix multiplication;software fault tolerance;computer applications;checkpointing;product version algorithm;computational modeling;redundancy;distributed environment;fault tolerant systems;fault tolerance;parallel algorithms reliability and robustness mathematical software;lifting equipment;mathematical software;matrix multiplication;fail stop failures;checksum matrix fault tolerance fail stop failure distributed environment scalapack matrix matrix multiplication processor miscalculation checksum relationship product version algorithm;software fault tolerance checkpointing distributed processing matrix multiplication;matrix matrix multiplication;fault tolerance checkpointing grid computing computer applications lifting equipment kernel fault tolerant systems computational modeling redundancy;processor miscalculation;grid computing;fail stop failure;message logging;parallel algorithms;scalapack	Fail-stop failures in distributed environments are often tolerated by checkpointing or message logging. In this paper, we show that fail-stop process failures in ScaLAPACK matrix-matrix multiplication kennel can be tolerated without checkpointing or message logging. It has been proved in previous algorithm-based fault tolerance that, for matrix-matrix multiplication, the checksum relationship in the input checksum matrices is preserved at the end of the computation no mater which algorithm is chosen. From this checksum relationship in the final computation results, processor miscalculations can be detected, located, and corrected at the end of the computation. However, whether this checksum relationship can be maintained in the middle of the computation or not remains open. In this paper, we first demonstrate that, for many matrix matrix multiplication algorithms, the checksum relationship in the input checksum matrices is not maintained in the middle of the computation. We then prove that, however, for the outer product version algorithm, the checksum relationship in the input checksum matrices can be maintained in the middle of the computation. Based on this checksum relationship maintained in the middle of the computation, we demonstrate that fail-stop process failures (which are often tolerated by checkpointing or message logging) in ScaLAPACK matrix-matrix multiplication can be tolerated without checkpointing or message logging.	application checkpointing;checksum;computation;fail-stop;fault tolerance;matrix multiplication;multiplication algorithm;outer product;scalapack	Zizhong Chen;Jack J. Dongarra	2008	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2008.58	fault tolerance;parallel computing;real-time computing;checksum;computer science;operating system;distributed computing;parallel algorithm	HPC	-18.57672752532926	48.594814635468005	185099
aec088dce6eb8fa7beef49ada46497933e77cc9d	transaction reordering to reduce aborts in software transactional memory	novel technique;application-specific information;performance improvement;transactional memory;offline pre-processing;software transactional memory;waste computing resource;concurrent execution;transaction reordering;different contention manager;repeat conflict	In transactional memory, conflicts between two concurrently executing transactions reduce performance, reduce scalability, and may lead to aborts, which waste computing resources. Ideally, concurrent execution of transactions would be ordered to minimise conflicts, but such an ordering is often complex, or unfeasible, to obtain. This paper identifies a pattern, called repeat conflicts, that can be a source of conflicts, and presents a novel technique, called steal-on-abort, to reduce the number of conflicts caused by repeat conflicts. Steal-on-abort operates at runtime, and requires no application-specific information or offline preprocessing. Evaluation using a sorted linked list, and STAMP-vacation with different contention managers show steal-on-abort to be highly effective at reducing repeat conflicts, which leads to a range of performance improvements.	benchmark (computing);blocking (computing);linked list;non-blocking algorithm;online and offline;preprocessor;run time (program lifecycle phase);scalability;software transactional memory	Mohammad Ansari;Mikel Luján;Christos Kotselidis;Kim Jarvis;Chris C. Kirkham;Ian Watson	2011	Trans. HiPEAC	10.1007/978-3-642-24568-8_10	parallel computing;real-time computing;computer science;distributed computing	Arch	-13.91404546888718	48.04759854003873	185763
32a548e5089703f7f6d775c8927889dd12abf35b	transparent accelerator migration in a virtualized gpu environment	virtualization;paper;fault tolerant;migration;software maintenance;resource allocation;resource manager;graphics processing unit kernel libraries programming computational modeling maintenance engineering runtime;software fault tolerance;gpu;demand systems;transparent load balancing transparent accelerator migration virtualized gpu environment virtual gpu accelerators virtualized execution environment fault tolerance on demand system maintenance resource management migration overhead reduction;execution environment;graphics processing units;nvidia;vocl;load balance;tesla m2070;vocl gpu virtualization opencl migration;computer science;program compilers;opencl;virtualisation;virtualisation graphics processing units program compilers resource allocation software fault tolerance software maintenance	This paper presents a framework to support transparent, live migration of virtual GPU accelerators in a virtualized execution environment. Migration is a critical capability in such environments because it provides support for fault tolerance, on-demand system maintenance, resource management, and load balancing in the mapping of virtual to physical GPUs. Techniques to increase responsiveness and reduce migration overhead are explored. The system is evaluated by using four application kernels and is demonstrated to provide low migration overheads. Through transparent load balancing, our system provides a speedup of 1.7 to 1.9 for three of the four application kernels.	fault tolerance;graphics processing unit;load balancing (computing);overhead (computing);responsiveness;speedup;x86 virtualization	Shucai Xiao;Pavan Balaji;James Dinan;Qian Zhu;Rajeev Thakur;Susan Coghlan;Heshan Lin;Gaojin Wen;Jue Hong;Wu-chun Feng	2012	2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)	10.1109/CCGrid.2012.26	parallel computing;real-time computing;virtualization;computer science;resource management;operating system	HPC	-17.113548201261064	52.99611113069745	186176
77af85b2708f3807cdca961d50064d46a1325104	performance and storage management characteristics of sas versions			sas	Daniel J. Squillace	1989				DB	-19.065564991626385	51.76791136655048	186374
e806479e136c2c8a144be8b4a0d65423d8f8efe8	a dynamic core grouping approach to improve raw architecture many-core processor performance	packet processing application;cache hierarchy;packet processing;performance evaluation;programming paradigm;many core processor;legacy applications;software design methodology;throughput hardware tiles computer architecture system performance mesh networks inspection;deep packet inspection system;inspection;system performance;run time dynamic core grouping approach;deep packet inspection;computer architecture;parallel architectures;programming paradigms;performance evaluation multiprocessing systems parallel architectures;deep packet inspection many core processor core grouping packet processing;hardware platforms;mesh networks;core grouping;raw architecture many core processor performance;mesh network;tiles;multiprocessing systems;software design;deep packet inspection system raw architecture many core processor performance hardware platforms software design methodology programming paradigms legacy applications packet processing application cache hierarchy run time dynamic core grouping approach;throughput;hardware	The ongoing move of hardware platforms to many-core processor challenges the traditional software design methodology. It is critical to develop new programming paradigms and efficient ways to port legacy applications. This paper analyzed a typical packet processing application and also the cache hierarchy and behavior of Raw architecture many-core processor. It presented an easy to implement run-time dynamic core grouping approach to improve the system performance. This approach reduced the cache swap latency by grouping neighbor cores attached to the mesh network. It optimized the scale of group by experimental data got beforehand. The test results showed this approach can improve the Deep Packet Inspection (DPI) system performance around 10% with very minor code change.	deep packet inspection;manycore processor;mesh networking;network packet;paging;programming paradigm;run time (program lifecycle phase);sql;software design	Zhitao Wan	2011	2011 Sixth International Symposium on Parallel Computing in Electrical Engineering	10.1109/PARELEC.2011.30	computer architecture;parallel computing;real-time computing;computer science;mesh networking;operating system;computer performance;programming paradigm;computer network	Arch	-14.375201114611833	50.43377385138432	186583
261b8f110eb755334efe298dab99020f930b2473	the neurotron monitor system	performance monitoring;monitoring system;performance improvement;measurement technique;dynamic behavior	The subject of performance monitoring and measurement has grown from infancy to childhood, and with this growth came substantial performance improvements even with superficial monitoring analysis. The recent increased interest in applying measurement techniques by manufacturers and users of large systems stems mainly from the high cost of development, purchase, and use of such systems. This cost obligates each to obtain quantitative information on the dynamic behavior of proposed or purchased equipment and software. This quantitative information is necessary when a determination is to be made of the difference between potential and actual performance of hardware and software.	computer hardware;dynamical system;programmer;real-time transcription;system monitor;the superficial	Richard A. Aschenbrenner;Lawrence Amiot;N. K. Natarajan	1971		10.1145/1479064.1479072	simulation;engineering;operations management;forensic engineering	Metrics	-18.253482192140684	47.720508427530795	186619
63d2c080ba1e35faa829f68312a233ebeaa5c245	an efficient i/o-redirection-based reconstruction scheme for erasure-coded storage clusters	random access memory;reliability;disk i o redirection based reconstruction scheme erasure coded storage clusters i o interference problem online reconstruction network bandwidth redirection scheme i o interference minimization user requests reconstruction requests rs coded ram region main memory preallocation user read write misses rebuilding node node reconstruction data rebuilt surviving nodes markov models ram rs system mttdl i o traces 9 node storage cluster system performance user response time reconstruction time;io重定向;deferred write erasure coded storage cluster reconstruction i o redirection;reconstruction;erasure coded storage cluster;reed solomon codes disc storage markov processes random access storage;interference;random access memory bandwidth time factors interference reliability markov processes educational institutions;i o redirection;time factors;重构;纠删码存储集群;期刊论文;bandwidth;markov processes;deferred write	This paper addresses an I/O interference problem encountered in on-line reconstruction of erasure-coded storage clusters, where user I/Os compete with reconstruction I/Os for both disk and network bandwidth. We propose a redirection scheme called `RAM-RS' to minimize the I/O interference among user and reconstruction requests. RAM-RS redirects user read/writes targeted at failed nodes to an RS-coded RAM region, which is formed by pre-allocated main memory in surviving nodes in the RS-coding manner. The RS-coded RAM region quickly serves all user read/write misses; therefore, a rebuilding node can devote its disk and network bandwidths to the node reconstruction. The RAM region substantially reduces the amount of data rebuilt by the rebuilding node, because (1) missed writes are buffered in the RAM region and (2) missed reads are satisfied by using surviving nodes to co-rebuild failed blocks. We build two Markov models to estimate the reliability of the RAM-RS system. Modeling results demonstrate that the MTTDL of RS-coded RAM region in a storage cluster is larger than that of the same cluster comprised of surviving nodes. We implement both RAM-RS and the traditional Redirection schemes in an erasure-coded storage cluster, on which real-world I/O traces are replayed. Experimental results show that compared with the Redirection scheme running on a 9-node storage cluster, RAM-RS improves system performance in terms of both user response time and reconstruction time by a factor of 1.78 and 1.20, respectively.	bandwidth (signal processing);compiler;computer cluster;computer data storage;input/output;interference (communication);markov chain;markov model;online and offline;random-access memory;reed–solomon error correction;response time (technology);tracing (software)	Jianzhong Huang;Xiao Qin;Xianhai Liang;Changsheng Xie	2015	IEEE Transactions on Computers	10.1109/TC.2015.2394399	embedded system;parallel computing;real-time computing;telecommunications;computer science;operating system;reliability;interference;markov process;bandwidth;statistics;computer network	OS	-12.624014622766982	52.92829352761571	186637
9573c14d3a12fd259437451a5e31b311db395388	pxfs: a persistent storage model for extreme scale	computational modeling throughput runtime servers scalability educational institutions computer architecture;user interfaces electronic engineering computing parallel machines;persistent storage model pxfs module orangefs user interface orange parallel file system parallex model file system model balanced supercomputing system message driven computation multithreaded distributed codes parallel applications storage subsystems exascale systems parallel computations scientific data sets aggregate computational performance dramatic growth flops energy efficiency continuing technological progress;parallel machines;electronic engineering computing;user interfaces	The continuing technological progress resulted in sustained increase in the number of transistors per chip as well as improved energy efficiency per FLOPS. This spurred a dramatic growth in aggregate computational performance of the largest supercomputing systems, yielding multiple Petascale implementations deployed in various locations over the world. Unfortunately, these advances did not translate to the required extent into accompanying I/O systems, which primarily saw the improvement in cumulative storage sizes required to match the ever expanding volume of scientific data sets, but little more in terms of architecture or effective access latency. Moreover, while new models of computations are formulated to handle the burden of efficiently structuring the parallel computations in anticipation of the arrival of Exascale systems, a meager progress is observed in the area of storage subsystems. New classes of algorithms developed for massively parallel applications, that gracefully handle the challenges of asynchrony, heavily multithreaded distributed codes, and message-driven computation, must be matched by similar advances in I/O methods and algorithms to produce a well performing and balanced supercomputing system. This paper discusses PXFS, a file system model for persistent objects inspired by the ParalleX model of execution that addresses many of these challenges. An early implementation of PXFS utilizing a well known Orange parallel file system as its back-end via asynchronous I/O layer is also described along with the preliminary performance data. The results show perfect scalability and 3× to 20× times speedup of I/O throughput performance comparing to OrangeFS user interface. Also the PXFS module on OrangeFS with 24 clients sees a 5× to 10× times more throughput than NFS.	aggregate data;algorithm;asynchronous i/o;benchmark (computing);clustered file system;code;computation;flops;ibm websphere extreme scale;input/output;memory-mapped i/o;orange;orangefs;parallel computing;persistence (computer science);petascale computing;runtime system;scalability;speedup;storage model;supercomputer;thread (computing);throughput;transistor;user interface	Shuangyang Yang;Maciej Brodowicz;Walter B. Ligon;Hartmut Kaiser	2014	2014 International Conference on Computing, Networking and Communications (ICNC)	10.1109/ICCNC.2014.6785457	parallel computing;real-time computing;computer science;distributed computing	HPC	-16.214768605255024	50.61979678002073	186768
f0bf5811acc624217105ca756db3cc9b5e73b9f2	lazy persistency: a high-performing and write-efficient software persistency technique		Emerging Non-Volatile Memories (NVMs) are expected to be included in future main memory, providing the opportunity to host important data persistently in main memory. However, achieving persistency requires that programs be written with failure-safety in mind. Many persistency models and techniques have been proposed to help the programmer reason about failure-safety. They require that the programmer eagerly flush data out of caches to make it persistent. Eager persistency comes with a large overhead because it adds many instructions to the program for flushing cache lines and incurs costly stalls at barriers to wait for data to become durable. To reduce these overheads, we propose Lazy Persistency (LP), a software persistency technique that allows caches to slowly send dirty blocks to the NVMM through natural evictions. With LP, there are no additional writes to NVMM, no decrease in write endurance, and no performance degradation from cache line flushes and barriers. Persistency failures are discovered using software error detection (checksum), and the system recovers from them by recomputing inconsistent results. We describe the properties and design of LP and demonstrate how it can be applied to loop-based kernels popularly used in scientific computing. We evaluate LP and compare it to the state-of-the-art Eager Persistency technique from prior work. Compared to it, LP reduces the execution time and write amplification overheads from 9% and 21% to only 1% and 3%, respectively.	cpu cache;cache (computing);checksum;computational science;computer data storage;dirty data;elegant degradation;error detection and correction;lazy evaluation;non-volatile memory;overhead (computing);persistence (computer science);programmer;run time (program lifecycle phase);software bug	Mohammad Alshboul;James Tuck;Yan Solihin	2018	2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)	10.1109/ISCA.2018.00044	parallel computing;operating system;error detection and correction;computer science;cache;write amplification;software;cpu cache;checksum;overhead (business);programmer	Arch	-15.671821927045277	50.158818649416745	187048
110631e23b57e5323dad5dfbfe761906cbc23e5b	disk cache-miss ratio analysis and design considerations	error recovery;analysis and design;data processing;input output;high end computing;trace driven simulation	The current trend of computer system technology is toward CPUs with rapidly increasing processing power and toward disk drives of rapidly increasing density, but with disk performance increasing very slowly if at all. The implication of these trends is that at some point the processing power of computer systems will be limited by the throughput of the input/output (I/O) system. A solution to this problem, which is described and evaluated in this paper, is disk cache. The idea is to buffer recently used portions of the disk address space in electronic storage. Empirically, it is shown that a large (e.g., 80-90 percent) fraction of all I/O requests are captured by a cache of an 8-Mbyte order-of-magnitude size for our workload sample. This paper considers a number of design parameters for such a cache (called cache disk or disk cache), including those that can be examined experimentally (cache location, cache size, migration algorithms, block sizes, etc.) and others (access time, bandwidth, multipathing, technology, consistency, error recovery, etc.) for which we have no relevant data or experiments. Consideration is given to both caches located in the I/O system, as with the storage controller, and those located in the CPU main memory. Experimental results are based on extensive trace-driven simulations using traces taken from three large IBM or IBM-compatible mainframe data processing installations. We find that disk cache is a powerful means of extending the performance limits of high-end computer systems.	access time;address space;algorithm;cpu cache;central processing unit;computer data storage;disk buffer;experiment;ibm pc compatible;input/output;mainframe computer;page cache;simulation;throughput;tracing (software)	Alan Jay Smith	1985	ACM Trans. Comput. Syst.	10.1145/3959.3961	bus sniffing;input/output;least frequently used;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;disk sector;cpu cache;data processing;computer hardware;cache;computer science;data buffer;write-once;disk array controller;cache invalidation;operating system;direct memory access;distributed computing;cache algorithms;cache pollution	DB	-13.163549288166093	52.962464149620914	187522
eb18d764ccf48888b6664483f9153dd30cb2791e	a new deadlock resolution protocol and message matching algorithm for the extreme-scale simulator	high performance computing;mathematics and computing;message passing interface;parallel discrete event simulation;performance prediction;mathematics and computing performance prediction	Investigating the performance of parallel applications at scale on future high-performance computing (HPC) architectures and the performance impact of different HPC architecture choices is an important component of HPC hardware/software co-design. The Extreme-scale Simulator (xSim) is a simulation toolkit for investigating the performance of parallel applications at scale. xSim scales to millions of simulated Message Passing Interface (MPI) processes. The xSim toolkit strives to limit simulation overheads in order to maintain performance and productivity criteria. This paper documents two improvements to xSim: (1) a new deadlock resolution protocol to reduce the parallel discrete event simulation overhead and (2) a new simulated MPI message matching algorithm to reduce the oversubscription management cost. These enhancements resulted in significant performance improvements. The simulation overhead for running the NASA Advanced Supercomputing Parallel Benchmark suite dropped from 1,020% to 238% for the conjugate gradient benchmark and 102% to 0% for the embarrassingly parallel benchmark. Additionally, the improvements were beneficial for reducing overheads in the highly accurate simulation mode of xSim, which is useful for resilience investigation studies for tracking intentional MPI process failures. In the highly accurate mode, the simulation overhead was reduced from 37,511% to 13,808% for conjugate gradient and from 3,332% to 204% for embarrassingly parallel. Copyright © 2016 John Wiley & Sons, Ltd.	algorithm;benchmark (computing);conjugate gradient method;deadlock;embarrassingly parallel;ibm websphere extreme scale;john d. wiley;message passing interface;overhead (computing);overselling;simulation;supercomputer	Christian Engelmann;Thomas Naughton	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3805	supercomputer;parallel computing;computer science;message passing interface;theoretical computer science;distributed computing	HPC	-17.362281642117257	48.92978854483011	187674
1830ab733011e2e01f31f68c33ec0933833b0460	a new metric for controlling granularity for parallel execution	time complexity	Granularity control is a method to improve parallel execution performan ce by limiting excessive parallelism. The general idea is that if the gain obtained by executin g a ask in parallel is less than the overheads required to support parallel execution, t hen the task is better executed sequentially. Traditionally, in logic programming task size is estimated from the sequential time-complexity of evaluating the task. Tasks are o nly executed in parallel if task size exceeds a pre-determined threshold. We argue in this paper that the estimation of complexity on its own is n ot a ideal metric for improving the performance of parallel programs through granu l rity control. We present a new metric for measuring granularity, based on a notion of distance. We present some initial results with two very simple methods of using this met ric for granularity control. We then discuss how more sophisticated granularity control meth ods can be devised using the new metric.	benchmark (computing);compile time;compiler;k-nearest neighbors algorithm;logic programming;parallel computing;run time (program lifecycle phase)	Kish Shen;Vítor Santos Costa;Andy King	1997	Journal of Functional and Logic Programming		parallel computing;real-time computing;distributed computing	HPC	-13.419028778958273	47.62827000604089	189767
bc802d17026bf21db99e178592b8da789d98247a	software-implemented fault injection methodology for design and validation of system fault tolerance	memory regions;verifiers;fault tolerant;swifi;application software;statistical data analysis software implemented fault injection system fault tolerance design system fault tolerance validation jet propulsion laboratory swifi parallel processing supercomputer space exploration missions software based strategies radiation induced transients system hardware components jifi system fault model fault tolerance architecture evaluation fault sensitivity fault detection strategies fault isolation strategies fault recovery strategies user specified cpu registers memory regions uniform random distribution verifiers classifiers run scripts fault injection campaigns;fault isolation strategies;statistical data analysis;system hardware components;space exploration;software fault tolerance;radiation induced transients;fault detection strategies;computer fault tolerance;software implemented fault injection;uniform random distribution;fault injection campaigns;software based strategies;computer architecture;fault tolerant computing;run scripts;computer testing;program testing;aerospace computing;fault recovery strategies;fault tolerant systems;fault tolerance architecture evaluation;system fault model;fault detection;classifiers;propulsion;software tools;space exploration missions;fault sensitivity;fault model;system fault tolerance design;fault injection;system fault tolerance validation;supercomputers;parallel processing;jifi;program testing fault tolerant computing parallel processing aerospace computing computer testing;design methodology fault tolerant systems hardware propulsion laboratories supercomputers space exploration software tools computer architecture application software;jet propulsion laboratory;hardware;design methodology;parallel processing supercomputer;user specified cpu registers	Presents our experience in developing a methodology and tool at the Jet Propulsion Laboratory (JPL) for software-implemented fault injection (SWIFI) into a parallel-processing supercomputer which is being designed for use in next-generation space exploration missions. The fault injector uses software-based strategies to emulate the effects of radiation-induced transients occurring in the system hardware components. JPL's SWIFI tool set, which is called JIFI (JPL's Implementation of a Fault Injector), is being used in conjunction with an appropriate system fault model to evaluate candidate hardware and software fault tolerance architectures, to determine the sensitivity of applications to faults, and to measure the effectiveness of fault detection, isolation and recovery strategies. JIFI has been validated to inject faults into user-specified CPU registers and memory regions with a uniform random distribution in location and time. Together with verifiers, classifiers and run scripts, JIFI enables massive fault injection campaigns and statistical data analysis.	fault injection;system fault tolerance	Raphael R. Some;Won S. Kim;Garen Khanoyan;Leslie Callum;Anil Agrawal;John J. Beahan	2001		10.1109/DSN.2001.941435	embedded system;parallel processing;fault tolerance;application software;parallel computing;real-time computing;propulsion;fault coverage;design methods;computer science;stuck-at fault;space exploration;operating system;fault model;general protection fault;fault detection and isolation;software fault tolerance	EDA	-17.79498224728165	48.694170120277185	189770
ab7b82b6118c3a67de1197758ff245b2f5378f1d	selecting threads for workload migration in software distributed shared memory systems	memory access;software distributed shared memory;selection policy;communication cost;memory access types;global sharing;load balance;data consistency;distributed shared memory	When threads are migrated from heavily loaded nodes to lightly loaded nodes for load balance in software distributed shared memory systems, the communication cost of maintaining data consistency is increased if migration threads are carelessly selected. Program performance is degraded when loss from increased communication exceeds the benefit from load balancing. This study addresses the problem with a novel selection policy called reduction of internode sharing costs. The main characteristic of this policy is simultaneously considering thread memory access types and global sharing. The experimental results show that this policy can reduce the communication of benchmark applications by 50% during load balancing.	distributed shared memory	Tyng-Yue Liang;Ce-Kuen Shieh;Jun-Qi Li	2002	Parallel Computing	10.1016/S0167-8191(02)00071-6	uniform memory access;distributed shared memory;shared memory;interleaved memory;parallel computing;real-time computing;distributed memory;computer science;load balancing;operating system;distributed computing;overlay;data consistency;data diffusion machine;memory management	HPC	-13.04946723193595	51.400743025786035	189818
77fdefa7f1ddf0ea1a6cfa16eaf033a7d04d805b	fast in-memory transaction processing using rdma and htm		DrTM is a fast in-memory transaction processing system that exploits advanced hardware features such as remote direct memory access (RDMA) and hardware transactional memory (HTM). To achieve high efficiency, it mostly offloads concurrency control such as tracking read/write accesses and conflict detection into HTM in a local machine and leverages the strong consistency between RDMA and HTM to ensure serializability among concurrent transactions across machines. To mitigate the high probability of HTM aborts for large transactions, we design and implement an optimized transaction chopping algorithm to decompose a set of large transactions into smaller pieces such that HTM is only required to protect each piece. We further build an efficient hash table for DrTM by leveraging HTM and RDMA to simplify the design and notably improve the performance. We describe how DrTM supports common database features like read-only transactions and logging for durability. Evaluation using typical OLTP workloads including TPC-C and SmallBank shows that DrTM has better single-node efficiency and scales well on a six-node cluster; it achieves greater than 1.51, 34 and 5.24, 138 million transactions per second for TPC-C and SmallBank on a single node and the cluster, respectively. Such numbers outperform a state-of-the-art single-node system (i.e., Silo) and a distributed transaction system (i.e., Calvin) by at least 1.9X and 29.6X for TPC-C.		Haibo Chen;Rong Chen;Xingda Wei;Jiaxin Shi;Yanzhe Chen;Zhaoguo Wang;Binyu Zang;Haibing Guan	2017	ACM Trans. Comput. Syst.	10.1145/3092701	real-time computing;parallel computing;online transaction processing;distributed computing;concurrency control;computer science;transaction processing;distributed transaction;transaction processing system;transactional memory;serializability;database transaction	OS	-14.6135905449425	50.0252481332613	190811
715cc8f921d3c58cfabf3244d3de7b00b87e3fcb	efficient and reliable lock-free memory reclamation based on reference counting	storage allocation;reference counting;text;shared memory;memory management;lock free;perforation;storage management;garbage reclamation;memory reclamation;garbage collection reliable lock free memory reclamation reference counting lock free dynamic data structures computer systems memory reuse lock free algorithms;garbage collection;upper bound;data structures memory management safety marine technology upper bound heuristic algorithms buildings system recovery java hazards;data structures;dynamic data structure;semiautomatic;data structure;storage allocation data structures storage management	We present an efficient and practical lock-free method for semiautomatic (application-guided) memory reclamation based on reference counting, aimed for use with arbitrary lock-free dynamic data structures. The method guarantees the safety of local as well as global references, supports arbitrary memory reuse, uses atomic primitives that are available in modern computer systems, and provides an upper bound on the amount of memory waiting to be reclaimed. To the best of our knowledge, this is the first lock-free method that provides all of these properties. We provide analytical and experimental study of the method. The experiments conducted have shown that the method can also provide significant performance improvements for lock-free algorithms of dynamic data structures that require strong memory management.	data structure;dynamic data;dynamization;experiment;memory management;non-blocking algorithm;reference counting	Anders Gidenstam;Marina Papatriantafilou;Håkan Sundell;Philippas Tsigas	2005	8th International Symposium on Parallel Architectures,Algorithms and Networks (ISPAN'05)	10.1109/ISPAN.2005.42	parallel computing;real-time computing;computer science;database	Embedded	-13.25507932404504	48.82413170130078	190987
e9ba779e03e588645c02ee8eae16bee125eb2971	error recovery mechanism for iscsi protocol based mobile nas cluster system		In this paper, we proposed an error recovery module for iSCSI protocol based NAS (Network Attached Storage) system in wireless network environment, which ensures reliability and efficiency data transmission.	experiment;iscsi;network-attached storage	Shaikh Muhammad Allayear;Sung-Soon Park;Md. Nawab Yousuf Ali;Mohammad Hasmat Ullah	2011		10.1007/978-3-642-32573-1_24	hyperscsi;embedded system;real-time computing;computer network	Mobile	-18.173774612970085	52.28561853935753	191017
ec20eb666574a97d3c7f85d988589b0fb8ee5466	improving i/o performance using virtual disk introspection	performance improvement;new performance optimizations;o performance;enhanced meta-data caching;virtualized workloads;virtual machine disk image;vmdi-introspection-based optimizations;storage array;virtual disk introspection;vmdi introspection;stringent new requirement;o optimizations	Storage consolidation due to server virtualization puts stringent new requirements on Storage Array (SA) performance. Virtualized workloads require new performance optimizations that cannot be totally addressed by merely using expensive hardware such as SSDs. This position paper presents Virtual Machine Disk Image (VMDI) introspection—a key technique for implementing a variety of virtualization-oriented I/O optimizations. VMDI introspection gives SAs an understanding of guest file system semantics, such as determining object types and classifying read and write operations. We explore possible approaches for VMDI introspection and then describe a set of VMDI-introspection-based optimizations. Our prototype implementation with enhanced meta-data caching and placement shows 11% to 20× performance improvement.	application programming interface;archive;disk array;disk image;ibm notes;input/output;introspection;object type (object-oriented programming);office open xml;prototype;requirement;semiconductor consolidation;server (computing);vmdk;virtual disk;virtual machine;virtual private server;virtual reality	Vasily Tarasov;Deepak Jain;Dean Hildebrand;Renu Tewari;Geoffrey H. Kuenning;Erez Zadok	2013			parallel computing;real-time computing;computer hardware;computer science	OS	-16.606080461389723	52.62885769598519	191406
a6b4dd1c9d8ea3d696f8009dcb25c30fe1eff625	a java util concurrent park contention tool	juc park contention tool;juc;lock profiling;thread park;java	Java Util Concurrent (JUC) is a widely used library in multithreaded Java applications. JUC provides a variety of tools such as locks, thread pools and blocking queues. Many of these constructs use Thread Park, a mechanism which allows threads to be blocked from execution, as a means of thread synchronization.  We modified the IBM Java Virtual Machine (JVM) creating a near zero overhead tool that measures and reports park contention data using a variety of metrics. This information can then be used to identify bottlenecks and consequently accelerate Java code.  We demonstrate the usefulness of this JUC Park Contention Tool in two example code patterns where it successfully identifies the JUC lock bottleneck. Furthermore, we also compare theoretical with measured values of the metrics in an example program. Finally, we use our tool to profile the Java benchmarks SPECjbb2005, SPECjbb2013 and the DaCapo benchmark suite.	benchmark (computing);blocking (computing);bottleneck (software);concurrent computing;dacapo;java virtual machine;lock (computer science);multithreading (computer architecture);overhead (computing);synchronization (computer science);thread (computing);thread pool;utility	Panagiotis Patros;Eric Aubanel;David Bremner;Michael Dawson	2015		10.1145/2712386.2712389	parallel computing;real-time computing;java concurrency;computer science;operating system;java	PL	-15.023611544365458	47.645912112055015	191489
8dee6c0a8438a995b1d2452b84c7544be5f00578	indirection systems for shingled-recording disk drives	write once storage;write once storage disc drives discrete event simulation hard discs magnetic recording;disc drives;storage system;unrestricted read write request;performance evaluation;magnetic heads;shingled recording disk drive;disk drives;data track;interference;computer architecture;disk drives magnetic recording magnetic heads interference hard disks heat assisted magnetic recording magnetic memory file systems computer architecture laboratories;disk cache based architecture;writing;indirection system;magnetic recording;s block architecture;shingled magnetic recording;hard disk drive;performance optimization;real storage device;hard discs;real storage device indirection system shingled recording disk drive shingled magnetic recording hard disk drive random write access data track storage system unrestricted read write request performance optimization disk cache based architecture s block architecture discrete event simulation;random write access;discrete event simulation	Shingled magnetic recording is a promising technology to increase the capacity of hard-disk drives with no significant cost impact. Its main drawback is that random-write access to the disk is restricted due to overlap in the layout of data tracks. For computing and storage systems to enjoy the increased capacity, it is necessary to mitigate these access restrictions, and present a storage device that serves unrestricted read/write requests with adequate performance. This paper proposes two different indirection systems to mask access restrictions and optimize performance. The first one is a diskcache based architecture that provides unrestricted access with manageable drop in performance. A second, more complex indirection system, utilizes a new storage unit called S-block. It is shown that the S-block architecture allows good sustained random-write performance, a point where the disk-cache architecture fails. The organization and algorithms of both architectures are specified in detail. Each was implemented and simulated as a discrete-event simulation, mimicking its operation on real storage devices. For the performance evaluation both synthetic workloads and traces from real workloads were used.	algorithm;computer data storage;dhrystone;file system permissions;hard disk drive;indirection;performance evaluation;random access;random-access memory;shingled magnetic recording;simulation;tracing (software)	Yuval Cassuto;Marco A. A. Sanvido;Cyril Guyot;David R. Hall;Zvonimir Bandic	2010	2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST)	10.1109/MSST.2010.5496971	parallel computing;real-time computing;computer hardware;computer science	HPC	-12.594886447835	52.57731973580614	191661
64f5b37e18980e9aea08628889bbdbaed5b71cca	a case of scientific study application i/o behavior	scientific application;measurement tool;performance evaluation;input output programs;computer aided software engineering application software high performance computing performance analysis laboratories computer science supercomputers data visualization multiprocessor interconnection networks operating systems;input output programs performance evaluation natural sciences computing;memory hierarchy;natural sciences computing;high performance;gflops computations scientific application i o behavior i o demands i o bottleneck problem high performance systems i o activity memory hierarchy measurement tools recurring patterns supercomputer applications	Characterizing the 110 demands of scientific applications is an integral part of the solution to the I t 0 botileneckproblem in high pegormame systems. Knowledge of the type, volume, and frequency of events that occur in common patterns of I t0 activity can provide insigh into the policies and mechanism needed at all levels of the memory hierarchy. This paper is a detailed case study of one scientijc application’s dynamic I t 0 behavior. Results from this study show that with appropriate memuremenr tools, dynamic I t 0 behavior can be characterized and regular, recurring patterns of 110 activity can be isolated.	floor and ceiling functions;input/output;memory hierarchy	Barbara K. Pasquale;George C. Polyzos	1994		10.1109/MASCOT.1994.284439	computational science;computer science;theoretical computer science;operating system	HPC	-16.026068364899476	49.277022410660564	191967
324add87ad00327e01ede4fd2015cd69d49ef55a	a hybrid udp-tcp (rhut) protocol for data replication	storage system;point to point;large dataset;rhut protocol;data replication;protocol incompatibility;hybrid udp tcp;satisfiability;transmission control protocol;point to multipoint replication architecture;transport protocols;data intensive engineering application;grid community;unpublished protocol;point to point replication architecture;geographically distributed application;access protocols distributed computing grid computing costs delay transport protocols data engineering libraries computer architecture availability;point to multipoint replication architecture hybrid udp tcp transmission control protocol transport protocol user datagram protocol rhut protocol data replication data intensive scientific application data intensive engineering application geographically distributed application grid community incompatible protocol unpublished protocol data accessing protocol incompatibility point to point replication architecture;data accessing;transport protocol;incompatible protocol;point of view;data intensive scientific application;user datagram protocol;transport protocols replicated databases;replicated databases;geographic distribution	Data-intensive scientific and engineering applications require both transfers of large amounts of data between storage systems and access to large amounts of data (gigabytes or tera bytes) by many geographically distributed applications. There are already a number of storage systems in use by the grid community, each of which was designed to satisfy specific requirements for storing, transferring and accessing large datasets. Unfortunately, most of these storage systems utilize incompatible and often unpublished protocols for accessing data and therefore use their own client libraries to access data. To overcome these incompatibilities in protocols, different point-to-point, and point-to-multipoint replication architectures based on UDP and/or TCP have been proposed like GridFTP (Allcock, 2003), FPFR (Izmailov et al.), BUDP (He et al., 2002), FTCP (Jin et al., 2004) etc. We have proposed a similar replication scheme called RHUT trying to combine the strengths of these protocols in an optimum way and eliminating their anomalies. This paper presents the architecture of RHUT in detail from an implementation point of view	byte;data-intensive computing;distributed computing;gigabyte;gridftp;library (computing);multipoint ground;point of view (computer hardware company);point-to-point protocol;point-to-multipoint communication;replication (computing);requirement	Imad Hashmi;Muhammad Abdul Qadir;Mohib ur Rehman	2006	IEEE John Vincent Atanasoff 2006 International Symposium on Modern Computing (JVA'06)	10.1109/JVA.2006.1	computer science;database;distributed computing;computer network	HPC	-17.52133833514984	52.779018012735754	192115
3f28d39ded890018d4a89b37181437ebfa811940	design and implementation of the cooperative cache for pvfs	cooperative cache;cluster computing;parallel virtual file system;cooperative caching;high speed networks;design and implementation;message passing;distributed file system;parallel i o;pvfs;high performance;article	Recently, there have been many efforts to get high performance in cluster computing with inexpensive PCs connected through high-speed networks. Some of them were to provide high bandwidth and parallelism in file service using a distributed file system. Other researches for distributed file systems include the cooperative cache that reduces servers’ load and improves overall performance. The cooperative cache shares file caches among clients so that a client can request a file to another client, not to the server, through interclient message passing. In various distributed file systems, PVFS (Parallel Virtual File System) provides high performance with parallel I/O in Linux widely used in cluster computing. However, PVFS doesn't support any file cache facility. This paper describes the design and implementation of the cooperative cache for PVFS (Coopc-PVFS). We show the efficiency of CoopcPVFS in comparison to original PVFS. As a result, the response time of CoopcPVFS is shorter than or similar to that of original PVFS.	benchmark (computing);cache (computing);clustered file system;computer cluster;dce distributed file system;input/output;linux;matrix multiplication;message passing;parallel i/o;parallel virtual file system;parallel computing;response time (technology);server (computing)	In-Chul Hwang;Hojoong Kim;Hanjo Jung;Dong-Hwan Kim;Hojin Ghim;Seung Ryoul Maeng;Jung Wan Cho	2004		10.1007/978-3-540-24685-5_6	self-certifying file system;parallel computing;message passing;computer cluster;computer science;operating system;distributed computing;open;distributed file system	HPC	-14.936652310883606	51.14054925083052	192457
0fd55bdd341f2a59bc6d0da3659167725844e550	virtual memory algorithms (preliminary version)	computer program;virtual memory;random access machine	Computer programs are usually written with the illusion that they will run on something like a random access machine (RAM) [AHU74], with a large memory, all locations of which are equally fast. In practice, this is far from the truth. In large machines, for example, the range of speeds from the fastest memory (registers at about Ions) to the slowest (disks or mass store at IOms or seconds) can bc a factor of a million or even a billion! Machine dcsigncrs attempt IO smooth out this range. to the ~‘xtcnt lhat is technologically feasible, by providing many levels of memory in hctwccn. ‘l’hcsc memory levels m;ry includes one or Iwo lcvcls of c:rThc. main memory, cxpandcd slow , :IIKI drums. I‘hc pro~r;un~. (II’ course, run in virtual memory. ‘l‘hc h;ndwarc and lhc 0pCrillill~ syslclll al-	algorithm;computer data storage;fastest;large hadron collider;random-access machine;random-access memory	Alok Aggarwal;Ashok K. Chandra	1988		10.1145/62212.62227	memory address;uniform memory access;memory footprint;shared memory;computer architecture;parallel computing;base and bounds;computer science;physical address;virtual memory;theoretical computer science;memory protection;overlay;conventional memory;extended memory;memory segmentation;data diffusion machine;memory map;virtual finite-state machine;non-uniform memory access;memory management	Arch	-12.238213609084214	50.39594905090635	192741
560b2ef799ae3ce0efe3e5c0b430f9fa8b37f7b9	tiled virtual memory for unix		M.Ly computer applications require the manipulation of large data arrays. These applÍcations can behave b-adly under a paged virtual memory (vM) system, due to poor memory access _p¿tterns. One solution to this problem is tiling, a technique in which the arays are partitioned into sub-arrays that map one-to-one with VM -pages. Software implementations of tiling have been described in the literature, but none pioviAe the speed and application transparency of a hardware solution. We have implemented a hardware based, tiled VM within a version of the UNIX operating system. Based on a novel memory management unit and supporting kemel software, this tiled VM has proven to be an efficient environment for manipulaiing 2-dimensional arrays of data In this paper we discuss the kemel changes required to implement our tiled VM. We then compare tiled and paged versions of our VM system, and show that tiling results in a 5Q-fold reduction in working set size for a common class of image processing algorithms.	algorithm;binary file;computer data storage;executable;image processing;memory management unit;one-to-one (data model);operating system;programmer;tiling window manager;unix;working set size;z/vm	James Franklin	1992			unix filesystem;memory footprint;operating system;unix;virtual memory;computer science	HPC	-16.61769689788699	46.78991462923831	193357
1b29e04348cc098d1457ad66bc7e56abf48b35a5	the optical file cabinet: a random-access file system for write-once optical disks	file system tree;optical control;reliability;error correction codes;optic disk;information retrieval;file systems optical recording operating systems error correction codes data structures error analysis law legal factors information retrieval optical control;storage management;optical file cabinet;user level software interface;long term storage;law;engineering workstation random access file system write once optical disks optical file cabinet operating system file system file system tree growing data structures large memory requirement reliability user level software interface long term storage tektronix 4404;error analysis;legal factors;growing data structures;operating system;engineering workstation;file system;data structures;storage management data structures file organisation office automation operating systems computers optical disc storage random access storage;optical recording;random access storage;optical disc storage;write once optical disks;large memory requirement;tektronix 4404;data structure;random access;operating systems computers;office automation;random access file system;file systems;operating systems;file organisation	The Optical File Cabinet (OFC) is patterned after a conventional office file cabinet where all versions of a stored item are retained, but the current version of each item is the easiest to find. The OFC retains the favorable aspects of write-once optical disks while preserving the existing relationship between the operating system and the file system. Alternate approaches to write-once storage and the characteristics of write-once optical disks are described. The salient features of the OFC are examined, namely, the file system tree, growing data structures, and the large memory requirement. The relationship between the OFC and the operating system is discussed. Also considered are reliability, the user-level software interface, long-term storage, simulating the OFC, and implementation of the OFC on a Tektronix 4404 engineering workstation.<<ETX>>	data structure;operating system;random access;simulation;user space;workstation;write once, run anywhere	Jason Gait	1988	Computer	10.1109/2.947	data structure;computer file;computer hardware;computer science;stub file;operating system;unix file types;reliability;database;programming language;random access;file control block	OS	-14.489715922128909	52.196435863878826	193608
d587c76c8c35b9e029bb3db5dbf06890a4624f24	using performance measurements to improve mapreduce algorithms	software performance	The Hadoop MapReduce software environment is used for parallel processing of distributively stored data. Data mining algorithms of increasing sophistication are being implemented in MapReduce, bringing new challenges for performance measurement and tuning. We focus on analyzing a job after completion, utilizing information collected from Hadoop logs and machine metrics. Our analysis, inspired by [1] [2], goes beyond conventional Hadoop JobTracker analysis by integrating more data and providing web browser visualization tools. This paper describes examples where measurements helped diagnose subtle issues and improve algorithm performance. Examples demonstrate the value of correlating detailed information that is not usually examined in standard Hadoop performance displays.	algorithm;apache hadoop;data mining;mapreduce;parallel computing;technical standard	Todd Plantenga;Yung Ryn Choe;Ann Yoshimura	2012		10.1016/j.procs.2012.04.210	computer science;operating system;data mining;database;world wide web	HPC	-16.21470917708661	49.45900636229688	193659
8b72d0183554e6cffe6bc2719c59bdc5eac39e1f	performance of a raid prototype	central processing units;prototypes;redundant components;operating system;random processes;memory systems;high throughput;memory bandwidth;memory computers;operating systems computers;disk array;sequential computers;hardware	The RAID group at U.C. Berkeley recently built a prototype disk array. This paper examines the performance limits of each component of the array usiug SCSI bus traces, Sprite operating system traces and user programs.The array performs successfully for a workload of small, random I/O operations, achieving 275 I/Os per second on 14 disks before the Sun4/280 host becomes CPU-limited. The prototype is less successful in delivering high throughput for large, sequential operations. Memory system contention on the Sun4/280 host limits throughput to 2.3 MBytes/sec under the Sprite Operating System. Throughput is also limited by the bandwidth supported by the VME backplane, disk controller and disks, and overheads associated with the SCSI protocol.We conclude that merely using a powerful host CPU and many disks will not provide the full bandwidth possible from disk arrays. Host memory bandwidth and throughput of disk controllers are equally important. In addition, operating systems should avoid unnecessary copy and cache flush operations that can saturate the host memory system.	backplane;cpu cache;central processing unit;disk array;disk controller;input/output;memory bandwidth;operating system;prototype;raid;scsi;sprite;throughput;tracing (software);vmebus	Ann L. Chervenak;Randy H. Katz	1991		10.1145/107971.107991	high-throughput screening;stochastic process;embedded system;parallel computing;disk array;disk operating system;disk controller;computer hardware;disk formatting;computer science;disk array controller;operating system;disk buffer;prototype;redundant array of independent memory;memory bandwidth	Metrics	-12.528283598974916	50.15703929378617	193785
59b64863aa6b80bd085e635051925f52bd4375dc	correlated set coordination in fault tolerant message logging protocols	node increase;fault tolerant message;failure probability;intermediate approach;correlated process;many-core node;costly payload;message payload;correlated set coordination;high overhead;exascale system;current expectation	node increase;fault tolerant message;failure probability;intermediate approach;correlated process;many-core node;costly payload;message payload;correlated set coordination;high overhead;exascale system;current expectation		Aurelien Bouteiller;Thomas Hérault;George Bosilca;Jack J. Dongarra	2011		10.1007/978-3-642-23397-5_6	parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	Theory	-17.851427648114708	49.46361976443436	193828
18e04dbd350ba6dcb5ed10a76476f929b6b5598e	the design and implementation of a log-structured file system	workload;sequences;files records;transfer;perforation;storage management;costs and benefits;recovery;container index;arrays;design and implementation;log structured file system;file system;writing;disk storage array;storage;unix file system;disks	This paper presents a new technique for disk storage management called a log-structured file system. A log-structured file system writes all modifications to disk sequentially in a log-like structure, thereby speeding up both file writing and crash recovery. The log is the only structure on disk; it contains indexing information so that files can be read back from the log efficiently. In order to maintain large free areas on disk for fast writing, we divide the log into segments and use a segment cleaner to compress the live information from heavily fragmented segments. We present a series of simulations that demonstrate the efficiency of a simple cleaning policy based on cost and benefit. We have implemented a prototype log-structured file system called Sprite LFS; it outperforms current Unix file systems by an order of magnitude for small-file writes while matching or exceeding Unix performance for reads and large writes. Even when the overhead for cleaning is included, Sprite LFS can use 70% of the disk bandwidth for writing, whereas Unix file systems typically can use only 5--10%.	cpu cache;central processing unit;computer data storage;disk storage;division by zero;input/output;live file system;log-structured file system;logic file system;overhead (computing);plasma cleaning;prototype;scalability;simulation;sprite;unix	Mendel Rosenblum;John K. Ousterhout	1991		10.1145/121132.121137	fork;self-certifying file system;recovery;torrent file;memory-mapped file;device file;computer file;computer hardware;computer science;cost–benefit analysis;stub file;versioning file system;operating system;fstab;unix file types;journaling file system;sequence;database;file descriptor;open;everything is a file;data file;file system fragmentation;writing;file control block;virtual file system	OS	-13.735926201935783	52.88844536997711	194097
a9eac6bd80638400ac780d49e3013fdac553b65b	improving the i/o performance in the reduce phase of hadoop	mapreduce;filesystem;hadoop	Hadoop is a popular open-source MapReduce implementation. In the cases of jobs wherein all the output files of all the relevant Map tasks are transmitted and consolidated into a single Reduce task, such as in TeraSort, the single Reduce task is the bottleneck task and is I/O bounded for processing many large output files. In most cases, including TeraSort, the intermediate data, which include the output files of the Map tasks, are large and accessed sequentially. For improving the performance of these jobs, it is important to increase the sequential access performance. In this paper, we focus on Hadoop sample job TeraSort, which is a single-Reduce-tasked job, and discuss a method for improving its performance. First, we perform TeraSort and demonstrate that the single Reduce task is the bottleneck task and is I/O bounded. Second, we show the sequential I/O speed of each zone of an HDD. Third, we introduce a static method for improving the performance of such single-Reduce-tasked jobs. The method statically controls block bitmaps of the filesystem and places the intermediate files in a faster zone, i.e., the outer range, of the HDD. Forth, we propose to improve this static method by controlling block bitmap dynamically. Lastly, we present performance evaluation of the proposed method and demonstrate that our method improves the performance.	apache hadoop;bitmap;hard disk drive;input/output;job stream;mapreduce;method (computer programming);open-source software;performance evaluation;sequential access	Eita Fujishima;Saneyasu Yamaguchi	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1145/2857546.2857595	embedded system;parallel computing;real-time computing;computer science;operating system;database	HPC	-13.891241134328414	53.05614226443459	194445
062041ec50dfc5998e4d1ba0f7e475884007b369	efficient locking techniques for databases on modern hardware		Traditional database systems are driven by the assumption that disk I/O is the primary bottleneck, overshadowing all other costs. However, future database systems will be dominated by many-core processors, large main memory, and low-latency semiconductor mass storage. In the increasingly common case that the working data set fits in memory or low-latency storage, new bottlenecks emerge: locking, latching, logging, and critical sections in the buffer manager. Prior work has addressed two of these – latching and logging. This paper addresses locking and proposes new mechanisms optimized for modern hardware. We devised new algorithms and methods to improve all components of database locking, including key range locking, intent locks, detection and recovery from deadlocks, and early lock release. Most of the techniques are easily applicable to other database systems. Experiments with Shore-MT, the transaction processing engine we used as the implementation basis, show throughput improvement by factors of 5 to 50.	algorithm;b+ tree;b-tree;bottleneck (software);box counting;cpu cache;central processing unit;computer data storage;concurrency (computer science);critical section;deadlock;experiment;fits;hash table;input/output;interpolation search;lock (computer science);manycore processor;mass storage;oracle database;page (computer memory);pipeline (computing);read-only memory;scalability;semiconductor;throughput;transaction processing	Hideaki Kimura;Goetz Graefe;Harumi A. Kuno	2012			database;throughput;deadlock;lock (computer science);bottleneck;computer hardware;mass storage;computer science;transaction processing	DB	-12.902021849378766	51.36586534064937	194479
3bda5c54350f49f1d8ba76e70a5279a709d40429	xenloop: a transparent high performance inter-vm network loopback	distributed application;virtual machine;virtual networks;inter vm communication;shared memory;web service;xen;transaction processing;high performance;high speed	Advances in virtualization technology have focused mainly on strengthening the isolation barrier between virtual machines (VMs) that are co-resident within a single physical machine. At the same time, a large category of communication intensive distributed applications and software components exist, such as web services, high performance grid applications, transaction processing, and graphics rendering, that often wish to communicate across this isolation barrier with other endpoints on co-resident VMs. State of the art inter-VM communication mechanisms do not adequately address the requirements of such applications. TCP/UDP based network communication tends to perform poorly when used between co-resident VMs, but has the advantage of being transparent to user applications. Other solutions exploit inter-domain shared memory mechanisms to improve communication latency and bandwidth, but require applications or user libraries to be rewritten against customized APIs - something not practical for a large majority of distributed applications. In this paper, we present the design and implementation of a fully transparent and high performance inter-VM network loopback channel, called XenLoop, in the Xen virtual machine environment. XenLoop does not sacrifice user-level transparency and yet achieves high communication performance between co-resident guest VMs. XenLoop intercepts outgoing network packets beneath the network layer and shepherds the packets destined to co-resident VMs through a high-speed inter-VM shared memory channel that bypasses the virtualized network interface. Guest VMs using XenLoop can migrate transparently across machines without disrupting ongoing network communications, and seamlessly switch between the standard network path and the XenLoop channel. In our evaluation using a number of unmodified benchmarks, we observe that XenLoop can reduce the inter-VM round trip latency by up to a factor of 5 and increase bandwidth by a up to a factor of 6.	loopback	Jian Wang;Kwame-Lante Wright;Kartik Gopalan	2008		10.1145/1383422.1383437	web service;shared memory;embedded system;parallel computing;real-time computing;transaction processing;computer science;virtual machine;operating system;distributed computing;computer network	HPC	-16.747154900622682	51.2970922142253	195174
225c557f0e0d5ae8ce903b04278f7e011ae610c7	projecting the performance of decision support workloads on systems	databases;tpc d;storage units;decision support;robust secure mobile software;application software;software performance evaluation;software systems;decision support workloads;processor sharing;parallel software systems development decision support workload peformance smart storage systems smartstor architecture embedded intelligence offloaded processing robust secure mobile software disk couped processing unit powerful processors processor sharing software architecture speedup multiple relation operations;artificial intelligence decision support systems software performance evaluation software architecture storage units memory architecture embedded systems;computer architecture;embedded systems;software architecture;embedded intelligence;disk couped processing unit;computer architecture hardware robustness application software computer science embedded software performance analysis databases microelectronics sun;decision support workload peformance;memory architecture;smartstor architecture;system design;decision support systems;intelligent storage;performance analysis;sun;database scalability;multiple relation operations;powerful processors;artificial intelligence;robustness;database offloading;smart storage systems;computer science;microelectronics;parallel software systems development;speedup;offloaded processing;database performance;embedded software;hardware	Recent developments in both hardware and software have made it worthwhile to consider embedding intelligence in storage to handle general purpose processing that can be ofloaded from the hosts. In particulal; low-cost processing power is now widely available and software can be made robust, secure and mobile. In this papel; we propose a general Smart Storage (SmartSTOR) architecture in which a processing unit that is coupled to one or more disks can be used to perform such ofloaded processing. A major part of the paper is devoted to understanding the performance potential of the SmartSTOR architecture for decision support workloads. Our analysis suggests that there is a definite performance advantage in using fewer but more powerful processors, a result that bolsters the case for sharing apowerful processor among multiple disks. As for software architecture, we find that the ofloading of database operations that involve only a single relation is not very promising. In order to achieve significant speedup, we have to consider the ofloading of multiple-relation operations. In general, i f embedding intelligence in storage is an inevitable architectural trend, we have to focus on developing parallel software systems that can effectively take advantage of the large number of processing units that will be in the system.	central processing unit;decision support system;robustness (computer science);software architecture;software system;speedup	Windsor W. Hsu;Alan Jay Smith;Honesty C. Young	1999		10.1109/ICPADS.2000.857725	embedded system;software architecture;application software;parallel computing;real-time computing;decision support system;embedded software;database tuning;speedup;computer science;operating system;database;distributed computing;microelectronics;robustness;software system;systems design	Arch	-17.656558535880173	51.186855890182166	195446
a84ee1bb57b6b3bda354a090ce1bfb188af33839	pacifier: record and replay for relaxed-consistency multiprocessors with distributed directory protocol	splash 2 pacifier relaxed consistency multiprocessors distributed directory protocol record and deterministic replay r r multithreaded programs sequential consistency violations scv relog general memory reordering logging and replay mechanism granule release consistency;protocols;memory management;buffer storage;recording multiprocessing programs multi threading;coherence;program processors buffer storage hardware protocols tin coherence memory management;tin;program processors;hardware	Record and Deterministic Replay (R&R) of multithreaded programs on relaxed-consistency multiprocessors with distributed directory protocol has been a long-standing open problem. The independently developed RelaxReplay [8] solves the problem by assuming write atomicity.  This paper proposes Pacifier, the first R&R scheme to provide a solution without assuming write atomicity. R&R for relaxed-consistency multiprocessors needs to detect, record and replay Sequential Consistency Violations (SCV). Pacifier has two key components: (i) Relog, a general memory reordering logging and replay mechanism that can reproduce SCVs in relaxed memory models, and (ii) Granule, an SCV detection scheme in the record phase with good precision, that indicates whether to record with Relog. We show that Pacifier is a sweet spot in the design space with a reasonable trade-off between hardware and log overhead. An evaluation with simulations mof 16, 32 and 64 processors with Release Consistency (RC) running SPLASH-2 applications indicates that Pacifier incurs 3.9% ~ 16% larger logs. The slowdown of Pacifier during replay is 10.1% ~ 30.5% compared to native execution	atomicity (database systems);central processing unit;granule (oracle dbms);machine code;memory model (programming);overhead (computing);release consistency;sequential consistency;simulation;single customer view;thread (computing)	Xuehai Qian;Benjamin Sahelices;Depei Qian	2014	2014 ACM/IEEE 41st International Symposium on Computer Architecture (ISCA)	10.1145/2678373.2665736	communications protocol;computer architecture;parallel computing;real-time computing;coherence;tin;computer science;operating system;programming language;memory management	Arch	-12.257637768620791	50.09224006922976	196662
8185ef04751b888537a8b92f514fbd427ada8a73	a new approach to file system cache writeback of application data	rate flushing;cache writeback;network file system;client server;buffer cache;feedback loop;file system;dirty pages;watermark flushing	In this paper we propose a new paradigm and algorithms to address cache writeback performance in servers and storage arrays. As servers and storage processors move to multi-core architecture, with ever increasing memory caches, the cost of flushing these caches to disk has become a problem. Traditional watermark based algorithms currently used in many storage arrays and NAS servers have a problem keeping up with the higher speeds of incoming application writes, often resulting in a performance penalty. The server's cache is generally used for hiding high disk latencies associated with file system data. In general, metadata performance was optimized, while application data was considered less sensitive to high latencies and was given lower priority or was written directly to disk. The new algorithms proposed here change the application data writeback from using watermark based flush to something that approximates the rate of the incoming application I/Os. The problem is more critical for network file systems where the complex client/server protocols can make writeback a serious performance barrier, particularly in light of very large I/Os and the lack of application commits. Our proposed algorithms are applicable to local file systems and remote servers as well as to storage arrays. We show test results based on dynamic traces of real file system dirty pages in the buffer cache and prove that rate based cache writeback algorithms are the most efficient replacement for watermark based flushing.	algorithm;cpu cache;cache (computing);central processing unit;client–server model;clustered file system;intel core (microarchitecture);multi-core processor;page cache;programming paradigm;server (computing);tracing (software)	Sorin Faibish;Peter Bixby;John Forecast;Philippe Armangau;Sitaram Pawar	2010		10.1145/1815695.1815699	parallel computing;page cache;cache;computer science;operating system;database;cache algorithms	HPC	-13.437052526041704	52.49286753673069	196989
d3c9b7be38f73f7de907cba6bb691f68f3dc9912	a cache coherency strategy for a client/server kbms	client server;cache coherence		cache (computing);cache coherence	Fernando de Ferreira Rezende;Victoria Hall	1997	Datenbank Rundbrief		database;cache coloring;cache invalidation;computer science;smart cache;cache;operating system;mesi protocol;write-once;mesif protocol;cache algorithms	Crypto	-12.60629314471734	50.052220055265096	197052
066dcd039e4c2a27c72a19a70a28475fb7ae557f	making lockless synchronization fast: performance implications of memory reclamation	epoch based reclamation;memory management;microbenchmark lockless synchronization memory reclamation concurrent application multiprocessor dynamic data structure quiescent state based reclamation epoch based reclamation hazard pointer based reclamation;multiprocessor;multiprocessing;storage management;memory reclamation;synchronisation;hazard pointer based reclamation;synchronisation concurrency control data structures multiprocessing systems storage management;lockless synchronization;synchronization;data structures;dynamic data structure;execution environment;concurrency control;global optimization;multiprocessing systems;data structures programming profession yarn linux system recovery computer science application software protection algorithm design and analysis content addressable storage;quiescent state based reclamation;concurrent application;high performance;data structure;algorithm design;microbenchmark	Achieving high performance for concurrent applications on modern multiprocessors remains challenging. Many programmers avoid locking to improve performance, while others replace locks with non-blocking synchronization to protect against deadlock, priority inversion, and convoying. In both cases, dynamic data structures that avoid locking, require a memory reclamation scheme that reclaims nodes once they are no longer in use. The performance of existing memory reclamation schemes has not been thoroughly evaluated. We conduct the first fair and comprehensive comparison of three recent schemes -quiescent-state-based reclamation, epoch-based reclamation, and hazard-pointer-based reclamation - using a flexible microbenchmark. Our results show that there is no globally optimal scheme. When evaluating lockless synchronization, programmers and algorithm designers should thus carefully consider the data structure, the workload, and the execution environment, each of which can dramatically affect memory reclamation performance	benchmark (computing);blocking (computing);data structure;deadlock;dynamic data;dynamization;hazard pointer;lock (computer science);maxima and minima;non-blocking algorithm;pointer (computer programming);polynomial-time approximation scheme;priority inversion;programmer	Thomas E. Hart;Paul E. McKenney;Angela Demke Brown	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639261	embedded system;synchronization;parallel computing;real-time computing;multiprocessing;data structure;computer science;operating system;distributed computing;programming language	Embedded	-13.222125680991617	49.309743405077064	197336
f1486a9f255d53809b4e5a3246600096677a92a6	mpi/io on dafs over via: implementation and performance evaluation	file servers;kernel;access network;comparative analysis;memory management;performance evaluation;information science;computer networks;network file system;direct access file system;computer architecture;data center;low latency;directional data;file system;performance analysis;access protocols;file sharing;high throughput;switches;high performance;data transfer;file systems memory management computer architecture access protocols kernel switches information science performance analysis computer networks file servers;file systems	In this paper, we describe an implementation of MPI-IO on top of the Direct Access File System (DAFS) standard. The implementation is realized by porting ROMIO on top of DAFS. We identify one of the main mismatches between MPI-IO and DAFS is memory management. Three different design alternatives for memory management are proposed, implemented, and evaluated. We find that memory management in the ADIO layer performs better in situations where the DAFS Provider uses Direct data transfer to handle I/O requests. For the other case of Inline data transfer, it may hurt performance. We propose that the DAFS Provider can expose such implementation information for applications to take full advantage of Inline and Direct data transfers and memory management. Comparative analysis of MPI-IO performance over DAFS, network file system (NFS) and local file system (LFS) shows that MPI-IO on DAFS over VIA on cLAN performs 1.6-5.6 times better than on NFS over UDP/IP on cLAN. The performance of MPI-IO on DAFS is found to be comparable to the performance on local file system. Additional experiments show that MPI-IO nonblocking I/O primitives implemented by DAFS nonblocking operations can completely overlap I/O and computation. These results show that MPI-IO on DAFS can take full advantage of DAFS features to achieve high performance I/O over VI Architectures.	client-side;clustered file system;coherence (physics);computation;direct access file system;experiment;file sharing;input/output;live file system;memory management;performance evaluation;random-access memory;serial ata;socket.io;user space	Jiesheng Wu;Dhabaleswar K. Panda	2002		10.1109/IPDPS.2002.1016566	high-throughput screening;qualitative comparative analysis;file server;data center;parallel computing;kernel;information science;network switch;network file system;computer science;operating system;database;distributed computing;file sharing;computer network;memory management;access network;low latency	OS	-17.301630682321946	52.17894780190922	197464
e001b51acb757d6c23a31f75d71ac125c01a7a0b	volap: a scalable distributed real-time olap system for high-velocity data		This paper presents VelocityOLAP (VOLAP), a distributed real-time OLAP system for high-velocity data. VOLAP makes use of dimension hierarchies, is highly scalable, exploits both multi-core and multi-processor parallelism, and can guarantee serializable execution of insert and query operations. In contrast to other high performance OLAP systems such as SAP HANA or IBM Netezza that rely on vertical scaling or special purpose hardware, VOLAP supports cost-efficient horizontal scaling on commodity hardware or modest cloud instances. Experiments on 20 Amazon EC2 nodes with TPC-DS data show that VOLAP is capable of bulk ingesting data at over 600 thousand items per second, and processing streams of interspersed insertions and aggregate queries at a rate of approximately 50 thousand insertions and 20 thousand aggregate queries per second with a database of 1 billion items. VOLAP is designed to support applications that perform large aggregate queries, and provides similar high performance for aggregations ranging from a few items to nearly the entire database.	aggregate data;amazon elastic compute cloud (ec2);commodity computing;cost efficiency;ibm tivoli storage productivity center;image scaling;multi-core processor;multiprocessing;online analytical processing;parallel computing;real-time transcription;sap hana;scalability;serializability;velocity (software development)	Frank Dehne;David Edward Robillard;Andrew Rau-Chaplin;Neil Burke	2018	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2017.2743072	sap hana;streams;online analytical processing;computer science;distributed computing;cloud computing;scalability;database;distributed database;ranging;queries per second	DB	-15.559245628401497	53.13835988074477	197888
95228c931949b3d6cb709ba36a53f419fa7b8253	a case against tiny tasks in iterative analytics		Big data systems such as Spark are built around the idea of splitting an iterative parallel program into tiny tasks with other aspects of system design built around this basic design principle. Unfortunately, in spite of immense engineering effort, tiny tasks have unavoidably large overheads. We use the example of logistic regression -- a common machine learning primitive -- to compare the performance of Spark to different designs that converge to a hand-coded parallel MPI-based implementation. We conclude that Spark leaves orders of magnitude performance on the table, due to its insistence on setting the granularity of a task to a single iteration. We counter a common argument for the tiny task approach --namely better resilience to faults -- by demonstrating that optimum job checkpoint intervals are far longer than the duration of the tiny tasks favored in Spark's design. We propose an alternative approach that relies on an auto-parallelizing compiler tightly integrated with the MPI runtime, illustrating the opposite end of the spectrum where task granularities are as large as possible.	automatic parallelization;big data;compiler;converge;data center;data system;iteration;job stream;logistic regression;machine learning;programming language;scheduling (computing);service-level agreement;systems design;toolchain;transaction processing system	Ehsan Totoni;Subramanya R. Dulloor;Amitabha Roy	2017		10.1145/3102980.3103004	compiler;just-in-time compilation;systems design;spark (mathematics);real-time computing;granularity;big data;spite;computer science;analytics;distributed computing	HPC	-16.089224528756855	50.84455352805602	198698
693996c91cc9cb7ba26c8dceaa5c0c4d79c209e4	a management architecture of cloud server systems	servers field programmable gate arrays ip networks system on chip multiprocessor interconnection switches;servers;system on chip;ip networks;field programmable gate arrays;integrated switch management architecture cloud server systems cloud computing data centers power consumption reconfigurable architecture lan 2d torus interconnection network distributed dhcp servers distributed arp proxies;switches;power aware computing cloud computing computer centres file servers local area networks;multiprocessor interconnection	With the development of the cloud computing, the servers we use today are not suitable for data centers very well because of high power consumption and low density. In this paper, we propose a cloud server of low power consumption, high density and good scalability. We employ a reconfigurable architecture to build the cloud server and we can change or update the architecture of the cloud server from remote easily. Furthermore, we build a LAN upon a 2D torus interconnection network using distributed DHCP servers and distributed ARP proxies cooperated with an integrated switch. The experimental result of the prototype shows the availability and high throughput of the network in the cloud server system.	associative entity;cloud computing;data center;experiment;interconnection;mac address;prototype;scalability;server (computing);throughput;torus interconnect;virtual private server	Hua Nie;Gongbo Li;Xingkui Liu;Xiaojun Yang;Keping Long	2014	2014 IEEE 20th International Conference on Embedded and Real-Time Computing Systems and Applications	10.1109/RTCSA.2014.6910544	system on a chip;embedded system;real-time computing;single-chip cloud computer;cloud computing;network switch;computer science;operating system;field-programmable gate array;client–server model;server;computer network;server farm	HPC	-18.32438221691473	52.56483822390788	198767
9376a2d69d06e39fd6fd27c9ce2f0817cc1dd4ef	delegated persist ordering	persistent memory;delegated ordering;relaxed consistency;memory persistency	Systems featuring a load-store interface to persistent memory (PM) are expected soon, making in-memory persistent data structures feasible. Ensuring persistent data structure recoverability requires constraints on the order PM writes become persistent. But, current memory systems reorder writes, providing no such guarantees. To complement their upcoming 3D XPoint memory, Intel has announced new instructions to enable programmer control of data persistence. We describe the semantics implied by these instructions, an ordering model we call synchronous ordering. Synchronous ordering (SO) enforces order by stalling execution when PM write ordering is required, exposing PM write latency on the execution critical path. It incurs an average slowdown of 7.21x over volatile execution without ordering in PM-write-intensive benchmarks. SO tightly couples enforcing order and flushing writes to PM, but this tight coupling is unneeded in many recoverable software systems. Instead, we propose delegated ordering, wherein ordering requirements are communicated explicitly to the PM controller, fully decoupling PM write ordering from volatile execution and cache management. We demonstrate that delegated ordering can bring performance within 1.93x of volatile execution, improving over SO by 3.73x.	3d xpoint;benchmark (computing);commitment ordering;coupling (computer programming);critical path method;directory (computing);feedback;ibm notes;in-memory database;order by;persistent data structure;persistent memory;programmer;requirement;schedule (computer science);serializability;software system;snoop	Aasheesh Kolli;Jeff Rosen;Stephan Diestelhorst;Ali G. Saidi;Steven Pelley;Sihang Liu;Peter M. Chen;Thomas F. Wenisch	2016	2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1109/MICRO.2016.7783761	persistence;embedded system;parallel computing;real-time computing;computer science;memory ordering	Arch	-14.02195359758906	49.51284450322301	198982
1dd7135deda0243ad5995e0b2e3983e9c69c21b4	communication overhead on distributed memory machines		Distributed memory machines (DMMs) can provide scalable and flexible high performance if the relatively high startup overhead on these machines can be amortised. This paper studies communication overhead on DMMs. Based on LogP , a communication cost model is developed to quantify separately the effects of send, receive and network contention on overall communication overhead. This cost model is applied to estimate communication overhead for four commercial multicomputer systems. Two observations for send and receive overheads on DMMs are discussed.	distributed memory	Shiping Chen;Jingling Xue	2001	Scalable Computing: Practice and Experience		parallel computing;real-time computing;computer science;distributed computing;overhead	HPC	-14.290452022643684	47.158744911670134	199329
