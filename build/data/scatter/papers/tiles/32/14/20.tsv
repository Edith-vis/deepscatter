id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
2e961d786d1817d64669a4454a641c59cbee2406	implementing ada tasking in a multiprocessing, multithreaded unix environment	implementing ada tasking;multithreaded unix environment	The Ada language was introduced in the early 80s to reduce software development and maintenance costs, improve softwiue reusability, and provide a standard language to support real-time and multiprocessing system development. The language was primarily intended for time-critical embedded military computer applications, aithough it is also a generaiPLUPO* lan~e supporting systems and applications programming, scientiilc programmingg, and commercial mltime system programming. Recent mandates have strengthened the U.S. Government commitment to the Ada language. P.L 101-511 (Nov. ’90) says that as of July 1991, aii DoD software shaii be written in Ada, except as waived by the secretary of Defense or his representative. NASA is using Ada for most new mission-critical applications and the standard language for ATC applications, such as AAS, etc., is Ada. In addition, the use of Ada is growing in the non-govermnent sector. The increasing use of Ada for reai-dme systems makes it extremely important to have efficient implementations of the language.	ada;advanced tactical center;computer;embedded system;masscomp;military computers;mission critical;multiprocessing;real-time computing;real-time transcription;requirement;software development;system programming;thread (computing);uniprocessor system;unix;window of opportunity	Karen L. Sielski	1992		10.1145/143557.144007	unix architecture;computer architecture;parallel computing;operating system	SE	-16.413554632740055	39.20902014161445	145114
96382d241e7de5c79621274caae4b2f16cc379dd	selector: a language construct for developing dynamic applications	lenguaje programacion;compilacion;haute performance;algorithmique;programming language;distributed computing;algorithmics;algoritmica;alto rendimiento;langage programmation;compilation;calculo repartido;high performance;calcul reparti	Fitting algorithms to meet input data characteristics and/or a changing computing environment is a tedious and error prone task. Programmers need to deal with code instrumentation details and implement the selection of which algorithm best suits a given data set. In this paper we describe a set of simple programming constructs for C that allows programmers to specify and generate applications that can select at run-time the best of several possible implementations based on measured run-time performance and/or algorithmic input values. We describe the application of this approach to a realistic linear solver for an engineering crash analysis code. The preliminary experimental results reveal that this approach provides an e ective mechanism for creating sophisticated dynamic application behavior with minimal e ort.	algorithm;cognitive dimensions of notations;compiler;curve fitting;high- and low-level;language construct;mikumikudance;numerical linear algebra;open road tolling;performance evaluation;programmer;solver	Pedro C. Diniz;Bing Liu	2002		10.1007/11596110_15	parallel computing;computer science;theoretical computer science;programming language;algorithmics;algorithm	PL	-17.749312879084812	38.56388872815202	145641
324a6be00ded100634cbd46caf2db80d03b5b963	future contracts	contracts;behavioral specifications;higher order functions;software reliability	Many recent research projects focus on language support for behavioral software contracts, that is, assertions that govern the boundaries between software building blocks such as procedures, classes, or modules. Contracts primarily help locate bugs in programs, but they also tend to affect the performance of the program, especially as they become complex.  In this paper, we introduce future contracts and parallel contract checking: software contracts annotated with future are checked in parallel with the main program, exploiting the now-common multiple-core architecture. We present both a model and a prototype implementation of our language design. Our model comprises a higher-order imperative language and we use it to prove the correctness of our design. Our implementation is robust enough to measure the performance of reasonably large benchmarks, demonstrating that the use of future contracts can lead to significant performance improvements.	assignment (computer science);benchmark (computing);computation;correctness (computer science);design by contract;entry point;imperative programming;intel core (microarchitecture);lisp;prototype;scheme;shared memory;software bug;speedup	Christos Dimoulas;Riccardo Pucella;Matthias Felleisen	2009		10.1145/1599410.1599435	simulation;computer science;programming language;higher-order function;software quality	PL	-17.28674579793818	32.673015749068774	146121
60f38ce211d342aa63fa72bf08b1a141cffa0c9e	parametric strategy iteration		Program behavior may depend on parameters, which are either configured before compilation time, or provided at runtime, e.g., by sensors or other input devices. Parametric program analysis explores how different parameter settings may affect the program behavior. In order to infer invariants depending on parameters, we introduce parametric strategy iteration. This algorithm determines the precise least solution of systems of integer equations depending on surplus parameters. Conceptually, our algorithm performs ordinary strategy iteration on the given integer system for all possible parameter settings in parallel. This is made possible by means of region trees to represent the occurring piecewise affine functions. We indicate that each required operation on these trees is polynomial-time if only constantly many parameters are involved. Parametric strategy iteration for systems of integer equations allows to construct parametric integer interval analysis as well as parametric analysis of differences of integer variables. It thus provides a general technique to realize precise parametric program analysis if numerical properties of integer variables are of concern.	algorithm;compiler;experiment;fragmentation (computing);input device;interval arithmetic;iteration;mathematical optimization;numerical analysis;program analysis;quantifier (logic);run time (program lifecycle phase);sensor;time complexity	Thomas Gawlitza;Martin D. Schwarz;Helmut Seidl	2014			mathematical optimization;parametric model	Logic	-15.988111583642386	32.62240930215758	146966
9c0756b48c2620c0ef75d0a3c71a6a3e02901825	generalized profile-guided iterator recognition		Iterators prescribe the traversal of data structures and determine loop termination, and many loop analyses and transformations require their exact identification. While recognition of iterators is a straight-forward task for affine loops, the situation is different for loops iterating over dynamic data structures or involving control flow dependent computations to determine the next data element. In this paper we propose a compiler analysis for recognizing loop iterators code for a wide class of loops. We initially develop a static analysis, which is then enhanced with profiling information to support speculative code optimizations. We have prototyped our analysis in the LLVM framework and demonstrate its capabilities using the SPEC CPU2006 benchmarks. Our approach is applicable to all loops and we show that we can recognize iterators in, on average, 88.1% of over 75,000 loops using static analysis alone, and up to 94.9% using additional profiling information. Existing techniques perform substantially worse, especially for C and C++ applications, and cover only 35-44% of the loops. Our analysis enables advanced loop optimizations such as decoupled software pipelining, commutativity analysis and source code rejuvenation for real-world applications, which escape analysis and transformation if loop iterators are not recognized accurately.	c++;computation;control flow;data element;data structure;dynamic data;dynamization;escape analysis;fortran;iterator;llvm;loop optimization;mathematical optimization;optimizing compiler;parallel computing;pipeline (computing);prototype;rewriting;software pipelining;speculative execution;static program analysis;tree traversal	Stanislav Manilov;Christos Vasiladiotis;Björn Franke	2018		10.1145/3178372.3179511	parallel computing;compiler;discrete mathematics;source code;computer science;profiling (computer programming);control flow;software pipelining;data structure;escape analysis;iterator	PL	-18.060765506510055	34.95699586271836	147817
97a236836489b48b76b0fe455dcbe6978c8e5a4a	i-structures: data structures for parallel computing	parallel calculus;langage fonctionnel;parallelisme;lenguaje programacion;programming language;lenguaje funcional;operational semantics;concurrent program;functional programming;parallelism;calculo paralelo;paralelismo;estructura datos;programa competidor;parallel computer;langage programmation;structure donnee;functional data;technical report;computer science;functional language;calcul parallele;data structure;large data;programme concurrent;structured data	It is difficult to achieve elegance, efficiency, and parallelism simultaneously in functional programs that manipulate large data structures. We demonstrate this through careful analysis of program examples using three common functional data-structuring approaches-lists using Cons, arrays using Update (both fine-grained operators), and arrays using make-array (a “bulk” operator). We then present I-structure as an alternative and show elegant, efficient, and parallel solutions for the program examples in Id, a language with I-structures. The parallelism in Id is made precise by means of an operational semantics for Id as a parallel reduction system. I-structures make the language nonfunctional, but do not lose determinacy. Finally, we show that even in the context of purely functional languages, I-structures are invaluable for implementing functional data abstractions.	functional programming;indeterminacy in concurrent computation;linked data structure;operational semantics;parallel computing;rewriting	Arvind;Rishiyur S. Nikhil;Keshav Pingali	1989	ACM Trans. Program. Lang. Syst.	10.1145/69558.69562	data model;computer science;technical report;theoretical computer science;data parallelism;programming language;functional programming;operational semantics;algorithm	PL	-17.76919709511161	32.66955393713907	147858
0f793cdd785ce39ad18e0f910a3f39bb7a34b446	using static single assignment form in a code optimizer	code optimization;register allocation;code generation;data dependence;power optimization;peephole optimization;static single assignment form	Static single assignment form represents data dependences elegantly and provides a basis for powerful optimizations. Table-driven techniques for peephole optimization and code generation are straightforward and effective. it is natural to want to use both together in a code optimizer. However, doing so reveals that static single assignment form does not remove all antidependences, and that it conflicts with table-driven code generation for 2-address machines. This paper describes these problems and how to solve them.	code generation (compiler);decision table;mathematical optimization;peephole optimization;program optimization;static single assignment form	Carl McConnell;Ralph E. Johnson	1992	LOPLAS	10.1145/151333.151368	dead code;parallel computing;real-time computing;peephole optimization;computer science;theoretical computer science;dead code elimination;program optimization;programming language;static single assignment form;register allocation;power optimization;code generation;unreachable code	PL	-17.87876400952909	34.925611347467054	148071
d8223bb59b0fccbff5ca432d58d59084bb25aa81	data management and control-flow aspects of an simd/spmd parallel language/compiler	processing element;index termsdata management;parallel machine prototype data management control flow aspects simd spmd parallel language compiler explicitly parallel programming language reconfigurable parallel processing systems processing elements single program multiple data mode version data dependent control flow pe address dependent control flow;pe address dependent control flow;reconfigurable parallel processing systems;fault tolerant;explicitly parallel programming language;database management systems;simd spmd parallel language compiler;data management;parallel programming;parallel machine prototype;processing elements;parallel programming language;data dependence;control flow;mixed mode;parallel machines;control flow aspects;mode version;data dependent control flow;program compilers;parallel languages parallel processing program processors parallel programming parallel machines concurrent computing control systems prototypes fault tolerance broadcasting;parallel languages;parallel processing;single program multiple data;program compilers database management systems parallel languages parallel programming	Features of an explicitly parallel programming language targeted for reconfigurable parallel processing systems, where the machine's -1processing elements (PE's) are capable of operating in both the SIMD and SPMD modes of parallelism, are described. The SPMD (Single Program-Multiple Data) mode of parallelism is a subset of the MIMD mode where all processors execute the same program. By providing all aspects of the language with an SIMD mode version and an SPMD mode version that are syntactically and semantically equivalent, the language facilitates experimentation with and exploitation of hybrid SlMDiSPMD machines. Language constructs (and their implementations) for data management, data-dependent control-flow, and PE-address dependent control-flow are presented. These constructs are based on experience gained from programming a parallel machine prototype, and are being incorporated into a compiler under development. Much of the research presented is applicable to general SIMD machines and MIMD machines. Zndex TermsCompilers, fault tolerance, languages, mixedmode parallelism, parallel processing, PASM, reconfigurable machines, SIMD, SPMD.	central processing unit;compiler;control flow;data dependency;fault tolerance;mimd;parallel computing;parallel language;parallel programming model;programming language;prototype;simd;spmd	Mark A. Nichols;Howard Jay Siegel;Henry G. Dietz	1993	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.207596	parallel processing;fault tolerance;computer architecture;parallel computing;data management;computer science;programming language;spmd;control flow;parallel programming model	HPC	-13.53204924989218	38.116682122967184	148198
4e2a36b06f4ea8315555e52db8c3cfc455141df8	parallel pointer analysis with cfl-reachability	vectors;scheduling;multicore processing;context;algorithm design and analysis;parallel processing;java	This paper presents the first parallel implementation of pointer analysis with Context-Free Language (CFL) reachability, an important foundation for supporting demand queries in compiler optimisation and software engineering. Formulated as a graph traversal problem (often with context- and field-sensitivity for desired precision) and driven by queries (issued often in batch mode), this analysis is non-trivial to parallelise. We introduce a parallel solution to the CFL-reachability-based pointer analysis, with context- and field-sensitivity. We exploit its inherent parallelism by avoiding redundant graph traversals with two novel techniques, data sharing and query scheduling. With data sharing, paths discovered in answering a query are recorded as shortcuts so that subsequent queries will take the shortcuts instead of re-traversing its associated paths. With query scheduling, queries are prioritised according to their statically estimated dependences so that more redundant traversals can be further avoided. Evaluated using a set of 20 Java programs, our parallel implementation of CFL-reachability-based pointer analysis achieves an average speedup of 16.2X over a state-of-the-art sequential implementation on 16 CPU cores.	batch processing;central processing unit;context-free language;courant–friedrichs–lewy condition;graph rewriting;graph traversal;java;mathematical optimization;multi-core processor;optimizing compiler;parallel computing;pointer (computer programming);pointer analysis;reachability;scheduling (computing);software engineering;speedup;tree traversal	Yu Su;Ding Ye;Jingling Xue	2014	2014 43rd International Conference on Parallel Processing	10.1109/ICPP.2014.54	multi-core processor;parallel processing;algorithm design;parallel computing;computer science;theoretical computer science;operating system;escape analysis;distributed computing;programming language;java;scheduling	SE	-18.874052803003604	34.507156863697986	148522
d49258bd417a5ca32550d49a5f8c06021f4d57ed	lift: a functional data-parallel ir for high-performance gpu code generation		Parallel patterns (e.g., map, reduce) have gained traction as an abstraction for targeting parallel accelerators and are a promising answer to the performance portability problem. However, compiling high-level programs into efficient low-level parallel code is challenging. Current approaches start from a high-level parallel IR and proceed to emit GPU code directly in one big step. Fixed strategies are used to optimize and map parallelism exploiting properties of a particular GPU generation leading to performance portability issues. We introduce the LIFT IR, a new data-parallel IR which encodes OpenCL-specific constructs as functional patterns. Our prior work has shown that this functional nature simplifies the exploration of optimizations and mapping of parallelism from portable high-level programs using rewrite-rules. This paper describes how LIFT IR programs are compiled into efficient OpenCL code. This is non-trivial as many performance sensitive details such as memory allocation, array accesses or synchronization are not explicitly represented in the LIFT IR. We present techniques which overcome this challenge by exploiting the pattern's high-level semantics. Our evaluation shows that the LIFT IR is flexible enough to express GPU programs with complex optimizations achieving performance on par with manually optimized code.		Michel Steuwer;Toomas Remmelg;Christophe Dubach	2017	2017 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)		parallel processing;synchronization;parallel computing;real-time computing;computer science;resource management;theoretical computer science;operating system;semantics;programming language;divergence	Arch	-15.321746635800336	36.71129392294993	149107
92c4ef6aa17c93bc30f95e6eede34c41342dc950	compactly representing parallel program executions	shared memory;code optimization;program comprehension;path profiling;dynamic program;dynamic program analysis;compact representation;data races;program debugging;data flow;parallel programs;program path compression;compact scheme	Collecting a program's execution profile is important for many reasons: code optimization, memory layout, program debugging and program comprehension. Path based execution profiles are more detailed than count based execution profiles, since they present the order of execution of the various blocks in a program: modules, procedures, basic blocks etc. Recently, online string compression techniques have been employed for collecting compact representations of sequential program executions. In this paper, we show how a similar approach can be taken for shared memory parallel programs. Our compaction scheme yields one to two orders of magnitude compression compared to the uncompressed parallel program trace on some of the SPLASH benchmarks. Our compressed execution traces contain detailed information about synchronization and control/data flow which can be exploited for post-mortem analysis. In particular, information in our compact execution traces are useful for accurate data race detection (detecting unsynchronized shared variable accesses that occurred in the execution).	basic block;data compaction;dataflow;debugging;grammar-based code;mathematical optimization;memory map;parallel computing;program comprehension;program optimization;race condition;sensor;shared variables;shared memory;tracing (software)	Ankit Goel;Abhik Roychoudhury;Tulika Mitra	2003		10.1145/781498.781530	program analysis;shared memory;data flow diagram;parallel computing;real-time computing;computer science;program optimization;dynamic program analysis;programming language;program animation	Arch	-18.51065376187806	34.85405915189776	149741
68e47435b13d280410ac3178c340bbe4c0e5b9e3	an interpreter for server-side hop	programming language;implementation;functional languages;interpreter;functional language;scheme	HOP is a Scheme-based multi-tier programming language for the Web. The client-side of a program is compiled to JavaScript, while the server-side is executed by a mix of natively compiled code and interpreted code. At the time where HOP programs were basic scripts, the performance of the server-side interpreter was not a concern; an inefficient interpreter was acceptable. As HOP expanded, HOP programs got larger and more complex. A more efficient interpreter was necessary. This new interpreter is described in this paper. It is compact, its whole implementation counting no more than 2.5 KLOC. It is more than twice faster than the old interpreter and consumes less than a third of its memory. Although it cannot compete with static or JIT native compilers, our experimental results show that it is amongst the fastest interpreters for dynamic languages.	client-side;compiler;fastest;interpreter (computing);javascript;just-in-time compilation;multitier architecture;programming language;scheme;server (computing);server-side;source lines of code;world wide web	Bernard P. Serpette;Manuel Serrano	2011		10.1145/2047849.2047851	computer architecture;parallel computing;scheme;interpreter pattern;interpreter;computer science;programming language;functional programming;implementation;interpreted language	PL	-18.98121641605445	35.12056674079994	149944
2bb68dce70d55d542f5aa0dd9a1292f37f5e5f60	msl: a synthesis enabled language for distributed implementations	fortran synthesis enabled language distributed implementation generative programming software synthesis bulk synchronous distributed memory kernels msl language c like language synthesis features high level notations array manipulation bulk synchronous parallelism semantic analysis automated bug finding technology computational kernels nontrivial distributed kernels nas parallel benchmark low level c code code generation;multilevel preconditioners;geodynamics;matrix free;vectorization;stokes;storage management c language distributed memory systems operating system kernels parallel programming program compilers program debugging;arrays programming generators kernel semantics synthesizers benchmark testing;variable viscosity	This paper demonstrates how ideas from generative programming and software synthesis can help support the development of bulk-synchronous distributed memory kernels. These ideas are realized in a new language called MSL, a C-like language that combines synthesis features with high level notations for array manipulation and bulk-synchronous parallelism to simplify the semantic analysis required for synthesis. The paper shows that by leveraging these high level notations, it is possible to scale the synthesis and automated bug-finding technologies that underlie MSL to realistic computational kernels. Specifically, we demonstrate the methodology through case studies implementing non-trivial distributed kernels---both regular and irregular---from the NAS parallel benchmarks. We show that our approach can automatically infer many challenging details from these benchmarks and can enable high level implementation ideas to be reused between similar kernels. We also demonstrate that these high level notations map easily to low level C code and show that the performance of this generated code matches that of handwritten Fortran.	automatic programming;benchmark (computing);distributed memory;fortran;high-level programming language;nas parallel benchmarks;parallel computing	Zhilei Xu;Shoaib Kamil;Armando Solar-Lezama	2014	SC14: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1109/SC.2014.31	parallel computing;computer science;theoretical computer science;operating system;vectorization;distributed computing;programming language;geodynamics;algebra	HPC	-13.279243321652306	37.934619792443705	150022
3059d5e309f01a1dd24ebd6a0625cf78ee00c0a8	automated programmable control and parameterization of compiler optimizations	optimising compilers;scientific application;parameterized scripts;optimized production technology;optimized code;portable high performance;programmable controllers;autogenerated poet scripts;code optimization;optimizing compiler;domain specific optimization;flexible parameterization;programmable control;program transformation;rose optimizer;interpreted program transformation language;arrays delay optimized production technology programmable control optimizing compilers;arrays;c language;compiler optimization;programmable controllers c language fortran optimising compilers;c c fortran source to source optimizing compiler;fortran;automated programmable control;optimizing compilers;high performance;domain specificity;rose optimizer automated programmable control compiler optimization flexible parameterization portable high performance c c fortran source to source optimizing compiler optimized code parameterized scripts interpreted program transformation language domain specific optimization autogenerated poet scripts	We present a framework which effectively combines programmable control by developers, advanced optimization by compilers, and flexible parameterization of optimizations to achieve portable high performance. We have extended ROSE, a C/C++/Fortran source-to-source optimizing compiler, to automatically analyze scientific applications and discover optimization opportunities. Instead of directly generating optimized code, our optimizer produces parameterized scripts in POET, an interpreted program transformation language, so that developers can freely modify the optimization decisions by the compiler and add their own domain-specific optimizations if necessary. The auto-generated POET scripts support extra optimizations beyond those available in the ROSE optimizer. Additionally, all the optimizations are parameterized at an extremely fine granularity, so the scripts can be ported together with their input code and automatically tuned for different architectures. Our results show that this approach is highly effective, and the code optimized by the auto-generated POET scripts can significantly outperform those optimized using the ROSE optimizer alone.	optimizing compiler;program transformation;rose;transformation language	Qing Yi	2011	International Symposium on Code Generation and Optimization (CGO 2011)	10.1109/CGO.2011.5764678	computer architecture;parallel computing;computer science;operating system;optimizing compiler;programming language	PL	-16.816542183285375	36.2968242625266	150880
0797ea42946d4fdbce7b4ce657ddfd5c3d60971a	compiling c for the earth multithreaded architecture	c language;parallel architectures;program compilers;earth multithreaded architecture;benchmark programs;compiler;compiler support;high-level c programs;irregular locality;irregular parallelism;multithreaded execution model;earth;high level languages;computer architecture;parallel processing;multithreading	Laurie J. Hendren, Xinan Tang, Yingchun Zhu, Shereen Ghobrial, Guang R. Gao, Xun Xue, Haiying Cai and Pierre Ouellet School of Computer Science McGill University Montreal, Quebec, CANADA H3A 2A7 Abstract Multithreaded architectures provide an opportunity for e ciently executing programs with irregular parallelism and/or irregular locality. This paper presents a strategy that makes use of the multithreaded execution model without exposing multithreading to the programmer. Our approach is to design simple extensions to C, and to provide compiler support that automatically translates high-level C programs into lower-level threaded programs. In this paper we present EARTH-C, our extended C language which contains simple constructs for specifying control parallelism, data locality, shared variables and atomic operations. Based on EARTH-C, we describe compiler techniques that are used for translating to lower-level Threaded-C programs for the EARTH multithreaded architecture. We demonstrate our approach with six benchmark programs. We show that even naive EARTH-C programs can lead to reasonable performance, and that more advanced EARTH-C programs can give performance very close to hand-coded threaded-C programs.	benchmark (computing);compiler;computer science;high- and low-level;linearizability;locality of reference;multithreading (computer architecture);parallel computing;programmer;shared variables;task parallelism;thread (computing)	Laurie J. Hendren;Xinan Tang;Yingchun Zhu;Guang R. Gao;Xun Xue;Haiying Cai;Pierre Ouellet	1996		10.1109/PACT.1996.552551	parallel processing;computer architecture;compiler;parallel computing;multithreading;computer science;operating system;earth;programming language;scheduling;high-level programming language;code generation	PL	-12.752181340384947	38.18365498406183	150997
e36927670231cfcd11d41847026498b0c272c6a0	supporting unbounded process parallelism in the spc programming model	data parallel;performance evaluation;parallel programming;programming model;process algebraic framework unbounded process parallelism spc programming model automatic mapping parallel programs target parallel machines compile time cost estimation programming model compile time transformation unbounded algorithm level parallelism;parallel machines;parallel programming model;cost estimation;parallel programs;process algebra;performance evaluation parallel programming parallel machines;parallel programming parallel processing automatic programming cost function computational efficiency concurrent computing parallel languages pipeline processing high performance computing prediction methods	In automatic mapping of parallel programs to target parallel machines the efficiency of the compile-time cost estimation needed to steer the optimization process is highly dependent on the choice of programming model. Recently a new parallel programming model, called SPC, has been introduced that specifically aims at the efficient computation of reliable cost estimates, paving the way for automatic mapping. In SPC all algorithm level parallelism is explicitly specified, relying on compile-time transformation of the possibly unbounded algorithm level (data) parallelism to that of the actual target machine. In this paper we present SPC's process-algebraic framework in terms of which we demonstrate that the transformations needed to efficiently support unbounded process parallelism at program level are straightforward.	parallel computing;programming model	Arjan J. C. van Gemund	1997		10.1109/HIPC.1997.634488	process calculus;parallel computing;declarative programming;embarrassingly parallel;programming domain;reactive programming;computer science;theoretical computer science;operating system;distributed computing;data parallelism;programming paradigm;inductive programming;programming language;implicit parallelism;cost estimate;task parallelism;cost efficiency;parallel programming model	HPC	-15.043992439994353	38.11947798205235	151046
c4fc7f7d5a548e608cfa4f577867ea8ddfe43925	data structures: pointers vs. arrays.when, where and why	data dependency;comparative analysis;multiprocessor;statement parallelism dynamic detection;lines of code;pascal like program;binary search tree;memory allocation;parallel execution;data structure;novice programmer	In the inevitable search for the “perfect” structure, the beginning programmer is faced with a multitude of possibilities. Stacks, arrays, queues, linked lists, trees, and, on a higher level, the decision whether the “best” choice would involve a physical implementation of arrays or pointers. The choice, particularly for the novice, is not easy. The purpose of the abstract is to facilitate that choice. Having perused a dozen textbooks dealing with introduction to data structures (the greater majority of which employ Pascal) one finds the “expected” comparative analysis between dynamic and static variables. The prevalent tendency indicates that the utilization of pointer (dynamic) variables more effectively controls memory allocation with the result being a generally more effective, possibly more expedient execution. The strange occurrence that I could not help the questioning was involved with the reasons why most treatments of data structures devote more time to the static approach as opposed to the dynamic one. And, on a different note, why so few textbooks actually compared the two parallel approaches for the same problem. To alleviate this, I considered solving a simple problem both ways. We wish to create a binary search tree from a list of up to 50000 numbers and, in addition, remove any duplicates from the list. A reasonable approach to this would be to statically declare an array of 50000 nodes, using subroutines to “getnode” (using an available pool of nodes created within the program), create a tree (i.e., the root), setleft and setright (to properly place an item in the tree). Such a program involves 5 modules, about 80 lines of code, a run time of under 2 seconds, 1089 page faults, and a peak page file (indicating the use of memory) of 3253. On the other hand, if one writes the program using pointer variables, the “getnode” procedure mentioned above can be replaced by a simple “new” function (for obtaining a new node), no storage for the numbers is required within the program and we see the following results: 72 lines of code, 4 modules, a run time of under 2 seconds, 861 page faults, and a peak page file of 924. An interesting observation is that the only “significant” difference in the “statistics” of the two versions is in the peak page file. There was emphatically no difference in run time. It would seem useful, therefore, to allow the novice programmer the option of attempting both approaches, while encouraging such comparisons as the one noted above. The “best” approach may not be an acceptable one for a particular user who feels burdened by a concept not fully understood. Thus, the when, where, and why can only be asked and answered by the programmer who truly experiences the greatest magnitude.	array data structure;experience;linked list;paging;pascal;pointer (computer programming);programmer;qualitative comparative analysis;queue (abstract data type);run time (program lifecycle phase);search tree;source lines of code;static variable;subroutine;tree (data structure);web page	Domenick J. Pinto	1988		10.1145/322609.323146	qualitative comparative analysis;binary search tree;real-time computing;multiprocessing;data structure;computer science;artificial intelligence;theoretical computer science;software engineering;database;programming language;source lines of code;algorithm;memory management	PL	-15.737785178011691	33.44508449019648	151549
cd64bc884ca1e4c38352ddc11eb2f2a5f5e8fb26	dalvik bytecode acceleration using fetch/decode hardware extension with hybrid execution	software;androids;computer architecture;virtual machine acceleration dalvik processor dalvik hardware extension android;humanoid robots;registers;hardware software java androids humanoid robots computer architecture registers;virtual machines android operating system java;test java program dalvik bytecode acceleration fetch decode hardware extension hybrid execution android operating system dalvik virtual machine vm optimized dalvik bytecode handler software native processor individual simple bytecodes individual complex bytecodes;hardware;java	"""The significant disadvantage of Android Operating System is Dalvik bytecode interpretation using Dalvik Virtual Machine (VM) [1], [2]. However there are many techniques [3] to improve the performance of VM. In this paper, we propose an alternative methodology which is """"Fetch/Decode Hardware Extension with hybrid Execution"""". It is a particular hardware that specially designed to fetch and decode Dalvik bytecode directly. In the hybrid execution stage complex bytecodes will be emulated by optimized Dalvik bytecode handler software of the native processor but simple bytecodes will be executed on hardware of the native processor directly. The outstanding success key of our technique is the Dalvik handler software optimization which utilized the extended hardware to reduce the operation steps in the original handler software. The experimental results show the speed up improvements on the individual simple bytecodes, the individual complex bytecodes, the test Java program of simple bytecodes and complex bytecodes can be achieved up to 22×, 3×, 10.44× and 2.12× respectively."""	android;emulator;java bytecode;mathematical optimization;operating system;program optimization;speedup;virtual machine	Surachai Thongkaew;Tsuyoshi Isshiki;Dongju Li;Hiroaki Kunieda	2014	2014 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2014.7032798	embedded system;parallel computing;real-time computing;computer science;humanoid robot;operating system;processor register;java	Arch	-17.21196498549586	36.159283601947635	152919
56a3fc8fc92cd363b008110ec15a24a3394e1d5e	automatic array alignment in data-parallel programs	distributed memory;data parallel;distributed processing;data processing;computer programs;mathematical models;data aggregation;control flow;data access;parallel processing computers;algorithms;parallel machines;fortran;alignment;language model	Data-parallel languages like Fortran 90 express parallelism in the form of operations on data aggregates such as arrays. Misalignment of the operands of an array operation can reduce program performance on a distributed-memory parallel machine by requiring nonlocal data accesses. Determining array alignments that reduce communication is therefore a key issue in compiling such languages. We present a framework for the automatic determination of array alignments in data-parallel languages such as Fortran 90. Our language model handles array sectioning, reductions, spreads, transpositions, and masked operations. We decompose alignment functions into three constituents: axis, stride, and offset. For each of these subproblems, we show how to solve the alignment problem for a basic block of code, possibly containing common subexpressions. Alignments are generated for all array objects in the code, both named program variables and intermediate results. The alignments obtained by our algorithms are more general than those provided by the “owner-computes” rule. Finally, we present some ideas for dealing with control flow, replication, and dynamic alignments that depend on loop induction variables.	algorithm;apache axis;array data structure;basic block;compiler;control flow;distributed memory;fortran;language model;nonlocal lagrangian;operand;parallel computing;parallel programming model	Siddhartha Chatterjee;John R. Gilbert;Robert Schreiber;Shang-Hua Teng	1993		10.1145/158511.158517	data aggregator;data access;array data structure;parallel computing;distributed memory;data processing;computer science;theoretical computer science;mathematical model;programming language;control flow;language model	PL	-12.580073410846412	37.029470768111764	152970
abdedfa1b4b0b091fca1ebcb8d16eb4679120dae	stackthreads/mp: integrating futures into calling standards	gnu c compiler;bitvector problems;interleaving semantics;shared memory;code optimization;code generation;code motion partial redundancy elimination;synchronization;data flow analysis;off the shelf	An implementation scheme of fine-grain multithreading that needs no changes to current calling standards for sequential languages and modest extensions to sequential compilers is described. Like previous similar systems, it performs an asynchronous call as if it were an ordinary procedure call, and detaches the callee from the caller when the callee suspends or either of them migrates to another processor. Unlike previous similar systems, it detaches and connects arbitrary frames generated by off-the-shelf sequential compilers obeying calling standards. As a consequence, it requires neither a frontend preprocessor nor a native code generator that has a builtin notion of parallelism. The system practically works with unmodified GNU C compiler (GCC). Desirable extensions to sequential compilers for guaranteeing portability and correctness of the scheme are clarified and claimed modest. Experiments indicate that sequential performance is not sacrificed for practical applications and both sequential and parallel performance are comparable to Cilk[8], whose current implementation requires a fairly sophisticated preprocessor to C. These results show that efficient asynchronous calls (a.k.a. future calls) can be integrated into current calling standard with a very small impact both on sequential performance and compiler engineering.	cilk plus;code generation (compiler);correctness (computer science);experiment;futures and promises;gnu compiler collection;machine code;multithreading (computer architecture);obedience (human behavior);parallel computing;preprocessor;process migration;shared memory;software portability;subroutine;thread (computing)	Kenjiro Taura;Kunio Tabata;Akinori Yonezawa	1999		10.1145/301104.301110	shared memory;synchronization;parallel computing;real-time computing;computer science;data-flow analysis;program optimization;programming language;code generation	PL	-18.918730577737957	33.894733803710096	153992
ab3bd71e795b32cee3d9560b8c8817f886f2c0c0	profiling data-dependence to assist parallelization: framework, scope, and optimization	program diagnostics parallel programming;hardware alias detection;coarser level profiling data dependence parallelization optimization sequential program dynamic binary instrumentation parwiz loop transformations program static structures memory accesses high light potential parallelism openmp parallel privatization vectorization technique data dependence profiling turning memory accesses static analysis;speculation;alias register;dynamic optimization	This paper describes a tool using one or more executions of a sequential program to detect parallel portions of the program. The tool, called Par wiz, uses dynamic binary instrumentation, targets various forms of parallelism, and suggests distinct parallelization actions, ranging from simple directive tagging to elaborate loop transformations. The first part of the paper details the link between the program's static structures (like routines and loops), the memory accesses performed by the program, and the dependencies that are used to highlight potential parallelism. This part also describes the instrumentation involved, and the general architecture of the system. The second part of the paper puts the framework into action. The first study focuses on loop parallelism, targeting OpenMP parallel-for directives, including privatization when necessary. The second study is an adaptation of a well-known vectorization technique based on a slightly richer dependence description, where the tool suggests an elaborate loop transformation. The third study views loops as a graph of (hopefully lightly) dependent iterations. The third part of the paper explains how the overall cost of data-dependence profiling can be reduced. This cost has two major causes: first, instrumenting memory accesses slows down the program, and second, turning memory accesses into dependence graphs consumes processing time. Par wiz uses static analysis of the original (binary) program to provide data at a coarser level, moving from individual accesses to complete loops whenever possible, thereby reducing the impact of both sources of inefficiency.	automatic parallelization;automatic vectorization;compiler;data dependency;dependence analysis;directive (programming);dynamic programming;feedback;graph (discrete mathematics);instrumentation (computer programming);iteration;loop optimization;mathematical optimization;openmp;parallel computing;program optimization;run time (program lifecycle phase);static program analysis;x86;x86-64	Alain Ketterlin;Philippe Clauss	2012	2012 45th Annual IEEE/ACM International Symposium on Microarchitecture	10.1109/MICRO.2012.47	speculation;parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language	Arch	-16.408670805382673	36.23220567664738	154508
9a61fa93433cde491b3d1ef6edabb20608db6fed	graph coloring vs. optimal register allocation for optimizing compilers	algorithme rapide;latencia;lenguaje programacion;optimal solution;processor architecture;cycle time;microprocessor;solution optimale;calculateur embarque;programming language;optimizing compiler;register allocation;latence;heuristic method;langage evolue;graph coloring;metodo heuristico;ejecucion programa;optimizacion compiladora;registre;program execution;embedded system;asignacion optima;execution programme;solucion optima;fast algorithm;compiler optimization;boarded computer;allocation optimale;langage programmation;lenguaje evolucionado;register file;microprocesseur;compilateur optimisation;fichier registre;latency;methode heuristique;algoritmo optimo;algorithme optimal;optimal allocation;optimal algorithm;optimizing compilers;high level language;microprocesador;algoritmo rapido;calculador embarque;optimisation compilateur;registro;register;heuristic algorithm;memory latency	Optimizing compilers play an important role for the efficient execution of programs written in high level programming languages. Current microprocessors impose the problem that the gap between processor cycle time and memory latency increases. In order to fully exploit the potential of processors, nearly optimal register allocation is of paramount importance. In the predominance of the x86 architecture and in the increased usage of high-level programming languages for embedded systems peculiarities and irregularities in their register sets have to be handled. These irregularities makes the task of register allocation for optimizing compilers more difficult than for regular architectures and register files. In this article we show how optimistic graph coloring register allocation can be extended to handle these irregularities. Additionally we present an exponential algorithm which in most cases can compute an optimal solution for register allocation and copy elimination. These algorithms are evaluated on a virtual processor architecture modeling two and three operand architectures with different register file sizes. The evaluation demonstrates that the heuristic graph coloring register allocator comes close to the optimal solution for large register files, but performs badly on small register files. For small register files the optimal algorithm is fast enough to replace a heuristic algorithm.	algorithm;cas latency;central processing unit;computation;embedded system;experiment;graph coloring;heuristic (computer science);high- and low-level;high-level programming language;instruction cycle;live cd;microprocessor;newton's method;operand;optimizing compiler;register allocation;register file;time complexity;x86	Ulrich Hirnschrott;Andreas Krall;Bernhard Scholz	2003		10.1007/978-3-540-45213-3_26	parallel computing;real-time computing;computer science;operating system;register renaming;stack register;optimizing compiler;distributed computing;processor register;programming language;register allocation;algorithm	Arch	-18.700698226046413	36.51624135875336	154878
d6bd2e52612be355aa5b9d0720ef3cce32f6755d	static analysis for guarded code	storage allocation;programacion paralela;parallel programming;optimizacion compiladora;analisis programa;program optimization;semantic information;compiler optimization;optimisation programme;program analysis;analyse programme;analisis semantico;allocation memoire;static analysis;analyse semantique;asignacion memoria;high performance;optimisation compilateur;semantic analysis;programmation parallele;optimizacion programa	Guarded(predicated) execution, as a new hardware feature, has been introduced into today's high performance processors. Guarded execution can signiicantly improve the performance of programs with conditional branches, and meanwhile also poses new challenges for conventional program analysis techniques. In this paper, we propose a static semantics inference mechanism to capture the semantics information of guards in the context of guarded code. Based on the semantics information , we extend the conventional deenitions regarding program analysis in guarded code, and develop the related guard-aware analysis techniques. These analyses include control ow analysis, data dependence analysis and data ow analysis as well.	central processing unit;data dependency;dependence analysis;programming language;static program analysis	Ping Hu	2000		10.1007/3-540-40889-4_4	program analysis;parallel computing;computer science;operating system;program optimization;optimizing compiler;database;predicate transformer semantics;programming language;static analysis;algorithm	SE	-18.34713958085117	34.523324134202134	155156
cec227604a53a65d5dc12f8a625b53d70ae626be	extracting parallelism in fortran by translation to a single assignment intermediate form	concurrent computing;automatic parallelism extraction;machine independent dataflow graph description language;prototypes;program interpreters;parallel programming;automatic programming;tree graphs;if1;automatic programming parallel programming parallel languages fortran program interpreters parallel processing;computational modeling;livermore loops single assignment intermediate form mustang fortran translation automatic parallelism extraction sequential fortran source program if1 machine independent dataflow graph description language sisal language parafrase 2 optimizing sisal compiler parallel executables multiple target platforms;parafrase 2;source code;fortran;computer science;parallel executables;sequential fortran source program;mustang;optimizing compilers;livermore loops;parallel languages;parallel processing concurrent computing optimizing compilers program processors computer science optimization methods tree graphs computational modeling prototypes;program processors;optimizing sisal compiler;parallel processing;fortran translation;sisal language;single assignment intermediate form;multiple target platforms;optimization methods	The paper presents MUSTANG, a system for translating Fortran to single assignment form in an effort to automatically extract parallelism. Specifically, a sequential Fortran source program is translated into IF1, a machine-independent dataflow graph description language that is the intermediate form for the SISAL language. During this translation, Parafrase 2 is used to detect opportunities for parallelization which are then explicitly introduced into the IF1 program. The resulting IF1 program is then processed by the Optimizing SISAL Compiler which produces parallel executables on multiple target platforms. The execution results of several Livermore Loops are presented and compared against Fortran and SISAL implementations on two different platforms. The results show that the translation is an efficient method for exploiting parallelism from the sequential Fortran source code. >	assignment (computer science);fortran;static single assignment form	Robert Barry;Paraskevas Evripidou	1994		10.1109/IPPS.1994.288281	computer architecture;parallel computing;computer science;programming language;implicit parallelism	HPC	-13.924755462877192	37.12989577041035	155202
bc163ddd2e9e346bb240839aabcc46cbcfcba1c9	palgol: a high-level dsl for vertex-centric graph processing with remote data access		Pregel is a popular distributed computing model for dealing with large-scale graphs. However, it can be tricky to implement graph algorithms correctly and efficiently in Pregel’s vertex-centric model, especially when the algorithm has multiple computation stages, complicated data dependencies, or even communication over dynamic internal data structures. Some domain-specific languages (DSLs) have been proposed to provide more intuitive ways to implement graph algorithms, but due to the lack of support for remote access — reading or writing attributes of other vertices through references — they cannot handle the above mentioned dynamic communication, causing a class of Pregel algorithms with fast convergence impossible to implement. To address this problem, we design and implement Palgol, a more declarative and powerful DSL which supports remote access. In particular, programmers can use a more declarative syntax called chain access to naturally specify dynamic communication as if directly reading data on arbitrary remote vertices. By analyzing the logic patterns of chain access, we provide a novel algorithm for compiling Palgol programs to efficient Pregel code. We demonstrate the power of Palgol by using it to implement several practical Pregel algorithms, and the evaluation result shows that the efficiency of Palgol is comparable with that of hand-written code.	algorithm;compiler;computation;data access;data dependency;data structure;digital subscriber line;distributed computing;domain-specific language;graph (abstract data type);graph (discrete mathematics);graph theory;list of algorithms;programmer;vertex (geometry)	Yongzhe Zhang;Hsiang-Shang Ko;Zhenjiang Hu	2017		10.1007/978-3-319-71237-6_15	computer science;programming language;theoretical computer science;distributed computing;computation;vertex (geometry);digital subscriber line;data access;convergence (routing);graph;data structure	Arch	-12.838167099475704	33.94773503500396	156267
5224c695c892feedfcfbd1e2679364bf721769a3	complementing ada with other programming languages	ada;fpga;design;programming language;hardware description language;robotics;profitability	This paper presents our experience in using Ada and the Ravenscar profile in a robotics non-profit association and in a robotics competition. While Ada is our primary and dominant language, we have complemented it with a hardware description language (Verilog) and an interactive language (Forth). We describe the interface between those languages, and the design choices that have been made to minimize the risks taken by leaving the Ada world. We also explain why we chose in some conditions to relax restrictions imposed by the use of the Ravenscar profile.	ada;forth;hardware description language;programming language;ravenscar profile;robotics;verilog	Samuel Tardieu;Alexis Polti	2009		10.1145/1647420.1647444	parallel computing;real-time computing;computer science;hardware description language;robotics;programming language;field-programmable gate array;profitability index	Robotics	-15.191423656945094	34.13265540492623	156360
6615df8da1bb4ff4165ba87c0c3773790a5f877e	spmd execution of programs with dynamic data structures on distributed memory machines	high level languages;application software;sequential program;compiler techniques;distributed memory machine;parallel programming;pointer based dynamic data structures;distributed data structure;runtime;spmd;software performance;data structures program processors parallel processing computer science memory architecture costs programming profession software performance application software runtime;memory architecture;data structures;programming profession;dynamic data structure;distributed dynamic pascal;distributed memory machines;pascal data structures high level languages parallel languages parallel programming;name based strategy;distribution dynamics;pascal;computer science;parallel languages;data structure;program processors;parallel processing;single program multiple data;name based strategy distributed memory machines spmd single program multiple data pointer based dynamic data structures distributed dynamic pascal compiler techniques sequential program	A combination of language features and compilation techniques that permits SPMD (single-program multiple-data) execution of programs with pointer-based dynamic data structures is presented. The Distributed Dynamic Pascal (DDP) language, which supports the construction and manipulation of local as well as distributed data structures, is described. The compiler techniques developed translate a sequential DDP program for SPMD execution in which all processors are provided with the same program but each processor executes only that part of the program which operates on the elements of the distributed data structures local to the processor. Therefore, the parallelism implicit in a sequential program is exploited. An approach for implementing pointers that is based on the generation of names for the nodes in a dynamic data structure is presented. The name-based strategy makes possible the dynamic distribution of data structures among the processors as well as the traversal of distributed data structures without interprocessor communication. >	data structure;distributed memory;dynamic data;dynamization;spmd	Rakesh Gupta	1992		10.1109/ICCL.1992.185487	computer architecture;application software;parallel computing;pascal;data structure;software performance testing;computer science;programming language;spmd;high-level programming language	PL	-13.450209906532029	38.313123864037365	156701
1012d5694550682512f8ec7a0f565376247adf1e	mpi pre-processor: generating mpi derived datatypes from c datatypes automatically	mpi preprocessor;mpi derived datatype generation;parallel processing grid computing middleware master slave investments supercomputers joining processes internet pipelines grain size;object oriented programming;sequential code;mpipp;mpi derived datatype generation mpi preprocessor c datatypes grid usage sequential code parallel execution aipe middleware mpipp software component;software component;message passing;c datatypes;middleware;source code;program processors grid computing message passing middleware object oriented programming parallel processing;aipe;grid usage;grid computing;parallel execution;program processors;parallel processing	The grid usage is facing the problem that consists in running existing sequential code for parallel execution transparently (ie. using source code without modification). AIPE is a middleware that deals with this problem. MPIPP is the software component we have developed to allow the generation of MPI derived datatypes from C datatype definitions automatically. The goal of this new tool is to make the building of complex messages easier for the end-user. Moreover, this paper shows that MPIPP goes farther in the complexity level of C datatypes that can be taken into account than any other similar tools have ever gone to	c++;component-based software engineering;fortran;java;message passing interface;middleware;procedural programming	Éric Renault;Christian Parrot	2006	2006 International Conference on Parallel Processing Workshops (ICPPW'06)	10.1109/ICPPW.2006.56	parallel processing;computer architecture;parallel computing;message passing;computer science;component-based software engineering;operating system;middleware;programming language;object-oriented programming;grid computing;source code	HPC	-11.968413338839841	38.38161046893228	156930
d10ea62aeca4458c7fd508d65b73b31f37cf594f	program optimization and parallelization using idioms	parallelisme;lenguaje programacion;optimisation;dependence analysis;compilateur;optimizacion;programming language;computational idioms;reduction;optimal method;performance;flot donnee;parallel prefix;paralelisacion;flujo datos;optimizacion compiladora;compiler;analyse;program optimization;algorithme;intermediate program representation;grafo;algorithm;parallelism;paralelismo;graph rewriting;parallelisation;graph;graphe;compiler optimization;parallelization;data flow analysis;langage programmation;parallel machines;optimization;analysis;procesador;fortran;rendimiento;processeur;data flow;machine model;optimisation compilateur;processor;scan operations;compilador;array data flow analysis;generic programming;algoritmo;program dependence graph;analisis	Programs in languages such as Fortran, Pascal, and C were designed and written for a sequential machine model. During the last decade, several methods to vectorize such programs and recover other forms of parallelism that apply to more advanced machine architectures have been developed (particularly for Fortran, due to its pointer-free semantics). We propose and demonstrate a more powerful translation technique for making such programs run efficiently on parallel machines which support facilities such as parallel prefix operations as well as parallel and vector capabilities. This technique, which is global in nature and involves a modification of the traditional definition of the program dependence graph (PDG), is based on the extraction of parallelizable program structures (“idioms”) from the given (sequential) program. The benefits of our technique extend beyond the above-mentioned architectures and can be viewed as a general program optimization method, applicable in many other situations. We show a few examples in which our method indeed outperforms existing analysis techniques.	automatic parallelization;fortran;mathematical optimization;parallel computing;pascal;pointer (computer programming);program dependence graph;program optimization;vector graphics	Shlomit S. Pinter;Ron Y. Pinter	1994	ACM Trans. Program. Lang. Syst.	10.1145/177492.177494	parallel computing;computer science;theoretical computer science;analysis;programming language	PL	-16.551108050191175	34.37641969395925	158206
7232af3c4a3c4641ece9818d1c484260ce33a266	accurate and efficient regression modeling for microarchitectural performance and power prediction	modelizacion;lenguaje programacion;sistema operativo;microprocessor;evaluation performance;optimisation;microarchitecture;statistical simulation;analisis estadistico;performance evaluation;measurement;optimizacion;programming language;evaluacion prestacion;inference regression;performance;simulation;aplicacion espacial;regression model;aprendizaje probabilidades;probabilistic approach;mediane;median;design space;statistical model;modelisation;modelo regresion;regression;simulacion estadistica;statistical analysis;operating system;simulation statistique;enfoque probabilista;approche probabiliste;modele regression;analyse statistique;inferencia;statistics;microarquitectura;langage programmation;statistical inference;error rate;apprentissage probabilites;design;systeme exploitation;performance prediction;optimization;microprocesseur;mediana;design space exploration;experimentation;modeling;microprocesador;power modeling;application spatiale;inference;probability learning;space application	We propose regression modeling as an efficient approach for accurately predicting performance and power for various applications executing on any microprocessor configuration in a large microarchitectural design space. This paper addresses fundamental challenges in microarchitectural simulation cost by reducing the number of required simulations and using simulated results more effectively via statistical modeling and inference.Specifically, we derive and validate regression models for performance and power. Such models enable computationally efficient statistical inference, requiring the simulation of only 1 in 5 million points of a joint microarchitecture-application design space while achieving median error rates as low as 4.1 percent for performance and 4.3 percent for power. Although both models achieve similar accuracy, the sources of accuracy are strikingly different. We present optimizations for a baseline regression model to obtain (1) application-specific models to maximize accuracy in performance prediction and (2) regional power models leveraging only the most relevant samples from the microarchitectural design space to maximize accuracy in power prediction. Assessing sensitivity to the number of samples simulated for model formulation, we find fewer than 4,000 samples from a design space of approximately 22 billion points are sufficient. Collectively, our results suggest significant potential in accurate and efficient statistical inference for microarchitectural design space exploration via regression models.	algorithmic efficiency;archive;baseline (configuration management);benchmark (computing);computation;consistency model;design space exploration;microarchitecture;microprocessor;performance prediction;simulation;statistical model	Benjamin C. Lee;David M. Brooks	2006		10.1145/1168857.1168881	statistical model;design;statistical inference;simulation;regression;performance;microarchitecture;computer science;median;measurement	Arch	-18.032770288958176	38.852824568972096	158489
3c5d8387e7d6809041212726c0044ea3b865ac47	should high level languages be used to write systems software?	computer performance evaluation;software event monitor;high level language;operating systems	Most of us write our programs in whatever language is most convenient for the problem at hand. Often this means, not so much that the language is well suited to the problem, but simply that it's the best suited of the choices available. Particularly with microprocessors and many minicomputers, we don't have a very wide choice of available software. Perhaps we have only an assembler or only a Basic interpreter.  Those who have a choice, or who are responsible for developing compilers and other basic systems software, must determine how much money to spend and where to spend it, becoming embroiled in such questions as what high level languages, if any, should be used or how important it is to develop a good assembler versus a good high level language compiler for our systems work.	assembly language;basic;compiler;high-level programming language;microprocessor;minicomputer;money	Dennis J. Frailey	1975		10.1145/800181.810317	parallel computing;real-time computing;computer science;programming language	PL	-15.186346480218527	34.186084465301036	158784
191fad81f4bee0af704b5411eca1c09b3e2a719d	what's the matter with kansas lava?	hardware realization;simulated behavior;larger circuit;functional hardware description language;kansas lava	Kansas Lava is a functional hardware description language implemented in Haskell. In the course of attempting to generate ever larger circuits, we have found the need to effectively test and debug the internals of Kansas Lava. This includes confirming both the simulated behavior of the circuit and its hardware realization via generated VHDL. In this paper we share our approach to this problem, and discuss the results of these efforts.	algorithmic program debugging;data structure;debugger;hardware description language;haskell;journal of functional programming;lazy evaluation;p (complexity);quickcheck;simulation;springer (tank);tracing (software);vhdl;wallace tree	Andrew Farmer;Garrin Kimmell;Andy Gill	2010		10.1007/978-3-642-22941-1_7	embedded system;simulation;computer graphics (images)	PL	-17.050072085407177	32.47533629951355	158995
ae1d69ff632595623ff1558378284eaa0a1821ce	peephole optimization	redundant instruction;peephole optimization;simple optimizing technique;final stage	Redundant instructions may be discarded during the final stage of compilation by using a simple optimizing technique called peephole optimization. The method is described and examples are given.	compiler;mathematical optimization;peephole optimization	William M. McKeeman	1965	Commun. ACM	10.1145/364995.365000	parallel computing;real-time computing;peephole optimization;computer science;algorithm	PL	-17.90949746310599	35.90036103000598	159617
4e6c47b03e20ab660f32587b75387ee901025d89	two program comprehension tools for automatic parallelization	runtime microwave integrated circuits parallel algorithms automatic control linear algebra partial differential equations algorithm design and analysis pattern recognition libraries program processors;parallel library reuse;program comprehension;software performance evaluation;program concept recognition;execution speed program comprehension tools automatic parallelization paramat pattern matching pap recognizer parallelizable algorithmic patterns generality;automatic code restructuring;parallelising compilers;pattern matching;software performance evaluation reverse engineering parallelising compilers software tools pattern matching;software tools;automatic program comprehension tools;automatic parallelization;reverse engineering	The authors compare two program-comprehension systems targeted to support automatic parallelization: PARAMAT (PARAllelization by pattern MATching) and the PAP (Parallelizable Algorithmic Patterns) Recognizer. The authors illuminate the main differences of each method, discussing the tradeoff between the speed of the first and the generality of the second.	automatic parallelization;parallel computing;program comprehension	Beniamino Di Martino;Christoph W. Kessler	2000	IEEE Concurrency	10.1109/4434.824311	computer architecture;parallel computing;computer science;theoretical computer science;operating system;pattern matching;programming language;reverse engineering;automatic parallelization	Visualization	-13.484990241317712	37.87547616127593	159735
e14343a796f9d8ecf6da793a8e7e9b51e82eab2f	sara: combining stack allocation and register allocation	stack allocation;text;fichero registro;compilateur;allocation pile;register allocation;resource allocation;generation code;relacion orden;localization;generacion codigo;code generation;ordering;localizacion;texte;compiler;relation ordre;localisation;asignacion pila;register file;fichier registre;asignacion recurso;allocation ressource;texto;integer linear program;compilador	Commonly-used memory units enable a processor to load and store multiple registers in one instruction. We showed in 2003 how to extend gcc with a stack-location-allocation (SLA) phase that reduces memory traffic by rearranging the stack and replacing some load/store instructions with load/store-multiple instructions. While speeding up the target code, our technique leaves room for improvement because of the phase ordering of register allocation before SLA. In this paper we present SARA which combines SLA and register allocation into a single phase. SARA creates a synergy among register assignment, spill-code generation, and SLA that makes the combined phase generate faster code than a sequence of the individual phases. We specify SARA by an integer linear program generated from the program text. We have implemented SARA in gcc, replacing gcc’s own implementation of register allocation. For our benchmarks, our results show that the target code is up to 16% faster than gcc with a separate SLA phase.	cplex;code generation (compiler);gnu compiler collection;heuristic (computer science);linear programming;location-allocation;register allocation;rematerialization;service-level agreement;software quality;stack-based memory allocation;synergy	V. Krishna Nandivada;Jens Palsberg	2006		10.1007/11688839_19	compiler;parallel computing;real-time computing;internationalization and localization;resource allocation;order theory;computer science;operating system;programming language;register allocation;register file;algorithm;code generation	PL	-18.647743208944615	36.5421305940068	160203
363ce870e54ba6b98b29df8ce053f5fced330525	runtime pointer disambiguation	alias analysis;hybrid analysis;dynamic guards;loop transformations;pointer disambiguation;code cloning;optimization	To optimize code effectively, compilers must deal with memory dependencies. However, the state-of-the-art heuristics available in the literature to track memory dependencies are inherently imprecise and computationally expensive. Consequently, the most advanced code transformations that compilers have today are ineffective when applied on real-world programs. The goal of this paper is to solve this conundrum through dynamic disambiguation of pointers. We provide different ways to determine at runtime when two memory locations can overlap. We then produce two versions of a code region: one that is aliasing-free - hence, easy to optimize - and another that is not. Our checks let us safely branch to the optimizable region. We have applied these ideas on Polly-LLVM, a loop optimizer built on top of the LLVM compilation infrastructure. Our experiments indicate that our method is precise, effective and useful: we can disambiguate every pair of pointer in the loop intensive Polybench benchmark suite. The result of this precision is code quality: the binaries we generate are 10% faster than those that Polly-LLVM produces without our optimization, at the -O3 optimization level of LLVM.	aliasing;analysis of algorithms;benchmark (computing);clang;computer science;dvd region code;experiment;heuristic (computer science);llvm;mathematical optimization;optimizing compiler;pointer (computer programming);polly (robot);prototype;run time (program lifecycle phase);software quality;speedup;word-sense disambiguation	Péricles Rafael Oliveira Alves;Fabian Gruber;Johannes Doerfert;Alexandros Lamprineas;Tobias Grosser;Fabrice Rastello;Fernando Magno Quintão Pereira	2015		10.1145/2814270.2814285	parallel computing;real-time computing;alias analysis;computer science;programming language	PL	-17.857076852607403	36.554854369005156	160242
d8b2ca0b30d490facc8eddd3116a5ce02e52d29f	flowpaths: compiling stack-based ir to hardware	java bytecode;programming language;hardware description language;system architecture;high level language;hardware implementation;intermediate representation	The performance of software executed on a microprocessor is adversely affected by the basic fetch-execute cycle. A further performance penalty results from the load-execute-store paradigm associated with the use of local variables in most high-level languages. Implementing the software algorithm directly in hardware such as on an FPGA can alleviate these performance penalties. Such implementations are normally developed in a hardware description language such as VHDL or Verilog. More recently, several methods for using C as a hardware description language and for compiling C programs to hardware have been researched. Several software-programming languages compile to an intermediate representation (IR) that is stack based such as Java to Java bytecodes. Forth is a programming language that minimizes the use of local variables by exchanging the load-execute-store paradigm for stack manipulation instructions. This paper introduces a new systems architecture for FPGAs, called flowpaths, which can implement Java bytecodes or software programs written in Forth directly in an FPGA without the need for a microprocessor core. In the flowpath implementation of Forth programs all stack manipulation instructions are represented as simple wire connections that take zero time to execute. In the flowpath implementation of Java bytecodes the normal load-execute-store paradigm is represented as a single sequential operation and stack-manipulation operations become combinational thus executing faster. This paper compares the use of flowpaths in an FPGA generated from Java bytecodes and a high-level Forth program for the Sieve of Eratosthenes with C, Java, and Forth executed on microprocessors and microprocessor cores on FPGAs. The results show that flowpaths perform within a factor of 2 of a minimal hand-crafted direct hardware implementation of the Sieve and orders of magnitude better than compiling the program to a microprocessor.	apl;algorithm;combinational logic;compiler;field-programmable gate array;hardware description language;high- and low-level;instruction cycle;intermediate representation;java applet;local variable;microprocessor;multi-core processor;programming paradigm;sieve of eratosthenes;stack-oriented programming language;systems architecture;vhdl;verilog	Darrin M. Hanna;Richard E. Haskell	2006	Microprocessors and Microsystems	10.1016/j.micpro.2005.07.001	embedded system;computer architecture;parallel computing;computer science;operating system;real time java;hardware description language;programming language;intermediate language;java;high-level programming language;algorithm;systems architecture	Arch	-16.816634155373745	35.901695781844786	161078
9398074118f68dccf8858883201cfa4bfc784c33	the impact of data dependence analysis on compilation and program parallelization	optimizing compiler;parallelizing compilers;data dependence;compiler optimization;program analysis;experimental evaluation;automatic parallelization	Optimizing compilers rely upon program analysis techniques to detect data dependences between program statements. Data dependence information captures the essential ordering constraints of the statements in a program that need to be preserved in order to produce valid optimized and parallel code. Data dependence testing is very important for automatic parallelization, vectorization and any other code transformation. In this paper we examine the impact of data dependence analysis in practice. A number of data dependence tests have been proposed in the literature. In each test there are different tradeoffs between accuracy and efficiency. We present an experimental evaluation of several data dependence tests, including the Banerjee test, the I-Test and the Omega test. We compare these tests in terms of data dependence accuracy, compilation efficiency, effectiveness in parallelization and program execution performance. We analyze the reasons why a data dependence test can be inexact and we explain how the examined tests handle such cases. We run various experiments using the Perfect Club Benchmarks and the scientific library Lapack. We present the measured accuracy of each test and the reasons for any approximation. We compare these tests in terms of efficiency and we analyze the tradeoffs between accuracy and efficiency. We also determine the impact of each data dependence test on the total compilation time. Finally, we measure the number of loops parallelized by each test and we compare the execution performance of each benchmark on a multiprocessor. Our results indicate that the Omega test is more accurate, but also very inefficient in the cases where the other two tests are inaccurate. In general the cost of the Omega test is high and a significant percentage of the total compilation time. Furthermore, the difference in accuracy of the Omega test over the Banerjee test and the I-Test does not improve parallelization and program execution performance.	approximation;automatic parallelization;automatic vectorization;benchmark (computing);data dependency;dependence analysis;experiment;lapack;multiprocessing;omega;optimizing compiler;parallel computing;program analysis;test case	Kleanthis Psarris;Konstantinos Kyriakopoulos	2003		10.1145/782814.782843	parallel computing;real-time computing;computer science;theoretical computer science;operating system;optimizing compiler;programming language	SE	-16.658119006622744	34.89788106084587	162321
959bcddcbb7f964df17f7c40e6a398c587eca0bd	a mapping approach between ir and binary cfgs dealing with aggressive compiler optimizations for performance estimation		In this work, we define a mapping approach between the compiler intermediate representation and the binary control flow graph for the purpose of performance estimation in native simulation. Our approach handles aggressive compiler optimizations such as loop unrolling without having to introduce any modification to the compiler. Our mapping approach experimentally leads to a good accuracy (0.59% error) while keeping a 25x speedup for native simulation compared to instruction set simulation.	binary code;context-free grammar;control flow graph;experiment;instruction set simulator;intermediate representation;loop unrolling;optimizing compiler;simulation;speedup	Omayma Matoussi;Frédéric Pétrot	2018	2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2018.8297365	loop unrolling;binary code;parallel computing;real-time computing;compiler;computer science;software;control flow graph;speedup;instruction set;optimizing compiler	EDA	-17.166175644431988	36.09860869151495	163047
d3d6a33337899358e29db32cae70ffc8ae8d321b	peephole optimization as a targeting and coupling tool	code generation;global optimization;pattern matching	The term peephole optimization is used to mean the pattern matching and conditional replacement performed on small sections of the intermediate form. The circular dependence between the code generation phases implies that local optimals are rarely global optimals. There are several reactions: (1) accept the local optimal, (2) develop intermediate goals whose achievement suggest global optimality, (3) retain the choices so that the decisions can be made later, (4) optimize the object code by replacing awkward or overly constrained segments of code with improved ones. Optimizing the object code has several advantages. First, code generation is greatly simplified. The code generator is allowed to forgo case analysis and utilize only a subset of the machine's instructions and addressing modes [BD88,DF84a,DF84b,DF87]. Second, a phase ordering problem often encountered in optimizations is reduced.	addressing mode;code generation (compiler);intermediate representation;mathematical optimization;object code;optimizing compiler;pattern matching;peephole optimization;reduction (complexity)	Vicki H. Allan	1989		10.1145/75362.75407	peephole optimization;computer science;theoretical computer science;pattern matching;algorithm;code generation;global optimization	PL	-15.913707684535353	32.4152313753753	163732
86c8f8c0cad85b189feade4b31f36d56ebd9f6c8	symbolic analysis of concurrency errors in openmp programs	mathematics;data race;concurrent computing;multiprocessing programs;smt solver;computability;parallel programming application program interfaces computability error analysis multiprocessing programs;parallel programming;smt solver openmp symbolic anslysis data race deadlock;computing;arrays;error analysis;symbolic anslysis;indexes;system recovery;synchronization;application program interfaces;concurrency error symbolic analysis portable parallel programming model viva64 pvs studio commercial dynamic analysis tools commercial static analysis tool sun thread analyzer intel thread checker oat tool student homework assignments schedule permutation data race detection smt solver satisfiability modulo theories openmp analysis toolkit openmp programs;openmp;deadlock;and information science;encoding;encoding system recovery synchronization arrays indexes concurrent computing	In this paper we present the OpenMP Analysis Toolkit (OAT), which uses Satisfiability Modulo Theories (SMT) solver based symbolic analysis to detect data races and deadlocks in OpenMP codes. Our approach approximately simulates real executions of an OpenMP program through schedule permutation. We conducted experiments on real-world OpenMP benchmarks and student homework assignments by comparing our OAT tool with two commercial dynamic analysis tools: Intel Thread Checker and Sun Thread Analyzer, and one commercial static analysis tool: Viva64 PVS Studio. The experiments show that our symbolic analysis approach is more accurate than static analysis and more efficient and scalable than dynamic analysis tools with less false positives and negatives.	code;conditional (computer programming);deadlock;dependence analysis;experiment;openmp;rose;satisfiability modulo theories;scalability;sensor;solver;source lines of code;static program analysis	Hongyi Ma;Steve Diersen;Chunhua Liao;Daniel J. Quinlan;Zijiang Yang	2013	2013 42nd International Conference on Parallel Processing	10.1109/ICPP.2013.63	synchronization;computer architecture;computing;parallel computing;concurrent computing;computer science;deadlock;operating system;computability;programming language;encoding	SE	-15.887867298456563	37.40208208594534	163804
b7ddb271c74a220639d58b68ebbc798316b2cdd9	mapping the data flow model of computation into an enhanced von neumann processor	data flow;data flow graph;model of computation	The SAM architecture is an enhanced von Neumann processor that contains inexpensive features for supporting data flow style of parallelism. The architecture gets is name from the basic instructions for supporting parallelism, Split and Merge. It is shown that these instructions can be used to implement the parallel structure of an arbitrary acyclic data flow graph. Features for supporting dynamic parallelism and multiple run-time environments are presented. Implementation issues for supporting instruction execution and the handling of faults and interrupts ar also discussed.	compiler;dataflow architecture;directed acyclic graph;dynamic programming;interrupt;model of computation;parallel computing;register file;sam;von neumann architecture	Peter M. Maurer	1988			merge (version control);von neumann architecture;parallel computing;architecture;theoretical computer science;computer science;model of computation;data flow diagram;data-flow analysis	HPC	-14.024841992831117	37.334348772888944	165898
96189c4aa50a9971d9221bc0a2427eb35a6a08ea	efficient evaluation strategies for structured concurrency constructs in parallel scheme systems	lazy evaluation;functional programming;efficient implementation;parallel implementation;automatic parallelization	In Scheme-based parallel Lisp systems there are proposed a number of structured concurrency constructs like pcall, par, par- and, par-or and plet, etc. A standard evaluation strategy for these structured constructs has been the eager task creation (ETC) that creates child processes to execute their argument expressions whenever these structured constructs are encountered. But the ETC strategy for structured concurrency constructs is known to be inefficient because of overhead caused by excessive process creation. In this paper we propose an efficient evaluation strategy for structured concurrency constructs, called the steal-based evaluation (SBE) strategy, which suppresses excessive process creation. SBE is based on an extended use of two basic actions steal and inlining of lazy task creation used in an efficient implementation of future. The idea of SBE is extended to give a parallel implementation of the delay construct and stream computation expressed with delay yielding the steal-based lazy evaluation (SLE). In Concluding Remarks there are discussed the following issues: a programming style called structural parallel symbolic programming which is based on a functional core of Scheme and the SBE strategy, Restricted Eager Task Creation to suppress excessive process creation under ETC, and a simple constraint-based automatic parallelization of functional programs.		Takayasu Ito	1995		10.1007/BFb0023054	computer architecture;parallel computing;computer science;programming language;automatic parallelization	Theory	-16.222493068312918	34.10266541974658	166179
bb328c94518bd994e3a5241cf0465d68153e32aa	co-designing software abstraction and optimisation for productivity and performance		Improving the execution time of applications is important, but there is a tendency to sacrifice programmability in its pursuit. This thesis investigates co-design approaches, in which APIs provide an abstraction that is strictly maintained using sound software engineering practices while performance is optimised within a managed runtime environment. Flexibility in APIs and weak encapsulation often results in hand-optimisation that restricts the effectiveness of performance improvements and obfuscates functionality. Domain specific applications contain semantics that general purpose languages cannot exploit during compilation. Hand-optimisation addresses this by manually improving the implementation of applications, requiring both expertise and time. Two application domains are used to demonstrate approaches for exploiting semantics to improve performance; MapReduce parallelism and SLAM in computer vision.Creating correct parallel software is challenging and, thus, frameworks have been developed to exploit the available performance of commodity hardware. MapReduce is a popular programming framework to facilitate the development of data analytics applications. Implementations hide the complexities of data management, scheduling and fault tolerance from users; as MapReduce frameworks evolve, new specialisations and optimisations are introduced. However, improvements often require manual integration into applications to enable performance gains. Hand-optimisation may be used because the semantics of the underlying abstraction or the scope of the compiler are unsuitable. This thesis demonstrates that the semantics of MapReduce may be used to extend the scope of the dynamic compiler. By analysing applications using a MapReduce framework with co-designed optimisation, it is possible to execute these applications in Java in a comparative time to hand-optimised C and C++. The benefits also include improved efficiency of memory management and reduction in the volume of the intermediate data generated. Hence, it is possible to speedup Java application performance twofold. Most importantly, it does not require any extension or rewriting of existing applications.Computer vision, SLAM in particular, contains a mix of regular and irregular vector operations. These are not addressed directly, for this domain, by existing abstractions because many of the data types used represent small vectors (2-7 elements). An array is the natural choice to contain the elements of a vector, but it is not optimal for performance or productivity. This thesis presents a new class collection for small vectors in Java using sound software engineering practice. By co-designing the data-level implementation with its interaction with the dynamic compiler, overheads introduced by the strict API have been eliminated during optimisation. This results in kernels, frequently used in SLAM applications, with improved performance relative to a popular C++ SLAM library. In addition to this, it is possible to demonstrate how the small vector implementation may exploit SIMD instructions and registers to improve performance further.When programmability is prioritised, performance should not be obtained by hand-optimisation because this tends to obfuscate application code. To compensate for this restriction, co-design approaches can extend the communication of application semantics. This thesis demonstrates that there is the potential for co-designed optimisations crossing abstraction boundaries for better performance without affecting productivity.	mathematical optimization	Colin Barrett	2015			parallel computing;real-time computing;computer science;theoretical computer science	Arch	-15.29035515330651	36.44157288404576	166241
04ef59d39d7c47ed8d471260d0e6ae52f4e820e5	partitioned memory models for program analysis	computer science partition memory models for program analysis new york university clark barrett wang;wei	Scalability is a key challenge in static analysis. For imperative languages like C, the approach taken for modeling memory can play a significant role in scalability. In this paper, we explore a family of memory models called partitioned memory models which divide memory up based on the results of a points-to analysis. We review Steensgaard’s original and field-sensitive points-to analyses as well as Data Structure Analysis (DSA), and introduce a new cell-based points-to analysis which more precisely handles heap data structures and type-unsafe operations like pointer arithmetic and pointer casting. We give experimental results on benchmarks from the software verification competition using the program verification framework in Cascade. We show that a partitioned memory model using our cell-based points-to analysis outperforms models using other analyses.	data structure;formal verification;imperative programming;memory model (programming);pointer (computer programming);pointer analysis;scalability;software verification;static program analysis	Wei Wang;Clark W. Barrett;Thomas Wies	2017		10.1007/978-3-319-52234-0_29	computer science;artificial intelligence;algorithm	PL	-17.835169040736346	32.665068154090385	166713
08f1ffb301b4e92c5118b2436c48e3c75b0a7402	alice a multi-processor reduction machine for the parallel evaluation cf applicative languages	processor architecture;building block;performance estimation;program transformation;functional programming;animation;optimization	The functional or applicative languages have long been regarded as suitable vehicles for overcoming many of the problems involved in the production and maintenance of correct and reliable software. However, their inherent inefficiences when run on conventional von Neumann style machines have prevented their widespread acceptance. With the declining cost of hardware and the increasing feasibility of multi-processor architectures this position is changing, for, in contrast to conventional programs where it is difficult to detect those parts that may be executed, concurrently, applicative programs are ideally suited to parallel evaluation.  In this paper we present a scheme for the parallel evaluation of a wide variety of applicative languages and provide an overview of the architecture of a machine on which it may be implemented.  First we describe the scheme, which may be characterized as performing graph reduction, at the abstract level and discuss mechanisms that allow several modes of parallel evaluation to be achieved efficiently. We also show how a variety of languages are supported.  We then suggest an implementation of the scheme that has the property of being highly modular; larger and faster machines being built by joining together smaller ones. Performance estimates illustrate that a small machine (of the size that we envisage would form the basic building block of large systems) would provide an efficient desk-top personal applicative computer, while the larger versions promise very high levels of performance Indeed. The machine is designed to be ultimately constructed from a small number of types of VLSI component.  Finally we compare our approach with the other proposes schemes for the parallel evaluation of applicative languages and discuss planned future developments.	alice;applicative programming language;graph reduction;multiprocessing;the machine;very-large-scale integration	John Darlington;Mike Reeve	1981		10.1145/800223.806764	anime;microarchitecture;computer science;theoretical computer science;programming language;functional programming;algorithm	PL	-15.986673065960185	38.54860447012432	166820
00b2dd14f882da560e1583cfb60692d55345091f	value-profile guided stride prefetching for irregular code	gestion memoire;compilateur;data prefetch;storage management;prechargement donnee;compiler;gestion memoria;performance improvement;integer program;compilador	"""Memory operations in irregular code are difficult to prefetch, as the future address of a memory location is hard to anticipate by a compiler. However, recent studies as well as our experience indicate that many irregular programs contain loads with near-constant strides. This paper presents a novel compiler technique to profile and prefetch for those loads. The profile captures not only the dominant stride values for each profiled load, but also the differences between the successive strides of the load. The profile information helps the compiler to classify load instructions into strongly or weakly strided and single-strided or phased multi-strided. The prefetching decisions guided by the load classifications are highly selective and beneficial. We obtain significant performance improvement for the CPU2000 integer programs running on Itanium? machines. For example, we achieve a 1.55x speedup for """"181.mcf"""", 1.15x for """"254.gap"""", 1.08x for """"197.parser"""" and smaller gains in other benchmarks. We also show that the performance gain is stable across profile data sets and that the profiling overhead is low. These benefits make the new technique suitable for a production compiler."""	link prefetching	Youfeng Wu;Mauricio J. Serrano;Rakesh Krishnaiyer;Wei Li;Jesse Fang	2002		10.1007/3-540-45937-5_22	compiler;parallel computing;real-time computing;computer science;operating system;programming language	EDA	-18.13249523063825	37.50322907752907	167162
68a19ea5095c9da9262ad03fcd534c12333f049d	why structured parallel programming matters	parallel algorithm;run time system;work stress;application sharing;message passing;structured parallel programming;data flow;parallel programs;parallel applications;program correctness	Simple parallel programming frameworks such as Pthreads, or the six function core of MPI, are universal in the sense that they support the expression of arbitrarily complex patterns of computation and interaction between concurrent activities. Pragmatically, their descriptive power is constrained only by the programmer’s creativity and capacity for attention to detail. Meanwhile, as our understanding of the structure of parallel algorithms develops, it has become clear that many parallel applications can be characterized and classified by their adherence to one or more of a number of generic patterns. For example, many diverse applications share the underlying control and data flow of the pipeline paradigm, whether expressed in terms of message passing, or by constrained access to shared data. A number of research programs, using terms such as skeleton, template, archetype and pattern, have sought to exploit this phenomenon by allowing the programmer to explicitly express such meta-knowledge in the program source, through the use of new libraries, annotations and control constructs, rather than leaving it implicit in the interplay of more primitive universal mechanisms. While early work stressed productivity and portability (the programmer is no longer required to repeatedly “reinvent the wheel”) we argue that the true significance of this approach lies in the capture of complex algorithmic knowledge which would be impossible to determine by static examination of an equivalent unstructured source. This enables developments in a number of areas. With respect to low-level performance, it allows the run-time system, library code or compiler to make clever optimizations based on detailed foreknowledge of the evolving computation. With respect to high-level performance, it enables a methodology of improvement through powerful restructuring transformations. Similarly, with respect to program correctness, it allows arguments to be pursued at a much coarser, more tractable grain than would otherwise be possible. M. Danelutto, D. Laforenza, M. Vanneschi (Eds.): Euro-Par 2004, LNCS 3149, p. 37, 2004. c © Springer-Verlag Berlin Heidelberg 2004	application-specific integrated circuit;cobham's thesis;compiler;computation;control flow;correctness (computer science);dataflow;euro-vo;field-programmable gate array;grid computing;high- and low-level;lecture notes in computer science;library (computing);message passing;money;posix threads;parallel algorithm;parallel computing;programmer;programming paradigm;reinventing the wheel;runtime system;springer (tank);topological skeleton	Murray Cole	2004		10.1007/978-3-540-27866-5_4	data flow diagram;parallel computing;message passing;real-time computing;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;programming language	HPC	-13.993591252510212	38.87210144313704	167222
301ccccc8b39b08879fd3f265ce4aa4a29f2471c	paws: collective interactions and data transfers	distributed data;data transmission;general and miscellaneous mathematics computing and information science;p codes;atomic functionalities paws collective interactions data transfers parallel applications collective port common component architecture cca ports translation components distributed data format parallel implementation mxn component parallel application work space data structures user defined distributed type casts optimized complex translation components;plasma welding laboratories programming profession data structures packaging timing shape component architectures us government protection;parallel programming;object oriented programming;data distribution;computing;mathematical methods and computing;97 mathematical methods and computing;data structures;application program interfaces;interactions;object oriented programming parallel programming data structures application program interfaces;and information science;parallel implementation;data structure;general and miscellaneous mathematics;99 general and miscellaneous mathematics computing and information science;parallel applications;parallel processing;data transfer	In this paper we discuss problems and solutions pertaining to the interaction of components representing parallel applications. We introduce the notion of a collective port which is an extension of the Common Component Architecture (CCA) ports and allows collective components representing parallel applications to interact as one entity. We further describe a class of translation components, which translate between the distributed data format used by one parallel implementation to that used by another. A well known example of such components is the MxN component which translates between data distributed on M processors to data distributed onN processors. We describe its implementation in Parallel Application Work Space (PAWS), as well as the data structures PAWS uses to support it. We also present a mechanism allowing the framework to invoke this component on the programmer’s behalf whenever such translation is necessary, freeing the programmer from treating collective component interactions as a special case. In doing that we introduce framework-based, user-defined distributed type casts. Finally, we discuss our initial experiments in building optimized complex translation components out of atomic functionalities.	central processing unit;common component architecture;data structure;experiment;interaction;object composition;programmer;prototype;requirement;type conversion	Katarzyna Keahey;Patricia K. Fasel;Susan M. Mniszewski	2001		10.1109/HPDC.2001.945175	parallel computing;data structure;computer science;theoretical computer science;operating system;distributed computing;programming language;data transmission	HPC	-12.600557067352463	38.31351045531483	168363
5753ea73b93b76aed76d56796332ca1531bce19c	automatic array inlining in java virtual machines	object inlining;just in time compilation;java virtual machine;performance;garbage collection;data flow analysis;just in time;optimization;array inlining;data structure;java	Array inlining expands the concepts of object inlining to arrays. Groups of objects and arrays that reference each other are placed consecutively in memory so that their relative offsets are fixed, i.e. they are colocated. This allows memory loads to be replaced by address arithmetic, which reduces the costs of field and array accesses. We implemented this optimization for Sun Microsystems' Java HotSpot VM. The optimization is performed automatically and requires no actions on the part of the programmer.  Arrays are frequently used for the implementation of dynamic data structures. Therefore, the length of arrays often varies, and fields referencing such arrays have to be changed whenever the array is reallocated. We present an efficient code pattern that detects these changes and allows the optimized access of such array fields. It is integrated into the array bounds check. We also claim that inlining array element objects into an array is not possible without a global data flow analysis.  The evaluation shows that our dynamic approach can optimize frequently accessed fields with a reasonable low compilation and analysis overhead. The peak performance of SPECjvm98 is improved by 10% on average, with a maximum of 25%.	array data structure;bounds checking;colocation centre;compiler;data-flow analysis;dataflow;dynamic data;dynamization;inline expansion;java;mathematical optimization;overhead (computing);programmer;virtual machine	Christian Wimmer;Hanspeter Mössenböck	2008		10.1145/1356058.1356061	hashed array tree;parallel array;array data structure;parallel computing;real-time computing;data structure;performance;computer science;operating system;data-flow analysis;just-in-time compilation;dynamic array;garbage collection;programming language;java	PL	-18.40339413904904	36.5617340618919	168377
fa70a708a4996968a719a3f8102543e661ed982e	automatic test case reduction for opencl	paper;code generation;compilers;nvidia geforce gtx titan;nvidia;computer science;opencl	We report on an extension to the C-Reduce tool, for automatic reduction of C test cases, to handle OpenCL kernels. This enables an automated method for detecting bugs in OpenCL compilers, by generating large random kernels using the CLsmith generator, identifying kernels that yield result differences across OpenCL platforms and optimisation levels, and using our novel extension to C-Reduce to automatically reduce such kernels to minimal forms that can be filed as bug reports. A major part of our effort involved the design of ShadowKeeper, a new plugin for the Oclgrind simulator that provides accurate detection of accesses to uninitialised data. We present experimental results showing the effectiveness of our method for finding bugs in a number of OpenCL compilers.	clang;compiler;eclipse;experiment;mathematical optimization;opencl api;plug-in (computing);sensor;software bug;test case;undefined behavior	Moritz Pflanzer;Alastair F. Donaldson;Andrei Lascu	2016		10.1145/2909437.2909439	parallel computing;computer hardware;computer science;operating system	SE	-16.658170186378676	35.522385962505666	168649
934b004387902f2a212c77f729fa3af3d9b119ee	communication generation for aligned and cyclic(k) distributions using integer lattice	data parallel;code generation;smith normal form analysis communication generation cyclic k distributions aligned distributions integer lattice spmd codes distributed arrays data parallel languages high performance fortran hpf communication set integer lattices inspector like run time codes;lattices parallel languages runtime programming profession equations message passing program processors computational modeling delay;message passing parallel languages fortran program compilers;high performance fortran;smith normal form;message passing;hpf;fortran;program compilers;distributed arrays;parallel languages;communication optimizations	Optimizing communication is a key issue in generating efficient SPMD codes in compiling distributed arrays on data parallel languages, such as High Performance Fortran. In HPF, the array distribution may involve alignment and cyclic(k)-distribution such that the enumeration of the local set and the enumeration of the communication set exhibit regular patterns which can be modeled as integer lattices. In the special case of unit-strided alignment, many techniques of the communication set enumeration have been proposed, while in the general case of the non-unit-strided alignment, inspector-like run-time codes are needed to build repeating pattern table or to scan over local elements such that the communication set can be constructed. Unlike other works on this problem of the general alignment and cyclic(k) distribution, our approach derives an algebraic solution for such an integer lattice that models the communication set by using the Smith-Normal-Form analysis, therefore, efficient enumeration of the communication set can be generated. Based on the integer lattice, we also present our algorithm for the SPMD code generation. In our approach, when the parameters are known, the SPMD program can be efficiently constructed without any inspector-like run-time codes.		Eric Hung-Yu Tseng;Jean-Luc Gaudiot	1999	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.752780	smith normal form;parallel computing;message passing;computer science;theoretical computer science;operating system;distributed computing;programming language;algorithm;code generation	Arch	-13.105537689899203	38.138447079436084	168984
179b7a2dd882e47a68877fc75ce5da912f056534	computing linear data dependencies in nested loop programs	nested loops;data dependence	An important topic in automatically parallelizing nested loop programs (for the implementation on a parallel machine or for the mapping onto VLSI circuits) is the computation of linear data dependencies. Recently, Feautrier has developed an algorithm which is based on the solution of a parametric integer linear program. In this paper another method is presented. This method only works for special cases but it should be simpler and more efficient. Apart of these special cases the extensions suggested in this paper provide a system of fewer inequalities and variables in the general case compared to the original algorithm of Feautrier. The key idea is the solution of a system of linear diophantine equations.	data dependency	Christian Heckler;Lothar Thiele	1994	Parallel Processing Letters	10.1142/S012962649400020X	combinatorics;parallel computing;real-time computing;nested loop join;computer science;theoretical computer science;distributed computing;algorithm	HPC	-13.421767314592607	35.187401780699446	169001
26b944c62b0367817489355287140ab6b434d8ef	accurate static estimators for program optimization	call graph;register allocation;optimization technique;frequency estimation;program optimization;vliw;markov model;profiling directed feedback;control flow;global scheduling;superscalars;software pipelining;static analysis;instruction scheduling;compiler optimizations	Determining the relative execution frequency of program regions is essential for many important optimization techniques, including register allocation, function inlining, and instruction scheduling. Estimates derived from profiling with sample inputs are generally regarded as the most accurate source of this information; static (compile-time) estimates are considered to be distinctly inferior. If static estimates were shown to be competitive, however, their convenience would outweigh minor gains from profiling, and they would provide a sound basis for optimization when profiling is impossible. We use quantitative metrics to compare estimates from static analysis to those derived from profiles. For C programs, simple techniques for predicting branches and loop counts suffice to estimate intraprocedural frequency patterns with high accuracy. To determine inter-procedural estimates successfully, we combine function-level information with a Markov model of control flow over the call graph to produce arc and basic block frequency estimates for the entire program. For a suite of 14 programs, including the C programs from the SPEC92 benchmark suite, we demonstrate that static estimates are competitive with those derived from profiles. Using simple heuristics, we can determine the most frequently executed blocks in each function with 81% accuracy. With the Markov model, we identify 80% of the frequently called functions. Combining the two techniques, we identify 76% of the most frequently executed call sites.	basic block;benchmark (computing);call graph;compile time;compiler;control flow;function-level programming;heuristic (computer science);inline expansion;instruction scheduling;markov chain;markov model;mathematical optimization;profiling (computer programming);program optimization;register allocation;scheduling (computing);static program analysis	Tim A. Wagner;Vance Maverick;Susan L. Graham;Michael A. Harrison	1994		10.1145/178243.178251	call graph;software pipelining;parallel computing;real-time computing;computer science;very long instruction word;theoretical computer science;program optimization;optimizing compiler;markov model;instruction scheduling;programming language;control flow;register allocation;static analysis	PL	-18.173437046517193	37.055269060406914	170061
9ce381a3e8606f5581889c7ec42059514a40ac77	extensions to the c programming language for simd/mimd parallelism		A superset of the C programming language that is applicable to the SIMD/MIMD mode processing environment of PASM is described. The language extensions for SIMD mode include the definition of parallel variables, functions, and expressions; a scheme for accessing parallel variables; and extended control structure semantics. Extensions for MIMD mode are realized by defining a preprocessor to convert a generalized CSP-like language to standard C with operating system calls inserted to support the parallelism. Extensions to the libraries of I/O and operating systems functions used in parallel mode are also discussed. The PASM parallel processing system is used as an example target machine.	ansi c;control flow;input/output;library (computing);mimd;operating system;parallel computing;preprocessor;simd;system call;the c programming language	James T. Kuehn;Howard Jay Siegel	1985			parallel computing;programming language;semantics;mimd;computer architecture;simd;expression (mathematics);preprocessor;subset and superset;parallel processing;parallel programming model;computer science	PL	-13.93526807878384	37.44771939878241	170167
776e47110fe90a44601f061d86abf2efaa0325fd	fortran program specialization	program transformation;program specialization;multi dimensional;partial evaluation;source language;binding time analysis;fortran;generic programming	We have developed and implemented a partial evaluator for a subset of Fortran 77. A partial evaluator is a tool for program transformation which takes as input a general program and a part of its input, and produces as output a specialized program. The goal is efficiency: a specialized program often runs an order of magnitude faster than the general program. The partial evaluator is based on the off-line approach and uses a binding-time analysis prior to the specialization phase. The source language includes multi-dimensional arrays, procedures and functions, as well as global storage. The system is presented and experimental results are given.	algorithmic efficiency;array data structure;fortran;interpreter (computing);name binding;online and offline;partial evaluation;partial template specialization;program transformation	Paul Kleinrubatscher;Albert Kriegshaber;Robert Zöchling;Robert Glück	1995	SIGPLAN Notices	10.1145/202176.202184	computer architecture;parallel computing;computer science;programming language;generic programming;partial evaluation	PL	-13.483459075040408	35.618845364255584	170464
87432763bb8cf24f4e3104aeef01b4d7f88cf820	basic crossassembler for the 8086		Abstract   Creation of even the simplest hand assembled program on an SDK-86 is a time consuming process because of the nature of the 8086 instruction set. The Intel assembler is expensive, and a crossassembler will save expense if there is access to a computer with a high level language. A crossassembler written in BASIC, AX86, is given, with examples of source input and output and a consideration of the syntax.	basic	Mike F. Smith	1979	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(79)90290-4	inline assembler;parallel computing;computer science;artificial intelligence;operating system;programming language;algorithm	EDA	-14.559893734977896	33.92437113536092	171586
1018252b8f14c4ec121632d8874265dfcdae3058	exact analysis of the cache behavior of nested loops	modelo dinamico;symbolic computation;evaluation performance;simplification;performance evaluation;hierarchized structure;efficiency;evaluacion prestacion;static tests;linear array;nested loops;layout problem;simulation;dynamic model;context free grammars;probleme agencement;implantation;program compression;program transformation;structure hierarchisee;cache memory;loops;first principle;transformation programme;classification;compilers;hierarchies;antememoria;calculo simbolico;computer programming;performance engineering;computer architecture;antememoire;transformacion programa;mathematical models;first principles calculations;modele dynamique;data transformation;arreglo;hierarchie memoire;problema disposicion;optimization;patterns;arrangement;symbolic analysis;memory hierarchy;behavior;jerarquia memoria;calcul symbolique;memory devices;implantacion;clasificacion;supercomputers;estructura jerarquizada;variable to fixed length codes;bytecode interpretation	We develop from first principles an exact model of the behavior of loop nests executing in a memory hicrarchy, by using a nontraditional classification of misses that has the key property of composability. We use Presburger formulas to express various kinds of misses as well as the state of the cache at the end of the loop nest. We use existing tools to simplify these formulas and to count cache misses. The model is powerful enough to handle imperfect loop nests and various flavors of non-linear array layouts based on bit interleaving of array indices. We also indicate how to handle modest levels of associativity, and how to perform limited symbolic analysis of cache behavior. The complexity of the formulas relates to the static structure of the loop nest rather than to its dynamic trip count, allowing our model to gain efficiency in counting cache misses by exploiting repetitive patterns of cache behavior. Validation against cache simulation confirms the exactness of our formulation. Our method can serve as the basis for a static performance predictor to guide program and data transformations to improve performance.	cpu cache;composability;forward error correction;kerrison predictor;nonlinear system;presburger arithmetic;simulation	Siddhartha Chatterjee;Erin Parker;Philip J. Hanlon;Alvin R. Lebeck	2001		10.1145/378795.378859	cache-oblivious algorithm;parallel computing;symbolic computation;cache coloring;loop interchange;performance engineering;first principle;computer science;loop nest optimization;theoretical computer science;cache invalidation;computer programming;efficiency;programming language;data transformation;cache algorithms;simplification;algorithm;behavior	PL	-18.331722219509004	35.50870467895214	172176
64674612694e614dd53364b622b8141dab07ba54	ort: a communication library for orthogonal processor groups	parallel programming parallel processing scalability functional programming runtime library parallel algorithms scientific computing;group spmd communication library mixed task and data parallelism	Many implementations on message-passing machines can benefit from an exploitation of mixed task and data parallelism. A suitable parallel programming model is a group-SPMD model, which requires a structuring of the processors into subsets and a partition of the program into multi-processor tasks. In this paper, we introduce a library support for the specification of message-passing programs in a group-SPMD style allowing different partitions in a single program. We describe the functionality and the implementation of the library functions and illustrate the library programming style with example programs. The examples show that the runtime on distributed memory machines can be considerably reduced by using the library.		Thomas Rauber;Robert Reilein;Gudula Rünger	2001		10.1109/SC.2001.10015	computer architecture;parallel computing;computer science;data parallelism;programming language;parallel extensions;task parallelism;parallel programming model	HPC	-12.520038838847562	38.91847374236394	172259
531dd5dee4961bbec160423c3b93712ab1f2b0e8	collection types for database programming in the bsp model	bulk synchronous parallel	We study the pragmatics of integrating collection types, that model a broad class of non-numerical applications, into the Bulk Synchronous Parallel (BSP) model which abstracts a diversity of parallel architectures using just four numerical parameters. We outline how the collection types have been built on-top of the direct BSP programming environment provided by BSPlib, give results on a SGI PowerChallenge and IBM SP2, and discuss how these types can help implement object databases.	bulk synchronous parallel;database;integrated development environment;numerical analysis	K. R. Sujithan;Jonathan M. D. Hill	1997			component-oriented database;parallel computing;bulk synchronous parallel;distributed computing;programming language;declarative programming;object-relational mapping;reactive programming;computer science	DB	-13.928322162751739	36.38174030397463	173239
0f7a505425507445d7a1109c492f9ae51037daa4	optimizing alpha executables on windows nt with spike	windows nt;optimizing alpha executables;control flow;application development	Vol. 9 No. 4 1997 3 Spike is a performance tool developed by DIGITAL to optimize Alpha executables on the Windows NT operating system. This optimization system has two main components: the Spike Optimizer and the Spike Optimization Environment. The Spike Optimizer reads in an executable, optimizes the code, and writes out the optimized version. The Optimizer uses profile feedback from previous runs of an application to guide its optimizations. Profile feedback is not commonly used in practice because it is difficult to collect, manage, and apply profile information. The Spike Optimization Environment provides a user-transparent profile feedback system that solves most of these problems, allowing a user to easily optimize large applications composed of many executables and dynamic link libraries (DLLs). Optimizing an executable image after it has been compiled and linked has several advantages. The Spike Optimizer can see the entire image and perform interprocedural optimizations, particularly with regard to code layout. The Optimizer can use profile feedback easily, because the executable that is profiled is the same executable that is optimized; no awkward mapping of profile data back to the source language takes place. Also, Spike can be used when the sources to an application are not available, which is beneficial when DIGITAL is working with independent software vendors (ISVs) to tune applications. Applications can be loosely classified into two categories: loop-intensive programs and call-intensive programs. Conventional compiler technology is well suited to loop-intensive programs. The important loops in a program in this category are within a single procedure, which is typically the unit of compilation. The control flow is predictable, and the compiler can use simple heuristics to determine the frequently executed parts of the procedure. Spike is designed for large, call-intensive programs; it uses interprocedural optimization and profile feedback. In call-intensive programs, the important loops span multiple procedures, and the loop bodies contain procedure calls. Consequently, optimizations on the loops must be interprocedural. The control flow is Optimizing Alpha Executables on Windows NT with Spike Robert S. Cohn David W. Goodwin P. Geoffrey Lowney	control flow;dynamic-link library;executable;heuristic (computer science);independent software vendor;interprocedural optimization;library (computing);linker (computing);mathematical optimization;operating system;optimizing compiler;program optimization;the spike (1997);windows nt	Robert S. Cohn;David W. Goodwin;P. Geoffrey Lowney	1998	Digital Technical Journal		computer architecture;computer science;compiler;parallel computing;windows nt;software;executable;control flow;emulation;interprocedural optimization;binary translation	PL	-17.404375995579077	36.61202194515483	173495
4fb5102a8569897f4ef71e024cb863172ef52662	finite state machine-based optimization of data parallel regular domain problems applied in low-level image processing	data parallel;optimisation;data communications aspects finite state machine optimization low level image processing software parallel computing software library preparallelized routines api regular domain problem memory management operation message passing parallel processing;formal specification;memory management;image processing;software libraries;image processing software parallel processing data communications aspects optimization;image processing parallel processing parallel programming software design software libraries runtime memory management communication system operations and management law legal factors;indexing terms;data communication;65;programming model;finite state machines;performance improvement;timing optimization;data communications aspects;parallel computer;message passing;formal specification parallel processing message passing software libraries data communication optimisation finite state machines;optimization;parallel programs;image processing software;finite state machine;parallel applications;parallel processing	A popular approach to providing nonexperts in parallel computing with an easy-to-use programming model is to design a software library consisting of a set of preparallelized routines, and hide the intricacies of parallelization behind the library's API. However, for regular domain problems (such as simple matrix manipulations or low-level image processing applications-in which all elements in a regular subset of a dense data field are accessed in turn) speedup obtained with many such library-based parallelization tools is often suboptimal. This is because interoperation optimization (or: time-optimization of communication steps across library calls) is generally not incorporated in the library implementations. We present a simple, efficient, finite state machine-based approach for communication minimization of library-based data parallel regular domain problems. In the approach, referred to as lazy parallelization, a sequential program is parallelized automatically at runtime by inserting communication primitives and memory management operations whenever necessary. Apart from being simple and cheap, lazy parallelization guarantees to generate legal, correct, and efficient parallel programs at all times. The effectiveness of the approach is demonstrated by analyzing the performance characteristics of two typical regular domain problems obtained from the field of low-level image processing. Experimental results show significant performance improvements over nonoptimized parallel applications. Moreover, obtained communication behavior is found to be optimal with respect to the abstraction level of message passing programs.	abstraction layer;algorithm;application programming interface;data parallelism;data structure;effective method;finite-state machine;global optimization;high- and low-level;image processing;inter-process communication;interoperation;lazy evaluation;library (computing);linear algebra;mathematical optimization;memory management;message passing;on the fly;overhead (computing);parallel computing;programming model;run time (program lifecycle phase);signal processing;speedup	Frank J. Seinstra;Dennis C. Koelma;Andrew D. Bagdanov	2004	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2004.55	parallel computing;message passing;index term;computer science;theoretical computer science;operating system;formal specification;database;distributed computing;programming paradigm;finite-state machine;programming language;algorithm;automatic parallelization;memory management	HPC	-13.839657372845663	37.50265305142208	173685
c128f07854a9717f489362fc390da6e423c3ca40	bit manipulation in fortran language	fortran	the list and ilvar [k] may be added, if it is maximized), and the inner loop is re-entered. Ia addition to the push-down list, storage is requircd for the current values of the i teration variables, and for their maxima, Thus if the depth of nesting is n, the storage required in addition to the instruction storage is 2n if all variables have the same maximum value, or 3 n 1 if the maximum values are different. The method suggested here has been used successfully with a combinatorial problem programmed in MAD. ROBEI¢T R. KORFtIAGE Purdue University Lafayette, Indiana RECEIVED MAY, 1964	bit manipulation;fortran;inner loop;mad;maxima	George D. Tobey	1965	Commun. ACM	10.1145/364955.364972	computer architecture;parallel computing;computer science;programming language;high-level programming language	Theory	-13.952566246904345	34.67310069046402	173798
dd138cf07376e91f7fc90f567cc33d17fbb6b381	partitioning dataflow analyses using types	dataflow analysis;data dependence;type inference;points to analysis	"""We present a simple method for partitioning a dataflow analysis problem into a series of related subproblems. We use type information (either declared by the programmer, or computed via nonstandard type inference) to conservatively approximate analysis-time data dependences between program quantities. This dependence information frees us from the need to model all program quantities simultaneously in a single, monolithic analysis. Instead, we can model quantities in any order, or even in parallel, so long as we respect the dependences. Our approach is independent of the means used to solve the subproblems, and enables the wider application of existing sparse approaches previously restricted to """"separable"""" dataflow problems. Preliminary experiments applying our technique to flow-sensitive points-to analysis of C programs have achieved storage savings of 1.3-7.2x over existing methods."""	approximation algorithm;data-flow analysis;dataflow;experiment;pointer analysis;programmer;sparse matrix;type inference	Erik Ruf	1997		10.1145/263699.263705	computer science;theoretical computer science;type inference;distributed computing;programming language;algorithm	PL	-16.134389458204154	34.916039571847946	173875
6c6f2edb419458daa59d0ac30d0f42133092b42a	solving the 24 puzzle with instance dependent pattern databases	optimal solution;look up table;expresion regular;solution optimale;search space;heuristic method;abstraction;metodo heuristico;abstraccion;tabla de consulta;solucion optima;expression reguliere;lookup table;table conversion;methode heuristique;regular expression	A pattern database(PDB) is a heuristic function in a form of a lookup table which stores the cost of optimal solutions for instances of subproblems. These subproblems are generated by abstracting the entire search space into a smaller space called the pattern space. Traditionally, the entire pattern space is generated and each distinct pattern has an entry in the pattern database. Recently, [10] described a method for reducing pattern database memory requirements by storing only pattern database values for a specific instant of start and goal state thus enabling larger PDBs to be used and achieving speedup in the search. We enhance their method by dynamically growing the pattern database until memory is full, thereby allowing using any size of memory. We also show that memory could be saved by storing hierarchy of PDBs. Experimental results on the large 24 sliding tile puzzle show improvements of up to a factor of 40 over previous benchmark results [8].	benchmark (computing);database;heuristic (computer science);lookup table;protein data bank;requirement;sed;speedup	Ariel Felner;Amir Adler	2005		10.1007/11527862_18	multiton pattern;state pattern;lookup table;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;database;mathematics;distributed computing;programming language;algorithm	DB	-14.1540492943257	34.058622352937284	174042
6dc471cc1a702cb45ec972c80daf1d8c1083469b	unification of static and dynamic analyses to enable vectorization		Modern compilers execute sophisticated static analyses to enable optimization across a wide spectrum of code patterns. However, there are many cases where even the most sophisticated static analysis is insufficient or where the computation complexity makes complete static analysis impractical. It is often possible in these cases to discover further opportunities for optimization from dynamic profiling and provide this information to the compiler, either by adding directives or pragmas to the source, or by modifying the source algorithm or implementation. For current and emerging generations of chips, vectorization is one of the most important of these optimizations. This paper defines, implements, and applies a systematic process for combining the information acquired by static analysis by modern compilers with information acquired by a targeted, high-resolution, low-overhead dynamic profiling tool to enable additional and more effective vectorization. Opportunities for more effective vectorization are frequent and the performance gains obtained are substantial: we show a geometric mean across several benchmarks of over 1.5x in speedup on the Intel Xeon Phi coprocessor.	algorithm;automatic vectorization;c++;compiler;computation;computer architecture;coprocessor;directive (programming);emoticon;fortran;han unification;image resolution;manycore processor;mathematical optimization;optimizing compiler;overhead (computing);parallel computing;speedup;static program analysis;thread (computing);user interface;xeon phi	Ashay Rane;Rakesh Krishnaiyer;Chris J. Newburn;James C. Browne;Leonardo Fialho;Zakhar Matveev	2014		10.1007/978-3-319-17473-0_24	parallel computing;theoretical computer science;programming language	PL	-15.625791754763016	36.28504178806086	174120
4afed6b6837aa0feda66eb32827d7d83bd14c2c1	implementation of an orchestration language as a haskell domain specific language	distributed computing;orc;coordination language;functional programming;coordination languages;parallel divide and conquer algorithms;scaling up;thread based programming;domain specific language;concurrent programs;divide and conquer;haskell;coordination;binary tree	Even though concurrent programming has been a hot topic of discussion in Computer Science for the past 30 years, the community has yet to settle on a, or a few standard approaches to implement concurrent programs. But as more and more cores inhabit our CPUs and more and more services are made available on the web the problem of coordinating different tasks becomes increasingly relevant. The present paper addresses this problem with an implementation of the orchestration language Orc as a domain specific language in Haskell. Orc was, therefore, realized as a combinator library using the lightweight threads and the communication and synchronization primitives of the Concurrent Haskell library. With this implementation it becomes possible to create orchestrations that re-use existing Haskell code and, conversely, re-use orchestrations inside other Haskell programs. The complexity inherent to distributed computation, entails the need for the classification of efficient, reusable, concurrent programming patterns. The paper discusses how the calculus of recursive schemes used in the derivation of functional programs, scales up to a distributed setting. It is shown, in particular, how to parallelize the entire class of binary tree hylomorphisms.	binary tree;central processing unit;combinator library;computation;computer science;concurrent haskell;concurrent computing;distributed computing;domain-specific language;orc;recursion	Marco Devesas Campos;Luís Soares Barbosa	2009	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2009.10.024	parallel computing;divide and conquer algorithms;binary tree;computer science;domain-specific language;programming language;functional programming;algorithm;parallel programming model	PL	-13.693870558891456	39.04981348731725	174544
f3d7f9252bc525db990566ed293d7dfa3e5e2e1d	efl: implementing and testing an embedded language which provides safe and efficient parallel execution	computers;python multiprocessing module;task trees;parallel processing parallel programming instruction sets operating systems computers arrays;mpi parallel programming task trees flexible computation parallel design patterns order independent execution python multiprocessing module;parallel programming;arrays;order independent execution;parallel design patterns;flexible computation;parallelism testing embedded language safe parallel execution efficient parallel execution deterministic parallel programming language safety parallel executions parallel code blocks sequential host language program efl precompiler efl blocks parallel building blocks parallel assignments task tree architecture work span method;mpi;software architecture parallel languages program compilers program testing safety critical software;parallel processing;operating systems;instruction sets	As a deterministic parallel programming language that guarantees safety of parallel executions, EFL was designed to allow the embedding of parallel code blocks into a sequential host language program. An EFL pre-compiler, which is described here, has been implemented that translates EFL blocks into the host language. The EFL pre-compiler and runtime supports parallel building blocks such as parallel assignments, parallel for loops, etc. EFL was successfully tested on a task tree architecture, using the work-span method to estimate parallelism and speedup, followed by experiments to measure actual execution times and calculate real speedups on a specific platform. Results show that speedup increases almost linearly with the size of a task tree.	assignment (computer science);baseline (configuration management);code::blocks;compiler;debugging;emoticon;enlightenment foundation libraries;experiment;map (parallel pattern);parallel building blocks;parallel computing;parallel programming model;programming language;prototype;python;software transactional memory;speedup;top-down and bottom-up design	David Dayan;Moshe Goldstein;Shimon Mizrahi;Max Rabin;Devora Berlowitz;Or Berlowitz;Elad Bussani Levy;Moshe Naaman;Mor Nagar;Ditsa Soudry;Raphael B. Yehezkael;Miroslav Popovic	2015	2015 4th Eastern European Regional Conference on the Engineering of Computer Based Systems	10.1109/ECBS-EERC.2015.21	parallel processing;computer architecture;parallel computing;computer science;message passing interface;operating system;instruction set;programming language	HPC	-13.763526241681022	37.51358925162085	175292
983139dc7760464db0cfdc3e9340b9ebd1c94a18	a practical improvement to the partial redundancy elimination in ssa form	static single assignment form;partial redundancy elimination	Partial redundancy elimination (PRE) is an interesting compiler optimization because of its effectiveness and generality. Among many PRE algorithms, the one in static single assignment form (SSAPRE) has benefits over other bit-vector-based PRE algorithms. It preserves the properties of the SSA form after PRE and exploits the sparsity of the SSA form, resulting in reduced analysis and optimization time. This paper presents a practical improvement of the SSAPRE algorithm that further reduces the analysis and optimization time. The underlying idea is removing unnecessary Φ’s during the Φ-Insertion phase that is the first step of SSAPRE. We classify the expressions into three categories: confined expressions, local expressions, and the others. We show that unnecessary Φ’s for confined and local expressions can be easily detected and removed. We implement our locality-based SSAPRE algorithm in a C compiler and evaluate its effectiveness with 20 applications from SPEC benchmark suites. In our measurements, on average 91% of Φ’s identified by the original demand-driven SSAPRE algorithm are unnecessary for PRE. Pruning these unnecessary Φ’s in the Φ-Insertion phase makes our locality-based SSAPRE algorithm 1.8 times faster, on average, than the original SSAPRE algorithm.	algorithm;assignment (computer science);basic block;benchmark (computing);bit array;embedded system;flash memory;locality of reference;mathematical optimization;operand;optimizing compiler;partial redundancy elimination;sparse matrix;static single assignment form	Jongsoo Park;Jaejin Lee	2008	JCSE		real-time computing;computer science;theoretical computer science;algorithm	PL	-17.435205592416526	34.88873637823795	175330
1322fd55045d22849bbc879193af44791e28e510	parameterized loop tiling	computacion informatica;data locality;fourier motzkin elimination;code generation;abstract syntax tree;program optimization;general methods;ciencias basicas y experimentales;bilinear form;loop tiling;scientific computing;parameterized tiling;coarse grained;grupo a;bounding box	Loop tiling is a widely used program optimization that improves data locality and enables coarse-grained parallelism. Parameterized tiled loops, where the tile sizes remain symbolic parameters until runtime, are quite useful for iterative compilers and autotuners that produce highly optimized libraries and codes. Although it is easy to generate such loops for (hyper-) rectangular iteration spaces tiled with (hyper-) rectangular tiles, many important computations do not fall into this restricted domain. In the past, parameterized tiled code generation for the general case of convex iteration spaces being tiled by (hyper-) rectangular tiles has been solved with bounding box approaches or with sophisticated and expensive machinery.  We present a novel formulation of the parameterized tiled loop generation problem using a polyhedral set called the outset. By reducing the problem of parameterized tiled code generation to that of generating standard loops and simple postprocessing of these loops, the outset method achieves a code generation efficiency that is comparable to existing code generation techniques, including those for fixed tile sizes. We compare the performance of our technique with several other tiled loop generation methods on kernels from BLAS3 and scientific computations. The simplicity of our solution makes it well suited for use in production compilers—in particular, the IBM XL compiler uses the inset-based technique introduced in this article for register tiling. We also provide a complete coverage of parameterized tiling of perfect loop nests by describing three related techniques: (i) a scheme for separating full and partial tiles; (ii) a scheme for generating tiled loops directly from the abstract syntax tree representation of loops; (iii) a formal characterization of parameterized loop tiling using bilinear forms and a Symbolic Fourier-Motzkin Elimination (SFME)-based parameterized tiled loop generation method.	abstract syntax tree;bilinear filtering;code generation (compiler);compiler;computation;fourier–motzkin elimination;iteration;library (computing);locality of reference;mathematical optimization;minimum bounding box;parallel computing;parse tree;polyhedron;program optimization;tiling window manager	Lakshminarayanan Renganarayanan;DaeGon Kim;Michelle Mills Strout;Sanjay V. Rajopadhye	2012	ACM Trans. Program. Lang. Syst.	10.1145/2160910.2160912	loop tiling;parallel computing;bilinear form;computer science;minimum bounding box;theoretical computer science;program optimization;programming language;code generation;abstract syntax tree	PL	-13.518360489425762	35.413981061529725	176163
ae605bc9452825b531501cb2c9346c1dce9b4f48	a methodology for specifying data distribution using only standard object-oriented features	conflict misses;object oriented language;xor based placement functions;parallelizing compilers;cache memory;data distribution;object oriented;parallel computer	Object-oriented class frameworks are promising alternatives to traditional languages and their parallelizing compilers due to their higher at&action facilities for encapsulating parallelism and distribution. We claim that language feature6 already available in cxisiting object-oriented languages such aa C* for constructing class t&reworks can be maximally exploited for achieving 6cp6ration of parallelism and data distribution, without language extaxsions or ad-hoc methodologies. We first model parallel computation a6 crtiered application of a function onto 6tructurcd elements. and, ba6cd on the model, we construct a class fmmcwcrk which formulates 1) data distribution 66 hierarchical decomposition of the 6tructured elements to layered objects and 2) parallelism as nested rruversuls across these objects. We evaluate the feasibiity of our proposal on the Pujitsu APlOtXl parallel computer by extending the EPEE class framework.	automatic parallelization;compiler;computation;hoc (programming language);parallel computing	Naohito Sato;Satoshi Matsuoka;Jean-Marc Jézéquel;Akinori Yonezawa	1997		10.1145/263580.263611	parallel computing;computer science;theoretical computer science;programming language;object-oriented programming	PL	-14.480830838435171	38.37741670207647	176839
638ac2a3b560f93a541678d98f9710381ba7cce9	taming hardware event samples for fdo compilation	sampling profile;performance counters;supervised learning;feedback directed optimization;web search;dynamic instrumentation	Feedback-directed optimization (FDO) is effective in improving application runtime performance, but has not been widely adopted due to the tedious dual-compilation model, the difficulties in generating representative training data sets, and the high runtime overhead of profile collection. The use of hardware-event sampling to generate estimated edge profiles overcomes these drawbacks. Yet, hardware event samples are typically not precise at the instruction or basic-block granularity. These inaccuracies lead to missed performance when compared to instrumentation-based FDO@. In this paper, we use multiple hardware event profiles and supervised learning techniques to generate heuristics for improved precision of basic-block-level sample profiles, and to further improve the smoothing algorithms used to construct edge profiles. We demonstrate that sampling-based FDO can achieve an average of 78% of the performance gains obtained using instrumentation-based exact edge profiles for SPEC2000 benchmarks, matching or beating instrumentation-based FDO in many cases. The overhead of collection is only 0.74% on average, while compiler based instrumentation incurs 6.8%-53.5% overhead (and 10x overhead on an industrial web search application), and dynamic instrumentation incurs 28.6%-1639.2% overhead.	algorithm;basic block;compiler;freedesktop.org;heuristic (computer science);mathematical optimization;overhead (computing);run time (program lifecycle phase);sampling (signal processing);smoothing;supervised learning;web search engine	Dehao Chen;Neil Vachharajani;Robert Hundt;Shih-Wei Liao;Vinodha Ramasamy;Paul Yuan;Wenguang Chen;Weimin Zheng	2010		10.1145/1772954.1772963	parallel computing;real-time computing;simulation;computer science;theoretical computer science;operating system;supervised learning	Arch	-17.654971659444374	37.64244321303069	177378
05fd8ccf4aa4f64bc6dee257866764a7087275d5	library support for orthogonal processor groups	wdm;data parallel;graph;message passing;cycle covering;parallel programming model;survivability	Many implementations on message-passing machines can benefit from an exploitation of mixed task and data parallelism. A suitable parallel programming model is a group-SPMD model which requires a structuring of the processors in to subsets and a partition of the program into multi-processor tasks. In this paper, we introduce a library support for the specification of message-passing programs in a group-SPMD style allowing different partitions in a single program. We describe the implementation of the library functions and illustrate the programming style.	central processing unit;data parallelism;message passing;multiprocessing;parallel computing;parallel programming model;programming style;spmd	Thomas Rauber;Robert Reilein;Gudula Rünger	2001		10.1145/378580.378729	combinatorics;parallel computing;message passing;computer science;theoretical computer science;operating system;distributed computing;graph;programming language;algorithm;wavelength-division multiplexing;parallel programming model	HPC	-12.295366831637049	39.13156325216467	177719
056e135d45767faa89ddaf43a6d7fc400cd65a9f	semiparallel execution of compiled lisp programs	concurrent computing;execution time;application software;processor scheduling;execution time compiled lisp programs parallel pipeline execution fully parallel semiparallel execution one dimensional processor arrays;one dimensional processor arrays;parallel programming;binary trees;systems engineering and theory;machine intelligence;pipelines;pipelines supercomputers processor scheduling europe application software binary trees machine intelligence systems engineering and theory marketing and sales concurrent computing;compiled lisp programs;lisp;europe;parallel pipeline execution;program processors;supercomputers;fully parallel semiparallel execution;program processors lisp parallel programming;marketing and sales	This paper deals with Lips programs whose forms are more general than previous programs. Extended parallel pipeline execution is explained for the whole of a compiled Lisp program. The extended parallel pipeline execution includes fully parallel/semiparallel execution by using one-dimensional processor arrays (ODPAs). The tradeoff between the scale of the ODPA and the execution time of the compiled Lisp program is examined. >	compiler;lisp	Masa-Aki Fukase;Tadao Nakamura	1991		10.1109/CMPSAC.1991.170266	computer architecture;application software;parallel computing;concurrent computing;binary tree;computer science;fexpr;operating system;lisp;pipeline transport;programming language	Arch	-13.390228409111021	38.23492593734786	178925
990cb0611559f16f7d9c3f8b8b1866c656229cf5	integrating task and data parallelism by means of coordination patterns	parallelisme;lenguaje programacion;decomposition domaine;domain decomposition;laplace equation;programming language;calculateur;ecuacion laplace;calculadora;calculator;descomposicion dominio;parallelism;paralelismo;langage programmation;coordinacion;fortran;equation laplace;coordination	This paper shows, by means of some examples, the suitability and expressiveness of a pattern-based approach to integrate task and data parallelism. Coordination skeletons or patterns express task parallelism among a collection of data parallel HPF tasks. Patterns specify the interaction among domains involved in the application along with the processor and data layouts. On the one hand, the use of domains, i.e. regions together with some interaction information, improves pattern reusability. On the other hand, the knowledge at the coordination level of data distribution belonging to the different HPF tasks is the key for an efficient implementation of the communication among them. Besides that, our system implementation requires no change to the runtime system support of the HPF compiler used. We also present some experimental results that show the efficiency of the model.	compiler;data parallelism;high performance fortran;interaction information;parallel computing;runtime system;task parallelism	Manuel Díaz;Bartolomé Rubio;Enrique Soler;José M. Troya	2001	Proceedings 15th International Parallel and Distributed Processing Symposium. IPDPS 2001	10.1007/3-540-45401-2_2	parallel computing;computer science;artificial intelligence;theoretical computer science;operating system;database;distributed computing;domain decomposition methods;data parallelism;programming language;instruction-level parallelism;implicit parallelism;algorithm;task parallelism;laplace's equation	HPC	-12.550856495832084	38.96483837343844	179585
fac24460646a69b25f1bb63758860a0df8df609c	root_numpy: the interface between root and numpy		At its core are functions for converting between ROOT TTrees and structured NumPy arrays. root_numpy can convert TTree branches (columns) of fundamental types and strings, as well as variable-length and fixed-length multidimensional arrays and (nested) std::vectors. root_numpy can also create columns in the output NumPy array from mathematical expressions like ROOT’s TTree::Draw(). root_numpy’s internals are written in Cython (Behnel et al. 2011), installed as compiled C++ extensions, and can handle data with comparable speed to ROOT as shown in the figure below. root_numpy can also convert between ROOT histograms and NumPy arrays, and sample or evaluate ROOT functions as NumPy arrays.		Edmund Noel Dawe;P. Ongmongkolkul;Giordon Stark	2017	J. Open Source Software	10.21105/joss.00307	computational science;numpy;computer science	Vision	-12.325863485463156	34.74848317302627	179950
729847da54ea596195d1e608237ab37976dbc0c9	a parallel lcc simulation system	microprocessors;cost function;clocks;logic;acceleration;circuit simulation;computational modeling;latches;circuit simulation computational modeling acceleration partitioning algorithms latches clocks discrete event simulation cost function microprocessors logic;partitioning algorithms;discrete event simulation	Cycle-based simulation at RTand gate level realized by a Levelized Compiled Code (LCC) technique represents a well established method for functional verification in processor design. We present a parallel LCC simulation system developed to run on loosely-coupled processor systems allowing significant simulation acceleration. It comprises three parallel simulators and a complex model partitioning environment. A key idea of our approach is to valuate circuit model partitions with respect to the expected parallel simulation run-time and to integrate corresponding cost functions into partitioning algorithms. Experimental results are given with respect to IBM processor models of different size. 1. Parallelization Approach TEXSIM and its successor MVLSIM are LCC simulators developed by IBM for the functional verification in microprocessor design. We have parallelized them for looselycoupled processor systems (such as parallel IBM SP machines or IBM RS/6000 workstation clusters) following the replicated worker principle. This means replicating the sequential simulator and extending it by communication and synchronization facilities. During parallel simulation, n simulator instances cooperate, each instance handling a part of the original circuit model. Two of the resulting simulators, parallelTEXSIM [1] and parallelMVLSIM, are statically balanced. The simulator dlbSIM [3] is capable of dynamically balancing the application-specific load of simulator instances in dependence of the overall load situation on involved processor nodes. With dlbSIM, at any point of simulation run-time a simulator instance handles a subset of an initially assigned set of model parts. 2. Circuit Model Partitioning Taking special fan-in cones as basic building blocks for partitions, we achieve a concentration of necessary synchronization and communication between simulator instances at cycle boundaries. Our complex partitioning environment parallelMAP allows combination and competition of algorithms within a hierarchical partitioning strategy. Partitioning is guided by a partition valuation function [2] comprising parameters related to both the circuit to be simulated and the host system the simulation is intended to run on. This way, run-time estimation of parallel simulation processes starting from circuit model partitions is possible. 3. Experimental Results We consider five circuit models representing processor structures of the IBM S/390 architecture. The number of their boxes at RT/gate level is given in brackets: CLKSTR6 (187120), PICMOFP (235721), PU M5X (252982), MBA98 (512843), ML100M0S (2657165). Figure 1 shows the expected speed-up of parallelMVLSIM simulation runs on an IBM SP2 machine in dependence of the number of processors (simulator instances). 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 40 50 60 70 80 90 PU_M5X PICMOFP CLKSTR6S MBA98 ML100M0S 0 5 10 15 Protos Processors S pe ed − up Figure 1. Expected simulation speed-up	algorithm;automatic parallelization;central processing unit;fan-in;ibm personal computer;ibm systems network architecture;microprocessor;parallel computing;processor design;rs/6000;simulation;value (ethics);workstation	Klaus Hering	2002		10.1109/IPDPS.2002.1016650	acceleration;computer architecture;parallel computing;real-time computing;computer science;discrete event simulation;operating system;logic simulation;distributed computing;logic;algorithm	EDA	-12.550270464710167	38.77755604662295	180035
088fc1ce230e410a4e9a16ff13fa51a6e4e0d6d9	incremental register reallocation	incremental optimization;storage allocation;compilacion;optimisation;compilateur;optimizacion;generation code;generacion codigo;code generation;compiler;registre;algorithme;algorithm;incremental register allocation;coloration graphe;coloracion diagrama;incremental target code generator;compilation;optimization;allocation memoire;asignacion memoria;graph coloration;incremental graph colouring;registro;register;compilador;algoritmo	Abstract#R##N##R##N#Current compiler technology performs extensive code transformations either to parallelize code or to perform optimizations that reduce space or time. The cost of the analyses for these transformations is high and is further magnified when edits are made, forcing complete recompilation. By extending the concept of incremental compilation to include fine-grained, high-quality target code generation, we develop techniques that reuse the information created by an optimizing compiler to incorporate program edits efficiently into optimized target code. Our techniques include developing models of register usage for both local and global register allocation schemes and algorithms for incrementally updating these structures in response to an editing change. Based on these changes, the target code is updated. As graph colouring is a widely accepted register allocation scheme, we present techniques to rebuild and recolour interference graphs incrementally. Results from experiments that compare the performance of the incremental target code generator with its non-incremental counterpart are reported, showing a saving when incorporating changes incrementally.		Mary P. Bivens;Mary Lou Soffa	1990	Softw., Pract. Exper.	10.1002/spe.4380201005	compiler;parallel computing;computer science;theoretical computer science;operating system;programming language;algorithm;code generation	SE	-18.637243681506625	36.33097029969797	180085
7551c7cc06d97e10d98dd986549fdf4369d26b47	incremental jit compiler for implicitly parallel functional language	lisp;functional programming;parallelising compilers;program interpreters;lisp-based programming language;automatic parallelization;execution model;execution threads;functional program;implicitly parallel functional language;incremental jit compiler;interpretation;just-in-time compilation;native code;parallel code running;runtime environment	We present a novel method for automatic parallelization of functional programs which combines interpretation and just-in-time compilation. We propose an execution model for a Lisp-based programming language which involves a runtime environment which is able to identify portions of code worth running in parallel and is able to spawn new threads of execution. Furthermore, in order to achieve better performance, runtime environment dynamically identifies expressions worth compiling and compiles them into a native code.	automatic parallelization;compiler;experiment;functional programming;implicit parallelism;inline expansion;interpreted language;just-in-time compilation;lisp;machine code;parallel computing;partial template specialization;programming language;run time (program lifecycle phase);runtime system;spawn (computing);thread (computing)	Petr Krajca	2013	2013 Federated Conference on Computer Science and Information Systems		computer architecture;parallel computing;dynamic compilation;computer science;programming language;tracing just-in-time compilation	PL	-16.175465049973376	36.58861028480368	180177
38b772af973cfca9ec89413dfc18079d577e0b45	parallel skeletons for variable-length lists in sketo skeleton library	data parallel;parallel computer;mandelbrot set;parallel programs;data structure	Skeletal parallel programming is a promising solution to simplify parallel programming. The approach involves providing generic and recurring data structures like lists and parallel computation patterns as skeletons that conceal parallel behaviors. However, when we focus on lists, which are usually implemented as one-dimensional arrays, their length is restricted and fixed in existing data parallel skeleton libraries. Due to this restriction, many problems cannot be coded using parallel skeletons. To resolve this problem, this paper proposes parallel skeletons for lists of variable lengths and their implementation within a parallel skeleton library called SkeTo. The proposed skeletons enable us to solve a wide range of problems including those of twin primes, Knight's tour, and Mandelbrot set calculations with SkeTo. We tested and confirmed the efficiency of our implementation of variable-length lists through various experiments.		Haruto Tanno;Hideya Iwasaki	2009		10.1007/978-3-642-03869-3_63	parallel computing;data structure;computer science;theoretical computer science;programming language;mandelbrot set;algorithm	Logic	-12.127652142212039	34.56182738161073	180607
5c5464d5540d4bb2a9087c9cd85e0f134d4a18c9	design and engineering of a dynamic binary optimizer	optimising compilers;dynamic class loading;design engineering design optimization optimizing compilers runtime software libraries pipelines joining processes program processors java power engineering and energy;concepcion sistema;programacion automatica;static compiler optimization;binary codes;interconnected components;static compiler optimization dynamic binary optimization dynamic class loading shared libraries interconnected components;automatic programming;systeme adaptatif;software engineering;dynamic binary optimization;program optimization;dynamical system;systeme dynamique;shared libraries;software engineering optimising compilers binary codes instruction sets;adaptive systems;computer instructions;system design;compiler optimization;adaptive system;load sharing;sistema adaptativo;binary optimizations;optimisation programme;runtime system adaptive systems binary optimizations dynamic optimization;runtime system;sistema dinamico;optimizing compilers;conception systeme;programmation automatique;instruction sets;dynamic optimization;optimizacion programa	In today's software, which increasingly utilizes dynamic class loading, shared libraries, and interconnected components, the power and reach of static compiler optimization is diminishing. An exciting new paradigm of transparent dynamic binary optimization is emerging, aimed at improving the performance of a program while it executes. Recently, several dynamic binary optimization systems have appeared in the literature. They all share a fundamental property: the ability to observe and modify instructions of the executing program immediately before they run. Importantly, recent advances allow this capability to be offered without having to pay the price of performance degradation. This paper describes the intricacies of a dynamic binary optimizer and how to build the core functionalities of observing and modifying executing instructions. We illustrate the major design decisions and tradeoffs and point to the important engineering challenges.	cpu cache;correctness (computer science);cryptography;data compression;debugging;dragon ball online;dynamic programming;elegant degradation;immutable object;java classloader;library (computing);mathematical optimization;optimizing compiler;programmer;programming paradigm;qr code;run time (program lifecycle phase);software modernization;static library	Evelyn Duesterwald	2005	Proceedings of the IEEE	10.1109/JPROC.2004.840302	binary code;computer architecture;parallel computing;real-time computing;computer science;adaptive system;dynamical system;program optimization;instruction set;optimizing compiler;systems design	Arch	-16.678134654751588	38.30065966457352	180805
5273196caf551835366ff230c002cc8de98fcf2b	parallel branch-and-bound search in parlog	lenguaje programacion;methode branch and bound;concurrent logic programming;algoritmo busqueda;programming language;algorithme recherche;prolog;sistema informatico;search algorithm;concurrent program;computer system;intelligence artificielle;logical programming;artificial intelligent;resolucion problema;branch and bound method;programmation logique;metodo branch and bound;data dependence;parlog;programa competidor;langage programmation;artificial intelligence;systeme informatique;inteligencia artificial;parallel programs;branch and bound;programacion logica;problem solving;resolution probleme;programme concurrent	The concurrent logic languages, of which Parlog is one, have been promoted as a new generation of software languages specifically designed for parallel programming. This paper investigates their application to a search problem commonly used as an illustration of artificial intelligence techniques, the 8-puzzle. It notes that programs written in the concurrent logic languages which do not pay attention to the parallelism can fall into two possible traps: either there is little real parallelism in them due to data dependencies, or there is too much parallelism and any practical architecture will be overwhelmed. A solution which controls the parallelism using user-defined priorities is proposed. This solution has the advantage of being architecture-independent.	15 puzzle;artificial intelligence;branch and bound;data dependency;parallel computing;parlog;search problem	Matthew M. Huntbach	1991	International Journal of Parallel Programming	10.1007/BF01408020	parallel computing;computer science;artificial intelligence;data parallelism;programming language;prolog;instruction-level parallelism;branch and bound;implicit parallelism;algorithm;task parallelism;search algorithm	HPC	-16.42312801623005	34.09511732525086	181226
63005b3f188d5cf6b897a2ed5e0655fc4f326aac	vectorizing compilers: a test suite and results	program compilers;program testing;fortran loops;automatic vectorizing compiler;loop scoring;mainframes;minisupercomputers;supercomputers;test suite;testing methodology;vectorizing fortran compilers	This report describes a collection of 100 Fortran loops used to test the effectiveness of an automatic vectorizing compiler. We present the results of compiling these loops using commercially available, vectorizing Fortran compilers on a variety of supercomputers, mini-supercomputers, and mainframes.	benchmark (computing);compiler;correctness (computer science);email;fortran;mainframe computer;minisupercomputer;netlib;run time (program lifecycle phase);supercomputer;test suite;vector graphics	David Callahan;Jack J. Dongarra;D. Levine	1988			computer architecture;parallel computing;computer science;data analysis;programming language	HPC	-13.147821541835876	36.54119343180727	181663
2e00aa73210df97f869cae27c6cc6ddc486ae176	data structures in context-addressed cellular memories	linked data;associative memory;high level language;data structure	This report discusses the capability of an associative memory to search some useful data bases. The report utilizes a simplified cell and a collection of “assembler language” instructions to show how sets and trees can be searched in the memory. An OR rail and an EXCLUSIVE-OR rail are discussed in relation to their use to search-ordered and unordered sets, strings, and tree data structures. Linked data structures are also discussed. This report is oriented toward the software aspects of the associative memory to lead to further research in the design of high-level languages that utilize the capability of the rails.	assembly language;content-addressable memory;database;exclusive or;high- and low-level;linked data structure	G. Jack Lipovski	1972	International Journal of Computer & Information Sciences	10.1007/BF00987252	linked data structure;parallel computing;data structure;computer science;theoretical computer science;operating system;linked data;content-addressable memory;database;programming language;high-level programming language;algorithm;memory map	Logic	-14.163752613100218	35.48208638169845	183022
04e7c736452116c6b40f29801b0e302a40739581	tracking data structures for postmortem analysis: (nier track)	debugging;virtual machine;history debugging runtime arrays instruments monitoring;instruments;history;stationary properties;nier track;tracing;runtime;arrays;virtual machine data structures postmortem analysis nier track backward analysis stationary properties temporal properties taeds data evolution;monitoring;virtual machines;postmortem analysis;data structures;temporal properties;taeds;tracing data structure debugging program analysis;virtual machines data structures;program analysis;data structure;data evolution;backward analysis	Analyzing the runtime behaviors of the data structures is important because they usually relate to the obscured program performance and understanding issues. The runtime evolution history of data structures creates the possibility of building a lightweight and non-checkpointing based solution for the backward analysis for validating and mining both the temporal and stationary properties of the data structure. We design and implement TAEDS, a framework that focuses on gathering the data evolution history of a program at the runtime and provides a virtual machine for programmers to examine the behavior of data structures back in time. We show that our approach facilitates many programming tasks such as diagnosing memory problems and improving the design of the data structures themselves.	application checkpointing;data structure;programmer;stationary process;virtual machine	Xiao Xiao;Jinguo Zhou;Charles Zhang	2011	2011 33rd International Conference on Software Engineering (ICSE)	10.1145/1985793.1985938	real-time computing;data structure;computer science;virtual machine;operating system;database;programming language	SE	-18.973543493720012	34.907400851860174	183223
8fbc91755dac0d445e2f899a3bad0e4ae387ea8e	vectorization beyond data dependences	data dependence;automatic parallelization	Data dependence between statements have long been used for detecting parallelism and converting sequential programs into parallel forms. Almost all existing automatic parallelization and vectorization schemes are based on enforcing original data dependence detected in programs. In this paper, we extend the traditional vectorization algorithm to reverse some data dependence involved in multiple-statement reductions for more parallelism. The extended algorithm can vectorize multiple-statement reductions in programs and generate more and thicker vector statements than the traditional algorithm. This extended algorithm can be used to enhance all existing vectorizing compilers for supercomput ers.	algorithm;automatic parallelization;automatic vectorization;compiler;data dependency;parallel computing;sensor;switch statement	Peiyi Tang;Nianshu Gao	1995		10.1145/224538.224652	computer architecture;parallel computing;computer science;theoretical computer science;programming language;automatic parallelization	PL	-16.179809713183584	35.0786685911214	183834
716542c8b1d416c8a0f60020ef0351e1dc686126	a comparison study of automatically vectorizing fortran compilers	high level languages;high performance computing;testing;data mining;power engineering and energy;power engineering computing;permission;performance analysis;fortran;supercomputers performance analysis power engineering computing permission recruitment testing high performance computing power engineering and energy graphics high level languages;supercomputers;graphics;recruitment	The performance of current vector processing supercomputers is highly dependent on the capabilities of their automatically vectorizing Fortran compilers to generate efficient vector code. In this paper we shall focus on the performance of these compilers, and attempt to compare their efficiency on different types of Fortran loop constructs. Our analysis will involve a short analysis of previous compiler comparison studies and we will introduce a new technique for evaluating and comparing compiler performance.	compiler;fortran;supercomputer;vector graphics;vector processor	Hiromu Nobayashi;Christopher Eoyang	1989	Proceedings of the 1989 ACM/IEEE Conference on Supercomputing (Supercomputing '89)	10.1145/76263.76356	computer architecture;supercomputer;parallel computing;computer science;graphics;operating system;data mining;software testing;programming language;high-level programming language	HPC	-12.709384901991967	36.737171023897915	184121
05c918f84cf93bef9ace3a42899e4065e567b8b2	discovering parallel pattern candidates in erlang	refactoring;parallelism;concurrency;patterns;erlang;paraphrase;skeletons	The ParaPhrase Refactoring Tool for Erlang PaRTE provides automatic, comprehensive and reliable pattern candidate discovery to locate parallelisable components in Erlang programs. It uses semi-automatic and semantics-preserving program transformations to reshape source code and to introduce high level parallel patterns that can be mapped adaptively to the available hardware resources. This paper describes the main PaRTE tools and demonstrates that significant parallel speedups can be obtained.	code refactoring;erlang (programming language);high-level programming language;program transformation;semiconductor industry	István Bozó;Viktoria Fordós;Zoltán Horváth;Melinda Tóth;Dániel Horpácsi;Tamás Kozsik;Judit Köszegi;Adam D. Barwell;Christopher Brown;Kevin Hammond	2014		10.1145/2633448.2633453	parallel computing;real-time computing;computer science;programming language	HPC	-15.523029420780803	36.91082859622566	184220
82eaf61c764840e2895ff0a80cde71e77a5142da	topological analysis and visualization of cyclical behavior in memory reference traces	topology data visualisation performance evaluation storage management;topology;metric space;high dimensionality;performance evaluation;storage management;memory performance;topological analysis;data visualisation;visualization;memory performance optimization topological analysis cyclical behavior visualization memory reference traces software visualization trace record linear flow recasting high dimensional point cloud metric space automatic circular structure detection cyclical runtime program behaviors recurrence visualization radial plots multiscale visual insights;automatic detection;data visualization visualization image color analysis vectors encoding runtime measurement;circular coordinates;point cloud;topological analysis memory reference traces circular coordinates visualization;memory reference traces;software visualization	We demonstrate the application of topological analysis techniques to the rather unexpected domain of software visualization. We collect a memory reference trace from a running program, recasting the linear flow of trace records as a high-dimensional point cloud in a metric space. We use topological persistence to automatically detect significant circular structures in the point cloud, which represent recurrent or cyclical runtime program behaviors. We visualize such recurrences using radial plots to display their time evolution, offering multi-scale visual insights, and detecting potential candidates for memory performance optimization. We then present several case studies to demonstrate some key insights obtained using our techniques.	adaptive sampling;behavioral pattern;brian;brushing and linking;chroma subsampling;computation;deterministic memory;digital footprint;emoticon;energy citations database;mathematical optimization;naive bayes classifier;performance tuning;persistence (computer science);point cloud;radial (radio);radial tree;recurrence relation;sampling (signal processing);sensor;software visualization;tracing (software);usability testing;x.690	A. N. M. Imroz Choudhury;Bei Wang;Paul Rosen;Valerio Pascucci	2012	2012 IEEE Pacific Visualization Symposium	10.1109/PacificVis.2012.6183557	computer vision;computer science;theoretical computer science;machine learning	Visualization	-18.98917005269089	38.3182299511646	186135
779b9a77852507796a573e05ed09e438402cf343	the honeywell modular microprogram machine: m3	kernel machine;computer architecture;microprogramming;structured programming;distributed computing	M3 is intended for research into unconventional special purpose stored program elements of computer systems (for example, a distributed computer Bus Interface Unit). The principal requirements for such a machine are flexibility and modularity. M3 consists of an application independent Kernel Machine to which application-dependent Functional Modules are attached. The Kernel Machine is vertically microprogrammed; it includes highly capable microinstruction sequencing logic which facilitates structured microprogramming. The Functional Modules contain the operand registers and the operators (e.g., adders, shifters, etc.) needed by the specific application. The operators may be invoked not only by the Kernel Machine but also by one another; control may be passed among operators in a sequential or hierarchical fashion; operators may execute concurrently with each other and with the Kernel Machine; operators may be re-entrant or recursive; operators may activate themselves, or cause microcode interrupt routines to be executed in the Kernel Machine.	interrupt handler;kernel method;microcode;operand;recursion;requirement;stored-program computer	E. Douglas Jensen;Richard Y. Kain	1977		10.1145/800255.810648		Arch	-16.025685733794116	38.82989917589409	186946
ca1da154bc548858c9dca0fe09e0ea63d40d9626	cyclo-dynamic dataflow	digital signal processing;cyclo dynamic data flow;boolean data flow model;real time systems data flow computing signal processing parallel languages scheduling;expression analysis;real time;cyclo static dataflow;semantics;dynamic data;data dependence;scheduling;signal processing;real time digital signal processing;control flow;cddf;data flow computing;data dependent control flow;data flow model;data flow languages;digital signal processing flow graphs power system modeling context modeling production automatic control programming profession sampling methods parallel processing runtime;data flow;automatic tools;parallel languages;boolean data flow model cyclo dynamic data flow cddf data flow model real time digital signal processing cyclo static dataflow data dependent control flow semantics automatic tools data flow languages;real time systems	In this paper we present cyclo-dynamic data flow (CDDF), a new data flow model for real time digital signal processing (DSP) applications. CDDF is an extension of cyclo-static dataflow (CSDF) [1,2] which keeps the interesting properties like analyzability and efficient compile time scheduling, while introducing data dependent control flow to improve the expressivity. The semantics are constructed such that extra knowledge about the internals of the actors, which is known to the programmer, can be expressed both in a natural way, and in a syntax that can be analyzed by automatic tools. In this paper we describe the proposed model in the context of already existing data flow languages, demonstrate its schedulability and its improved analyzability as compared to the Boolean data flow model [ 3].	backward differentiation formula;central processing unit;compile time;control flow;data flow diagram;dataflow;digital signal processing;dynamic data;encoder;programmer;scheduling (computing);source-to-source compiler	Piet Wauters;Marc Engels;Rudy Lauwereins;Jean A. Peperstraete	1996		10.1109/EMPDP.1996.500603	parallel computing;real-time computing;computer science;programming language	Embedded	-15.216239731860327	37.46419884064026	187337
bb66d0c058ce13932eeb9c2b54343c843b42a0ad	flexible pointer analysis using assign-fetch graphs	pointer analysis;summary based analysis;computer science;static analysis;applied mathematics;reading and writing	"""We propose a new abstraction for pointer analysis that represents reads and writes to memory instead of traditional points-to relations. Compared to points-to graphs, our Assign-Fetch Graph (AFG) leads to concise procedure summaries that can be used in any calling context. Also, its flexibility supports new analysis techniques with different trade-offs between speed and precision.  For efficiency, we build a summary for each procedure that assumes distinct pointers from the environment are not aliased and restore soundness when the summary is used in a context with aliases.  We present two pointer analysis techniques based on our AFG. The first takes the flow-insensitive view adopted by many authors; the second considers statement ordering. In addition to being more precise, we find that this """"flow-aware"""" analysis runs faster. We conclude with experimental results showing it is practical."""	approximation algorithm;cp/m;columbia (supercomputer);computation;computer programming;context-sensitive grammar;function pointer;lam/mpi;pointer (computer programming);pointer aliasing;pointer analysis;side effect (computer science);time complexity;william l. burke	Marcio Buss;Daniel Brand;Vugranam C. Sreedhar;Stephen A. Edwards	2008		10.1145/1363686.1363746	computer science;artificial intelligence;theoretical computer science;operating system;escape analysis;database;function pointer;pointer swizzling;tagged pointer;programming language;world wide web;pointer analysis;static analysis;algorithm	SE	-18.18444576860672	33.08193448996388	189485
c3e3280098c57aab11d3d29ac560039335a6a361	an algebraic front-end for the production and use of numeric programs	minor expansion;front end;algebraic computation;symbolic determinant;symbolic linear system;programming environment;divide and conquer algorithm;modular algorithm;fortran;interval arithmetic;cramer s method;parallel processing	We describe a programming environment which combines the Macsyma algebraic manipulation system with convenient and direct access to numeric Fortran run-time libraries. With this system it is also convenient to generate, compile, load, and invoke totally new Fortran programs which may have been produced by combining algebraically derived formulas and program “templates”. These facilities, available on VAX-11 computers, provide an environment for the generation and testing of advanced scientific software. Enhancements of Fortran for high-precision calculations, interval arithmetic, and other purposes are also supported.	compiler;computer;fortran;integrated development environment;interval arithmetic;library (computing);macsyma;random access;vax-11	Douglas H. Lanam	1981		10.1145/800206.806400	computer science;theoretical computer science;programming language;algorithm	HPC	-12.07527863817466	35.2426270742807	189911
316481ad025ba2217365122396d0cc6f346fa715	a software science model of compile time	compiler performance;lenguaje programacion;compilation process;compilacion;discrimination rate;program modularity;programmation;performance evaluation;programming language;ada;time measurement;program characteristics;data collection;performance index;ingenieria logiciel;counting strategy;software engineering;software performance;programacion;fundamental relation;predictive power;halstead theory of software science;marine vehicles;ada compilers;military computing embedded computing software performance time measurement force control performance analysis aerospace electronics marine vehicles predictive models statistics;significant relationships;analyse performance;performance analysis;aerospace electronics;performance index software science model compile time halstead theory of software science compilation process compiler performance index nonlinear model ada compilers fundamental relation program modularity data collection counting strategy complexity measures significant relationships program characteristics predictive power compiler performance discrimination rate;statistics;compiler performance index;genie logiciel;langage programmation;complexity measures;compilation;predictive models;ada language;program compilers;programming;software science model;embedded computing;military computing;program compilers performance evaluation;compile time;nonlinear model;analisis eficacia;force control	Halstead’s theory of software science is used to describe the compilation process and generate a compiler performance index. A nonlinear model of compile time is estimated for four Ada compilers. A fundamental relation between compile time and program modularity is proposed. Issues considered include data collection procedures, the development of a counting strategy, the analysis of the complexity measures used, and the investigation of significant relationships between program characteristics and compile time. The results suggest that the model has a high predictive power and provides interesting insights into compiler performance phenomena. The research suggests that the discrimination rate of a compiler is a valuable performance index and is preferred to average compile time statistics.	ada;compile time;compiler;complexity;modular programming;nonlinear system	Wade H. Shaw;James W. Howatt;Robert S. Maness;Dennis M. Miller	1989	IEEE Trans. Software Eng.	10.1109/32.24703	programming;compile time;process performance index;parallel computing;real-time computing;ada;software performance testing;compiler correctness;computer science;operating system;software engineering;predictive modelling;compilation error;programming language;time;data collection	SE	-17.87118787506541	38.806571339313514	190350
512e19f69011155c9618c650a7d4e2e525a4d1dd	program analysis and transformation for holistic optimization of database applications	dalvik bytecode;java programming;data;code generation;program transformation;android;web service;computer programming;design and implementation;jimple;soot;program analysis;static analysis;programs;performance optimization;query rewriting;database query	We describe DBridge, a novel program analysis and transformation tool to optimize database and web service access. Traditionally, rewrite of queries and programs are done independently, by the database query optimizer and the language compiler respectively, leaving out many optimization opportunities. Our tool aims to bridge this gap by performing holistic transformations, which include both program and query rewrite.  There has been earlier research in this area involving program analysis and transformation for automatically rewriting database applications to perform optimizations; for example, our earlier work has addressed batching or asynchronous submission of iterative queries, and prefetching query results. DBridge implements these techniques for Java programs and internally uses Soot, a Java optimization framework, for static analysis and transformation. DBridge can perform such optimizations on Java programs that use the JDBC API to access the database. It is currently being extended to handle the Hibernate API, and Web Services.  In this paper, we describe the program transformations that DBridge can perform. We then discuss the design and implementation of DBridge with a focus on how the Soot framework has been used to achieve these goals. Finally, we conclude by discussing some of the future directions for our tool.	application programming interface;cpu cache;compiler;database;experiment;holism;iterative method;jdbc;java;mathematical optimization;program transformation;query optimization;rewrite (programming);rewriting;static program analysis;web application;web service	Karthik Ramachandra;Ravindra Guravannavar;S. Sudarshan	2012		10.1145/2259051.2259057	computer science;theoretical computer science;database;programming language	DB	-19.096192645561025	34.497799836523	190684
691cb89cc962c38d8c34a6a05430d302273659e1	the guppy language: an update		As a possible successor and perhaps replacement language for occam-π, we have been working piecemeal for the past few years on a new language – Guppy. Rather than being a completely new language, it aims to rationalise and simplify the concurrent programming concepts and idioms that we have explored whilst developing occam-π. This short talk will look at the current state of the language and its implementation, that is now able to compile and run a simple “commstime” benchmark, scheduled by the existing occam-π run-time system (CCSP).	benchmark (computing);compiler;concurrent computing;occam-π;runtime system;update (sql);while	Fred R. M. Barnes	2013			theoretical computer science;distributed computing;computer science;guppy	PL	-17.823401193728035	33.437831713440666	190824
3c6fcadb477fad4eada73998ca9c27ffec6556b2	constant propagation: a fresh, demand-driven look	iterative solver;demand driven analysis;compiler optimization;code size;constant propagation;ssa	Constant propagation is a well-known static compiler technique in which values of variables which are determined to be integral constants can be passed to expressions which use these constants. Code reduction (preventing unnecessary compilation), bounds propagation, and dead-code elimination are some of the optimizations which can bene t from this analysis. In this paper, we present a new method for detecting constants, based upon an optimistic demand-driven recursive solver, as opposed to more traditional iterative solvers. The problem with iterative solvers is that they may require expression evaluation many times, while we only classify each node after all the de nitions which reach the expressions within that node are classi ed. To consider conditional code, we augment the standard Static Single Assignment (SSA) form with merge operators called -functions, adapted from the interpretable Gated Single Assignment (GSA) model [16]. We show that our approach is fast and nds the same class of constants as other approaches. We also present preliminary experimental results which show the number of intra-procedural constants found in common high-performance Fortran codes.	compiler;constant folding;dead code elimination;global storage architecture;high performance fortran;iteration;iterative method;recursion;sensor;software propagation;solver;static single assignment form	Eric Stoltz;Michael Wolfe;Michael P. Gerlek	1994		10.1145/326619.326791	parallel computing;real-time computing;computer science;theoretical computer science;optimizing compiler;programming language;sparse conditional constant propagation;constant folding	PL	-16.35996316055582	34.78544494385501	192203
7d82df6def80f2664c9648ce99bb289fb2860a60	system level design of microcontroller applications applications	system level design;microcontroller applications;graphic user interface	1 I n t r o d u c t i o n A closer look at the number of microcontrollers in use in electronic products of our daily life reveals tha t microcontrollers have a relatively huge share of the market. So microcontroller can be found in portable telephones, fax, vending, and washing machines, automobiles and many other products. Comparing unit sales microcontrollers even outpace microprocessors [6]. Although, as it is engaged by these investigations, microcontrollers obviously represent a very important realization technology, this circumstance is not reflected in the applied design methodology. Still a majori ty of microcontroller applications are programmed in assembler because designers argue that the code generated by a high level language is not as efficient as code generated by an assembler. This objection might be legitimate at very limited memory capacities of only a few bytes however, there are many microcontrollers available which offer up to 1 kB of internal RAM. In this case good results can also be achieved when using high level languages. Furthermore, programming in assembler has the disadvantage that porting of such applications to other microcontroller platforms is combined with large expenditures. A further disadvantage of the employed design methodics is the circumstance that microcontroller applications are mostly developed directly at the target which might be good on the one hand, since one can work with the real envi-	assembly language;byte;fax;high-level programming language;internal ram;level design;microcontroller;microprocessor;random-access memory;washing machine	Christoph Schaffer;Johannes Zeindl	1997		10.1007/BFb0025079	computer-on-module	HCI	-14.960475889044325	34.10677005993193	193290
0abcbb48b8f6b078d2800533d160d9fa7ae1687c	recomputation based implementations of and-or parallel prolog		We argüe that in order to exploit both Independent And-and Or-parallelism in Pro-log programs there is advantage in recomputing some of the independent goals, as opposed to all their solutions being reused. We present an abstract model, called the Composition-Tree, for representing and-or parallelism in Prolog Programs. The Composition-tree closely mirrors sequential Prolog execution by recomputing some independent goals rather than fully re-using them. We also outline two environment representation techniques for And-Or parallel execution of full Prolog based on the Composition-tree model abstraction. We argüe that these techniques have advantages over earlier proposals for exploiting and-or parallelism in Prolog.	aurora;cyber-shot;high- and low-level;memory management;muse;parallel computing;prolog;prometheus	Gopal Gupta;Manuel V. Hermenegildo	1992			implementation;theoretical computer science;distributed computing;computer science;prolog	PL	-14.23825669660667	38.261545862740704	193952
04218fe8e7665156c7ffeb85a05a288a42d418e1	automating branch-and-bound for dynamic programs	dynamic programming;search space;dynamic program;automatic transformation;branch and bound	Dynamic programming is a powerful technique for solving optimization problems efficiently. We consider a dynamic program as simply a recursive program that is evaluated with memoization and lookup of answers. In this paper we examine how, given a function calculating a bound on the value of the dynamic program, we can optimize the compilation of the dynamic program function. We show how to automatically transform a dynamic program to a number of more efficientversions making use of the bounds function. We compare the different transformed versions on a number of example dynamic programs, and show the benefits in search space and time that can result.	branch and bound;compiler;dynamic programming;lookup table;mathematical optimization;memoization;recursion	Jakob Puchinger;Peter J. Stuckey	2008		10.1145/1328408.1328421	mathematical optimization;dynamic compilation;computer science;theoretical computer science;dynamic programming;programming language;branch and bound;algorithm	PL	-15.58286854276246	32.45242188440315	194706
f5f7c64a1b60fa2992044828f1fd099118b05c33	automated synthesis of divide and conquer parallelism		This paper focuses on automated synthesis of divide-andconquer parallelism, which is a common parallel programming skeleton supported by many cross-platform multithreaded libraries. The challenges of producing (manually or automatically) a correct divide-and-conquer parallel program from a given sequential code are two-fold: (1) assuming that individual worker threads execute a code identical to the sequential code, the programmer has to provide the extra code for dividing the tasks and combining the computation results, and (2) sometimes, the sequential code may not be usable as is, and may need to be modified by the programmer. We address both challenges in this paper. We present an automated synthesis technique for the case where no modifications to the sequential code are required, and we propose an algorithm for modifying the sequential code to make it suitable for parallelization when some modification is necessary. The paper presents theoretical results for when this modification is efficiently possible, and experimental evaluation of the technique and the quality of the produced parallel programs.	algorithm;computation;library (computing);parallel computing;programmer;thread (computing)	Azadeh Farzan;Victor Nicolet	2017	CoRR		parallel computing;divide and conquer algorithms;computer science	PL	-15.038945909788477	36.776942604927434	195140
2bd7d27c0ec331eed3991622bebc100ce795115c	code generation for complex subscripts in data-parallel programs	tratamiento datos;algoritmo paralelo;data parallel;parallel algorithm;algorithm complexity;programacion paralela;generation code;complejidad algoritmo;generacion codigo;code generation;data processing;parallel programming;traitement donnee;optimizacion compiladora;algorithme parallele;memory access;complexite algorithme;high performance fortran;compiler optimization;lexicographic order;optimisation compilateur;programmation parallele	Data parallel languages like High Performance Fortran, demand efficient compile and run-time techniques for tasks such as address generation. Array references with arbitrary affine subscripts can make the task of compilers for such languages highly involved. This paper deals with the efficient address generation in programs with array references having two types of commonly encountered affine references, namely coupled subscripts and subscripts containing multiple induction variables (MIVs). Methods discussed in the paper utilize the repetitive pattern of the memory accesses. In the case of MIV, we address this issue by presenting runtime techniques which enumerate the set of addresses in lexicographic order. Our approach to the problem incorporates a general approach of computing in O(k) time, the start element on a processor for a given global start element. Several methods are proposed and evaluated here for generating the access sequences for MIV based on problem parameters. With coupled subscripts, we present two construction techniques, namely searching and hashing which minimize the time needed to construct the tables. Extensive experiments were conducted and the results were then compared with [8] to indicate the efficiency of our approach.	algorithm;block size (cryptography);compiler;cryptographic hash function;data parallelism;enumerated type;experiment;high performance fortran;iteration;lexicographical order;overhead (computing);parallel computing;rice's theorem;time complexity	J. Ramanujam;Swaroop Dutta;Arun Venkatachar	1997		10.1007/BFb0032683	parallel computing;data processing;computer science;artificial intelligence;theoretical computer science;operating system;lexicographical order;optimizing compiler;database;mathematics;distributed computing;parallel algorithm;programming language;algorithm;code generation	HPC	-13.500971138497764	35.45877375456725	195653
0d731840a6bbd8c0f85018c4c65b885c14045623	a ring-oriented approach for block matrix factorizations on shared and distributed memory architectures	matrix factorization;memory management;numerical solution;distributed memory architecture;computer architecture;factorization;matrices;distributed memory multicomputer;parallel processing;mathematics computers information science management law miscellaneous	A block (column) wrap-mapping approach for design of parallel block matrix factorization algorithms that are (trans)portable over and between shared memory multiprocessors (SMM) and distributed memory multicomputers (DMM) is presented. By reorganizing the matrix on the SMM architecture, the same ring-oriented algorithms can be used on both SMM and DMM systems with all machine dependencies comprised to a small set of communication routines. The algorithms are described on high level with focus on portability and scalability aspects. Implementation aspects of the LU, Cholesky, and QR factorizations and machine speciic communication routines for some SMM and DMM systems are discussed. Timing results show that our portable algorithms have similar performance as machine speciic implementations.	algorithm;cholesky decomposition;digital molecular matter (dmm);distributed memory;high-level programming language;scalability;shared memory;the matrix	Krister Dackland;Erik Elmroth;Bo Kågström	1993			distributed shared memory;parallel computing;computer science;theoretical computer science;distributed computing	HPC	-12.219476817477362	38.91694355801447	195840
50592249ddbae67d3d87c20793e5dd6b639f5357	symbolic wcet computation		Parametric Worst-case execution time (WCET) analysis of a sequential program produces a formula that represents the worst-case execution time of the program, where parameters of the formula are user-defined parameters of the program (as loop bounds, values of inputs, or internal variables, etc).  In this article we propose a novel methodology to compute the parametric WCET of a program. Unlike other algorithms in the literature, our method is not based on Integer Linear Programming (ILP). Instead, we follow an approach based on the notion of symbolic computation of WCET formulae. After explaining our methodology and proving its correctness, we present a set of experiments to compare our method against the state of the art. We show that our approach dominates other parametric analyses and produces results that are very close to those produced by non-parametric ILP-based approaches, while keeping very good computing time.	algorithm;best, worst and average case;correctness (computer science);experiment;integer programming;linear programming;run time (program lifecycle phase);symbolic computation;worst-case execution time	Clément Ballabriga;Julien Forget;Giuseppe Lipari	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3147413	theoretical computer science;parallel computing;worst-case execution time;computation;correctness;computer science;symbolic computation;parametric statistics;integer programming	Embedded	-15.751520356185459	32.756653889449176	196331
adfc0ec12042abbc019917b3d298f5cfcd49115a	simd vectorization of nested loop based on strip mining	dependence distance;support vector machines;data processing;strips support vector machines parallel processing data mining fellows registers data processing;simd extension;data mining;local parallel simd extension dependence distance dependence cycle strip mining;support vector machines data mining parallel processing;fellows;registers;dependence cycle;strip mining;simd vectorization codegen multilevel loop vector code generation algorithm vector machine simd extension strip mining nested loop;strips;parallel processing;local parallel	The difference between vector machine and SIMD extension is analyzed at the very start. The multilevel loop vector code generation algorithm termed Codegen put forward by Kennedy and other fellows can't be directly applied to SIMD extension as it is oriented to vector machine. The vectorization algorithm in state-of-the-art compilers can only process one level of nested loop. In order to vectorize the entire nested loop, a vectorization algorithm based on strip mining called simdcodegen is proposed. Firstly, the formation reason of dependence circles is analyzed and the role of strip mining played in elimination of dependence circles is discussed. Then on the basis of codegen, strip mining is applied on each level of the loop recursively to explore the SIMD parallelism in the nested loop. Although strip mining is always legitimate, the executing cost increases after strip mining. To assure that strip mining is beneficial, that is to say, strip mining can break some dependence circles, cycle broken test is applied before implementation of strip mining. Effectiveness of this method is verified by the experimental results.	algorithm;automatic vectorization;code generation (compiler);compiler;control flow;parallel computing;recursion;simd	Jinlong Xu;Huihui Sun;Rongcai Zhao	2015	2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2015.7176176	support vector machine;parallel processing;strips;parallel computing;real-time computing;data processing;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;database;processor register	SE	-15.797664951800694	35.24249935718923	196463
cbb8c6733e8a0ce04afd32b56f2a980766fc05f7	linear branch entropy: characterizing and optimizing branch behavior in a micro-architecture independent way	analytical models;history;measurement;performance;indexing;optimising compilers entropy;linear branch entropy easy to predict branches compiler optimizations hard to predict branches branch classification models single branch entropy profile miss rate prediction branch misprediction rate specific branch predictor microarchitecture independent way branch behavior optimization branch behavior characterization;predictive models;entropy;correlation;performance experimentation measurement;experimentation;entropy history predictive models measurement correlation indexing analytical models	In this paper, we propose linear branch entropy, a new metric for characterizing branch behavior. Linear branch entropy is independent of the configuration of a specific branch predictor, but is highly correlated with the branch misprediction rate of any predictor. In particular, we empirically derive a linear relationship between linear branch entropy and branch misprediction rate, which enables predicting miss rates for a range of branch predictors using a single branch entropy profile. Linear branch entropy is more accurate than previously proposed branch classification models, such as taken rate and transition rate. In addition, linear branch entropy provides insight for both analyzing an application’s inherent branch behavior as well as for understanding a branch predictor’s performance for easy-to-predict versus hard-to-predict branches. We present several case studies, ranging from comparing state-of-the-art branch predictors to compiler optimizations. More in particular, we find that the winner of the latest branch predictor competition outperforms the runners-up on easy-to-predict branches, but performs worse on hard-to-predict branches. We also show that using linear branch entropy to guide if-conversion in compilers leads to better performance compared to standard if-conversion heuristics.	branch misprediction;branch predictor;heuristic (computer science);kerrison predictor;optimizing compiler	Sander De Pestel;Stijn Eyerman;Lieven Eeckhout	2017	IEEE Transactions on Computers	10.1109/TC.2016.2601323	search engine indexing;entropy;mathematical optimization;performance;branch target predictor;computer science;theoretical computer science;predictive modelling;correlation;branch and bound;algorithm;measurement	Arch	-18.133129289110208	37.527807011609745	197168
2d8be5e1b88ac9919984b9369f7045fbb0af0d08	simplifying scalable graph processing with a domain-specific language	bi directional data flow analysis;single assignment form;demand and control driven execution	Large-scale graph processing, with its massive data sets, requires distributed processing. However, conventional frameworks for distributed graph processing, such as Pregel, use non-traditional programming models that are well-suited for parallelism and scalability but inconvenient for implementing non-trivial graph algorithms. In this paper, we use Green-Marl, a Domain-Specific Language for graph analysis, to intuitively describe graph algorithms and extend its compiler to generate equivalent Pregel implementations. Using the semantic information captured by Green-Marl, the compiler applies a set of transformation rules that convert imperative graph algorithms into Pregel's programming model. Our experiments show that the Pregel programs generated by the Green-Marl compiler perform similarly to manually coded Pregel implementations of the same algorithms. The compiler is even able to generate a Pregel implementation of a complicated graph algorithm for which a manual Pregel implementation is very challenging.	algorithm;compiler;distributed computing;domain-specific language;experiment;graph (abstract data type);graph (discrete mathematics);graph theory;imperative programming;list of algorithms;parallel computing;programming model;scalability	Sungpack Hong;Semih Salihoglu;Jennifer Widom;Kunle Olukotun	2014		10.1145/2544137.2544162	parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language	PL	-12.862170114597225	34.00664164009573	199249
