id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
fd29ec2eec586666e9161006cd14e568eea02d98	wpcb-tree: a novel flash-aware b-tree index using a write pattern converter		For the past few years, flash memory has been widely used because of its prominent advantages such as fast access speed, nonvolatility, high reliability, and low power consumption. However, flash memory still has several drawbacks that need to be overcome, e.g., the erase-before-write characteristic and a limited life cycle. Among these drawbacks, the erase-before-write characteristic causes the B-tree implementation on flash memory to be inefficient because it generates many erase operations. This study introduces a novel B-tree index structure using a write pattern converter (WPCB-tree) for flash memory. A WPCB-tree can minimize the risk of data loss and can improve the performance of the B-tree on flash memory. This WPCB-tree uses some blocks of flash memory as a buffer that temporarily stores all updated nodes. When the buffer is full, a buffer block is selected by a greedy algorithm, then the node pages in the block are converted into a sequential write pattern, and finally they are written into flash memory. In addition, in the case that all key values of a leaf node are continuously inserted, the WPCB-tree does not split the leaf node. As a result, this mechanism helps the WPCB-tree reduce the number of write operations on the flash memory. The experimental results show that the proposed B-tree variant on flash memory yields a better performance than that of other existing variants of the B-tree.	b-tree;embedded system;flash memory;greedy algorithm;hardware restriction;overhead (computing);personal computer;random access;sequential access;server (computing);tree (data structure)	Van Phi Ho;Dong-Joo Park	2018	Symmetry	10.3390/sym10010018	parallel computing;combinatorics;mathematics;tree (data structure);b-tree;data loss;greedy algorithm;flash memory	DB	-12.353104736009596	54.48952136007982	137236
d527dcfc945fef1d58077628b7f92432a866e392	dynamic load balancing and pricing in grid computing with communication delay	dynamic load balancing;grid computing environment;information exchange;communication cost;communication delay;load distribution;load balance;distributed algorithm;grid computing	Due to the emergence of Grid computing over the Internet, there is presently a need for dynamic load balancing algorithms which take into account the characteristics of Grid computing environments. In this paper, we consider a Grid architecture where computers belong to dispersed administrative domains or groups which are connected with heterogeneous communication bandwidths. We address the problem of determining which group an arriving job should be allocated to and how its load can be distributed among computers in the group to optimize the performance. We propose algorithms which guarantee finding a load distribution over computers in a group that leads to the minimum response time or computational cost. We then study the effect of pricing on load distribution by considering a simple pricing function. We develop three fully distributed algorithms to decide which group the load should be allocated to, taking into account the communication cost among groups. These algorithms use different information exchange methods and a resource estimation technique to improve the accuracy of load balancing. We conducted extensive simulations to evaluate the performance of the proposed algorithms and strategies.	algorithmic efficiency;computation;computer;convex function;distributed algorithm;emergence;feasible region;first-order predicate;grid computing;hessian;information exchange;karush–kuhn–tucker conditions;lagrange multiplier;lagrangian system;load balancing (computing);nash equilibrium;overhead (computing);polyhedron;response time (technology);simulation;tucker decomposition	Qin Zheng;Chen-Khong Tham;Bharadwaj Veeravalli	2007	Journal of Grid Computing	10.1007/s10723-007-9093-5	round-robin dns;distributed algorithm;network load balancing services;parallel computing;real-time computing;information exchange;computer science;weight distribution;load balancing;distributed computing;grid computing	HPC	-18.782116064676476	59.63496817018443	137336
ad2a86059e701aa5a28aeec22cb2b1ea410801bd	exploring forensic implications of the fusion drive	computer forensics;forensics;fusion drive	This paper explores the forensic implications of Apple’s Fusion Drive. The Fusion Drive is an example of auto-tiered storage. It uses a combination of a flash drive and a magnetic drive. Data is moved between the drives automatically to maximize system performance. This is different from traditional caches because data is moved and not simply copied. The research included understanding the drive structure, populating the drive, and then accessing data in a controlled setting to observe data migration strategies. It was observed that all the data is first written to the flash drive with 4 GB of free space always maintained. If data on the magnetic drive is frequently accessed, it is promoted to the flash drive while demoting other information. Data is moved at a block-level and not a file-level. The Fusion Drive didn’t alter the timestamps of files with data migration.	hierarchical storage management;memory hierarchy;population;usb flash drive	Shruti Gupta;Marcus Rogers	2014	JDFSL		embedded system;computer hardware;engineering;computer security	OS	-12.089896542486311	54.05670952151157	137413
01942ac8b425122fa6422463b821447d38c24e2e	shark: fast data analysis using coarse-grained distributed memory	databases;distributed memory;resilient distributed dataset;fault tolerant;query processing;large dataset;shark;data processing;learning machines;data analysis;machine learning;spark;data warehouse;coarse grained;distributed shared memory;memory devices	Shark is a research data analysis system built on a novel coarse-grained distributed shared-memory abstraction. Shark marries query processing with deep data analysis, providing a unified system for easy data manipulation using SQL and pushing sophisticated analysis closer to data. It scales to thousands of nodes in a fault-tolerant manner. Shark can answer queries 40X faster than Apache Hive and run machine learning programs 25X faster than MapReduce programs in Apache Hadoop on large datasets.	apache hadoop;apache hive;database;distributed memory;distributed shared memory;fault tolerance;machine learning;mapreduce;sql	Cliff Engle;Antonio Lupher;Reynold Xin;Matei Zaharia;Michael J. Franklin;Scott Shenker;Ion Stoica	2012		10.1145/2213836.2213934	distributed shared memory;fault tolerance;parallel computing;distributed memory;data processing;spark;computer science;data warehouse;data mining;database;data analysis;programming language	DB	-17.03300858872865	53.97360958731568	137971
144b1ba05cb64b493858b665cc38374f3ef7e332	ssdalloc: hybrid ssd/ram memory management made easy	slower form;hybrid main memory management;intrusive application change;memory footprint;ram memory management;memory hierarchy;hard drive;solid-state disk;hybrid ssd;ram capacity;raw performance	We introduce SSDAlloc, a hybrid main memory management system that allows developers to treat solid-state disk (SSD) as an extension of the RAM in a system. SSDAlloc moves the SSD upward in the memory hierarchy, usable as a larger, slower form of RAM instead of just a cache for the hard drive. Using SSDAlloc, applications can nearly transparently extend their memory footprints to hundreds of gigabytes and beyond without restructuring, well beyond the RAM capacities of most servers. Additionally, SSDAlloc can extract 90% of the SSD’s raw performance while increasing the lifetime of the SSD by up to 32 times. Other approaches either require intrusive application changes or deliver only 6– 30% of the SSD’s raw performance.	b+ tree;data center;gigabyte;hard disk drive;hash table;memcached;memory hierarchy;memory management;paging;random-access memory;solid-state drive	Anirudh Badam;Vivek S. Pai	2011			parallel computing;real-time computing;computer hardware;operating system	OS	-14.211059867592839	53.510834192655686	139286
4f506a71d2ae50cc439c104aeabb89fcd9617ac5	a simply energy-efficient migration algorithm of processes with virtual machines in server clusters		Virtual services on computation resources like CPUs and storages are supported to applications by using virtual machines in server clusters. Virtual machines with application processes can migrate from host servers to guest servers. In this paper, we discuss a process migration approach to reducing total electric energy consumption of servers by taking advantage of virtual machines in a cluster. We newly propose a Simple Globally-Energy-Aware Migration (SGEAM) algorithm. Here, a host server is first found to perform a process issued by an application in a cluster and then the process is performed on a virtual machine. Next, a virtual machine migrates from a host server to another guest server in order to reduce the electric energy consumption of servers. Here, the amount of computation to be performed by current processes on a virtual machine is simply estimated only by using the number of the current processes and then the total electric energy to be consumed by each server is estimated. Then, a guest server where a virtual machine migrates from a host server is selected so that the total electric energy to be consumed by not only the host server and guest server but also the other servers can be reduced. We show the total electric energy consumption of servers can be reduced in the SGEAM algorithm compared with other non-migration and migration algorithms in the evaluation.	algorithm;central processing unit;computation;process migration;server (computing);virtual machine	Ryo Watanabe;Dilawaer Duolikun;Tomoya Enokido;Makoto Takizawa	2017	JoWUA		cluster (physics);efficient energy use;virtual machine;distributed computing;computer science	HPC	-18.04057847412607	58.71556811519462	139498
1e5526ff053990acc9005b92bd08abc4225d137d	high performance temporal indexing on modern hardware	cache storage;buffer storage;historical nodes transaction time databases disk storage temporal indexing latch free infrastructure bw tree llama cache storage subsystem llama mapping table historical data migration cloud storage index node time splitting difficulty;indexing hardware buffer storage cleaning cache storage;indexing;multi version indexing modern hardware latch free log structured temporal database;transaction processing cache storage concurrency control database indexing temporal databases;cleaning;hardware	Transaction time databases can be put to a number of valuable uses, auditing, regulatory compliance, readable backups, and enabling multi-version concurrency control. While additional storage for retaining multiple versions is unavoidable, compression and the declining cost of disk storage largely removes that impediment to supporting multi-version data. Not clear has been whether effective indexing of historical versions, can be achieved at high performance. The current paper shows how temporal indexing can exploit the latch-free infrastructure provided for the Bw-tree by the LLAMA cache/storage subsystem to support high performance. Further, it demonstrates how the LLAMA mapping table can be exploited to simultaneously enable migration of historical data, e.g. to cloud storage, while overcoming the index node time splitting difficulty that has arisen in the past when historical nodes are migrated.	associative entity;backup;cloud storage;concurrency (computer science);database;disk storage;multiversion concurrency control;transaction time	David B. Lomet;Faisal Nawab	2015	2015 IEEE 31st International Conference on Data Engineering	10.1109/ICDE.2015.7113368	search engine indexing;parallel computing;real-time computing;computer science;data mining;database	DB	-13.603134745994565	54.39700431039001	140604
8e7ade3fdfd4aa4d0bdbe79926598a88fe61aea2	temporality a nvram-based virtualization platform	nvram;random access memory;nonvolatile random access memory nvram based virtualization platform temporality virtual machines virtual persistent memory volatile data safety overall recovery time reduction performance degradation prevention;virtualization;memory management;power system restoration;resource management power system faults power system restoration cloud computing memory management random access memory virtualization;resource management;cloud computing nvram virtualization;power system faults;virtualisation data protection random access storage virtual machines;cloud computing	Power failures in data centers and Cloud Computing infrastructures can cause loss of data and impact revenue. Existing best practice such as persistent logging and checkpointing add overhead during operation and increase recovery time. Other solutions like the use of an uninterruptable power supply incur additional costs and are maintenance-intensive. Novel persistent main memory, i.e. memory that retains stored data without an external source of power, firstly prevents data loss in case of a power outage, secondly reduces the time for a system reboot and thirdly enables to continue operation at full-speed after a recovery. Yet new architectures and programming models are required to utilize persistent main memory. We present Temporality a virtualization layer that runs virtual machines in persistent memory and offers virtual persistent memory. It can be used as a basis for future Cloud platforms to allow applications the utilization of persistent memory without any changes. It provides safety of volatile data, significantly decreases overall recovery time and prevents subsequent performance degradation.	application checkpointing;best practice;booting;cloud computing;computer data storage;data center;downtime;elegant degradation;non-volatile random-access memory;overhead (computing);paging;persistence (computer science);persistent memory;reboot (computing);uninterruptible power supply;virtual machine;x86 virtualization	Vasily A. Sartakov;Arthur Martens;Rüdiger Kapitza	2015	2015 IEEE 34th Symposium on Reliable Distributed Systems (SRDS)	10.1109/SRDS.2015.42	uniform memory access;embedded system;interleaved memory;full virtualization;parallel computing;real-time computing;virtualization;thin provisioning;cloud computing;computer science;physical address;virtual memory;resource management;operating system;database;distributed computing;overlay;non-volatile random-access memory;extended memory;computer security;computing with memory;computer network;memory management	Arch	-13.553504763659959	55.045875946760546	140959
a9a0880bc3aadd625f7e231faedcc5cb6c77f48a	flashqueryfile: flash-optimized layout and algorithms for interactive ad hoc sql on big data		High performance storage layer is vital for allowing interactive ad hoc SQL analytics (OLAP style) over Big Data. The paper makes a case for leveraging flash in the Big Data stack to speed up queries. State-ofthe-art Big Data layouts and algorithms are optimized for hard disks (i.e., sequential access is emphasized over random access) and result in suboptimal performance on flash given its drastically different performance characteristics. While existing columnar and row-columnar layouts are able to reduce disk IO compared to row-based layouts, they still end up reading significant columnar data irrelevant to the query as they only employ coarse-grained, intra-columnar data skipping which doesn’t work across all queries. FlashQueryFile’s specialized columnar data layouts, selection, and projection algorithms fully exploit fast random accesses and high internal I/O parallelism of flash to allow fast and I/O-efficient query processing and fine-grained, intra-columnar data skipping to minimize data read per query. FlashQueryFile results in 11X-100X TPC-H query speedup and 38%-99.08% reduction in data read compared to flash-based HDD-optimized row-columnar data layout and its associated algorithms.	algorithm;big data;column-oriented dbms;data structure;database;disk storage;full table scan;hard disk drive;hoc (programming language);ibm tivoli storage productivity center;input/output;online analytical processing;parallel computing;random access;relevance;row hammer;sql;sequential access;speedup;throughput	Rini T. Kaushik	2014			computer science;theoretical computer science;data mining;database	DB	-14.178155855841146	54.05792303828316	141964
2237d2c21f0963ec110e3cd9cbaa84b1a6118b59	crux: locality-preserving distributed systems		Distributed systems achieve scalability by balancing load across many machines, but wide-area distribution can introduce worst-case response latencies proportional to the network’s delay diameter. Crux is a general framework to build locality-preservingdistributed systems, by transforming some existing scalable distributed algorithm A into a new algorithmĀ that guarantees for any two clientsu and v interacting via service requests to Ā, these interactions exhibit worst-case response latencies proportional to the network delay between u andv. Locality-preserving PlanetLab deployments of a memcached distributed cache, a bamboo distributed hash table, and aredis publish/subscribe service indicate that Crux is effective and applicable to a variety of existing distributed algorithms. Crux achieves several orders of magnitude latency improvement for localized interactions at the cost of increasing per-node overheads, as each physical node must participate in multiple instances of algorithmA.	bamboo;best, worst and average case;distributed algorithm;distributed cache;distributed computing;distributed hash table;graph (abstract data type);interaction;locality of reference;memcached;planetlab;prototype;publish–subscribe pattern;scalability	Michael F. Nowlan;Jose Faleiro;Bryan Ford	2014	CoRR		distributed algorithm;parallel computing;real-time computing;computer science;operating system;distributed computing	Networks	-18.925479998121997	55.83265852138815	142236
29c2c2026fef304b5b2375b93506ab8c01a8c860	effective prediction of job processing times in a large-scale grid environment	prediction method;dynamic exponential smoothing method;grid applications;processor scheduling;large scale;dynamic exponential smoothing method job processing times grid environment;grid environment;exponential smoothing;job processing times;large scale systems prediction methods smoothing methods grid computing data analysis accuracy time measurement economic forecasting environmental economics delay;processor scheduling grid computing;grid computing	Grid applications that use a considerable number of processors for their computations need effective predictions of the expected computation times on the different nodes. Currently, there are no effective prediction methods available that satisfactorily cope with those ever-changing dynamics of computation times in a grid environment. Motivated by this, in this paper we develop the dynamic exponential smoothing (DES) method to predict job processing times in a grid environment. To compare predictions of DES to those of the existing prediction methods, we have performed extensive experiments in a real large-scale grid environment. The results illustrate a strong and consistent improvement of DES in comparison with the existing prediction methods	central processing unit;computation;experiment;exponential backoff;smoothing;time complexity	Menno Dobber;Robert D. van der Mei;Ger Koole	2006	2006 15th IEEE International Conference on High Performance Distributed Computing	10.1109/HPDC.2006.1652183	exponential smoothing;real-time computing;computer science;theoretical computer science;distributed computing;grid computing	HPC	-17.469499065847415	59.33993461331809	142652
95aa237a7c50a026b15a9fee731889da020bc8a2	impatience is a virtue: revisiting disorder in high-performance log analytics		There is a growing interest in processing real-time queries over out-of-order streams in this big data era. This paper presents a comprehensive solution to meet this requirement. Our solution is based on Impatience sort, an online sorting technique that is based on an old technique called Patience sort. Impatience sort is tailored for incrementally sorting streaming datasets that present themselves as almost sorted, usually due to network delays and machine failures. With several optimizations, our solution can adapt to both input streams and query logic. Further, we develop a new Impatience framework that leverages Impatience sort to reduce the latency and memory usage of query execution, and supports a range of user latency requirements, without compromising on query completeness and throughput, while leveraging existing efficient in-order streaming engines and operators. We evaluate our proposed solution in Trill, a high-performance streaming engine, and demonstrate that our techniques significantly improve sorting performance and reduce memory usage – in some cases, by over an order of magnitude.	big data;merge sort;patience sorting;real-time clock;requirement;sorting algorithm;stream (computing);streaming algorithm;throughput;usability;web analytics	Badrish Chandramouli;Jonathan Goldstein;Yinan Li	2018	2018 IEEE 34th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2018.00067	real-time computing;streams;throughput;latency (engineering);database;big data;sort;out-of-order execution;computer science;analytics;sorting	DB	-18.499638741133065	55.17571080802076	142702
03f189fb9a2331e64a918b67f38cdfb04e53863b	impact of the execution context on grid job performances	production grid;grid latency;job submission managers;large scale systems runtime refining job production systems quality of service time measurement delay systems supercomputers processor scheduling robustness;execution context;egee grid infrastructure;submission date;job performance;submission date execution context grid jobs grid latency egee grid infrastructure statistically correlated production grid job submission managers job execution sites;job execution sites;grid jobs;statistically correlated;grid computing	In this paper, we examine how the execution context of grid jobs can help to refine submission strategies on a production grid. On this kind of infrastructure, the latency highly impacts performances. We present experiments that quantify the dependencies between the grid latency and both internal and external context parameters on the EGEE grid infrastructure. We show how job submission managers, job execution sites and the submission date can be statistically correlated to grid performances.	egi;experiment;job stream;performance	Tristan Glatard;Diane Lingrand;Johan Montagnat;Michel Riveill	2007	Seventh IEEE International Symposium on Cluster Computing and the Grid (CCGrid '07)	10.1109/CCGRID.2007.62	parallel computing;real-time computing;computer science;job performance;database;management;drmaa;grid computing	HPC	-19.10976059278523	60.015921919468866	142956
5d4807c6c97904fc205f80c2231e68d78646409c	on the performance of trace locality of reference	memory management;performance evaluation;locality of reference;data prefetching;data access;reference data;memory hierarchy	In this paper, trace locality of reference (LoR) is identified as a mechanism to predict the behavior of a variety of systems. If two objects were accessed nearby in the past and the first one is accessed again, trace LoR predicts that the second one will be accessed in near future. To capture trace LoR, trace graph is introduced. Although trace LoR can be observed in a variety of systems, but the focus of this paper is to characterize it for data accesses in memory management systems. In this field, it is compared with recency-based prediction (LRU stack) and it is shown that not only the model is much simpler, but also it outperforms recency-based prediction in all cases. The paper examines various parameters affecting trace LoR such as object size, caching effects (address reference stream versus miss address stream), and access type (read, write, or both). It shows that object size does not have meaningful effects on trace LoR; in average the predictability of miss address stream is 30% better than address reference stream; and identifying access type can increase predictability. Finally, two enhancements are introduced to the model: history and multiple LRU prediction. A main contribution of this paper is the introduction of the n-stride prediction. For a prediction to be useful, we should have sufficient time to load the object, and n-stride prediction shows that trace LoR can predict an access far ahead from its occurrence.	locality of reference	Ali Mahjur;Amir-Hossein Jahangir;A. H. Gholamipour	2005	Perform. Eval.	10.1016/j.peva.2004.10.018	locality of reference;data access;parallel computing;real-time computing;reference data;computer science;operating system;database;memory management	PL	-12.692302292251103	54.7152479701573	143477
1fe381a30d5260c081c406012ac6b5a550c618f3	eafr: an energy-efficient adaptive file replication system in data-intensive clusters	servers energy consumption delays power demand bandwidth mathematical model adaptive systems;telecommunication services data handling energy conservation file servers parallel processing power consumption synchronisation telecommunication power management;power consumption energy efficient adaptive file replication system eafr data intensive clusters file heterogeneity server heterogeneity data availability file system efficiency replication delay request response delay waste resources network bandwidth service requests replica servers time varying file popularities	In data intensive clusters, a large amount of files are stored, processed and transferred simultaneously. To increase the data availability, some file systems create and store three replicas for each file in randomly selected servers across different racks. However, they neglect the file heterogeneity and server heterogeneity, which can be leveraged to further enhance data availability and file system efficiency (in terms of replication delay and request response delay). As files have heterogeneous popularities, a rigid number of three replicas may not provide immediate response to an excessive number of read requests to hot files, and waste resources (including energy) for replicas of cold files that have few read requests. Also, servers are heterogeneous in network bandwidth, hardware configuration and capacity (i.e., the maximal number of service requests that can be supported simultaneously), it is crucial to select replica servers to ensure low replication delay and request response delay. In this paper, we propose an Energy-Efficient Adaptive File Replication System (EAFR), which incorporates three components. It is adaptive to time-varying file popularities to achieve a good tradeoff between data availability and efficiency. Higher popularity of a file leads to more replicas and vice versa. Also, to achieve energy efficiency, servers are classified into hot servers and cold servers with different energy consumption, and cold files are stored in cold servers. Further, EAFR selects a server with sufficient capacity (including network bandwidth and capacity) to hold a replica. Experimental results on a real-world cluster show the effectiveness of EAFR in reducing file read latency, replication time, and power consumption in large clusters.	central processing unit;computer cluster;data-intensive computing;function overloading;ibm notes;locality of reference;maximal set;microsoft research;randomness;server (computing)	Yuhua Lin;Haiying Shen	2015	2015 24th International Conference on Computer Communication and Networks (ICCCN)	10.1109/ICCCN.2015.7288402	self-certifying file system;embedded system;real-time computing;torrent file;device file;computer science;stub file;operating system;ssh file transfer protocol;journaling file system;open;distributed file system;file system fragmentation;server;replication;computer network	OS	-15.905793546168397	56.56285940203807	143618
626ef55458b410dadfa4df518045866ef88fdc02	adaptation of distributed file system to vdi storage by client-side cache		Distributed File system was widely used as a cloud computing backend storage with high scalability, low cost, and reasonable sequential I/O performance. VDI I/O workload is mostly composed of small random I/O and distributed file system does not have enough performance for this I/O pattern. Compensating for this gap, we applied a cache using memory and SSD (Solid State Drive) to distributed file system. This cache was implemented in the file system’s client side. It uses memory as write cache and SSD as read cache with write-back policy for minimizing the I/O response time. Based distributed file system was implemented on user-level using Fuse (File system in User space) framework. When the client-side cache was used, the file server which previously could not support even 1024 users was improved enough to support more than 3000 users. By applying the client-side cache, it is possible to solve the performance drawback of the distributed file system used for VDI storage.	cpu cache;cache (computing);client-side;cloud computing;clustered file system;dce distributed file system;desktop virtualization;file server;input/output;response time (technology);scalability;server (computing);solid-state drive;user space	Chei-Yol Kim;Sangmin Lee;Young-Kyun Kim;Dae-Wha Seo	2016	JCP	10.17706/jcp.11.1.10-17	self-certifying file system;parallel computing;cache coloring;page cache;torrent file;memory-mapped file;device file;computer file;cache;computer science;stub file;versioning file system;cache invalidation;operating system;unix file types;ssh file transfer protocol;journaling file system;database;open;distributed file system;file system fragmentation;cache algorithms;cache pollution;file control block	OS	-13.99497838067561	53.48489340945182	143720
283b9bd4b43b7f82fa23a7b23f3c2b4307073204	an efficient submesh allocation scheme for two-dimensional meshes with little overhead	multiprocessor interconnection networks;maximal free submesh;processor scheduling;resource allocation;resource allocation multiprocessor interconnection networks processor scheduling;allocation;mesh;forbidden region;computer society topology multiprocessing systems very large scale integration delay detection algorithms;allocation overhead submesh allocation two dimensional mesh systems complexity performance;right border line	ÐThis paper presents a submesh allocation scheme for two-dimensional mesh systems. The submesh detection process considers only those available free submeshes that border from the left on some allocated submeshes or have their left boundaries aligned with that of the mesh. Fragmentation in the system can be reduced as a result. More importantly, we present an efficient approach to facilitate the detection of such available submeshes. The basic idea of the approach is to place the allocated submeshes of the busy set in a certain order so as to reduce the complexity of subtraction operations required for submesh detection. The method is simple and causes an amount of overhead which is only a fraction of that produced by previous algorithms. Extensive simulation has been conducted to evaluate the performance of the proposed scheme. The results show that when allocation overhead is considered, the proposed scheme may outperform previous methods. Index TermsÐAllocation, forbidden region, maximal free submesh, mesh, right border line.	algorithm;fragmentation (computing);maximal set;overhead (computing);reduction (complexity);scheme;simulation	Ge-Ming Chiu;Shin-Kung Chen	1999	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.770192	parallel computing;real-time computing;resource allocation;computer science;distributed computing;computer network	DB	-13.049357752524916	58.67899405340569	144458
accbfc0624c6bf214a63d6c7a83472b3be66972b	streaming data from hdd to gpus for sustained peak performance	paper;biology;cuda;tesla s2050;nvidia;computational biology;nvidia quadro fx 6000	In the context of the genome-wide association studies (GWAS), one has to solve long sequences of generalized least-squares problems; such a task has two limiting factors: execution time –often in the range of days or weeks– and data management –data sets in the order of Terabytes. We present an algorithm that obviates both issues. By pipelining the computation, and thanks to a sophisticated transfer strategy, we stream data from hard disk to main memory to GPUs and achieve sustained peak performance; with respect to a highly-optimized CPU implementation, our algorithm shows a speedup of 2.6x. Moreover, the approach lends itself to multiple GPUs and attains almost perfect scalability. When using 4 GPUs, we observe speedups of 9x over the aforementioned implementation, and 488x over a widespread biology library.	algorithm;central processing unit;computation;computer data storage;gsc bus;generalized least squares;graphics processing unit;hard disk drive;hypertext transfer protocol;moore's law;period-doubling bifurcation;pipeline (computing);run time (program lifecycle phase);scalability;speedup;stream (computing);terabyte;tesla (microarchitecture)	Lucas Beyer;Paolo Bientinesi	2013	CoRR		parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing	HPC	-17.613564883868662	54.03887110848155	145236
f4440942e1b5fee67d92e17a23a07da29af2e71d	in-transit molecular dynamics analysis with apache flink		In this paper, an on-line parallel analytics framework is proposed to process and store in transit all the data being generated by a Molecular Dynamics (MD) simulation run using staging nodes in the same cluster executing the simulation. The implementation and deployment of such a parallel workflow with standard HPC tools, managing problems such as data partitioning and load balancing, can be a hard task for scientists. In this paper we propose to leverage Apache Flink, a scalable stream processing engine from the Big Data domain, in this HPC context. Flink enables to program analyses within a simple window based map/reduce model, while the runtime takes care of the deployment, load balancing and fault tolerance. We build a complete in transit analytics workflow, connecting an MD simulation to Apache Flink and to a distributed database, Apache HBase, to persist all the desired data. To demonstrate the expressivity of this programming model and its suitability for HPC scientific environments, two common analytics in the MD field have been implemented. We assessed the performance of this framework, concluding that it can handle simulations of sizes used in the literature while providing an effective and versatile tool for scientists to easily incorporate on-line parallel analytics in their current workflows.	apache flink;apache hbase;big data;care-of address;data domain;deploy;disk staging;distributed database;equilibrium;fault tolerance;load balancing (computing);mapreduce;molecular dynamics;online and offline;programming model;scalability;simulation;stream processing;executing - querystatuscode	Henrique C. Zan&#250;z;Bruno Raffin;Omar A. Mures;Emilio J. Padr&#243;n	2018		10.1145/3281464.3281469	big data;stream processing;programming paradigm;scalability;load balancing (computing);distributed database;python (programming language);analytics;distributed computing;computer science	HPC	-18.717967165562385	54.6877043544942	145396
eaf504d40ba3094467bdf0d355be793270f5f5c9	position: short object lifetimes require a delete-optimized storage system	storage system;satisfiability;data cache;file system;clustered data;reading and writing	Early file systems were designed with the expectation that data would typically be read from disk many times before being deleted; on-disk structures were therefore optimized for reading. As main memory sizes increased, more read requests could be satisfied from data cached in memory, motivating file system designs that optimize write performance. Here, we describe how one might build a storage system that optimizes not only reading and writing, but creation and deletion as well. Efficiency is achieved, in part, by automating deletion based on relative retention values rather than requiring data be deleted explicitly by an application. This approach is well suited to an emerging class of applications that process data at consistently high rates of ingest. This paper explores trade-offs in clustering data by retention value and age and examines the effects of allowing the retention values to change under application control.	cluster analysis;computer data storage	Fred Douglis;John Palmer;Elizabeth S. Richards;David Tao;William H. Tetzlaff;John M. Tracey;Jian Yin	2004		10.1145/1133572.1133593	real-time computing;computer hardware;computer science;database	OS	-12.270415443663758	54.40852478557328	145875
6442dca752fa1875b76d45e9e22f4cab341f3fd6	a static workload balance scheduling algorithm	processor scheduling;resource allocation;parallel programming;scheduling algorithm;computational complexity;load balancing static workload balance scheduling static scheduling processors scheduling upper diagonal matrix upper bounds computational complexity;scheduling algorithm processor scheduling dynamic scheduling computer science concurrent computing educational institutions equations parallel processing runtime parallel programming;load balance;computational complexity resource allocation processor scheduling parallel programming	This article studies a static scheduling method based on workload balancing. An equation is presented for the case when the workload is equally distributed onto all the processors. An efficient load balance scheduling algorithm is developed assuming that the workload has certain properties. Finally, some computational results are given for the product between an upper diagonal matrix and a vector.	algorithm;scheduling (computing)	Tatiana Tabirca;Sabin Tabirca;Len Freeman;Laurence Tianruo Yang	2002		10.1109/ICPPW.2002.1039735	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;load balancing;genetic algorithm scheduling;operating system;two-level scheduling;deadline-monotonic scheduling;stride scheduling;distributed computing;scheduling;least slack time scheduling;lottery scheduling;computational complexity theory;round-robin scheduling;scheduling;multiprocessor scheduling;i/o scheduling;proportionally fair	EDA	-12.631195386645844	60.367359035168406	146418
a69c7251e5e94d3adcd7061631475430c96d3909	minerva: proactive disk scheduling for qos in multitier, multitenant cloud environments	libraries;virtual machines cloud computing data handling parallel processing quality of service scheduling storage allocation;distributed architectures;performance attributes;processor scheduling;distributed processing;interconnections;internet web technologies;servers;scheduling and task partitioning;performance attributes internet web technologies scheduling and task partitioning distributed architectures;object oriented databases;task scheduling;storage automation;servers processor scheduling cloud computing throughput storage automation interconnections distributed processing task scheduling object oriented databases;minerva heavy disk contention virtual machine cluster mapreduce data intensive computing client side coordination predictive models multitier multitenant cloud environments qos proactive disk scheduling;containers;cloud computing;throughput	In recent years, Internet-connected devices have become ubiquitous, leading to increased demand for computing and storage resources. The cloud has emerged as a cost-effective and scalable solution to infrastructure requirements, making it possible to consolidate several disparate computing resources into a single physical machine. However, while the availability of multicore CPUs, high-capacity memory modules, and virtualization technologies has increased the density of converged infrastructure, disk I/O has remained a substantial performance bottleneck, especially when dealing with high-capacity mechanical disks. Here, the authors investigate how proactive disk scheduling, backed by predictive models and client-side coordination, can influence the overall throughput and responsiveness of a cluster in data-intensive computing environments. They evaluate their framework with a representative MapReduce job on a 1,200-virtual-machine cluster, demonstrating a 21 percent improvement in completion time under heavy disk contention.	central processing unit;client-side;dimm;data-intensive computing;i/o scheduling;input/output;mapreduce;multi-core processor;multitenancy;multitier architecture;predictive modelling;proactive parallel suite;quality of service;requirement;responsiveness;scalability;schedule (project management);scheduling (computing);throughput	Matthew Malensek;Sangmi Lee Pallickara;Shrideep Pallickara	2016	IEEE Internet Computing	10.1109/MIC.2016.48	throughput;parallel computing;real-time computing;cloud computing;computer science;operating system;database;distributed computing;server;computer network	HPC	-18.172489540760417	58.34346137267416	146937
76811359291031dc6507073e94041e3ebeb48de0	distributed and high performance big-file cloud storage based on key-value store		This research proposes a new Big File Cloud (BFC) with its architecture and algorithms to solve difficult problems of cloud-based storage using the advantages of key-value stores. There are many problems when designing an efficient storage engine for cloud-based storage systems with strict requirements such as big-file processing, lightweight meta-data, low latency, parallel I/O, deduplication, distributed, high scalability. Keyvalue stores have many advantages and outperform traditional relational database in storing data for heavy load systems. This paper contributes a low-complicated, fixed-size meta-data design, which supports fast and highly-concurrent, distributed file I/O, several algorithms for resumable upload, download and simple data deduplication method for static data. This research applies the advantages of ZDB an in-house key-value store which was optimized with auto-increment integer keys for solving big-file storage problems efficiently. The results can be used for building scalable distributed data cloud storage that support big-files with sizes up to several terabytes.		Thanh Trung Nguyen;Minh Hieu Nguyen	2016	IJNDC	10.2991/ijndc.2016.4.3.3	converged storage	OS	-15.634292168647764	54.200557808209474	147061
a0b27a863d3916ba92ed097c758665386bbb19d1	statistical predictors of computing power in heterogeneous clusters	computers;protocols;cluster computing;workstation clusters scheduling statistical analysis;computational modeling power engineering computing processor scheduling cloud computing grid computing computer simulation high performance computing power engineering and energy computer science performance evaluation;heterogeneous cluster;heterogeneous computing;processor scheduling;simulation;heterogeneous clusters;scheduling cluster computing heterogeneous computing;simulation experiment;computational modeling;statistical analysis;statistical predictors;scheduling;computing power;production;workstation clusters;heterogeneous clusters statistical predictors computing power;context;simulation environment	If cluster C<inf>1</inf> consists of computers with a faster mean speed than the computers in cluster C<inf>2</inf>, does this imply that cluster C<inf>1</inf> is more productive than cluster C<inf>2</inf>? What if the computers in cluster C<inf>1</inf> have the same mean speed as the computers in cluster C<inf>2</inf>: is the one with computers that have a higher variance in speed more productive? Simulation experiments are performed to explore the above questions within a formal framework for measuring the performance of a cluster. Simulation results show that both mean speed and variance in speed (when mean speeds are equal) are typically correlated with the performance of a cluster, but not always; these statements are quantified statistically for our simulation environments. In addition, simulation results also show that: (1) If the mean speed of computers in cluster C<inf>1</inf> is faster by at least a threshold amount than the mean speed of computers in cluster C<inf>2</inf>, then C<inf>1</inf> is more productive than C<inf>2</inf>. (2) If the computers in clusters C<inf>1</inf> and C<inf>2</inf> have the same mean speed, then C<inf>1</inf> is more productive than C<inf>2</inf> when the variance in speed of computers in cluster C<inf>1</inf> is higher by at least a threshold amount than the variance in speed of computers in cluster C<inf>2</inf>.	computer;experiment;simulation	Ron Chi-Lung Chiang;Anthony A. Maciejewski;Arnold L. Rosenberg;Howard Jay Siegel	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470869	parallel computing;real-time computing;computer science;distributed computing	HPC	-15.833785938178563	59.025826045205875	147289
8709cdff814ba366e62b14291b7a5c8ed69f0418	optimal service elasticity in large-scale distributed systems		A fundamental challenge in large-scale cloud networks and data centers is to achieve highly efficient server utilization and limit energy consumption, while providing excellent user-perceived performance in the presence of uncertain and time-varying demand patterns. Auto-scaling provides a popular paradigm for automatically adjusting service capacity in response to demand while meeting performance targets, and queue-driven auto-scaling techniques have been widely investigated in the literature. In typical data center architectures and cloud environments however, no centralized queue is maintained, and load balancing algorithms immediately distribute incoming tasks among parallel queues. In these distributed settings with vast numbers of servers, centralized queue-driven auto-scaling techniques involve a substantial communication overhead and major implementation burden, or may not even be viable at all.  Motivated by the above issues, we propose a joint auto-scaling and load balancing scheme which does not require any global queue length information or explicit knowledge of system parameters, and yet provides provably near-optimal service elasticity. We establish the fluid-level dynamics for the proposed scheme in a regime where the total traffic volume and nominal service capacity grow large in proportion. The fluid-limit results show that the proposed scheme achieves asymptotic optimality in terms of user-perceived delay performance as well as energy consumption. Specifically, we prove that both the waiting time of tasks and the relative energy portion consumed by idle servers vanish in the limit. At the same time, the proposed scheme operates in a distributed fashion and involves only constant communication overhead per task, thus ensuring scalability in massive data center operations. Extensive simulation experiments corroborate the fluid-limit results, and demonstrate that the proposed scheme can match the user performance and energy consumption of state-of-the-art approaches that do take full advantage of a centralized queue.	asymptotically optimal algorithm;auto-tune;autoscaling;centralized computing;cloud computing;data center;distributed computing;elasticity (data store);experiment;image scaling;load balancing (computing);overhead (computing);perceived performance;programming paradigm;scalability;server (computing);simulation;vanish (computer science)	Debankur Mukherjee;Souvik Dhara;Sem C. Borst;Johan van Leeuwaarden	2017	POMACS	10.1145/3084463	real-time computing;energy consumption;scalability;cloud computing;data center;distributed computing;demand patterns;load balancing (computing);computer science;server;queue	Metrics	-19.093773160514953	59.451289790562406	147349
fd2a42c9b18b6756ad4af3de45e7459e352fa7c4	adaptive methods for irregular parallel discrete event simulation workloads		Parallel Discrete Event Simulations (PDES) running at large scales involve the coordination of billions of very fine grain events distributed across a large number of processes. At such large scales optimistic synchronization protocols, such as TimeWarp, allow for a high degree of parallelism between processes, but with the additional complexity of managing event rollback and cancellation. This can become especially problematic in models that exhibit imbalance resulting in low event efficiency, which increases the total amount of work required to run a simulation to completion. Managing this complexity becomes key to achieving a high degree of performance across a wide range of models. In this paper, we address this issue by analyzing the relationship between synchronization cost and event efficiency. We first look at how these two characteristics are coupled via the computation of Global Virtual Time (GVT). We then introduce dynamic load balancing, and show how, when combined with low overhead GVT computation, we can achieve higher efficiency with less synchronization cost. In doing so, we achieve up to 2x better performance on a variety of benchmarks and models of practical importance.	algorithmic efficiency;benchmark (computing);blocking (computing);central processing unit;computation;computer simulation;degree of parallelism;distributed computing;load balancing (computing);non-blocking algorithm;overhead (computing);parallel computing;scalability;unbalanced circuit;x86 virtualization	Eric Mikida;Laxmikant V. Kalé	2018		10.1145/3200921.3200936	real-time computing;discrete event simulation;distributed algorithm;rollback;synchronization;degree of parallelism;computation;load balancing (computing);distributed computing;dynamic load testing;computer science	HPC	-16.042560232466144	57.939484496574536	148601
de6dee3f57e508057f890405e42d23384c1b2f71	on the performance of nearest-neighbors load balancing algorithms in parallel systems	performance evaluation;processor scheduling;resource allocation;load management iterative algorithms concurrent computing computer architecture operating systems topology parallel processing plasma simulation partial differential equations distributed computing;performance evaluation processor scheduling resource allocation parallel processing;parallel systems;nearest neighbor;load distribution;load balance;parallel processing;load balancing load balancing algorithms parallel systems nearest neighbors dasud	DASUD (Diffusion Algorithm Searching Unbalanced Domains) is a totally distributed load-balancing algorithm which belongs to the nearest-neighbors class. DASUD detects unbalanced domains (a processor and its immediate neighbors) and corrects this situation by allowing load movements between non-connected processors. DASUD has been evaluated by comparison with two well-known nearest-neighbors load balancing strategies, namely, the GDE (Generalized Dimension Exchange) and the SID (Sender Initiated Diffusion) by considering a large set of initial load distributions. These distributions were applied to ring, torus and hypercube topologies, and the number of processors ranged from 8 to 128. From these experiments we have observed that DASUD outperforms the other strategies used in the comparison as it provides the best trade-off between the balance degree obtained at the final state and the number of iterations required to reach this state.	central processing unit;experiment;iteration;k-nearest neighbors algorithm;load balancing (computing);unbalanced circuit	Ana Carolina Castro Côrtes;Ana Ripoll;Miquel A. Senar;P. Pons;Emilio Luque	1999		10.1109/EMPDP.1999.746661	parallel computing;real-time computing;computer science;distributed computing	HPC	-13.633260583394124	59.552166225421104	149951
a1aab0182494c4627c7d148c2dd86c93aadc3f8b	kbf: towards approximate and bloom filter based key-value storage for cloud computing systems	encoding radiation detectors cloud computing decoding data structures indexes noise;bloom filter;decoding;key value storage;radiation detectors;key value storage bloom filter;indexes;data structures;encoding;noise;cloud computing	As one of the most popular cloud services, data storage has attracted great attention in recent research efforts. Key-value (k-v) stores have emerged as a popular option for storing and querying billions of key-value pairs. So far, existing methods have been deterministic. Providing such accuracy, however, comes at the cost of memory and CPU time. In contrast, we present an approximate k-v storage for cloud-based systems that is more compact than existing methods. The tradeoff is that it may, theoretically, return errors. Its design is based on the probabilistic data structure called “bloom filter”, where we extend the classical bloom filter to support key-value operations. We call the resulting design as the kBF (key-value bloom filter). We further develop a distributed version of the kBF (d-kBF) for the unique requirements of cloud computing platforms, where multiple servers cooperate to handle a large volume of queries in a load-balancing manner. Finally, we apply the kBF to a practical problem of implementing a state machine to demonstrate how the kBF can be used as a building block for more complicated software infrastructures.	approximation algorithm;attribute–value pair;bloom filter;central processing unit;centralized computing;cloud computing;computer data storage;data structure;emoticon;finite-state machine;load balancing (computing);overhead (computing);requirement;sensor;synthetic intelligence	Sisi Xiong;Yanjun Yao;Shuangjiang Li;Qing Cao;Tian He;Hairong Qi;Leon M. Tolbert;Yong Liu	2017	IEEE Transactions on Cloud Computing	10.1109/TCC.2014.2385063	database index;real-time computing;data structure;cloud computing;computer science;noise;theoretical computer science;bloom filter;operating system;database;particle detector;encoding	DB	-15.338923408429453	53.967328531522035	150070
a20a30407f5526dabee04a6b1646f1b76dce670c	simulation of clustering algorithms in oodbs in order to evaluate their performances	cluster algorithm;cluster computing;performance evaluation;system performance;simulation experiment;evaluation methodology;object oriented database	A good object clustering is critical to the performance of object-oriented databases. However, it always involves some kind of overhead for the system. The aim of this paper is to propose a modelling methodology in order to evaluate the performances of different clustering policies. This methodology has been used to compare the performances of three clustering algorithms found in the literature (Cactis, CK and ORION) that we considered representative of the current research in the field of object clustering. The actual performance evaluation was performed using simulation. Simulation experiments showed that the Cactis algorithm is better than the ORION algorithm and that the CK algorithm totally outperforms both other algorithms in terms of response time and clustering overhead.	algorithm;cluster analysis;database;experiment;overhead (computing);performance evaluation;response time (technology);simulation	Jérôme Darmont;Amnar Attoui;Michel Gourgand	1997	Simul. Pr. Theory	10.1016/S0928-4869(96)00013-4	correlation clustering;data stream clustering;parallel computing;simulation;computer cluster;computer science;theoretical computer science;operating system;cure data clustering algorithm;data mining;computer performance;cluster analysis	Web+IR	-16.386730474463977	60.29812451240168	150109
2a96fa28e6328b15aa8ff7388b42b7fd8771a858	memory management for scalable web data servers	file servers;memory management;storage management;disk i o bottleneck;simulation;tcpip;data replication control;buffer management;buffer storage;client server systems;intra cluster network traffic;data replication;dynamic control;scalable web data servers;telecommunication traffic;network servers;client server;internet;monitoring;cpu bottleneck;average response time minimization;network traffic;client server memory management techniques;telecommunication traffic internet client server systems storage management file servers buffer storage;memory duplication control;clustering algorithms;world wide web;buffer management algorithms;server cluster architecture;memory management web server network servers file servers clustering algorithms switches algorithm design and analysis tcpip operating systems monitoring;web server;workloads;switches;aggregate memory capacity;algorithm design and analysis;hybrid algorithm;operating systems;average response time minimization client server memory management techniques scalable web data servers world wide web cpu bottleneck disk i o bottleneck server cluster architecture buffer management algorithms aggregate memory capacity data replication control intra cluster network traffic memory duplication control hybrid algorithm simulation workloads	Popular wcb sites are already experiencin,g very heavy loads, and th.ese loads will only increase as the 11 umber of users accessing them grows. These loads creafe both CPU and I/O bottlenecks. One promising solution already bein,g employed to eliminate the CPU botlleneck i s to replace a single processor server with a cluster of servers. Our goal in. this paper is to develop b u f w management algorithms that exploit the aggregale m.emory capacity of th.e machines in such a server cluster t o attack the 1/0 bottleneck. The key challenge i n designing such buffer managem,en.t algorithms turns out to be controllin,g data replication so as to achieve a good balancc between. intra-cluster network t ra f ic and disk I/O. Ai one extreme, the straightforward applicalion. of client-server memory management techniques t o this cluster arch.iteciure causes duplication in mem.ory am.ong Ihe servers and this tends to reduce network i ra f lc but increases disk I/O, whereas at the oth,er exf r m i e , eliminating all duplicates tends to iacrease n.etwork iraftic wh.ile reducin.g disk I/O. Accordingly, we present a new algorith.m, Hybrid, that dynamically CORirols the amount of duplication. Through a detailed sirnulation, we sh.ow that on workloads characteristic of those experienxed b y Web servers, the Hybrid algorithm correctly trades off intra-cluster network t ra f ic and disk 1/0 to min.imite average respome time.	central processing unit;client–server model;computer cluster;division by zero;hybrid algorithm;input/output;memory management;replication (computing);server (computing)	Shivakumar Venkataraman;Miron Livny;Jeffrey F. Naughton	1997		10.1109/ICDE.1997.582018	file server;algorithm design;real-time computing;the internet;hybrid algorithm;network switch;computer science;operating system;database;distributed computing;internet protocol suite;cluster analysis;web server;client–server model;server;replication;memory management	DB	-16.153371721981475	56.10463408807654	151006
38a7d8cd0afce1c546ead4636b32f4005c74dd08	dynamic protocol tuning algorithms for high performance data transfers	data replication;engin;conference paper;data intensive applications;computer science dynamic protocol tuning algorithms for high performance data transfers state university of new york at buffalo tevfik kosar arslan;throughput optimization;application level protocol tuning;wide area networks	As the data requirements of commercial and scientific applications continue to increase at an unprecedented rate, obtaining optimal end-to-end data transfer performance becomes of crucial importance for a broad range of data-intensive applications. Achieving optimal end-to-end data transfer performance requires effectively utilizing the available network bandwidth and resources, yet in practice the transfers seldom reach the levels of utilization they potentially could. Tuning protocol parameters such as pipelining, parallelism, and concurrency can significantly increase the network utilization and the transfer performance, however determining the best combination for these parameters is a challenging task since that would depend on several factors such as network characteristics, dataset characteristics and end system characteristics. In this talk, I’ll present novel algorithms for application-level tuning of protocol parameters to maximize the data transfer throughput especially in wide-area networks. The contributions of this research include: (1) analysis and prediction of optimal protocol parameter combinations based on the dataset and network characteristics using historical data as well as real-time data; (2) algorithms to cluster the datasets into comparable partitions and transfer multiple partitions concurrently for maximum transfer throughput where the transfer parameters for each partition is optimized individually; (3) dynamic monitoring of the instantaneous data transfer throughput and online tuning of the protocol parameters to detect and remedy possible transfer slowdowns. We have developed several heuristic solutions that estimate the application-layer protocol parameters, including the number of parallel data streams per file (for large file optimization), the level of control channel pipelining (for small file optimization), and the level of concurrent file transfers to fill the long fat network pipes (for all files). The developed algorithms employ novel techniques to group and transfer set of files in order to yield the maximum possible transfer throughput. The experimental results show that our proposed heuristic algorithms outperform state-of-art solution by up to 5 times. Although the heuristic solutions boost the data transfer throughput for networks with stable or predictable background traffic, they fall short to optimize the data transfers when network conditions change unpredictably. To address this issue, we propose predictive end-to-end data transfer optimization algorithm based on historical data analysis and real-time background traffic probing, dubbed HARP. Combining historical data analysis with real time sampling enables our algorithms to tune the application level data transfer parameters accurately and efficiently to achieve close-to-optimal end-to-end data transfer throughput with very low overhead. Our experimental analysis over a variety of network settings shows that HARP outperforms the best heuristics by up to 50% in terms of the achieved throughput.	algorithm;bandwidth-delay product;concurrency (computer science);data-intensive computing;end system;end-to-end encryption;end-to-end principle;harp;heuristic (computer science);mathematical optimization;overhead (computing);parallel computing;pipeline (computing);real-time clock;real-time data;requirement;sampling (signal processing);throughput	Engin Arslan;Brandon Ross;Tevfik Kosar	2013		10.1007/978-3-642-40047-6_72	parallel computing;real-time computing;computer science;operating system;database;distributed computing;replication;computer network	HPC	-18.151348175020328	57.435059468267546	151568
0fce1b6d8e0292d41f2cd8eea508228a749789b7	eager scheduling with lazy retry in multiprocessors	hypercube;multiprocessor;processor allocation;multiprocessor systems;system performance;equal opportunities;task scheduling;parallel processing	Task scheduling is concerned with the sequence in which tasks entering a multiprocessor system are served. Task scheduling policies can be generally classified as  eager scheduling  and  lazy scheduling . The former attempts to schedule the tasks whenever there are free processors available, while the latter delays the scheduling of some tasks so as to accommodate more appropriate tasks. In this paper we propose a topology-independent hybrid policy that combines the benefits of both. The new scheduling strategy, called the eager scheduling with lazy retry (ESLR) policy, tries to schedule a task eagerly upon its arrival. If the scheduling fails, then the task is rescheduled after a delay period. This delay in rescheduling gives tasks with different processor requests an equal opportunity to compete for free processors. The proposed scheduling policy is independent of the topology of the underlying multiprocessor. To study its performance, we simulated and compared several scheduling policies on hypercube multiprocessors. The results show that this ESLR policy can achieve a better system performance than previous approaches. We will study various considerations in implementing the ESLR policy and discuss their effects on the system performance.	lazy evaluation;retry;scheduling (computing)	Huey-Ling Chen;Chung-Ta King	2000	Future Generation Comp. Syst.	10.1016/S0167-739X(99)00082-5	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel processing;parallel computing;real-time computing;earliest deadline first scheduling;multiprocessing;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;two-level scheduling;deadline-monotonic scheduling;distributed computing;scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling;multiprocessor scheduling;hypercube	Arch	-12.44523274678882	59.7105023199877	152312
18e74df64bdad475e585b165640c8db3dce00125	does the content defined chunking really solve the local boundary shift problem?		Data chunking is one of the most important issues in a deduplication system, which not only determines the effectiveness of deduplication such as deduplication ratio, but also impacts the modification overhead. It breaks the file into chunks to find out the redundancy by fingerprint comparisons. The content-defined chunking algorithms such as TTTD, BSW CDC, and RC, can resist the boundary shift problem caused by small modifications. However, we observe that there exist a lot of consecutive maximum chunk sequences in various benchmarks. These consecutive maximum chunk sequences will lead to local boundary shift problem when facing small modifications. Based on this observation, we propose a new chunking algorithm, Elastic Chunking. By leveraging dynamic adjustment policy, elastic chunk can quickly find the boundary to remove the consecutive maximum chunk sequences. To evaluate the performance, we implement a prototype and conduct extensive experiments based on synthetic and realistic datasets. Compared with TTTD, BSW CDC and RC algorithms, proposed chunking algorithm can achieve the higher deduplication ratio and throughput.	benchmark (computing);cns;data deduplication;elastic net regularization;existential quantification;experiment;fingerprint;fog computing;overhead (computing);prototype;rc algorithm;server (computing);shallow parsing;synthetic intelligence;throughput;vii	Wenlong Tian;Ruixuan Li;Zhiyong Xu;Weijun Xiao	2017	2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2017.8280445	throughput;probability distribution;real-time computing;redundancy (engineering);computer science;theoretical computer science;data deduplication;rc algorithm;chunking (psychology);algorithm design	HPC	-13.904171897081982	55.34708139393181	153602
5db5a9b833bf4f34c71f81ed9ee072e802d5bd56	pepa based performance modeling for robust resource allocations amid varying processor availability		Computational environments that are comprised of high performance parallel and/or distributed computing systems, often suffer performance degradation due to unforeseen variations in problem, algorithm, and system characteristics. In such environments, the underlying computational resource is considered to be non-dedicated. Therefore, to maintain a desired level of performance, there is a need for a study of the robustness of resource allocations. A robust resource allocation is expected to guarantee a predetermined level of performance, and optimize the execution performance, whenever possible. The goal of performance modeling and evaluation is to understand the behavior of the system (which includes the application and the computational system) and identify aspects of the system that are sensitive from a performance point of view. In general, much work has been focused on formulating robustness metrics for a number of resource allocation techniques via simulation, or using performance modeling for evaluating metrics, such as throughput and utilization in concurrent systems. Therefore, to the best of our knowledge, performance modeling for evaluating response times of resource allocations in parallel and distributed computing systems, and the related robustness analysis is still an open problem. The authors have validated the use of the performance evaluation process algebra (PEPA) for analyzing the robustness of static resource allocations for dedicated parallel computational resources in earlier research. In the work presented here, an Eclipse workbench that uses an implementation of PEPA, has been employed to numerically implement the model to obtain the performance of a number of static resource allocations obtained for a parallel execution environment with non-dedicated computational resources. The novelty of the proposed approach is to study the impact of varying processor/machine availability on the robustness of resource allocations. Moreover, as a novel approach, the variability in processor/machine availability is introduced into the performance modeling of the overall computational system. The performance is obtained via a numerical analysis of the performance of the modeled execution of applications on parallel computational resources with varying availability. Further, the results of the numerical analysis are used for evaluating the robustness of the static resource allocations. The work presented herein extends the benefits of using PEPA models towards a cost effective and low overhead analysis of robustness of resource allocations for a non-dedicated parallel computing system.	algorithm;central processing unit;computation;computational resource;computer performance;concurrency (computer science);distributed computing environment;eclipse;elegant degradation;embedded system;experiment;heart rate variability;holism;library (computing);mean time between failures;numerical analysis;overhead (computing);pepa;parallel computing;performance evaluation;performance prediction;process calculus;robustness (computer science);run time (program lifecycle phase);scheduling (computing);simulation;stochastic process;systems design;throughput;workbench	Srishti Srivastava;Ioana Banicescu	2018	2018 17th International Symposium on Parallel and Distributed Computing (ISPDC)	10.1109/ISPDC2018.2018.00018	open problem;throughput;real-time computing;robustness (computer science);computational resource;distributed computing;process calculus;workbench;pepa;computer science;resource allocation	HPC	-14.083771205062343	58.84325388241772	153764
850943a92b899aafa9052869a75a61960bd6529f	software orchestrated flash array	flash memory;key value store;object based storage device;versioning	Because of the explosive growth of flash disk usage in portable computers and mobile devices, building data center storage systems using these commodity flash disks has become a compelling option. A vanilla approach to integrating multiple flash disks into a storage system is to put them under the control of a RAID controller. However, this approach incurs longer write latency than necessary and leads to reduction in the overall system lifetime. This paper describes a log-structured flash array architecture called SOFA (Software Orchestrated Flash Array), in which the flash translation layer (FTL) sits on top of rather than below disk array management logic, and is judiciously partitioned between a central control server and the on-disk controllers. By directly managing the resource usage in individual flash disks, SOFA is able to avoid the load and wear imbalance problems associated with existing flash disk array systems. Empirical measurements of an industrial-strength SOFA prototype show it could execute more than 7,000 random 8KB writes per second per SATA-2 flash disks, and sustains the same per-disk random write performance when the system holds up to 24 flash disks.	adobe flash;computer data storage;data center;disk array controller;disk storage;ftl: faster than light;flash file system;flash memory controller;mobile device;portable computer;prototype;raid;random access;sofa;server (computing);solid-state drive	Tzi-cker Chiueh;Weafon Tsao;Hou-Chiang Sun;Ting-Fang Chien;An-Nan Chang;Cheng-Ding Chen	2014		10.1145/2611354.2611360	flash file system;embedded system;real-time computing;computer hardware;computer science;software versioning;operating system;flash memory emulator;bad sector;programming language;associative array	OS	-13.411905980815668	53.859984392473045	154086
7ef329f9b8bc57ec874a6b66b5125d827380bd09	vm-aware adaptive storage cache prefetching		Storage cache prefetching is an effective technique for reducing the access latency in hierarchical storage systems when the access pattern is predictable based on access locality.In Infrastructure-as-a-Service (IaaS) clouds, however, storage virtualization significantly rearranges data placement, thereby reducing the spatial locality observed in the host operating system (OS). Moreover, IaaS clouds consolidate applications with various workloads that may change over time.Therefore, the access pattern changes both spatially and temporally.This paper proposes an adaptive storage cache prefetching scheme that uses structural and statistical information inside virtual machines (VMs). Observation of the application's file usage and internal file-layout information in the guest OS allows the host OS to capture spatial and temporal locality during storage access.In addition, application-level performance statistics allow the host OS to tune the prefetch speed adaptively to prevent performance degradation due to excessive prefetching.We implemented a prototype cache prefetching system that cooperates with Linux and PostgreSQL in a VM.Experiments using the TPCx-V benchmark showed that VM-awareness improved the performance by 17.1% compared with traditional prefetching.Our system achieved 3.15 times better performance than an existing non-prefetching caching system.	benchmark (computing);cpu cache;cache (computing);cache prefetching;cloud computing;elegant degradation;hierarchical storage management;hit (internet);input/output;link prefetching;linux;locality of reference;operating system;postgresql;principle of locality;prototype;relocation (computing);semiconductor consolidation;storage virtualization;tag cloud;temporal logic;virtual machine	Keiichi Matsuzawa;Takahiro Shinagawa	2017	2017 IEEE International Conference on Cloud Computing Technology and Science (CloudCom)	10.1109/CloudCom.2017.20	locality of reference;instruction prefetch;real-time computing;latency (engineering);virtualization;storage virtualization;locality;cache;computer science;virtual machine	HPC	-13.065382286196604	54.172087898430874	154141
0fca45a11404145b81b4e316c039aa98c2310830	scheduling parallel virtual machine in xen based on credit	virtual machine;job shop scheduling;processor scheduling;vcpu scheduling parallel virtual machine xen multiprocessor system parallel program software development virtual machine systems concurrent program scheduling scheduling parallel applications vm parallel virtual cpu;virtual machines operating systems computers parallel programming scheduling;concurrent program;schedules processor scheduling synchronization job shop scheduling benchmark testing dynamic scheduling;synchronism scheduling virtual machine concurrent program i o event;synchronization;schedules;synchronism scheduling;i o event;benchmark testing;dynamic scheduling	With the development of multi-processor system, parallel program becomes a very important direction of software development. But in virtual machine systems, the performance of concurrent program scheduling is poor. In order to improve the performance of scheduling parallel applications in Virtual Machine (VM), this paper raises a parallel schedule algorism called PSA, which is based on Credit scheduler in Xen VMM. This new algorism support for scheduling the parallel Virtual CPUs (vCPUs) synchronously, it reduces the effect of asynchrony of parallel vCPUs by the interrupt of I/O bounded workloads, and it keeps the synchronization of parallel vCPUs when asynchrony emerges in the process of scheduling. The evaluation shows remarkable improvement of the PSA in scheduling the parallel vCPUs with different workloads, compared to the original Credit scheduler, co-scheduler and UVF: the demand-based coordinated scheduler.	algorism;asynchronous i/o;central processing unit;concurrent computing;input/output;multiprocessing;parallel virtual machine;polar surface area;scheduling (computing);software development;virtual machine manager	Xiaobo Ding;Zhong Ma;Xinfa Dai;Alin Zhong	2014	2014 Asia-Pacific Services Computing Conference	10.1109/APSCC.2014.32	fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;synchronization;benchmark;parallel computing;real-time computing;earliest deadline first scheduling;gang scheduling;flow shop scheduling;dynamic priority scheduling;schedule;computer science;virtual machine;rate-monotonic scheduling;operating system;two-level scheduling;deadline-monotonic scheduling;stride scheduling;scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling;multiprocessor scheduling;i/o scheduling	HPC	-14.553078722891014	59.996963301990306	154238
4d6a28ad272a2a26a689baa6d8a12a661ba99f5d	a load balancing method based on node features in a heterogeneous hadoop cluster		In a heterogeneous cluster, how to handle load balancing is an urgent problem. This paper proposes a method of load balancing based on node features. The method first analyses the main indexes that determine node performance. Then, a formula is defined to describe the node performance based on the contributions of those indexes. We combine node performance with node busy status to calculate the relative load value. By analysing the relative load value of each node and the cluster storage utilization rate, the recommended value of the storage utilization rate for each node is calculated. Finally, the balancer threshold is generated dynamically based on the current cluster’s disk load. The results of experiments show that the load balancing method proposed in this paper provides a more reasonable equilibrium for heterogeneous clusters, improves efficiency and substantially reduces the execution time.		Pengcheng Yang;Honghao Gao;Huahu Xu;Minjie Bian;Danqi Chu	2017		10.1007/978-3-030-00916-8_32	utilization rate;heterogeneous cluster;distributed computing;computer science;cluster (physics);load balancing (computing);cloud computing	Robotics	-17.673925246374047	58.44923344206773	154288
ab9717d6a1fd25e47b6dc174fcc6084acd416fc7	an adaptive strategy for scheduling data-intensive applications in grid environments	grid scheduling;adaptive scheduling grid computing processor scheduling resource management application software computer science large scale systems file systems file servers concurrent computing;data intensive application;file servers;trace based simulation;process utilization;queuing time;data intensive job data intensive application grid environment meta scheduling process utilization average turnaround time grid scheduling adaptive scheduling computational requirement data requirement job scheduling data transfer queuing time trace based simulation;concurrent computing;application software;processor scheduling;resource management;data requirement;data intensive job;computational requirement;average turnaround time;computational modeling;meta scheduling;scheduling;grid environment;adaptive scheduling;bandwidth;computer science;grid computing;job scheduling;program processors;data transfer;scheduling grid computing;file systems;large scale systems;data models	Data-intensive applications are becoming increasingly common in Grid environments. These applications require enormous volume of data for the computation. Most conventional meta-scheduling approaches are aimed at computation intensive application and they do not take data requirement of the applications into account, thus leading to poor performance. Efficient scheduling of data-intensive applications in Grid environments is a challenging problem. In addition to process utilization and average turnaround time, it is important to consider the worst-case turnaround time in evaluating the performance of Grid scheduling strategies. In this paper, we propose an adaptive scheduling scheme that takes into account both the computational requirements and the data requirements of the jobs while making scheduling decisions. In our scheme, data transfer is viewed in par with computation and explicitly considered when scheduling. Jobs are dispatched to the sites that are optimal in terms of both data transfer time and computation time. In addition, our scheme overlaps a job's data transfer time with its own queuing time and other jobs' computation time as much as possible. Trace-based simulations show that the proposed scheme can gain significant performance benefits for data-intensive jobs.	best, worst and average case;computation;data-intensive computing;job stream;meta-scheduling;requirement;scheduling (computing);simulation;time complexity	Wantao Liu;Rajkumar Kettimuthu;Bo Li;Ian T. Foster	2010	2010 17th International Conference on Telecommunications	10.1109/ICTEL.2010.5478755	fair-share scheduling;data modeling;file server;application software;parallel computing;real-time computing;concurrent computing;dynamic priority scheduling;computer science;resource management;job scheduler;operating system;two-level scheduling;distributed computing;round-robin scheduling;computational model;scheduling;multiprocessor scheduling;bandwidth;grid computing	HPC	-17.074467813078957	59.86329434932735	154385
55ce0391cae3d7663a26bc6bb1a1e5618b8b9475	upbit: scalable in-memory updatable bitmap indexing	upbit;efficient updates;bitmap index;update bitvectors;fence pointers	Bitmap indexes are widely used in both scientific and commercial databases. They bring fast read performance for specific types of queries, such as equality and selective range queries. A major drawback of bitmap indexes, however, is that supporting updates is particularly costly. Bitmap indexes are kept compressed to minimize storage footprint; as a result, updating a bitmap index requires the expensive step of decoding and then encoding a bitvector. Today, more and more applications need support for both reads and writes, blurring the boundaries between analytical processing and transaction processing. This requires new system designs and access methods that support general updates and, at the same time, offer competitive read performance. In this paper, we propose scalable in-memory Updatable Bitmap indexing (UpBit), which offers efficient updates, without hurting read performance. UpBit relies on two design points. First, in addition to the main bitvector for each domain value, UpBit maintains an update bitvector, to keep track of updated values. Effectively, every update can now be directed to a highly-compressible, easy-to-update bitvector. While update bitvectors double the amount of uncompressed data, they are sparse, and as a result their compressed size is small. Second, we introduce fence pointers in all update bitvectors which allow for efficient retrieval of a value at an arbitrary position. Using both synthetic and real-life data, we demonstrate that UpBit significantly outperforms state-of-the-art bitmap indexes for workloads that contain both reads and writes. In particular, compared to update-optimized bitmap index designs UpBit is 15-29x faster in terms of update time and 2.7x faster in terms of read performance. In addition, compared to read-optimized bitmap index designs UpBit achieves efficient and scalable updates (51-115x lower update latency), while allowing for comparable read performance, having up to 8% overhead.	bit array;bitmap index;in-memory database;overhead (computing);range query (data structures);real life;scalability;sparse matrix;succinct data structure;synthetic intelligence;transaction processing	Manos Athanassoulis;Zheng Yan;Stratos Idreos	2016		10.1145/2882903.2915964	computer science;bitmap index;theoretical computer science;data mining;database	DB	-13.483619901989686	54.235148400772836	155064
4cf5d9c929d6549d099cfd4401b2a56d303b358e	reducing service cost based on the skewness of data popularity for cloud storage systems	service nodes service cost reduction data popularity skewness cloud storage systems np complete problem parallelized greedy algorithm;graph cloud storage data popularity skewness availability;data popularity;availability;storage management;cost reduction;greedy algorithms;redundancy availability equations bipartite graph mathematical model data models servers;computational complexity;graph;storage management cloud computing computational complexity cost reduction greedy algorithms parallel algorithms;skewness;cloud storage;cloud computing;parallel algorithms	Reducing service cost has been a popular topic in recent studies on cloud storage systems. One of the basic techniques is to power down parts of service nodes. However, it will reduce the availability of data objects, which is the overriding concern of users. So we mathematically formulate the problem to close maximum service nodes under the constraint of a given high availability. According to our analysis, this problem is NP complete. In practical systems, few data objects get most accesses while large number of objects get few. So we propose a parallelized greedy algorithm to power down service nodes based on the skew ness of data popularity.	cloud storage;earthbound;experiment;greedy algorithm;high availability;np (complexity);np-completeness;parallel computing;simulation	Zhen Huang;Yuxing Peng;Yisong Lin	2013	2013 IEEE Seventh International Symposium on Service-Oriented System Engineering	10.1109/SOSE.2013.52	availability;skewness;greedy algorithm;cloud computing;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;graph;computational complexity theory;statistics	DB	-15.681779826488674	57.21827677186027	155100
56b4f0a64ed86c6125fc374ebb13cce02cd85e0f	a study of partitioned dimm tree management for multimedia server systems	big memory server;dimm tree architecture;in memory computing;multimedia server system;main memory	In-memory computing systems have been attracting considerable attention as a method for servicing high-quality multimedia contents. In-memory computing was intended to store entire data sets in the main memory of a computer to eliminate the need to access slow mechanical hard discs and increase the ability to process complex and large volumes of data. Prior studies have proposed a dual inline memory module (DIMM) tree architecture (DTA) as a new structure for implementing the in-memory computing system. The DTA can apply a partitioned DIMM tree policy to efficiently manage memory. However, the partitioned DIMM tree has several drawbacks, including hardware overhead resulting from additional fields in both the translation lookaside buffer (TLB) and the page table and the demand for an additional fast partition area for the fast partition page table (FPPT). To overcome these drawbacks, this paper proposes an advanced TLB management policy for the partitioned DIMM tree, DIMM tree TLB and two new partitioned DIMM tree management policies, fast-FPPT and slow-FPPT. We model the proposed policies using C language and verify them by special workloads in experiments employing a large-capacity memory system. The experimental results show how the proposed policies influence system performance and confirm that they overcome problems in the existing DTA. The simulations revealed a similarity between the performance of systems using the proposed policies and that of the existing DTA model. However, as the proposed policies demand a considerably lower hardware cost than the existing DTA model, the proposed policies are more practical.	computer data storage;digital television adapter;experiment;fully buffered dimm;in-memory processing;mathematical optimization;memory module;multiprocessing;overhead (computing);page table;power optimization (eda);server (computing);simulation;translation lookaside buffer	Young-Kyu Kim;Yong-Hwan Lee;Byungin Moon	2016	Multimedia Tools and Applications	10.1007/s11042-016-3382-6	parallel computing;real-time computing;telecommunications;computer science;in-memory processing;operating system;database;programming language;world wide web;computer security;algorithm;computer network	DB	-12.040354733921726	53.90883651214612	155292
73e64200564b25944805b6f66f2ea780f4ad55b9	a buffer model for evaluating the performance of r-tree packing algorithms	stochastic modeling;communication networks;multimedia systems;computer architecture;real time systems	Performance studies of various R-tree algorithms, have used the number of nodes accessed as the primary metric. In real databases some portion of the tree is buffered in main memory. Since such buffering can significantly affect performance, we postulate that performance prediction should be baaed on number of 1/0s required to satisfy a query. To this end we present an analytical model that predicts the number of disk accesses for an LRU buffer manager given the minimum bounding rectangles (MBRs) of the R-tree nodes and a buffer size. Our model can be used to evaluate the quality of any R-tree update operation, such as node splitting policies [3] or packing algorithms [4, 6, 8], asmeasured by query performance of the resulting tree. The model is very accurate and simple to understand, making it easy for researchers to integrate it into their studies. It can eerily be modified to accommodate different buffer management policies – for example, keeping the top few levels of the R-tree in the buffer. Furthermore, the model ia not only applicable to R-trees, but can easily be modified to predict B-tree performance.	algorithm;b-tree;computer data storage;computer performance;database;performance prediction;r-tree;set packing	Scott T. Leutenegger;Mario A. López	1996		10.1145/233013.233054	real-time computing;computer science;theoretical computer science;distributed computing;stochastic;statistics;systems design	DB	-13.01463849085693	56.605720038312704	156234
56572083aa54b01ed528c2350c961973a0948940	resource-elasticity support for distributed memory hpc applications		Computer simulations are alternatives to the scientific method in domains where physical experiments are unfeasible or impossible. When the amount of memory and processing speed required is large, simulations are executed in distributed memory High Performance Computing (HPC) systems. These systems are usually shared among its users. A resource manager with a batch scheduler is used to fairly and efficiently share the resources of these systems among its users. Current large HPC systems have thousands of compute nodes connected over a high-performance network. Users submit batch job descriptions where the number of resources required by their simulations are specified. Batch job descriptions are queued and scheduled based on priorities and submission times. The parallel efficiency of a simulation depends on the number of resources allocated to it. It is challenging for users to specify allocation sizes that produce adequate parallel efficiencies. A resource allocation can be too small and the parallel efficiency of the application may be adequate, but its performance may not be scaled to its maximum potential. A resource allocation can be too large and therefore the parallel efficiency of the application may be degraded due to synchronization overheads. Unfortunately, in current systems these resource allocations cannot be adapted once the applications of a job start. A resource manager and MPI library combination that adds resource-elasticity support for HPC applications is proposed in this work. The resource manager is extended with operations to adapt the resources of running applications in jobs; in addition, new scheduling techniques are added to it. The MPI library has been extended with operations that enable resource adaptations as changes in the number of processes in world communicators. The goal is to optimize system-wide efficiency metrics through adjustments to the resource allocations of running applications. Resource allocations are adjusted continuously based on performance feedback from running applications.	batch processing;distributed memory;elasticity (cloud computing);elasticity (data store);experiment;job scheduler;job stream;message passing interface;scheduling (computing);simulation;speedup	Isaías A. Comprés Ureña	2017			distributed memory;elasticity (economics);resource management;real-time computing;resource allocation;distributed computing;computer science	HPC	-16.44347868589793	59.82231174809601	156426
2268cb24bd0ec6a6f634e1e61b554053a08447c9	decreasing power consumption with energy efficient data aware strategies	energy;data backfilling;data replication;data cluster;data intensive computing;power consumption;qlfu;data grid	Regardless of whether data is stored in a cluster, grid, or cloud, data management is being recognized as a significant bottleneck. Computing elements can be located far away from the data storage elements. The energy efficiency of the data centers storing this data is one of the biggest issues in data intensive computing. In order to address such issues, we are designing and analyzing a series of energy efficient data aware strategies involving data replication and CPU scheduling. In this paper, we present a new strategy for data replication, called Queued Least-Frequently-Used (QLFU), and study its performance to determine if it is an energy efficient strategy. We also study the benefits of using a data aware CPU scheduling strategy, called data backfilling, which uses job preemption in order to maximize CPU usage and allows for longer periods of suspension time to save energy. We measure the performance of QLFU and existing replica strategies on a small green cluster to study the running time and power consumption of the strategies with and without data backfilling. Results from this study have demonstrated that energy efficient data management can reduce energy consumption without negatively impacting response time.		Susan V. Vrbsky;Jeffrey M. Galloway;Robert Carr;Rahul Nori;David Grubic	2013	Future Generation Comp. Syst.	10.1016/j.future.2012.12.016	parallel computing;real-time computing;energy;computer science;data cluster;operating system;data-intensive computing;data grid;database;distributed computing;data efficiency;replication;computer network	Arch	-15.54190330517006	56.69735477046261	156725
7a559b9c23a0c9019c674d76869a8543d7109877	distributed event stream processing with non-deterministic finite automata	continuous queries;publish subscribe system;continuous query;non deterministic finite automata;single machine;event streams;pattern matching;publish subscribe;complex event processing;experimental evaluation;stream processing;process engineering;nfa	Efficient matching of incoming events to persistent queries is fundamental to event pattern matching, complex event processing, and publish/subscribe systems. Recent processing engines based on non-deterministic finite automata (NFAs) have demonstrated scalability in the number of queries that can be efficiently executed on a single machine. However, existing NFA based systems are limited to processing events on a single machine. Consequently, their event processing capacity cannot be increased by adding more machines.  In this paper, we present an experimental evaluation of different methods for distributing an event processing system that is based on NFAs across multiple machines in a cluster. Our results show that careful input stream partitioning gives close to linear performance scaleup for CPU bound workloads.	automata theory;central processing unit;complex event processing;deterministic finite automaton;event stream processing;finite-state machine;nondeterministic finite automaton;pattern matching;publish–subscribe pattern;scalability;stream (computing)	Lars Brenna;Johannes Gehrke;Mingsheng Hong;Dag Johansen	2009		10.1145/1619258.1619263	real-time computing;nondeterministic finite automaton;stream processing;computer science;complex event processing;pattern matching;database;distributed computing;publish–subscribe pattern;programming language	DB	-18.55200168336282	55.16719122871648	157477
3eb0660f970a76a078d2db96c4e6714d5f0c1484	robus: fair cache allocation for data-parallel workloads	multi tenancy;proportional fairness;data analytics;cache management	Systems for processing big data---e.g., Hadoop, Spark, and massively parallel databases---need to run workloads on behalf of multiple tenants simultaneously. The abundant disk-based storage in these systems is usually complemented by a smaller, but much faster, cache. Cache is a precious resource: Tenants who get to use the cache can see two orders of magnitude performance improvement. Cache is also a limited and hence shared resource: Unlike a resource like a CPU core which can be used by only one tenant at a time, a cached data item can be accessed by multiple tenants at the same time. Cache, therefore, has to be shared by a multi-tenancy-aware policy across tenants, each having a unique set of priorities and workload characteristics.  In this paper, we develop cache allocation strategies that speed up the overall workload while being fair to each tenant. We build a novel fairness model targeted at the shared resource setting that incorporates not only the more standard concepts of Pareto-efficiency and sharing incentive, but we also define envy freeness via the notion of core from cooperative game theory. Our cache management platform, ROBUS, uses randomization over small time batches, and we develop a proportionally fair allocation mechanism that satisfies the core property in expectation. We show that this algorithm and related fair algorithms can be approximated to arbitrary precision in polynomial time. We evaluate these algorithms on a ROBUS prototype implemented on Spark with RDD store used as cache. Our evaluation on an industry-standard workload shows that our algorithms score high on both performance and fairness metrics across a wide variety of practical multi-tenant setups.	apache hadoop;approximation algorithm;arbitrary-precision arithmetic;big data;cpu cache;central processing unit;data item;fairness measure;game theory;multitenancy;parallel database;pareto efficiency;polynomial;proportionally fair;prototype;spark;time complexity	Mayuresh Kunjir;Brandon Fain;Kamesh Munagala;Shivnath Babu	2017		10.1145/3035918.3064018	parallel computing;real-time computing;cache coloring;multitenancy;database;smart cache;data analysis;cache algorithms	DB	-14.123664927261755	57.72236863710348	157772
94debe23b189960f0d12d970ff344249442ac64e	identifying volatile data from multiple memory dumps in live forensics	memory analysis;conference_paper;natural extension;volatile data;dynamic data;live forensics;indexation;data structure;dynamic analysis	One of the core components of live forensics is to collect and analyze volatile memory data. Since the dynamic analysis of memory is not possible, most live forensic approaches focus on analyzing a single snapshot of a memory dump. Analyzing a single memory dump raises questions about evidence reliability; consequently, a natural extension is to study data from multiple memory dumps. Also important is the need to differentiate static data from dynamic data in the memory dumps; this enables investigators to link evidence based on memory structures and to determine if the evidence is found in a consistent area or a dynamic memory buffer, providing greater confidence in the reliability of the evidence. This paper proposes an indexing data structure for analyzing pages from multiple memory dumps in order to identify static and dynamic pages.	brute-force search;core dump;data buffer;data structure;dynamic data;memory management;snapshot (computer storage);volatile memory	Frank Y. W. Law;Patrick P. F. Chan;Siu-Ming Yiu;Benjamin Tang;Pierre K. Y. Lai;K. P. Chow;Ricci S. C. Ieong;Michael Y. K. Kwan;Wing-Kai Hon;Lucas Chi Kwong Hui	2010		10.1007/978-3-642-15506-2_13	computer science;data mining;internet privacy;world wide web	Arch	-13.231986673550283	55.48774556708567	158346
fbfa5c68df2c061daeee37915aec7a450d40584b	topk ordering on distributed systems		"""Ranking has wide range of applications like social choice\citeSC, recommendation systems\citeRS, web search\citeWS, crowd sourcing \citeCS etc. \textttTeraSort is a distributed algorithm, commonly used in systems like Hadoop MapReduce, for sorting large datasets. However, in most applications of interest we do not desire complete ordering of data, rather only a few items which have the highest ranks. In this paper we propose Coded Partial Sort to obtain partially sorted data from large datasets using distributed computing systems. We intend to find \texttttopK ordered elements of a dataset by optimally utilizing servers in distributed network.\\ Coded Partial Sort modifies conventional \textttTeraSort algorithm to remove data irrelevant for partial ordering and applies ideas of """"coding"""" to improve run-time performance by significantly decreasing communication load of Uncoded Partial Sort\citeUs. We empirically evaluate the performance of tCoded and Uncoded Partial Sort on Amazon EC2 clusters for experimental settings of interest."""	amazon elastic compute cloud (ec2);apache hadoop;crowdsourcing;distributed algorithm;distributed computing;mapreduce;relevance;sorting	Anup Kumar Sahoo;Nikhil Karamchandani	2018		10.1145/3266276.3266280	partially ordered set;distributed algorithm;coding (social sciences);recommender system;sort;computer science;distributed computing;server;ranking;sorting	HPC	-18.694505778325166	55.839019560173746	158956
2edd3c8ffa26191ad4312fe62d5b8f403806dd86	mics: mingling chained storage combining replication and erasure coding	mics diverse workload conditions i o throughput consistency level fault tolerance services users semantics pipeline random access memory consistency read write protocols erasure coded segments high reliability n way replication mingling chained storage scheme workloads storage schemes read write efficiency reliable storage cloud storage systems read write performance low space cost erasure coding mingling chained storage combining replication;microwave integrated circuits;protocols;replication;reliability;storage management cloud computing fault tolerant computing random access storage software reliability;会议论文;manganese;servers;tradeoff;erasure coding;mics;microwave integrated circuits encoding servers reliability manganese cloud computing protocols;encoding;cloud storage;tradeoff mics cloud storage erasure coding replication;cloud computing	High reliability, low space cost, and efficient read/write performance are all desirable properties for cloud storage systems. Due to the inherent conflicts, however, simultaneously achieving optimality on these properties is unrealistic. Since reliable storage is indispensable prerequisite for services with high availability, tradeoff should therefore be made between space and read/write efficiency when storage scheme is designed. N-way Replication and Erasure Coding, two extensively-used storage schemes with high reliability, adopt opposite strategies on this tradeoff issue. However, unbalanced tradeoff designs of both schemes confine their effectiveness to limited types of workloads and system requirements. To mitigate such applicability penalty, we propose MICS, a MIngling Chained Storage scheme that combines structural and functional advantages from both N-way replication and erasure coding. Qualitatively, MICS provides efficient read/write performance and high reliability at reasonably low space cost. MICS stores each object in two forms: a full copy and certain amount of erasure-coded segments. We establish dedicated read/write protocols for MICS leveraging the unique structural advantages. Moreover, MICS provides high read/write efficiency with Pipeline Random-Access Memory consistency to guarantee reasonable semantics for services users. Evaluation results demonstrate that under same fault tolerance and consistency level, MICS outperforms N-way replication and pure erasure coding in I/O throughput by up to 34.1% and 51.3% respectively. Furthermore, MICS shows superior performance stability over diverse workload conditions, in which case the standard deviation of MICS is 70.1% and 29.3% smaller than those of other two schemes.	cache (computing);cloud storage;consistency model;data center;erasure code;fault tolerance;high availability;input/output;pram consistency;random-access memory;requirement;synthetic intelligence;system requirements;throughput;unbalanced circuit	Yan Tang;Jianwei Yin;Wei Lo;Ying Li;Shuiguang Deng;Kexiong Dong;Calton Pu	2015	2015 IEEE 34th Symposium on Reliable Distributed Systems (SRDS)	10.1109/SRDS.2015.25	replication;parallel computing;real-time computing;cloud computing;computer science;manganese;operating system;reliability;database;distributed computing;computer security;encoding;computer network	OS	-15.508871495586908	53.592861534624795	159524
454c30e6fa55756899331904cb54beda61813366	load balancing in parallel and distributed processing of tree-based multiple-task jobs	parallel and distributed system;automatic control;key management;dynamic load balancing algorithms distributed processing parallel processing tree based multiple task jobs tree based multiple task jobs batch processing task allocation incoming workload problem specific static algorithms prescheduling load balancing algorithms;dynamic load balancing;application software;resource allocation;distributed processing;distributed computing;problem specific static algorithms;trees mathematics;parallel and distributed processing;load management;batch process;prescheduling load balancing algorithms;batch processing computers;load management distributed processing decision making decision trees application software throughput distributed computing character recognition pattern recognition automatic control;pattern recognition;incoming workload;load balance;tree based multiple task jobs;batch processing;decision trees;character recognition;resource allocation parallel processing trees mathematics batch processing computers;parallel processing;dynamic load balancing algorithms;task allocation;throughput	The efficient processing of tree-based multiple-task jobs arriving in batches to parallel and distributed systems is presented. This type of processing can be found in such application fields as, for example, automatic diagnostics and document recognition. A key management issue in such systems is task allocation. Load balancing is often proposed as the task allocation approach. The load balancing algorithms are employed to improve the throughput of the system by distribution of the incoming workload evenly among the processors in the system. In the paper the problem-specijc static algorithms as well as general dynamic load balancing algorithms have been developed for supporting batch processing of tree-based multiple-task jobs. The proposed algorithms have been studied through simulation. Pre-scheduling load balancing algorithms have been evaluated as superior to other algorithms investigated.	algorithm;batch processing;central processing unit;distributed computing;key management;load balancing (computing);scheduling (computing);simulation;throughput	Leszek Borzemski	1995		10.1109/EMPDP.1995.389150	network load balancing services;parallel computing;real-time computing;computer science;load balancing;distributed computing	HPC	-14.126213262339148	60.11487827591773	159704
0f46a65d5590e1f1309f8b4da463f7ed979fef25	on-demand data elevation in hierarchical multimedia storage servers	on-demand data elevation;hierarchical multimedia storage servers	Given the present cost of memories and the very large storage and bandwidth requirements of large-scale multimedia databases, hierarchical storage servers (which consist of RAM, disk storage, and robot-based tertiary libraries) are becoming increasingly popular. However, related research is scarce and employs tertiary storage for storage augmentation purposes only. This work, exploiting the ever-increasing performance ooered by (particularly) modern tape library products, aims to utilize tertiary storage in order to augment the system's performance. We consider the issue of elevating continuous data from its permanent place in tertiary for display purposes. Our primary goals are to save on the secondary storage bandwidth that traditional techniques require for the display of continuous objects, while requiring no additional RAM buuer space. To this end we develop algorithms for sharing the responsibility for Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. the playback between the secondary and ter-tiary devices and for placing the blocks of continuous objects on tapes, and show how they achieve the above goals. We study these issues for diierent commercial tape library products with diierent bandwidth and tape capacity and in environments with and without the multiplexing of tape libraries.	algorithm;auxiliary memory;computer data storage;database;disk storage;hierarchical storage management;library (computing);multiplexing;random-access memory;requirement;robot;tape library;vldb	Peter Triantafillou;Thomas Papadakis	1997			operating system;database;world wide web;server	DB	-15.67519295810481	55.31665347910974	159997
54f2ac705ac6686a5dff632c02d23c196297b66e	wattdb: an energy-proportional cluster of wimpy nodes	fault tolerant;real time;energy proportionality	The constant growth of data in all businesses leads to bigger database servers. While peak load times require fast and heavyweight hardware to guarantee performance, idle times are a waste of energy and money. Todays DBMSs have the ability to cluster several servers for performance and fault tolerance. Nevertheless, they do not support dynamic powering of the cluster's nodes based on the current workload.  In this demo, we propose a newly developed DBMS running on clustered commodity hardware, which is able to dynamically power nodes. The demo allows the user to interact with the DBMS and adjust workloads, while the cluster's reaction is shown in real-time.	array dbms;commodity computing;database server;fault tolerance;load (computing);load profile;real-time clock;real-time computing	Daniel Schall;Volker Hudlet	2011		10.1145/1989323.1989461	embedded system;fault tolerance;real-time computing;computer science;operating system;database	HPC	-18.14624307251627	56.91533709488788	160684
17633a751e148a3953be04dd0fe1e8a93c67033c	a compacting real-time memory management system	memory management;real time	We propose a real real-time memory management system called Compact-fit that offers both time and space predictability. Compact-fit is a compacting memory management system for allocating, deallocating, and accessing memory in real time. The system provides predictable memory fragmentation and response times that are constant or linear in the size of the request, independently of the global memory state. We present two Compact-fit implementations and compare them to established memory management systems, which all fail to provide predictable memory fragmentation. The experiments confirm our theoretical complexity bounds and demonstrate competitive performance. In addition, we can control the performance versus fragmentation tradeoff via our concept of partial compaction. The system can be parameterized with the needed level of compaction, improving the performance while keeping memory fragmentation predictable.	data compaction;experiment;fragmentation (computing);management system;memory management;real-time clock;real-time computing;real-time transcription	Silviu S. Craciunas;Christoph M. Kirsch;Hannes Payer;Ana Sokolova;Horst Stadler;Robert Staudinger	2008			uniform memory access;distributed shared memory;interleaved memory;parallel computing;real-time computing;simulation;distributed memory;computer science;operating system;static memory allocation;overlay;extended memory;flat memory model;cache-only memory architecture;memory map;memory management	OS	-12.550841712661498	57.03629663248442	160804
3f339760068da1db481c670490aa146975f892d6	data infrastructure at linkedin	database indexing;distributed data;fault tolerant;linkedin;routing;storage management;log data data infrastructure projects linkedin social networking sites core data sets request processing requirements open source projects voldemort fault tolerant key value store data bus database change delivery espresso distributed data store flexible schemas secondary indexing kafka messaging system activity events;software fault tolerance;companies;public domain software;indexes;servers;storage management database indexing distributed databases public domain software social networking online software fault tolerance;pipelines;indexation;social networking online;distributed databases;linkedin indexes companies servers routing pipelines;social networking sites;open source	Linked In is among the largest social networking sites in the world. As the company has grown, our core data sets and request processing requirements have grown as well. In this paper, we describe a few selected data infrastructure projects at Linked In that have helped us accommodate this increasing scale. Most of those projects build on existing open source projects and are themselves available as open source. The projects covered in this paper include: (1) Voldemort: a scalable and fault tolerant key-value store, (2) Data bus: a framework for delivering database changes to downstream applications, (3) Espresso: a distributed data store that supports flexible schemas and secondary indexing, (4) Kafka: a scalable and efficient messaging system for collecting various user activity events and log data.	attribute–value pair;core data;data infrastructure;data store;distributed computing;downstream (software development);fault tolerance;inter-process communication;key-value database;open-source software;requirement;routing;scalability	Aditya Auradkar;Chavdar Botev;Shirshanka Das;Dave De Maagd;Alex Feinberg;Phanindra Ganti;Lei Gao;Bhaskar Ghosh;Kishore Gopalakrishna;Brendan Harris;Joel Koshy;Kevin Krawez;Jay Kreps;Shi Lu;Sunil Nagaraj;Neha Narkhede;Sasha Pachev;Igor Perisic;Lin Qiao;Tom Quiggle;Jun Rao;Bob Schulman	2012	2012 IEEE 28th International Conference on Data Engineering	10.1109/ICDE.2012.147	database index;computer science;data mining;database;world wide web;distributed database	DB	-19.07587399148382	53.73883166486656	161327
688826a3099daf69332e9f3f70c0cc31eb58b265	maiterstore: a hot-aware, high-performance key-value store for graph processing		Recently, many cloud-based graph computation frameworks are proposed, such as Pregel, GraphLab and Maiter. Most of them exploit the in-memory storage to obtain fast random access which is required for many graph computation. However, the exponential growth in the scale of large graphs and the limitation of the capacity of main memory pose great challenges to these systems on their scalability. In this work, we present a high-performance key-value storage system, called MaiterStore, which addresses the scalability challenge by using solid state drives (SSDs). We treat SSDs as an extension of memory and optimize the data structures for fast query of the large graphs on SSDs. Furthermore, observing that hot-spot property and skewed powerlaw degree distribution are widely existed in real graphs, we propose a hot-aware caching (HAC) policy to effectively manage the hot vertices (frequently accessed vertices). HAC can conduce to the substantial acceleration of the graph iterative execution. We evaluate MaiterStore through extensive experiments on real large graphs and validate the high performance of our system as the graph storage.	attribute–value pair;cloud computing;computation;computer data storage;data structure;degree distribution;experiment;graph (abstract data type);high-availability cluster;hot spare;in-memory database;iterative method;key-value database;random access;scalability;solid-state drive;time complexity;vertex (geometry)	Dong Chang;Yanfeng Zhang;Ge Yu	2014		10.1007/978-3-662-43984-5_9	graph database	OS	-16.19818900046759	54.369717884442494	161674
c9109f1333bf699ba3edb5b6e80a902e5662738d	an optimization framework for adjoint-based climate simulations: a case study of the zebiak-cane model	adjoint;zc model;data stack;adaptive;parallelizing	Conventional adjoint-based climate simulations are generally time-consuming because they require the frequent reading of historical data from hard disks, adopt a calculate-all implementation strategy, and work in a serial way. To improve the computational efficiency of these simulations, this paper proposes a framework with a series of optimizations, including an I/O optimization, several implementation improvements, and two parallelizing schemes. To demonstrate the feasibility and performance of the proposed framework, a Zebiak-Cane (ZC) case model is studied and presented in this paper. Experimental results show that the proposed framework can greatly improve the computational efficiency of climate simulations.	automatic parallelization;computation;disk storage;hard disk drive;input/output;mathematical optimization;simulation	Shijin Yuan;Shicheng Wen;Hongyu Li;Xinfeng Zhang	2014	IJHPCA	10.1177/1094342013495096	mathematical optimization;parallel computing;simulation;computer science;theoretical computer science;adaptive behavior	HPC	-16.827523738670518	54.38362209295068	161755
f8fe361b3433cba4e660ac0f24bc496376e45cf1	an efficient gear-shifting power-proportional distributed file system		Recently, power-aware distributed file systems for efficient big data processing have increasingly moved toward power proportional designs. However, inefficient gear-shifting in such systems is an important issue that can seriously degrade their performance. To address this issue, we propose and evaluate an efficient gear-shifting power proportional distributed file system. The proposed system utilizes flexible data placement that reduces the amount of reflected data and has an architecture that improves the metadata management to achieve high-efficiency gear-shifting. Extensive empirical experiments using actual machines based on the HDFS demonstrated that the proposed system gains up to (22,%) better throughput-per-watt performance. Moreover, a suitable metadata management setting corresponding to the amount of data updated while in low gear is found from the experimental results.	clustered file system;dce distributed file system	Hieu Hanh Le;Satoshi Hikida;Haruo Yokota	2015		10.1007/978-3-319-22852-5_14	self-certifying file system;torrent file;device file;computer file;network file system;versioning file system;unix file types;ssh file transfer protocol;open;distributed file system;file system fragmentation;file area network;replication;file control block;virtual file system	HPC	-13.9987005155366	53.974666980594336	161828
3f7a6c03f901fc3893170fa1d9ee83a2bac59e94	a partitioner-centric model for samr partitioning trade-off optimization: part ii	current partitioning;structured adaptive mesh application;execution time;dynamic property;part ii;dynamic behavior;structured adaptive grid;partitioner-centric model;grid hierarchy;samr partitioning trade-off optimization;data migration;different adaptive simulation;cost function;adaptive mesh refinement;computer applications;time management;parallel processing;concurrent computing;application software;predictive models;scalability;load balance;load balancing	Optimal partitioning of structured adaptive mesh applications necessitates dynamically determining and optimizing for the most time-inhibiting factor, such as data migration and communication volume. However, a trivial monitoring of an application evaluates the current partitioning rather than the inherent properties of the grid hierarchy. We present a model that given a structured adaptive grid, determines ab initio to what extent the partitioner should focus on reducing the amount of data migration to reduce execution time. This model contributes to the meta-partitioner, our ultimate aim of being able to select and configure the optimal partitioner based on the dynamic properties of the grid hierarchy and the computer. We validate the predictions of this model by comparing them with actual measurements (via traces) from four different adaptive simulations. The results show that the proposed model generally captures the inherent optimization-need in SAMR applications. We conclude that our model is a useful contribution, since tracking and adapting to the dynamic behavior of such applications lead to potentially large decreases in execution times.	adaptive filter;adaptive mesh refinement;experiment;mathematical optimization;requirement;run time (program lifecycle phase);simulation;tracing (software)	Johan Steensland;Jaideep Ray	2004	Workshops on Mobile and Wireless Networking/High Performance Scientific, Engineering Computing/Network Design and Architecture/Optical Networks Control and Management/Ad Hoc and Sensor Networks/Compil	10.1109/ICPPW.2004.1328022	parallel processing;parallel computing;real-time computing;concurrent computing;computer science;load balancing;theoretical computer science;operating system;distributed computing	HPC	-17.00440527615327	58.61319782226153	162543
18d7ffeb9f0cdc9f6504376881a828fb5afef336	miyakodori: a memory reusing mechanism for dynamic vm consolidation	virtualization;memory management;compounds;virtual machines cloud computing computer centres storage management;storage management;computer centres;computer aided software engineering;servers;virtual machines;energy consumption;iaas cloud virtualization live migration;iaas cloud;servers memory management benchmark testing energy consumption compounds computer aided software engineering educational institutions;benchmark testing;cloud computing;dynamic consolidation systems miyakodori memory reusing mechanism dynamic vm consolidation infrastructure as a service datacenters virtual machines resource utilization live migration techniques large data transfer;live migration	In Infrastructure-as-a-Service datacenters, the placement of Virtual Machines (VMs) on physical hosts are dynamically optimized in response to resource utilization of the hosts. However, existing live migration techniques, used to move VMs between hosts, need to involve large data transfer and prevents dynamic consolidation systems from optimizing VM placements efficiently. In this paper, we propose a technique called “memory reusing” that reduces the amount of transferred memory of live migration. When a VM migrates to another host, the memory image of the VM is kept in the source host. When the VM migrates back to the original host later, the kept memory image will be “reused”, i.e. memory pages which are identical to the kept pages will not be transferred. We implemented a system named MiyakoDori that uses memory reusing in live migrations. Evaluations show that MiyakoDori significantly reduced the amount of transferred memory of live migrations and reduced 87% of unnecessary energy consumption when integrated with our dynamic VM consolidation system.	cloud computing;data center;page (computer memory);prototype;semiconductor consolidation;virtual machine	Soramichi Akiyama;Takahiro Hirofuchi;Ryousei Takano;Shinichi Honiden	2012	2012 IEEE Fifth International Conference on Cloud Computing	10.1109/CLOUD.2012.56	embedded system;benchmark;interleaved memory;real-time computing;virtualization;cloud computing;computer science;virtual machine;operating system;computer-aided software engineering;server;memory management	HPC	-13.077913469918098	54.20692842185512	162583
37d726e7627126bd48d883d94bba63b7ae625246	the data reliability techniques in bc-kvdb	reliability;data integrity;reliability servers synchronization monitoring writing data models cloud computing;storage management;会议论文;column storage;sequence write performance data reliability techniques distributed storage system cloud storage bc kvdb big cloud key value db multimaster mechanism single point of failure problem secondary master flexible replica management technique key value data chunk hbase random read write test;key value db;key value db cloud storage reliability column storage replica;cloud storage;replicated databases;replica;cloud computing;storage management cloud computing data integrity replicated databases	Key-Value DB has been become a novel distributed storage framework for cloud storage. It mainly focuses on solving the problems of horizontal scalability and high throughput for cloud storage. However, data reliability has become bottleneck problem for the distributed Key-Value DB. In this paper, we introduce a novel distributed storage system: BC-kvDB (Big Cloud key-value DB). In BC-kvDB, we propose multi-master mechanism to solve the problem of single point of failure for secondary master. Besides, we also introduce a novel flexible replica management technique for key-value data chunk. The experiment results expose that BC-kvDB performs better than Hbase on random read/write test. It also achieved a factor of 2.5 improvements in sequence write performance than Hbase.	apache hbase;attribute–value pair;cloud storage;clustered file system;computer data storage;data redundancy;database engine;multi-master replication;random access;reliability engineering;replication (computing);scalability;self-organization;single point of failure;thread-local storage;throughput	Guangjun Wu;Yanqin Zhang;Dongan Wang;Shupeng Wang;Ming Chen	2012	2012 IEEE 2nd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2012.6664450	parallel computing;converged storage;cloud computing;computer science;operating system;data integrity;reliability;database;world wide web	HPC	-14.25625516399697	55.00384878768852	162978
421a57a536788e39e9de93294cda1ff35d12a28a	an energy efficient load balancing algorithm based on the active time of cores		Server cluster systems are widely used to realize scalable and high performance computing systems with virtual machine technologies. A large amount of electric energy is consumed in a server cluster system since a server cluster system is composed of large number of servers and multiple servers consume electric energy to perform application processes on multiple virtual machines. In order to design and implement an energy-efficient server cluster system, it is necessary to realize energy-efficient load balancing algorithms. In this paper, the active time-based (ATB) algorithm is proposed to select a virtual machine for each request process so that the total electric energy of a server cluster to perform computation type application processes can be reduced. In the ATB algorithm, it is not necessary to collect a state of every process on every virtual machine to estimate the electric energy of each server. We evaluate the ATB algorithm in terms of the total electric energy of a server cluster compared with the energy consumption laxity based (ECLB) and basic round-robin (RR) algorithms.	algorithm;load balancing (computing)	Tomoya Enokido;Dilawaer Duolikun;Makoto Takizawa	2017		10.1007/978-3-319-69811-3_16	computer network;scalability;distributed computing;energy consumption;computer science;efficient energy use;virtual machine;computer cluster;algorithm;load balancing (computing);server;supercomputer	EDA	-18.050115797360903	58.78694341280549	163323
7bf0d52dc35f4fbf1a00fa139b9f7e2f71f912bb	capi-flash accelerated persistent read cache for apache cassandra		In real-world NoSQL deployments, users have to trade off CPU, memory, I/O bandwidth and storage space to achieve the required performance and efficiency goals. Data compression is a vital component to improve storage space efficiency, but reading compressed data increases response time. Therefore, compressed data stores rely heavily on using the memory as a cache to speed up read operations. However, as large DRAM capacity is expensive, NoSQL databases have become costly to deploy and hard to scale. In our work, we present a persistent caching mechanism for Apache Cassandra on a high-throughput, low-latency FPGA-based NVMe Flash accelerator (CAPI-Flash), replacing Cassandra's in-memory cache. Because flash is dramatically less expensive per byte than DRAM, our caching mechanism provides Apache Cassandra with access to a large caching layer at lower cost. The experimental results show that for read-intensive workloads, our caching layer provides up to 85% improved throughput and also reduces CPU usage by 25% compared to default Cassandra.	algorithm;apache cassandra;apache hbase;byte;cpu cache;cache (computing);central processing unit;data compression;data store;dynamic random-access memory;experiment;field-programmable gate array;flash memory;google bigtable;high-throughput computing;in-memory database;input/output;microsoft cryptoapi;nosql;overhead (computing);profiling (computer programming);response time (technology);rocksdb;software deployment;terabyte;throughput	Bedri Sendir;Madhusudhan Govindaraju;Rei Odaira;H. Peter Hofstee	2018	2018 IEEE 11th International Conference on Cloud Computing (CLOUD)	10.1109/CLOUD.2018.00035	nosql;computer science;cpu time;throughput;nvm express;real-time computing;speedup;cache;non-volatile memory;benchmark (computing)	DB	-13.725497943093597	53.42268478024709	163336
0b78b13ec94de8ab09151618d16371b38f598d5c	scheduling with storage constraints	performance guarantee;memory management;processor scheduling approximation algorithms scheduling algorithm embedded system constraint optimization memory management computer aided instruction embedded computing cost function physics;constraint optimization;cost function;approximation algorithms;processor scheduling;storage management;computer aided instruction;approximation algorithms storage constraints cumulative memory occupation scheduling multi system on chip embedded systems instruction code;embedded system;approximation theory;physics;embedded systems;multi system on chip;scheduling algorithm;storage constraints;system on chip;scheduling;list scheduling;system on chip approximation theory scheduling storage management;cumulant;embedded computing;cumulative memory occupation;instruction code	Cumulative memory occupation is a quite intuitive but not so studied constraint in scheduling. The interest in such a constraint is present in multi-system-on-chip, embedded systems for storing instruction code, or in scientific computation for storing results. Memory occupation seen as a constraint is impossible to solve with approximation algorithms. We believe that transforming the constraint into a second objective to optimize helps to deal with such constraints. The problem addressed in this paper is to schedule tasks on identical processors in order to minimize both maximum completion time and maximum cumulative memory occupation. For independent tasks, a family of algorithms with good approximation ratios based on a PTAS is given. Several approximation ratios are proved to be impossible to achieve with any schedule. The precedence constrained case is then studied and a family of performance guaranteed algorithms based on List Scheduling is proposed. Finally, optimizing the mean completion time as a third objective is also studied and a tri-objective algorithm is given.	approximation algorithm;central processing unit;computation;computational science;embedded system;list scheduling;opcode;ptas reduction;schedule (computer science);scheduling (computing);system on a chip;triangular function	Erik Saule;Pierre-François Dutot;Grégory Mounié	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536292	constrained optimization;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;scheduling;approximation algorithm	Embedded	-12.589192657251987	60.27895099826481	163394
9aac47df96587be38c38f53543e0426fa4a42ebf	resource failure impact on job execution in grid	failure pattern resource failure job execution grid environment heterogeneous distributed resources geographically distributed resources job scheduling pull method gridsim simulation toolkit;scheduling fault tolerant computing grid computing resource allocation;resource management grid computing fault tolerance geographic information systems processor scheduling distributed computing computer networks application software scheduling algorithm communications technology;failure pattern;oscillations;gridsim simulation toolkit;resource failure;fault tolerant;execution time;processor scheduling;resource allocation;resource management;biological system modeling;pull method;fault tolerant computing;fault tolerant systems;geographically distributed resources;geographic information systems;scheduling;fault tolerance;grid environment;allocation policy;heterogeneous distributed resources;failure pattern resource failure allocation policy execution time fault tolerance pull method;grid computing;job scheduling;geographic distribution;job execution	Grid environment, being a collection of heterogeneous and geographically distributed resources, is prone to many kinds of failures such as process failures, resource and network failures. In this paper, we address the problem of resource failure. Resources in grid oscillate between being available and unavailable to the grid. When and how they do so, depends on the failure characteristics of the machines, the policies of resource owners and the scheduling policies. The research work involves implementation of job scheduling in a grid based on pull method for handling resource failure using GridSim simulation toolkit. We also demonstrate how the job execution time varies under different resource failure conditions using different failure patterns.	job scheduler;run time (program lifecycle phase);scheduling (computing);simulation;software design pattern	P. K. Suri;Manpreet Singh	2009	2009 International Conference on Advances in Recent Technologies in Communication and Computing	10.1109/ARTCom.2009.32	fault tolerance;parallel computing;real-time computing;computer science;resource management;operating system;distributed computing	HPC	-17.735151188322714	59.89516882438409	163681
8aacd44a2af76b986b67cf27fb35bcfdbc831841	eraid: a queueing model based energy saving policy	energy efficiency;control systems;degradation;measurement;energy efficient;high performance computing;energy consumption;queueing model;energy storage;delay energy storage degradation control systems throughput high performance computing energy management measurement energy consumption energy efficiency;energy saving;disk array;energy management;throughput	Recently energy consumption becomes an ever critical concern for both low-end and high-end storage servers. In this paper, we propose an energy saving policy, eRAID (energy-efficient RAID), for mirrored redundant disk array systems. eRAID saves energy by spinning down partial or entire mirror disk group with controllable performance degradation. We first develop an energy-saving model for multi-disk environments by taking into account both disk characteristics and workload features. Then, we develop a queueing model based performance (response time and throughput) control scheme for eRAID. Experimental results show that eRAID can save up to 32% energy without violating predefined performance degradation constraints.	cpu cache;disk array;disk storage;elegant degradation;load balancing (computing);nested raid levels;queueing theory;raid;response time (technology);server (computing);server-side;throughput;tracing (software)	Dong Li;Jun Wang	2006	14th IEEE International Symposium on Modeling, Analysis, and Simulation	10.1109/MASCOTS.2006.23	embedded system;real-time computing;computer science;control system;operating system;efficient energy use	Embedded	-14.855004821598266	56.162470370059815	163715
c527b0f2bd1d3079e536252f0d3d7bafdea6e9c0	dynamic grid load sharing with adaptive dissemination protocols	hybrid grids;hybrid clouds;grid resource scheduling;information dissemination;grid load balancing	Scheduling in large scale dynamic grids comprising eclectic collections of resources is increasingly difficult. Autonomous resource neighborhoods may wish to determine the level of grid offered load that they can or will accept; different sites may wish to attract different amounts of load, to satisfy some desired property within a grid economy. This changes the traditional notion of load sharing, which generally assumes that the desired equilibrium should be an equal distribution of load across all participating machines, because they are under the jurisdiction of a single site, and therefore more likely to implement one common policy. In large-scale grids, nodes and neighborhoods should instead get a portion of the load that best matches their local policies for supporting and admitting grid jobs. This article describes information dissemination protocols that can distribute load in this way, without using load rebalancing through job migration, which is more difficult and costly in large-scale heterogeneous grids. Essentially, nodes adjust their advertising rates and aggressiveness to influence where jobs get scheduled. We report experimental results with example resource configurations in which each resource neighborhood determines its ideal grid load and disseminates accordingly. In turn, each neighborhood attracts the requisite amount of resource requests from the grid. Moreover, performance does not degrade: overall query satisfaction rates are within 9% of both adaptive dissemination protocols that use static adaptation policies, and static dissemination protocols that may be custom-tailored to specific resource and load distributions.	ibm notes;job stream;overhead (computing);rejection sampling;scheduling (computing);simulation	Deger Cenk Erdil;Michael J. Lewis	2010	The Journal of Supercomputing	10.1007/s11227-010-0507-y	parallel computing;real-time computing;simulation;computer science;operating system;distributed computing;computer security	HPC	-18.818192835073734	59.57146534679655	164352
fd5e1e54b1b2b67e441cca5d037bb39748daa1f1	dynamic allocation of power delivery paths in consolidated data centers based on adaptive ups switching		Although technique known as server consolidation approach in a data center can reduce the overall power consumption, the Power Usage Effectiveness (PUE) of the data center will still be negatively affected with presence of distributed Uninterruptible Power Supplies (UPSs). The impact on the PUE arises from the fact that all UPS modules are kept running to maintain power availability for only a few active servers during off-peak periods. To address this problem, in this paper technique for reducing power consumption in a data center by consolidating the UPSs used during off peak periods is proposed. The proposed technique achieves power savings by leveraging a micro Automatic Transfer Switch (micro-ATS) at the server end. The novelty of this work lies in developed adaptive algorithm that continuously looks for opportunities to reduce the number of UPSs by offloading under-loaded UPSs to a neighboring UPS whenever that neighboring UPS can handle the extra load. In various simulated scenarios involving corporate data centers, our approach demonstrates the ability to save more power and achieve lower PUE degradation compared with state-of-the-art approaches such as server consolidation. Specifically, the proposed approach achieves a savings of approximately 20% to 40% in a data center’s power consumption, depending on the data center’s off-peak periods, which can be accomplished using only 80% of the UPS modules in the data center.	adaptive algorithm;data center;elegant degradation;frequency capping;heuristic;mathematical optimization;memory management;power usage effectiveness;semiconductor consolidation;server (computing);uninterruptible power supply;ups (debugger)	Fawaz Al Hazemi;Yuyang Peng;Chan-Hyun Youn;Josip Lorincz;Chao Li;Guo Song;Raouf Boutaba	2018	Computer Networks	10.1016/j.comnet.2018.08.004	adaptive algorithm;computer network;novelty;computer science;data center;power usage effectiveness;transfer switch;server	HPC	-13.790536677420267	56.83830676230382	164607
28f9e79b4e4e293c875057a91571797bb5ffab0e	active caching for kvs dynamic scaling	indexes layout web and internet services facebook computers writing;kvs;i o scheduler kvs page cache;page cache;i o scheduler time slice active caching kvs dynamic scaling performance improvement load size key value store dbms database management system large scale internet services cassandra open source kvs implementation node joining time evaluation disk i o disk access analysis file access page cache sequential i o performance;i o scheduler;scheduling cache storage database management systems input output programs paged storage public domain software	Load size to a service in the Internet remarkably changes every hour. Thus, it is expected that service system scale should be changed dynamically according to load size. KVS (Key-Value Store) is scalable DBMS (database management system) and widely used in large-scale Internet services. In this paper, we focus on Cassandra, a popular open-source KVS implementation, and discuss methods for improving dynamic scaling performance. First, we evaluate node joining time, which is time to complete adding a node to a running KVS system, and show that its bottleneck process is disk I/O in the existing nodes. Second, we analyze disk accesses in the bottleneck nodes and point it out that some heavily accessed files cause a large number of disk accesses. Third, we propose two methods for decreasing node joining time. One method is to reduce disk accesses significantly by keeping the heavily accessed file in the page cache. The other method is to increase sequential I/O performance by enlarging time slice of I/O scheduler. Lastly, we evaluate our methods and demonstrate that our method can improve scaling-out performance of Cassandra.	apache cassandra;cache (computing);i/o scheduling;image scaling;input/output;internet;key-value database;open-source software;page cache;preemption (computing);scalability;scheduling (computing);web service	Shohei Miyokawa;Taiki Tokuda;Saneyasu Yamaguchi	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.67	real-time computing;page cache;computer science;operating system;database	OS	-13.821955380007307	53.65991095722954	164741
a2dbfb0b2af5135de68755fdd119576f334cb09f	research and application of the intelligent flow	computational modeling cloud computing analytical models instruction sets conferences fault tolerance;distributed processing;parallel programming;parallel programming cloud computing distributed processing;mapreduce intelligent flow distributed computing hadoop;intelligent flow fault tolerance intelligent flow design cloud platform intelligent flow model execution intelligent flow dynamic access;cloud computing	This paper designs an Intelligent flow which is computed on the cloud platform, the main points are the designation of the Intelligent flow, analyzing and recomposing the Intelligent flow model, and implementation of the program on cloud platform. The experiment is focused on the model execution, dynamic access and fault-tolerant of the intelligent flow executed on the platform. It shows that intelligent flow has a good result test on the cloud platform.	cloud computing;fault tolerance	Yang Liu;Lingyu Xu;Liang Chen;Fei Zhong	2013	2013 IEEE International Conference on Green Computing and Communications and IEEE Internet of Things and IEEE Cyber, Physical and Social Computing	10.1109/GreenCom-iThings-CPSCom.2013.407	real-time computing;cloud computing;computer science;theoretical computer science;operating system;distributed computing	Robotics	-18.896283268713216	59.911246120963234	165233
6abf5107efc723c655956f027b4a67565b048799	the adaptive radix tree: artful indexing for main-memory databases	database indexing;cache storage;vegetation arrays subspace constraints indexing;adaptive radix tree prefix lookup range scan sorted order data art performance internal node worst case space consumption deletion insertion read only search tree lookup performance point query main memory index hash table on cpu cache utilization balanced binary search tree in memory data structure index structure performance main memory database system ram min memory capacity artful indexing;tree data structures;tree searching cache storage database indexing table lookup tree data structures;tree searching;table lookup	Main memory capacities have grown up to a point where most databases fit into RAM. For main-memory database systems, index structure performance is a critical bottleneck. Traditional in-memory data structures like balanced binary search trees are not efficient on modern hardware, because they do not optimally utilize on-CPU caches. Hash tables, also often used for main-memory indexes, are fast but only support point queries. To overcome these shortcomings, we present ART, an adaptive radix tree (trie) for efficient indexing in main memory. Its lookup performance surpasses highly tuned, read-only search trees, while supporting very efficient insertions and deletions as well. At the same time, ART is very space efficient and solves the problem of excessive worst-case space consumption, which plagues most radix trees, by adaptively choosing compact and efficient data structures for internal nodes. Even though ART's performance is comparable to hash tables, it maintains the data in sorted order, which enables additional operations like range scan and prefix lookup.	b+ tree;benchmark (computing);best, worst and average case;bottleneck (engineering);cpu cache;central processing unit;compare-and-swap;computer data storage;data structure;disjoint-set data structure;fan-out;guid partition table;hash table;ibm tivoli storage productivity center;in-memory database;lazy evaluation;lookup table;neural coding;paging;radix tree;random-access memory;read-only memory;red–black tree;search tree;tree (data structure);trie	Viktor Leis;Alfons Kemper;Thomas Neumann	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544812	segment tree;database index;red–black tree;parallel computing;binary search tree;radix tree;computer science;trie;theoretical computer science;order statistic tree;interval tree;database;fractal tree index;search tree;tree;ternary search tree;k-d-b-tree;programming language;tree traversal	DB	-13.06800798502685	53.639324936754306	165316
316b3b674e4210b321cde874f3a55390b515d096	asep: an adaptive sequential prefetching scheme for second-level storage system	sequential prefetching;active time point loss;second level buffer caches;prefetching depth	In model storage systems, the multilevel buffer caches hierarchy is widely used to improve the I/O performance of disks. In the hierarchy, the referenced pages in second-level buffer cache have larger reuse distance that is the number of accesses between two references to the same block in a reference sequence. These reuse distances have close value with their lifetimethe time they are conserved in buffer cache. Therefore, this tiny difference can be more easily eliminated by the prefetched (not yet accessed) data that reduces the lifetime of referenced pages. This leads more pages than those replaced by prefetching to lose their re-access opportunity. This anomaly influence can significantly reduce the overall hit ratio of buffer cache and, unfortunately, it is ignored by traditional sequential prefetching algorithms. To address this problem, we propose an Adaptive SEquential Prefetching (named ASEP) that uncovers this anomaly influence and adaptively adjusts the prefetching depth by considering the access characteristics in second-level buffer cache. We extensively evaluate ASEP by conducting trace driven experiments with a prototype implements in Linux (software RAID-MD). The experiments’ results, under varied workloads from transaction processing applications to Web searching applications, show that ASEP outperforms the default sequential prefetching scheme in Linux kernel and other heuristic schemes, with the response time improvement by up to 49.7% and the cache hit ratio improvement ranging from 0.2~ 8.5%.	algorithm;anomaly detection;cpu cache;cache (computing);experiment;heuristic;hit (internet);input/output;link prefetching;linux;page cache;prototype;raid;response time (technology);transaction processing	Xiaodong Shi;Dan Feng	2012	JCP	10.4304/jcp.7.8.1853-1864	parallel computing;real-time computing;computer hardware;computer science;operating system;world wide web	OS	-12.44489011541855	54.237573466089565	165812
1b3aedc03e7b743e340a731702333bcc95134229	speculative region-based memory management for big data systems	big data systems;managed languages;region based memory management;language;performance optimization	Most real-world Big Data systems are written in managed languages. These systems suffer from severe memory problems due to the massive volumes of objects created to process input data. Allocating and deallocating a sea of objects puts a severe strain on the garbage collector, leading to excessive GC efforts and/or out-of-memory crashes. Region-based memory management has been recently shown to be effective to reduce GC costs for Big Data systems. However, all existing region-based techniques require significant user annotations, resulting in limited usefulness and practicality. This paper reports an ongoing project, aiming to design and implement a novel speculative region-based technique that requires only minimum user involvement. In our system, objects are allocated speculatively into their respective regions and promoted into the heap if needed. We develop an object promotion algorithm that scans regions for only a small number of times, which will hopefully lead to significantly improved memory management efficiency. We also present an OpenJDK-based implementation plan and an evaluation plan.	algorithm;big data;data system;garbage collection (computer science);iteration;memory safety;openjdk;out of memory;region-based memory management;speculative execution	Khanh Nguyen;Lu Fang;Guoqing Xu;Brian Demsky	2015		10.1145/2818302.2818308	real-time computing;simulation;engineering;database	DB	-17.536167641630605	54.67969195866121	165911
2f88172c487098d777b073495c30e867edf959b7	gpu-accelerated block-max query processing		In this paper, we propose a method for parallel top-k query processing on GPU(s). We employ a novel partitioning strategy which splits the posting lists according to document ID numbers. Individual GPU threads simultaneously perform top-k query processing within their allocated subsets of posting lists, the results of the query are merged to give the final top-k results. We further design a CPU-GPU cooperative query processing method, where a majority of queries involving shorter posting lists are processed on the GPU side. We experiment with AND, OR, WAND, and Block-Max WAND (BMW) queries, with experimental results showing a promising improvement in query throughput, particularly in the case of BMW queries.	algorithm;central processing unit;database;graphics processing unit;parallel computing;query optimization;query throughput	Haibing Huang;Mingming Ren;Yue Zhao;Rebecca J. Stones;Rui Zhang;Gang Wang;Xiaoguang Liu	2017		10.1007/978-3-319-65482-9_15	parallel computing;computer science;online aggregation;query throughput;query expansion;web query classification;sargable;query language;rdf query language;query optimization	DB	-14.585019554633265	56.48196922075242	166511
3c303d7a2916f0788d51b2e98abcec71553c4d57	resource optimization for speculative execution in a mapreduce cluster	pattern clustering;optimisation;clustering algorithms approximation algorithms algorithm design and analysis optimization load modeling google simulation;public domain software;data analysis;naive method resource optimization problem mapreduce cluster large scale data analytics resource management enhanced speculative execution algorithm ese;public domain software data analysis optimisation pattern clustering;theoretical analysis mapreduce job scheduling speculative execution	The MapReduce paradigm is now the de facto standard for large-scale data analytics. In this paper we address the resource management issues in MapReduce Cluster. Speculative execution (task backup) plays an important role in resource management. We propose two different strategies and build two models to formulate the backup issue as an optimization problem when the cluster is lightly loaded. Moreover, we present an Enhanced Speculative Execution (ESE) algorithm when the cluster is heavily loaded and adopt the approximate analysis to get an optimal value for the parameter in the algorithm. The simulation results show that the algorithm can reduce the job completion time by 50% while consuming much less resource compared to the naive method without backup.	approximation algorithm;backup;extensible storage engine;mapreduce;mathematical optimization;optimization problem;programming paradigm;simulation;speculative execution	Huanle Xu;Wing Cheong Lau	2013	2013 21st IEEE International Conference on Network Protocols (ICNP)	10.1109/ICNP.2013.6733646	parallel computing;real-time computing;computer science;operating system;database;data analysis;public domain software	HPC	-17.442274586144762	57.26437752183928	166893
f13bb8cc112ce90d915b55abc752e4e9b9a268ac	an update-overhead-aware caching policy for write-optimized file systems on smr disks		To accommodate the sheer volume of data in the era of Big Data and Cloud Computing, both new storage medium technologies and high-performance file systems are proposed. For storage medium, Shingled Magnetic Recording (SMR) increases the areal density by overlapping adjacent tracks so as to provide larger storage capacity. On the other hand, write-optimized indexes (WOI) file systems are also studied and can now outperform conventional file systems with orders of magnitude. However, the main drawback of SMR is the random-write restriction because random-write operations will cause the extra overhead of rewriting data stored in overlapped tracks. The rewriting overhead is amplified by the recursive entry update behavior of WOI-based file systems. Therefore, the rewriting issue becomes a serious performance overhead when adopting SMR drives as underlying storage devices for WOI-based file systems. To mitigate the write amplification problem when deploying WOI-based file systems on SMR disks, this paper proposes the update-overhead-aware caching policy to reduce the update overhead with the help of flash-based storage devices. To evaluate the performance of the proposed design, the B+-tree as a case study. The experimental results are promising.	b+ tree;big data;cache (computing);cloud computing;copy-on-write;overhead (computing);random access;recursion;rewriting;shingled magnetic recording	Shuo-Han Chen;Wei-Shin Li;Min-Hong Shen;Yi-Han Lien;Tseng-Yi Chen;Tsan-sheng Hsu;Hsin-Wen Wei;Wei-Kuan Shih	2017	2017 IEEE 36th International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2017.8280486	real-time computing;big data;cloud computing;computer science;shingled magnetic recording;benchmark (computing);write amplification;rewriting	OS	-13.123758095942296	54.041395166754995	166906
7376fdfcd0e819ea170c5fef8e9764ab3936147e	decentralized list scheduling	list algorithms;performance guarantee;cluster computing;work stealing;scheduling;scheduling problem;list scheduling;task graphs;potential function;lower bound	Classical list scheduling is a very popular and efficient technique for scheduling jobs for parallel and distributed platforms. It is inherently centralized. However, with the increasing number of processors, the cost for managing a single centralized list becomes too prohibitive. A suitable approach to reduce the contention is to distribute the list among the computational units: each processor only has a local view of the work to execute. Thus, the scheduler is no longer greedy and standard performance guarantees are lost. The objective of this work is to study the extra cost that must be paid when the list is distributed among the computational units. We first present a general methodology for computing the expected makespan based on the analysis of an adequate potential function which represents the load imbalance between the local lists. We obtain an equation giving the evolution of the potential by computing its expected decrease in one step of the schedule. Our main theorem shows how to solve such equations to bound the makespan. Then, we apply this method to several scheduling problems, namely, for unit independent tasks, for weighted independent tasks and for tasks with precedence constraints. More precisely, we prove that the time for scheduling a global workload W composed of independent unit tasks on m processors is equal to W/m plus an additional term proportional to log2W . We provide a lower bound which shows that this is optimal up to a constant. This result is extended to the case of weighted independent tasks. In the last setting, precedence task graphs, our analysis leads to an improvement on the bound of Arora et al. (2001). We end with some experiments using a simulator. The distribution of the makespan is shown to fit existing probability laws. Moreover, the simulations give a better insight into the additive term whose value is shown to be around 3 log2W confirming the precision of our analysis.	central processing unit;centralized computing;computation;experiment;greedy algorithm;job stream;list scheduling;makespan;schedule (computer science);scheduling (computing);simulation;utility functions on indivisible goods	Marc Tchiboukdjian;Nicolas Gast;Denis Trystram	2013	Annals OR	10.1007/s10479-012-1149-7	list update problem;fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;mathematical optimization;real-time computing;dynamic priority scheduling;computer cluster;computer science;operations management;self-organizing list;two-level scheduling;mathematics;distributed computing;upper and lower bounds;round-robin scheduling;scheduling	Theory	-15.733485948633849	59.98042252113115	167544
bc4e52a186d809792e57ae819f6764acdec210f3	effective processor load balancing using multi-objective parallel extremal optimization		The paper presents how Extremal Optimization can be used in a parallel multi-objective load balancing algorithm applied in execution of distributed programs. Extremal Optimization is used to find task migration which dynamically improves processor load balance in a distributed system. In the proposed multi-objective approach we use three objectives relevant to distributed processor load balancing in execution of program tasks. They are: computational load balance of processors, the volume of inter-processor communication and task migration metrics. In the algorithms additional criteria are used which are based on some knowledge on the influence of the computational and communication loads on task execution. The proposed algorithms are assessed by simulation experiments with distributed execution of program macro data flow graphs. Two methods of finding compromise solutions based on the Pareto front were used: one based on a geometric (Euclidean) distance of solutions and the second one based on the Manhattan (taxicab geometry) distance. The influence of the distance geometry on the final solutions is discussed.	algorithm;central processing unit;computation;dataflow;distributed computing;euclidean distance;experiment;extremal optimization;fitness function;load balancing (computing);mathematical optimization;pareto efficiency;program optimization;quantum number;simulation;taxicab geometry	Ivanoe De Falco;Eryk Laskowski;Richard Olejnik;Umberto Scafuri;Ernesto Tarantino;Marek Tudruj	2018		10.1145/3205651.3208289	mathematical optimization;extremal optimization;multi-objective optimization;computer science;macro;data flow diagram;taxicab geometry;compromise;load balancing (computing);euclidean geometry	HPC	-14.561141807519956	59.07025812424576	168000
f2ec23bfa99e582f2f96d65d411e06666a5860a8	tajo: a distributed data warehouse system on large clusters	directed graphs;hive large clusters relational data volumes parallel databases database community suboptimal execution strategies relational distributed data warehouse system shared nothing clusters hadoop distributed file system hdfs storage layer query execution engine mapreduce framework tajo cluster query planning worker coordinator local query engine directed acyclic graph physical operators dag local query engine control distributed data flow query processing tajo architecture tpc h queries tajo cluster nodes web based user interface;pattern clustering;query processing;storage management;parallel databases;internet;information dissemination;relational databases;data warehouses;distributed control;user interfaces;pipeline processing;engines query processing planning fault tolerance fault tolerant systems catalogs;user interfaces data warehouses directed graphs distributed control information dissemination internet parallel databases pattern clustering pipeline processing query processing relational databases storage management	The increasing volumes of relational data let us find an alternative to cope with them. Recently, several hybrid approaches (e.g., HadoopDB and Hive) between parallel databases and Hadoop have been introduced to the database community. Although these hybrid approaches have gained wide popularity, they cannot avoid the choice of suboptimal execution strategies. We believe that this problem is caused by the inherent limits of their architectures. In this demo, we present Tajo, a relational, distributed data warehouse system on shared-nothing clusters. It uses Hadoop Distributed File System (HDFS) as the storage layer and has its own query execution engine that we have developed instead of the MapReduce framework. A Tajo cluster consists of one master node and a number of workers across cluster nodes. The master is mainly responsible for query planning and the coordinator for workers. The master divides a query into small tasks and disseminates them to workers. Each worker has a local query engine that executes a directed acyclic graph of physical operators. A DAG of operators can take two or more input sources and be pipelined within the local query engine. In addition, Tajo can control distributed data flow more flexible than that of MapReduce and supports indexing techniques. By combining these features, Tajo can employ more optimized and efficient query processing, including the existing methods that have been studied in the traditional database research areas. To give a deep understanding of the Tajo architecture and behavior during query processing, the demonstration will allow users to submit TPC-H queries to 32 Tajo cluster nodes. The web-based user interface will show (1) how the submitted queries are planned, (2) how the query are distributed across nodes, (3) the cluster and node status, and (4) the detail of relations and their physical information. Also, we provide the performance evaluation of Tajo compared with Hive.	apache hadoop;apache hive;computer cluster;dce distributed file system;dataflow;directed acyclic graph;distributed data flow;ibm tivoli storage productivity center;mapreduce;parallel database;performance evaluation;physical information;pipeline (computing);shared nothing architecture;user interface;web application;world wide web	Hyunsik Choi;Jihoon Son;Haemi Yang;Hyoseok Ryu;Byungnam Lim;Soohyung Kim;Yon Dohn Chung	2013	2013 IEEE 29th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2013.6544934	sargable;query optimization;query expansion;web query classification;the internet;directed graph;relational database;computer science;query by example;data warehouse;data mining;database;user interface;web search query;world wide web;query language	DB	-17.086920668945165	53.86318117197838	168001
4abd82b4a14cd1774548ebc55a0d49f9d20698c7	optimizing the time cost of the parallel structures with a limited number of processors	computational complexity;optimisation;parallel architectures;resource allocation;scheduling;shared memory systems;allocation algorithm;limited processor number;minimum time cost;optimal scheduling algorithms;parallel structures;parallelism degree;shared memory environment;time cost optimization	We develop an allocation algorithm that, in conjunction with optimal scheduling algorithms, yields the minimum time cost for a parallel structure. We assume a shared memory environment and a limited number of processors that is less than the degree of parallelism of the parallel structure	microprocessor;optimizing compiler	Tahany A. Fergany;Reda A. Ammar;Mohamad R. Neilforoshan-Dardashti	1993			parallel computing;real-time computing;computer science;analysis of parallel algorithms;distributed computing;data parallelism;task parallelism;cost efficiency	Arch	-13.384501654631228	60.11075972993455	168123
cd97d4f64c7c6d2d63348dff23407d4a647ed89d	application-level scheduling using aop	automatic generation;priority scheduling;qos;monitoring system;aspect oriented programming;software development;scheduler;performance tuning;aspect;admission control	Achieving sufficient execution performance is a challenging goal of software development. Unfortunately, violating performance requirements is often revealed at a late stage of the development. Fixing a performance problem at such a late stage is difficult in terms of cost and time. To solve this problem, this paper presents QoSWeaver, which is a tool suite for developing application-level scheduling using aspects. QoSWeaver weaves scheduling code written in an aspect into web application code. The scheduling code gets an application thread to voluntarily yield its execution to implement a custom scheduling policy. The idea of scheduling at the application level is not new, but aspect-oriented programming (AOP) makes it more realistic by separation of scheduling code. For fine-grained scheduling, QoSWeaver provides a profile-based pointcut generator, which automatically generates appropriate pointcuts. To investigate the ability of QoSWeaver for implementing practical scheduling policies, we used QoSWeaver for tuning the performance of a river monitoring system named Kasendas, which is a web application system. For reliable examination, Kasendas was originally developed by an outside corporation and then it was tuned by the authors with QoSWeaver. The authors could successfully improve the performance of Kasendas under heavy workload. The cost of the performance tuning was a reasonably small. Furthermore, our approach achieved better performance than other techniques such as admission control and priority scheduling provided by the JVM or Linux. We could implement various policies such as deadlock-aware or adaptive scheduling.	adaptive grammar;aspect-oriented programming;deadlock;dynamic priority scheduling;linux;performance tuning;pointcut;requirement;scheduling (computing);software development;web application	Kenichi Kourai;Hideaki Hibino;Shigeru Chiba	2009	Trans. Aspect-Oriented Software Development	10.1007/978-3-642-02059-9_1	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;real-time computing;earliest deadline first scheduling;aspect-oriented programming;quality of service;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;software development;genetic algorithm scheduling;operating system;foreground-background;two-level scheduling;deadline-monotonic scheduling;stride scheduling;distributed computing;scheduling;least slack time scheduling;instruction scheduling;lottery scheduling;programming language;round-robin scheduling	OS	-12.0634609032209	58.64590134545108	168582
127f9d5326351d92621b47b55a896cfd51654970	ng2c: pretenuring garbage collection with dynamic generations for hotspot big data applications		Big Data applications suffer from unpredictable and unacceptably high pause times due to Garbage Collection (GC). This is the case in latency-sensitive applications such as on-line credit-card fraud detection, graph-based computing for analysis on social networks, etc. Such pauses compromise latency requirements of the whole application stack and result from applications' aggressive buffering/caching of data, exposing an ill-suited GC design, which assumes that most objects will die young and does not consider that applications hold large amounts of middle-lived data in memory.   To avoid such pauses, we propose NG2C, a new GC algorithm that combines pretenuring with user-defined dynamic generations. By being able to allocate objects into different generations, NG2C is able to group objects with similar lifetime profiles in the same generation. By allocating objects with similar lifetime profiles close to each other, i.e. in the same generation, we avoid object promotion (copying between generations) and heap fragmentation (which leads to heap compactions) both responsible for most of the duration of HotSpot GC pause times.   NG2C is implemented for the OpenJDK 8 HotSpot Java Virtual Machine, as an extension of the Garbage First GC. We evaluate NG2C using Cassandra, Lucene, and GraphChi with three different GCs: Garbage First (G1), Concurrent Mark Sweep (CMS), and NG2C. Results show that NG2C decreases the worst observable GC pause time by up to 94.8% for Cassandra, 85.0% for Lucene and 96.45% for GraphChi, when compared to current collectors (G1 and CMS). In addition, NG2c has no negative impact on application throughput or memory usage.	algorithm;big data;concurrent mark sweep collector;credit card fraud;fragmentation (computing);garbage collection (computer science);java hotspot virtual machine;java virtual machine;observable;online and offline;openjdk;requirement;social network;throughput	Rodrigo Bruno;Luís Picciochi Oliveira;Paulo José Azevedo Vianna Ferreira	2017		10.1145/3092255.3092272	real-time computing;latency (engineering);parallel computing;garbage;computer science;virtual machine;garbage collection;compromise;java;heap (data structure);fragmentation (computing)	PL	-12.46969722721272	55.55063600840282	169772
39e6ee4c909fc0dfa03e542035559f3ec4216e24	memory resource allocation for file system prefetching: from a supply chain management perspective	memory cache;resource allocation;prefetching;file system;data prefetching;data access;dynamic adaptation;supply chain management;dynamic configuration	As an important technique to hide disk I/O latency, prefetching has been widely studied, and dynamic adaptive prefetching techniques have been deployed in diverse storage environments. However, two issues are not well addressed by previous research: (1) how to handle the prefetching resource allocation between concurrent sequential access streams with different request rates, and (2) how to coordinate prefetching at multiple levels in the data access path.  Interestingly, we found that these problems bear a strong resemblance to situations long studied in the field of supply chain management (SCM), used by retailers such as Wal-Mart. In this paper, we demonstrate how to perform the problem mapping and then apply SCM principles in practice, particularly from the branch of inventory theory, to improve data prefetching performance in storage systems. More specifically, we applied (1) two SCM policies to dynamically configure the sequential prefetching parameters, and (2) an SCM solution to correct the access pattern information distortion in multi-level prefetching. We implemented these SCM-based strategies in the Linux kernel prefetching algorithm and a multi-level storage simulator, and evaluated the performance with three types of workloads. The results indicate that the SCM approaches are able to generate up to a 55.0% of performance improvement for a real-world server workload benchmark, and up to 33.3% for a combination of Linux I/O-intensive applications.	algorithm;benchmark (computing);cpu cache;data access;data mart;distortion;input/output;inventory theory;linux;sequential access;server (computing);write-ahead logging	Zhe Zhang;Amit Kulkarni;Xiaosong Ma;Yuanyuan Zhou	2009		10.1145/1519065.1519075	data access;parallel computing;real-time computing;supply chain management;cpu cache;resource allocation;computer science;operating system	OS	-14.352134234650896	54.89077206292329	170359
3a6b7b4c0889fa8768a82a47c47cb7c681a0e866	placement of replicated tasks for distributed stream processing systems	optimal solution;replication;component;placement;task;system performance;simulation experiment;message oriented middleware;graphs and networks;distributed stream processing;stream processing;data flow	We propose an algorithm for placing tasks of data flows for streaming systems onto servers within a message-oriented middleware where certain tasks can be replicated. Our work is centered on the idea that certain transformations are stateless and can therefore be replicated. Replication in this case can cause workloads to be partitioned among multiple machines, thus enabling message processing to be parallelized and lead to improvements in performance. We propose a guided replication approach for this purpose that iteratively computes the optimal placement of replicas where each subsequent iteration of the algorithm takes as input optimal solutions computed in the previous run. As a result, the system performance is consistently improved, which eventually converges as shown in simulation results. We demonstrate, through simulation experiments with both simple and complex task flow graphs and network topologies that introducing our replication mechanism can lead to improvements in runtime performance. When system resources are scarce, the benefits of applying our replication mechanism are even greater.	algorithm;experiment;iteration;message-oriented middleware;network topology;parallel computing;profile-guided optimization;replication (computing);run time (program lifecycle phase);simulation;stateless protocol;stream processing	Geetika T. Lakshmanan;Ying Li;Robert E. Strom	2010		10.1145/1827418.1827450	data flow diagram;replication;parallel computing;real-time computing;stream processing;computer science;message oriented middleware;operating system;component;database;distributed computing;computer performance;programming language;placement	HPC	-14.965119957847168	58.95731407846718	170443
19ffc4f5129ed9d39f498f4eb901024c514263c7	unified address translation for memory-mapped ssds with flashmap	performance evaluation;datacenter;flash memories disc storage dram chips;power management;ssd file mapping mechanisms unified address translation memory mapped ssds flashmap map data dram capacity memory mapped ssd content virtual memory layer flash translation layer ftl address translations ssd interface memory mapped ssd files page tables os memory manager;energy storage	Applications can map data on SSDs into virtual memory to transparently scale beyond DRAM capacity, permitting them to leverage high SSD capacities with few code changes. Obtaining good performance for memory-mapped SSD content, however, is hard because the virtual memory layer, the file system and the flash translation layer (FTL) perform address translations, sanity and permission checks independently from each other. We introduce FlashMap, an SSD interface that is optimized for memory-mapped SSD-files. FlashMap combines all the address translations into page tables that are used to index files and also to store the FTL-level mappings without altering the guarantees of the file system or the FTL. It uses the state in the OS memory manager and the page tables to perform sanity and permission checks respectively. By combining these layers, FlashMap reduces critical-path latency and improves DRAM caching efficiency. We find that this increases performance for applications by up to 3.32x compared to state-of-the-art SSD file-mapping mechanisms. Additionally, latency of SSD accesses reduces by up to 53.2%.	dynamic random-access memory;ftl: faster than light;flash file system;flash memory controller;memory management;operating system;page table;solid-state drive	Jian Huang;Anirudh Badam;Moinuddin K. Qureshi;Karsten Schwan	2015	2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)	10.1145/2749469.2750420	data center;parallel computing;computer hardware;computer science;operating system;energy storage	Arch	-12.53097267205638	53.53555137877919	171873
f4f53c963c4fae89a7904ee4bd7a44e32a34933a	forensic analysis of b-tree file system (btrfs)		Abstract This paper identifies forensically important artifacts of B-tree file system (Btrfs), analyses changes that they incur due to node-balancing during file and directory operations, and based on the observed file system state-change proposes an evidence-extraction procedure. The findings suggested that retrieving forensic evidence in a fresh B-tree file system is difficult, the probability of evidence-extraction increases as the file system ages, internal nodes are the richest sources of forensic data, degree of evidence-extraction depends upon whether nodes are merged or redistributed, files with size less than 1 KB and greater than 4 KB have highest chances of recovery, and files with size 3–4 KB have least chances of recovery.		Wasim Ahmad Bhat;Mohamad Ahtisham Wani	2018	Digital Investigation	10.1016/j.diin.2018.09.001	forensic science;computer science;data mining;b-tree;file system;directory	OS	-13.451442538344612	55.611159302896745	172512
27ca89ca0167ab7fe60e843ae84cdc24fa7275e2	partitioning for scalable complex event processing on data streams		Many applications processing dynamic data require to filter, aggregate, join as well as to recognize event patterns in streams of data in an online fashion. However, data analysis and complex event processing (CEP) on high volume and/or high rate streams are challenging tasks. Typically, partitioning techniques are leveraged for achieving low latency and scalable processing. Unfortunately, sequence-based operations such as CEP operations as well as long-running continuous queries make partitioning much more difficult than for batch-oriented approaches.	complex event processing;scalability	Omran Saleh;Heiko Betz;Kai-Uwe Sattler	2014		10.1007/978-3-319-10518-5_15	streams;real-time computing;data stream;database;scalability;latency (engineering);computer science;data stream mining;complex event processing;dynamic data	DB	-18.578732025253547	54.93817724409798	174831
97b59734d2d2a30e3511c4a84e095b0eaeea8357	on the benefit of processor coallocation in multicluster grid systems	resource selection;minimization;communication intensive parallel applications processor coallocation multicluster grid systems processor scheduling resource selection phase;processor scheduling delay grid computing communications technology computational modeling performance analysis;parallel job scheduling;processor scheduling;multicluster;resource management;resource selection phase;data mining;grid;coallocation;scheduling;bandwidth;processor coallocation;multicluster grid systems;communication intensive parallel applications;processor scheduling grid computing;ethernet networks;grid computing;parallel applications;grid system;simulation environment;parallel job scheduling coallocation grid multicluster	In multicluster grid systems, parallel applications may benefit from processor coallocation, that is, the simultaneous allocation of processors in multiple clusters. Although coallocation allows the allocation of more processors than available in a single cluster, it may severely increase the execution time of applications due to the relatively slow wide-area communication. The aim of this paper is to investigate the benefit of coallocation in multicluster grid systems, despite this drawback. To this end, we have conducted experiments in a real multicluster grid environment, as well as in a simulated environment, and we evaluate the performance of coallocation for various applications that range from computation-intensive to communication-intensive and for various system load settings. In addition, we compare the performance of scheduling policies that are specifically designed for coallocation. We demonstrate that considering latency in the resource selection phase improves the performance of coallocation, especially for communication-intensive parallel applications.	central processing unit;computation;experiment;grid systems corporation;load (computing);run time (program lifecycle phase);scheduling (computing);virtual reality	Omer Ozan Sonmez;Hashim H. Mohamed;Dick H. J. Epema	2010	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2009.121	parallel computing;real-time computing;computer science;resource management;operating system;distributed computing;grid;scheduling;bandwidth;grid computing	HPC	-15.288798546491654	59.22349681982967	174938
a1ef29d76889a876d9ffd3204cf1f510ab310f1e	predicting free computing capacities on individual machines	desktop grids;measures;grid computing;prediction	The basic idea of grid computing is a better use of underutilized resources. Following this idea, desktop grids target ordinary workstations, which are very powerful today. However, due to the priority of the local users, it is impossible to exclusively reserve com puting time for grid jobs on these machines. Consequently, an already running grid job might be delayed or even canceled. A forecast of future available computing capacities could alleviate this problem. Such a prediction would be especially useful for the allocation of the most appro priate machines and for making stochastic assertions on the completion of submitted jobs. In this paper we discuss suitable approaches for predicting the availability of computer resources. We develop measures to finally make a comparison of the approaches, which is based on empirical data from available workstations.		Alek Opitz;Hartmut König	2009		10.1007/978-3-642-01671-4_15	simulation;measure;prediction;computer science;theoretical computer science;operating system;data mining;database;distributed computing;grid computing	Theory	-16.52969086593724	59.3095136011239	175316
7194b1aa06ac93c1aba370c0d65944a1a8fd66ed	clustered page-level mapping for flash memory-based storage devices	random access memory;memory management;performance evaluation;smart phones;loading;flash memories loading random access memory file systems smart phones performance evaluation memory management;page level mapping ftl clustered page clustered page level mapping flash memory based storage devices consumer devices smart phones smart tv tablet pc nand flash memory low power consumption software layer flash translation layer file systems garbage collection cost cold separation storage access patterns host system cold data flash memory block groups logical address regions ftl map loading overhead high cost monitoring overhead k associative version k cpm high block utilizations storage i o performance;embedded storage nand flash memory flash translation layer clustered page mapping;storage management flash memories;flash memories;file systems	Recent consumer devices such as smartphones, smart TVs and tablet PCs adopt NAND flash memory as storage device due to its advantages of small size, reliability, low power consumption, and high performance. The unique characteristics of NAND flash memory require an additional software layer, called flash translation layer (FTL), between traditional file systems and flash memory. In order to reduce the garbage collection cost, FTLs generally try to separate hot and cold data. Previous hot and cold separation techniques monitor the storage access patterns within storage device, or exploit file system hints from host system. This paper proposes a novel clustered page-level mapping, called CPM, which can separate hot and cold data efficiently by allocating different flash memory block groups to different logical address regions. CPM can reduce the FTL map loading overhead during garbage collection and it does not require any high-cost monitoring overhead or host hint. This paper also proposes a K-associative version of CPM, called K-CPM, which allows different logical address regions to share a physical block group in order to achieve high block utilizations. Experimental results show that CPM improves the storage I/O performance by about 54% compared with a previous page-level mapping FTL, and K-CPM further improves the performance by about 19.4% compared with CPM.	ftl: faster than light;flash file system;flash memory controller;garbage collection (computer science);input/output;locality of reference;overhead (computing);portable storage device;principle of locality;smart tv;smartphone;tablet computer	Hyukjoong Kim;Dongkun Shin	2015	IEEE Transactions on Consumer Electronics	10.1109/TCE.2015.7064110	flash file system;embedded system;parallel computing;computer hardware;computer science;operating system;flash memory emulator;computer data storage;computer memory;universal memory;sequential access memory;memory management	OS	-12.281310434285313	53.96832469330413	175729
32e49fc16b74d53b787625734322f4ae6d4a1f7a	global dynamic load-balancing for decentralised distributed simulation	distributed processing;resource allocation;decentralised distributed simulation;decentralised information collection;global dynamic load-balancing;load-balancing improvement;local dynamic load balancing;logical processes;partitioning mechanisms;re-balancing decision making;run-time analysis;simulation environment re-partitioning	Distributed simulations require partitioning mechanisms to operate, and the best partitioning algorithms try to load-balance the partitions. Dynamic load-balancing, i.e. re-partitioning simulation environments at run-time, becomes essential when the load in the partitions change. In decentralised distributed simulation the information needed to dynamically load-balance seems difficult to collect and to our knowledge, all solutions apply a local dynamic load balancing: partitions exchange load only with their neighbours (more loaded partitions to less loaded ones). This limits the effect of the load-balancing. In this paper, we present a global dynamic load-balancing of decentralised distributed simulations. Our algorithm collects information in a decentralised fashion and makes re-balancing decisions based on the load processed by every logical processes. While our algorithm has similar results to others in most cases, we show an improvement of the load-balancing up to 30% in some challenging scenarios against only 12.5% for a local dynamic load-balancing.	algorithm;load balancing (computing);simulation	Quentin Bragard;Anthony Ventresque;Liam Murphy	2014	Proceedings of the Winter Simulation Conference 2014		real-time computing;computer science;load balancing;theoretical computer science;distributed computing	HPC	-16.385443342307052	58.94152474030778	176065
f963a5bab702915567f7cc3d85dd40a77110362a	parallelizing multiple pipelines of one query in a main memory database cluster		"""To fully use the advanced resources of a main memory database cluster, we take independent parallelism into account to parallelize multiple pipelines of one query. However, scheduling resources to multiple pipelines is an intractable problem. Traditional static approaches to this problem may lead to a serious waste of resources and suboptimal execution order of pipelines, because it is hard to predict the actual data distribution and fluctuating workloads at compile time. In response, we propose a dynamic scheduling algorithm, List with Filling and Preemption (LFPS), based on two techniques. (1) Adaptive filling improves resource utilization by issuing more extra pipelines to adaptively fill idle resource """"holes"""" during execution. (2) Cost-based preemption strictly guarantees scheduling the pipelines on a critical path first at run time. We implement LFPS in our prototype database system. Under the workloads of TPC-H, experiments show our work improves the finish time of parallelizable pipelines from one query up to 2.3X than a static approach and 1.7X than a serialized execution."""	algorithm;automatic parallelization;compile time;compiler;computational complexity theory;computer data storage;critical path method;experiment;high-availability cluster;ibm tivoli storage productivity center;in-memory database;parallel computing;pipeline (computing);preemption (computing);program dependence graph;prototype;run time (program lifecycle phase);scheduling (computing)	Dalmau J Lancaster;Chuliang Weng;Li Wang;Aoying Zhou	2018	2018 IEEE 34th International Conference on Data Engineering (ICDE)	10.1109/ICDE.2018.00123	database;critical path method;scheduling (computing);dynamic priority scheduling;computer science;compile time;preemption;idle	DB	-16.598028452324808	55.684570524482766	177181
457d720426e458e9476dd374e3dade566776a254	a balanced parallel distributed sorting implemented with pgx.d		Sorting has been one of the most challenging studied problems in different scientific researches. Although many techniques and algorithms has been proposed on the theory of efficient parallel sorting implementation, however achieving the desired performance on the variety of architectures with the large number of processors is still the challenging issue. Maximizing the parallelism level in the application can be achieved by minimizing the overhead due to load imbalance and waiting time due to the memory latencies. In this paper, we present a distributed sorting implemented in PGX.D, a fast distributed graph processing system, which outperforms Spark distributed sorting by around 2x-3x by hiding communication latencies and minimizing unnecessary overheads. Furthermore, it shows that the proposed PGX.D sorting method handles duplicated data efficiently and always results in having load balance for different input data distribution types.	application programming interface;big data;binary search algorithm;central processing unit;graph (abstract data type);high- and low-level;load balancing (computing);overhead (computing);parallel computing;samplesort;sorting algorithm;time complexity	Zahra Khatami;Sungpack Hong;Jinsu Lee;Siegfried Depner;Hassan Chafi	2016	CoRR		parallel computing;sorting;computer science	HPC	-16.554768552292444	54.52034937550249	177481
5a8114a0b4fd56fcc159596a53895656fef1e484	alea 2: job scheduling simulator	higher simulation speed;average slowdown;job scheduling;common problem;various job scheduling technique;cluster scheduling simulator alea;event-based simulator;alea simulator;job scheduling simulator;average response time;common scheduling;visualization;grid;simulation;cluster;scheduling	This work describes the Grid and cluster scheduling simulator Alea 2 designed for study, testing and evaluation of various job scheduling techniques. This event-based simulator is able to deal with common problems related to the job scheduling like the heterogeneity of jobs, resources, and the dynamic runtime changes such as the arrivals of new jobs or the resource failures and restarts. The Alea 2 is based on the popular GridSim toolkit [31] and represents a major extension of the Alea simulator, developed in 2007 [16]. The extension covers both improved design, extended functionality as well as the improved scalability and the higher simulation speed. Finally, new visualization interface was introduced into the simulator. The main part of the simulator is a complex scheduler which incorporates several common scheduling algorithms working either on the queue or the schedule (plan) based principle. Additional data structures are used to maintain information about the resource status, the objective functions and for collection and visualization of the simulation results. Many typical objectives such as the machine usage, the average slowdown or the average response time are included. The paper concludes with an example of the Alea 2 execution using a real-life workload, discussing also the scalability of the simulator.	algorithm;data structure;information visualization;job scheduler;job stream;real life;response time (technology);scalability;schedule (computer science);scheduling (computing);simulation	Dalibor Klusácek;Hana Rudová	2010			real-time computing;computer architecture simulator;simulation;visualization;computer science;operating system;distributed computing;grid;java;scheduling;cluster	HPC	-16.44617065434599	60.12865370519917	178576
d2d27103b8a1646716cc531a829e5faf7127f53c	flash-optimized b+-tree	flash memory;indexing method;indexing;indexation;b tree;lazy update	With the rapid increasing capacity of flash memory, flash-aware indexing techniques are highly desirable for flash devices. The unique features of flash memory, such as the erase-before-write constraint and the asymmetric read/write cost, severely deteriorate the performance of the traditional B+-tree algorithm. In this paper, we propose an optimized indexing method, called lazy-update B+-tree, to overcome the limitations of flash memory. The basic idea is to defer the committing of update requests to the B+-tree by buffering them in a segment of main memory. They are later committed in groups so that the cost of each write operation can be amortized by a bunch of update requests. We identify a victim selection problem for the lazy-update B+-tree and develop two heuristic-based commit policies to address this problem. Simulation results show that the proposed lazy-update method, along with a well-designed commit policy, greatly improves the update performance of the traditional B+-tree while preserving the query efficiency.	amortized analysis;b+ tree;computer data storage;flash memory;heuristic;lazy evaluation;selection algorithm;simulation;windows update	Sai Tung On;Haibo Hu;Yu Li;Jianliang Xu	2010	Journal of Computer Science and Technology	10.1007/s11390-010-9341-1	b-tree;search engine indexing;real-time computing;computer science;theoretical computer science;database	DB	-12.830546782736322	54.45599829282869	179101
7fb5d07836f38186a4385c5e4a9816b8de2914a6	ccindex: a complemental clustering index on distributed ordered tables for multi-dimensional range queries	range query;distributed database;index;multi dimensional;theoretical analysis;clustering;indexation;high performance;range queries	Massive scale distributed database like Google’s BigTable and Yahoo!’s PNUTS can be modeled as Distributed Ordered Table, or DOT, which partitions data regions and supports range queries on key. Multidimensional range queries on DOTs are fundamental requirements; however, none of existing schemes work well while considering three critical issues: high performance, low space overhead, and high reliability. This paper introduces CCIndex scheme, short for Complemental Clustering Index, to solve all three issues. CCIndex creates several Complemental Clustering Index Tables for performance, leverages region-to-server information to estimate result size, and supports incremental data recovery. This paper builds a prototype on Apache HBase. Theoretical analysis and micro-benchmarks show that CCIndex consumes 5.3% ~ 29.3% more space, has the same reliability, and gains 11.4 times range queries throughput of secondary index scheme. Synthetic application benchmark shows that CCIndex query throughput is 1.9 ~ 2.1 times	apache hbase;benchmark (computing);cluster analysis;column (database);computer cluster;data recovery;dhrystone;distributed database;experience;failure;google bigtable;mathematical optimization;mysql cluster;overhead (computing);pnuts;prototype;query throughput;range query (data structures);requirement;scalability;server (computing);shared nothing architecture	Yongqiang Zou;Jia Liu;Shicai Wang;Li Zha;Zhiwei Xu	2010		10.1007/978-3-642-15672-4_22	range query;computer science;data mining;database;distributed database;information retrieval	DB	-14.818354997703343	54.48255278116013	179122
0fe22b538c4a378e5fc119bb86d0c66515d469e8	a load balancing approach based on a genetic machine learning algorithm	difference operator;adaptive genetic algorithm;resource allocation;load management machine learning algorithms workstations costs machine learning clustering algorithms genetic algorithms software packages software testing master slave;classifier system;cluster of workstations;operating systems load balancing nondedicated cluster configurations genetic machine learning algorithm classifier system workstation cluster master slave cow workstation network now;genetics;machine learning;network of workstation;software package;pattern classification;cost effectiveness;genetic algorithms;load balance;workstation clusters;learning artificial intelligence;parallel processing resource allocation workstation clusters genetic algorithms learning artificial intelligence pattern classification operating systems computers;operating systems computers;parallel processing	Cluster configurations are a cost effective scenarios which are becoming common options to enhance several classes of applications in many organizations. In this article, we present a research work to enhance the load balancing, on dedicated and non-dedicated cluster configurations, based on a genetic machine learning algorithm. Our approach is characterized by an on time assignment scheme using a classifier system. Classifier systems are learning machine algorithms, based on high adaptable genetic algorithms. We developed a software package which was designed to test the proposed scheme in a master-slave Cow (cluster of workstation) and Now (network of workstation) environment. Experimental results, from two different operating systems, indicate the enhanced capability of our load balancing approach to adapt in cluster configurations.	autostereogram;genetic algorithm;learning classifier system;load balancing (computing);machine learning;operating system;run time (program lifecycle phase);statistical classification;workstation	Mario A. R. Dantas;Alex R. Pinto	2005	19th International Symposium on High Performance Computing Systems and Applications (HPCS'05)	10.1109/HPCS.2005.8	parallel processing;parallel computing;genetic algorithm;cost-effectiveness analysis;resource allocation;computer science;load balancing;theoretical computer science;operating system;distributed computing	HPC	-14.791139775680488	59.29930186941136	179182
0d523fe0656e85b158686de58614faba031c1101	autonomic query parallelization using non-dedicated computers: an evaluation of adaptivity options	parallel programs	Writing parallel programs that can take advantage of non-dedicated processors is much more difficult than writing such programs for networks of dedicated processors. In a non-dedicated environment such programs must use autonomic techniques to respond to the unpredictable load fluctuations that prevail in the computational environment. In adaptive query processing (AQP), several techniques have been proposed for dynamically redistributing processor load assignments throughout a computation to take account of varying resource capabilities, but we know of no previous study that compares their performance. This paper presents a simulation-based evaluation of these autonomic parallelization techniques in a uniform environment and compares how well they improve the performance of the computation. Four published strategies are compared with a new algorithm that seeks to overcome some weaknesses identified in the existing approaches. In addition, we explore the use of techniques from online algorithms to provide a firm foundation for determining when to adapt in two of the existing algorithms. The evaluations identify situations in which each strategy may be used effectively and in which it should be avoided.	adaptive grammar;autonomic computing;best, worst and average case;central processing unit;computation;computer;control theory;database;dependability;electrical load;experiment;hash table;information retrieval;online algorithm;parallel computing;query plan;selectivity (electronic);simulation;smart environment;source data;state (computer science);tag (game);winsock	Norman W. Paton;Jorge Buenabad Chávez;Mengsong Chen;Vijayshankar Raman;Garret Swart;Inderpal Narang;Daniel M. Yellin;Alvaro A. A. Fernandes	2006	2006 IEEE International Conference on Autonomic Computing	10.1007/s00778-007-0090-x	real-time computing;computer science;theoretical computer science;database;distributed computing;programming language	DB	-17.411059214353013	59.85285676767471	179241
b8d36ceb336d70ffb8e629baeaaca7de888feaec	scheduling tasks with precedence constraints on hybrid multi-core machines	generic algorithm;approximation algorithms;high performance computing;processor scheduling;accelerators;two phase solving method;gpu;classical list algorithm;linear programming formulation;scheduling algorithms;precedence constraints;graphics processing units;hybrid parallel multicore machines;assignment phase;schedules;linear programming;hybrid platforms task scheduling precedence constraints hybrid parallel multicore machines cpu gpu accelerators high performance computing two phase solving method linear programming formulation classical list algorithm assignment phase generic algorithm;hybrid platforms;schedules approximation algorithms approximation methods graphics processing units scheduling algorithms algorithm design and analysis;processor scheduling graphics processing units linear programming parallel processing;approximation methods;task scheduling;cpu;algorithm design and analysis;parallel processing	In this work, we are interested in scheduling dependent tasks for hybrid parallel multi-core machines, composed of CPUs with additional accelerators (GPUs). The objective is to minimize the make span, which is a crucial problem for reaching the potential of new platforms in High Performance Computing. We provide an approximation algorithm with a performance guarantee of 6 to solve this problem. The algorithm is a two-phase solving method: a first phase based on rounding the solution provided by solving a linear programming formulation for the assignment of the tasks to the resources. A second phase uses a classical list algorithm to schedule the tasks according to the assignment phase. The proposed approach is the first generic algorithm with a performance guarantee for scheduling tasks with precedence constraints on hybrid platforms with CPUs and GPUs resources.	approximation algorithm;central processing unit;generic programming;graphics processing unit;linear programming formulation;multi-core processor;rounding;scheduling (computing);two-phase commit protocol	Safia Kedad-Sidhoum;Florence Monna;Denis Trystram	2015	2015 IEEE International Parallel and Distributed Processing Symposium Workshop	10.1109/IPDPSW.2015.119	parallel processing;algorithm design;parallel computing;real-time computing;genetic algorithm;schedule;computer science;linear programming;theoretical computer science;operating system;central processing unit;scheduling	HPC	-13.111935165352925	60.437415653555924	179406
a5228060067190a62861c3df7a73db4244c2b5a5	wabrm: a work-load aware balancing and resource management framework for swift on cloud	resource management;distributed storage system;work load balancing;swift	Fueled by increasing demand of big data processing, distributed storage systems have been more and more widely used by enterprises. However, in these systems, few storage nodes holding enormous amount of hotspot data could become bottlenecks. This stems from the fact that most typical distributed storage systems mainly provide data amount balancing mechanisms without considering the difference of access load between different storage nodes. To eliminate bottlenecks and tune the performance, there is a demand for such systems to employ a work-load aware balancing and resource management framework to optimize the performance and computation resource utilization.#R##N##R##N#In this paper, we propose WABRM, a load balancing and resource management framework for Work-load Aware Balancing and Resource Management in Swift, a typical distributed storage system. By designing such an optimization framework, it is possible to eliminate bottlenecks caused by hotspot data. Our experimental results show that the framework can achieve its goals.	swift (programming language)	Zhenhua Wang;Haopeng Chen;Yunmeng Ban	2013		10.1007/978-3-319-03859-9_39	real-time computing;simulation;computer science;resource management;swift;distributed computing;programming language	HPC	-17.840291386208605	57.60210868827394	179429
07a5be23a9e28f74ec4da0c029b8d19d8f1491e1	balanced task clustering in scientific workflows	task clustering;runtime delays clustering algorithms engines educational institutions optimization;pattern clustering;data locality;scientific workflow;runtime;engines;clustering algorithms;optimization;load balance;natural sciences computing;delays;task clustering scientific workflow data locality load balance;trace based simulation shows balanced task clustering scientific workflows runtime imbalance dependency imbalance task clustering task balancing methods;pattern clustering natural sciences computing	Scientific workflows can be composed of many fine computational granularity tasks. The runtime of these tasks may be shorter than the duration of system overheads, for example, when using multiple resources of a cloud infrastructure. Task clustering is a runtime optimization technique that merges multiple short tasks into a single job such that the scheduling overhead is reduced and the overall runtime performance is improved. However, existing task clustering strategies only provide a coarse-grained approach that relies on an over-simplified workflow model. In our work, we examine the reasons that cause Runtime Imbalance and Dependency Imbalance in task clustering. Next, we propose quantitative metrics to evaluate the severity of the two imbalance problems respectively. Furthermore, we propose a series of task balancing methods to address these imbalance problems. Finally, we analyze their relationship with the performance of these task balancing methods. A trace-based simulation shows our methods can significantly improve the runtime performance of two widely used workflows compared to the actual implementation of task clustering.	algorithm;balanced clustering;cloud computing;cluster analysis;computer cluster;data dependency;experiment;heart rate variability;load balancing (computing);mathematical optimization;overhead (computing);run time (program lifecycle phase);scheduling (computing);trace-based simulation	Weiwei Chen;Rafael Ferreira da Silva;Ewa Deelman;Rizos Sakellariou	2013	2013 IEEE 9th International Conference on e-Science	10.1109/eScience.2013.40	constrained clustering;parallel computing;real-time computing;computer science;distributed computing	HPC	-16.873088544170066	57.397510987192774	180179
06f31f1d1e024db8bd7f16528bb2c7c4c4d8358c	loop scheduling for heterogeneity	processor scheduling scheduling algorithm load management parallel machines parallel programming asynchronous transfer mode workstations dynamic programming computer science costs;optimisation;resource allocation;compiler algorithms;parallel programming;network contention loop scheduling heterogeneity compile time parallel programming compiler algorithms load balancing communication optimizations;loop scheduling;optimal scheduling;network contention;scheduling;load balancing;load balance;parallel programs;communication optimizations;parallel programming scheduling resource allocation optimisation;heterogeneity;heterogeneous network;compile time	In this paper, we study the problem of scheduling parallel loops at compile-time for a heterogeneous network of machines. We consider heterogeneity in three aspects of parallel programming: program, processor and network. A heterogeneous program has parallel loops with different amount of work in each iteration; heterogeneous processors have different speeds; and a heterogeneous network has different cost of communication between processors. We propose a simple yet comprehensive model for use in compiling for a network of processors, and develop compiler algorithms for generating optimal and sub-optimal schedules of loops for load balancing, communication optimizationsand network contention. Experiments show that a significant improvement of performance is achieved using our techniques.	algorithm;central processing unit;compile time;compiler;experiment;iteration;load balancing (computing);loop scheduling;parallel computing;scheduling (computing)	Michal Cierniak;Wei Li;Mohammed J. Zaki	1995		10.1109/HPDC.1995.518697	parallel computing;real-time computing;computer science;load balancing;operating system;distributed computing	HPC	-13.97423384626565	59.639287370439604	180404
654c28545606d470d6c9c380d9cfb98bb17ed71a	analysis and improvement of makespan and utilization for mapreduce	analytical models biological system modeling mathematical model aggregates conferences performance analysis simulation;analytical models;utilization mapreduce performance makespan;simulation;biological system modeling;aggregates;performance analysis;mathematical model;conferences	A MapReduce cluster is usually shared by multiple users or products, aiming at accelerating their own job. In contrast, the utilization of the cluster is the main concern for the system itself. MapReduce jobs are split into independent tasks during execution. However, in practice, the number of tasks per stage for each job and the system settings are often sub-optimal to support certain workload. As far as we know, few research focused on the impacts of influential factors such as task granularity, slot count and workload, and the performance analysis from the perspectives of each job and system. We construct models to describe the effects of these factors on the performance from both the per-job and system perspectives. Based on the understanding provided by analytical model, we discuss the optimal settings of job and system parameters, then propose a batch scheduling policy instead of FIFO, so as to improve the system utilization and reduce the average job processing time. The simulation results show that the average gap time is reduced by 10% and the mean job make span is improved by 15% when each job is preempted 6 times on average using the batch scheduling policy.	computer simulation;fifo (computing and electronics);job scheduler;makespan;mapreduce;multi-user;profiling (computer programming);scheduling (computing);sticky bit	Yin Li;Chuang Lin;Fengyuan Ren	2013	2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/HPCC.and.EUC.2013.69	parallel computing;real-time computing;simulation;computer science;job scheduler;mathematical model;distributed computing;job queue	HPC	-15.253926358141124	60.19127463769618	180703
2fafaf2679ab7c6a2b0c9428734d42a4daa361cf	evaluating dynamics and bottlenecks of memory collaboration in cluster systems	memory collaboration;distributed system;storage management pattern clustering software engineering;pattern clustering;end to end memory collaboration cluster systems highly integrated distributed systems memory hierarchy design disk swapping free remote memory peak load requirements remote memory static system configurations dynamic memory collaboration run time memory usage fluctuations autonomous collaborative memory system acms memory resources real world applications noncollaborative memory system;shared memory;system configuration;dis aggregated memory;storage management;remote memory;clustered architecture memory sharing memory collaboration dis aggregated memory remote memory kernel swapping infniband;software engineering;memory sharing;real world application;infniband;cluster system;memory systems;cost effectiveness;memory hierarchy;memory management servers collaboration random access memory protocols monitoring heuristic algorithms;kernel swapping;clustered architecture	With the fast development of highly-integrated distributed systems (cluster systems), designers face interesting memory hierarchy design choices while attempting to avoid the notorious disk swapping. Swapping to the free remote memory through Memory Collaboration has demonstrated its cost-effectiveness compared to over provisioning the cluster for peak load requirements. Recent memory collaboration studies propose several ways on accessing the under-utilized remote memory in static system configurations, without detailed exploration of the dynamic memory collaboration. Dynamic collaboration is an important aspect given the run-time memory usage fluctuations in clustered systems. Further, as the interest in memory collaboration grows, it is crucial to understand the existing performance bottlenecks, overheads, and potential optimization. In this paper we address these two issues. First, we propose an Autonomous Collaborative Memory System (ACMS) that manages memory resources dynamically at run time to optimize performance. We implement a prototype realizing the proposed ACMS, experiment with a wide range of real-world applications, and show up to 3x performance speedup compared to a non-collaborative memory system without perceivable performance impact on nodes that provide memory. Second, we analyze, in depth, the end-to-end memory collaboration overhead and pinpoint the corresponding bottlenecks.	application control management system;clustered file system;computer cluster;distributed computing;end-to-end principle;hot swapping;load profile;mathematical optimization;memory hierarchy;memory management;overhead (computing);paging;prototype;provisioning;requirement;run time (program lifecycle phase);speedup	Ahmad Samih;Ren Wang;Christian Maciocco;Tsung-Yuan Charlie Tai;Ronghui Duan;Jiangang Duan;Yan Solihin	2012	2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)	10.1109/CCGrid.2012.59	uniform memory access;distributed shared memory;shared memory;interleaved memory;parallel computing;real-time computing;cost-effectiveness analysis;distributed memory;computer science;physical address;operating system;static memory allocation;database;distributed computing;overlay;flat memory model;computing with memory;cache-only memory architecture;memory map;memory management	Arch	-17.12724062210334	54.995347410077095	180758
691fbaf86a00ed24728030ddf6edc549a8af2f22	cloudflow: a data-aware programming model for cloud workflow applications on modern hpc systems	programming model;concurrency;hpc;mapreduce;article;data aware	Traditional High-Performance Computing (HPC) based big-data applications are usually constrained by having to move large amount of data to compute facilities for real-time processing purpose. Modern HPC systems, represented by High-Throughput Computing (HTC) andMany-Task Computing (MTC) platforms, on the other hand, intend to achieve the long-held dream of moving compute to data instead. This kind of data-aware scheduling, typically represented by HadoopMapReduce, has been successfully implemented in its Map Phase, whereby eachMap Task is sent out to the compute node where the corresponding input data chunk is located. However, HadoopMapReduce limits itself to a one-map-to-one-reduce framework, leading to difficulties for handling complex logics, such as pipelines or workflows. Meanwhile, it lacks built-in support and optimizationwhen the input datasets are shared amongmultiple applications and/or jobs. The performance can be improved significantly when the knowledge of the shared and frequently accessed data is taken into scheduling decisions. To enhance the capability of managing workflow in modern HPC system, this paper presents CloudFlow, a HadoopMapReduce based programmingmodel for cloudworkflow applications. CloudFlow is built on top of MapReduce, which is proposed not only being data aware, but also shared-data aware. It identifies the most frequently shared data, from both task-level and job-level, replicates them to each compute node for data locality purposes. It also supports user-defined multiple Mapand Reduce functions, allowing users to orchestrate the required data-flow logic. Mathematically, we prove the correctness of the whole scheduling framework by performing theoretical analysis. Further more, experimental evaluation also shows that the execution runtime speedup exceeds 4X compared to traditional MapReduce implementation with a manageable time overhead. © 2014 Elsevier B.V. All rights reserved. ∗ Corresponding author. E-mail addresses: f_zhang@mit.edu (F. Zhang), qmalluhi@qu.edu.qa (Q.M. Malluhi), telsayed@qu.edu.qa (T. Elsayed), samee.khan@ndsu.edu (S.U. Khan), lik@newpaltz.edu (K. Li), albert.zomaya@sydney.edu.au (A.Y. Zomaya). http://dx.doi.org/10.1016/j.future.2014.10.028 0167-739X/© 2014 Elsevier B.V. All rights reserved.	amazon elastic compute cloud (ec2);big data;canonical account;cloud computing;correctness (computer science);dataflow;function-level programming;high-throughput computing;locality of reference;mapreduce;mathematical optimization;overhead (computing);pipeline (computing);programmer;programming model;real-time clock;scheduling (computing);shortest path problem;speedup;string searching algorithm;task computing;throughput	Fan Zhang;Qutaibah M. Malluhi;Tamer Elsayed;Samee Ullah Khan;Keqin Li;Albert Y. Zomaya	2015	Future Generation Comp. Syst.	10.1016/j.future.2014.10.028	supercomputer;parallel computing;real-time computing;concurrency;computer science;operating system;database;distributed computing;programming paradigm;programming language	HPC	-18.25857939659715	56.095154126687376	180916
c6f37c13eddff83bb628b154e6995c5eb406fa37	scheduling algorithms for dedicated nodes in alchemi grid	resource scheduling;dedicated resources scheduling computational grid;computational grid;grid applications;processor scheduling;grid software;supercomputing power;alchemi grid;search algorithm;net platform;arrays;scheduling algorithm;performance improvement;scheduling algorithms;scheduling;dedicated resources scheduling;bandwidth;scheduling dispatching grid computing;payloads;dedicated nodes;peer to peer computing;rabin karp string searching algorithm;grid computing;dispatching jobs;algorithm design and analysis;dispatching;scheduling algorithm grid computing computer networks computer architecture application software testing performance evaluation processor scheduling dispatching software algorithms;grid software scheduling algorithms dedicated nodes alchemi grid supercomputing power net platform rabin karp string searching algorithm dispatching jobs	Computational grids are useful tools for bringing supercomputing power to users by using idle resources in the network. In the following paper we give a short overview of architecture of the Alchemi grid developed on .Net platform. We created a grid application, which utilizes Rabin-Karp string searching algorithm to test Alchemi grid performances in situation when requests put diverse demands for computing resources to the different grid nodes. Scheduling and dispatching jobs to the computing resources is a critical activity of the grid software. We present a scheduling algorithm which showed performance improvements to the original algorithm shipped with Alchemi grid software.	bandwidth (signal processing);desktop computer;heuristic;interrupt;performance;rabin–karp algorithm;scheduling (computing);search algorithm;string searching algorithm;supercomputer	Zeljko Stanfel;Goran Martinovic;Zeljko Hocenski	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811676	grid file;parallel computing;real-time computing;computer science;distributed computing;scheduling;grid computing	HPC	-18.39744104869941	59.013680550730214	181358
6b39cad88961ea3dbd74e434ea8ee93a62cd1a81	cisp-growth: a contiguous item sequential pattern mining algorithm with application level io patterns	databases;contiguous item sequential pattern mining;sp;storage system;memory management;information loss;c miner;c miner sp cisp kcl kirchhoff s current law clospan;application level io pattern;layout;data mining;computers noise level databases distributed processing computational modeling data mining optimization scheduling algorithm prefetching pollution;arrays;pattern discovery;application io optimization;cisp growth;kcl kirchhoff s current law;cisp;application io optimization cisp growth contiguous item sequential pattern mining application level io pattern pattern discovery storage system c miner;clospan;optimization;sequential pattern mining;correlation;sequential pattern	The previous study of pattern discovery in storage systems focus on sequential pattern (SP) mining in lower level traces, but they don’t scale well to the application level. For patterns in application level are mostly composed of Contiguous Item Sequential Pattern (CISP) which are much simpler than SP, so it’s inefficient for the previous studies to mine CISP with clumsy SP mining algorithms. We propose a novel algorithm CISP-Growth which is more preferable for mining application level IO patterns. The CISP-Growth only scan the origin sequence in one-pass and make patterns grew among slices to avoid the inefficiency and information loss in C-Miner approach. The experiment result shows that the CISP-Growth outperforms C-Miner prominently in mining with real application IO traces and the simulation result also proves the effectiveness of CISP in application IO optimizations.	algorithm;computer data storage;data mining;experiment;mathematical optimization;online optimization;performance evaluation;sequential pattern mining;simulation;stepping level;tracing (software)	Jingliang Zhang;Junwei Zhang;Jian-Gang Zhang;Xiaoming Han;Lu Xu	2009	2009 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2009.23	sequential pattern mining;layout;computer science;data science;operating system;data mining;database;correlation;memory management	DB	-14.748300847940094	55.78765296334981	182197
370440d2a6f51c4d25c4141c7b8ac4665cad37cb	non-work-conserving effects in mapreduce: diffusion limit and criticality	diffusion approximation;processor sharing;heavy traffic;mapreduce;multi server queue	Sequentially arriving jobs share a MapReduce cluster, each desiring a fair allocation of computing resources to serve its associated map and reduce tasks. The model of such a system consists of a processor sharing queue for the MapTasks and a multi-server queue for the ReduceTasks. These two queues are dependent through a constraint that the input data of each ReduceTask are fetched from the intermediate data generated by the MapTasks belonging to the same job. A more generalized form of MapReduce queueing model can capture the essence of other distributed data processing systems that contain interdependent processor sharing queues and multi-server queues.  Through theoretical modeling and extensive experiments, we show that, this dependence, if not carefully dealt with, can cause non-work-conserving effects that negatively impact system performance and scalability. First, we characterize the heavy-traffic approximation. Depending on how tasks are scheduled, the number of jobs in the system can even exhibit jumps in diffusion limits, resulting in prolonged job execution times. This problem can be mitigated through carefully applying a tie-breaking rule for ReduceTasks, which as a theoretical finding has direct engineering implications. Second, we empirically validate a criticality phenomenon using experiments. MapReduce systems experience an undesirable performance degradation when they have reached certain critical points, another finding that offers fundamental guidance on managing MapReduce systems.	approximation;criticality matrix;distributed computing;elegant degradation;experiment;interdependence;job stream;mapreduce;queueing theory;scalability;server (computing)	Jian Tan;Yandong Wang;Weikuan Yu;Li Zhang	2014		10.1145/2591971.2592007	parallel computing;real-time computing;heavy traffic approximation;computer science;operating system;distributed computing;statistics;computer network	Metrics	-14.538357819550288	57.776911357934836	182210
80325d152fa093edfe4223ca1b8d1244d11eb470	large-scale incremental processing with mapreduce	big data processing;incremental processing;data deduplication;mapreduce;hadoop	An important property of today’s big data processing is that the same computation is often repeated on datasets evolving over time, such as web and social network data. While repeating full computation of the entire datasets is feasible with distributed computing frameworks such as Hadoop, it is obviously inefficient and wastes resources. In this paper, we present HadUP (Hadoop with Update Processing), a modified Hadoop architecture tailored to large-scale incremental processing with conventional MapReduce algorithms. Several approaches have been proposed to achieve a similar goal using tasklevel memoization. However, task-level memoization detects the change of datasets at a coarse-grained level, which often makes such approaches ineffective. Instead, HadUP detects and computes the change of datasets at a fine-grained level using a deduplication-based snapshot differential algorithm (D-SD) and update propagation. As a result, it provides high performance, especially in an environment where tasklevelmemoization has no benefit. HadUP requires only a small amount of extra programming cost because it can reuse the code for the map and reduce functions of Hadoop. Therefore, the development of HadUP applications is quite easy. © 2013 Elsevier B.V. All rights reserved.	algorithm;apache hadoop;big data;computation;data deduplication;distributed computing;mapreduce;memoization;snapshot (computer storage);social network;software propagation	DaeWoo Lee;Jin-Soo Kim;Seung Ryoul Maeng	2014	Future Generation Comp. Syst.	10.1016/j.future.2013.09.010	parallel computing;data deduplication;computer science;operating system;data mining;database;world wide web	HPC	-17.146347027539317	54.117003748002425	182218
6a112776ef5bca455cc08feceb45c3b2dc4aad05	a pattern adaptive nand flash memory storage structure	nand flash memory;flash memory;cache storage;random access memory;nand circuits cache storage disc storage flash memories;electronic mail;secondary storage;disc storage;resource manager;resource management;solid state disk;disk access pattern pattern adaptive nand flash memory storage structure flash memory based solid state disk pattern adaptive ssd structure write cache flash translation layer s ftl;indexing terms;solid state disk disk access pattern flash translation layer nand flash memory secondary storage;chip;simulation experiment;flash translation layer;games;nand circuits;flash memory resource management random access memory solids games electronic mail parallel processing;disk access pattern;parallel processing;flash memories;solids	To enhance performance of flash memory-based solid state disk (SSD), large logically chained blocks can be assembled by binding adjacent flash blocks across several flash memory chips. However, flash memory does not allow in-place overwriting and thus the operations that merge writes on these blocks suffer a visible decrease in performance. Furthermore, when small random writes are spread over the disk address space, performance tends to be degraded significantly. We thus present a technique to manage random writes efficiently to achieve stable SSD performance. In this paper, we propose a pattern adaptive SSD structure, which classifies access patterns as either random or sequential. The structure primarily consists of a write cache and a flash translation layer that separates groups of writes by access pattern (S-FTL). Separately managing the two types of write patterns enables greater parallelism and reduces the cost of large block management, thus enhancing the performance of the proposed SSD. Simulation experiments show that the proposed pattern adaptive structure can provide 39 percent decrease in extra flash block erase overhead on the average, and write performance can be improved by around 60 percent, compared with a basic FTL applied to existing parallel SSD structures.	address space;cpu cache;experiment;ftl: faster than light;flash file system;flash memory controller;in-place algorithm;offset binary;overhead (computing);overwriting (computer science);page cache;parallel computing;simulation;solid-state drive;write buffer	Seung-Ho Park;Jung-Wook Park;Shin-Dug Kim;Charles C. Weems	2012	IEEE Transactions on Computers	10.1109/TC.2010.212	chip;flash file system;auxiliary memory;games;parallel computing;real-time computing;index term;computer hardware;computer science;resource management;operating system;solid	OS	-11.946749809314216	53.77777905374231	182634
c0b9345ce0adb81bccf24ef5971f3a12f9c0ed13	analyzing the waiting energy consumption of nosql databases	databases;computers;nosql database;waiting energy consumption;loading;green cloud computing nosql database nosql db ec evaluation ec optimization waiting energy consumption energy waste computer idleness wec regularities measurement approaches test cases;energy consumption;energy consumption computers optimization loading databases central processing unit parallel processing;mapreduce nosql database waiting energy consumption;optimization;mapreduce;parallel processing;central processing unit;sql cloud computing energy consumption green computing optimisation	"""NoSQL database (NoSQL DB) covers the shortage of traditional database and has been widely used in recent years. Currently, researches on NoSQL DB mainly focus on performance issues, few of them are about energy consumption (EC) evaluation and optimization. Waiting Energy Consumption (WEC) is another critical reason causing energy waste besides computer idleness. Study the WEC regularities of NoSQL DB facilitates achieving real """"green computing"""". This paper first analyzes the model and measurement approaches of EC, then designs test cases to study the WEC regularities, finally proposes approaches of """"reducing WEC"""" for EC optimization. Plenty of experiments show that, despite that NoSQL DB is an application of """"green cloud computing"""", the ECs of selected NoSQL DBs are widely divergent, and some of them remain to be further optimized."""	algorithm;apache hbase;apache hive;central processing unit;cloud computing;crusader: no remorse;database;experiment;input/output;mapreduce;mathematical optimization;oracle nosql db;performance;programming model;scheduling (computing);shared nothing architecture;test case;wireless experimental centre	Tiantian Li;Ge Yu;Xuebing Liu;Jie Song	2014	2014 IEEE 12th International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2014.56	computer science;operating system;data mining;database	DB	-17.301424883718873	57.00884926775081	183092
0bc3a093b476137159e44537641ebdca69a5272b	using inter-file similarity to improve intra-file compression	storage management data compression source code software;electronic mail encoding dictionaries encyclopedias indexes measurement compression algorithms;intrafile references interfile similarity intrafile compression storage systems near atomic access differential compression shared common strings file collection compression decompression latency compression effectiveness lz style within file compressor source code e mails lz algorithm;lz factorization differential compression;lz factorization;differential compression	In storage systems with vast numbers of files, compression techniques should exploit of inter-file similarity, while allowing for near-atomic access to individual files. In differential compression, collections of files are compressed by identifying shared common strings. Therefore, some files are represented largely by references to strings in other files. In addition, a file in the collection can be (further) compressed by identifying common strings within the file itself. At the cost of decompression latency, but a possible gain in compression effectiveness, an LZ-style within-file compressor could resolve these references to other files. To quantify the compression gain, we experiment with a variety of file collections, from emails to source code, and test against multiple measures. If the LZ scheme honors the inter-file references, then there is only minimal improvement. If the LZ algorithm replaces inter-file references with intra-file references, then up to 3% compression improvement is witnessed for mildly similar files, and over 200% improvement for highly similar files.	algorithm;code coverage;computer file;data compression;data differencing;delta encoding;email;experiment;lz77 and lz78;python;register file;ruby;string (computer science);suffix array	Angelos Molfetas;Anthony Wirth;Justin Zobel	2014	2014 IEEE International Congress on Big Data	10.1109/BigData.Congress.2014.35	data compression;solid compression;computer hardware;computer science;theoretical computer science;database;lossless compression;volume	ML	-14.1041441907232	54.66428513252026	183103
142650f2f2132bf7de90fc031dd9f4825147b56b	an empirical study of ftl performance in conjunction with file system pursuing data integrity	non volatile ram;data integrity;flash translation layer;empirical study;metadata	Due to the detachability of Flash storage, which is a dominant portable storage, data integrity stored in Flash storages becomes an important issue. This study considers the performance of Flash Translation Layer (FTL) schemes embedded in Flash storages in conjunction with file system behavior that pursue high data integrity. To assure extreme data integrity, file systems synchronously write all file data to storage accompanying hot write references. In this study, we concentrate on the effect of hot write references on Flash storage, and we consider the effect of absorbing the hot write references via nonvolatile write cache on the performance of the FTL schemes in Flash storage. In so doing, we quantify the performance of typical FTL schemes for a realistic digital camera workload that contains hot write references through experiments on a real system environment. Results show that for the workload with hot write references FTL performance does not conform with previously reported studies. We also conclude that the impact of the underlying FTL schemes on the performance of Flash storage is dramatically reduced by absorbing the hot write references via nonvolatile write cache.	data integrity;ftl: faster than light	In Hwan Doh;Myoung Sub Shim;Eunsam Kim;Jongmoo Choi;Donghee Lee;Sam H. Noh	2010	IEICE Transactions		flash file system;parallel computing;computer hardware;computer science;operating system;data integrity;non-volatile random-access memory;empirical research;metadata	DB	-12.076965673100537	54.0219073337071	183314
e11f2cbea984530c67468e822dcf9599789ce300	scalable linked data stream processing via network-aware workload scheduling	linked data;graph partitioning;semantic flow processing;workload scheduling;complex event processing;stream processing	In order to cope with the ever-increasing data volume, distributed stream processing systems have been proposed. To ensure scalability most distributed systems partition the data and distribute the workload among multiple machines. This approach does, however, raise the question how the data and the workload should be partitioned and distributed. A uniform scheduling strategy—a uniform distribution of computation load among available machines—typically used by stream processing systems, disregards network-load as one of the major bottlenecks for throughput resulting in an immense load in terms of intermachine communication. In this paper we propose a graph-partitioning based approach for workload scheduling within stream processing systems. We implemented a distributed triple-stream processing engine on top of the Storm realtime computation framework and evaluate its communication behavior using two real-world datasets. We show that the application of graph partitioning algorithms can decrease inter-machine communication substantially (by 40% to 99%) whilst maintaining an even workload distribution, even using very limited data statistics. We also find that processing RDF data as single triples at a time rather than graph fragments (containing multiple triples), may decrease throughput indicating the usefulness of semantics.	algorithm;benchmark (computing);computation;critical graph;degree of parallelism;distributed computing;experiment;graph partition;interaction;linked data;load balancing (computing);microsoft outlook for mac;parallel computing;prototype;resource description framework;scalability;scheduling (computing);stream processing;throughput;while	Lorenz Fischer;Thomas Scharrenbach;Abraham Bernstein	2013			parallel computing;real-time computing;computer science;distributed computing	DB	-18.684936057360765	55.84537216563408	183449
648cdead96ab228392fb14afe15d15bcb463c888	efficient dds monitoring system for large amount of data		This paper proposes a system for efficient monitoring and error detection in a Data Distribution Service (DDS) environment in which a large number of messages are transmitted and received. For monitoring and error detection, messages are stored in the database, and message processing is handled in parallel in order to maintain the real-time characteristics of the DDS. To avoid compromising the real-time characteristics of DDS, the monitoring functions are grouped into three priorities according to their importance, and separate queues are operated according to priority to allocate tasks to parallel processors. The system will provide an environment that can perform in-depth analysis and monitoring while maintaining the real-time characteristics of DDS.	central processing unit;data distribution service;error detection and correction;parallel computing;real-time clock;real-time computing	Min-Young Son;Dong-Seong Kim;Joong-Hyuk Cha	2018	2018 14th IEEE International Workshop on Factory Communication Systems (WFCS)	10.1109/WFCS.2018.8402384	real-time computing;error detection and correction;task analysis;computer science;data distribution service;queue	Embedded	-17.700473072117973	56.577674254145734	184420
7157618159ccd4545d5a74a48925815a27b10b21	an integrated power consumption model for communication and transaction based applications	integrated power consumption model;epclb;distributed system;degradation;extended power consumption laxity based algorithm;servers power demand computational modeling biological system modeling degradation ecosystems;digital ecosystems;power consumption models;transaction processing client server systems power aware computing;energy efficient;computer model;biological system modeling;client server systems;computation resource;peer to peer system;distributed systems digital ecosystems energy efficient selection algorithm peer to peer system power consumption models green it;power aware computing;communication resource;servers;computational modeling;round robin;total power;ecosystems;energy efficient selection algorithm;electric power consumption;electric power;power consumption;distributed systems;transaction processing;green it;transaction based application integrated power consumption model electric power consumption digital ecosystems distributed systems computation resource communication resource epclb extended power consumption laxity based algorithm;power demand;transaction based application	In order to realize digital ecosystems, the total electric power consumption of computers and networks have to be reduced. In distributed systems, clients issue requests to servers and then servers send replies to clients. Here, application processes are composed of a pair of modules which mainly consume computation and communication resources, respectively. Based on the measurement of the power consumption of a server, we newly discuss a power consumption model of a server for applications. We propose an EPCLB (extended power consumption laxity-based) algorithm to select one of servers for applications so that the total power consumption of servers can be reduced. We evaluate the EPCLB algorithm in terms of execution time and power consumption compared with the traditional round-robin algorithm.	algorithm;computation;computer;distributed computing;ecosystem;round-robin scheduling;run time (program lifecycle phase);server (computing)	Tomoya Enokido;Ailixier Aikebaier;Makoto Takizawa	2011	2011 IEEE International Conference on Advanced Information Networking and Applications	10.1109/AINA.2011.82	computer simulation;green computing;real-time computing;ecosystem;simulation;electric power;degradation;transaction processing;computer science;operating system;database;distributed computing;efficient energy use;computational model;server;computer network;inter-process communication	Embedded	-18.02500405064069	59.21945532289761	184482
8003ddbae118e4e24242654f52a45592861996c3	proxy responses by fpga-based switch for mapreduce stragglers		In parallel processing applications, a few worker nodes called “stragglers”, which execute their tasks significantly slower than other tasks, increase the execution time of the job. In this paper, we propose a network switch based straggler handling system to mitigate the burden of the compute nodes. We also propose how to offload detecting stragglers and computing their results in the network switch with no additional communications between worker nodes. We introduce some approximate techniques for the proxy computation and response at the switch; thus our switch is called “ApproxSW.” As a result of a simulation experiment, the proposed approximation based on task similarity achieves the best accuracy in terms of quality of generated Map outputs. We also analyze how to suppress unnecessary proxy computation by the ApproxSW. We implement ApproxSW on NetFPGA-SUME board that has four 10Gbit Ethernet (10GbE) interfaces and a Virtex-7 FPGA. Experimental results shows that the ApproxSW functions do not degrade the original 10GbE switch performance. key words: FPGA, MapReduce, straggler	approximation algorithm;computation;field-programmable gate array;mapreduce;netfpga;network switch;parallel computing;run time (program lifecycle phase);sensor;simulation	Koya Mitsuzuka;Michihiro Koibuchi;Hideharu Amano;Hiroki Matsutani	2018	IEICE Transactions		field-programmable gate array;artificial intelligence;computer vision;parallel computing;computer science	HPC	-16.70600617948393	56.46183327107889	184644
2b17d1ba063b6b03e4300299a48f3d5a69f8c46c	dynamic data replication in lcg 2008	dynamic replication;automated replication;controlled replication;simple replication strategy;complex grid system;grid simulator optorsim;grid site;grid site policy;lhc computing grid;lawrence berkeley national laboratory;dynamic data replication	To provide performant access to data from high energy physics experiments such as the Large Hadron Collider (LHC), controlled replication of files among grid sites is required. Dynamic, automated replication in response to jobs may also be useful, and has been investigated using the grid simulator OptorSim. In this paper, results from simulation of the LHC Computing Grid in 2008, in a physics analysis scenario, are presented. These show, first, that dynamic replication does give improved job throughput; second, that for this complex grid system, simple replication strategies such as LRU and LFU are as effective as more advanced economic models; third, that grid site policies which allow maximum resource sharing are more effective; and lastly, that dynamic replication is particularly effective when data access patterns include some files being accessed more often than others, such as with a Zipf-like distribution.	data access;dynamic data;experiment;job stream;large hadron collider;least frequently used;replication (computing);simulation;throughput;worldwide lhc computing grid;zipf's law	Caitriana Nicholson;David G. Cameron;A. T. Doyle;A. Paul Millar;Kurt Stockinger	2008	Concurrency and Computation: Practice and Experience	10.1002/cpe.1314	parallel computing;dynamic data;large hadron collider;data management;computer science;economic model;theoretical computer science;operating system;data grid;database;distributed computing;world wide web;grid computing	HPC	-18.049676070103533	58.77507180463783	185170
eed67e7db3a77ba45ba52f098de49eb853e8ad15	durable and energy efficient in-memory frequent-pattern mining		It is a significant problem to efficiently identify the frequently occurring patterns in a given dataset, so as to unveil the trends hidden behind the dataset. This paper is motivated by the serious demands of a high-performance in-memory frequent-pattern mining strategy, with joint optimization over the mining performance and system durability. While the widely used frequent-pattern tree (FP-tree) serves as an efficient approach for frequent-pattern mining, its construction procedure often makes it unfriendly for nonvolatile memories (NVMs). In particular, the incremental construction of FP-tree could generate many unnecessary writes to the NVM and greatly degrade the energy efficiency, because NVM writes typically take more time and energy than reads. To overcome the drawbacks of FP-tree on NVMs, this paper proposes evergreen FP-tree (EvFP-tree), which includes a lazy counter and a minimum-bit-altered (MBA) encoding scheme to make FP-tree friendly for NVMs. The basic idea of the lazy counter is to greatly eliminate the redundant writes generated in FP-tree construction. On the other hand, the MBA encoding scheme is to complement existing wear-leveling techniques to evenly write each memory cell to extend the NVM lifetime. As verified by experiments, EvFP-tree greatly enhances the mining performance and system lifetime by 40.28% and 87.20% on average, respectively. And EvFP-tree reduces the energy consumption by 50.30% on average.	data mining;durability (database systems);experiment;flash memory;in-memory database;lazy evaluation;line code;mathematical optimization;memory cell (binary);non-volatile memory;online algorithm;online and offline;performance tuning;two-phase locking;wear leveling;whole earth 'lectronic link	Duo Liu;Yi Lin;Po-Chun Huang;Xiao Zhu;Liang Liang	2017	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2017.2681077	real-time computing;computer science;energy consumption;encoding (memory);efficient energy use;durability;non-volatile memory;memory cell	DB	-11.879455753304441	54.59819133746003	185273
34ceedd1e8e287e3ec350d32131f6895f732f85c	a data-driven adaptive model-identification based large-scale sensor management system: application to self powered neutron detectors	databases;foss clustering principal component analysis adaptive re clustering fault diagnosis parallel implementation;databases mathematical model data models computational modeling adaptation models covariance matrices inductors;computational modeling;covariance matrices;inductors;mathematical model;adaptation models;data models	In this paper we propose an adaptive approach to manage large number of correlated sensors. Our approach is able to extract information (models) from these sensors that is relevant for performing fault diagnosis of these sensors. Such a situation involving large number of correlated sensors is encountered in large core nuclear reactors, for example. Since fault diagnosis methods are computationally intensive, it is helpful to organize the sensors into groups such that strongly correlated sensors belong to the same group. However, the groups/clusters need to be reorganized depending on operating conditions. We propose an adaptive method that is scalable to a large number of sensors and can adapt to changing operating conditions. Also, within each cluster, it is often required to adaptively rebuild new models/relations for sensors inside that cluster. We use the k-means algorithm for obtaining clusters and Principal Component Analysis (PCA) for finding relations between the sensors within a cluster. We demonstrate that significant speedup is achieved by parallelizing the various aspects of the above computation. A key requirement in managing a large number of sensors is the data and processing management. We demonstrate and compare a serial and parallel implementation of this method using SQLite for database management, Python for numerical computations, the Pycluster module for clustering and the Python multiprocessing module for code parallelization. The method is demonstrated for the above nuclear reactor application: with 140 sensors and 14,000 measurements for each sensor. The method turns out to scale very easily to such a large number. The implementation codes of our approach have been made available online. The utilized packages all being open source (FOSS) helps in the use of these codes in various safety critical applications which typically require complete verification/ratification. The cost saved due to the FOSS aspect of our implementation is another advantage.	adaptive system;algorithm;central processing unit;cluster analysis;code;communications protocol;computation;database;k-means clustering;linear model;multiprocessing;numpy;numerical analysis;open-source software;parallel computing;principal component analysis;python;reactor (software);real-time clock;sqlite;scalability;semi-supervised learning;semiconductor industry;sensor;server (computing);situated;speedup;system identification	Nahit Pawar;Madhu N. Belur;Mani Bhushan;Akhilanand Pati Tiwari;M. G. Kelkar;M. Pramanik;Virendra Singh	2014	2014 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2014.6867471	real-time computing;computer science;theoretical computer science;data mining	Robotics	-16.944092455189505	57.172539961902906	185840
6ceb71831fb543b148188fb57be757ab726ec3bc	dynamic versus adaptive processor allocation policies for message passing parallel computers: an empirical comparison	multiprocessor systems;parallel computer;message passing	When a job arrives at a space-sharing multiprocessor system , a decision has to be made regarding the number and the speciic identities of the processors to be allocated to it. An adaptive policy may consider the state of the system at arrival time but it does not allow pre-emption of any of the running jobs. A dynamic partitioning policy may preempt one or more of the currently running jobs to accommodate the new arrival. In this paper performance of dynamic and adaptive policies is investigated experimentally on a message passing architecture (Intel Paragon). The workload model is based on matrix computation applications commonly found on large systems used for scientiic programming. Results are reported for single and multiclass cases. A sensitivity analysis with respect to workload speedup characteristics is presented. Our results show that if the preemption overheads are kept low, dynamic polices result in noticeable improvement in overall performance of the system.	central processing unit;computation;experiment;intel paragon;job stream;message passing;multiprocessing;numerical linear algebra;parallel computing;preemption (computing);speedup;time of arrival	Jitendra Padhye;Lawrence W. Dowdy	1996		10.1007/BFb0022296	computer architecture;parallel computing;message passing;computer science;distributed computing;message broker	HPC	-13.081651580522506	59.59066953365752	185888
11c7d538faa06f5cfde52dae687cdc25dd26c3f1	practical prefetching via data compression	lempel ziv;moving object;virtual memory;data compression;cache replacement;data structure	An important issue that affects response time performance in current OODB and hypertext systems is the I/O involved in moving objects from slow memory to cache. A promising way to tackle this problem is to use prefetching, in which we predict the user's next page requests and get those pages into cache in the background. Current databases perform limited prefetching using techniques derived from older virtual memory systems. A novel idea of using data compression techniques for prefetching was recently advocated in [KrV, ViK], in which prefetchers based on the Lempel-Ziv data compressor (the UNIX compress command) were shown theoretically to be optimal in the limit. In this paper we analyze the practical aspects of using data compression techniques for prefetching. We adapt three well-known data compressors to get three simple, deterministic, and universal prefetchers. We simulate our prefetchers on sequences of page accesses derived from the OO1 and OO7 benchmarks and from CAD applications, and demonstrate significant reductions in fault-rate. We examine the important issues of cache replacement, size of the data structure used by the prefetcher, and problems arising from bursts of “fast” page requests (that leave virtually no time between adjacent requests for prefetching and book keeping). We conclude that prediction for prefetching based on data compression techniques holds great promise.	cpu cache;computer-aided design;data compression;data structure;database;hypertext;input/output;lempel–ziv–welch;link prefetching;prefetcher;response time (technology);simulation;unix	Kenneth M. Curewitz;P. Krishnan;Jeffrey Scott Vitter	1993		10.1145/170035.170077	data compression;parallel computing;real-time computing;data structure;computer science;virtual memory;theoretical computer science;database;programming language	Arch	-12.673651274054334	55.35936033820327	186251
a6c0f186701a52a6619a9f4eb53612c1cb8ef32f	mapreduce framework energy adaptation via temperature awareness	energy awareness;heterogeneous cluster;mapreduce;power	MapReduce has become a popular framework for Big Data applications. While MapReduce has received much praise for its scalability and efficiency, it has not been thoroughly evaluated for power consumption. Our goal with this paper is to explore the possibility of scheduling in a power-efficient manner without the need for expensive power monitors on every node. We begin by considering that no cluster is truly homogeneous with respect to energy consumption. From there we develop a MapReduce framework that can evaluate the current status of each node and dynamically react to estimated power usage. In so doing, we shift work toward more energy efficient nodes which are currently consuming less power. Our work shows that given an ideal framework configuration, certain nodes may consume only 62.3 % of the dynamic power they consumed when the same framework was configured as it would be in a traditional MapReduce implementation.	best, worst and average case;big data;central processing unit;coefficient;hard disk drive;mapreduce;network switch;scalability;scheduling (computing)	Jessica Hartog;Elif Dede;Madhusudhan Govindaraju	2013	Cluster Computing	10.1007/s10586-013-0270-y	parallel computing;real-time computing;computer science;operating system;power;distributed computing	HPC	-18.74935984000399	57.91884420897225	186622
e8a145e87342b8496c1bf404566e7326f869d1fb	a study of data locality in yarn	scheduling data handling digital simulation input output programs parallel processing resource allocation;yarn;data locality;simulation;resource management;yarn containers delays bandwidth benchmark testing resource management scheduling;scheduling;bandwidth;hadoop;data locality i o operation resource allocation data intensive system hadoop ylocsim yarn locality simulator tool yarn delay scheduler behavior;scheduling hadoop data locality yarn simulation;benchmark testing;delays;containers	Co-locating the computation as close as possible to the data is an important consideration in the current data intensive systems. This is known as data locality problem. In this paper, we analyze the impact of data locality on YARN, which is the new version of Hadoop. We investigate YARN delay scheduler behavior with respect to data locality for a variety of workloads and configurations. We address in this paper three problems related to data locality. First, we study the trade-off between the data locality and the job completion time. Secondly, we observe that there is an imbalance of resource allocation when considering the data locality, which may under-utilize the cluster. Thirdly, we address the redundant I/O operations when different YARN containers request input data blocks on the same node. Additionally, we propose YARN Locality Simulator (YLocSim), a simulator tool that simulates the interactions between YARN components in a real cluster and reports the data locality percentages in real time. We validate YLocSim over a real cluster setup and use it in our study.	apache hadoop;computation;data-intensive computing;input/output;interaction;locality of reference;scheduling (computing)	Yehia Elshater;Patrick Martin;Dan Rope;Mike McRoberts;Craig Statchuk	2015	2015 IEEE International Congress on Big Data	10.1109/BigDataCongress.2015.33	parallel computing;real-time computing;computer science;distributed computing	HPC	-16.7000011145459	57.22990381346611	186831
eb36912148f05019c42358cdea7a04be0aac9abb	empowering stream processing through edge clouds		CHive is a new streaming analytics platform to run distributed SQL-style queries on edge clouds. However, CHive is currently tightly coupled to a specific stream processing system (SPS), Apache Storm. In this paper we address the decoupling of the CHive query planner and optimizer from the runtime environment, and also extend the latter to support pluggable runtimes through a common API. As runtimes, we currently support Apache Spark and Flink streaming. The fundamental contribution of this paper is to assess the cost of employing interstream parallelism in SPS. Experimental evaluation indicates that we can enable popular SPS to be distributed on edge clouds with stable overhead in terms of throughput	apache spark;apache storm;application programming interface;coupling (computer programming);ibm 1401 symbolic programming system;mathematical optimization;microsoft edge;overhead (computing);parallel computing;runtime system;sql;stream processing;streaming media;the chive;throughput	Sérgio Esteves;Nico Janssens;Bart Theeten;Luís Veiga	2017	SIGMOD Record	10.1145/3156655.3156661	computer science;database;throughput;stream processing;spark (mathematics);analytics;distributed computing	DB	-18.63794654449314	54.26873878242134	187342
4c0a4b80ee38352ff9f2c2507ffa5b8ea6f9b70a	skewed distributions in semi-stream joins: how much can caching help?	semi stream processing;join;front stage cache;performance optimization	Semi-stream join algorithms join a fast data stream with a disk-based relation. This is important, for example, in real-time data warehousing where a stream of transactions is joined with master data before loading it into a data warehouse. In many important scenarios, the stream input has a skewed distribution, which makes certain performance optimisations possible. We propose two such optimisation techniques: 1) a caching technique for frequently used master data, and 2) a technique for selective load shedding of stream tuples. The caching technique is fine-grained, operating on a tuple-level. Furthermore, it is generic in the sense that it can be applied to different semistream join algorithms to deal with high loads. We analyze it by combining it with various well-known semi-stream joins, and show that it improves the service rate by more than 40% for typical data with skewed distributions. The load shedding technique sheds the fraction of the stream that is most expensive to join. In contrast to existing approaches, the service rate improves under load shedding. We present experimental data showing significant improvements as compared to related approaches and perform a sensitivity analysis for various internal parameters.		M. Asif Naeem;Gillian Dobbie;Christof Lutteroth;Gerald Weber	2017	Inf. Syst.	10.1016/j.is.2016.09.007	parallel computing;real-time computing;computer science;operating system;data mining;database	DB	-19.097513714628064	55.233551151061924	187391
3f0e08adef959a1c56ae68f364b6638046e07b7d	sustainable gpu computing at scale	statistical multiplexing computer graphic equipment coprocessors parallel machines;mean time between failure;data parallel;semantic statistical multiplexing fault tolerant gpu computing data parallel processing tuple switching network;theoretical model;paper;automatic system level multiple gpu failure containment sustainable gpu computing general purpose gpu computing supercomputers mean time between failures energy consumption sustainable high performance computing application two tier semantic statistical multiplexing framework statistical multiplexing layer sustainability analysis automatic system level multiple cpu failure containment;fault tolerant;data parallel processing;energy efficient computing;computer graphic equipment;statistical multiplexing;coprocessors;cuda;computer experiment;energy consumption;tesla s1070;fault tolerance;fault tolerant gpu computing;high performance computer;nvidia;openmpi;tuple switching network;parallel machines;cublas;computer science;switching network;semantic statistical multiplexing;energy saving;multiplexing parallel processing graphics processing unit semantics scalability switches peer to peer computing	General purpose GPU (GPGPU) computing has produced the fastest running supercomputers in the world. For continued sustainable progress, GPU computing at scale also need to address two open issues: a) how increase applications mean time between failures (MTBF) as we increase supercomputer's component counts, and b) how to minimize unnecessary energy consumption. Since energy consumption is defined by the number of components used, we consider a sustainable high performance computing (HPC) application can allow better performance and reliability at the same time when adding computing or communication components. This paper reports a two-tier semantic statistical multiplexing framework for sustainable HPC at scale. The idea is to leverage the powers of statistic multiplexing to tame the nagging HPC scalability challenges. We include the theoretical model, sustainability analysis and computational experiments with automatic system level multiple CPU/GPU failure containment. Our results show that assuming three times slowdown of the statistical multiplexing layer, for an application using 1024 processors with 35\% checkpoint overhead, the two-tier framework will produce sustained time and energy savings for MTBF less than 6 hours. With 5% checkpoint overhead, 1.5 hour MTBF would be the break even point. These results suggest the practical feasibility for the proposed two-tier framework.	application checkpointing;central processing unit;computation;computational problem;emoticon;experiment;fastest;general-purpose computing on graphics processing units;graphics processing unit;ibm websphere extreme scale;internet protocol suite;maximal set;mean time between failures;mission critical;multiplexing;multitier architecture;network packet;online transaction processing;overhead (computing);packet switching;run time (program lifecycle phase);scalability;semantic network;service-oriented architecture;supercomputer;tame;theory;transaction processing system	Justin Y. Shi;Moussa Taifi;Abdallah Khreishah;Jie Wu	2011	2011 14th IEEE International Conference on Computational Science and Engineering	10.1109/CSE.2011.55	fault tolerance;parallel computing;real-time computing;computer science;theoretical computer science;operating system;data mining;database;programming language;computer network	HPC	-16.434790086939365	53.76301310204049	188029
04f0650ab41af3aad8c0476a9b06826c55356b99	reducing network congestion and synchronization overhead during aggregation of hierarchical data		Hierarchical data representations have been shown to be effective tools for coping with large-scale scientific data. Writing hierarchical data on supercomputers, however, is challenging as it often involves all-to-one communication during aggregation of low-resolution data which tends to span the entire network domain, resulting in several bottlenecks. We introduce the concept of indexing templates, which succinctly describe data organization and can be used to alter movement of data in beneficial ways. We present two techniques, domain partitioning and localized aggregation, that leverage indexing templates to alleviate congestion and synchronization overheads during data aggregation. We report experimental results that show significant I/O speedup using our proposed schemes on two of today's fastest supercomputers, Mira and Shaheen II, using the Uintah and S3D simulation frameworks.	computation;data aggregation;dataflow;discrete wavelet transform;experiment;fastest;fractal;hierarchical data format;hierarchical database model;input/output;internationalization and localization;network congestion;news aggregator;octree;simulation;speedup;supercomputer;throughput;vii	Sidharth Kumar;Duong Hoang;Steve Petruzza;John Edwards;Valerio Pascucci	2017	2017 IEEE 24th International Conference on High Performance Computing (HiPC)	10.1109/HiPC.2017.00034	parallel computing;distributed computing;data aggregator;parallel i/o;network congestion;speedup;search engine indexing;synchronization;computer science;hierarchical database model;overhead (business)	HPC	-17.708335057745614	54.10569637263087	188162
b9d5572f70e5b3c4287b17ba23c223e9515d3714	load-balanced and locality-aware scheduling for data-intensive workloads at extreme scales	work stealing;data aware scheduling;data intensive computing;key value stores;many task computing	Data driven programming models such as many-task computing (MTC) have been prevalent for running data-intensive scientific applications. MTC applies over-decomposition to enable distributed scheduling. To achieve extreme scalability, MTC proposes a fully distributed task scheduling architecture that employs as many schedulers as the compute nodes to make scheduling decisions. Achieving distributed load balancing and best exploiting data-locality are two important goals for the best performance of distributed scheduling of data-intensive applications. Our previous research proposed a data-aware work stealing technique to optimize both load balancing and data-locality by using both dedicated and shared task ready queues in each scheduler. Tasks were organized in queues based on the input data size and location. Distributed key-value store was applied to manage task metadata. We implemented the technique in MATRIX, a distributed MTC task execution framework. In this work, we devise an analytical sub-optimal upper bound of the proposed technique; compare MATRIX with other scheduling systems; and explore the scalability of the technique at extreme scales. Results show that the technique is not only scalable, but can achieve performance within 15% of the sub-optimal solution. Copyright © 2015 John Wiley & Sons, Ltd.	data-intensive computing;john d. wiley;key-value database;load balancing (computing);locality of reference;many-task computing;scalability;scheduling (computing);task computing;work stealing	Ke Wang;Kan Qiao;Iman Sadooghi;Xiaobing Zhou;Tonglin Li;Michael Lang;Ioan Raicu	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3617	fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;two-level scheduling;data-intensive computing;database;distributed computing;round-robin scheduling	HPC	-17.336662414088774	57.98587724375578	188193
2fad3cda00bb52eca884f6c5fc1fb9c86a8f4ad8	edelta: a word-enlarging based fast delta compression approach	会议论文	Delta compression, a promising data reduction approach capable of finding the small differences (i.e., delta) among very similar files and chunks, is widely used for optimizing replicate synchronization, backup/archival storage, cache compression, etc. However, delta compression is costly because of its time-consuming wordmatching operations for delta calculation. Our indepth examination suggests that there exists strong wordcontent locality for delta compression, which means that contiguous duplicate words appear in approximately the same order in their similar versions. This observation motivates us to propose Edelta, a fast delta compression approach based on a word-enlarging process that exploits word-content locality. Specifically, Edelta will first tentatively find a matched (duplicate) word, and then greedily stretch the matched word boundary to find a likely much longer (enlarged) duplicate word. Hence, Edelta effectively reduces a potentially large number of the traditional time-consuming word-matching operations to a single word-enlarging operation, which significantly accelerates the delta compression process. Our evaluation based on two case studies shows that Edelta achieves an encoding speedup of 3X∼10X over the state-of-the-art Ddelta, Xdelta, and Zdelta approaches without noticeably sacrificing the compression ratio.	backup;delta encoding;greedy algorithm;locality of reference;mathematical optimization;prototype;rewriting;self-replicating machine;speedup;throughput;wan optimization;xdelta	Wen Xia;Chunguang Li;Hong Jiang;Dan Feng;Yu Hua;Leihua Qin;Yucheng Zhang	2015			real-time computing;speech recognition;computer science;theoretical computer science;incremental encoding;delta encoding	OS	-13.694258338220406	54.903197674782156	188880
81a821a23d9d313a2265738ec51725aaee6394b5	graphmp: i/o-efficient big graph analytics on a single commodity machine		Recent studies showed that single-machine graph processing systems can be as highly competitive as cluster-based approaches on large-scale problems. While several out-of-core graph processing systems and computation models have been proposed, the high disk I/O overhead could significantly reduce performance in many practical cases. In this paper, we propose GraphMP to tackle big graph analytics on a single machine. GraphMP achieves low disk I/O overhead with three techniques. First, we design a vertex-centric sliding window (VSW) computation model to avoid reading and writing vertices on disk. Second, we propose a selective scheduling method to skip loading and processing unnecessary edge shards on disk. Third, we use a compressed edge cache mechanism to fully utilize the available memory of a machine to reduce the amount of disk accesses for edges. Extensive evaluations have shown that GraphMP could outperform existing single-machine out-of-core systems such as GraphChi, X-Stream and GridGraph by up to 51, and can be as highly competitive as distributed graph engines like Pregel+, PowerGraph and Chaos.	edge computing;graph (abstract data type);input/output;model of computation;out-of-core algorithm;overhead (computing);scheduling (computing);x-stream network	Peng Sun;Yonggang Wen;Ta Nguyen Binh Duong;Xiaokui Xiao	2018	CoRR		data mining;sliding window protocol;parallel computing;cache;computation;scheduling (computing);vertex (geometry);computer science;input/output;graph;analytics	OS	-16.269740106105722	54.363221337580214	189477
8a08ec7ba6b6aa7e94ebc19275d3c596a3ffc88e	improving search-based parallel job scheduler.	parallel job scheduling;goal orientation;sdsc;search algorithm;linux cluster;object model	To balance performance goals and allow administrators to declaratively specify high-level objective, we have proposed a goal-oriented scheduling framework by designing an objective model and a scheduling policy based on combinatorial search techniques to achieve the objective. In this work, we further evaluate our new policy on various real workloads including (1) ten monthly workloads that ran on a Linux cluster from NCSA; (2) ten monthly workloads that ran on the 100node IBM SP2 at the Swedish Royal Institute of Technology; and (3) twelve monthly workloads that ran on the 144-node IBM SP from the SDSC Blue Horizon. We also improve the execution efficiency of the search algorithm by (1) adding simple pruning; and (2) adjusting the heuristic during the search.	combinatorial search;heuristic;high- and low-level;job scheduler;job stream;linux;maxima and minima;national center for supercomputing applications;san diego supercomputer center;scheduling (computing);search algorithm;tree (data structure)	Sangsuree Vasupongayya	2006			parallel computing;real-time computing;object model;computer cluster;computer science;job scheduler;goal orientation;distributed computing;job queue;programming language;search algorithm	OS	-17.663689094948392	60.14651695430432	189982
81101de025f1d2efcb26e7b1838deab040699547	bps: a balanced partial stripe write scheme to improve the write performance of raid-6	reliability;performance evaluation;layout;arrays;aggregates;partial stripe writet;load balancing;mds codes;optimization;computer science;cloud computing;raid 6	Nowadays RAID is widely used due to its large capacity, high performance and high reliability. With the increasing requirement of reliability in storage systems and fast development of cloud computing, RAID-6, which can tolerate concurrent failures of any two disks, receives more attention than ever. However, the write performance of RAID-6 systems is a bottleneck to serve various applications. In the last two decades, many approaches are proposed to enhance the write performance of RAID-6, but they have several limitations, such as unbalanced I/O distribution and high I/O cost. To address this problem, in this paper, we propose a Balanced Partial Stripe (BPS) write scheme to improve the write performance of RAID-6 systems. The basic idea of BPS is reorganizing the distribution of write data blocks according to a global point of view on modified parities, and flushing these blocks to storage devices at once. Therefore, it can significantly reduce the total number of parity updates and balance the I/O workload. BPS has three main advantages: 1) BPS decreases the number of I/O operations and aggregate the fragmented I/Os, which improves the I/O performance, 2) BPS provides a balanced partial stripe write approach for RAID-6, 3) BPS can be applied with various erasure codes. To demonstrate the effectiveness of our scheme, we conduct simulations on DiskSim to evaluate different partial stripe write approaches. The results show that, compared to typical partial stripe write approaches, BPS reduces the average access time by up to 37.14%, and decreases the number of write operations by up to 26.24%.	access time;aggregate data;cloud computing;data striping;erasure code;input/output;magnetic stripe card;point of view (computer hardware company);simulation;standard raid levels;stripes;unbalanced circuit	Congjin Du;Chentao Wu;Jie Li;Minyi Guo;Xubin He	2015	2015 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2015.39	layout;parallel computing;real-time computing;cloud computing;computer science;load balancing;operating system;reliability;distributed computing	HPC	-11.997221074801784	54.375405950836715	190295
c78d83584f5888bb1bb83f8637a582c65ce8b4ee	exploiting the behavior of the failed job in high performance computing system		As demand for high-performance computing power is increasing, operation management technologies like check-pointing, failure-aware task scheduling, and system simulations are becoming more important for the stable operation of the system. To maintain and manage a stable system, a detailed analysis of failed tasks is necessary. For this, this paper intends to analyze the characteristics of failed jobs in high performance computing system. Our contributions can be viewed in three ways. Firstly, it offers detailed analysis results of failed jobs based on the job logs of a currently operating supercomputer. Secondly, it offers not only an overall statistical analysis result but also identifies the distribution of the failed job submission inter-arrival time. Thirdly, it analyzes the occurrence probability of the event using hazard rate.	job stream;rate–distortion theory;scheduling (computing);simulation;supercomputer;time of arrival	Ju-Won Park;Eunhye Kim	2018	2018 18th International Conference on Computational Science and Applications (ICCSA)	10.1109/ICCSA.2018.8439570	real-time computing;distributed computing;scheduling (computing);computer science;supercomputer;stochastic process	HPC	-16.31930858799357	60.36563208299271	190338
5cbf767f86224a553a57ff20e310f9252052c120	wscom: online task scheduling with data transfers	directed graphs;random graph;pattern clustering;work stealing;processor scheduling;resource allocation;online scheduling;software performance evaluation;software performance evaluation directed graphs pattern clustering processor scheduling resource allocation;load balancing;list scheduling;load balance;task scheduling;data transfers;makefile dag wscom online task scheduling data transfer dag task scheduling makefile execution task topology work stealing algorithm dag topology communicating task clustering graph decomposition random graphs;high performance;work stealing load balancing online scheduling data transfers;data transfer;graph decomposition;program processors clustering algorithms topology network topology algorithm design and analysis processor scheduling bandwidth	This paper considers the online problem of task scheduling with communication. All information on tasks and communication are not available in advance except the DAG of task topology. This situation is typically encountered when scheduling DAG of tasks corresponding to Make files executions. To tackle this problem, we introduce a new variation of the work-stealing algorithm: WSCOM. These algorithms take advantage of the knowledge of the DAG topology to cluster communicating tasks together and reduce the total number of communications. Several variants are designed to overlap communication or optimize the graph decomposition. Performance is evaluated by simulation and our algorithms are compared with off-line list-scheduling algorithms and classical work-stealing from the literature. Simulations are executed on both random graphs and a new trace archive of Make file DAG. These experiments validate the different design choices taken. In particular we show that WSCOM is able to achieve performance close to off-line algorithms in most cases and is even able to achieve better performance in the event of congestion due to less data transfer. Moreover WSCOM can achieve the same high performances as the classical work-stealing with up to ten times less bandwidth.	archive;directed acyclic graph;experiment;network congestion;online algorithm;online and offline;performance;random graph;schedule (project management);scheduling (computing);simulation;work stealing	Jean-Noël Quintin;Frédéric Wagner	2012	2012 12th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (ccgrid 2012)	10.1109/CCGrid.2012.21	parallel computing;real-time computing;computer science;load balancing;distributed computing	Arch	-13.867438806995917	60.41456440890918	190436
43d3aa78344b990be47a46c995b579d93f801f6b	a realistic model and an efficient heuristic for scheduling with heterogeneous processors	electronic mail;communication networks;processor scheduling;distributed processing;computer architecture;workstations;high performance computer;system testing;list scheduling;decision process;load balance;processor scheduling delay electronic mail computer architecture workstations system testing communication networks distributed processing;heterogeneous network	Scheduling computational tasks on processors is a key issue for high-performance computing. Although a large number of scheduling heuristics have been presented in the literature, most of them target only homogeneous resources. Moreover, these heuristics often rely on a model where the number of processors is bounded but where the communication capabilities of the target architecture are not restricted. In this paper, we deal with a more realistic model for heterogeneous networks of workstations, where each processor can send and/or receive at most one message at any given time-step. First, we state a complexity result that shows that the model is at least as difficult as the standard one. Then, we show how to modify classical list scheduling techniques to cope with the new model. Next we introduce a new scheduling heuristic which incorporates load-balancing criteria into the decision process of scheduling and mapping ready tasks. Experimental results conducted using six classical testbeds (LAPLACE, LU, STENCIL, FORK-JOIN, DOOLITTLE, and LDMt) show very promising results.	central processing unit;complexity;fork (software development);heuristic (computer science);list scheduling;load balancing (computing);scheduling (computing);stencil buffer;supercomputer;testbed;workstation	Olivier Beaumont;Vincent Boudet;Yves Robert	2002		10.1109/IPDPS.2002.1015663	fair-share scheduling;fixed-priority pre-emptive scheduling;open-shop scheduling;parallel computing;real-time computing;earliest deadline first scheduling;heterogeneous network;workstation;gang scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;load balancing;operating system;two-level scheduling;deadline-monotonic scheduling;distributed computing;scheduling;least slack time scheduling;lottery scheduling;round-robin scheduling;system testing;multiprocessor scheduling;computer network	HPC	-14.297729019336982	59.86112996817949	190631
c1f9f665dedd4029c5a44e28a6996550419a2085	sp-cache: load-balanced, redundancy-free cluster caching with selective partition		Data-intensive clusters increasingly employ inmemory solutions to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hot spots, which significantly degrade the benefits of in-memory caching. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory overhead due to cache redundancy or incur nontrivial encoding/decoding complexity. In this paper, we propose an effective approach to achieve load balancing without cache redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on their popularity and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for a hot file—too few partitions are incapable of mitigating hot spots, while too many are susceptible to stragglers. We implemented SP-Cache in Alluxio, a popular in-memory distributed storage for data-intensive clusters. EC2 deployment and trace-driven simulations show that, compared to the state-ofthe-art solution called EC-Cache [1], SP-Cache reduces the file access latency by up to 40% in both the mean and the tail, using 40% less memory.	algorithm;cpu cache;cache (computing);clustered file system;code;data-intensive computing;high memory;in-memory database;input/output;load balancing (computing);memory footprint;overhead (computing);simulation;software deployment;tame	Yinghao Yu;R. S. Huang;Wei Wang;Jun Zhang;Khaled Ben Letaief	2018			computer science;redundancy (engineering);memory management;parallel computing;encoding (memory);cache;load management;distributed data store;distributed computing;load balancing (computing);server	OS	-15.571426324963602	53.9426307753719	190943
fe1db1c7424fadeeca024d7a7d50255d10dd8086	performance optimization of communication subsystem in scale-out distributed storage		Scale-out distributed storage systems have recently gained high attentions with the emergence of big data and cloud computing technologies. However, these storage systems sometimes suffer from performance degradation, especially when the communication subsystem is not fully optimized. The problem becomes worse as the network bandwidth and its corresponding traffic increase. In this paper, we first conduct an extensive analysis of communication subsystem in Ceph, an object-based scale-out distributed storage system. Ceph uses asynchronous messenger framework for inter-component communication in the storage cluster. Then, we propose three major optimizations to improve the performance of Ceph messenger. These include i) deploying load balancing algorithm among worker threads based on the amount of workloads, ii) assigning multiple worker threads (we call dual worker) per single connection to maximize the overlapping activity among threads, and iii) using multiple connections between storage servers to maximize bandwidth usage, and thus reduce replication overhead. The experimental results show that the optimized Ceph messenger outperforms the original messenger implementation up to 40% in random writes with 4K messages. Moreover, Ceph with optimized communication subsystem shows up to 13% performance improvement as compared to original Ceph.	4k resolution;algorithm;big data;cloud computing;clustered file system;computer data storage;distributed database;elegant degradation;emergence;inter-process communication;load balancing (computing);object-based language;overhead (computing);program optimization;scalability	Uiseok Song;Bodon Jeong;Sungyong Park;Kwonyong Lee	2017	2017 IEEE 2nd International Workshops on Foundations and Applications of Self* Systems (FAS*W)	10.1109/FAS-W.2017.157	scalability;real-time computing;big data;cloud computing;thread (computing);asynchronous communication;distributed data store;computer science;server;load balancing (computing)	HPC	-15.738817391905554	53.779182033992264	192160
7c97cd14356bb58eae5c27ad538d3e3305dc9c5f	an o(p + log p) algorithm of discrete fgdls	piecewise constant techniques;iteration bounds o p log p algorithm discrete fgdls feedback guided dynamic loop scheduling parallel loop sequential outer loop balanced workload scheduling;processor scheduling;resource allocation;program control structures;loop scheduling;computational complexity;piecewise constant techniques processor scheduling program control structures resource allocation parallel algorithms computational complexity;scheduling algorithm processor scheduling dynamic scheduling computer science feedback loop educational institutions parallel processing performance loss integral equations computer numerical control;parallel algorithms	Feedback guided dynamic loop scheduling (FGDLS) is a recent dynamic method (Bull, 1998) that aims to schedule a parallel loop within a sequential outer loop. The method uses the feedback from the execution times of the current parallel loop to guide the scheduling of the next parallel loop. In this paper we propose an O(p + log p) algorithm for the FGDLS method in the discrete case. This approach applies the balanced workload scheduling (Tabirca et al., 2002) to the piecewise constant workloads to obtain the iteration bounds.	liu hui's π algorithm	Tatiana Tabirca;Sabin Tabirca;Len Freeman;Laurence Tianruo Yang	2003		10.1109/ICPPW.2003.1240367	fair-share scheduling;fixed-priority pre-emptive scheduling;mathematical optimization;loop inversion;parallel computing;real-time computing;loop fission;flow shop scheduling;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;two-level scheduling;distributed computing;parallel algorithm;computational complexity theory;round-robin scheduling;i/o scheduling;trace scheduling	Theory	-12.335819925014425	60.44140827704178	193226
4181f72896ef389cac0f22cee23643fdc99ef437	datanet: a data distribution-aware method for sub-dataset analysis on distributed file systems	data structures data handling;motion pictures;algorithm design and analysis motion pictures distributed databases clustering algorithms business file systems facebook;datanet distribution aware algorithm bloomfilter technique hashmap elasticmap elastic storage structure workload balanced computing distribution aware computing content clustering hadoop file system distributed file system subdataset analysis data distribution aware method;business;facebook;distributed databases;clustering algorithms;algorithm design and analysis;file systems	In this paper, we study the problem of sub-dataset analysis over distributed file systems, e.g, the Hadoop file system. Our experiments show that the sub-datasets' distribution over HDFS blocks can often cause the corresponding analysis to suffer from a seriously imbalanced parallel execution. This is because the locality of individual sub-datasets is hidden by the Hadoop file system and the content clustering of sub-datasets results in some computational nodes carrying out much more workload than others. We conduct a comprehensive analysis on how the imbalanced computing patterns occur and their sensitivity to the size of a cluster. We then propose a novel method to optimize sub-dataset analysis over distributed storage systems referred to as DataNet. DataNet aims to achieve distribution-aware and workload-balanced computing and consists of the following three parts. Firstly, we propose an efficient algorithm with linear complexity to obtain the meta-data of sub-dataset distributions. Secondly, we design an elastic storage structure called ElasticMap based on the HashMap and BloomFilter techniques to store the meta-data. Thirdly, we employ a distribution-aware algorithm for sub-dataset applications to achieve a workload-balance in parallel-execution. Our proposed method can benefit different sub-dataset analyses with various computational requirements. Experiments are conducted on PRObEs Marmot 128-node cluster testbed and the results show the performance benefits of DataNet.	algorithm;apache hadoop;bloom filter;cluster analysis;clustered file system;experiment;hash table;locality of reference;requirement;testbed	Jun Wang;Jiangling Yin;Jian Zhou;Xuhong Zhang;Ruijun Wang	2016	2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)	10.1109/IPDPS.2016.33	self-certifying file system;algorithm design;parallel computing;computer science;theoretical computer science;operating system;database;distributed computing;cluster analysis;distributed database;algorithm;computer network	HPC	-16.491713872240265	55.707888568259825	193244
8764a654ed5220d875f9cb7e438a73844d8a534a	distributed storage optimization for small data with high density in internet of vehicles	automobiles;data mining;connectors;distributed databases;vehicle dynamics;cloud computing	In the Internet of Vehicles, small data with high density collected by vehicle sensors is periodically uploaded to the cloud database. When the amount of online vehicles is large, the cloud server will be challenged by the soaring small datum. Firstly, in order to realize the goal of parallel processing between storage manager and calculation manager and the load balancing of the nodes, the thesis divides the uploaded memory datum into analysis channel and storage channel according to the traffic type. In the storage node, the caching technology that supports dynamic adjustment is introduced, and it can guarantee the stability of the system based on the storage pressure by dynamic adjusting caching force. In the last part of the thesis, system architecture with good expansibility is designed to support the storage manager and calculation manager increasing their storage/calculation nodes.	cache (computing);cloud database;geodetic datum;internet;load balancing (computing);parallel computing;sensor;server (computing);systems architecture;virtual private server	Hongbo Zhang;Huibing Zhang;Xiaoli Hu	2016	2016 IEEE First International Conference on Data Science in Cyberspace (DSC)	10.1109/DSC.2016.107	embedded system;real-time computing;converged storage;engineering;database	DB	-18.2634503638508	56.75114016798336	193724
7f1b9b82a781b10976265c7fcae46bb8ecd9d155	skipping-oriented partitioning for columnar layouts		As data volumes continue to grow, modern database systems increasingly rely on data skipping mechanisms to improve performance by avoiding access to irrelevant data. Recent work [39] proposed a fine-grained partitioning scheme that was shown to improve the opportunities for data skipping in row-oriented systems. Modern analytics and big data systems increasingly adopt columnar storage schemes, and in such systems, a row-based approach misses important opportunities for further improving data skipping. The flexibility of column-oriented organizations, however, comes with the additional cost of tuple reconstruction. In this paper, we develop Generalized Skipping-Oriented Partitioning (GSOP), a novel hybrid data skipping framework that takes into account these rowbased and column-based tradeoffs. In contrast to previous columnoriented physical design work, GSOP considers the tradeoffs between horizontal data skipping and vertical partitioning jointly. Our experiments using two public benchmarks and a real-world workload show that GSOP can significantly reduce the amount of data scanned and improve end-to-end query response times over the state-of-theart techniques.	big data;column-oriented dbms;data system;database;end-to-end encryption;experiment;physical design (electronics);relevance	Liwen Sun;Michael J. Franklin;Jiannan Wang;Eugene Wu	2016	PVLDB	10.14778/3025111.3025123	data mining;tuple;workload;database;big data;physical design;computer science;analytics	DB	-14.271582678886475	54.31165379496145	194632
28b9804c3fac26b5e61dc570a175a590b3162193	towards a systematic approach to the dynamic adaptation of structured parallel computations using model predictive control	adaptiveness;model predictive control;structured parallel programming;performance modeling	Adaptiveness is an essential feature for distributed parallel applications executed on dynamic environments like Grids and Clouds. Being adaptive means that parallel components can change their configuration at run-time (by modifying their parallelism degree or switching to a different parallel variant) to face irregular workload or to react to uncontrollable changes of the execution platform. A critical problem consists in the definition of adaptation strategies able to select optimal reconfigurations (minimizing operating costs and reconfiguration overhead) and achieve the stability of control decisions (avoiding unnecessary reconfigurations). This paper presents an approach to apply Model Predictive Control (a form of optimal control studied in Control Theory) to adaptive parallel computations expressed according to the Structured Parallel Programming methodology. We show that predictive control is amenable to achieve stability and optimality by relying on the predictability of structured parallelism patterns and the possibility to express analytical cost models of their QoS metrics. The approach has been exemplified on two case-studies, providing a first assessment of its potential and feasibility.	algorithmic efficiency;algorithmic skeleton;branch and bound;central processing unit;cloud computing;computation;control theory;evolutionary algorithm;grid computing;heuristic (computer science);high- and low-level;high-level programming language;mathematical optimization;optimal control;optimization problem;overhead (computing);parallel computing;sampling (signal processing);software development process;stencil buffer	Gabriele Mencagli;Marco Vanneschi	2014	Cluster Computing	10.1007/s10586-014-0346-3	parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;model predictive control	HPC	-13.795890851209842	58.254652009750615	195142
6c891ec8fb91f02969708500a4d434ec68923217	improving multi-dimensional query processing with data migration in distributed cache infrastructure	query processing;data migration policy multidimensional query processing distributed cache infrastructure distributed query processing systems cached objects large scalable distributed systems large scale distributed caching system scheduling policy cache hit ratio system load balance system load imbalance multiple query scheduling policy leverage cached objects distributed query processing framework distributed scalable caching system autonomic cached data migrations query scheduling policy;servers query processing load management distributed databases throughput dynamic scheduling;servers;load management;distributed databases;scheduling cache storage query processing resource allocation;dynamic scheduling;throughput	In distributed query processing systems where caching infrastructure is distributed and scales with the number of servers, it is becoming more important to orchestrate and leverage a large number of cached objects in distributed caching systems seamlessly as the present trend is to build large scalable distributed systems by connecting small heterogeneous machines. With a large scale distributed caching system, a scheduling policy must consider both cache hit ratio and system load balance to optimize multiple queries. A scheduling policy that considers system load but not cache hit ratio often fails to reuse cached data by not assigning a query to the sever that has data objects the query needs. On the contrary, a scheduling policy that considers cache hit ratio but not system load balance may suffer from system load imbalance. To maximize the overall system throughput and to reduce query response time, a multiple query scheduling policy must balance system load and also leverage cached objects. In this paper, we present a distributed query processing framework that exhibits high cache hit ratio while achieving good system load balance. In order to seamlessly manage our distributed scalable caching system, our framework performs autonomic cached data migrations to improve cache hit ratio. Our experiments show that our proposed query scheduling policy and data migration policy significantly improve system throughput by achieving high cache hit ratio while avoiding system load imbalance.	autonomic computing;cpu cache;cache (computing);database;distributed cache;distributed computing;experiment;heterogeneous computing;hit (internet);load (computing);load balancing (computing);query optimization;response time (technology);round-robin scheduling;scalability;scheduling (computing);throughput	Youngmoon Eom;Jinwoong Kim;Deukyeon Hwang;Jaewon Kwak;Minho Shin;Beomseok Nam	2014	2014 21st International Conference on High Performance Computing (HiPC)	10.1109/HiPC.2014.7116906	query optimization;throughput;parallel computing;real-time computing;dynamic priority scheduling;cache;computer science;operating system;database;distributed computing;distributed database;server	HPC	-18.045755709591635	57.79569143380839	195276
7e69fdd5b833e536fa443c3b89f0ec7ec8055fd1	heuristic optimization of speedup and benefit/cost for parallel database scans on shared-memory multiprocessors	storage allocation;resource limitation;database system;resource allocation;heuristic method;shared memory multi processor;parallel programming;parallel database system;multi user;benefit cost ratio;parallel databases;memory allocation heuristic optimization benefit cost ratio parallel database scans shared memory multiprocessors parallel dbms asynchronous disk prefetching processor parallelism scan operations heuristic methods processor allocation database scan operations speedup optimization data production rate data consumption rate resource time product resource consumption intelligent resource management parallel multi user database systems dynamic optimization;shared memory systems;cost optimization;heuristic optimization;cost function resource management database systems prefetching parallel processing optimization methods production computational intelligence deductive databases runtime;distributed databases;storage allocation parallel programming parallel algorithms distributed databases shared memory systems resource allocation;dynamic optimization;shared memory multiprocessor;parallel algorithms	Previous work on parallel database systems has paid little attention to the interaction of asynchronous disk prefetching and processor parallelism. This paper investigates this issue for scan operations on shared–memory multiprocessors. Two heuristic methods are developed for the allocation of processors and memory to optimize either the speedup or the benefit/cost ratio of database scan operations. The speedup optimization balances the data production rate of the disks and the data consumption rate of the processors, aiming at optimal speedup while ensuring that resources are not allocated unnecessarily. The benefit/cost optimization considers explicitly the resource consumption of a scan operation and aims to allocate processors and memory so that the ratio of the speedup attained to the operation’s resource–time product is maximized. Such an awareness of resource consumption is crucial for intelligent resource management in parallel multi–user database systems, for example, to ensure adequate resource limits for operations that exhibit only small marginal gains in speedup. Both developed heuristics are computationally low–cost and thus suitable for dynamic optimization at runtime.	approximation algorithm;cpu cache;central processing unit;dynamic programming;emoticon;experiment;heuristic (computer science);input/output;marginal model;mathematical optimization;moira burke;multiprocessing;parallel computing;parallel database;query plan;resource management (computing);run time (program lifecycle phase);shared memory;simulation;speedup;throughput	Michael Rys;Gerhard Weikum	1994		10.1109/IPPS.1994.288200	parallel computing;real-time computing;speedup;computer science;distributed computing	DB	-13.84688977889208	58.58584107589245	195439
3cc4399467728edfa127f067b6cb05a4cded1eaf	efficient technique for overcoming data migration in dynamic disk arrays	storage allocation;eficacia sistema;storage system;processor scheduling;architecture memoire;storage management;performance systeme;data migration;system performance;organizacion memoria;computer architecture;architecture ordinateur;memory architecture;organisation memoire;arquitectura ordenador;memory organization;ordonnancement processeur;allocation memoire;asignacion memoria;disk array	We introduce a new concept for storage management on multi-disk systems, called dynamic redundancy. It associates the redundancy level which characterizes the storage organization to sets of data and not to storage areas while minimizing data migration for reconfiguration. The result is a high quality multi-disk storage system, capable to support various storage organizations at once. It provides transformations between them with a minimum of (if any) data migration.	disk array;logical disk manager	Soraya Zertal;Claude Timsit	1999		10.1007/978-3-540-46642-0_1	embedded system;parallel computing;real-time computing;data migration;converged storage;disk array;computer science;operating system;computer performance;memory organisation	DB	-14.558427807882833	55.693614905831375	195891
0f3d6e6ae4c5e156b7a2db3ea104c87de0dc06b2	dynamic task migration in home-based software dsm systems	computers;computation subtask;resource utilization;degradation;benchmark applications;electronic mail;jiajia software dsm system;dynamic task migration;application software;resource allocation;program control structures;resource management;software systems access protocols computers content addressable storage resource management degradation context aware services application software random variables electronic mail;software systems;home based software dsm systems;random variables;metacomputing environments;natural science foundation of china;program code;program control structures distributed shared memory systems resource allocation distributed programming;task allocation schemes;dynamic loop level task migration scheme;distributed programming;distributed shared memory systems;access protocols;task allocation schemes dynamic task migration home based software dsm systems resource utilization metacomputing environments remote communication distributed shared memory systems computation subtask program code data subtask dynamic loop level task migration scheme jiajia software dsm system task migration scheme benchmark applications;content addressable storage;remote communication;task migration scheme;context aware services;data subtask	As more and more software DSM systems with their unique APIs surface, it becomes imperative for the industry to come up with a standardized API to facilitate users in using different types of DSM systems. A multithreaded software DSM, Orion, has been developed to provide POSIX-thread ( pthread) like API which avoids creating another unique set of API and helps in porting pthread programs to a distributed environment. Orion implements home-based consistency model, which is a recent development in the DSM field that has open up many areas for further research and development. In this paper, we also present 2 adaptive schemes for home-based DSM systems: home migration and dynamic adaptation between write-invalidation and write-update protocols. The two fully automatic schemes aim to involve minimal user intervention and yet deliver good performances with some speedups ranging from 2% to 79% observed in some 8 benchmarks tested.	application programming interface;consistency model;imperative programming;posix threads;performance;thread (computing)	Weisong Shi;Weiwu Hu;Zhimin Tang;M. Rasit Eskicioglu	1999		10.1109/HPDC.1999.805317	random variable;in situ resource utilization;application software;parallel computing;real-time computing;degradation;resource allocation;computer science;resource management;operating system;distributed computing;management;software system	HPC	-19.05304658778013	57.61709521068603	196177
5a5fc6c1c79a8535431e004374a5faad7ef0e0df	a distributed algorithm for the replica placement problem	servers approximation algorithms approximation methods algorithm design and analysis resource management equations distributed algorithms;distributed algorithms;cache storage;replication;approximate algorithm;distributed algorithms approximation theory cache storage computational complexity data handling;approximation algorithms;approximation method;approximation algorithm;resource manager;resource management;approximation theory;servers;computational complexity;distributed replication group;approximation methods;data handling;approximation algorithm replication distributed replication group distributed algorithm;distributed algorithm;algorithm design;computational complexity distributed algorithm replica placement problem data objects replication data caching network bandwidth reduction object replication distributed replication group distributed approximation algorithm;algorithm design and analysis	Caching and replication of popular data objects contribute significantly to the reduction of the network bandwidth usage and the overall access time to data. Our focus is to improve the efficiency of object replication within a given distributed replication group. Such a group consists of servers that dedicate certain amount of memory for replicating objects requested by their clients. The content replication problem we are solving is defined as follows: Given the request rates for the objects and the server capacities, find the replica allocation that minimizes the access time over all servers and objects. We design a distributed approximation algorithm that solves this problem and prove that it provides a 2-approximation solution. We also show that the communication and computational complexity of the algorithm is polynomial with respect to the number of servers, the number of objects, and the sum of the capacities of all servers. Finally, we perform simulation experiments to investigate the performance of our algorithm. The experiments show that our algorithm outperforms the best existing distributed algorithm that solves the replica placement problem.	access time;approximation algorithm;centralized computing;communication complexity;computational complexity theory;distributed algorithm;experiment;multi-master replication;overhead (computing);polynomial;replication (computing);server (computing);simulation;time complexity	Sharrukh Zaman;Daniel Grosu	2011	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2011.27	algorithm design;distributed algorithm;parallel computing;computer science;resource management;theoretical computer science;database;distributed computing;approximation algorithm;algorithm	DB	-15.554903051340853	57.765483913304685	198095
0153664ba0f4d37155357c9c76b82bbc786de3ab	an evaluation of distributed concurrency control		Increasing transaction volumes have led to a resurgence of interest in distributed transaction processing. In particular, partitioning data across several servers can improve throughput by allowing servers to process transactions in parallel. But executing transactions across servers limits the scalability and performance of these systems. In this paper, we quantify the effects of distribution on concurrency control protocols in a distributed environment. We evaluate six classic and modern protocols in an in-memory distributed database evaluation framework called Deneva, providing an apples-to-apples comparison between each. Our results expose severe limitations of distributed transaction processing engines. Moreover, in our analysis, we identify several protocol-specific scalability bottlenecks. We conclude that to achieve truly scalable operation, distributed concurrency control solutions must seek a tighter coupling with either novel network hardware (in the local area) or applications (via data modeling and semantically-aware execution), or both.	data modeling;distributed concurrency control;distributed database;distributed transaction;in-memory database;independence day: resurgence;networking hardware;scalability;throughput;transaction processing	Rachael Harding;Dana Van Aken;Andrew Pavlo;Michael Stonebraker	2017	PVLDB	10.14778/3055540.3055548	optimistic concurrency control;real-time computing;transaction processing;distributed transaction;computer science;concurrency control;database;distributed computing;online transaction processing;non-lock concurrency control;serializability;acid;distributed concurrency control	DB	-18.199299409173303	53.47942408594284	198236
bae8730ee6079ee478fe588f1274885665f79f27	caco: an efficient cauchy coding approach for cloud storage systems	encoding redundancy schedules reed solomon codes cloud computing optimal scheduling;reed solomon codes;cauchy matrix;redundancy;optimal scheduling;fault tolerance;schedules;xor scheduling cloud storage fault tolerance reed solomon codes cauchy matrix;encoding;xor scheduling;cloud storage;cloud computing	"""Users of cloud storage usually assign different redundancy configurations (i.e., <inline-formula><tex-math>$(k,m,w)$ </tex-math><alternatives><inline-graphic xlink:type=""""simple"""" xlink:href=""""zhang-ieq1-2428701.gif""""/></alternatives></inline-formula>) of erasure codes, depending on the desired balance between performance and fault tolerance. Our study finds that with very low probability, one coding scheme chosen by rules of thumb, for a given redundancy configuration, performs best. In this paper, we propose CaCo, an efficient Cauchy coding approach for data storage in the cloud. First, CaCo uses Cauchy matrix heuristics to produce a matrix set. Second, for each matrix in this set, CaCo uses XOR schedule heuristics to generate a series of schedules. Finally, CaCo selects the shortest one from all the produced schedules. In such a way, CaCo has the ability to identify an optimal coding scheme, within the capability of the current state of the art, for an arbitrary given redundancy configuration. By leverage of CaCo's nature of ease to parallelize, we boost significantly the performance of the selection process with abundant computational resources in the cloud. We implement CaCo in the Hadoop distributed file system and evaluate its performance by comparing with “Hadoop-EC” developed by Microsoft research. Our experimental results indicate that CaCo can obtain an optimal coding scheme within acceptable time. Furthermore, CaCo outperforms Hadoop-EC by 26.68-40.18 percent in the encoding time and by 38.4-52.83 percent in the decoding time simultaneously."""	apache hadoop;cloud computing;cloud storage;clustered file system;computation;computational resource;computer data storage;erasure code;exclusive or;fault tolerance;heuristic (computer science);microsoft research;schedule (computer science);xlink	Guangyan Zhang;Guiyong Wu;Shupeng Wang;Jiwu Shu;Weimin Zheng;Keqin Li	2016	IEEE Transactions on Computers	10.1109/TC.2015.2428701	embedded system;fault tolerance;parallel computing;real-time computing;cauchy matrix;cloud computing;telecommunications;schedule;computer science;theoretical computer science;operating system;redundancy;algorithm;encoding	HPC	-15.099274736319753	55.34984233115014	198611
175b968ac2ad252bafa8dcbf487aa85318b056d9	amc: an adaptive multi-level cache algorithm in hybrid storage systems	multi level cache;technology;computer science software engineering;adaptive algorithm;solid state drive;science technology;hybrid storage;computer science;computer science theory methods	Hybrid storage systems that consist of flash-based solid state drives (SSDs) and traditional disks are now widely used. In hybrid storage systems, there exists a two-level cache hierarchy that regard dynamic random access memory (DRAM) as the first level cache and SSD as the second level cache for disk storage. However, this two-level cache hierarchy typically uses independent cache replacement policies for each level, which makes cache resource management inefficient and reduces system performance. In this paper, we propose a novel adaptive multi-level cache (AMC) replacement algorithm in hybrid storage systems. The AMC algorithm adaptively adjusts cache blocks between DRAM and SSD cache levels using an integrated solution. AMC uses combined selective promote and demote operations to dynamically determine the level in which the blocks are to be cached. In this manner, the AMC algorithm achieves multi-level cache exclusiveness and makes cache resource management more efficient. By using real-life storage traces, our evaluation shows the proposed algorithm improves hybrid multi-level cache performance and also increases the SSD lifetime compared with traditional multi-level cache replacement algorithms. Copyright © 2015 John Wiley & Sons, Ltd.	advanced mezzanine card;cpu cache;cache (computing);disk storage;dynamic random-access memory;john d. wiley;page replacement algorithm;random access;real life;solid-state drive;tracing (software)	Yuxia Cheng;Wenzhi Chen;Zonghui Wang;Xinjie Yu;Yang Xiang	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3530	bus sniffing;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;database;distributed computing;adaptive replacement cache;smart cache;cache algorithms;cache pollution;mesif protocol;technology	HPC	-12.231190592223781	54.43159369375047	199208
1ff7c9527fafffbd8d8ee0fef4199e1b4e2c21ed	characterizing the dynamic behavior of workload execution in svm systems	dynamic behavior;performance loss;svm system;parallel workload characteristic;svm protocol;workload execution;false sharing;sharing unit size;efficient svm consistency protocol;page size;overall system performance;synchronization resource;synchronisation;resource allocation;system performance;protocols	The overhead associated with software management of shared virtual memory (SVM) systems can seriously impact overall system performance. One way to remedy this situation is to design more efficient SVM consistency protocols. In this paper we study a number of parallel workload characteristics that can negatively impact the performance of SVM systems. We attempt to quantify the sources of performance loss in some parallel workloads. Our goal is to better understand these characteristics, enabling us to develop SVM protocols that can adjust to dynamics in workload behavior. This paper has three main contributions: i) we measure the contention for synchronization resources, showing how applications exhibit distinct phases during their execution, ii) we quantify the relationship between page size and fragmentation/false sharing while varying the sharing unit size, and iii) we study the synergies between the contention for synchronization resources and fragmentation/false sharing, providing hints for developing improved protocols.	false sharing;fragmentation (computing);ip fragmentation;overhead (computing);page (computer memory);software project management;synergy	Salvador Petit;Julio Sahuquillo;Ana Pont;David R. Kaeli	2004	16th Symposium on Computer Architecture and High Performance Computing	10.1109/CAHPC.2004.12	distributed shared memory;communications protocol;synchronization;parallel computing;real-time computing;resource allocation;computer science;operating system;distributed computing;computer performance	Arch	-16.633322728937152	57.05132937799282	199394
05d254bfb0c78a237dc23ee5c8647fbcaf357ada	distributed scheduling for many-cores using cooperative game theory	dynamic programming;game theory;multi agent systems many core distributed scheduling;processor scheduling;centralized optimal scheduler distributed scheduling many cores cooperative game theory processing cores multithreaded tasks parallel processing task processing requirements partial core reallocation scheduling problem dynamic programming based scheduler;dynamic scheduling throughput optimal scheduling processor scheduling benchmark testing game theory dynamic programming;distributed scheduling;multi agent systems;processor scheduling dynamic programming game theory multiprocessing systems parallel processing;optimal scheduling;many core;benchmark testing;dynamic scheduling;throughput	Many-cores are envisaged to include hundreds of processing cores etched on to a single die and will execute tens of multi-threaded tasks in parallel to exploit their massive parallel processing potential. A task can be sped up by assigning it to more than one core. Moreover, processing requirements of tasks are in a constant state of flux and some of the cores assigned to a task entering a low processing requirement phase can be transferred to a task entering high requirement phase, maximizing overall performance of the system.  This scheduling problem of partial core reallocations can be solved optimally in polynomial time using a dynamic programming based scheduler. Dynamic programming is an inherently centralized algorithm that uses only one of the available cores for scheduling-related computations and hence is not scalable. In this work, we introduce a distributed scheduler that disburses all scheduling-related computations throughout the many-core allowing it to scale up. We prove that our proposed scheduler is optimal and hence converges to the same solution as the centralized optimal scheduler. Our simulations show that the proposed distributed scheduler can result in 1000x reduction in per-core processing overhead in comparison to the centralized scheduler and hence is more suited for scheduling on many-cores.	algorithm;centralized computing;computation;dynamic programming;game theory;manycore processor;overhead (computing);parallel computing;requirement;scalability;scheduling (computing);simulation;thread (computing);time complexity	Anuj Pathania;Vanchinathan Venkataramani;Muhammad Shafique;Tulika Mitra;Jörg Henkel	2016	2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2897937.2898009	fair-share scheduling;fixed-priority pre-emptive scheduling;game theory;benchmark;throughput;parallel computing;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;dynamic programming;two-level scheduling;deadline-monotonic scheduling;distributed computing;scheduling;lottery scheduling;round-robin scheduling;scheduling	EDA	-12.934316017063951	60.366258938010645	199942
