id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
1793e7ed2e397456393a8492f716111e2751591f	civl: the concurrency intermediate verification language	libraries;verification;concurrent computing;standards;government;program transformation;parallel programming;cuda;concurrency;symbolic execution;model checking;message systems;graphics processing units;openmp;pthreads;mpi;intermediate representation	There are many ways to express parallel programs: message-passing libraries (MPI) and multithreading/GPU language extensions such as OpenMP, Pthreads, and CUDA, are but a few. This multitude creates a serious challenge for developers of software verification tools: it takes enormous effort to develop such tools, but each development effort typically targets one small part of the concurrency landscape, with little sharing of techniques and code among efforts.  To address this problem, we present CIVL: the Concurrency Intermediate Verification Language. CIVL provides a general concurrency model capable of representing programs in a variety of concurrency dialects, including those listed above. The CIVL framework currently includes front-ends for the four dialects, and a back-end verifier which uses model checking and symbolic execution to check a number of properties, including the absence of deadlocks, race conditions, assertion violations, illegal pointer dereferences and arithmetic, memory leaks, divisions by zero, and out-of-bound array indexing; it can also check that two programs are functionally equivalent.	cuda;concurrency (computer science);deadlock;graphics processing unit;library (computing);memory leak;message passing;model checking;multithreading (computer architecture);openmp;posix threads;pointer (computer programming);race condition;software verification;symbolic execution;thread (computing)	Stephen F. Siegel;Manchun Zheng;Ziqing Luo;Timothy K. Zirkel;Andre V. Marianiello;John G. Edenhofner;Matthew B. Dwyer;Michael S. Rogers	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807635	model checking;parallel computing;verification;concurrent computing;concurrency;computer science;theoretical computer science;operating system;distributed computing;non-lock concurrency control;programming language;government	HPC	-17.346797485848985	32.28803795672688	155126
f0801fe4233bb373d4b1dd31468d118bed918b07	symbolic synthesis of masking fault-tolerant distributed programs	formal methods;program transformation;distributed programs;program synthesis;symbolic algorithms;fault tolerance	We focus on automated addition of masking fault-tolerance to existing fault-intolerant distributed programs. Intuitively, a program is masking fault-tolerant, if it satisfies its safety and liveness specifications in the absence and presence of faults. Masking fault-tolerance is highly desirable in distributed programs, as the structure of such programs are fairly complex and they are often subject to various types of faults. However, the problem of synthesizing masking fault-tolerant distributed programs from their fault-intolerant version is NP-complete in the size of the program’s state space, setting the practicality of the synthesis problem in doubt. In this paper, we show that in spite of the high worst-case complexity, synthesizing moderate-sized masking distributed programs is feasible in practice. In particular, we present and implement a BDD-based synthesis heuristic for adding masking fault-tolerance to existing fault-intolerant distributed programs automatically. Our experiments validate the efficiency and effectiveness of our algorithm in the sense that synthesis is possible in reasonable amount of time and memory. We also identify several bottlenecks in synthesis of distributed programs depending upon the structure of the program at hand. We conclude that unlike verification, in program synthesis, the most challenging barrier is not the state explosion problem by itself, but the time complexity of the decision procedures.	algorithm;best, worst and average case;bottleneck (software);experiment;fault tolerance;heuristic;liveness;np-completeness;program synthesis;state space;time complexity;unsharp masking;worst-case complexity	Borzoo Bonakdarpour;Sandeep S. Kulkarni;Fuad Abujarad	2011	Distributed Computing	10.1007/s00446-011-0139-3	fault tolerance;real-time computing;formal methods;computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-13.844723928059416	28.71238471968932	155245
85926edb13d7434db18ae9cb6c8135b497644b03	an approach for extracting a small unsatisfiable core	unsatisfiable core;resolution;sat	The thesis addresses the problem of finding a small unsatisfiable core of an unsatisfiable CNF formula. The proposed algorithm, CoreTrimmer , iterates over each internal node d in the resolution graph that ‘consumes’ a large number of clauses M (i.e. a large number of original clauses are present in the unsat core, whose sole purpose is proving d) and attempts to prove them without the M clauses. If this is possible, it transforms the resolution graph into a new graph that does not have the M clauses at its core. CoreTrimmer can be integrated into a fixpoint framework similarly to Malik and Zhang’s fix-point algorithm (run till fix). We call this option trim till fix. Experimental evaluation on a large number of industrial CNF unsatisfiable formulas shows that trim till fix doubles, on average, the number of reduced clauses in comparison to run till fix. It is also better when used as a component in a bigger system that enforces short timeouts.	algorithm;conjunctive normal form;fixed point (mathematics);iteration;timeout (computing);tree (data structure)	Roman Gershman;Maya Koifman;Ofer Strichman	2008	Formal Methods in System Design	10.1007/s10703-008-0051-z	resolution;computer science;algorithm	Logic	-14.169286237978271	26.076760308820255	155530
7fe69506dd336f302af807b2d2705f3a17f45919	atomicity refinement and trace reduction theorems	coarse grained;reduction method	Assertional methods tend to be useable for abstract, coarse-grained versions of concurrent algorithms, but quickly become intractable for more realistic , ner-grained implementations. Various trace-reduction methods have been proposed to transfer properties of coarse-grained versions to ner-grained versions. We show that a more direct approach, involving the explicit construction of an (inductive) invariant for the ner-grained version, is theoretically more powerful, and also more appropriate for computer-aided veriication.	algorithm;atomicity (database systems);usability	E. Pascal Gribomont	1996		10.1007/3-540-61474-5_79	computer science;theoretical computer science	Logic	-17.543952873320272	29.621376554313198	156443
0a01997c30f745525677e312284f52a42e078e93	uppdmc: a distributed model checker for fragments of the mu-calculus	flxed point;fixed point;model checking;distributed models;workstation cluster	We present UppDMC, a distributed model-checking tool. It is tailored for checking finite-state systems and μ-calculus specifications with at most one alternation of minimal and maximal fixed-point operators. This fragment is also known as L2μ. Recently, efficient game-based algorithms for this logic have been outlined. We describe the implementation of these algorithms within UppDMC and study their performance on practical examples. Running UppDMC on a simple workstation cluster, we were able to check liveness properties of the largest examples given in the VLTS Benchmark Suite, for which no answers were previously known.	algorithm;benchmark (computing);linear temporal logic;liveness;maximal set;modal μ-calculus;model checking;real life;state space;workbench;workstation	Fredrik Holmén;Martin Leucker;Marcus Lindström	2005	Electr. Notes Theor. Comput. Sci.	10.1016/j.entcs.2004.10.021	model checking;real-time computing;computer science;theoretical computer science;distributed computing;fixed point;programming language	Logic	-13.236600427029254	25.94588517487013	156772
581fd6f8cc53d6b2e5bac23ac59bacc0be6f2ed6	automated verification of functional correctness of race-free gpu programs	program verification;gpgpu;smt;symbolic execution	We study an automated verification method for functional correctness of parallel programs running on graphics processing units (GPUs). Our method is based on Kojima and Igarashi’s Hoare logic for GPU programs. Our algorithm generates verification conditions (VCs) from a program annotated by specifications and loop invariants, and passes them to off-the-shelf SMT solvers. It is often impossible, however, to solve naively generated VCs in reasonable time. A main difficulty stems from quantifiers over threads due to the parallel nature of GPU programs. To overcome this difficulty, we additionally apply several transformations to simplify VCs before calling SMT solvers. Our implementation successfully verifies correctness of several GPU programs, including matrix multiplication optimized by using shared memory. In contrast to many existing verification tools for GPU programs, our verifier succeeds in verifying fully parameterized programs: parameters such as the number of threads and the sizes of matrices are all symbolic. We empirically confirm that our simplification heuristics is highly effective for improving efficiency of the verification procedure.	algorithm;computer graphics;correctness (computer science);decision problem;experiment;formal verification;general-purpose computing on graphics processing units;graphics processing unit;heuristic (computer science);hoare logic;houdini;level of detail;linear algebra;loop invariant;mathematical optimization;matrix multiplication;nonlinear system;programmer;quantifier (logic);rewrite (programming);rewriting;scalability;shared memory;simultaneous multithreading;symbolic computation;symbolic execution;theory;thread (computing);verification and validation;verification condition generator	Kensuke Kojima;Akifumi Imanishi;Atsushi Igarashi	2016		10.1007/978-3-319-48869-1_7	theoretical computer science;high-level verification	PL	-16.363123990030843	25.746688094104655	156910
97501b140313cc85bdf7ee02c1d9951f28384033	techniques to facilitate symbolic execution of real-world programs	saswat;computer science techniques to facilitate symbolic execution of real world programs georgia institute of technology mary jean harrold anand	Symbolic execution is a program-analysis technique that is used to address several problems that arise in developing high-quality software. Despite the fact that the symbolic-execution technique is well understood, and performing symbolic execution on simple programs is straightforward, it is still not possible to apply the technique to the general class of large, real-world software. A symbolic-execution system can be effectively applied to large, real-world software if it has at least two features: efficiency and automation. However, efficient and automatic symbolic execution of real-world programs is a lofty goal due to both theoretical and practical reasons. Theoretically, achieving this goal requires solving an intractable problem (i.e., solving constraints). Practically, achieving this goal requires overwhelming effort to implement a symbolic-execution system that can precisely and automatically symbolically execute real-world programs. #R##N#This research makes three major contributions. 1. Three new techniques that address three important problems of symbolic execution. Compared to existing techniques, the new techniques (2) reduce the manual effort that may be required to symbolically execute those programs that either generate complex constraints or parts of which cannot be symbolically executed due to limitations of a symbolic-execution system. (b) improve the usefulness of symbolic execution (e.g., expose more bugs in a program) by enabling discovery of more feasible paths within a given time budget. 2. A novel approach that uses symbolic execution to generate test inputs for Apps that run on modern mobile devices such as smartphones and tablets. 3. Implementations of the above techniques and empirical results obtained from applying those techniques to real-world programs that demonstrate their effectiveness.	symbolic execution	Saswat Anand	2012			simulation;computer science;concolic testing;algorithm	SE	-16.75153685954986	25.73008408540015	157087
6f3445bfb6ea1356181dfabbb98d55d903a21388	computational analysis of run-time monitoring - fundamentals of java-mac	java programming;bepress selected works;abstraction;monitorable properties;monitoring and checking;computational complexity;run time verification monitorable properties abstraction;computer analysis;process algebra;run time verification	A run-time monitor shares computational resources, such as memory and CPU time, with the target program. Furthermore, heavy computation performed by a monitor for checking target program’s execution with respect to requirement properties can be a bottleneck to the target program’s execution. Therefore, computational characteristics of run-time monitoring cause a significant impact on the target program’s execution. We investigate computational issues on run-time monitoring. The first issue is the power of run-time monitoring. In other words, we study the class of properties run-time monitoring can evaluate. The second issue is computational complexity of evaluating properties written in process algebraic language. Third, we discuss sound abstraction of the target program’s execution, which does not change the result of property evaluation. This abstraction can be used as a technique to reduce monitoring overhead. Theoretical understanding obtained from these issues affects the implementation of Java-MaC, a toolset for the run-time monitoring and checking of Java programs. Finally, we demonstrate the abstraction-based overhead reduction technique implemented in Java-MaC through a case study. This research was supported in part by ONR N00014-97-1-0505, NSF CCR-9988409, NSF CCR-0086147, NSF CISE-9703220, and ARO DAAD19-01-1-0473. 1 Email: moonjoo@secui.com 2 Email: {kannan,lee,sokolsky}@saul.cis.upenn.edu 3 Email: vmahesh@cs.uiuc.edu c ©2002 Published by Elsevier Science B. V.	bottleneck (engineering);central processing unit;computation;computational complexity theory;computational resource;computer monitor;java;linear algebra;overhead (computing);process calculus	Moonjoo Kim;Sampath Kannan;Insup Lee;Oleg Sokolsky;Mahesh Viswanathan	2002	Electr. Notes Theor. Comput. Sci.	10.1016/S1571-0661(04)80578-4	process calculus;real-time computing;computer science;theoretical computer science;abstraction;programming language;computational complexity theory;abstraction model checking	Logic	-17.463658054993275	31.394568314743626	157165
0bf620a113028c116ab574d65e83804318060827	incremental, inductive coverability	finite-state hardware verification;finite number;well-structured transition system;smt solvers;petri net;ic3 procedure;safety verification;expand-enlarge-and-check algorithm;inductive coverability;broadcast protocol;downward-finite well-structured transition system	We give an incremental, inductive (IC3) procedure to check coverability of well-structured transition systems. Our procedure generalizes the IC3 procedure for safety verification that has been successfully applied in finite-state hardware verification to infinite-state wellstructured transition systems. We show that our procedure is sound, complete, and terminating for downward-finite well-structured transition systems —where each state has a finite number of states below it— a class that contains extensions of Petri nets, broadcast protocols, and lossy channel systems. We have implemented our algorithm for checking coverability of Petri nets. We describe how the algorithm can be efficiently implemented without the use of SMT solvers. Our experiments on standard Petri net benchmarks show that IC3 is competitive with state-of-the-art implementations for coverability based on symbolic backward analysis or expand-enlarge-and-check algorithms both in time and space usage.	algorithm;experiment;inductive reasoning;lossy compression;newman's lemma;petri net;simultaneous multithreading;space–time tradeoff;well-structured transition system	Johannes Kloos;Rupak Majumdar;Filip Niksic;Ruzica Piskac	2013		10.1007/978-3-642-39799-8_10	real-time computing;computer science;theoretical computer science;algorithm	Logic	-12.61274129776645	26.46567113858022	159413
9b7c14c68682c045993c6c5004469019a9c6dcc6	a technique for testing command and control software	difference equation;command and control;indexation;predicator;natural phenomena	A technique for testing embedded-microprocessor command and control programs is described. The continuity inherent in functions computed by programs which monitor natural phenomena is exploited by a simple difference equation-based algorithm to predict a program's next output from its preceding ones. The predicted output is compared with the actual output while indexing through the program's domain. Outputs which cannot be predicted are flagged as potential errors. Data are presented which show that this technique can be a sensitive measure of a program's correctness.	algorithm;correctness (computer science);dynamic data;embedded system;list of version control software;maxdiff;microprocessor;parameter (computer programming);real-time clock;real-time computing;recurrence relation;scott continuity;test case;warez	Marvin L. Watkins	1982	Commun. ACM	10.1145/358468.358472	command and control;simulation;computer science;control theory;differential equation	SE	-18.548814870661612	29.96630021979825	161079
aab88e87444b96007a7b42b39fe2c0b22ed84dec	local abstract verification and refinement of security protocols	verification;abstraction;refinement;security protocols;abstract interpretation;security protocol	The verification problem for security protocols is undecidable, but it is feasible to verify protocols by abstract interpretation. This paper presents a method based on local abstraction and refinement for verifying security protocols terminably. Local abstraction produces a safe approximation of the security protocol, modeled as a set of Horn logic rules. Refinement removes false attacks introduced by local abstraction. In contrast with methods based on global abstraction, our method abstracts only certain rules that can lead to non-termination when computing fixpoints, that is, it does not abstract all rules. We implement this method in a verification tool SPVT and are able to verify well-known protocols. Moreover, our experiments indicate that local abstraction is less costly than global abstraction.	abstract interpretation;approximation;cryptographic protocol;divergence (computer science);experiment;formal verification;horn clause;refinement (computing);rewriting;tokenization (data security);undecidable problem;verification and validation	Ti Zhou;Mengjun Li;Zhoujun Li	2008		10.1145/1456396.1456399	verification;abstraction inversion;computer science;theoretical computer science;cryptographic protocol;refinement;abstraction;programming language;algorithm	Logic	-17.987583330267782	26.731738252931567	161808
8d0209a30394ce11be68070717cc021837f09bf3	a fragment of linear temporal logic for universal very weak automata		Many temporal specifications used in practical model checking can be represented as universal very weak automata (UVW). They are structurally simple and their states can be labeled by simple temporal logic formulas that they represent. For complex temporal properties, it can be hard to understand why a trace violates a property, so when employing UVWs in model checking, this information helps with interpreting the trace. At the same time, the simple structure of UVWs helps the model checker with finding short traces. While a translation from computation tree logic (CTL) with only universal path quantifiers to UVWs has been described in earlier work, complex temporal properties that define sequences of allowed events along computations of a system are easier to describe in linear temporal logic (LTL). However, no direct translation from LTL to UVWs with little blow-up is known. In this paper, we define a fragment of LTL that gives rise to a simple and efficient translation from it to UVW. The logic contains the most common shapes of safety and liveness properties, including all nestings of “Until”-subformulas. We give a translation from this fragment to UVWs that only has an exponential blow-up in the worst case, which we show to be unavoidable. We demonstrate that the simple shape of UVWs helps with understanding counter-examples in a case study.	automata theory;automaton;best, worst and average case;compiler;computation tree logic;context-free grammar;context-free language;data structure;debugging;experiment;formal methods;linear temporal logic;liveness;logic synthesis;model checking;simulation;time complexity;tracing (software);uvw mapping	Keerthi Adabala;Rüdiger Ehlers	2018		10.1007/978-3-030-01090-4_20	uvw mapping;model checking;discrete mathematics;theoretical computer science;linear temporal logic;temporal logic;computer science	Logic	-13.04348503905233	25.534804307587255	161887
16e8b1737c3bf82af39dd36f00d5e961b3185286	property directed abstract interpretation		Recently, Bradley proposed the PDR/IC3 model checking algorithm for verifying safety properties, where forward and backward reachability analyses are intertwined, and guide each other. Many variants of Bradley’s original algorithm have been developed and successfully applied to both hardware and software verification. However, these algorithms have been presented in an operational manner, in disconnect with the rich literature concerning the theoretical foundation of static analysis formulated by abstract interpretation. Inspired by PDR, we develop a nonstandard semantics which computes for every 0 ≤ N an over-approximation of the set of traces of length N leading to a safety violation. The over approximation is precise, in the sense that it only includes traces that do not start at an initial state, unless the program is unsafe, and in this case the semantics aborts at a special error state. In a way, the semantics computes multiple over-approximations of bounded unsafe program behaviors using a sequence of abstractions whose precision grows automatically with N. We show that existing PDR algorithms can be described as a specific implementation of our semantics, performing an abstract interpretation of the program, but instead of aiming for a fixpoint, they stop early when either the backward analysis finds a counterexample or the states comprising one of the bounded traces provides sufficient evidence that the program is safe. This places PDR within the solid framework of abstract interpretation, and thus provides a unified explanation of the different PDR algorithms as well as a new proof of their soundness.	abstract interpretation;algorithm;approximation;david w. bradley;design review (u.s. government);fixed point (mathematics);model checking;reachability;software verification;st-connectivity;static program analysis;tracing (software);verification and validation	Noam Rinetzky;Sharon Shoham	2016		10.1007/978-3-662-49122-5_5	computer science;theoretical computer science;programming language;algorithm	Logic	-14.831810156348384	26.90879862294695	162313
e4461cd8444e69a92d302db7c71419704192af71	on the static and dynamic extents of delimited continuations	control and prompt;delimited continuations;cps transformation;delimited continuation;continuation passing style;direct style;shift and reset;control operators;continuation passing style cps;defunctionalization	We show that breadth-first traversal exploits the difference between the static delimited-control operator shift (alias S) and the dynamic delimited-control operator control (alias F). For the last 15 years, this difference has been repeatedly mentioned in the literature but it has only been illustrated with one-line toy examples. Breadth-first traversal fills this vacuum. We also point out where static delimited continuations naturally give rise to the notion of control stack whereas dynamic delimited continuations can be made to account for a notion of ‘control queue.’	breadth-first search;call stack;control flow;control theory;delimited continuation;delimiter;tree traversal	Dariusz Biernacki;Olivier Danvy;Chung-chieh Shan	2006	Sci. Comput. Program.	10.1016/j.scico.2006.01.002	delimited continuation;call-with-current-continuation;real-time computing;computer science;programming language;algorithm	PL	-18.92980526529359	30.687574178249225	164243
2feaa2c66d8a7bd181e774df2d8f2215ebf1e2b6	synthesis of fault-tolerant embedded systems using games: from theory to practice	fault tolerant;embedded system;fault tolerant system;constraint solving	In this paper, we present an approach for fault-tolerant synthesis by combining predefined patterns for fault-tolerance with algorithmic game solving. A non-fault-tolerant system, together with the relevant fault hypothesis and faulttolerant mechanism templates in a pool are translated into a distributed game, and we perform an incomplete search of strategies to cope with undecidability. The result of the game is translated back to executable code concretizing fault-tolerant mechanisms using constraint solving. The overall approach is implemented to a prototype tool chain and is illustrated using examples.	constraint satisfaction problem;embedded system;executable;fault tolerance;prototype;toolchain;undecidable problem	Chih-Hong Cheng;Harald Ruess;Alois Knoll;Christian Buckl	2011		10.1007/978-3-642-18275-4_10	fault tolerance;computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-15.579735261220806	27.221942982599387	164867
fef1e24eb4a48284bd9ff905207d39470cb67500	testing conditions for communicating stream x-machine systems	specification language;machine design;test methods;design for test;parallel processing	X-machines were proposed by Holcombe as a possible specification language and since then a number of further investigations have demonstrated that the model is intuitive and easy to use. In particular, stream X-machines (SXM), a particular class of X-machines, have been found to be extremely useful in practice. Furthermore, a method of testing systems specified as SXMs exists and is proved to detect all faults of the implementation provided that the system meets certain “design for test conditions”. Recently, a system of communicating SXMs was introduced as a means of modelling parallel processing. This paper proves that each communicating machine component can be transformed in a straightforward manner so that the entire system will behave like a single stream X-machine - the equivalent SXM of the system. The paper goes on to investigate the applicability of the SXM testing method to a system of communicating SXMs and identifies a class of communicating SXMs for which the equivalent SXM of the system meets the “design for test conditions”.	automaton;communicating finite-state machine;correctness (computer science);dfa minimization;design for testing;dummy variable (statistics);extended memory;parallel computing;specification language;stream x-machine;test set;whole earth 'lectronic link	Florentin Ipate;Mike Holcombe	2002	Formal Aspects of Computing	10.1007/s001650200021	parallel processing;specification language;computer science;theoretical computer science;design for testing;test method;programming language;algorithm	SE	-16.5299455696014	30.9518598204567	164894
01086201f2ebfb88adff184051ce836303ebb748	rigorous examination of reactive systems	verification;competition;model checking;reactive system;event condition action system;model based testing;program analysis	The goal of the RERS challenge is to evaluate the effectiveness of various verification and validation approaches on reactive systems, a class of systems that is highly relevant for industrial critical applications. The RERS challenge brings together researchers from different areas of software verification and validation, including static analysis, model checking, theorem proving, symbolic execution, and testing. The challenge provides a forum for experimental comparison of different techniques on specifically designed verification tasks. These benchmarks are automatically synthesized to exhibit chosen properties, and then enhanced to include dedicated dimensions of difficulty, such as conceptual complexity of the properties (e.g., reachability, safety, liveness), size of the reactive systems (a few hundred lines to millions of lines), and complexity of language features (arrays and pointer arithmetic). The STTT special section on RERS describes the results of the evaluations and the different analysis techniques that were used in the RERS challenges 2012 and 2013.	adaptive server enterprise;algorithm;array data structure;automated theorem proving;benchmark (computing);black box;darpa network challenge;liveness;model checking;pointer (computer programming);reachability;software verification and validation;static program analysis;symbolic execution	Falk Howar;Malte Isberner;Maik Merten;Bernhard Steffen;Dirk Beyer;Corina S. Pasareanu	2014	International Journal on Software Tools for Technology Transfer	10.1007/s10009-014-0337-y	program analysis;model checking;model-based testing;verification;simulation;competition;reactive system;computer science;theoretical computer science;programming language;algorithm	SE	-18.457812264194402	28.06231325264024	166680
958041e741114765c24bc9ad794426cd23ee7b01	program transformation of embedded systems		This thesis explores the use of program transformation tools in the analysis and compilation of programs for embedded systems. The intrinsic difficulty in programming for these small-scale systems arises from the discrete effects that executing an instruction produces. In larger scale systems these effects are masked by the large number of instructions executed to perform a typical action. The central claim of this thesis is that program transformation tools can aid the programmer in understanding and controlling these effects and therefore reduce the complexity of the problem; both in difficulty and in scale. We have investigated this method on two cases that are typical of the problems in embedded software development; software timing and precision. In the former case we have developed a novel abstract interpretation that computes the timing distance between locations in a program binary. Our new technique for analysing the structure of a program can then be combined with the set of timing distances to generate models of the timing behaviour of the program. We demonstrate this work in the context of verifying legacy applications. In the later case we look at the problem of roundoff error in software. Given the small datasizes on typical embedded processors, the programmer is normally responsible for handling the rounding of values and ensuring that datatypes are sufficiently large to represent data without an error. We present a domain specific language that allows this property to be expressed directly. Analysis of how to propagate the variable precisions in this domain is investigated, and we conclude with a technique to automatically generate a compiler for the domain.	abstract interpretation;central processing unit;compiler;domain-specific language;embedded software;embedded system;program transformation;programmer;round-off error;rounding;software development;verification and validation	Andrew David Moss	2005			computer engineering;program transformation;computer science	Embedded	-16.862619478755423	32.13103086620218	166823
580bcddbfb2399972b269a4ec5e7d0a22e4dbad1	game-based safety checking with mage	game models;software model checking;game semantics;compositional verification;refinement;symbolic automata;experimental model;on the fly;data approximation	Mage is a new experimental model checker based on game semantics. It adapts several techniques including lazy (on-the-fly) modelling, symbolic modelling, C.E.G.A.R. and approximated counterexample certification to game models. It demonstrates the potential for truly compositional verification of real software.	approximation algorithm;game semantics;lazy evaluation;model checking;xojo	Adam Bakewell;Dan R. Ghica	2007		10.1145/1292316.1292326	computer science;game semantics;theoretical computer science;refinement;programming language;symbolic trajectory evaluation;algorithm	Logic	-15.873710467238649	26.164445139964712	167719
9b5669fb952e9b15ac09ddc6abc404c07ecdbadd	on-the-fly reachability and cycle detection for recursive state machines	programmation booleenne;program behavior;developpement logiciel;boolean programming;modelizacion;distributed system;verificacion modelo;systeme reparti;reachability;securite;maquina estado finito;vivacidad;espace etat;verification modele;state machine;comportamiento programa;program verification;programacion booleana;vivacite;modelisation;automate a pile;performance programme;verificacion programa;sistema repartido;model checking;state space method;methode espace etat;desarrollo logicial;asequibilidad;envoi message;state space;software development;safety;message passing;liveness;on the fly;atteignabilite;programme recursif;comportement programme;eficacia programa;program performance;programa recursivo;recursive program;push down automaton;espacio estado;machine etat fini;verification programme;seguridad;modeling;finite state machine;reachability analysis;automata a pila;analyse atteignabilite;metodo espacio estado	Searching the state space of a system using enumerative and on-the-fly depth-first traversal is an established technique for model checking finite-state systems. In this paper, we propose algorithms for on-the-fly exploration of recursive state machines, or equivalently pushdown systems, which are suited for modeling the behavior of procedural programs. We present algorithms for reachability (is a bad state reachable?) as well as for fair cycle detection (is there a reachable cycle with progress?). We also report on an implementation of these algorithms to check safety and liveness properties of recursive boolean programs, and its performance on existing benchmarks.	algorithm;benchmark (computing);best, worst and average case;cycle detection;data structure;depth-first search;dynamic data;dynamization;edge dominating set;graph (abstract data type);liveness;model checking;program analysis;reachability;recursion (computer science);regression testing;simultaneous localization and mapping;stack (abstract data type);state space;test suite;verification and validation	Rajeev Alur;Swarat Chaudhuri;Kousha Etessami;P. Madhusudan	2005		10.1007/978-3-540-31980-1_5	model checking;message passing;real-time computing;simulation;systems modeling;computer science;state space;software development;finite-state machine;programming language;reachability;algorithm;liveness	Logic	-16.587245219678703	27.99878690477341	169890
0597eadbca45d2d0b324f7d50b75af30a19f2f18	extending ctl with actions and real time	real time;formal semantics;specification language;unified modelling language uml;model checking	In this paper, we present the logic ATCTL, which is intended to be used for model checking models that have been specified in a lightweight version of the Unified Modelling Language (UML). Elsewhere, we have defined a formal semantics for LUML to describe the models. This paper’s goal is to give a specification language for properties that fits LUML; LUML includes states, actions and real time. ATCTL extends CTL with concurrent actions and real time. It is based on earlier extensions of CTL by De Nicola and Vaandrager (ACTL) and Alur et al. (TCTL). This makes it easier to adapt existing model checkers to ATCTL. To show that we can check properties specified in ATCTL in models specified in LUML, we give a small example using the Kronos model checker.	fits;kronos;model checking;semantics (computer science);specification language;unified modeling language	David N. Jansen;Roel Wieringa	2002	J. Log. Comput.	10.1093/logcom/12.4.607	model checking;specification language;computer science;theoretical computer science;formal semantics;programming language;algorithm	Logic	-18.635290652575506	27.643360653183425	170410
34ad2a3bd019d0b8250bba2f80d031bcc61477ff	directed test generation using symbolic grammars	symbolic test generation;grammar based testing;symbolic grammar;execution-directed symbolic test generation;concolic execution;symbolic grammars;test input;random testing;symbolic execution;test generation;valid input;exhaustive enumeration;pure enumerative test generation;testing c programs;symbolic constant;context free grammar	"""We present CESI, an algorithm that combines exhaustive enumeration of test inputs from a structured domain with symbolic execution driven test generation. CESI is a hybrid of two predominant techniques: specification-based enumerative test generation (which exhaustively generates all possible inputs satisfying some constraint) and symbolic directed test generation (which explores program paths based on symbolic path constraint solving). We target programs whose valid inputs are determined by some context free grammar. We introduce symbolic grammars, where the original tokens are replaced with symbolic constants, that link enumerative grammar-based input generation with symbolic directed testing. Symbolic grammars abstract the concrete input syntax, thus reducing the set of input strings that must be enumerated exhaustively. For each enumerated input string, which may contain symbolic constants, symbolic execution based test generation instantiates the constants based on program execution paths. The """"template"""" generated by enumerating valid strings reduces the burden on the symbolic execution to generate syntactically valid inputs and hence exercise interesting code paths. Together, symbolic grammars provide a link between exhaustive enumeration of valid inputs and execution-directed symbolic test generation. In preliminary experiments, CESI is better than if both enumerative and symbolic techniques are used alone."""	algorithm;constraint satisfaction problem;context-free grammar;experiment;string (computer science);symbolic execution	Rupak Majumdar;Ru-Gang Xu	2007		10.1145/1295014.1295039	random testing;computer science;theoretical computer science;software engineering;symbolic data analysis;context-free grammar;programming language;concolic testing;symbolic trajectory evaluation;algorithm;satisfiability	SE	-19.001126532702223	25.302853880161727	171308
74335bc18abe16fa6ce475e0ad499ab2d84de421	comparing verification systems: interactive consistency in acl2	verification;tolerancia falta;automated proof discovery verification systems interactive consistency acl2 fault tolerant computing oral messages algorithm machine supported verifications ehdm pvs benchmark problem specification systems om algorithm acl2 logic higher order functions strong typing lambda abstraction full quantification logical constructs;formal specification;fault tolerant;benchmark problem;software fault tolerance;systeme informatique tolerant panne;fault tolerance fault tolerant systems cost function automatic logic units specification languages algorithm design and analysis;program verification;specification language;theorem proving;fault tolerant computer systems;algorithme;demonstration theoreme;formal verification;specification and verification;computational logic;specification languages;theorem proving program verification software fault tolerance formal logic formal specification;fault tolerance;automatic theorem proving;formal logic;algorithms;lenguaje especificacion;higher order functions;verificacion;langage specification;tolerance faute	Achieving interactive consistency among processors in the presence of faults is an important problem in fault tolerant computing, first cleanly formulated by Lamport, Pease, and Shostak and solved in selected cases with their Oral Messages (OM) algorithm. Several machine-supported verifications of this algorithm have been presented, including a particularly elegant formulation and proof by John Rushby using EHDM and PVS. Rushby proposes interactive consistency as a benchmark problem for specification and verification systems. We present a formalization of the OM algorithm in the ACL2 logic and compare our formalization and proof to his. We draw some conclusions concerning the range of desirable features for verification systems. In particular, while higher-order functions, strong typing, lambda abstraction, and full quantification have some value they come with a cost; moreover, many uses of such features can be easily translated into simpler logical constructs, which facilitate more automated proof discovery. We offer a cautionary note about comparing systems with respect to a small set of problems in a limited domain. Index Terms —Formal verification, automatic theorem proving, fault tolerance, computational logic, specification languages. —————————— ✦ ——————————	acl2;algorithm;automated theorem proving;benchmark (computing);central processing unit;computational logic;fault tolerance;formal verification;higher-order function;lambda calculus;prototype verification system;specification language	William D. Young	1997	IEEE Trans. Software Eng.	10.1109/32.588536	fault tolerance;computer science;theoretical computer science;programming language;algorithm	Logic	-14.001581790721513	28.78239842027224	171329
cbf424e88e28221dcfee6f4a2dda94c632d39e55	reduction of interrupt handler executions for model checking embedded software	formal model;model checking;state space;transition systems;partial order reduction;state explosion;static analysis;embedded software	Interrupts play an important role in embedded software. Unfortunately, they aggravate the state-explosion problem that model checking is suffering from. Therefore, we propose a new abstraction technique based on partial order reduction that minimizes the number of locations where interrupt handlers need to be executed during model checking. This significantly reduces state spaces while the validity of the verification results is preserved. The paper details the underlying static analysis which is employed to annotate the programs before verification. Moreover, it introduces a formal model which is used to prove that the presented abstraction technique preserves the validity of the branchingtime logic CTL*-X by establishing a stutter bisimulation equivalence between the abstract and the concrete transition system. Finally, the effectiveness of this abstraction is demonstrated in a case study.	approximation algorithm;blocking (computing);embedded software;embedded system;formal language;heuristic (computer science);interrupt handler;live variable analysis;model checking;partial order reduction;pointer (computer programming);pointer analysis;reaching definition;refinement (computing);static program analysis;stutter bisimulation;transition system;turing completeness	Bastian Schlich;Thomas Noll;Jörg Brauer;Lucas Brutschy	2009		10.1007/978-3-642-19237-1_5	model checking;partial order reduction;parallel computing;real-time computing;embedded software;computer science;state space;formal equivalence checking;programming language;abstraction model checking;static analysis;algorithm	Logic	-17.996186407610853	28.128785076974072	171823
157c0cd2c96abf62d4d1e491e0fe62f23ed7b3cf	on the characterization of until as a fixed point under clocked semantics	flxed point;temporal logic;fixed point;hardware design	Modern hardware designs are typically based on multiple clocks. While a singly-clocked hardware design is easily described in standard temporal logics, describing a multiply-clocked design is cumbersome. Thus, it is desirable to have an easier way to formulate properties related to clocks in a temporal logic. In [2] a relatively simple solution built on top of the traditional ltl semantics was suggested. The suggested semantics was examined relative to a list of design goals, and it was shown that it answered all requirements except for preserving the least fixed point characterization of the until operator under multiple clocks. In this work we show that with a minor addition to the semantics of [2] this requirement is met as well.	clock rate;fixed point (mathematics);least fixed point;requirement;temporal logic	Dana Fisman	2007		10.1007/978-3-540-77966-7_6	linear temporal logic;temporal logic;computer science;theoretical computer science;fixed point;algorithm	PL	-14.182138628778041	25.380511574598735	172116
99a9005e75946163e75d334eb8e97757d790b0a3	input space partitioning to enable massively parallel proof		Real-world applications often include large, empirically defined discrete-valued functions. When proving properties about these applications, the proof naturally breaks into one case per entry in the first function reached, and again into one case per entry in the next function, and continues splitting. This splitting yields a combinatorial explosion of proof cases that challenges traditional proof approaches. While each proof case represents a mathematical path from inputs to outputs through these functions, the full set of cases is not available up front, preventing a straightforward application of parallelism. Here we describe an approach that slices the input space, creating a partition based on pre-computed mathematical paths such that each slice has only a small number of proof cases. These slices are amenable to massively parallel proof. We evaluate this approach using an example model of an adaptive cruise control, where proofs are conducted in a highly parallel PVS environment.		Ashlie B. Hocking;M. Anthony Aiello;John C. Knight;Nikos Arechiga	2017		10.1007/978-3-319-57288-8_10	parallel computing;theoretical computer science;massively parallel;distributed computing	HPC	-16.117334502951067	25.91813953383767	172359
d1589c9406969610fef21a9f067b97ba22c7c811	optimal code from flow graphs	unconditional jump;optimal translation;repeat-until statement;if-then-else statement;optimal code;linear-time algorithm;linear sequence;flow graph;atomic statement;minimal number	Abstract This paper considers the problem of generating a linear sequence of instructions from a flow graph so as to minimize the number of jumps. We show that for programs constructed from atomic statements with semicolon, if-then, if-then-else , and repeat-until , the minimal number of unconditional jumps is bounded from above by e + 1 and from below by max {e − b + 1, ⌜(e + 1)/2⌝} , where e is the number of if-then-else statements and b is the number of repeat-until statements. We show that these bounds are tight and present a linear-time algorithm for finding the optimal translation of such a program.		M. V. S. Ramanath;Marvin H. Solomon	1982	Comput. Lang.	10.1016/0096-0551(82)90020-0	algorithm	Logic	-15.599596620448626	31.847534048215895	173228
0f04ae2e428261cf98209f2cdcf0e1938be9a5f6	qf bv model checking with property directed reachability	upper bound;approximation algorithms;cognition;model checking;optimization;hardware	In 2011, property directed reachability (PDR) was proposed as an efficient algorithm to solve hardware model checking problems. Recent experimentation suggests that it outperforms interpolation-based verification, which had been considered the best known algorithm for this purpose for almost a decade. In this work, we present a generalization of PDR to the theory of quantifier free formulae over bitvectors (QF_BV), illustrate the new algorithm with representative examples and provide experimental results obtained from experimentation with a prototype implementation.	algorithm;design review (u.s. government);interpolation;model checking;prototype;quantifier (logic);reachability;st-connectivity	Tobias Welp;Andreas Kuehlmann	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;real-time computing;computer science;theoretical computer science;operating system;programming language;algorithm	EDA	-13.8088955547304	27.104739889489025	173611
171ee0710879f5dfdef0eda1ec5df89c336e93ef	fully symbolic timed model checking using constraint matrix diagrams	boolean constraint fully symbolic timed model checking constraint matrix diagrams reachability analysis timed automata matrix based state space representations diagram based state space representations difference bound matrices clock difference diagrams clock restriction diagrams;boolean functions;atomic clocks;difference bound matrices;fixed point;clock difference diagrams;constraint matrix diagrams;automata;indexes;cost accounting;formal verification;binary decision diagrams;model checking;matrix based state space representations;data structures;state space;automata theory;reachability analysis automata theory boolean functions formal verification;timed automata;state space representation;fully symbolic;clock restriction diagrams;diagram based state space representations;fully symbolic timed model checking;difference bound matrices fully symbolic timed automata reachability analysis binary decision diagrams;data structure;reachability analysis;boolean constraint;data structures boolean functions cost accounting indexes automata atomic clocks;binary decision diagram	We present constraint matrix diagrams (CMDs), a novel data structure for the fully symbolic reach ability analysis of timed automata. CMDs combine matrix-based and diagram-based state space representations generalizing the concepts of difference bound matrices (DBMs), clock difference diagrams (CDDs), and clock restriction diagrams (CRDs). The key idea is to represent convex parts of the state space as (partial) DBMs which are, in turn, organized in a CDD/CRD-like ordered and reduced diagram. The location information is incorporated as a special Boolean constraint in the matrices. We describe all CMD operations needed for the construction of the transition relation and the reach ability fixed point computation. Based on a prototype implementation, we compare our technique with the timed model checkers RED and Uppaal, and furthermore investigate the impact of two different reduced forms on the time and space consumption.	automata theory;cdd;computation;constraint logic programming;data structure;diagram;fixed point (mathematics);heuristic (computer science);microsoft outlook for mac;model checking;prototype;reachability;real-time clock;real-time computing;refinement (computing);state space;timed automaton;uppaal	Rüdiger Ehlers;Daniel Fass;Michael Gerke;Hans-Jörg Peter	2010	2010 31st IEEE Real-Time Systems Symposium	10.1109/RTSS.2010.36	atomic clock;model checking;database index;data structure;formal verification;computer science;state space;state-space representation;automata theory;fixed point;automaton;boolean function;programming language;binary decision diagram;algorithm;cost accounting	Embedded	-12.026361696432806	27.051735440821172	174302
01b2dd1dff652172869836309cebdb19bcd5f5d9	structural counter abstraction	complex concurrent system;well-structured transition system;complex shared heap structure;numerical abstraction;fair termination;termination provers;depth-bounded system;traditional counter abstraction;termination analysis;structural counter abstraction;concurrent infinite-state system	Depth-Bounded Systems form an expressive class of well-structured transition systems. They can model a wide range of concurrent infinite-state systems including those with dynamic thread creation, dynamically changing communication topology, and complex shared heap structures. We present the first method to automatically prove fair termination of depth-bounded systems. Our method uses a numerical abstraction of the system, which we obtain by systematically augmenting an over-approximation of the system’s reachable states with a finite set of counters. This numerical abstraction can be analyzed with existing termination provers. What makes our approach unique is the way in which it exploits the well-structuredness of the analyzed system. We have implemented our work in a prototype tool and used it to automatically prove liveness properties of complex concurrent systems, including nonblocking algorithms such as Treiber’s stack and several distributed processes. Many of these examples are beyond the scope of termination analyses that are based on traditional counter abstractions.	algorithm;approximation;concurrency (computer science);counter (digital);liveness;numerical analysis;prototype;well-structured transition system;whole earth 'lectronic link	Kshitij Bansal;Eric Koskinen;Thomas Wies;Damien Zufferey	2013		10.1007/978-3-642-36742-7_5	real-time computing;computer science;distributed computing;programming language;algorithm	PL	-18.16670725540616	26.95782829573362	174805
b7f7749bf62dcf2768858f222673a0a504ac75df	branching time semantics for uml 2.0 sequence diagrams	systeme temps reel;anotacion;graph theory;distributed system;evaluation performance;diagrama secuencia;verificacion modelo;teoria grafo;systeme reparti;formal specification;relation ordre partiel;performance evaluation;analyse statique;uml sequence diagram;sistema temporizado;lenguaje uml;diagramme etat;evaluacion prestacion;branching;metodo formal;timed system;methode formelle;verification modele;semantics;langage modelisation unifie;automate temporise;annotation;sistema complejo;program verification;diagramme sequence;semantica;semantique;theorie graphe;analisis estatica;formal method;automata contemporizado;specification formelle;especificacion formal;verificacion programa;enrejado;sistema repartido;diagrama estado;model checking;systeme complexe;complex system;treillis;partial ordering;ramificacion;unified modelling language;tâche controle;performance analysis;consistency checking;systeme temporise;state diagram;ramification;horloge;real time system;checking task;tarea control;timed automata;sistema tiempo real;relacion orden parcial;static analysis;verification programme;clock;sequence diagram;reloj;lattice;partial order;time constraint	This paper presents formal definitions for UML Sequences Diagrams based on branching time semantics and partial orders in a denotational style. The obtained graphs are close to lattices and specify faithfully the intended behaviors rather than trace based semantics. We also define few generalized algebraic operations on graphs so that it makes it easy to provide formal definitions in a compositional manner to interaction operators. Next we extend our formalism with logical clocks and time formulas over values of these clocks to express timing constraints of complex systems. We present also some algorithms to extract time annotations that adorn sequence diagrams and transform them into timing constraints in our timed graphs. Obviously, this approach alleviates more the hard task of consistency checking between UML diagrams, specifically interaction diagrams with regards to state diagrams. Timeliness and performance analysis of timed graphs related to sequence diagrams could take advantages of works on model checking of timed automata.	sequence diagram;unified modeling language	Youcef Hammal	2006		10.1007/11888116_20	partially ordered set;sequence diagram;formal methods;computer science;artificial intelligence;graph theory;theoretical computer science;semantics;programming language;story-driven modeling;algorithm	SE	-16.74632040862945	27.59513899577255	175118
4797cd5cef35a1e2c30a14b1acf858e894cfe775	alternate and learn: finding witnesses without looking all over	symbolic bug detection technique;focused bug detection;forward exploration;callee invariants;callee procedure;failed exploration attempt;program fragment;future exploration;program control flow graph;program call graph	Most symbolic bug detection techniques perform search over the program control flow graph based on either forward symbolic execution or backward weakest preconditions computation. The complexity of determining inter-procedural all-path feasibility makes it difficult for such analysis to judge up-front whether the behavior of a particular caller or callee procedure is relevant to a given property violation. Consequently, these methods analyze several program fragments irrelevant to the property, often repeatedly, before arriving at a goal location or an entrypoint, thus wasting resources and diminishing their scalability. This paper presents a systematic and scalable technique for focused bug detection which, starting from the goal function, employs alternating backward and forward exploration on the program call graph to lazily infer a small scope of program fragments, sufficient to detect the bug or show its absence. The method learns caller and callee invariants for procedures from failed exploration attempts and uses them to direct future exploration towards a scope pertinent to the violation.	backtracking;call graph;computation;control flow graph;future search;java;mike lesser;open-source software;pointcast (dotcom);precondition;predicate transformer semantics;relevance;scalability;software bug;symbolic execution	Nishant Sinha;Nimit Singhania;Satish Chandra;Manu Sridharan	2012		10.1007/978-3-642-31424-7_42	simulation;computer science;artificial intelligence;algorithm	Logic	-18.290427879441346	31.190466133348963	175344
02834f878860dabe95097f406d5f1814b3fc0036	context-enhanced directed model checking	search method;standard model;model checking;concurrent systems;state space;state explosion;empirical evaluation	Directed model checking is a well-established technique to efficiently tackle the state explosion problem when the aim is to find error states in concurrent systems. Although directed model checking has proved to be very successful in the past, additional search techniques provide much potential to efficiently handle larger and larger systems. In this work, we propose a novel technique for traversing the state space based on interference contexts. The basic idea is to preferably explore transitions that interfere with previously applied transitions, whereas other transitions are deferred accordingly. Our approach is orthogonal to the model checking process and can be applied to a wide range of search methods. We have implemented our method and empirically evaluated its potential on a range of non-trivial case studies. Compared to standard model checking techniques, we are able to detect subtle bugs with shorter error traces, consuming less memory and time.	automata theory;concurrency (computer science);heuristic (computer science);interference (communication);iteration;model checking;relevance;search algorithm;software bug;state space;timed automaton;tracing (software)	Martin Wehrle;Sebastian Kupferschmid	2010		10.1007/978-3-642-16164-3_7	model checking;partial order reduction;real-time computing;computer science;theoretical computer science;abstraction model checking;algorithm	SE	-16.99582176756688	28.342706909347985	175416
dd99c78857cc7cfd0718a3fe556f91dcb1a4c9a1	instruction execution sequence confirmation	machine design	Acquiring extremely dependable results from computers requires attention to all of the stages from program and machine design through execution of the program. One of the smaller, but still important, stages is that of verifying that the sequence of instructions executed by the processor was exactly the sequence specified by the compiler. A surprisingly small addition to the logic of the processor (and code produced by the compiler) is sufficient to achieve this.	compiler;computer;dependability;verification and validation	Kay P. Litchfield	1994	SIGARCH Computer Architecture News	10.1145/181799.181804	computer architecture;parallel computing;real-time computing;computer science;operating system;programming language	Arch	-16.64247344057807	31.858527897433884	175419
05da3ac6dad0c32d93c486978e2454326ccabac4	symbolic parametric safety analysis of linear hybrid systems with bdd-like data-structures	verification;directed graphs;symbol manipulation;model checking index terms data structures bdd hybrid automata verification;formal specification;bdd;model checking bdd like data structure hybrid restriction diagrams linear hybrid automata symbolic parametric safety analysis formal verification;formal verification;binary decision diagrams;model checking;safety analysis;data structures;index terms data structures;state space;hybrid system;automata theory;hybrid automata;weakest precondition;safety data structures boolean functions binary decision diagrams automata cost accounting computer society systems engineering and theory character generation protocols;data structure;symbol manipulation binary decision diagrams formal verification automata theory data structures formal specification directed graphs;verification model	We introduce a new BDD-like data structure called hybrid-restriction diagrams (HRDs) for the representation and manipulation of linear hybrid automata (LHA) state-spaces and present algorithms for weakest precondition calculations. This permits us to reason about the valuations of parameters that make safety properties satisfied. Advantages of our approach include the ability to represent discrete state information and concave polyhedra in a unified scheme, as well as to save both memory consumptions and manipulation times when processing the same substructures in state-space representations. Our experimental results document its efficiency in practice.	algorithm;automata theory;benchmark (computing);concave function;data structure;diagram;hybrid automaton;hybrid system;paxos (computer science);polyhedron;precondition;predicate transformer semantics;state space;subsumption architecture	Farn Wang	2005	IEEE Transactions on Software Engineering	10.1109/TSE.2005.13	model checking;verification;directed graph;data structure;formal verification;computer science;state space;theoretical computer science;automata theory;formal specification;predicate transformer semantics;programming language;algorithm;hybrid system	SE	-12.831182696025184	27.331683786483794	176956
0f0c5511d06df02094af355760e90096878c01d2	symbolic crosschecking of floating-point and simd code	simd;paper;symbolic crosschecking;performance;klee fp;software engineering;sse;computer vision;symbolic execution;package;control flow;algorithms;streaming simd extensions;floating point;computer science;open source;bounded verification	We present an effective technique for crosschecking an IEEE 754 floating-point program and its SIMD-vectorized version, implemented in KLEE-FP, an extension to the KLEE symbolic execution tool that supports symbolic reasoning on the equivalence between floating-point values.  The key insight behind our approach is that floatingpoint values are only reliably equal if they are essentially built by the same operations. As a result, our technique works by lowering the Intel Streaming SIMD Extension (SSE) instruction set to primitive integer and floating-point operations, and then using an algorithm based on symbolic expression matching augmented with canonicalization rules.  Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation.  We applied KLEE-FP to OpenCV, a popular open source computer vision library. KLEE-FP was able to successfully crosscheck 51 SIMD/SSE implementations against their corresponding scalar versions, proving the bounded equivalence of 41 of them (i.e., on images up to a certain size), and finding inconsistencies in the other 10.	algorithm;branching factor;computer vision;control flow;data parallelism;open-source software;opencv;parallel computing;streaming simd extensions;symbolic computation;symbolic execution;turing completeness	Peter Collingbourne;Cristian Cadar;Paul H. J. Kelly	2011		10.1145/1966445.1966475	parallel computing;simd;performance;computer science;floating point;theoretical computer science;operating system;symbolic data analysis;programming language;package;control flow	OS	-16.65439700660756	31.712107209633754	177179
e8776529319841c21f678510ed1cde24a1cdf3bb	symbolic trajectory evaluation for word-level verification: theory and implementation	x based abstraction;symbolic trajectory evaluation;word level verification;computer science;hardware;smt solving	Symbolic trajectory evaluation (STE) is a model checking technique that has been successfully used to verify many industrial designs. Existing implementations of STE reason at the level of bits, allowing signals in a circuit to take values from a lattice comprised of three elements: 0, 1, and X . This limits the amount of abstraction that can be achieved, and presents limitations to scaling STE to even larger designs. The main contribution of this paper is to show how much more abstract lattices can be derived automatically from register-transfer level descriptions, and how a model checker for the general theory of STE instantiated with such abstract lattices can be implemented in practice. We discuss several implementation issues, including how word-level circuits can be symbolically simulated using a new encoding for words that allows representing X values of sub-words succinctly. This gives us the first practical word-level STE engine, called STEWord. Experiments on a set of designs similar to those used in industry show that STEWord scales better than bit-level STE, as well as word-level bounded model checking.	bit-level parallelism;debugging;decision problem;formal verification;heuristic (computer science);image scaling;integrated information theory;jacobi method;level design;model checking;rakesh agrawal (computer scientist);refinement (computing);register-transfer level;satisfiability modulo theories;simultaneous multithreading;solver;symbolic trajectory evaluation;systemverilog	Supratik Chakraborty;Zurab Khasidashvili;Carl-Johan H. Seger;Rajkumar Gajavelly;Tanmay Haldankar;Dinesh Chhatani;Rakesh Mistry	2017	Formal Methods in System Design	10.1007/s10703-017-0268-9	computer science;theoretical computer science;programming language;symbolic trajectory evaluation;algorithm	Logic	-16.264464367647264	29.79464170748225	177356
d31c7f3472ba4f3e1bbf01bd39c8758b6e848956	formal method for self-timed design	vlsi;diagrams;logic design;boolean equations;compact event model-change diagrams;formal synthesis procedure;initial specification;self-timed circuit design;set of equivalent transformations	Formal method of self-timed circuit design, based on compact event model - change diagrams (CD), is suggested. This model (CD) seems to be very attractive because of convenient tools for describing the semantics of concurrency which allows to enhance the specification from distributive class of processes to semimodular ones. The necessary and sufficient conditions of CD correctness and polynomial algorithms of their analysis are introduced. The formal synthesis procedure consider as the set of equivalent transformations of initial specification (inserting to it the additional signals) that exclude some incorrectnesses (contradiction, abnormality and so on). The boolean equations of implementing self-timed circuit can be easily obtained from the corrected description.	algorithm;circuit design;concurrency (computer science);correctness (computer science);diagram;event (computing);formal methods;polynomial	Michael Kishinevsky;Alex Kondratyev;Alexander Taubin	1991			discrete mathematics;theoretical computer science;formal specification;mathematics;algorithm	EDA	-14.694577922111819	28.5589586683479	178178
08355c7530df40daf0b31a075f670cbf713a4ec2	path-sensitive resource analysis compliant with assertions	verification;dynamic programming;program diagnostics;program control structures;formal methods;abstraction;trees mathematics;iterative methods;context timing concrete abstracts scalability arrays algorithm design and analysis;trees mathematics dynamic programming iterative methods program control structures program diagnostics;hybrid systems;dynamic programming algorithm path sensitive resource analysis program worst case resource usage program executions program point selection symbolic simulation loop iterations symbolic execution tree infeasible path elimination dominated path elimination	We consider the problem of bounding the worst-case resource usage of programs, where assertions about valid program executions may be enforced at selected program points. It is folklore that to be precise, path-sensitivity (up to loops) is needed. This entails unrolling loops in the manner of symbolic simulation. To be tractable, however, the treatment of the individual loop iterations must be greedy in the sense once analysis is finished on one iteration, we cannot backtrack to change it. We show that under these conditions, enforcing assertions produces unsound results. The fundamental reason is that complying with assertions requires the analysis to be fully sensitive (also with loops) wrt. the assertion variables.  We then present an algorithm where the treatment of each loop is separated in two phases. The first phase uses a greedy strategy in unrolling the loop. This phase explores what is conceptually a symbolic execution tree, which is of enormous size, while eliminates infeasible paths and dominated paths that guaranteed not to contribute to the worst case bound. A compact representation is produced at the end of this phase. Finally, the second phase attacks the remaining problem, to determine the worst-case path in the simplified tree, excluding all paths that violate the assertions from bound calculation. Scalability, in both phases, is achieved via an adaptation of a dynamic programming algorithm.	assertion (software development);backtrack;best, worst and average case;cobham's thesis;control flow;dynamic programming;greedy algorithm;iteration;scalability;symbolic execution;symbolic simulation	Duc-Hiep Chu;Joxan Jaffar	2013	2013 Proceedings of the International Conference on Embedded Software (EMSOFT)	10.1109/EMSOFT.2013.6658593	embedded system;real-time computing;verification;formal methods;computer science;theoretical computer science;operating system;dynamic programming;distributed computing;abstraction;iterative method;programming language;algorithm;hybrid system	Embedded	-18.05976126548416	30.87298055498174	178378
0a4e617157fa43baeba441909de14b799a6e06db	automated test data generation using an iterative relaxation method	iterative refinement;path testing;relaxation methods;automated test data generation;predicate sliccs;test data generation;linear constraint;dynamic test data generation;predicate residuals;linear functionals;gaussian elimination;input dependency set	An important problem that arises in path oriented testing is the generation of test data that causes a program to follow a given path. In this paper, we present a novel program execution based approach using an iterative relaxation method to address the above problem. In this method, test data generation is initiated with an arbitrarily chosen input from a given domain. This input is then iteratively refined to obtain an input on which all the branch predicates on the given path evaluate to the desired outcome. In each iteration the program statements relevant to the evaluation of each branch predicate on the path are executed, and a set of linear constraints is derived. The constraints are then solved to obtain the increments for the input. These increments are added to the current input to obtain the input for the next iteration. The relaxation technique used in deriving the constraints provides feedback on the amount by which each input variable should be adjusted for the branches on the path to evaluate to the desired outcome.When the branch conditions on a path are linear functions of input variables, our technique either finds a solution for such paths in one iteration or it guarantees that the path is infeasible. In contrast, existing execution based approaches may require an unacceptably large number of iterations for relatively long paths because they consider only one input variable and one branch predicate at a time and use backtracking. When the branch conditions on a path are nonlinear functions of input variables, though it may take more then one iteration to derive a desired input, the set of constraints to be solved in each iteration is linear and is solved using Gaussian elimination. This makes our technique practical and suitable for automation.	approximation algorithm;backtracking;gaussian elimination;iteration;linear function;linear programming relaxation;n-gram;nonlinear system;numerical analysis;pointer (computer programming);relaxation (approximation);relaxation (iterative method);string (computer science);test automation;test data generation	Neelam Gupta;Aditya P. Mathur;Mary Lou Soffa	1998		10.1145/288195.288321	basis path testing;mathematical optimization;gaussian elimination;test data generation;algorithm	SE	-15.654814574807421	26.227081327957865	178456
6cb0c11d2c5691c94883193cf761f53c525a0f26	pushdown flow analysis with abstract garbage collection		"""In the static analysis of functional programs, pushdown flow analysis and abstract garbage collection push the boundaries of what we can learn about programs statically. This work illuminates and poses solutions to theoretical and practical challenges that stand in the way of combining the power of these techniques. Pushdown flow analysis grants unbounded yet computable polyvariance to the analysis of return-flow in higher-order programs. Abstract garbage collection grants unbounded polyvariance to abstract addresses which become unreachable between invocations of the abstract contexts in which they were created. Pushdown analysis solves the problem of precisely analyzing recursion in higher-order languages; abstract garbage collection is essential in solving the """"stickiness"""" problem. Alone, our benchmarks demonstrate that each method can reduce analysis times and boost precision by orders of magnitude. We combine these methods. The challenge in marrying these techniques is not subtle: computing the reachable control states of a pushdown system relies on limiting access during transition to the top of the stack; abstract garbage collection, on the other hand, needs full access to the entire stack to compute a root set, just as concrete collection does. Conditional pushdown systems were developed for just such a conundrum, but existing methods are ill-suited for the dynamic nature of garbage collection. We show fully precise and approximate solutions to the feasible paths problem for pushdown garbage-collecting control-flow analysis. Experiments reveal synergistic interplay between garbage collection and pushdown techniques, and the fusion demonstrates """"better-than-both-worlds"""" precision."""		J. Ian Johnson;Ilya Sergey;Christopher Earl;Matthew Might;David Van Horn	2014	J. Funct. Program.	10.1017/S0956796814000100	computer science;theoretical computer science;distributed computing;programming language;algorithm	PL	-18.80794778401165	29.60694768627647	179294
01b4a073b769992ae395a7aec9402f80acc74daa	forasec: formal analysis of security vulnerabilities in sequential circuits		Security vulnerability analysis of Integrated Circuits using conventional design-time validation and verification techniques is generally a computationally intensive task and incomplete by nature, under limited resources and time. To overcome this limitation, we propose a novel methodology based on model checking to formally analyze security vulnerabilities in sequential circuits considering side-channel parameters like propagation delay, switching and leakage power. In particular, we present a novel algorithm to efficiently partition the state-space into corresponding smaller state-spaces for faster security analysis of complex sequential circuits and thereby mitigating the associated state-space explosion due to their feedback loops. We analyze multiple ISCAS89 and trust-hub benchmarks to demonstrate the efficacy of our framework in identifying security vulnerabilities.		Faiq Khalid;Imran Hafeez Abbassi;Semeen Rehman;Osman Hasan;Muhammad Shafique	2018	CoRR			Security	-14.586799945363104	29.02514939184389	179664
0e85d1a721b755180342aa96aeca7ea0c1d130f2	mechanical verification of automatic synthesis of fault-tolerant programs	tolerancia falta;developpement logiciel;teoria demonstracion;theorie preuve;formal specification;automatic proving;fault tolerant;proof theory;metodo formal;methode formelle;program transformation;demostracion automatica;logical programming;specification programme;transformation programme;program synthesis;formal method;specification formelle;theorem proving;demonstration automatique;especificacion formal;demonstration theoreme;theorem prover;transformacion programa;fault tolerant system;formal verification;programmation logique;desarrollo logicial;fault tolerance;software development;sistema tolerando faltas;verification formelle;systeme tolerant les pannes;formal specication;demostracion teorema;program specification;programacion logica;tolerance faute;especificacion programa	Fault-tolerance is a crucial property in many systems. Thus, mechanical verification of algorithms associated with synthesis of faulttolerant programs is desirable to ensure their correctness. In this paper, we present the mechanized verification of algorithms that automate the addition of fault-tolerance to a given fault-intolerant program using the PVS theorem prover. By this verification, not only we prove the correctness of the synthesis algorithms, but also we guarantee that any program synthesized by these algorithms is correct by construction. Towards this end, we formally define a uniform framework for formal specification and verification of fault-tolerance that consists of abstract definitions for programs, specifications, faults, and levels of fault-tolerance, so that they are independent of platform and architecture. The essence of synthesis algorithms involves fixpoint calculations. Hence, we also develop a reusable library for fixpoint calculations on finite sets in PVS.	algorithm;alternating bit protocol;atomicity (database systems);automated theorem proving;byzantine fault tolerance;consensus (computer science);correctness (computer science);deterministic algorithm;fail-safe;fault tolerance;fault-tolerant computer system;fixed point (mathematics);formal specification;formal verification;library (computing);mask (computing);nondeterministic algorithm;prototype verification system;token ring;verification and validation	Sandeep S. Kulkarni;Borzoo Bonakdarpour;Ali Ebnenasir	2004		10.1007/11506676_3	fault tolerance;formal methods;computer science;theoretical computer science;high-level verification;programming language;algorithm;functional verification	Logic	-16.006133260782036	27.277457084680144	180006
49414eb752fd35f6c5726c81f82d451f53e1859b	verification of asynchronous circuits by bdd-based model checking of petri nets	decision diagram;boolean function;speed independent;asynchronous circuit;model checking;petri net;reachability analysis;binary decision diagram	This paper presents a methodology for the veriication of speed-independent asynchronous circuits against a Petri net speciica-tion. The technique is based on symbolic reachability analysis, modeling both the speciication and the gate-level network behavior by means of boolean functions. These functions are eeciently handled by using Binary Decision Diagrams. Algorithms for verifying the correctness of designs, as well as several circuit properties are proposed. Finally, the applicability of our veriication method has been proven by checking the correctness of diierent benchmarks.	algorithm;benchmark (computing);binary decision diagram;correctness (computer science);model checking;petri net;reachability;verification and validation	Oriol Roig;Jordi Cortadella;Enric Pastor	1995		10.1007/3-540-60029-9_50	model checking;discrete mathematics;influence diagram;stochastic petri net;asynchronous circuit;computer science;theoretical computer science;distributed computing;boolean function;binary decision diagram;petri net;algorithm	EDA	-14.371162356798871	28.088716558064785	180353
31d5f3acc4b75c836ab1fcf8d9e74a27bde8d087	detecting temporal logic predicates in distributed programs using computation slicing	distributed system;metodo polinomial;systeme reparti;logica temporal;runtime verification;relation ordre partiel;tranchage;espacio ordenado;logique arbre calcul;temps polynomial;temporal logic;espace etat;distributed computing;distributed programs;abstraction;program verification;satisfiability;abstraccion;analisis programa;formal method;logica ctl;polynomial time algorithm;verificacion programa;slicing;sistema repartido;state space method;methode espace etat;polynomial method;partial ordering;state space;algorithme reparti;espace ordonne;predicate detection;polynomial time;chapeado;calculo repartido;ordered space;algoritmo repartido;relacion orden parcial;program analysis;state explosion;analyse programme;espacio estado;methode polynomiale;verification programme;distributed algorithm;calcul reparti;logique temporelle;branching temporal logic ctl;metodo espacio estado;tiempo polinomial;partial order	Detecting whether a finite execution trace (or a computation ) of a distributed program satisfies a given predicate, calle d predicate detection, is a fundamental problem in distribut ed systems. It finds applications in many domains such as test ing, debugging, and monitoring of distributed programs. Howeve r pr dicate detection suffers from the state explosion prob lem – the number of possible global states of the program increase s exponentially with the number of processes. To solve this problem, we generalize an effective abstracti on technique calledcomputation slicing . We present polynomialtime algorithms to compute slices with respect to temporal l ogic predicates from a “regular” subset of CTL, that contain s temporal operators EF, EG, and AG. Furthermore, we show that ese slices contain precisely those global states of the original computation that satisfy the predicate. Using temporal predicate slices, we give an efficient (polyn mial in the number of processes) predicate detection algor ithm for a subset of CTL that we call regular CTL. Regular CTL conta ins nested temporal predicates for which, to the best of our knowledge, there did not previously exist efficient pred icate detection algorithms. Then we show that we can enlarge the subset of CTL and still obtain effective results. Our alg orithm has been implemented as part of a tool for analysis of distributed programs. We illustrate the effectiveness of o ur techniques on several protocols achieving speedups of ov er three orders of magnitude in one example, compared to partial orde r state-space search of SPIN. Furthermore, we were able to complete the verification for 250 processes for a partial ord er trace.	advanced configuration and power interface;algorithm;bit slicing;computation;debugging;entity framework;eurographics;object-relational database;regular expression;spin;sensor;state space search;temporal logic	Alper Sen;Vijay K. Garg	2003		10.1007/978-3-540-27860-3_17	partially ordered set;functional predicate;distributed algorithm;computation tree logic;computer science;hard-core predicate;distributed computing;ctl*;algorithm	Logic	-15.92818423961862	26.91612585365754	180483
7dded96f515d6afde3a664c3c3ec74f158e1281b	a quantifier-free smt encoding of non-linear hybrid automata	continuous systems;computability;formal verification automata theory computability continuous systems discrete systems encoding;standard linear case quantifier free smt encoding nonlinear hybrid automata embedded systems feature integrated discrete dynamics feature integrated continuous dynamics complexity source time invariants continuous transition satisfiability modulo theory based emerging techniques hybrid systems verification discrete reasoning first order theories quantifier free theories linear hybrid automata transition systems quantifier elimination procedures sequential nature continuous evolution enforcing discrete time points;formal verification;automata theory;discrete systems;encoding polynomials automata design automation standards embedded systems electronic mail;encoding	Hybrid systems are a clean modeling framework for embedded systems, which feature integrated discrete and continuous dynamics. A well-known source of complexity comes from the time invariants, which represent an implicit quantification of a constraint over all time points of a continuous transition. Emerging techniques based on Satisfiability Modulo Theory (SMT) have been found promising for the verification and validation of hybrid systems because they combine discrete reasoning with solvers for first-order theories. However, these techniques are efficient for quantifier-free theories and the current approaches have so far either ignored time invariants or have been limited to linear hybrid automata1. In this paper, we propose a new method that encodes a class of hybrid systems into transition systems with quantifier-free formulas. The method does not rely on expensive quantifier elimination procedures. Rather, it exploits the sequential nature of the transition system to split the continuous evolution enforcing the invariants on the discrete time points. This pushes the application of SMT-based techniques beyond the standard linear case.	automata theory;embedded system;first-order predicate;hybrid automaton;hybrid system;invariant (computer science);modulo operation;nonlinear system;quantifier (logic);transition system;verification and validation	Alessandro Cimatti;Sergio Mover;Stefano Tonetta	2012	2012 Formal Methods in Computer-Aided Design (FMCAD)		formal verification;computer science;theoretical computer science;discrete system;automata theory;computability;algorithm;encoding	Logic	-12.932632081255253	26.723376512514058	183422
043376d0454c003b642463e37ac7ca99fd838819	statically validating must summaries for incremental compositional dynamic test generation	static checking;regression testing;verification condition generator;search algorithm;control flow graph;theorem prover;symbolic execution;test generation;static analysis;automated theorem proving	Compositional dynamic test generation can achieve significant scalability by memoizing symbolic execution sub-paths as test summaries. In this paper, we formulate the problem of statically validating symbolic test summaries against code changes. Summaries that can be proved still valid using a static analysis of a new program version do not need to be retested or recomputed dynamically. In the presence of small code changes, incrementality can considerably speed up regression testing since static checking is much cheaper than dynamic checking and testing. We provide several checks ranging from simple syntactic ones to ones that use a theorem prover. We present preliminary experimental results comparing these approaches on three large Windows applications.	automated theorem proving;memoization;microsoft research;regression testing;scalability;static program analysis;symbolic execution	Patrice Godefroid;Shuvendu K. Lahiri;Cindy Rubio-González	2011		10.1007/978-3-642-23702-7_12	computer science;theoretical computer science;automated theorem proving;programming language;algorithm	SE	-19.037627538237395	29.42011421787377	183582
9a4707c8b7efd69c2eeb5b37a35d91482539b894	cunf: a tool for unfolding and verifying petri nets with read arcs		Cunf is a tool for building and analyzing unfoldings of Petri nets with read arcs. An unfolding represents the behaviour of a net by a partial order, effectively coping with the state-explosion problem stemming from the interleaving of concurrent actions. C-net unfoldings can be up to exponentially smaller than Petri net unfoldings, and recent work proposed algorithms for their construction and verification. Cunf is the first implementation of these techniques, it has been carefully engineered and optimized to ensure that the theoretical gains are put into practice.	arcs (computing);algorithm;forward error correction;net (polyhedron);petri net;stemming;unfolding (dsp implementation)	César Rodríguez;Stefan Schwoon	2013		10.1007/978-3-319-02444-8_42	real-time computing;computer science;distributed computing;petri net;algorithm	Logic	-16.724344383789255	28.39008356225319	183693
afb978379b7e6f02c88a8820d6c3c534f9c5f867	correct compilation of specifications to deterministic asynchronous circuits	verification;speed independent;power method;asynchronous circuit;concurrency;speed independent circuits;compilation;asynchronous circuits	Powerful methods have been developedby A. Martin and others whereby asynchronous circuits may be automatically constructed by starting from high-level speciications and incremen-tally transformingthem into asynchronous circuits. In this paper we make the informal arguments for the correctness of this compilation process mathematically rigorous. With rigorously justiied transformations, speciications may be translated into circuits that provably meet their speciica-tion. A full proof of the correctness of the circuit compiler is given. Other results of independent interest include: the process model takes fairness of gates into account, hazard-freeness is formally deened, and all hazard-free circuits constructed solely of and, or, not gates and C elements are proven to behave deterministically to any outside observer. A novel notion of equivalence is used to justify the correctness of the compiler.	asynchronous circuit;compiler;correctness (computer science);fairness measure;high- and low-level;process modeling;turing completeness	Scott F. Smith;Amy E. Zwarico	1995	Formal Methods in System Design	10.1007/BF01384076	asynchronous system;computer architecture;parallel computing;real-time computing;verification;concurrency;asynchronous circuit;power iteration;computer science	PL	-15.590009360760495	28.57821087334441	183967
2aaa81a8ef136f188774d9ccbcf9ddab574d6d87	temporal verification of behavioral descriptions in vhdl	circuit cad;formal verification;specification languages;temporal logic;vhdl;vhsic hardware description language;behavioral descriptions;behavioral models;behavioral specifications;control flow;reified temporal logics;temporal scheduling	This paper presents an approach for verifying the temporal scheduling of behavioral models of VHDL. The aim is to verifi that the controljlow of a behavioral description satisfies its behavioral specifications described in a formalism based on reified temporal logics and on a notion of physical activity.	formal system;reification (computer science);scheduling (computing);vhdl;verification and validation	Djamel Boussebha;Norbert Giambiasi;Janine Magnier	1992			behavioral modeling;real-time computing;strategic planning;temporal logic;formal verification;vhdl;computer science;theoretical computer science;pressing;very-large-scale integration;physical fitness;programming language;control flow;satisfiability	Logic	-15.817320690089211	29.0254231764194	184559
c69e32c048bc046cf8f2fc4a799c42c21a32b34e	extended rtl in the specification and verification of an industrial press	qa 76 software;computer programming	 . Extended Real Time Logic (ERTL) is proposed for the modellingand analysis of hybrid systems, taking as a basis Real Time Logic(RTL). RTL is a first order logic with uninterpreted predicates which relateevents of a system to the time of their occurrence, thereby providingthe means for reasoning about the absolute timing properties of real-timesystems. The extensions provided by ERTL allow reasoning about systembehaviour in both value and time domains by parametrising predicatesin terms... 		Rogério de Lemos;Jon G. Hall	1995		10.1007/BFb0020939	computer architecture;verification;software verification;computer science;software engineering;programming language	EDA	-18.068395051902996	27.810238281834987	184768
a92db24a45893dcee3dc2fa1c502a59d1843a30f	property dependent abstraction of control structure for software verification	developpement logiciel;algorithmic construction of abstract semantics;formal specification;algorithmique;generic model;system modeling;software verification;software systems;property oriented abstraction;program verification;formal method;specification formelle;especificacion formal;verificacion programa;formal verification;model checking;control structure;algorithmics;algoritmica;desarrollo logicial;state space;software development;verification formelle;state explosion;verification programme	In this paper we present a technique to compute abstract models for formal system verification. The method reduces the state space by eliminating those parts of a system model which are not required to check a property. The abstract model depends on the property, which is a formula of the next-less fragment of CTL∗. The algorithm reads a system description, annotates it with abstract sub-models for the statements, which are finally composed as abstract model for the system. In the paper we introduce the core algorithm and illustrate it by an example.	abstraction (software engineering);algorithm;embedded system;formal system;heuristic (computer science);imperative programming;iterative method;liveness;model checking;programming language;recursion;regular semantics;software verification;state space;subject reduction;unreachable memory;usability	Thomas Firley;Ursula Goltz	2002		10.1007/3-540-45614-7_29	model checking;formal methods;systems modeling;formal verification;software verification;computer science;state space;theoretical computer science;software development;formal specification;programming language;control flow;algorithmics;algorithm;software system	Logic	-17.43412389127602	27.52691497788019	184775
0e20b6abe027a3514cc8d538f204d26ce158c4ff	formal verification of counterflow pipeline architecture	asynchronous system;theorem prover;formal verification;automata theory;higher order logic	Some properties of the Sproull counterflow pipeline architecture are formally verified using automata theory and higher order logic in the HOL theorem prover. The proof steps are presented. Despite the pipeline being a non-deterministic asynchronous sys tem, the verification proceeded with minimal time and effort. Because this work is directly associated with the asynchronous processor design technology currently being investigated in the Labs, this report was printed as a courtesy by Sun Microsystems Laboratories.	formal verification;pipeline (computing)	Paul Loewenstein	1995		10.1007/3-540-60275-5_70	asynchronous system;formal methods;higher-order logic;formal verification;computer science;theoretical computer science;automata theory;high-level verification;automated theorem proving;runtime verification;programming language;intelligent verification;algorithm	Arch	-16.246322246669436	28.457500711045274	185389
ac85bb1359ef20cb149970539d44a5b400772e80	model checking multirate hybrid systems with restricted convex polyhedron	automatic verification;infinite state space model checking multirate hybrid system restricted convex polyhedron automatic system verification constraint system multirate zone multirate hybrid automata state spaces timed computation tree logic;clocks;multirate hybrid automata;computer model;trees mathematics;automata;automata clocks cost accounting computational modeling reachability analysis labeling atmospheric modeling;cost accounting;computation tree logic;computational modeling;formal verification;model checking;timed computation tree logic hybrid systems model checking multirate hybrid automata;state space;finite automata;hybrid system;formal logic;timed computation tree logic;rational number;constraint system;trees mathematics finite automata formal logic formal verification;hybrid automata;atmospheric modeling;reachability analysis;labeling;hybrid systems	Model checking is a promising and powerful approach to automatic verification of systems. To deal with the model checking issue of multirate hybrid systems, a constraint system called multirate zone is formalized for the representation and manipulation of multirate hybrid automata state-spaces. A multirate zone is a restricted convex polyhedron represented by a conjunction of inequalities comparing either a variable value or a linear expression of two variables to a rational number. Model checking procedures for multirate hybrid systems based on timed computation tree logic are given. The Multirate zone is proved to be closed to the operations required in these model checking procedures, which enables it to be used as the basis for the infinite state-space exploring of multirate hybrid automata.	automata theory;computation tree logic;hybrid automaton;hybrid system;model checking;polyhedron;state space	Haibin Zhang	2011	2011 Fifth International Conference on Theoretical Aspects of Software Engineering	10.1109/TASE.2011.21	computer science;theoretical computer science;finite-state machine;programming language;algorithm;hybrid system	Logic	-11.881573795976642	25.6307443262967	186252
eccc5dbc63b11f0016c7824f575ae119d72a089e	variable reuse for efficient image computation	modelizacion;metodo polinomial;concepcion asistida;computer aided design;diseno circuito;optimisation;analyse symbolique;diagrama binaria decision;diagramme binaire decision;optimum;ordered set;temps polynomial;optimizacion;reutilizacion;analisis simbolico;relacion orden;ramasse miettes;circuit design;metodo formal;accounting;methode formelle;ordering;ensemble ordonne;comptabilite;reuse;formal method;recogemigas;modelisation;relation ordre;polynomial method;optimo;garbage collector;polynomial time;conception assistee;identificateur;optimization;conception circuit;contabilidad;symbolic analysis;methode polynomiale;modeling;reachability analysis;identificador;reutilisation;analyse atteignabilite;conjunto ordenado;identifier;binary decision diagram;tiempo polinomial	Image computation, that is, computing the set of states reachable from a given set in one step, is a crucial component in typical tools for BDD-based symbolic reachability analysis. It has been shown that the size of the intermediate BDDs during image computation can be dramatically reduced via conjunctive partitioning of the transition relation and ordering the conjuncts for facilitating early quantification. In this paper, we propose to enhance the effectiveness of these techniques by reusing the quantified variables. Given an ordered set of conjuncts, if the last conjunct that uses a variable u appears before the first conjunct that uses another variable v, then v can be renamed to u, assuming u will be quantified immediately after its last use. In general, multiple variables can share the same identifier so the BDD nodes that are inactive but not garbage collected may be activated. We give a polynomial-time algorithm for generating the optimum number of variables that are required for image computation and show how to modify the image computation accounting for variable reuse. The savings for image computation are demonstrated on ISCAS'89 and Texas'97 benchmark models.	algorithm;benchmark (computing);binary decision diagram;computation;garbage collection (computer science);identifier;quantifier (logic);reachability;time complexity	Zijiang Yang;Rajeev Alur	2004		10.1007/978-3-540-30494-4_30	time complexity;discrete mathematics;formal methods;systems modeling;identifier;order theory;computer science;theoretical computer science;computer aided design;circuit design;reuse;mathematics;symbolic data analysis;garbage collection;programming language;binary decision diagram;algorithm	AI	-15.561530661011567	28.10902057955668	187292
17c3aae5a5a19088bd120708476c762539040b4f	generating properties for runtime monitoring from software specification patterns	runtime verification;property patterns;temporal logic;java mac;property specification;formal software specifications;future interval logic;runtime monitoring;software specification;run time verification	The paper presents an approach to support run-time verification of software systems that combines two existing tools, Prospec and Java-MaC, into a single framework. Prospec can be used to clarify natural language specifications for sequential, concurrent, and nondeterministic behavior. In addition, the tool assists the user in reading, writing, and understanding formal specifications through the use of property patterns and visual abstractions. Currently, Prospec automatically generates a specification written in Future Interval Logic (FIL). The goal is to automate the generation of MEDL formulas that can be used by the Java-MaC tool to check run-time compliance of system execution to properties. The paper describes the mapping that translates FIL formulas into MEDL formulas and demonstrates its correctness.	algorithm;correctness (computer science);formal language;formal specification;interval temporal logic;java;natural language;run time (program lifecycle phase);software system;specification language	Oscar Mondragon;Ann Q. Gates;Humberto Mendoza;Oleg Sokolsky	2005	International Journal of Software Engineering and Knowledge Engineering	10.1142/S021819400700315X	software requirements specification;real-time computing;temporal logic;computer science;artificial intelligence;theoretical computer science;formal specification;runtime verification;programming language	SE	-18.482966490453165	27.627508674513653	187351
44b5182e9f3e9a0085290828a35ca0d5cb364aed	formalizing push-relabel algorithms		We present a formalization of push-relabel algorithms for computing the maximum flow in a network. We start with Goldberg’s et al. generic push-relabel algorithm, for which we show correctness and the time complexity bound of O(V E). We then derive the relabel-tofront and FIFO implementation. Using stepwise refinement techniques, we derive an efficient verified implementation. Our formal proof of the abstract algorithms closely follows a standard textbook proof, and is accessible even without being an expert in Isabelle/HOL— the interactive theorem prover used for the formalization.		Peter Lammich;S. Reza Sefidgar	2017	Archive of Formal Proofs		machine learning;artificial intelligence;computer science	Theory	-15.40871706237089	25.85910788310272	188038
542c26c828d3a4f79fdadcf98286f3cf82819cab	translation validation of loop invariant code optimizations involving false computations		Code motion based optimizations are used quite often in electronic design automation (EDA) tools to improve the quality of synthesis results. Ensuring the correctness of such transformation is necessary for reliability of EDA tools. A value propagation (VP) based equivalence checking method of finite state machine with datapaths (FSMD) was proposed in [1] to specifically verify code motion across loops. In this work, we identify some scenarios involving loop invariant code motion where the VP based equivalence checking method fails to establish the equivalence between two actually equivalent FSMDs. We propose an enhancement over the VP based equivalence checking method [1] to overcome this limitation. Experimental results demonstrate that our method can handle the scenario where the VP based equivalence checking method fails.	computation;control flow;correctness (computer science);electronic design automation;finite state machine with datapath;finite-state machine;formal equivalence checking;loop invariant;loop-invariant code motion;newton's method;optimizing compiler;software propagation;solver;turing completeness;z3 (computer)	Ramanuj Chouksey;Chandan Karfa;Purandar Bhaduri	2017		10.1007/978-981-10-7470-7_72	real-time computing;finite-state machine;finite state machine with datapath;formal equivalence checking;equivalence (measure theory);loop-invariant code motion;theoretical computer science;computer science;correctness;loop invariant;formal verification	EDA	-15.33271623409038	29.07435242836493	188345
428438c2bc63b6eb4565a9e0c4ce13b06d8d47a1	beyond the weakly hard model: measuring the performance cost of deadline misses		Most works in schedulability analysis theory are based on the assumption that constraints on the performance of the application can be expressed by a very limited set of timing constraints (often simply hard deadlines) on a task model. This model is insufficient to represent a large number of systems in which deadlines can be missed, or in which late task responses affect the performance, but not the correctness of the application. For systems with a possible temporary overload, models like the m-K deadline have been proposed in the past. However, the m-K model has several limitations since it does not consider the state of the system and is largely unaware of the way in which the performance is affected by deadline misses (except for critical failures). In this paper, we present a state-based representation of the evolution of a system with respect to each deadline hit or miss event. Our representation is much more general (while hopefully concise enough) to represent the evolution in time of the performance of time-sensitive systems with possible time overloads. We provide the theoretical foundations for our model and also show an application to a simple system to give examples of the state representations and their use. 2012 ACM Subject Classification Computer systems organization → Embedded software	correctness (computer science);embedded software;scheduling analysis real-time systems	Paolo Pazzaglia;Luigi Pannocchi;Alessandro Biondi;Marco Di Natale	2018		10.4230/LIPIcs.ECRTS.2018.10	computer science;real-time computing;distributed computing	Embedded	-12.927034799192008	29.890279362811544	189498
9d9f19230ff5cf302b6b539fcffdead963fbcd25	synthesizing imperative programs for introductory programming assignments		We present a novel algorithm that synthesizes imperative programs for introductory programming courses. Given a set of input-output examples and a partial program, our algorithm generates a complete program that is consistent with every example. Our key idea is to combine enumerative program synthesis and static analysis, which aggressively prunes out a large search space while guaranteeing to find, if any, a correct solution. We have implemented our algorithm in a tool, called SIMPL, and evaluated it on 30 problems used in introductory programming courses. The results show that SIMPL is able to solve the benchmark problems in 6.6 seconds on average.	algorithm;benchmark (computing);imperative programming;program synthesis;simpl;static program analysis	Sunbeom So;Hakjoo Oh	2017	CoRR		theoretical computer science;computer science;program synthesis;programming language;static analysis	PL	-16.423993776060026	26.016854835497327	189553
b31964cd53e903e39d6e8bee81f1a99238ed5195	transforming sos specifications to linear processes	labeled transition system;higher order;structured operational semantics;linear process;process algebra	This paper describes an approach to transform a Structural Operational Semantics given as a set of deduction rules to a Linear Process Specification. The transformation is provided for deduction rules in De Simone format, including predicates. The Linear Process Specifications are specified in the syntax of the mCRL2 language, that, with help of the underlying (higher-order) re-writer/tool-set, can be used for simulation, labeled transition system generation and verification of behavioral properties. We illustrate the technique by showing the effect of the transformation from the Structural Operational Semantics specification of a simple process algebra to a Linear Process Specification.	mcrl2;natural deduction;operational semantics;process calculus;process specification;simulation;system generation;transition system	Frank P. M. Stappers;Michel A. Reniers;Sven Weber	2011		10.1007/978-3-642-24431-5_15	process calculus;higher-order logic;computer science;theoretical computer science;programming language;operational semantics;algorithm	PL	-17.72699522508598	26.70604842123146	189941
747c36bc6cd513e6421a8154c278b4e2aeebd96e	a framework for testing safety and effective computability	fiabilidad;reliability;bottom up;metodologia;generalized magic set;securite;computability;algoritmo recursivo;coaccion;contrainte;adorned program;graph clique;program transformation;estrategia;ejecucion programa;magic set transformation;logical programming;program verification;transformation programme;methodologie;program execution;monotonie;analisis programa;connected graph;strategy;verificacion programa;transformacion programa;constraint;algorithme recursif;programmation logique;execution programme;fiabilite;calculabilite;monotonicity;safety;recursive algorithm;proof of correctness;program analysis;monotonia;logic programs;clique graphe;analyse programme;methodology;verification programme;seguridad;programacion logica;strategie;magic program;adorned rule set;calculabilidad	This paper presents a methodology for testing a general logic program containing function symbols and built-in predicates for safety and effective computability. Safety is the property that the set of answers for a given query is finite. A related issues is whether the evaluation strategy can effectively compute all answers and terminate. We consider these problems under the assumption that queries are evaluated using a bottom-up fixpoint computation. We also approximate the use of function symbols by considering Datalog programs with infinite base relations over which finiteness constraints and monotonicity constraints are considered. One of the main results of this paper is a recursive algorithm, check_clique, to test the safety and effective computability of predicates in arbitrarily complex cliques. This algorithm takes certain procedures as parameters, and its applicability can be strengthened by making these procedures more sophisticated. We specify the properties required of these procedures precisely, and present a formal proof of correctness for algorithm check_clique. This work provides a framework for testing safety and effective computability of recursive programs, and is based on a clique by clique analysis. The results reported here form the basis of the safety testing for the LDL language, being implemented at MCC.	approximation algorithm;bottom-up parsing;canonical account;clique (graph theory);computability;computable function;computation;correctness (computer science);datalog;fixed point (mathematics);formal proof;logic programming;microelectronics and computer technology corporation;recursion (computer science);terminate (software);top-down and bottom-up design	Ravi Krishnamurthy;Raghu Ramakrishnan;Oded Shmueli	1988	J. Comput. Syst. Sci.	10.1006/jcss.1996.0009	program analysis;clique;combinatorics;discrete mathematics;monotonic function;strategy;computer science;connectivity;top-down and bottom-up design;methodology;reliability;mathematics;computability;constraint;algorithm;recursion	DB	-15.198996582416054	27.457713857332106	190654
02afb1a4a45cfea0c1aeffca4a441e6541a5d34b	state of the art: dynamic symbolic execution for automated test generation	dynamic symbolic execution;automated test generation;prospects;high code coverage;low false positives;challenges and solutions;tools analysis	Dynamic symbolic execution for automated test generation consists of instrumenting and running a program while collecting path constraint on inputs from predicates encountered in branch instructions, and of deriving new inputs from a previous path constraint by an SMT (Satisfiability Modulo Theories) solver in order to steer next executions toward new program paths. It has been introduced into several applications, such as automated test generation, automated filter generation andmalware analysismainly for its two intrinsic properties: low false positives and high code-coverage. In this paper, we focus on the topics that are closely related to automated test generation. Our contributions are five-fold. First, we summarize the theoretical foundation of dynamic symbolic execution. Second, we highlight the challenges when turning ideas into reality. Besides, we describe the state-of-the-art solutions including advantages and disadvantages for those challenges. In addition, twelve typical tools are analyzed and many properties of those tools are censused. Finally, we outline the prospects of this research field	code coverage;computation;constraint logic programming;external variable;instrumentation (computer programming);malware analysis;mathematical optimization;parallel computing;pointer (computer programming);rewriting;satisfiability modulo theories;software testing;solver;symbolic execution;test automation	Ting Chen;Xiaosong Zhang;Shi-ze Guo;Hong-yuan Li;Yue Wu	2013	Future Generation Comp. Syst.	10.1016/j.future.2012.02.006	parallel computing;real-time computing;computer science;artificial intelligence;theoretical computer science;operating system;database;distributed computing;programming language;concolic testing;algorithm	SE	-18.61624715696865	28.969402234404157	191281
5aa25e7f641201be403c5dbe3a661e78f2371fd8	stressing symbolic scheduling techniques within aircraft maintenance optimization	coloured petri net;cost function;satisfiability;boolean satisfiability;high level synthesis;scheduling;planning;timed automata;petri nets;sat solver;petri net;sat solvers;real time systems;binary decision diagram	Scheduling, or planning, is widely recognized as a very important step in several domains such as high level synthesis, real-time systems, and every-day applications. Given a problem described by a number of actions and their relationships, finding a schedule, or a plan, means to find a way to perform all the actions minimizing a specific cost function. The goal of this paper is to develop, analyze and compare different scheduling techniques on a new scheduling/planning problem. The new application domain is aircraft maintenance. It shares with previous ones the underlying problem definition, but it also unveils brand new challenging characteristics, and a different optimization target. We show how to model the problem in a suitable way, and how to solve it with different methodologies going from Satisfiability solvers and Binary Decision Diagrams, to Timed Automata and Coloured Petri Nets. New ideas are put forward in the different domains having efficiency and scalability as main targets. Experimental results stress the different techniques, showing their application range and limits, and defining advantages and disadvantages of the underlying models. Overall, general-purpose tools have been easily applied to our problem, but failed as far as efficiency was concerned. The satisfiability-based approach proved to be faster and more scalable, being able to solve instances 3− 4 times larger. To sum up, our contributions range from modeling the aircraft maintenance problem as a scheduling instance, to coding this problem with home-made and general-purpose tools, to dovetailing exact and heuristic techniques, and comparing these techniques in terms of efficiency and scalability.	application domain;approximation algorithm;automated planning and scheduling;binary decision diagram;boolean satisfiability problem;central processing unit;coloured petri net;general-purpose modeling;heuristic;high-level programming language;high-level synthesis;hoc (programming language);loss function;mathematical optimization;performance;real-time clock;real-time computing;scalability;schedule (computer science);scheduling (computing);timed automaton	Viviana Bruno;Luz Garcia;Sergio Nocco;Stefano Quer	2008	JSAT		mathematical optimization;real-time computing;computer science;theoretical computer science;boolean satisfiability problem;petri net;algorithm	AI	-12.903705812307422	30.872958476081184	191400
24153752e1e35b5bc068fe46ea6c98c3c074a12b	program verification as probabilistic inference	verification;modelizacion;teoria demonstracion;lenguaje programacion;graphic method;belief networks;reseau croyance;forward and backward analysis;chaine markov;mise a jour;cadena markov;theorie preuve;metodo monte carlo;learning;programming language;proof theory;factor graph;automated recovery;echantillonnage gibbs;gibbs sampling;methode monte carlo;abstraction;intelligence artificielle;probabilistic approach;program verification;abstraccion;probabilistic inference;approche deterministe;actualizacion;deterministic approach;modelisation;verificacion programa;automated recovery machine;methode graphique;machine learning;markov chain monte carlo;enfoque probabilista;approche probabiliste;monte carlo method;theory;over and under approximation;inferencia;enfoque determinista;langage programmation;graphical model;metodo grafico;invariante;artificial intelligence;algorithms;proof of correctness;inteligencia artificial;factor graphs;muestreo gibbs;verification programme;modeling;invariant;probleme direct;inference;problema directo;updating;belief network;direct problem;markov chain	In this paper, we propose a new algorithm for proving the validity or invalidity of a pre/postcondition pair for a program. The algorithm is motivated by the success of the algorithms for probabilistic inference developed in the machine learning community for reasoning in graphical models. The validity or invalidity proof consists of providing an invariant at each program point that can be locally verified. The algorithm works by iteratively randomly selecting a program point and updating the current abstract state representation to make it more locally consistent (with respect to the abstractions at the neighboring points). We show that this simple algorithm has some interesting aspects: (a) It brings together the complementary powers of forward and backward analyses; (b) The algorithm has the ability to recover itself from excessive under-approximation or over-approximation that it may make. (Because the algorithm does not distinguish between the forward and backward information, the information could get both under-approximated and over-approximated at any step.) (c) The randomness in the algorithm ensures that the correct choice of updates is eventually made as there is no single deterministic strategy that would provably work for any interesting class of programs. In our experiments we use this algorithm to produce the proof of correctness of a small (but non-trivial) example. In addition, we empirically illustrate several important properties of the algorithm.	abstract interpretation;approximation algorithm;array data structure;backward compatibility;correctness (computer science);emoticon;experiment;formal verification;graphical model;graphical user interface;hidden variable theory;hoare logic;interprocedural optimization;invariant (computer science);machine learning;pointer (computer programming);postcondition;randomness;sampling (signal processing)	Sumit Gulwani;Nebojsa Jojic	2007		10.1145/1190216.1190258	forward algorithm;weighted majority algorithm;computer science;artificial intelligence;machine learning;factor graph;algorithm	PL	-17.0964415150887	26.640402803134613	191698
0165e4a0a0572ca2d60f99692c9d30f889325161	synthesising controllers from real-time specifications	concepcion asistida;computer aided design;controleur logique programmable;programmable controllers;controlador logica programable;integrated circuit;gestion production;calculus programmable control control system synthesis logic embedded system timing real time systems industrial control machinery production industries formal verification;implementation;temporal logic;real time;specification;methode formelle;controller;circuito integrado;invitation a emettre;computational complexity real time systems temporal logic logic cad formal verification programmable controllers automata theory calculus;indexing terms;interval temporal logic;duration calculus semantics controller synthesis real time specifications real time controllers interval temporal logic duration calculus programmable logic controller automata plc automata;production management;formal method;controller synthesis;ejecucion;formal verification;duration calculus;supervisor;programmable logical controller;especificacion;computational complexity;calculus;gestion produccion;temps reel;controleur;batch process;conception assistee;tiempo real;automata theory;verification formelle;polling;logic cad;programmable logic controller;circuit integre;invitacion a transmitir;real time systems;timing	This paper presents a novel source-level dynamic analysis methodology and tool for High-Level Synthesis (HLS). It not only for the first time enables HLS to offer source-level design debugging on the `synthesized' RTL designs, but also allows the designer to analyze their dynamic characteristics, such as resource utilization, power consumption, etc., at the algorithmic (source) level. This technology has been proven in the industry as the critical element for successfully designing a microcontroller with 300+ instructions with Matisse, an interactive HLS system. Additionally, we demonstrate the use of this technology for architectural power optimization.	critical graph;debugging;high-level synthesis;level design;mathematical optimization;microcontroller;power optimization (eda);real-time transcription	Henning Dierks	1997	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.739057	control engineering;embedded system;electronic engineering;real-time computing;formal methods;computer science;computer aided design;programmable logic controller;programming language;algorithm	EDA	-15.771804996171442	29.499828064137322	191717
95740e81ce369641bb1d909f7d134e145e466885	using abstract interpretation to define a strictness type inference system	reconfiguration;multimedia;user interface management system;direct manipulation;distributed computing;multiple program specialization;graphical user interfaces;logic programming;optimization;abstract interpretation;type inference;compile time analysis	This paper presents a general framework to develop strictness analysis combining the advantage of the type znference approach and the advantage of the abstract interpretation techniques. Strictness analysts has been initially developed by Alan Mycroft [MYc81] using the denotational semantics framework and abstract interpretation. Extension to higher order functions has been developed by Burn, Hankin and Abramsky [B HA85]. However the computation complexity is still very high and prevents from using this analysis with time critical application. To reduce the cost of strictness analysis, type inference techniques were suspected to allow the construction of simple and efficient algorithms. The innovative paper of Kuo and Mishra [KuoMis89] certainly was a promising step in this direction. The results of Hankin Iz Le M6tayer [HanM4t94], the one of Wright [Wri91] and the one of Solberg [SolNielNie194] achieved to convince the community that this approach leads to efficient algorithms. However, there is no generic method to define inference systems that infer strictness properties. More annoying are the fact that	abstract interpretation;algorithm;computation;denotational semantics;higher-order function;inference engine;schedule (computer science);strictness analysis;type inference	Bruno Monsuez	1995		10.1145/215465.215574	real-time computing;computer science;control reconfiguration;theoretical computer science;type inference;graphical user interface;programming language;logic programming	PL	-17.534789421997502	26.12028192045117	192240
eff9f41ca17a1de284abd8f6bf06767db58d1c22	parsing electronic circuits in a logic grammar	automatic circuit understanding;comprehension circuit;structure circuit;trees mathematics circuit analysis computing formal logic grammars;electronic circuit;hierarchical structure;understanding circuit;representacion conocimientos;causal analysis;sistema experto;parse trees;syntactic processing;top down;words;dcsg top down parsing mechanism;circuit design;circuito electronico;trouble shooting;logical programming;trees mathematics;indexing terms;function block;functional blocks circuit design trouble shooting structural analysis causal analysis automatic circuit understanding circuit structures sentence words logic grammar definite clause set grammar parse trees dcsg top down parsing mechanism hierarchical structures;functional blocks;hierarchical structures;grammars;electronic circuits logic circuits circuit analysis circuit simulation voltage computational modeling logic programming natural languages circuit synthesis knowledge representation;sentence;programmation logique;circuit structure;circuit structures;formal logic;systeme expert;logic programs;logic grammar;knowledge representation;traitement syntaxique;circuit analysis computing;structural analysis;representation connaissances;programacion logica;circuit electronique;definite clause set grammar;structure analysis;expert system	Understanding circuits is a prerequisite for circuit design and trouble shooting. Circuit understanding by engineers is described as a process that starts with a structural analysis and then proceeds to a causal analysis. As a step toward automatic circuit understanding, a method for analyzing circuit structures is presented. In this method, a circuit is reviewed as a sentence and its elements as words. Circuit structures are defined by rules written in a logic grammar called definite clause set grammar (DCSG). Given circuits are decomposed into parse trees by the DCSG top-down parsing mechanism. These parse trees represent hierarchical structures of functional blocks. This representation is presented as one step in the process of automatic understanding of circuit structures. >		Takushi Tanaka	1993	IEEE Trans. Knowl. Data Eng.	10.1109/69.219732	natural language processing;computer science;artificial intelligence;theoretical computer science;machine learning;structural analysis;programming language;expert system;algorithm	DB	-14.346610284987129	30.399069771772783	192327
384b4c45bf212be4544a1468a97368c2f8e4f170	automated verification of behavioral equivalence for microprocessors	automatic control;verification;microprocessors;concepcion asistida;computer aided design;all one polynomial;tamarack 3 microprocessor;general purpose microprocessor design;boolean functions;data path;prolog;sequential circuits;abstract data types;flip flops;false negative;synchronous sequential circuit;intelligence artificielle;logical programming;abstract data type;indexing terms;equivalence comportementale;nonterminating procedures;artificial intelligent;input output;redundant representation;interactive theorem prover;bit parallel multiplier;automata;automated verification;theorem prover;finite state machines;formal verification;logic programming;programmation logique;behavioral equivalence;incorrect implementations;data structures;synchronous sequential circuits;input output behavior;conception microprocesseur;workstations;type abstrait;finite field arithmetic;logic testing;ibm computers;microprocessors data structures boolean functions sequential circuits automatic control latches automata design methodology workstations computer errors;conception assistee;artificial intelligence;ibm rs 6000 workstation;tipo abstracto;circuit cad;microprocessor design;latches;inteligencia artificial;1 to 26 s automated hardware verification behavioral equivalence interactive theorem prover synchronous sequential circuits input output behavior data path data control register transfer level latches nonterminating procedures false negatives prolog ibm rs 6000 workstation tamarack 3 microprocessor general purpose microprocessor design hol logic programming incorrect implementations abstract data types artificial intelligence computer aided design finite state machines;data control;logic programs;verificacion;machine etat fini;hol;programacion logica;circuit sequentiel synchrone;register transfer level;1 to 26 s	Presents a new method for verifying, in a fully automated way, that two synchronous sequential circuits have the same input/output behavior. The method applies to designs in which a distinction between data path and control can be made, and in particular to microprocessors. The verification is carried out at the register-transfer level. In contrast with previous methods, our procedure is not limited by the total number of latches in the circuit: it runs in time that is independent of the width of the data path. A price has to be paid for this: the procedure does not always terminate, and may produce false negatives. We argue, however, that these problems should not come up when verifying general purpose microprocessors. We have implemented the procedure in Prolog on an IBM RS/6000 workstation, and have tried it on the Tamarack-3 microprocessor previously verified by J.J. Joyce (1990) with the interactive theorem prover HOL at the University of Cambridge. We have verified the equivalence of several alternative implementations to the original one, in times ranging from 11 to 26 s, and we have detected the errors in several incorrect implementations, in times ranging from 1 to 26 s. >	formal verification;microprocessor;turing completeness	Francisco Corella	1994	IEEE Trans. Computers	10.1109/12.250616	embedded system;computer architecture;parallel computing;computer science;electrical engineering;theoretical computer science;operating system;mathematics;finite-state machine;programming language;abstract data type;algorithm	EDA	-14.669640668534619	30.667656347583772	192922
0dfacb3fab6388c8ed5812bdc7b0395f04a766b1	debugging from high level down to gate level	specification languages c language computer debugging hardware software codesign logic gates;software;debugging;control systems;protocols;dependence analysis;concurrent computing;hardware software codesign;logic design;computer debugging;gate level debugging;post silicon debug high level design system level design dependence analysis equivalence checking;systemc level design;data mining;system on a chip;c based hardware designs;c language;logic synthesis tool;dependence analysis c based hardware designs logic synthesis tool specc system level design systemc level design high level debugging gate level debugging;logic synthesis;logic gates;high level debugging;specification languages;control system synthesis;graph representation;system level design;hardware design;debugging control system synthesis data mining hardware productivity algorithm design and analysis logic design system level design control systems concurrent computing;productivity;high level design;equivalence checking;specc system level design;algorithm design;algorithm design and analysis;concrete;hardware;post silicon debug	C-based hardware designs are now accepted as means to increase design productivity. Starting with rather algorithmic design descriptions, incremental refinements are applied to generate high-level synhesizable descriptions which are further processed by high-level and logic synthesis tools. C-based system level design descriptions, such as in SpecC [?] and SystemC [?], can give concise and global views on the behaviors of the designs as well as structures, and various types of dependencies, such as control, data, concurrency, and others, can be extracted quickly. These dependencies can be the bases for efficient and effective debugging for all levels of design descriptions. In this paper, graph representations for various dependencies which are extracted from C-based descriptions are introduced. Then techniques on their uses for debugging in various design levels are discussed. We present static and dynamic tracing methods for dependence analysis as well as techniques that try to establish mapping between implementations and C-based design descriptions.	concurrency (computer science);debugging;dependence analysis;high- and low-level;high-level programming language;level design;logic synthesis;spatial light modulator;specc;systemc	Masahiro Fujita;Yoshihisa Kojima;Amir Masoud Gharehbaghi	2009	2009 46th ACM/IEEE Design Automation Conference	10.1145/1629911.1630077	embedded system;algorithm design;computer architecture;parallel computing;logic synthesis;real-time computing;concurrent computing;computer science;programming language	EDA	-15.536224158759389	29.746894427948497	193132
b8c140fcaa821ff3b3bc5cadee09aed3bba4bcc9	using range-equivalent circuits for facilitating bounded sequential equivalence checking		This paper presents a method based on range-equivalent circuit technique for SAT-based bounded sequential equivalence checking. Given two sequential circuits to be verified, instead of straightforward unrolling the miter of two sequential circuits, we iteratively minimize the miter with a range-equivalent circuit technique before adding a new timeframe. This is because the previous timeframes can be considered as a pattern generator that feeds input patterns to the next timeframe. Experimental results show that the proposed method saved up to 91% of time for reaching the same bounded depth compared with previous work on IWLS2005 benchmarks.	benchmark (computing);boolean satisfiability problem;central processing unit;equivalent circuit;formal equivalence checking;pattern language;turing completeness;verification and validation	Yung-Chih Chen;Wei-An Ji;Chih-Chung Wang;Ching-Yi Huang;Chia-Cheng Wu;Chia-Chun Lin;Chun-Yao Wang	2018	2018 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)	10.1109/VLSI-DAT.2018.8373231	real-time computing;discrete mathematics;computer science;formal equivalence checking;miter joint;sequential logic;logic gate;benchmark (computing);digital pattern generator;bounded function;equivalent circuit	EDA	-13.4494692241651	26.741183783188745	193397
228f4cc0b71d25df53daa931c48b36a5c7e8a611	directed model checking for b: an evaluation and new techniques	industrial case study;tool support;b method;spin;search;model checking;industrial case studies;directed model checking;breadth first search;empirical evaluation	ProB is a model checker for high-level formalisms such as B, Event-B, CSP and Z. ProB uses a mixed depth-first/breadth-first search strategy, and in previous work we have argued that this can perform better in practice than pure depth-first or breadth-first search, as employed by low-level model checkers. In this paper we present a thorough empirical evaluation of this technique, which confirms our conjecture. The experiments were conducted on a wide variety of B and Event-B models, including several industrial case studies. Furthermore, we have extended ProB to be able to perform directed model checking, where each state is associated with a priority computed by a heuristic function. We evaluate various heuristic functions, on a series of problems, and find some interesting candidates for detecting deadlocks and finding specific target states.	b-method;breadth-first search;communicating sequential processes;deadlock;depth-first search;experiment;heuristic (computer science);high- and low-level;model checking;sensor;z notation	Michael Leuschel;Jens Bendisposto	2010		10.1007/978-3-642-19829-8_1	b-method;model checking;simulation;breadth-first search;computer science;theoretical computer science;spin;programming language;algorithm	SE	-17.60862782825055	27.99886330470369	193847
d44706b0a9cd2609e27ecadcd7c4fba22ef7b5ed	generating deterministic $\omega$-automata for most ltl formulas by the breakpoint construction		Temporal logics like LTL are frequently used for the specification and verification of reactive systems. To this end, LTL formulas are typically translated to nondeterministic Buchi automata so that the LTL verification problem is reduced to a nonemptiness problem of ω-automata. While nondeterministic automata are sufficient for this purpose, many other applications require deterministic ω-automata. Unfortunately, the known determinization procedures for Buchi automata like Safra’s procedure are extremely difficult to implement, and the currently available implementations are only able to handle very small examples. In this paper, we present a new symbolic translation of a remarkably large fragment of LTL formulas to equivalent deterministic ω-automata. Our method is based on (1) a syntactically defined fragment of the temporal logic LTL together with a linear-time translation procedure to equivalent nondeterministic symbolic ω-automata, and (2) a (semi)-symbolic determinization procedure for this fragment. The fragment that we consider is complete in the sense that every LTL formula is equivalent to a formula in this fragment, and in practice, we found that most formulas occurring in real specifications already belong to this fragment.	automaton;breakpoint	Andreas Morgenstern;Klaus Schneider;Sven Lamberti	2008			breakpoint;nondeterministic algorithm;büchi automaton;temporal logic;algorithm;mathematics;omega	Logic	-13.858041641823284	26.46282654989738	194363
09b1d6fc11ce8eb48a6d013f4a0423e1e5c9d47f	complexity issues in automated synthesis of failsafe fault-tolerance	index terms fault tolerance;program synthesis automated failsafe fault tolerance synthesis safety specification failsafe fault tolerant distributed programs np completeness monotonic specifications polynomial time program state space byzantine agreement distributed consensus atomic commitment distributed nonmasking fault tolerant programs formal methods;formal specification;automatic addition of fault tolerance;fault tolerant;distributed consensus;distributed processing;byzantine agreement;formal methods;distributed programs;software fault tolerance;indexing terms;program synthesis;satisfiability;formal method;computational complexity;state space;fault tolerance;distributed programs index terms fault tolerance automatic addition of fault tolerance formal methods program synthesis;polynomial time;fault tolerance safety state space methods polynomials algorithm design and analysis fault diagnosis fault tolerant systems automation manuals design methodology;distributed processing software fault tolerance formal specification computational complexity	We focus on the problem of synthesizing failsafe fault-tolerance where fault-tolerance is added to an existing (fault-intolerant) program. A failsafe fault-tolerant program satisfies its specification (including safety and liveness) in the absence of faults. However, in the presence of faults, it satisfies its safety specification. We present a somewhat unexpected result that, in general, the problem of synthesizing failsafe fault-tolerant distributed programs from their fault-intolerant version is NP-complete in the state space of the program. We also identify a class of specifications, monotonic specifications, and a class of programs, monotonic programs, for which the synthesis of failsafe fault-tolerance can be done in polynomial time (in program state space). As an illustration, we show that the monotonicity restrictions are met for commonly encountered problems, such as Byzantine agreement, distributed consensus, and atomic commitment. Furthermore, we evaluate the role of these restrictions in the complexity of synthesizing failsafe fault-tolerance. Specifically, we prove that if only one of these conditions is satisfied, the synthesis of failsafe fault-tolerance is still NP-complete. Finally, we demonstrate the application of monotonicity property in enhancing the fault-tolerance of (distributed) nonmasking fault-tolerant programs to masking.	byzantine fault tolerance;consensus (computer science);fail-safe;liveness;np-completeness;non-monotonic logic;polynomial;state (computer science);state space;time complexity	Sandeep S. Kulkarni;Ali Ebnenasir	2005	IEEE Transactions on Dependable and Secure Computing	10.1109/TDSC.2005.29	fault tolerance;real-time computing;formal methods;computer science;distributed computing	SE	-13.79396612101826	28.842160677651634	194404
4efb91bbfde6ecf0449e401c8a7c5f5c73cb7387	saturation-based incremental ltl model checking with inductive proofs	qa75 electronic computers computer science szamitastechnika;szamitogeptudomany	Efficient symbolic and explicit model checking approaches have been developed for the verification of linear time temporal properties. Nowadays, advances resulted in the combination of on-the-fly search with symbolic encoding in a hybrid solution providing many results by now. In this work, we propose a new hybrid approach that leverages the so-called saturation algorithm both as an iteration strategy during the state space generation and in a new incremental fixed-point computation algorithm to compute strongly connected components (SCCs). In addition, our solution works on-the-fly during state space traversal and exploits the decomposition of the model as an abstraction to inductively prove the absence of SCCs with cheap explicit runs on the components. When a proof cannot be shown, the incremental symbolic fixed-point algorithm will find the SCC, if one exists. Evaluation on the models of the Model Checking Contest shows that our approach outperforms similar algorithms for concurrent systems.	algorithm;computation;concurrency (computer science);fixed-point iteration;inductive reasoning;mathematical induction;model checking;partial order reduction;state space;strongly connected component;time complexity;tree traversal	Vince Molnár;Dániel Darvas;András Vörös;Tamás Bartha	2015		10.1007/978-3-662-46681-0_58	model checking;discrete mathematics;computer science;artificial intelligence;theoretical computer science;mathematics;programming language;abstraction model checking;symbolic trajectory evaluation;algorithm	Logic	-13.734613240405448	26.245729432300973	194690
cb4ac7ed7f2a2a9d41893fd36e3a2b17b893508b	optimising ordering strategies for symbolic model checking of railway interlockings	possible reduction;full state space exploration;model checker;model checking;canonical representation;safety analysis;compact representation;possible optimisations;symbolic model checker;symbolic model checking;railway interlockings	Interlockings implement Railway Signalling Principles which ensure the safe movements of trains along a track system. They are safety critical systems which require a thorough analysis. We are aiming at supporting the safety analysis by automated tools, namely model checkers. Model checking provides a full state space exploration and is thus intrinsically limited in the problem’s state space. Current research focuses on extending these limits and pushing the boundaries. In our work we investigate possible optimisations for symbolic model checking. Symbolic model checkers exploit a compact representation of the model using Binary Decision Diagram. These structures provide a canonical representation which allows for reductions. The compactness of this data structure and possible reductions are dependent on two orderings: the ordering of variables and the ordering in which sub-structures are manipulated. This paper reports on findings of how a near to optimal ordering can be generated for the domain of interlocking verification.	binary decision diagram;control table;data model;data structure;experiment;linkage (software);model checking;persistence (computer science);reduction (complexity);state space;the australian	Kirsten Winter	2012		10.1007/978-3-642-34032-1_24	simulation;computer science;artificial intelligence;theoretical computer science;machine learning;distributed computing;symbolic trajectory evaluation;algorithm	SE	-14.26507034947197	26.91151528295239	194830
c675ed5cb1055684af0689ab86250e4b47b2e3e4	implementation and performance of probabilistic inference pipelines		In order to handle real-world problems, state-of-the-art probabilistic logic and learning frameworks, such as ProbLog, reduce the expensive inference to an efficient Weighted Model Counting. To do so ProbLog employs a sequence of transformation steps, called an inference pipeline. Each step in the probabilistic inference pipeline is called a pipeline component. The choice of the mechanism to implement a component can be crucial to the performance of the system. In this paper we describe in detail different ProbLog pipelines. Then we perform a empirical analysis to determine which components have a crucial impact on the efficiency. Our results show that the Boolean formula conversion is the crucial component in an inference pipeline. Our main contributions are the thorough analysis of ProbLog inference pipelines and the introduction of new pipelines, one of which performs very well on our benchmarks.	benchmark (computing);breadth-first search;compiler;conditional (computer programming);knowledge compilation;pipeline (computing);pipeline (software)	Dimitar Sht. Shterionov;Gerda Janssens	2015		10.1007/978-3-319-19686-2_7	computer science;theoretical computer science;machine learning;data mining	PL	-15.458134760576192	25.418895403117034	194987
4b7de1f1c08cdf8254c57eff908499a9751caf8f	checking relational specifications with binary decision diagrams	static verification;satisfiability;ordered binary decision diagram;software architecture;graph rewriting;coordination;binary decision diagram	Checking a specification in a language based on sets and relations (such as Z) can be reduced to the problem of finding satisfying assignments, or models, of a relational formula. A new method for finding models using ordered binary decision diagrams (BDDs) is presented that appears to scale better than existing methods.Relational terms are replaced by matrices of boolean formulae. These formulae are then composed to give a boolean translation of the entire relational formula. Throughout, boolean formulae are represented with BDDs; from the resulting BDD, models are easily extracted.The performance of the BDD method is compared to our previous method based instead on explicit enumeration. The new method performs as well or better on most of our examples, but can also handle specifications that, until now, we have been unable to analyze.	binary decision diagram;boolean algebra;boolean satisfiability problem;ontology components	Craig Damon;Daniel O Jackson;Somesh Jha	1996		10.1145/239098.239110	software architecture;computer science;theoretical computer science;software engineering;binary decision diagram;algorithm;graph rewriting;satisfiability	SE	-18.758965060209523	25.5357325551193	195153
4a57df27f4a4aabedb5aa3be69a9499f2ff3b030	uppaal in 1995	automatic verification;timed automata;real time systems	Uppaal1 is a tool suite for automatic veri cation of safety and bounded liveness properties of real-time systems modeled as networks of timed automata [12, 9, 4], developed during the past two years. In this paper, we summarize the main features of Uppaal in particular its various extensions developed in 1995 as well as applications to various case-studies, review and provide pointers to the theoretical foundation.	automata theory;liveness;pointer (computer programming);real-time clock;real-time computing;timed automaton;uppaal	Johan Bengtsson;Kim G. Larsen;Fredrik Larsson;Paul Pettersson;Wang Yi	1996		10.1007/3-540-61042-1_66	computer science;timed automaton	Logic	-18.145006450175945	27.802739576989293	195495
371da79aa2adf589f3f27902856001f4c8580c9a	rarity based guided state space search	functional verification;simulation;formal methods;formal method;state space;coverage;bdds;state explosion;industrial design	State explosion is a common problem when verifying la~ye designs. Partial state exploration using guided techniques has become a subject of wide research. We propose a rarity-based metric for state prioritization and several techniques which use the metric for enhanced state space search. These techniques use latch toggle activity and latch support for partitioning the design. The computation overhead for these techniques is minimal. Coverage results on the large industrial designs show the effectiveness of our approach. K e y w o r d s : Coverage, Functional Verification, Formal Methods, Simulation, BDDs.	computation;feature toggle;formal methods;linear algebra;overhead (computing);simulation;state space search;verification and validation	Malay K. Ganai;Adnan Aziz	2001		10.1145/368122.368878	real-time computing;formal methods;formal verification;computer science;state space;theoretical computer science;programming language;algorithm;functional verification	EDA	-14.718192001225946	28.61212232699348	196929
069df1f82bc13a727ec33377ba066e29057450dd	dynamic slicing based on redex trails	slicing;dynamic slicing;lazy functional logic programming;redex trails;program debugging;functional logic programming;data structure	Tracing computations is a widely used methodology for program debugging. Lazy languages, in particular, pose new demands on tracing techniques since following the actual trace of a computation is generally useless. Typically, they rely on the construction of a redex trail, a graph that describes the reductions of a computation and its relationships. While tracing provides a significant help for locating bugs, the task still remains complex. A well-known debugging technique for imperative programs is based on dynamic slicing, a method to find the program statements that influence the computation of a value for a specific program input.In this work, we introduce a novel technique for dynamic slicing in lazy functional logic languages. Rather than starting from scratch, our technique relies on (a slight extension of) redex trails. We provide a method to compute a correct and minimal dynamic slice from the redex trail of a computation. A clear advantage of our proposal is that one can enhance existing tracers with slicing capabilities with a modest implementation effort, since the same data structure (the redex trail) can be used for both tracing and slicing.	array slicing;computation;curry;data structure;debugging;higher-order programming;imperative programming;lazy evaluation;program slicing;reduction strategy (code optimization);software bug	Claudio Ochoa;Josep Silva;Germán Vidal	2004		10.1145/1014007.1014020	program slicing;data structure;computer science;theoretical computer science;functional logic programming;programming language;algorithm	SE	-18.57298394556461	32.01147058790436	197000
be1f2a10bb70dde2f324a794b015ceb8b933e907	specification by existing design plus use-cases	software;electronic mail;sequential circuits;logic gates;logic functions;integrated circuit modeling;combinational circuits	It is difficult to specify a system completely with formal methods. There are don't care situations which may not be so clearly defined, and behaviors of some special cases are hard to describe. Recently, it has been found that if the changes inside a design are local (limited within a set of sub-circuits), complete verification becomes feasible with small numbers of simulations. This gives us a way to specify a system as a modification of an existing design. By defining which portions of the existing design should be modified in which ways, it can become a design for the new specification. In this paper, we propose such a specification method, i.e., specifying new designs by giving existing designs and use-cases that discribes the difference from the new specification. The difference may be completely described with a small set of simulation patterns. Illustrative examples and some preliminary experimental results are shown.	don't-care term;formal methods;simulation	Yusuke Kimura;Masahiro Fujita	2016	2016 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2016.7748253	electronic engineering;logic gate;computer science;theoretical computer science;sequential logic;combinational logic;algorithm	EDA	-14.656067682705482	28.72947135733329	197071
4440ec22680eeeb1fc6ef825bac6d01f455df724	testing from a nondeterministic finite state machine using adaptive state counting	adaptive testing;formal specification;program debugging finite state machines program verification formal specification;state counting index terms software engineering software program verification testing and debugging nondeterministic finite state machine adaptive testing;testing and debugging;program verification;indexing terms;software engineering;65;input output;finite state machines;research paper;state counting;finite state machines software verification and validation software requirements and specifications software debugging;program debugging nondeterministic finite state machine adaptive state counting preset test suites test generation termination software engineering program verification;software program verification;index terms software engineering;test generation;program debugging;finite state machine;nondeterministic finite state machine	The problem of generating a checking experiment from a nondeterministic finite state machine has been represented in terms of state counting. However, test techniques that use state counting traditionally produce preset test suites. We extend the notion of state counting in order to allow the input/output sequences observed in testing to be utilized: Adaptive state counting is introduced. The main benefit of the proposed approach is that it may result in a reduction in the size of the test suite used. An additional benefit is that, where a failure is observed, it is possible to terminate test generation at this point.	adaptive algorithm;deterministic finite automaton;finite-state machine;global serializability;input/output;nondeterministic finite automaton;terminate (software);test case;test suite	Robert Mark Hierons	2004	IEEE Transactions on Computers	10.1109/TC.2004.85	input/output;real-time computing;index term;computer science;theoretical computer science;operating system;formal specification;finite-state machine;computerized adaptive testing;programming language;generalized nondeterministic finite automaton;algorithm	SE	-17.75335125307299	29.207622088699903	197717
54f20f50f8524385065fde4ca40dfc786aa6ef0a	at-most-once message delivery. a case study in algorithm verification	case study;algorithm verification;at-most-once message delivery	The at-most-once message delivery problem involves delivering a sequence of messages submitted by a user at one location to another user at another location. If no failures occur, all messages should be delivered in the order in which they are submitted, each exactly once. If failures (in particular, node crashes or timing anomalies) occur, some messages might be lost, but the remaining messages should not be reordered or duplicated. This talk examines two of the best-known algorithms for solving this problem: the clock-based protocol of [3] and the five-packet interchange protocol of [2]. It is shown that both of these protocols can be understood as implementations of a common (untimed) protocol that we call the generic protocol. It is also shown that the generic protocol meets the problem specification. The development is carried out in the context of (timed and untimed) automata [7, 8] and [6], using simulation techniques [7]. It exercises many aspects of the relevant theory, including timed and untimed automata, refinement mappings, forward and backward simulations, history and prophecy variables. The theory provides insight into the algorithms, and vice versa. In this short paper, we simply give formal descriptions of the problem specification and of the two algorithms, leaving detailed discussion of the proof for the talk and for a later paper.	algorithm;automata theory;automaton;network packet;refinement (computing);simulation	Butler W. Lampson;Nancy A. Lynch;Jørgen F. Søgaard-Andersen	1992		10.1007/BFb0084800	real-time computing;theoretical computer science;distributed computing	Crypto	-12.904198895075094	29.803248376627906	198392
38ac43be2862ba58022d5731a4d5272c659bd053	swarm synthesis of convergence for symmetric protocols	automated design;protocols;symmetric protocols;software tool;convergence;software tools fault tolerant computing parallel processing protocols;protocols convergence system recovery transient analysis color educational institutions nominations and elections;software tool convergence swarm synthesis nondeterministic method convergence algorithmic addition nonstabilizing symmetric protocols randomization method parallelization method parallel framework computer cluster computational resources selfstabilizing protocols;graph coloring;automatic generation;self stabilization;fault tolerant computing;leader election;software tools;symmetric protocols self stabilization convergence automated design;parallel processing	This paper presents a novel non-deterministic method for algorithmic addition of convergence to non-stabilizing symmetric protocols. The proposed method exploits randomization and parallelization in order to expand the scope of the search for self-stabilizing versions of non-stabilizing protocols. Such a non-deterministic method enables an embarrassingly parallel framework that exploits the computational resources of computer clusters for automated design of self-stabilizing protocols. We have implemented our approach in a software tool and have synthesized several new self-stabilizing solutions for well-known protocols in the literature (e.g., maximal matching, graph coloring and leader election on a ring). Our case studies demonstrate that the proposed method is able to automatically generate self-stabilizing versions of non-stabilizing protocols in cases where existing automated methods fail. As a result, the proposed approach increases the likelihood of success in synthesizing the self-stabilizing versions of non-stabilizing protocols.	algorithm;communications protocol;computational resource;computer cluster;correctness (computer science);deadlock;embarrassingly parallel;formal verification;graph coloring;leader election;locality of reference;matching (graph theory);maximal set;np (complexity);network topology;overlay network;parallel computing;polynomial;problem domain;programming tool;self-stabilization;state space;swarm;time complexity;transition system	Ali Ebnenasir;Aly Farahat	2012	2012 Ninth European Dependable Computing Conference	10.1109/EDCC.2012.22	self-stabilization;communications protocol;parallel processing;parallel computing;convergence;computer science;theoretical computer science;operating system;graph coloring;leader election;distributed computing;programming language	EDA	-12.43413446807968	31.482178980341814	198522
b97eb3a811baad7c56dc14c63c862137debb7d92	path-oriented reachability verification of a class of nonlinear hybrid automata using convex programming	formal model;convex programming;continuous variable;dynamic system;bounded model checking;performance improvement;hybrid automata;off the shelf;reachability analysis	Hybrid automata are well-studied formal models for dynamical systems. However, the analysis of hybrid automata is extremely difficult, and even state-of-the-art tools can only analyze systems with few continuous variables and simple dynamics. Because the reachability problem for general hybrid automata is undecidable, we give a path-oriented reachability analysis procedure for a class of nonlinear hybrid automata called convex hybrid automata. Our approach encodes the reachability problem along a path of a convex hybrid automaton as a convex feasibility problem, which can be efficiently solved by off-the-shelf convex solvers, such as CVX. Our path-oriented reachability verification approach can be applied in the frameworks of bounded model checking and counterexample-guided abstraction refinement with the goal of achieving significant performance improvement for this subclass of hybrid automata.	automata theory;convex optimization;dynamical system;hybrid automaton;model checking;nonlinear system;reachability problem;refinement (computing);solver;undecidable problem	Lei Bu;Jianhua Zhao;Xuandong Li	2010		10.1007/978-3-642-11319-2_9	mathematical optimization;convex optimization;quantum finite automata;dynamical system;automata theory;timed automaton	Logic	-12.948457990613543	26.645462947517068	198778
a57487aadff60402f72a40f061d19f2e4888975e	ilpc: a novel approach for scalable timing analysis of synchronous programs	ilpc;execution time;worst case reaction time analysis;flight control;logical time;model checking;aerospace control;static timing analysis;existing approach;max-plus algebra;worst-case execution time;integer programming;scability;logical tick;worst case execution time;linear programming;integer linear programming;airbus a-380;static computation;synchronous programs;program logical time map;wcrt analysis;synchronous program;safety critical system design;synchronous languages;reachability;program diagnostics;synchronous languages;safety-critical software;longer analysis time;state space explosion problem;logical ticks;integer linear programming;scalable timing analysis;program verification;physical time;novel approach;complex embedded systems;reachability analysis	Synchronous programs have been widely used in the design of safety critical systems such as the flight control of Airbus A-380. To validate the implementations of synchronous programs, it is necessary to map the program's logical time (measured in logical ticks) to physical time (the execution time on a given processor). The static computation of the worst-case execution time of logical ticks is called Worst Case Reaction Time (WCRT) analysis. Several approaches for WCRT analysis exist: max-plus algebra, model checking, reachability and integer linear programming (ILP). Of these approaches, reachability, model checking and ILP provide reasonably precise worst case estimates at the expense of longer analysis time. Apart from max-plus based approaches, which can produce large overestimates, the existing approaches suffer from the state space explosion problem. In this paper, we develop a new ILP based approach, called ILPc, which exploits the concurrency explicitly in the ILP formulation to avoid the state space explosion problem. Through extensive benchmarking we demonstrate the efficacy of the approach: for complex programs, ILPc is often orders of magnitude faster compared to the existing approaches, while achieving same level of precision. Thus, this paper paves the way for scalable WCRT analysis of complex embedded systems designed using the synchronous approach.	amortized analysis;best, worst and average case;computation;concurrency (computer science);control flow;embedded system;integer programming;iterative method;iterative refinement;linear programming;model checking;multistage interconnection networks;reachability;refinement (computing);run time (program lifecycle phase);scalability;state space;static timing analysis;worst-case execution time;xfig	Jia Jie Wang;Partha S. Roop;Sidharta Andalam	2013	2013 International Conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES)			Embedded	-14.26359374334335	30.001850901008613	198782
5484692b2b45d2bcb1408c72d95c91e5da7118c8	on reasoning with the global time assumption	distributed system;global time;distributed programs;atomic registers;correctness;concurrency;partial orders;partial order	Concurrency in distributed systems is usually modeled by a nondeterministic interleaving of atomic events. The consequences of this interleaving (or global time) assumption on the specifications and proofs of distributed programs are examined in this paper. A construction for atomic registers is presented; this construction has the surprising property that it is correct with respect to a specification based on partial orders but is incorrect with respect to a naively derived specification based on global time.	atomic formula;distributed computing;forward error correction;nondeterministic algorithm	Ambuj K. Singh	1992	LOPLAS	10.1145/130616.130624	partially ordered set;correctness;concurrency;computer science;theoretical computer science;distributed computing;programming language;algorithm	Logic	-17.468250165441887	28.800721168751423	199608
