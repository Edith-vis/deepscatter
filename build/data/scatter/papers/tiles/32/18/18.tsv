id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c629ac7e6fd14f8ee173bd6e2c305330183630f6	on the power of dna-computing	temps polynomial;complexite calcul;probleme np complet;camino hamiltoniano;clase complejidad;algorithme;classe complexite;complexity class;hamiltonian path problem;computational complexity;directed graph;graphe oriente;polynomial time;chemin hamiltonien;algorithms;grafo orientado;problema np completo;dna computing;np complete problem;hamiltonian path;tiempo polinomial	In [Adl94] Adleman used biological manipulations with DNA s trings to solve some instances of the Directed Hamiltonian Path Problem. Lipton [Lip94] showed how to extend this idea to solve any NP problem. We prove that exactly t he problems inPNP = p2 can be solved in polynomial time using Lipton’s model. Vario us modifications of Lipton’s model, based on other DNA manipulations, are investig ated systematically, and it is proved that their computational power in polynomial time ca n be characterized by one of the complexity classes P, p2 , or p3 .	complexity class;computation;dna computing;hamiltonian path problem;np (complexity);p (complexity);polynomial;time complexity;variometer	Diana Rooß;Klaus W. Wagner	1996	Inf. Comput.	10.1006/inco.1996.0094	hamiltonian path;time complexity;complexity class;combinatorics;np-complete;directed graph;computer science;mathematics;hamiltonian path problem;computational complexity theory;dna computing;algorithm	Theory	16.222796632936053	22.082920002585283	159512
0c26b8830faba2970b635cac4bbfb109f40baec1	optimal ancilla-free clifford+t approximation of z-rotations		We consider the problem of approximating arbitrary single-qubit z-rotations by ancilla-free Clifford+T circuits, up to given epsilon. We present a fast new probabilistic algorithm for solving this problem optimally, i.e., for finding the shortest possible circuit whatsoever for the given problem instance. The algorithm requires a factoring oracle (such as a quantum computer). Even in the absence of a factoring oracle, the algorithm is still near-optimal under a mild number-theoretic hypothesis. In this case, the algorithm finds a solution of T -count m+O(log(log(1/ε))), where m is the T -count of the second-to-optimal solution. In the typical case, this yields circuit approximations of T -count 3 log 2 (1/ε) +O(log(log(1/ε))). Our algorithm is efficient in practice, and provably efficient under the above-mentioned number-theoretic hypothesis, in the sense that its expected runtime is O(polylog(1/ε)).	ancilla bit;approximation;integer factorization;quantum computing;qubit;randomized algorithm;theory	Neil J. Ross;Peter Selinger	2015	Quantum Information & Computation			Theory	10.871975182845015	20.67549997143078	159623
83af91bf84075f484b7fa5bf159ee3ee155fde5f	sparse suffix tree construction in optimal time and space		Suffix tree (and the closely related suffix array) are fundamental structures capturing all substrings of a given text essentially by storing all its suffixes in the lexicographical order. In some applications, such as sparse text indexing, we work with a subset of b interesting suffixes, which are stored in the so-called sparse suffix tree. Because the size of this structure is Θ(b), it is natural to seek a construction algorithm using only O(b) words of space assuming read-only random access to the text. We design a linear-time Monte Carlo algorithm for this problem, hence resolving an open question explicitly stated by Bille et al. [TALG 2016]. The best previously known algorithm by I et al. [STACS 2014] works in O(n log b) time. As opposed to previous solutions, which were based on the divide-and-conquer paradigm, our solution proceeds in n/b rounds. In the r-th round, we consider all suffixes starting at positions congruent to r modulo n/b. By maintaining rolling hashes, we can lexicographically sort all interesting suffixes starting at such positions, and then we can merge them with the already considered suffixes. For efficient merging, we also need to answer LCE queries efficiently (and in small space). By plugging in the structure of Bille et al. [CPM 2015] we obtain O(n + b log b) time complexity. We improve this structure by a recursive application of the so-called difference covers, which then implies a linear-time sparse suffix tree construction algorithm. We complement our Monte Carlo algorithm with a deterministic verification procedure. The verification takes O(n√log b) time, which improves upon the bound of O(n log b) obtained by I et al. [STACS 2014]. This is obtained by first observing that the pruning done inside the previous solution has a rather clean description using the notion of graph spanners with small multiplicative stretch. Then, we are able to decrease the verification time by applying difference covers twice. Combined with the Monte Carlo algorithm, this gives us an O(n√log b)-time and O(b)-space Las Vegas algorithm. Work done while the author held a post-doctoral position at Warsaw Center of Mathematics and Computer Science. Supported by Polish budget funds for science in 2013-2017 as a research project under the ‘Diamond Grant’	computer science;emoticon;las vegas algorithm;lexicographical order;modulo operation;monte carlo algorithm;programming paradigm;random access;read-only memory;recursion;rolling hash;stacs;sparse matrix;suffix array;suffix tree;the diamond age;time complexity	Pawel Gawrychowski;Tomasz Kociumaka	2017			generalized suffix tree;mathematical optimization;combinatorics;computer science;theoretical computer science;machine learning;mathematics;programming language;approximation algorithm;algorithm;locality-sensitive hashing;statistics	Theory	12.530040810783964	25.232453361594132	159950
9f830f17caf888d123677273c961db13f6a33c24	approximating the number of double cut-and-join scenarios	fpras;genome rearrangement;comparative genomics;dcj;mcmc	Please check your proof carefully and mark all corrections at the appropriate place in the proof (e.g., by using on-screen annotation in the PDF file) or compile them in a separate list. Note: if you opt to annotate the file with software other than Adobe Reader then please also highlight the appropriate place in the PDF file. To ensure fast publication of your paper please return your corrections within 48 hours. Location in article Query / Remark click on the Q link to go Please insert your reply or correction at the corresponding line in the proof Q1 Please confirm that given names and surnames have been identified correctly. Q2 Property 17 mentioned here has been changed to Observation 17. Please check, and correct if necessary. a b s t r a c t The huge number of solutions in genome rearrangement problems calls for algorithms for counting and sampling in the space of solutions, rather than drawing one arbitrary scenario. A closed formula exists for counting the number of DCJ scenarios between co-tailed genomes, but no polynomial result has been published so far for arbitrary genomes. We prove here that it admits a Fully Polynomial time Randomized Approximation Scheme. We use an MCMC almost uniform sampler and prove that it converges to the uniform distribution in fully polynomial time. The MCMC can be used to quickly draw a sample of DCJ scenarios from a prescribed distribution and test some hypotheses on genome evolution.	compiler;line level;markov chain monte carlo;p (complexity);polynomial;polynomial-time approximation scheme;portable document format;randomized algorithm;sampling (signal processing);time complexity	István Miklós;Eric Tannier	2012	Theor. Comput. Sci.	10.1016/j.tcs.2012.03.006	mathematical optimization;combinatorics;markov chain monte carlo;mathematics;comparative genomics;statistics	Theory	15.280659695605177	21.337195177510043	160352
c3baa41b9e2c6ad4771ea8b6c5aa234408f03362	range counting over multidimensional data streams	data stream;range counting;multidimensional data;data streams;e approximation	We consider the problem of approximate range counting over streams of d-dimensional points. In the data stream model, the algorithm makes a single scan of the data, which is presented in an arbitrary order, and computes a compact summary (called a sketch). The sketch, whose size depends on the approximation parameter ε, can be used to count the number of points inside a query range within additive error εn, where n is the size of the stream. We present several results, deterministic and randomized, for both rectangle and halfplane ranges.	approximation algorithm;counting problem (complexity);randomized algorithm;streaming algorithm;utility functions on indivisible goods	Subhash Suri;Csaba D. Tóth;Yunhong Zhou	2004		10.1145/997817.997844	theoretical computer science;data mining;mathematics;data stream mining;statistics	Theory	13.732334983568803	23.967389924962994	160980
b9f84ee7998e162980448af4c97cb9ae66fb1b25	time-table scheduling using neural network algorithms	neural network algorithms;educational institute;time table scheduling;neural nets;graph coloring;np complete problems;loosely synchronous problems;graph partitioning;educational administrative data processing;neural nets educational administrative data processing;polynomial time;time table scheduling np complete problems educational institute graph coloring graph partitioning loosely synchronous problems neural network algorithms parallel machines;scheduling problem;parallel machines;large classes;neural network	A demonstration is presented of how to use neural network algorithms to schedule classes in an educational institute. Such a scheduling problem is basically a graph-coloring or graph-partitioning problem which belongs to the large class of NP (nondeterministic polynomial time)-complete problems, and it is difficult to solve. G.C. Fox and W. Furmanski (1988) proposed some neural network algorithms to decompose loosely synchronous problems onto parallel machines. The author adopts these algorithms to schedule timetables. The algorithms can be implemented on a digital computer or on analog neural networks	algorithm;artificial neural network;schedule;scheduling (computing)	T. L. Yu	1990		10.1109/IJCNN.1990.137582	time complexity;np-complete;computer science;graph partition;theoretical computer science;machine learning;graph coloring;time delay neural network;distributed computing;artificial neural network	ML	14.956868304894156	21.69173097062871	161738
461987ef7770d6149dfc8318786d17131eb80b74	the symmetric group defies strong fourier sampling	hidden subgroup problem;graph isomorphism;68q17;fourier sampling;81r05;quantum computing;43a65;symmetric group	The dramatic exponential speedups of quantum algorithms over their best existing classical counterparts were ushered in by the technique of Fourier sampling, introduced by Bernstein and Vazirani and developed by Simon and Shor into an approach to the hidden subgroup problem. This approach has proved successful for abelian groups, leading to efficient algorithms for factoring, extracting discrete logarithms, and other number-theoretic problems. We show, however, that this method cannot resolve the hidden subgroup problem in the symmetric groups, even in the weakest, information-theoretic sense. In particular, we show that the Graph Isomorphism problem cannot be solved by this approach. Our work implies that any quantum approach based upon the measurement of coset states must depart from the original framework by using entangled measurements on multiple coset states.		Cristopher Moore;Alexander Russell;Leonard J. Schulman	2008	SIAM J. Comput.	10.1137/050644896	quantum fourier transform;mathematical optimization;combinatorics;discrete mathematics;hidden subgroup problem;mathematics;graph isomorphism;symmetric group;quantum computer;algorithm;algebra	Theory	10.240085650982866	21.317572693079818	162358
280620915b2dfe16e5abed185a50ce848c1548ad	dna based algorithms for solving both max-sat and max-w-sat problems	dna;graph theory;lipton s algorithm;max sat;biocomputing;max w sat;molecular beacon;component;computability;graph theory biocomputing computability;biological system modeling;space solution;satisfiability;computational modeling;molecular beacons;double strand;molecular beacons dna based algorithms max w sat problems weighted maximum satisfiability problem lipton algorithm space solution fixed length dna sequences;biological information theory;weighted maximum satisfiability problem;lipton algorithm;fixed length dna sequences;dna dna computing laboratories computational modeling biological information theory biological system modeling;dna computing;dna based algorithms;max w sat problems;dna sequence;molecular beacon component dna computing max w sat max sat lipton s algorithm	In this paper, we propose three DNA algorithms for the weighted maximum satisfiability problem (MAX-W-SAT for short). The first one is an extension of Lipton's algorithm for SAT. The space solution is represented by a graph and the weights are encoded in form of pieces which are appended to the double strands representing the solutions. The second algorithm proposes a different encoding of information (clauses and weights) aiming at reducing the quantity of DNA required, and the last one proposes to represent all the weights by fixed length DNA sequences and using molecular beacons to access them. The designed algorithms solve MAX-SAT problem by assuming all the clauses having the same weight.	algorithm;graph (discrete mathematics);max;maximum satisfiability problem	Souhila Sadeg;Habiba Drias;Hafid Aid;Samir Mazouz	2010	2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA)	10.1109/BICTA.2010.5645331	combinatorics;theoretical computer science;mathematics;algorithm	EDA	15.961980549772948	22.278891250935338	162636
527ce568698075a82681630e718719ad586b093d	finding pathway structures in protein interaction networks	camino mas corto;modelizacion;shortest path;performance guarantee;distributed database;proteine;time complexity;trajectoire optimale;availability;routing;disponibilidad;heuristic method;biologia molecular;routage;base repartida dato;bioinformatique;plus court chemin;probabilistic algorithm;metodo heuristico;probabilistic approach;camino optimo;structure proteine;chemin optimal;modelisation;protein structure;complexite temps;optimal path;base de donnees repartie;optimal trajectory;enfoque probabilista;approche probabiliste;molecular biology;trayectoria optima;sensibilite elevee;proteina;methode heuristique;bioinformatica;alta sensibilidad;information system;protein interaction;complejidad tiempo;algoritmo optimo;protein;algorithme optimal;optimal algorithm;modeling;disponibilite;high sensitivity;systeme information;pathway structure;protein interaction network;bioinformatics;sistema informacion;biologie moleculaire;enrutamiento	"""The increased availability of data describing biological interactions provides important clues on how complex chains of genes and proteins interact with each other. Most previous approaches either restrict their attention to analyzing simple substructures such as paths or trees in these graphs, or use heuristics that do not provide performance guarantees when general substructures are analyzed. We investigate a formulation to model pathway structures directly and give a probabilistic algorithm to find an optimal path structure in $O(4^{k}n^{2t}k^{t+\log(t+1)+2.92}t^{2})$ time and $O(n^{t}k\log k+m)$ space, where n and m are respectively the number of vertices and the number of edges in the given network, k is the number of vertices in the path structure, and t is the maximum number of vertices (i.e., """"width"""") at each level of the structure. Even for the case t = 1 which corresponds to finding simple paths of length k, our time complexity $4^{k}n^{O(1)}$ is a significant improvement over previous probabilistic approaches. To allow for the analysis of multiple pathway structures, we further consider a variant of the algorithm that provides probabilistic guarantees for the top suboptimal path structures with a slight increase in time and space. We show that our algorithm can identify pathway structures with high sensitivity by applying it to protein interaction networks in the DIP database."""	gene regulatory network;heuristic (computer science);interaction;randomized algorithm;time complexity;vertex (geometry)	Songjian Lu;Fenghui Zhang;Jianer Chen;Sing-Hoi Sze	2007	Algorithmica	10.1007/s00453-007-0155-7	time complexity;availability;protein structure;routing;combinatorics;systems modeling;computer science;artificial intelligence;mathematics;shortest path problem;randomized algorithm;distributed database;information system;algorithm	Comp.	16.756893225639974	24.05058654600025	162790
3eb2a5bdc6d058ee2c99e05479d0e1d5ebfa380d	finding compact structural motifs	heuristique;68t20;performance guarantee;approximate algorithm;proteine;aplicacion;temps polynomial;heuristica;variety;deteccion;approximation algorithm;performance;search strategy;detection;polynomial;68wxx;41a10;protein structure;aproximacion polinomial;np hardness;structural genomics;informatique theorique;polinomio;approximation polynomiale;strategie recherche;algoritmo aproximacion;polynomial time;proteina;heuristics;sequence motif;rendimiento;variedad;compact structural motif;algorithme approximation;variete;protein;application;polynome;68w25;polynomial time approximation scheme;polynomial approximation;exhaustive search;computer theory;estrategia investigacion;tiempo polinomial;informatica teorica	Protein structural motif detection has important applications in structural genomics. Compared with sequence motifs, structural motifs are more sensitive in revealing the evolutionary relationships among proteins. A variety of algorithms have been proposed to attack this problem. However, they are either heuristic without theoretical performance guarantee, or inefficient due to employing exhaustive search strategies. This paper studies a reasonably restricted version of this problem: the compact structural motif problem. We prove that this restricted version is still NP-hard, and we present a polynomial-time approximation scheme to solve it. This is the first approximation algorithm with a guaranteed ratio for the protein structural motif problem.		Dongbo Bu;Ming Li;Shuai Cheng Li;Jianbo Qian;Jinbo Xu	2009	Theor. Comput. Sci.	10.1016/j.tcs.2009.03.023	time complexity;structural genomics;protein structure;mathematical optimization;combinatorics;polynomial-time approximation scheme;performance;computer science;heuristics;brute-force search;mathematics;variety;approximation algorithm;algorithm;polynomial;sequence motif	ECom	16.470607750102076	23.67360763302403	163820
23de0ea42b4628d481ba1378a02b771c0d8a32a2	on sampling scj rearrangement scenarios		The Single Cut or Join (SCJ) operation on genomes, generalizing chromosome evolution by fusions and fissions, is the computationally simplest known model of genome rearrangement. While most genome rearrangement problems are already hard when comparing three genomes, it is possible to compute in polynomial time a most parsimonious SCJ scenario for an arbitrary number of genomes related by a binary phylogenetic tree. Here we consider the problems of sampling and counting the most parsimonious SCJ scenarios. We show that both the sampling and counting problems are easy for two genomes, and we relate SCJ scenarios to alternating permutations. However, for an arbitrary number of genomes related by a binary phylogenetic tree, the counting and sampling problems become hard. We prove that if a Fully Polynomial Randomized Approximation Scheme or a Fully Polynomial Almost Uniform Sampler exist for the most parsimonious SCJ scenario, then RP = NP. The proof has a wider scope than genome rearrangements: the same result holds for parsimonious evolutionary scenarios on any set of discrete characters.	hybrid genome assembly;maximum parsimony (phylogenetics);occam's razor;phylogenetic tree;phylogenetics;polynomial;polynomial-time approximation scheme;rp (complexity);randomized algorithm;sampling (signal processing);time complexity;tree rearrangement	István Miklós;Sándor Z. Kiss;Eric Tannier	2013	CoRR		mathematical optimization;combinatorics;discrete mathematics;mathematics	Theory	15.658604464867372	21.2484485597623	165380
49be5da41330d79a0d65407275fd272017f7371b	monte carlo and markov chain techniques for network reliability and sampling	system reliability;fiabilite systeme;chaine markov;cadena markov;metodo monte carlo;relacion convergencia;echantillonnage;aproximacion;heuristic method;simulation;combinatorial problems;methode monte carlo;simulacion;metodo heuristico;taux convergence;convergence rate;reseau;red;experimental result;approximation;fiabilidad sistema;sampling;general methods;monte carlo method;link failure;resultado experimental;methode heuristique;network reliability;monte carlo;muestreo;resultat experimental;network;markov chain	We outline a heuristic to approximate various reliability related parameters of communications networks under link failures. Our scheme is based on Monte Carlo and Markov chain simulation techniques. We shall present the ideas of these Monte Carlo and Markov chain techniques in terms of a speci c reliability measure. However, the general method could be applicable to other reliability measures and, in fact, to other combinatorial problems. We present initial experimental results which suggest that our approach is e cient in the computational complexity sense (running time polynomial in the size of the input); furthermore, our results suggest practical applicability for medium size networks and single-edge parameters. As an example, we present the results of our experiments on a network that was posed for analysis by Applied Research at Bellcore: We estimated all single-edge parameters on a single DEC-5000 in less than 4 hours. The software that supported our experiments involves approximately 3000 lines of C-code and is easy to adapt to other applications. Presented at the DIMACSWorkshop on Computational Support for Discrete Mathematics, March, 1992. Supported by a Fannie and John Hertz Foundation fellowship, National Science Foundation Grant No. CCR-8920505, and the Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) under NSF-STC88-09648. Work partially completed while this author was a summer intern at Bellcore. Bell Communications Research, Morristown, NJ 07960.	approximation algorithm;computation;computational complexity theory;discrete mathematics;experiment;heuristic;ibm notes;markov chain;monte carlo method;polynomial;simulation;telecommunications network;theoretical computer science;time complexity	Adam L. Buchsbaum;Milena Mihail	1992	Networks	10.1002/net.3230250305	econometrics;hybrid monte carlo;markov chain monte carlo;monte carlo molecular modeling;mathematics;algorithm;statistics;monte carlo method	Theory	15.01362260348564	22.509692390621023	166422
48e06a72bab0e28c4af9ef52d6f0b48576985b10	the ajtai random class of lattices	public key cryptography;pire cas;worst case;provable security;cryptographie cle publique;ensemble ajtai;ajtai random class;provably secure cryptosystem;cryptosysteme sure;secure cryptosystem;enrejado;treillis;criptografia;cryptography;cryptographie;ajtai set;classe aleatoire ajtai;lattice	Ajtai has recently given a reduction from the problem of approximating a short basis for a lattice in the worst case, to the problem of nding a short lattice vector for a uniformly chosen lattice in a certain random class of lattices. Here we give an explicit formula for the number of lattices of the type used by Ajtai. We also prove some results about the average volume of the fundamental cell of such a lattice. c © 1999 Elsevier Science B.V. All rights reserved.	best, worst and average case;fundamental domain	Thomas W. Cusick	1999	Theor. Comput. Sci.	10.1016/S0304-3975(99)00063-8	combinatorics;discrete mathematics;computer science;cryptography;theoretical computer science;provable security;lattice;mathematics;map of lattices;lattice problem	Theory	10.268322645457724	24.356249257279398	167038
0737acab6039d1ccfd35e70feff6695424d50b99	degenerate string comparison and applications		A generalised degenerate string (GD string) Ŝ is a sequence of n sets of strings of total size N , where the ith set contains strings of the same length ki but this length can vary between different sets. We denote the sum of these lengths k0, k1, . . . , kn−1 by W . This type of uncertain sequence can represent, for example, a gapless multiple sequence alignment of width W in a compact form. Our first result in this paper is an O(N+M)-time algorithm for deciding whether the intersection 1 Partially supported by the project UNIPI PRA_2017_44 “Advanced computational methodologies for the analysis of biomedical data”. 2 Partially supported by the project UNIPI PRA_2017_44 “Advanced computational methodologies for the analysis of biomedical data”. 3 Partially supported by the project MIUR-SIR CMACBioSeq “Combinatorial methods for analysis and compression of biological sequences” grant n. RBSI146R5L and the project UNIPI PRA_2017_44 “Advanced computational methodologies for the analysis of biomedical data”. 4 Partially supported by the Royal Society project IE 161274 “Processing uncertain sequences: combinatorics and applications”. 5 Partially supported by the project MIUR-SIR CMACBioSeq “Combinatorial methods for analysis and compression of biological sequences” grant n. RBSI146R5L, the Royal Society project IE 161274 “Processing uncertain sequences: combinatorics and applications”, and the project UNIPI PRA_2017_44 “Advanced computational methodologies for the analysis of biomedical data”. © Mai Alzamel, Lorraine A.K Ayad, Giulia Bernardini, Roberto Grossi, Costas S. Iliopoulos, Nadia Pisanti, Solon P. Pissis and Giovanna Rosone; licensed under Creative Commons License CC-BY 18th International Workshop on Algorithms in Bioinformatics (WABI 2018). Editors: Laxmi Parida and Esko Ukkonen; Article No. 21; pp. 21:1–21:14 Leibniz International Proceedings in Informatics Schloss Dagstuhl – Leibniz-Zentrum für Informatik, Dagstuhl Publishing, Germany 21:2 Degenerate String Comparison and Applications of two GD strings of total sizes N and M , respectively, over an integer alphabet, is non-empty. This result is based on a combinatorial result of independent interest: although the intersection of two GD strings can be exponential in the total size of the two strings, it can be represented in only linear space. A similar result can be obtained by employing an automata-based approach but its cost is alphabet-dependent. We then apply our string comparison algorithm to compute palindromes in GD strings. We present an O(min{W,n2}N)-time algorithm for computing all palindromes in Ŝ. Furthermore, we show a similar conditional lower bound for computing maximal palindromes in Ŝ. Finally, proof-of-concept experimental results are presented using real protein datasets. 2012 ACM Subject Classification Theory of computation → Pattern matching	algorithm;automata theory;bioinformatics;comparison of programming languages (string functions);informatics;lorraine borman;maximal set;multiple sequence alignment;pattern matching;theory of computation;time complexity;wabi-sabi	Mai Alzamel;Lorraine A. K. Ayad;Giulia Bernardini;Roberto Grossi;Costas S. Iliopoulos;Nadia Pisanti;Solon P. Pissis;Giovanna Rosone	2018		10.4230/LIPIcs.WABI.2018.21	linear space;combinatorics;palindrome;gapless playback;multiple sequence alignment;degenerate energy levels;upper and lower bounds;computer science;exponential function;integer	Theory	12.874348631710204	20.643153197418286	168029
0cc5a1049acfeb1f032f93d8336110d184e2a204	error analysis in minimax trees	game tree;error analysis;model error;error propagation;informatique theorique;heuristic evaluation;game tree search;game playing;zero sum game;computer theory;informatica teorica	Game tree search deals with the problems that arise, when computers play two-person-zerosum-games such as chess, checkers, othello, etc. The greatest success of game tree search so far, was the victory of the chess machine ‘Deep Blue’ vs. G. Kasparov (ICCA J. 20 (1997) 95), the best human chess player in the world at that time. In spite of the enormous popularity of computer chess and in spite of the successes of game tree search in game playing programs, we do not know much about a useful theoretical background that could explain the usefulness of (selective) search in adversary games. We introduce a combinatorial model, which allows us to model errors of a heuristic evaluation function, with the help of coin tosses. As a result, we can show that searching in a game tree will be ‘useful’ if, and only if, there are at least two leaf-disjoint strategies which prove the root value. In addition, we show that the number of leaf-disjoint strategies, contained in a game tree, determines the order of the quality of a heuristic minimax value. The model is integrated into the context of average-case analyses. c © 2003 Elsevier B.V. All rights reserved.	adversary (cryptography);best, worst and average case;computer chess;evaluation function;heuristic evaluation;minimax;one-to-one (data model);polynomial;reversi;the turk	Ulf Lorenz;Burkhard Monien	2004	Theor. Comput. Sci.	10.1016/j.tcs.2002.10.004	combinatorial game theory;null-move heuristic;minimax;transposition table;combinatorics;example of a game without a value;simulation;game tree;extensive-form game;simultaneous game;expectiminimax tree;computer science;artificial intelligence;propagation of uncertainty;information set;quiescence search;machine learning;repeated game;errors-in-variables models;mathematics;horizon effect;normal-form game;zero-sum game;monte carlo tree search;sequential game;heuristic evaluation;game complexity;algorithm;statistics	AI	12.502109196030368	19.588231618638883	168577
6c783de05f8ec82324a60e32e3449beea40d38b0	non-adaptive learning a hidden hipergraph		We give a new deterministic algorithm that non-adaptively learns a hidden hypergraph from edge-detecting queries. All previous non-adaptive algorithms either run in exponential time or have nonoptimal query complexity. We give the first polynomial time non-adaptive learning algorithm for learning hypergraph that asks almost optimal number of queries.	decision tree model;deterministic algorithm;polynomial;sensor;time complexity	Hasan Abasi;Nader H. Bshouty;Hanna Mazzawi	2015	CoRR			Theory	13.60866049836963	20.573749575406005	169406
1aa882e41981860815d15e6416c2ba4d92e76434	notes on computational-to-statistical gaps: predictions using statistical physics		In these notes we describe heuristics to predict computational-tostatistical gaps in certain statistical problems. These are regimes in which the underlying statistical problem is information-theoretically possible although no efficient algorithm exists, rendering the problem essentially unsolvable for large instances. The methods we describe here are based on mature, albeit non-rigorous, tools from statistical physics. These notes are based on a lecture series given by the authors at the Courant Institute of Mathematical Sciences in New York City, on May 16th, 2017.	algorithm;courant–friedrichs–lewy condition;heuristic (computer science);information theory	Afonso S. Bandeira;Amelia Perry;Alexander S. Wein	2018	CoRR		rendering (computer graphics);cavity method;statistical physics;mathematics;heuristics;mathematical sciences	ML	13.351984016604527	19.41853358914433	169740
30cc544997bac1cc26624f10be1cbc3508294c2c	a bound on the precision required to estimate a boolean perceptron from its average satisfying assignment	assignment;06e30;hypercube;asignacion;fonction booleenne;sample size;funcion discreta;propiedad geometrica;mathematiques discretes;52c07;aplicacion;seuil;boolean functions;matematicas discretas;vecteur binaire;tamano muestra;propriete geometrique;aproximacion;geometry;discrete mathematics;assignation;taille echantillon;threshold;boolean function;taux croissance;euclidean distance;satisfiability;fonction seuil;input;tasa crecimiento;approximation;moyenne;upper bound;unit;discrete function;fonction discrete;funcion umbral;funcion booliana;focus of attention;promedio;center of gravity;geometrical properties;growth rate;inductive learning;entree ordinateur;borne inferieure;echantillon;distancia;68q15;average;sample;threshold function;umbral;52c35;68q32;perceptron;entrada ordenador;borne superieure;application;threshold functions;muestra;lower bound;distance;unite;cota superior;unidad;cota inferior;hipercubo	A Boolean perceptron is a linear threshold function over the discrete Boolean domain {0, 1}n. That is, it maps any binary vector to 0 or 1, depending on whether the vector’s components satisfy some linear inequality. In 1961, Chow showed that any Boolean perceptron is determined by the average or “center of gravity” of its “true” vectors (those that are mapped to 1), together with the total number of true vectors. Moreover, these quantities distinguish the function from any other Boolean function, not just from other Boolean perceptrons. In this paper we go further, by identifying a lower bound on the Euclidean distance between the average satisfying assignment of a Boolean perceptron and the average satisfying assignment of a Boolean function that disagrees with that Boolean perceptron on a fraction of the input vectors. The distance between the two means is shown to be at least ( /n)O(log(n/ ) log(1/ )). This is motivated by the statistical question of whether an empirical estimate of this average allows us to recover a good approximation to the perceptron. Our result provides a mildly superpolynomial upper bound on the growth rate of the sample size required to learn Boolean perceptrons in the “restricted focus of attention” setting. In the process we also find some interesting geometrical properties of the vertices of the unit hypercube.	algorithm;approximation;bit array;boolean algebra;coefficient;computational learning theory;empirical risk minimization;euclidean distance;graph coloring;knapsack problem;linear inequality;local optimum;local search (optimization);map;overfitting;perceptron;polynomial;probably approximately correct learning;social inequality;time complexity	Paul W. Goldberg	2006	SIAM J. Discrete Math.	10.1137/S0895480103426765	combinatorics;discrete mathematics;boolean network;maximum satisfiability problem;karp–lipton theorem;mathematics;geometry;boolean function;upper and lower bounds;complete boolean algebra;boolean satisfiability problem;algorithm;parity function	Theory	16.05600257534562	23.797052322428044	169898
19c8a1930ebf7f0589b94c3aba69547fd82ffc18	learning without interaction requires separation		One of the key resources in large-scale learning systems is the number of rounds of communication between the server and the clients holding the data points. We study this resource for systems with two types of constraints on the communication from each of the clients: local differential privacy and limited number of bits communicated. For both models the number of rounds of communications is captured by the number of rounds of interaction when solving the learning problem in the statistical query (SQ) model. For many learning problems known efficient algorithms require many rounds of interaction. Yet little is known on whether this is actually necessary. In the context of classification in the PAC learning model, Kasiviswanathan et al. [KLNRS11] constructed an artificial class of functions that is PAC learnable with respect to a fixed distribution but cannot be learned by an efficient non-interactive (or one-round) SQ algorithm. Here we show that a similar separation holds for learning linear separators and decision lists without assumptions on the distribution. To prove this separation we show that noninteractive SQ algorithms can only learn function classes of low margin complexity, that is classes of functions that can be represented as large-margin linear separators.	algorithm;computational complexity theory;data point;decision list;differential privacy;interactivity;probably approximately correct learning;server (computing);sound quality	Amit Daniely;Vitaly Feldman	2018	CoRR		differential privacy;machine learning;data point;decision list;mathematics;artificial intelligence	ML	13.423284346825056	20.371950213715728	169989
b835a6e8a834fcb6716cd63568da37eee7d1ffb2	merlinization of complexity classes above bqp		Tomoyuki Morimae ∗ and Harumichi Nishimura † Department of Computer Science, Gunma University, 1-5-1 Tenjincho Kiryushi Gunma, 376-0052, Japan 2 Graduate School of Informatics, Nagoya University, Furocho, Chikusaku, Nagoya, Aichi, 464-8601, Japan Abstract We study how complexity classes above BQP, such as postBQP, postBQPFP, and SBQP, change if we “Merlinize” them, i.e., if we allow an extra input quantum state (or classical bit string) given by Merlin as witness. Main results are the following three: First, the Merlinized version of postBQP is equal to PSPACE. Second, if the Merlinized postBQP is restricted in such a way that the postselection probability is equal to all witness states, then the class is equal to PP. Finally, the Merlinization does not change the class SBQP.	bqp;bit array;complexity class;computer science;iso 8601;informatics;pspace;postbqp;postselection;quantum state;tomoyuki nishita	Tomoyuki Morimae;Harumichi Nishimura	2017	Quantum Information & Computation		discrete mathematics;postselection;mathematics;quantum state;quantum complexity theory;complexity class;pspace;bqp;bit array;postbqp	Theory	12.341023761559027	21.46061760929467	170308
496774783ed548d9d337a79e852415693dd6cda5	randomly rounding rationals with cardinality constraints and derandomizations	randomized rounding;satisfiability;col;random variable;randomized algorithm	We show how to generate randomized roundings of rational vectors that satisfy hard cardinality constraints and allow large deviations bounds. This improves and extends earlier results by Srinivasan (FOCS 2001), Gandhi et al. (FOCS 2002) and the author (STACS 2006). Roughly speaking, we show that also for rounding arbitrary rational vectors randomly or deterministically, it suffices to understand the problem for {0, 1 2 } vectors (which typically is much easier). So far, this was only known for vectors with entries in 2Z, l ∈ N. To prove the general case, we exhibit a number of results of independent interest, in particular, a quite useful lemma on negatively correlated random variables, an extension of de Werra’s (RAIRO 1971) coloring result for unimodular hypergraphs and necessary (and sufficient, though we do not prove this here) condition for a unimodular hypergraph to have a perfectly balanced non-trivial partial coloring. We also show a new solution for the general derandomization problem for rational matrices. Topic: Algorithms and datastructures, randomized algorithms, derandomization, theory.	deterministic algorithm;graph coloring;randomized algorithm;randomness;rounding;stacs;symposium on foundations of computer science;unimodular polynomial matrix	Benjamin Doerr	2007		10.1007/978-3-540-70918-3_38	random variable;mathematical optimization;randomized rounding;combinatorics;discrete mathematics;computer science;mathematics;randomized algorithm;algorithm;satisfiability	Theory	14.115517395650551	20.401209844553694	170614
805a1271ea3b4f193fc0fda6d989b6aa205a959e	decision trees for entity identification: approximation algorithms and hardness results	approximate algorithm;decision tree;ramsey theory;probability distribution;fault detection;greedy algorithm	We consider the problem of constructing decision trees for entity identification from a given relational table. The input is a table containing information about a set of entities over a fixed set of attributes and a probability distribution over the set of entities that specifies the likelihood of the occurrence of each entity. The goal is to construct a decision tree that identifies each entity unambiguously by testing the attribute values such that the average number of tests is minimized. This classical problem finds such diverse applications as efficient fault detection, species identification in biology, and efficient diagnosis in the field of medicine. Prior work mainly deals with the special case where the input table is binary and the probability distribution over the set of entities is uniform. We study the general problem involving arbitrary input tables and arbitrary probability distributions over the set of entities. We consider a natural greedy algorithm and prove an approximation guarantee of O(rK • log N), where N is the number of entities and K is the maximum number of distinct values of an attribute. The value rK is a suitably defined Ramsey number, which is at most log K. We show that it is NP-hard to approximate the problem within a factor of Ω(log N), even for binary tables (i.e. K=2). Thus, for the case of binary tables, our approximation algorithm is optimal up to constant factors (since r2=2). In addition, our analysis indicates a possible way of resolving a Ramsey-theoretic conjecture by Erdos.	approximation algorithm;binary number;bitwise operation;decision tree;entity;fault detection and isolation;greedy algorithm;hardness of approximation;np-hardness;ramsey's theorem;table (database);theory	Venkatesan T. Chakaravarthy;Vinayaka Pandit;Sambuddha Roy;Pranjal Awasthi;Mukesh K. Mohania	2007		10.1145/1265530.1265538	probability distribution;combinatorics;greedy algorithm;discrete mathematics;computer science;decision tree;ramsey theory;mathematics;fault detection and isolation;algorithm	DB	15.04962012828653	21.38827310409416	170766
74289572067a8ba3dbe1abf84d4a352b8bb4740f	preventing fairness gerrymandering: auditing and learning for subgroup fairness		We introduce a new family of fairness definitions that interpolate between statistical and individual notions of fairness, obtaining some of the best properties of each. We show that checking whether these notions are satisfied is computationally hard in the worst case, but give practical oracle-efficient algorithms for learning subject to these constraints, and confirm our findings with experiments.	algorithm;best, worst and average case;experiment;fairness measure;interpolation	Michael Kearns;Seth Neel;Aaron Roth;Zhiwei Steven Wu	2018			computational problem;mathematical optimization;machine learning;statistic;oracle;artificial intelligence;parity (mathematics);linear regression;mathematics;heuristic;polynomial;convergence (routing)	ML	14.311371083896535	19.484945790681593	170781
199ba2f18b524e3d0b3d9d1ee61aaa92664251f9	tabu search for dna sequencing with false negatives and false positives	integer linear programming;false negative;low complexity;dna sequencing with errors;deoxyribonucleic acid;sequencing by hybridization;polynomial time;tabu search;false positive;dna sequence;integer linear program;heuristic algorithm	The paper deals with the problem of DNA (deoxyribonucleic acid) sequencing by hybridization. A computational phase of this approach, i.e. a construction of a DNA sequence from oligonucleotides, is NP-hard in the strong sense in case of errors. Thus, since the last problem does not admit a polynomial time solution, a need arises to construct ef®cient heuristics solving the problem. In the paper, such a heuristic algorithm based on tabu search is proposed. Computational tests have proved its low complexity and high accuracy for both types of errors: false negatives and false positives. Ó 2000 Elsevier Science B.V. All rights reserved.	algorithm;computation;computational complexity theory;entity framework;greedy algorithm;heuristic (computer science);search algorithm;tabu search;time complexity	Jacek Blazewicz;Piotr Formanowicz;Marta Kasprzak;Wojciech T. Markiewicz;Jan Weglarz	2000	European Journal of Operational Research	10.1016/S0377-2217(99)00456-7	sequencing by hybridization;heuristic;time complexity;mathematical optimization;dna sequencing;integer programming;type i and type ii errors;tabu search;computer science;bioinformatics;mathematics;dna;algorithm	AI	16.35868483561981	22.94256440903653	170963
d1c44da0926b0103ec719f8ec3a367ed0fe8cb71	all bits ax+b mod p are hard (extended abstract)	one-way function;prime p;single bit;mod p;extended abstract;bits ax;one way function	In this paper we show that for any one-way function f , being able to determine any single bit in ax+bmod p for a random Ω(jxj)-bit primep and randoma;b with probability only slightly better than 50% is equivalent to inverting f (x).	emoticon;one-way function	Mats Näslund	1996		10.1007/3-540-68697-5_10	combinatorics;discrete mathematics;computer science;mathematics;one-way function;algorithm	Theory	10.845757178569164	23.221911149562068	171113
4c6e637f66bd292ef3bbe10bb1d08c7150906e0c	impossibility of independence amplification in kolmogorov complexity theory	independent strings;dep;random strings;randomness extraction;kolmogorov complexity	The paper studies randomness extraction from sources with bounded independence and the issue of independence amplification of sources, using the framework of Kolmogorov complexity. The dependency of strings x and y is dep(x, y) = max{C(x)−C(x | y), C(y)−C(y | x)}, where C(·) denotes the Kolmogorov complexity. It is shown that there exists a computable Kolmogorov extractor f such that, for any two n-bit strings with complexity s(n) and dependency α(n), it outputs a string of length s(n) with complexity s(n) − α(n) conditioned by any one of the input strings. It is proven that the above are the optimal parameters a Kolmogorov extractor can achieve. It is shown that independence amplification cannot be effectively realized. Specifically, if (after excluding a trivial case) there exist computable functions f1 and f2 such that dep(f1(x, y), f2(x, y)) ≤ β(n) for all n-bit strings x and y with dep(x, y) ≤ α(n), then β(n) ≥ α(n)−O(logn).	computable function;computational complexity theory;emoticon;existential quantification;extractor (mathematics);kolmogorov complexity;randomness extractor	Marius Zimand	2010		10.1007/978-3-642-15155-2_61	combinatorics;discrete mathematics;computer science;data execution prevention;mathematics	Theory	10.104272355080735	25.22806995431093	173549
dfe258c262df2cdb4923213f4ccdb4656c69050a	local reconstruction of low-rank matrices and subspaces	matrix rigidity;local reconstruction;subspace approximation;sublinear time algorithms;low rank matrix reconstruction	We study the problem of reconstructing a low-rank matrix, where the input is an n × m matrix M over a field F and the goal is to reconstruct a (near-optimal) matrix M ′ that is lowrank and close to M under some distance function ∆. Furthermore, the reconstruction must be local, i.e., provides access to any desired entry of M ′ by reading only a few entries of the input M (ideally, independent of the matrix dimensions n and m). Our formulation of this problem is inspired by the local reconstruction framework of Saks and Seshadhri (SICOMP, 2010). Our main result is a local reconstruction algorithm for the case where ∆ is the normalized Hamming distance (between matrices). Given M that is -close to a matrix of rank d < 1/ (together with d and ), this algorithm computes with high probability a rank-d matrix M ′ that is O( √ d )-close to M . This is a local algorithm that proceeds in two phases. The preprocessing phase reads only Õ( √ d/ 3) random entries of M , and stores a small data structure. The query phase deterministically outputs a desired entry M ′ i,j by reading only the data structure and 2d additional entries of M . We also consider local reconstruction in an easier setting, where the algorithm can read an entire matrix column in a single operation. When ∆ is the normalized Hamming distance between vectors, we derive an algorithm that runs in polynomial time by applying our main result for matrix reconstruction. For comparison, when ∆ is the truncated Euclidean distance and F = R, we analyze sampling algorithms by using statistical learning tools.	data structure;deterministic algorithm;euclidean distance;hamming distance;local algorithm;machine learning;polynomial;preprocessor;siam journal on computing;sampling (signal processing);the matrix;time complexity;with high probability	Roee David;Elazar Goldenberg;Robert Krauthgamer	2015	Electronic Colloquium on Computational Complexity (ECCC)	10.1002/rsa.20720	matrix splitting;matrix function;mathematical optimization;hollow matrix;combinatorics;eigendecomposition of a matrix;sparse matrix;nonnegative matrix;single-entry matrix;band matrix;square matrix;mathematics;geometry;state-transition matrix;block matrix;matrix;integer matrix;symmetric matrix	Theory	14.453471995115553	23.56855504643084	174638
2085c58a80bb1421b469c73c650160b88e3d8e28	analysis of the space of search trees under the random insertion algorithm	insertion;arbre recherche;espacio;research tree;loi probabilite;ley probabilidad;algoritmo recursivo;espace;probability law;search trees;algorithme recursif;arbol investigacion;insercion;space;recursive algorithm	Abstract   The probability distribution of  S   n  , the space occupied by a search tree with branch factor  m  grown by the random insertion of  n  keys, is investigated. We sharpen the already known formula for the average of  S   n   and demonstrate that its variance experiences a surprising phase transition—from being asymptotically linear in  n  for 3 ≤  m  ≤ 26, to being a superlinear function of  n  for  m  > 26. We also prove that  S   n   is asymptotically normal if  m  ≤ 15.	algorithm;insertion sort	Hosam M. Mahmoud;Boris Pittel	1989	J. Algorithms	10.1016/0196-6774(89)90023-0	insertion;combinatorics;computer science;space;mathematics;algorithm;recursion	Theory	14.676214864949422	23.166684977625184	174807
ee69f73143b3956d8b48648084d29e47aa91c5a9	on the size of weights for threshold gates	neural nets;computational complexity;threshold function;68r05	We prove that if n is a power of 2 then there is a threshold function that on n inputs that requires weights of size around 2 (n log n)=2?n. This almost matches the known upper bounds. Warning: Essentially this paper has been published in SIAM Journal on Discrete Mathematics and is hence subject to copyright restrictions. It is for personal use only.	discrete mathematics;power of two	Johan Håstad	1994	SIAM J. Discrete Math.	10.1137/S0895480192235878	discrete mathematics;mathematics;computational complexity theory;artificial neural network;algorithm	Theory	11.464609935517775	21.86695795179065	174852
bf6d434eed90f4ee1622585cb11ea06ccd6076ea	(gap/s)eth hardness of svp		We prove the following quantitative hardness results for the Shortest Vector Problem in the ℓ<sub><i>p</i></sub> norm (SVP_p), where <i>n</i> is the rank of the input lattice.   For “almost all” <i>p</i> > <i>p</i><sub>0</sub> ≈ 2.1397, there is no 2<sup><i>n</i>/<i>C</i><sub><i>p</i></sub></sup>-time algorithm for SVP_p for some explicit (easily computable) constant <i>C</i><sub><i>p</i></sub> > 0 unless the (randomized) Strong Exponential Time Hypothesis (SETH) is false. (E.g., for <i>p</i> ≥ 3, <i>C</i><sub><i>p</i></sub> < 1 + (<i>p</i>+3) 2<sup>−<i>p</i></sup> + 10 <i>p</i><sup>2</sup> 2<sup>−2<i>p</i></sup>.)   For any 1 ≤ <i>p</i> ≤ ∞, there is no 2<sup><i>o</i>(<i>n</i>)</sup>-time algorithm for SVP_p unless the non-uniform Gap-Exponential Time Hypothesis (Gap-ETH) is false. Furthermore, for each such <i>p</i>, there exists a constant γ<sub><i>p</i></sub> > 1 such that the same result holds even for γ<sub><i>p</i></sub>-approximate SVP_p.   For <i>p</i> > 2, the above statement holds under the weaker assumption of randomized Gap-ETH. I.e., there is no 2<sup><i>o</i>(<i>n</i>)</sup>-time algorithm for γ<sub><i>p</i></sub>-approximate SVP_p unless randomized Gap-ETH is false.   See http://arxiv.org/abs/1712.00942 for a complete exposition.		Divesh Aggarwal;Noah Stephens-Davidowitz	2018		10.1145/3188745.3188840	mathematics;mathematical analysis;combinatorics;lattice (order);lattice problem;exponential time hypothesis;norm (mathematics)	Theory	10.502111499255504	21.3656583693984	174892
65c9f1fa65958bf427c1553cf948e8324238528d	tight bounds on expected time to add correctly and add mostly correctly	arithmetique ordinateur;algorithme;algorithm;computer arithmetic;addition;aritmetica ordenador;adiccion;algoritmo	We consider the problem of adding two n-bit numbers which are chosen independently and uniformly at random where the adder is circuit of AND, OR, and NOT gates of fanin two. The fastest currently known worst-case adder has running time logn+O( p logn) [Khrapchenko]. We rst present a circuit which adds at least 1 fraction of pairs of numbers correctly and has running time log log (n ) +O( p log log (n )). We then prove that this running time is optimal. Next we present a circuit which always produces the correct answer. We show this circuit adds two n-bit numbers from the uniform distribution in expected 1 2 logn+ O( p logn) time, a speed up factor of two over the best possible running time of a worst-case adder. We prove that this expected running time is optimal. Computer Science Division, UC Berkeley, CA 94720. Supported by NSF grant number CCR-9201092. Computer Science Division, UC Berkeley, CA 94720. Supported by National Physical Science Consortium (NPSC) Fellowship.	adder (electronics);best, worst and average case;computer science;consortium;fan-in;fastest;ibm notes;time complexity;uc browser	Peter Gemmell;Mor Harchol-Balter	1994	Inf. Process. Lett.	10.1016/0020-0190(94)90031-0	combinatorics;discrete mathematics;mathematics;addition;algorithm	Theory	11.59888254805012	21.99458960191069	175010
b98e1296f9e958c729e17f786ea69eded32ee7ab	pseudorandom generators for low degree polynomials from algebraic geometry codes		Constructing pseudorandom generators for low degree polynomials has received a considerable attention in the past decade. Viola [CC 2009], following an exciting line of research, constructed a pseudorandom generator for degree d polynomials in n variables, over any prime field. The seed length used is O(d log n + d2d), and thus this construction yields a non-trivial result only for d = O(log n). Bogdanov [STOC 2005] presented a pseudorandom generator with seed length O(d4 log n). However, it is promised to work only for fields of size Ω(d10 log n). The main result of this paper is a construction of a pseudorandom generator for low degree polynomials based on algebraic geometry codes. Our pseudorandom generator works for fields of size Ω(d6) and has seed length O(d4 log n). The running time of our construction is nO(d 4). We postulate a conjecture concerning the explicitness of a certain Riemann-Roch space in function fields. If true, the running time of our pseudorandom generator would be reduced to nO(1). We also make a first step at affirming the conjecture. ∗Department of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 76100, Israel. Email: gil.cohen@weizmann.ac.il. Supported by an ISF grant and by the I-CORE Program of the Planning and Budgeting Committee. †The Blavatnik School of Computer Science, Tel-Aviv University, Israel, 69978. Email: amnon@cs.tau.ac.il. Supported by ISF grant no. 1090/10. ISSN 1433-8092 Electronic Colloquium on Computational Complexity, Report No. 155 (2013)	algebraic equation;cc system;code;computer science;electronic colloquium on computational complexity;email;ink serialized format;international standard serial number;polynomial;pseudorandom generator;pseudorandom number generator;pseudorandomness;symposium on theory of computing;time complexity	Gil Cohen;Amnon Ta-Shma	2013	Electronic Colloquium on Computational Complexity (ECCC)		difference polynomials;pseudorandom generators for polynomials;combinatorics;discrete mathematics;mathematics;pseudorandom generator;pseudorandomness;pseudorandom generator theorem;algebra	Theory	11.879551277214832	21.632649761632788	175791
65ec8f0317d11c8e537c7d85d58219fc8d13d727	infeasibility of instance compression and succinct pcps for np	one way function;collision resistant hash functions;satisfiability;kernalization;dominating set;probabilistically checkable proofs;computational complexity;instance complexity;probabilistically checkable proof systems;polynomial time;integer program	"""The OR-SAT problem asks, given Boolean formulae @f""""1,...,@f""""m each of size at most n, whether at least one of the @f""""i's is satisfiable. We show that there is no reduction from OR-SAT to any set A where the length of the output is bounded by a polynomial in n, unless NP@?coNP/poly, and the Polynomial-Time Hierarchy collapses. This result settles an open problem proposed by Bodlaender et al. (2008) [6] and Harnik and Naor (2006) [20] and has a number of implications. (i) A number of parametric NP problems, including Satisfiability, Clique, Dominating Set and Integer Programming, are not instance compressible or polynomially kernelizable unless NP@?coNP/poly. (ii) Satisfiability does not have PCPs of size polynomial in the number of variables unless NP@?coNP/poly. (iii) An approach of Harnik and Naor to constructing collision-resistant hash functions from one-way functions is unlikely to be viable in its present form. (iv) (Buhrman-Hitchcock) There are no subexponential-size hard sets for NP unless NP is in co-NP/poly. We also study probabilistic variants of compression, and show various results about and connections between these variants. To this end, we introduce a new strong derandomization hypothesis, the Oracle Derandomization Hypothesis, and discuss how it relates to traditional derandomization assumptions."""	np (complexity)	Lance Fortnow;Rahul Santhanam	2011	J. Comput. Syst. Sci.	10.1016/j.jcss.2010.06.007	time complexity;combinatorics;discrete mathematics;dominating set;computer science;mathematics;computational complexity theory;one-way function;algorithm;satisfiability	Theory	11.604695220871566	20.16249168282029	180344
080ce45f5e54b93b177d5f4ea10c24c304835a43	deterministic time-space trade-offs for k-sum	004;3sum ksum time space tradeoff algorithm	Given a set of numbers, the k-SUM problem asks for a subset of k numbers that sums to zero. When the numbers are integers, the time and space complexity of k-SUM is generally studied in the word-RAM model; when the numbers are reals, the complexity is studied in the real-RAM model, and space is measured by the number of reals held in memory at any point. We present a time and space efficient deterministic self-reduction for the k-SUM problem which holds for both models, and has many interesting consequences. To illustrate: 3-SUM is in deterministic time O(n2 lg lg(n)/ lg(n)) and space O (√ n lg(n) lg lg(n) ) . In general, any polylogarithmic-time improvement over quadratic time for 3-SUM can be converted into an algorithm with an identical time improvement but low space complexity as well. 3-SUM is in deterministic time O(n2) and space O( √ n), derandomizing an algorithm of Wang. A popular conjecture states that 3-SUM requires n2−o(1) time on the word-RAM. We show that the 3-SUM Conjecture is in fact equivalent to the (seemingly weaker) conjecture that every O(n.51)-space algorithm for 3-SUM requires at least n2−o(1) time on the word-RAM. For k ≥ 4, k-SUM is in deterministic O(nk−2+2/k) time and O( √ n) space. 1998 ACM Subject Classification F.2.1 Numerical Algorithms and Problems	algorithm;dspace;dtime;numerical method;polylogarithmic function;random-access memory;time complexity	Andrea Lincoln;Virginia Vassilevska Williams;Joshua R. Wang;Richard Ryan Williams	2016		10.4230/LIPIcs.ICALP.2016.58	mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	12.347877701506007	23.164711350661527	180352
8de28840907b24be8c237c3fdc212c789ff8c907	phase transitions and random quantum satisfiability	statistical mechanics;satisfiability;decision problem;quantum phase transition;phase transition;quantum computer;quantum physics;computational complexity;disordered system;neural network	Alongside the effort underway to build quantum computers, it is important to better understand which classes of problems they will find easy and which others even they will find intractable. We study random ensembles of the QMA1-complete quantum satisfiability (QSAT) problem introduced by Bravyi [1]. QSAT appropriately generalizes the NP-complete classical satisfiability (SAT) problem. We show that, as the density of clauses/projectors is varied, the ensembles exhibit quantum phase transitions between phases that are satisfiable and unsatisfiable. Remarkably, almost all instances of QSAT for any hypergraph exhibit the same dimension of the satisfying manifold. This establishes the QSAT decision problem as equivalent to a, potentially new, graph theoretic problem and that the hardest typical instances are likely to be localized in a bounded range of clause density.	boolean satisfiability problem;computer;decision problem;karp's 21 np-complete problems;movie projector;norm (social);quantum computing;theory;true quantified boolean formula	Christopher R. Laumann;Roderich Moessner;Antonello Scardicchio;S. L. Sondhi	2009	CoRR		phase transition;combinatorics;discrete mathematics;quantum phase transition;statistical mechanics;decision problem;mathematics;quantum computer;computational complexity theory;quantum algorithm;physics;algorithm;quantum mechanics;satisfiability	Theory	12.15647519558241	18.850955807201686	181098
0e0ad702ab7a394377b38714fe04cae804e99da3	deterministically testing sparse polynomial identities of unbounded degree	derandomization;polynomial identity testing;upper bound;number theory;theory of computing;theory of computation;arithmetic circuits;arithmetic progression;arithmetic circuit	We present two deterministic algorithms for the arithmetic circuit identity testing problem. The running time of our algorithms is polynomially bounded in s and m, where s is the size of the circuit and m is an upper bound on the number monomials with non-zero coefficients in its standard representation. The running time of our algorithms also has a logarithmic dependence on the degree of the polynomial but, since a circuit of size s can only compute polynomials of degree at most 2s, the running time does not depend on its degree. Before this work, all such deterministic algorithms had a polynomial dependence on the degree and therefore an exponential dependence on s. Our first algorithm works over the integers and it requires only black-box access to the given circuit. Though this algorithm is quite simple, the analysis of why it works relies on Linnik’s Theorem, a deep result from number theory about the size of the smallest prime in an arithmetic progression. Our second algorithm, unlike the first, uses elementary arguments and works over any integral domains, but it uses the circuit in a less restricted manner. In both cases the running time has a logarithmic dependence on the largest coefficient of the polynomial.	algorithm;arithmetic circuit complexity;black box;coefficient;color gradient;monomial;polynomial;sparse matrix;time complexity	Markus Bläser;Moritz Hardt;Richard J. Lipton;Nisheeth K. Vishnoi	2009	Inf. Process. Lett.	10.1016/j.ipl.2008.09.029	combinatorics;number theory;discrete mathematics;arithmetic progression;theory of computation;degree of a polynomial;mathematics;arithmetic circuit complexity;upper and lower bounds;pseudo-polynomial time;algorithm;algebra	Theory	10.217257507735045	22.15810481514153	181183
87b8043d28144d18b7764dfef4bdbdc9d5f350a6	improving ppsz for 3-sat using critical variables	004;satisfiability;data structure;sat satisfiability randomized exponential time algorithm 3 sat 4 sat	A critical variable of a satisfiable CNF formula is a variable that has the same value in all satisfying assignments. Using a simple case distinction on the fraction of critical variables of a CNF formula, we improve the running time for 3-SAT from O(1.32216) by Rolf [9] to O(1.32153). Using a different approach, Iwama et al. [4] very recently achieved a running time of O(1.32113). Our method nicely combines with theirs, yielding the currently fastest known algorithm with running time O(1.32065). We also improve the bound for 4-SAT from O(1.47390) [5] to O(1.46928), where O(1.46981) can be obtained using the methods of [5] and [9].	algorithm;boolean satisfiability problem;conjunctive normal form;fastest;time complexity	Timon Hertli;Robin A. Moser;Dominik Scheder	2011		10.4230/LIPIcs.STACS.2011.237	combinatorics;discrete mathematics;data structure;#sat;computer science;mathematics;programming language;algorithm;satisfiability	Robotics	10.12552758907428	19.40226142094242	181794
3b5e8de602ae6482d3cfdf17d0b814c030492007	classification of highly nonlinear boolean power functions with a randomised algorithm for checking normality	boolean functions;power functions;asymmetric monte carlo;normality;power function;monte carlo algorithm;monte carlo;boolean function;exhaustive search	A Boolean function is called normal if it is constant on flats of certain dimensions. This property is relevant for the construction and analysis of cryptosystems. This paper presents an asymmetric Monte Carlo algorithm to determine whether a given Boolean function is normal. Our algorithm is far faster than the best known (deterministic) algorithm of Daum et al. In a first phase, it checks for flats of low dimension whether the given Boolean function is constant on them and combines such flats to flats of higher dimension in a second phase. This way, the algorithm is much faster than exhaustive search. Moreover, the algorithm benefits from randomising the first phase. As an application, we determine the level of normality for several, highly nonlinear Boolean power functions.	boolean algebra;brute-force search;cryptosystem;deterministic algorithm;monte carlo algorithm;monte carlo method;nonlinear system;randomized algorithm	An Braeken;Christopher Wolf;Bart Preneel	2004	IACR Cryptology ePrint Archive		monte carlo algorithm;boolean function;brute-force search;monte carlo method;monte carlo method in statistical physics;discrete mathematics;hybrid monte carlo;algorithm;rejection sampling;mathematical optimization;monte carlo integration;mathematics	Theory	10.228982928075332	22.216478180479363	181872
1759ad89989995cb4d6142a316afc0499c0f6b07	on the (non) np-hardness of computing circuit complexity	circuit lower bounds;reductions;004;np completeness;projections;minimum circuit size problem;circuit lower bounds minimum circuit size problem np completeness projections reductions	The Minimum Circuit Size Problem (MCSP) is: given the truth table of a Boolean function f and a size parameter k, is the circuit complexity of f at most k? This is the definitive problem of circuit synthesis, and it has been studied since the 1950s. Unlike many problems of its kind, MCSP is not known to be NP-hard, yet an efficient algorithm for this problem also seems very unlikely: for example, MCSP ∈ P would imply there are no pseudorandom functions. Although most NP-complete problems are complete under strong “local” reduction notions such as polylogarithmic-time projections, we show that MCSP is provably not NP-hard under O(n1/2−ε)-time projections, for every ε > 0, and is not NP-hard under randomized O(n1/5−ε)-time projections, for every ε > 0. We prove that the NP-hardness of MCSP under (logtime-uniform) AC0 reductions would imply extremely strong lower bounds: NP 6⊂ P/poly and E 6⊂ i.o.-SIZE(2δn) for some δ > 0 (hence P = BPP also follows). We A preliminary version of this paper appeared in the Proceedings of the 30th IEEE Conference on Computational Complexity, 2015 [21]. ∗Supported by NSF CCF-1212372. †Supported in part by a David Morgenthaler II Faculty Fellowship, a Sloan Fellowship, and NSF CCF-1212372. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. ACM Classification: F.2.3, F.1.3 AMS Classification: 68Q15, 68Q17	ac0;acm computing classification system;bpp (complexity);binary file;binary search algorithm;church encoding;circuit complexity;complexity class;computational complexity theory;dyadic transformation;encode;exptime;ibm notes;karp's 21 np-complete problems;nc (complexity);np-completeness;np-hardness;p/poly;parsing;period-doubling bifurcation;polylogarithmic function;polynomial;polynomial-time reduction;power of two;pseudorandomness;randomized algorithm;sparse matrix;theory of computing;time complexity;unary coding;unary operation;uniform resource identifier	Cody Murray;Richard Ryan Williams	2014	Electronic Colloquium on Computational Complexity (ECCC)	10.4230/LIPIcs.CCC.2015.365	combinatorics;discrete mathematics;np-complete;computer science;mathematics;algorithm;reduction	Theory	11.942370872358984	21.419325372480433	182451
0982f463bf08ec4e49c1c2391129ab7960434727	certification of algorithm 224: evaluation of determinant		p r o c e d u r e TREESORT 3 (M, n) ; v a l u e n; a r r a y M; i n t e g e r n; c o m m e n t TREESORT 3 is a major revision of TREESORT [R. W. Floyd, Alg. 113, Comm. A C M 5 (Aug. 1962), 434] suggested by H E A P S O R T [J. W. J. Williams, Alg. 232, Comm. A CM 7 (June 1964), 347] from which it differs in being an in-place sort. I t is shor ter and probably faster, requir ing fewer comparisons and only one division. I t sorts the ar ray M[1 :n], requir ing no more t han 2 X ( 2 ~ ' p 2 ) X (pl ) , or approximate ly 2 X n X ( log2(n ) l ) comparisons and half as many exchanges in the worst case to sort n = 2 Tp 1 items. The a lgor i thm is most easily followed if M is t hough t of as a tree, wi th M[j+2] the fa ther of M[j] for 1 < j ~ n; b e g i n p r o c e d u r e exchange (x,y); real x,y; b e g i n r e a l t; t := x; x := y; y := t e n d exchange; p r o c e d u r e siftup (i ,n); v a l u e i, n; i n t e g e r i, n; c o m m e n t M[i] is moved upward in the subtree of M[l :n] of which it is the root ; b e g i n r ea l copy; i n t e g e r j ; copy := M[i]; loop: j := 2 X i ; i f j =< n t h e n b e g i n i f j < n t h e n b e g i n i f M [ j + i ] > M[j] t h e n j := j + 1 e n d ; i f M[j] > copy t h e n b e g i n M[i] := M[j]; i := j ; go to loop e n d e n d ; M[i] := copy e n d siftup; i n t e g e r i ; fo r i := n ÷ 2 s t e p --1 u n t i l 2 do siftup (i ,n); for i := n s t e p --1 u n t i l 2 do b e g i n siftup (1,i) ; c o m m e n t M[j+2] ~ M[j] for 1 < j ~ i ; exchange (M[1], M[i]); c o m m e n t M[i:n] is fully sor ted; e n d e n d TREESORT 3	approximation algorithm;best, worst and average case;binary heap;fo (complexity);goto;han unification;hough transform;in-place algorithm;loop (programming language);shor's algorithm;tree (data structure);tree sort	Vic Hasselblad;Jeff Rulifson	1964	Commun. ACM	10.1145/355588.365108	theoretical computer science;certification;computer science	NLP	13.524214168107733	23.024971880478816	182638
3071eefaca2766d90bdb43d5ee7e1a34d50b6029	binary partitions with approximate minimum impurity		The problem of splitting attributes is one of the main steps in the construction of decision trees. In order to decide the best split, impurity measures such as Entropy and Gini are widely used. In practice, Decision-tree inducers use heuristics for finding splits with small impurity when they consider nominal attributes with a large number of distinct values. However, there are no known guarantees for the quality of the splits obtained by these heuristics. To fill this gap, we propose two new splitting procedures that provably achieve near-optimal impurity. The first one, Hypercube Cover, is closely related with the well-established Twoing method [Breiman et al. 84]. We prove that Hypercube Cover provides a 2-approximation of the minimum impurity for a broad class of impurity measures that includes both Gini and Entropy. In addition, we propose a very simple and efficient procedure that provides a constant approximation for the same class of impurity measures. We complement our study with a number of experiments that provide evidence that our methods are interesting candidates to be employed in splitting nominal attributes with many values during decision tree/random forest induction	approximation;computational complexity theory;decision tree;experiment;heuristic (computer science);horner's method;random forest;whole earth 'lectronic link	Eduardo Sany Laber;Marco Molinaro;Felipe de A. Mello Pereira	2018			discrete mathematics;artificial intelligence;impurity;computer science;pattern recognition;binary number	ML	17.223774597421894	20.94715339615229	183435
5711952baf4787613d46f5679e22c317f984c68c	the monotone complexity of k-clique on random graphs	68q87;68q17;monotone circuits;average case complexity;clique;quasi sunflowers	It is widely suspected that Erd\H{o}s-R\'enyi random graphs are a source of hard instances for clique problems. Giving further evidence for this belief, we prove the first average-case hardness result for the $k$-clique problem on monotone circuits. Specifically, we show that no monotone circuit of size $O(n^{k/4})$ solves the $k$-clique problem with high probability on $\ER(n,p)$ for two sufficiently far-apart threshold functions $p(n)$ (for instance $n^{-2/(k-1)}$ and $2n^{-2/(k-1)}$). Moreover, the exponent $k/4$ in this result is tight up to an additive constant. One technical contribution of this paper is the introduction of {\em quasi-sunflowers}, a new relaxation of sunflowers in which petals may overlap slightly on average. A ``quasi-sunflower lemma'' (\`a la the Erd\H{o}s-Rado sunflower lemma) leads to our novel lower bounds within Razborov's method of approximations.	approximation;best, worst and average case;circuit complexity;clique (graph theory);clique problem;linear algebra;linear programming relaxation;random graph;utility functions on indivisible goods;with high probability;monotone	Benjamin Rossman	2010	2010 IEEE 51st Annual Symposium on Foundations of Computer Science	10.1137/110839059	clique;mathematical optimization;combinatorics;discrete mathematics;average-case complexity;mathematics;algorithm	Theory	10.299485741853017	20.695556876898547	184455
a35506d907ece830a7eb50d1c5228e2b5d866afc	more efficient algorithms for closest string and substring problems	informatica;closest string;parameterized complexity;algorithm complexity;science nature;temps polynomial;time complexity;approximation algorithms;probleme np complet;efficient algorithm;complejidad algoritmo;approximation algorithm;alfabeto;biology;biologia;68wxx;input;68w32;closest substring;polynomial time algorithm;41a10;complexite temps;complexite algorithme;pregunta respuesta;exact algorithm;algoritmo aproximacion;entree ordinateur;68xx;polynomial time;algorithme polynomial;question reponse;ciencias naturales;problema np completo;informatique;computer science;fixed parameter algorithm;natural science;entrada ordenador;algorithme approximation;complejidad tiempo;computational biology;68w25;alphabet;biologie;np complete problem;question answering;tiempo polinomial	The closest string problem and the closest substring problem are all natural theoretical computer science problems and find important applications in computational biology. Given $n$ input strings, the closest string (substring) problem finds a new string within distance $d$ to (a substring of) each input string and such that $d$ is minimized. Both problems are NP-complete. In this paper we propose new algorithms for these two problems. For the closest string problem, we developed an exact algorithm with time complexity $O(n|\Sigma|^{O(d)})$, where $\Sigma$ is the alphabet. This improves the previously best known result $O(nd^{O(d)})$ and results into a polynomial time algorithm when $d=O(\log n)$. By using this algorithm, a polynomial time approximation scheme (PTAS) for the closest string problem is also given with time complexity $O(n^{O(\epsilon^{-2})})$, improving the previously best known $O(n^{O(\epsilon^{-2}\log\frac{1}{\epsilon})})$ PTAS. A new algorithm for the closest substring problem is also proposed. Finally, we prove that a restricted version of the closest substring problem has the same parameterized complexity as the closest substring, answering an open question in the literature.	algorithm;closest string;substring	Bin Ma;Xiaoming Sun	2009	SIAM J. Comput.	10.1137/080739069	time complexity;mathematical optimization;combinatorics;approximate string matching;computer science;substring;mathematics;approximation algorithm;algorithm;string searching algorithm	Theory	15.877528575852978	25.028914683484476	184764
5debf68b4dfb8d94be08fce79669b67745f1ec67	list-decoding reed-muller codes over small fields	list decoding;self correctors;reed muller code;minimum distance;error rate;reed muller codes;information theory;fitting polynomials	We present the first local list-decoding algorithm for the rth order Reed-Muller code RM(2,m) over F for r ≥ 2. Given an oracle for a received word R: Fm -< F, our randomized local list-decoding algorithm produces a list containing all degree r polynomials within relative distance (2-r - ε) from R for any ε < 0 in time poly(mr,ε-r). The list size could be exponential in m at radius 2-r, so our bound is optimal in the local setting. Since RM(2,m) has relative distance 2-r, our algorithm beats the Johnson bound for r ≥ 2. In the setting where we are allowed running-time polynomial in the block-length, we show that list-decoding is possible up to even larger radii, beyond the minimum distance. We give a deterministic list-decoder that works at error rate below J(21-r), where J(δ) denotes the Johnson radius for minimum distance δ. This shows that RM(2,m) codes are list-decodable up to radius η for any constant η < 1/2 in time polynomial in the block-length. Over small fields Fq, we present list-decoding algorithms in both the global and local settings that work up to the list-decoding radius. We conjecture that the list-decoding radius approaches the minimum distance (like over F), and prove this holds true when the degree is divisible by q-1.	johnson bound;list decoding;polynomial;randomized algorithm;reed–muller code;time complexity	Parikshit Gopalan;Adam R. Klivans;David Zuckerman	2008		10.1145/1374376.1374417	reed–muller code;combinatorics;discrete mathematics;information theory;mathematics;algorithm;statistics	Theory	10.9033214380918	22.981236217385973	184997
55d1417e918390978b945c97bc2bdd9cb5f7204d	a 7/2-approximation algorithm for the maximum duo-preservation string mapping problem	004;polynomial approximation max duo preservation string mapping problem min common string partition problem local search	This paper presents a simple 7/2-approximation algorithm for the max duo-preservation string mapping (MPSM) problem. This problem is complementary to the classical and well studied min common string partition problem (MCSP), that computes the minimal edit distance between two strings when the only operation allowed is to shift blocks of characters. The algorithm improves on the previously best known 4-approximation algorithm by computing a simple local optimum. 1998 ACM Subject Classification F.2.2 Nonnumerical Algorithms and Problems	algorithm;edit distance;local optimum;maxima and minima;partition problem	Nicolas Boria;Gianpiero Cabodi;Paolo Camurati;Marco Palena;Paolo Pasini;Stefano Quer	2016		10.4230/LIPIcs.CPM.2016.11	mathematical optimization;combinatorics;discrete mathematics;approximate string matching;commentz-walter algorithm;string;computer science;wagner–fischer algorithm;boyer–moore string search algorithm;mathematics;string-to-string correction problem;string metric;string searching algorithm	Theory	15.561000524476631	24.47088792045778	186009
0d2fbbbe4af8841f6389e07d0d018a5068bb6cb5	polynomial-time algorithms for probabilistic solutions of parameter-dependent linear matrix inequalities	optimisation sous contrainte;constrained optimization;elipsoide;optimal solution;desigualdad matricial lineal;metodo polinomial;algoritmo aleatorizado;randomized algorithms;teorema existencia;computacion informatica;temps polynomial;complexite calcul;stopping rule;existence theorem;grupo de excelencia;algorithme randomise;probabilistic approach;stopping rules;regla parada;feasibility;optimizacion con restriccion;polynomial time algorithm;probabilistic solutions;aleatorizacion;complejidad computacion;linear matrix inequality;ellipsoide;polynomial method;computational complexity;ciencias basicas y experimentales;enfoque probabilista;approche probabiliste;polynomial time;parameter dependent linear matrix inequalities;randomized algorithm;randomisation;inegalite matricielle lineaire;optimization;randomization;methode polynomiale;ellipsoid;theoreme existence;practicabilidad;faisabilite;regle arret;tiempo polinomial	A randomized approach is considered for a feasibility problem on a parameter-dependent linear matrix inequality (LMI). In particular, a gradient-based and an ellipsoid-based randomized algorithms are improved by introduction of a stopping rule. The improved algorithms stop after a bounded number of iterations and this bound is of polynomial order in the problem size. When the algorithms stop, either of the following two events occurs: (i) they find with high confidence a probabilistic solution, which satisfies the given LMI for most of the parameter values; (ii) they detect in an approximate sense the non-existence of a deterministic solution, which satisfies the given LMI for all the parameter values. These results are important because the original randomized algorithms have issues to be settled on detection of convergence, on the speed of convergence, and on the assumption of feasibility. The improved algorithms can be adapted for an optimization problem constrained by a parameter-dependent LMI. A numerical example shows the efficacy of the proposed algorithms.	algorithm;linear matrix inequality;polynomial;time complexity	Yasuaki Oishi	2007	Automatica	10.1016/j.automatica.2006.09.020	randomized algorithms as zero-sum games;mathematical optimization;constrained optimization;combinatorics;probabilistic analysis of algorithms;calculus;mathematics;randomized algorithm	Theory	14.626219524288459	19.47706578114046	187846
bc10237763fafdc2b3d71a072886c6d0b7476b91	complexity of controlling nearly single-peaked elections revisited		In this paper, we investigate the complexity of Constructive Control by Adding/Deleting Votes (CCAV/CCDV) for r -approval, Condorcet, Maximin and Copelandα in k-axes and k-candidate partition single-peaked elections. In general, we prove that CCAV and CCDV for most of the voting correspondences mentioned above are NP-hard even when k is a very small constant. Exceptions are CCAV and CCDV for Condorcet and CCAV for r -approval in k-axes singlepeaked elections, which we show to be fixed-parameter tractable with respect to k . In addition, we give a polynomial-time algorithm for recognizing 2-axes elections, resolving an open question.	algorithm;cobham's thesis;minimax;parameterized complexity;time complexity	Yongjie Yang	2018			computer science;artificial intelligence;machine learning;constructive;condorcet method;discrete mathematics;partition (number theory);parameterized complexity;voting;minimax	AI	15.63913072103484	19.569105256952124	187935
5e5d1a2c5c8d760cd90e0c75ec2b0e97a4303f60	inner-product based wavelet synopses for range-sum queries	busqueda informacion;transformation ondelette;funcion haar;database system;fonction orthogonale;range query;threshold detection;fonction haar;algorithme glouton;geometrie algorithmique;information retrieval;haar function;heuristic method;temps lineaire;erreur quadratique moyenne;interrogation base donnee;computational geometry;interrogacion base datos;inner product;metodo heuristico;tiempo lineal;by product;greedy heuristic;busquedas dentro de un rango;detection seuil;deteccion umbral;requete a intervalle;recherche information;sous produit;mean square error;subproducto;linear time;greedy algorithm;algoritmo gloton;orthogonal function;geometria computacional;methode heuristique;transformacion ondita;error medio cuadratico;funcion ortogonal;database query;wavelet transformation	In recent years wavelet based synopses were shown to be effective for approximate queries in database systems. The simplest wavelet synopses are constructed by computing the Haar transform over a vector consisting of either the raw-data or the prefix-sums of the data, and using a greedy-euristic to select the wavelet coefficients that are kept in the synopsis. The greedy-heuristic is known to be optimal for point queries w.r.t. the mean-squared-error, but no similar efficient optimality result was known for range-sum queries, for which the effectiveness of such synopses was only shown experimentally.#R##N##R##N#We construct an operator that defines a norm that is equivalent to the mean-squared error over all possible range-sum queries, where the norm is measured on the prefix-sums vector. We show that the Haar basis (and in fact any wavelet basis) is orthogonal w.r.t. the inner product defined by this novel operator. This allows us to use Parseval-based thresholding, and thus obtain the first linear time construction of a provably optimal wavelet synopsis for range-sum queries. We show that the new thresholding is very similar to the greedy-heuristic that is based on point queries.#R##N##R##N#For the case of range-sum queries over the raw data, we define a similar operator, and show that Haar basis is not orthogonal w.r.t. the inner product defined by this operator.		Yossi Matias;Daniel Urieli	2006		10.1007/11841036_46	greedy algorithm;computational geometry;computer science;calculus;mathematics;geometry;algorithm	DB	14.749929124221012	24.215171946507823	188750
dbeb6077ff96a58340ca49d95943071beffe8cf5	a note on sampling a tape-file	random sampling;data processing	The problem of selecting a random sample of precisely <italic>n</italic> records from a tape film containing <italic>N</italic> records sometimes arises in data processing applications. If a means of obtaining a random sample of <italic>n</italic> integers <italic>r</italic><subscrpt>1</subscrpt>, <italic>r</italic><subscrpt>2</subscrpt>, … ,<italic>r<subscrpt>n</subscrpt></italic> selected from the <italic>N</italic> integers 1, 2, … , <italic>N</italic> is available, the problem is theoretically simple. All that need be done is to then select the <italic>r</italic><subscrpt>1</subscrpt>th, <italic>r</italic><subscrpt>2</subscrpt>th, … , <italic>r<subscrpt>n</subscrpt></italic>th records.	sampling (signal processing)	Terence G. Jones	1962	Commun. ACM	10.1145/367766.368159	sampling;combinatorics;data processing;computer science;theoretical computer science;mathematics;statistics	Theory	14.468478849273017	23.49506277088201	189014
85fec281988a93623297ca3744cd230f16b21289	the decision-tree complexity of element distinctness	decision tree;complexite calcul;lower bounds;arbol decision;complejidad computacion;computational complexity;borne inferieure;decision trees;arbre decision;lower bound;cota inferior	Abstract   The E lement  D istinctness  problem is to determine, given  n  variables from some domain, whether the variables have distinct values. We prove that every decision tree for E lement  D istinctness  (over any domain of size at least  n ) requires height Ω( n √log  n ).	decision tree model;element distinctness problem	Ravi B. Boppana	1994	Inf. Process. Lett.	10.1016/0020-0190(94)00154-5	combinatorics;computer science;decision tree;mathematics;algorithm	DB	17.297709620338754	25.031308956884395	189623
72a97e11870a8212baf8932f655b539316ea0c8d	exact quantum query complexity of \text exact_k, l^n		In the decision tree model, one’s task is to compute a boolean function f : {0, 1} → {0, 1} on an input x ∈ {0, 1} that is accessible via queries to a black box (the black box hides the bits xi). In the quantum case, classical queries and computation are replaced by unitary transformations. A quantum algorithm is exact if it always outputs the correct value of f (in contrast to the standard model of quantum algorithms where the algorithm is allowed to be incorrect with a small probability). The minimum number of queries for an exact quantum algorithm computing the function f is denoted by QE(f). We consider the following n bit function with 0 ≤ k ≤ l ≤ n:	black box;computation;decision tree model;quantum algorithm	Andris Ambainis;Janis Iraids;Daniel Nagaj	2017		10.1007/978-3-319-51963-0_19	combinatorics;discrete mathematics;theoretical computer science;mathematics	Theory	10.567080402201968	23.637301602053412	189784
b41470c0d7ddbc6db0537e5aa8eba5cdee8620eb	search to decision reduction for the learning with errors over rings problem	silicon;lattices;encryption;noise cryptography;spherical noise distribution decision reduction learning with error rings problem;polynomials;vectors;cryptography;lattices polynomials search problems encryption vectors silicon;search problems;noise	In this short note, we give a self-contained proof of equivalence between the search and decision versions of the Learning with Error Problem over Rings problem (Lyuba-shevsky, Peikert, Regev 2010) for spherical noise distributions.	learning with errors;turing completeness	Vadim Lyubashevsky	2011	2011 IEEE Information Theory Workshop	10.1109/ITW.2011.6089491	combinatorics;discrete mathematics;theoretical computer science;mathematics	Theory	10.512888814992493	22.838149731483064	191769
30d957d425bbe6e98d2d8184b0bdd198fbd94c0c	a note on property testing sum of squares and multivariate polynomial interpolation		In this paper, we investigate property testing whether or not a degree d multivariate polynomial is a sum of squares or is far from a sum of squares. We show that if we require that the property tester always accepts YES instances and uses random samples, nΩ(d) samples are required, which is not much fewer than it would take to completely determine the polynomial. To prove this lower bound, we show that with high probability, multivariate polynomial interpolation matches arbitrary values on random points and the resulting polynomial has small norm. We then consider a particular polynomial which is non-negative yet not a sum of squares and use pseudo-expectation values to prove it is far from being a sum of squares. . Institute for Advanced Study. Supported by the Simons Collaboration for Algorithms and Geometry and by the NSF under agreement No. CCF-1412958. Part of this work was done while at Cornell University. Yale University	algorithm;ibm notes;keneth alden simons;polynomial hierarchy;polynomial interpolation;property testing;with high probability	Aaron Potechin;Liu Yang	2017	CoRR		homogeneous polynomial;total sum of squares;combinatorics;matrix polynomial;discrete mathematics;explained sum of squares;partition of sums of squares;mathematics;mathematical optimization;lack-of-fit sum of squares;square-free polynomial;residual sum of squares	Theory	12.13207508536759	22.03223303616772	192239
b1d61399fec519ba888f0a4c633d946487d35a1b	on the complexity of computational problems regarding distributions	reductions;statistical difference;variation distance;approximation;entropy;zero knowledge;sampleable distributions;promise problems	We consider two basic computational problems regarding discrete probability distributions: (1) approximating the statistical difference (aka variation distance) between two given distributions, and (2) approximating the entropy of a given distribution. Both problems are considered in two different settings. In the first setting the approximation algorithm is only given samples from the distributions in question, whereas in the second setting the algorithm is given the “code” of a sampling device (for the distributions in question). We survey the know results regarding both settings, noting that they are fundamentally different: The first setting is concerned with the number of samples required for determining the quantity in question, and is thus essentially information theoretic. In the second setting the quantities in question are determined by the input, and the question is merely one of computational complexity. The focus of this survey is actually on the latter setting. In particular, the survey includes proof sketches of three central results regarding the latter setting, where one of these proofs has only appeared before in the second author’s PhD Thesis.	approximation algorithm;bpp (complexity);black box;computation;computational complexity theory;computational problem;exptime;information theory;obfuscation (software);sampling (signal processing);time complexity	Oded Goldreich;Salil P. Vadhan	2011		10.1007/978-3-642-22670-0_27	combinatorics;discrete mathematics;mathematics	Theory	11.004297121123322	21.20942009492	193077
9bcde5d6d012ddf17abe421bc05aef43e841e6fc	hidden number problem with hidden multipliers, timed-release crypto, and noisy exponentiation	nombre cache;temps polynomial;elliptic curve;exponential sums;hidden number;courbe elliptique;exponentiation bruit;estimation erreur;lattice reduction;crypto liberation temporisee;curva eliptica;error estimation;criptografia;cryptography;calcul numerique;weil pairing;numerical computation;calculo numerico;estimacion error;polynomial time;borne inferieure;timed release crypto;reduction treillis;noisy exponentiation;cryptographie;exponential sum;somme exponentielle;hidden number problem;lower bound;cota inferior;tiempo polinomial	We consider a generalisation of the hidden number problem recently introduced by Boneh and Venkatesan. The initial problem can be stated as follows: recover a number a ∈ Fp such that for many known random t ∈ Fp approximations to the values of batcp are known. Here we study a version of the problem where the “multipliers” t are not known but rather certain approximations to them are given. We present a probabilistic polynomial time solution when the error is small enough, and we show that the problem cannot be solved if the error is sufficiently large. We apply the result to the bit security of “timed-release crypto” introduced by Rivest, Shamir and Wagner, to noisy exponentiation black-boxes and to the bit security of the “inverse” exponentiation. We also show that it implies a certain bit security result for Weil pairing on elliptic curves.	approximation;black box;pp (complexity);time complexity	Nick Howgrave-Graham;Phong Q. Nguyen;Igor E. Shparlinski	2003	Math. Comput.	10.1090/S0025-5718-03-01495-9	combinatorics;mathematical analysis;discrete mathematics;lattice reduction;cryptography;mathematics;geometry;exponential sum;elliptic curve;algorithm;statistics;algebra	Crypto	10.468123003469723	24.0850042681271	193234
9b6e7bff028165e00a758dcf1e59343f55698d62	the optimality of correlated sampling		In the correlated sampling problem, two players, say Alice and Bob, are given two distributions, say P and Q respectively, over the same universe and access to shared randomness. The two players are required to output two elements, without any interaction, sampled according to their respective distributions, while trying to minimize the probability that their outputs disagree. A well-known protocol due to Holenstein, with close variants (for similar problems) due to Broder, and to Kleinberg and Tardos, solves this task with disagreement probability at most 2δ/(1+ δ), where δ is the total variation distance between P and Q. This protocol has been used in several different contexts including sketching algorithms, approximation algorithms based on rounding linear programming relaxations, the study of parallel repetition and cryptography. In this note, we give a surprisingly simple proof that this protocol is in fact tight. Specifically, for every δ ∈ (0, 1), we show that any correlated sampling scheme should have disagreement probability at least 2δ/(1 + δ). This partially answers a recent question of Rivest. Our proof is based on studying a new problem we call constrained agreement. Here, Alice is given a subset A ⊆ [n] and is required to output an element i ∈ A, Bob is given a subset B ⊆ [n] and is required to output an element j ∈ B, and the goal is to minimize the probability that i 6= j. We prove tight bounds on this question, which turn out to imply tight bounds for correlated sampling. Though we settle basic questions about the two problems, our formulation also leads to several questions that remain open. ∗Department of Mathematics and Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge MA 02139. Supported in part by NSF Award CCF-1420692. bavarian@mit.edu. †Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge MA 02139. Supported in part by NSF CCF-1420956, NSF CCF-1420692 and CCF-1217423. badih@mit.edu. ‡Harvard John A. Paulson School of Engineering and Applied Sciences. Part of this work supported by NSF Award CCF1565641. seladh@gmail.com. §Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge MA 02139. Supported in part by NSF CCF-1420956 and NSF CCF-1420692. pritish@mit.edu. ¶Institute Professor, MIT. This work supported by the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370. ‖Harvard John A. Paulson School of Engineering and Applied Sciences. Part of this work supported by NSF Award CCF1565641 and a Simons Investigator Award. madhu@cs.harvard.edu.	alice and bob;andrei broder;approximation algorithm;cryptography;ibm notes;keneth alden simons;linear programming;mit computer science and artificial intelligence laboratory;randomness;regular expression;rounding;sampling (signal processing)	Mohammad Bavarian;Badih Ghazi;Elad Haramaty;Pritish Kamath;Ronald L. Rivest;Madhu Sudan	2016	Electronic Colloquium on Computational Complexity (ECCC)		mathematical optimization;combinatorics;mathematics;algorithm;statistics	Theory	12.218845800761011	21.49632052067809	195363
3fcc20dee97f04a6109938d6cfaeca47adb1edb7	on enumerating monomials and other combinatorial structures by polynomial interpolation	interpolation;black box polynomial;enumeration complexity;np hardness;circuit	We study the problem of generating the monomials of a black box polynomial in the context of enumeration complexity. We present three new randomized algorithms for restricted classes of polynomials with a polynomial or incremental delay, and the same global running time as the classical ones. We introduce TotalBPP, IncBPP and DelayBPP, which are probabilistic counterparts of the most common classes for enumeration problems. Our interpolation algorithms are applied to algebraic representations of several combinatorial enumeration problems, which are so proved to belong to the introduced complexity classes. In particular, the spanning hypertrees of a 3-uniform hypergraph can be enumerated with a polynomial delay. Finally, we study polynomials given by circuits and prove that we can derandomize the interpolation algorithms on classes of bounded-depth circuits. We also prove the hardness of some problems on polynomials of low degree and small circuit complexity, which suggests that our good interpolation algorithm for multilinear polynomials cannot be generalized to degree 2 polynomials. This article is an improved and extended version of Strozecki (Mathematical Foundations of Computer Science, pp. 629–640, 2010) and of the third chapter of the author’s Ph.D. Thesis (Strozecki, Ph.D. Thesis, 2010).	alain fournier;black box;circuit complexity;complexity class;computable function;computer science;degree (graph theory);degree of a polynomial;enumerated type;file spanning;lexicographical order;linear algebra;maximal set;monomial;p (complexity);polynomial delay;polynomial interpolation;randomized algorithm;schwartz–zippel lemma;time complexity;windows legacy audio components;with high probability	Yann Strozecki	2012	Theory of Computing Systems	10.1007/s00224-012-9442-z	difference polynomials;polynomial matrix;combinatorics;discrete mathematics;lagrange polynomial;elementary symmetric polynomial;interpolation;polynomial interpolation;degree of a polynomial;mathematics;linear interpolation;algorithm	Theory	12.291986153735147	21.04972462588574	195521
4dc4b706ea70167cc18b2f4de9d148ce201e5c17	3sum, 3xor, triangles		Pǎtraşcu (STOC ’10) reduces the 3SUM problem to listing triangles in a graph. In the other direction, we show that if one can solve 3SUM on a set of size n in time n1+ then one can list t triangles in a graph with m edges in time Õ(m1+ t1/3− /3). Our result builds on and extends works by the Paghs (PODS ’06) and by Vassilevska and Williams (FOCS ’10). We make our reductions deterministic using tools from pseudorandomness. We then re-execute both Pǎtraşcu’s reduction and ours for the variant 3XOR of 3SUM where integer summation is replaced by bit-wise xor. As a corollary we obtain that if 3XOR is solvable in linear time but 3SUM requires quadratic randomized time, or vice versa, then the randomized time complexity of listing m triangles in a graph with m edges is m4/3 up to a factor mα for any α > 0. ∗Supported by NSF grant CCF-0845003. Email: {zahra,viola}@ccs.neu.edu ISSN 1433-8092 Electronic Colloquium on Computational Complexity, Revision 1 of Report No. 9 (2013)	3sum;belief revision;decision problem;electronic colloquium on computational complexity;email;exclusive or;ibm notes;international standard serial number;pseudorandomness;randomized algorithm;symposium on foundations of computer science;symposium on principles of database systems;symposium on theory of computing;time complexity	Zahra Jafargholi;Emanuele Viola	2013	Electronic Colloquium on Computational Complexity (ECCC)		mathematical optimization;combinatorics;discrete mathematics;mathematics;geometry;algorithm	Theory	12.162387024037642	21.58560275400924	195532
ce9b7bfc7a6ddeb4a577e32d9eff06a1c3379b6f	lattice reduction algorithms: theory and practice	theory and practice;public key;lattice reduction;security requirements	Lattice reduction algorithms have surprisingly many applications in mathematics and computer science, notably in cryptology. On the one hand, lattice reduction algorithms are widely used in publickey cryptanalysis, for instance to attack special settings of RSA and DSA/ECDSA. On the other hand, there are more and more cryptographic schemes whose security require that certain lattice problems are hard. In this talk, we survey lattice reduction algorithms, present their performances, and discuss the differences between theory and practice. Intuitively, a lattice is an infinite arrangement of points in R spaced with sufficient regularity that one can shift any point onto any other point by some symmetry of the arrangement. The simplest non-trivial lattice is the hypercubic lattice Z formed by all points with integral coordinates. The branch of number theory dealing with lattices (and especially their connection with convex sets) is known as geometry of numbers [24,41,12,5], and its origins go back to two historical problems: higher-dimensional generalizations of Euclid’s gcd algorithm and sphere packings. More formally, a lattice L is a discrete subgroup of R, or equivalently, the set of all integer combinations of n linearly independent vectors b1, . . . ,bn in R : L = {a1b1 + · · · + anbn, ai ∈ Z}. Such a set (b1, . . . ,bn) is called a basis of the lattice. The goal of lattice reduction is to find reduced bases, that is bases consisting of reasonably short and nearly orthogonal vectors. This is related to the reduction theory of quadratic forms developed by Lagrange [19], Gauss [11] and Hermite [14]. Lattice reduction algorithms have proved invaluable in many fields of computer science and mathematics (see the book [30]), notably public-key cryptanalysis where they have been used to break knapsack cryptosystems [32] and special cases of RSA and DSA, among others (see [26,21] and references therein). Reduced bases allow to solve the following important lattice problems, either exactly or approximately: – The most basic computational problem involving lattices is the shortest vector problem (SVP), which asks to find a nonzero lattice vector of smallest norm, given a lattice basis as input. SVP can be viewed as a geometric generalization of gcd computations: Euclid’s algorithm actually computes the K.G. Paterson (Ed.): Eurocrypt 2011, LNCS 6632, pp. 2–6, 2011. c © International Association for Cryptologic Research 2011 Lattice Reduction Algorithms: Theory and Practice 3 smallest (in absolute value) non-zero linear combination of two integers, since gcd(a, b)Z = aZ+bZ, which means that we are replacing the integers a and b by an arbitrary number of vectors b1, . . . ,bn with integer coordinates. Since SVP is NP-hard under randomized reductions [3] (see [17,34] for surveys on the hardness of lattice problems), one is also interested in approximating SVP, i.e. to output a nonzero lattice vector of norm not much larger than the smallest norm. – The inhomogeneous version of SVP is called the closest vector problem (CVP); here we are given an arbitrary target vector in addition to the lattice basis and asked to find the lattice point closest to that vector. A popular particular case of CVP is Bounded Distance Decoding (BDD), where the target vector is known to be somewhat close to the lattice. The first SVP algorithm was Lagrange’s reduction algorithm [19], which solves SVP exactly in dimension two, in quadratic time. In arbitrary dimension, there are two types of SVP algorithms: 1. Exact algorithms. These algorithms provably find a shortest vector, but they are expensive, with a running time at least exponential in the dimension. Intuitively, these algorithms perform an exhaustive search of all extremely short lattice vectors, whose number is exponential in the dimension (in the worst case): in fact, there are lattices for which the number of shortest lattice vectors is already exponential. Exact algorithms can be split in two categories: (a) Polynomial-space exact algorithms.They are based on enumeration which dates back to the early 1980s with work by Pohst [33], Kannan [16], and Fincke-Pohst [6]. In its simplest form, enumeration is simply an exhaustive search for the best integer combination of the basis vectors. The best deterministic enumeration algorithm is Kannan’s algorithm [16], with super-exponentialworst-case complexity, namelynpolynomialtime operations (see [13]), where n denotes the lattice dimension. The enumerationalgorithmsused inpractice (suchas thatofSchnorr-Euchner [37]) haveaweaker preprocessing thanKannan’s algorithm [16], and theirworstcase complexity is 2 2) polynomial-time operations. But it is possible to obtain substantial speedups using pruning techniques: pruning was introduced by Schnorr-Euchner [37] and Schnorr-Hörner [38] in the 90s, and recently revisited by Gama, Nguyen and Regev [10], where it was shown that one can reach a 2 heuristic speedup over basic enumeration. (b) Exponential-space exact algorithms. These algorithms have a better asymptotic running time, but they all require exponential space 2. The first algorithm of this kind is the randomized sieve algorithm of Ajtai, Kumar and Sivakumar (AKS) [4], with exponential worst-case complexity of 2 polynomial-time operations. Micciancio and Voulgaris [22] recently presented an alternative deterministic algorithm, which solves both CVP and SVP within 2 polynomial-time operations. Interestingly, there are several heuristic variants [31,23,43] of AKS with running time 2, where the O() constant is much less than that of the best provable	approximation algorithm;basis (linear algebra);best, worst and average case;brute-force search;computation;computational problem;convex set;cryptosystem;deterministic algorithm;expspace;emoticon;euclid;eurocrypt;general number field sieve;heuristic;integral cryptanalysis;knapsack cryptosystems;lattice problem;lattice reduction;lecture notes in computer science;performance;polynomial;preprocessor;provable security;public-key cryptography;randomized algorithm;schnorr group;speedup;time complexity;worst-case complexity	Phong Q. Nguyen	2011		10.1007/978-3-642-20465-4_2	lattice-based cryptography;discrete mathematics;lattice reduction;computer science;theoretical computer science;mathematics;public-key cryptography;computer security;algorithm;lattice problem	Theory	14.637816101011339	20.69184073520619	195665
bec3ce008fd7457a6e0c5a4bbbeb85819db1677f	on the complexity of robust stable marriage		Robust Stable Marriage is a variant of the classical Stable Marriage problem, where the robustness of a given stable matching is measured by the number of modifications required for repairing it in case an unforeseen event occurs. An (a, b)-supermatch is defined as a stable matching in which if any a (non-fixed) men/women break up, it is possible to find another stable matching by changing the partners of those a men/women and also the partners of at most b other couples. In this paper, we focus on the complexity of finding an (a, b)-supermatch. In order to show deciding if there exists an (a, b)-supermatch is NPcomplete, we first introduce a SAT formulation that is NP-complete by using Schaefer’s Dichotomy Theorem. Then, we show the equivalence between the SAT formulation and finding a (1, 1)-supermatch on a specific family of instances.	np-completeness;stable marriage problem;turing completeness	Begum Genc;Mohamed Siala;Gilles Simonin;Barry O'Sullivan	2017		10.1007/978-3-319-71147-8_30	robustness (computer science);discrete mathematics;combinatorics;np-complete;stable roommates problem;equivalence (measure theory);mathematics;combinatorial optimization;existential quantification;stable marriage problem	ECom	16.614394434987048	18.860753343185774	195995
f375f67295e240f0a45bf6819eb14e1da7e60687	special issue “conference on computational complexity 2006” guest editors’ foreword	computational complexity	This special issue contains full versions of three papers that were presented at the 21st Annual IEEE Computational Complexity Conference (CCC 2006), held in Prague during July 16–20, 2006. These papers were invited by the conference program committee, chaired by Manindra Agrawal. All submitted papers were then subject to the normal refereeing process of this journal. The first paper of this issue is Polynomial Identity Testing for Depth 3 Circuits by Neeraj Kayal and Nitin Saxena. It gives a deterministic polynomialtime algorithm for testing if a given arithmetic circuit of depth 3, with constant top fan-in, computes an identically zero polynomial. This is currently the best derandomization result for a special case of Polynomial Identity Testing, one of the central problems in BPP that is not known to be in P . This paper received both the best paper and best student paper awards at the conference. The next paper, A Generic Time Hierarchy With One Bit of Advice by Dieter van Melkebeek and Konstantin Pervyshev, addresses a classical question of time hierarchy. For a number of semantic complexity classes (such as BPP , ZPP , AM , etc.) with a small amount of advice, the authors show that in more time one can decide more languages. The final paper of this issue, Every linear threshold function has a lowweight approximator by Rocco Servedio, proves a tight result showing that every given linear threshold function can be well approximated by another linear threshold function with small integer weights. This has applications to approximate counting and computational learning theory. Further papers were selected and will appear in later issues of this journal. We want to thank the authors of these papers for submitting them to the special issue and revising them within a short time frame, and the referees who helped us immensely with their thorough and timely reviews. We also thank the PC Chair Manindra Agrawal and Editor-in-Chief Joachim von zur Gathen for inviting us to edit this special issue!	am broadcasting;approximation algorithm;arithmetic circuit complexity;bpp (complexity);complexity class;computational complexity theory;computational learning theory;constant function;fan-in;neeraj kayal;polynomial identity testing;randomized algorithm;zpp (complexity)	Venkatesan Guruswami;Valentine Kabanets	2007	computational complexity	10.1007/s00037-007-0225-x	computer science;computational complexity theory	Theory	11.99452080178718	21.35014094261796	196253
5f9888cbea51efcae02ca006921b6d8d5034519b	online conflict-free coloring for intervals	disque;pire cas;insertion;online algorithm;68w40;randomized algorithms;52c45;disk;conflict free coloring;probability;maximo;loi probabilite;ley probabilidad;two dimensions;color;performance;maximum;algorithme deterministe;68wxx;approche deterministe;disco;deterministic approach;deterministic algorithms;insercion;probability distribution;probabilidad;enfoque determinista;probabilite;randomized algorithm;couleur;68q25;online algorithms;rendimiento;68w20;calcul 2 dimensions;branching processes;two dimensional calculations;05c15	We consider an online version of the conflict-free coloring of a set of points on the line, where each newly inserted point must be assigned a color upon insertion, and at all times the coloring has to be conflict-free, in the sense that in every interval I there is a color that appears exactly once in I. We present several deterministic and randomized algorithms for achieving this goal, and analyze their performance, that is, the maximum number of colors that they need to use, as a function of the number n of inserted points. We first show that a natural and simple (deterministic) approach may perform rather poorly, requiring Ω(√n) colors in the worst case. We then modify this approach, to obtain an efficient deterministic algorithm that uses a maximum of Θ(log2 n) colors. Next, we present two randomized solutions. The first algorithm requires an expected number of at most O(log2 n) colors, and produces a coloring which is valid with high probability, and the second one, which is a variant of our efficient deterministic algorithm, requires an expected number of at most O(log n log log n) colors but always produces a valid coloring. We also analyze the performance of the simplest proposed algorithm when the points are inserted in a random order, and present an incomplete analysis that indicates that, with high probability, it uses only O(log n) colors. Finally, we show that in the extension of this problem to two dimensions, where the relevant ranges are disks, n colors may be required in the worst case. The average-case behavior for disks, and cases involving other planar ranges, are still open.	best, worst and average case;color;deterministic algorithm;graph coloring;randomized algorithm;with high probability	Ke Chen;Amos Fiat;Haim Kaplan;Meital Levy;Jirí Matousek;Elchanan Mossel;János Pach;Micha Sharir;Shakhar Smorodinsky;Uli Wagner;Emo Welzl	2005	SIAM J. Comput.	10.1137/S0097539704446682	online algorithm;mathematical optimization;combinatorics;mathematics;randomized algorithm;algorithm;statistics	Theory	15.203750459079894	22.870569591052366	196758
587b7419712243f0ed02c86c9b549b6760c002ca	generator of randomized pseudorandom numbers			pseudorandomness;randomized algorithm	Jan Havel;A. N. Morozevich;Vyacheslav N. Yarmolik	1983	Kybernetika		discrete mathematics;mathematical optimization;mathematics;pseudorandom number generator	Theory	10.511564205162658	22.710536813425062	197390
0a7d54b4b43a9f2057b323c9e60be5e40cbaf26c	how to construct random functions	forecasting;prevision;random number generator;one way function;computability theory;complexity theory;senal seudo aleatoria;funcion aleatoria;electronic generator;probabilistic algorithm;polynomial time algorithm;theory of computing;computational complexity;criptografia;cryptography;random function;pseudorandom signal;polynomial time;generateur electronique;cryptographie;generador electronico;signal pseudoaleatoire;fonction aleatoire;device modeling;probability and statistics	A constructive theory of randomness for functions, based on computational complexity, is developed, and a pseudorandom function generator is presented. This generator is a deterministic polynomial-time algorithm that transforms pairs (<italic>g</italic>, <italic>r</italic>), where <italic>g</italic> is <italic>any</italic> one-way function and <italic>r</italic> is a random <italic>k</italic>-bit string, to polynomial-time computable functions ƒ<italic><subscrpt>r</subscrpt></italic>: {1, … , 2<italic><supscrpt>k</supscrpt></italic>} → {1, … , 2<italic><supscrpt>k</supscrpt></italic>}. These ƒ<italic><subscrpt>r</subscrpt></italic>'s cannot be distinguished from <italic>random</italic> functions by any probabilistic polynomial-time algorithm that asks and receives the value of a function at arguments of its choice. The result has applications in cryptography, random constructions, and complexity theory.	algorithm;computable function;computational complexity theory;cryptography;one-way function;pp (complexity);polynomial;pseudorandom function family;pseudorandomness;randomness;time complexity	Oded Goldreich;Shafi Goldwasser;Silvio Micali	1986	J. ACM	10.1145/6490.6503	probability and statistics;mathematical optimization;combinatorics;discrete mathematics;computability theory;forecasting;computer science;cryptography;theoretical computer science;random function;mathematics;algorithm;statistics;algebra	Theory	10.074132545572922	23.58478416919545	198032
9ed2b1b7ffe5ce6e484e203b81105d14e7378a59	further applications of random sampling to computational geometry	range query;probabilistic method;random sampling;computational geometry;arrangement of hyperplanes;divide and conquer;voronoi diagram	"""1 Introduction This paper gives several new demonstrations of the usefulness of random sampling techniques in computational geometry. One new algorithm creates a search structure for arrangements of hyperplanes by sampling the hyperplanes and using information from the resulting arrangement to divide and conquer. This algorithm requires randomized O(s d+`) preprocess-ing time to build a search structure for an arrangement of s hyperplanes in d dimensions. The structure has a query time that is worst-case O(logs). (The bound holds for any fixed ~ > 0, with the constant factors dependent on d and ~.) Using point-plane du-ality, the algorithm may be used for answering halfs-pace range queries. Another algorithm finds random samples of simplices to determine the separation distance of two polytopes. The algorithm uses random-ized O(n[ d/2j) time, where n is the total number of vertices of the two polytopes. This matches previous results [DK851 for the case d : 3 and extends them. Another algorithm samples points in the plane to determine their order k Voronoi diagram, and requires randomized O(sk)o(s ~) time for s points. This sharpens the bound O(sk 2 logs) for Lee's algorithm [Lee821, and O(s 2 logs + s(s-k) log2 s) for Chazelle and Edelsbrunner's algorithm ICE851. Finally, random sampling is used to show that any set of s points in E 3 has O(sk 2 log 9 s/(log log s) 6) distinct j-sets with j < k. (For S C E d, a set S' C S with IS'} =j is a j-set of S if there is a halfspace h + with S' = S fqh+.) This sharpens with respect to k the previous bound O(sk 5) [CP851. The proof of the bound given here is an instance of a """"probabilistic method"""" IES741. Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 1.1 The problems and results The use of random sampling to divide and conquer is quite old: the partitioning step of quicksort may be viewed as an example. This paper describes several new applications of this technique. Searching arrangements. Given a set …"""	baroque;best, worst and average case;binary logarithm;computation;computational geometry;monte carlo method;preprocessor;quicksort;randomized algorithm;range query (data structures);sampling (signal processing);voronoi diagram	Kenneth L. Clarkson	1986		10.1145/12130.12173	range query;sampling;mathematical optimization;combinatorics;discrete mathematics;divide and conquer algorithms;voronoi diagram;computational geometry;probabilistic method;mathematics;arrangement of hyperplanes	Theory	14.69902839411595	23.28232741762595	198237
c8e110d981c3f5370fda975012ad78006338da70	an upper bound for the solvability of a random stable roommates instance	random preferences;stable matching;proposal algorithm;upper bound;probabilistic analysis;partition	Abstract#R##N##R##N#It is well-known that not all instances of the stable roommates problem admit a stable matching. Here we establish the first nontrivial upper bound on the limiting behavior of Pn, the probability that a random roommates instance of size n has a stable matching, namely, limn∞Pn⩽ e1/2/2 (=0.8244…). © 1994 John Wiley & Sons, Inc.		Boris Pittel;Robert W. Irving	1994	Random Struct. Algorithms	10.1002/rsa.3240050307	partition;mathematical optimization;combinatorics;discrete mathematics;probabilistic analysis of algorithms;stable marriage problem;mathematics;stable roommates problem;upper and lower bounds	Theory	17.038715464237146	18.71626669109231	198242
236e89e3b5bf0dea1dfa1f84de705f54981ab9e8	minimal average cost of searching for a counterfeit coin: restricted model	ley uniforme;optimisation;sequential algorithm;combinatorics;optimizacion;loi probabilite;ley probabilidad;combinatoria;huffman tree;combinatoire;cout moyen;average cost;informatique theorique;probability distribution;coste medio;distribution uniforme;coin weighing problem;optimization;algoritmo optimo;algorithme optimal;optimal algorithm;arbre huffman;loi uniforme;probleme pesage monnae;uniform distribution;computer theory;algorithme sequentiel;informatica teorica	The following restricted model of coin-weighing problem is considered: there is a heavier coin in a set of n coins, n - 1 of which are good coins having the same weight. The test device is a two-arms balance scale and each test-set is of the form A : B with |A| = |B| ≤ l, where l ≥ 1 is a given integer. We present an optimal sequential algorithm requiring the minimal average cost of weighings when the probability distribution on the coin set is uniform distribution.		Wen An Liu;Hong Yong Ma	2006	Discrete Applied Mathematics	10.1016/j.dam.2006.03.017	probability distribution;combinatorics;group testing;calculus;mathematics;uniform distribution;algorithm;huffman coding	Theory	16.15952631617185	24.327259229980722	198442
86b8dae4092d2c52a6a087fcc169b015aa07cbcd	limits of preprocessing	combinatorial problems;global constraint;satisfiability;constraint satisfaction;artificial intelligent;bayesian reasoning;theoretical analysis;computational complexity;polynomial time	We present a first theoretical analysis of the power of polynomial-time preprocessing for important combinatorial problems from various areas in AI. We consider problems from Constraint Satisfaction, Global Constraints, Satisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a complexity theoretic assumption, none of the considered problems can be reduced by polynomial-time preprocessing to a problem kernel whose size is polynomial in a structural problem parameter of the input, such as induced width or backdoor size. Our results provide a firm theoretical boundary for the performance of polynomial-time preprocessing algorithms for the considered prob-	algorithm;constraint satisfaction;mathematical optimization;polynomial kernel;preprocessor;theory;time complexity;turing machine;turing test	Stefan Szeider	2011	CoRR		time complexity;mathematical optimization;combinatorics;constraint satisfaction;computer science;artificial intelligence;machine learning;mathematics;computational complexity theory;bayesian inference;constraint satisfaction problem;algorithm;satisfiability	AI	11.04636301203058	18.936195687458888	198615
b4f2e4898b163d201d534d37571720a28f9e143c	random debaters and the hardness of approximating stochastic functions	game theory;approximate algorithm;complexity theory;satisfiability;game theory computational complexity;theory of computing;computational complexity;polynomial time;games against nature stochastic functions random probabilistically checkable debate system rpcds probabilistic polynomial time verifier coins pspace pspace hard functions dynamic graph reliability stochastic satisfiability mah jongg stochastic coloring stochastic generalized geography;stochastic processes polynomials law legal factors geography writing;hardness of approximation	"""A probabilistically checkable debate system (PCDS) for a language L consists of a probabilistic polynomial-time veri er V and a debate between Player 1, who claims that the input x is in L, and Player 0, who claims that the input x is not in L. It is known that there is a PCDS for L in which V ips O(logn) coins and reads O(1) bits of the debate if and only if L is in PSPACE ([Condon et al., Proc. 25th ACM Symposium on Theory of Computing, 1993, pp. 304{315]). In this paper, we restrict attention to RPCDS's, which are PCDS's in which Player 0 follows a very simple strategy: On each turn, Player 0 chooses uniformly at random from the set of legal moves. We prove the following result. Theorem: L has an RPCDS in which the veri er ips O(logn) coins and reads O(1) bits of the debate if and only if L is in PSPACE. This new characterizationof PSPACE is used to show that certain stochastic PSPACE-hard functions are as hard to approximate closely as they are to compute exactly. Examples of such functions include optimization versions of Dynamic Graph Reliability, Stochastic Satis ability, Mah-Jongg, Stochastic Generalized Geography, and other \games against nature"""" of the type introduced in [Papadimitriou, J. Comput. System Sci., 31 (1985), pp. 288{301]."""	approximation algorithm;generalized geography;mathematical optimization;pp (complexity);pspace;probabilistically checkable proof;stochastic gradient descent;symposium on theory of computing;time complexity	Anne Condon;Joan Feigenbaum;Carsten Lund;Peter W. Shor	1994		10.1109/SCT.1994.315796	combinatorics;discrete mathematics;stochastic optimization;mathematics;stochastic;algorithm	Theory	12.057611135718274	20.92272003989484	199123
5604daae8935ef15ea000d4e97a10b7801259a61	unifying the landscape of cell-probe lower bounds	lower bounds;data structures;cell probe complexity;68p05;range queries	We show that a large fraction of the data-structure lower bounds known today in fact follow by reduction from the communication complexity of lopsided (asymmetric) set disjointness. This includes lower bounds for: • high-dimensional problems, where the goal is to show large space lower bounds. • constant-dimensional geometric problems, where the goal is to bound the query time for space O(n · polylogn). • dynamic problems, where we are looking for a trade-off between query and update time. (In this case, our bounds are slightly weaker than the originals, losing a lg lgn factor.) Our reductions also imply the following new results: • an Ω(lgn/ lg lgn) bound for 4-dimensional range reporting, given space O(n · polylogn). This is quite timely, since a recent result [39] solved 3D reporting in O(lg lgn) time, raising the prospect that higher dimensions could also be easy. • a tight space lower bound for the partial match problem, for constant query time. • the first lower bound for reachability oracles. In the process, we prove optimal randomized lower bounds for lopsided set disjointness.	communication complexity;data structure;oracle machine;randomized algorithm;reachability	Mihai Patrascu	2011	SIAM J. Comput.	10.1137/09075336X	range query;mathematical optimization;combinatorics;discrete mathematics;data structure;computer science;mathematics;programming language;algorithm	Theory	12.886417343000318	23.66839734706059	199173
716c2ca57cc9165073239fade552c0abdaa2674c	on the complexity of approximate sum of sorted list		We consider the complexity for computing the approximate sum a1 + a2 + · · · + an of a sorted list of numbers a1 ≤ a2 ≤ · · · ≤ an. We show an algorithm that computes an (1 + ǫ)-approximation for the sum of a sorted list of nonnegative numbers in an O(1 ǫ min(log n, log(xmax x min)) · (log 1 ǫ + log log n)) time, where xmax and xmin are the largest and the least positive elements of the input list, respectively. We prove a lower bound Ω(min(log n, log(xmax x min)) time for every O(1)-approximation algorithm for the sum of a sorted list of nonnegative elements. We also show that there is no sublinear time approximation algorithm for the sum of a sorted list that contains at least one negative number.	approximation algorithm;computation;computational complexity theory;computational model;cynthia dwork;dijkstra's algorithm;execution unit;maxima and minima;randomized algorithm;sorting algorithm;time complexity	Bin Fu	2013		10.1007/978-3-642-38756-2_29	combinatorics;discrete mathematics;mathematics;algorithm	Theory	12.982410926257431	23.981435750302296	199810
29c86fa3819ce7256470802b28c3d9460f375eaa	approximation algorithms for bandwidth consecutive multicolorings - (extended abstract)			approximation algorithm	Yuji Obata;Takao Nishizeki	2014		10.1007/978-3-319-08016-1_18	approximation algorithm;mathematical optimization;mathematics;bandwidth (signal processing)	Theory	16.792532149086682	20.69043392407441	199836
0aee806b3a4137a9761ffae2dc2ec842f5f6dcb8	adaptive intersection and t-threshold problems	comparison model;t-threshold problem;adaptive intersection;text database system;boolean query;k set;non-deterministic complexity;lower bound	"""Consider the problem of computing the intersection of k sorted sets. In the comparison model, we prove a new lower bound which depends on the non-deterministic complexity of the instance, and implies that the algorithm of Demaine, López-Ortiz and Munro [2] is usually optimal in this """"adaptive"""" sense. We extend the lower bound and the algorithm to the t-Threshold Problem, which consists in finding the elements which are in at least t of the k sets. These problems are motivated by boolean queries in text database systems."""	algorithm;boolean algebra;database	Jérémy Barbay;Claire Mathieu	2002				Theory	12.478903235160987	25.187146754222	199843
