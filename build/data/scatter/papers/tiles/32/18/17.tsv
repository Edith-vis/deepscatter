id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
200100558668c3a35c175fe95ea961109c455aa1	a backtracking-based algorithm for computing hypertree-decompositions	artificial intelligent;tree decomposition;heuristic algorithm;data structure	Hypertree decompositions of hypergraphs are a generalization of tree decompositions of graphs. The corresponding hypertreewidth is a measure for the cyclicity and therefore tractability of the encoded computation problem. Many NP-hard decision and computation problems are known to be tractable on instances whose structure corresponds to hypergraphs of bounded hypertree-width. Intuitively, the smaller the hypertree-width, the faster the computation problem can be solved. In this paper, we present the new backtracking-based algorithm det-k-decomp for computing hypertree decompositions of small width. Our benchmark evaluations have shown that det-k-decomp significantly outperforms opt-k-decomp, the only exact hypertree decomposition algorithm so far. Even compared to the best heuristic algorithm, we obtained competitive results as long as the hypergraphs are not too large.	algorithm;backtracking;benchmark (computing);cobham's thesis;computation;computational problem;decomposition method (constraint satisfaction);heuristic (computer science);np-hardness;tree decomposition	Georg Gottlob;Marko Samer	2007	CoRR			AI	13.735913878073646	16.436735711703303	176616
5e7e98626dafc41d36d4247dec04cec3d20f3204	network discovery and verification	graph theory;random graph;online algorithm;complex networks;approximate algorithm;random graphs approximation algorithms complex networks internet discovery online algorithms;approximation algorithms;routing;complex network;combinatorial optimization problem;internet discovery;approximation;random graphs;network topology;approximation theory;internet;network discovery;energy consumption;internet approximation theory graph theory;network verification;peer to peer computing internet ip networks computer science complex networks routing robustness network topology energy consumption;competitive analysis;robustness;ip networks;computer science;online algorithms;peer to peer computing;network verification internet local measurement graph theory combinatorial optimization problem approximation network discovery;article;on line algorithm;set cover;local measurement;competitive ratio	"""Due to its fast, dynamic, and distributed growth process, it is hard to obtain an accurate map of the Internet. In many cases, such a map-representing the structure of the Internet as a graph with nodes and links-is a prerequisite when investigating properties of the Internet. A common way to obtain such maps is to make certain local measurements at a small subset of the nodes, and then to combine these in order to """"discover"""" (an approximation of) the actual graph. Each of these measurements is potentially quite costly. It is thus a natural objective to minimize the number of measurements which still discover the whole graph. We formalize this problem as a combinatorial optimization problem and consider it for two different models characterized by different types of measurements. We give several upper and lower bounds on the competitive ratio (for the online network discovery problem) and the approximation ratio (for the offline network verification problem) in both models. Furthermore, for one of the two models, we compare four simple greedy strategies in an experimental analysis"""	approximation algorithm;combinatorial optimization;competitive analysis (online algorithm);graph (discrete mathematics);greedy algorithm;internet;mathematical optimization;online and offline;optimization problem	Zuzana Beerliova;Felix Eberhard;Thomas Erlebach;Alexander Hall;Michael Hoffmann;Matús Mihalák;L. Shankar Ram	2005	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.2006.884015	competitive analysis;random graph;online algorithm;computer science;graph theory;theoretical computer science;machine learning;distributed computing;complex network	ECom	12.947915382678763	14.365079458885521	177045
0af052d38f47f6f15cbd43a594abda6a03a2abff	interactive route search in the presence of order constraints	geographic information system;probabilistic data;search;route;heuristic algorithms;path;interactive algorithms	A route search is an enhancement of an ordinary geographic search. Instead of merely returning a set of entities, the result is a route that goes via entities that are relevant to the search. The input to the problem consists of several search queries, and each query defines a type of geographical entities. When visited, some of the entities succeed in satisfying the user while others fail to do so; however, only the probability of success is known prior to arrival. The main task is to find a route that visits at least one satisfying entity of each type. In an interactive search, the route is computed in steps. In each step, only the next entity of the route is given to the user, and after visiting that entity, the user provides a feedback specifying whether the entity satisfies her. This paper investigates interactive route search in the presence of order constraints that specify that some types of entities should be visited before others. We present heuristic algorithms for interactive route search for two cases, depending on whether the constraints define a complete order or a partial one. The main challenge is to utilize the feedback in order to compute a route that is shorter and has a higher degree of success, compared to routes that are computed non-interactively. We also discuss how to compare the results of the algorithms and introduce suitable measures for doing so. Experiments on real-world data illustrate the efficiency and effectiveness of our algorithms. ∗The work of these authors was supported by the GermanIsraeli Foundation for Scientific Research & Development (Grant 2165-1738.6/2007). †The work of this author was supported by the GermanIsraeli Foundation for Scientific Research & Development (Grant 973-150.6/2007). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were presented at The 36th International Conference on Very Large Data Bases, September 13-17, 2010, Singapore. Proceedings of the VLDB Endowment, Vol. 3, No. 1 Copyright 2010 VLDB Endowment 2150-8097/10/09... $ 10.00.	atm turbo;computer science;entity;experiment;feedback;greedy algorithm;heuristic;interactivity;network switch;preprocessor;web search query	Roy Levin;Yaron Kanza;Eliyahu Safra;Yehoshua Sagiv	2010	PVLDB	10.14778/1920841.1920861	beam search;route;computer science;theoretical computer science;data mining;route planning software;distributed computing;geographic information system;path	DB	11.559094560307159	14.04089287453252	177955
8d070c9a17004a579313dd2107d71c049be0298a	the (h, k)-server problem on bounded depth trees	distributed algorithms;community detection;stochastic block models;spectral analysis;averaging dynamics	We study the k-server problem in the resource augmentation setting i.e., when the performance of the online algorithm with k servers is compared to the offline optimal solution with h ≤ k servers. The problem is very poorly understood beyond uniform metrics. For this special case, the classic kserver algorithms are roughly (1 + 1/ )-competitive when k = (1 + )h, for any > 0. Surprisingly however, no o(h)-competitive algorithm is known even for HSTs of depth 2 and even when k/h is arbitrarily large. We obtain several new results for the problem. First we show that the known k-server algorithms do not work even on very simple metrics. In particular, the Double Coverage algorithm has competitive ratio Ω(h) irrespective of the value of k, even for depth-2 HSTs. Similarly the Work Function Algorithm, that is believed to be optimal for all metric spaces when k = h, has competitive ratio Ω(h) on depth-3 HSTs even if k = 2h. Our main result is a new algorithm that is O(1)-competitive for constant depth trees, whenever k = (1 + )h for any > 0. Finally, we give a general lower bound that any deterministic online algorithm has competitive ratio at least 2.4 even for depth-2 HSTs and when k/h is arbitrarily large. This gives a surprising qualitative separation between uniform metrics and depth-2 HSTs for the (h, k)-server problem, and gives the strongest known lower bound for the problem on general metrics. ∗This work was supported by NWO grant 639.022.211, ERC consolidator grant 617951, and NCN grant DEC2013/09/B/ST6/01538. It was carried out while Ł. Jeż was a postdoc at TU/e. †TU Eindhoven, Netherlands. {n.bansal,m.elias,g.koumoutsos}@tue.nl ‡University of Wrocław, Poland. lje@cs.uni.wroc.pl 0 ar X iv :1 60 8. 08 52 7v 2 [ cs .D S] 1 1 A pr 2 01 7	competitive analysis (online algorithm);k-server problem;online algorithm;online and offline;server (computing)	Nikhil Bansal;Marek Eliás;Lukasz Jez;Grigorios Koumoutsos	2017		10.1137/1.9781611974782.65	distributed algorithm;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;algorithm	Theory	16.756036195458854	16.402217139220017	178237
6bd0772073d0efc8939fb3cf0102c3e77dd3a647	how to solve the zebra problem, or path consistency the easy way	zebra problem;path consistency		local consistency	Barbara M. Smith	1992			machine learning	NLP	11.20234739008686	15.618415713596795	178522
366ff196bac282eda0852c274876e3c1c88b266c	communication-efficient computation load scheduling for edge computing systems		In this paper, we develop a stochastic communication-efficient computation load scheduling framework to complete the computation tasks with coded MapReduce considering the intrinsic tradeoff between the communication and computation loads. Our goal is to minimize the communication load under time-varying excess computational resources. We first reduce this problem to a task scheduling problem by exploiting the property of the computing repetition in the coded MapReduce framework. Since the task scheduling problem is still a stochastic optimization problem, it is generally difficult to solve. In the offline setting, we obtain the optimal computation load scheduling algorithm by adopting the augmented Lagrangian method. In the online setting, we derive a lower bound on competitive ratio over all online algorithms by competitive analysis. Furthermore, we make full use of past state information of computing resources to pre-planing and proposed improvements based on the above algorithm in a learning manner. Finally, our proposed algorithm is evaluated by simulation to demonstrate that the performance gap between the online and offline algorithms is fairly small and to demonstrate the superiority of the proposed algorithms over the conventional algorithms.		Minghui Zhao;Wei Wang;Yitu Wang;Zhaoyang Zhang	2018	2018 10th International Conference on Wireless Communications and Signal Processing (WCSP)	10.1109/WCSP.2018.8555593	online algorithm;real-time computing;competitive analysis;computer science;stochastic optimization;scheduling (computing);computation;job shop scheduling;augmented lagrangian method;edge computing	Robotics	13.750226011192908	11.396325869659842	179246
9c525b7a88463d450b595c44ac70bbc5d850bb03	"""recursive method to solve the problem of """"gambling with god"""""""	recursive method;recursive formula;gambling with god problem	Suppose Alice gambles with God who is the dealer. There are n total rounds in the game and God can choose any m rounds to win and the other n-m rounds to lose. At first Alice has holdings a. In each round, Alice can increase her holdings by q times the amount she wagers if she wins. So what strategy should Alice take to ensure the maximum total holdings in the end? And how much is the total final holdings? It is called the ''Gambling with God'' problem. In this paper, a recursive method is proposed to solve the problem, which shows the extensive application of recursive methods.	recursion (computer science)	Huang Shao;Wang Chao	2012	Discrete Applied Mathematics	10.1016/j.dam.2011.09.009	mathematical optimization;simulation;artificial intelligence;mathematics;operations research;algorithm	Logic	15.266453731087852	16.00830188987733	179905
63521de48e9dcbf94c623544263ae2a97ce95d5c	randomized and adversarial load balancing	carga dinamica;equilibrado;estimacion;dynamic load balancing;charge systeme;probability;algorithm analysis;generic model;carga sistema;equilibrio de carga;equilibrage charge;charge dynamique;dynamic load;carga repartida;algorithme;algorithm;estimation;probabilidad;probabilite;balancing;load balancing;charge repartie;analyse algorithme;procesador;load balance;processeur;equilibrage;analisis algoritmo;processor;generation charge;system load;algoritmo;distributed load;modele generation randomisee	In this paper we consider dynamic load balancing algorithms for randomized and adversarial load generation models. Consider a system of n processors. In our randomized generation models every processor may generate a task with a certain probability at each time step, leading to an expected system load of 0(n). We present a load balancing algorithm that assures that with high probability no processor has a load exceeding O(log log n) at an arbitrary point of time. This improves upon the 0 ((log log n)“) bound of [41. that is, the new tasks either arrive one after the other, or more than one task is allowed to arrive at the same time. Another important parameter is the question of z&en new tasks get generated. Here, one can imagine probabilistic, deterministic, or even adversarial approaches. Furthermore, it is often distinguished between static and dynamic models. In the first model we have a fixed set of tasks to be distributed (cf. classic “balls into bins” games), whereas in the second one new tasks are generated as time goes on. In the case of the adversarial load generation model every processor can change its load by some constant at each time step. Thus, the system load may become arbitrarily large. We present a balancing algorithm and show that if at some point of time T no processor has a load exceeding some constant times the average, with high probability this holds for the next polynomial number of steps. Furthermore, we show that if the system is unstable at some point of time (meaning that there are processors with load much more than the average), then our algorithm recovers the system within expected poly(n) steps. Load consumption describes how long it takes for the processors to work on their tasks. Again, this can be modelled by probabilistic, deterministic, or adversarial distributions. The aim of load balancing heavily depend on the type of system, generally described by resource bottlenecks. If the transferring of tasks is expensive, as few as possible transfers are desirable. Since idle processor time is expensive, we want to minimize the number of idle processors, or since memory is the critical resource, minimizing the maximum load of any processor is the primary goal. Furthermore, the kind of application or a certain response behavior (i.e. real time requirements) has to be taken into account for the choice of the balancing algorithm.	adversary (cryptography);central processing unit;control theory;deterministic algorithm;load (computing);load balancing (computing);polynomial;probabilistic turing machine;randomized algorithm;requirement;with high probability	Petra Berenbrink;Tom Friedetzky;Angelika Steger	1999		10.1145/305619.305638	parallel computing;real-time computing;computer science;load balancing;algorithm;statistics	Theory	15.163925367037319	13.12625002076918	182219
bbfdf1484340dbffd4bb8453a576230efbf35205	efficient web searching using temporal factors	search engine;sequencage;design of algorithms;information retrieval;aproximacion;problema np duro;web searching;probabilistic approach;approximation;algorithme;algorithm;random;np hard problem;sequencing;probleme np difficile;recherche information;enfoque probabilista;approche probabiliste;matching;vlsp;tâche appariement;tarea apareamiento;variable length sequencing problem;web search;recuperacion informacion;matching task;algoritmo	We study the issues involved in the design of algorithms for performing information gathering more e!ciently, by taking advantage of anticipated variations in access times in di:erent regions at di:erent times of the day or week. We look at the problem theoretically, as a generalisation of single processor sequencing with release times and deadlines, in which performance times (lengths) of the tasks can change in time. The new problem is called Variable Length Sequencing Problem (VLSP). We show that although the decision version of VLSP seems to be intractable in the general case, it can be solved optimally for lengths 1 and 2. This result opens the possibility of practicable algorithms to schedule searches e!ciently when expected access times can be categorised as either slow or fast. Some algorithms for more general cases are examined and complexity results derived. c © 2001 Elsevier Science B.V. All rights reserved.	access time;algorithm;approximation algorithm;complexity;decision problem;exec shield;experiment;graph coloring;heuristic;np-hardness;robot;scheduling (computing);web crawler;web page;web search engine	Artur Czumaj;Ian Finch;Leszek Gasieniec;Alan Gibbons;Paul H. Leng;Wojciech Rytter;Michele Zito	2001	Theor. Comput. Sci.	10.1016/S0304-3975(00)00366-2	matching;computer science;artificial intelligence;theoretical computer science;approximation;sequencing;np-hard;mathematics;algorithm;search engine	Theory	17.19228140865708	11.327342494831418	183579
00bba1a2589048d771a72ff2b0c39e2aa8761d2f	efficient methods for qualitative spatial reasoning	heuristic method;qualitative spatial reasoning;artificial intelligent;phase transition;is success;temporal reasoning	The theoretical properties of qualitative spatial reasoning in the RCC-8 framework have been analyzed extensively. However, no empirical investigation has been made yet. Our experiments show that the adaption of the algorithms used for qualitative temporal reasoning can solve large RCC-8 instances, even if they are in the phase transition region { provided that one uses the maximal tractable subset of RCC-8 that has been identiied by us. In particular, we demonstrate that the orthogonal combination of heuristic methods is successful in solving almost all apparently hard instances in the phase transition region up to a certain size in reasonable time.	algorithm;automated reasoning;cobham's thesis;experiment;heuristic;maximal set;spatial–temporal reasoning	Bernhard Nebel;Jochen Renz	2001	J. Artif. Intell. Res.	10.1613/jair.872	phase transition;qualitative reasoning;artificial intelligence;machine learning;mathematics;reasoning system;algorithm	AI	12.955237974070464	16.205504895478725	186758
1c10465ef779e47725717d6f6f990011402e7aca	rich coresets for constrained linear regression	cs ds;cs lg	A rich coreset is a subset of the data which contains nearly all the essential information. We give deterministic, low order polynomial-time algorithms to construct rich coresets for simple and multiple response linear regression, together with lower bounds indicating that there is not much room for improvement upon our results.	algorithm;coreset;time complexity	Christos Boutsidis;Petros Drineas;Malik Magdon-Ismail	2012	CoRR		coreset;mathematics;artificial intelligence;linear regression;pattern recognition	Theory	17.12694267387328	17.932527226519106	188008
8f3e37b0eb80982bfa7e503c3398786a7af094bf	the reordering buffer problem on the line revisited		The reordering bu↵er problem (or also sorting bu↵er problem) was introduced by Räcke, Sohler, and Westermann in 2002 [14] and has been extensively studied since then. In this problem, a metric space is given1 and a sequence of items arrive online. Each item is associated with a point in the metric space. We allow multiple items to be associated with the same point. An online algorithm can store up to k items in a bu↵er, but once the bu↵er is full, the algorithm has to process at least one of the items stored in the bu↵er. To process an item from the bu↵er, the algorithm moves a single server in the metric space to the point corresponding to that item. The goal is to minimize the total distance that the server has to travel to process the entire input sequence. The problem is reasonably well understood for some metric spaces. For uniform metric spaces for example, a deterministic O( p log k)-competitive algorithm is known, which is close to the lower bound of ⌦( p log k/ log log k) [1]. Similarly, [4] gives a O(log log k)-competitive randomized online algorithm, which is asymptotically tight [1]. For other metric spaces however, the picture is less clear. We will refrain from listing all known results in detail, but there have been a number of papers investigating this online problem for di↵erent metrics spaces and settings [2, 3, 7, 8, 9, 10, 11, 12, 13]. However, in this column, we will focus on line metric spaces. The last notable result for this metric was obtained eleven years ago by Gamzu and Segev [11]. Their main result is a deterministic O(log n)-competitive online algorithm for a line metric space with n evenly spaced points. In the reminder, we will sketch a slightly simplified and improved version of this result.	online algorithm;randomized algorithm;server (computing);sorting	Matthias Englert	2018	SIGACT News	10.1145/3197406.3197418	theoretical computer science;computer science	Theory	16.645916388117943	15.499104363958564	188012
b6829273f8b0e8ee2f4a9ee5f8d2d1e6639fcbd0	approximation in batch and multiprocessor scheduling		This thesis is about scheduling problems where jobs arrive over time. Depending on the problem, we consider the case that each job has a deadline, or the relaxation that the sum of flow times or completion times need to be minimized. Since most of the discussed problems are NP-hard, the goal is to find polynomial time algorithms with provable approximation guarantee, preferably in an on-line setting. In the first part of this thesis, we consider batch scheduling problems for the case that each job has a deadline, and hence two jobs may be added to the same batch if their due intervals intersect. We first present a framework that unifies all batch cost structures discussed in this part. For instance max-batching, where the cost of each batch is the maximum weight of any contained job. We show that maxbatching is strongly NP-hard in this context if the size of each batch is additionally restricted by a constant capacity constraint, and we also give a polynomial time approximation scheme (PTAS) for this case. Moreover, we consider a minmaxvariant of max-batching which finds application in the area of data aggregation. We show that this variant is strongly NP-hard as well, and we present a quasipolynomial time approximation scheme (QPTAS) and moreover a PTAS for the case that the due interval lengths are constants. Finally, we show that the closely related batch cost structure used in joint replenishment results in an APX-hard problem, which is hence not likely to admit an approximation scheme, but we show that it admits a randomized 5/3-approximation algorithm. The results of this part are published in [1, 2, 3, 4]. In the second part of this thesis, multiprocessor scheduling problems are discussed. We give the first proof that the competitive ratio of the well-known algorithm SRPT is strictly smaller than 2 for completion time scheduling on identical machines. Specifically, we give the upper bound 1.86. Since it is harder to find approximation guarantees for flow time scheduling, we investigate this case in the context of speed-scaling, where it is allowed to arbitrarily increase the speed of each processor subject to some reasonable penalty. For this model, we present a general approach to transform single processor algorithms into multiprocessor algorithms. This yields new or improved constant approximation guarantees for basically all variants of speed-scaled multiprocessor scheduling. The results of this part are published in [5, 6].	apx;competitive analysis (online algorithm);data aggregation;image scaling;job scheduler;job stream;linear programming relaxation;multiprocessing;multiprocessor scheduling;np-hardness;online and offline;ptas reduction;polynomial;polynomial-time approximation scheme;provable prime;quasi-polynomial;randomized algorithm;scheduling (computing);shortest remaining time;strong np-completeness;time complexity;whole earth 'lectronic link	Tim Nonner	2010			parallel computing;multiprocessor scheduling;fair-share scheduling;rate-monotonic scheduling;computer science	Theory	15.869408593991254	11.444301764054106	189316
db4fcd9bfd2ad10a916ce609a25dabcc4294cbe0	editor's foreword special issue on parallel and distributed computing, part i	parallel and distributed computing	"""Recent years have seen phenomenol advances in the technologies of parallel computers and distributed computer systems. To the algorithm designer, both types of systems offer the potential of tremendous computational power. The other side of the coin is that in order to realize that potential, several challenging problems must often be solved. This special issue of Algorithmica is devoted to such problems arising in the areas of parallel and distributed computing. The quality and quantity of submitted papers was extremely high--so high, in fact, that this special issue is presented in two installments. This is Part II of the special double issue; the reader is referred to Volume 3, Number 1 for the first part. The first three papers of this sequel present efficient parallel algorithms for several important combinatorial problems in the class NC. The model of computation used in each case is the parallel random-access machine (PRAM). In the first paper, """"Parallel Computational Geometry,"""" Alok Aggarwal, Bernard Chazelle, Leo Guibas, Colm O'Dfinlaing, and Chee Yap give fast parallel algorithms for a wide range of construction problems in computational geometry. The problems include constructing convex hulls and Voronoi diagrams, triangulating a simple polygon, finding the minimal-area triangle enclosing a convex polygon, and data structures for multidimensional queries. For inputs of size n, the algorithms run in time T= O(log k n), for some k-<3, on a concurrent-read exclusive-write (CREW) PRAM with P = n processors. For some of the algorithms, the product P • T matches the lower bound for the execution time in the sequential RAM model, and hence in those cases the algorithms are optimal; put another way, the corresponding problems are in the class PC* defined by Vitter and Simons in their May 1986 paper in IEEE Transactions on Computers. The next paper, """"The Accelerated Centroid Decomposition Technique for Optimal Parallel Tree Evaluation in Logarithmic Time,"""" by Richard Cole and Uzi Vishkin, makes two basic contributions: it introduces an interesting treecontraction technique called accelerated centroid decomposition (ACD) that leads to optimal parallel algorithms for tree evaluation and other problems on trees. The algorithms run in O(log n) time using n/log n processors on the exclusive-read exclusive-write (EREW) PRAM model, the weakest of the PRAM variants. The second contribution is more methodological in nature. Based on their experiences, the authors isolate four techniques (list ranking, the Euler tour technique, centroid decomposition, and ACD) as basic building blocks for constructing optimal parallel algorithms for tree problems."""	algorithmica;alok r. chaturvedi;central processing unit;computational geometry;computer;convex hull;data structure;distributed computing;euler method;euler tour technique;ieee transactions on computers;keneth alden simons;leo (computer);list ranking;model of computation;nc (complexity);parallel algorithm;parallel computing;parallel random-access machine;run time (program lifecycle phase);voronoi diagram	Jeffrey Scott Vitter	1988	Algorithmica	10.1007/BF01762107	computer science;mathematics;distributed computing	Theory	15.060632830380737	14.51479391414103	189540
9872957cb3248516ec4be9b967136e7b22b0d205	online scheduling with lookahead: multipass assembly lines	heuristic;online scheduling;assembly lines;scheduline;non omniscient;competitive analysis;greedy;lookahead	This paper describes our use of competitive analysis and the on-line model of computation in a product development setting; speciically, we use competitive analysis to evaluate on-line scheduling strategies for controlling a new generation of networked reprographic machines (combination printer-copier-fax machines servicing a network) currently being developed by companies such as Xerox Corporation. We construct an abstract machine model, the multipass assembly line, which not only models networked reprographic machines but also models several common manufacturing environments such as a robotic assembly line or a mixed product assembly line. We consider on-line algorithms with nite lookahead because these machines typically have limited knowledge of the future. We rst prove some lower bounds on the performance of any on-line algorithm with nite lookahead. We then show that simple greedy algorithms achieve competitive ratios that are close to these general lower bounds. In particular, we show that lookahead improves the competitive ratio of these simple greedy algorithms from approximately 2 (with no lookahead) to being arbitrarily close to 1 (for large lookahead). This implies these simple greedy algorithms are realistic candidates for eld use in future reprographic products.	abstract machine;carry-lookahead adder;competitive analysis (online algorithm);fax;greedy algorithm;lookahead carry unit;model of computation;new product development;online algorithm;online and offline;parsing;photocopier;printer (computing);robot;scheduling (computing)	Rajeev Motwani;Vijay A. Saraswat;Eric Torng	1998	INFORMS Journal on Computing	10.1287/ijoc.10.3.331	competitive analysis;mathematical optimization;real-time computing;heuristic;computer science;machine learning;distributed computing	Theory	16.142915881680114	12.99333426197414	190226
e0e35f008a1c1e7f86a15fab5ae2e4523481ada2	multidimensional balanced allocations	balls and bins;distributed search;load balance	We consider a multidimensional variant of the balls-and-bins problem, where balls correspond to random D-dimensional 0-1 vectors. This variant is motivated by a problem in load balancing documents for distributed search engines. We demonstrate the utility of the power of two choices in this domain.	distributed web crawling;load balancing (computing);power of two;web search engine	Andrei Z. Broder;Michael Mitzenmacher	2005			mathematical optimization;computer science;load balancing;mathematics;distributed computing	Theory	15.955894095643636	14.731832850917726	191271
7047c8afe61ae37fd7074eb54d1cb7a5a27e7420	finding efficient nonlinear functions by means of genetic programming	genetic program;message authentication	The design of highly nonlinear functions is relevant for a number of different applications, ranging from database hashing to message authentication. But, apart from useful, it is quite a challenging task. In this work, we propose the use of genetic programming for finding functions that optimize a particular nonlinear criteria, the ava- lanche effect, using only very efficient operations, so that the resulting functions are extremely efficient both in hardware and in software.	genetic programming;nonlinear programming	Julio César Hernández Castro;Pedro Isasi Viñuela;Cristóbal Luque del Arco-Calderón	2003		10.1007/978-3-540-45224-9_161	computer science;theoretical computer science;distributed computing;algorithm	Theory	14.05337798012153	14.426197322895288	196584
