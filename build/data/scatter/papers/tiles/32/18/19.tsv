id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
cc5877fb6d57c5c29f90f83fe70b4178d3a34c63	faster two dimensional scaled matching	teoria imagen;analisis imagen;occupation time;text;image theory;multimedia;image processing;theory of discrete image processing;theorie image;procesamiento imagen;texte;multidimensional pattern matching;traitement image;multimedia systems;dictionnaire;temps occupation;pattern matching;scaled pattern matching;dictionaries;tiempo ocupacion;time use;image analysis;digital image;concordance forme;texto;analyse image;diccionario	The rapidly growing need for analysis of digitized images in multimedia systems has lead to a variety of interesting problems in multidimensional pattern matching. One of the problems is that of scaled matching, finding all appearances of a pattern, proportionally enlarged according to an arbitrary real-sized scale, in a given text. The currently fastest known algorithm for this problem uses techniques from dictionary matching to solve the problem in O(nm 3+n 2 mlog m) time using O(nm 3+n 2) space, where T is a two-dimensional n×n text array and P is a two-dimensional m×m pattern array. We present a new approach for solving the scaled matching problem improving both the running time and the space requirements. Our algorithm runs in O(n 2 m) time and uses O(n 2) space.	algorithm;dictionary;fastest;pattern matching;requirement;time complexity	Amihood Amir;Eran Chencinski	2008	Algorithmica	10.1007/s00453-008-9173-3	image analysis;speech recognition;image processing;computer science;theoretical computer science;pattern matching;mathematics;digital image;algorithm	Theory	14.843844399919606	26.572122912533313	159789
12af91b99c3d8d8388690530c481bcec9483e1ca	finite random variates using differential search trees		Differential search trees have been used for selection with replacement, and for a form of selection without replacement. We show that they can be extended to many different types of selection, both with and without replacement. In addition, virtually every aspect of a differential search tree can be modified dynamically. We provide algorithms for making these modifications. Virtually all differential search tree algorithms are straightforward and easy to implement, especially with our preferred implementation, which is both simple and efficient. Differential search tree operations are virtually all logarithmic with the exception of building the tree and dynamically adding leaves to the tree, which are both linear.		Peter M. Maurer	2017			avl tree;tree rotation;red–black tree;optimal binary search tree;k-ary tree;random binary tree;mathematical optimization;fractal tree index;mathematics;ternary search tree	NLP	14.497740733649971	28.31032308641493	160042
678076601f5f78c7d983aa168404d1d0d039a042	generalized x-code: an efficient raid-6 code for arbitrary size of disk array	number of disks;size of disk array;computational complexity;generalized x code;storage	Many RAID-6 codes have been proposed in the literature, but each has its limitations. Horizontal code has the ability to adapt to the arbitrary size of a disk array but its high computational complexity is a major shortcoming. In contrast, the computational complexity of vertical code (e.g. X-code) often achieves the theoretical optimality, but vertical code is limited to using a prime number as the size of the disk array In this article, we propose a novel efficient RAID-6 code for arbitrary size of disk array: generalized X-code. We move the redundant elements along their calculation diagonals in X-code onto two specific disks and change two data elements into redundant elements in order to realize our new code. The generalized X-code achieves optimal encoding and updating complexity and low decoding complexity; in addition, it has the ability to adapt to arbitrary size of disk array. Furthermore, we also provide a method for generalizing horizontal code to achieve optimal encoding and updating complexity while keeping the code's original ability to adapt to arbitrary size of disk array.	code;computational complexity theory;disk array;standard raid levels	Xianghong Luo;Jiwu Shu	2012	TOS	10.1145/2339118.2339121	systematic code;parallel computing;computer science;theoretical computer science;distributed computing;computational complexity theory	Security	10.363110054135943	29.564170914418277	160143
f0a31a1d41f5d9e45ffe3efc6c88266e4ad9dc8a	general methods for adding range restrictions to decomposable searching problems	informatica;search problem;gestion memoire;mathematics;storage access;storage management;search strategy;problema investigacion;costo;natuurwetenschappen;gestion memoria;ordered by external client;general methods;estructura datos;acces memoire;strategie recherche;acceso memoria;structure donnee;landbouwwetenschappen;probleme recherche;data structure;wiskunde en informatica wiin;estrategia investigacion;cout	In this paper we consider the problem of adding range restrictions to decomposable searching problems. Two classes of structures for this problem are described. The first class consists of structures that use little storage and preprocessing time but still have reasonable query time. The second class consists of structures that have a much better query time, at the cost of an increase in the amount of storage and preprocessing time. Both classes of structures can be tuned to obtain different trade-offs. First we only describe static structures. To dynamize the structures a new type of weight-balanced multiway tree (the MWB-iree) is introduced that is used as an underlying structure. The MWB-tree might be useful in other applications as well.		Hans W. Scholten;Mark H. Overmars	1989	J. Symb. Comput.	10.1016/S0747-7171(89)80002-1	data structure;input/output;search problem;mathematics;algorithm	Theory	15.383711386014793	27.51489901754922	160175
370f8155eb0ea0930f15a60aa3a09ba3abf63ea6	data structure for representing a graph: combination of linked list and hash table	hash table;adjacency matrix;data structure	In this article we discuss a data structure, which combines advantages of two different ways for representing graphs: adjacency matrix and collection of adjacency lists. This data structure can fast add and search edges (advantages of adjacency matrix), use linear amount of memory, let to obtain adjacency list for certain vertex (advantages of collection of adjacency lists). Basic knowledge of linked lists and hash tables is required to understand this article. The article contains examples of implementation on Java.	adjacency list;adjacency matrix;data structure;graph (discrete mathematics);hash table;java;linked list	Maxim A. Kolosovskiy	2009	CoRR		adjacency list;hash table;data structure;breadth-first search;computer science;theoretical computer science;data mining;database;hash list;programming language;adjacency matrix	ML	16.474923885153594	29.557132655870095	160206
1e28b7cd86082656bf86c01009627be6803d0242	asymptotically tight bounds for performing bmmc permutations on parallel disk systems	lower and upper bound;asymptotic optimality;lower bound	We give asymptotically equal lower and upper bounds for the number of parallel I/O operations required to perform bit-matrix-multiply/complement (BMMC) permutations on parallel disk systems. In a BMMC permutation on N records, where N is a power of 2, each (lgN )-bit source address x maps to a corresponding (lgN)-bit target address y by the matrix equation y = Ax c, where matrix multiplication is performed over GF (2). The characteristic matrix A is (lgN ) (lgN ) and nonsingular over GF (2). Under the Vitter-Shriver parallel-disk model with N records, D disks, B records per block, and M records of memory, we show a universal lower bound of N BD 1 + rank lg(M=B) parallel I/Os for performing a BMMC permutation, where is the lower left lg(N=B) lgB submatrix of the characteristic matrix. We also present an algorithm that uses at most 2N BD l rank lg(M=B) m + 2 parallel I/Os, which asymptotically matches the lower bound and improves upon the BMMC and bit-permute/complement (BPC) algorithms in [4]. When rank is low, this method is an improvement over the general-permutation bound of N BD lg(N=B) lg(M=B) . We introduce a new subclass of BMMC permutations, called memoryload-dispersal (MLD) permutations, which can be performed in one pass. This subclass, which is used in the BMMC algorithm, extends the catalog of one-pass permutations appearing in [4]. Although many BMMC permutations of practical interest fall into subclasses that might be explicitly invoked within the source code, we show how to detect in at most N=BD+ l lg(N=B)+1 D m parallel I/Os whether a given vector of target addresses speci es a BMMC permutation. Thus, one can determine e ciently at run time whether a permutation to be performed is BMMC and then avoid the general-permutation algorithm and save parallel I/Os by using our algorithm.	algorithm;bit-reversal permutation;blu-ray;grammatical framework;input/output;matrix multiplication;parallel i/o;permutation pattern;polynomial matrix;power of two;pseudorandom permutation;run time (program lifecycle phase);the matrix	Thomas H. Cormen;Leonard F. Wisniewski	1993		10.1145/165231.165248	topology;geometry;upper and lower bounds	ML	13.223133621572106	29.72537164042414	160717
3cc611234faa22e5aafb3cbf91a571ef70b65e22	"""selection and sorting in the """"restore"""" model"""		We consider the classical selection and sorting problems in a model where the initial permutation of the input has to be restored after completing the computation. While the requirement of the restoration is stringent compared to the classical versions of the problems, this model is more relaxed than a read-only memory where the input elements are not allowed to be moved within the input array. We first show that for a sequence of n integers, selection (finding the median or more generally the k-th smallest element for a given k) can be done in O(n) time using O(lgn) words of extra space in this model. In contrast, no linear-time selection algorithm is known which uses polylogarithmic space in the read-only memory model. For sorting n integers in this model, we first present an O(n lgn)-time algorithm using O(lgn) words of extra space. When the universe size U is polynomial in n, we give a faster O(n)-time algorithm (analogous to radix sort) which uses O(n) words of extra space for an arbitrarily small constant ε > 0. More generally, we show how to match the time bound of any word-RAM integer-sorting algorithms using O(n) words of extra space. In sharp contrast, there is an Ω(n/S)-time lower bound for integer sorting using O(S) bits of space in the read-only memory model. Extension of our results to arbitrary input types beyond integers is not possible: for “indivisible” input elements, we can prove the same Ω(n/S) lower bound for sorting in our model. En route, we develop linear-time in-place algorithms to extract leading bits of the input array and to compress and decompress strings with low entropy; these techniques may be of independent interest.	circuit restoration;computation;dijkstra's algorithm;in-place algorithm;indivisible;integer sorting;polylogarithmic function;polynomial;radix sort;random-access memory;read-only memory;selection algorithm;sorting algorithm;time complexity	Timothy M. Chan;J. Ian Munro;Venkatesh Raman	2014		10.1137/1.9781611973402.74		Theory	11.938789067054822	26.423273745653436	160741
7db37ee76c2df08aed40f4e11dd108d32b0716e4	some parallel algorithms for integer factorisation	algoritmo paralelo;arithmetique ordinateur;parallel algorithm;cle publique;public key cryptosystem;algorithme parallele;number theory;computer arithmetic;factorization;public key;factorizacion;digital signature;criptografia;cryptography;llave publica;factorisation;aritmetica ordenador;signature numerique;cryptographie;theorie nombre;parallel machines;teoria numeros;digital number	Algorithms for finding the prime factors of large composite numbers are of practical importance because of the widespread use of public key cryptosystems whose security depends on the presumed difficulty of the factorisation problem. In recent years the limits of the best integer factorisation algorithms have been extended greatly, due in part to Moore’s law and in part to algorithmic improvements. It is now routine to factor 100-decimal digit numbers, and feasible to factor numbers of 155 decimal digits (512 bits). We describe several integer factorisation algorithms, consider their suitability for implementation on parallel machines, and give examples of their current capabilities.	cryptosystem;integer factorization;monoid factorisation;moore's law;parallel algorithm;public-key cryptography	Richard P. Brent	1999		10.1007/3-540-48311-X_1	arithmetic;number theory;computer science;theoretical computer science;mathematics;factorization;algorithm	Theory	14.227156428935954	31.683795036965194	162364
692ec5dfaf4feafe6b2bdb655066717dd7f0d27b	building cartesian trees from free trees with k leaves	range queries;cartesian tree;data structures	Abstract   One can build a Cartesian tree from an  n -element sequence in   O  (  n  )   time, and from an  n -node free tree in   O  (  n  log    n  )   time (with a matching worst-case lower bound in the comparison model of computation). We connect these results together by describing a Cartesian tree construction algorithm based on a “bitonicity transform” running in   O  (  n  log    k  )   time on a free tree with  k  leaves, noting that a path is the special case of a tree with just 2 leaves. We also provide a matching worst-case lower bound in the comparison model.	cartesian closed category	Brian C. Dean;Raghuveer Mohan	2013	Inf. Process. Lett.	10.1016/j.ipl.2013.02.014	cartesian tree;segment tree;combinatorics;discrete mathematics;tree rotation;vantage-point tree;range tree;k-ary tree;interval tree;mathematics;algorithm;avl tree	DB	15.261664291697027	28.621013356604863	162634
abd7592823cbeb648a5b9c3732250fb09e89ec9d	multiple matching of rectangular patterns	string matching;aspect ratio	We describe the first .efiicient algorithm for simultaneously matching multiple rectangular patterns of varying sizes and aspect, ratios in a rectangular text. Efficient means significantly better asymptotically than known al,qorithrns that handle one height, width, or aspect ratio at a time. Our algorithm features an interesting use of multidimensional range searching, as well as new adaptations of several known techniques for two climensional string matching. We also extend our algorithm to a dynamic setting where the set of patterns can change over time.	range searching;string searching algorithm	Ramana M. Idury;Alejandro A. Schäffer	1993		10.1145/167088.167116	aspect ratio;approximate string matching;computer science;mathematics;algorithm;string searching algorithm	Theory	14.489826596118107	26.503104925256252	162817
20743017e5980abf9922c83bfe0b225e8cab6d01	bonsai: a compact representation of trees	daaptive prediction;compression texte;adaptive prediction;trie structures;text compression;invertible hash function;compact representation;hash table;adaptative prediction;prevision adaptative;fonction hachage inversible;computer science;table hachage;hash trees;hash tree;compact hashing;hachage compact	This paper shows how trees can be stored in a very compact form, called ‘Bonsai’, using hash tables. A method is described that is suitable for large trees that grow monotonically within a predefined maximum size limit. Using it, pointers in any tree can be represented within 6+ log2n bits per nodewhere n is the maximum number of children a node can have. We first describe a general way of storing trees in hash tables, and then introduce the idea of compact hashing which underlies the Bonsai structure. These two techniques are combined to give a compact representation of trees, and a practical methodology is set out to permit the design of these structures. The new representation is compared with two conventional tree implementations in terms of the storage required per node. Examples of programs that must store large trees within a strict maximum size include those that operate on trie structures derived from natural language text. We describe how the Bonsai technique has been applied to the trees that arise in text compression and adaptive prediction, and include a discussion of the design parameters that work well in practice.	bonsai;data compression;hash table;insertion sort;natural language;pointer (computer programming);real-time transcription;tree (data structure);trie;vax-11	John J. Darragh;John G. Cleary;Ian H. Witten	1993	Softw., Pract. Exper.	10.1002/spe.4380230305	hash table;computer science;theoretical computer science;weight-balanced tree;programming language;algorithm;hash tree	Graphics	11.638811648450313	27.4412929152103	163199
3d93237c47664de9f2c3075cc97fe66e90c90b49	some theory and practice of greedy off-line textual substitution	data compression;statistics usa councils physics computing labeling costs;tree data structures;parallel architectures;data structures greedy off line textual substitution steepest descent approach structural inference compression text string substring pointers contracted text string performance computational issues experiments massively disseminated data compression dedicated architectures parallel implementation encoding suffix tree;tree data structures word processing data compression parallel architectures encoding;parallel implementation;encoding;word processing;steepest descent	Greedy o -line textual substitution refers to the following steepest descent approach to compression or structural inference. Given a long textstring x, a substring w is identi ed such that replacing all instances of w in x except one by a suitable pair of pointers yields the highest possible contraction of x; the process is then repeated on the contracted textstring, until substrings capable of producing contractions can no longer be found. This paper examines computational issues and performances resulting from implementations of this paradigm in preliminary applications and experiments. There is enough motivation for studying this and the many other conceivable variants of greedy o -line methods that fall in the wide and relatively unexplored gap between the classical, linear and polar methods introduced by [15] and [16], and the generally intractable optimal macro schemes [12]. Apart from intrinsic interest, these methods may nd use in the compression of massively disseminated data, e.g., of the kind considered in [8], and lend themselves to e cient parallel implementation, perhaps on dedicated architectures such as, e.g., in [7]. Given a string x over an alphabet and any substring w of x, we denote by f w	computation;experiment;gradient descent;greedy algorithm;performance;programming paradigm;substring	Alberto Apostolico;Stefano Lonardi	1998		10.1109/DCC.1998.672138	data compression;gradient descent;computer science;theoretical computer science;machine learning;mathematics;tree;algorithm;encoding;statistics	Theory	11.622548404455301	27.73901850289722	163287
3163d9c4154e5de6ddc7b7c332df6fe4b8eadad4	alternative algorithms for order-preserving matching	simd;avx;sse;order preserving matching;string searching;fm index	The problem of order-preserving matching is to find all substrings in the text which have the same relative order and length as the pattern. Several online and one offline solution were earlier proposed for the problem. In this paper, we introduce three new solutions based on filtration. The two online solutions rest on the SIMD (Single Instruction Multiple Data) architecture and the offline solution is based on the FM-index scheme. The online solutions are implemented using two different SIMD instruction sets, SSE (streaming SIMD extensions) and AVX (Advanced Vector Extensions). Our main emphasis is on the practical efficiency of algorithms. Therefore, we show with practical experiments that our new solutions are faster than the previous solutions.	advanced vector extensions;algorithm;experiment;fm-index;online and offline;streaming simd extensions;substring	Tamanna Chhabra;M. Oguzhan Külekci;Jorma Tarhio	2015			discrete mathematics;parallel computing;simd;computer science;theoretical computer science;fm-index	Web+IR	12.224337653273997	28.18442229306231	163511
1bc938eb877e09330593893c7c9a147a30ca3622	fast parameterized matching with q-grams	two dimensions;2 dimensional;2 dimensional string matching;parameterized matching;string matching	Two strings parameterize match if there is a bijection defined on the alphabet that transforms the first string character by character into the second string. The problem of finding all parameterized matches of a pattern in a text has been studied in both one and two dimensions but the research has been centered on developing algorithms with good worst-case performance. We present algorithms that solve this problem in sublinear time on average for moderately repetitive patterns.	grams	Leena Salmela;Jorma Tarhio	2008	J. Discrete Algorithms	10.1016/j.jda.2007.11.001	combinatorics;two-dimensional space;discrete mathematics;approximate string matching;commentz-walter algorithm;string;theoretical computer science;mathematics;string metric;string searching algorithm	Theory	13.604962941656076	26.893612208496243	163647
dcc3d6b54a2d5a79ea64ae4b6f41a0efa2bd56b8	an improved algorithm for traversing binary trees without auxiliary stack	binary tree		algorithm;recursion (computer science)	John Michael Robson	1973	Inf. Process. Lett.	10.1016/0020-0190(73)90018-5	random binary tree;optimal binary search tree;cartesian tree;red–black tree;combinatorics;binary search tree;tree rotation;binary expression tree;binary tree;computer science;treap;theoretical computer science;self-balancing binary search tree;k-d tree;k-ary tree;interval tree;mathematics;weight-balanced tree;ternary search tree;threaded binary tree;tree traversal;algorithm	DB	16.06330032967419	28.607387806633106	164601
5f98e6752764218a6ba5e53a559c4c2abe64c86b	on the dynamic finger conjecture for splay trees. part i: splay sorting log n-block sequences	arbre recherche;dynamic finger conjecture;tree;sorting;information retrieval;amortized analysis;amortization;arbol;68q20;68p10;tria;amortissement comptable;theorem proving;demonstration theoreme;amortizacion financiera;arbol investigacion;arbol binario;recherche information;binary search tree;triage;arbre binaire;splay tree;68p05;arbre;conjecture doigt dynamique;68q25;arbre binaire recherche;recuperacion informacion;finger search tree;demostracion teorema;search tree;binary tree	A special case of the Dynamic Finger Conjecture is proved; this special case introduces a number of useful techniques.	sorting;splay tree	Richard Cole;Bud Mishra;Jeanette P. Schmidt;Alan Siegel	2000	SIAM J. Comput.	10.1137/S0097539797326988	combinatorics;discrete mathematics;binary search tree;amortized analysis;binary tree;computer science;sorting;splay tree;mathematics;automated theorem proving;search tree;tree;programming language;amortization;algorithm	Theory	15.956508291139484	27.37361128410144	165158
0032d13e96645bbf124269fd9e3659c1845b0cc6	on the optimal space complexity of consensus for anonymous processes		The optimal space complexity of consensus in asynchronous shared memory was an open problem for two decades. For a system of n processes, no algorithm using a sublinear number of registers is known. Up until very recently, the best known lower bound due to Fich, Herlihy, and Shavit was $${\varOmega }(\sqrt{n})$$ Ω(n) registers. Fich, Herlihy, and Shavit first proved their lower bound for the special case of the problem where processes are anonymous (i.e. they run the same algorithm) and then extended it to the general case. In this paper we close the gap for the anonymous case of the problem. We show that any consensus algorithm from read–write registers for anonymous processes that satisfies nondeterministic solo termination has to use $${\varOmega }(n)$$ Ω(n) registers in some execution. This implies an $${\varOmega }(n)$$ Ω(n) lower bound on the space complexity of deterministic obstruction-free and randomized wait-free consensus, matching the upper bound. We introduce new techniques for marshalling anonymous processes and their executions, in particular, the concepts of leader–follower pairs and reserving executions, that play a critical role in the lower bound argument and will hopefully be more generally applicable.	chandra–toueg consensus algorithm;dspace;marshalling (computer science);maurice herlihy;non-blocking algorithm;randomized algorithm;robertson–seymour theorem;shared memory;structure of observed learning outcome	Rati Gelashvili	2018	Distributed Computing	10.1007/s00446-018-0331-9	distributed computing;algorithm	Theory	10.896377381850462	30.825235868431754	167454
1f91bc7ce1641235f3c41eec0a8774f4834e5a82	comparing insertion schemes used to update 3-2 trees		Abstract   A set of difference equations which represents transformations that occur to classes of subtrees of 3-2 trees under random insertions is developed. A solution of these equations represents the average relative frequency of subtree occurrence. These results are used in estimating average storage and insertion costs. A technique is presented for estimating an implementation parameter (the probability of a key shifting to a neighboring node under insertion) which allows comparison of various insertion strategies.		James A. Larson;William E. Walden	1979	Inf. Syst.	10.1016/0306-4379(79)90014-0	theoretical computer science;algorithm	DB	14.533254838978547	29.038845158784923	167586
a166c1af3e28ba40efb7574171771fe0fc54d68b	run-based trie involving the structure of arbitrary bitmask rules			mask (computing);trie	Kenji Mikawa;Ken Tanaka	2015	IEICE Transactions		combinatorics;discrete mathematics;computer science;theoretical computer science;machine learning;decision tree;mathematics;x-fast trie	DB	16.415921115659255	29.101198538918368	168107
eb0cdfde40cd42ece12225e784e64b12c9f68df4	optimal bucket sorting and overlap representations	parallel algorithm;oplimality;upper bound;bucket sorting;minimal interval and circular are overlap representations;optimal algorithm;lower bound;parallel algorithms	In this paper, we investigate the problems of sorting integers by bucket sort and of computing minimal interval and circular are overlap representations, and give several optimal algorithms. We show that, given n linearly ranged integers, sorting can be done • in O(log n) time with O(n/log n ) EREW PRAM processors, or in O(logn/log logn) time with O(logn log n / log n) Common CRCW PRAM processors, if the integers are distinct; • in O(log n /log log n) time with O(n log log n / log n ) Arbitrary CRCW PRAM processors, if a constant upper bounded number of integers have a constant lower bounded multiplicity. We also show that, among other things, given an s× t interval or circular are overlap representation matrix, • a minimal interval overlap representation can be obtained in O(log(st)) time with O(st /log(st)) EREW PRAM processors, or in O(log t / log log t) time with O( st log log t / logt ) Common CRCW PRAM processors, or in O(1) time with O(st) BSR processors; • a minimal circular are overlap representa...	bucket sort;overlap–add method;sorting	Lin H Chen	1997	Parallel Algorithms Appl.	10.1080/10637199708915621	combinatorics;discrete mathematics;parallel computing;computer science;mathematics;parallel algorithm;upper and lower bounds;algorithm	Theory	13.75029185292109	30.77728564709421	168523
13d10589267babc985b261a7258ab324a0d29766	sorting within distance bound on a mesh-connected processor array	random sequence;lower bound	An algorithm is developed which sorts random sequences of keys on the n × n square mesh in the expected time 2n. The algorithm is shown to be optimal, that is, the matching (2n) lower bound on the expected-time of algorithms sorting randomly ordered inputs is proved.	processor array;sorting	Bogdan S. Chlebus	1989		10.1007/3-540-51859-2_18	computer science;random sequence;upper and lower bounds;statistics	Arch	12.445706088397488	31.386021174299735	169103
1ec1b8ecbad801927f7ea3cf3fac8199a6d0788e	an optimal erew parallel algorithm for parenthesis matching	parallel algorithm;conference_paper;book		parallel algorithm;parallel random-access machine	Wai Wan Tsang;Tak Wah Lam;Francis Y. L. Chin	1989			parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm	HPC	12.970815773497305	32.24037467558694	170153
e6dce575be5ea60c364d64abc95922e886e3c186	complexity of the parallel givens facotrization on shared memory architectures	linear algebra;parallel algorithm;shared memory;time complexity;optimal algorithm	We study the complexity of the parallel Givens factorization of a square matrix of size n on shared memory multicomputers with p processors. We show how to construct an optimal algorithm using a greedy technique. We deduce that the time complexity is equal to:                Topt (p) = \fracn2 2p + p + o(n) for 1 \leqslant p \leqslant \fracn2 + Ö</font >2 T_{opt} (p) = \frac{{n^2 }}{{2p}} + p + o(n) for 1 \leqslant p \leqslant \frac{n}{{2 + \sqrt 2 }}            and that the minimum number of processors in order to compute the Givens factorization in optimal time Topt is equal to Popt=n/2+2.  These results complete previous analysis presented in the case where the number of processors is unlimited.    	shared memory	Michel Cosnard;El Mostafa Daoudi;Yves Robert	1989		10.1007/3-540-51859-2_9	uniform memory access;distributed shared memory;time complexity;shared memory;parallel computing;average-case complexity;computer science;theoretical computer science;linear algebra;operating system;distributed computing;parallel algorithm;algorithm	HPC	13.542474682193175	31.619143822321572	171077
d77411194f079ee3f463c56c101580a29f329de1	succinct ordinal trees based on tree covering	difference operator;abstract data type;limit set;degree sequence;time use;information theoretic	Various methods have been used to represent a tree of n nodes in essentially the information-theoretic minimum space while supporting various navigational operations in constant time, but different representations usually support different operations. Our main contribution is a succinct representation of ordinal trees, based on that of Geary et al. (7), that supports all the navigational operations supported by various succinct tree representations while requiring only 2n + o(n) bits. It also supports efficient level-order traversal, a useful ordering previously supported only with a very limited set of operations (8).#R##N##R##N#Our second contribution expands on the notion of a single succinct representation supporting more than one traversal ordering, by showing that our method supports two other encoding schemes as abstract data types. In particular, it supports extracting a word (O(lg n) bits) of the balanced parenthesis sequence (11) or depth first unary degree sequence (3; 4) in O(f(n)) time, using at most n/f(n) +o(n) additional bits, for any f(n) in O(lg n) and Ω(1).	ordinal data	Meng He;J. Ian Munro;S. Srinivasa Rao	2007		10.1007/978-3-540-73420-8_45	limit set;mathematical optimization;combinatorics;discrete mathematics;computer science;mathematics;programming language;abstract data type;algorithm	NLP	11.547776223627661	26.74522769257768	171726
273c998c8e64dfd21172ec65e744f708b7545fc7	d-dimensional range search on multicomputers	distributed memory;algoritmo paralelo;parallel distributed memory setting;sequential data structure;base donnee;distributed memory systems;range query;parallel algorithm;parallel construction algorithms;tree data structures broadcasting spatial databases parallel machines global communication sorting distributed computing councils user generated content parallel algorithms;d dimensional cartesian space;running times;query processing;multicomputers;ordinateur parallele;memoria compartida;h relations;report modes;search algorithm;processors d dimensional range search multicomputers range tree data structure multidimensional point sets geometric applications database applications parallel distributed memory setting d dimensional cartesian space coarse grained multicomputer distributed range tree computation time sequential data structure h relations range queries associative function report modes parallel construction algorithms parallel search algorithms running times sequential time;computational geometry;coarse grained multiprocesseur;database;base dato;multiprocesseur a gros grain;range search;tree data structures;recherche alignement;parallel computation;algorithme parallele;range tree;multi dimensional;parallel search algorithms;geometric applications;calculo paralelo;arbol binario;computational complexity;estructura datos;arbre binaire;associative function;ordenador paralelo;parallel computer;structure donnee;search problems;database applications;computation time;coarse grained;memoire repartie;d dimensional range search;calcul parallele;data structure;article;database theory;sequential time;distributed range tree;database theory search problems tree data structures distributed memory systems computational complexity parallel algorithms computational geometry query processing;range queries;multidimensional point sets;coarse grained multicomputer;processors;parallel algorithms;binary tree	The range tree is a fundamental data structure for multidimensional point sets, and, as such, is central in a wide range of geometric and database applications. In this paper we describe the first nontrivial adaptation of range trees to the parallel distributed memory setting (BSP-like models). Given a set of n points in d -dimensional Cartesian space, we show how to construct on a coarse-grained multicomputer a distributed range tree T in time O( s / p + T c (s,p)) , where s = n log d-1 n is the size of the sequential data structure and T c (s,p) is the time to perform an h -relation with h=Θ (s/p) . We then show how T can be used to answer a given set Q of m=O(n) range queries in time O((s log m)/p + T c (s,p)) and O((s log m)/p + T c (s,p) + k/p) , where k is the number of results to be reported. These parallel construction and search algorithms are both highly efficient, in that their running times are the sequential time divided by the number of processors, plus a constant number of parallel communication rounds.	black box;cartesian closed category;central processing unit;data structure;distributed memory;parallel algorithm;parallel computing;range query (data structures);range searching;range tree;search algorithm;segment tree	Afonso Ferreira;Claire Mathieu;Andrew Rau-Chaplin;Stéphane Ubéda	1997	Algorithmica	10.1007/PL00008260	range query;combinatorics;parallel computing;data structure;computational geometry;computer science;theoretical computer science;distributed computing;parallel algorithm;algorithm	Theory	13.23768123222658	29.39142485434589	172335
bd1c1695e7d5f82d358f4b2aca66bd5a8a64f851	an algorithm for parallel evaluation of functions	algorithme c;evaluation fonction;algorithm analysis;analyse algorithme	Abstract   A technique is shown for speeding up parallel evaluation of functions. An algorithm is presented that evaluates powers  x   n   in time  O (√log  n ) using  O ( n ) processors and certain preprocessed data, while the known algorithms take [log 2   n ] steps of multiplication or addition.	algorithm	Eliezer L. Lozinskii	1983	J. Algorithms	10.1016/0196-6774(83)90026-3	theoretical computer science;mathematics;algorithm	Theory	13.935899455664222	30.51869837683331	173494
6cab78d47b96eed942110673f5749d2f8c8b3c59	worst-case analysis of set union algorithms	fonction ackermann;algorithm analysis;asymptotic optimality;worse case method;worst case analysis;equivalence;analysis of algorithm;methode cas pire;analyse algorithme;structure donnee;data structure;union ensemble	This paper analyzes the asymptotic worst-case running time of a number of variants of the well-known method of path compression for maintaining a collection of disjoint sets under union. We show that two one-pass methods proposed by van Leeuwen and van der Weide are asymptotically optimal, whereas several other methods, including one proposed by Rein and advocated by Dijkstra, are slower than the best methods.	asymptotically optimal algorithm;best, worst and average case;disjoint-set data structure;time complexity;whole earth 'lectronic link	Robert E. Tarjan;Jan van Leeuwen	1984	J. ACM	10.1145/62.2160	equivalence;combinatorics;data structure;computer science;mathematics;programming language;algorithm	Theory	16.531598046203733	27.381350840137223	175367
1010b78009532e9ffb17bcf0473c2b866f511442	space-efficient private search with applications to rateless codes	search and retrieval;keyword search;simultaneous equations	Private keyword search is a technique that allows for searching and retrieving documents matching certain keywords without revealing the search criteria. We improve the space efficiency of the Ostrovsky et al. Private Search [9] scheme, by describing methods that require considerably shorter buffers for returning the results of the search. Our basic decoding scheme recursive extraction, requires buffers of length less than twice the number of returned results and is still simple and highly efficient. Our extended decoding schemes rely on solving systems of simultaneous equations, and in special cases can uncover documents in buffers that are close to 95% full. Finally we note the similarity between our decoding techniques and the ones used to decode rateless codes, and show how such codes can be extracted from encrypted documents.	andrei broder;code;cryptography;cryptosystem;decoding methods;dining cryptographers problem;division by two;encryption;equation solving;eurocrypt;lecture notes in computer science;list of cryptographers;overhead (computing);pascal;pseudonymity;recursion;recursion (computer science);sap business one;search algorithm;self-censorship;set packing;simulation;simultaneous equations model;springer (tank);transmitter;web search engine	George Danezis;Claudia Díaz	2007		10.1007/978-3-540-77366-5_15	simultaneous equations;theoretical computer science;data mining	Crypto	10.447857341839281	29.169117294825323	175811
00990946476bfa390461117ffb72dab800b0820e	compact b-trees	b trees;key compression;searching;variable length;linear time	A B-tree is compact if it is minimal in number of nodes, hence has optimal space utilization, among equally capacious B-trees of the same order. The space utilization of compact B-trees is analyzed and is compared with that of noncompact B-trees and of (node)-visit-optimal B-trees, which minimize the expected number of nodes visited per key access. Compact B-trees can be as much as a factor of 2.5 more space-efficient than visit-optimal B-trees; and the node-visit cost of a compact tree is never more than 1 + the node-visit cost of an optimal tree. Finally, an in-place compactification algorithm is presented which operates in linear time in the size of the file.	b-tree;in-place algorithm;time complexity	Arnold L. Rosenberg;Lawrence Snyder	1979		10.1145/582095.582102	b-tree;time complexity;mathematical optimization;computer science	DB	14.048599364709933	28.25926816921228	175829
e13c78f8213cec5678f8a00ba76955b75923d3fb	using minimum path cover to boost dynamic programming on dags: co-linear chaining extended		Aligning sequencing reads on graph representations of genomes is an important ingredient of pan-genomics (Marschall et al. Briefings in Bioinformatics, 2016). Such approaches typically find a set of local anchors that indicate plausible matches between substrings of a read to subpaths of the graph. These anchor matches are then combined to form a (semi-local) alignment of the complete read on a subpath. Co-linear chaining is an algorithmically rigorous approach to combine the anchors. It is a well-known approach for the case of two sequences as inputs. Here we extend the approach so that one of the inputs can be a directed acyclic graph (DAGs), e.g. a splicing graph in transcriptomics or variant graph in pan-genomics. The extension of co-linear chaining to DAGs turns out to have a tight connection to the minimum path cover problem that asks us to find a minimum-cardinality set of paths that cover all the nodes of a DAG. We study the case when the size k of a minimum path cover is small, which is often the case in practice. First, we propose an algorithm for finding a minimum path cover of a DAG (V,E) in O(k|E| log |V |) time, improving all known time-bounds when k is small and the DAG is not too dense. An immediate consequence is an improved space/time tradeoff for reachability queries in arbitrary directed graphs. Second, we introduce a general technique for extending dynamic programming (DP) algorithms from sequences to DAGs. This is enabled by our minimum path cover algorithm, and works by mimicking the DP algorithm for sequences on each path of the minimum path cover. This technique generally produces algorithms that are slower than their counterparts on sequences only by a factor k. We illustrate this on two classical problems extended to labeled DAGs: longest increasing subsequence and longest common subsequence. For the former we obtain an algorithm with running time O(k|E| log |V |). This matches the optimal solution to the classical problem variant when, e.g., the input sequence is modeled as a path. We obtain an analogous result for the longest common subsequence problem. Finally, we apply this technique to the co-linear chaining problem, that is a generalization of both of the above two problems. The algorithm for this problem turns out to be more involved, needing further ingredients, such as an FM-index tailored for large alphabets, and a two-dimensional range search tree modified to support range maximum queries. We implemented the new co-linear chaining approach. Experiments on splicing graphs show that the new method is efficient also in practice.	briefings in bioinformatics;davis–putnam algorithm;directed acyclic graph;directed graph;dynamic programming;fm-index;hash table;longest common subsequence problem;longest increasing subsequence;path cover;range searching;reachability;regular expression;search tree;semiconductor industry;substring;time complexity	Anna Kuosmanen;Topi Paavilainen;Travis Gagie;Rayan Chikhi;Alexandru I. Tomescu;Veli Mäkinen	2018		10.1007/978-3-319-89929-9_7	discrete mathematics;directed acyclic graph;path cover;theoretical computer science;mathematics;substring;chaining;dynamic programming;graph	Theory	13.826456846656018	25.93275131080349	175952
de0ac6b171c7dea33f40769cb33765ec044662c1	ray tracing using dynamic subtree - algorithm and speed evaluation	algorithme rapide;trazado rayos;complexite;algorithme arvo;algoritmo busqueda;concordance;tree;modelo markov;reconocimiento palabra;computer graphics;algorithme recherche;arbol;performance;sous arbre;complejidad;search algorithm;trace rayon;concordancia;tratamiento lenguaje;complexity;grafismo;graphisme;recherche;sintesis imagen;image synthesis;markov model;language processing;image realiste;fast algorithm;traitement langage;ray tracing;arbre;graphism;speech recognition;synthese image;reconnaissance parole;subtree;rendimiento;parole continue;modele markov;investigacion;algoritmo rapido;realistic image synthesis;arbre hierarchique	Abstract#R##N##R##N#For fast ray tracing computation, it is essential to determine efficiently which object is projected on each pixel of the image plane. The data structure plays a major role in this process, and a hierarchical tree data structure of the objects often is used. This paper proposes a very fast algorithm based on use of a subtree of given hierarchical tree.#R##N##R##N##R##N##R##N#Experimental results show that the number of intersection computations can be reduced greatly by the use of subtrees and that the computation cost for dynamic construction of a subtree is low. Analysis of the algorithm supports the experimental results. Experimental comparison with Arvo's algorithm [6], one of the fastest existing algorithms, shows that our algorithm is faster for primary rays.	algorithm;ray tracing (graphics);tree (data structure)	Tadashi Naruse;Mikio Shinya;Takafumi Saito	1993	Systems and Computers in Japan	10.1002/scj.4690240407	computer science;artificial intelligence;mathematics;tree;algorithm	Robotics	15.63987560687767	30.402685456042576	176706
1979960988dee588719f63fb93a89f902c4f941b	speeding up pattern matching by text sampling	space time;suffix array;sampling technique;pattern matching;indexation;string matching	We introduce a novel alphabet sampling technique for speeding up both online and indexed string matching. We choose a subset of the alphabet and select the corresponding subsequence of the text. Online or indexed searching is then carried out on that subsequence, and candidate matches are verified in the full text. We show that this speeds up online searching, especially for moderate to long patterns, by a factor of up to 5. For indexed searching we achieve indexes that are as fast as the classical suffix array, yet occupy space less than 0.5 times the text size (instead of 4) plus text. Our experiments show no competitive alternatives in a wide space/time range.	pattern matching;sampling (signal processing)	Francisco Claude;Gonzalo Navarro;Hannu Peltola;Leena Salmela;Jorma Tarhio	2008		10.1007/978-3-540-89097-3_10	sampling;computer science;theoretical computer science;machine learning;pattern matching;space time;pattern recognition;mathematics;programming language;string searching algorithm	Theory	12.463360155050532	27.652595900112843	176742
588b41d5204a8750a507728f70e851f7d0bb4769	towards real-time suffix tree construction	busqueda informacion;sufijo;modelizacion;metodo caso peor;eje troncal;text;alfabeto infinito;algoritmo busqueda;information retrieval;algorithme recherche;suffix;real time;search algorithm;interrogation base donnee;index structure;chaine caractere;interrogacion base datos;texte;reseau federateur;suffix tree;modelisation;indexing;recherche information;temps reel;indexation;estructura datos;cadena caracter;indizacion;methode cas pire;tiempo real;structure donnee;suffixe;backbone;infinite alphabet;alphabet infini;texto;modeling;worst case method;data structure;database query;character string	The quest for a real-time suffix tree construction algorithm is over three decades old. To date there is no convincing understandable solution to this problem. This paper makes a step in this direction by constructing a suffix tree online in time O(log n) per every single input symbol. Clearly, it is impossible to achieve better than O(log n) time per symbol in the comparison model, therefore no true real time algorithm can exist for infinite alphabets. Nevertheless, the best that can be hoped for is that the construction time for every symbol does not exceed O(log n) (as opposed to an amortized O(log n) time per symbol, achieved by current known algorithms). To our knowledge, our algorithm is the first that spends in the worst caseO(log n) per every single input symbol.#R##N##R##N#We also provide a simple algorithm that constructs online an indexing structure (the BIS) in time O(log n) per input symbol, where n is the number of text symbols input thus far. This structure and fast LCP (Longest Common Prefix) queries on it, provide the backbone for the suffix tree construction. Together, our two data structures provide a searching algorithm for a pattern of length m whose time is $O(min(m {\rm log} |{\it \Sigma}|,m + {\rm log} n) + tocc)$, where tocc is the number of occurrences of the pattern.	real-time transcription;suffix tree	Amihood Amir;Tsvi Kopelowitz;Moshe Lewenstein;Noa Lewenstein	2005		10.1007/11575832_9	search engine indexing;speech recognition;systems modeling;data structure;string;computer science;artificial intelligence;database;mathematics;programming language;algorithm;search algorithm	Theory	11.243720840904947	27.569146875137598	176761
a0fd2d97ccfd0159f0cd13239d5db67ded689639	a note on zero error algorithms having oracle access to one np query	interrogation base donnee;chaine caractere;interrogacion base datos;probabilistic approach;combinatorial problem;probleme combinatoire;problema combinatorio;enfoque probabilista;approche probabiliste;cadena caracter;database query;oracle;character string	It is known that ${{\rm S}_2^p} \subseteq {{{\rm ZPP}}^{{\rm NP}}}$ [3]. The reverse direction of whether ZPPNP is contained in S$_{\rm 2}^{p}$ remains open. We show that if the zero-error algorithm is allowed to ask only one query to the NP oracle (for any input and random string), then it can be simulated in S$_{\rm 2}^{p}$. That is, we prove that ${{{\rm ZPP}}^{{\rm NP}[1]}} \subseteq {{\rm S}_2^p}$.		Jin-Yi Cai;Venkatesan T. Chakaravarthy	2005		10.1007/11533719_35	oracle;string;computer science;theoretical computer science;database;programming language;algorithm	Theory	14.990706880023362	25.709454551527738	177310
d75d9067233e11663c83b09b4eda875e9c3d7958	a parallel solution to the extended set union problem with unlimited backtracking	memory management;find;backtrack;multiunion;logic;constant time performance parallel solution extended set union problem unlimited backtracking erew pram model dynamic partition find union backtrack restore setunion multiunion data structure k parallel union find trees constant parallel time optimal work;restore;extended set union problem;tree data structures;constant time performance;parallel solution;constant parallel time;dynamic partition;computational complexity;data structures;backtracking;random access storage;parallel machines;optimal work;union;tree searching;computational complexity parallel machines parallel algorithms tree data structures backtracking tree searching random access storage;setunion;erew pram model;data structure;k parallel union find trees;tree data structures data structures;unlimited backtracking;parallel algorithms	In this paper, we study on the EREW-PRAM model a parallel solution to the extended set union problem with unlimited backtracking which maintains a dynamic partition Π of an n-element set S subject to the usual operations Find, Union, Backtrack and Restore as well as the new operations SetUnion, MultiUnion. The SetUnion operation is a special case of the commonly known Union operation aimed to unify two prespecified set-names, while MultiUnion operation deals with a batch of Union operations. A new data structure, called k-Parallel Union Find (or, k-PUF) trees, is introduced to represent a disjoint set in Π. The structure is defined for a wide range for the parameter k, but the more interesting results are achieved for k = logn log logn . In this case, using thek-PUF trees, both SetUnion and Restore operations are performed in constant parallel time requiring optimal work O(k). This constant-time performance is not achievable parallelizing the existing data structures. Moreover, using p = O(k) processors, MultiUnion for a batch of p operations is performed in O(k) time, requiring optimal work O(pk).	backtrack;backtracking;central processing unit;data structure;disjoint-set data structure;parallel computing;parallel random-access machine	Maria Cristina Pinotti;Vincenzo A. Crupi;Sajal K. Das	1996		10.1109/IPPS.1996.508055	parallel computing;data structure;computer science;proof of o(log*n) time complexity of union–find;theoretical computer science;distributed computing;programming language;logic;algorithm;backtracking;memory management	Theory	11.634012987966377	30.38252748092861	177607
4ebce843297395b5e2b0339f36b5faea3d7cef6b	on constructing binary space partitioning trees	computer graphic;binary space partition;greedy algorithm;lower bound	Binary Space Partitioning Trees have several applications in computer graphics. We prove that there exist n-polygon problem instances with an O(n2) lower bound on tree size. We also show that a greedy algorithm may result in constructing a tree with O(n2) nodes, while there exist a tree for the same n-polygon instance with only O(n) nodes. Finally, we formulate six different heuristics and test their performance.	binary space partitioning;computer graphics;existential quantification;greedy algorithm;heuristic (computer science)	Ravinder Krishnaswamy;Ghasem S. Alijani;Shyh-Chang Su	1990		10.1145/100348.100383	random binary tree;optimal binary search tree;ball tree;red–black tree;greedy algorithm;binary search tree;tree rotation;binary expression tree;exponential tree;binary tree;computer science;theoretical computer science;scapegoat tree;self-balancing binary search tree;k-d tree;k-ary tree;interval tree;upper and lower bounds;ternary search tree;threaded binary tree;tree traversal;avl tree	Theory	16.99769055408129	28.16315532352066	178877
8a11ef1eea846a9e97ba6f31061e7aaa57e51120	a polynomial-delay polynomial-space algorithm for extracting frequent diamond episodes from event sequences	frequent diamond episode;polynomial-delay polynomial-space algorithm;event sequences;input event sequence;frequent diamond episodes;mining parameter;artificial event sequence;polynomial-space algorithm polyfreqdmd;diamond episode;event sequence	In this paper, we study the problem of mining frequent diamond episodes efficiently from an input event sequence with sliding a window. Here, a diamond episode is of the form a → E → b, which means that every event of E follows an event a and is followed by an event b. Then, we design a polynomial-delay and polynomial-space algorithm PolyFreqDmd that finds all of the frequent diamond episodes without duplicates from an event sequence in O(|Σ|2n) time per an episode and in O(|Σ| + n) space, where Σ and n are an alphabet and the length the event sequence, respectively. Finally, we give experimental results on artificial event sequences with varying several mining parameters to evaluate the efficiency of the algorithm.	algorithm;information;pspace;polynomial;polynomial delay	Takashi Katoh;Hiroki Arimura;Kouichi Hirata	2009		10.1007/978-3-642-01307-2_18	real-time computing;pattern recognition;data mining	DB	12.582532503621588	29.118101941047474	179108
d833b59d9621778c34eed07a8e39c90fab0ac1e9	dynamic connectivity for axis-parallel rectangles	calcul matriciel;mise a jour;range query;producto matriz;geometrie algorithmique;interrogation base donnee;computational geometry;interrogacion base datos;dynamic connectivity;actualizacion;busquedas dentro de un rango;dynamic data structures;requete a intervalle;data structures;dynamic data structure;estructura datos;geometria computacional;structure donnee;matrix multiplication;matrix calculus;intersection graphs;produit matrice;data structure;database query;calculo de matrices;matrix product;updating	In this paper we give a fully dynamic data structure to maintain the connectivity of the intersection graph of n axis-parallel rectangles. The amortized update time (insertion and deletion of rectangles) is $O(n^{10/11}\mathop {\mathrm {polylog}}n)=O(n^{0.910})$ and the query time (deciding whether two given rectangles are connected) is O(1). It slightly improves the update time (O(n 0.94)) of the previous method while drastically reducing the query time (near O(n 1/3)). In addition, our method does not use fast matrix multiplication results and supports a wider range of queries.	amortized analysis;apache axis;clique cover;data structure;dynamic connectivity;dynamic data;matrix multiplication;polylogarithmic function;time complexity;web search query	Peyman Afshani;Timothy M. Chan	2008	Algorithmica	10.1007/s00453-008-9234-7	combinatorics;discrete mathematics;data structure;matrix multiplication;mathematics;algorithm	Theory	16.659268453624552	27.831289880370814	180678
507eb42440ea2af7772dd34235609336f49ff634	priority queues based on braun trees		This theory implements priority queues via Braun trees. Insertion and deletion take logarithmic time and preserve the balanced nature of Braun trees.	insertion sort;priority queue;time complexity	Tobias Nipkow	2014	Archive of Formal Proofs		priority queue;computer science;distributed computing	Theory	14.531119228193917	28.586806556309572	180997
07bce8d5be4c9b60351f6dcfdc2ef74fb1b8f7e2	regular expression searching on compressed text	search algorithm;compressed pattern matching;regular language;automata;pattern matching;regular languages;ziv lempel;bit parallelism;regular expression	We present a solution to the problem of regular expression searching on compressed te format we choose is the Ziv–Lempel family, specifically the LZ78 and LZW variants. Given a of lengthu compressed into length n, and a pattern of length m, we report all theR occurrences o the pattern in the text in O (2m+mn+Rm logm) worst case time. On average this drops to O (m2+ (n+Rm) logm) or O(m2+n+Ru/n) for most regular expressions. This is the first nontrivial re for this problem. The experimental results show that our compressed search algorithm needs time necessary for decompression plus searching, which is currently the only alternative.  2003 Elsevier B.V. All rights reserved.	best, worst and average case;data compression;lz77 and lz78;lempel–ziv–welch;regular expression;search algorithm	Gonzalo Navarro	2003	J. Discrete Algorithms	10.1016/S1570-8667(03)00036-4	discrete mathematics;regular language;computer science;theoretical computer science;mathematics;programming language;algorithm	AI	12.606384285420491	27.34031249109996	181150
03284a4b3051bd058613828e5b7a7b80d5acbeef	sparse compact directed acyclic word graphs	pattern matching;directed acyclic word graph;mathematics all;sequence analysis;sux tree;on line algorithm;text indexing;natural language processing	The suffix tree of string w represents all suffixes of w, and thus it supports full indexing of w for exact pattern matching. On the other hand, a sparse suffix tree of w represents only a subset of the suffixes of w, and therefore it supports sparse indexing of w. There has been a wide range of applications of sparse suffix trees, e.g., natural language processing and biological sequence analysis. Word suffix trees are a variant of sparse suffix trees that are defined for strings that contain a special word delimiter #. Namely, the word suffix tree of string w = w1w2 · · ·wk, consisting of k words each ending with #, represents only the k suffixes of w of the form wi · · ·wk. Recently, we presented an algorithm which builds word suffix trees in O(n) time with O(k) space, where n is the length of w. In addition, we proposed sparse directed acyclic word graphs (SDAWGs) and an on-line algorithm for constructing them, working in O(n) time and space. As a further achievement of this research direction, this paper introduces yet a new text indexing structure named sparse compact directed acyclic word graphs (SCDAWGs). We show that the size of SCDAWGs is smaller than that of word suffix trees and SDAWGs, and present an SCDAWG construction algorithm that works in O(n) time with O(k) space and in an on-line manner.	delimiter;deterministic acyclic finite state automaton;dictionary;directed acyclic graph;natural language processing;online algorithm;online and offline;pattern matching;sequence analysis;sparse matrix;spatial database;suffix array;suffix tree	Shunsuke Inenaga;Masayuki Takeda	2006			natural language processing;feedback arc set;computer science;machine learning;pattern matching;sequence analysis;pattern recognition;moral graph;programming language;directed acyclic word graph;tree;directed acyclic graph	Theory	13.411207267875584	27.2663433613668	181374
b0f1f5b76fc3c9265ebab27d4bc971b03a670e50	minimal indices for successor search - (extended abstract)		We give a new successor data structure which improves upon the index size of the P\v{a}tra\c{s}cu-Thorup data structures, reducing the index size from $O(n w^{4/5})$ bits to $O(n \log w)$ bits, with optimal probe complexity. Alternatively, our new data structure can be viewed as matching the space complexity of the (probe-suboptimal) $z$-fast trie of Belazzougui et al. Thus, we get the best of both approaches with respect to both probe count and index size. The penalty we pay is an extra $O(\log w)$ inter-register operations. Our data structure can also be used to solve the weak prefix search problem, the index size of $O(n \log w)$ bits is known to be optimal for any such data structure. #R##N#The technical contributions include highly efficient single word indices, with out-degree $w/\log w$ (compared to the $w^{1/5}$ out-degree of fusion tree based indices). To construct such high efficiency single word indices we device highly efficient bit selectors which, we believe, are of independent interest.		Sarel Cohen;Amos Fiat;Moshe Hershcovitch;Haim Kaplan	2013		10.1007/978-3-642-40313-2_26	combinatorics;theoretical computer science;mathematics;algorithm	Theory	12.284046378690515	26.06964565703615	182029
d8ba417c067513204e5628c169dd8b0ee2e39cee	i/o efficient algorithms for serial and parallel suffix tree construction	estensibilidad;genetique;tratamiento datos;sufijo;distributed system;algoritmo paralelo;core storage;sequencage;massively parallel systems;systeme reparti;computacion informatica;parallel algorithm;localite;genetica;efficient algorithm;suffix;disk based;data collection;memoria externa;database;sequence indexing;chaine caractere;base dato;data processing;bioinformatique;memoria central;locality;traitement donnee;stockage donnee;supercomputer;time series;memoire centrale;genetics;algorithme parallele;paralelismo masivo;suffix tree;supercomputador;sequencing;data storage;sistema repartido;indexing;ciencias basicas y experimentales;human genome;indexation;estructura datos;cadena caracter;genome;indizacion;base de donnees;parallel;presupuesto;almacenamiento datos;algorithms;structure donnee;budget;extensibilite;scalability;memoire externe;experimental evaluation;genoma;bioinformatica;external memory;suffixe;grupo a;experimentation;data structure;parallelisme massif;algorithm design;massive parallelism;genome indexing;superordinateur;character string;bioinformatics	Over the past three decades, the suffix tree has served as a fundamental data structure in string processing. However, its widespread applicability has been hindered due to the fact that suffix tree construction does not scale well with the size of the input string. With advances in data collection and storage technologies, large strings have become ubiquitous, especially across emerging applications involving text, time series, and biological sequence data. To benefit from these advances, it is imperative that we have a scalable suffix tree construction algorithm.  The past few years have seen the emergence of several disk-based suffix tree construction algorithms. However, construction times continue to be daunting—for example, indexing the entire human genome still takes over 30 hours on a system with 2 gigabytes of physical memory. In this article, we will empirically demonstrate and argue that all existing suffix tree construction algorithms have a severe limitation—to glean reasonable disk I/O efficiency, the input string being indexed must fit in main memory. This limitation is attributed to the poor locality exhibited by existing suffix tree construction algorithms and inhibits both sequential and parallel scalability. To deal with this limitation, we will show that through careful algorithm design, one of the simplest suffix tree construction algorithms can be rearchitected to build a suffix tree in a tiled manner, allowing the execution to operate within a fixed main memory budget when indexing strings of any size. We will also present a parallel extension of our algorithm that is designed for massively parallel systems like the IBM Blue Gene. An experimental evaluation will show that the proposed approach affords an improvement of several orders of magnitude in serial performance when indexing large strings. Furthermore, the proposed parallel extension is shown to be scalable—it is now possible to index the entire human genome on a 1024 processor IBM Blue Gene system in under 15 minutes.	algorithm design;blue gene;computer data storage;data structure;emergence;gigabyte;imperative programming;input/output;locality of reference;scalability;string (computer science);suffix tree;time series	Amol Ghoting;Konstantin Makarychev	2010	ACM Trans. Database Syst.	10.1145/1862919.1862922	algorithm design;search engine indexing;human genome;supercomputer;scalability;data structure;data processing;string;computer science;trie;theoretical computer science;time series;sequencing;computer data storage;parallel;database;parallel algorithm;compressed suffix array;fractal tree index;programming language;algorithm;genome;string searching algorithm;data collection	DB	10.627402782493435	28.56416700134882	182693
4c950a86e3c83e5e54f66acc76d364af268ab8b8	longest-match string searching for ziv-lempel compression	data compression;digital search tree;recherche;searching;programming theory;estructura datos;theorie programmation;structure donnee;technical report;compresion dato;data structure;compression donnee	Ziv-Lempel coding is currently one of the more practical data compression schemes. It operates by replacing a substring of a text with a pointer to its longest previous occurrence in the input, for each coding step. Decoding a compressed le is very fast, but encoding involves searching at each coding step to nd the longest match for the next few characters. This paper presents eight data structures that can be used to accelerate the searching, including adaptations of four methods normally used for exact match searching. The algorithms are evaluated analytically and empirically, indicating the trade-o s available between compression speed and memory consumption. Two of the algorithms are well-known methods of nding the longest match { the time-consuming linear search, and the storage-intensive trie (digital search tree.) The trie is adapted along the lines of a PATRICIA tree to operate economically. Hashing, binary search trees, splay trees, and the Boyer-Moore searching algorithm are traditionally used to search for exact matches, but we show how these can be adapted to nd longest matches. In addition, two data structures speci cally designed for the application are presented.	boyer–moore string search algorithm;data compression;data structure;lempel–ziv–welch;linear search;pointer (computer programming);radix tree;search tree;splay tree;string searching algorithm;substring;trie	Timothy C. Bell;David Kulp	1993	Softw., Pract. Exper.	10.1002/spe.4380230705	data compression;data structure;computer science;technical report;theoretical computer science;ternary search tree;programming language;algorithm	Theory	11.696834253988037	27.822239041543572	184045
5979bd9ac5f7de8a9d97a87f7ad7fc1027ab9b48	sets of k-independent strings	lower and upper bound;k independent string;kolmogorov complexity;algorithmic information theory	A bit string is random (in the sense of algorithmic information theory) if it is incompressible, i.e., its Kolmogorov complexity is close to its length. Two random strings are independent if knowing one of them does not simplify the description of other, i.e., the conditional complexity of each string (using the other as a condition) is close to its length. We may define independence of a k-tuple of strings in the same way. In this paper we address the following question: what is that maximal cardinality of a set of n-bit strings if any k elements of this set are independent (up to a certain constant)? Lower and upper bounds that match each other (with logarithmic precision) are provided. 1 Randomness and independence Algorithmic information theory identifies randomness with incompressibility. The Kolmogorov complexity of a random string x is defined as minimal length of a program that generates x, provided that the programming language is optimal in terms of program length. There are several versions of this notion (see, e.g., [1, 2] for the exact definition and basic results about Kolmogorov complexity), and we use the so-called plain complexity denoted by C(x). We also consider the conditional complexity of a string x when some other string y is given as a condition; it is defined as minimal length of a program that maps y to x, and is denoted by C(x|y). These notions are defined up to O(1) additive term. We may also speak about complexities of natural numbers and tuples of strings using some computable encoding, and also use these objects as conditions. For simplicity we omit parentheses and write, e.g., C(x,y) instead of C((x,y)) for complexity of the pair (x,y). A bit string x is considered as random if it is incompressible, i.e., its complexity C(x) is close to its length |x|. Since we deal with finite strings, there is no sharp dividing line between random and non-random strings. Instead, we define randomness deficiency of a n-bit string x as ∗The first three authors were supported in part by NSC grant 96-2213-E-002-024. A.S. was supported in part by NAFIT ANR-08-EMER-009-01 grant. †Dept. Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan. Email: b89053@csie.ntu.edu.tw. ‡Corresponding author. Dept. Computer Science Information and Engineering, National Taiwan University, No.1, Sec. 4, Roosevelt Road, Taipei, Taiwan 106. Email: lyuu@csie.ntu.edu.tw. TEL: 886-2-33664888. FAX: 886-2-23628167. §Dept. Computer Science Information and Engineering, National Taiwan University, Taipei, Taiwan. Email: d91010@csie.ntu.edu.tw. ¶Laboratoire Informatique Fondamentale, Marseille, France, CNRS & Univ. Aix–Marseille. On leave from IITP RAS, Moscow, Russia. Email: alexander.shen@lif.univ-mrs.fr.	aix;algorithmic information theory;angular defect;bit array;computable function;computer science;email;fax;information engineering;kolmogorov complexity;map;maximal set;national supercomputer centre in sweden;programming language;randomness;the european library;utility functions on indivisible goods	Yen-Wu Ti;Ching-Lueh Chang;Yuh-Dauh Lyuu;Alexander Shen	2010	Int. J. Found. Comput. Sci.	10.1142/S0129054110007271	kolmogorov structure function;combinatorics;discrete mathematics;string kernel;computer science;mathematics;algorithm;algorithmic information theory	Theory	11.055302723073511	25.584779021803683	184229
66747d1d53935003af947cf5129d701f8a0cea07	exact obdd bounds for some fundamental functions	lower bounds;computational complexity;ordered binary decision diagrams	Ordered binary decision diagrams (OBDDs) are nowadays the most common dynamic data structure or representation type for Boolean functions. Among the many areas of application are verification, model checking, computer aided design, relational algebra, and symbolic graph algorithms. Although many even exponential lower bounds on the OBDD size of Boolean functions are known, there are only few functions where the OBDD size is even asymptotically known exactly. In this paper the exact OBDD sizes of the fundamental functions multiplexer and addition of n-bit numbers are determined.	algorithm;binary decision diagram;computer-aided design;data structure;dynamic data;graph theory;model checking;multiplexer;relational algebra;time complexity	Beate Bollig;Niko Range;Ingo Wegener	2007		10.1007/978-3-540-77566-9_15	combinatorics;discrete mathematics;computer science;theoretical computer science;mathematics;computational complexity theory;algorithm	Logic	17.10058693280741	26.401074641005415	184799
d412180259f9f9b9e2e6d46eca854296ba82c21c	simple minimal perfect hashing in less space	hachage;random access memory;memoria acceso directo;longueur mot;aleatorizacion;dictionnaire;hashing;word length;hash table;memoire acces direct;estructura datos;longitud palabra;dictionaries;randomized algorithm;randomisation;structure donnee;hash function;model of computation;randomization;data structure;diccionario	A minimal perfect hash function for a set S is an injective mapping from S to {0, . . . |S|-1}. Taking as our model of computation a unit-cost RAM with a word length of w bits, we consider the problem of constructing minimal perfect hash functions with constant evaluation time for arbitrary subsets of U = {0, . . . 2w - 1}. Pagh recently described a simple randomized algorithm that, given a set S ⊆ U of size n, works in O(n) expected time and computes a minimal perfect hash function for S whose representation, besides a constant number of words, is a table of at most (2+Ɛ)n integers in the range {0,. . ., n-1}, for arbitrary fixed Ɛ > 0. Extending his method, we show how to replace the factor of 2 + Ɛ by 1 + Ɛ.	perfect hash function	Martin Dietzfelbinger;Torben Hagerup	2001		10.1007/3-540-44676-1_9	discrete mathematics;hash function;perfect hash function;dynamic perfect hashing;data structure;computer science;theoretical computer science;database;mathematics;distributed computing;programming language;algorithm	Vision	12.425409154975338	26.83719456537854	185028
9ebb43ad0a9c1fc0a1278fcaba04d987ff9b933d	exponential lower bounds on the size of obdds representing integer divistion	grafo aciclico;fonction booleenne;diagrama binaria decision;diagramme binaire decision;boolean function;graphe acyclique;acyclic graph;ordered binary decision diagram;combinatorial problem;probleme combinatoire;problema combinatorio;funcion booliana;informatique theorique;lower bound;computer theory;binary decision diagram;informatica teorica	An Ordered Binary Decision Diagram (OBDD) is a directed acyclic graph representing a Boolean function. The size of OBDDs largely depends on the variable ordering. In this paper, we show the size of the OBDD representing the i-th bit of the output of n-bit/n-bit integer division is (2 (n?i)=8) for any variable ordering. We also show that _-OBDDs, ^-OBDDs and-OBDDs representing integer division has the same lower bounds on the size. We develop new methods for proving lower bounds on the size of _-OBDDs, ^-OBDDs and-OBDDs.	binary decision diagram;directed acyclic graph;time complexity	Takashi Horiyama;Shuzo Yajima	1997		10.1007/3-540-63890-3_19	combinatorics;discrete mathematics;computer science;mathematics;boolean function;upper and lower bounds;binary decision diagram;directed acyclic graph;algorithm	Theory	17.093078813690568	26.48065152443922	185479
6acd9bff91561174666a125dd3bd19e688436156	dynamic fully-compressed suffix trees	data compression;information retrieval;suffix tree;data structure	Suffix trees are by far the most important data structure in stringology, with myriads of applications in fields like bioinformatics, data compression and information retrieval. Classical representations of suffix trees require O(n log n) bits of space, for a string of size n. This is considerably more than the n log 2 σ bits needed for the string itself, where σ is the alphabet size. The size of suffix trees has been a barrier to their wider adoption in practice. A recent so-called fully-compressed suffix tree (FCST) requires asymptotically only the space of the text entropy. FCSTs, however, have the disadvantage of being static, not supporting updates to the text. In this paper we show how to support dynamic FCSTs within the same optimal space of the static version and executing all the operations in polylogarithmic time. In particular, we are able to build the suffix tree within optimal space.	bioinformatics;computer data storage;data compression;data structure;information retrieval;polylogarithmic function;string (computer science);suffix tree;time complexity	Luís M. S. Russo;Gonzalo Navarro;Arlindo L. Oliveira	2008		10.1007/978-3-540-69068-9_19	data compression;generalized suffix tree;longest common substring problem;data structure;computer science;theoretical computer science;machine learning;mathematics;compressed suffix array;programming language;fm-index;algorithm	Theory	12.067577058364456	27.658558785305473	187042
70a6f7370dff1699c5be0d35afc1d4eee1275856	a fast sorting algorithm, a hybrid of distributive and merge sorting	sorting algorithm			M. van der Nat	1980	Inf. Process. Lett.	10.1016/0020-0190(80)90069-1	merge sort;adaptive sort;hybrid algorithm;computer science;external sorting;bitonic sorter;sorting algorithm;block sort;mathematics;merge algorithm;algorithm	Theory	12.60720319156941	31.779642564396518	187282
a316b9da49131dd5303f37e2ced3f33d612d93e3	an optimal algorithm for solving the towers of hanoi problem with the least storage used	data structure;mathematic;game;algorithm	The Towers of Hanoi problem is a classical problem in puzzles, games, mathematics, data structures, and algorithms. In this letter, a least memory used algorithm is proposed by combining the source array and target array for comparing the sizes of disk and labeling the disks in the towers of Hanoi problem. As a result, the proposed algorithm reduces the space needed from 2n+2 to n+5, where n represents the disks number. key words: Towers of Hanoi, data structure, algorithm, mathematic, game	algorithm;data structure;disk storage;tower of hanoi	Yu-Kumg Chen;Chen-An Fang;Fan-Chieh Cheng	2011	IEICE Transactions		games;game theory;data structure;computer science;calculus;mathematics;programming language;algorithm	Theory	16.012779469746313	25.646240210568262	187599
95567bc7be70904d1b19167d1baad2384f12b634	a parallel tree difference algorithm	arbre graphe;tree difference;dynamic programming;universal hashing;tree edit;structured text;execution time;tree graph;dynamic program;algorithme parallele;accumulations;tree contraction;programmation dynamique;temps execution;tiempo ejecucion;arbol grafo;parallel algorithms	Abstract   We describe a tree difference and edit sequence algorithm with expected sequential execution time O( n  log log  n ) and expected parallel execution time O(log  n ), for trees of size  n . The algorithm assumes unique labels and permits operations only on leaves and frontier subtrees. Despite this assumption, its application domain includes structured text and hypertext, and it is much faster than existing algorithms based on dynamic programming.	algorithm;longest common subsequence problem	David B. Skillicorn	1996	Inf. Process. Lett.	10.1016/S0020-0190(96)00171-8	combinatorics;computer science;theoretical computer science;dynamic programming;universal hashing;parallel algorithm;programming language;tree;algorithm	DB	14.725561851341395	27.969057737013525	188102
4cf5aa3fb893a8d4ae69393feb2d1d7e2cf45622	how evenly should one divide to conquer quickly?	algorithm analysis;complexite temps;recursivite;diviser et resoudre;merging sort;balancing;iteration;analyse algorithme;recursivity;tri fusion;equilibrage;problem solving;resolution probleme	It is generally believed that the time-cost of solving any size n problem using two-way divide-and-conquer is minimized by balancing-that is, by dividing the problem into subproblems of size [n/2] and [n/21 A counter-example is presented: balanced division, applied to finding the greatest and least elements of a size n set, will in the worst case force 11% more comparisons to be made than an optimal division such as into subsets of sizes 2 and n 2. A necessary condition and a slightly stronger sufficient condition are given for balancing to be cost-optimal. Even if balancing is cost-optimal, it may not be the only cost-optimal division strategy; a necessary and sufficient condition is given for a division strategy to be ‘balanced enough to be cost-optimal. As an application, a new iterative merge-sorting algorithm is presented which requires no more comparisons than the balanced one of Erkio and Peltola (1977) but merges subarrays consisting of consecutive elements of the whole array.	best, worst and average case;cost efficiency;iterative method;sorting algorithm	T. R. Walsh	1984	Inf. Process. Lett.	10.1016/0020-0190(84)90086-3	mathematical optimization;recursion;combinatorics;iteration;computer science;mathematics;programming language;algorithm	Theory	15.026592836152007	30.378820842613997	188356
3f1f2536b6cb4959e392201ed929719a969079ec	parallel comparison of run-length-encoded strings on a linear systolic array	dynamic programming algorithm;systolic array;longest common subsequence;run length encoding;algorithms;string comparison	The length of the longest common subsequence (LCS) between two strings of M and N characters can be computed by an O(M × N) dynamic programming algorithm, that can be executed in O(M + N) steps by a linear systolic array. It has been recently shown that the LCS between run-length-encoded (RLE) strings of m and n runs can be computed by an O(nM + Nm − nm) algorithm that could be executed in O(m + n) steps by a parallel hardware. However, the algorithm cannot be directly mapped on a linear systolic array because of its irregular structure.#R##N##R##N#In this paper, we propose a modified algorithm that exhibits a more regular structure at the cost of a marginal reduction of the efficiency of RLE. We outline the algorithm and we discuss its mapping on a linear systolic array.	run-length encoding;systolic array	Alessandro Bogliolo;Valerio Freschi	2007	Inf. Sci.	10.1016/j.ins.2006.07.024	parallel computing;systolic array;computer science;theoretical computer science;dynamic programming;longest common subsequence problem;mathematics;run-length encoding;algorithm	DB	12.552164870218531	28.626928858790873	188440
03f0fb7eac6ac7fdfe40335e66d84f2c687f04a7	a simple yet time-optimal and linear-space algorithm for shortest unique substring queries	regularity;unique substring;repetitiveness;shortest unique substring	We revisit the problem of finding shortest unique substring (SUS) proposed recently by Pei et al. (2013) 12. We propose an optimal O ( n ) time and space algorithm that can find an SUS for every location of a string of size n and thus significantly improve their O ( n 2 ) time complexity. Our method also supports finding all the SUSes covering every location, whereas theirs can find only one SUS for every location. Further, our solution is simpler and easier to implement and is more space efficient in practice, since we only use the inverse suffix array and the longest common prefix array of the string, while their algorithm uses the suffix tree of the string and other auxiliary data structures. Our theoretical results are validated by an empirical study with real-world data that shows our method is at least 8 times faster and uses at least 20 times less memory. The speedup gained by our method against Pei et al.u0027s can become even more significant when the string size increases due to their quadratic time complexity. We also have compared our method with the recent Tsuruta et al.u0027s (2014) 14 proposal, another independent O ( n ) time and space algorithm for SUS finding. The empirical study shows that both methods have nearly the same processing speed. However, ours uses at least 4 times less memory for finding one SUS and at least 2 times less memory for finding all SUSes, both covering every string location.	algorithm;substring;time complexity	Atalay Mert Ileri;M. Oguzhan Külekci;Bojian Xu	2015	Theor. Comput. Sci.	10.1016/j.tcs.2014.11.004	longest common substring problem;combinatorics;discrete mathematics;approximate string matching;substring;substring index;mathematics;longest repeated substring problem;algorithm;string searching algorithm	Theory	12.710213231426955	27.402106262631065	188689
051b5475d4b106807265dacddb085955b6d4060f	evaluation of xpath queries with preducates: an eulerian cycle theory based sequencing approach	null		eulerian path;xpath	Yun Shen;Ling Feng	2011	Comput. Syst. Sci. Eng.		computer science;theoretical computer science;database;xpath;eulerian path	DB	12.487903193820244	28.289359935744482	189255
43f232308c437b957a246741d3befa782950ac36	constant-space string-matching in sublinear average time	chaine caractere;algorithme;algorithm;cadena caracter;appariement chaine;string matching;character string;algoritmo	Given two strings: pattern P of length m and text T of length n. The string-matching problem is to nd all occurrences of the pattern P in the text T. We present a simple string-matching algorithms which works in average o(n) time with constant additional space for one-dimensional texts and two-dimensional arrays. This is the rst attempt to the small-space string-matching problem in which sublinear time algorithms are delivered. More precisely we show that all occurrences of one-or two-dimensional patterns can be found in O(n r) average time with constant memory, where r is the repetition size (size of the longest repeated subword) of P.	string searching algorithm;substring;time complexity	Maxime Crochemore;Leszek Gasieniec;Wojciech Rytter	1999	Theor. Comput. Sci.	10.1016/S0304-3975(98)00259-X	arithmetic;combinatorics;string;computer science;mathematics;algorithm;string searching algorithm	Theory	13.750188593543115	27.018291926694864	190094
e8a830d9fb3f1edb7a830e5817880195c06a9432	optimal parallel construction of heaps	parallelisme;algoritmo paralelo;parallel algorithm;algorithmique;algorithm analysis;abstract data type;algorithme parallele;analysis of algorithms;parallelism;paralelismo;algorithmics;algoritmica;heaps;type abstrait;analyse algorithme;tipo abstracto;analisis algoritmo;parallel algorithms	It is shown that @(n/p + log log n) time is sufficient for constructing an implicit heap with p processors in the parallel comparison tree model. This is optimal in terms of both time and processor-time product, given the parallel lower bound for selection and the sequential lower bound for heap construction. This result resolves an open problem on the existence of timeand cost-optimal parallel heap construction algorithms.	algorithm;central processing unit;cost efficiency	C. M. Khoong	1993	Inf. Process. Lett.	10.1016/0020-0190(93)90139-Z	binomial heap;fibonacci heap;combinatorics;discrete mathematics;heap;min-max heap;skew heap;double-ended priority queue;computer science;binary heap;d-ary heap;mathematics;parallel algorithm;algorithmics;algorithm	Theory	13.96525598490855	30.79230639997602	190368
b9e6ce6d23f7d34e09e119f1429164a75706a420	embedding a complete binary tree into the supercube	embedding;supercube;tree network;binary tree	Consider a complete binary tree with 2” I nodes and a supercube with the same number of nodes. We present a new embedding method to map the complete binary tree into the supercube with dilation I _ Our simple mapping method is quite competitive with the previous result. Key~~~-ds: Embedding; Tree network; Supercube	binary tree;dilation (morphology);tree network	Kuo-Liang Chung	1997	Journal of Systems Architecture	10.1016/S1383-7621(96)00075-6	random binary tree;optimal binary search tree;red–black tree;tree rotation;binary expression tree;exponential tree;binary tree;computer science;order statistic tree;self-balancing binary search tree;embedding;k-ary tree;interval tree;threaded binary tree;tree traversal	Theory	16.94681574706877	29.24228611973237	190652
707e7ff645e58cfbc21ab76072ec819aeb47eb14	compressed communication complexity of longest common prefixes		We consider the communication complexity of fundamental longest common prefix (Lcp) problems. In the simplest version, two parties, Alice and Bob, each hold a string, A and B, and we want to determine the length of their longest common prefix l = Lcp(A,B) using as few rounds and bits of communication as possible. We show that if the longest common prefix of A and B is compressible, then we can significantly reduce the number of rounds compared to the optimal uncompressed protocol, while achieving the same (or fewer) bits of communication. Namely, if the longest common prefix has an LZ77 parse of z phrases, only O(lg z) rounds and O(lg l) total communication is necessary. We extend the result to the natural case when Bob holds a set of strings B1, . . . , Bk, and the goal is to find the length of the maximal longest prefix shared by A and any of B1, . . . , Bk. Here, we give a protocol with O(log z) rounds and O(lg z lg k + lg l) total communication. We present our result in the public-coin model of computation but by a standard technique our results generalize to the private-coin model. Furthermore, if we view the input strings as integers the problems are the greater-than problem and the predecessor problem.	alice and bob;communication complexity;lcp array;lz77 and lz78;maximal set;model of computation;parsing	Philip Bille;Mikko Berggren Ettienne;Roberto Grossi;Inge Li Gørtz;Eva Rotenberg	2018		10.1007/978-3-030-00479-8_7	combinatorics;discrete mathematics;mathematics;lcp array;communication complexity;upper and lower bounds;integer;prefix	Theory	12.208686854678664	26.278065927801322	190900
4514c53fc8e225da3783b15d8c7a1424b754e873	toward optimal epsilon-approximate nearest neighbor algorithms	hachage;arbre recherche;approximate algorithm;approximation algorithms;information retrieval;approximation algorithm;nearest neighbor problem;point location;searching;algorithme;algorithm;vecino mas cercano;hashing;arbol investigacion;recherche information;data structures;estructura datos;nearest neighbor;algoritmo aproximacion;plus proche voisin;approximate nearest neighbor;nearest neighbour;structure donnee;recuperacion informacion;algorithme approximation;search tree;data structure;algoritmo	We combine the recent optimal predecessor algorithm with a recent randomized stratified tree algorithm for an ?-approximate nearest neighbor to give an algorithm for an ?-approximate nearest neighbor in a fixed-dimensional space that is optimal with respect to universe size. We also give a deterministic version of the stratified tree algorithm.	algorithm	Matthew Cary	2001	J. Algorithms	10.1006/jagm.2001.1193	nearest-neighbor chain algorithm;large margin nearest neighbor;r-tree;nearest neighbour algorithm;ball tree;nearest neighbor graph;hash function;best bin first;data structure;computer science;machine learning;point location;pattern recognition;cover tree;search tree;nearest neighbor search;programming language;k-nearest neighbors algorithm;approximation algorithm;algorithm	Theory	15.947496170681829	27.285700287508206	191770
472d32dca3495686d86bfd4a6e5429ec7937c31c	efficient evaluation of set expressions	set expression;data structures;comparison based algorithms;doctoral thesis;algorithms;lower bound	In this thesis, we study the problem of evaluating set expressions over sorted sets in the comparison model. The problem arises in the context of evaluating search queries in text database systems; most text search engines maintain an inverted list, which consists of a set of documents that contain each possible word. Thus, answering a query is reduced to computing the union, the intersection, or a more complex set expression over sets of documents containing the words in the query. At the first step, for a given expression on a number of sets and the sizes of the sets, we investigate the worst-case complexity of evaluating the expression in terms of the sizes of the sets. We prove lower bounds and provide algorithms with the matching running time up to a constant factor. We then refine the problem further and design an algorithm that computes such expressions according to the degree by which the input sets are interleaved rather than only considering sets sizes. We prove the optimality of our algorithm by way of presenting a matching lower bound sensitive to the interleaving measure. The algorithms we present are different in the set of set operators they allow in input expressions. We provide algorithms that are worst-case optimal for inputs with union, intersection, and symmetric difference operators. One of the algorithms we provide also supports minus and complement operators and is conjectured to be optimal when an input is allowed to contain these operators as well. We also provide a worst-case optimal algorithm for the form of problem where the input may contain “threshold” operators, which generalize union and intersection operators: for a number t, a t-threshold operator selects elements that appear in at least in t of the operand sets. Finally, the adaptive algorithm we provide supports union and intersection operators.	adaptive algorithm;database;forward error correction;inverted index;operand;time complexity;web search engine;web search query;worst-case complexity	Mehdi Mirzazadeh	2014			computer science;theoretical computer science;machine learning;algorithm	DB	12.728145835031814	25.579426552225794	192445
c9949554e66d85ed52b608424eccee7fd815c798	efficient 3-dimensional glv method for faster point multiplication on some gls elliptic curves	procesamiento informacion;algorithm analysis;elliptic curve;implementation;endomorfismo;courbe elliptique;point multiplication algorithm;68wxx;2 dimensional;journal;11g07;algorithme;algorithm;11t71;enrejado;14h52;curva eliptica;treillis;criptografia;informatique theorique;cryptography;information processing;lattice basis;cryptographie;endomorphism;analyse algorithme;3 dimensional;06bxx;multiplicacion;implementacion;traitement information;endomorphisme;multiplication;glv method;analisis algoritmo;computer theory;lattice;algoritmo;informatica teorica	We discover that two distinct efficient endomorphisms can both exist on some Galbraith-Lin-Scott (GLS) elliptic curves Galbraith et al. (2009) [4]. By using them we generalize the Gallant-Lambert-Vanstone (GLV) method Gallant et al. (2001) [5] for faster point multiplication on these curves to dimension 3, and give some implementation result which shows that our 3-dimensional GLV (3GLV) method runs in 0.897 the time of 2-dimensional GLV (2GLV) method as Galbraith et al. did in Galbraith et al. (2009) [4] for the point multiplication on these curves.	computable function;dimension 3;generalized least squares;grating light valve;lattice reduction;lenstra–lenstra–lovász lattice basis reduction algorithm;principle of good enough	Zhenghua Zhou;Zhi Hu;Maozhi Xu;Wangan Song	2010	Inf. Process. Lett.	10.1016/j.ipl.2010.08.014	discrete mathematics;information processing;cryptography;lattice;mathematics;elliptic curve;endomorphism;implementation;multiplication;algorithm	ML	14.683736760558151	31.40358620386741	192493
4fc75483790968245c80603a81043c3d0a0b3c29	binary search networks: a new method for key searching	insertion;delecion;optimisation;arbre recherche binaire;algoritmo busqueda;optimizacion;llave investigacion;algorithme recherche;search algorithm;circuit vlsi;reseau;red mallada cerrada;red;vlsi circuit;reseau maille;insercion;arbol investigacion binaria;binary search tree;estructura datos;search key;meshed network;binary search;optimization;structure donnee;circuito vlsi;cle recherche;data structure;network;deletion	A novel method for key searching, binary search networks, is proposed, and its search, insertion, and deletion algorithms are presented. A binary search network is an extension of a binary search tree which is widely used as a practical key search method.#R##N##R##N#Some properties of binary search networks are discussed, and the optimization problem of minimizing a search cost is remarked upon. The advantages and disadvantages of binary search networks relative to binary search trees are also discussed.	binary search algorithm	Toshitsugu Yuba;Mamoru Hoshi	1987	Inf. Process. Lett.	10.1016/0020-0190(87)90199-2	linear search;optimal binary search tree;interpolation search;beam search;insertion;binary search tree;data structure;geometry of binary search trees;computer science;artificial intelligence;theoretical computer science;self-balancing binary search tree;jump search;mathematics;incremental heuristic search;iterative deepening depth-first search;uniform binary search;best-first search;ternary search tree;programming language;algorithm;exponential search;binary search algorithm;dichotomic search;search algorithm	DB	16.496099391226878	27.052012578004334	192710
211573b230404a4c5b89e65af98d95fc1cf45ebe	finger search in grammar-compressed strings		Grammar-based compression, where one replaces a long string by a small context-free grammar that generates the string, is a simple and powerful paradigm that captures many popular compression schemes. Given a grammar, the random access problem is to compactly represent the grammar while supporting random access, that is, given a position in the original uncompressed string report the character at that position. In this paper we study the random access problem with the finger search property, that is, the time for a random access query should depend on the distance between a specified index f, called the finger, and the query index i. We consider both a static variant, where we first place a finger and subsequently access indices near the finger efficiently, and a dynamic variant where also moving the finger such that the time depends on the distance moved is supported. Let n be the size the grammar, and let N be the size of the string. For the static variant we give a linear space representation that supports placing the finger in O(log N) time and subsequently accessing in O(log D) time, where D is the distance between the finger and the accessed index. For the dynamic variant we give a linear space representation that supports placing the finger in O(log N) time and accessing and moving the finger in O(log D + log log N) time. Compared to the best linear space solution to random access, we improve a O(log N) query bound to O(log D) for the static variant and to O(log D + log log N) for the dynamic variant, while maintaining linear space. As an application of our results we obtain an improved solution to the longest common extension problem in grammar compressed strings. To obtain our results, we introduce several new techniques of independent interest, including a novel van Emde Boas style decomposition of grammars.	context-free grammar;context-free language;finger search;programming paradigm;random access	Philip Bille;Anders Roy Christiansen;Patrick Hagge Cording;Inge Li Gørtz	2017	Theory of Computing Systems	10.1007/s00224-017-9839-9	mathematical optimization;combinatorics;speech recognition;computer science;artificial intelligence;algorithm	Theory	13.235791591275522	26.777052185919285	193260
1bf3bad98ffedc59413c965a7b3e969eaaa7edbd	on external memory graph traversal	cylindricity;minimum cyclinder;metrology;zone cylinder;depth first search;external memory;external memory algorithms;breadth first search;data structure;roundness	We describe a new external memory data structure, the buffered repository tree, and use it to provide the first non-trivial external memory algorithm for directed breadth-first search (BFS) an d improved external algorithm for directed depth-first searc h. We also demonstrate the equivalence of various formulations of ext ernal undirected BFS, and we use these to give the first I/O-optimal BFS algorithm for undirected trees.	breadth-first search;data structure;depth-first search;ext js javascript framework;graph (discrete mathematics);graph traversal;input/output;out-of-core algorithm;tree traversal;turing completeness	Adam L. Buchsbaum;Michael H. Goldwasser;Suresh Venkatasubramanian;Jeffery Westbrook	2000			combinatorics;data structure;breadth-first search;computer science;theoretical computer science;mathematics;programming language;algorithm	Theory	16.37195150012662	28.986135238401946	195159
89bd3a58edce51756bbd27241d8c913504077035	external memory orthogonal range reporting with fast updates	external memory;data structure	In this paper we describe data structures for orthogonal range reporting in external memory that support fast update operations. The query costs either match the query costs of the best previously known data structures or differ by a small multiplicative factor.	coefficient;data structure;range searching	Yakov Nekrich	2011		10.1007/978-3-642-25591-5_41	auxiliary memory;data structure;computer science;theoretical computer science;data mining;database;mathematics;programming language	Theory	13.899919119185121	28.246448144481267	195975
5d49c8a04169d6585c0bdc2667561c614b39a84d	an approximation to the greedy algorithm for differential compression	compression algorithm;suffix array;linear time;greedy algorithm	"""We present a new differential compression algorithm that combines the hash value techniques and suffix array techniques of previous work. The term """"differential compression"""" refers to encoding a file (a version file) as a set of changes with respect to another file (a reference file). Previous differential compression algorithms can be shown empirically to run in linear time, but they have certain drawbacks; namely, they do not find the best matches for every offset of the version file. Our algorithm, hsadelta (hash suffix array delta), finds the best matches for every offset of the version file, with respect to a certain granularity and above a certain length threshold. The algorithm has two variations depending on how we choose the block size. We show that if the block size is kept fixed, the compression performance of the algorithm is similar to that of the greedy algorithm, without the associated expensive space and time requirements. If the block size is varied linearly with the reference file size, the algorithm can run in linear time and constant space. We also show empirically that the algorithm performs better than other state-of-the-art differential compression algorithms in terms of compression and is comparable in speed."""	approximation;data differencing;greedy algorithm	Ramesh C. Agarwal;Karan Gupta;Shaili Jain;Suchitra Amalapurapu	2006	IBM Journal of Research and Development	10.1147/rd.501.0149	data compression;time complexity;mathematical optimization;greedy algorithm;computer science;theoretical computer science;mathematics;algorithm	Theory	11.642432862340618	28.39353914901937	196894
5b0f0585a85a710ef644544993cd0dc9486ba3d7	prefix and suffix transreversals on binary and ternary strings	ternary strings;genome rearrangement;prefix and suffix transreversals;binary strings;transreversals	The problem of sorting by a genome rearrangement event asks for the minimum number of that event required to sort the elements of a given permutation. In this paper, we study a variant of the rearrangement event called prefix and suffix transreversal. A transreversal is an operation which reverses the first block before exchanging two adjacent blocks in a permutation. A prefix (suffix) transreversal always reverses and moves a prefix (suffix) of the permutation to another location. Interestingly, we will apply transreversal not on permutations but on strings over an alphabet of fixed size. We determine the minimum number of prefix and suffix transreversals required to sort the binary and ternary strings, with polynomial time algorithms for these sorting problems.	bitwise operation	Md. Khaledur Rahman;Mohammad Sohel Rahman	2015	J. Discrete Algorithms	10.1016/j.jda.2015.03.009	generalized suffix tree;combinatorics;discrete mathematics;mathematics;compressed suffix array;algorithm	Logic	14.093182627486097	27.043767485025363	197005
6d9a740b6ab98a4ef4ae420325f7f01fde326bdc	knapsack problems for nl	probleme sac a dos;repetition;clase complejidad;combinatorial problems;probleme nl complet;concatenacion;problema mochila;knapsack unaire;concatenation;knapsack problem;classe complexite;complexity class;computational complexity;donnee binaire;repeticion;dato binario;binary data		nl (complexity)	Birgit Jenner	1995	Inf. Process. Lett.	10.1016/0020-0190(95)00017-7	concatenation;complexity class;combinatorics;discrete mathematics;computer science;mathematics;computational complexity theory;knapsack problem;algorithm	DB	17.078302598670554	25.713252829373126	197226
9b7218d53ec23ac35977e7362b8e8bdce23d6451	algorithm engineering for fundamental sorting and graph problems		Fundamental Algorithms build a basis knowledge for every computer science undergraduate or a professional programmer. It is a set of basic techniques one can find in any (good) coursebook on algorithms and data structures. Thus, if one checks the contents of one such recent book by Mehlhorn and Sanders [105] he would find that we address most of the algorithmic problems covered in the book. In particular, we present current advances in such classical topics as sorting, graph traversals, shortest paths, minimum spanning trees, graph matchings and partitioning One can argue that all these problems are folklore with optimal worst-case behaviour. It is certainly true as long as the assumptions we use to derive the runtime bounds hold for a particular problem or available architecture at hand. In most of the coursebooks it is a Random Access Machine (RAM) model assuming that the input fits into the main memory of the computer and access time is unform throughout the size of the available memory. Unfortunately, it is often not the case for the real world architectures that posses a number of memory hierarchy layers including several levels of cache, main memory and an external hard drive. For instance, classical graph traversal algorithms become orders of magnitude slower as soon as data does not fit into the main memory and the operating system is forced to use a hard drive. Another problem for the classical coursebook algorithms is a rapidly growing parallelism. Currently it ranges from several cores in commodity home workstations (multicores) up to hundreds of cores available in Graphical Processing Units (GPUs). Classical approaches often become inefficient or even inapplicable in such settings. Even being on the safe side, that is in sequential RAM model, the worst-case optimality alone does not guarantee the best performance in practice. The reason for it is that the worst cases are rare in practice and thus the theoretically worse algorithms often perform better than their worst-case optimal counterparts. Clever heuristics often have further positive impact on the performance. In this thesis we try to close the gap between theoretically worst-case optimal classical algorithms and the real-world circumstances one face under the assumptions imposed by the data size, limited main memory or available parallelism.	access time;algorithm engineering;best, worst and average case;computer data storage;computer science;data structure;fits;file spanning;graph theory;graph traversal;graphics processing unit;hard disk drive;heuristic (computer science);memory hierarchy;minimum spanning tree;operating system;parallel computing;programmer;random access;random-access machine;random-access memory;shortest path problem;sorting;tree traversal;workstation	Vitaly Osipov	2014			combinatorics;discrete mathematics;graph (abstract data type);algorithm engineering;sorting;graph rewriting;graph;mathematics	Theory	11.045924109278367	30.622251400953637	197536
96216b981d71724defffcb2967a4389ba89211d9	parallel sorting with constant time for comparisons	parallel algorithm;parallel sorting;sorting;acyclic orientation;graph;transitive closure	We prove that there exist graphs with n vertices and at most $2n^{5/3} \log n$ edges for which every acyclic orientation has in its transitive closure at least $\begin{pmatrix} n \\ 2 \end{pmatrix} - 10n^{5/3} $ arcs. We conclude that with $2n^{5/3} \log n$ parallel processors n items may be sorted with all comparisons arranged in two time intervals. We also show that $\frac{1}{9}n^{3/2} $ processors are not sufficient to achieve the same end. These results are extended to parallel sorting in k time intervals, and related to other work on parallel sorting. The existence of sorting algorithms achieving the bounds is proved by nonconstructive methods. (The constants quoted in the abstract are somewhat improved in the paper.)	sorting	Roland Häggkvist;Pavol Hell	1981	SIAM J. Comput.	10.1137/0210034	combinatorics;discrete mathematics;computer science;sorting;mathematics;parallel algorithm;graph;transitive closure;algorithm	Theory	14.259547687057191	30.54146478965768	198246
40b95b5a265ccc24ec22900c7e3db5fd1b729a92	a faster and more accurate heuristic for cyclic edit distance computation	cyclic edit distance;chain code;q gram distance;cyclic strings;circular sequences	Sequence comparison is the core computation of many applications involving textual representations of data. Edit distance is the most widely used measure to quantify the similarity of two sequences. Edit distance can be defined as the minimal total cost of a sequence of edit operations to transform one sequence into the other; for a sequence x of length m and a sequence y of length n , it can be computed in time O(mn ) . In many applications, it is common to consider sequences with circular structure: for instance, the orientation of two images or the leftmost position of two linearised circular DNA sequences may be irrelevant. To this end, an algorithm to compute the cyclic edit distance in time O(mn log m ) was proposed (Maes, 2003 [18]) and several heuristics have been proposed to speed up this computation. Recently, a new algorithm based on q -grams was proposed for circular sequence comparison (Grossi et al., 2016 [13]). We extend this algorithm for cyclic edit distance computation and show that this new heuristic is faster and more accurate than the state of the art. The aim of this letter is to give visibility to this idea in the pattern recognition community. © 2017 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).	approximation algorithm;computation;edit distance;expectation propagation;experiment;fastest;heuristic (computer science);image retrieval;pattern recognition;relevance;sequence alignment	Lorraine A. K. Ayad;Carl Barton;Solon P. Pissis	2017	Pattern Recognition Letters	10.1016/j.patrec.2017.01.018	computer vision;damerau–levenshtein distance;combinatorics;edit distance;computer science;wagner–fischer algorithm;theoretical computer science;mathematics;chain code;string-to-string correction problem;jaro–winkler distance;algorithm	Vision	13.820707363284502	26.202424918061165	199084
4a2c525cf0e8a0eb4b0632c2a77bf38c2629141b	truly efficient parallel algorithms: c-optimal multisearch for an extension of the bsp model (extended abstract)	parallel algorithm;work in progress	In this paper we design and analyse parallel algorithms with the goal to get exact bounds on their speed-ups on real machines. For this purpose we define an extension of Valiant's BSP model, BSP*, that rewards blockwise communication, and uses Valiant's notion of c-optimality. Intuitively a c-optimal parallel algorithm for p processors achieves speed-up close to p/c. We consider the Multisearch problem: Assume a strip in 2D to be partitioned into m segments. Given n query points in the strip, the task is to locate, for each query, its segment. For m  n we present a deterministic BSP* algorithm that is 1-optimal, if n = (p log2 p). For m > n, we present a randomized BSP* algorithm that is (1 + )-optimal for arbitrary >0, m  2p and n=(p log2 p). Both results hold for a wide range of BSP* parameters where the range becomes larger with growing input sizes m and n. We further report on implementation work in progress. Previous parallel algorithms for Multisearch were far away from being c-optimal in our model and do not consider blockwise communication.	parallel algorithm	Armin Bäumker;Wolfgang Dittrich;Friedhelm Meyer auf der Heide	1995		10.1007/3-540-60313-1_131	parallel computing;computer science;theoretical computer science;work in process;distributed computing;parallel algorithm	Theory	11.176714994847279	31.301421296775054	199214
5addde4627f8f82cdc0d58546d687d2364ed84b6	data exchange: on the complexity of answering queries with inequalities	certain answers;procesamiento informacion;algorithm analysis;complexite calcul;data exchange;pregunta documental;complejidad computacion;computational complexity;complexite donnee;informatique theorique;information processing;query;analyse algorithme;query answering;traitement information;analisis algoritmo;requete;computer theory;algorithme echange;informatica teorica			Aleksander Madry	2005	Inf. Process. Lett.	10.1016/j.ipl.2005.03.001	data exchange;information processing;computer science;artificial intelligence;data mining;computational complexity theory;algorithm	DB	16.233708298514266	26.013688166978792	199320
