id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c1bff13380a0b79b26169d0c250bc4fc6d2ab5c0	$o(\log_2m)$ self-organizing map algorithm without learning of neighborhood vectors	unsupervised learning;escherichia coli;computational reduction;learning algorithm;algoritmo busqueda;computational learning;apprentissage calcul;bacterie;algorithme recherche;aprendizaje calculo;subdividing method;search algorithm;intelligence artificielle;algorithme apprentissage;neighborhood function;enterobacteriaceae;codon frequency;self organizing map som;self organising feature maps;computational complexity;autoorganizacion;artificial intelligence;self organization;binary search;self organized map;bacteria;inteligencia artificial;reseau neuronal;algoritmo aprendizaje;escherichia coli e coli;red neuronal;autoorganisation;neural network	In this letter, a new self-organizing map (SOM) algorithm with computational cost O(log2M) is proposed where M2 is the size of a feature map. The first SOM algorithm with O(M2 ) was originally proposed by Kohonen. The proposed algorithm is composed of the subdividing method and the binary search method. The proposed algorithm does not need the neighborhood functions so that it eliminates the computational cost in learning of neighborhood vectors and the labor of adjusting the parameters of neighborhood functions. The effectiveness of the proposed algorithm was examined by an analysis of codon frequencies of Escherichia coli (E. coli) K12 genes. These drastic computational reduction and accessible application that requires no adjusting of the neighborhood function will be able to contribute to many scientific areas	algorithmic efficiency;binary search algorithm;computation;escherichia coli;organizing (structure);self-organization;self-organizing map	Hiroki Kusumoto;Yoshiyasu Takefuji	2006	IEEE Transactions on Neural Networks	10.1109/TNN.2006.882370	unsupervised learning;self-organization;bacteria;computer science;artificial intelligence;machine learning;computational complexity theory;escherichia coli;artificial neural network;algorithm;binary search algorithm;search algorithm	ML	11.377784378632287	-29.700316021802042	158370
ab710eaeef3453a13eecdda0a8093317b82c56a9	learning rule for time delay in fuzzy cognitive maps	hebbian learning;fuzzy cognitive maps;fuzzy cognitive map	Time is considered as an important factor in modeling and operation of dynamic systems. However, few studies have considered time factor in modeling and inference of fuzzy cognitive maps (FCMs), besides, no studies have dealt with time delay in learning of FCMs. Therefore, we propose a learning rule for temporal FCMs involving postand pre-delay time by extending Oja’s learning rule. We show the effectiveness of the proposed rule through simulations which solve a time-delayed chemical plant control problem. key words: Oja’s learning rule, Hebbian learning, fuzzy cognitive maps, time delay	broadcast delay;dynamical system;erkki oja;fuzzy cognitive map;hebbian theory;learning rule;simulation	In Keun Lee;Soon Hak Kwon	2010	IEICE Transactions		fuzzy cognitive map;computer science;artificial intelligence;machine learning;data mining;oja's rule	ML	11.591897200552236	-28.17505430893094	158655
1dcaa94eb0d672717379d8e37c970697efcab69a	on-chip training of memristor based deep neural networks	stander symposium poster	This paper presents on-chip training circuits for memristor based deep neural networks utilizing unsupervised and supervised learning methods. Memristor crossbar circuits allow neural algorithms to be implemented very efficiently, but could be prone to device variations and faults. On chip training circuits would allow the training algorithm to account for device variability and faults in these circuits. We have utilized autoencoders for layer-wise pre-training of the deep networks and utilized the back-propagation algorithm for supervised fine tuning. Our design utilizes two memristors per synapse for higher precision of weights. We have demonstrated successful training of memristor based deep networks for the MNIST digit classification and the KDD intrusion detection datasets.	algorithm;artificial neural network;backpropagation;crossbar switch;data mining;deep learning;intrusion detection system;mnist database;memristor;software propagation;spatial variability;supervised learning;synapse	Raqibul Hasan;Tarek M. Taha;Chris Yakopcic	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966300	neuroscience;electrical engineering;artificial intelligence	ML	13.674180922947064	-27.15380349636073	158981
6dbb6f070c61c2b92cdb42ffe023eeb25a2262c8	training neural networks with additive noise in the desired signal	speaker identification;random noise adaptive filters fir filters iir filters backpropagation multilayer perceptrons;convergence;neural networks;supervised learning;random generation;multilayer perceptrons;additive noise;additive random noise;indexing terms;backpropagation;simulated annealing;random generator;random noise;adaptive filters;global optimization strategy;adaptive systems;neural net work;backpropagation algorithm;adaptive system;backpropagation algorithms;infinite impulse response;global optimization;intelligent networks;neural networks additive noise intelligent networks backpropagation algorithms adaptive filters supervised learning convergence simulated annealing adaptive systems iir filters;fir filters;noise variance global optimization strategy adaptive systems additive random noise random generator supervised learning;local minima;adaptive filter;training algorithm;iir filters;noise variance;neural network	A new global optimization strategy for training adaptive systems such as neural networks and adaptive filters [finite or infinite impulse response (FIR or IIR)] is proposed in this paper. Instead of adding random noise to the weights as proposed in the past, additive random noise is injected directly into the desired signal. Experimental results show that this procedure also speeds up greatly the backpropagation algorithm. The method is very easy to implement in practice, preserving the backpropagation algorithm and requiring a single random generator with a monotonically decreasing step size per output channel. Hence, this is an ideal strategy to speed up supervised learning, and avoid local minima entrapment when the noise variance is appropriately scheduled.	adaptive filter;adaptive system;additive white gaussian noise;algorithm;artificial neural network;backpropagation;entrapment of medical device or device component;finite impulse response;global optimization;infinite impulse response;mathematical optimization;maxima and minima;nerve compression syndrome;neural network simulation;noise (electronics);norm (social);random number generation;sample variance;schedule (document type);supervised learning;utility functions on indivisible goods;weight	Chuan Wang;José Carlos Príncipe	1999	IEEE transactions on neural networks	10.1109/72.809097	adaptive filter;mathematical optimization;computer science;backpropagation;adaptive system;machine learning;pattern recognition;artificial neural network	ML	16.62144472890799	-29.952643019715065	159150
4689881acb08eb591fcd086c7e87253ce523f1c4	estimation of oil saturation using neural network.	neural network		artificial neural network	Hong Li;Ali K. Setoodehnia;Kamal Shahrabi;Zahra Shahrabi	2005			probabilistic neural network	ML	11.730837642137875	-26.08149443723913	159158
4a18187bbec96890a5f6e7928bdd8329e5131778	deep feedback gmdh-type neural network using principal component-regression analysis and its application to medical image recognition of abdominal multi-organs	evolutionary computation;gmdh;deep neural networks;medical image recognition	The deep feedback Group Method of Data Handling (GMDH)-type neural network is proposed and applied to the medical image recognition of abdominal organs such as the liver and spleen. In this algorithm, the principal component-regression analysis is used for the learning calculation of the neural network, and the accurate and stable predicted values are obtained. The neural network architecture is automatically organized so as to fit the complexity of the medical images using the prediction error criterion defined as Akaike’s Information Criterion (AIC) or Prediction Sum of Squares (PSS). The recognition results show that the deep feedback GMDH-type neural network algorithm is useful for the medical image recognition of abdominal organs.		Tadashi Kondo;Junji Ueno;Shoichiro Takao	2015	JRNAL	10.2991/jrnal.2015.2.2.6	computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network	ML	12.931689544066996	-25.966237270068774	159355
84de82876aea9e2f0c85c1a59f4638a0ec99a13d	characteristics of small scale non-monotonic neuron networks having large potentiality for learning	probability;activation function;neurons numerical simulation information processing machine learning learning systems convergence biomembranes laboratories intelligent systems intelligent networks;probability nonmonotonic neuron networks deterministic boltzmann machine 2 parity problem 4 parity problem learning ability;nonlinear problem;boltzmann machine;learning artificial intelligence;neuronal network;probability boltzmann machines learning artificial intelligence;boltzmann machines;numerical simulation	We report a study on learning ability of a deterministic Boltzmann machine (DBM) with neurons which have a nonmonotonic activation function. We use an end-cut-off-type function with a threshold parameter '/spl theta/' as the nonmonotonic function. Numerical simulations of nonlinear problems, such as the 2-parity problem and the 4-parity problem, show that the DBM network with nonmonotonic neurons has higher learning ability compared to the network with monotonic neurons.	neuron	Mitsunaga Kinjo;Shigeo Sato;Koji Nakajima	2000		10.1109/IJCNN.2000.860768	computer simulation;boltzmann machine;biological neural network;computer science;artificial intelligence;theoretical computer science;machine learning;probability;activation function;restricted boltzmann machine;computational learning theory	ML	17.19009537790577	-26.641465590660243	159417
299f84eb26a05b4743e6af230de11d40169f44a1	initializing of an rbf network by a genetic algorithm	prototype selection;simulated annealing;radial basis functions rbf networks;radial basis function;pattern recognition;genetic algorithm;genetic algorithms;k nn condensing techniques;nonparametric classifiers;rbf network;prototypes selection	In this paper we use a genetic algorithm (GA) for selecting the initial seed points (prototypes, kernels) for a Radial Basis Function (RBF) classifier. The chromosome is directly mapped onto the training set and represents a subset: it contains 1 at the ith position if the ith element of the set is included, and 0, otherwise. Thus the GA serves a condensing technique that can hopefully lead to a small subset which still retains relevant classification information. We propose to use the set corresponding to the best chromosome from the final population as the seed points of the RBF network. Simulated annealing is used to tune the parameters of the radial function without changing kernels location. Experimental results with IRIS and two-spirals data sets are presented.	genetic algorithm;radial (radio);radial basis function network;simulated annealing;software release life cycle;test set	Ludmila I. Kuncheva	1997	Neurocomputing	10.1016/S0925-2312(96)00035-5	genetic algorithm;hierarchical rbf;computer science;artificial intelligence;machine learning;pattern recognition	ML	13.83206140253388	-25.300271560047467	159812
75a3d37e567467dc0db80a9251298e0eb9c03adc	predicting response times in the internet with radial basis functions	radial basis function		internet;radial (radio);radial basis function	Günther A. Hoffmann	1998			artificial intelligence;machine learning;the internet;radial basis function;computer science	Networks	11.72171147620188	-26.5224602130354	160225
1d4966fa04b26866038ad151e384de846343364f	realized through a marriage with modular-networks	mnsom;self organizing maps;electronic mail;spine;neural networks;modular networks;computational intelligence;multilayer perceptrons;adaptive control;modular network;data processing;adaptive control self organizing map som modular network mnsom;computational intelligence adaptive control intelligent agent neural networks multilayer perceptrons recurrent neural networks spine electronic mail biological neural networks data processing;self organising feature maps;modular network som;self organising feature maps adaptive control;self organizing map;intelligent agent;self organized map;som;recurrent neural networks;adaptive control modular networks self organizing maps modular network som neural networks;biological neural networks;neural network	This paper presents a new development of self-organizing maps (SOM), realized by combining them with the idea of a modular network. This we called a modular network SOM (mnSOM) in which each reference vector unit of a conventional SOM is replaced by a functional module. Since users can choose the functional module from any trainable architecture such as neural networks, the mnSOM is very flexible as well as having a high data processing ability. In this paper, we first introduce the basic idea and then describe its theory. Finally we introduce some applications of mnSOMs.	artificial neural network;modular programming;organizing (structure);self-organization;self-organizing map	Tetsuo Furukawa;Kazuhiro Tokunaga	2007	2007 IEEE Symposium on Foundations of Computational Intelligence	10.1109/FOCI.2007.371539	self-organizing map;computer science;artificial intelligence;machine learning;data mining;artificial neural network	Arch	11.34439346767741	-28.144110401178974	160434
f3cdf200ee395dbcf32c5c4170379cbf3a67a3f1	dynamic fuzzy neural networks-a novel approach to function approximation	erbium;fuzzy neural network;learning algorithm;fuzzy neural nets;neurons dynamic fuzzy neural networks function approximation takagi sugeno kang fuzzy systems extended radial basis function neural networks learning algorithm hierarchical on line self organizing learning;neural networks;takagi sugeno kang fuzzy systems;fuzzy logic;extended radial basis function neural networks;hierarchical on line self organizing learning;radial basis function;function approximation;self organising feature maps;rbf neural network;backpropagation algorithms;simulation study;takagi sugeno kang;self organization;neurons;learning artificial intelligence;fuzzy neural networks;modeling;self organising feature maps fuzzy neural nets fuzzy logic learning artificial intelligence;dynamic fuzzy neural networks;fuzzy systems;fuzzy neural networks neural networks function approximation fuzzy systems neurons recruitment modeling fuzzy logic backpropagation algorithms erbium;fuzzy system;recruitment	In this paper, an architecture of dynamic fuzzy neural networks (D-FNN) implementing Takagi-Sugeno-Kang (TSK) fuzzy systems based on extended radial basis function (RBF) neural networks is proposed. A novel learning algorithm based on D-FNN is also presented. The salient characteristics of the algorithm are: 1) hierarchical on-line self-organizing learning is used; 2) neurons can be recruited or deleted dynamically according to their significance to the system's performance; and 3) fast learning speed can be achieved. Simulation studies and comprehensive comparisons with some other learning algorithms demonstrate that a more compact structure with higher performance can be achieved by the proposed approach.		Shiqian Wu;Meng Joo Er	2000	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.836384	fuzzy logic;radial basis function;erbium;self-organization;systems modeling;function approximation;computer science;artificial intelligence;neuro-fuzzy;machine learning;artificial neural network;fuzzy control system	ML	11.392236579909678	-28.73531109184476	160778
1d926bc13daec8b20e1722c371d1bf28c6ba8fe9	analysis of a nature inspired firefly algorithm based back-propagation neural network training	international journal of computer applications ijca	Optimization algorithms are normally influenced by meta-heuristic approach. In recent years several hybrid methods for optimization are developed to find out a better solution. The proposed work using meta-heuristic Nature Inspired algorithm is applied with back-propagation method to train a feed-forward neural network. Firefly algorithm is a nature inspired meta-heuristic algorithm, and it is incorporated into back-propagation algorithm to achieve fast and improved convergence rate in training feed-forward neural network. The proposed technique is tested over some standard data set. It is found that proposed method produces an improved convergence within very few iteration. This performance is also analyzed and compared to genetic algorithm based back-propagation. It is observed that proposed method consumes less time to converge and providing improved convergence rate with minimum feed-forward neural network design	artificial neural network;backpropagation;converge;feedforward neural network;firefly algorithm;genetic algorithm;heuristic (computer science);iteration;mathematical optimization;network planning and design;rate of convergence;software propagation	Sudarshan Nandy;Partha Pratim Sarkar;Achintya Das	2012	CoRR	10.5120/6401-8339	mathematical optimization;computer science;artificial intelligence;machine learning;population-based incremental learning	ML	14.0712459980102	-23.987199745610916	161188
7c8ab847762fed40ed08ac518f5ac2eafae37645	a shift invariant pattern recognition mechanism and its implementation by associative memory	shift invariant pattern recognition;associative memory		content-addressable memory;pattern recognition	Jennie Si;Anthony N. Michel	1995	Neural Parallel & Scientific Comp.		content-addressable memory;mathematics;pattern recognition;invariant (mathematics);artificial intelligence	Vision	13.222626527104753	-29.532663315761614	161505
399aa9c32811422448160f6de3f836f9a721113b	polynomial pipelined neural network and its application to financial time series prediction	forecasting;methode diviser pour regner;prevision;processus non stationnaire;marche financier;pipelined network;polynomial neural network;financial time series prediction;datos financieros;metodo dividir para vencer;semantics;intelligence artificielle;donnee financiere;time series;semantica;semantique;financial data;higher order;divide and conquer method;non stationary condition;serie temporelle;financial time series;financial market;serie temporal;taux change;exchange rate;artificial intelligence;procesador oleoducto;condition non stationnaire;rapport signal bruit;relacion senal ruido;inteligencia artificial;non stationary process;condicion no estacionaria;reseau neuronal;signal to noise ratio;processeur pipeline;exchange rate time series;proceso no estacionario;divide and conquer;red neuronal;tasa cambio;pipeline processor;mercado financiero;neural network	A novel type of higher order pipelined neural network, the polynomial pipelined neural network, is presented. The network is constructed from a number of higher order neural networks concatenated with each other to predict highly nonlinear and nonstationary signals based on the engineering concept of divide and conquer. It is evaluated in financial time series application to predict the exchange rate between the US Dollar and 3 other currencies. The network demonstrates more accurate forecasting and an improvement in the signal to noise ratio over a number of benchmarked neural network.	artificial neural network;concatenation;genetic algorithm;list of amd fx microprocessors;network architecture;nonlinear system;polynomial;quad flat no-leads package;signal processing;signal-to-noise ratio;simulation;time series	Abir Jaafar Hussain;Adam Knowles;Paulo J. G. Lisboa;Wael El-Deredy;Dhiya Al-Jumeily	2006		10.1007/11941439_64	econometrics;probabilistic neural network;divide and conquer algorithms;higher-order logic;forecasting;computer science;artificial intelligence;time series;time delay neural network;semantics;signal-to-noise ratio;artificial neural network;financial market;algorithm;statistics	ML	11.726536430132464	-28.62254345388244	161663
0c333af935243d81c2b36945e97172c44befa867	fault detection and isolation using artificial neural networks	fault detection and isolation;artificial neural network		artificial neural network;fault detection and isolation	In Soo Lee;Gordon K. Lee	2006			artificial neural network;fault detection and isolation;distributed computing;computer science	Robotics	12.33932899542092	-26.788470548756216	162185
cbc826f4865a0bfe4a91b1720cca20e42b38c0ae	a design and implementation of reconfigurable architecture for neural networks based on systolic arrays	red sistolica;distributed system;field programmable gate array;arquitectura red;multiplier;learning algorithm;systeme reparti;reconfigurable architectures;circuit vlsi;systolic array;intelligence artificielle;algorithme apprentissage;red puerta programable;architecture reseau;reseau porte programmable;system on a chip;network analysis;chip;reconfigurable architecture;vlsi circuit;sistema repartido;multiplicateur;systolic network;sistema sobre pastilla;design and implementation;grande vitesse;transfer function;funcion traspaso;backpropagation algorithm;reseau systolique;algorithme retropropagation;artificial intelligence;fonction transfert;network architecture;gran velocidad;systeme sur puce;inteligencia artificial;circuito vlsi;reseau neuronal;analyse circuit;algoritmo aprendizaje;high speed;architecture reconfigurable;red neuronal;multiplicador;analisis circuito;neural network;algoritmo retropropagacion	This paper proposes a reconfigurable architecture for VLSI implementation of BP neural networks with on-chip learning. Basing on systolic arrays, this architecture can flexibly adapt to neural networks with different scales, transfer functions or learning algorithms by reconfiguration of basic processing components,. Three kinds of reconfigurable processing units (RPU) are proposed firstly basing on the analysis of neural network’s reconfiguration. Secondly, the paper proposes a reconfigurable systolic architecture and the method of mapping BP networks into this architecture. The implementation of an instance on FPGA is introduced in the last. The results show that this flexible architecture can also achieve a high learning speed of 432M CUPS (Connection Updated Per Second) at 100MHz using 22 multipliers.	algorithm;artificial neural network;cups;central processing unit;code;electronic design automation;field-programmable gate array;machine learning;semiconductor intellectual property core;systolic array;very-large-scale integration	Qin Wang;Ang Li;Zhancai Li;Yong Wan	2006		10.1007/11760191_193	chip;system on a chip;reference architecture;embedded system;network architecture;network analysis;systolic array;computer science;artificial intelligence;backpropagation;machine learning;transfer function;multiplier;artificial neural network;field-programmable gate array	Arch	15.25078652646223	-26.93875229560239	162334
81b4d9dbddb2896d272c0f46e3124860b0132786	evolving connectionist systems and evolving brains			connectionism;evolving classification function	Nikola K. Kasabov	1998			machine learning;artificial intelligence;connectionism;computer science	ML	11.87751770204133	-27.387920073764192	162781
4dda165c6cd24c236c1624e896a43ddf45acd490	learning algorithm of neural networks on spherical cap	interpolation;learning;spherical cap;neural network;bp algorithm	This paper investigates the learning algorithm of neural network on the spherical cap. Firstly, we construct the inner weights and biases from sample data, such that the network has the interpolation property on the sampling points. Secondly, we construct the BP network and BP learning algorithm. Finally, we analyze the generalization ability for the constructed networks and give the numerical experiments.	algorithm;artificial neural network;experiment;interpolation;numerical analysis;sampling (signal processing)	Zhixiang Chen;Jinjie Hu;Feilong Cao	2015	JNW	10.4304/jnw.10.3.152-158	mathematical optimization;wake-sleep algorithm;interpolation;computer science;artificial intelligence;machine learning;artificial neural network;generalization error	ML	15.005466431324102	-28.975959050356273	163153
09732a195779f2900e901cc7d5787a448283af91	automatic radar waveform recognition based on neural network			artificial neural network;radar;waveform	Wenbin Liu;Qing Wang;Qi Guo	2018	Mechatronic Systems and Control	10.2316/Journal.201.2018.2.201-2978	computer vision;artificial neural network;radar;waveform;computer science;artificial intelligence	Robotics	11.768198417800043	-26.17842093689708	163236
e4040e89266df88836fb3d7dfff8470cc7fac8f7	a parallel ga-bp neural network algorithm for solving inverse heat conduction problem	inverse heat conduction problem;training genetic algorithms clustering algorithms heuristic algorithms algorithm design and analysis computers biological cells;neural nets;heat conduction;backpropagation;physics computing;genetic algorithms;physics computing backpropagation genetic algorithms heat conduction inverse problems neural nets;speedup ihcp bp neural network ga parallel;data parallelism parallel ga bp neural network algorithm inverse heat conduction problem nonlinear mappings back propagation neural network genetic algorithm optimization algorithm;inverse problems;neural network	Inverse heat conduction problem (IHCP) is a nonlinear, ill-posed problem of huge calculation, which is difficult to be solved properly. This paper proposes a parallel GA-BP algorithm on cluster for solving IHCP, which combine the ability of nonlinear mappings of forward back propagation (BP) neural network and the capacity of global optimization of genetic algorithm (GA). The processes of optimization algorithm and data parallelism are introduced mainly in this paper. The experimental results show that the method can achieve high speedup and improve computational efficiency.	algorithm;artificial neural network;software release life cycle	Li Ma;Qingping Guo	2011		10.1109/EMEIT.2011.6024005	mathematical optimization;genetic algorithm;computer science;inverse problem;backpropagation;theoretical computer science;machine learning;thermal conduction;artificial neural network	ML	13.394171713504377	-24.31239079894232	163633
06453671c1ae7ce6d7a389e404a492986b53047e	complex-valued radial basic function network, part i: network architecture and learning algorithms	metodo cuadrado menor;non linear processing;traitement signal;methode moindre carre;unsupervised clustering;senal compleja;learning algorithm;complex function;least squares method;tratamiento no lineal;learning;complex valued signal;canal transmision;complex signal;traitement non lineaire;signal complexe;algorithme;aprendizaje;algorithm;apprentissage;radial basis function network;canal transmission;transmission channel;signal processing;funcion compleja;fonction complexe;procesamiento senal;orthogonal least squares algorithm;algoritmo	The paper proposes a complex radial basis function network. The network has complex centres and connection weights, but the nonlinearity of its hidden nodes remains a real-valued function. This kind of network is able to generate complicated nonlinear decision surfaces or to approximate an arbitrary nonlinear function in complex multi-dimensional space, and it provides a powerful tool for nonlinear signal processing involving complex signals. The paper is divided into two parts. The first part introduces the network architecture and derives both block-data and recursive learning algorithms for this complex radial basis function network. The complex orthogonal least squares algorithm is a batch learning algorithm capable of constructing an adequate network structure, while a complex version of the hybrid clustering and least squares algorithm offers real-time adaptation capability. The identification of a nonlinear communications channel model is used to illustrate these two learning algorithms. In the second part of the paper, a practical application of this complex radial basis function network is demonstrated using digital communications channel equalisation. Zusammenfassung. In dieser Arbeit wird ein komplexes 'Radial-Basis-Function'-Netzwerk vorgeschlagen. Das Netzwerk hat komplexe Zentren und Verbindungs-Gewichte, aber die Nichtlinearit~it seiner verborgenen Knoten bleibt eine reellwertige Funktion. Diese Art von Netzwerk ermrglicht die Erzeugung komplizierter nichtlinearer Entscheidungsfl~ichen oder die Approximation beliebiger nichtlinearer Funktionen im komplexen multidimensionalen Raum, and es stellt ein wirksames Hilfsmittel zur nichtlinearen Signalverarbeitung unter komplexen Signalen dar. Die Arbeit ist in zwei Teile gegliedert. Der erste Teil fiihrt in die Netzwerk-Architektur ein und leitet Blockdaten und rekursive Lernalgorithmen f~ir die komplexen Radial-Basis-Funkionen her. Der komplexe orthogonale Least Squares-Algorithmus stellt einen Batch-Lernalgorithmus dar, der die Konstrnktion einer ad~iquaten Netzwerk-Struktur ermrglicht, w~ihrend eine komplexe Version des Hybrid Clustering und Least-Squares Algorithmus die Realzeit-Adaption ermrglicht. Die Identifikation eines nichtlinearen 0bertragungskanal-Modells wird zur Veranschaulichung dieser beiden Lerualgorithmen benutzt. Im zweiten Teil der Arbeit wird eine praktische Anwendung des komplexen Radial-Basis-Function-Algorithmus zur t,)bertragungskanal-Entzerrung demonstriert. R~sum~. Nous proposons dans cet article un rrseau de fonctions radiales complexes. Ce rrseau poss~de des centres et des coefficients de pondrration complexes mais la non-linrarit6 dans les noeuds cach6s reste une fonction h valeurs rrelles. Ce type de rrseau est capable de grnrrer des surfaces de drcision non-linraire compliqures ou d'approximer une fonction dans une espace multi-dimensionnel complexe, et il constitue un outil puissant pour le traitement non-linraire de signaux complexes. Cet article est divis6 en deux parties. Dans la premiere nous introduisons l'architecture du rrseau et drrivons des algorithmes d'apprentissage en bloc et rrcursifs pour ce r6seau de fonctions radiales complexes. L'algorithme aux moindres carrrs orthogonaux complexe est une technique d'apprentissage en bloc capable de construire une structure de rrseau adrquate tandis qu'une version complexe de l'algorithme hybride de regroupement et de moindres carrrs permet une adaptation en temps rrel. L'identification d 'un canal de communication non-linraire est utilisre pour illustrer ces deux algorithmes d'apprentissage. Dans la seconde partie de l'article, nous montrons un exemple pratique d'application de ce rrseau de fonctions radiales complexes h l'6galisation d 'un canal de communication digital.	approximation algorithm;bibliothèque de l'école des chartes;channel (communications);cluster analysis;coefficient;council for educational technology;decision boundary;die (integrated circuit);eine and zwei;espace;least squares;linear algebra;machine learning;network architecture;nonlinear system;radial (radio);radial basis function network;real-time clock;recursion;signal processing	Sheng Chen;Stephen McLaughlin;Bernard Mulgrew	1994	Signal Processing	10.1016/0165-1684(94)90187-2	radial basis function;computer science;artificial intelligence;complex-valued function;machine learning;signal processing;mathematics;network simulation;least squares;radial basis function network;algorithm	ML	11.994951462549732	-29.338683816897184	164565
3ad9a63600fd85b6f4fb92e1fe746906dc84b9ae	on the learning machine with compensatory aggregation based neurons in quaternionic domain		Abstract The nonlinear spatial grouping process of synapses is one of the fascinating methodologies for neuro-computing researchers to achieve the computational power of a neuron. Generally, researchers use neuron models that are based on summation (linear), product (linear) or radial basis (nonlinear) aggregation for the processing of synapses, to construct multi-layered feed-forward neural networks, but all these neuron models and their corresponding neural networks have their advantages or disadvantages. The multi-layered network generally uses for accomplishing the global approximation of input–output mapping but sometimes getting stuck into local minima, while the nonlinear radial basis function (RBF) network is based on exponentially decaying that uses for local approximation to input–output mapping. Their advantages and disadvantages motivated to design two new artificial neuron models based on compensatory aggregation functions in the quaternionic domain. The net internal potentials of these neuron models are developed with the compositions of basic summation (linear) and radial basis (nonlinear) operations on quaternionic-valued input signals. The neuron models based on these aggregation functions ensure faster convergence, better training, and prediction accuracy. The learning and generalization capabilities of these neurons are verified through various three-dimensional transformations and time series predictions as benchmark problems.		Sushil Kumar;Bipin Kumar Tripathi	2019	J. Computational Design and Engineering	10.1016/j.jcde.2018.04.002	synapse;mathematical optimization;engineering;artificial neural network;exponential growth;nonlinear system;maxima and minima;neuron;convergence (routing);artificial neuron	ML	15.147468254179312	-27.78980288781729	164566
f072e3abee4c9189213c872687be664f0a1af1bc	spectral learning of restricted boltzmann machines				Aurelien Decelle;Giancarlo Fissore;Cyril Furtlehner	2017	CoRR		boltzmann machine;machine learning;artificial intelligence;computer science	NLP	13.228223340967945	-28.33843767905944	165402
ff16fb0a36e1fad31bf87693d72502faa3b0910e	adaptive brain emotional decayed learning for online prediction of geomagnetic activity indices	belbic;solar winds;online learning;long term forgetting;amygdala;adaptive bel	In this paper we propose adaptive brain-inspired emotional decayed learning to predict Kp, AE and Dst indices that characterize the chaotic activity of the earth's magnetosphere by their extreme lows and highs. In mammalian brain, the limbic system processes emotional stimulus and consists of two main components: Amygdala and Orbitofrontal Cortex (OFC). Here, we propose a learning algorithm for the neural basis computational model of Amygdala–OFC in a supervised manner and consider a decay rate in Amygdala learning rule. This added decay rate has in fact a neurobiological basis and yields to better learning and adaptive decision making as illustrated here. In the experimental studies, various comparisons are made between the proposed method named ADBEL, Multilayer Perceptron (MLP), Adaptive Neuro-Fuzzy Inference System (ANFIS) and Locally Linear Neuro-Fuzzy (LLNF). The main features of the presented predictor are the higher accuracy at all points especially at critical points, lower computational complexity and adaptive training. Hence, the presented model can be utilized in adaptive online prediction problems. & 2013 Elsevier B.V. All rights reserved.	adaptive neuro fuzzy inference system;algorithm;branch predictor;computation;computational complexity theory;computational model;learning rule;memory-level parallelism;multilayer perceptron;neuro-fuzzy;optical fiber communications conference and exposition (ofc);pixel	Ehsan Lotfi;Mohammad R. Akbarzadeh-Totonchi	2014	Neurocomputing	10.1016/j.neucom.2013.02.040	artificial intelligence;machine learning;solar wind;statistics	AI	14.888593596744515	-29.377232359100066	165574
2b12432846434fdc680b000a56d105014206a10d	multistability and multiperiodicity analysis of complex-valued neural networks	会议论文	Multistability and multiperiodicity of neual networks are usually considered in the application of associative memory. In this paper, we study the multistability and multiperiodicity of complex- valued neural networks (CVNNs for short) with one step piecewise linear activation functions. By separating the CVNN into its real and imagi- nary parts and using state decomposition, we can easily increase the storage capacity by using less neurons. Simulation results are given to illustrative the effectiveness of the theoretical results.	artificial neural network	Jin Hu;Jun Wang	2014		10.1007/978-3-319-12436-0_8	computer science;artificial intelligence;machine learning;control theory	ML	15.457609379498452	-27.103297315426484	166206
63e0e505346a362290d77f0c985920073e37734e	adaptative time constants improve the dynamic features of recurrent neural networks			artificial neural network;recurrent neural network	Jean-Philippe Draye;Davor Pavisic;Guy Cheron;Gaetan Libert	1996			machine learning;artificial intelligence;pattern recognition;computer science;time constant;recurrent neural network	Robotics	12.881596431676968	-27.24483126142124	167288
96fb1d7300ac355894d3272156894199487a9183	a node pruning algorithm based on optimal brain surgeon for feedforward neural networks	feedforward neural network;brain;systeme nerveux central;hombre;encefalo;sistema nervioso central;cerebro;encephale;cerveau;chirurgie;human;surgery;cirugia;encephalon;reseau neuronal;algoritmo optimo;algorithme optimal;optimal algorithm;red neuronal;central nervous system;training algorithm;homme;neural network	In this paper, a node pruning algorithm based on optimal brain surgeon is proposed for feedforward neural networks. First, the neural network is trained to an acceptable solution using the standard training algorithm. After the training process, the orthogonal factorization is applied to the output of the nodes in the same hidden layer to identify and prune the dependant nodes. Then, a unit-based optimal brain surgeon(UB-OBS) pruning algorithm is proposed to prune the insensitive hidden units to further reduce the size of the neural network, and no retraining is needed. Simulations are presented to demonstrate the effectiveness of the proposed approach.	algorithm;artificial neural network;computer simulation;feedforward neural network;optical burst switching	Jinhua Xu;Daniel W. C. Ho	2006		10.1007/11759966_78	feedforward neural network;computer science;artificial intelligence;central nervous system;machine learning;cerebro;artificial neural network	ML	10.674498694187415	-29.874150698863073	167854
4d0a8527bd8158f89175f7fd745653fde3e0c41f	financial predictions based on bootstrap-neural networks.	neural network	ES Netw r 0 A l 0 ug ra 2 NN Neu e l '2 l s i 000 icia pr Artif (A p on B 8 ro m e 2 ce iu l-edi pos g 6 ngs ym i 2-S u E an m , urope)	artificial neural network;cma-es	Anna Lombardi;Antonio Vicino	2000			computer science;machine learning;artificial neural network	ML	11.653907688860459	-26.574658221203723	168818
bdda5e1c61dddd3b790308f5f1f9423ac57b0763	fast self-growing and self-organizing feature map using automatic learning parameters	institut fur flugfuhrung;self organized feature map		machine learning;organizing (structure);self-organizing map	Karin Haese	1998			speech recognition;computer science;artificial intelligence;machine learning	NLP	12.344540983112998	-27.404648539364718	169238
69cc27709a626d1fc452eb8478befbf22da687bc	a neural network with o(n) neurons for ranking n numbers in o(1/n) time	computational complexity self organising feature maps comparators circuits recurrent neural nets analogue circuits sorting;sorting;analog circuits;neural networks neurons intelligent networks operational amplifiers sorting recurrent neural networks circuits convergence biology computing computer networks;recurrent neural networks neurons numbers set sorting networks nonlinear synapses comparators weighted interconnections self organizing feature maps analog circuits hardware implementation neural network hardware;comparators circuits;self organising feature maps;computational complexity;sorting network;self organized feature map;analogue circuits;recurrent neural nets;recurrent neural network;hardware implementation;neural network	In this paper, we propose a neural network for ranking a given set of N numbers in O(1/N) time. The ordering of a set of numbers based on their relative magnitudes, which is analogous to sorting, is a fundamental operation in many algorithms. In comparison with other sorting networks reported in the literature, the proposed network requires fewer neurons, and fewer interconnections between neurons. The interconnections use nonlinear synapses which are composed of comparators, and do not require any weighted interconnections between neurons, as used in conventional neural networks. The proposed network has many applications, including as a component of self-organizing feature maps and other systems where sorting is a frequent operation.	algorithm;artificial neural network;comparator;entity–relationship model;feedforward neural network;interconnection;local interconnect network;map;network convergence;neuron;nonlinear system;operational amplifier;organizing (structure);requirement;self-organization;sorting network;steady state;synapse;technological convergence;very-large-scale integration	Jayadeva;Sheikh Amanur Rahman	2004	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2004.835665	winner-take-all;stochastic neural network;types of artificial neural networks;analogue electronics;sorting network;computer science;sorting;artificial intelligence;recurrent neural network;theoretical computer science;machine learning;physical neural network;time delay neural network;computational complexity theory;artificial neural network	ML	15.876964090873681	-26.307059424021297	170864
963ca1a7337adb631487e3231b94830c333118d1	application of rbf neural networks based on a new hybrid optimization algorithm in flotation process	metodo cuadrado menor;recursive least square;methode recursive;metodo adaptativo;methode moindre carre;optimisation;analisis componente principal;analyse amas;mise a jour;least squares method;optimizacion;competitividad;fonction base radiale;metodo recursivo;recursive method;methode adaptative;prior knowledge;high precision;classification;preparacion serie fabricacion;actualizacion;cluster analysis;radial basis function;rbf neural network;principal component analysis;precision elevee;radial basis function neural network;adaptive method;inferencia;prediction accuracy;analyse composante principale;competitiveness;precision elevada;analisis cluster;rival penalized competitive learning;optimization;quality index;process planning;reseau neuronal;funcion radial base;optimal algorithm;competitivite;preparation gamme fabrication;clasificacion;red neuronal;inference;updating;neural network	An inferential estimation strategy of quality indexes of flotation process based on principal component analysis (PCA) and radial basis function neural network (RBFNN) is proposed. Firstly, the process prior knowledge and PCA method are used to simplify the networks’ input dimension and to choose the secondary variables. Then a new hybrid optimization algorithm of RBFNN is developed. The algorithm includes simplified rival penalized competitive learning method (SRPCL) to make an adaptive clustering of networks’ input pattern and recursive least squares method (LSM) with forgetting factor to update networks’ weights. The simulation results show that this inference estimation strategy has high predictive accuracy in flotation process.	algorithm;artificial neural network;radial basis function	Yong Zhang;Jie-Sheng Wang	2006		10.1007/11760191_157	radial basis function;biological classification;computer science;artificial intelligence;machine learning;mathematics;cluster analysis;least squares;artificial neural network;statistics;principal component analysis	ML	11.459306597545822	-30.497249167800142	171434
9c415baed8d4c137329cb1dc32592f4842559ff7	arbitrarily sized cell placement by self-organizing neural networks	sample vectors;solution procedure;arbitrarily sized cell placement;network topology;circuit layout cad;overlap penalty function;vlsi;kohonen's self-organization algorithm;total wire lengths;self-organizing neural networks;self-organising feature maps;iterations;cell growing-up algorithm;iterative methods;computer networks;simple object access protocol;self organization;neural network;cost function;penalty function;convergence;information science;neural networks	A new self-organizing neural network is described. It can solve arbitrarily sized cell placement problem with various constraints on their connection and dimension. The solution procedure modifies Kohonen's self-organization algorithm to adapt to the subclass of self-organization problems in which the sample vectors are not easily available, as well as in the case of the cell placement problem. For arbitrarily sized cell placement, the overlap penalty function and the cell growing-up algorithm are introduced to the authors' solution model where sizes of the cells are considered during the self-organization process in order to reduce overlaps among the cells. Their procedure is convergent in a reasonable number of iterations, and the resulting total wire lengths are at least the same as previous results	neural networks;organizing (structure)	Ray-I Chang;Pei-Yung Hsiao	1993			mathematical optimization;self-organization;convergence;computer science;theoretical computer science;machine learning;soap;penalty method;iterative method;very-large-scale integration;artificial neural network;network topology	ML	15.522140486890088	-25.594122179979504	171882
e412bae8b63a9c723894838abbf2a9856b17dc4b	the role of the rbf training in a neural model for object grasping	unsupervised learning;neural model;multilayer perceptrons;function block;ccd image sensors;hopfield neural network;radial basis function networks;neural system;learning speed object grasping neural model contact points gripper contact forces competitive hopfield neural network approximate polygon stable grasping training methods;grippers neural networks shape charge coupled devices charge coupled image sensors industrial training computational intelligence humans robot vision systems cameras;ccd image sensors radial basis function networks multilayer perceptrons unsupervised learning industrial manipulators;industrial manipulators;neural network	This paper presents a neural system to determine three contact points between a gripper and an object of arbitrary shape. The neural system is composed by three functional blocks to capture and process the image, establish the contact points and estimate the contact forces. The second block is formed by two neural networks. The first network (Competitive Hopfield Neural Nctwork) determines an approximate polygon for an object outline. A second network, a RBF or MLP model, defines three contact points. The results suggest that the neural system always reaches stable grasping for known and unknown objects. Moreover, the training methods used by the RBF model injluences signgcantly the performance and the learning velocity of the system.	approximation algorithm;artificial neural network;hopfield network;memory-level parallelism;radial basis function;robot end effector;velocity (software development)	C. M. O. Valente;A. Schammass;Aluizio F. R. Araújo;Glauco Augusto de Paula Caurin	1999		10.1109/IROS.1999.813042	unsupervised learning;stochastic neural network;computer vision;cellular neural network;probabilistic neural network;types of artificial neural networks;computer science;engineering;artificial intelligence;recurrent neural network;machine learning;time delay neural network;deep learning;artificial neural network	Robotics	14.269766736197306	-28.94216486619042	171965
158e0736881d8291e1113ef66ff45376accfd69f	artificial neural network methods for the prediction of framework crystal structures of zeolites from xrd data	x ray diffraction;small deviation;prediccion;information structure;metodo estadistico;calcul neuronal;neural computation;statistical data;decouverte;validacion cruzada;structure information;estimacion densidad;62m20;estimation densite;fonction base radiale;estructura informacion;62g07;62m45;statistical method;statistical regression;62jxx;zeolite;density estimation;radial basis function;general regression neural network;62h30;methode statistique;regresion estadistica;identification;estructura datos;discoveries;validation croisee;donnee statistique;identificacion;structure donnee;crystal structure;cross validation;dato estadistico;reseau neuronal;descubierta;regression statistique;funcion radial base;crystallography;data structure;prediction;red neuronal;computacion neuronal;reseau neuronal artificiel;artificial neural network;neural network	Extracting information about the structures of zeolites and other crystalline materials from X-ray diffraction (XRD) data simply by using statistical methods may provide an impetus for the discovery and identification of unknown materials. In this study, the possibility of using artificial neural network methods for relating framework crystal structures to XRD data reported in literature was investigated. Generalized Regression Neural Networks and Radial Basis Function-Based Neural Networks were utilized in the investigations. The results obtained by neural networks, using fivefold cross validation technique, were compared to the actual values as well as to those determined by multilinear regression. The predictions made by these neural network methods were, in general, more reliable than those performed by regression. The best predictions were achieved for the estimation of the framework densities of zeolites, which provided quite small deviations from the actual values.	artificial neural network;crystal structure;guilty gear xrd;nonlinear system;radial (radio);radial basis function	Melkon Tatlier	2010	Neural Computing and Applications	10.1007/s00521-010-0386-4	identification;radial basis function;density estimation;prediction;computer science;crystal structure;artificial intelligence;machine learning;artificial neural network;cross-validation;regression analysis;statistics;models of neural computation	ML	11.706762998374051	-30.287711404843026	172256
a770b996effb57942cf4ce1645e5d248d6a1ccd3	training neural networks for classification using growth probability-based evolution	self adaptive;neural networks;search space;evolution algorithms;growth probability;benchmark problem;low complexity;classification;growth rate;evolutionary algorithm;classification accuracy;gaussian distribution;neural network	In this paper, a novel evolutionary algorithm (EA) based on a newly formulated parameter, i.e., growth probability ðPgÞ is used to evolve the near optimal weights and the number of hidden neurons in neural networks (NNs). Training NNs with growth probability based evolution (NN-GP) initializes networks with only one hidden neuron and the networks are allowed to grow until a suitable size. Growing of neurons is not restricted to one hidden neuron at a time as the optimal number of hidden neurons for the NNs might be a few neurons more than what it represents now. If this solution in the search space is far, networks have to add several number of hidden neurons. Growth rate is based on Gaussian distribution thus providing a way to escape local optima. A self-adaptive version (NNSAGP) with the aim of evolving the growth probability in parallel with NNs during each generation is also proposed. The evolved networks are applied to widely used real-world benchmark problems. Simulation results show that the proposed approach is effective for evolving NNs with good classification accuracy and low complexity. r 2007 Elsevier B.V. All rights reserved.	artificial neural network;artificial neuron;benchmark (computing);evolutionary algorithm;local optimum;simulation	Ji Hua Ang;K. C. Tan;Abdullah Al Mamun	2008	Neurocomputing	10.1016/j.neucom.2007.10.011	normal distribution;winner-take-all;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;artificial neural network	ML	14.610500807506304	-24.727963851351106	172680
b0a2a3ddb883316029f1282ce6749b0b732f1ff9	coarse coding resource-allocating network	learning process;hipersuperficie;architecture systeme;champ recepteur;learning;resource allocation;reconstruction;campo receptor;sel buiding neural network;input output;aprendizaje;codificacion;apprentissage;efficient implementation;coding;arquitectura sistema;receptive field;asignacion recurso;reseau neuronal;allocation ressource;system architecture;red neuronal;hypersurface;codage;reconstruccion;neural network;learning paradigm for cc ran	"""In recent years localized receptive fields have been the subject of intensive research, due to their learning speed and efficient reconstruction of hypersurfaces. A very efficient implementation for such a network was proposed recently by Platt (1991). This resource-allocating network (RAN) allocates a new neuron whenever an unknown pattern is presented at its input layer. In this paper we introduce a new network architecture and learning paradigm. The aim of our approach is to incorporate """"coarse coding"""" to the resource-allocating network. The network presented here provides for each input coordinate a separate layer, which consists of one-dimensional, locally tuned gaussian neurons. In the following layer multidimensional receptive fields are built by using pi-neurons. Linear neurons aggregate the outputs of the pi-neurons in order to approximate the required input-output mapping. The learning process follows the ideas of the resource-allocating network of Platt but due to the extended architecture of our network other improvements of the learning process had to be defined. Compared to the resource-allocating network a more compact network with comparable accuracy is obtained."""	aggregate data;approximation algorithm;artificial neuron;network architecture;programming paradigm	Gustavo Deco;Jürgen Ebmeyer	1993	Neural Computation	10.1162/neco.1993.5.1.105	input/output;hypersurface;linear network coding;neuroscience;simulation;resource allocation;computer science;artificial intelligence;machine learning;mathematics;network simulation;coding;communication;receptive field;artificial neural network;systems architecture	ML	12.224002843416455	-29.467850036544952	172699
dd0c80548603eafbba9a9482c48212ee7d36f04d	implementation of hopfield associative memory with evolutionary algorithm and mc-adaptation rule for pattern storage		This paper describes the strategy for implementation of Hopfield neural network as associative memory with the genetic algorithm and the Monte Carlo-(MC-) adaptation rule for pattern storage. In the Hopfield type neural networks of associative memory, the appropriate arrangement of synaptic weights provides an associative memory feature in the network. The fixed-point stable state of this model represents the appropriate storage of the input patterns. The aim is to obtain the optimal weight matrix for efficient recalling of any prototype input pattern. The performance of the Hopfield neural network, especially the capacity and the quality of the recalling, can be greatly improved by making use of genetic algorithm and MC-adaptation rule. The experiments consider a neural network trained with multiple numbers of patterns using the Hebbian learning rule. In most cases, the recalling of patterns using genetic algorithm with MC-adaptation rule seems to give better results than the conventional hebbian rule, MC-adaptation rule and simple genetic algorithm recalling techniques.	content-addressable memory;evolutionary algorithm;hopfield network	Somesh Kumar;Rajkumar Goel;Manu Pratap Singh	2011		10.1007/978-81-322-0491-6_21	artificial intelligence;theoretical computer science;machine learning;bidirectional associative memory;hopfield network	Theory	14.859560697413585	-25.5486782971086	173103
4a3f96209bacd24f0461eb60a3730205ef1f695a	speed up method for neural network learning by using gpgpu		Neural network is a mathematical models for machine learning methods. This model apply to the many types of classification problem. And recently, many applications with neural network are required to process a big data in real time. In this paper, we discuss how to make the processing time of the neural network learning faster by using GPU. GPGPU is a technique by which GPUs are used for a general computation approach. GPU is a dedicated circuit to draw the graphics, so it has a characteristic in which many simple arithmetic circuits are implemented. This characteristic is applied to not only graphic processing but also general purpose like this proposed method. In order to employ it effectively, the calculation of the neural network learning are implemented to process simultaneously. The calculations which the neurons in the layer and many patterns are processed is parallelized. And we propose the parallelize method for calculation of back propagation. As the result, the proposed method is 25 times faster than the non-parallelized.	algorithm;artificial neural network;backpropagation;big data;computation;general-purpose computing on graphics processing units;graphics processing unit;integrated circuit;leased line;machine learning;mathematical model;parallel computing;simulation;software propagation	Yuta Tsuchida;Michifumi Yoshioka	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505093	parallel computing;computer science;theoretical computer science;machine learning;time delay neural network;deep learning;artificial neural network	Robotics	13.962241266469787	-26.555367589586112	173504
f8862c5d6e99fdebea0a482c2ffa6f40433890d0	dynamic optimal training for competitive neural networks	unsupervised learning;competitive neural networks;image compression;clustering;pattern classification	This paper introduces an unsupervised learning algorithm for optimal training of competitive neural networks. The learning rule of this algorithm is derived from the minimization of a new objective criterion using the gradient descent technique. Its learning rate and competition difficulty are dynamically adjusted throughout iterations. Numerical results that illustrate the performance of this algorithm in unsupervised pattern classification and image compression are also presented, discussed, and compared to those provided by other well-known algorithms for several examples of real test data.	algorithm;application domain;artificial neural network;gradient descent;heuristic;image compression;iteration;learning rule;loss function;neural network software;numerical analysis;numerical method;optimization problem;robustness (computer science);test data;unsupervised learning	Mohammed Madiafi;Abdelaziz Bouroumi	2014	Computing and Informatics		unsupervised learning;self-organizing map;wake-sleep algorithm;image compression;computer science;artificial intelligence;machine learning;pattern recognition;deep learning;cluster analysis;competitive learning;artificial neural network	ML	14.515472394514589	-29.428781411513658	173580
e46cfbdb55f40a1801eb6a1233e5220615a1429a	inductive bias strength in knowledge-based neural networks: application to magnetic resonance spectroscopy of breast tissues	breast tissue;neural network application;magnetic resonance spectroscopy;prior knowledge;inductive bias;31;real world application;training and generalization performance;learning with hints;network architecture;31p magnetic resonance spectroscopy;expert knowledge;knowledge based neural networks;artificial neural network;neural network;knowledge base	The integration of symbolic knowledge with artificial neural networks is becoming an increasingly popular paradigm for solving real-world applications. The paradigm provides means for using prior knowledge to determine the network architecture, to program a subset of weights to induce a learning bias which guide network training, and to extract knowledge from trained networks. The role of neural networks then becomes that of knowledge refinement. It thus provides a methodology for dealing with uncertainty in the prior knowledge. We address the open question of how to determine the strength of the inductive bias of programmed weights; we present a quantitative solution which takes the network architecture, the prior knowledge, and the training data into consideration. We apply our solution to the difficult problem of analyzing breast tissue from magnetic resonance spectroscopy (MRS); the available database is extremely limited and cannot be adequately explained by expert knowledge alone.	artificial neural network;body tissue;inductive bias;mammary gland parenchyma;minimal recursion semantics;network architecture;programming paradigm;refinement (computing);resonance;subgroup;weight	Christian W. Omlin;Sean Snyders	2003	Artificial intelligence in medicine	10.1016/S0933-3657(03)00062-9	network architecture;inductive bias;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	12.677186834982148	-29.27175233358467	173810
597c14b1df7cd263e2c1f085027a0b6779623e8f	classical conditioning as an example of natural models useful in machine learning	machine learning;classical conditioning		machine learning	William W. McMillan	1987			classical conditioning;algorithmic learning theory;computer science;machine learning	ML	12.49205242891187	-28.488585419681193	173855
dc6d3deae0c64791478a201576d186face4be7f8	sine neural network (snn) with double-stage weights and structure determination (ds-wasd)	linear independence;function approximation;double stage weights and structure determination ds wasd;sine neural network snn	To solve complex problems such as multi-input function approximationbyusingneural networks and toovercome the inherent defects of traditional back-propagation neural networks, a single hidden-layer feed-forward sineactivated neural network, sine neural network (SNN), is proposed and investigated in this paper. Then, a double-stage weights and structure determination (DS-WASD) method, which is based on the weights direct determination method and the approximation theory of using linearly independent functions, is developed to train the proposed SNN. Such a DS-WASD method can efficiently and automatically obtain the relatively optimal SNN structure. Numerical results illustrate the validity and efficacy of the SNN model and the DSWASD method. That is, the proposed SNN model equipped with theDS-WASDmethodhas great performanceof approximation on multi-input function data.	approximation theory;artificial neural network;backpropagation;software propagation;spiking neural network	Yunong Zhang;Lu Qu;Jinrong Liu;Dongsheng Guo;Mingming Li	2016	Soft Comput.	10.1007/s00500-014-1491-6	linear independence;function approximation;computer science;artificial intelligence;machine learning;mathematics;algorithm	ML	16.081738164493363	-28.348800130590732	173895
6b5b024f5acb06a2c96ab5a5c91fade11b95383a	an approach for on-line extraction of fuzzy rules using a self-organising fuzzy neural network	fuzzy neural network;learning algorithm;fuzzy set;procesamiento informacion;algorithm performance;learning;fuzzy rules;algoritmo recursivo;conjunto difuso;ensemble flou;algorithme apprentissage;aprendizaje;apprentissage;algorithme recursif;resultado algoritmo;information processing;performance algorithme;autoorganizacion;self organization;sistema difuso;recursive algorithm;systeme flou;reseau neuronal;traitement information;algoritmo aprendizaje;red neuronal;autoorganisation;fuzzy system;neural network	This paper presents a hybrid neural network, called the self-organising fuzzy neural network (SOFNN), to extract fuzzy rules from the training data. The first hidden layer of this network consists of ellipsoidal basis function (EBF) neurons. Every EBF neuron in the SOFNN has both a centre vector and a width vector. Neurons are organised by the network itself. The methods of the structure and parameter learning, based on new adding and pruning techniques and a recursive learning algorithm, are simple and effective, with a high accuracy and a compact structure. Simulations show that the SOFNN has the capability to encode fuzzy rules in the resulting network.	artificial neural network;neuro-fuzzy;online and offline;self-organization	Gang Leng;T. Martin McGinnity;Girijesh Prasad	2005	Fuzzy Sets and Systems	10.1016/j.fss.2004.03.001	self-organization;information processing;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;mathematics;fuzzy set;artificial neural network;algorithm;recursion	NLP	11.196404045159943	-29.785030602438056	174149
b2e4a95a5c4616c61aba08e38f7206803eb3bd5c	sequential extreme learning machine incorporating survival error potential	sequential learning machine;extreme learning machine;information measure	A sequential extreme learning machine incorporating a noise compensation scheme via an information measure is developed. In this design, the computationally simple extreme learning machine architecture is maintained while survival error information potential function provides a mechanism for noise compensation. The error compensation is updated online via an error codebook design where an error tolerant and stable solution is obtained. The developed method is tested on chaotic time sequence as well as benchmark data sets. Experimental results show potential applications for the developed method.	benchmark (computing);codebook;error-tolerant design;time series	Lei Sun;Badong Chen;Kar-Ann Toh;Zhiping Lin	2015	Neurocomputing	10.1016/j.neucom.2014.12.029	computer science;online machine learning;machine learning;pattern recognition;data mining;computational learning theory;active learning;generalization error	ML	15.264966980181708	-29.87697888897739	174344
6257a557aa3d06a4c14943ea64f1dae88680f4e2	an optimized rbf neural network algorithm based on partial least squares and genetic algorithm for classification of small sample	small sample classification;partial least squares;rbf neural network;genetic algorithm;pls ga rbf algorithm	Display OmittedThis paper's Graphical abstractWhen using the RBF neural network to deal with small samples with high feature dimension and few numbers, too many inputs are difficult to determine the numbers of hidden layer neurons, it influences the design structure of the network, the redundancies or correlative data will influence the training of the network, and relatively few number of samples make network train non-completed or over-fitted, thereby affecting the operating efficiency and recognition accuracy of neural network.For the problem of small sample classification, two aspects of RBF neural network are optimized. Firstly, the original data reduces their feature dimension by PLS algorithm, then the low dimensional data is used as network input, it regard as external optimization. And then, using genetic algorithm to optimize RBF, the optimization way adopts hybrid coding and simultaneous evolving for hidden layer neurons and connection weights, this step regard as internal optimization. By these two consecutive optimizations, an optimized RBF neural network algorithm based on PLS and GA (PLS-GA-RBF algorithm) for small sample is established, which facilitates the hidden layer of network design, and improves the network training speed and generalization ability, thereby improving the operating efficiency and recognition accuracy of the network.The new algorithm is ingenious combination of the advantages of three algorithms, it realize the external optimization by PLS and internal optimization by GA. PLS-GA-RBF algorithm can fit more complex nonlinear recognition problems, and is more suitable for the small sample classification, which with high feature dimension and fewer numbers.In order to verify the reliability of the PLS-GA-RBF algorithm, multiple instances is used to validate and analysis. In this paper, four different experiments are arranged; among them are three small sample test and one large sample test. The purpose of the arrangement large sample test is to compare of validation. The result is satisfactory, which means the new algorithm has unique superiority in dealing with the small sample. The nature of small sample is well-analyzed.PLS is employed to reduce feature dimension of small sample, which obtained the relatively ideal low-dimensional data as the inputs of neural network.Unlike previous studies, the optimized GA-RBF algorithm is adopts the way of hybrid coding and simultaneous evolving for hidden layer neurons and connection weights.By two consecutive optimization, combining the advantages of three algorithms of PLS, GA, and RBF, a reliable small sample classification algorithm (PLS-GA-RBF) is established.Four different groups of experiments are arranged to valuate the classification ability of PLS-GA-RBF algorithm. Radial basis function (RBF) neural network can use linear learning algorithm to complete the work formerly handled by nonlinear learning algorithm, and maintain the high precision of the nonlinear algorithm. However, the results of RBF would be slightly unsatisfactory when dealing with small sample which has higher feature dimension and fewer numbers. Higher feature dimension will influence the design of neural network, and fewer numbers of samples will cause network training incomplete or over-fitted, both of which restrict the recognition precision of the neural network. RBF neural network has some drawbacks, for example, it is hard to determine the numbers, center and width of the hidden layer's neurons, which constrain the success of training. To solve the above problems, partial least squares (PLS) and genetic algorithm(GA)are introduced into RBF neural network, and better recognition precision will be obtained, because PLS is good at dealing with the small sample data, it can reduce feature dimension and make low-dimensional data more interpretative. In addition, GA can optimize the network architecture, the weights between hidden layer and output layer of the RBF neural network can ease non-complete network training, the way of hybrid coding and simultaneous evolving is adopted, and then an accurate algorithm is established. By these two consecutive optimizations, the RBF neural network classification algorithm based on PLS and GA (PLS-GA-RBF) is proposed, in order to solve some recognition problems caused by small sample. Four experiments and comparisons with other four algorithms are carried out to verify the superiority of the proposed algorithm, and the results indicate a good picture of the PLS-GA-RBF algorithm, the operating efficiency and recognition accuracy are improved substantially. The new small sample classification algorithm is worthy of further promotion.		Weikuan Jia;Dean Zhao;Ling Ding	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.07.037	mathematical optimization;probabilistic neural network;genetic algorithm;computer science;artificial intelligence;machine learning;brooks–iyengar algorithm;pattern recognition;time delay neural network;partial least squares regression	ML	14.978163947221557	-28.03373915974229	174414
429ed3d2a498db6582d1e80e70f87359249ea08c	kernel machines, neural networks, and graphical models	graphical model;neural network;kernel machine		graphical model;kernel method;neural networks;turing machine	Paolo Frasconi;Alessandro Sperduti;Antonina Starita	2006	Intelligenza Artificiale		polynomial kernel;computer science;kernel method;types of artificial neural networks;deep learning;tree kernel;graph kernel;artificial intelligence;machine learning;graphical model;radial basis function kernel	ML	13.160176376429442	-28.33193734468015	174576
edf717b8852d3af690eba84db2b3e5faea8a16a3	training neural nets through stochastic minimization	minimisation;minimization;learning algorithms;metodo estadistico;optimisation;optimizacion;learning;performance;statistical method;minimizacion;backpropagation;algorithme;aprendizaje;algorithm;retropropagation;apprentissage;stochastic optimization;neural net;methode statistique;optimization;rendimiento;reseau neuronal;retropropagacion;multilayer neural network;back propagation;red neuronal;stochastic search;neural network;algoritmo	-The revival of multilayer neural networks in the mid 1980s originated from the discovery of the back propagation technique as a feasible training procedure. In spite of its shortcomings, it is probably the most widespread technique for training feedforward nets. In recent years, several deterministic methods more efficient than back propagation have been proposed. In this paper a stochastic minimization algorithm, the iterated adaptive memory stochastic search, is described that does not use gradient information and is found to perform better than back propagation on the encoder and parity problems. Keywords--Stochastic optimization, Learning algorithms, Back propagation.	algorithm;artificial neural network;backpropagation;encoder;feed forward (control);feedforward neural network;gradient descent;iteration;mathematical optimization;software propagation;stochastic optimization	Roberto Brunelli	1994	Neural Networks	10.1016/0893-6080(94)90088-4	computer science;artificial intelligence;backpropagation;machine learning;artificial neural network;algorithm	ML	17.30905379596374	-27.79910755479537	176026
4b2f7c4c1980159a81550dc0bffff3a6c8f41926	implementation of a digital modular chip for a reconfigurable artificial neural network	building block;chip;arsenic;artificial neural network	Artificial Neural Networks (ANNs) are fast becoming an integral part of today's computing arsenal [4]. This paper discusses the issues involved in the design of a General Purpose Reconfigurable Artificial Neural Network (GPRNN). The IBM fabricated Basic Neural Unit (BNU) is used as a building block for the GPRNN. As a first step, a fully reconfigurable 2-BNU VLSI circuit is designed, implemented and tested.	artificial neural network	Simin H. Pakzad;Paul Plaskonos	1993		10.1007/3-540-56891-3_63	embedded system;electronic engineering;engineering;machine learning	NLP	13.910420551126604	-26.29691866681886	177468
3b70b92e41d52376e861eb7022cd6e3d443e506a	a fast modified constructive-covering algorithm for binary multi-layer neural networks	multi layer neural networks;learning process;binary neural networks;geometrical learning;constructive learning;neural network;linearly separablility	For binary neural networks (BNNs), constructive covering frameworks have been investigated recently. While these frameworks are fast, they have limitations of generalization and accurate classification for learning from limited number of samples. In this paper, we propose modified constructive-covering algorithm (MCCA), which consists of two processes: generalization process and modification process. Errors introduced in the generalization process are revised in the modification process by adding modification neurons. In our approach, we visualize hidden neurons in terms of hypershperes. The learning process is the geometrical expansion process of these hypershperes. Through our experimental work in Section 5, we conclude that, MCCA is not sensitive to the order in which the input sequence is given. In addition, MCCA results in simple neural network structures by less training time. r 2006 Elsevier B.V. All rights reserved.	artificial neural network;biological neural networks;generalization (psychology);layer (electronics);neural network simulation;numerous;revision procedure;algorithm	Di Wang;Narendra S. Chaudhari	2006	Neurocomputing	10.1016/j.neucom.2005.12.124	computer science;artificial intelligence;theoretical computer science;machine learning;artificial neural network	AI	15.120211564685084	-28.039633774185404	177493
6497a45aa1a5951537671efcc408c96320b6db07	performance evaluation for training a distributed backpropagation implementation	multiprocessor interconnection networks;feed forward;learning algorithm;parallel algorithm;performance evaluation;feedforward neural network training;backpropagation;performance evaluation backpropagation multiprocessor interconnection networks parallel algorithms;lan interconnection;neural net;back propagation algorithm;artificial neural network training;lan interconnection distributed backpropagation algorithm performance evaluation feedforward neural network training artificial neural network training parallelized neural net learning algorithm;back propagation;algorithm design and analysis artificial neural networks backpropagation algorithms workstations feedforward systems performance analysis performance evaluation parallel algorithms neural networks local area networks;artificial neural network;distributed backpropagation algorithm;parallel algorithms;parallelized neural net learning algorithm	This paper presents the results of some experiments in parallelizing the training phase of a feed-forward, artificial neural network. More specifically, we develop and analyze a parallelization strategy of the widely used neural net learning algorithm called back-propagation. We describe an approach for parallelizing the back- propagation algorithm. We implemented these algorithms on several LANs, permitting us to evaluate and analyze their performances based on the results of actual runs. We were interested on the qualitative aspect of the analysis, in order to achieve a fair understanding of the factors determining the behavior of this parallel algorithms. We were interested in discovering and dealing with some of the specific circumstances that have to be considered when a parallelized neural net learning algorithm is to be implemented on a set of workstations in a LAN. Part of our purpose is to investigate whether it is possible to exploit the computational resources of such a set of workstations.	artificial neural network;automatic parallelization;backpropagation;computational resource;experiment;parallel algorithm;parallel computing;performance evaluation;requirement;software propagation;workstation	Sorin Babii	2007	2007 4th International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2007.375524	computer science;backpropagation;theoretical computer science;machine learning;distributed computing;parallel algorithm;artificial neural network	AI	10.207551087623086	-28.159790521402968	178636
2bf2bc46acee1707ef306bf129b297183fafa060	design of mlp using evolutionary strategy with variable length chromosomes	genetic operator;evolutionary computation;variable length chromosome evolutionary strategy es multi layer perceptron mlp neural network;multilayer perceptrons;variable length chromosome;training;neurons biological cells computer architecture artificial neural networks classification algorithms training accuracy;backpropagation;mutation operator;mathematical operators;bp algorithm evolutionary strategy variable length chromosomes multilayer perceptron design neural network neuron number genetic operators mutation operator crossover operator;computer architecture;accuracy;artificial neural networks;mlp neural network;biological cells;multilayer perceptrons backpropagation evolutionary computation mathematical operators;crossover operator;genetic operators;evolutionary strategy es;multi layer perceptron mlp;classification algorithms;evolutionary strategy;neuron number;multi layer perceptron;neurons;multilayer perceptron design;variable length chromosomes;neural network;bp algorithm	This paper presents a novel approach in designing MLP neural networks by using evolutionary strategy with variable length chromosomes. In particular, unlike other similar approaches in which the maximum number of neurons must be determined beforehand, the proposed method can grow a network as large as possible with less computational cost. By redefining genetic operators such as mutation and crossover, the evolutionary approach can evolve chromosomes with different lengths; therefore, various networks with different number of neurons in hidden layer can be achieved. The empirical result shows that the evolutionary strategy proposed in this paper can be compared favorably to other alternative approaches for classification problems.	algorithmic efficiency;artificial neural network;computation;crossover (genetic algorithm);definition;genetic operator;iterative and incremental development;memory-level parallelism;mutation (genetic algorithm)	Abbas Sarraf Shirazi;Tahereh Seyedena	2008	2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing	10.1109/SNPD.2008.120	evolutionary programming;computer science;artificial intelligence;backpropagation;genetic operator;machine learning;accuracy and precision;evolution strategy;multilayer perceptron;artificial neural network;algorithm	AI	14.566435776266756	-24.864403935566912	179293
bf430859368823e8c68e53b25a99fbdb69707129	competitive stochastic neural networks for vector quantization of images	stochastic neural networks;competitive neural networks;radial basis function networks;vector quantization;radial basis function network;classification rules;stochastic approximation;nearest neighbour;vector quantizer;neural network	A stochastic approximation to the nearest neighbour (NN) classification rule is proposed. This approximation is called Local Stochastic Competition 0.22). Some corivergence properties of LSC are discussed, and experimental results are presented. The approach shows a great potential for speeding up the codification process, with an affordable loss of codification quality.	artificial neural network;semi-continuity;stochastic approximation;stochastic neural network;vector quantization	Manuel Graña;Alicia D'Anjou;Ana Isabel González;F. Xabier Albizuri;Marie Cottrell	1995	Neurocomputing	10.1016/0925-2312(94)00072-Z	stochastic neural network;stochastic approximation;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;radial basis function network;vector quantization;artificial neural network	ML	13.307135825098598	-28.38613882692532	179598
cb81d13ab63acb9fa0fddc29439a9b9560b23d45	classification of landsat remote sensing images by a fuzzy unsupervised clustering algorithm	information processing	intelligence in animals, which are based in the capacities for self-organization in brain dynamics. Furthermore, voiceor motionactivated surveillance recorders can be triggered by prescribed inputs to limit their scope of intake, but once their gates are open, transcription is passive and representational, not active or meaningful. What is missing from passive devices is the goal-directedness contributed by the observer. Abstract	prism (surveillance program);self-organization;transcription (software)	Walter J. Freeman	1994	Inf. Sci.	10.1016/S0020-0255(98)10100-7	simulation;computer science;artificial intelligence;machine learning	AI	10.92319832607729	-27.277161494100785	180759
d17f835b376b3c38615321b777320e54acd91fe1	q-learning and redundancy reduction in classifier systems with internal state	evaluation function;hebbian learning;learning algorithm;multilayer perceptrons;reinforcement learning;classifier system;intelligence artificielle;algorithme apprentissage;classification;perceptron multicouche;apprentissage renforce;feature extraction;sharing rule;artificial intelligence;multi layer perceptron;inteligencia artificial;algoritmo aprendizaje;clasificacion;apprentissage hebbien	The Q-Credit Assignment (QCA) is a method, based on Q-learning, for allocating credit to rules in Classiier Systems with internal state. It is more powerful than other proposed methods, because it correctly evaluates shared rules, but it has a large computational cost, due to the Multi-Layer Perceptron (MLP) that stores the evaluation function. We present a method for reducing this cost by reducing redundancy in the input space of the MLP through feature extraction. The experimental results show that the QCA with Redundancy Reduction (QCA-RR) preserves the advantages of the QCA while it signiicantly reduces both the learning time and the evaluation time after learning.	algorithmic efficiency;evaluation function;feature extraction;memory-level parallelism;perceptron;q-learning;qualitative comparative analysis	Antonella Giani;Andrea Sticca;Fabrizio Baiardi;Antonina Starita	1998		10.1007/BFb0026707	hebbian theory;feature extraction;biological classification;computer science;artificial intelligence;machine learning;evaluation function;multilayer perceptron;reinforcement learning;algorithm	EDA	10.89667152590796	-30.436903654779663	180806
0f6b2f789fdcd4aa0d782265c4400edddac5fb96	functional-link nets with genetic-algorithm-based learning for robust nonlinear interval regression analysis	fuzzy regression;learning algorithm;cost function;interval regression analysis;regression model;upper bound;functional link net;robust method;genetic algorithm;regression analysis;upper and lower bounds;lower bound;robust learning algorithm;fitness function	Interval regression analysis has been a useful tool for dealing with uncertain and imprecise data. Since the available data often contain outliers, robust methods for interval regression analysis are necessary. This paper proposes a genetic-algorithm-based method for determining two functional-link nets for the robust nonlinear interval regression model: one for identifying the upper bound of data interval, in the robust nonlinear interval regression model, in the fitness function, not only the cost function with different weighting schemes but also the number of training data included in the interval model is taken into account. As for resisting outliers, the effects of training data beyond or beneath the estimated data interval on the determination of upper and lower bounds can be greatly reduced during the training phase when these data are located in the rejection region. Simulation results demonstrate that the proposed method performs well for contaminated data sets by resisting outliers and including all regular data in the data intervals. & 2008 Elsevier B.V. All rights reserved.	fitness function;genetic algorithm;loss function;nonlinear system;numerical analysis;rejection sampling;simulation	Yi-Chung Hu	2009	Neurocomputing	10.1016/j.neucom.2008.07.002	segmented regression;computer science;machine learning;pattern recognition;mathematics;upper and lower bounds;robust regression;statistics	AI	12.546332528166761	-30.53368919820887	181293
69b67b468b8fa193491e861aa33ea548938b6b0e	application of artificial neural network method to analyze user's behavior using clickstream data			artificial neural network;clickstream	Hei-Fong Ho;Chih-Chuan Chen	2017		10.3233/978-1-61499-828-0-94	machine learning;clickstream;artificial neural network;artificial intelligence;computer science	ML	10.545836318059845	-26.718371982075613	181523
2a5553dd0bf7e297b9f7c1c4aad943283f31f71a	retraining the neural network for data visualization	high dimensionality;feed forward neural network;multidimensional data;backpropagation algorithm;data visualization;neural network	In this paper, we discuss the visuaHzation of multidimensional data. A well-known procedure for mapping data from a high-dimensional space onto a lower-dimensional one is Sammon's mapping. The algorithm is oriented to minimize the projection error. We investigate an unsupervised backpropagation algorithm to train a multilayer feed-forward neural network (SAMANN) to perform the Sammon's nonlinear projection. Sammon mapping has a disadvantage. It lacks generalization, which means that new points cannot be added to the obtained map without recalculating it. The SAMANN network offers the generalization ability of projecting new data, which is not present in the original Sammon's projection algorithm. Retraining of the network when the new data points appear has been analyzed in this paper.	algorithm;artificial neural network;backpropagation;data point;data visualization;feedforward neural network;nonlinear system;sammon mapping	Viktor Medvedev;Gintautas Dzemyda	2006		10.1007/0-387-34224-9_4	feedforward neural network;probabilistic neural network;computer science;backpropagation;machine learning;pattern recognition;data mining;time delay neural network;data visualization;artificial neural network	ML	13.857527391932544	-30.270573469470467	181533
f659d78e4ea4358bce04f464d226a1a30f20c380	context dependent controller by overlapped neural networks for performance metrics revision	performance metrics revision;feedforward neural networks;measurement;neural networks;neural nets;feeds;overlapped neural networks context dependent neural network performance metrics revision;traffic control;backpropagation;back propagation neural network;performance metric;context dependent controller;neural networks measurement biological neural networks context modeling artificial neural networks computational modeling feeds feedforward neural networks scanning probe microscopy computer science;artificial neural networks;computational modeling;context dependent neural network;context dependent;backpropagation neural networks;computer science;scanning probe microscopy;neural nets backpropagation;context modeling;regulatory neural network;overlapped neural networks;biological neural networks;regulatory neural network context dependent controller overlapped neural networks performance metrics revision backpropagation neural networks;neural network	We introduce a novel approach for problems regardless of sufficiency or accuracy of their historical observations or lab simulation data. Our approach is based on imposing a context of problem performance metrics into networks and gaining the enhancement towards its satisfactory state. We use an overlapped system of back propagation neural networks for our purpose. A main neural network is responsible for mapping input and output relation while a regulatory neural network evaluates the performance metrics satisfaction. We provide special training and testing algorithms for the overlapped system that guarantees a synchronized solution for both neural networks. An example of traffic control problem is simulated. The result of simulation shows a great enhancement of the solution using our approach	algorithm;artificial neural network;backpropagation;belief revision;convolutional neural network;european neural network society;feedforward neural network;holographic principle;input/output;johnson–nyquist noise;neural networks;proceedings of the ieee;quantum state;simulation;software propagation;super paper mario;tor messenger	Atef Mohamed;Ruizhong Wei	2006	2006 5th IEEE International Conference on Cognitive Informatics	10.1109/COGINF.2006.365538	computer science;artificial intelligence;machine learning;data mining;time delay neural network	Robotics	12.698642392876062	-25.7531315654403	181936
10c9a9d31d0c1e8ba8d852697d9a5316a91c8f18	high performance training of feedforward and simple recurrent networks	feedforward neural network;sequential recursive auto associative memory sraam;feedforward neural networks;error function;training;singular value decomposition;conjugate gradient method;feature space;stopping criterion;recurrent network;simple recurrent network;sum of squares;network architecture;backward error;recurrent neural networks;recurrent neural network;recursive auto associative memory;high performance	TRAINREC is a system for training feedforward and recurrent neural networks that incorporates several ideas. It uses the conjugate-gradient method which is demonstrably more efficient than traditional backward error propagation. We assume epoch-based training and derive a new error function having several desirable properties absent from the traditional sum-of-squared-error function. We argue for skip (shortcut) connections where appropriate and the preference for a sigmoidal yielding values over the [-1,1] interval. The input feature space is often over-analyzed, but by using singular value decomposition, input patterns can be conditioned for better learning often with a reduced number of input units. Recurrent networks, in their most general form, require special handling and cannot be simply a re-wiring of the architecture without a corresponding revision of the derivative calculations. There is a careful balance required among the network architeucture (specifically, hidden and feedback units), the amount of training applied, and the ability of the network to generalize. These issues often hinge on selecting the proper stopping criterion. Discovering methods that work in theory as well as in... Read complete abstract on page 2.	artificial neural network;conjugate gradient method;experiment;feature vector;feedforward neural network;keyboard shortcut;propagation of uncertainty;recurrent neural network;sigmoid function;singular value decomposition;skip list;software propagation;wiring;workstation	Barry L. Kalman;Stanley C. Kwasny	1997	Neurocomputing	10.1016/0925-2312(95)00132-8	feedforward neural network;computer science;artificial intelligence;recurrent neural network;machine learning;control theory;statistics	ML	16.44522156911165	-30.220257228784003	182065
4f751f5d4e07a79bd1c2c150ba4927a480e18fa7	a color pattern recognition problem based on the multiple classes random neural network model	adaptive thresholding;learning algorithm;image processing;queuing theory;random neural network;multiple classes random neural network;gradient descent;color pattern recognition;retrieval process;pattern recognition;linear equations;neural network	Gelenbe has modeled the neural network using an analogy with the queuing theory. Recently, Fourneau and Gelenbe have proposed an extension of this model, called multiple classes random neural network (RNN) model. The purpose of this paper is to describe the use of the multiple classes RNN model to recognize patterns having di/erent colors. We propose a learning algorithm for the recognition of color patterns based upon the non-linear equations of the multiple classes RNN model using gradient descent of a quadratic error function. In addition, we propose a progressive retrieval process with adaptive threshold value. c © 2004 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;backpropagation;color;computation;content-addressable memory;gradient descent;image resolution;linear equation;mimd;network model;nonlinear system;pattern recognition;pixel;queueing theory;random neural network;run time (program lifecycle phase);simd;supervised learning	José Aguilar	2002	Neurocomputing	10.1016/j.neucom.2004.03.005	gradient descent;image processing;random neural network;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;mathematics;thresholding;linear equation;queueing theory;artificial neural network	ML	14.08030283694645	-29.1290713750609	183073
cb408465aa258cac800a657caeddedc1ffe5ec28	iterative design of regularizers based on data by minimizing generalization errors	minimisation;gaussian noise;iterative method;generalization error;parameter estimation linear regression input variables gaussian distribution neural networks computational complexity computer errors gaussian noise mean square error methods covariance matrix;neural networks;neural nets;input variables;linear regression model;linear regression;iterative methods;regularizers;computational complexity;iterative design;mean square error methods;generalization errors;generalisation artificial intelligence;minimisation linear regression model regularizers generalization errors parameter estimation iterative method;parameter estimation;minimisation neural nets parameter estimation iterative methods generalisation artificial intelligence;gaussian distribution;computer errors;covariance matrix	In our previous study (1998) we proposed a theoretical evaluation of generalization errors. However, it suffered from from serious difficulties: 1) it assumes that true model parameters and noise variance are known a priori; and 2) it assumes that input variables are mutually independent. These assumptions prevent its application to real data. The present paper succeeds in overcoming these two difficulties. A key idea is to iteratively estimate these parameters and generalization errors from data. Introducing correlations between input variables is not intrinsically difficult, although it makes computation much more complex than the cases where input variables are mutually independent.	iterative design;iterative method	Masumi Ishikawa;Hirohito Shimada;Shun-ichi Amari	2000		10.1109/IJCNN.2000.857805	econometrics;computer science;linear regression;machine learning;mathematics;iterative method;artificial neural network;statistics	ML	16.942595888863867	-30.197077088734865	184194
62dce065eda3f82ef3ac2b04affc7eb68c7c56fe	label distribution learning based on ensemble neural networks				Yansheng Zhai;Jianhua Dai;Hong Shi	2018		10.1007/978-3-030-04182-3_52		ML	10.839674939368734	-26.768516024422052	184516
01cb7069352f7db55c66216d8225769a01e180f3	fast evolutionary learning with batch-type self-organizing maps	algorithme rapide;distance function;learning algorithm;algorithm performance;variable structure;algorithme apprentissage;algoritmo genetico;data distribution;batch map;resultado algoritmo;fast algorithm;self organizing map;performance algorithme;algorithme genetique;algorithme evolutionniste;genetic algorithm;self organized map;algoritmo evolucionista;som;evolutionary algorithm;reseau neuronal;evolutionary learning;algoritmo aprendizaje;algoritmo rapido;red neuronal;fitness function;neural network	Although no distance function over the input data is definable, it is still possible to implement the self-organizing map (SOM) process using evolutionary-learning operations. The process can be made to converge more rapidly when the probabilistic trials of conventional evolutionary learning are replaced by averaging using the so-called Batch Map version of the self-organizing map. Although no other condition or metric than a fitness function between the input samples and the models is assumed, an order in the map that complies with the ‘functional similarity’ of the models can be seen to emerge. There exist two modes of use of this new principle: representation of nonmetric input data distributions by models that may have variable structures, and fast generation of evolutionary cycles that resemble those defined by the genetic algorithms. The spatial order in the array of models can be utilized for finding more uniform variations, such as crossings between functionally similar models.	converge;crossing number (graph theory);fitness function;genetic algorithm;organizing (structure);self-organization;self-organizing map	Teuvo Kohonen	1999	Neural Processing Letters	10.1023/A:1018681526204	genetic algorithm;self-organizing map;metric;computer science;artificial intelligence;machine learning;evolutionary algorithm;mathematics;fitness function;artificial neural network;algorithm	ML	12.165490296919984	-30.971090711555938	184626
4dcd7c21a5262bb7f9f0304b6222c6250f7adde3	generating three binary addition algorithms using reinforcement programming	computer program;binary addition;learning algorithm;learning model;reinforcement learning;automatic programming;automatic generation;neural net;function approximation;automatic program generation;source code;general binaries	"""Reinforcement Programming (RP) is a new technique for automatically generating a computer program using reinforcement learning methods. This paper describes how RP learned to generate code for three binary addition problems: simulate a full adder circuit, increment a binary number, and add two binary numbers. Each problem is presented as an extension of the one previous to it, which provides an introduction to the practical application of RP. Each solution uses a dynamic, episodic form of delayed Q-Learning algorithm. """"Dynamic"""" means that grows the policy during learning, and prunes it before the policy is translated to source code. This is different from Q-Learning models that use fixed-size tables or neural net function approximators to store q-values associated with (state, action) pairs. The states, actions, rewards, other parameters, and results of experiments are presented for each of the three problems."""	adder (electronics);artificial neural network;benchmark (computing);binary number;computer program;experiment;exploit (computer security);in-place algorithm;iteration;iterative method;lookup table;q-learning;rp (complexity);randomness;reinforcement learning;simulation;sorting;transfer function;turing machine	Spencer K. White;Tony R. Martinez;George L. Rudolph	2010		10.1145/1900008.1900072	temporal difference learning;computer science;theoretical computer science;machine learning;learning classifier system;algorithm	AI	16.348854791364516	-24.866256069480755	185067
f81b11f36095fc80d1260de0b3abbcef67b79ceb	evolutionary ordered neural network with a linked-list encoding scheme	encoding scheme evolutionary ordered neural network linked list encoding scheme evolutionary design neural network architecture neurons one dimensional array order information genetic operation genotype permutation problem evolutionary programming mutation operators xor 3 parity problems optimal neural network architecture;genetic operator;evolutionary design;evolutionary programming;neural net architecture;encoding neural net architecture genetic algorithms;genetic algorithm;genetic algorithms;neural networks encoding artificial neural networks neurons information processing evolutionary computation decoding electronic mail genetic algorithms genetic mutations;encoding;neural network	AbsiTaciThis paper proposes an evolutionary design of a neural network architecture, with a one-dimensional linked list encoding scheme. In this scheme, neurons are arranged in one-dimensional array, and the order informations of neurons play important roles in genetic operation. Due to one-dimensional structure, encoding from neural network architecture to genotype becomes easy, and genetic operation can be easily applied. To avoid the permutation problem, we choose evolutionary programming (EP) rather than genetic algorithm (GA), i.e., we apply mutation operators only in order to generate offspring. The proposed scheme is applied to XOR and 3-parity problems, and optimal neural network architectures can be found with this encoding scheme.	array data structure;artificial neural network;codi;continuous design;evolutionary programming;exclusive or;expectation propagation;genetic algorithm;line code;linked list;network architecture	Chi-Ho Lee;Jong-Hwan Kim	1996		10.1109/ICEC.1996.542680	evolutionary programming;computer science;theoretical computer science;machine learning;evolutionary acquisition of neural topologies;genetic representation;time delay neural network;algorithm	AI	14.929178640833257	-25.20205828918928	185161
ddcd998d25692af48443d163b7788b6b76f92c86	sequence input-based quantum-inspired neural networks with applications	quantum rotation gate;quantum inspired neuron;quantum inspired neural networks;journal;quantum computation;multi qubits controller not gate	To enhance the approximation and generalization ability of artificial neural networks (ANNs) by employing the principle of quantum rotation gate and controlled-not gate, a quantum-inspired neuron with sequence input is proposed. In the proposed model, the discrete sequence input is represented by the qubits, which, as the control qubits of the controlled-not gate after being rotated by the quantum rotation gates, control the target qubit for reverse. The model output is described by the probability amplitude of state $$|1\rangle $$ | 1 〉 in the target qubit. Then a quantum-inspired neural networks (QINN) is designed by employing the quantum-inspired neurons to the hidden layer and the common neurons to the output layer. The algorithm of QINN is derived by employing the Levenberg–Marquardt algorithm. Simulation results of some benchmark problems show that, under a certain condition, the QINN is obviously superior to the classical ANN.	activation function;approximation;artificial neural network;benchmark (computing);computational complexity theory;controlled not gate;information processing;inverter (logic gate);levenberg–marquardt algorithm;neuron;probability amplitude;quantum computing;qubit;scott continuity;simulation	Panchi Li;Hong Xiao	2013	Neural Processing Letters	10.1007/s11063-013-9316-7	quantum fourier transform;quantum information;theoretical computer science;quantum network;controlled not gate;control theory;quantum circuit;mathematics;quantum computer;quantum algorithm;quantum gate;quantum error correction	ML	14.523855773460872	-27.28843010212899	185243
fcde68094fe1bf57c6fa1e995bd2bf4d89af68cf	constructing text filters based on bayesian network learning	bayesian network		bayesian network	Wai Lam;Kon Fan Low	1998			artificial intelligence;probabilistic neural network;machine learning;computer science;bayesian programming;variable-order bayesian network;dynamic bayesian network;bayesian network;wake-sleep algorithm;pattern recognition	AI	12.502626981352403	-28.19010612597073	186571
44f38a426344690595b3348f7168ec8699c0232e	neural network simulation and pandemonium	neural network	"""An interesting problem in neural network simulations is to explain how patterned neural connections are organized during learning. Q~e of the authors (AOYAMA, I 985) constructed a Neural Network Simulator l; (hereafter referred to as NNS) as a software simulator. NNS can be run on conventional personal computers under MS-DOS or on Unix machines (e.g., SUN3). Alsq& in principle N~ can simulate other models such as Perceptron, CognitronZ"""" and Pandemonium ~', all of which have multilayered networks. (Pandemonium was proposed for pattern recognition by O.Selfridge in 1959). Excitatory and inhibitatory neurons are included in the layers. Principles of modifying the synaptic connections are included in the models and not in the simulator, NNS. The variables of A B C D £ neural nets are stored in data files which thus . . . . ,.~..~ allows a separation of network set up and analysis. '~he Simple Model"""" (referred to as SM) was the first model calculated on NNS by AOYAMA. This SM has basically the same structures as Pandemonium's and its 90 layers are divided into four blocks. Each block is named after the functions needed in pattern discrimination, image, feature, cognitive and decision demon, respectively. They are connected cascadedly in the order named. SM has six steps in a single complete learning cycle. In gene-ral, the most important point in models is how to determine """"effectiveness"""" of nerve impulse transmissions, xl00 In SM, the synaptic connections between neurons"""	action potential;artificial neural network;cognition;dos;ms-dos;nearest neighbor search;neural network software;pandemonium!;pattern recognition;perceptron;personal computer;simulation;synaptic package manager;unix	Kunio Suzuki;Chiaki Aoyama	1988	Neural Networks	10.1016/0893-6080(88)90261-4	nervous system network models;probabilistic neural network;computer science;machine learning;physical neural network;time delay neural network;artificial neural network	ML	13.181840690156461	-28.081768050726904	187794
1a7c11772248f9f2c10317b904305a6d143fb850	multilayer spline-based fuzzy neural network with deterministic initialization		A multilayer spline-based fuzzy neural network (MS-FNN) is proposed. It is based on the concept of multilayer perceptron (MLP) with B-spline receptive field functions (Spline Net). In this paper, B-splines are considered in the framework of fuzzy set theory as membership functions such that the entire network can be represented in form of fuzzy rules. MS-FNN does not rely on tensor-product construction of basis functions. Instead, it is constructed as a multilayered superposition of univariate synaptic functions and thus avoids the curse of dimensionality similarly to MLP, yet with improved local properties. Additionally, a fully deterministic initialization procedure based on principal component analysis is proposed for MS-FNN, in contrast to the usual random initialization of multilayer networks. Excellent performance of MS-FNN with one and two hidden layers, different activation functions, and B-splines of different orders is demonstrated for time series prediction and classification problems.	activation function;algorithm;artificial neural network;b-spline;basis function;brute-force search;curse of dimensionality;epoch (reference date);fuzzy set;mathematical optimization;memory-level parallelism;multilayer perceptron;neuro-fuzzy;principal component analysis;quad flat no-leads package;set theory;sigmoid function;spline (mathematics);synaptic package manager;time series;weatherstar	Vitaliy Kolodyazhniy	2011			neuro-fuzzy	ML	14.63109623274013	-28.366802635574608	188151
f26c70e048f1a35e45f10cd134ef36c5929b701d	some experiments around a neural network for multimodal associations	online learning;noise robustness;neurons networks;experimental evaluation;neuronal network;bidirectional associative memory;neural network	This paper presents a study of the model of triple BAM by [11] which is an improved variation of the original BAM model by [7]. This class of model aims at integrating different sensory inputs in order to memorize a unified and distributed representation. An experimental evaluation of the model is presented that underlines its limitations in terms of noise robustness and learning capacities. A new model is presented in order to overcome those initial limitations by introducing a new online learning algorithm adapted from the PRLAB initial algorithm that improve both noise robustness and learning capacities. Finally, model properties and limitations are considered and discussed within the context of multi-modal integration and brain modeling.	algorithm;artificial neural network;modal logic;multimodal interaction;online machine learning	Yann Boniface;Reghis Abdelmalek	2006			computer science;artificial intelligence;theoretical computer science;machine learning	ML	12.405794199970549	-28.755685266026802	188373
5afcbab3425484c2e8bb48b31ed545088f0da347	lightning prediction using satellite atmospheric sounding data and feed-forward artificial neural network			artificial neural network;automatic sounding;feedforward neural network	Elton Rafael Alves;Carlos Tavares da Costa;Lopes Márcio Nirlando Gomes;Brígida Ramati Pereira da Rocha;José Alberto Silva de Sá	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-161152	mathematics;machine learning;artificial intelligence;artificial neural network;feed forward;lightning;atmospheric sounding;satellite	Robotics	11.842053750585155	-26.17478072878187	188439
b4e9f1d38be71a267bf7cac149f12903daae443d	median m-type radial basis function neural network	neural networks;image classification;radial basis function;radial basis function neural network;rank m type estimators;neural network;radial basis functions	In this paper we present the capability of the Median M-Type Radial Basis Function (MMRBF) Neural Network in image classification applications. The proposed neural network uses the Median M-type (MM) estimator in the scheme of radial basis function to train the neural network. Other RBF based algorithms were compared with our approach. From simulation results we observe that the MMRBF neural network has better classification capabilities.	artificial neural network;radial (radio);radial basis function	José A. Moreno-Escobar;Francisco J. Gallegos Funes;Volodymyr I. Ponomaryov	2007		10.1007/978-3-540-76725-1_55	feedforward neural network;radial basis function;probabilistic neural network;computer science;artificial intelligence;recurrent neural network;machine learning;pattern recognition;time delay neural network;activation function;radial basis function network;artificial neural network	ML	13.753254413649092	-27.93487877730234	188477
a6f90c28f1f6ae345ab3ba4c67d1757177556bd4	lti ode-valued neural networks	dynamical neural network;parallel problem solving;article;complex valued neural network	A dynamical version of the classical McCulloch & Pitts’ neural model is introduced in this paper. In this new approach, artificial neurons are characterized by: i) inputs in the form of differentiable continuous-time signals, ii) linear time-invariant ordinary differential equations (LTI ODE) for connection weights, and iii) activation functions evaluated in the frequency domain. It will be shown that this new characterization of the constitutive nodes in an artificial neural network, namely LTI ODE-valued neural network (LTI ODEVNN), allows solving multiple problems at the same time using a single neural structure. Moreover, it is demonstrated that LTI ODEVNNs can be interpreted as complex-valued neural networks (CVNNs). Hence, research on this topic can be applied in a straightforward form. Standard boolean functions are implemented to illustrate the operation of LTI ODEVNNs. Concluding the paper, several future research lines are highlighted, including the need for developing learning algorithms for the newly introduced LTI ODEVNNs.	activation function;algorithm;artificial neural network;artificial neuron;decision problem;dynamical system;feedback;futures studies;heterodyne;language technologies institute;linear time-invariant theory;logic gate;machine learning;open dynamics engine;open research;parallel computing;perceptron;time complexity;time-invariant system	Manel Velasco;Enric X. Martín;Cecilio Angulo;Pau Martí	2014	Applied Intelligence	10.1007/s10489-014-0548-7	lti system theory;artificial intelligence;machine learning;time delay neural network	AI	15.152563569947832	-26.649281711599222	188546
d540052046184929a8521da0fada54e2073108a6	funcom: a constrained learning algorithm for fuzzy neural networks	optimisation sous contrainte;equation derivee partielle;modelisation linguistique;constrained optimization;methode a pas;fuzzy neural constrained optimization method;systeme equation;partial differential equation;ecuacion derivada parcial;linguistique;fuzzy neural network;learning algorithm;structure slash;linguistic modeling;reseau neuronal flou;intelligence artificielle;algorithme apprentissage;lagrange multiplier;time series;artificial intelligent;input output;optimizacion con restriccion;fnn;step method;linguistica;sistema ecuacion;parameter learning;equation system;serie temporelle;multiplicateur lagrange;serie temporal;multiplicador lagrange;non linearite;artificial intelligence;no linealidad;nonlinearity;sistema difuso;inteligencia artificial;systeme flou;reseau neuronal;algoritmo aprendizaje;constrained optimization problem;red neuronal;fuzzy system;linguistic modelling;funcom;fuzzy model;neural network;metodo a paso;linguistics	A novel learning algorithm, the FUNCOM (Fuzzy Neural Constrained Optimization Method) is suggested in this paper, for training fuzzy neural networks. The training task is formulated as a constrained optimization problem, whose objective is twofold: (i) minimization of an error measure, leading to successful approximation of the input=output mapping and (ii) optimization of an additional functional, which aims at formulating suitable internal representations of the fuzzy model. Optimization of the above functionals is carried out under the constraints imposed by the fuzzy system, which appear in the form of state equations. A fuzzy adaptation scheme is also suggested, which continuously modi es the step size during training, with the scope to improve the learning attributes of the algorithm. The FUNCOM qualities are investigated by a series of simulation examples. Comparisons with other learning algorithms are given and discussed, indicating the e ectiveness of the proposed algorithm. c © 2000 Published by Elsevier Science B.V. All rights reserved. Arti cial intelligence; Linguistic modeling; Constrained optimization; Structure=parameter learning	algorithm;approximation;artificial neural network;constrained optimization;constraint (mathematics);fuzzy control system;machine learning;mathematical optimization;optimization problem;simulation	Paris A. Mastorocostas;Ioannis B. Theocharis	2000	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00363-1	input/output;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;time series;mathematics;information fuzzy networks;lagrange multiplier;fuzzy set operations;partial differential equation;artificial neural network;algorithm	AI	10.607332614251364	-29.368301956241627	189022
2c155b468bcfc4bd81374b073d068b23b999d8ba	ensembles of genetically trained artificial neural networks for survival analysis		We have developed a prognostic index model for survival data based on an ensemble of artificial neural networks that optimizes directly on the concordance index. Approximations of the c-index are avoided with the use of a genetic algorithm, which does not require gradient information. The model is compared with Cox proportional hazards (COX) and three support vector machine (SVM) models by Van Belle et al. [10] on two clinical data sets, and only with COX on one artificial data set. Results indicate comparable performance to COX and SVM models on clinical data and superior performance compared to COX on non-linear data.	approximation;artificial neural network;belle (chess machine);bible concordance;genetic algorithm;gradient descent;nonlinear system;support vector machine	Jonas Kalderstam;Patrik Edén;Mattias Ohlsson	2013			machine learning;proportional hazards model;artificial intelligence;artificial neural network;genetic algorithm;support vector machine;pattern recognition;concordance;survival analysis;computer science;data set	ML	11.199034321114693	-24.323197758949483	189130
9fb4ee07822c89366ef58aafeb4e97619312de57	fault diagnosis of nonlinear analog circuits using neural networks with wavelet and fourier transforms as preprocessors	electronic circuits;neural networks;fourier transform;backpropagation neural network;diagnostic accuracy;analog circuits;wavelet transform;principal component analysis;nonlinear circuits;fault diagnosis;neural network	A neural-network based analog fault diagnostic system is developed for nonlinear circuits. This system uses wavelet and Fourier transforms, normalization and principal component analysis as preprocessors to extract an optimal number of features from the circuit node voltages. These features are then used to train a neural network to diagnose soft and hard faulty components in nonlinear circuits. Our neural network architecture has as many outputs as there are fault classes where these outputs estimate the probabilities that input features belong to different fault classes. Application of this system to two sample circuits using SPICE simulations shows its capability to correctly classify soft and hard faulty components in 95% of the test data. The accuracy of our proposed system on test data to diagnose a circuit as faulty or fault-free, without identifying the fault classes, is 99%. Because of poor diagnostic accuracy of backpropagation neural networks reported in the literature (Yu et al., Electron. Lett., Vol. 30, 1994), it has been suggested that such an architecture is not suitable for analog fault diagnosis (Yang et al., IEEE Trans. on CAD, Vol. 19, 2000). The results of the work presented here clearly do not support this claim and indicate this architecture can provide a robust fault diagnostic system.	analogue electronics;backpropagation;computer-aided design;electron;network architecture;neural networks;nonlinear system;preprocessor;principal component analysis;spice;simulation;test data;wavelet;yang	Farzan Aminian;Mehran Aminian	2001	J. Electronic Testing	10.1023/A:1012864504306	fourier transform;electronic circuit;electronic engineering;analogue electronics;computer science;engineering;stuck-at fault;theoretical computer science;machine learning;artificial neural network;wavelet transform;principal component analysis	ML	14.248640444630574	-26.472820659632493	189363
de63a03629610c875359f2058ff5c0a54955ce5c	advances in neural networks, intelligent control and information processing			information processing;intelligent control;neural networks	Qingshan Liu;Jun Wang;Zhigang Zeng	2016	Neurocomputing	10.1016/j.neucom.2016.01.085	artificial neural network;machine learning;artificial intelligence;mathematics;intelligent control;information processing	ML	12.542368732500211	-26.82436653184129	189451
8a1e28918ce86766c8ffb9d3f814d68647a396f7	kernel least-squares models using updates of the pseudoinverse	metodo cuadrado menor;noyau reproduisant;espace hilbert;methode moindre carre;calcul neuronal;selection problem;neural computation;problema seleccion;espacio hilbert;least squares method;methode noyau;temps lineaire;linear least square;regression model;non linear model;modele non lineaire;tiempo lineal;fonction perte;funcion perdida;hilbert space;modelo regresion;modelo no lineal;pseudoinverse;seudoinverso;least squares problem;loss function;reproducing kernel hilbert space;modele regression;metodo nucleo;linear time;least square;kernel method;reseau neuronal;mercer kernel;red neuronal;computacion neuronal;noyau mercer;neural network;probleme selection	Sparse nonlinear classification and regression models in reproducing kernel Hilbert spaces (RKHSs) are considered. The use of Mercer kernels and the square loss function gives rise to an overdetermined linear least-squares problem in the corresponding RKHS. When we apply a greedy forward selection scheme, the least-squares problem may be solved by an order-recursive update of the pseudoinverse in each iteration step. The computational time is linear with respect to the number of the selected training samples.	algorithmic efficiency;apricot kernel oil;experiment;greedy algorithm;hilbert space;iteration;kernel (operating system);least squares;loss function;moore–penrose pseudoinverse;nonlinear system;recursion;sparse;statistical classification;stepwise regression;time complexity	Edin Andelic;Martin Schafföner;Marcel Katz;Sven E. Krüger;Andreas Wendemuth	2006	Neural Computation	10.1162/neco.2006.18.12.2928	computer science;moore–penrose pseudoinverse;machine learning;calculus;reproducing kernel hilbert space;mathematics;geometry;least squares;artificial neural network;block matrix pseudoinverse	AI	12.183749580597455	-30.42397661483583	189579
34d1a8e46543ea868293df65fed35ed2c9658a00	bio-inspired parameter tunning of mlp networks for gene expression analysis	biology computing;hidden layers;learning rate;bioinspired parameter tunning;learning algorithm;multilayer perceptrons biology computing genetics;bioinspired optimization techniques;activation function;optimization technique;multilayer perceptrons;gene expression analysis parameter tuning bio inspired neural network;optimization artificial neural networks gallium tuning neurons error analysis pancreas;genetics;error analysis;artificial neural networks;tuning;gene expression analysis;parameter tuning;pancreas;bio inspired;genetic algorithm;genetic algorithms;optimization;bioinspired optimization techniques bioinspired parameter tunning mlp networks gene expression analysis artificial neural networks hidden layers activation function learning algorithm genetic algorithms;neurons;mlp networks;artificial neural network;gallium;neural network	The performance of artificial neural networks is largely influenced by the value of their parameters. Among these free parameters, one can mention those related with the network architecture, e.g., number of hidden neurons, number of hidden layers, activation function, and those associated with a learning algorithm, e.g., learning rate. Optimization techniques, often genetic algorithms, have been used to tune neural networks parameter values. Lately, other techniques inspired in Biology have been investigated. In this paper, we compare the influence of different bio-inspired optimization techniques on the accuracy obtained by the networks in the domain of gene expression analysis. The experimental results show the potential of use this techniques for parameter tuning of neural networks.	activation function;ant colony optimization algorithms;artificial neural network;baseline (configuration management);bioinformatics;british informatics olympiad;clonal selection algorithm;experiment;gene expression programming;genetic algorithm;mathematical optimization;memory-level parallelism;network architecture;optimizing compiler;particle swarm optimization;quad flat no-leads package;weka	André Luis Debiaso Rossi;André Carlos Ponce de Leon Ferreira de Carvalho;Carlos Soares	2008	2008 Eighth International Conference on Hybrid Intelligent Systems	10.1109/HIS.2008.152	types of artificial neural networks;computer science;bioinformatics;artificial intelligence;machine learning	ML	13.849309960314594	-24.251101540347758	189720
21e986559ce11ccab0615c883ae2d4162eda638d	fusion of artificial neural networks for learning capability enhancement: application to medical image classification		Artificial neural network (ANN) is one of the commonly used tools for computational applications. The specific advantages of ANN are high accuracy, less convergence time, less computational complexity, and so forth. However, all these merits are not available in the same ANN. Even though back propagation neural (BPN) networks are accurate, their computational complexity is significantly high. BPN networks are also not stable. On the other hand, Hopfield neural network (HNN) is better than BPN in terms of computational efficiency. But the accuracy of HNN is low. In this work, a modified ANN is proposed to overcome this specific problem. The modified ANN is a fusion of BPN and HNN. The technical concepts of BPN and HNN are mixed in the training algorithm of the proposed back propagation-Hopfield network (BPHN). The objective of this fusion is to improve the performance of conventional ANN. Magnetic resonance brain image classification experiments are used to analyse the proposed BPHN. Experimental results have suggested improvement in the learning process of the proposed BPHN. A comparative analysis with the conventional networks is performed to validate the performance of the proposed approach.	artificial neural network;computer vision	D. Jude Hemanth;J. Anitha;Bernadetta Kwintiana Ane	2017	Expert Systems	10.1111/exsy.12225	artificial intelligence;machine learning;computer science;computational complexity theory;artificial neural network;backpropagation;contextual image classification;hopfield network;convergence (routing)	ML	14.609386111409314	-29.18948225944735	189731
541b3068d5bd29a7c894f24fa581f27b0a9e6f64	a new genetic approach for neural network design	settore inf 01 informatica;computer model;evolutionary design;genetics;optimization problem;backpropagation algorithm;evolutionary algorithm;analytic solution;neural network	Summary. Neuro-genetic systems, a particular type of evolving systems, have become a very important topic of study in evolutionary design. They are biologicallyinspired computational models that use evolutionary algorithms (EAs) in conjunction with neural networks (NNs) to solve problems. EAs are based on natural genetic evolution of individuals in a defined environment and they are useful for complex optimization problems with huge number of parameters and where the analytical solutions are difficult to obtain. This work present an approach to the joint optimization of neural network structure and weights, using backpropagation algorithm as a specialized decoder, and defining a simultaneous evolution of architecture and weights of neural networks.	artificial neural network	Antonia Azzini;Andrea Tettamanzi	2008		10.1007/978-3-540-75396-4_10	evolutionary programming;stochastic neural network;computer science;artificial intelligence;machine learning;evolutionary algorithm;evolutionary acquisition of neural topologies;algorithm	AI	15.6313477896697	-24.049044364980166	190540
4b03720e983acb5e87b3c1e642b81ad2361b2b5b	shaping schedules as a method for accelerated learning		"""In psychology, shaping is defined as the process of building new responses by reinforcing successive approximations to a desired response [1]. The notion of shaping was developed by behavioral psychologists for training emimah and is used as an analogy for supervised neural network """"training. ~ Shaping is a schedule of learning that begins with presenting associatious that are simple to learn. As the simple associations are learned, progressively more difficult associations are used in the training. This incremental approach to learning can dramatically decrease the time required to learn complex associations, and can be used to bias how a neural network carries out an association. The authors have been experimenting with classification problems using the back-propagation learning algorithm [2]. This is a supervised learning paradigm in which a feed-forward neural network learns to associate a set of input patterns with a set of output patterns. TypicMly a set of exemplars, input/output pattern pa~s to be associated, are presented. Real world pattern classification problems may require a huge number of exemplars because of the difficulty of the task they model. If a large number of highly varied exemplars are used, the largely varied patterns create large contradictory error signals that may cause a bark-propagation network to either learn very slowly or to stabilize to an undesirable """"local m i n i m a ~ (such as cla~siying all input pa t t e rns a~ being in the same class). An al ternat ive to present ing a huge number of exemplars all at once is to use shaping. First a subset of the exemplars is presented. W hen the network can correctly classify the first set of pa t terns , more members of the exemplar set are added. This processes continues unt i l all of the members of the complete exemplar set are used to t ra in the network. A shaping schedule is a heirarchical approach to learning. The neural network can be viewed as learning a high dimensional decision surface that partitions the input space [3]. The first stages of shaping allow the network to quickly find simple coarse partitions of the input space. These partitions are stretched, bent and further refined in the later stages of the shaping schedule. Therefore, a shaping schedule is most effective when a classification problem requires a complex decision surface. An analogous way of thinking of shaping is that it is directing, and therefore limiting, the search involved in the learning process. The objective in developing a shaping schedule is to maintain a large directed gradient such that the learning can asurf~ through error space. Shaping was used to teach a neural network to distinguish between images of A's, B's, C's, D's, and blank images independent of rotation, brightness, and contrast, in the presence large amounts of added noise [4]. The learning time for this task with shaping was approximately four times faster than learning the same task without the shaping schedule. The authors' present work includes other emperical studies and analytical studies to determine how to best design a shaping schedule for a particular classification problem."""	algorithm;approximation;artificial neural network;backpropagation;decision boundary;experiment;feedforward neural network;gradient;input/output;noise shaping;programming paradigm;software propagation;supervised learning;traffic shaping	Alexis Wieland;Russell Leighton	1988	Neural Networks	10.1016/0893-6080(88)90268-7	machine learning;artificial intelligence;mathematics;schedule	ML	17.10503112542922	-30.66489389285409	190555
d39168c654655ce1e2585cf49d1dd144596edc44	a method for noise filtering with feed-forward neural networks: analysis and comparison with low-pass and optimal filtering	feedforward neural network;random frequency mixtures;chaotic signals;computer simulations;feed forward neural network;sine waves wiener filtering additive white noise attractor back propagation learning chaotic signals computer simulations feed forward neural networks low pass filtering noise filtering optimal filtering random frequency mixtures;neural nets;optimal filtering;white noise digital simulation learning systems low pass filters neural nets signal processing;low pass;low pass filter;attractor;learning systems;feed forward neural networks;signal processing;low pass filtering;network configuration;wiener filtering;noise filtering;additive white noise;back propagation learning;low pass filters;computer simulation;back propagation;white noise;sine waves;digital simulation;neural network	C. Klimasauskas (1989) discussed the use of layered, feedforward neural networks that use the back-propagation learning rule for noise filtering. An analysis is presented of a similar neural network method for noise filtering, and, using computer simulations, it is compared to predictions of the analysis and to the noise-filtering properties of Wiener (or optimal) filtering and low-pass filtering. The signals used for comparison are two chaotic signals and two random frequency mixtures of sine waves, all corrupted with additive white noise. The network method compares favorably when filtering chaotic signals, and its performance can be approximately predicted by a simple equation that is based on the network configuration. Even networks that are constructed poorly will do some filtering, although knowledge of the dimension of the attractor for the signal being filtered is useful for determining an optimal configuration	artificial neural network;feedforward neural network;low-pass filter	B. Anderson;D. Montgomery	1990		10.1109/IJCNN.1990.137571	median filter;feedforward neural network;speech recognition;low-pass filter;computer science;machine learning;artificial neural network	ML	16.597064504445054	-27.592431459357297	190734
35c28c58318ccc94e18dcc7141811bf9f20c52bd	transform image coding by feedforward neural networks	feedforward neural network;image coding;feedforward;image processing;circuit vlsi;procesamiento imagen;boucle anticipation;traitement image;codificacion;ciclo anticipacion;vlsi circuit;dsp processor;coding;circuito vlsi;reseau neuronal;local minima;red neuronal;codage;neural network		artificial neural network;feedforward neural network	O. Martinelli;Lucio Prina Ricotti;Susanna Ragazzini;C. Spallaccini	1993	I. J. Circuit Theory and Applications	10.1002/cta.4490210509	feedforward neural network;electronic engineering;probabilistic neural network;image processing;computer science;artificial intelligence;machine learning;maxima and minima;time delay neural network;coding;feed forward;artificial neural network	ML	15.786046325678546	-27.499305417276325	190735
26e61f451c96cd6d06182ce219518c42f662c6f2	extreme learning machine with enhanced variation of activation functions		The main aim of this paper is to stress the fact that the sufficient variability of activation functions (AF) is important for an Extreme Learning Machine (ELM) approximation accuracy and applicability. A slight modification of the standard ELM procedure is proposed, which allows increasing the variance of each AF, without losing too much from the simplicity of random selection of parameters. The proposed modification does not increase the computational complexity of an ELM training significantly. Enhancing the variation of AFs results in reduced output weights norm, better numerical conditioning of the output weights calculation, smaller errors for the same number of the hidden neurons. The proposed approach works efficiently together with the Tikhonov regularization of ELM.	activation function;anisotropic filtering;approximation;computational complexity theory;elm;numerical analysis;spatial variability;t-norm	Jacek Kabzinski	2016		10.5220/0006066200770082	computer science;artificial intelligence;machine learning;algorithm	ML	15.857642076821762	-29.890474425982017	190751
477255a8986d55ecc55a52ef18d276d977e75733	qubit inspired neural network towards its practical applications	quantum backpropagation learning quantum computing qbit neural network multilayered neural network;neural nets;benchmark problem;backpropagation;quantum computing backpropagation neural nets;quantum computer;brain function;quantum computing;neural networks biological neural networks quantum computing computer networks multi layer neural network computational efficiency neurons parity check codes iris night vision;computational efficiency;data classification;multilayer neural network;back propagation;neural network	Neural networks have attracted much interest in the two decades for their potential to describe brain function realistically. Quantum computing is a likely candidate for improving the computational efficiency of neural networks, since it has been very successful in doing so for a selected set of computational problems. We have proposed qubit neural network that is a multilayered neural network composed of qubit inspired neurons with quantum back propagation learning and confirmed the performance concerning basic benchmark problems such as 4-bit and 6-bit parity check problems. In this paper, we examine this qubit neural network through more practical problems, for example, the Iris data classification and the night vision processing.	4-bit;backpropagation;benchmark (computing);computation;computational problem;neural networks;parity bit;performance;quantum computing;qubit;simulation;software propagation	Katsuhiro Mori;Teijiro Isokawa;Noriaki Kouda;Nobuyuki Matsui;Haruhiko Nishimura	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246684	stochastic neural network;nervous system network models;catastrophic interference;cellular neural network;probabilistic neural network;types of artificial neural networks;neural cryptography;computer science;artificial intelligence;recurrent neural network;backpropagation;theoretical computer science;machine learning;physical neural network;time delay neural network;deep learning;quantum computer;artificial neural network	ML	15.71363234385262	-26.040667830575792	191267
1b154cdca69aa6663ba8b23d4481dfb82544990b	artificial wavelet neuro-fuzzy model based on parallel wavelet network and neural network	learning process;activation function;lyapunov function;neuro fuzzy model;neuro fuzzy;stability analysis;wavelet;wavelet network;fuzzy model;neural network	From the well-known advantages and valuable features of wavelets when used in neural network, two type of networks (i.e., SWNN and MWNN) have been proposed. These networks are single hidden layer network. Each neuron in the hidden layer is comprised of wavelet and sigmoidal activation functions. First model is derived from adding the outputs of wavelet and sigmoidal activation functions, while in the second model outputs of wavelet and sigmoidal activation function are multiplied together. Using these proposed networks in consequent part of the neuro-fuzzy model, which result summation wavelet neuro-fuzzy and multiplication wavelet neuro-fuzzy models, are also proposed. Different types of wavelet function are tested with proposed networks and fuzzy models on four different types of examples. Convergence of the learning process is also guaranteed by performing stability analysis using Lyapunov function.	artificial neural network;neuro-fuzzy;wavelet	Ahmad Banakar;Mohammad Fazle Azeem	2008	Soft Comput.	10.1007/s00500-007-0238-z	wavelet;von neumann stability analysis;lyapunov function;computer science;artificial intelligence;neuro-fuzzy;machine learning;cascade algorithm;mathematics;activation function;artificial neural network	ML	15.145076330905091	-27.615493962209143	191432
7d8f7113eb75dd14d203075d462cba5b52270678	radial basis function networks in nonparametric classification and function learning	rate of convergence;radial basis function networks;convergence rates nonparametric classification function learning normalized radial basis function networks parameter learning;radial basis function network;computer experiment;learning artificial intelligence pattern classification radial basis function networks;pattern classification;learning artificial intelligence;radial basis function networks intelligent networks convergence kernel pattern recognition vectors approximation error estimation error computer science computer networks	In this paper we apply normalized radial basis function networks to function learning and in nonparametric classification. A simple parameter learning technique is proposed and convergence and the rates of convergence of the empirically trained networks are studied theoretically and in computer experiments.	algorithm;computer experiment;machine learning;radial (radio);radial basis function;radial basis function network;simulation;transfer function;vergence	Balázs Kégl;Adam Krzyzak;Heinrich Niemann	1998		10.1109/ICPR.1998.711206	radial basis function;computer experiment;radial basis function kernel;computer science;artificial intelligence;machine learning;pattern recognition;rate of convergence;activation function;radial basis function network	ML	16.40712891522232	-29.535065632928738	192196
ec789a671f3b8e7f350d6d635743bea25da25e9b	independent component analysis based on improved quantum genetic algorithm: application in hyperspectral images	learning algorithm;convergence;premature convergence;iterative algorithms;anomaly detection;hyperspectral sensors;independent component analysis;biological cells;quantum mechanics;numerical computation;independent component analysis genetic algorithms hyperspectral imaging hyperspectral sensors quantum computing biological cells iterative algorithms quantum mechanics convergence genetic mutations;genetic algorithm;genetic algorithms;genetic mutations;hyperspectral imaging;quantum computing;hyperspectral image;neuronal activity	To avoid the restriction of neuron activation functions of neural learning algorithm and the disadvantage of getting into local optimum solution with general numerical computation method, a novel independent component analysis (ICA) based on improved quantum genetic algorithm (IQGA) is proposed in our paper. Moreover, Han’s quantum genetic algorithm (QGA) is improved by adopting the quantum crossover and quantum mutation to overcome the premature convergence and increase the search capability in our work. The proposed algorithm is applied to hyperspectral anomaly detection. The effectiveness of the algorithm is evaluated by HYDICE hyperspectral images. It is demonstrated that the proposed algorithm has better detection effect and time efficiency than QGA based ICA for the hyperspectral anomaly detection task.	anomaly detection;computation;genetic algorithm;han unification;independent computing architecture;independent component analysis;local optimum;neuron;numerical analysis;premature convergence	Na Li;Peng Du;Huijie Zhao	2005	Proceedings. 2005 IEEE International Geoscience and Remote Sensing Symposium, 2005. IGARSS '05.	10.1109/IGARSS.2005.1525875	mathematical optimization;anomaly detection;genetic algorithm;computer science;theoretical computer science;hyperspectral imaging;machine learning;mathematics;remote sensing;population-based incremental learning	Robotics	16.779469375561785	-26.713268924201984	192513
48002bac037a46c10f7f88e79db5ceedc6450bb7	hierarchical neuro-fuzzy quadtree models	tolerancia falta;hierarchical neurofuzzy quadtree;hierarchical system;learning process;cluster;learning algorithm;fuzzy system models;fuzzy system model;fuzzy set;recursive partitioning;systeme neuronal flou;processus apprentissage;hnfq;learning;amas;gradient method;quadtree recursive partitioning;algoritmo recursivo;systeme hierarchise;conjunto difuso;ensemble flou;aprendizaje;methode gradient;sistema jerarquizado;apprentissage;algorithme recursif;particion;function approximation;metodo gradiente;fonction appartenance;identification;neuro fuzzy;fault tolerance;neurofuzzy system;neuro fuzzy system;partition;membership function;identificacion;sistema difuso;recursive algorithm;monton;systeme flou;funcion pertenencia;tolerance faute;fuzzy system;neural network;modele systeme flou	Hybrid neuro-fuzzy systems have been in evidence during the past few years, due to its attractive combination of the learning capacity of arti2cial neural networks with the interpretability of the fuzzy systems. This article proposes a new hybrid neuro-fuzzy model, named hierarchical neuro-fuzzy quadtree (HNFQ), which is based on a recursive partitioning method of the input space named quadtree. The article describes the architecture of this new model, presenting its basic cell and its learning algorithm. The HNFQ system is evaluated in three well known benchmark applications: the sinc(x; y) function approximation, the Mackey Glass chaotic series forecast and the two spirals problem. When compared to other neuro-fuzzy systems, the HNFQ exhibits competing results, with two major advantages it automatically creates its own structure and it is not limited to few input variables. c © 2002 Elsevier Science B.V. All rights reserved.	algorithm;approximation;artificial neural network;benchmark (computing);fuzzy control system;neuro-fuzzy;quadtree;recursion	Flávio Joaquim de Souza;Marley M. B. R. Vellasco;Marco Aurélio Cavalcanti Pacheco	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00145-2	partition;identification;fault tolerance;membership function;function approximation;computer science;gradient method;artificial intelligence;neuro-fuzzy;machine learning;mathematics;fuzzy set;hierarchical control system;artificial neural network;algorithm;fuzzy control system;recursive partitioning;recursion;cluster	AI	10.529455945433739	-29.507741400254737	192646
6d07a04e5171cbff0383f208f251308e27644aae	simultaneous estimation of signal and noise in the constructive neural networks	neural network		neural networks	Shahram Hosseini;Christian Jutten	1998			constructive;artificial intelligence;machine learning;probabilistic neural network;mathematics;types of artificial neural networks;time delay neural network;stochastic neural network;artificial neural network;recurrent neural network;rectifier (neural networks)	ML	13.115838556885127	-27.28935815419606	193040
188fd02af5b713dd973a0a94d3692d43af5a6850	a pictorial pattern recognition based on an associative memory by use of the reaction-diffusion equation	subsystems;system engineering;control systems;chemical processes;neural nets;multivariable systems;pictorial pattern recognition;biological control systems;systems engineering and theory;chemical engineering;decentralized system;reaction diffusion equation;neighboring information;neural nets pattern recognition content addressable storage multivariable systems diffusion chemical reactions;information processing;pattern recognition associative memory information processing biological systems systems engineering and theory equations;pattern recognition;biological systems;associative memory;chemical reactions;humans;local interaction;neural unit;content addressable storage;diffusion;neighboring information pictorial pattern recognition associative memory reaction diffusion equation visual information processing biological systems neural unit global task autonomous decentralized systems system engineering local interaction subsystems;visual processing;visual information processing;autonomous decentralized systems;chemical technology;global task	Visual information process in the biological systems is performed simultaneously and in parallel at each neural unit. That is, Q global task with no supervisor is achieved by the interaction among Q lot of subsystems. I n this sense, the visual process has a share of the common functional strzlcture with ‘Autonomous Decentralized Systems’ which is attracted much attention in the fields of the system engineering. In the present paper, we attempt to design a pictorial pattern recognition system based on the concept of the autonomous decentralized system. The local interaction am.ong subsystems is described by reactiondigusion equations, in which each subsystem utilizes only the neighboring information around itself.	autonomous decentralized system;autonomous robot;biological system;content-addressable memory;decentralised system;decentralized autonomous organization;image;pattern recognition;systems engineering	Satoshi Ito;Hideo Yuasa;Koji Ito;Masami Ito	1995		10.1109/ISADS.1995.398980	computer vision;chemical reaction;information processing;decentralised system;computer science;control system;chemical process;diffusion;reaction–diffusion system	Robotics	12.810549860949425	-26.35965416087829	193093
115d9ea7ec6c9315e69a29ca27f270f506910f87	ensembles of evolutionary extreme learning machines through differential evolution and fitness sharing	feedforward neural nets evolutionary computation;sociology statistics neurons cancer vectors glass training;de evolutionary extreme learning machine ensemble method differential evolution fitness sharing single hidden layer feedforward neural network real world pattern classification problems elms evolutionary algorithms sharing function method sharing radius representative specification method benchmark classification tasks	Extreme Learning Machine (ELM) is a single-hidden-layer feedforward neural network which has been applied into many real world pattern classification problems. Recently, ELMs have been built in an automatic way through evolutionary algorithms. Most works, nonetheless, do not uses all population obtained, but choose only one individual in the last generation. In an attempt to improve performance, an ensemble is a more promising choice because a pool of classifiers might produce higher accuracy than merely using the information from only one classifier among them. One of the most important factors for optimum accuracy is the diversity of the classifier pool. In this work, an enhanced Differential Evolution incorporating sharing function method is used to generate a pool of ELMs. Fitness Sharing that shares resources if the distance between the individuals is smaller than the sharing radius is a representative specification method, which produces diverse results than standard evolutionary algorithms that converge to only one solution. Experimental results on 14 well known benchmark classification tasks suggest that our method can generate ensembles that are more effective than ensembles solely through DE and traditional ensemble methods.	artificial neural network;benchmark (computing);converge;differential evolution;ensemble learning;evolutionary algorithm;feedforward neural network	Tiago P. F. de Lima;Teresa Bernarda Ludermir	2014	2014 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2014.6889956	evolutionary programming;evolutionary music;interactive evolutionary computation;computer science;artificial intelligence;machine learning;evolutionary acquisition of neural topologies;data mining;fitness approximation;evolutionary robotics	ML	13.993775371201174	-24.071348978320117	193868
0419ff97c5cc592ff2b8a7309c0418b9db8fade4	sub-optimal multiuser detector using a time-varying gain chaotic simulated annealing neural network	optimal solution;time varying;multiuser detection;chaotic behavior;convergence;cdma system;chaos;detectors chaos simulated annealing neural networks multiuser detection recurrent neural networks multiaccess communication time varying systems concrete hopfield neural networks;time varying systems;hopfield neural nets;telecommunication computing;simulated annealing;time varying systems chaos code division multiple access convergence hopfield neural nets multiuser detection simulated annealing telecommunication computing;optimization problem;simulation experiment;code division multiple access;gradient convergence sub optimal multiuser detector time varying gain chaotic simulated annealing neural network cdma system hopfield type neural networks recurrent neural network;time varying gain chaotic simulated annealing neural network;sub optimal multiuser detector;global optimization;recurrent neural network;gradient convergence;neural network;hopfield type neural networks	This paper proposes a sub-optimal multiuser detector (MUD) algorithm for CDMA system based on the neural network with Time-varying Gain Chaotic Simulated Annealing Neural Network (TGCSANN), and gives a concrete model of the MUD after appropriate transformations and mappings. By refraining from the serious local optimal problem of Hopfield-type neural networks, the TGCSANN makes use of the time-varying and chaotic simulated annealing parameters of the recurrent neural network to control the evolving behavior of the network so that the network undergoes the transition from chaotic behavior to gradient convergence. It has richer and more flexible dynamics rather than conventional neural networks, so that it can be expected to have much ability to search for globally optimal or sub-optimal solutions. Simulation experiments have been performed to show the effectiveness and validation of the proposed neural network based method for MUD.	algorithm;artificial neural network;chaos theory;experiment;gradient;hopfield network;mud;maxima and minima;multi-user;recurrent neural network;simulated annealing;simulation	Yunxiao Jiang;Zifa Zhong;Jun-an Yang;Min Zhang	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.695	stochastic neural network;computer science;artificial intelligence;recurrent neural network;machine learning;control theory	ML	16.03746024877959	-25.284084167055504	194150
8625622e73d32b8617880a14987a2bd9128083cb	speech steganalysis using evolutionary restricted boltzmann machines	support vector machines;speech;cats particle swarm optimization genetic algorithms statistics speech support vector machines;steganography audio watermarking boltzmann machines genetic algorithms learning artificial intelligence particle swarm optimisation speech processing;particle swarm optimization;contrastive divergence learning algorithm speech steganalysis evolutionary restricted boltzmann machines rbm steganalysis tool speech files audio files particle swarm optimization pso genetic algorithm ga artificial bees colony abc cat swarm optimization cso steghide hide4pgp freqsteg;statistics;genetic algorithms;cats;steganalysis deep belief network restricted boltzmann machines evolutionary algorithms particle swarm optimization artificial bees colony cat swarm optimization genetic algorithm steganography	This paper presents a new method to train Restricted Boltzmann Machines (RBMs) using Evolutionary Algorithms (EAs), where RBMs are used in the first step of a steganalysis tool for speech/audio files. The following EAs have been tested: Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Artificial Bees Colony (ABC) and Cat Swarm Optimization (CSO). Our method has been tested with three steganographic techniques: StegHide, Hide4PGP, and FreqSteg. A fourth technique combining the three steganographic methods has also been tested. The results are compared to the conventional contrastive divergence learning algorithm. All EAs outperform the contrastive divergence algorithm.	chief security officer;evolutionary algorithm;genetic algorithm;mathematical optimization;particle swarm optimization;recurrent neural network;restricted boltzmann machine;steganalysis;steganography	Catherine Paulin;Sid-Ahmed Selouani;Eric Hervet	2016	2016 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2016.7744409	support vector machine;mathematical optimization;multi-swarm optimization;speech recognition;meta-optimization;genetic algorithm;computer science;speech;artificial intelligence;machine learning;particle swarm optimization;metaheuristic;statistics	AI	14.039719640920037	-24.86279578452992	194792
cfc61eb1cd3201cd1c77f1715481285102737a36	particle swarm optimization of neural network architectures andweights	learning artificial intelligence data mining knowledge based systems;learning algorithm;rule based evolutionary online learning systems;usu;perforation;reinforcement learning;rule based;classifier system;learning classifier systems;learning classifier system;online learning;data mining;xcs classifier system;machine learning;function approximation;xcs classifier system learning classifier systems rule based evolutionary online learning systems data mining reinforcement learning;generating function;learning artificial intelligence;machine learning learning systems psychology hybrid intelligent systems function approximation machine learning algorithms art genetic algorithms production systems reservoirs;knowledge based systems	The optimization of architecture and weights of feed forward neural networks is a complex task of great importance in problems of supervised learning. In this work we analyze the use of the particle swarm optimization algorithm for the optimization of neural network architectures and weights aiming better generalization performances through the creation of a compromise between low architectural complexity and low training errors. For evaluating these algorithms we apply them to benchmark classification problems of the medical field. The results showed that a PSO-PSO based approach represents a valid alternative to optimize weights and architectures of MLP neural networks.	algorithm;artificial neural network;benchmark (computing);mathematical optimization;memory-level parallelism;particle swarm optimization;performance;supervised learning	Marcio Carvalho;Teresa Bernarda Ludermir	2007	7th International Conference on Hybrid Intelligent Systems (HIS 2007)	10.1109/HIS.2007.66	semi-supervised learning;computer science;artificial intelligence;online machine learning;machine learning;pattern recognition;learning classifier system;stability;active learning	ML	13.962986408351245	-24.978833060896655	194958
a4e26dbfd2e7136eb2aa7f5872d3037d9b76319c	a pseudoinverse learning algorithm for feedforward neural networks with stacked generalization applications to software reliability growth data	fast learning;linear algebra;feedforward neural network;feedforward neural networks;learning rate;learning algorithm;supervised learning;generalized linear algebra;exact solution;inner product;general techniques;gradient descent;it adoption;pseudoinverse learning algorithm;network architecture;generalization	A supervised learning algorithm, Pseudoinverse Learning Algorithm (PIL), for feedforward neural networks is developed. The algorithm is based on generalized linear algebraic methods, and it adopts matrix inner products and pseudoinverse operations. Incorporating with network architecture of which the number of hidden layer neuron is equal to the number of examples to be learned, the algorithm eliminates learning errors by adding hidden layers and will give an exact solution (perfect learning). Unlike the existing gradient descent algorithm, the PIL is a feedforward only, fully automated algorithm, including no critical user-dependent parameters such as learning rate or momentum constant. The algorithm is tested on case studies with stacked generalization applications to software reliability growth data. The results indicate that the proposed algorithm is very e1cient for the investigation on the computation-intensive generalization techniques. c © 2003 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;computation;ensemble learning;feedforward neural network;gradient descent;linear algebra;network architecture;neuron;pattern recognition;python imaging library;real-time clock;real-time computing;software quality;software reliability testing;supervised learning	Ping Guo;Michael R. Lyu	2004	Neurocomputing	10.1016/S0925-2312(03)00385-0	rprop;feedforward neural network;mathematical optimization;wake-sleep algorithm;weighted majority algorithm;computer science;theoretical computer science;linear algebra;machine learning;fsa-red algorithm;supervised learning;stability;block matrix pseudoinverse;population-based incremental learning;generalization error	ML	16.371749710455273	-29.743102852517985	195269
916264daf3a923588fa699fb423bdc165460cd99	rccn: radial basis competitve and cooperative network	optimal learning radial basis bidirectional competitive and cooperative network rccn bidirectional mapping network radial basis function units accommodation boundaries hierarchical learning scheme mapping scheme many to many relation ellipsoidal boundaries noisy samples;interpolation;radial basis function units;rccn;neural networks;mapping scheme;optimal learning;neural networks education shape computer science computer architecture interpolation information processing backpropagation algorithms robot kinematics propulsion;automatic generation;computer architecture;learning artificial intelligence feedforward neural nets;hierarchical learning scheme;radial basis function;bidirectional mapping network;shape;degeneration;information processing;backpropagation algorithms;propulsion;feedforward neural nets;noisy samples;computer science;learning artificial intelligence;accommodation boundaries;ellipsoidal boundaries;radial basis bidirectional competitive and cooperative network;many to many relation;robot kinematics	This paper presents a new neural network architecture referred to here as Radial Basis Bidirectional Competitive and Cooperative Network (RCCN). RCCN is designed to perform bidirectional many-to-many mapping between X c R m and Y c R n . In RCCN, the many-to-many relation represented by a collection of teaching samples is modeled in terms of a number of reference X-Y points and the correlation matrices associated with individual reference points. The correlation matrix represents the distribution of samples in the proximity of the reference point, and defines the local distance measure associated with the reference point. The learning scheme is based on the competition of reference points in terms of Mahalanobis distance with Hierarchical Self-organization, which provides the capability of automatic determination of necessary number of reference points. The bidirectional nmny-to-many mapping is achieved by the cooperation of nearby reference points through clustering and interpolation. Our simulations have revealed that RCCN has the suficient capability of efficient learning and accurate mapping, even for the complex many-to-many relations.	artificial neural network;cluster analysis;interpolation;many-to-many;network architecture;radial (radio);self-organization;simulation	Sukhan Lee;Shunichi Shimoji	1992		10.1109/TAI.1992.246370	radial basis function;propulsion;information processing;interpolation;shape;computer science;artificial intelligence;machine learning;artificial neural network;robot kinematics	Vision	14.231596597094741	-30.302708309661142	195389
233f746a23c4494e4914df027876192166581d98	a modified gradient-based neuro-fuzzy learning algorithm and its convergence	strong convergence;learning rate;learning algorithm;zero order takagi sugeno inference system;convergence;takagi sugeno;fuzzy rules;constant learning rate;modified gradient based neuro fuzzy learning algorithm;neuro fuzzy;adaptive method;membership function;gaussian membership function;fuzzy system	Neuro-fuzzy approach is known to provide an adaptive method to generate or tune fuzzy rules for fuzzy systems. In this paper, a modified gradient-based neuro-fuzzy learning algorithm is proposed for zero-order Takagi-Sugeno inference systems. This modified algorithm, compared with conventional gradient-based neuro-fuzzy learning algorithm, reduces the cost of calculating the gradient of the error function and improves the learning efficiency. Some weak and strong convergence results for this algorithm are proved, indicating that the gradient of the error function goes to zero and the fuzzy parameter sequence goes to a fixed value, respectively. A constant learning rate is used. Some conditions for the constant learning rate to guarantee the convergence are specified. Numerical examples are provided to support the theoretical findings.	algorithm;gradient;neuro-fuzzy	Wei Wu;Long Li;Jie Yang;Yan Liu	2010	Inf. Sci.	10.1016/j.ins.2009.12.030	mathematical optimization;convergence;membership function;wake-sleep algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;online machine learning;machine learning;control theory;mathematics;information fuzzy networks;stability;fuzzy control system;population-based incremental learning;generalization error	AI	15.93457133451654	-29.28699985853935	195408
719ce873c46a0c3bdd834b1d965f4513019cc11c	protein secondary structure prediction using multiple neural network likelihood models	likelihood model;bayesian inference;protein secondary structure;neural network	Predicting Alpha-helicies, Beta-sheets and Turns of a proteins secondary structure is a complex non-linear task that has been approached by several techniques such as Neural Networks, Genetic Algorithms, Decision Trees and other statistical or heuristic methods. This project introduces a new machine learning method by combining Bayesian Inference with offline trained Multilayered Perceptron (MLP) models as the likelihood for secondary structure prediction of proteins. With varying window sizes of neighboring amino acid information, the information is extracted and passed back and forth between the Neural Net and the Bayesian Inference process until the posterior probability of the secondary structure converges.	dos;decision tree;experiment;genetic algorithm;heuristic;machine learning;memory-level parallelism;neural networks;nonlinear system;online and offline;parallel computing;perceptron;protein structure prediction	Seong-Gon Kim;Yong-Gi Kim	2010	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2010.10.4.314	computer science;machine learning;pattern recognition;data mining;bayesian statistics	AI	13.165116346018568	-29.087987402719495	195800
a5191d2d02a22062ef079a6106af6cf6d26579d1	a topology exploiting genetic algorithm to control dynamic systems	learning algorithm;high dimensionality;vector space;dynamic system;domain knowledge;control system;machine learning;genetic algorithm	Learning to control systems with unknown dynamics is a challenging task for machine learning algorithms. One of the problems that arises here is that the input space is a high dimensional real vector space, while many learning algorithms usually work with a preferably small set of discrete units. Partitioning the input space into a small proper set of subspaces however requires some domain knowledge which is not available to the learning algorithm, while choosing a relatively large set of subspaces makes learning a lot more difficult. In this paper we examine the feasibility for genetic algorithms to exploite the continuity of the input space, such that learning is made more efficient for larger sets of subspaces. Every gene of the genotype represents one subspace, while the alleles specify the action that has to be taken in that part of the input space. The genes are structured on the genotype such that adjacent genes are representatives of neighboring points in the higher dimensional input space. By exploiting this property the GA is capable of finding a solution quite efficiently although the genotype has considerable length.	dynamical system;genetic algorithm	Dirk Thierens;Leo Vercauteren	1990		10.1007/BFb0029739	algorithm design;meta-optimization;genetic algorithm;wake-sleep algorithm;vector space;cultural algorithm;computer science;control system;artificial intelligence;theoretical computer science;dynamical system;machine learning;pattern recognition;fsa-red algorithm;learning classifier system;stability;linde–buzo–gray algorithm;active learning;domain knowledge;population-based incremental learning;generalization error	Robotics	15.474462861499166	-24.405531029480972	196068
0dbc5f2c543504675e4f62f6d96821fc927e0699	"""recognition system of pattern similarity in time series signals using neural network being composed of """"biochemical neurons"""""""			artificial neural network;time series	Kouji Tanaka;Tatsuya Sekiguchi;Kumiko Kawamoto;Masahiro Okamoto	1998				ML	12.255729922217697	-27.389668326949277	196145
15a10509828fde6a175474d34a09712c3496bf42	a spiking neural network based autonomous reinforcement learning model and its application in decision making		In this paper, we propose an autonomous spiking neural network model for decision making. The model is an expansion of the basal ganglia circuitry with automatic environment perception, which constructs environmental states automatically from image inputs. The work in this paper has the following contributions: (1) In our model, the simplified Hodgkin-Huxley computing model is developed to achieve calculation efficiency closed to the LIF model and is used to obtain and test the ionic level properties in cognition. (2) A spike based motion perception mechanism is proposed to extract key elements for learning process from raw pixels without large amount of training. We apply our model in the “flappy bird” game and it play well after dozens of trainings. The model gets similar learning performance with human at the start of training. Besides, our model simulates cognitive defects when blocking some of sodium or potassium ion channels in the Hodgkin-Huxley model and this is an exploration of cognition deep into ionic level.		Guixiang Wang;Yi Zeng;Bo Xu	2016		10.1007/978-3-319-49685-6_12	artificial intelligence;machine learning;learning classifier system	AI	10.036611024162475	-27.75016231492865	196287
e0b02fec5323946c0dc4d8253e1f9b9586a30b9d	one-to-many association ability of chaotic quaternionic multidirectional associative memory		In this paper, we investigate one-to-many association ability of multi-valued patterns in the Chaotic Quaternionic Multidirectional Associative Memory (CQMAM). The CQMAM is based on the Multi- directional Associative Memory and composed of quaternionic neurons and chaotic quaternionic neurons, it can realize one-to-many associations of M-tuple multi-valued patterns. Although the conventional Chaotic Complex-valued Multidirectional Associative Memory with variable scal- ing factor (CCMAM) can realize one-to-many associations of M-tuple multi-valued patterns, the one-to-many association ability of the CQ- MAM is better than that of the conventional CCMAM.	content-addressable memory;one-to-many (data model)	Takumi Okutsu;Yuko Osana	2014		10.1007/978-3-319-12637-1_14	arithmetic;artificial intelligence;machine learning;mathematics;bidirectional associative memory	Robotics	12.941550122101878	-29.07841633180759	197018
85b19756128a28b8ec6ba7233b513d73d076aa6c	on retrieval performance of associative memory by complex-valued synergetic computer	image recognition;neural nets;neural nets content addressable storage image retrieval;computational method;neural net work;information processing;complex valued neural networks associative memory complex valued synergetic computer image retrievals;pattern recognition;associative memory;self organization;prototypes image restoration noise computers image recognition pattern recognition neural networks;content addressable storage;chemical reaction;independent component;neural network;image retrieval	Properties and performances of associative memories, based on Complex-valued Synergetic Computer (CVSC), are explored in this paper. All the parameters of CVSC are encoded by complex values. CVSC is extended from the conventional Synergetic Computer (RVSC) in which the parameters are real values. Performances of associative memories in CVSC are investigated through a problem of image retrievals where the input images are partially occluded or noise-affected. From the experimental results concerning the retrieval performances related to various sizes of images and different levels of defectiveness of input images, we found that CVSC outperforms RVSC.	content-addressable memory;grayscale;numerical analysis;performance;prototype;simulation;synergetics (fuller);synergetics (haken);synergy	Masaaki Kimura;Teijiro Isokawa;Haruhiko Nishimura;Nobuyuki Matsui	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033383	computer vision;self-organization;chemical reaction;computer science;artificial intelligence;machine learning;artificial neural network	Vision	13.373181339812248	-29.403131572122952	197176
d90bb96b81d53fa58673fee8948e6e389d5896ce	neural network with multiple-valued activation function	neural network;multiple-valued activation function		activation function;artificial neural network	Zhong-yu Chen	1996				AI	13.144506168678813	-27.236089176900244	197254
ed0a02ef8f79e04af9e613933567c439d5e61a2d	performance improvement of backpropagation algorithm by automatic activation function gain tuning using fuzzy logic	gain tuning;activation function;backpropagation;fuzzy logic;performance improvement;function approximation;backpropagation algorithm;pattern recognition;fuzzy logic system;simulation study;neural network	We propose a method for improving the performance of the backpropagation algorithm using a fuzzy logic system for automatically tuning the activation function gain. Instead of a fixed activation function gain, the fuzzy logic system is used to dynamically adjust the gain, based upon a set of problem domain heuristics derived from a preliminary simulation study. The efficacy of the proposed method is verified by means of simulations on a parity problem, a function approximation problem, and a pattern recognition problem. The results show that the proposed method improves considerably on the performance of the general backpropagation algorithm, including when using momentum.	activation function;algorithm;backpropagation;fuzzy logic	Ki Hwan Eom;Kyung Kwon Jung;Harsha Sirisena	2003	Neurocomputing	10.1016/S0925-2312(02)00576-3	computer science;fuzzy number;backpropagation;machine learning;control theory;mathematics;artificial neural network	AI	15.766001400433819	-29.235989505802937	197308
285af2e6397630822287a12bfd7e3d68bf295ca3	associative memory design via path embedding into a graph	busqueda informacion;tecnologia electronica telecomunicaciones;memoire associative;computacion informatica;camino grafo;almacenamiento informacion;hopfield network;point equilibre;graph path;information retrieval;hopfield neural nets;equilibrium point;hopfield neural network;punto equilibrio;information storage;recherche information;reseau neuronal hopfield;ciencias basicas y experimentales;chemin graphe;associative memory;memoria asociativa;stockage information;reseau neuronal;tecnologias;grupo a;information storage and retrieval;red neuronal;neural network	A graph theoretical procedure for storing a set of n-dimensional binary vectors as asymptotically stable equilibrium points of a discrete Hopfield neural network is presented. The method gives an auto-associative memory which stores an arbitrary memory set completely. Spurious memories might occur only in a small neighborhood of the original memory vectors, so cause small errors.	artificial neural network;autoassociative memory;content-addressable memory;graph theory;hopfield network	Mehmet Kerem Müezzinoglu;Cüneyt Güzelis	2004	Neural Processing Letters	10.1023/B:NEPL.0000035601.72544.61	equilibrium point;computer science;artificial intelligence;machine learning;hopfield network;artificial neural network;algorithm	ML	13.200286725585663	-30.184715455949288	198786
f19b99c04b09ab5d45040cedaa3591af6ac674d9	recurrent neural networks and robust time series prediction	feedforward neural network;puget power electric demand time series;prediccion;filtering;feedforward neural networks;filtrage;learning algorithm;neural networks;etude theorique;outlier filtering;algoritmo recursivo;filtrado;least square method;algorithme apprentissage;time series;load forecasting;experimental result;filtering and prediction theory;recurrent neural networks robustness autoregressive processes neural networks predictive models parameter estimation feedforward neural networks filtering algorithms least squares methods load forecasting;dynamical system;stochastic processes filtering and prediction theory recurrent neural nets time series learning artificial intelligence parameter estimation;systeme dynamique;narma p q model;moving average;algorithme recursif;filtering algorithms;recurrent network;stochastic processes;autoregressive processes;robustesse;serie temporelle;robust time series prediction;estudio teorico;least squares estimate;resultado experimental;serie temporal;robustness;predictive models;recursive algorithm;recurrent neural nets;recurrent neural networks;recurrent neural network;synthetic data;parameter estimation;theoretical study;neural network model;sistema dinamico;learning artificial intelligence;reseau neuronal;narma p q model recurrent neural networks robust time series prediction outlier filtering parameter estimation puget power electric demand time series;resultat experimental;prediction;red neuronal;least squares methods;time series prediction;neural network;robustez	We propose a robust learning algorithm and apply it to recurrent neural networks. This algorithm is based on filtering outliers from the data and then estimating parameters from the filtered data. The filtering removes outliers from both the target function and the inputs of the neural network. The filtering is soft in that some outliers are neither completely rejected nor accepted. To show the need for robust recurrent networks, we compare the predictive ability of least squares estimated recurrent networks on synthetic data and on the Puget Power Electric Demand time series. These investigations result in a class of recurrent neural networks, NARMA(p,q), which show advantages over feedforward neural networks for time series with a moving average component. Conventional least squares methods of fitting NARMA(p,q) neural network models are shown to suffer a lack of robustness towards outliers. This sensitivity to outliers is demonstrated on both the synthetic and real data sets. Filtering the Puget Power Electric Demand time series is shown to automatically remove the outliers due to holidays. Neural networks trained on filtered data are then shown to give better predictions than neural networks trained on unfiltered time series.	algorithm;biological neural networks;estimated;estimation theory;feedforward neural network;holidays;least squares;recurrent neural network;synthetic data;synthetic intelligence;time series	Jerome T. Connor;R. Douglas Martin;Les E. Atlas	1994	IEEE transactions on neural networks	10.1109/72.279188	econometrics;feedforward neural network;computer science;artificial intelligence;recurrent neural network;machine learning;time series;time delay neural network;artificial neural network;statistics	ML	12.024181818633448	-30.183076217512642	198879
3fa43b4301245d77fc0942cab24612612e0bdb08	the design of self-organizing fuzzy neural networks based on ga-ecpso and mbp	learning process;fuzzy neural network;fuzzy neural nets;mean shift;backpropagation;satisfiability;simulation experiment;particle swarm optimizer;hybrid learning;self organization;genetic algorithm;genetic algorithms;near optimal parameters solution self organizing fuzzy neural networks ga ecpso mbp hybrid learning algorithm mean shift clustering mean firing strength optimal network structure genetic algorithm enhancing chaotic particle swarm optimization modified back propagation;network structure;particle swarm optimisation backpropagation fuzzy neural nets genetic algorithms;fuzzy neural networks fuzzy sets clustering algorithms fuzzy systems training data partitioning algorithms algorithm design and analysis genetic algorithms particle swarm optimization neural networks;particle swarm optimisation;back propagation	A novel hybrid learning algorithm which automates the design of the FNNs is proposed in this paper. It is based on two-stage learning process. First, mean shift clustering (MSC) and mean firing strength (MFS) are combined to identify the structure. The MSC is used to generate the initial network structure and parameters of each neuron and MFS refines the initial network to produce the optimal network structure. Next, genetic algorithm enhancing chaotic particle swarm optimization (GA-ECPSO) and modified back-propagation (MBP) are proposed to learn the free parameters. The GA-ECPSO is used to seek the near-optimal parameters solution and MBP continues the learning process until the terminal condition is satisfied. The simulation experiment demonstrates the superior performance of the algorithm.	artificial neural network;backpropagation;cluster analysis;genetic algorithm;mathematical optimization;mean shift;mean squared error;million book project;moose file system;neuron;offline learning;online and offline;online machine learning;organizing (structure);particle swarm optimization;self-organization;simulation;software propagation;software release life cycle	Liang Zhao;Fei-Yue Wang	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413794	mathematical optimization;genetic algorithm;computer science;artificial intelligence;backpropagation;machine learning;artificial neural network	Robotics	13.018021464866898	-24.539841343867987	199220
c5a5550c59c6a2b0918aad2132d67abeea5312e1	recognition of spatiotemporal patterns using a nonmonotone neural network with hidden neurons	neural network	It has been shown that a nonmonotone neural network model can recognize spatiotemporal patterns without expanding them into spatial patterns. We improve the recognition ability of this model by introducing hidden neurons. We also show a simple method of training the hidden neurons. Computer simulation shows that this model can recognize complicated spatiotemporal patterns.	artificial neural network;computer simulation;network model;neuron;spatiotemporal pattern	Satoshi Murakami;Masahiko Morita;Naoto Sakamoto	1998			machine learning;artificial intelligence;time delay neural network;artificial neural network;pattern recognition;spiking neural network;recurrent neural network;echo state network;computer science	ML	13.346731290595903	-27.635511755269093	199879
5f74742f73ee6da22ddf157cd99e27eb690ee6d0	pipelined chebyshev functional link artificial recurrent neural network for nonlinear adaptive filter	nonlinear filters;pipelined recurrent neural network prnn;chebyshev filters;chaotic time series prediction pipelined chebyshev functional link artificial recurrent neural network nonlinear adaptive filter modification real time recurrent learning algorithm computational complexity nonlinear functional expansion nonlinear colored signal prediction nonstationary speech signal prediction;nonlinear functional expansion;speech processing;nonlinear colored signal prediction;time series adaptive filters chebyshev filters computational complexity learning artificial intelligence nonlinear filters recurrent neural nets signal processing speech processing;chaotic time series;recurrent neural network rnn adaptive filter chebyshev functional link artificial neural network cflann pipelined architecture pipelined recurrent neural network prnn;time series;computer architecture;chebyshev functional link artificial neural network cflann;adaptive filters;first order;nonstationary speech signal prediction;chebyshev approximation recurrent neural networks adaptive filters pipeline processing parallel processing computational efficiency computer architecture costs computational complexity computer simulation;computational complexity;chaotic time series prediction;signal processing;pipelined chebyshev functional link artificial recurrent neural network;recurrent neural nets;recurrent neural networks;recurrent neural network;learning artificial intelligence;chebyshev approximation;computational efficiency;computer simulation;real time recurrent learning;adaptive filter;nonlinear adaptive filter;parallel processing;modification real time recurrent learning algorithm;pipeline processing;artificial neural network;pipelined architecture;recurrent neural network rnn	A novel nonlinear adaptive filter with pipelined Chebyshev functional link artificial recurrent neural network (PCFLARNN) is presented in this paper, which uses a modification real-time recurrent learning algorithm. The PCFLARNN consists of a number of simple small-scale Chebyshev functional link artificial recurrent neural network (CFLARNN) modules. Compared to the standard recurrent neural network (RNN), those modules of PCFLARNN can simultaneously be performed in a pipelined parallelism fashion, and this would lead to a significant improvement in its total computational efficiency. Furthermore, contrasted with the architecture of a pipelined RNN (PRNN), each module of PCFLARNN is a CFLARNN whose nonlinearity is introduced by enhancing the input pattern with Chebyshev functional expansion, whereas the RNN of each module in PRNN utilizing linear input and first-order recurrent term only fails to utilize the high-order terms of inputs. Therefore, the performance of PCFLARNN can further be improved at the cost of a slightly increased computational complexity. In addition, due to the introduced nonlinear functional expansion of each module in PRNN, the number of input signals can be reduced. Computer simulations have demonstrated that the proposed filter performs better than PRNN and RNN for nonlinear colored signal prediction, nonstationary speech signal prediction, and chaotic time series prediction.	adaptive filter;algorithm;artificial neural network;biological neural networks;computation;computational complexity theory;computer simulation;first-order reduction;nonlinear programming;nonlinear system;parallel computing;random neural network;real-time clock;recurrent neural network;reduction (complexity);time series	Haiquan Zhao;Jiashu Zhang	2010	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2009.2024313	computer simulation;adaptive filter;parallel processing;computer science;artificial intelligence;recurrent neural network;theoretical computer science;machine learning;signal processing;speech processing;artificial neural network	ML	17.15974629886993	-25.951535310864962	199955
