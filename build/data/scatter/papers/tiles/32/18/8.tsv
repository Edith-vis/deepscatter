id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
8daeead1d34f9139a3b5f1911ec9e6914d9d02f1	supervised item response models for informative prediction		Supporting human decision-making is a major goal of data mining. The more decision-making is critical, the more interpretability is required in the predictive model. This paper proposes a new framework to build a fully interpretable predictive model for questionnaire data, while maintaining a reasonable prediction accuracy with regard to the final outcome. Such a model has applications in project risk assessment, in healthcare, in social studies, and, presumably, in any real-world application that relies on questionnaire data for informative and accurate prediction. Our framework is inspired by models in item response theory (IRT), which were originally developed in psychometrics with applications to standardized academic tests. We extend these models, which are essentially unsupervised, to the supervised setting. For model estimation, we introduce a new iterative algorithm by combining Gauss–Hermite quadrature with an expectation–maximization algorithm. The learned probabilistic model is linked to the metric learning framework for informative and accurate prediction. The model is validated by three real-world data sets: Two are from information technology project failure prediction and the other is an international social survey about people’s happiness. To the best of our knowledge, this is the first work that leverages the IRT framework to provide informative and accurate prediction on ordinal questionnaire data.	data mining;expectation–maximization algorithm;gauss–hermite quadrature;gene prediction;information;item response theory;iterative method;ordinal data;predictive modelling;risk assessment;statistical model;unsupervised learning	Tsuyoshi Idé;Amit Dhurandhar	2016	Knowledge and Information Systems	10.1007/s10115-016-0976-2	econometrics;computer science;data science;machine learning;data mining;statistics	ML	14.139844462396555	-49.25339590549017	170994
3f307d5b29f0cfd4e10aabec91b56fc3bae95443	semi-random subspace method for writeprint identification	diversity;writeprint;individual author feature set iafs;principal component analysis pca;random subspace method rsm	The anonymous nature of online messages distribution causes a series of moral and legal issues. By analyzing identity cues people leave behind their texts, i.e., writeprint, potential authors can be identified individually. But writeprint identification is a difficult learning task, because of the high redundancy in stylistic feature set and high similarity of some authors' writing-style. In this paper, we propose a novel method, called semi-random subspace (Semi-RS), to simultaneously address the two problems. Different from the conventional random subspace method (RSM) which samples features from the whole feature set in a completely random way, the proposed Semi-RS randomly samples features on each individual-author feature set (IAFS) partitioned from the whole feature set. More specifically, we first divide the whole feature set into several IAFSs in a deterministic way, then construct a set of base classifiers on different randomly sampled feature sets from each IAFS, and finally combine all base classifiers for the final decision. Experimental results on the benchmark dataset demonstrate the effectiveness of the proposed method which improves previously reported results. In addition, we analyze the diversity of algorithm, reveals that Semi-RS constructs more diverse base classifiers than conventional RSMs.	random subspace method	Zhi Liu;Zongkai Yang;Sanya Liu;Yinghui Shi	2013	Neurocomputing	10.1016/j.neucom.2012.11.015	random subspace method;computer science;machine learning;pattern recognition;data mining;statistics	Vision	16.708307331433698	-50.625511177568306	171523
850a5a15558188f1f41e525e64a2fade43cfb10c	context-sensitive mtl networks for machine lifelong learning	neural network;lifelong learning	Context-sensitive Multiple Task Learning, or csMTL, is presented as a method of inductive transfer that uses a single output neural network and additional contextual inputs for learning multiple tasks. The csMTL method is tested on three task domains and shown to produce hypotheses for a primary task that are significantly better than standard MTL hypotheses when learning in the presence of related and unrelated tasks. A new measure of task relatedness, based on the context input weights, is shown to have promise. The paper also outlines a machine lifelong learning system that uses csMTL for sequentially learning multiple tasks. The approach satisfies a number of important requirements for knowledge retention and inductive transfer including the elimination of redundant outputs, representational transfer for rapid but effective short-term learning and functional transfer via task rehearsal for long-term consolidation.	artificial neural network;context-sensitive grammar;requirement;semiconductor consolidation	Daniel L. Silver;Ryan Poirier	2007			natural language processing;artificial intelligence;machine learning;artificial neural network;inductive transfer;computer science;lifelong learning	ML	16.54477103208146	-48.34076333291788	171727
1029cf9b79ed780b13f1af9a5c56396cd58d821c	how to select an optimal neural model of chemical reactivity?	mlp;grnn;neural model;classification;data representation;regression;biological activity;general regression neural network;performance model;ethylbenzene dehydrogenase;network optimization;linear networks;neural network	The paper aims at methodological studies on selection of optimal neural network that performs modeling of chemical reactivity of a given group of chemical compounds. The problem (prediction of biological activity in enzymatic reaction catalyzed by ethylbenzene dehydrogenase) is taken as a case study for assessment of various types of neural networks. The main goal of the study is to select the best type of the network, optimal dimension of the layers, proper parameters of the network as well as the optimal form of data representation for purpose of neural-based modeling of complex empirical data. Various approaches (linear networks, regression and classification multiple layer perceptrons, generalized regression neural networks) are compared and tested.		Maciej Szaleniec;Ryszard Tadeusiewicz;Malgorzata Witko	2008	Neurocomputing	10.1016/j.neucom.2008.01.003	probabilistic neural network;regression;types of artificial neural networks;biological classification;computer science;artificial intelligence;machine learning;biological activity;data mining;time delay neural network;external data representation;artificial neural network	NLP	11.279471318304816	-49.95218674121746	172155
1a099d4108ea5c25e874ab16a1beefa565faa77c	automatic feature selection in the sopfs dissolution profiles prediction problem		This work addressed the problem of dimensionality reduction in the drug dissolution profile prediction task. The learning problem is assumed as a multi-output learning task, since dissolution profiles are recorded in nonuniform sampling times, which avoid the use of basic function-on-scalar regression approaches. Ensemblebased tree methods are used for prediction, and also for the selection of the most relevant features, because they are able to deal with high dimensional feature spaces, when the number of training samples is small. All the drugs considered corresponds to rapid release solid oral pharmaceutical forms. Six different feature selection schemes were tested, including sequential feature selection and genetic algorithms, along with a feature scoring procedure, which was proposed in order to get a consensus about the best subset of variables. The performance was evaluated in terms of the similitude factor used in the drug industry for dissolution profile comparison. The feature selection methods were able to reduce the dimensionality of the feature space in 79.2%, without loss in the performance of the prediction system. The results confirm that in the dissolution profile prediction problem, especially for different solid oral pharmaceutical forms, variables from different components and phases of the drug development must be considered.	curse of dimensionality;dimensionality reduction;feature extraction;feature selection;feature vector;gene prediction;genetic algorithm;machine learning;nonuniform sampling;sampling (signal processing);test set	J. E. Salazar Jiménez;J. D. Sánchez Carvajal;B. Quiros-Gómez;Julián D. Arias-Londoño	2017		10.5220/0006141800520058	machine learning;pattern recognition	AI	10.602013963694288	-46.09421449645474	175567
0994ba289db89adb539ae046a21db39787250254	selecting features from multiple feature sets for svm committee-based screening of human larynx	genetic search;learning process;design process;human computer interaction;manniska datorinteraktion interaktionsdesign;statistical significance;genetics;classification committee;variable selection;human larynx;feature selection;support vector machine;classification accuracy	This paper is concerned with a two stage procedure for designing a sequential SVM committee and selecting features for the committee from multiple feature sets. It is assumed that features of one type comprise one feature set. Selection of both features and hyper-parameters of SVM classifiers comprising the committee is integrated into one learning process based on genetic search. The designing process focuses on feature selection for pair-wise classification implemented by the SVM. In the first stage, a series of pair-wise SVM are designed starting from the original feature sets as well as from sets created by simple random selection from the original ones. Outputs of the SVM are then converted into probabilities and used as inputs to the second stage SVM. When testing the technique in a three-class classification problem of voice data, a statistically significant improvement in classification accuracy was obtained if compared to parallel committees. The number of feature types and features selected for the pair-wise classification are class specific.	support vector machine	Antanas Verikas;Adas Gelzinis;Marina Kovalenko;Marija Bacauskiene	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.03.025	support vector machine;design process;computer science;machine learning;pattern recognition;data mining;statistical significance;feature selection;feature	ML	10.13427657591727	-45.593827892910255	175648
01f2ca640c39685efb76a9883c619d13015c1bde	network elucidation template: a framework for human-guided network inference	optimal solution;network inference;human guided search;visualization;levels of abstraction;optimization;network flow	Network elucidation is the problem of inferring all parameters of a network from a subset of those parameters. We introduce the Network Elucidation Template (NET), which provides a framework upon which algorithms for such problems can be built. NET algorithms take advantage of novel methods for collaboration between human operators and computers. They use visualizations of the peculiar structures that appear in optimal solutions to aid the parameter search. By design, NET is at a high enough level of abstraction to describe a class of algorithms, as opposed to a single algorithm. Given a problem, and the structure of that problem, an effective instantiation of the template into an algorithm can be created. We describe one such instantiation: using a network flow framework to implement a NET algorithm for uncovering smuggling networks; as well as the general template. 2010 Elsevier Ltd. All rights reserved.	algorithm;complex systems;computation;computer;computer science;consciousness;consistency model;explicit modeling;flow network;interactivity;iteration;jan dietz;jargon;linear programming;machine learning;mathematical optimization;operations research;optimization problem;polyhedron;sampling (signal processing);statistical model;systems biology;universal instantiation;user interface;web service	Leo Lopes;Jay Konieczka;Victor Foulk;Parker Antin	2010	Computers & Industrial Engineering	10.1016/j.cie.2010.01.013	mathematical optimization;flow network;visualization;network formation;computer science;theoretical computer science;machine learning;data mining;mathematics;network simulation	AI	13.346773656873733	-50.933402821362	175832
9f1a529c8d23b9425fbbf23626a4946d85bef951	a behavior analysis method towards product quality management		Quality management is the basic activity in industrial production. Assuring the authenticity of testing datasets is extremely important for the quality of products. Many visual tools or association analysis methods are used to judge the authenticity of testing data, but it could not precisely capture behavior pattern and time consuming. In this paper, we propose a complete framework to excavate the features of testing datasets and analyze the testing behavior. This framework uses min-max normalization method to pre-process datasets and optimized k-means algorithm to label the training datasets, then SVM algorithm is applied to verify the accuracy of our framework. Using this framework, we can get the features of dataset and homologous behavior model to distinguish the quality of datasets. Some experiments are carried to measure the complete framework and we use various visual formats to show these results and to verify our method.		Congcong Ye;Chun Li;Guoqiang Li;Lihong Jiang;Hongming Cai	2017		10.1007/978-3-319-94845-4_2	normalization (statistics);data mining;support vector machine;quality management;industrial production;behavioral pattern;test data;computer science	DB	12.025775989681405	-46.82816816229912	175938
4ce777b863f7656a928d2d7831247a699213f264	multi-centrality graph spectral decompositions and their application to cyber intrusion detection	attack classification multicentrality graph spectral decomposition cyber intrusion detection principal component analysis graph walk statistics centrality measures graph distances reference nodes single graph analysis multicentrality graph pca method mc gpca dictionary learning method multicentrality graph dictionary learning mc gdl anomalous connectivity pattern;principal component analysis dictionaries feature extraction matrix decomposition laplace equations sparse matrices intrusion detection;spectral analysis graph theory principal component analysis security of data signal processing	Many modern datasets can be represented as graphs and hence spectral decompositions such as graph principal component analysis (PCA) can be useful. Distinct from previous graph decomposition approaches based on subspace projection of a single topological feature, e.g., the Fiedler vector of centered graph adjacency matrix (graph Laplacian), we propose spectral decomposition approaches to graph PCA and graph dictionary learning that integrate multiple features, including graph walk statistics, centrality measures and graph distances to reference nodes. In this paper we propose a new PCA method for single graph analysis, called multi-centrality graph PCA (MC-GPCA), and a new dictionary learning method for ensembles of graphs, called multi-centrality graph dictionary learning (MC-GDL), both based on spectral decomposition of multi-centrality matrices. As an application to cyber intrusion detection, MC-GPCA can be an effective indicator of anomalous connectivity pattern and MC-GDL can provide discriminative basis for attack classification.	adjacency matrix;algebraic connectivity;centrality;dictionary;intrusion detection system;laplacian matrix;machine learning;principal component analysis;statistical classification	Pin-Yu Chen;Sutanay Choudhury;Alfred O. Hero	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472539	graph power;graph energy;graph bandwidth;null graph;graph property;clique-width;theoretical computer science;machine learning;pattern recognition;mathematics;voltage graph;distance-hereditary graph;graph;quartic graph;strength of a graph;adjacency matrix	ML	16.576886257505485	-47.08786862293524	176622
2bf6cc4780ac3327dc1a653d2b4b64bd99c2439a	empirical validation of structural metrics for predicting understandability of conceptual schemas for data warehouse		Data warehouse (DW) quality depends on its data models (conceptual, logical and physical model). Multidimensional (MD) modeling has been widely recognized as the backbone of data modeling for DW. Recently, some of the authors have proposed a set of structural metrics to assess quality of MD conceptual models. They have found the significant relationship between metrics and understandability of DW conceptual schemas using various correlation analysis techniques such as Spearman’s, Pearson etc. However, advanced statistical and machine learning methods have not been used to predict effect of each metric on understandability. In this paper, our focus is on predicting the effect of structural metrics on understandability of conceptual schemas using (i) statistical method (logistic regression analysis) that include univariate and multivariate analysis, (ii) machine learning methods (Decision Trees, Naive Bayesian Classifier) and (iii) compare the performance of these statistical and machine learning methods. The results obtained show that some of the metrics individually have a significant effect on the understandability of MD conceptual schema. Further, few of the metrics have a significant combined effect on understandability of conceptual schema. The results also show that the performance of Naive Bayesian Classifier prediction method is better than logistic regression analysis and Decision Trees methods.	bayesian network;conceptual schema;data model;data modeling;database schema;decision tree;dreamwidth;internet backbone;lr parser;logistic regression;machine learning;molecular dynamics;nc (complexity);naive bayes classifier;nethack;nippon decimal classification;random forest;receiver operating characteristic;role-based collaboration;self-replicating machine;sensitivity and specificity;support vector machine	Manoj Kumar;Anjana Gosain;Yogesh Singh	2014	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-013-0159-4	computer science;machine learning;pattern recognition;data mining	DB	11.129350825361136	-49.53548050182417	177402
0863961f4cc7db72822ac78518594a87540166d8	ensemble of dissimilarity based classifiers for cancerous samples classification	euclidean distance;linear discriminate analysis;gene expression;k nearest neighbor;difference set;support vector machine;dna microarray	DNA Microarray technology allow us to identify cancer samples considering the gene expression levels across a collection of related samples. Several classifiers such as Support Vector Machines (SVM), k Nearest Neighbors (k-NN) or Diagonal Linear Discriminant Analysis (DLDA) have been applied to this problem. However, they are usually based on Euclidean distances that fail to reflect accurately the sample proximities. Several classifiers have been extended to work with non-Euclidean dissimilarities although none outperforms the others because they misclassify a different set of patterns. In this paper, we combine different kind of dissimilarity based classifiers to reduce the misclassification errors. The diversity among classifiers is induced considering a set of complementary dissimilarities for three different type of models. The experimental results suggest that the algorithm proposed helps to improve classifiers based on a single dissimilarity and a widely used combination strategy such as Bagging.	bootstrap aggregating;euclidean distance;k-nearest neighbors algorithm;linear discriminant analysis;microarray;support vector machine	Ángela Blanco;Manuel Martín-Merino;Javier De Las Rivas	2007		10.1007/978-3-540-75286-8_18	random subspace method;support vector machine;gene expression;dna microarray;computer science;machine learning;pattern recognition;data mining;euclidean distance;mathematics;k-nearest neighbors algorithm;genetics;difference set	ML	11.298581729257332	-45.14373521723333	177757
933add53eaabf04a73a38c95b39e380016734eaa	analyzing pathways using sat-based approaches	biological process	A network of reactions is a commonly used paradigm for representing knowledge about a biological process. How does one understand such generic networks and answer queries using them? In this paper, we present a novel approach based on translation of generic reaction networks to Boolean weighted MaxSAT. The Boolean weighted MaxSAT instance is generated by encoding the equilibrium configurations of a reaction network by weighted boolean clauses. The important feature of this translation is that it uses reactions, rather than the species, as the boolean variables. Existing weighted MaxSAT solvers are used to solve the generated instances and find equilibrium configurations. This method of analyzing reaction networks is generic, flexible and scales to large models of reaction networks. We present a few case studies to validate our claims.	biological network;maximum satisfiability problem;programming paradigm;semantic network;steady state	Ashish Tiwari;Carolyn L. Talcott;Merrill Knapp;Patrick Lincoln;Keith Laderoute	2007		10.1007/978-3-540-73433-8_12	biology;biochemistry;combinatorics;discrete mathematics;mathematics;biological process;genetics;algorithm	AI	12.213711249960248	-51.6980185443847	178649
733e2a495a638b5df538fdfefe492ec3dd08208a	noise robust classification based on spread spectrum	recurrent neural network;spread spectrum;feature space;cross validation;feature vector	In this paper we develop a robust classification mechanism based on a connectionist model in order to learn and classify objects from arbitrary feature spaces. Thereby a joint approach of recurrent neural networks and spread spectrum symbol encoding is implemented in order to classify any kind of objects that can be represented by feature vectors. Our main contribution is to adapt the spread spectrum method from signal transmission technology to classification of feature vectors, which are encoded for neural processing by means of unique spreading sequences. The idea behind this data spreading approach is related to the field of errorcorrecting output coding, but is furthermore characterized by a despreading mechanism that results in high classification accuracy and robustness against noisy or incomplete data. We applied our technique to four publicly available classification benchmarks, which stem from three different domains namely biology, geography and medicine. In the case of the MUSK2 molecule dataset (biology), ten-fold cross-validation of our technique revealed a classification accuracy of 97.7% at maximum, which is about 7% better than any published algorithm. In presence of a noise level of up to 25.0%, still an accuracy of 75.9% was achieved.	algorithm;artificial neural network;benchmark (computing);bootstrap aggregating;business object;connectionism;cross-validation (statistics);domain-driven design;feature vector;image noise;information processing;multiplexing;noise (electronics);random neural network;recurrent neural network;sparse matrix	Jörn David	2009		10.1137/1.9781611972795.57	t-cell receptor;spread spectrum;dna;pattern recognition;artificial intelligence;computer science;machine learning;gene;plasmid;restriction site;bacteria;recombinant dna	ML	12.891312157916008	-49.88257125003561	181463
8b2bf7b66533915995a44cce7c74ad4e051bd737	comparing dynamic pso algorithms for adapting classifier ensembles in video-based face recognition	heuristic algorithms face recognition optimization training biological system modeling particle swarm optimization feature extraction;dynamic programming;video streaming;classifier ensemble;biometrics access control;image classification;greedy algorithms;fuzzy set theory;face recognition;video streaming dynamic algorithms pso classifier ensembles face recognition biometric models particle swarm optimization supervised incremental learning data acquisition adaptive classification systems fuzzy artmap classifiers greedy search process;video streaming biometrics access control data acquisition dynamic programming face recognition fuzzy set theory greedy algorithms image classification learning artificial intelligence particle swarm optimisation;learning artificial intelligence;particle swarm optimisation;data acquisition	Biometric models are typically designed a priori using limited number of samples acquired from complex environments that change in time during operations. Therefore, these models are often poor representatives of the biometric trait to be recognized. To circumvent this problem, ensemble of classifiers can be used to integrate solutions obtained from multiple diverse classifiers. In this paper, two dynamic particle swarm optimization (DPSO) algorithms are compared for the evolution of classifier ensembles during supervised incremental learning of newly-acquired data samples in video-based face recognition. Using the properties of these population-based optimization algorithms, an incremental DPSO learning strategy for adaptive classification systems (ACSs) is employed to evolve a pool of fuzzy ARTMAP classifiers while an heterogeneous ensemble is selected through a greedy search process that seeks to maximize both performance and diversity. The performance of dynamic niching PSO (DNPSO) and speciation PSO (SPSO) algorithms is assessed in terms of classification rate, resource requirements and diversity for different incremental learning scenarios of new data blocks extracted from real-world video streams. Simulation results indicate that both DPSO algorithms can efficiently create accurate ensembles while reducing computational complexity. In addition, directly selecting representative subswarm particles to form diversified classifier ensembles significantly reduces the computational complexity.	biometrics;computational complexity theory;facial recognition system;genetic algorithm;greedy algorithm;mathematical optimization;neural ensemble;particle swarm optimization;requirement;simulation;streaming media	Jean-François Connolly;Eric Granger;Robert Sabourin	2011	2011 IEEE Workshop on Computational Intelligence in Biometrics and Identity Management (CIBIM)	10.1109/CIBIM.2011.5949226	computer science;machine learning;pattern recognition;data mining	Vision	13.744233211948401	-45.924541731238826	181930
a7daa84030b463f089f72340519080daa430c0e5	data dimensionality reduction with application to simplifying rbf network structure and improving classification performance	separability correlation measure;neural networks;rbf neural networks;performance index;radial basis function networks neural networks feature extraction computer networks data mining linear discriminant analysis principal component analysis mutual information entropy data preprocessing;complexity;indexing terms;data mining;journal article;computer networks;drntu engineering electrical and electronic engineering;structural complexity;overlaps;radial basis function networks;radial basis function;scm;feature extraction;rbf neural network;principal component analysis;high dimensional data;pattern classification;mutual information;classifiers;pattern classification radial basis function networks;entropy;data preprocessing;linear discriminant analysis;dimensional reduction;rbf network;computer simulation;structural complexity rbf neural networks scm data dimensionality reduction overlaps classifiers separability correlation measure complexity;data dimensionality reduction	For high dimensional data, if no preprocessing is carried out before inputting patterns to classifiers, the computation required may be too heavy. For example, the number of hidden units of a radial basis function (RBF) neural network can be too large. This is not suitable for some practical applications due to speed and memory constraints. In many cases, some attributes are not relevant to concepts in the data at all. In this paper, we propose a novel separability-correlation measure (SCM) to rank the importance of attributes. According to the attribute ranking results, different attribute subsets are used as inputs to a classifier, such as an RBF neural network. Those attributes that increase the validation error are deemed irrelevant and are deleted. The complexity of the classifier can thus be reduced and its classification performance improved. Computer simulations show that our method for attribute importance ranking leads to smaller attribute subsets with higher accuracies compared with the existing SUD and Relief-F methods. We also propose a modified method for efficient construction of an RBF classifier. In this method we allow for large overlaps between clusters corresponding to the same class label. Our approach significantly reduces the structural complexity of the RBF network and improves the classification performance.	artificial neural network;biological neural networks;computation;computer simulation;dimensionality reduction;linear separability;memory disorders;naive bayes classifier;preprocessor;radial (radio);radial basis function network;relevance;small;spectinomycin	Xiuju Fu;Lipo Wang	2003	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/TSMCB.2003.810911	computer simulation;entropy;structural complexity;radial basis function;process performance index;complexity;index term;feature extraction;computer science;machine learning;pattern recognition;data mining;data pre-processing;mutual information;linear discriminant analysis;artificial neural network;principal component analysis;clustering high-dimensional data	ML	10.537680641602229	-46.54032817120379	184169
30a8a0c8e83b896262179640ac6af1ccf5d1f061	multitask learning and benchmarking with clinical time series data		Health care is one of the most exciting frontiers in data mining and machine learning. Successful adoption of electronic health records (EHRs) created an explosion in digital clinical data available for analysis, but progress in machine learning for healthcare research has been di cult to measure because of the absence of publicly available benchmark data sets. To address this problem, we propose four clinical prediction benchmarks using data derived from the publicly available Medical Information Mart for Intensive Care (MIMIC-III) database. These tasks cover a range of clinical problems including modeling risk of mortality, forecasting length of stay, detecting physiologic decline, and phenotype classi cation. We formulate a heterogeneous multitask problem where the goal is to jointly learn multiple clinically relevant prediction tasks based on the same time series data. To address this problem, we propose a novel recurrent neural network (RNN) architecture that leverages the correlations between the various tasks to learn a better predictive model. We validate the proposed neural architecture on this benchmark, and demonstrate that it outperforms strong baselines, including single task RNNs. ACM Reference format: Hrayr Harutyunyan, Hrant Khachatrian and David C. Kale, Aram Galstyan. 2017. Multitask Learning and Benchmarking with Clinical Time Series Data. In Proceedings of ACM Conference, Washington, DC, USA, July 2017 (Conference’17), 11 pages. DOI: 10.1145/nnnnnnn.nnnnnnn	artificial neural network;benchmark (computing);computer multitasking;data mart;data mining;experiment;long short-term memory;loss function;mimic;machine learning;random neural network;recurrent neural network;self-replication;sensor;time series	Hrayr Harutyunyan;Hrant Khachatrian;David C. Kale;Aram Galstyan	2017	CoRR		computer science;data science;machine learning;data mining	ML	14.5838036923952	-47.99815884917816	191414
4ee45893b3e5af7302e8855e7d28289f21bebc13	invariant verification of nonlinear hybrid automata networks of cardiac cells	verification;invariants;biological networks;hybrid systems	Verification algorithms for networks of nonlinear hybrid automata (HA) can aid us understand and control biological processes such as cardiac arrhythmia, formation of memory, and genetic regulation. We present an algorithm for over-approximating reach sets of networks of nonlinear HA which can be used for sound and relatively complete invariant checking. First, it uses automatically computed input-to-state discrepancy functions for the individual automata modules in the networkA for constructing a low-dimensional modelM. Simulations of both A and M are then used to compute the reach tubes for A. These techniques enable us to handle a challenging verification problem involving a network of cardiac cells, where each cell has four continuous variables and 29 locations. Our prototype tool can check bounded-time invariants for networks with 5 cells (20 continuous variables, 29 locations) typically in less than 15 minutes for up to reasonable time horizons. From the computed reach tubes we can infer biologically relevant properties of the network from a set of initial states.	approximation algorithm;automata theory;computer simulation;discrepancy function;hybrid automaton;invariant (computer science);nonlinear system;prototype	Zhenqi Huang;Chuchu Fan;Alexandru Mereacre;Sayan Mitra;Marta Z. Kwiatkowska	2014		10.1007/978-3-319-08867-9_25	biological network;verification;computer science;artificial intelligence;theoretical computer science;invariant;algorithm;hybrid system	Logic	12.808579654915722	-51.65954841526459	191823
0d073b501b5c6cc16b63588fa544b4a06fe0f5cf	higgs boson discovery with boosted trees	higgs boson;machine learning;gradient boosting	The discovery of the Higgs boson is remarkable for its importance in modern Physics research. The next step for physicists is to discover more about the Higgs boson from the data of the Large Hadron Collider (LHC). A fundamental and challenging task is to extract the signal of Higgs boson from background noises. The machine learning technique is one important component in solving this problem. In this paper, we propose to solve the Higgs boson classification problem with a gradient boosting approach. Our model learns ensemble of boosted trees that makes careful tradeoff between classification error and model complexity. Physical meaningful features are further extracted to improve the classification accuracy. Our final solution obtained an AMS of 3.71885 on the private leaderboard, making us the top 2% in the Higgs boson challenge.	algorithm;boson sampling;feature engineering;gradient boosting;large hadron collider;machine learning;scalability;verilog-ams;xgboost	Tianqi Chen;Tong He	2014			higgs boson;computer science;machine learning;gradient boosting	ML	16.52911799400573	-46.36304809344942	193026
ad7d64c6fd005ca2f95b8c1834fa082d91576293	brain tumor classification based on clustered discrete cosine transform in compressed domain	discrete cosine transform dct;support vector machine svm;magnetic resonance image mri	This study presents a novel method to classify the brain tumors by means of efficient and integrated m ethods so as to increase the classification accuracy. In c onventional systems, the problem being the same to extract the feature sets from the database and classify tumors based on the features sets. The main idea in pletho ra of earlier researches related to any classification me thod is to increase the classification accuracy.The actual need is to achieve a better accuracy in classification, by extracting more relevant feature sets after dime nsionality reduction. There exists a trade-off between accurac y and the number of feature sets. Hence the focus i n this study is to implement Discrete Cosine Transform (DC T) on the brain tumor images for various classes. U sing DCT, by itself, it offers a fair dimension reductio n in feature sets.Later on, sequentially K-means al gorithm is applied on DCT coefficients to cluster the feature sets. These cluster information are considered as r efined feature sets and classified using Support Vector Ma chine (SVM) is proposed in this study. This method of using DCT helps to adjust and vary the performance of classification based on the count of the DCT coefficients taken into account. There exists a goo d demand for an automatic classification of brain t umors which grealtly helps in the process of diagnosis. I n this novel work, an average of 97% and a maximum of 100% classification accuracy has been achieved. Thi s research is basically aiming and opening a new wa y of classification under compressed domain. Hence this s udy may be highly suitable for diagnosing under m obile computing and internet based medical diagnosis.	coefficient;discrete cosine transform;grey goo;k-means clustering;my girlfriend is a cyborg	V. Anitha;S. Murugavalli	2014	JCS	10.3844/jcssp.2014.1908.1916	machine learning;pattern recognition;data mining	ML	11.771963473341378	-45.96170441086295	195884
2cd42edb3dde07493d34d0e03f9c3ddc6225ae92	adaptive multiclass support vector machine for multimodal data analysis		Multimodal data commonly exists in human lives. Early analysis usually concentrates on mining information based on single modality. Recent studies show that learning tasks could be greatly enhanced by analyzing data from the aspect of multimodality. This paper deals with classifying multimodal data comprised of visual and acoustic contents. Different data features are fused under a hierarchical structure to achieve a good semantic understanding. Then, to accomplish accurate classification, an adaptive support vector machine method (ASVM) is proposed. The method is support vector machine with hyperparameters controlled by a novel and efficient artificial bee colony algorithm. First, a micro colony is set as the number of hyperparameters is usually less than 5. Second, one position inheritance based on roulette wheel selection is used. Third, discarded solutions are mutated by position shift operation instead of random reinitialization. The ASVM method is first verified on classical data sets demonstrating the goodness of the proposed method. Then the proposed method is applied on a multimodal data set. Each sample includes both image and audio data features. Experimental results show that the ASVM method is more effective and robust than the compared methods. © 2017 Elsevier Ltd. All rights reserved.	acoustic cryptanalysis;artificial bee colony algorithm;fitness proportionate selection;modality (human–computer interaction);multimodal interaction;support vector machine	Xin Zhang;Xiu Zhang	2017	Pattern Recognition	10.1016/j.patcog.2017.05.006	hyperparameter optimization;support vector machine;machine learning;feature selection;artificial intelligence;pattern recognition;multiclass classification;computer science	AI	12.399950877854893	-45.08567217240984	196763
a5092f944d1fd00d90d082df1efe68dfabf580f6	improved classification of medical data using abductive network committees trained on different feature subsets	heart disease;neural networks;diabetes;abductive networks;network committee;network ensemble;feature selection;classification accuracy;breast cancer;medical diagnosis;neural network	This paper demonstrates the use of abductive network classifier committees trained on different features for improving classification accuracy in medical diagnosis. In an earlier publication, committee members were trained on different subsets of the training set to ensure enough diversity for improved committee performance. In situations characterized by high data dimensionality, i.e. a large number of features and a relatively few training examples, it may be more advantageous to split the feature set rather than the training set. We describe a novel approach for tentatively ranking the features and forming subsets of uniform predictive quality for training individual members. The abductive network training algorithm is used to select optimum predictors from the feature set at various levels of model complexity specified by the user. Using the resulting tentative ranking, the features are grouped into mutually exclusive subsets of approximately equal predictive power for training the members. The approach is demonstrated on three standard medical diagnosis datasets (breast cancer, heart disease, and diabetes). Three-member committees trained on different feature subsets and using simple output combination methods reduce classification errors by up to 20% compared to the best single model developed with the full feature set. Results are compared with those reported previously with members trained through splitting the training set. Training abductive committee members on feature subsets of approximately equal predictive power achieves both diversity and quality for improved committee performance. Ensemble feature subset selection can be performed using GMDH-based learning algorithms. The approach should be advantageous in situations characterized by high data dimensionality.	abductive reasoning;algorithm;approximation;feature selection;group method of data handling;heart diseases;machine learning;mammary neoplasms;subgroup;test set	R. E. Abdel-Aal	2005	Computer methods and programs in biomedicine	10.1016/j.cmpb.2005.08.001	computer science;breast cancer;machine learning;medical diagnosis;pattern recognition;data mining;artificial neural network	ML	10.374297938413447	-45.64888970605276	196882
