id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e39b1f60b22a2da4e1268d61f8fda3ece3b3350f	classification of foreign language mobile learning strategy based on principal component analysis and support vector machine		To improve the classification accuracy of foreign language mobile learning (m-learning) strategies applied by college students, an evaluation model based on principal component analysis (PCA) and support vector machine (SVM) is proposed. PCA was first employed to reduce the dimensionality of an evaluation system of foreign language m-learning strategies and the correlation between the indices in the system was eliminated. The first 5 principal components were extracted and a classification model based on SVM was established by taking the extracted principal components as its inputs. Gaussian radial basis function was adopted as the kernel function and the optimal SVM model was realized by adjusting the parameters C and g. The classification result was compared with those produced by a BP neural network model and a single SVM model. The simulation results prove that the PCA-SVM model has a simpler algorithm, faster calculating speed, higher classification accuracy and better generalization ability.	principal component analysis;support vector machine	Shuai Hu;Yan Gu;Yingxin Cheng	2015		10.1007/978-3-319-38771-0_36	speech recognition;computer science;machine learning;pattern recognition;relevance vector machine;active learning;structured support vector machine	ML	12.84329590109839	-39.562751528067764	171118
2b7a1fe784d0b9d63a7a1502d541759ff6f7f2b4	local decomposition for rare class analysis	supervised learning;support vector;large scale;k means clustering support vector machines;prediction accuracy;support vector machine;rare class analysis;k means clustering;large classes;local clustering	Given its importance, the problem of predicting rare classes in large-scale multi-labeled data sets has attracted great attentions in the literature. However, the rare-class problem remains a critical challenge, because there is no natural way developed for handling imbalanced class distributions. This paper thus fills this crucial void by developing a method for Classification using lOcal clusterinG (COG). Specifically, for a data set with an imbalanced class distribution, we perform clustering within each large class and produce sub-classes with relatively balanced sizes. Then, we apply traditional supervised learning algorithms, such as Support Vector Machines (SVMs), for classification. Indeed, our experimental results on various real-world data sets show that our method produces significantly higher prediction accuracies on rare classes than state-of-the-art methods. Furthermore, we show that COG can also improve the performance of traditional supervised learning algorithms on data sets with balanced class distributions.	algorithm;cluster analysis;clustering coefficient;cog (project);machine learning;nonlinear system;oversampling;sampling (signal processing);supervised learning;support vector machine	Junjie Wu;Hui Xiong;Peng Wu;Jian Chen	2007		10.1145/1281192.1281279	support vector machine;fuzzy clustering;computer science;machine learning;pattern recognition;data mining;mathematics;cluster analysis;supervised learning	ML	13.977932374432378	-41.564169467661124	171499
ad413debc11f4d8dbe4321dfda075d965d84ae65	an ensemble learning framework for credit card fraud detection based on training set partitioning and clustering		The popularity of credit card has greatly facilitated the transactions between merchants and cardholders. However, credit card fraud has been derived, which results in losses of billions of euros every year. In recent years, machine learning and data mining technology have been widely used in fraud detection and achieved favorable performances. Most of these studies use the technology of under-sampling to deal with the high imbalance of credit card data. However, it will potentially discard some relevant training samples which will weaken the ability of the classifier. In this paper, we propose an ensemble learning framework based on training set partitioning and clustering. It turns out that the proposed framework not only ensures the integrity of the sample features, but also solves the high imbalance of the dataset. A main feature of our framework is that every base estimator can be trained in parallel. This improves the efficiency of the framework. We show the effectiveness of our proposed ensemble framework by experimental results on a real credit card transaction dataset.		Hongyu Wang;Ping Zhu;Xueqiang Zou;Sujuan Qin	2018	2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)	10.1109/SmartWorld.2018.00051	distributed computing;estimator;credit card fraud;computer science;data mining;cluster analysis;ensemble learning;popularity;training set;database transaction	Web+IR	14.386707044258383	-40.77035940787196	171530
85b825ef28b733b0f56a4afa46c478bcb74eb138	an empirical study on the effect of imbalanced data on bleeding detection in endoscopic video	support vector machines;training;hemorrhaging;training data;sensitivity;feature extraction;decision trees	In biomedical applications including classification of endoscopic videos, class imbalance is a common problem arising from the significant difference between the prior probabilities of different classes. In this paper, we investigate the performance of different classifiers for varying training data distribution in case of bleeding detection problem through three experiments. In the first experiment, we analyze the classifier performance for different class distribution with a fixed sized training dataset. The experiment provides the indication of the required class distribution for optimum classification performance. In the second and third experiments, we investigate the effect of both training data size and class distribution on the classification performance. From our experiments, we found that a larger dataset with moderate class imbalance yields better classification performance compared to a small dataset with balanced distribution. Ensemble classifiers are more robust to the variation in training dataset compared to single classifier.	class;ensemble learning;experiment;hemorrhage;large;probability;silo (dataset)	Farah Deeba;Shahed Khan Mohammed;Francis Minhthang Bui;Khan A. Wahid	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7591262	support vector machine;training set;sensitivity;feature extraction;computer science;machine learning;decision tree;pattern recognition;data mining;one-class classification	Vision	13.846079018225733	-42.27208264469364	172086
e3b6b0741d59a78a84d846bc4a4863c474cdcaa5	extracting initial and reliable negative documents to enhance classification performance	information retrieval;text classification	Most existing text classification work assumes that training data are completely labeled. In real life, some information retrieval problems can only be described as learning a binary classifier from a set of incompletely labeled examples, where a small set of labeled positive examples and a very large set of unlabeled ones are provided. In this case, all of the traditional text classification methods can't work properly. In this paper, we propose a method called Weighted Voting Classifier, which is an improved 1-DNF algorithm. Experimental results on the Reuters-21578 set show that our algorithm Weighting Voting Classifier outperforms PEBL and one-class SVM in terms of F measure. Weighting Voting Classifier can achieve high F score when comparing with PEBL and one-class SVM. Furthermore, the reduction of iterations is 2.26 when comparing the method of PEBL with ours.		Wanli Zuo	2006		10.1007/11683568_9	computer science;machine learning;pattern recognition;data mining	HPC	15.11867327608082	-40.94572437240916	172783
797b2bcbedc2b650fff4622b88a3d0d13110c19b	transferring naive bayes classifiers for text classification	text classification;naive bayes classifier	A basic assumption in traditional machine learning is that the training and test data distributions should be identical. This assumption may not hold in many situations in practice, but we may be forced to rely on a different-distribution data to learn a prediction model. For example, this may be the case when it is expensive to label the data in a domain of interest, although in a related but different domain there may be plenty of labeled data available. In this paper, we propose a novel transfer-learning algorithm for text classification based on an EM-based Naive Bayes classifiers. Our solution is to first estimate the initial probabilities under a distribution D of one labeled data set, and then use an EM algorithm to revise the model for a different distribution Du of the test data which are unlabeled. We show that our algorithm is very effective in several different pairs of domains, where the distances between the different distributions are measured using the KullbackLeibler (KL) divergence. Moreover, KL-divergence is used to decide the trade-off parameters in our algorithm. In the experiment, our algorithm outperforms the traditional supervised and semi-supervised learning algorithms when the distributions of the training and test sets are increasingly different.	document classification;expectation–maximization algorithm;experiment;graphical user interface;kl-one;kullback–leibler divergence;machine learning;naive bayes classifier;nice (unix);semi-supervised learning;semiconductor industry;statistical classification;supervised learning;test data;weitao yang	Wenyuan Dai;Gui-Rong Xue;Qiang Yang;Yong Yu	2007			naive bayes classifier;computer science;machine learning;pattern recognition;data mining;supervised learning;statistics	AI	16.733154260926675	-39.863102786146364	173117
2e2294f1c51da90f73dc6d377f25ed2431988864	genetic training instance selection in multiobjective evolutionary fuzzy systems: a coevolutionary approach	fuzzy rule based system;training set selection;multiobjective evolutionary fuzzy systems moefs;pareto optimisation;fuzzy set;complexity theory;large datasets;approximation method;large dataset;pareto front;prototypes;rule based;training;nonparametric statistic;training accuracy biological cells approximation methods complexity theory prototypes fuzzy sets;fuzzy set theory;coevolution;fuzzy sets;genetics;regression analysis computational complexity fuzzy set theory genetic algorithms knowledge based systems learning artificial intelligence pareto optimisation pattern classification;training set selection large datasets multiobjective evolutionary fuzzy systems moefs regression problems;accuracy;biological cells;regression problems;computational complexity;indexation;membership function;pattern classification;genetic algorithm;genetic algorithms;regression analysis;approximation methods;learning artificial intelligence;evolutionary learning;knowledge based systems;fuzzy system;pareto front approximations genetic training instance selection multiobjective evolutionary fuzzy systems coevolutionary approach multiobjective evolutionary learning fuzzy rule based systems fitness evaluation training set classification problems regression context moel framework frbs single objective genetic algorithm soga membership function parameters fuzzy sets nonparametric statistical tests;generalization capability	When dealing with datasets that are characterized by a large number of instances, multiobjective evolutionary learning (MOEL) of fuzzy rule-based systems (FRBSs) suffers from high computational costs, mainly because of the fitness evaluation. The use of a reduced set of representative instances in place of the overall training set (TS) would considerably lessen the computational effort. Even though a large number of papers have proposed instance selection approaches, mainly in classification problems, how this selection should be performed, especially in the context of regression, is still an open issue. In this paper, we tackle the instance selection problem in the framework of MOEL of FRBSs through a coevolutionary approach. In the execution of the MOEL, periodically, a single-objective genetic algorithm (SOGA) evolves a population of reduced TSs. The SOGA aims to maximize a purposely defined index which measures how much the Pareto fronts computed by using, respectively, the reduced TS and the overall TS are close to each other: The closer the fronts, the more the reduced TS is representative of the overall TS. During the execution of the MOEL, the rule base and the membership function parameters of the fuzzy sets are concurrently learned by maximizing the accuracy and minimizing the complexity. We tested our approach on 12 large datasets. We adopted reduced TSs composed of 5%, 10%, and 20% of the overall TS. Using nonparametric statistical tests, we verified that with 10% and 20% of the overall TS, the Pareto front approximations that are generated by our coevolutionary approach are comparable with the ones generated by applying the MOEL with the overall TS, although the coevolution allows us to save up to 86.36% of the execution time. In addition, the analysis of the behavior of three representative solutions on the test set highlights that the use of the reduced TSs does not affect the generalization capabilities of the generated FRBSs.	approximation;computation;evolutionary algorithm;fuzzy control system;fuzzy rule;fuzzy set;genetic algorithm;pareto efficiency;rule-based system;run time (program lifecycle phase);selection algorithm;test set;whole earth 'lectronic link	Michela Antonelli;Pietro Ducange;Francesco Marcelloni	2012	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2011.2173582	rule-based system;computer science;artificial intelligence;knowledge-based systems;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy control system	Web+IR	10.851117537787806	-40.62690167888395	173507
27fab3b6fb1a29dfcbe72c7655611cd9c68c9ae1	semi-supervised classification method based on spectral clustering	unlabeled data;spectral clustering;labeled data;semi supervised classification	With the rapid development of data collection and storage technology, there are plentiful unlabeled data but very few and often expensive labeled data in real-word applications. Thus, semi-supervised learning algorithms have attracted much attention. In this paper, we propose a new semi-supervised classification algorithm benefiting from spectral clustering called SC-SSL. First, we introduce spectral clustering to partition all labeled and unlabeled data into clusters. Second, we build a classifier using all labeled data and predict the probabilities (weights) of classes that each unlabeled instance belongs to for each cluster. Third, for each cluster, we add those unlabeled instances whose labels with the maximum weights as same as the cluster label into the labeled data. Fourth, in terms of the new labeled data set, we reconstruct the classifier. We repeat the above processing of steps 2 and 3 till meeting the stopping condition. Finally, extensive experiments reveal that our SC-SSL algorithm can sufficiently use the information of unlabeled data to get a robust classifier by spectral clustering, and it maintains a higher classification accuracy compared to several well known semi-supervised algorithms.	computer cluster;machine learning;semi-supervised learning;semiconductor industry;spectral clustering	Xi Chen	2014	JNW	10.4304/jnw.9.2.384-392	semi-supervised learning;computer science;machine learning;pattern recognition;data mining;cluster analysis;spectral clustering	ML	13.751367025076297	-40.72922865490017	173766
240b737ca2a59cc65b1c131f043b49edf5d09a4f	learning actively for sequence classification		In this investigation, we discuss a classification issue of sequence data. Generally, we assume a set of training data to construct classifiers, but the construction is not easy to obtain such data set. We take an approach of probabilistic classification based on Hidden Markov Model (HMM). We build a classifier to each class, apply to sequence data and estimate the class of the maximum likelihood. HMM requires less amount of training data but these data help HMM to work better. We propose an active learning approach to construct classifiers. The basic idea is that HMM takes a new training data autonomously to polish up the classifiers whenever HMM expects the more likelihood.	active learning (machine learning);hidden markov model;markov chain;statistical classification	Maoto Inoue;Takao Miura	2017	2017 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2017.36	active learning;maximum likelihood;computer science;hidden markov model;classifier (linguistics);probabilistic classification;pattern recognition;artificial intelligence;training set	AI	16.109269563938017	-38.266757065959524	174489
10225eafd5469da2ab7acdff930fedda6a90c94c	identifying reliable independent components via split-half comparisons	unsupervised learning;sensitivity and specificity;evoked potentials;independent component analysis;brain mapping;principal component analysis;reproducibility of results;algorithms;humans;electroencephalography;electroencephalogram;independent component;visual cortex;pattern recognition visual	Independent component analysis (ICA) is a family of unsupervised learning algorithms that have proven useful for the analysis of the electroencephalogram (EEG) and magnetoencephalogram (MEG). ICA decomposes an EEG/MEG data set into a basis of maximally temporally independent components (ICs) that are learned from the data. As with any statistic, a concern with using ICA is the degree to which the estimated ICs are reliable. An IC may not be reliable if ICA was trained on insufficient data, if ICA training was stopped prematurely or at a local minimum (for some algorithms), or if multiple global minima were present. Consequently, evidence of ICA reliability is critical for the credibility of ICA results. In this paper, we present a new algorithm for assessing the reliability of ICs based on applying ICA separately to split-halves of a data set. This algorithm improves upon existing methods in that it considers both IC scalp topographies and activations, uses a probabilistically interpretable threshold for accepting ICs as reliable, and requires applying ICA only three times per data set. As evidence of the method's validity, we show that the method can perform comparably to more time intensive bootstrap resampling and depends in a reasonable manner on the amount of training data. Finally, using the method we illustrate the importance of checking the reliability of ICs by demonstrating that IC reliability is dramatically increased by removing the mean EEG at each channel for each epoch of data rather than the mean EEG in a prestimulus baseline.	algorithm;baseline (configuration management);calcium-independent phospholipase a2;checking (action);electroencephalography;global optimization;independent computing architecture;independent component analysis;machine learning;magnetoencephalography;maxima and minima;resampling (statistics);topography;unsupervised learning	David M. Groppe;Scott Makeig;Marta Kutas	2009	NeuroImage	10.1016/j.neuroimage.2008.12.038	psychology;unsupervised learning;independent component analysis;speech recognition;electroencephalography;computer science;machine learning;pattern recognition;brain mapping;principal component analysis	ML	16.98106435078406	-40.95028348644445	174523
8971f7df63183656ca8ea2d85b389156ded96102	developing new fitness functions in genetic programming for classification with unbalanced data	loss measurement;biological patents;machine learning algorithms;learning process;genetic program;biomedical journals;text mining;europe pubmed central;training;training accuracy genetics machine learning loss measurement machine learning algorithms feature extraction;citation search;class imbalance;citation networks;classification;genetics;pattern classification data handling genetic algorithms learning artificial intelligence;accuracy;machine learning;research articles;abstracts;genetic programming gp;feature extraction;open access;unbalanced data;life sciences;pattern classification;clinical guidelines;genetic algorithms;binary classification;gp learning process fitness functions genetic programming unbalanced data machine learning algorithms biased classifiers data sets minority class majority class training criteria binary classification class imbalance unbalanced training data;data handling;full text;learning artificial intelligence;data classification;unbalanced data classification fitness function genetic programming gp;rest apis;orcids;europe pmc;biomedical research;fitness function;bioinformatics;literature search	Machine learning algorithms such as genetic programming (GP) can evolve biased classifiers when data sets are unbalanced. Data sets are unbalanced when at least one class is represented by only a small number of training examples (called the minority class) while other classes make up the majority. In this scenario, classifiers can have good accuracy on the majority class but very poor accuracy on the minority class(es) due to the influence that the larger majority class has on traditional training criteria in the fitness function. This paper aims to both highlight the limitations of the current GP approaches in this area and develop several new fitness functions for binary classification with unbalanced data. Using a range of real-world classification problems with class imbalance, we empirically show that these new fitness functions evolve classifiers with good performance on both the minority and majority classes. Our approaches use the original unbalanced training data in the GP learning process, without the need to artificially balance the training examples from the two classes (e.g., via sampling).	approximation algorithm;area under curve;binary classification;citation:bib:pt:fetal body weight estimation formula:nar;class;cross reactions;cross-validation (statistics);equilibration disorder;estimation theory;fitness function;genetic programming;glanzmann thrombasthenia, type a;large;linear separability;machine learning;naive bayes classifier;performance;problem domain;rewards;sampling (signal processing);solutions;statistic (data);support vector machine;unbalanced circuit	Urvesh Bhowan;Mark Johnston;Mengjie Zhang	2012	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2011.2167144	binary classification;text mining;genetic algorithm;feature extraction;biological classification;computer science;artificial intelligence;machine learning;group method of data handling;pattern recognition;data mining;accuracy and precision;fitness function	ML	13.545880491924892	-41.867883110906796	174653
a87d9fa6a2ee41519a776967eb515a5180fb88a6	gender recognition using fisherfaces and a fuzzy iterative self-organizing technique	databases;training;recognition rate fisherfaces fuzzy iterative self organizing technique gender recognition method feature extraction reduced dimensional space optimal fuzzy cluster centers fuzzy isodata model fuzzy nearest neighbor fuzzy isodata method clustering centers;face recognition;feature extraction;principal component analysis;clustering algorithms;feature extraction databases face face recognition clustering algorithms training principal component analysis;face;fuzzy nearest neighbor gender recognition fisherfaces fuzzy isodata;gender issues face recognition fuzzy set theory	This paper proposes a new gender recognition method by employing Fisherfaces and the fuzzy iterative self-organizing technique (ISODATA). The proposed method first uses Fisherfaces to extract suitable features from the reduced dimensional space. Then, the optimal fuzzy cluster centers can be calculated by applying the fuzzy ISODATA model to learn and cluster the gender features. Finally, the fuzzy nearest-neighbor is used for classification. The proposed method inherits the advantages of Fisherfaces and the fuzzy ISODATA method, which can extract suitable features for recognition and obtain the best clustering centers without the need for priori. Experimental results show the proposed method outperforms the mainstream methods in recognition rate and testing time.	algorithm;cluster analysis;feature extraction;iteration;iterative method;organizing (structure);self-organization	Yijun Du;Xiaobo Lu;Wujun Chen;Qianzhou Xu	2013	2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2013.6816192	facial recognition system;face;feature extraction;fuzzy classification;computer science;machine learning;pattern recognition;data mining;cluster analysis;principal component analysis	Robotics	15.325644841998693	-44.93892816458405	174658
661362226e99582143b7cbdd4614b107f0eb183f	an improved algorithm of unbalanced data svm	fuzzy c-means cluster;svm;unbalanced data;undersampling	Since SVM is unfair to the rare class for the classification of unbalanced data, a new balancing strategy based on common strategy of undersampling the training data is presented. Firstly, the fuzzy C-means clustering algorithm is used to cluster the unbalanced data sets, and choose the negative class samples whose memberships are greater than a certain threshold (supposing the number of positive class samples is less than that of negative class samples). The selected samples and the positive class of original samples are combined into a new training data set. After that, the new data set are used to train a support vector machine. At last, the simulations on unbalanced data show that the proposed algorithm can compensate the ill-effect of tendency when support vector machine are utilized to deal with the unbalanced data classification. Moreover, compared with the traditional support vector machine and some other improved algorithm, the proposed algorithm performs superior classification ability. © 2010 Springer-Verlag Berlin Heidelberg.	algorithm;unbalanced circuit	Bing Zhou;Minghu Ha;Chao Wang	2010		10.1007/978-3-642-14880-4_60	ranking svm	ML	13.552119980824763	-40.761928228931524	175704
d2ceb8f9881436c509a00317f4122de5f278670e	mejora del desempeño de svm usando un ga y la creación de puntos artificiales para conjuntos de datos no balanceados		Real world data sets are regularly unbalanced, which is a crucial problem in Machine Learning, it causes a low accuracy in most classification techniques. SVM have reported excellent generalization capability in recent years; however, working with unbalanced data sets have poor performance due to the retrieved hyperplane is biased towards the majority class. This article presents a new algorithm capable of generating artificial points within the minority class and on the border between classes from the positive Support Vectors, these new points are added to the training data set in order to reduce the imbalance between classes, improving the performance of SVM in the majority of tests. 93 Research in Computing Science 116 (2016) pp. 93–105; rec. 2016-03-17; acc. 2016-05-16	algorithm;customer support;linear algebra;machine learning;software release life cycle;test set;unbalanced circuit	José Hernández Santiago;Jair Cervantes;Carlos Hiram Moreno Montiel;Beatriz Hernández Santiago	2016	Research in Computing Science		speech recognition;mathematics	ML	13.588767961291008	-40.92216447332284	176273
9914ab16554dad6274bf9402d8cb908b194ffd68	experimental analysis of naïve bayes classifier based on an attribute weighting framework with smooth kernel density estimations	smooth kernel density estimation framework;attribute weighting;mutual information;bayesian models	Naïve Bayes learners are widely used, efficient, and effective supervised learning methods for labeled datasets in noisy environments. It has been shown that naïve Bayes learners produce reasonable performance compared with other machine learning algorithms. However, the conditional independence assumption of naïve Bayes learning imposes restrictions on the handling of real-world data. To relax the independence assumption, we propose a smooth kernel to augment weights for the likelihood estimation. We then select an attribute weighting method that uses the mutual information metric to cooperate with the proposed framework. A series of experiments are conducted on 17 UCI benchmark datasets to compare the accuracy of the proposed learner against that of other methods that employ a relaxed conditional independence assumption. The results demonstrate the effectiveness and efficiency of our proposed learning algorithm. The overall results also indicate the superiority of attribute-weighting methods over those that attempt to determine the structure of the network.	adjusted winner procedure;algorithm;averaged one-dependence estimators;benchmark (computing);experiment;kernel (operating system);machine learning;mutual information;naive bayes classifier;naivety;overfitting;supervised learning	Zhong-Liang Xiang;Xiang-Ru Yu;Dae-Ki Kang	2015	Applied Intelligence	10.1007/s10489-015-0719-1	naive bayes classifier;machine learning;pattern recognition;data mining;mutual information	ML	16.8227298035233	-39.737971293077536	177718
f863361fb690b2677692aaa01a96f596a32b6b50	incorporating mixed pixels in the training, allocation and testing stages of supervised classifications	fuzzy classification;maximum likelihood;supervised classification;fully fuzzy classification;mixed pixels;neural network	Conventional supervised classifiers cannot accommodate mixed pixels directly but may be modified to do so throughout the classification process. Here mixed pixels are included in all three stages of maximum likelihood and neural network classifications. The results show that by accommodating for mixed pixels in the classification, more accurate, appropriate and useful outputs may be derived.	pixel;supervised learning	Giles M. Foody;Reet K. Tiwari	1996	Pattern Recognition Letters	10.1016/S0167-8655(96)00095-5	fuzzy classification;computer science;machine learning;pattern recognition;mathematics;maximum likelihood;artificial neural network;statistics	Vision	16.377139194020067	-43.24543206490796	178011
39e48ed659c93b51637b8bd268261a7361539966	a novel combination of negative and positive selection in artificial immune systems	detectors vegetation immune system binary trees time complexity;artificial immune systems detector storage complexity r chunk matching rule binary representation detection phase generated detector set new data instance positive selection algorithms anomaly detection one class learning problems nsa negative selection algorithms immunology multidisciplinary research area ais;learning artificial intelligence artificial immune systems;learning artificial intelligence;artificial immune systems	Artificial Immune System (AIS) is a multidisciplinary research area that combines the principles of immunology and computation. Negative Selection Algorithms (NSA) is one of the most popular models of AIS mainly designed for one-class learning problems such as anomaly detection [1]. Positive Selection Algorithms (PSA) is the twin brother of NSA with similar performance for AIS [2]. Both NSAs and PSAs comprise of two phases: generating a set D of detectors from a given set S of selves (detector generation phase); and then detecting if a given cell (new data instance) is self or non-self using the generated detector set (detection phase). In this paper, we propose a novel approach to combining NSAs and PSAs that employ binary representation and r-chunk matching rule. The new algorithm achieves smaller detector storage complexity and potentially better detection time in comparison with single NSAs or PSAs.	anomaly detection;artificial immune system;best, worst and average case;binary number;binary tree;computation;email filtering;experiment;polar surface area;selection algorithm;sensor;time complexity	Van Truong Nguyen;Nguyen Xuan Hoai;Chi Mai Luong	2013	The 2013 RIVF International Conference on Computing & Communication Technologies - Research, Innovation, and Vision for Future (RIVF)	10.1109/RIVF.2013.6719857	computer science;artificial intelligence;machine learning;data mining	Robotics	10.459423035917544	-40.6755348277936	178141
6d78ca1c251b49ecdb7e73a1a1d0433bdd412514	moa concept drift active learning strategies for streaming data		We present a framework for active learning on evolving data streams, as an extension to the MOA system. In learning to classify streaming data, obtaining the true labels may require major effort and may incur excessive cost. Active learning focuses on learning an accurate model with as few labels as possible. Streaming data poses additional challenges for active learning, since the data distribution may change over time (concept drift) and classifiers need to adapt. Conventional active learning strategies concentrate on querying the most uncertain instances, which are typically concentrated around the decision boundary. If changes do not occur close to the boundary, they will be missed and classifiers will fail to adapt. We propose a software system that implements active learning strategies, extending the MOA framework. This software is released under the GNU GPL license.	active learning (machine learning);concept drift;decision boundary;gnu;international association of privacy professionals;moa;software system;stream (computing);streaming media	Indrė Žliobaitė;Albert Bifet;Geoff Holmes;Bernhard Pfahringer	2011			simulation;engineering;machine learning;data mining;active learning	ML	16.750373491325963	-40.316511396770736	179158
a208c9e2455cd780d8c7f34906be28214a1a4d39	daar: a discrimination-aware association rule classifier for decision support		Undesirable correlations between sensitive attributes (such as race, gender or personal status) and the class label (such as recruitment decision and approval of credit card), may lead to biased decision in data analytics. In this paper, we investigate how to build discrimination-aware models even when the available training set is intrinsically discriminating based on the sensitive attributes. We propose a new classification method called Discrimination-Aware Association Rule classifier (DAAR), which integrates a new discrimination-aware measure and an association rule mining algorithm. We evaluate the performance of DAAR on three real datasets from different domains and compare DAAR with two non-discrimination-aware classifiers (a standard association rule classification algorithm and the state-of-the-art association rule algorithm SPARCCC), and also with a recently proposed discrimination-aware decision tree method. Our comprehensive evaluation is based on three measures: predictive accuracy, discrimination score and inclusion score. The results show that DAAR is able to effectively filter out the discriminatory rules and decrease the discrimination severity on all datasets with insignificant impact on the predictive accuracy. We also find that DAAR generates a small set of rules that are easy to understand and applied by users, to help them make discrimination-free decisions.	association rule learning	Ling Luo;Wei Liu;Irena Koprinska;Fang Chen	2017	T. Large-Scale Data- and Knowledge-Centered Systems	10.1007/978-3-662-55608-5_3	personal status;decision support system;decision tree;small set;machine learning;classifier (linguistics);association rule learning;artificial intelligence;training set;data analysis;computer science	ML	10.362891843165093	-38.13252642008008	179317
a5fd4885848e9bb564e35368ca9dffb0cabceb9d	effect of metalearning on feature selection employment.		Feature Selection is important to improve learning performance, reduce computational complexity and decrease required storage. There are multiple methods for feature selection, with varying impact and computational cost. Therefore, choosing the right method for a given data set is important. In this paper, we analyze the advantages of metalearning for feature selection employment. This issue is relevant because a wrong decision may imply additional processing, when FS is unnecessarily applied, or in a loss of performance, when not used in a problem for which it is appropriate. Our results showed that, although there is an advantage in using metalearning, these gains are not yet sufficiently relevant, which opens the way for new research to be carried out in the area.	algorithmic efficiency;computation;computational complexity theory;computer performance;feature selection	Silvia Nunes das Dôres;Carlos Soares;Duncan D. A. Ruiz	2017			feature selection;artificial intelligence;pattern recognition;metalearning;computer science	AI	15.622774168125956	-39.24416843743519	179463
6ef0b27daeb7d6f5b86fa888389ae15d7bdd574b	boosted pre-loaded mixture of experts for low-resolution face recognition	boosted mixture of experts;mixture of experts;boosting;low resolution face recognition;neural networks ensemble	A modified version of Boosted Mixture of Experts (BME) for low-resolution face recognition is presented in this paper. Most of the methods developed for low-resolution face recognition focused on improving the resolution of face images and/or special feature extraction methods that can deal effectively with low-resolution problem. However, we focused on the classification step of face recognition process in this paper. Using Neural Networks (NN) combinations is an efficient approach to deal with complex classification problems, such as the low-resolution face recognition which involves high-dimensional feature sets and highly overlapped classes. Mixture of Experts (ME) and boosting methods are two of the most popular and interesting NN combining methods, which have great potential for improving performance in classification. A modified combining approach based on both features of ME and boosting is presented in order to deal with this complex classification problem efficiently. Previous works [1,2] made attempts to incorporate the complementary features of boosting method in ME training algorithm to boost the performance. These approaches called Boosted Mixture of Experts (BME) have some drawbacks. Based on the analysis of the problems of previous approaches, some modifications are suggested in this paper. A modification in the preloading (initialization) procedure of ME is proposed to address the limitations of previous approaches and overcome them using a two stages pre-loading procedure. In our suggested approach, both the error and confidence measures are used as the difficulty criteria in boosting-based partitioning of the problem space. Regarding the nature of this approach, we call the proposed method Boosted Pre-loaded Mixture of Experts (BPME). The proposed method is tested in a low-resolution face recognition problem and compared to the other variations of ME and boosting method. The experiments are conducted using low-resolution variations of two common face databases including the ORL and Yale databases. The experimental results show that BPME method has significant better recognition rates against the other compared combining methods in various tested conditions including different quality grades of face images and different sizes of the training set.	algorithm;artificial neural network;boosting (machine learning);database;experiment;facial recognition system;feature extraction;image resolution;neural network software;problem domain;python;return loss;statistical classification;test set	Reza Ebrahimpour;Naser Sadeghnejad;Saeed Masoudnia;Seyed Ali Asghar AbbasZadeh Arani	2012	Int. J. Hybrid Intell. Syst.	10.3233/HIS-2012-0153	computer science;machine learning;pattern recognition;data mining;boosting	Vision	14.235624624988708	-43.7330879308701	179537
0dd24abf4dcd6315a29767a74560ad3badf95c93	how noisy data affects geometric semantic genetic programming		Noise is a consequence of acquiring and pre-processing data from the environment, and shows fluctuations from different sources---e.g., from sensors, signal processing technology or even human error. As a machine learning technique, Genetic Programming (GP) is not immune to this problem, which the field has frequently addressed. Recently, Geometric Semantic Genetic Programming (GSGP), a semantic-aware branch of GP, has shown robustness and high generalization capability. Researchers believe these characteristics may be associated with a lower sensibility to noisy data. However, there is no systematic study on this matter. This paper performs a deep analysis of the GSGP performance over the presence of noise. Using 15 synthetic datasets where noise can be controlled, we added different ratios of noise to the data and compared the results obtained with those of a canonical GP. The results show that, as we increase the percentage of noisy instances, the generalization performance degradation is more pronounced in GSGP than GP. However, in general, GSGP is more robust to noise than GP in the presence of up to 10% of noise, and presents no statistical difference for values higher than that in the test bed.	elegant degradation;genetic and evolutionary computation conference;genetic programming;human error;machine learning;miranda;preprocessor;reactive-ion etching;robustness (computer science);sensor;signal processing;signal-to-noise ratio;symbolic regression;synthetic data;synthetic intelligence;testbed	Luis Fernando Miranda;Luiz Otávio Vilas Boas Oliveira;Joao Francisco B. S. Martins;Gisele Lobo Pappa	2017		10.1145/3071178.3071300	symbolic regression;artificial intelligence;noisy data;machine learning;human error;robustness (computer science);computer science;signal processing;genetic programming	HPC	14.853457748572866	-41.877374750175036	180441
ff4554d60f6917f84d948965715da2940b4f58af	prediction of expected performance for a genetic programming classifier	problem difficulty;supervised learning;prediction of expected performance	The estimation of problem difficulty is an open issue in genetic programming (GP). The goal of this work is to generate models that predict the expected performance of a GP-based classifier when it is applied to an unseen task. Classification problems are described using domain-specific features, some of which are proposed in this work, and these features are given as input to the predictive models. These models are referred to as predictors of expected performance. We extend this approach by using an ensemble of specialized predictors (SPEP), dividing classification problems into groups and choosing the corresponding SPEP. The proposed predictors are trained using 2D synthetic classification problems with balanced datasets. The models are then used to predict the performance of the GP classifier on unseen real-world datasets that are multidimensional and imbalanced. This work is the first to provide a performance prediction of a GP system on test data, while previous works focused on predicting training performance. Accurate predictive models are generated by posing a symbolic regression task and solving it with GP. These results are achieved by using highly descriptive features and including a dimensionality reduction stage that simplifies the learning and testing process. The proposed approach could be extended to other classification algorithms and used as the basis of an expert system for algorithm selection.	algorithm selection;dimensionality reduction;domain-specific language;expert system;genetic programming;performance prediction;predictive modelling;statistical classification;symbolic regression;synthetic intelligence;test data	Yuliana Martínez;Leonardo Trujillo;Pierrick Legrand;Edgar Galván López	2016	Genetic Programming and Evolvable Machines	10.1007/s10710-016-9265-9	computer science;artificial intelligence;machine learning;pattern recognition;data mining;supervised learning	AI	14.653999219909613	-42.37149979410563	181639
9fdd15060cd3331e0fee56d5a9dd755129f0fd05	an efficient online active learning algorithm for binary classification	online active learning;margin based criterion;binary classification;iteratively decreased threshold	We propose a new online active learning algorithm for binary classification.Our algorithm uses a margin-based criterion with iteratively decreased threshold.Our algorithm requires less queries to achieve comparable classification accuracy.Our algorithm incurs a smaller computation overhead at the same time. Active learning is an important class of machine learning where labels are queried when necessary. Most active learning algorithms need to iteratively retrain the classifier when new labeled data are obtained. Such a batch learning process can incur a high overhead in both time and memory. In this paper, we propose a new online active learning algorithm for binary classification. Our algorithm uses the margin-based criterion, which compares the margin of instances with a threshold to decide whether it should be queried. Especially, we propose Iteratively Decreased Threshold (IDT), a new threshold update method for the margin-based criterion. By iteratively decreasing the threshold with IDT, our algorithm can effectively reduce the number of queried instances. In addition, as evaluating the margin-based criterion involves only simple inner productions, our algorithm is also very efficient to evaluate. We compare our algorithm with other state-of-the-art online active learning algorithms on six data sets, demonstrating that it requires less queries to achieve the same classification accuracy, and incurs a smaller computation overhead at the same time.		Dehua Liu;Peng Zhang;Qinghua Zheng	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.08.010	binary classification;margin;computer science;machine learning;pattern recognition;data mining;active learning;population-based incremental learning	Vision	15.3522319723553	-39.75543400553075	181779
29ae1f28f99fdf4d3eae2a7da18673e0ba322a6e	exploiting multiple classifier types with active learning	decision tree;ensemble of classifiers;active learning;adaptive informative sampling;multiple classifiers;evolutionary algorithm;artificial neural network	Many approaches to active learning involve training one classifier by periodically choosing new data points about which the classifier has the least confidence, but designing a confidence measure without bias is nontrivial. An alternative approach is to train an ensemble of classifiers by periodically choosing data points that cause maximal disagreement among them. Many classifiers with different underlying structures could fit this framework, but some classifiers are more suitable for different data sets than others. The question then arises as to how to find the most suitable classifier for a given data set. In this work, an evolutionary algorithm is proposed to address this problem. The algorithm starts with a combination of artificial neural networks and decision trees, and iteratively adapts the ratio of the classifier types according to a replacement strategy. Experiments with synthetic and real data sets show that when the algorithm considers both fitness and classifier type for replacement, the population becomes saturated with accurate instantiations of the more suitable classifier type. This allows the algorithm to perform consistently well across data sets, without having to determine a priori a suitable classifier type.	active learning (machine learning);artificial neural network;data point;decision tree;ensemble forecasting;evolutionary algorithm;experiment;maximal set;statistical classification;synthetic intelligence	Zhenyu Lu;Josh C. Bongard	2009		10.1145/1569901.1570228	random subspace method;margin classifier;margin;cascading classifiers;quadratic classifier;computer science;machine learning;decision tree;evolutionary algorithm;linear classifier;pattern recognition;data mining;active learning;artificial neural network	ML	15.570731439301044	-39.45970383611777	181862
f038f83db9fa240729f9805eba23617945b94bb0	undersampling techniques to re-balance training data for large scale learning-to-rank		Learning-to-rank (LtR) algorithms for information retrieval use the supervised learning framework to learn a ranking function from a training set consisting of query-document pairs. In this study we investigate the imbalanced nature of LtR training sets, which generally contain very few relevant documents as compared to the number of irrelevant documents. The need to include as many relevant documents as possible in the training set is well-known, but we ask the question as to how many irrelevant documents are needed in order to learn a good ranking function. We employ both random and deterministic undersampling techniques to reduce the number of irrelevant documents. Minimizing the training set size reduces the training time which is an important factor in large scale LtR. Extensive experiments on Letor benchmark datasets reveal that the performance of a LtR algorithm trained on a much smaller training set remains similar to that of the original training set. Thus this study suggests that for large scale LtR tasks, we can leverage undersampling techniques to reduce training time with negligible effect on performance.	learning to rank;test set;undersampling	Muhammad Ibrahim;Mark James Carman	2014		10.1007/978-3-319-12844-3_38	computer science;machine learning;pattern recognition;data mining	Vision	15.852042451239459	-40.68415269634028	182519
48ec72bbd20058444701c2c43cc875d391e9c36f	on the diversity-performance relationship for majority voting in classifier ensembles	classifier ensemble;ensemble method;classifier system;distribution pattern;upper and lower bounds;majority voting;multiple classifier system;information theoretic	Combining multiple classifier systems (MCS') has been shown to outperform single classifier system. It has been demonstrated that improvement for ensemble performance depends on either the diversity among or the performance of individual systems. A variety of diversity measures and ensemble methods have been proposed and studied. It remains a challenging problem to estimate the ensemble performance in terms of the performance of and the diversity among individual systems. In this paper, we establish upper and lower bounds for Pm (performance of the ensemble using majority voting) in terms of P (average performance of individual systems) and D (average entropy diversity measure among individual systems). These bounds are shown to be tight using the concept of a performance distribution pattern (PDP) for the input set. Moreover, we showed that when P is big enough, the ensemble performance Pm resulting from a maximum (information-theoretic) entropy PDP is an increasing function with respect to the diversity measure D. Five experiments using data sets from various applications domains are conducted to demonstrate the complexity, richness, and diverseness of the problem in estimating the ensemble performance.		Yun Sheng Chung;D. Frank Hsu;Chuan Yi Tang	2007		10.1007/978-3-540-72523-7_41	machine learning;pattern recognition;data mining;mathematics;ensemble learning	ECom	12.258262143485945	-39.51403046569918	182820
8ff1548e70dd280a1201c44009df73c59703639f	learning in imbalanced relational data	databases;similar numbers;relational data;probability;student identification imbalanced relational data relational learning technique probabilistic relational model imbalanced class problem university relational database;imbalanced data;probabilistic relational models;bayesian methods;testing;imbalanced class;relational database;traditional learning;university relational database;new model;data distribution;computing and communication sciences;imbalanced relational data;real world data;relational databases educational administrative data processing learning artificial intelligence probability;educational administrative data processing;student identification;data files;the australian standard research classification 280000 information;relational learning technique;relational systems;classification algorithms;imbalanced class problem;relational databases;probabilistic logic;learning artificial intelligence;relational learning;probabilistic relational model;relational databases bayesian methods costs probability distribution inference algorithms decision trees regression analysis algorithm design and analysis voting laboratories;data models	Traditional learning techniques learn from flat data files with the assumption that each class has a similar number of examples. However, the majority of real-world data are stored as relational systems with imbalanced data distribution, where one class of data is over-represented as compared with other classes. We propose to extend a relational learning technique called Probabilistic Relational Models (PRMs) to deal with the imbalanced class problem. We address learning from imbalanced relational data using an ensemble of PRMs and propose a new model: the PRMs-IM. We show the performance of PRMs-IM on a real university relational database to identify students at risk.	algorithm;attribute-value system;instant messaging;relational database;relational model	Amal Saleh Ghanem;Svetha Venkatesh;Geoff A. W. West	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761095	statistical classification;relational model;statistical relational learning;relational database;computer science;machine learning;pattern recognition;data mining	DB	15.362042703924546	-42.372514386107156	182928
afa30f539f28048fd34e403242c39446289f7124	on feature selection: a new filter model	feature selection	"""We focus on the filter approach of feature selection. We exploit geometrical information of the learning set to build an estimation criterion based on a quadratic entropy. The distribution of this criterion is approximately normal, that allows the construction of a non pararnctric~l statistical test to assess the relevance of feature subsets. We use the critical threshold of this test, callcci the test o["""" Relative Certainty Gain, in a forward selection algorithm. We present some experimental results both on synthetic a~ld natural domains of the UCT databa~ rep~sitory, which show significa~ltly improvements on ttne accuracy estinmtes. Introduction While the problem of feature selection has always been at the center of statistic researches, it is only recently that this problem received attention in computer scicnce. Beyond the intcntion of improving the performance of their algorithn~s, machine learning researchers studied featurc selection methods to face the explosion of data (not always relevant) provided by recent data collecting technologies (the Web for instance). Prom a theoretical standpoint, the selection of a good feature subset is of little interest. Actually, a Bayesian classifier is monotonic, i.e., adding features can not decrc~.sc the model’s performance. This is generally true only for infinite learning sets for which the estimate errors can be ignored. In fact, practical algorithms not always being perfect, the monotonicity assumption rarely holds (Kohavi 1994). Thus, irrelevant or weakly relevant features may"""" reduce the accuracy of the model. A study in (Thrun et al. 1991) shows that with the C4.5 algorithm (Quinlan 1993) non deletion of weakly relevant featur~ generates deeper decision trees with lower performanc~ than those obtained without these features. In (Aha 1992), the author shows that the storage of the IB3 algorithm increases exponentially with the number of irrelevant features. Same sort of conclusions arc presented in (Langley and Iba 1993). These results have encouraged scienti~s to elaborate sophisticated feature sclection methods aUowing to: °Col,.right ~ 1999, American Association for Artificial Intelligence (~vw.aaai.org). All rights reserved. ̄ Reduce classifier’s cost and complexity. ̄ Improve model accuracy. ̄ Improve the visualization and comprehensibility of induced concepts. According to the terminology proposed in (John, Kohavi and Pfleger 1994), two approaches are available: the wrapper and filter models. In filter models, the accuracy of the future induced classifier is assessed using statistical techniques. The method """"filter out"""" irrelevant features before the induction process. In wrapper methods, we search for a good subset of features using the induction algorithm. The principle is generally based on the optimization of the accuracy rate, estimated by one of the following methods: holdout, cross-validation (Kohavi 1995), or bootstrap (Efron and Tibshirani 1993). Whatever the method of feature selection we use, the goal is always to assess the relevance of alternative subsets. A survey of relevance definitions is proposed in (Blum and Langley 1997). In this article, we consider the filter approach to find relevant features. We will explain in detail arguments about this choice. We exploit characteristics of a neighborhood graph built on the learning set, to compute a new estimation criterion based on a quadratic entropy. We show that the distribution of this criterion is approximately normal, that allows the construction of a non parametrical test to assess the quality of feature subsets. We use this statistical test (more precisely the critical threshold) in a forward selection. Finally, we present some experimental results on benchmarks of the UCI database repository, comparing performances of sdected feature subsets with rcsults obtained in thc original spaces. Feature Selection and Filter Model"""	artificial intelligence;bayesian network;blum axioms;c4.5 algorithm;computation;cross-validation (statistics);data acquisition;database;decision tree;feature selection;high-level programming language;information theory;machine learning;mathematical induction;mathematical optimization;naive bayes classifier;performance;relevance;ross quinlan;selection algorithm;stepwise regression;synthetic intelligence;test set;world wide web	Marc Sebbna	1999			minimum redundancy feature selection;computer science;machine learning;feature;dimensionality reduction	AI	15.122482036896656	-39.394615185002216	183384
3991dd769d30e5aabe94a24c896dc9ba0332559e	a practical feature selection based on an optimal feature subset and its application for detecting lung nodules in chest radiographs	bagging ensemble feature selection feature selection methods optimal feature selection optimal feature subset;medical image processing diagnostic radiography feature selection image classification lung;image classification;lung;computer aided diagnosis practical feature selection approach optimal feature subset lung nodule detection chest radiographs ensemble systems classifiers single cad system multilevel optimal feature selection method mofs bagging ensemble attribute selection algorithm relieff;feature extraction lungs design automation solid modeling bagging computational modeling artificial neural networks;medical image processing;feature selection;diagnostic radiography	The traditional motivation behind feature selection algorithms such as a genetic algorithm, a forward stepwise and a backward stepwise selections [1], is to find the best feature subset for a task using one particular learning algorithm. The idea is to select a optimal subset of attributes which are as representative as possible of the original data. However, it has been often found that no single classifier is entirely satisfactory for a particular task. Therefore, how to further improve the performance of these single systems on the basis of the previous optimal feature subset is a very important issue. Ensemble systems, also known as committees of classifiers, are composed of individual classifiers, organized in a parallel way and their outputs are combined in a combination method, which provides the final output of the system. Given the success of ensembles, ensembles allow us to get higher accuracy and sensitivity, which are often not achievable with single models. Based on the above, we propose a practical feature selection approach that is based on an optimal feature subset of a single CAD system, which is referred to as a multilevel optimal feature selection method (MOFS) in this paper. Through MOFS, we select the different optimal feature subsets in order to eliminate features that are redundant or irrelevant and obtain optimal features, and then a bagging ensemble with a MOFS method is proposed. Experimental results indicates that the accuracy of the bagging ensemble using a MOFS method is superior to that of a single CAD system and is also superior to that of the ensemble using an attribute selection algorithm based on ReliefF.	caddie;complement (complexity);computer-aided design;feature selection;genetic algorithm;radiography;relevance;selection algorithm;sensor;stepwise regression;wavelet	Haoyan Guo;Yuanzhi Cheng;Dazheng Wang;Li Guo	2013	2013 6th International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2013.6746994	contextual image classification;minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;feature selection;feature	AI	13.065252665518159	-44.14575956478733	183784
da0717077a176b956e2208936edd6df425f1a683	an ensemble kernel classifier with immune clonal selection algorithm for automatic discriminant of primary open-angle glaucoma	kernel principal component analysis;ensemble kernel classifier;immune clonal selection algorithm;support vector machine;primary open angle glaucoma	An ensemble kernel classifier is proposed in this paper by integrating a kernel principal component analysis (KPCA) with a support vector machine (SVM) as well as an immune clonal selection algorithm (ICSA). The KPCA approach is used to extract features, whereas the SVM technique is employed to deal with classification, and the ICSA is applied to optimize the parameters of the proposed scheme. The in order to produce various SVM classifiers with different kernels. Regardless of whether the data is linear or nonlinear, an optimum classification result can be obtained. In order to demonstrate the effectiveness of the classifier, it is applied to discriminate the primary open-angle glaucoma (POAG) using a standard classification dataset. Experimental results reveal that the proposed ensemble classifier is accurate and more effective when compared to other approaches in the literature. It is envisaged that ensemble kernel classifier could hold a high potential in classification of pattern recognition problems. & 2011 Elsevier B.V. All rights reserved.	artificial neural network;clonal selection algorithm;discriminant;ensemble learning;feature selection;interaction;international computer security association;kernel (operating system);kernel principal component analysis;markov chain;mathematical optimization;nonlinear system;pattern recognition;search algorithm;software release life cycle;stochastic neural network;support vector machine;time complexity	Lijun Cheng;Yongsheng Ding;Kuangrong Hao;Yi-Fan Hu	2012	Neurocomputing	10.1016/j.neucom.2011.09.030	margin classifier;support vector machine;kernel method;kernel embedding of distributions;radial basis function kernel;quadratic classifier;kernel principal component analysis;computer science;machine learning;pattern recognition;data mining;mathematics	AI	11.602455888170594	-42.15082959986221	184404
3f0194e1e1824f26b27c3e9ef222a58bbc62c672	a ruleset reduction algorithm for the xcs learning classifier system	time complexity;learning classifier system;data mining;machine learning	XCS is a learning classifier system based on the original work by Stewart Wilson in 1995. It is has recently been found competitive with other state of the art machine learning techniques on benchmark data mining problems. For more general utility in this vein, however, issues are associated with the large numbers of classifiers produced by XCS; these issues concern both readability of the combined set of rules produced, and the overall processing time. The aim of this work is twofold, to produce reduced classifier sets which can more readily be understandable as rules, and to speedup processing via reduction of classifier set size during operation of XCS. A number of algorithmic modifications are presented, both in the operation of XCS itself and in the postprocessing of the final set of classifiers. We describe a technique of qualifying classifiers for inclusion in action sets, which enables classifier sets to be generated prior to passing to a reduction algorithm, allowing reliable reductions to be performed with no performance penalty. The concepts of ‘spoilers’ and ‘uncertainty’ are introduced, which help to characterise some of the peculiarities of XCS in terms of operation and performance. A new reduction algorithm is described which we show to be similarly effective to Wilson’s recent technique, but with considerably more favourable time complexity, and we therefore suggest that it may be preferable to Wilson's algorithm in many cases with particular requirements concerning the speed/performance tradeoff.	algorithm;benchmark (computing);data mining;learning classifier system;machine learning;requirement;speedup;time complexity	Phillip William Dixon;David W. Corne;Martin J. Oates	2002		10.1007/978-3-540-40029-5_2	semi-supervised learning;unsupervised learning;margin classifier;multi-task learning;instance-based learning;wake-sleep algorithm;online machine learning;machine learning;pattern recognition;data mining;learning classifier system;stability;computational learning theory;active learning;generalization error	ML	14.079206935865622	-39.15830460542946	184789
6a06b9821b2bd97f7c9ac98a936a737d6b19ce8b	active learning for classifying data streams with unknown number of classes	active learning;bayesian online learning;concept drift;concept evolution;data streams	Abstract The classification of data streams is an interesting but also a challenging problem. A data stream may grow infinitely making it impractical for storage prior to processing and classification. Due to its dynamic nature, the underlying distribution of the data stream may change over time resulting in the so-called concept drift or the possible emergence and fading of classes, known as concept evolution . In addition, acquiring labels of data samples in a stream is admittedly expensive if not infeasible at all. In this paper, we propose a novel stream-based active learning algorithm (SAL) which is capable of coping with both concept drift and concept evolution by adapting the classification model to the dynamic changes in the stream. SAL is the first AL algorithm in the literature to explicitly take account of these concepts. Moreover, using SAL, only labels of samples that are expected to reduce the expected future error are queried. This process is done while tackling the problem of sampling bias so that samples that induce the change (i.e., drifting samples or samples coming from new classes) are queried. To efficiently implement SAL, the paper proposes the application of non-parametric Bayesian models allowing to cope with the lack of prior knowledge about the data stream. In particular, Dirichlet mixture models and the stick breaking process are adopted and adapted to meet the requirements of online learning. The empirical results obtained on real-world benchmarks demonstrate the superiority of SAL in terms of classification performance over the state-of-the-art methods using average and average class accuracy.	active learning (machine learning);algorithm;bayesian network;benchmark (computing);class;concept drift;coping behavior;emergence;entity name part qualifier - adopted;mathematical model;mixture model;novelty detection;requirement;sampling (signal processing);sampling - surgical action;signal-to-noise ratio;statistical classification;unbalanced circuit	Saad Mohamad;Moamar Sayed Mouchaweh;Abdelhamid Bouchachia	2018	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2017.10.004	mixture model;machine learning;data stream;artificial intelligence;fading;data stream mining;dirichlet distribution;active learning;concept drift;bayesian probability;computer science	ML	14.908981768271364	-38.03980251969733	185084
4cc09f0b20b47310684888b36d7a1bf8e245e0c2	a signal invariant wavelet function selection algorithm	analysis of variance;biomedical signals;genetic algorithm;wavelet function selection	This paper addresses the problem of mother wavelet selection for wavelet signal processing in feature extraction and pattern recognition. The problem is formulated as an optimization criterion, where a wavelet library is defined using a set of parameters to find the best mother wavelet function. For estimating the fitness function, adopted to evaluate the performance of the wavelet function, analysis of variance is used. Genetic algorithm is exploited to optimize the determination of the best mother wavelet function. For experimental evaluation, solutions for best mother wavelet selection are evaluated on various biomedical signal classification problems, where the solutions of the proposed algorithm are assessed and compared with manual hit-and-trial methods. The results show that the solutions of automated mother wavelet selection algorithm are consistent with the manual selection of wavelet functions. The algorithm is found to be invariant to the type of signals used for classification.	addresses (publication format);analysis of variance;entity name part qualifier - adopted;estimated;feature extraction;fitness function;genetic algorithm;mathematical optimization;pattern recognition;sample variance;selection algorithm;signal processing;solutions;thrombocytopenia;wavelet analysis	Girisha Garg	2015	Medical & Biological Engineering & Computing	10.1007/s11517-015-1354-z	wavelet;mathematical optimization;second-generation wavelet transform;continuous wavelet transform;machine learning;pattern recognition;cascade algorithm;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fast wavelet transform;lifting scheme;wavelet transform	Visualization	10.644514023554132	-44.725217173366396	185483
276c6a050d391ce18ea592bc407d0e6c7be38da6	learning with only multiple instance positive bags	negative instances multiple instance positive bags multiple instance learning negative bags mil methods positive multiple instance pmi;learning artificial intelligence;training prediction algorithms kernel face support vector machines nickel clustering algorithms	In traditional multiple instance learning (MIL), both positive and negative bags are required to learn a prediction function. However, a high human cost is needed to know the label of each bag-positive or negative. Only positive bags contain our focus (positive instances) while negative bags consist of noise or background (negative instances). So we do not expect to spend too much to label the negative bags. Contrary to our expectation, nearly all existing MIL methods require enough negative bags besides positive ones. In this paper we propose an algorithm called “Positive Multiple Instance” (PMI), which learns a classifier given only a set of positive bags. So the annotation of negative bags becomes unnecessary in our method. PMI is constructed based on the assumption that the unknown positive instances in positive bags be similar each other and constitute one compact cluster in feature space and the negative instances locate outside this cluster. The experimental results demonstrate that PMI achieves the performances close to or a little worse than those of the traditional MIL algorithms on benchmark and real data sets. However, the number of training bags in PMI is reduced significantly compared with traditional MIL algorithms.	algorithm;approximation algorithm;benchmark (computing);feature vector;multiple instance learning;performance;theory;time complexity	Zhigang Wang;Zeng-Shun Zhao;Changshui Zhang	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727217	instance-based learning;computer science;machine learning;pattern recognition;data mining	ML	15.760644755260447	-40.61838266649757	185701
840a58d408a1d1875835d6a1b4fd99f7e06db86b	scalable extensions of the relieff algorithm for weighting and selecting features on the multi-label learning context	label ranking;multi label classification;relieff algorithm;feature weighting;feature selection;multi label learning	Multi-label learning has become an important area of research due to the increasing number of modern applications that contain multi-label data. The multi-label data are structured in a more complex way than single-label data. Consequently the development of techniques that allow the improvement in the performance of machine learning algorithms over multi-label data is desired. The feature weighting and feature selection algorithms are important feature engineering techniques which have a beneficial impact on the machine learning. The ReliefF algorithm is one of the most popular algorithms to feature estimation and it has proved its usefulness in several domains. This paper presents three extensions of the ReliefF algorithm for working in the multi-label learning context, namely ReliefF-ML, PPT-ReliefF and RReliefF-ML. PPT-ReliefF uses a problem transformation method to convert the multi-label problem into a single-label problem. ReliefF-ML and RReliefF-ML adapt the classic ReliefF algorithm in order to handle directly the multi-label data. The proposed ReliefF extensions are evaluated and compared with previous ReliefF extensions on 34 multi-label datasets. The results show that the proposed ReliefF extensions improve preceding extensions and overcome some of their drawbacks. The experimental results are validated using several nonparametric statistical tests and confirm the effectiveness of the proposal for a better multi-label learning.	algorithm;baseline (configuration management);experiment;feature engineering;feature selection;feature vector;interaction;lr parser;lazy evaluation;machine learning;mechwarrior: living legends;multi-label classification;multi-level cell;peres–horodecki criterion;relevance;scalability;synthetic data;whole earth 'lectronic link	Oscar Gabriel Reyes Pupo;Carlos Morell;Sebastián Ventura	2015	Neurocomputing	10.1016/j.neucom.2015.02.045	computer science;machine learning;pattern recognition;data mining;feature selection	AI	17.164163154613416	-40.92920699635249	185725
7ff753e491ac77d60029e16e7b1965708fef6c03	an improved branch and bound algorithm for feature selection	optimal solution;branch and bound algorithm;classification;feature subset selection;pattern classification;feature selection;solution tree	Feature selection plays an important role in pattern classification. In this paper, we present an improved Branch and Bound algorithm for optimal feature subset selection. This algorithm searches for an optimal solution in a large solution tree in an efficient manner by cutting unnecessary paths which are guaranteed not to contain the optimal solution. Our experimental results demonstrate the effectiveness of the new algorithm.	algorithm;branch and bound;feature selection	Xue-wen Chen	2003	Pattern Recognition Letters	10.1016/S0167-8655(03)00020-5	mathematical optimization;minimum redundancy feature selection;biological classification;computer science;machine learning;pattern recognition;feature selection;branch and bound	AI	10.738440698094324	-43.60828641214335	185978
1a976d0aa2d01f71bf4883b4e35887d55b985cda	on the stopping criteria for k-nearest neighbor in positive unlabeled time series classification problems	transductive learning;positive unlabeled learning;self training;similarity measures;biology and life sciences;k nearest neighbor;classifiers;time series classification;variance	Positive unlabeled time series classification has become an important area during the last decade, as often vast amounts of unlabeled time series data are available but obtaining the corresponding labels is difficult. In this situation, positive unlabeled learning is a suitable option to mitigate the lack of labeled examples. In particular, self-training is a widely used technique due to its simplicity and adaptability. Within this technique, the stopping criterion, i.e., the decision of when to stop labeling, is a critical part, especially in the positive unlabeled context. We propose a self-training method that follows the positive unlabeled approach for time series classification and a family of parameter-free stopping criteria for this method. Our proposal uses a graphical analysis, applied to the minimum distances obtained by the k-Nearest Neighbor as the base learner, to estimate the class boundary. The proposed method is evaluated in an experimental study involving various time series classification datasets. The results show that our method outperforms the transductive results obtained by previous models.	time series	Mabel González Castellanos;Christoph Bergmeir;Isaac Triguero;Yanet Rodríguez;José Manuel Benítez	2016	Inf. Sci.	10.1016/j.ins.2015.07.061	semi-supervised learning;transduction;computer science;machine learning;pattern recognition;data mining;mathematics;variance;k-nearest neighbors algorithm	DB	15.356277039720231	-38.747346116960365	186026
474642f91be515fc50538e1f9df924d76ae886bf	a comparative study on the performance of several ensemble methods with low subsampling ratio	decision tree;ensemble method;time complexity;bagging;bundling;low subsampling ratio;comparative study;adaboost;error rate;scalability	In ensemble methods each base learner is trained on a resampled version of the original training sample with the same size. In this paper we have used resampling without replacement or subsampling to train base classifiers with low subsample ratio i.e., the size of each subsample is smaller than the original training sample. The main objective of this paper is to check if the scalability performance of several well known ensemble methods with low subsample ratio are competent and compare them with their original counterpart. We have selected three ensemble methods: Bagging, Adaboost and Bundling. In all the ensemble methods a full decision tree is used as the base classifier. We have applied the subsampled version of the above ensembles in several well known benchmark datasets to check the error rate. We have also checked the time complexity of each ensemble method with low subsampling ratio. From the experiments, it is apparent that in the case of bagging and adaboost with low subsampling ratio for most of the cases the error rate is inversely related with subsample size, while for bundling it is opposite. Overall performance of the ensemble methods with low subsampling ratio from experiments showed that bundling is superior in accuracy with low subsampling ratio in almost all the datasets, while bagging is superior in reducing time complexity.	chroma subsampling	Faisal Zaman;Hideo Hirose	2010		10.1007/978-3-642-12101-2_33	adaboost;time complexity;scalability;bootstrap aggregating;word error rate;computer science;machine learning;decision tree;comparative research;pattern recognition;statistics	HCI	14.474035189519528	-40.911472673909806	186536
7e0c667e71c636a1a7aa6103cbd07e291063a918	feature selection via maximizing fuzzy dependency	distance learning;feature selection;fuzzy rough sets;fuzzy dependency	Feature selection is an important preprocessing step in pat tern analysis and machine learning. The key issue in feature selection is to evaluate q uality of candidate features. In this work, we introduce a weighted distance learning algorithm for fea ture selection via maximizing fuzzy dependency. We maximize fuzzy dependency between features an d decision by distance learning and then evaluate the quality of features with the learned weigh t vector. The features deriving great weights are considered to be useful for classification learn ing. We test the proposed technique with some classical methods and the experimental results show th e proposed algorithm is effective.	experiment;feature selection;machine learning;mathematical optimization;mutual information;pattern recognition;preprocessor;rough set;selection algorithm;set theory	Qinghua Hu;Pengfei Zhu;Jinfu Liu;Yongbin Yang;Daren Yu	2010	Fundam. Inform.	10.3233/FI-2010-222	distance education;fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;fuzzy set operations;feature	AI	11.112909088392946	-39.56569345571934	186620
4fba467a2155ff5f5706abe3bae627d259349136	an ensemble learning approach to lip-based biometric verification, with a dynamic selection of classifiers		Abstract Machine learning approaches are largely focused on pattern or object classification, where a combination of several classifier systems can be integrated to help generate an optimal or suboptimal classification decision. Multiple classification systems have been extensively developed because a committee of classifiers, also known as an ensemble, can outperform the ensemble’s individual members. In this paper, a classification method based on an ensemble of binary classifiers is proposed. Our strategy consists of two phases: (1) the competence of the base heterogeneous classifiers in a pool is determined, and (2) an ensemble is formed by combining those base classifiers with the greatest competences for the given input data. We have shown that the competence of the base classifiers can be successfully calculated even if the number of their learning examples was limited. Such a situation is particularly observed with biometric data. In this paper, we propose a new biometric data structure, the Sim coefficients, along with an efficient data processing technique involving a pool of competent classifiers chosen by dynamic selection.	biometrics;ensemble learning;verification and validation	Piotr Porwik;Rafal Doroz;Krzysztof Wrobel	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.08.037	machine learning;abstract machine;artificial intelligence;biometrics;binary number;classifier (linguistics);data structure;ensemble learning;data processing;computer science	NLP	13.082886656877474	-44.10591953335459	187472
c5fb64faeedfd4ae1f2cdd2a3371c2706da7b143	hybrid probabilistic sampling with random subspace for imbalanced data learning	sampling method;ensemble learning;class imbalance;classification;random subspace method	Class imbalance is one of the challenging problems for machine learning in many real-world applications. Other issues, such as within-class imbalance and high dimensionality, can exacerbate the problem. We propose a method HPSDRS that combines two ideas: Hybrid Probabilistic Sampling technique ensemble with Diverse Random Subspace to address these issues. HPS improves the performance of traditional re-sampling algorithms with the aid of probability function, since it is not sufficient to simply manipulate the class sizes for imbalanced data with complex distribution. Moreover, DRS ensemble employs the minimum overlapping mechanism to provide diversity and weighted voting, so as to improve the generalization performance. The experimental results demonstrate that our method is efficient for learning from imbalanced data and can achieve better results than state-of-the-art methods for imbalanced data.	algorithm;bag-of-words model;benchmark (computing);contingency (philosophy);curse of dimensionality;ensemble kalman filter;experiment;feature vector;machine learning;multinomial logistic regression;oversampling;sampling (signal processing);synthetic data;text corpus;text mining	Peng Cao;Dazhe Zhao;Osmar R. Zaïane	2014	Intell. Data Anal.	10.3233/IDA-140686	random subspace method;sampling;biological classification;computer science;machine learning;pattern recognition;data mining;ensemble learning;statistics	AI	15.06130941828474	-41.57550591498301	187517
b14e5db5117205e33ae76a04294e0d98d7f24f89	using boosting to simplify classification models	classification model simplification;edge distribution;overfitting;mathematics;margin balancing;iterations;margin analysis;inverse sub contexts;information technology;generalisation error;error analysis pattern classification modelling generalisation artificial intelligence data mining;interpretability;data mining;data set partitioning;training data boosting classification model simplification ensemble classifiers classification error unseen cases overfitting generalisation error diagnostic measures edge analysis margin analysis inverse sub contexts border region detection edge distribution margin balancing iterations data set partitioning confounding removal mislabelled observations interpretability;artificial intelligent;training data;error analysis;boosting;border region detection;mislabelled observations;confounding removal;unseen cases;acoustical engineering;pattern classification;classification error;artificial intelligence;robustness;ensemble classifiers;generalisation artificial intelligence;computer science;edge analysis;computer errors;diagnostic measures;boosting computer errors mathematics error analysis information technology computer science australia robustness data mining acoustical engineering;australia	"""Ensemble classification techniques such as bagging, boosting and arcing algorithms have been shown to lead to reduced classification errors on unseen cases and seem immune to the problem of overfitting. Several explanations for the reduction in generalisation error have been presented, with recent authors defining and applying diagnostics such as """"edge"""" and """"margin"""". These measures provide insight into the behaviour of ensemble classifiers, but can they be exploited further? In this paper, a four-stage classification procedure in introduced, which is based on an extension of edge and margin analysis. This new procedure allows inverse sub-contexts and difficult border regions to be detected using properties of the edge distribution. It is widely known that ensemble classifiers 'balance' the margin as the number of iterations increases. However, by exploiting this balancing property and flagging observations whose edges (and margins) are not 'balanced', data sets can often be partitioned into sub contexts and the classification can be made more robust as confounding within a data set is removed. In the majority of cases, the sub-contexts detected are inverse to each other or, quite possibly, the smaller sub-context contains mis-labelled observations. The majority of classification techniques have not been adapted to detect contexts within a data set, and the generalisation error reported in studies to date is based on the entire data set and can be improved by partitioning the data set in question. The aim of this study is to move towards interpretability, and it is shown that, by training on a sub-set of the original training data, we gain simplicity of models and reduced generalisation error."""	algorithm;boosting (machine learning);ensemble kalman filter;ensemble learning;generalization error;iteration;overfitting	Virginia Wheway	2001		10.1109/ICDM.2001.989565	training set;iteration;acoustical engineering;computer science;machine learning;pattern recognition;data mining;overfitting;information technology;boosting;statistics;robustness	ML	14.845662293331193	-39.57600952874522	187549
774eded6de493f4354dbcd16f17f0308227e9e38	optimizing linear discriminant error correcting output codes using particle swarm optimization	sub classes;support vector machines;particle swarm optimization;error correcting output codes	Error Correcting Output Codes reveal an efficient strategy in dealing with multi-class classification problems. According to this technique, a multi-class problem is decomposed into several binary ones. On these created sub-problems we apply binary classifiers and then, by combining the acquired solutions, we are able to solve the initial multiclass problem. In this paper we consider the optimization of the Linear Discriminant Error Correcting Output Codes framework using Particle Swarm Optimization. In particular, we apply the Particle Swarm Optimization algorithm in order to optimally select the free parameters that control the split of the initial problem’s classes into sub-classes. Moreover, by using the Support Vector Machine as classifier we can additionally apply the Particle Swarm Optimization algorithm to tune its free parameters. Our experimental results show that by applying Particle Swarm Optimization on the Sub-class Linear Discriminant Error Correcting Output Codes framework we get a significant improvement in the classification performance.	algorithm;binary classification;code (cryptography);discriminant;mathematical optimization;multiclass classification;optimizing compiler;particle swarm optimization;phase-shift oscillator;support vector machine	Dimitrios Bouzas;Nikolaos Arvanitopoulos;Anastasios Tefas	2011		10.1007/978-3-642-21738-8_11	support vector machine;mathematical optimization;multi-swarm optimization;computer science;machine learning;pattern recognition;mathematics;particle swarm optimization;metaheuristic	ML	10.286205821156168	-42.20158672743181	187747
cd4a6185c82da73c520f1fdfa09c3ba0114ab25e	selecting samples for labeling in unbalanced streaming data environments	pattern clustering;sampling methods data analysis pattern classification pattern clustering random processes;grid density;stream data;thirty six dimension data set improvement unbalanced streaming data environment random sample selection extremely unbalanced stream data set labeling human resources data point labelling minority class samples classification model training streaming data ensemble classifier update minority class clusters grid density algorithm synthetic data set grid sizes data set dimensionality reduction data space sampling efficiency improvement eight dimension data set improvement;grid density labeling stream data classification;classification;data analysis;random processes;pattern classification;labeling clustering algorithms algorithm design and analysis data mining training data models classification algorithms;sampling methods;labeling	In this paper we proposed an alternative approach to random selection for labeling extremely unbalanced stream data sets where one class is only 1-10% of the entire data set. Labeling, especially when human resources are needed, is often time consuming and expensive. In an extremely unbalanced data set, usually a lot of data points need to be labeled to get enough minority class samples. The goal of this research was to reduce the total number of samples needed in the labeling process of training new classification models for updating streaming data ensemble classifier. Our proposed approach is to find minority class clusters using the grid density algorithm, and sample minority class instances inside those regions. The result from the synthetic data set showed that efficiency of our proposed approaches varies with different grid sizes. Results on real world data sets confirmed that observation, and showed that when the data set has high dimensionality, dimensionality reduction was useful for reducing the number of grids in the data space increasing sampling efficiency. Our best results showed 19.4% improvement for an eight-dimension data set without dimensionality reduction, and 27.4% improvement for a thirty-six-dimension data set with dimensionality reduction.	algorithm;data point;dataspaces;dimensionality reduction;ensemble learning;sampling (signal processing);sequence labeling;stream (computing);streaming media;synthetic data;unbalanced circuit	Hanqing Hu;Mehmed M. Kantardzic;Tegjyot Singh Sethi	2013	2013 XXIV International Conference on Information, Communication and Automation Technologies (ICAT)	10.1109/ICAT.2013.6684046	data stream clustering;computer science;machine learning;pattern recognition;data mining	HPC	13.718329197027836	-41.1571167686985	187795
a81e5def280574576abc0ad79a5f50e89df04d2d	a chaos embedded gsa-svm hybrid system for classification	gravitational search algorithm;chaotic search;feature selection;support vector machine;parameter optimization	Parameter optimization and feature selection influence the classification accuracy of support vector machine (SVM) significantly. In order to improve classification accuracy of SVM, this paper hybridizes chaotic search and gravitational search algorithm (GSA) with SVM and presents a new chaos embedded GSA-SVM (CGSA-SVM) hybrid system. In this system, input feature subsets and the SVM parameters are optimized simultaneously, while GSA is used to optimize the parameters of SVM and chaotic search is embedded in the searching iterations of GSA to optimize the feature subsets. Fourteen UCI datasets are employed to calculate the classification accuracy rate in order to evaluate the developed CGSA-SVM approach. The developed approach is compared with grid search and some other hybrid systems such as GA-SVM, PSO-SVM and GSA-SVM. The results show that the proposed approach achieves high classification accuracy and efficiency compared with well-known similar classifier systems.	embedded system;experiment;feature selection;fitness function;global storage architecture;hybrid system;iteration;mathematical optimization;particle swarm optimization;phase-shift oscillator;search algorithm;software release life cycle;support vector machine	Chaoshun Li;Xueli An;Ruhai Li	2014	Neural Computing and Applications	10.1007/s00521-014-1757-z	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection	AI	10.121606177131367	-42.02623706797236	188588
252ee96c762c918a304245906ab496083e6325a1	multi-class intelligent fault diagnosis approach based on modified relevance vector machine	standards;support vector machines;training;testing;classification algorithms;decision trees;fault diagnosis	In order to solve the problem of fault data with small sample and nonlinear in fault diagnosis and improve support vector machine, a fault diagnostic approach based on the multi-class classification method of One-Against-Rest (OAR) algorithm and decision tree is proposed combined with relevance vector machine. The above classification method modifies the current OAR algorithm using decision tree during the testing phase. To be specific, the K classifiers of OAR algorithm are arranged to form a decision tree in descending order and to reduce the average testing numbers of classifiers is the optimization object. Meanwhile, the threshold value of distance function is set and function value of each classifier is calculated in the sequence of decision tree. Once the function value of the i-th classifier exceeds the threshold, the testing sample will be assigned to the i-th class without any other evaluations. If none of values exceeds the threshold, the sample is classified to the class of the maximal decision function value as same as that of OAR. Theoretical analysis and experimental results both demonstrate that the presented approach performs better than traditional methods in terms of diagnosis time, diagnosis accuracy and diagnosis efficiency.	algorithm;analogue electronics;decision tree;mathematical optimization;maximal set;multiclass classification;nonlinear system;opensimulator;relevance vector machine;sorting;sparse matrix;support vector machine	Jianshe Kang;Kun Wu;Kuo Chi;Yinxia Du	2016	2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)	10.1109/INCoS.2016.66	statistical classification;support vector machine;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;software testing	ML	11.572369841855238	-39.333922246199236	188723
0acf1a74e6ed8c323192d2b0424849820fe88715	support vector machine active learning with applications to text classification	numerous real-world;support vector machine;training set;pool-based active learning;significant success;training instance;text classification;active learning method;active learning;new algorithm;classification;machine learning;support vector machines	Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.	active learning (machine learning);algorithm;document classification;machine learning;randomness;statistical classification;support vector machine;test set;version space learning	Simon Tong;Daphne Koller	2001	Journal of Machine Learning Research		semi-supervised learning;unsupervised learning;support vector machine;multi-task learning;instance-based learning;wake-sleep algorithm;computer science;online machine learning;machine learning;pattern recognition;data mining;supervised learning;stability;relevance vector machine;computational learning theory;active learning;structured support vector machine;generalization error	ML	15.882012369695913	-39.7523927265963	188732
b041f6a3a55f3bd1613dc3f026846c4bd6f3fea5	unsupervised feature selection based on decision graph		In applications of algorithms, feature selection has got much attention of researchers, due to its ability to overcome the curse of dimensionality, reduce computational costs, increase the performance of the subsequent classification algorithm and output the results with better interpretability. To remove the redundant and noisy features from original feature set, we define local density and discriminant distance for each feature vector, wherein local density is used for measuring the representative ability of each feature vector, and discriminant distance is used for measuring the redundancy and similarity between features. Based on the above two quantities, the decision graph score is proposed as the evaluation criterion of unsupervised feature selection. The method is intuitive and simple, and its performances are evaluated in the data classification experiments. From statistical tests on the averaged classification accuracies over 16 real-life dataset, it is observed that the proposed method obtains better or comparable ability of discriminant feature selection in 98% of the cases, compared with the state-of-the-art methods.	algorithm;curse of dimensionality;decision tree;discriminant;experiment;feature selection;feature vector;performance;real life;unsupervised learning	Jinrong He;Yingzhou Bi;Lixin Ding;Zhaokui Li;Shenwen Wang	2016	Neural Computing and Applications	10.1007/s00521-016-2737-2	minimum redundancy feature selection;feature vector;machine learning;pattern recognition;data mining;mathematics;k-nearest neighbors algorithm;feature;statistics;dimensionality reduction	AI	11.832223658023787	-43.518342598053124	188738
3ae3c14afc8a7b6bf93b95371dd98e2fc9b872c1	using class imbalance learning for cross-company defect prediction		Cross-company defect prediction (CCDP) is a practical way that trains a prediction model by exploiting one or multiple projects of a source company and then applies the model to target company. Unfortunately, the performance of such CCDP models is susceptible to the high imbalanced nature between the defect-prone and non-defect classes of CC data. Class imbalance learning is applied to alleviate this issue. Because many class imbalance learning methods have been proposed, there is an imperative need to analyze and compare the performance of these methods for CCDP. Although prior empirical studies have proven AdaBoost.NC algorithm achieves the best performance for defect prediction. This observation leads us to conduct a careful empirical study the issues of if and how class imbalance learning methods can benefit cross-company defect prediction. We investigate different types of class imbalance learning methods, including under-sampling technique, over-sampling technique and over sampling followed by under-sampling technique on the cross-company defect prediction performance over 15 publicly available datasets. Experimental results show that under-sampling technique achieves the best overall performance in terms of the gmeasure among those methods we studied. Keywords—software defect prediction;cross-company defect prediction; class imbalance learning	adaboost;algorithm;experiment;imperative programming;oversampling;real life;sampling (signal processing);software bug;software engineer	Xiao Yu;Mingsong Zhou;Xu Chen;Lijun Deng	2017		10.18293/SEKE2017-035	machine learning;computer science;systems engineering;artificial intelligence	SE	14.992321142478328	-43.644741432020915	188743
bcc85230b4fccf54b5c3d8dcce34f85be392435c	a hybrid method for improved stability prediction in construction projects: a case study of stope hangingwall stability		Abstract Artificial intelligence (AI) approaches have proliferated in stability prediction of construction projects in the past decade. However, the application of AI approaches did not reach the peak of its potential due to the inappropriate handling of missing data and the omission of state-of-the-art techniques. In the present contribution, we proposed a hybrid method for the improved stability prediction of construction projects based on individual machine learning (ML) algorithms, input missing data imputation, semi-supervised learning and the classifier ensemble. Seven ML algorithms were selected to build individual classifiers for the classifier ensemble. 5-fold cross validation was used as the validation method and the performance measures were chosen to be the confusion matrix, the receiver operating characteristic (ROC) curve and the area under ROC curve (AUC). Exhaustive grid search and firefly algorithm were used for hyper-parameters and weights tuning respectively. The capability of the proposed method was verified using an underground construction dataset, the stope hangingwall (HW) dataset. The case study shows that the input missing data imputation and semi-supervised learning improved the predictive performance of ML algorithms on HW stability prediction. The highest and average AUC values on the testing set were increased to 0.954 and 0.923 respectively on the expanded dataset, compared with 0.879 and 0.860 on the original complete dataset. Further improvement was obtained through the classifier ensemble, with the AUC value being increased to 0.976. Harnessing such method extends recent efforts for stability prediction in construction projects, and can significantly accelerate the project design and stability management.		Chongchong Qi;Andy Fourie;Guowei Ma;Xiaolin Tang	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.07.035	missing data;semi-supervised learning;hyperparameter optimization;imputation (statistics);mathematics;artificial intelligence;cross-validation;firefly algorithm;machine learning;receiver operating characteristic;confusion matrix	Logic	12.102445203877597	-40.98180060301791	188769
259dd9047b626ac806bbf2dddb7005069724cbec	non-negative sparse-based semiboost for software defect prediction	software defect prediction;ensemble learning;semi supervised learning;non negative sparse based semiboost nssb	Software defect prediction is an important decision support activity in software quality assurance. The limitation of the labelled modules usually makes the prediction difficult, and the class-imbalance characteristic of software defect data leads to negative influence on decision of classifiers. Semi-supervised learning can build high-performance classifiers by using large amount of unlabelled modules together with the labelled modules. Ensemble learning achieves a better prediction capability for class-imbalance data by using a series of weak classifiers to reduce the bias generated by the majority class. In this paper, we propose a new semi-supervised software defect prediction approach, non-negative sparse-based SemiBoost learning. The approach is capable of exploiting both labelled and unlabelled data and is formulated in a boosting framework. In order to enhance the prediction ability, we design a flexible non-negative sparse similarity matrix, which can fully exploit the similarity of historical data by incorporating the non-negativity constraint into sparse learning for better learning the latent clustering relationship among software modules. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that non-negative sparse-based SemiBoost learning outperforms several representative state-of-the-art semi-supervised software defect prediction methods. Copyright © 2016 John Wiley & Sons, Ltd.	boosting (machine learning);cluster analysis;decision support system;ensemble learning;john d. wiley;negativity (quantum mechanics);semi-supervised learning;semiconductor industry;similarity measure;software bug;software quality assurance;sparse matrix;supervised learning;test data	Tiejian Wang;Zhiwu Zhang;Xiaoyuan Jing;Yanli Liu	2016	Softw. Test., Verif. Reliab.	10.1002/stvr.1610	computer science;machine learning;pattern recognition;data mining;ensemble learning	SE	14.934326421839142	-44.0228639073009	188787
0c0496d2dbbc723d689ee23c39441c86c167695f	learning coordination classifiers	classifier combination;independent identically distributed;classification accuracy	We present a new approach to ensemble classification that requires learning only a single base classifier. The idea is to learn a classifier that simultaneously predicts pairs of test labels—as opposed to learning multiple predictors for single test labels— then coordinating the assignment of individual labels by propagating beliefs on a graph over the data. We argue that the approach is statistically well motivated, even for independent identically distributed (iid) data. In fact, we present experimental results that show improvements in classification accuracy over single-example classifiers, across a range of iid data sets and over a set of base classifiers. Like boosting, the technique increases representational capacity while controlling variance through a principled form of classifier combination.	boosting (machine learning);statistical classification	Yuhong Guo;Russell Greiner;Dale Schuurmans	2005			random subspace method;margin classifier;quadratic classifier;computer science;machine learning;linear classifier;pattern recognition;data mining	AI	16.865931081344563	-38.26460970635814	189231
62aef23258137c36987c063e3daa8b6f82030b92	ensemble learning for generalised eigenvalues proximal support vector machines	ensemble learning;support vector machines;generalisation ability;gepsvm;generalised eigenvalues;pattern classification;nonparallel hyperplanes;artificial intelligence;svm	In this paper, to improve the generalisation ability of generalised eigenvalues proximal support vector machines GEPSVM, we propose an ensemble GEPSVM, called EnGEP for short. Note that GEPSVM is not sensitive to different weights of the points, to increase the potential diversity of GEPSVM, firstly, we introduce an extra parameter in GEPSVM, which gives different penalties for two non-hyperplanes determines by GEPSVM. Then, we use a novel bagging strategy to ensemble GEPSVM with additional parameters. Experimental results both on artificial and benchmark datasets show that our EnGEP improves the generalisation performance of GEPSVM greatly, and it also reveals the effectiveness of our EnGEP.	ensemble learning;support vector machine	Weijie Chen;Yuanhai Shao;Yibo Jiang;Chongpu Xia	2013	IJCAT	10.1504/IJCAT.2013.054359	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;ensemble learning	ML	14.613941500951668	-40.68985915323613	189977
ced7772a77bad608a09360449bd81e74a41da585	special issue on “solving complex machine learning problems with ensemble methods”		Ensemble methods have become a popular tool within the machine learning community for dealing with diverse types of learning problems. The main reason for this is the good generalization performance and the robustness against noise of these techniques. In particular, ensemble methods provide a straightforward way of improving the predictive performance of base classifiers simply by combining several of the decisions made by these predictors. Furthermore, ensemble methods may be very useful to alleviate over-fitting problems that typically arise in the presence of noisy or corrupted data. The consequence is that multiple classifier systems represent a solution that is typically considered in applications where high predictive performance is strictly required. It is not a coincidence that ensemble methods were very successful in recent machine learning competitions. The main goal of this special issue is to spread the knowledge about ensemble strategies that not only focus on supervised classification, but can also be used to solve difficult and general machine learning problems related to different fields of expertise. In that sense, the target audience of the special issue includes not only members of the ensemble learning community, but also researchers from other fields that could benefit from using such techniques to address interesting research problems. A total of 35 submissions were received in this special issue, from which 12 papers were eventually accepted for publication. Each submission went through an rigorous peer-review process. All papers received at least two independent reviews while some papers received more than three. Three of the papers included in this issue are extended papers selected from the COPEM workshop that was held in Prague, Czech Republic, on September 27th, 2013. The COPEM workshop was co-located with the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML/PKDD 2013). The accepted papers cover diverse topics of interest in ensemble learning such as feature selection, clustering, ensemble pruning, classical ensemble methods and several challenging applications. They are all briefly summarized in the following paragraphs. The paper by H. Njah and S. Jamoussi describes an ensemble method that can be used to address the problem of inferring a directed graphical model, i.e., a Bayesian network, to describe interactions among different genes. Such a gene regulatory network can be very useful to further understand how the expression level of different genes relates to each other. Furthermore, practitioners may try to exploit these interactions with the aim of drug development or disease treatment. A difficulty found in this process is, however, that the micro-array data employed for inference is high-dimensional and particularly scarce, which may lead to over-fitting in the induction process. The mentioned paper describes a fuzzy ensemble clustering method that can be used to find small sub-sets of highly correlated genes to over-come the problem of high-dimensionality in microarray data. These authors also describe a method to generate a committee of Bayesian networks in which different algorithm for structure learning are combined. Given the networks generated by these methods, a final combination stage produces a final Bayesian network. The proposed method is tested on synthetic and real data, and is biologically validated on data corresponding to patients suffering from amyotrophic lateral sclerosis. The paper by R. Kanawati applies ensemble methods in the task of discovering local communities (or ego-centred communities) in complex networks. Common methods for this task use a greedy strategy that maximizes a quality function. Those nodes, from the neighborhood of the starting node that maximize the quality function, are added incrementally to the community. The author proposes to employ ensemble methods (ensemble ranking techniques as well as clustering ensembles) in order to aggregate the results obtained from several quality functions. The experimental evaluation on real-world networks shows that the method proposed outperforms other approaches form the literature and leads to superior performance. An interesting application is presented in the paper by J. Mendes-Moreira and his colleagues. They provide an ensemble method for predicting long-term time duration of a journey using public transportation. To create the ensemble, they employ a large number of baseline classifiers using different parameters settings. Their results confirm the utility of heterogeneous ensembles for long-term travel time prediction. In the paper by R.S. Smith and T. Windeatt, the authors employ ensemble methods for solving the problem of facial action unit recognition where the objective is to classify the facial movements in one of the 44 predefined classes. The problem is treated as multi-class and the authors employ error-correcting output code (ECOC) models where spectral coefficients are used in the weighted ECOC decoding. Additionally, the authors propose the use of Platt scaling in order to convert the scores of the individual classifiers into probabilities. The experimental results show that the proposed methodology achieves good results in facial recognition tasks and also that the score-to-probability calibration technique is deemed necessary when combining ECOC outputs. The paper by M. Kim deals with problems having structured outputs where the labels are correlated (sequence tagging problems). The author proposes a boosting-based methodology which is used in order to combine the base classifiers inferred from a Conditional Random Field (CRF) model. More specifically, the	aggregate data;baseline (configuration management);bayesian network;cluster analysis;coefficient;complex network;conditional random field;data mining;ecml pkdd;ensemble learning;error detection and correction;facial recognition system;feature selection;gene regulatory network;graphical model;greedy algorithm;image scaling;interaction;lateral thinking;machine learning;microarray;overfitting;supervised learning;synthetic data	Daniel Hernández-Lobato;Ioannis Katakis;Gonzalo Martínez-Muñoz;Ioannis Partalas	2015	Neurocomputing	10.1016/j.neucom.2014.09.038	computer science;artificial intelligence;theoretical computer science;machine learning	AI	16.164966882917227	-41.69006424386206	191038
10dd5ac23ec470379da5bf27e0f6ece71b65af8b	outcome prediction of dota2 based on naïve bayes classifier		Although DOTA2 is a popular game around the world, no clear algorithm or software are designed to forecast the winning probability by analyzing the lineups. However, the author finds that Naive Bayes classifier, one of the most common classification algorithm, can analyze the lineups and predict the outcome according to the lineups and gives an improved Naive Bayes classifier. Using the DOTA2 data set published in the UCI Machine Learning Repository, we test Naive Bayes classifier's prediction of respective winning probability of both sides in the game. The results show that Naive Bayes classifier is a practical tool to analyze the lineups and predict the outcome based on players' choices.	algorithm;dota 2;machine learning;naive bayes classifier	Kaixiang Wang;Wenqian Shang	2017	2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS)	10.1109/ICIS.2017.7960061	naive bayes classifier;algorithm design;software;machine learning;probabilistic classification;statistical classification;bayes classifier;artificial intelligence;pattern recognition;computer science;bayes error rate	ML	11.416294385855144	-38.404354446707266	191134
be559addebb04b067eec4be08b99dea7e08944a2	a hybrid decision tree training method using data streams	concept drift;decision tree;nearest hyperrectangle;incremental learning;pattern recognition	Classical classification methods usually assume that pattern recognition models do not depend on the timing of the data. However, this assumption is not valid in cases where new data frequently become available. Such situations are common in practice, for example, spam filtering or fraud detection, where dependencies between feature values and class numbers are continually changing. Unfortunately, most classical machine learning methods (such as decision trees) do not take into consideration the possibility of the model changing, as a result of so-called concept drift and they cannot adapt to a new classification model. This paper focuses on the problem of concept drift, which is a very important issue, especially in data mining methods that use complex structures (such as decision trees) for making decisions. We propose an algorithm that is able to co-train decision trees using a modified NGE (Nested Generalized Exemplar) algorithm. The potential for adaptation of the proposed algorithm and the quality thereof are evaluated through computer experiments, carried out on benchmark datasets from the UCI Machine Learning Repository.	algorithm;algorithmic efficiency;benchmark (computing);co-training;computation;computer experiment;concept drift;data mining;decision tree learning;email filtering;experiment;genetic algorithm;knowledge representation and reasoning;machine learning;pattern recognition;teaching method;test set	Michal Wozniak	2010	Knowledge and Information Systems	10.1007/s10115-010-0345-5	decision tree learning;computer science;artificial intelligence;concept drift;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;database;id3 algorithm;statistics	ML	14.645373538432713	-38.32589519694499	191495
4e3d067938c6666965910fe97c428bb5ef5b59f0	active learning using dirichlet processes for rare class discovery and classification		Real-world classification problems, such as visual surveillance and network intrusion detection, often contain common yet uninteresting background classes and rare but interesting classes, that need to be both discovered and classified. Active learning offers a suitable solution to joint rare class discovery and classification, by minimising the manual labelling of training data. A novel active learning approach is proposed, which automatically balances the competing goals of new class discovery and improving classification. Crucially it is free of tuneable parameters. Using Dirichlet processes a new active learning criterion is formulated, based on first computing the probability that unlabelled exemplars are from a new class, in addition to existing classes, and subsequently the probability of misclassification, which is then used for query selection. The proposed approach works with any probabilistic classification model and its effectiveness is demonstrated on multiple problems.	active learning (machine learning);algorithm;heuristic;intrusion detection system;probabilistic turing machine;statistical classification	Tom S. F. Haines;Tao Xiang	2011		10.5244/C.25.9	computer science;machine learning;pattern recognition;data mining;one-class classification	ML	16.97805685580941	-40.222346932949065	191754
052bf25c48aad651c86f544d4782cd4d01753378	phenotype prediction with semi-supervised classification trees		In this work, we address the task of phenotypic traits prediction using methods for semi-supervised learning. More specifically, we propose to use supervised and semi-supervised classification trees as well as supervised and semi-supervised random forests of classification trees. We consider 114 datasets for different phenotypic traits referring to 997 microbial species. These datasets present a challenge for the existing machine learning methods: they are not labelled/annotated entirely and their distribution is typically imbalanced. We investigate whether approaching the task of phenotype prediction as a semi-supervised learning task can yield improved predictive performance. The results suggest that the semi-supervised methodology considered here is especially helpful when using single trees, especially when the amount of labeled data ranges from 20 to 40%. Similar improvements can be seen when the presence of the phenotype is very imbalanced.	decision tree;machine learning	Jurica Levatic;Maria Brbic;Tomaz Stepisnik Perdih;Dragi Kocev;Vedrana Vidulin;Tomislav Smuc;Fran Supek;Saso Dzeroski	2017		10.1007/978-3-319-78680-3_10	labeled data;semi-supervised learning;phenotype;decision tree;binary classification;random forest;phenotypic trait;computer science;pattern recognition;artificial intelligence	ML	13.738202504416455	-43.2299717099912	191891
14ef619739b22993ee0824b549772f4383a3b1bc	bagging and boosting negatively correlated neural networks	algorithms computer simulation models statistical neural networks computer statistics as topic;diversity;heart disease;machine learning algorithms;neural networks;ensemble learning;neural nets;negboost algorithm;bagging;constructive approach;benchmark problem;correlation methods;machine learning correlated neural network ensemble negative correlation learning algorithm negbagg algorithm negboost algorithm negative bagging boosting algorithm;negative correlation learning;journal article;neural nets correlation methods learning artificial intelligence;negbagg algorithm;neural network nn ensemble design;boosting;correlated neural network ensemble;machine learning;soybean;neurons;learning artificial intelligence;negative correlation learning algorithm;negative bagging boosting algorithm;neural network nn ensemble design bagging boosting constructive approach diversity generalization negative correlation learning;breast cancer;generalization;algorithm design and analysis;benchmark testing;credit cards;australia;neural network;bagging boosting neural networks algorithm design and analysis neurons machine learning algorithms benchmark testing machine learning australia credit cards	In this paper, we propose two cooperative ensemble learning algorithms, i.e., NegBagg and NegBoost, for designing neural network (NN) ensembles. The proposed algorithms incrementally train different individual NNs in an ensemble using the negative correlation learning algorithm. Bagging and boosting algorithms are used in NegBagg and NegBoost, respectively, to create different training sets for different NNs in the ensemble. The idea behind using negative correlation learning in conjunction with the bagging/boosting algorithm is to facilitate interaction and cooperation among NNs during their training. Both NegBagg and NegBoost use a constructive approach to automatically determine the number of hidden neurons for NNs. NegBoost also uses the constructive approach to automatically determine the number of NNs for the ensemble. The two algorithms have been tested on a number of benchmark problems in machine learning and NNs, including Australian credit card assessment, breast cancer, diabetes, glass, heart disease, letter recognition, satellite, soybean, and waveform problems. The experimental results show that NegBagg and NegBoost require a small number of training epochs to produce compact NN ensembles with good generalization.	algorithm;benchmark (computing);biological neural networks;boosting (machine learning);concentrate dosage form;decorrelation;diabetes mellitus;ensemble learning;epoch (reference date);generalization (psychology);heart diseases;karp's 21 np-complete problems;machine learning;sixty nine;waveform	Md. Monirul Islam;Xin Yao;S. M. Shahriar Nirjon;Muhammad Asiful Islam;Kazuyuki Murase	2008	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2008.922055	generalization;algorithm design;benchmark;bootstrap aggregating;computer science;artificial intelligence;breast cancer;machine learning;pattern recognition;boosting;artificial neural network	ML	13.63751660537719	-42.13526198322301	192056
58cabf699985737dd05954106357f7e275dd1a8e	branch and bound based feature elimination for support vector machine based classification of hyperspectral images	classical combinatorial problem;geophysical image processing;regression scenario;classification algorithm;hyperspectral imaging genetic algorithms support vector machines classification algorithms correlation training;small sample size;support vector machines;branch and bound search;training;genetic algorithm based feature selection;combinatorial problems;image classification;classification scenario;hyperspectral image classification classical combinatorial problem pattern recognition data mining classification scenario regression scenario branch and bound search bhattacharya distance based feature selection hyperspectral data support vector machine classifiers genetic algorithm based feature selection feature reduction small sample size situations hyperspectral imaging feature elimination;data mining;hybrid approach;feature reduction;feature elimination;classification algorithms;hyperspectral data;pattern recognition;genetic algorithm;small sample size situations;genetic algorithms;feature selection;support vector machines data mining genetic algorithms geophysical image processing geophysical techniques image classification pattern recognition;support vector machine;correlation;hyperspectral imaging;support vector machine classifiers;hyperspectral image classification;branch and bound;hyperspectral image;bhattacharya distance based feature selection;geophysical techniques;support vector machines feature selection branch and bound genetic algorithm hyperspectral imaging	Feature selection (FS) is a classical combinatorial problem in pattern recognition and data mining. It finds major importance in classification and regression scenarios. In this paper, a hybrid approach that combines branch-and-bound (BB) search with Bhattacharya distance based feature selection is presented for classifying hyperspectral data using Support Vector Machine (SVM) classifiers. The performance of this hybrid approach is compared to another hybrid approach that uses genetic algorithm (GA) based feature selection in place of BB. It is also compared to baseline SVMs with no feature reduction. Experimental results using hyperspectral data show that under small sample size situations, BB approach performs better than GA and SVM with no feature selection.	baseline (configuration management);branch and bound;data mining;feature selection;genetic algorithm;pattern recognition;software release life cycle;statistical classification;support vector machine	Sathishkumar Samiappan;Saurabh Prasad;Lori M. Bruce;Eric A. Hansen	2011	2011 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2011.6049725	support vector machine;genetic algorithm;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;feature	Vision	10.998280325629077	-42.80728228240386	193658
1fb5ecee73bada63421ee50b6d94b2994e930bfb	a learning scheme for a fuzzy k-nn rule	nn rules;learning procedure;fuzzy decisions;probability of misclassification	The performance of a fuzzy k-NN rule depends on the number k and a fuzzy membership-array W[I, mR], where l and m R denote the number of classes and the number of elements in the reference set X R respectively. The proposed learning procedure consists in iterative finding such k and W which minimize the error rate estimated by the 'leaving one out' method.	iterative method;k-nearest neighbors algorithm	Adam Józwik	1983	Pattern Recognition Letters	10.1016/0167-8655(83)90064-8	defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set operations	AI	12.101893732274323	-39.37765763050125	194349
8f15c6233f95b1906c74a37e1d001e4f7e213ef9	decision trees and their families in imbalanced pattern recognition: recognition with and without rejection		Decision trees are considered to be among the best classifiers. In this work we use decision trees and its families to the problem of imbalanced data recognition. Considered are aspects of recognition without rejection and with rejection: it is assumed that all recognized elements belong to desired classes in the first case and that some of them are outside of such classes and are not known at classifier’s training stage. The facets of imbalanced data and recognition with rejection affect different real world problems. In this paper we discuss results of experiment of imbalanced data recognition on the case study of music notation symbols. Decision trees and three methods of joining decision trees (simple voting, bagging and random forest) are studied. These methods are used for recognition without and with rejection.		Wladyslaw Homenda;Wojciech Lesinski	2014		10.1007/978-3-662-45237-0_22	artificial intelligence;computer science;machine learning;optical music recognition;voting;random forest;pattern recognition;decision tree;notation;classifier (linguistics)	Vision	15.247898613267068	-42.420087662320604	194367
3b74b12d1d30847e1db48763dbbc2b9880cdb2fb	classifier subset selection for biomedical named entity recognition	classifier subset selection;classifier ensemble;biomedical named entity recognition;named entity recognition;weighted voting;subset selection;genetic algorithm;genetic algorithms;difference set;classifier ensembles;majority voting;natural language processing	Classifier ensembling approach is considered for biomedical named entity recognition task. A vote-based classifier selection scheme having an intermediate level of search complexity between static classifier selection and real-valued and class-dependent weighting approaches is developed. Assuming that the reliability of the predictions of each classifier differs among classes, the proposed approach is based on selection of the classifiers by taking into account their individual votes. A wide set of classifiers, each based on a different set of features and modeling parameter setting are generated for this purpose. A genetic algorithm is developed so as to label the predictions of these classifiers as reliable or not. During testing, the votes that are labeled as being reliable are combined using weighted majority voting. The classifier ensemble formed by the proposed scheme surpasses the full object F-score of the best individual classifier by 2.75% and it is the highest score achieved on the data set considered.	f1 score;genetic algorithm;named-entity recognition	Nazife Dimililer;Ekrem Varoglu;Hakan Altinçay	2008	Applied Intelligence	10.1007/s10489-008-0124-0	margin classifier;margin;genetic algorithm;cascading classifiers;classifier;quadratic classifier;computer science;artificial intelligence;machine learning;pattern recognition;data mining	Web+IR	11.034746051822172	-41.56165200971606	195146
f4cb7f47ef17a1182663c90eca1d3dcec2b9e7d2	weight learning in weighted elm classification model based on genetic algorithms		In cost sensitive classification problems we often suppose to have a known cost matrix in which each element represents the cost of mistakenly classifying an object from one class into another. Weighted least square, which does not equally consider individual classes and therefore assigns a different weight to each class of samples, is a typical approach to dealing with cost sensitive classification problems. Theoretically and experimentally it is confirmed that reasonable class weights will greatly improve classification ability of a learning model. Unfortunately we only know that these weights depend generally on cost matrix but very few methods can be used to specifically determine these weights according to cost matrix. This paper proposes a weighted least square (WLS) model of random weight network and then successfully uses the model in cost sensitive classification. A genetic algorithm to determine weights of different sample classes based on a cost matrix is given. Model analysis and experimental simulations are conducted. Considering the total misclassification cost as the evaluation index, a comparative study shows that our WLS model is far superior to the existing cost sensitive ELM and cost sensitive naive Bayes models.	elm;experiment;genetic algorithm;least squares;naive bayes classifier;simulation	Peng Yao;Xizhao Wang	2018	2018 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2018.8526986	support vector machine;genetic algorithm;naive bayes classifier;machine learning;decision tree;artificial intelligence;computer science;matrix (mathematics);data modeling;pattern recognition;least squares	AI	12.178177587244285	-38.96160998922964	195662
82cb592b82defabbbddaa2583539ec0da630394c	tracking recurring contexts using ensemble classifiers: an application to email filtering	cluster algorithm;concept drift;data stream;data mining;machine learning	Concept drift constitutes a challenging problem for the machine learning and data mining community that frequently appears in real world stream classification problems. It is usually defined as the unforeseeable concept change of the target variable in a prediction task. In this paper, we focus on the problem of recurring contexts, a special sub-type of concept drift, that has not yet met the proper attention from the research community. In the case of recurring contexts, concepts may re-appear in future and thus older classification models might be beneficial for future classifications. We propose a general framework for classifying data streams by exploiting stream clustering in order to dynamically build and update an ensemble of incremental classifiers. To achieve this, a transformation function that maps batches of examples into a new conceptual representation model is proposed. The clustering algorithm is then applied in order to group batches of examples into concepts and identify recurring contexts. The ensemble is produced by creating and maintaining an incremental classifier for every concept discovered in the data stream. An experimental study is performed using (a) two new real-world concept drifting datasets from the email domain, (b) an instantiation of the proposed framework and (c) five methods for dealing with drifting concepts. Results indicate the effectiveness of the proposed representation and the suitability of the concept-specific classifiers for problems with recurring contexts.	algorithm;benchmark (computing);cluster analysis;concept drift;data mining;email filtering;emoticon;experiment;incremental compiler;machine learning;map;news aggregator;organizing (structure);statistical classification;streaming media;universal instantiation	Ioannis Katakis;Grigorios Tsoumakas;Ioannis P. Vlahavas	2009	Knowledge and Information Systems	10.1007/s10115-009-0206-2	computer science;artificial intelligence;concept drift;machine learning;pattern recognition;data mining;database;data stream mining	AI	13.997580005704675	-38.07504950744649	195702
741104eafbd0ce17920d54d832762482b48dc32c	an improved method to construct basic probability assignment based on the confusion matrix for classification problem	basic probability assignment;belief function;classification;confusion matrix;dempster shafer evidence theory;transmembrane proteins	The determination of basic probability assignment (BPA) is a crucial issue in the application of Dempster-Shafer evidence theory. Classification is a process of determining the class label that a sample belongs to. In classification problem, the construction of BPA based on the confusion matrix has been studied. However, the existing methods do not make full use of the available information provided by the confusion matrix. In this paper, an improved method to construct the BPA is proposed based on the confusion matrix. The proposed method takes into account both the precision rate and the recall rate of each class. An illustrative case regarding the prediction of transmembrane protein topology is given to demonstrate the effectiveness of the proposed method.	confusion matrix	Xinyang Deng;Qi Liu;Yong Deng;Sankaran Mahadevan	2016	Inf. Sci.	10.1016/j.ins.2016.01.033	transmembrane protein;confusion matrix;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics	AI	10.37320257506857	-39.73344045403017	196100
6684514eb61bc33994b62e1b427d1cecdf394362	a selective sampling approach to active feature selection	learning;feature selection and ranking;random sampling;selective sampling;sampling;dimensionality reduction;machine learning;evaluation measure;feature selection;dimensional reduction	Feature selection, as a preprocessing step to machine learning, has been very effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. Traditional feature selection methods resort to random sampling in dealing with data sets with a huge number of instances. In this paper, we introduce the concept of active feature selection, and investigate a selective sampling approach to active feature selection in a filter model setting. We present a formalism of selective sampling based on data variance, and apply it to a widely used feature selection algorithm Relief. Further, we show how it realizes active feature selection and reduces the required number of training instances to achieve time savings without performance deterioration. We design objective evaluation measures of performance, conduct extensive experiments using both synthetic and benchmark data sets, and observe consistent and significant improvement. We suggest some further work based on our study and experiments.  2004 Elsevier B.V. All rights reserved.	benchmark (computing);dhrystone;experiment;feature selection;machine learning;monte carlo method;preprocessor;relevance;sampling (signal processing);selection algorithm;semantics (computer science)	Huan Liu;Hiroshi Motoda;Lei Yu	2004	Artif. Intell.	10.1016/j.artint.2004.05.009	sampling;feature extraction;computer science;machine learning;pattern recognition;data mining;feature selection;feature;dimensionality reduction	AI	17.268273390685316	-40.29850290195788	196830
f71e4b90ce0a5fa15b7a148e97485c0b92d816a9	editing training data for knn classifiers with neural network ensemble	neural network ensemble	Since kNN classifiers are sensitive to outliers and noise contained in the training data set, many approaches have been proposed to edit the training data so that the performance of the classifiers can be improved. In this paper, through detaching the two schemes adopted by the Depuration algorithm, two new editing approaches are derived. Moreover, this paper proposes to use neural network ensemble to edit the training data for kNN classifiers. Experiments show that such an approach is better than the approaches derived from Depuration, while these approaches are better than or comparable to Depuration.	artificial neural network;k-nearest neighbors algorithm;test set	Yuan Jiang;Zhi-Hua Zhou	2004		10.1007/978-3-540-28647-9_60	random subspace method;computer science;machine learning;pattern recognition;data mining	Vision	13.377057533325969	-40.97119878141988	197172
eae3aeda110449c2c4d8bf389aca7b6b5e078d8e	an optimized band selection scheme for hyperspectral imagery analysis	band selection;pso;会议论文;hyperspectral image	A particle swarm optimization (PSO)-based automatic system to determine the number of optimal band sets and corresponding bands is proposed. A simple searching criterion function, called minimum estimated abundance covariance (MEAC), requiring class signatures only, is used in this method. The experimental results show that the PSO-based algorithm outperforms the popular sequential forward selection (SFS) method. The two PSOs can achieve the balance between the two objectives, and it may provide even better accuracy than a single-objective PSO.	algorithm;antivirus software;clustered file system;loss function;mathematical optimization;particle swarm optimization;stepwise regression;psos	Hongjun Su;Qian Du;Peijun Du	2013	2013 5th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)	10.1109/WHISPERS.2013.8080694	computer vision;pattern recognition	Vision	11.30672134883995	-43.34765002779749	197411
2c0fa024ea9ba558e8cd393afdafe5fe42280641	cost-sensitive selective naive bayes classifiers for predicting the increase of the h-index for scientific journals	selective naive bayes;h index;cost sensitive learning approach;neurosciences journals	Machine learning community is not only interested in maximizing classification accuracy, but also in minimizing the distances between the actual and the predicted class. Some ideas, like the cost-sensitive learning approach, are proposed to face this problem. In this paper, we propose two greedy wrapper forward cost-sensitive selective naive Bayes approaches. Both approaches readjust the probability thresholds of each class to select the class with the minimum-expected cost. The first algorithm (CSSNB-Accuracy) considers adding each variable to the model and measures the performance of the resulting model on the training data. The variable that most improves the accuracy, that is, the percentage of well classified instances between the readjusted class and actual class, is permanently added to the model. In contrast, the second algorithm (CS-SNB-Cost) considers adding variables that reduce the misclassification cost, that is, the distance between the readjusted class and actual class. We have tested our algorithms on the bibliometric indices prediction area. Considering the popularity of the well-known h-index, we have researched and built several prediction models to forecast the annual increase of the h-index for Neurosciences journals in a four-year time horizon. Results show that our approaches, particularly CS-SNB-Accuracy, achieved higher accuracy values than the analyzed costsensitive classifiers and Bayesian classifiers. Furthermore, we also noted that the CS-SNB-Cost always achieved a lower average cost than all analyzed cost-sensitive and cost-insensitive classifiers. These costsensitive selective naive Bayes approaches outperform the selective naive Bayes in terms of accuracy and average cost, so the cost-sensitive learning approach could be also applied in different probabilistic classification approaches. & 2014 Elsevier B.V. All rights reserved.	bibliometrics;feature selection;google scholar;greedy algorithm;machine learning;naive bayes classifier;scopus;statistical classification;whole earth 'lectronic link	Alfonso Ibáñez;Concha Bielza;Pedro Larrañaga	2014	Neurocomputing	10.1016/j.neucom.2013.08.042	naive bayes classifier;computer science;machine learning;pattern recognition;data mining;statistics	AI	12.44386034979076	-38.512805850343334	197460
6cbb0087bf9448f8c242fbda3bdd539bf3b8d7e0	a first approach towards the usage of classifiers’ performance to create fuzzy measures for ensembles of classifiers: a case study on highly imbalanced datasets		In this work we study the possibility of learning fuzzy measures from classifiers’ performance for improving the standard aggregation methods in classifier ensembles. Fuzzy measures are set-valued functions, which are not necessarily additive, and they are the basis for constructing non-linear fuzzy integrals, such as Choquet or Sugeno integral. These integrals have shown to be very useful in the aggregation of interacting criteria, since this interaction can be well modeled by a fuzzy measure. Classifier ensembles are composed of several classifiers and are aimed at improving the performance of every one of their counterparts. There are two main aspects about ensembles, first, how to build them, and second, how to combine the outputs of all their members. In this work, we focus on the second part, which is a key factor to obtain a successful ensemble. More specifically, we focus on the usage of fuzzy measures for the aggregation phase aiming at taking into account the coalitions and interactions among the members of the ensemble. Our hypothesis is that taking such information into account can lead to better performance. Moreover, we propose to directly obtain the fuzzy measure from data by considering the performance of each subset of classifiers in the ensemble. This way, one needs not include any additional learning for the fuzzy measure that can easily lead to overfitting. In order to test the usefulness of the proposed fuzzy measure, we will consider a set of 33 highly imbalanced datasets and we will develop a complete experimental study comparing the proposed combination scheme with other approaches commonly considered in the literature.	aggregate function;bootstrap aggregating;computer performance;ensembles of classifiers;experiment;fuzzy measure theory;interaction;neural ensemble;nonlinear system;overfitting;sugeno integral;utility functions on indivisible goods	Mikel Uriz;Daniel Paternain;Humberto Bustince;Mikel Galar	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491440	overfitting;ensembles of classifiers;machine learning;fuzzy logic;artificial intelligence;classifier (linguistics);computer science;sugeno integral	Robotics	12.370293902737995	-41.45263787497807	197624
0e051b494f01e7236a0df80d1e9708781d11fc6b	local sparsity control for naive bayes with extreme misclassification costs	high recall classification;text mining;naive bayes;data mining;naive bayes classifier;local features;feature selection;text categorization	In applications of data mining characterized by highly skewed misclassification costs certain types of errors become virtually unacceptable. This limits the utility of a classifier to a range in which such constraints can be met. Naive Bayes, which has proven to be very useful in text mining applications due to high scalability, can be particularly affected. Although its 0/1 loss tends to be small, its misclassifications are often made with apparently high confidence. Aside from efforts to better calibrate Naive Bayes scores, it has been shown that its accuracy depends on document sparsity and feature selection can lead to marked improvement in classification performance. Traditionally, sparsity is controlled globally, and the result for any particular document may vary. In this work we examine the merits of local sparsity control for Naive Bayes in the context of highly asymmetric misclassification costs. In experiments with three benchmark document collections we demonstrate clear advantages of document-level feature selection. In the extreme cost setting, multinomial Naive Bayes with local sparsity control is able to outperform even some of the recently proposed effective improvements to the Naive Bayes classifier. There are also indications that local feature selection may be preferable in different cost settings.	benchmark (computing);data mining;experiment;feature selection;naive bayes classifier;scalability;sparse matrix;text mining	Aleksander Kolcz	2005		10.1145/1081870.1081888	bayes classifier;text mining;naive bayes classifier;computer science;machine learning;pattern recognition;data mining;bayes error rate;feature selection	ML	16.321143655774893	-39.64667552882379	197667
eb1b8ef836f87cc6954f1da33d7dbdc52020912a	a simple plug-in bagging ensemble based on threshold-moving for classifying binary and multiclass imbalanced data	bagging ensembles;binary classification;imbalanced data;multiclass classification;posterior calibration;resampling	Class imbalance presents a major hurdle in the application of classification methods. A commonly taken approach is to learn ensembles of classifiers using rebalanced data. Examples include bootstrap averaging (bagging) combined with either undersampling or oversampling of the minority class examples. However, rebalancing methods entail asymmetric changes to the examples of different classes, which in turn can introduce their own biases. Furthermore, these methods often require specifying the performance measure of interest a priori, i.e., before learning. An alternative is to employ the threshold moving technique, which applies a threshold to the continuous output of a model, offering the possibility to adapt to a performance measure a posteriori, i.e., a plug-in method. Surprisingly, little attention has been paid to this combination of a bagging ensemble and threshold-moving. In this paper, we study this combination and demonstrate its competitiveness. Contrary to the other resampling methods, we preserve the natural class distribution of the data resulting in well-calibrated posterior probabilities. Additionally, we extend the proposed method to handle multiclass data. We validated our method on binary and multiclass benchmark data sets by using both, decision trees and neural networks as base classifiers. We perform analyses that provide insights into the proposed method.	artificial neural network;benchmark (computing);bitwise operation;class;competitive analysis (online algorithm);decision trees;decision tree;ensembles of classifiers;neural network simulation;oversampling;plug (physical object);plug-in (computing);probability;statistical classification;trees (plant);undersampling	Guillem Collell;Drazen Prelec;Kaustubh R. Patil	2018		10.1016/j.neucom.2017.08.035	artificial neural network;machine learning;ensembles of classifiers;artificial intelligence;pattern recognition;mathematics;binary classification;decision tree;resampling;data set;multiclass classification;posterior probability	ML	15.65569623683736	-39.21328497261787	198040
3b68773c70f3103b46cb8fddd98ce9aa5def1171	a feature selection method based on feature grouping and genetic algorithm	feature grouping;symmetrical uncertainty;genetic algorithm;feature selection	Feature selection technique has shown its power in analyzing the high dimensional data and building the efficient learning models. This study proposes a feature selection method based on feature grouping and genetic algorithm FS-FGGA to get a discriminative feature subset and reduce the irrelevant and redundancy data. Firstly, it eliminates the irrelevant features using the symmetrical uncertainty between features and class labels. Then, it groups the features by Approximate Markov blanket. Finally, genetic algorithm is applied to search the optimal feature subset from the different groups. Experiments on the eight public datasets demonstrate the effectiveness and superiority of FS-FGGA in comparison with SVM-RFE and ECBGS in most cases.	feature selection;genetic algorithm;xslt/muenchian grouping	Xiaohui Lin;Xiaomei Wang;Niyi Xiao;Xin Huang;Jue Wang	2015		10.1007/978-3-319-23862-3_15	minimum redundancy feature selection;computer science;machine learning;pattern recognition;data mining;feature selection;feature;dimensionality reduction	NLP	12.176138286888458	-44.61794788592755	198130
3a4ab1165aec6973fe15e483ee0708b607e7a095	classification from positive, unlabeled and biased negative data		Positive-unlabeled (PU) learning addresses the problem of learning a binary classifier from positive (P) and unlabeled (U) data. It is often applied to situations where negative (N) data are difficult to be fully labeled. However, collecting a non-representative N set that contains only a small portion of all possible N data can be much easier in many practical situations. This paper studies a novel classification framework which incorporates such biased N (bN) data in PU learning. The fact that the training N data are biased also makes our work very different from those of standard semi-supervised learning. We provide an empirical risk minimization-based method to address this PUbN classification problem. Our approach can be regarded as a variant of traditional example-reweighting algorithms, with the weight of each example computed through a preliminary step that draws inspiration from PU learning. We also derive an estimation error bound for the proposed method. Experimental results demonstrate the effectiveness of our algorithm in not only PUbN learning scenarios but also ordinary PU leaning scenarios on several benchmark datasets.	algorithm;benchmark (computing);binary classification;empirical risk minimization;estimation theory;one-class classification;semi-supervised learning;semiconductor industry;statistical classification;supervised learning	Lucas Samuel Perinazzo Pauvels;Gang Niu;Masashi Sugiyama	2018	CoRR		machine learning;pu learning;empirical risk minimization;artificial intelligence;binary classification;computer science	ML	16.888704602585975	-39.34592780331264	198259
8815cec07636e69dba36f101c0c4e14bfc7a9ca9	evaluation and classification of otoneurological data with new data analysis methods based on machine learning	otoneurology;neural networks;variable evaluation;data analysis methods;spectrum;multilayer perceptron;classification;variable selection;complex data;machine learning;positive predictive value;classification accuracy;neural network	We improved the classification ability of multilayer perceptron networks by constructing a set of networks of as many as output classes and investigated the influence of different input variables on the classification. We have developed methods named scattering, spectrum and response analysis to express the classification complexity, especially the overlap of output classes, to disentangle the relation between the input variables and output classes of perceptron neural networks, and to establish the importance of input variables. The methods were tested by exploring complicated otoneurological data. In contrast to the variable selection problem, our methods characterize the importance of variables for classification and also describe the importance of the different values of each variable for output (disease) classes. When complex data is distributed in a biased manner between disease classes, we improved classification accuracy by developing a network set called NetSet, which increased average sensitivity and positive predictive value for at least 10% up to 85% and 83% respectively, compared to our earlier neural network classifications with the same data, which clarified class distribution effects and supported our comprehension of the significance of input.	machine learning	Markku Siermala;Martti Juhola;Jorma Laurikkala;Kati Iltanen;Erna Kentala;Ilmari Pyykkö	2007	Inf. Sci.	10.1016/j.ins.2006.11.002	spectrum;biological classification;computer science;machine learning;classification rule;pattern recognition;data mining;multilayer perceptron;data analysis;one-class classification;artificial neural network;statistics;complex data type	AI	10.275657066219532	-38.48395926007496	199172
455906abef7308e664f65a2578360d12394e921a	accuracy-based classification em: combining clustering with prediction	expectation maximization;clustering;mixture models;prediction	"""Expectation-Maximization (EM) is well-known for its use in clustering. During operation, EM makes a """"soft"""" assignment of each row to multiple clusters in proportion to the likelihood of each cluster. Classification EM (CEM) is a variant of EM that makes a """"hard"""" assignment of each row to its most likely class. This paper presents a variant of CEM, which we call Accuracy-Based CEM (ABCEM), where the goal is prediction rather clustering. ABCEM first assigns each row to the most likely class based on the input columns, and then estimates performance of this assignment by evaluating the mean squared prediction error (MSPE) on the output columns, and proceeds as in CEM to update clusters and re-assign each row to the new clusters. Finally, the optimal clustering is selected to minimize the MSPE, selecting a local optimum from the left, and thus the procedure can also be viewed as a principled version of early stopping which uses only the training set. Our results show that ABCEM is nearly 40% more accurate than CEM."""	statistical classification	Stephanie Sapp;Armand Prieditis	2013		10.1007/978-3-642-39712-7_35	prediction;expectation–maximization algorithm;computer science;machine learning;pattern recognition;mixture model;data mining;mathematics;cluster analysis;statistics	ML	15.760540497583026	-39.36871064648928	199653
34ae31cf6f759eca9b40935d1d06b0980331b1de	random subspaces and subsampling for 2-d face recognition	image sampling;nearest neighbor searches;face recognition nearest neighbor searches principal component analysis filtering testing classification tree analysis computer science decision trees pixel pattern recognition;filtering;image sampling face recognition image classifier random subspace methodology feature extraction;decision tree;image classifier;image classification;random subspace methodology;testing;face recognition;learning artificial intelligence face recognition feature extraction image classification image sampling;feature extraction;principal component analysis;pixel;pattern recognition;classification tree analysis;computer science;learning artificial intelligence;nearest neighbor classifier;decision trees	Random subspaces are a popular ensemble construction technique that improves the accuracy of weak classifiers. It has been shown, in different domains, that random subspaces combined with weak classifiers such as decision trees and nearest neighbor classifiers can provide an improvement in accuracy. In this paper, we apply the random subspace methodology to the 2-D face recognition task. The main goal of the paper is to see if the random subspace methodology can do as well, if not better, than the single classifier constructed on the tuned face space. We also propose the use of a validation set for tuning the face space, to avoid bias in the accuracy estimation. In addition, we also compare the random subspace methodology to an ensemble of subsamples of image data. This work shows that a random subspaces ensemble can outperform a well-tuned single classifier for a typical 2-D face recognition problem. The random subspaces approach has the added advantage of requiring less careful tweaking.	chroma subsampling;decision tree;face space;facial recognition system;tweaking;weak value	Nitesh V. Chawla;Kevin W. Bowyer	2005	2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)	10.1109/CVPR.2005.286	facial recognition system;random subspace method;random forest;computer science;machine learning;decision tree;pattern recognition;data mining	Vision	14.993078111800456	-43.08714266525124	199727
