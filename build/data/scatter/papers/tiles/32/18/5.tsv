id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
55d137955476b1703b3ac9ae43bd4075a37020ee	designing neural networks in culture	biological patents;brain on chip neural network design lithographic techniques cell culture surface nerve cells glial cells neural engineering patterning muscle;brain;biomedical journals;cell culture;biocontrol;neural networks;influence function;three dimensions;text mining;neural nets;europe pubmed central;lithographic techniques;citation search;glial cell;neural culture;citation networks;pharmaceutical technology;neuroscience;chip;neural network design;patterning brain on chip cellular lithography neural culture neural engineering;lithography;electrodes;biomedical engineering;research articles;cell culture surface;brain on chip;abstracts;glial cells;open access;neural engineering;patterning;life sciences;clinical guidelines;pattern recognition;lab on a chip;photolithography biocontrol biomedical engineering brain cellular biophysics lab on a chip neural nets neurophysiology pattern recognition;neurons;cellular lithography;photolithography;neurophysiology;neural networks neurons biological neural networks brain biomedical engineering laboratories lithography electrodes neuroscience pharmaceutical technology;full text;cellular biophysics;rest apis;biological neural networks;orcids;europe pmc;muscle;biomedical research;neural network;nerve cells;bioinformatics;literature search	Technology has advanced to where it is possible to design and grow - with predefined geometry and surprisingly good fidelity - living networks of neurons in culture dishes. Here we overview the elements of design, emphasizing the lithographic techniques that alter the cell culture surface which in turn influences the attachment and growth of the neural networks. Advanced capability in this area makes it possible to design networks of desired complexity. Other issues addressed include the influence of glial cells and media on activity and the potential for extending the designs into three dimensions. Investigators are advancing the art and science of analyzing and controlling through stimulation the function of the neural networks, including the ability to take advantage of their geometric form in order to influence functional properties.	attachments;neural networks	Bruce C. Wheeler;Gregory J. Brewer	2010	Proceedings of the IEEE	10.1109/JPROC.2009.2039029	chip;three-dimensional space;muscle;lab-on-a-chip;computer science;engineering;electrode;nanotechnology;biological pest control;biological engineering;cell culture;photolithography;artificial neural network	Robotics	15.43659327212441	-67.40060999829593	177500
0b28d69250fced8b6a76c98f4b9ba2f77d04b58a	biological modeling of complex chemotaxis behaviors for c. elegans under speed regulation—a dynamic neural networks approach		In this paper, the modeling of several complex chemotaxis behaviors of C. elegans is explored, which include food attraction, toxin avoidance, and locomotion speed regulation. We first model the chemotaxis behaviors of food attraction and toxin avoidance separately. Then, an integrated chemotaxis behavioral model is proposed, which performs the two chemotaxis behaviors simultaneously. The novelty and the uniqueness of the proposed chemotaxis behavioral models are characterized by several attributes. First, all the chemotaxis behavioral model sare on biological basis, namely, the proposed chemotaxis behavior models are constructed by extracting the neural wire diagram from sensory neurons to motor neurons, where sensory neurons are specific for chemotaxis behaviors. Second, the chemotaxis behavioral models are able to perform turning and speed regulation. Third, chemotaxis behaviors are characterized by a set of switching logic functions that decide the orientation and speed. All models are implemented using dynamic neural networks (DNN) and trained using the real time recurrent learning (RTRL) algorithm. By incorporating a speed regulation mechanism, C. elegans can stop spontaneously when approaching food source or leaving away from toxin. The testing results and the comparison with experiment results verify that the proposed chemotaxis behavioral models can well mimic the chemotaxis behaviors of C. elegans in different environments.	afferent neuron;approximation algorithm;artificial neural network;behavior;behavioral modeling;chemotaxis;departure - action;diagram;motor neurons;neural network simulation;neural tube defects;neurons, efferent;nonlinear system;rbfox2 gene;algorithm;mab-20 protein, c elegans	Jian-Xin Xu;Xin Deng	2012	Journal of Computational Neuroscience	10.1007/s10827-012-0437-1	psychology;simulation;artificial intelligence;communication	ML	15.935518642042934	-68.08918176556595	178067
d1307bc19e11f332fc7303d72bbb94ef27852576	effect of ultrasound on bone fracture healing: a computational bioregulatory model	angiogenesis;bone healing;computational model;ultrasound;vascular endothelial growth factor	Bone healing is a complex biological procedure in which several cellular actions, directed by biochemical and mechanical signals, take place. Experimental studies have shown that ultrasound accelerates bone ossification and has a multiple influence on angiogenesis. In this study a mathematical model predicting bone healing under the presence of ultrasound is demonstrated. The primary objective is to account for the ultrasound effect on angiogenesis and more specifically on the transport of the Vascular Endothelial Growth Factor (VEGF). Partial differential equations describing the spatiotemporal evolution of cells, growth factors, tissues and ultrasound acoustic pressure and velocity equations determining the development of the blood vessel network constitute the present model. The effect of the ultrasound characteristics on angiogenesis and bone healing is investigated by applying different boundary conditions of acoustic pressure at the periosteal region of the bone model, which correspond to different intensity values. The results made clear that ultrasound enhances angiogenesis mechanisms during bone healing. The proposed model could be regarded as a step towards the monitoring of the effect of ultrasound on bone regeneration.		Maria G. Vavva;Konstantinos N. Grivas;Aurélie Carlier;Demosthenes Polyzos;Liesbet Geris;Hans Van Oosterwyck;Dimitrios I. Fotiadis	2018	Computers in biology and medicine	10.1016/j.compbiomed.2018.06.024	vascular endothelial growth factor;computer vision;artificial intelligence;computer science;ossification;bone healing;vegf receptors;ultrasound;blood vessel;angiogenesis;cell biology;therapeutic ultrasound	AI	10.585908363058573	-66.59907874724604	180990
64dd0a5575e2a29a7bea4ae78e651d9d2678b7a9	lateral inhibition through delta-notch signaling: a piecewise affine hybrid model	equation differentielle;sistema hibrido;systeme discret;dierential equation;modelo hibrido;differential equation;non linear model;continuous system;modele non lineaire;modele hybride;hybrid model;activation;ecuacion diferencial;systeme continu;modelo no lineal;differentiation;single cell;differenciation;linearisation morceau;red celular;sistema continuo;activacion;cell network;reseau cellulaire;hybrid system;linearizacion trozo;lateral inhibition;diferenciacion;conmutador;sistema discreto;piecewise linearization;reachability analysis;discrete system;commutateur;analyse atteignabilite;selector switch;systeme hybride;nonlinear model	Biological cell networks exhibit complex combinations of both discrete and continuous behaviors: indeed, the dynamics that govern the spatial and temporal increase or decrease of protein concentration inside a single cell are continuous differential equations, while the activation or deactivation of these continuous dynamics are triggered by discrete switches which encode protein concentrations reaching given thresholds. In this paper, we model as a hybrid system a striking example of this behavior in a biological mechanism called Delta-Notch signaling, which is thought to be the primary mechanism of cell differentiation in a variety of cell networks. We present results in both simulation and reachability analysis of this hybrid system. We emphasize how the hybrid system model is computationally superior (for both simulation and analysis) to other nonlinear models in the literature, without compromising faithful modeling of the biological phenomena.	arbiter (electronics);automata theory;benchmark (computing);biological system;ccir system a;cell signaling;diagram;encode;experiment;finite-state machine;gene regulatory network;hybrid system;lateral computing;lateral thinking;network switch;nonlinear system;reachability;simulation	Ronojoy Ghosh;Claire J. Tomlin	2001		10.1007/3-540-45351-2_21	lateral inhibition;computer science;discrete system;calculus;control theory;mathematics;differentiation;differential equation;algorithm;hybrid system	Logic	16.810157514929042	-67.71161555913895	182426
02d539d50919eb1184443bdf224ff084a72958e7	a computational model of the hippocampus that represents environmental structure and goal location, and guides movement	goal oriented behavior;topological navigation;reverse replay;computational model	Hippocampal place cells (PCs) are believed to represent environmental structure. However, it is unclear how and which brain regions represent goals and guide movements. Recently, another type of cells that fire around a goal was found in rat hippocampus (we designate these cells as goal place cells, GPCs). This suggests that the hippocampus is also involved in goal representation. Assuming that the activities of GPCs depend on the distance to a goal, we propose an adaptive navigation model. By monitoring the population activity of GPCs, the model navigates to shorten the distance to the goal. To achieve the distance-dependent activities of GPCs, plastic connections are assumed between PCs and GPCs, which are modified depending on two reward-triggered activities: activity propagation through PC–PC network representing the topological environmental structure, and the activity of GPCs with different durations. The former activity propagation is regarded as a computational interpretation of “reverse replay” phenomenon found in rat hippocampus. Simulation results confirm that after reaching a goal only once, the model can navigate to the goal along almost the shortest path from arbitrary places in the environment. This indicates that the hippocampus might play a primary role in the representation of not only the environmental structure but also the goal, in addition to guiding the movement. This navigation strategy using the population activity of GPCs is equivalent to the taxis strategy, the simplest and most basic for biological systems. Our model is unique because this simple strategy allows the model to follow the shortest path in the topological map of the environment.	activity tracker;assumed;biological system;computation (action);computational model;movement;place cells;short;shortest path problem;simulation;software propagation	Jumpei Matsumoto;Yoshinari Makino;Haruki Miura;Masafumi Yano	2011	Biological Cybernetics	10.1007/s00422-011-0454-6	psychology;simulation;computer science;artificial intelligence;machine learning;communication;computational model;physics	ML	14.579809257958486	-72.70135401497053	182547
b471594ba02a4dedb955dfab710c68b604131982	a bcm theory of meta-plasticity for online self-reorganizing fuzzy-associative learning	self reorganizing;meta plasticity principles;sliding threshold;time varying;hebbian learning;time variant;fuzzy neural network;self adjusting systems fuzzy neural nets hebbian learning inference mechanisms;fuzzy neural nets;bienenstock cooper munro theory;fuzzy associative learning;association learning computer simulation fuzzy logic humans mechanics models neurological neural networks computer nonlinear dynamics online systems pattern recognition automated time factors;hebbian theory;s p 500 stock index online self reorganizing fuzzy associative learning time invariant conditions bienenstock cooper munro theory hebbian theory online reasoning associative learning meta plasticity principles time variant online learning neurological learning theory;neurofuzzy;time invariant conditions;training;self adjusting systems;inference mechanisms;bienenstock cooper munro bcm;online self reorganizing fuzzy associative learning;anti hebbian;associative learning;online learning;neurological learning theory;learning systems;dynamic environment;synaptic plasticity;indexation;dissociative;cognition;meta plasticity;stability analysis;java learning systems fuzzy neural networks fluid dynamics hebbian theory stability impedance fuzzy reasoning competitive intelligence design engineering;clustering algorithms;self organization;time variant online learning;neurons;s p 500 stock index;time varying anti hebbian bienenstock cooper munro bcm dissociative fuzzy associative learning fuzzy neural network meta plasticity neurofuzzy online learning online reasoning self organizing self reorganizing sliding threshold synaptic plasticity time variant;online reasoning;self organizing;data models	Self-organizing neurofuzzy approaches have matured in their online learning of fuzzy-associative structures under time-invariant conditions. To maximize their operative value for online reasoning, these self-sustaining mechanisms must also be able to reorganize fuzzy-associative knowledge in real-time dynamic environments. Hence, it is critical to recognize that they would require self-reorganizational skills to rebuild fluid associative structures when their existing organizations fail to respond well to changing circumstances. In this light, while Hebbian theory (Hebb, 1949) is the basic computational framework for associative learning, it is less attractive for time-variant online learning because it suffers from stability limitations that impedes unlearning. Instead, this paper adopts the Bienenstock-Cooper-Munro (BCM) theory of neurological learning via meta-plasticity principles (Bienenstock et al., 1982) that provides for both online associative and dissociative learning. For almost three decades, BCM theory has been shown to effectively brace physiological evidence of synaptic potentiation (association) and depression (dissociation) into a sound mathematical framework for computational learning. This paper proposes an interpretation of the BCM theory of meta-plasticity for an online self-reorganizing fuzzy-associative learning system to realize online-reasoning capabilities. Experimental findings are twofold: 1) the analysis using S&P-500 stock index illustrated that the self-reorganizing approach could follow the trajectory shifts in the time-variant S&P-500 index for about 60 years, and 2) the benchmark profiles showed that the fuzzy-associative approach yielded comparable results with other fuzzy-precision models with similar online objectives.	bcm theory;benchmark (computing);braces-orthopedic appliances;cary cooper;depressive disorder;hebbian theory;learning disorders;loop invariant;mathematics;mental suffering;online machine learning;organizing (structure);real-time locating system;reasoning;structure of suspensory ligament of breast;synaptic package manager;time-invariant system;chemosensitization/potentiation	Javan Tan;Hiok Chai Quek	2010	IEEE Transactions on Neural Networks	10.1109/TNN.2010.2046747	hebbian theory;computer science;artificial intelligence;machine learning;artificial neural network	ML	14.870618913355205	-72.35428984431829	182611
26c520a73220d908adf787cbc4f4082060103e1c	determining the contributions of divisive and subtractive feedback in the hodgkin-huxley model	model analysis;model reduction;negative feedback;robustness;rhythmic spiking;hodgkin huxley	The Hodgkin-Huxley (HH) model is the basis for numerous neural models. There are two negative feedback processes in the HH model that regulate rhythmic spiking. The first is an outward current with an activation variable n that has an opposite influence to the excitatory inward current and therefore provides subtractive negative feedback. The other is the inactivation of an inward current with an inactivation variable h that reduces the amount of positive feedback and therefore provides divisive feedback. Rhythmic spiking can be obtained with either negative feedback process, so we ask what is gained by having two feedback processes. We also ask how the different negative feedback processes contribute to spiking. We show that having two negative feedback processes makes the HH model more robust to changes in applied currents and conductance densities than models that possess only one negative feedback variable. We also show that the contributions made by the subtractive and divisive feedback variables are not static, but depend on time scales and conductance values. In particular, they contribute differently to the dynamics in Type I versus Type II neurons.	action potential;conductance (graph);direct inward dial;gain;hodgkin disease;hodgkin–huxley model;huxley: the dystopia;negative feedback;positive feedback;density	Sevgi Sengül;Robert Clewley;Richard Bertram;Joël Tabak	2014	Journal of Computational Neuroscience	10.1007/s10827-014-0511-y	psychology;neuroscience;artificial intelligence;control theory;mathematics;communication;negative feedback;hodgkin–huxley model;robustness	ML	17.278841388315552	-71.73733301737452	182901
d3797f679a6a46e186a0911f721898c89419b68a	a hippocampus ca3 model with autoassociative and heteroassociative memory functions	hippocampus ca3 model;brain;hippocampus mathematical model alzheimer s disease parkinson s disease neurons brain pathogens associative memory biological neural networks artificial neural networks;neurological diseases;izhikevich artificial neurons;hippocampus;biological system modeling;brain models;hopfield neural nets;autoassociative memory function;medical computing;three layered hopfield like neural network;computational modeling;spiking neurons;heteroassociative memory function;pathogenesis;matlab platform hippocampus ca3 model autoassociative memory function heteroassociative memory function brain neurological diseases alzheimer disease parkinson disease pathogenesis three layered hopfield like neural network izhikevich artificial neurons spiking neurons;functional model;mathematical model;diseases;associative memory;parkinson disease;neurons;neurophysiology;alzheimer disease;neurophysiology brain models diseases hopfield neural nets medical computing;biological neural networks;neural network;matlab platform	Hippocampus is one of brain areas which are related with some neurological diseases relating with memory such as Alzheimer and Parkinson. To understand pathogenesis of these diseases requires the development of detailed models of hippocampus function. The CA3 region is one of the most important regions of hippocampus to form associative memory. In this paper, a detailed model of hippocampus CA3 function was developed according to the structure of the CA3 region. The hippocampus CA3 functional model is a three-layered Hopfieldlike neural network with 250 Izhikevich artificial neurons as its spiking neurons. The model is simulated under MATLAB platform and the results show that: the third layer has heteroassociative memory function; the first and the second layer can implement autoassociative memory. Keywords-hippocampus CA3 model; autoassociative memory ; heteroassociative memory	artificial neural network;artificial neuron;autoassociative memory;content-addressable memory;function model;matlab;memory bound function	Wangxiong Zhao;Qingli Qiao;Dan Wang	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5305250	computer science;function model;artificial intelligence;machine learning;mathematical model;hippocampus;bidirectional associative memory;computational model;neurophysiology;autoassociative memory;artificial neural network;statistics	Robotics	16.90612085125468	-68.86197239963396	183080
bc4fe4b35a75e0939ae35c12cfa38cb44b10f439	models of emerging contexts in risky and complex decision settings	information structure;neural nets;inference mechanisms;multiple constraints;decision theory;inference mechanisms constraint theory neural nets decision theory;constraint theory;context modeling constraint theory economic forecasting neural networks bifurcation cognition;inference assessment risky decision complex decision multiple constraint satisfaction information structuring neural network decision stage;neural network;principal component	Key components of the multiple constraint satisfaction frameworks are explored in a series of experiments set in complex and ambiguous domains. All cases show the prevalence and importance of a purposeful structuring of the information by the participants. The participants gradually generate coherence even without increasing information. In accordance with multiple constraint satisfaction predictions, the assessments of inferences increasingly spread apart. Also, the correlations between the dependent variable (the decision) and the independent variables, as well as between the independent variables, consistently grow stronger as the participants progress through the decision stages. The information structuring, a gradual simplification of the component structure, is captured as principal components associated with the various decision stages. Neural networks predict the judgments in the various decision stages relatively well. Finally, the role of the ongoing structuring of the underlying information was explored through the application of trained networks to data in other decision stages.		C. Gustav Lundberg	2004		10.1109/HICSS.2004.1265220	optimal decision;influence diagram;decision theory;decision analysis;decision engineering;computer science;artificial intelligence;machine learning;data mining;decision rule;evidential reasoning approach;evidential decision theory;artificial neural network;business decision mapping;principal component analysis	ECom	12.84694442327217	-73.0567631277318	184081
ee21e4d6fe1c5467059be67738e576827bf4c2f9	a complex systems approach to an interpretation of dynamic brain activity i: chaotic itinerancy can provide a mathematical basis for information processing in cortical transitory and nonstationary dynamics	high dimensionality;dynamic system;complex system;information processing;laboratory experiment;brain activation;human brain;information theory	The transitory activity of neuron assemblies has been observed in various areas of animal and human brains. We here highlight some typical transitory dynamics observed in laboratory experiments and provide a dynamical systems interpretation of such behaviors. Using the information theory of chaos, it is shown that a certain type of chaos is capable of dynamically maintaining the input information rather than destroying it. Taking account of the fact that the brain works in a noisy environment, the hypothesis can be proposed that chaos exhibiting noise-induced order is appropriate for the representation of the dynamics concerned. The transitory dynamics typically observed in the brain seems to appear in high-dimensional systems. A new dynamical systems interpretation for the cortical dynamics is reviewed, cast in terms of high-dimensional transitory dynamics. This interpretation differs from the conventional one, which is usually cast in terms of low-dimensional attractors. We focus our attention on, in particular, chaotic itinerancy, a dynamic concept describing transitory dynamics among “exotic attractors”, or “attractor ruins”. We also emphasize the functional significance of chaotic itinerancy. 1 The background of lecture I Cortical activity appears to be unstable. A single time series of such activity is often observed to be “nonstationary” with aperiodic changes between synchronized and desynchronized states, transitory dynamics between quasi-attractors (“attractor ruins” in our words), propagation of activity with various frequencies and the appearance and disappearance of phase-related synchronizations. Based on model studies, we propose the hypothesis that chaotic itinerancy can be universally observed not only in cortical synaptic networks but also in cortical gap junction systems, though in the latter it appears in a different way. Before proceeding to the main issue, let us raise a naive but fundamental question concerning brain function. What is the brain doing? There can, of course, be various possible answers, but probably the most fundamental one would be that it is interpreting signals, not only those arising from the external world, but also those created within itself. The reason is the following. The purpose of cognition may be to discover the nature of objects. The purpose of movement may be to influence the rule of behavior, as well as to achieve cognitive purpose. If complete information is provided in advance of these processes, then the brain can uniquely solve the problem concerned and does not need interpretation. Usually, however, the information that the brain receive is only incomplete one. Therefore, the brain must interpret the information and determine its nature. This consideration leads us to the study of hermeneutics of the brain [1–6]. What kind of level of activity should be considered ? The dynamic features of the brain might mainly appear in its activity at the mesoscopic level. At the microscopic level, that is, at single channel or single neuron levels, the brain activity appears to be highly irregular and nonstationary. On the other hand, at the macroscopic level, that is, at the level of a functional area or of the whole brain, the activity appears to be much less disordered. According to a conventional theory of phase transitions developed in physics, a macroscopic ordered motion can appear as the result of cooperativity of microscopic elements of the system concerned. The ordered motion can be described by one or more order parameters. In equilibrium phase transitions, two thermal equilibrium states are interchanged by changes in a control parameter such as temperature, magnetic field, etc. In a neighborhood of the critical point of the transition, complex nonequilibrium motions appear. The characteristic scales in space of the system in such a critical regime range from microscopic to macroscopic. Then, the time-dependent phenomena can be observed at the mesoscopic level, and such a temporal evolution indicates the appearance of ordered motions. A similar situation can also occur in far-from-equilibrium systems, where in place of thermal equilibrium states various kinds of nonequilibrium stationary states can form the basis of phenomena. The state transitions in far-fromequilibrium systems can be described by bifurcations. We here apply this framework to the transitions that appear during the dynamic brain activity. The term “critical point” is replaced by the term “bifurcation point” within this framework. Dissipative systems are typical far-from-equilibrium systems, where a stationary inflow and outflow of energy or matter plays a role in maintaining the stationary state. In such a system, order parameters often behave time-dependently. Thus, the motions can be described by the time-dependency of quantities such as density functions, where a density function is, in general, a function of space, time and other quntities such as the membrane potential of neurons and the calcium concentrations inside the membrane, namely ρ(x, s, t) = ρ(x, (v, c), t). Thus, it is called a mesoscopic-level description. A typical equation of motion at this level is the Navier-Stokes equation of hydrodynamics. The description of complex spatio-temporal patterns in the brain must be made at the mesoscopic level. A further crucial problem stems from the fact that the brain works in a noisy environment. Its mechanism has not yet been clarified. In this situation, we assume that the interplay between noise and dynamical systems provides clue for solving the problem of how the brain treats noisy signals. Among others, noise-induced order [32], stochastic resonance [8], and chaotic resonance [9] are noteworthy. Noise-induced order occurs in a certain class of chaotic dynamical systems. At a particular noise level, a transition appears from the chaotic state to the ordered state, characterized by the appearance of sharp peak in the power spectrum, an abrupt decrease of Kolmogorov-Sinai entropy, and a change of the Lyapunov exponent from positive to negative values. The orbit is aggregated rather than segregated by the noise. In multi-stable systems, the most stable state is usually selected by adding noise, provided that the noise itself does not destabilize the stable states. If external noise is superimposed upon a periodic force, the state may change to resonate the periodic force. This is termed a stochastic resonance. A similar resonance may occur in the chaotic environment, where chaos replaces noise. This is called a chaotic resonance. This idea was suggested, based on the observations that both chaos and noise are ubiquitous in the brain. Curious transitory phenomena have been observed in various conditions in animal and even human brains. Among others, the typical phenomena appear as chaotic transitions between quasi-attractors [10–13], “nonstationary” alteration between synchronized and desynchronized states [14], the propagation of wave packets of γand δrange activity [15, 16], and the synchronization of epochs with large phase differences that appear irregularly [17]. These curious phenomena possess the common feature that the autonomic transition between states is both chaotic and itinerant. These states can be described by a conventional attractor. Actually, there have been a number of interpretation of brain activity in terms of the attractor concept. However, a conventional attractor, that is, a geometric attractor, is an inappropriate model for the interpretation of these transitory dynamics. Because the transitory dynamics appear to be transition between states, such a state should be described as unstable. Thus, we must find a new model for such a state, in other words, we must introduce a new model for the dynamic process such that it allows for both stability and instability. Here, instability describes a transition from such a state, and stability describes the return to the original state. We must extend this process further to successive transitions between multiple states. We have proposed chaotic itinerancy as a most appropriate concept for such transitory dynamics [18–20, 3, 4]. For further details of the background of this study, in particular, for the significance of the interpretation of brain dynamics in terms of high-dimensional dynamical systems and also for the potential role of chaotic itinerancy, the readers can refer to the recent magnum opus [4] of one of the authors (IT). The references on complex systems (see, for example, the reference [22]) are also highly recommended. 2 Theoretical basis for the interpretation of cortical dynamic activity There seems to be a common mechanism underlying all the dynamic behaviors described above. To study such an underlying mechanism, chaotic dynamical systems of high dimension and the information theory of chaotic dynamical systems are required.	autonomic computing;bifurcation theory;chaos theory;cognition;complex systems;control theory;critical point (network science);dissipative system;dynamical system;electroencephalography;experiment;hebbian theory;image noise;information processing;information theory;instability;interpretation (logic);kolmogorov complexity;lyapunov fractal;mips magnum;mathematical model;mesoscopic physics;navier–stokes equations;neural oscillation;neuron;noise (electronics);rössler attractor;software propagation;spectral density;stationary process;stationary state;stochastic resonance;synaptic package manager;time series;wave packet	Ichiro Tsuda;Hiroshi Fujii	2003		10.1007/978-3-540-27862-7_6	computer science;artificial intelligence;machine learning;communication	ML	16.496186819588456	-69.63889816380575	184594
8d5a768668e53f901fd275472b8232023ebe30be	retrocausality in quantum phenomena and chemical evolution	measurement dynamics;time reversal symmetry;reaction cycle;retrocausality;chemical evolution	The interplay between retrocausality and the time-reversal symmetry of the dynamical law of quantum mechanics underscores the significance of the measurement dynamics with the use of indivisible and discrete quantum particles to be mediated. One example of empirical evidence demonstrating the significance of retrocausality going along with time-reversal symmetry is seen in the operation of a reaction cycle to be expected in chemical evolution. A reaction cycle can hold itself when the causative operation of the cycle remains robust, even when facing frequent retrocausal interventions of a quantum-mechanical origin. Quantum mechanics in and of itself has potential in raising a reaction cycle in the prebiotic phase of chemical evolution, even without any help of artefactual scaffoldings of an external origin.	dynamical system;indivisible;quantum mechanics;retrocausality;t-symmetry	Koichiro Matsuno	2016	Information	10.3390/info7040062	retrocausality;t-symmetry;quantum mechanics	Theory	14.55739551341554	-69.78232773953289	188969
9640f15e55aeafc828a588d503846854644897d5	a comparative analysis of multi-conductance neuronal models in silico	comparative analysis;neural model;real time;parameter space;in silico	We demonstrate that a previously presented flexible silicon–neuron architecture can implement three disparate conductance-based neuron models with both fast and slow dynamics. By exploiting the real-time nature of this physical implementation, we mapped the model dynamics across a large region of parameter space. We also found that two of these dynamically different models represent points in a contiguous bursting space that spans between the two models. By systematically varying the model parameters, we also found that multiple, diverse trajectories in parameter space connected the two canonical bursting points. In addition, we found that the combination of parameter values keeps the neuron in the bursting region. These findings demonstrate the usefulness of the silicon–neuron architecture as a neural-modeling tool and illustrate its versatility as a platform for a multi-behavioral neuron that resembles its living analog.	analog;conductance (graph);hodgkin–huxley model;neuron;numerous;population parameter;qualitative comparative analysis;real-time clock;real-time computing;silicon	Stephen P. DeWeerth;Michael S. Reid;Edgar A. Brown;Robert J. Butera	2006	Biological Cybernetics	10.1007/s00422-006-0111-7	qualitative comparative analysis;computer science;bioinformatics;artificial intelligence;machine learning;mathematics;parameter space;statistics	ML	16.60594536859488	-68.88850150823835	190161
900791b417ecbd33f30dff25cf8b9318adc56725	a parameter-space search algorithm tested on a hodgkin–huxley model	conductance based;cost function;neural model;computer model;conductance based hodgkin huxley model;search algorithm;stochastic gradient descent;hodgkin huxley model;gradient descent;parameter space;neural modeling;parameter space exploration;hodgkin huxley;oscillatory neural behavior	We demonstrate a parameter-space search algorithm using a computational model of a single-compartment neuron with conductance-based Hodgkin–Huxley dynamics. To classify bursting (the desired behavior), we use a simple cost function whose inputs are derived from the frequency content of the neural output. Our method involves the repeated use of a stochastic gradient descent-type algorithm to locate parameter values that allow the neural model to produce bursting within a specified tolerance. We demonstrate good results, including those showing that the utility of our algorithm improves as the pre-defined allowable parameter ranges increase and that the initial approach to our method is computationally efficient.	algorithmic efficiency;anatomical compartments;computation;computational model;conductance (graph);hodgkin–huxley model;huxley: the dystopia;loss function;lymphoma, non-hodgkin;multi-compartment model;neuron;population parameter;search algorithm;stochastic gradient descent	Michael S. Reid;Edgar A. Brown;Stephen P. DeWeerth	2007	Biological Cybernetics	10.1007/s00422-007-0156-2	psychology;computer simulation;mathematical optimization;neuroscience;computer science;artificial intelligence;machine learning;hodgkin–huxley model	AI	16.915322709892457	-69.6656699935466	191129
7e2f687cf83c5decc6bcd860e7949d8ab7ed608e	agent-based modeling of intracellular transport	the european physical journal b;effective potential;spatial scale;degree of freedom;pattern formation;condensed matter;agent based model;langevin equation;journal;spatial distribution;intracellular transport;epj;complex systems;log normal distribution;computer simulation	We develop an agent-based model of the motion and pattern formation of vesicles. These intracellular particles can be found in four different modes of (undirected and directed) motion and can fuse with other vesicles. While the size of vesicles follows a log-normal distribution that changes over time due to fusion processes, their spatial distribution gives rise to distinct patterns. Their occurrence depends on the concentration of proteins which are synthesized based on the transcriptional activities of some genes. Hence, differences in these spatio-temporal vesicle patterns allow indirect conclusions about the (unknown) impact of these genes. By means of agent-based computer simulations we are able to reproduce such patterns on real temporal and spatial scales. Our modeling approach is based on Brownian agents with an internal degree of freedom, θ, that represents the different modes of motion. Conditions inside the cell are modeled by an effective potential that differs for agents dependent on their value θ. Agent’s motion in this effective potential is modeled by an overdampted Langevin equation, changes of θ are modeled as stochastic transitions with values obtained from experiments, and fusion events are modeled as space-dependent stochastic transitions. Our results for the spatio-temporal vesicle patterns can be used for a statistical comparison with experiments. We also derive hypotheses of how the silencing of some genes may affect the intracellular transport, and point to generalizations of the model.	agent-based model;computer simulation;experiment;graph (discrete mathematics);logic synthesis;pattern formation;spatial scale	Mirko Birbaumer;Frank Schweitzer	2011	CoRR	10.1140/epjb/e2011-20283-x	computer simulation;classical mechanics;complex systems;effective potential;nanotechnology;log-normal distribution;pattern formation;degrees of freedom;physics;quantum mechanics	Robotics	13.911952640305815	-66.74991482497025	191371
4eb0afd93a7247f629a1245f92b21b1de048ac4f	reliability of signal transmission in stochastic nerve axon equations	stochastic spatial model neuron;hodgkin huxley equations	We introduce a method for computing probabilities for spontaneous activity and propagation failure of the action potential in spatially extended, conductance-based neuronal models subject to noise, based on statistical properties of the membrane potential. We compare different estimators with respect to the quality of detection, computational costs and robustness and propose the integral of the membrane potential along the axon as an appropriate estimator to detect both spontaneous activity and propagation failure. Performing a model reduction we achieve a simplified analytical expression based on the linearization at the resting potential (resp. the traveling action potential). This allows to approximate the probabilities for spontaneous activity and propagation failure in terms of (classical) hitting probabilities of one-dimensional linear stochastic differential equations. The quality of the approximation with respect to the noise amplitude is discussed and illustrated with numerical results for the spatially extended Hodgkin-Huxley equations. Python simulation code is supplied on GitHub under the link https://github.com/deristnochda/Hodgkin-Huxley-SPDE .	action potential;approximation algorithm;axon;computation (action);conductance (graph);financial cost;heart failure;hodgkin–huxley model;huxley: the dystopia;kidney failure, chronic;liver failure, acute;membrane potentials;neural oscillation;numerical analysis;probability;providing (action);python;rest;simulation;software propagation;spontaneous order	Martin Sauer;Wilhelm Stannat	2015	Journal of Computational Neuroscience	10.1007/s10827-015-0586-0	psychology;mathematical optimization;theoretical computer science;mathematics;statistics	ML	11.579368802390826	-69.22981519126074	194309
0c5de6c1bd56cdf810bda60588903d5941641043	predicting drug absorption rates through human skin	drugs;chemicals;absorption;skin permeability drug absorption rates human skin bloodstream gaussian process;skin absorption biodiffusion biomedical measurement drugs gaussian distribution physiological models;drug absorption rates;percutaneous absorption;gaussian processes;skin permeability;biodiffusion;skin;training;conference paper;high definition video;structure permeability relationships;human skin;permeability;gaussian process;bloodstream;high definition video skin permeability chemicals gaussian processes training absorption;physiological models;biomedical measurement;gaussian distribution	Predicting the rate at which a substance will pass through human skin and into the bloodstream is a problem of current interest. We use Gaussian Process modeling to train a set of predictors using every combination of six molecular features. We find that only three of the features are needed for our best predictor. This result could be useful in the further analysis of skin permeability.	best practice;gaussian process;kerrison predictor;process modeling	Yi Sun;Lun Tak Lam;Gary Patrick Moss;Maria Prapopoulou;Rod Adams;Neil Davey;David Gray;Marc B. Brown	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596603	gaussian process;statistics	Robotics	10.335453114899778	-70.53185244267485	194520
e854ddc51f5b89f112c3c746d1657ef172f9e540	a stochastic neural firing generated at a hopf bifurcation and its biological relevance		The integer multiple firing patterns, generated in the rabbit depressor baroreceptors under the different static blood pressure, were observed between the resting state and the periodic firing and were characterized to be stochastic but not chaotic by a series of nonlinear time series estimations. These patterns exhibited very similar characteristics to those observed in the experimental neural pacemaker. Using I na,p + I K models with dynamics of a supercritical Hopf bifurcation, we successfully simulated the bifurcation process of firing patterns and observed the induction of the integer multiple firing patterns by adding noise. The results strongly suggest that the integer multiple firing rhythms generated by rabbit baroreceptors result from the interplay between noise and the system’s dynamics. Because of the important normal physiological function of baroreceptors, the biological significance of noise and the noise-induced firing rhythms at a Hopf bifurcation is interesting to be addressed.	bifurcation theory;hopf bifurcation;neural oscillation;relevance	Huijie Shang;Rongbin Xu;Jin Zhou;Shi-Yuan Han	2017		10.1007/978-3-319-70093-9_58	hopf bifurcation;biological applications of bifurcation theory;nonlinear system;baroreceptor;control theory;mathematics	ML	17.026490691855976	-70.96548883725112	196189
9fe128edea6a6d8743d27eb4142f6bf11ec3173c	neuronal motifs of long term and short term memory functions	short term memory biological neural networks motif excitatory inhibitory long term memory;biological system modeling;biological neural networks signal processing conferences biological system modeling neurons educational institutions;signal processing;neurons;biological neural networks;conferences	With the help of the models created and in the light of experimental studies, it is tried to understand the biological networks of memory structures and their motifs which is the basic building block of them. These are believed to be basic building blocks of neural networks and to perform important functional roles in them. In this study we examined long term and short term memory functions of all neuronal motifs that consist of tree neurons. All motifs consists of three neurons which are modelled with single compartment Hodgkin Huxley neuronal model. The results show that a lot of motifs have revealed long-term and short-term memory behavior.	artificial neural network;hodgkin–huxley model;huxley: the dystopia;long short-term memory;multi-compartment model	Ahmet Turan;Temel Kayikçioglu	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830464	computer vision;computer science;artificial intelligence;machine learning;signal processing	ML	16.075571450917607	-68.43646483225274	197710
8bc9a7ce301cb0da5311c18054b0eedf46ffc76a	processing of nested and cross-serial dependencies: an automaton perspective on srn behaviour	network implementation of symbolic structure;srns;nested and cross serial dependencies;language processing;simple recurrent network;fractal representations;multi task networks;data structure;random access	Language processing involves the identification and establishment of both nested (stack-like) and cross-serial (queue-like) dependencies. This paper analyses the behaviour of simple recurrent networks (SRNs) trained to handle these types of dependency individually and simultaneously. We provide new converging evidence that SRNs store sequences in a fractal data structure similar to a binary expansion. We provide evidence that the process of recalling a stored string by an SRN depletes the stored data structure, much like the operations of a symbolic stack or queue. Trained networks do not seem to operate like random access arrays, where a pointer into a data structure can retrieve data without altering the contents of the data structure. In addition, we demonstrate that networks trained to model both types of dependencies do not implement a more complex, but unified, representation, but rather implement two independent data structures, similar to a stack and queue.	automaton;cross-serial dependencies;recurrent neural network	Christo Kirov;Robert Frank	2012	Connect. Sci.	10.1080/09540091.2011.641939	data structure;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm;random access	Logic	17.12973182453724	-66.75022449025019	198500
9eaac4ee0c0951ee4150760e7eb9c7b58f0788ce	dynamics and physiological roles of stochastic firing patterns near bifurcation points		Different stochastic neural firing patterns or rhythms that appeared near polarization or depolarization resting states were observed in biological experiments on three nervous systems, and closely matched those simulated near bifurcation points between stable equilibrium point and limit cycle in a theoretical model with noise. The distinct dynamics of spike trains and interspike interval histogram (ISIH) of these stochastic rhythms were identified and found to build a relationship to the coexisting behaviors or fixed firing frequency of four different types of bifurcations. Furthermore, noise evokes coherence resonances near bifurcation points and plays important roles in enhancing information. The stochastic rhythms corresponding to Hopf bifurcation points with fixed firing frequency exhibited stronger coherence degree and a sharper peak in the power spectrum of the spike trains than those corresponding to saddle-node bifurcation points without fixed firing frequency. Moreover, the stochastic firing patterns changed to a depolarization resting state as the extracellular potassium concentration increased for the injured nerve fiber related to pathological pain or static blood pressure level increased for aortic depressor nerve fiber, and firing frequency decreased, which were different from the physiological viewpoint that firing frequency increased with increasing pressure level or potassium concentration. This shows that rhythms or firing patterns can reflect pressure or ion concentration information related to pathological pain information. Our results present the dynamics of stochastic firing patterns near bifurcation points, which are helpful for the identification of both dynamics and physiological roles of complex neural firing patterns or rhythms, and the roles of noise.	bifurcation theory	Bing Jia;Huaguang Gu	2017	I. J. Bifurcation and Chaos	10.1142/S0218127417501139	hopf bifurcation;mathematical analysis;limit cycle;mathematics;control theory;resting state fmri;depolarization;spectral density;coherence (physics)	AI	17.108727941267084	-71.46241676065412	198531
9b1b7967a991ee41feea9e8bec7bd02326c66f83	a 5-component mathematical model for salt-induced hypertension in dahl-s and dahl-r rats	electronic engineering;multiple components;numerical technique;genie biomedical;hypertension;high salt;blood pressure;dahl r;biomedical engineering;mathematical model;component model;ingenieria biomedica;model fitting;dahl s;salt intake	Salt-induced hypertension has been demonstrated in a variety of species including rats, monkeys, chimpanzees and humans. Until recently, the multiple phases of this blood pressure increase due to high salt intake had not been closely studied. This work builds upon a recent study, which developed a grey-box multi-component model of salt-induced hypertension in the Dahl-S rat. The previous 3-component model has been extended here to include additional model dynamics to improve the model fit and add new important elements to the model response. The model was optimised using numerical techniques with experimental data from 4 different protocols with Dahl-S, Dahl-R and FF2 hybrid rats. Results show a marked improvement over the previous model and confirm the merit of the 5-component model structure. A comparison between the model dynamics for different rat strains has also been included.	component-based software engineering;humans;hypertensive disease;mathematical model;mathematics;monkeys;numerical analysis;numerous;pan troglodytes;protocols documentation;rats, inbred dahl;sodium chloride, dietary	Violeta McLoone;John V. Ringwood;Bruce van Vliet	2011	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.04.008	computer science;blood pressure;mathematical model;component object model;diabetes mellitus;statistics		10.257605774215532	-69.44993940720701	198998
