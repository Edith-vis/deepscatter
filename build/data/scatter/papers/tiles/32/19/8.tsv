id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
4f58c47a497e914beb9ef58894cc9bdd69b7f47f	learning in presence of ontology mapping errors	mapping errors ontology learning algorithms;learning algorithm;nasty classification noise;ontology mapping;semantically disparate data sources;ontologies artificial intelligence;pac;ontologies artificial intelligence learning artificial intelligence;ontology mapping errors;pac semantically disparate data sources ontology mapping errors nasty classification noise;prediction model;learning artificial intelligence	The widespread use of ontologies to associate semantics with data has resulted in a growing interest in the problem of learning predictive models from data sources that use different ontologies to model the same underlying domain (world of interest). Learning from such \emph{semantically disparate} data sources involves the use of a mapping to resolve semantic disparity among the ontologies used. Often, in practice, the mapping used to resolve the disparity may contain errors and as such the learning algorithms used in such a setting must be robust in presence of mapping errors. We reduce the problem of learning from semantically disparate data sources in the presence of mapping errors to a variant of the problem of learning in the presence of nasty classification noise. This reduction allows us to transfer theoretical results and algorithms from the latter to the former.	algorithm;binocular disparity;machine learning;ontology (information science);predictive modelling;semantic integration;web ontology language	Neeraj Koul;Vasant Honavar	2010	2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2010.138	natural language processing;semantic integration;computer science;artificial intelligence;machine learning;data mining;predictive modelling	Robotics	23.938617545205126	-46.421532021301786	171156
7d21e75b76378c4a9348e610f0d398c8c8af5ee1	deep-fs: a feature selection algorithm for deep boltzmann machines		A Deep Boltzmann Machine is a model of a Deep Neural Network formed from multiple layers of neurons with nonlinear activation functions. The structure of a Deep Boltzmann Machine enables it to learn very complex relationships between features and facilitates advanced performance in learning of highlevel representation of features, compared to conventional Artificial Neural Networks. Feature selection at the input level of Deep Neural Networks has not been well studied, despite its importance in reducing the input features processed by the deep learning model, which facilitates understanding of the data. This paper proposes a novel algorithm, Deep Feature Selection (Deep-FS), which is capable of removing irrelevant features from large datasets in order to reduce the number of inputs which are modelled during the learning process. The proposed Deep-FS algorithm utilizes a Deep Boltzmann Machine, and uses knowledge which is acquired during training to remove features at the beginning of the learning process. Reducing inputs is important because it prevents the network from learning the associations between the irrelevant features which negatively impact on the acquired knowledge of the network about the overall distribution of the data. The Deep-FS method embeds feature selection in a Restricted Boltzmann Machine which is used for training a Deep Boltzmann Machine. The generative property of the Restricted Boltzmann Machine is used to reconstruct eliminated features and calculate reconstructed errors, in order to evaluate the impact of eliminating features. The performance of the proposed approach was evaluated with experiments conducted using the MNIST, MIR-Flickr, GISETTE, MADELON and PANCAN datasets. The results revealed that the proposed Deep-FS method enables improved feature selection without loss of accuracy on the MIR-Flickr dataset, where Deep-FS reduced the number of input features by removing 775 features without reduction in performance. With regards to the MNIST dataset, Deep-FS reduced the number of input features by more than 45%; it reduced the network error from 0.97% to 0.90%, and also reduced processing and classification time by more than 5.5%. Additionally, when compared to classical feature selection methods, Deep-FS returned higher accuracy. The experimental results on GISETTE, MADELON and PANCAN showed that Deep-FS reduced 81%, 57% and 77% of the number of input features, respectively. Moreover, the proposed feature selection method reduced the classifier training time by 82%, 70% and 85% on GISETTE, MADELON and PANCAN datasets, respectively. Experiments with various datasets, comprising a large number of features and samples, revealed that the proposed DeepFS algorithm overcomes the main limitations of classical feature selection algorithms. More specifically, most classical methods require, as a prerequisite, a pre-specified number of features to retain, however in Deep-FS this number is identified automatically. Deep-FS performs the feature selection task faster than classical feature selection algorithms which makes it suitable for deep learning tasks. In addition, DeepFS is suitable for finding features in large and big datasets which are normally stored in data batches for faster and more efficient processing. © 2018 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license. ( http://creativecommons.org/licenses/by/4.0/ ) ∗ Corresponding author. E-mail addresses: aboozar.taherkhani@ntu.ac.uk (A. Taherkhani), georgina.cosma@ntu.ac.uk (G. Cosma), martin.mcginnity@ntu.ac.uk (T.M. McGin-		Aboozar Taherkhani;Georgina Cosma;T. Martin McGinnity	2018	Neurocomputing	10.1016/j.neucom.2018.09.040	boltzmann constant;machine learning;deep learning;artificial neural network;feature selection;algorithm;boltzmann machine;mathematics;nonlinear system;mnist database;pattern recognition;restricted boltzmann machine;artificial intelligence	ML	21.643614498582973	-48.717258909017445	172125
ad950883e45df79d89ade9fd90bff179c51eb1cc	layer-wise training to create efficient convolutional neural networks		Recent large CNNs have delivered impressive performance but their storage requirement and computational cost limit a wide range of their applications in mobile devices and large-scale Internet industry. Works focusing on storage compression have led a great success. Recently how to reduce computational cost draws more attention. In this paper, we propose an algorithm to reduce computational cost, which is often solved by sparsification and matrix decomposition methods. Since the computation is dominated by the convolutional operations, we focus on the compression of convolutional layers. Unlike sparsification and matrix decomposition methods which usually derive from mathematics, we receive inspiration from transfer learning and biological neural networks. We transfer the knowledge in state-of-the-art large networks to compressed small ones, via layer-wise training. We replace the complex convolutional layers in large networks with more efficient modules and keep their outputs in each-layer consistent. Modules in the compressed small networks are more efficient, and their design draws on biological neural networks. For AlexNet model, we achieve 3.62× speedup, with 0.11% top-5 error rate increase. For VGG model, we achieve 5.67× speedup, with 0.43% top-5 error rate increase.		Linghua Zeng;Xinmei Tian	2017		10.1007/978-3-319-70096-0_65	the internet;convolutional neural network;machine learning;artificial neural network;word error rate;speedup;artificial intelligence;computation;deep learning;matrix decomposition;computer science	ML	22.284308551907124	-50.45201095646523	173019
4f7e5871acd286ac4520cba1c4622ff5591053d0	predicting media interestingness via biased discriminant embedding and supervised manifold regression		In this paper, we describe our model designed for automatic prediction of media interestingness. Specifically, a two-stage learning framework is proposed. In the first stage, supervised dimensionality reduction is employed to discover the key discriminant information embedded in the original feature space. We present a new algorithm dubbed biased discriminant embedding (BDE) to extract discriminant features with discrete labels and use supervised manifold regression (SMR) to extract discriminant features with continuous labels. In the second stage, SVM is utilized for prediction. Experimental results validate the effectiveness of our approaches.	algorithm;dimensionality reduction;discriminant;embedded system;feature vector;flickr;shingled magnetic recording	Yang Liu;Zhonglei Gu;Tobey H. Ko	2017			manifold;discriminant;mathematics;embedding;artificial intelligence;pattern recognition	AI	21.73697913285839	-46.040748610665325	173195
c9475a8324d46ee8dccc96cc98ba61285bf264ee	blind domain adaptation with augmented extreme learning machine features	extreme learning machines elms;object recognition;cybernetics;blind domain adaptation da;visual recognition blind domain adaptation blind da augmented extreme learning machine aelm visual classification performance source domain data unsupervised fashion feature reconstruction;visual classification;visual classification blind domain adaptation da extreme learning machines elms object recognition;training;training adaptation models data models training data visualization joining processes cybernetics;training data;visualization;joining processes;object recognition image classification image reconstruction learning artificial intelligence;adaptation models;data models	In practical applications, the test data often have different distribution from the training data leading to suboptimal visual classification performance. Domain adaptation (DA) addresses this problem by designing classifiers that are robust to mismatched distributions. Existing DA algorithms use the unlabeled test data from target domain during training time in addition to the source domain data. However, target domain data may not always be available for training. We propose a blind DA algorithm that does not require target domain samples for training. For this purpose, we learn a global nonlinear extreme learning machine (ELM) model from the source domain data in an unsupervised fashion. The global ELM model is then used to initialize and learn class specific ELM models from the source domain data. During testing, the target domain features are augmented with the reconstructed features from the global ELM model. The resulting enriched features are then classified using the class specific ELM models based on minimum reconstruction error. Extensive experiments on 16 standard datasets show that despite blind learning, our method outperforms six existing state-of-the-art methods in cross domain visual recognition.	acclimatization;addresses (publication format);algorithm;digit structure;domain adaptation;elm;experiment;generalization (psychology);learning disorders;nonlinear system;pixel;speeded up robust features;statistical classification;test data;unsupervised learning;visually impaired persons	Muhammad Uzair;Ajmal S. Mian	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2016.2523538	data modeling;training set;speech recognition;visualization;cybernetics;computer science;artificial intelligence;cognitive neuroscience of visual object recognition;machine learning;pattern recognition	ML	24.359647507122105	-46.276246499641076	173261
e37d75f316e6317d7268529c3889d58cc2e9d963	ensembles of multiple models and architectures for robust brain tumour segmentation		Deep learning approaches such as convolutional neural nets have consistently outperformed previous methods on challenging tasks such as dense, semantic segmentation. However, the various proposed networks perform differently, with behaviour largely influenced by architectural choices and training settings. This paper explores Ensembles of Multiple Models and Architectures (EMMA) for robust performance through aggregation of predictions from a wide range of methods. The approach reduces the influence of the meta-parameters of individual models and the risk of overfitting the configuration to a particular database. EMMA can be seen as an unbiased, generic deep learning model which is shown to yield excellent performance, winning the first position in the BRATS 2017 competition among 50+ participating teams.	artificial neural network;convolutional neural network;deep learning;emma;overfitting	Konstantinos Kamnitsas;Wenjia Bai;Enzo Ferrante;Steven G. McDonagh;Matthew Sinclair;Nick Pawlowski;Martin Rajchl;Matthew C. H. Lee;Bernhard Kainz;Daniel Rueckert;Ben Glocker	2017		10.1007/978-3-319-75238-9_38	machine learning;artificial intelligence;pattern recognition;overfitting;artificial neural network;deep learning;computer science;multiple models;segmentation	ML	21.75464122778899	-52.07895087676986	173454
c68fbc1f4aa72d30974f8a3071054e3b227137fd	generating natural language adversarial examples		Deep neural networks (DNNs) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97% and 70%, respectively. We additionally demonstrate that 92.3% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of DNNs in the natural language domain.	adversary (cryptography);algorithm;artificial neural network;authorization;black box;mathematical optimization;n. ahmed;natural language;perturbation theory;sentiment analysis;textual entailment	Moustafa Alzantot;Yash Sharma;Ahmed Elgohary;Bo-Jhang Ho;Mani B. Srivastava;Kai-Wei Chang	2018			adversarial system;machine learning;robustness (computer science);computer science;artificial intelligence;natural language processing;textual entailment;semantics;natural language;artificial neural network;population;sentiment analysis	NLP	19.25480860073581	-51.56536306989284	173530
847b52929af7abd5d127f519e39a429adc12c138	unlabeled data improvesword prediction	vision problems;unlabeled data;semisupervised learning machine learning algorithms labeling clustering algorithms predictive models geometry computer science tagging explosions search engines;image processing;manifolds;training;task model;image collections labeling;semisupervised multitask model;laplace equations;image edge detection;gradient descent;multitask learning problem unlabeled data word prediction improvement image collections labeling manifold regularization semisupervised multitask model vision problems gradient descent;image color analysis;multi task learning;learning artificial intelligence gradient methods image processing;word prediction;gradient methods;fasteners;approximation methods;learning artificial intelligence;word prediction improvement;manifold regularization;multitask learning problem	Labeling image collections is a tedious task, especially when multiple labels have to be chosen for each image. In this paper we introduce a new framework that extends state of the art models in word prediction to incorporate information from unlabeled examples, using manifold regularization. To the best of our knowledge this is the first semi-supervised multi-task model used in vision problems. The new model can be solved using gradient descent and is fast and efficient. We show remarkable improvements for cases with few labeled examples for challenging multi-task learning problems in vision (predicting words for images and attributes for objects).	computer multitasking;experiment;gradient descent;manifold regularization;multi-task learning;semi-supervised learning;semiconductor industry;statistical classification	Nicolas Loeff;Ali Farhadi;Ian Endres;David A. Forsyth	2009	2009 IEEE 12th International Conference on Computer Vision	10.1109/ICCV.2009.5459347	gradient descent;multi-task learning;computer vision;manifold;image processing;computer science;machine learning;pattern recognition	Vision	23.450466429217947	-47.32095484070379	174549
d1b8a721192d589b4667bb6e95a45877bb637566	conditional autoencoders with adversarial information factorization		Generative models, such as variational auto-encoders (VAE) and generative adversarial networks (GAN), have been immensely successful in approximating image statistics in computer vision. VAEs are useful for unsupervised feature learning, while GANs alleviate supervision by penalizing inaccurate samples using an adversarial game. In order to utilize benefits of these two approaches, we combine the VAE under an adversarial setup with auxiliary label information. We show that factorizing the latent space to separate the information needed for reconstruction (a continuous space) from the information needed for image attribute classification (a discrete space), enables the capability to edit specific attributes of an image.	computer vision;encoder;feature learning;generative adversarial networks;scene statistics;variational principle	Antonia Creswell;Anil A. Bharath;Biswa Sengupta	2017	CoRR		discrete space;generative grammar;auxiliary label;adversarial system;machine learning;factorization;mathematics;feature learning;artificial intelligence	ML	24.5903168097521	-48.87324183984921	175882
2e38c5b7d424a023acd072e83fe3c2f75dee6e84	modulated convolutional networks		Despite great effectiveness of very deep and wide Convolutional Neural Networks (CNNs) in various computer vision tasks, the significant cost in terms of storage requirement of such networks impedes the deployment on computationally limited devices. In this paper, we propose new modulated convolutional networks (MCNs) to improve the portability of CNNs via binarized filters. In MCNs, we propose a new loss function which considers the filter loss, center loss and softmax loss in an end-to-end framework. We first introduce modulation filters (M-Filters) to recover the unbinarized filters, which leads to a new architecture to calculate the network model. The convolution operation is further approximated by considering intra-class compactness in the loss function. As a result, our MCNs can reduce the size of required storage space of convolutional filters by a factor of 32, in contrast to the full-precision model, while achieving much better performances than state-of-the-art binarized models. Most importantly, MCNs achieve a comparable performance to the full-precision Resnets and WideResnets. The code will be available publicly soon.		Xiaodi Wang;Baochang Zhang;Ce Li;Rongrong Ji;Jungong Han;Xianbin Cao;Jianzhuang Liu	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00094	architecture;software deployment;convolutional neural network;machine learning;network model;modulation;pattern recognition;convolution;artificial intelligence;computer science;softmax function;software portability	Vision	23.187181610286363	-51.64205232207015	175913
fcc12426e4143b787cd0c1d3c910bade05d8773f	layer-wise training of deep networks using kernel similarity		Deep learning has shown promising results in many machine learning applications. The hierarchical feature representation built by deep networks enable compact and precise encoding of the data. A kernel analysis of the trained deep networks demonstrated that with deeper layers, more simple and more accurate data representations are obtained. In this paper, we propose an approach for layer-wise training of a deep network for the supervised classification task. A transformation matrix of each layer is obtained by solving an optimization aimed at a better representation where a subsequent layer builds its representation on the top of the features produced by a previous layer. We compared the performance of our approach with a DNN trained using back-propagation which has same architecture as ours. Experimental results on the real image datasets demonstrate efficacy of our approach. We also performed kernel analysis of layer representations to validate the claim of better feature encoding.	backpropagation;deep learning;kernel (operating system);machine learning;mathematical optimization;nonlinear system;overfitting;rendering (computer graphics);software propagation;transformation matrix	Mandar Kulkarni;Shirish Subhash Karande	2017	CoRR		architecture;machine learning;kernel (linear algebra);polynomial kernel;mathematics;deep learning;kernel method;tree kernel;transformation matrix;radial basis function kernel;pattern recognition;artificial intelligence	ML	24.20068904508871	-50.95926540527791	176349
289196f160b24c68e63f8b47d4d3fd49310718a2	too big to fail: what you need to know before attacking a machine learning system		There is an emerging arms race in the field of adversarial machine learning (AML). Recent results suggest that machine learning (ML) systems are vulnerable to a wide range of attacks; meanwhile, there are no systematic defenses. In this position paper we argue that to make progress toward such defenses, the specifications for machine learning systems must include precise adversary definitions—a key requirement in other fields, such as cryptography or network security. Without common adversary definitions, new AML attacks risk making strong and unrealistic assumptions about the adversary’s capabilities. Furthermore, new AML defenses are evaluated based on their robustness against adversarial samples generated by a specific attack algorithm, rather than by a general class of adversaries. We propose the FAIL adversary model, which describes the adversary’s knowledge and control along four dimensions: data Features, learning Algorithms, training Instances and crafting Leverage. We analyze several common assumptions, often implicit, from the AML literature, and we argue that the FAIL model can represent and generalize the adversaries considered in these references. The FAIL model allows us to consider a range of adversarial capabilities and enables systematic comparisons of attacks against ML systems, providing a clearer picture of the security threats that these attacks raise. By evaluating how much a new AML attack’s success depends on the strength of the adversary along each of the FAIL dimensions, researchers will be able to reason about the real effectiveness of the attack. Additionally, such evaluations may suggest promising directions for investigating defenses against the ML threats.	machine learning;need to know	Tudor Dumitras;Yigitcan Kaya;Radu Marginean;Octavian Suciu	2018		10.1007/978-3-030-03251-7_17	computer science;computer security;adversarial system;adversary model;cryptography;adversary;robustness (computer science);network security;need to know;adversarial machine learning;machine learning;artificial intelligence	AI	18.50189876126949	-51.366933652745786	177144
549410ecdfde8ce2dfdaa45e7ef4da4b8449977f	maximum margin output coding		In this paper we study output coding for multi-label prediction. For a multi-label output coding to be discriminative, it is important that codewords for different label vectors are significantly different from each other. In the meantime, unlike in traditional coding theory, codewords in output coding are to be predicted from the input, so it is also critical to have a predictable label encoding. To find output codes that are both discriminative and predictable, we first propose a max-margin formulation that naturally captures these two properties. We then convert it to a metric learning formulation, but with an exponentially large number of constraints as commonly encountered in structured prediction problems. Without a label structure for tractable inference, we use overgenerating (i.e., relaxation) techniques combined with the cutting plane method for optimization. In our empirical study, the proposed output coding scheme outperforms a variety of existing multi-label prediction methods for image, text and music classification.	channel capacity;cobham's thesis;code word;coding theory;compressed sensing;computation;convex optimization;cutting-plane method;data mining;detection theory;error detection and correction;forney algorithm;forward error correction;ieee transactions on information theory;interdependence;international conference on machine learning;iterative method;john d. wiley;journal of artificial intelligence research;journal of machine learning research;kohn–sham equations;large margin nearest neighbor;linear programming relaxation;mach;margin classifier;markov chain;markov random field;mathematical optimization;multi-label classification;nips;nyquist–shannon sampling theorem;proceedings of the ieee;sparse matrix;structured prediction;support vector machine;tridiagonal matrix algorithm;yoram moses	Yi Zhang;Jeff G. Schneider	2012			mathematical optimization;shannon–fano coding;variable-length code;machine learning;pattern recognition;mathematics	ML	21.980666433172544	-47.32532898092872	177381
182015c5edff1956cbafbcb3e7bbe294aa54f9fc	selecting receptive fields in deep networks		Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer. Unfortunately, for such large architectures the number of parameters can grow quadratically in the width of the network, thus necessitating hand-coded “local receptive fields” that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality). In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods. Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric. This approach allows us to harness the advantages of local receptive fields (such as improved scalability, and reduced data requirements) when we do not know how to specify such receptive fields by hand or where our unsupervised training algorithm has no obvious generalization to a topographic setting. We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered networks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0% and 60.1% accuracy, respectively.	algorithm;backpropagation;deep learning;feature learning;high- and low-level;locality of reference;performance;principle of locality;requirement;stl (file format);scalability;supervised learning;test set;topography;unsupervised learning;vector quantization	Adam Coates;Andrew Y. Ng	2011			computer science;artificial intelligence;theoretical computer science;machine learning	ML	22.447633789673144	-51.17255991727012	178266
cbec1de269cb8e4e59d452b61e4b5b7add86e7cc	coupled deep autoencoder for single image super-resolution	biological patents;biomedical journals;image resolution;neural networks;manifolds;text mining;europe pubmed central;single image super resolution sr autoencoder deep learning;citation search;citation networks;image reconstruction image resolution feature extraction encoding neural networks manifolds dictionaries;research articles;abstracts;feature extraction;image reconstruction;open access;dictionaries;life sciences;clinical guidelines;full text;encoding;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Sparse coding has been widely applied to learning-based single image super-resolution (SR) and has obtained promising performance by jointly learning effective representations for low-resolution (LR) and high-resolution (HR) image patch pairs. However, the resulting HR images often suffer from ringing, jaggy, and blurring artifacts due to the strong yet ad hoc assumptions that the LR image patch representation is equal to, is linear with, lies on a manifold similar to, or has the same support set as the corresponding HR image patch representation. Motivated by the success of deep learning, we develop a data-driven model coupled deep autoencoder (CDA) for single image SR. CDA is based on a new deep architecture and has high representational capability. CDA simultaneously learns the intrinsic representations of LR and HR image patches and a big-data-driven function that precisely maps these LR representations to their corresponding HR representations. Extensive experimentation demonstrates the superior effectiveness and efficiency of CDA for single image SR compared to other state-of-the-art methods on Set5 and Set14 datasets.	.cda file;autoencoder;autostereogram;big data;blurred vision;cesarean section and delivery litter results domain;deep learning;display resolution;end-to-end principle;feedforward neural network;hl7publishingsubsection <operations>;high-resolution scheme;hoc (programming language);image resolution;inference;lr parser;map;mathematical optimization;morphologic artifacts;name;neural coding;noise reduction;representation (action);ringing (signal);sparse;super-resolution imaging;manifold;negative regulation of phospholipase c-activating g-protein coupled receptor signaling pathway	Kun Zeng;Jun Yu;Ruxin Wang;Cuihua Li;Dacheng Tao	2017	IEEE Transactions on Cybernetics	10.1109/TCYB.2015.2501373	iterative reconstruction;text mining;image resolution;manifold;feature extraction;computer science;artificial intelligence;data science;machine learning;data mining;artificial neural network;encoding	Vision	24.595001333167488	-50.38474295109776	178480
50b87c7d5eb02ecb666a40c13ee806f1898d4111	nonnegative/binary matrix factorization with a d-wave quantum annealer		D-Wave quantum annealers represent a novel computational architecture and have attracted significant interest. Much of this interest has focused on the quantum behavior of D-Wave machines, and there have been few practical algorithms that use the D-Wave. Machine learning has been identified as an area where quantum annealing may be useful. Here, we show that the D-Wave 2X can be effectively used as part of an unsupervised machine learning method. This method takes a matrix as input and produces two low-rank matrices as output-one containing latent features in the data and another matrix describing how the features can be combined to approximately reproduce the input matrix. Despite the limited number of bits in the D-Wave hardware, this method is capable of handling a large input matrix. The D-Wave only limits the rank of the two output matrices. We apply this method to learn the features from a set of facial images and compare the performance of the D-Wave to two classical tools. This method is able to learn facial features and accurately reproduce the set of facial images. The performance of the D-Wave shows some promise, but has some limitations. It outperforms the two classical codes in a benchmark when only a short amount of computational time is allowed (200-20,000 microseconds), but these results suggest heuristics that would likely outperform the D-Wave in this benchmark.	computation;d-wave two;machine learning;quantum annealing;simulated annealing	Daniel O'Malley;Velimir V. Vesselinov;Boian S. Alexandrov;Ludmil B. Alexandrov	2018		10.1371/journal.pone.0206653	nonnegative matrix	ML	17.848730830827392	-47.75290340345673	178663
4a03a01c20c0c10342e7a99f1e6fc7c5f9c1fd6c	does one size really fit all?: evaluating classifiers in bag-of-visual-words classification	computer vision;classifier comparison;visualization;bag of visual words	Bag-of-Visual-Words (BoVW) features that quantize and count local gradient distributions in images similar to counting words in texts have proven to be powerful image representations. In combination with supervised machine learning approaches, models for various visual concepts can be learned. While kernel-based Support Vector Machines have emerged as a de facto standard an extensive comparison of different supervised machine learning approaches has not been performed so far. In this paper we compare and discuss the performance of eight different classification models to be applied in BoVW approaches for image classification: Naïve Bayes, Logistic Regression, k-nearest neighbors, Random Forests, AdaBoost and linear Support Vector Machines (SVM) as well as generalized Gaussian kernel SVMs. Our results show that despite kernel-based SVMs performing best on the official Caltech-101 dataset, ensemble methods fall only shortly behind. In addition we present an approach for intuitive heat map-like visualization of the obtained models that help to better understand the reasons of a specific classification result.	adaboost;bag-of-words model in computer vision;caltech 101;ensemble learning;gradient;heat map;k-nearest neighbors algorithm;kernel (operating system);logistic regression;machine learning;naive bayes classifier;random forest;supervised learning;support vector machine	Christian Hentschel;Harald Sack	2014		10.1145/2637748.2638424	least squares support vector machine;computer science;machine learning;linear classifier;pattern recognition;data mining;bag-of-words model in computer vision	ML	19.439861070326675	-45.24034324426111	178954
2cf5ff18580c6073e555cf34793d71a63b40d406	multiple instance learning for soft bags via top instances	support vector machines image classification image representation learning artificial intelligence;support vector machine multiple instance learning mil soft bag classification labeling noise bag level representation large margin algorithm;supervised learning support vector machines labeling noise particle separators kernel algorithm design and analysis	A generalized formulation of the multiple instance learning problem is considered. Under this formulation, both positive and negative bags are soft, in the sense that negative bags can also contain positive instances. This reflects a problem setting commonly found in practical applications, where labeling noise appears on both positive and negative training samples. A novel bag-level representation is introduced, using instances that are most likely to be positive (denoted top instances), and its ability to separate soft bags, depending on their relative composition in terms of positive and negative instances, is studied. This study inspires a new large-margin algorithm for soft-bag classification, based on a latent support vector machine that efficiently explores the combinatorial space of bag compositions. Empirical evaluation on three datasets is shown to confirm the main findings of the theoretical analysis and the effectiveness of the proposed soft-bag classifier.	algorithm;automatic image annotation;comparison and contrast of classification schemes in linguistics and metadata;ibm notes;linear separability;multiple instance learning;support vector machine	Weixin Li;Nuno Vasconcelos	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7299056	instance-based learning;machine learning;pattern recognition;data mining;mathematics	Vision	23.722796092118674	-45.16836265378298	180075
8633732d9f787f8497c2696309c7d70176995c15	multi-objective convolutional learning for face labeling	labeling face training testing hair image edge detection semantics;random processes face recognition image classification image colour analysis learning artificial intelligence neural nets nonparametric statistics;helen dataset multiobjective convolutional learning face labeling conditional random field unary classifier pairwise classifier multiobjective learning method single unified deep convolutional network optimization distinct nonstructured loss function unary label likelihood encoding pairwise label dependency encoding nonparametric prior rgb image lfw dataset	This paper formulates face labeling as a conditional random field with unary and pairwise classifiers. We develop a novel multi-objective learning method that optimizes a single unified deep convolutional network with two distinct non-structured loss functions: one encoding the unary label likelihoods and the other encoding the pairwise label dependencies. Moreover, we regularize the network by using a nonparametric prior as new input channels in addition to the RGB image, and show that significant performance improvements can be achieved with a much smaller network size. Experiments on both the LFW and Helen datasets demonstrate state-of-the-art results of the proposed algorithm, and accurate labeling results on challenging images can be obtained by the proposed algorithm for real-world applications.	algorithm;conditional random field;experiment;loss function;unary operation	Sifei Liu;Jimei Yang;Chang Huang;Ming-Hsuan Yang	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298967	computer vision;computer science;artificial intelligence;machine learning;pattern recognition;statistics	Vision	24.356254121570878	-47.30311440559358	180204
0866b8fd99723b4f1ad84697e24342855766ace5	hashing with binary autoencoders	visual databases cryptography image processing image retrieval optimisation vectors;image retrieval binary autoencoders image databases binary hashing real valued image binary vector binary space hash function optimization;optimization binary codes decoding linear programming principal component analysis hamming distance training	An attractive approach for fast search in image databases is binary hashing, where each high-dimensional, real-valued image is mapped onto a low-dimensional, binary vector and the search is done in this binary space. Finding the optimal hash function is difficult because it involves binary constraints, and most approaches approximate the optimization by relaxing the constraints and then binarizing the result. Here, we focus on the binary autoencoder model, which seeks to reconstruct an image from the binary code produced by the hash function. We show that the optimization can be simplified with the method of auxiliary coordinates. This reformulates the optimization as alternating two easier steps: one that learns the encoder and decoder separately, and one that optimizes the code for each image. Image retrieval experiments show the resulting hash function outperforms or is competitive with state-of-the-art methods for binary hashing.	approximation algorithm;autoencoder;binary code;bit array;database;encoder;experiment;hash function;image retrieval;mathematical optimization	Miguel Á. Carreira-Perpiñán;Ramin Raziperchikolaei	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7298654	random binary tree;feature hashing;hash table;double hashing;hash function;linear hashing;perfect hash function;dynamic perfect hashing;binary independence model;theoretical computer science;machine learning;self-balancing binary search tree;universal hashing;pattern recognition;mathematics;k-independent hashing;uniform binary search;bit-length;binary pattern;2-choice hashing	Vision	19.9573361764649	-46.60747256504997	180639
97d4f9dbddb27c4b3046b20cb24e47b9be01a8bc	earliness-aware deep convolutional networks for early time series classification		We present Earliness-Aware Deep Convolutional Networks (EA-ConvNets), an end-to-end deep learning framework, for early classification of time series data. Unlike most existing methods for early classification of time series data, that are designed to solve this problem under the assumption of the availability of a good set of pre-defined (often hand-crafted) features, our framework can jointly perform feature learning (by learning a deep hierarchy of shapelets capturing the salient characteristics in each time series), along with a dynamic truncation model to help our deep feature learning architecture focus on the early parts of each time series. Consequently, our framework is able to make highly reliable early predictions, outperforming various state-of-the-art methods for early time series classification, while also being competitive when compared to the state-of-the-art time series classification algorithms that work with fully observed time series data. To the best of our knowledge, the proposed framework is the first to perform data-driven (deep) feature learning in the context of early classification of time series data. We perform a comprehensive set of experiments, on several benchmark data sets, which demonstrate that our method yields significantly better predictions than various state-of-the-art methods designed for early time series classification. In addition to obtaining high accuracies, our experiments also show that the learned deep shapelets based features are also highly interpretable and can help gain better understanding of the underlying characteristics of time series data.	algorithm;artificial neural network;benchmark (computing);convolutional neural network;deep learning;end-to-end principle;experiment;feature learning;network architecture;nonlinear system;time series;truncation	Wenlin Wang;Changyou Chen;Wenqi Wang;Piyush Rai;Lawrence Carin	2016	CoRR		computer science;machine learning;pattern recognition;data mining	ML	23.46917224561764	-50.334216236910144	182593
cd436f05fb4aeeda5d1085f2fe0384526571a46e	information bottleneck domain adaptation with privileged information for visual recognition		We address the unsupervised domain adaptation problem for visual recognition when an auxiliary data view is available during training. This is important because it allows improving the training of visual classifiers on a new target visual domain when paired additional source data is cheaply available. This is the case when we learn from a source of RGB plus depth data, for then test on a new RGB domain. The problem is challenging because of the intrinsic asymmetry caused by the missing auxiliary view during testing and from which discriminative information should be carried over to the new domain. We jointly account for the auxiliary view during training and for the domain shift by extending the information bottleneck method, and by combining it with risk minimization. In this way, we establish an information theoretic principle for learning any type of visual classifier under this particular settings. We use this principle to design a multi-class large-margin classifier with an efficient optimization in the primal space. We extensively compare our method with the state-of-the-art on several datasets, by effectively learning from RGB plus depth data to recognize objects and gender from a new RGB domain.	domain adaptation;experiment;information bottleneck method;iteration;margin classifier;mathematical optimization;source data;theory;view (sql)	Saeid Motiian;Gianfranco Doretto	2016		10.1007/978-3-319-46478-7_39	computer vision;pattern recognition	Vision	24.55334451985494	-45.41970880601912	182793
e9da812e004d845f43e51cc6abbea18ffb463f76	incremental pcanet: a lifelong learning framework to achieve the plasticity of both feature and classifier constructions		The plasticity in our brain gives us promising ability to learn and know the world. Although great successes have been achieved in many fields, few bio-inspired methods have mimiced this ability. They are infeasible when the data is time-varying and the scale is large because they need all training data loaded into memory. Furthermore, even the popular deep convolutional neural network (CNN) models have relatively fixed structures. Through incremental PCANet, this paper aims at exploring a lifelong learning framework to achieve the plasticity of both feature and classifier constructions. The proposed model mainly comprises of three parts: Gabor filters followed by maxpooling layer offering shift and scale tolerance to input samples, cascade incremental PCA to achieve the plasticity of feature extraction and incremental SVM to pursue plasticity of classifier construction. Different from CNN, the plasticity in our model has no back propogation (BP) process and don’t need huge parameters. Experiments have been done and their results validate the plasticity of our models in both feature and classifier constructions and further verify the hypothesis of physiology that the plasticity of high layer is better than the low layer.		Wang-Li Hao;Zhaoxiang Zhang	2016		10.1007/978-3-319-49685-6_27	natural language processing;computer science;machine learning;pattern recognition	NLP	22.150880108272947	-51.69913047640364	182969
da9020fe7c1fe0678f494254d1791cf462fc5afd	classification of rice grains using fuzzy artmap neural network	method of moments;fuzzy neural nets;fuzzy artmap;scaling phenomena;real time applications rotational invariant features properties rice grain classification fuzzy artmap neural networks scaled invariant zernike moment based feature extractors rice grain image information extraction incremental supervised learning multi dimensional map neural networks fa accuracy learning time reduction fast computation techniques higher lower zernike polynomials;supervised learning;computational techniques;food processing industry;image classification;fuzzy neural networks neural networks polynomials data mining systems engineering and theory supervised learning backpropagation algorithms feature extraction communication industry multilayer perceptrons;higher order;feature extraction;zernike moment;art neural nets;real time application;scale invariance;scaling phenomena food processing industry image classification art neural nets fuzzy neural nets zernike polynomials method of moments feature extraction;zernike polynomials;neural network	In this paper, a scaled invariant Zernike moment based feature extractor has been used to extract the relevant information from rice grain images for the purpose of classification. An incremental supervised learning and multidimensional map neural network, called fuzzy artmap (FA), has been proposed to reduce the learning time while maintaining high accuracy. A fast computation technique that uses the higher order Zernike polynomials to derive the lower order Zernike polynomials has been proposed to improve the computation speed of Zernike moments in real time applications.	artificial neural network	Chong-Yaw Wee;R. Paramesran;Fumiaki Takeda;Takeo Tsuzuki;Hiroshi Kadota;Satoshi Shimanouchi	2002		10.1109/APCCAS.2002.1115197	computer vision;contextual image classification;higher-order logic;method of moments;feature extraction;computer science;food processing;machine learning;scale invariance;zernike polynomials;pattern recognition;mathematics;supervised learning;artificial neural network	NLP	17.427210722030885	-46.15390803536566	183993
a8fcbe1152a5146ed7c9682225e4d23166ddb40c	deep variational network embedding in wasserstein space		Network embedding, aiming to embed a network into a low dimensional vector space while preserving the inherent structural properties of the network, has attracted considerable attentions recently. Most of the existing embedding methods embed nodes as point vectors in a low-dimensional continuous space. In this way, the formation of the edge is deterministic and only determined by the positions of the nodes. However, the formation and evolution of real-world networks are full of uncertainties, which makes these methods not optimal. To address the problem, we propose a novel Deep Variational Network Embedding in Wasserstein Space (DVNE) in this paper. The proposed method learns a Gaussian distribution in the Wasserstein space as the latent representation of each node, which can simultaneously preserve the network structure and model the uncertainty of nodes. Specifically, we use 2-Wasserstein distance as the similarity measure between the distributions, which can well preserve the transitivity in the network with a linear computational cost. Moreover, our method implies the mathematical relevance of mean and variance by the deep variational model, which can well capture the position of the node by the mean vectors and the uncertainties of nodes by the variance. Additionally, our method captures both the local and global network structure by preserving the first-order and second-order proximity in the network. Our experimental results demonstrate that our method can effectively model the uncertainty of nodes in networks, and show a substantial gain on real-world applications such as link prediction and multi-label classification compared with the state-of-the-art methods.	algorithmic efficiency;calculus of variations;first-order predicate;global network;multi-label classification;relevance;similarity measure;time complexity;unsupervised learning;variational principle;vertex-transitive graph	Dingyuan Zhu;Peng Cui;Daixin Wang;Wenwu Zhu	2018		10.1145/3219819.3220052	machine learning;computer science;similarity measure;global network;vector space;deep learning;transitive relation;embedding;artificial intelligence;gaussian	ML	21.60030337476262	-46.71416693096843	184460
f6c8c453aece2506fc660c06e048e7a550bb320b	deep learning from noisy image labels with quality embedding		There is an emerging trend to leverage noisy image datasets in many visual recognition tasks. However, the label noise among datasets severely degenerates the performance of deep learning approaches. Recently, one mainstream is to introduce the latent label to handle label noise, which has shown promising improvement in the network designs. Nevertheless, the mismatch between latent labels and noisy labels still affects the predictions in such methods. To address this issue, we propose a probabilistic model, which explicitly introduces an extra variable to represent the trustworthiness of noisy labels, termed as the quality variable. Our key idea is to identify the mismatch between the latent and noisy labels by embedding the quality variables into different subspaces, which effectively minimizes the influence of label noise. At the same time, reliable labels are still able to be applied for training. To instantiate the model, we further propose a contrastive-additive noise network (CAN), which consists of two important layers: 1) the contrastive layer that estimates the quality variable in the embedding space to reduce the influence of noisy labels and 2) the additive layer that aggregates the prior prediction and noisy labels as the posterior to train the classifier. Moreover, to tackle the challenges in optimization, we deduce an SGD algorithm with the reparameterization tricks, which makes our method scalable to big data. We validate the proposed method on a range of noisy image datasets. Comprehensive results have demonstrated that CAN outperforms the state-of-the-art deep learning approaches.		Jiangchao Yao;Jiajie Wang;Ivor W. Tsang;Ya Zhang;Jun Sun;Chengqi Zhang;Rui Zhang	2018	IEEE Transactions on Image Processing	10.1109/TIP.2018.2877939	computer vision;linear subspace;trustworthiness;scalability;big data;artificial intelligence;noise measurement;deep learning;pattern recognition;mathematics;statistical model;machine learning;embedding	ML	22.731529695594254	-45.39107526400675	186109
34a83abd3fc748f431fd421b25cec9670a97b79a	parameterization of a convolutional autoencoder for reconstruction of small images		A convolutional autoencoder (CAE) is formed by combining a convolutional neural network and an autoencoder, to take both their advantages in reconstructing the output from a compact, latent representation of the input. However, to our best knowledge, there is no exact recommendation for parameterizing a CAE, such as deciding the number of neurons in the hidden bottleneck layer of a CAE to avoid “underfitting” and “overfitting” of the network. Hence, a framework for deriving an optimum set of CAE parameters for the reconstruction of input images based on the standard MNIST data set is presented in this paper. The robustness of the parameters on a different image sizeu0027s data set, like the SVHN, is then verified. Our results show that for small ( $28times 28$ ) and ( $32 times 32$ ) pixelsu0027 input images, having 2560 neurons at the hidden bottleneck layer and 32 convolutional feature maps can result in optimum reconstruction performance for the CAEs. In addition, the quantitative Mean-Square-Error and the qualitative (2D visualization of the neuronsu0027 activation, the histogram statistics and estimated source entropy at the hidden layers) analysis methodology provided by this work can provide a good framework for deciding the parameter values of the CAEs to provide good representations of the input image.		Lai Meng Tang;Li Hong Lim;Paul Siebert	2018	2018 15th International Conference on Control, Automation, Robotics and Vision (ICARCV)	10.1109/ICARCV.2018.8581254	control theory;robustness (computer science);autoencoder;overfitting;convolutional neural network;parametrization;histogram;computer science;mnist database;image resolution;artificial intelligence;pattern recognition	Robotics	24.2191095540039	-50.590351533004096	186346
5d21006fa32ff69f6b0a646f26ce0db84f2f4d33	evaluation of pooling operations in convolutional architectures for object recognition	object recognition;aggregation function;error rate;shift invariant;invariant feature	A common practice to gain invariant features in object recognition models is to aggregate multiple low-level features over a small neighborhood. However, the differences between those models makes a comparison of the properties of different aggregation functions hard. Our aim is to gain insight into different functions by directly comparing them on a fixed architecture for several common object recognition tasks. Empirical results show that a maximum pooling operation significantly outperforms subsampling operations. Despite their shift-invariant properties, overlapping pooling windows are no significant improvement over non-overlapping pooling windows. By applying this knowledge, we achieve state-of-the-art error rates of 4.57% on the NORB normalized-uniform dataset and 5.6% on the NORB jittered-cluttered dataset.	aggregate data;aggregate function;chroma subsampling;convolutional neural network;high- and low-level;microsoft windows;outline of object recognition;select (sql);window function	Dominik Scherer;Andreas C. Müller;Sven Behnke	2010		10.1007/978-3-642-15825-4_10	word error rate;computer science;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;shift-invariant system;statistics	Vision	23.925768823944175	-50.110297998426184	186434
cbae6e54fabc10dd4b3ca71b0376ec6675c59e22	tensor networks for dimensionality reduction, big data and deep learning		Large scale multidimensional data are often available as multiway arrays or higher-order tensors which can be approximately represented in distributed forms via low-rank tensor decompositions and tensor networks. Our particular emphasis is on elucidating that, by virtue of the underlying low-rank approximations, tensor networks have the ability to reduce the dimensionality and alleviate the curse of dimensionality in a number of applied areas, especially in large scale optimization problems and deep learning. We briefly review and provide tensor links between low-rank tensor network decompositions and deep neural networks. We elucidating, through graphical illustrations, that low-rank tensor approximations and sophisticated contractions of core tensors, tensor networks have the ability to perform distributed computations on otherwise prohibitively large volume of data/parameters. Our focus is on the Hierarchical Tucker, tensor train (TT) decompositions and MERA tensor networks in specific applications.	big data;deep learning;dimensionality reduction	Andrzej Cichocki	2018		10.1007/978-3-319-67946-4_1	tensor;artificial neural network;deep learning;curse of dimensionality;big data;machine learning;computation;dimensionality reduction;mathematics;artificial intelligence;optimization problem	ML	20.728814668454678	-46.358613267841115	186437
4682d98cc9273473ebaf85b9395351955857e93c	deep multiple description coding by learning scalar quantization		In this paper, we propose a deep multiple description coding framework, whose quantizers are adaptively learned via the minimization of multiple description compressive loss. Firstly, our framework is built upon auto-encoder networks, which have multiple description multi-scale dilated encoder network and multiple description decoder networks. Secondly, two entropy estimation networks are learned to estimate the informative amounts of the quantized tensors, which can further supervise the learning of multiple description encoder network to represent the input image delicately. Thirdly, a pair of scalar quantizers accompanied by two importance-indicator maps is automatically learned in an end-to-end self-supervised way. Finally, multiple description structural dis-similarity distance loss is imposed on multiple description decoded images in pixel domain for diversified multiple description generations rather than on feature tensors in feature domain, in addition to multiple description reconstruction loss. Through testing on two commonly used datasets, it is verified that our method is beyond several state-of-the-art multiple description coding approaches in terms of coding efficiency.		Lijun Zhao;Huihui Bai;Anhong Wang;Yao Zhao	2018	CoRR		theoretical computer science;encoder;scalar (physics);tensor;entropy estimation;quantization (signal processing);multiple description coding;quantization (physics);algorithmic efficiency;computer science	AI	24.408291204081163	-50.860716612233404	186679
96ceb1f3f5c3aec42ce8fc7549e06c63cdbfbb07	a gradient-based split criterion for highly accurate and transparent model trees		Machine learning algorithms aim at minimizing the number of false decisions and increasing the accuracy of predictions. However, the high predictive power of advanced algorithms comes at the costs of transparency. State-of-the-art methods, such as neural networks and ensemble methods, often result in highly complex models that offer little transparency. We propose shallow model trees as a way to combine simple and highly transparent predictive models for higher predictive power without losing the transparency of the original models. We present a novel split criterion for model trees that allows for significantly higher predictive power than state-ofthe-art model trees while maintaining the same level of simplicity. This novel approach finds split points which allow the underlying simple models to make better predictions on the corresponding data. In addition, we introduce multiple mechanisms to increase the transparency of the resulting trees.	algorithm;artificial neural network;ensemble learning;experiment;gradient descent;machine learning;mathematical optimization;one-hot;predictive modelling	Klaus Broelemann;Gjergji Kasneci	2018	CoRR		machine learning;artificial neural network;ensemble learning;mathematics;artificial intelligence;transparency (graphic);predictive power	ML	21.569066463648415	-50.159665277047694	186735
81774d440f5d3bff188a8c7585a31bb08eb11cd6	theoretic analysis and extremely easy algorithms for domain adaptive feature learning		Domain adaptation problems arise in a variety of applications, where a training dataset from the textit{source} domain and a test dataset from the textit{target} domain typically follow different distributions. The primary difficulty in designing effective learning models to solve such problems lies in how to bridge the gap between the source and target distributions. In this paper, we provide comprehensive analysis of feature learning algorithms used in conjunction with linear classifiers for domain adaptation. Our analysis shows that in order to achieve good adaptation performance, the second moments of the source domain distribution and target domain distribution should be similar. Based on our new analysis, a novel extremely easy feature learning algorithm for domain adaptation is proposed. Furthermore, our algorithm is extended by leveraging multiple layers, leading to a deep linear model. We evaluate the effectiveness of the proposed algorithms in terms of domain adaptation tasks on the Amazon review dataset and the spam dataset from the ECML/PKDD 2006 discovery challenge.	algorithm;domain adaptation;ecml pkdd;feature learning;linear classifier;linear model;machine learning;spamming;theory	Wenhao Jiang;Cheng Deng;Wei Liu;Feiping Nie;Korris Fu-Lai Chung;Heng Huang	2017		10.24963/ijcai.2017/272	machine learning;artificial intelligence;domain adaptation;linear model;computer science;algorithm;feature learning	AI	19.939210974454245	-48.66156558688049	186954
3e08a3912ebe494242f6bcd772929cc65307129c	few-shot image recognition by predicting parameters from activations		In this paper, we are interested in the few-shot learning problem. In particular, we focus on a challenging scenario where the number of categories is large and the number of examples per novel category is very limited, e.g. 1, 2, or 3. Motivated by the close relationship between the parameters and the activations in a neural network associated with the same category, we propose a novel method that can adapt a pre-trained neural network to novel categories by directly predicting the parameters from the activations. Zero training is required in adaptation to novel categories, and fast inference is realized by a single forward pass. We evaluate our method by doing few-shot image recognition on the ImageNet dataset, which achieves the state-of-the-art classification accuracy on novel categories by a significant margin while keeping comparable performance on the large-scale categories. We also test our method on the MiniImageNet dataset and it strongly outperforms the previous state-of-the-art methods.		Siyuan Qiao;Chenxi Liu;Wei Shen;Alan L. Yuille	2018	2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2018.00755	artificial intelligence;artificial neural network;machine learning;computer vision;pattern recognition;linearity;visualization;computer science;inference;training set	Vision	23.896982702331698	-50.667014447506176	187088
87c198f86bc6eaa9439d309904081ac294382abb	constructing interactive visual classification, clustering and dimension reduction models for n-d data		Abstract: The exploration of multidimensional datasets of all possible sizes and dimensions is a long-standing challenge in knowledge discovery, machine learning, and visualization. While multiple efficient visualization methods for n-D data analysis exist, the loss of information, occlusion, and clutter continue to be a challenge. This paper proposes and explores a new interactive method for visual discovery of n-D relations for supervised learning. The method includes automatic, interactive, and combined algorithms for discovering linear relations, dimension reduction, and generalization for non-linear relations. This method is a special category of reversible General Line Coordinates (GLC). It produces graphs in 2-D that represent n-D points losslessly, i.e., allowing the restoration of n-D data from the graphs. The projections of graphs are used for classification. The method is illustrated by solving machine-learning classification and dimension-reduction tasks from the domains of image processing, computer-aided medical diagnostics, and finance. Experiments conducted on several datasets show that this visual interactive method can compete in accuracy with analytical machine learning algorithms.	algorithm;circuit restoration;clutter;dimensionality reduction;experiment;image processing;lossless compression;machine learning;nonlinear system;supervised learning	Boris Kovalerchuk;Dmytro Dovhalets	2017	Informatics	10.3390/informatics4030023	supervised learning;visualization;interactive visual analysis;cluster analysis;dimensionality reduction;visual analytics;interactive visualization;line coordinates;machine learning;artificial intelligence;computer science	ML	17.36438609778863	-47.89890436803483	187840
f7af5aaec8f7ff19ca6d0be7ffefe72a12a73b8a	graph spectral regularization for neural network interpretability		Deep neural networks can learn meaningful representations of data. However, these representations are hard to interpret. For example, visualizing a latent layer is generally only possible for at most three dimensions. Neural networks are able to learn and benefit from much higher dimensional representations but these are not visually interpretable because neurons have arbitrary ordering within a layer. Here, we utilize the ability of a human observer to identify patterns in structured representations to visualize higher dimensions. To do so, we propose a class of regularizations we call Graph Spectral Regularizations that impose graph structure on latent layers. This is achieved by treating activations as signals on a predefined graph and constraining those activations using graph filters, such as low pass and wavelet-like filters. This framework allows for any kind of graphs and filters to achieve a wide range of structured regularizations depending on the inference needs of the data. First, we show a synthetic example where a graphstructured layer reveals topological features of the data. Next, we show that a smoothing regularization imposes semantically consistent ordering of nodes when applied to capsule nets. Further, we show that the graph-structured layer, using wavelet-like spatially localized filters, can form local receptive fields for improved image and biomedical data interpretation. In other words, the mapping between latent layer, neurons and the output space becomes clear due to the localization of the activations. Finally, we show that when structured as a grid, the representations create coherent images that allow for image processing techniques such as convolutions.	call graph;coherence (physics);convolution;graph (discrete mathematics);image processing;matrix regularization;neural networks;smoothing;stellar classification;synthetic intelligence;wavelet	Alexander Tong;David van Dijk;Jay S. Stanley;Matthew Amodio;Guy Wolf;Smita Krishnaswamy	2018	CoRR		machine learning;visualization;state space;laplacian matrix;artificial neural network;artificial intelligence;mathematics;black box;regularization (mathematics);interpretability;data set;pattern recognition	ML	22.45675150443703	-48.33155301370655	188233
e31fa9510047c0df23fb4dd37ee7c70783a3fa60	out-of-distribution detection using an ensemble of self supervised leave-out classifiers		As deep learning methods form a critical part in commercially important applications such as autonomous driving and medical diagnostics, it is important to reliably detect out-of-distribution (OOD) inputs while employing these algorithms. In this work, we propose an OOD detection algorithm which comprises of an ensemble of classifiers. We train each classifier in a self-supervised manner by leaving out a random subset of training data as OOD data and the rest as in-distribution (ID) data. We propose a novel margin-based loss over the softmax output which seeks to maintain at least a margin m between the average entropy of the OOD and in-distribution samples. In conjunction with the standard cross-entropy loss, we minimize the novel loss to train an ensemble of classifiers. We also propose a novel method to combine the outputs of the ensemble of classifiers to obtain OOD detection score and class prediction. Overall, our method convincingly outperforms Hendrycks et al. [7] and the current state-of-the-art ODIN [13] on several OOD detection benchmarks.	algorithm;anomaly detection;artificial neural network;autonomous car;benchmark (computing);computation;computational resource;cross entropy;deep learning;loss function;overhead (computing);softmax function;time complexity	Apoorv Vyas;Nataraj Jammalamadaka;Xia Zhu;Dipankar Das;Bharat Kaul;Theodore L. Willke	2018		10.1007/978-3-030-01237-3_34	machine learning;anomaly detection;deep learning;computer science;training set;artificial intelligence;softmax function	ML	20.57230886832544	-51.218476754564165	188425
d604f18cdfa80cc1db4d9a9d0edcead2b4df03d5	extremely low bit neural network: squeeze the last bit out with admm		Although deep learning models are highly effective for various learning tasks, their high computational costs prohibit the deployment to scenarios where either memory or computational resources are limited. In this paper, we focus on compressing and accelerating deep models with network weights represented by very small numbers of bits, referred to as extremely low bit neural network. We model this problem as a discretely constrained optimization problem. Borrowing the idea from Alternating Direction Method of Multipliers (ADMM), we decouple the continuous parameters from the discrete constraints of network, and cast the original hard problem into several subproblems. We propose to solve these subproblems using extragradient and iterative quantization algorithms that lead to considerably faster convergency compared to conventional optimization methods. Extensive experiments on image recognition and object detection verify that the proposed algorithm is more effective than state-ofthe-art approaches when coming to extremely low bit neural network.	algorithm;artificial neural network;augmented lagrangian method;computation;computational resource;computer vision;constrained optimization;constraint (mathematics);convolutional neural network;deep learning;experiment;heuristic (computer science);iterative method;linear programming;mathematical optimization;object detection;optimization problem;software deployment	Cong Leng;Savanna Gornisiewicz;Hao Li;Shenghuo Zhu;Rong Jin	2018			mathematical optimization;low bit;quantization (signal processing);artificial neural network;constrained optimization;deep learning;computer science;object detection;artificial intelligence	AI	20.624418282866483	-47.31979247384501	188773
2750dbc60d5ccc8fbe5e4babae6cfab543940f1a	distance-based image classification: generalizing to new classes at near-zero cost	nearest class mean classifiers;linear svm;metric learning approach;zero shot learning;measurement;support vector machines;imagenet hierarchy;training images;training;measurement training support vector machine classification covariance matrices image classification image retrieval training data;image classification;zero shot class prior;distance based image classification;imagenet hierarchy distance based image classification near zero cost large scale image classification methods training images negligible cost distance based classifiers k nearest neighbor k nn nearest class mean classifiers ncm classifiers metric learning approach richer class representations imagenet 2010 challenge dataset ncm performance linear svm current state of the art performance imagenet 10k dataset zero shot class prior;metric learning;k nearest neighbors classification;support vector machines image classification learning artificial intelligence;large scale image classification methods;training data;algorithms artificial intelligence computer simulation image enhancement image interpretation computer assisted models theoretical pattern recognition automated;current state of the art performance;near zero cost;covariance matrices;k nn;ncm classifiers;ncm performance;imagenet 10k dataset;image retrieval metric learning k nearest neighbors classification nearest class mean classification large scale image classification transfer learning zero shot learning;transfer learning;nearest class mean classification;support vector machine classification;k nearest neighbor;distance based classifiers;negligible cost;learning artificial intelligence;imagenet 2010 challenge dataset;large scale image classification;richer class representations;image retrieval	We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 106 training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current state-of-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available.	arabic numeral 0;class;computer vision;experiment;generalization (psychology);imagenet;k-nearest neighbors algorithm;silo (dataset);single linkage cluster analysis;statistical classification;orders - hl7publishingdomain	Thomas Mensink;Jakob J. Verbeek;Florent Perronnin;Gabriela Csurka	2013	IEEE Transactions on Pattern Analysis and Machine Intelligence	10.1109/TPAMI.2013.83	support vector machine;training set;contextual image classification;image retrieval;transfer of learning;computer science;machine learning;pattern recognition;data mining;k-nearest neighbors algorithm;measurement	Vision	24.335274551506473	-46.266494259908164	188815
c93f8333c1f395451b30a2d9d1eaa053c0feefe7	malicious software classification using transfer learning of resnet-50 deep neural network		Malicious software (malware) has been extensively used for illegal activity and new malware variants are discovered at an alarmingly high rate. The ability to group malware variants into families with similar characteristics makes possible to create mitigation strategies that work for a whole class of programs. In this paper, we present a malware family classification approach using a deep neural network based on the ResNet-50 architecture. Malware samples are represented as byteplot grayscale images and a deep neural network is trained freezing the convolutional layers of ResNet-50 pre-trained on the ImageNet dataset and adapting the last layer to malware family classification. The experimental results on a dataset comprising 9,339 samples from 25 different families showed that our approach can effectively be used to classify malware families with an accuracy of 98.62%.	artificial neural network;deep learning;experiment;feature extraction;gist;grayscale;imagenet;malware;pixel;randomness extractor	Edmar R. S. De Rezende;Guilherme C. S. Ruppert;Tiago Carvalho;Fabio Tozeto Ramos;Paulo de Geus	2017	2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2017.00-19	artificial neural network;transfer of learning;artificial intelligence;computer science;convolutional code;machine learning;grayscale;visualization;feature extraction;residual neural network;malware;pattern recognition	ML	19.933666128164006	-51.32102504651703	190138
b1db9b9ace4f71a8a59188f0aa7e96687337e2eb	robust adaptive label propagation by double matrix decomposition		In this paper, we investigate the robust transductive label prediction problem. Technically, a Robust Adaptive Label Propagation framework by Double Matrix Decomposition, called ALP-MD, is proposed for the semi-supervised data classification. Compared with existing transductive label propagation models, our ALP-MD improves the classification power by performing label prediction in the clean data space and clean label space at the same time. More specifically, our ALP-MD clearly integrates the idea of double matrix decomposition into the process of label prediction for the noise removal. Since the predicted soft labels usually contains noise and mixed signs, our ALP-MD explicitly decomposes the predicted soft label matrix into a clean soft label matrix and a noise term and then estimates the hard label based on the clean soft label matrix for more accurate classification. In addition, ALP-MD also involves a regularization term to model the noise in data, integrates the adaptive weights learning into the process of robust label prediction and moreover performs the weights learning in the clean data space. Thus, our ALP-MD can explicitly ensure the learned weights to be informative as much as possible and to be joint optimal for both representation and classification, and potentially enhance the label prediction ability. Extensive comparisons demonstrated its effectiveness.		Huan Zhang;Zhao Zhang;Sheng Li;Qiaolin Ye;Mingbo Zhao;Meng Wang	2018	2018 24th International Conference on Pattern Recognition (ICPR)	10.1109/ICPR.2018.8545594	artificial intelligence;data classification;sparse matrix;matrix (mathematics);transduction (machine learning);regularization (mathematics);pattern recognition;data modeling;matrix decomposition;computer science	ML	22.48305573253952	-45.30949513318079	190282
0850367549a453373898480410bd26ac2a914364	discrete variational autoencoders		Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We introduce a novel class of probabilistic models, comprising an undirected discrete component and a directed hierarchical continuous component, that can be trained efficiently using the variational autoencoder framework. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data; and outperforms state-of-the-art methods on the permutation-invariant MNIST, OMNIGLOT, and Caltech-101 Silhouettes datasets.	autoencoder;backpropagation;calculus of variations;caltech 101;electronic component;graph (discrete mathematics);latent variable;mnist database;pixel;variational principle	Jason Tyler Rolfe	2016	CoRR		combinatorics;discrete mathematics;machine learning;discrete system;mathematics	ML	22.81187664666125	-48.3161857591793	191367
eeae96c97fc26cf2355d286870bfcd1dea70a385	a mapreduce cortical algorithms implementation for unsupervised learning of big data		In the big data era, the need for fast robust machine learning techniques is rapidly increasing. Deep network architectures such as cortical algorithms are challenged by big data problems which result in lengthy and complex training. In this paper, we present a distributed cortical algorithm implementation for the unsupervised learning of big data based on a combined node-data parallelization scheme. A data sparsity measure is used to divide the data before distributing the columns in the network over many computing nodes based on the MapReduce framework. Experimental results on multiple datasets showed an average speedup of 8.1× compared to serial implementations.	algorithm;big data;column (database);machine learning;mapreduce;overfitting;parallel computing;sparse matrix;speedup;unsupervised learning	Nadine Hajj;Yara Rizk;Mariette Awad	2015		10.1016/j.procs.2015.07.310	computer science;theoretical computer science;machine learning;data mining	ML	18.518354217424722	-46.02285939951203	191642
5e39deb4bff7b887c8f3a44dfe1352fbcde8a0bd	supervised cosmos autoencoder: learning beyond the euclidean loss!		Autoencoders are unsupervised deep learning models used for learning representations. In literature, autoencoders have shown to perform well on a variety of tasks spread across multiple domains, thereby establishing widespread applicability. Typically, an autoencoder is trained to generate a model that minimizes the reconstruction error between the input and the reconstructed output, computed in terms of the Euclidean distance. While this can be useful for applications related to unsupervised reconstruction, it may not be optimal for classification. In this paper, we propose a novel Supervised COSMOS Autoencoder which utilizes a multi-objective loss function to learn representations that simultaneously encode the (i) “similarity” between the input and reconstructed vectors in terms of their direction, (ii) “distribution” of pixel values of the reconstruction with respect to the input sample, while also incorporating (iii) “discriminability” in the feature learning pipeline. The proposed autoencoder model incorporates a Cosine similarity and Mahalanobis distance based loss function, along with supervision via Mutual Information based loss. Detailed analysis of each component of the proposed model motivates its applicability for feature learning in different classification tasks. The efficacy of Supervised COSMOS autoencoder is demonstrated via extensive experimental evaluations on different image datasets. The proposed model outperforms existing algorithms on MNIST, CIFAR-10, and SVHN databases. It also yields state-of-the-art results on CelebA, LFWA, Adience, and IJB-A databases for attribute prediction and face recognition, respectively.	algorithm;autoencoder;benchmark (computing);computer vision;convolutional neural network;cosine similarity;deep learning;encode;euclidean distance;facial recognition system;feature learning;loss function;mnist database;mutual information;neural networks;pixel;programming paradigm;unsupervised learning	Maneet Singh;Shruti Nagpal;Mayank Vatsa;Richa Singh;Afzel Noore	2018	CoRR		machine learning;autoencoder;mahalanobis distance;euclidean distance;deep learning;mutual information;computer science;pattern recognition;feature learning;cosine similarity;mnist database;artificial intelligence	AI	23.48879648820478	-47.18516895752951	191926
1323e811e3ae8dbf3a4dd336a91d69d5e9324e8f	two challenges of correct validation in pattern recognition	overfitting;metalearning;pattern recognition;validation;qa0075 electronic computers computer science;crossvalidation	*Correspondence: Thomas Nowotny , Centre for Computational Neuroscience and Robotics, School of Engineering and Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK e-mail: t.nowotny@sussex.ac.uk Supervised pattern recognition is the process of mapping patterns to class labels that define their meaning.The core methods for pattern recognition have been developed by machine learning experts but due to their broad success, an increasing number of non-experts are now employing and refining them. In this perspective, I will discuss the challenge of correct validation of supervised pattern recognition systems, in particular when employed by nonexperts. To illustrate the problem, I will give three examples of common errors that I have encountered in the last year. Much of this challenge can be addressed by strict procedure in validation but there are remaining problems of correctly interpreting comparative work on exemplary data sets, which I will elucidate on the example of the well-used MNIST data set of handwritten digits.	common lisp;computation;computational neuroscience;email;informatics;mnist database;machine learning;pattern recognition;robotics;whole earth 'lectronic link	Thomas Nowotny	2014	Front. Robotics and AI	10.3389/frobt.2014.00005	feature;computer science;artificial intelligence;machine learning;data mining;overfitting	AI	20.999308471986392	-45.09770905545281	192520
b45c6cd20ea8be4d55c8b59dc7b6ed634053009a	efficient discriminative convolution using fisher weight map		Convolutional neural networks (CNNs) have been studied for a long time, and recently gained increasingly more attention. Deep CNNs have especially achieved remarkably high performance on many visual recognition tasks due to their high levels of flexibility. However, since CNNs require numerous parameters to be tuned via iterative operations through layers, their computational cost is immense. Moreover, they often require a huge number of training samples and technical tricks, such as unsupervised pretraining and heuristic tuning, to successfully train the system. In this work, we present a very simple method of layer-wise convolution. We can obtain discriminative filters by using a Fisher weight map, which well separates convolved images between categories. This operation can be deterministically solved as a simple eigenvalue problem and no back propagation or hyper-parameters are needed. Because our method is layer-wise and based on a simple eigenvalue problem, it is computationally efficient. Also, it is relatively stable with a moderate amount of training samples and is capable of learning densely from high-dimensional descriptors without dropping connections between neurons, which is a common practice in conventional implementations of CNNs. We demonstrated the promising performance of our method in extensive experiments with two datasets. Our network used together with appropriate pooling and rectification techniques achieved remarkably high performance that was distinctly comparable to those that were state-of-the-art.	algorithmic efficiency;artificial neural network;backpropagation;computation;convolution;convolutional neural network;deep blue (chess computer);deterministic algorithm;experiment;glossary of computer graphics;gradient descent;heuristic;iterative method;mnist database;minimum-weight triangulation;rectifier;rectifier (neural networks);software propagation;unsupervised learning;you watch the k foundation burn a million quid	Hideki Nakayama	2013		10.5244/C.27.100	arithmetic;mathematical analysis;fisher kernel	ML	22.595253563435374	-50.63021028596608	193169
2d00c140fbe5fd65631d6071960ab6a853530fbd	saltinet: scan-path prediction on 360 degree images using saliency volumes		We introduce SaltiNet, a deep neural network for scan-path prediction trained on 360-degree images. The model is based on a temporal-aware novel representation of saliency information named the saliency volume. The first part of the network consists of a model trained to generate saliency volumes, whose parameters are fit by back-propagation computed from a binary cross entropy (BCE) loss over downsampled versions of the saliency volumes. Sampling strategies over these volumes are used to generate scan-paths over the 360-degree images. Our experiments show the advantages of using saliency volumes, and how they can be used for related tasks. Our source code and trained models available at https://github.com/massens/saliency-360salient-2017.	artificial neural network;backpropagation;cross entropy;decimation (signal processing);deep learning;experiment;gibbs sampling;software propagation	Marc Assens;Kevin McGuinness;Xavier Giró;Noel E. O'Connor	2017	2017 IEEE International Conference on Computer Vision Workshops (ICCVW)	10.1109/ICCVW.2017.275	convolutional neural network;machine learning;cross entropy;image processing;kadir–brady saliency detector;computer vision;artificial intelligence;sampling (statistics);pattern recognition;artificial neural network;salience (neuroscience);deep learning;computer science	Vision	23.864673702237393	-51.55590880068628	194785
063e5be439030fd0ba54a9636d101aa6b8bc5d2a	deep learning of binary hash codes for fast image retrieval	neural nets binary codes convolution image representation image retrieval learning artificial intelligence;binary codes;semantics;visualization;machine learning;image representation;mnist datasets binary hash codes approximate nearest neighbor search convolutional neural networks cnn effective deep learning framework fast image retrieval data labels hidden layer latent concepts supervised methods image representations class labels pairwised inputs large scale datasets cifar 10 datasets;neurons;image retrieval binary codes image representation machine learning neurons semantics visualization;image retrieval	Approximate nearest neighbor search is an efficient strategy for large-scale image retrieval. Encouraged by the recent advances in convolutional neural networks (CNNs), we propose an effective deep learning framework to generate binary hash codes for fast image retrieval. Our idea is that when the data labels are available, binary codes can be learned by employing a hidden layer for representing the latent concepts that dominate the class labels. The utilization of the CNN also allows for learning image representations. Unlike other supervised methods that require pair-wised inputs for binary code learning, our method learns hash codes and image representations in a point-wised manner, making it suitable for large-scale datasets. Experimental results show that our method outperforms several state-of-the-art hashing algorithms on the CIFAR-10 and MNIST datasets. We further demonstrate its scalability and efficacy on a large-scale dataset of 1 million clothing images.	algorithm;artificial neural network;binary code;convolutional neural network;deep learning;hash function;image retrieval;mnist database;multilayer perceptron;nearest neighbor search;scalability;supervised learning	Kevin Lin;Huei-Fang Yang;Jen-Hao Hsiao;Chu-Song Chen	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2015.7301269	computer vision;binary code;visual word;visualization;image retrieval;computer science;theoretical computer science;machine learning;pattern recognition;semantics;automatic image annotation	Vision	23.754325932189087	-51.239692155898325	195091
20cc4bfdb648fd7947c71252589fc867d4d16933	pairwise confusion for fine-grained visual classification		Fine-Grained Visual Classification (FGVC) datasets contain small sample sizes, along with significant intra-class variation and interclass similarity. While prior work has addressed intra-class variation using localization and segmentation techniques, inter-class similarity may also affect feature learning and reduce classification performance. In this work, we address this problem using a novel optimization procedure for the end-to-end neural network training on FGVC tasks. Our procedure, called Pairwise Confusion (PC) reduces overfitting by intentionally introducing confusion in the activations. With PC regularization, we obtain state-ofthe-art performance on six of the most widely-used FGVC datasets and demonstrate improved localization ability. PC is easy to implement, does not need excessive hyperparameter tuning during training, and does not add significant overhead during test time.	artificial neural network;consortium;convolutional neural network;end-to-end principle;experiment;facebook platform;feature learning;mathematical optimization;overfitting;overhead (computing);performance tuning;radio frequency;rapid refresh	Abhimanyu Dubey;Otkrist Gupta;Pei Guo;Ramesh Raskar;Ryan Farrell;Nikhil Naik	2018		10.1007/978-3-030-01258-8_5	artificial intelligence;artificial neural network;pattern recognition;confusion;overfitting;computer science;machine learning;sample size determination;regularization (mathematics);pairwise comparison;hyperparameter;feature learning	Vision	22.566601105353847	-46.52441434165069	195604
15285d8ae6d2fef3dfecaeacbf5a246bfc7b3137	mitigating adversarial effects through randomization		Convolutional neural networks have demonstrated their powerful ability on various tasks in recent years. However, they are extremely vulnerable to adversarial examples. I.e., clean images, with imperceptible perturbations added, can easily cause convolutional neural networks to fail. In this paper, we propose to utilize randomization to mitigate adversarial effects. Specifically, we use two randomization operations: random resizing, which resizes the input images to a random size, and random padding, which pads zeros around the input images in a random manner. Extensive experiments demonstrate that the proposed randomization method is very effective at defending against both single-step and iterative attacks. Our method also enjoys the following advantages: 1) no additional training or fine-tuning, 2) very few additional computations, 3) compatible with other adversarial defense methods. By combining the proposed randomization method with an adversarially trained model, it achieves a normalized score of 0.924 (ranked No.2 among 107 defense teams) in the NIPS 2017 adversarial examples defense challenge, which is far better than using adversarial training alone with a normalized score of 0.773 (ranked No.56). The code is public available at https: //github.com/cihangxie/NIPS2017_adv_challenge_defense.	adversary (cryptography);artificial neural network;computation;convolutional neural network;experiment;https;iterative method;nips;perturbation theory;randomized algorithm	Cihang Xie;Jianyu Wang;Zhishuai Zhang;Zhou Ren;Alan L. Yuille	2017	CoRR		uniformization (probability theory);machine learning;artificial intelligence;pattern recognition;convolutional neural network;randomization;normalization (statistics);computer science;padding;adversarial system;ranking	ML	19.11315809168743	-50.989295822658626	195879
91f88cced4015d99e5234a45ddf0e127e53bdf21	factored 3-way restricted boltzmann machines for modeling natural images	natural images;boltzmann machine	Deep belief nets have been successful in modeling handwritten characters, but it has proved more difficult to apply them to real images. The problem lies in the restricted Boltzmann machine (RBM) which is used as a module for learning deep belief nets one layer at a time. The Gaussian-Binary RBMs that have been used to model real-valued data are not a good way to model the covariance structure of natural images. We propose a factored 3-way RBM that uses the states of its hidden units to represent abnormalities in the local covariance structure of an image. This provides a probabilistic framework for the widely used simple/complex cell architecture. Our model learns binary features that work very well for object recognition on the “tiny images” data set. Even better features are obtained by then using standard binary RBM’s to learn a deeper model.	approximation algorithm;cuda;cell (microprocessor);deep belief network;gist;graphics processing unit;hidden variable theory;hybrid memory cube;image scaling;map;outline of object recognition;partition function (mathematics);restricted boltzmann machine;sampling (signal processing);sampling in order	Marc'Aurelio Ranzato;Alex Krizhevsky;Geoffrey E. Hinton	2010			artificial intelligence;machine learning;mathematics;algorithm	ML	23.344512398961655	-49.072800631272784	196935
7a9dff49861ad7bad87d83e2c29d5148ae21aa36	semi-supervised deep metrics for image registration		Deep metrics have been shown effective as similarity measures in multi-modal image registration; however, the metrics are currently constructed from aligned image pairs in the training data. In this paper, we propose a strategy for learning such metrics from roughly aligned training data. Symmetrizing the data corrects bias in the metric that results from misalignment in the data (at the expense of increased variance), while random perturbations to the data, i.e. dithering, ensures that the metric has a single mode, and is amenable to registration by optimization. Evaluation is performed on the task of registration on separate unseen test image pairs. The results demonstrate the feasibility of learning a useful deep metric from substantially misaligned training data, in some cases the results are significantly better than from Mutual Information. Data augmentation via dithering is, therefore, an effective strategy for discharging the need for well-aligned training data; this brings deep metric registration from the realm of supervised to semi-supervised machine learning.	dither;image registration;machine learning;mathematical optimization;modal logic;mutual information;semiconductor industry;standard test image;supervised learning	Alireza Sedghi;Jie Luo;Alireza Mehrtash;Steven D. Pieper;Clare M. Tempany;Tina Kapur;Parvin Mousavi;William M. Wells	2018	CoRR		artificial intelligence;computer science;mutual information;pattern recognition;dither;image registration;training set;standard test image	Vision	22.442088297370685	-49.78509399400288	197101
484a1f26a16bf35e568456ee7271da0cdd7e1ff1	single image super-resolution using multiple extreme learning machine regressors		This paper presents a new technique to solve the single image super resolution reconstruction problem based on multiple extreme learning machine regressors, called here MELM. The MELM employs a feature space of low resolution images, divided in subspaces, and one regressor is trained for each one. In the training task, we employ a color dataset containing 91 images, with approximately 5.3 million pixels, and PSNR and SSIM as metric evaluation. For the experiments we use two datasets, Set 5 and Set 14, to evaluate the results. We observe MELM improves reconstruction quality in about 0.44 dB PSNR in average for Set 5, when compared with a global ELM regressor (GELM), trained for the entire feature space. The proposed method almost reaches deep learning reconstruction quality, without depending on large datasets and long training times, giving a competitive trade off between performance and computational costs.	autostereogram;cluster analysis;computation;deep learning;dictionary;experiment;feature vector;feedforward neural network;global optimization;image resolution;k-means clustering;mathematical optimization;peak signal-to-noise ratio;pixel;reconstruction conjecture;structural similarity;super-resolution imaging;teaching method	Daniel Luis Cosmo;Fernando Kentaro Inaba;Evandro O. T. Salles	2017	2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)	10.1109/SIBGRAPI.2017.59	pixel;linear subspace;iterative reconstruction;feature extraction;extreme learning machine;deep learning;machine learning;feature vector;pattern recognition;mathematics;image resolution;artificial intelligence	Vision	24.301965464724084	-51.11226857471516	197535
32204f069dbca87f08fb7bbcaa74879d1e98e506	sparse boltzmann machines with structure learning as applied to text analysis		We are interested in exploring the possibility and benefits of structure learning for deep models. As the first step, this paper investigates the matter for Restricted Boltzmann Machines (RBMs). We conduct the study with Replicated Softmax, a variant of RBMs for unsupervised text analysis. We present a method for learning what we call Sparse Boltzmann Machines, where each hidden unit is connected to a subset of the visible units instead of all of them. Empirical results show that the method yields models with significantly improved model fit and interpretability as compared with RBMs where each hidden unit is connected to all visible units.	multilayer perceptron;overfitting;restricted boltzmann machine;softmax function;sparse	Zhourong Chen;Nevin Lianwen Zhang;Dit-Yan Yeung;Peixian Chen	2017			computer science;artificial intelligence;machine learning;pattern recognition;statistics	AI	22.13367605491111	-45.179841946252914	197817
edf60d081ffdfa80243217a50a411ab5407c961d	recognizing an action using its name: a knowledge-based approach	adaptive multi model rank preserving mapping amrm;semantic correlation;action recognition	Existing action recognition algorithms require a set of positive exemplars to train a classifier for each action. However, the amount of action classes is very large and the users’ queries vary dramatically. It is impractical to pre-define all possible action classes beforehand. To address this issue, we propose to perform action recognition with no positive exemplars, which is often known as the zero-shot learning. Current zero-shot learning paradigms usually train a series of attribute classifiers and then recognize the target actions based on the attribute representation. To ensure the maximum coverage of ad-hoc action classes, the attribute-based approaches require large numbers of reliable and accurate attribute classifiers, which are often unavailable in the real world. In this paper, we propose an approach that merely takes an action name as the input to recognize the action of interest without any pre-trained attribute classifiers and positive exemplars. Given an action name, we first build an analogy pool according to an external ontology, and each action in the analogy pool is related to the target action at different levels. The correlation information inferred from the external ontology may be noisy. We then propose an algorithm, namely adaptive multi-model rank-preserving mapping (AMRM), to train a classifier for action recognition, which is able to evaluate the relatedness of each video in the analogy pool adaptively. As multiple mapping models are employed, our algorithm has better capability to bridge the gap between visual features and the semantic information inferred from the ontology. Extensive experiments demonstrate that our method achieves the promising performance for action recognition only using action names, while no attributes and positive exemplars are available.	action potential;algorithm;attribute grammar;experiment;hoc (programming language)	Chuang Gan;Yi Yang;Linchao Zhu;Deli Zhao;Yueting Zhuang	2016	International Journal of Computer Vision	10.1007/s11263-016-0893-6	computer vision;computer science;artificial intelligence;machine learning;pattern recognition	Vision	24.356471214787685	-48.27615814225699	198031
4239ce7a06b12285c9abbe903e9dc8a14f3aef9f	learning flexible representations of stochastic processes on graphs		Graph convolutional networks adapt the architecture of convolutional neural networks to learn rich representations of data supported on arbitrary graphs by replacing the convolution operations of convolutional neural networks with graph-dependent linear operations. However, these graph-dependent linear operations are developed for scalar functions supported on undirected graphs. We propose both a generalization of the underlying graph and a class of linear operations for stochastic (time-varying) processes on directed (or undirected) graphs to be used in graph convolutional networks. By parameterizing the proposed linear operations using functional calculus, we can achieve arbitrarily low learning complexity. The combined graphical model and filtering approach is shown to model richer behaviors and display greater flexibility in learning representations than product graph methods.	apollonian network;artificial neural network;convolution;convolutional neural network;directed graph;graph (discrete mathematics);graphical model;stochastic process	Addison W. Bohannon;Brian M. Sadler;Radu V. Balan	2018	2018 IEEE Data Science Workshop (DSW)	10.1109/DSW.2018.8439911	functional calculus;convolutional neural network;architecture;machine learning;harmonic analysis;nonlinear dimensionality reduction;convolution;artificial intelligence;mathematics;feature learning;graphical model	ML	22.720327664354134	-48.33706074980696	198806
3f55dd6f9c02ff01811259836d3ce494d9570ff3	functional gradient boosting based on residual network perception		Residual Networks (ResNets) have become stateof-the-art models in deep learning and several theoretical studies have been devoted to understanding why ResNet works so well. One attractive viewpoint on ResNet is that it is optimizing the risk in a functional space by combining an ensemble of effective features. In this paper, we adopt this viewpoint to construct a new gradient boosting method, which is known to be very powerful in data analysis. To do so, we formalize the gradient boosting perspective of ResNet mathematically using the notion of functional gradients and propose a new method called ResFGB for classification tasks by leveraging ResNet perception. Two types of generalization guarantees are provided from the optimization perspective: one is the margin bound and the other is the expected risk bound by the sample-splitting technique. Experimental results show superior performance of the proposed method over state-of-the-art methods such as LightGBM.	artificial neural network;benchmark (computing);convolutional neural network;deep learning;flow network;gradient boosting;mathematical optimization	Atsushi Nitanda;Taiji Suzuki	2018			machine learning;residual;gradient boosting;residual neural network;deep learning;perception;computer science;artificial intelligence	AI	22.059551773184495	-47.60686671787537	198988
625afaba9ec6bc5125c217be5cc176ce283e3794	curriculumnet: weakly supervised learning from large-scale web images		We present a simple yet efficient approach capable of training deep neural networks on large-scale weakly-supervised web images, which are crawled raw from the Internet by using text queries, without any human annotation. We develop a principled learning strategy by leveraging curriculum learning, with the goal of handling a massive amount of noisy labels and data imbalance effectively. We design a new learning curriculum by measuring the complexity of data using its distribution density in a feature space, and rank the complexity in an unsupervised manner. This allows for an efficient implementation of curriculum learning on large-scale web images, resulting in a highperformance CNN model, where the negative impact of noisy labels is reduced substantially. Importantly, we show by experiments that those images with highly noisy labels can surprisingly improve the generalization capability of the model, by serving as a manner of regularization. Our approaches obtain state-of-the-art performance on four benchmarks: WebVision, ImageNet, Clothing-1M and Food-101. With an ensemble of multiple models, we achieved a top-5 error rate of 5.2% on the WebVision challenge [18] for 1000-category classification. This result was the top performance by a wide margin, outperforming second place by a nearly 50% relative error rate. Code and models are available at: https://github.com/MalongTech/CurriculumNet.	approximation error;artificial neural network;benchmark (computing);deep learning;experiment;feature vector;imagenet;signal-to-noise ratio;supervised learning;unsupervised learning	Sheng Guo;Weilin Huang;Haozhi Zhang;Chenfan Zhuang;Dengke Dong;Matthew R. Scott;Dinglong Huang	2018		10.1007/978-3-030-01249-6_9	supervised learning;pattern recognition;machine learning;the internet;word error rate;artificial neural network;feature vector;computer science;artificial intelligence;regularization (mathematics);approximation error;annotation	ML	21.801033162710308	-50.258043355679746	199022
ee546e6120597068cd422f8d95dfbf6b5d2a5980	a multiclass svm classification approach for intrusion detection		As the number of threats to the computer network and network-based applications is increasing, there is a need for a robust intrusion detection system that can ensure security against threats. To detect and defend against a specific attack, the pattern of the attack should be known a priori. Classification of attacks is a useful way to identify the unique patterns of different type of attack. As a result, KDDCup99, NSLKDD and GureKDD datasets are used in this experiment to improve the learning process and study different attack patterns thoroughly. This paper proposed a multi-class Support Vector Machine classifier(MSVM), using one versus all method, to identify one attack uniquely, which in turn helps to defend against the known as well as unknown attacks. Experimentally, the proposed scheme provides better detection accuracy, fewer false positives, and lesser training and generalization error in comparison to the existing approach.	intrusion detection system;multiclass classification	Santosh Kumar Sahu;Sanjay Kumar Jena	2016		10.1007/978-3-319-28034-9_23	multiclass classification;structured support vector machine	ML	18.164127306066106	-50.829791646967315	199064
2a4744bf5acfd888f0dcda3057367b0e0eccaa29	curriculum learning of multiple tasks	graph structure curriculum learning multitask learning generalization bound criterion object categorization optimization problem;tin;optimisation graph theory learning artificial intelligence object detection	Sharing information between multiple tasks enables algorithms to achieve good generalization performance even from small amounts of training data. However, in a realistic scenario of multi-task learning not all tasks are equally related to each other, hence it could be advantageous to transfer information only between the most related tasks. In this work we propose an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly. Subsequently, we address the question of curriculum learning of tasks, i.e. finding the best order of tasks to be learned. Our approach is based on a generalization bound criterion for choosing the task order that optimizes the average expected classification performance over all tasks. Our experimental results show that learning multiple related tasks sequentially can be more effective than learning them jointly, the order in which tasks are being solved affects the overall performance, and that our model is able to automatically discover a favourable order of tasks.	algorithm;computer multitasking;memory bound function;multi-task learning;tree (data structure)	Anastasia Pentina;Viktoriia Sharmanska;Christoph H. Lampert	2015	2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	10.1109/CVPR.2015.7299188	process;multi-task learning;tin;computer science;artificial intelligence;theoretical computer science;machine learning	Vision	24.252333705814273	-45.51588667482542	199186
