id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
e9e0f15e4510a87a4fc74d981a305491fd75faaf	the neurophysiological bases of cognitive computation using rough set theory	complex objects;imprecise computation;visual areas;bottom up;rough set theory;object categorization;shape recognition;top down processing;top down processes;single cell;universal turing machine;receptive field;brain function;neuronal activity;physical properties;symbolic representation	A popular view is that the brain works in a similar way to a digital computer or a Universal Turing Machine by processing symbols. Psychophysical experiments and our amazing capability to recognize complex objects (like faces) in different light and context conditions argue against symbolic representation and suggest that concept representation related to similarities may be a more appropriate model of brain function. In present work, by looking into anatomical and neurophysiological basis of how we classify objects shapes, we propose to describe computational properties of the brain by rough set theory (Pawlak, 1992 [1]). Concepts representing objects physical properties in variable environment are weak (not precise), but psychophysical space shows precise object categorizations. We estimate brain expertise in classifications of the object’s components by analyzing single cell responses in the area responsible for simple shape recognition ([2]). Our model is based on the receptive field properties of neurons in different visual areas: thalamus, V1 and V4 and on feedforward (FF) and feedback (FB) interactions between them. The FF pathways combine properties extracted in each area into a vast number of hypothetical objects by using “driver logical rules”, in contrast to “modulator logical rules” of the FB pathways. The FB pathways function may help to change weak concepts of objects physical properties into their crisp classification in psychophysical space.	categorization;cognitive science;computation;computer;experiment;feedback;feedforward neural network;interaction;modulation;rough set;set theory;universal turing machine	Andrzej W. Przybyszewski	2008	Trans. Rough Sets	10.1007/978-3-540-89876-4_16	computer science;artificial intelligence;machine learning;top-down and bottom-up design;algorithm	ML	21.426002247448938	-66.70034081233848	177713
38beebd77612044b21d6d4cd406851a2813dfed0	recognition of object position and shape by the spread pattern of spatial spreading associative neural network		The spatial spreading associative neural network, which imitates the spatial recognition system in the brain, not only recognizes the position of an object irrespective of its shape, but also recognizes its shape irrespective of its position when a single object is in the input pattern. When several objects are presented in the input pattern, the spatial spreading associative neural network recognizes the shapes of several objects simultaneously, if these objects do not overlap each other. The population output envelope of positional memory (recognition) neurons tends to be discrete to each direction of several objects when these objects are separated. In practical terms, the spatial spreading associative neural network can recognize the positions and shapes of several objects discretely by selective attention to each respective object. © 1999 Scripta Technica, Syst Comp Jpn, 30(14): 7487, 1999	artificial neural network;digi-comp i;focal (programming language);parallel computing;pattern formation	Kiyomi Nakamura;Noriki Kinoshita	1999	Systems and Computers in Japan	10.1002/(SICI)1520-684X(199912)30:14%3C74::AID-SCJ8%3E3.0.CO;2-O	computer simulation;computer vision;network architecture;attention;vector calculus;image processing;computer science;artificial intelligence;posterior parietal cortex	ML	20.558917065730213	-67.76202452002549	177832
b1a4b83f9ff2a3580e86cf6c00bb68d8c8da8069	the time dimension for scene analysis	temporal correlation;desincronizacion;object recognition;representation theory;analisis escena;analyse scene;enlace;desynchronization time dimension scene analysis neural computation binding problem sensory element perceptron theory figure ground separation temporal correlation oscillatory correlation oscillatory dynamics;versatility binding desynchronization figure ground separation oscillatory correlation scene analysis segmentation synchronization temporal correlation time dimension;desynchronization;correlation theory;perceptrons;versatility;analyse temporelle;theorie representation;array signal processing;segmentation;correlation methods;desynchronisation;analisis temporal;figure ground separation;time analysis;synchronisation;liaison;computational complexity;synchronization;feature extraction;algorithms artificial intelligence image interpretation computer assisted pattern recognition automated time factors video recording visual perception;binding problem;visual perception;sincronizacion;separation figure fond;sensor fusion;time dimension;reseau neuronal;image analysis layout biological neural networks books detectors neuroscience impedance multi layer neural network multilayer perceptrons pattern recognition;binding;red neuronal;teoria representacion;oscillatory correlation;correlation methods sensor fusion feature extraction object recognition perceptrons correlation theory array signal processing visual perception computational complexity;neural network;scene analysis	A fundamental issue in neural computation is the binding problem, which refers to how sensory elements in a scene organize into perceived objects, or percepts. The issue of binding is hotly debated in recent years in neuroscience and related communities. Much of the debate, however, gives little attention to computational considerations. This review intends to elucidate the computational issues that bear directly on the binding issue. The review starts with two problems considered by Rosenblatt to be the most challenging to the development of perceptron theory more than 40 years ago, and argues that the main challenge is the figure-ground separation problem, which is intrinsically related to the binding problem. The theme of the review is that the time dimension is essential for systematically attacking Rosenblatt's challenge. The temporal correlation theory as well as its special form-oscillatory correlation theory-is discussed as an adequate representation theory to address the binding problem. Recent advances in understanding oscillatory dynamics are reviewed, and these advances have overcome key computational obstacles for the development of the oscillatory correlation theory. We survey a variety of studies that address the scene analysis problem. The results of these studies have substantially advanced the capability of neural networks for figure-ground separation. A number of issues regarding oscillatory correlation are considered and clarified. Finally, the time dimension is argued to be necessary for versatile computing.	appendix;artificial intelligence;artificial neural network;binary image;binding problem;biological neural networks;clarify;community;computation (action);face;frank rosenblatt;functional requirement;multilayer perceptron;natural language processing;neuroscience discipline;perceptron;physical object;pixel;reasoning;recurrence relation;regular expression;social inequality;spatial organization;time complexity;universal turing machine;universality probability;xfig	DeLiang Wang	2005	IEEE Transactions on Neural Networks	10.1109/TNN.2005.852235	synchronization;computer science;artificial intelligence;machine learning;mathematics;artificial neural network	ML	21.549092493928086	-67.28218586704968	178271
424dd037b8d8ac1be0863052fe310e7e3266256c	a model of multisensory integration and its influence on hippocampal spatial cell responses		Head direction (HD) cells, grid cells, and place cells, often dubbed spatial cells, are neural correlates of spatial navigation. We propose a computational model to study the influence of multisensory modalities, especially vision, and proprioception on responses of these cells. A virtual animal was made to navigate within a square box along a synthetic trajectory. Visual information was obtained via a cue card placed at a specific location in the environment, while proprioceptive information was derived from curvature-modulated limb oscillations associated with the gait of the virtual animal. A self-organizing layer was used to encode HD information from both sensory streams. The sensory integration (SI) of HD from both modalities was performed using a continuous attractor network with local connectivity, followed by oscillatory path integration and lateral anti-Hebbian network, where spatial cell responses were observed. The model captured experimental findings which investigated the role of visual manipulation (cue card removal and cue card rotation) on these spatial cells. The model showed a more stable formation of spatial representations via the visual pathway compared to the proprioceptive pathway, emphasizing the role of visual input as an anchor for HD, grid, and place responses. The model suggests the need for SI at the HD level for formation of such stable representations of space essential for effective navigation.	computational model;consciousness;cue card;encode;evolutionary computation;gene regulatory network;hebbian theory;lateral thinking;modulation;organizing (structure);self-organization;spatial navigation;synthetic intelligence	Karthik Soman;Vignesh Muralidharan;V. Srinivasa Chakravarthy	2018	IEEE Transactions on Cognitive and Developmental Systems	10.1109/TCDS.2017.2752369	machine learning;artificial intelligence;computer vision;computer science;multisensory integration;neural correlates of consciousness;spatial memory;streams;hippocampal formation;sensory system;path integration;attractor network	Visualization	20.037978309686945	-67.87965183674778	178486
3c5f75df8bf6aade8f533a66909fb717cb9d5fd4	a gradient rule for the plasticity of a neuron's intrinsic excitability	intrinsic plasticity;excitabilidad;learning process;plasticite intrinseque;excitability;apprentissage synaptique;neurone biologique;neurone cortical;excitabilite;theorie information;reseau neuronal;regle gradient;cortical neurons;red neuronal;information theory;neural network;teoria informacion	While synaptic learning mechanisms have always been a core topic of neural computation research, there has been relatively little work on intrinsic learning processes, which change a neuron’s excitability. Here, we study a single, continuous activation model neuron and derive a gradient rule for the intrinsic plasticity based on information theory that allows the neuron to bring its firing rate distribution into an approximately exponential regime, as observed in visual cortical neurons. In simulations, we show that the rule works efficiently.	artificial neural network;computation;gradient;information theory;neuron;simulation;synaptic package manager;time complexity	Jochen Triesch	2005		10.1007/11550822_11	information theory;anti-hebbian learning;computer science;artificial intelligence;machine learning;artificial neural network;statistics	ML	20.882312400019746	-70.6236143434014	178493
a3cf565318bec2c7b8918d4f08e50aceff9d8b43	an analysis of connectivity and function in hippocampal associative memory	performance measure;information loss;hippocampus;attractor neural networks;storage capacity;associative memory;attractor neural network;neuronal activity;neural network	We discuss eeects of realistically dilute connectivity on a neural network previously proposed as a model of hippocampal associative memory. Several criteria for setting neuronal activation thresholds are studied. Even with optimal thresholding, dilution induces a substantial information loss in stored patterns compared to presented patterns. Consequently, a stricter constraint than previous ones arises on the model's storage capacity. Furthermore, we argue that such constraints depend sensitively on the speciic, subjective criteria chosen for storage quality. We thus propose that additional performance measures be considered. In particular, the relationship between ring rates of original and attractor patterns is discussed.	artificial neural network;content-addressable memory;thresholding (image processing)	Miguel Maravall	1999	Neurocomputing	10.1016/S0925-2312(99)00048-X	computer science;artificial intelligence;machine learning;hippocampus;bidirectional associative memory;artificial neural network;premovement neuronal activity	ML	21.70282514683911	-70.28721763486475	178923
259c19a236cecdcf2a73e3feeaa4ad1ceed3b13a	integrated mechanisms of anticipation and rate-of-change computations in cortical circuits	models neurological;rate of change;neurons afferent;neuronal plasticity;attention;motion perception;evoked potentials visual;nerve net;humans;action potentials;systems integration;visual cortex	Local neocortical circuits are characterized by stereotypical physiological and structural features that subserve generic computational operations. These basic computations of the cortical microcircuit emerge through the interplay of neuronal connectivity, cellular intrinsic properties, and synaptic plasticity dynamics. How these interacting mechanisms generate specific computational operations in the cortical circuit remains largely unknown. Here, we identify the neurophysiological basis of both the rate of change and anticipation computations on synaptic inputs in a cortical circuit. Through biophysically realistic computer simulations and neuronal recordings, we show that the rate-of-change computation is operated robustly in cortical networks through the combination of two ubiquitous brain mechanisms: short-term synaptic depression and spike-frequency adaptation. We then show how this rate-of-change circuit can be embedded in a convergently connected network to anticipate temporally incoming synaptic inputs, in quantitative agreement with experimental findings on anticipatory responses to moving stimuli in the primary visual cortex. Given the robustness of the mechanism and the widespread nature of the physiological machinery involved, we suggest that rate-of-change computation and temporal anticipation are principal, hard-wired functions of neural information processing in the cortical microcircuit.	acclimatization;action potential;cerebral cortex;computation (action);computer simulation;depressive disorder;embedded system;embedding;generic drugs;hl7publishingsubsection <operations>;information processing;integrated circuit;interaction;microcircuit device component;neuronal plasticity;synaptic package manager	Gabriel D. Puccini;Maria V. Sanchez-Vives;Albert Compte	2007	PLoS Computational Biology	10.1371/journal.pcbi.0030082	neuroplasticity;neuroscience;attention;motion perception;action potential;system integration	ML	18.892663018809447	-70.53448921205832	179032
4c9ff6de2822ca1eaaef4ff3199460d6ed3572e3	evolving spiking neural networks for audiovisual information processing	reconnaissance visage;simulation ordinateur;modelizacion;metodo adaptativo;speaker authentication;brain;online classification;procesamiento informacion;image processing;systeme nerveux central;facies;auditory pathway;reconocimiento de cara;authentication;procesamiento imagen;hombre;methode adaptative;data fusion;encefalo;classification;traitement image;authentification;speaker recognition;modelisation;sistema nervioso central;spiking neural network;cerebro;autenticacion;face recognition;encephale;computer experiment;audio and visual pattern recognition;cerveau;fusion donnee;adaptive method;information processing;human;reconnaissance locuteur;sensory integration;pattern recognition;reconocimiento de locutor;encephalon;simulacion computadora;reconnaissance forme;information system;reseau neuronal;reconocimiento patron;traitement information;fusion datos;modeling;computer simulation;clasificacion;red neuronal;central nervous system;homme;neural network	This paper presents a new modular and integrative sensory information system inspired by the way the brain performs information processing, in particular, pattern recognition. Spiking neural networks are used to model human-like visual and auditory pathways. This bimodal system is trained to perform the specific task of person authentication. The two unimodal systems are individually tuned and trained to recognize faces and speech signals from spoken utterances, respectively. New learning procedures are designed to operate in an online evolvable and adaptive way. Several ways of modelling sensory integration using spiking neural network architectures are suggested and evaluated in computer experiments.	architecture as topic;artificial neural network;authentication;computer experiment;face;hearing problem;information processing;information system;inspiration function;neural network simulation;pattern recognition;spiking neural network	Simei Gomes Wysoski;Lubica Benusková;Nikola K. Kasabov	2010	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2010.04.009	computer simulation;speaker recognition;speech recognition;information processing;image processing;computer science;artificial intelligence;authentication;artificial neural network;spiking neural network	ML	23.858484686146642	-68.01110657573109	179265
08de006a06e2734338fec75cfd7823aa6516e3db	a biophysical neural accumulator model of decision making in an antisaccade task	young men;saccadic reaction time;computer model;antisaccade;error probability;superior colliculus;computational model;cortical neurons;climbing activity	A biophysical cortico-colicular model of saccade initiation based on competitive integration of planned and reactive cortical saccade decision signals in the intermediate layer of the superior colliculus is introduced. The variable slopes of the climbing activities of the input cortical decision signals are produced from variability in the ionic and synaptic conductances of cortical neurons. The model reproduces the unimodal distributions of saccade reaction times for correct antisaccades and erroneous prosaccades as well as the variability of saccade reaction times and the overall error probabilities in a large sample of 2006 young men performing an antisaccade task. r 2006 Elsevier B.V. All rights reserved.	accumulator (computing);ionic;spatial variability;synaptic package manager	Vassilis Cutsuridis;Ioannis Kahramanoglou;Nikolaos Smyrnis;Ioannis Evdokimidis;Stavros J. Perantonis	2007	Neurocomputing	10.1016/j.neucom.2006.06.002	computer simulation;simulation;computer science;probability of error;computational model;statistics	AI	19.29027239106913	-72.37502134458133	179318
f1b4010ce67d19547afd470fb1ee5e5cbf47758c	towards representation of a perceptual color manifold using associative memory for color constancy	modelizacion;learning algorithm;memoire associative;high dimensionality;analisis estadistico;learning;nonlinear line of attraction;color constancy;color;perceptual color constancy;localization;virgule fixe;hopfield neural nets;punto fijo;intelligence artificielle;localizacion;multidimensional analysis;algorithme apprentissage;probabilistic approach;proceso adquisicion;acquisition process;coma fija;color perception;fixed point;pattern association;aprendizaje;modelisation;observacion;apprentissage;localisation;analyse n dimensionnelle;statistical analysis;state space method;percepcion visual;methode espace etat;reseau neuronal hopfield;point fixe;eclairage;enfoque probabilista;approche probabiliste;state space;analyse statistique;observation;analisis n dimensional;perception visuelle;associative memory;memoria asociativa;color correction;artificial intelligence;couleur;attraction;visual perception;reseau neuronal recurrent;lighting;inteligencia artificial;recurrent neural nets;recurrent neural network;atraccion;algoritmo aprendizaje;modeling;vision;fix point;processus acquisition;metodo espacio estado;alumbrado	In this paper, we propose the concept of a manifold of color perception through empirical observation that the center-surround properties of images in a perceptually similar environment define a manifold in the high dimensional space. Such a manifold representation can be learned using a novel recurrent neural network based learning algorithm. Unlike the conventional recurrent neural network model in which the memory is stored in an attractive fixed point at discrete locations in the state space, the dynamics of the proposed learning algorithm represent memory as a nonlinear line of attraction. The region of convergence around the nonlinear line is defined by the statistical characteristics of the training data. This learned manifold can then be used as a basis for color correction of the images having different color perception to the learned color perception. Experimental results show that the proposed recurrent neural network learning algorithm is capable of color balance the lighting variations in images captured in different environments successfully.		Ming-Jung Seow;Vijayan K. Asari	2009	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2008.09.010	multidimensional analysis;vision;computer vision;systems modeling;internationalization and localization;visual perception;computer science;state space;artificial intelligence;recurrent neural network;machine learning;lighting;mathematics;fixed point;color vision;observation;color constancy;statistics;manifold alignment	AI	23.575014360018013	-68.35667441577861	180286
7bb11aa8a0f6656316e12334f59a5c85cd9fd58d	bayesian inference in spiking neurons	bayesian inference;spiking neural network;spiking neurons;belief propagation	We propose a new interpretation of spiking neurons as Bayesian integrators accumulating evidence over time about events in the external world or the body, and communicating to other neurons their certainties about these events. In this model, spikes signal the occurrence of new information, i.e. what cannot be predicted from the past activity. As a result, firing statistics are close to Poisson, albeit providing a deterministic representation of probabilities. We proceed to develop a theory of Bayesian inference in spiking neural networks, recurrent interactions implementing a variant of belief propagation. Many perceptual and motor tasks performed by the central nervous system are probabilistic, and can be described in a Bayesian framework [4, 3]. A few important but hidden properties, such as direction of motion, or appropriate motor commands, are inferred from many noisy, local and ambiguous sensory cues. These evidences are combined with priors about the sensory world and body. Importantly, because most of these inferences should lead to quick and irreversible decisions in a perpetually changing world, noisy cues have to be integrated on-line, but in a way that takes into account unpredictable events, such as a sudden change in motion direction or the appearance of a new stimulus. This raises the question of how this temporal integration can be performed at the neural level. It has been proposed that single neurons in sensory cortices represent and compute the log probability that a sensory variable takes on a certain value (eg Is visual motion in the neuron’s preferred direction?) [9, 7]. Alternatively, to avoid normalization issues and provide an appropriate signal for decision making, neurons could represent the log probability ratio of a particular hypothesis (eg is motion more likely to be towards the right than towards the left) [7, 6]. Log probabilities are convenient here, since under some assumptions, independent noisy cues simply combine linearly. Moreover, there are physiological evidence for the neural representation of log probabilities and log probability ratios [9, 6, 7]. However, these models assume that neurons represent probabilities in their firing rates. We argue that it is important to study how probabilistic information are encoded in spikes. Indeed, it seems spurious to marry the idea of an exquisite on-line integration of noisy cues with an underlying rate code that requires averaging on large populations of noisy neurons and long periods of time. In particular, most natural tasks require this integration to take place on the time scale of inter-spike intervals. Spikes are more efficiently signaling events ∗Instituteof Cognitive Science, 69645 Bron, France thananalog quantities. In addition, a neural theory of inference with spikes will bring us closer to the physiological level and generate more easily testable predictions. Thus, we propose a new theory of neural processing in which spike trains provide a deterministic, online representation of a log-probability ratio. Spikes signals events, eg that the log-probability ratio has exceeded what could be predicted from previous spikes. This form of coding was loosely inspired by the idea of ”energy landscape” coding proposed by Hinton and Brown [2]. However, contrary to [2] and other theories using rate-based representation of probabilities, this model is self-consistent and does not require different models for encoding and decoding: As output spikes provide new, unpredictable, temporally independent evidence, they can be used directly as an input to other Bayesian neurons. Finally, we show that these neurons can be used as building blocks in a theory of approximate Bayesian inference in recurrent spiking networks. Connections between neurons implement an underlying Bayesian network, consisting of coupled hidden Markov models. Propagation of spikes is a form of belief propagation in this underlying graphical model. Our theory provides computational explanations of some general physiological properties of cortical neurons, such as spike frequency adaptation, Poisson statistics of spike trains, the existence of strong local inhibition in cortical columns, and the maintenance of a tight balance between excitation and inhibition. Finally, we discuss the implications of this model for the debate about temporal versus rate-based neural coding. 1 Spikes and log posterior odds 1.1 Synaptic integration seen as inference in a hidden Markov chain We propose that each neuron codes for an underlying ”hidden” binary variable, xt, whose state evolves over time. We assume that xt depends only on the state at the previous time step,xt−dt, and is conditionally independent of other past states. The state xt can switch from 0 to 1 with a constant rate ron = 1 dt limdt→0 P (xt = 1|xt−dt = 0), and from 1 to 0 with a constant rateroff . For example, these transition rates could represent how often motion in a preferred direction appears the receptive field and how long it is likely to stay there. The neuron infers the state of its hidden variable from N noisy synaptic inputs, considered to be observationsof the hidden state. In this initial version of the model, we assume that these inputs are conditionally independent homogeneous Poisson processes, synapse i emitting a spike between time t andt + dt (st = 1) with constant probabilityq i ondt if xt = 1, and another constant probability q offdt if xt = 0. The synaptic spikes are assumed to be otherwise independent of previous synaptic spikes, previous states and spikes at other synapses. The resulting generative model is a hidden Markov chain (figure 1-A). However, rather thanestimatingthe state of its hidden variable and communicating this estimate to other neurons (for example by emitting a spike when sensory evidence for xt = 1 goes above a threshold) the neuron reports and communicates its c rtaintythat the current state is1. This certainty takes the form of the log of the ratio of the probability that the hidden state is 1, and the probability that the state is 0, given all the synaptic inputs received so far:Lt = log (P (xt=1|s0→t) P (xt=0|s0→t) ) . We uses0→t as a short hand notation for the N synaptic inputs received at present a d in the past. We will refer to it as the log odds ratio. Thanks to the conditional independencies assumed in the generative model, we can compute this Log odds ratio iteratively. Taking the limit as dt goes to zero, we get the following differential equation: L̇ = ron ( 1 + e−L )− roff ( 1 + e ) + ∑ i wiδ(s i t − 1)− θ dt t s t s dt t s off on q q , t x dt t x dt t x off on r r . off on r r . off on q q , off on q q , A.	action potential;approximation algorithm;bayesian network;belief propagation;binary data;c date and time functions;code;cognitive science;column (database);generative model;graphical model;hidden markov model;hidden variable theory;interaction;log probability;markov chain;neural coding;neuron;online and offline;population;rise of nations;software propagation;spiking neural network;synapse;synaptic package manager	Sophie Denève	2004			random neural network;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;bayesian inference;statistics;belief propagation;spiking neural network	ML	20.017190519348187	-72.47726867563968	180529
f23b477e30a18140fcaf601cfcdff2f0c7efff7a	stimulus-driven unsupervised synaptic pruning in large neural networks	background noise;topology;feed forward;ruido aleatorio;time dependent;feedforward;sistema temporizado;bruit aleatoire;base donnee temporelle;integrate and fire;topologie;timed system;intelligence artificielle;probabilistic approach;random networks;boucle anticipation;spatial database;topologia;recurrence;random noise;enrejado;ciclo anticipacion;sinapsis;stimulation;treillis;synaptic plasticity;enfoque probabilista;approche probabiliste;recurrencia;base donnee spatiale;systeme temporise;artificial intelligence;ruido fondo;spatial data structures;estimulacion;plasticite synaptique;temporal databases;base dato especial;inteligencia artificial;reseau neuronal;plasticidad sinaptica;bruit fond;red neuronal;neural network;lattice;structure donnee spatiale;synapse	We studied the emergence of cell assemblies out of locally connected random networks of integrate-and-fire units distributed on a 2D lattice stimulated with a spatiotemporal pattern in presence of independent random background noise. Networks were composed of 80% excitatory and 20% inhibitory units with initially balanced synaptic weights. Excitatory–excitatory synapses were modified according to a spike-timing-dependent synaptic plasticity (stdp) rule associated with synaptic pruning. We show that the application, in presence of background noise, of a recurrent pattern of stimulation let appear cell assemblies characterized by an internal pattern of converging projections and a feed-forward topology not observed with an equivalent random stimulation.	neural networks;synaptic package manager	Javier Iglesias;Jan Eriksson;Beatriz Pardo;Marco Tomassini;Alessandro E. P. Villa	2005		10.1007/11565123_6	telecommunications;computer science;artificial intelligence;feed forward;artificial neural network	ML	20.278101423166103	-70.47196159118744	180715
038bdd0a24212c53c6c764560f218d7f6b99590d	physiological gain leads to high isi variability in a simple model of a cortical regular spiking cell	simulation ordinateur;traitement signal;eficacia sistema;tecnologia electronica telecomunicaciones;computacion informatica;etude experimentale;information transmission;integrate and fire;performance systeme;grupo de excelencia;fire frequency;system performance;regular spiking;ciencias basicas y experimentales;random walk;signal processing;simulacion computadora;integrate and fire neuron;transmision informacion;reseau neuronal;tecnologias;transmission information;grupo a;experimental measurement;procesamiento senal;computer simulation;cortical neurons;estudio experimental;red neuronal;neural network;steady state	To understand the interspike interval (ISI) variability displayed by visual cortical neurons (Softky & Koch, 1993), it is critical to examine the dynamics of their neuronal integration, as well as the variability in their synaptic input current. Most previous models have focused on the latter factor. We match a simple integrate-and-fire model to the experimentally measured integrative properties of cortical regular spiking cells (McCormick, Connors, Lighthall, & Prince, 1985). After setting RC parameters, the postspike voltage reset is set to match experimental measurements of neuronal gain (obtained from in vitro plots of firing frequency versus injected current). Examination of the resulting model leads to an intuitive picture of neuronal integration that unifies the seemingly contradictory and random walk pictures that have previously been proposed. When ISIs are dominated by postspike recovery, arguments hold and spiking is regular; after the memory of the last spike becomes negligible, spike threshold crossing is caused by input variance around a steady state and spiking is Poisson. In integrate-and-fire neurons matched to cortical cell physiology, steady-state behavior is predominant, and ISIs are highly variable at all physiological firing rates and for a wide range of inhibitory and excitatory inputs.	action potential;biological neuron model;cell physiology;experiment;heart rate variability;in vitro [publication type];information sciences institute;interval arithmetic;koch snowflake;neuronal ceroid-lipofuscinoses;picture;prince;qt interval feature (observable entity);sample variance;spatial variability;steady state;synaptic package manager;physiological aspects	Todd W. Troyer;Kenneth D. Miller	1997	Neural Computation	10.1162/neco.1997.9.5.971	computer simulation;telecommunications;computer science;artificial intelligence;machine learning;signal processing;communication;steady state;random walk;artificial neural network;statistics	ML	20.740838782238658	-71.84840537828443	180737
83cde7c02bbe54319fcbfc18bee88303096ab19c	neural mechanisms of motion detection, integration, and segregation: from biology to artificial image processing systems	signal image and speech processing;image processing;quantum information technology spintronics;motion detection	Object motion can be measured locally by neurons at different stages of the visual hierarchy. Depending on the size of their receptive field apertures they measure either localized or more global configurationally spatiotemporal information. In the visual cortex information processing is based on the mutual interaction of neuronal activities at different levels of representation and scales. Here, we utilize such principles and propose a framework for modelling neural computational mechanisms of motion in primates using biologically inspired principles. In particular, we investigate motion detection and integration in cortical areas V1 and MT utilizing feedforward and modulating feedback processing and the automatic gain control through center-surround interaction and activity normalization. We demonstrate that the model framework is capable of reproducing challenging data from experimental investigations in psychophysics and physiology. Furthermore, the model is also demonstrated to successfully deal with realistic image sequences from benchmark databases and technical applications.	activity diagram;automatic gain control;benchmark (computing);cisco pix;computation;computer simulation;database;experiment;feedback;feedforward neural network;gene regulatory network;image processing;information processing;invariant (computer science);lateral thinking;modulation;motion estimation;relevance;synthetic intelligence;temporal logic;top-down and bottom-up design;visual hierarchy;word-sense disambiguation	Jan D. Bouecke;Émilien Tlapale;Pierre Kornprobst;Heiko Neumann	2011	EURASIP J. Adv. Sig. Proc.	10.1155/2011/781561	computer vision;image processing;computer science;artificial intelligence;machine learning;algorithm	ML	21.776868913287206	-68.01665514137372	182264
100db0206121f90ebe2e82ca0bc66589b0df1369	multi-layer perceptron including glial pulse and switching between learning and non-learning	brain;multilayer perceptrons;neurons standards switches noise simulation logistics calcium;multilayer perceptron mlp neuron outputs connected glias d serine human memory neighboring glia neuron membrane potential nervous cell nonlearning learning glial pulse;neurophysiology brain cellular biophysics learning artificial intelligence multilayer perceptrons;neurophysiology;learning artificial intelligence;cellular biophysics	A glia is a nervous cell which is existing in a brain. This cell changes a Ca2+ concentration. This ion affects a neuron membrane potential and it is propagated to the neighboring glia. Moreover, the Ca2; directly affects the human memory by increasing of a D-serine. From these functions, we propose a Multi-Layer Perceptron (MLP) including glial pulse and switching between a learning and non-learning. In this method, the neurons in the hidden-layer received the pulse from connected glias. The pulse is generated depending on the neuron outputs and it is propagated to the neighboring glias and neurons. Moreover, the neurons are separated to some groups. Each group periodically switches a learning term and a non-learning term. Each group starts the learning term having a small lag each other. We consider that a performance of the MLP improves by two different methods influencing each other. By two simulations, we confirm that the MLP obtains the high solving ability by using our methods.	memory-level parallelism;network switch;neuron;perceptron;quad flat no-leads package;simulation	Chihiro Ikuta;Yoko Uwate;Yoshifumi Nishio;Guoan Yang	2013	2013 IEEE International Symposium on Circuits and Systems (ISCAS2013)	10.1109/ISCAS.2013.6572290	computer science;artificial intelligence;machine learning;neurophysiology	Arch	17.774521780467015	-69.41004680843969	182267
793a69815c2c7a26e3623a951fa3c11a5cda5398	a self-organized neural comparator	proposed neural comparator adapts;input stream;self-organized neural comparator;sensory input stream;neural activity;input comparison;unsupervised neural circuitry;respective input;multilayer feedforward neural network;actual sensory input;different neural population	Learning algorithms need generally the ability to compare several streams of information. Neural learning architectures hence need a unit, a comparator, able to compare several inputs encoding either internal or external information, for instance, predictions and sensory readings. Without the possibility of comparing the values of predictions to actual sensory inputs, reward evaluation and supervised learning would not be possible. Comparators are usually not implemented explicitly. Necessary comparisons are commonly performed by directly comparing the respective activities one-to-one. This implies that the characteristics of the two input streams (like size and encoding) must be provided at the time of designing the system. It is, however, plausible that biological comparators emerge from self-organizing, genetically encoded principles, which allow the system to adapt to the changes in the input and the organism. We propose an unsupervised neural circuitry, where the function of input comparison emerges via self-organization only from the interaction of the system with the respective inputs, without external influence or supervision. The proposed neural comparator adapts in an unsupervised form according to the correlations present in the input streams. The system consists of a multilayer feedforward neural network, which follows a local output minimization (anti-Hebbian) rule for adaptation of the synaptic weights. The local output minimization allows the circuit to autonomously acquire the capability of comparing the neural activities received from different neural populations, which may differ in population size and the neural encoding used. The comparator is able to compare objects never encountered before in the sensory input streams and evaluate a measure of their similarity even when differently encoded.	acclimatization;algorithm;artificial neural network;biological neural networks;comparator device component;electronic circuit;feedforward neural network;fuzzy logic;hebbian theory;neural coding;one-to-one (data model);organizing (structure);physical object;population;reading (activity);reason applied by forcast logic to project this vaccine:finding:point in time:^patient:nominal;rule (guideline);self-organization;supervised learning;synaptic package manager;unsupervised learning;weight	Guillermo A. Ludueña;Claudius Gros	2013	Neural Computation	10.1162/NECO_a_00424	simulation;computer science;artificial intelligence;machine learning	ML	18.41542584093791	-66.69307512726013	183190
3ca1d4b069dc8637fff94de7cd27ff208017dd02	tuning bat lso neurons to interaural intensity differences through spike-timing dependent plasticity	interaural intensity difference;auditory pathway;spike time dependent plasticity;fire behavior;lateral superior olive	Bats, like other mammals, are known to use interaural intensity differences (IID) to determine azimuthal position. In the lateral superior olive (LSO) neurons have firing behaviors which vary systematically with IID. Those neurons receive excitatory inputs from the ipsilateral ear and inhibitory inputs from the contralateral one. The IID sensitivity of a LSO neuron is thought to be due to delay differences between the signals coming from both ears, differences due to different synaptic delays and to intensity-dependent delays. In this paper we model the auditory pathway until the LSO. We propose a learning scheme where inputs to LSO neurons start out numerous with different relative delays. Spike timing-dependent plasticity (STDP) is then used to prune those connections. We compare the pruned neuron responses with physiological data and analyse the relationship between IID’s of teacher stimuli and IID sensitivities of trained LSO neurons.	auditory pathway structure;behavior;chiroptera;dependent ml;eprs gene;gene regulatory network;lateral computing;lateral superior olivary nucleus;lateral thinking;local shared object;mammals;neuron;neurons;superior olivary complex;synaptic package manager	Bertrand Fontaine;Herbert Peremans	2007	Biological Cybernetics	10.1007/s00422-007-0178-9	neuroscience;speech recognition;engineering;communication	ML	18.413742536210183	-71.86542056908263	183880
e9b290f90d69bd281dcd8b872b56526ec5bf92cc	dynamical behavior of an electronic neuron of commutation	circuit commutation;electronic circuit;concepcion circuito;circuit design;integrate and fire;circuito electronico;model development;switching circuit;circuito conmutacion;conception circuit;reseau neuronal;circuit electronique;red neuronal;neural network;dynamic behavior	In this work we present the design of an electronic model of a single commutation neuron and illustrate some of its dynamic behavior. This kind of electronic neuron shows changes in the activity of its potential when it is equal to threshold level constant. In particular, the neuron model developed presents commutation processes in its dynamical behavior. That is, the model is integrative as a leaky integrator below the threshold level and shoots when it reaches it; then the neuron's potential decays similar to an integrate and fire model. The electronic neuron can commute between, at least, two different kinds of oscillatory behavior. As a consequence of this, the neural response can be formed with different combinations of spikes and pulses.	dynamical system;neuron	A. Padrón;J. L. Pérez;A. Herrera;R. Prieto	2000		10.1007/10720076_31	electronic circuit;computer science;circuit design;artificial neural network	Theory	19.39226542385218	-70.6667180112046	184035
30152d0b6beaa7074a5fb37072604dbf738dacef	neural systems as nonlinear filters	volterra series;feedforward neural network;symbolic synapses;time scale;biological system;transmission synaptique;dn;neural networks;filters;nonlinear filter;dynamic network;synaptic transmission;systeme biologique;nonlinear systems;neural system;synapse biologique;synapse symbolique;temporal pattern;systeme non lineaire;filtre;reseau neuronal;transmision sinaptica;weight change;reseau neuronal artificiel;sistema biologico;biological synapses;synapses;artificial neural network;neural network;synapse	Experimental data show that biological synapses behave quite differently from the symbolic synapses in all common artificial neural network models. Biological synapses are dynamic; their weight changes on a short timescale by several hundred percent in dependence of the past input to the synapse. In this article we address the question how this inherent synaptic dynamics (which should not be confused with long term learning) affects the computational power of a neural network. In particular, we analyze computations on temporal and spatiotemporal patterns, and we give a complete mathematical characterization of all filters that can be approximated by feedforward neural networks with dynamic synapses. It turns out that even with just a single hidden layer, such networks can approximate a very rich class of nonlinear filters: all filters that can be characterized by Volterra series. This result is robust with regard to various changes in the model for synaptic dynamics. Our characterization result provides for all nonlinear filters that are approximable by Volterra series a new complexity hierarchy related to the cost of implementing such filters in neural systems.	acclimatization;adaptive filter;approximation algorithm;artificial neural network;blum axioms;computation;computation (action);computational complexity theory;computational model;computer simulation;confusion;depressive disorder;dynamical system;feedforward neural network;lateral thinking;lotka–volterra equations;mathematics;new foundations;nonlinear system;occur (action);particle filter;polynomial;spatiotemporal pattern;synapse;synapse;synapses;synaptic package manager;time-invariant system;anatomical layer;facilitation	Wolfgang Maass;Eduardo D. Sontag	2000	Neural Computation	10.1162/089976600300015123	neuroscience;computer science;synapse;artificial intelligence;machine learning;artificial neural network	ML	20.436345335128895	-70.42760902566543	184779
acee5502007153e89d25642caf02b0e82532a4d1	spontaneous local gamma oscillation selectively enhances neural                     network responsiveness	animals;oscillations;interneurons;models neurological;feed forward;membrane potential;brain;gamma oscillation;potentials;neural networks;gain modulation;synaptic interactions;neuronal synchronization;interneuron networks;monkey striate cortex;attention;synaptic transmission;single neuron function;biological clocks;mechanism;cat visual cortex;pyramidal neurons;nerve net;humans;neurons;spatial locality;neural network model;action potentials;neuronal network;vision;firing pattern;article;computer simulation;visual cortex;hodgkin huxley;neural network	Synchronized oscillation is very commonly observed in many neuronal systems and might play an important role in the response properties of the system. We have studied how the spontaneous oscillatory activity affects the responsiveness of a neuronal network, using a neural network model of the visual cortex built from Hodgkin-Huxley type excitatory (E-) and inhibitory (I-) neurons. When the isotropic local E-I and I-E synaptic connections were sufficiently strong, the network commonly generated gamma frequency oscillatory firing patterns in response to random feed-forward (FF) input spikes. This spontaneous oscillatory network activity injects a periodic local current that could amplify a weak synaptic input and enhance the network's responsiveness. When E-E connections were added, we found that the strength of oscillation can be modulated by varying the FF input strength without any changes in single neuron properties or interneuron connectivity. The response modulation is proportional to the oscillation strength, which leads to self-regulation such that the cortical network selectively amplifies various FF inputs according to its strength, without requiring any adaptation mechanism. We show that this selective cortical amplification is controlled by E-E cell interactions. We also found that this response amplification is spatially localized, which suggests that the responsiveness modulation may also be spatially selective. This suggests a generalized mechanism by which neural oscillatory activity can enhance the selectivity of a neural network to FF inputs.	acclimatization;amplifier;artificial neural network;biological neural networks;cerebral cortex;feedforward neural network;hodgkin disease;huxley: the dystopia;inhibition;interaction;interferon type ii;interneurons;mercury:mcnt:pt:feed:qn;modulation;network model;neural oscillation;neuron;neurons;responsiveness;selectivity (electronic);self-control as a personality trait;spontaneous order;synaptic package manager	Se-Bum Paik;Tribhawan Kumar;Donald A. Glaser	2009	PLoS Computational Biology	10.1371/journal.pcbi.1000342	vision;neuroscience;attention;membrane potential;mechanism;artificial intelligence;oscillation;action potential;feed forward;artificial neural network;hodgkin–huxley model;neurotransmission	ML	18.150638255985854	-71.16565563494046	184820
cb594f6d9ef187fbb8254bb1ec74b253d901ae5f	dynamic behaviour of a model of the muscle stretch reflex	reflexe etirement;fibre nerveuse sensitive ib;fibre nerveuse sensitive ii;transmission synaptique;neurona motora;systeme nerveux central;coordinacion sensoriomotora;motricite;spinal cord;neurona renshaw;adaptive control;controle moteur;fibra nerviosa sensitiva ia;spinal cord circuitry;motricidad;dynamic behaviour;motor neuron;synaptic transmission;medula espinal;moelle epiniere;sensorimotor coordination;sistema nervioso central;sensitive nerve fiber ib;neurone intermediaire;sensitive nerve fiber ii;motricity;modele simulation;interneuron;neurone renshaw;sensitive nerve fiber ia;reflejo estiramiento;fibre nerveuse sensitive ia;movement control;modelo simulacion;reseau neuronal;coordination sensorimotrice;transmision sinaptica;stretch reflex;fibra nerviosa sensitiva ii;neurone moteur;renshaw neuron;neurona intermediaria;simulation model;red neuronal;central nervous system;neural network	A computer simulation is used to examine the ability of the muscle stretch reflex circuitry to control limb position when all signals from higher levels of the nervous system are assumed to be constant. The computer model allows the easy incision and excision of different neuronal types so that their roles in the reflex response can be examined in a way that is not possible experimentally. Motoneurone excitation from Group Ia and II afferents, and inhibition from Group Ia inhibitory interneurones, Renshaw cells, and Group Ib afferents, are considered. A circuit in which moloneurone excitation comes purely from Group Ia afferents, which respond to muscle length and rate of change of length, exhibits oscillations in limb position at a frequency comparable to the tremor known as clonus. Good, nonoscillatory dynamic performance is achieved if most of the motoneurone excitation has no velocity component. For certain strengths of the inhibitory synapses, reciprocal inhibition from Group Ia inhibitory interneurones, and inhibition of the homonymous motoneurones by Renshaw cells and Group Ib afferents, results in improvements in the dynamic performance. For large loads, performance is enhanced if the damping from the Renshaw cells and Group Ib afferents is reduced. A simple adaptive scheme that involves inhibition of the Renshaw cells and the Group Ib interneurones by the Group II afferents provides a mechanism by which the damping is automatically attenuated for large loads.		Bruce P. Graham;Stephen J. Redman	1993	Neural Networks	10.1016/S0893-6080(09)80005-1	adaptive control;computer science;central nervous system;simulation modeling;artificial neural network;neurotransmission	ML	20.24763876499301	-70.69956435822972	185008
10e6d409b71d3c3fe636874c28871a9f7fb77bf0	pattern categorization and generalization with a virtual neuromolecular architecture	generalizacion;arquitectura red;reference neurons;learning;structural response;dynamic model;search algorithm;cytoskeleton;architecture reseau;cytosquelette;signal integrity;experimental result;aprendizaje;memory access;computer architecture;apprentissage;generalisation;neuromolecular computing;citoesqueleto;automate cellulaire;selection combining;molecular processes;resultado experimental;pattern recognition;intraneuronal dynamics;network architecture;reconnaissance forme;second messengers;reseau neuronal;reconocimiento patron;resultat experimental;evolutionary learning;cellular automata;cellular automaton;generalization;red neuronal;neural network;automata celular	A multilevel neuromolecular computing architecture has been developed that provides a rich platform for evolutionary learning. The architecture comprises a network of neuron-like modules with internal dynamics modeled by cellular automata. The dynamics are motivated by the hypothesis that molecular processes operative in real neurons (in particular processes connected with second messenger signals and cytoskeleton-membrane interactions) subserve a signal integrating function. The objective is to create a repertoire of special purpose dynamic pattern processors through an evolutionary search algorithm and then to use memory manipulation algorithms to select combinations of processors from the repertoire that are capable of performing coherent pattern recognition/neurocontrol tasks. The system consists of two layers of cytoskeletally controlled (enzymatic) neurons and two layers of memory access neurons (called reference neurons) divided into a collection of functionally comparable subnets. Evolutionary learning can occur at the intraneuronal level through variations in the cytoskeletal structures responsible for the integration of signals in space and time, through variations in the location of elements that represent readin or readout proteins, and through variations in the connectivity of the neurons. The memory manipulation algorithms that orchestrate the repertoire of neuronal processors also use evolutionary search procedures. The network is capable of performing complicated pattern categorization tasks and of doing so in a manner that balances specificity and generalization. Copyright 1996 Elsevier Science Ltd.		Jong-Chen Chen;Michael Conrad	1997	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(96)00076-7	cellular automaton;generalization;computer science;artificial intelligence;machine learning;mathematics;algorithm	ML	21.376782321599244	-68.77865561885044	185285
7f11afbb23b2f52f09b7902118f87681bfe01527	a sensorimotor model for computing intended reach trajectories	control systems;musculoskeletal system;kinematics;network analysis;eyes;parietal lobe;monkeys;vision	The presumed role of the primate sensorimotor system is to transform reach targets from retinotopic to joint coordinates for producing motor output. However, the interpretation of neurophysiological data within this framework is ambiguous, and has led to the view that the underlying neural computation may lack a well-defined structure. Here, I consider a model of sensorimotor computation in which temporal as well as spatial transformations generate representations of desired limb trajectories, in visual coordinates. This computation is suggested by behavioral experiments, and its modular implementation makes predictions that are consistent with those observed in monkey posterior parietal cortex (PPC). In particular, the model provides a simple explanation for why PPC encodes reach targets in reference frames intermediate between the eye and hand, and further explains why these reference frames shift during movement. Representations in PPC are thus consistent with the orderly processing of information, provided we adopt the view that sensorimotor computation manipulates desired movement trajectories, and not desired movement endpoints.	artificial neural network;cns disorder;computation (action);experiment;frame (physical object);parietal lobe;posterior parietal cortex;primary peritoneal carcinoma;primates;cell transformation	Cevat Üstün	2016		10.1371/journal.pcbi.1004734	vision;computer vision;kinematics;network analysis;computer science;control system	ML	19.132478093759147	-68.08837024128955	185433
95f519ae17a5b4e51884f990a4ff7e392878c067	two computational regimes of a single-compartment neuron separated by a planar boundary in conductance space	sensibilite;49k40;neurone;calcul neuronal;models neurological;neural computation;brain;dynamic system;dynamical system;systeme dynamique;sensitivity;neurona;62j10;37xx;neurons;sistema dinamico;reseau neuronal;red neuronal;computacion neuronal;neuron;hodgkin huxley;sensibilidad;variance;neural network;variancia	Recent in vitro data show that neurons respond to input variance with varying sensitivities. Here we demonstrate that Hodgkin-Huxley (HH) neurons can operate in two computational regimes: one that is more sensitive to input variance (differentiating) and one that is less sensitive (integrating). A boundary plane in the 3D conductance space separates these two regimes. For a reduced HH model, this plane can be derived analytically from the V nullcline, thus suggesting a means of relating biophysical parameters to neural computation by analyzing the neuron's dynamical system.	anatomical compartments;artificial neural network;computation (action);conductance (graph);dynamical system;hodgkin–huxley model;huxley: the dystopia;in vitro [publication type];lymphoma, non-hodgkin;multi-compartment model;neuron;sample variance	Brian Nils Lundstrom;Sungho Hong;Matthew H. Higgs;Adrienne L. Fairhall	2008	Neural Computation	10.1162/neco.2007.05-07-536	neuroscience;artificial intelligence;dynamical system;mathematics;communication;artificial neural network;statistics	ML	20.300168324375523	-70.95331667496693	185767
e40128245ca22bc8a0e1b88bc1a0f83eda14b824	systems biology of synaptic plasticity: a review on n-methyl-d-aspartate receptor mediated biochemical pathways and related mathematical models	systems biology;memory formation;ltp;ltd;synaptic plasticity	Synaptic plasticity, an emergent property of synaptic networks, has shown strong correlation to one of the essential functions of the brain, memory formation. Through understanding synaptic plasticity, we hope to discover the modulators and mechanisms that trigger memory formation. In this paper, we first review the well understood modulators and mechanisms underlying N-methyl-D-aspartate receptor dependent synaptic plasticity, a major form of synaptic plasticity in hippocampus, and then comment on the key mathematical modelling approaches available in the literature to understand synaptic plasticity as the integration of the established functionalities of synaptic components.		Yang He;Don Kulasiri;Sandhya Samarasinghe	2014	Bio Systems	10.1016/j.biosystems.2014.06.005	biology;developmental plasticity;neuroscience;anti-hebbian learning;homeostatic plasticity;metaplasticity;synaptic scaling	ML	18.374396985360992	-70.76494072662086	185884
c4c9fd9bcdc85c0b5a96e9af6668e9d4058ec56f	modelling reduced excitability in aged ca1 neurons as a calcium-dependent process	pyramidal cell;sahp;l type calcium channel;potassium channel;aging;l type ca2 channels;calcium dependent modulation;l type ca 2;compartmental model;calcium channel;computer simulation	We use a multi-compartmental model of a CA1 pyramidal cell to study changes in hippocampal excitability that result from aging-induced alterations in calcium-dependent membrane mechanisms. The model incorporates Nand L-type calcium channels which are respectively coupled to fast and slow afterhyperpolarization potassium channels. Model parameters are calibrated using physiological data. Computer simulations reproduce the decreased excitability of aged CA1 cells, which results from increased internal calcium accumulation, subsequently larger postburst slow afterhyperpolarization, and enhanced spike frequency adaptation. We find that aging-induced alterations in CA1 excitability can be modelled with simple coupling mechanisms that selectively link specific types of calcium channels to specific calcium-dependent potassium channels. r 2004 Elsevier B.V. All rights reserved.	action potential;calibration (statistics);computer simulation;excited state;exclusive or;multi-compartment model;tree accumulation	Maria E. Markaki;Stelios C. Orphanoudakis;Panayiota Poirazi	2005	Neurocomputing	10.1016/j.neucom.2004.10.023	computer simulation;calcium-activated potassium channel;sk channel;potassium channel;computer science;bk channel;calcium channel;slow afterhyperpolarization;q-type calcium channel;t-type calcium channel;l-type calcium channel	AI	17.95834578931058	-72.10417173954716	186012
f007b45bf82a1df82acf40bdef506afc5f5e7bef	the layer effect on multi-layer cellular neural networks	variant templates;multi layer cellular networks;topological entropy;learning problems	The present investigation elucidates how the number of layers/variance of templates influences the phenomena of multi-layer cellular neural networks (MCNNs). This study relates to learning problems for MCNNs. We show that the greater the number of templates that MCNNs adopt, the richer the phenomena that are derived, while equivalently, such neural networks are more efficient as regards the learning aspect. Additionally, the MCNNs with more layers exhibit more phenomena than the ones with fewer layers. A novel phenomenon is seen in the study of the effect of the number of layers with respect to fixed templates.	artificial neural network;layer (electronics)	Jung-Chao Ban;Chih-Hung Chang	2013	Appl. Math. Lett.	10.1016/j.aml.2013.01.013	theoretical computer science;quantum mechanics;topological entropy in physics	ML	18.463593656581427	-69.46478921456439	186271
1352dcaa1665b12e0bb22277767b2a3e4c54b502	performance and features of multi-layer perceptron with impulse glial network	oscillations;neurons time series analysis oscillators biological neural networks noise joining processes;multilayer perceptrons;multilayer perceptrons generalisation artificial intelligence learning artificial intelligence;multi layer perceptron;generalisation artificial intelligence;generalization capability multilayer perceptron impulse glial network brain feature glias connection oscillation propagation oscillation generation learning performance impulse output network parameter dependency;learning artificial intelligence;generalization capability	We have proposed the glial network which was inspired from the feature of the brain. The glial network is composed by glias connecting each other. All glias generate oscillations and these oscillations propagate in the glial network. We confirmed that the glial network improved the learning performance of the Multi-Layer Perceptron (MLP). In this article, we investigate the MLP with the impulse glial network. The glias generate only impulse output, however they make the complex output by correlating with each other. We research the proposed networks' parameter dependency. Moreover, we show that the proposed network possess better learning performance and better generalization capability than the conventional MLPs.	computer simulation;memory-level parallelism;multilayer perceptron;quad flat no-leads package	Chihiro Ikuta;Yoko Uwate;Yoshifumi Nishio	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033549	computer science;artificial intelligence;theoretical computer science;machine learning;multilayer perceptron;oscillation	Vision	17.68630323761885	-69.07139308309087	186786
8f48f98d709062ad58d2b79a6b042c46f7c69777	top-down cortical influences in visual expectation	visual expectation;macaque monkey;top-down modulation;task-specific coherent oscillatory networks;visual cortex;visual system;top-down cortical influence;visual perception;neurons;expected stimuli;neural nets	Visual perception depends on prior experience. Previous encounters with visual objects allow an organism to form expectations about future encounters, and to use those expectations to tune the visual system to more efficiently process expected visual inputs. This paper explores the proposition that visual expectation involves top-down modulation of neurons in low-level areas of visual cortex in anticipation of expected stimuli. It reports evidence that top-down modulation occurs within task-specific coherent oscillatory networks in the visual cortex of a macaque monkey, and that this modulation is related to stimulus processing efficiency.	coherence (physics);color vision;high- and low-level;modulation;top-down and bottom-up design;visual objects	Steven L. Bressler;Craig G. Richter;Yonghong Chen;Mingzhou Ding	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246707	organism;computer vision;blindsight;visual system;visual search;visual perception;p200;computer science;resource management;time series;top-down and bottom-up design;mediation;n2pc;gaze-contingency paradigm;artificial neural network;biased competition theory	Robotics	19.777703953555125	-69.08815423762825	187689
c43f27ada39a6d305be6fe32a1c5a84fde45e147	estimating the afferent and efferent temporal interval entropy of neuronal discharge for single spike trains	communication system;temporal variability;cerebellum;model;spike train;entropy;matching estimator;purkinje cell;timing	We de-ne a biological communication system at the level of a single neuron and quantify the temporal variability of a!erent and e!erent impulse patterns by means of an interval entropy measure. Two signal transmission conditions which bound a physiologically plausible range of transmission possibilities are explored. The number of e!erent synapses is predicted by matching estimates of the mean e!erent entropy to the total a!erent entropy. c © 2002 Elsevier Science B.V. All rights reserved.		Allan D. Coop;George N. Reeke	2003	Neurocomputing	10.1016/S0925-2312(02)00749-X	entropy;machine learning;mathematics;communications system	ML	20.293257674014995	-72.65175232047763	187691
162111d2f438cbd15d3ae8e22d7c27c7975e9b9c	"""""""complex receptive field"""" obtained by infomax coincides with the energy model for complex cells in the visual cortex."""	receptive field;visual cortex;complex cell			Kenji Okajima	1998			binocular neurons;orientation column;receptive field	ML	20.762160077369344	-68.46662076766972	188089
4a767922eef831b174c09f534afa35df15b27256	local active information storage as a tool to understand distributed neural information processing	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;information storage;research articles;distributed computation;abstracts;open access;life sciences;clinical guidelines;complex systems;voltage sensitive dye imaging;full text;local information dynamics;visual system;predictive coding;rest apis;orcids;europe pmc;biomedical research;neural dynamics;bioinformatics;literature search	Every act of information processing can in principle be decomposed into the component operations of information storage, transfer, and modification. Yet, while this is easily done for today's digital computers, the application of these concepts to neural information processing was hampered by the lack of proper mathematical definitions of these operations on information. Recently, definitions were given for the dynamics of these information processing operations on a local scale in space and time in a distributed system, and the specific concept of local active information storage was successfully applied to the analysis and optimization of artificial neural systems. However, no attempt to measure the space-time dynamics of local active information storage in neural data has been made to date. Here we measure local active information storage on a local scale in time and space in voltage sensitive dye imaging data from area 18 of the cat. We show that storage reflects neural properties such as stimulus preferences and surprise upon unexpected stimulus change, and in area 18 reflects the abstract concept of an ongoing stimulus despite the locally random nature of this stimulus. We suggest that LAIS will be a useful quantity to test theories of cortical function, such as predictive coding.	computer data storage;computers;distributed computing;dyes;hl7publishingsubsection <operations>;information processing;mathematical optimization;mathematics;theory;voltage-sensitive dye imaging	Michael Wibral;Joseph T. Lizier;Sebastian Vögler;Viola Priesemann;Ralf Galuske	2014		10.3389/fninf.2014.00001	psychology;complex systems;text mining;neuroscience;visual system;computer science;bioinformatics;artificial intelligence;data science;data mining	ML	19.230347925419533	-70.78063236486592	188993
8d2d0a4dea8561f0548010da6f58f48bf8507d1c	a phase dynamic model of systematic error in simple copying tasks	nonlinear oscillators;visuomotor transformation;phase dynamics;copying;handwriting	A crucial insight into handwriting dynamics is embodied in the idea that stable, robust handwriting movements correspond to attractors of an oscillatory dynamical system. We present a phase dynamic model of visuomotor performance involved in copying simple oriented lines. Our studies on human performance in copying oriented lines revealed a systematic error pattern in orientation of drawn lines, i.e., lines at certain orientation are drawn more accurately than at other values. Furthermore, human subjects exhibit “flips” in direction at certain characteristic orientations. It is argued that this flipping behavior has its roots in the fact that copying process is inherently ambiguous—a line of given orientation may be drawn in two different (mutually opposite) directions producing the same end result. The systematic error patterns seen in human copying performance is probably a result of the attempt of our visuomotor system to cope with this ambiguity and still be able to produce accurate copying movements. The proposed nonlinear phase-dynamic model explains the experimentally observed copying error pattern and also the flipping behavior with remarkable accuracy.	dynamical system;experiment;handwriting recognition;human reliability;mathematical model;mental orientation;movement;nonlinear system;plant roots	Saguna Dubey;Sandeep Sambaraju;Sarat Chandra Cautha;Vednath Arya;V. S. Chakravarthy	2009	Biological Cybernetics	10.1007/s00422-009-0330-9	simulation;computer science;artificial intelligence;communication	Vision	19.167006075998952	-68.98274696371938	189717
b0bcf67d2d728037d2e092d77173fd9249c89f6f	activity driven adaptive stochastic resonance	stochastic resonance;membrane potential;spike train;cortical neurons	Cortical neurons in vivo show fluctuations in their membrane potential of the order of several milli-volts. Using simple and biophysically realistic models of a single neuron we demonstrate that noise induced fluctuations can be used to adaptively optimize the sensitivity of the neuron's output to ensembles of subthreshold inputs of different average strengths. Optimal information transfer is achieved by changing the strength of the noise such that the neuron's average firing rate remains constant. Adaptation is fast, because only crude estimates of the output rate are required at any time.	acclimatization;estimated;membrane potentials;milli;milliinternational unit;neuron;neurons;stochastic resonance;video-in video-out;volt	Gregor Wenning;Klaus Obermayer	2001	Physical review letters		membrane potential;machine learning;mathematics;stochastic resonance	ML	19.290492235022676	-72.33474110898273	189849
cb9c5c003216ba4d054cf6c0e8a607f17ccc6c5c	investigation of driver performance with night-vision and pedestrian-detection systems—part 2: queuing network human performance modeling	vision system;file attente;modelizacion;teoria cognitiva;imageria termica;reinforcement learning process;contexto atestado;evaluation performance;vision ordenador;peaton;queueing network;eye;espectro ir proximo;near infrared spectrum;methode empirique;detecteur image;pedestrian safety;performance evaluation;image processing;poison control;eye movement strategies driver performance night vision pedestrian detection systems queuing network human performance modeling computational cognitive model far infrared sensor less cluttered images near infrared sensor reinforcement learning process;surveillance;injury prevention;infrared thermography;queueing theory;computer model;movimiento ocular;evaluacion prestacion;reinforcement learning;metodo empirico;simulation;empirical method;procesamiento imagen;hombre;scotopic vision;queue;safety literature;cognitive theory;less cluttered images;traffic safety;injury control;vehicle driver;red cola espera;humans computational modeling layout computer architecture computer networks finite impulse response filter image generation image sensors sensor systems night vision;traitement image;vision escotopica;theorie cognitive;computer vision;home safety;modelisation;far infrared sensor;thermographie ir;near infrared;injury research;safety abstracts;computational modeling;vigilancia;computer models;apprentissage renforce;human factors;thermal imaging;night vision;cluttered environment;reseau file attente;queueing theory cognition driver information systems eye learning artificial intelligence night vision;vision scotopique;queuing network human performance modeling;pedestrian detection systems;pedestrian;eye movement;occupational safety;cognition;pedestrian detection;safety;human;detecteur ir;model development;computational cognitive model;cognicion;human performance modeling;conductor vehiculo;safety research;termografia ir;imagerie thermique;accident prevention;near infrared sensor;vision ordinateur;violence prevention;far infrared	This paper introduces a queueing network-based computational model to explain driver performance in a pedestrian-detection task assisted with night-vision-enhancement systems. The computational cognitive model simulated the pedestrian-detection task using images displayed by two night-vision systems as input stimuli. The system equipped with a far-infrared (FIR) sensor generated less-cluttered images than the system equipped with a near-infrared (NIR) sensor. Using a reinforcement learning process, the model developed eye-movement strategies for each night-vision system. The differences in eye-movement strategies generated different eye-movement behaviors, in accord with the empirical findings.	cognitive model;computational model;finite impulse response;pedestrian detection;queueing theory;reinforcement learning	Ji Hyoun Lim;Yili Liu;Omer Tsimhoni	2010	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2010.2049844	cognitive model;computer vision;simulation;cognition;computer science;engineering;suicide prevention;artificial intelligence;human factors and ergonomics;injury prevention;image sensor;far infrared;queueing theory;scotopic vision;queue;eye movement;behavior	Robotics	23.44468405654389	-67.53635904925355	189905
1d8fb281783c796dc941074204d368fc31d22a12	development and spatial structure of cortical feature maps: a model study	spatial structure	"""Feature selective cells in the primary visual cortex of several species are organized in hierarchical topographic maps of stimulus features like """"position in visual space"""", """"orientation"""" and"""" ocular dominance"""". In order to understand and describe their spatial structure and their development, we investigate a self-organizing neural network model based on the feature map algorithm. The model explains map formation as a dimension-reducing mapping from a high-dimensional feature space onto a two-dimensional lattice, such that """"similarity"""" between features (or feature combinations) is translated into """"spatial proximity"""" between the corresponding feature selective cells. The model is able to reproduce several aspects of the spatial structure of cortical maps in the visual cortex."""	algorithm;artificial neural network;feature vector;map;network model;organizing (structure);self-organization;topography	Klaus Obermayer;Helge J. Ritter;Klaus Schulten	1990			computer vision;computer science;machine learning;pattern recognition;mathematics	ML	20.415732108431076	-67.66706212003102	190013
8ce413ebd71c0223c38341a44fa78d587acba4a1	a bio-inspired early-level image representation and its contribution to object recognition	object recognition;visual information processing bioinspired early level image representation object recognition visual stimulus biological visual system retinal ganglion cells neurobiological findings computational model image presentation method image segmentation;radio frequency image segmentation image representation object recognition visualization computational modeling biology;segmentation;image representation;object recognition image representation image segmentation;object recognition image representation segmentation	A visual stimulus is represented by the biological visual system at several levels, from low to high levels they are, photoreceptor cells, GCs, LGN cells and visual cortical neurons. Retinal ganglion cells (GCs) at the early level need to represent raw data only once, but meet a wide number of diverse requests from different vision-based tasks. This means the information representation at this level is general and not task-specific. Neurobiological findings have attributed this universal adaptation to GCs' RF mechanisms. For the purposes of developing a highly efficient image representation method that can facilitate information processing and interpretation at later stages, here we design a computational model to simulate the GC's non-classical RF. This new image presentation method can extract major structural features from raw data, and is consistent with other statistical measures of the image. Based on the new representation, the performances of other state-of-the-art algorithms in segmentation can be upgraded remarkably. This work concludes that applying sophisticated representation schema at early state is an efficient and promising strategy in visual information processing.	algorithm;computation;computational model;information processing;outline of object recognition;performance;radio frequency;simulation	Wei Hui;Qingsong Zuo	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.731	image texture;computer vision;pyramid;feature detection;image processing;form perception;computer science;cognitive neuroscience of visual object recognition;machine learning;segmentation-based object categorization;pattern recognition;image segmentation;scale-space segmentation;segmentation	Vision	22.046885432222634	-66.59011167248661	190261
d4f49f260db38077a03ea2c6f044c3f43d45e3a7	neural modeling of flow rendering effectiveness	perceptual theory;visualization;contour perception;data visualization;visual cortex;flow visualization	It has been previously proposed that understanding the mechanisms of contour perception can provide a theory for why some flow rendering methods allow for better judgments of advection pathways than others. In the present paper we develop this theory through a numerical model of the primary visual cortex of the brain (Visual Area 1) where contour enhancement is understood to occur according to most neurological theories. We apply a two-stage model of contour perception to various visual representations of flow fields evaluated by Laidlaw et al [2001]. In the first stage, contour enhancement is modeled based on Li's [1998] cortical model. In the second stage, a model of contour integration is proposed designed to support the task of advection path tracing. The model yields insights into the relative strengths of different flow visualization methods for the task of visualizing advection pathways.	mathematical model;numerical analysis;path tracing;theory	Daniel Pineo;Colin Ware	2008		10.1145/1394281.1394313	computer vision;simulation;visualization;flow visualization;computer science;communication;data visualization;statistics	ML	23.580568456191955	-70.02976708548711	191568
b672d6f1fb6d710f32100ac559e7b3411cc074bc	neuronal population oscillations of rat hippocampus during epileptic seizures	oscillations;brain;recuperacion;procesamiento informacion;methode empirique;mammalia;relaxation oscillator;systeme nerveux central;convulsion;metodo descomposicion;metodo empirico;hippocampus;epileptic seizure;empirical method;methode decomposition;oscillateur relaxation;hombre;vertebrata;recovery;encefalo;neuronal oscillation;rodentia;animal;hipocampo;decomposition method;sistema nervioso central;cerebro;relaxation oscillation;hippocampe;encephale;relaxation oscillators;oscilacion relajacion;cerveau;cognition;information processing;human;oscillation relaxation;cognicion;recuperation;encephalon;rat;rata;traitement information;in vivo;central nervous system;homme;empirical mode decomposition	Neuronal population oscillations in the hippocampus have an important effect in the information processing in the brain and the generation of epileptic seizures. In this paper, we investigate the neuronal population oscillations in the hippocampus of epileptic rats in vivo using an empirical mode decomposition (EMD) method. A neuronal population oscillation can be decomposed into several relaxation oscillations, which possess a recovery and release phase, with the different frequencies that ranges from 0 to 600 Hz. The natures of relaxation oscillations at the pre-ictal, seizure onset and ictal states are distinctly different. The analysis of relaxation oscillations show that the gamma wave is a lead relaxation oscillation at the pre-ictal stage, then it moves to beta oscillation or theta oscillation while the ictal stage starts; the fast relaxation oscillations are associated with the slow relaxation oscillations in the CA1 or CA3, in particular, the fast relaxation oscillations are associated on the recovery phase of the slow relaxation oscillations during the pre-ictal interval, however move to the release phase of the slow relaxation oscillations during the ictal interval. Comparison of the relaxation oscillations in CA1 and CA3 shows that the neurons in the CA1 are more active during the epileptic seizures than during the pre-ictal stage. These findings demonstrate that this method is very helpful to decompose neuronal population for understanding the underlying mechanism of epileptic seizures.	ca1 field;ca3 field;epilepsy;hertz (hz);hilbert–huang transform;information processing;linear programming relaxation;neural ensemble;neural oscillation;onset (audio);relaxation oscillator;seizures;very helpful;video-in video-out;yag rx mode:prid:pt:eye.left:nom	Xiaoli Li;John G. R. Jefferys;John Fox;Xin Yao	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2008.06.002	information processing;artificial intelligence;relaxation oscillator	ML	20.531223572813015	-71.46781259555894	192958
72489dd26e2da6962cbb7ece4572639338d561aa	supervised learning through neuronal response modulation	calcul neuronal;hebbian learning;neural computation;supervised learning;62m45;modulacion;approximation fonction;function approximation;apprentissage surveille;synaptic plasticity;analyse correlation;plasticite synaptique;apprentissage supervise;reseau neuronal;plasticidad sinaptica;aprendizaje supervisado;supervision;red neuronal;computacion neuronal;analisis correlacion;apprentissage hebbien;neural network;correlation analysis;modulation	Neural networks that are trained to perform specific tasks must be developed through a supervised learning procedure. This normally takes the form of direct supervision of synaptic plasticity. We explore the idea that supervision takes place instead through the modulation of neuronal excitability. Such supervision can be done using conventional synaptic feedback pathways rather than requiring the hypothetical actions of unknown modulatory agents. During task learning, supervised response modulation guides Hebbian synaptic plasticity indirectly by establishing appropriate patterns of correlated network activity. This results in robust learning of function approximation tasks even when multiple output units representing different functions share large amounts of common input. Reward-based supervision is also studied, and a number of potential advantages of neuronal response modulation are identified.	approximation;artificial neural network;excited state;hebbian theory;modulation;neuronal plasticity;supervised learning;synaptic package manager	Christian D. Swinehart;L. F. Abbott	2005	Neural Computation	10.1162/0899766053019980	psychology;synaptic plasticity;neuroscience;hebbian theory;function approximation;computer science;artificial intelligence;machine learning;supervised learning;artificial neural network;models of neural computation;modulation	ML	21.024130070799604	-70.36918346270454	193412
1fdcbb5e52690a5ca24e4bc926ba8a2e6394a03f	azimuthal sound localisation with electronic lateral superior olive		Since the lateral superior olive (LSO) is the first nucleus in the audi- tory pathway where binaural inputs converge, it is thought to be involved in azimuthal localization of sounds by calculating the interaural level difference (ILD). The electronic LSO can be used for azimuthal localization in robotics. Thus, in this paper we demonstrate the design, fabrication and test results from a silicon chip which performs azimuthal localization based on the Reed and Blum's model of the population response of the LSO in brain.	lateral thinking	Anu Aggarwal	2015		10.1007/978-3-319-23983-5_22	acoustics;engineering;communication;cartography	AI	20.0806695574374	-71.96720929355986	193563
94cfb936ef13e59101c769250bf578c735a72668	ergodicity and parameter estimates in auditory neural circuits	auditory pathway;ergodic theory;ergodicity;circular statistics;interspike interval;probability distribution function;sensory modality;spike timing jitter;vector strength	This paper discusses ergodic properties and circular statistical characteristics in neuronal spike trains. Ergodicity means that the average taken over a long time period and over smaller population should equal the average in less time and larger population. The objectives are to show simple examples of design and validation of a neuronal model, where the ergodicity assumption helps find correspondence between variables and parameters. The methods used are analytical and numerical computations, numerical models of phenomenological spiking neurons and neuronal circuits. Results obtained using these methods are the following. They are: a formula to calculate vector strength of neural spike timing dependent on the spike train parameters, description of parameters of spike train variability and model of output spiking density based on assumption of the computation realized by sound localization neural circuit. Theoretical results are illustrated by references to experimental data. Examples of neurons where spike trains have and do not have the ergodic property are then discussed.	acronyms;action potential;computation (action);computer simulation;email;english language;ergodic theory;ergodicity;estimated;large;license;manuscripts;numerical analysis;organophosphates;pharmacogenetics;population parameter;programmer;selaginella;small;sound localization;spatial variability;the spike (1997)	Peter G. Toth;Petr Marsalek;Ondrej Pokora	2017		10.1007/s00422-017-0739-5	ergodicity;probability density function;machine learning;ergodic theory;artificial intelligence;computation;spike train;biological neural network;sound localization;population;mathematics	ML	20.653428795006178	-72.70794560940384	194974
a3ab57b9060aa35bb5ec06aaccb5abc1f21f2d68	selective spiking in neuronal populations		The use of extrinsic stimulation to control activity in neuronal networks i.e., neurocontrol, is a key problem in control engineering and neuroscience. Here, we study the general problem of selective spiking in a population of neurons. The goal is to use an input stimulus in order to induce a spike in a specific neuron of a population while keeping all others suppressed. We formulate a strict version of this problem for the class of Integrate-and-Fire neuron models, which amounts to an optimal control problem with state constraints. While possible to solve in low dimensions, the strict problem is harder to handle for larger networks. Thus, we relax the problem via regularization and derive the ensuing optimal controls for selective spiking. The properties of the solution are highlighted through several examples. The results provide a tractable, scalable solution for a baseline neurocontrol problem.	action potential;approximation;baseline (configuration management);cobham's thesis;computational neuroscience;control engineering;coupling (computer programming);mathematical model;matrix regularization;neuron;optimal control;population;programming paradigm;scalability;selectivity (electronic);synaptic package manager;video-in video-out	Anirban Nandi;Heinz Schättler;ShiNung Ching	2017	2017 American Control Conference (ACC)	10.23919/ACC.2017.7963377	control theory;computer science;scalability;stimulus (physiology);regularization (mathematics);mathematical optimization;optimal control;population	ML	22.770954648812406	-71.38465141790742	195068
fbc4d19e1b7a10baf61c219391187b527a59a800	glomerular microcircuits in the olfactory bulb	modelizacion;animals;interneurons;glomerulus;microcircuito;neural pathways;arquitectura circuito;integrated circuit;computer model;circuit architecture;circuito integrado;olfacion;synaptic transmission;odeur;microcircuit;modelisation;computational modeling;olor;normalization;architecture circuit;olfaction;decorrelation;humans;neurons;dendrites;reseau neuronal;olfactory pathways;modeling;computer simulation;olfactory bulb;red neuronal;circuit integre;neural network;odor;smell	Microcircuits in the olfactory bulb have long received particular attention from both experimentalists and theoreticians, due in part to an abundance of dendrodendritic interactions and other specialized modifications to the canonical cortical circuit architecture. Recent experimental and theoretical results have elucidated the mechanisms and function of these circuits and their presumed contributions to olfactory stimulus processing and odor perception. We here review the architecture and functionality of a prominent olfactory bulb microcircuit: the glomerular network.		Christiane Linster;Thomas A. Cleland	2009	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2009.07.013	computer simulation;computer science;machine learning;olfaction	ML	20.603724312543847	-69.8688241533209	195093
e3d9eff3c7f26d7aecb8fe4376ddf1993df70bf9	neural network mechanisms underlying stimulus driven variability reduction	animals;models neurological;network effect;synaptic transmission;excitatory postsynaptic potentials;feedback physiological;single cell;reproducibility of results;parameter space;nerve net;spike train;humans;receptive field;neurons;neural network model;action potentials;computer simulation;neural network	It is well established that the variability of the neural activity across trials, as measured by the Fano factor, is elevated. This fact poses limits on information encoding by the neural activity. However, a series of recent neurophysiological experiments have changed this traditional view. Single cell recordings across a variety of species, brain areas, brain states and stimulus conditions demonstrate a remarkable reduction of the neural variability when an external stimulation is applied and when attention is allocated towards a stimulus within a neuron's receptive field, suggesting an enhancement of information encoding. Using an heterogeneously connected neural network model whose dynamics exhibits multiple attractors, we demonstrate here how this variability reduction can arise from a network effect. In the spontaneous state, we show that the high degree of neural variability is mainly due to fluctuation-driven excursions from attractor to attractor. This occurs when, in the parameter space, the network working point is around the bifurcation allowing multistable attractors. The application of an external excitatory drive by stimulation or attention stabilizes one specific attractor, eliminating in this way the transitions between the different attractors and resulting in a net decrease in neural variability over trials. Importantly, non-responsive neurons also exhibit a reduction of variability. Finally, this reduced variability is found to arise from an increased regularity of the neural spike trains. In conclusion, these results suggest that the variability reduction under stimulation and attention is a property of neural circuits.	artificial neural network;bifurcation theory;code;ephrin type-b receptor 1, human;exhibits as topic;experiment;heart rate variability;integrated circuit;multiscroll attractor;network model;neural network simulation;neural oscillation;neuron;neurons;population parameter;spatial variability;spontaneous order;universal conductance fluctuations	Gustavo Deco;Etienne Hugues	2012		10.1371/journal.pcbi.1002395	neuroscience;computer science;artificial intelligence;machine learning;excitatory postsynaptic potential;parameter space;receptive field;action potential;artificial neural network;neurotransmission	ML	18.49919990013059	-71.29123241008539	195328
3117bb1fdbe4e0ea95c23bad1e29e67147402bcb	neuronal calcium wave propagation varies with changes in endoplasmic reticulum parameters: a computer model		Calcium () waves provide a complement to neuronal electrical signaling, forming a key part of a neuron’s second messenger system. We developed a reaction-diffusion model of an apical dendrite with diffusible inositol triphosphate (), diffusible , receptors (s), endoplasmic reticulum (ER) leak, and ER pump (SERCA) on ER. is released from ER stores via s upon binding of and . This results in -induced--release (CICR) and increases spread. At least two modes of wave spread have been suggested: a continuous mode based on presumed relative homogeneity of ER within the cell and a pseudo-saltatory model where regeneration occurs at discrete points with diffusion between them. We compared the effects of three patterns of hypothesized distribution: (1) continuous homogeneous ER, (2) hotspots with increased density ( hotspots), and (3) areas of increased ER density (ER stacks). All three modes produced waves with velocities similar to those measured in vitro (approximately 50–90 m /sec). Continuous ER showed high sensitivity to density increases, with time to onset reduced and speed increased. Increases in SERCA density resulted in opposite effects. The measures were sensitive to changes in density and spacing of hotspots and stacks. Increasing the apparent diffusion coefficient of   substantially increased wave speed. An extended electrochemical model, including voltage-gated calcium channels and AMPA synapses, demonstrated that membrane priming via AMPA stimulation enhances subsequent wave amplitude and duration. Our modeling suggests that pharmacological targeting of s and SERCA could allow modulation of wave propagation in diseases where dysregulation has been implicated.	action potential;calcium;coefficient;complement system proteins;computer simulation;dendrites;endoplasmic reticulum;erdős–rényi model;hotspot (wi-fi);modulation;neuron;numerical weather prediction;onset (audio);pharmacology;priming exercise;pseudo brand of pseudoephedrine;second messenger systems;software propagation;synapses;tissue membrane;apical dendrite;cellular targeting;inositol triphosphate	Samuel A. Neymotin;Robert A. McDougal;Mohamed A. Sherif;Christopher P. Fall;Michael L. Hines;William W. Lytton	2015	Neural Computation	10.1162/NECO_a_00712	telecommunications	ML	17.447185618493116	-72.20455455061999	195371
47c1852581682a704e5fb421e38d14077d6d7c22	simulation of neocortical epileptiform activity using parallel computing	parallel computing;pyramidal cell;gap junction;activity pattern;network model;network bursting;parallel computer;eeg;electroencephalogram;epilepsy	A scalable network model intended for study of neocortical epileptiform activity was built on the pGENESIS neural simulator. The model included super7cial and deep pyramidal cells plus four types of inhibitory neurons. An electroencephalogram (EEG) simulator was attached to the model to validate model behavior and to determine the contributions of inhibitory and excitatory neuronal populations to the EEG signal. We examined e9ects of overall excitation and inhibition on activity patterns in the network, and found that the network-bursting patterns occur within a narrow range of the excitation–inhibition space. Further, we evaluated synchronization e9ects produced by gap junctions during synchronous and asynchronous states. c © 2004 Elsevier B.V. All rights reserved.	electroencephalography;network model;parallel computing;population;scalability;simulation	Wim van Drongelen;Hyong Lee;Mark Hereld;David Jones;Matthew Cohoon;Frank Elsen;Michael E. Papka;Rick L. Stevens	2004	Neurocomputing	10.1016/j.neucom.2004.01.186	gap junction;computer science;network model;machine learning	AI	17.96435640149661	-70.77796862420088	195673
df6e9641cd8ce4465c53721e14acf4152af38366	nonlinear dynamics in a neural network (parallel) processor	silicon;neural networks;nerve;hopf bifurcation;fixed point;single channel;limit cycle;nonlinear dynamics;diodes;modeling;neural network	We consider an iterative map derived from the device equations for a silicon p+-nn+ diode, which simulates a biological neuron. This map has been extended to a coupled neuron circuit consisting of two of these artificial neurons connected by a filter circuit, which could be used as a single channel of a parallel asynchronous processor. The extended map output is studied under different conditions to determine the effect of various parameters on the pulsing pattern. As the control parameter is increased, fixed points (both stable and unstable) as well as a limit cycle appear. On further increase, a Hopf bifurcation is seen causing the disappearance of the limit cycle. The increasing control parameter, which is related to a decrease in the bias applied to the circuit, also causes variation in the location of the fixed points. This variation could be important in applications to neural networks. The control parameter value at which the fixed points appear and the bifurcation occurs can be varied by changing the weightage of the filter circuit. The modeling outputs, are compared with the experimental outputs.	artificial neural network;artificial neuron;asynchronous circuit;bifurcation theory;control theory;diode;fixed point (mathematics);hopf bifurcation;iterative method;limit cycle;nonlinear dynamics;parallel computing	A. G. Unil Perera;S. G. Matsik;S. R. Betarbet	1995		10.1117/12.211986	systems modeling;fixed point;silicon;limit cycle;artificial neural network;hopf bifurcation;diode	ML	17.53183141985203	-70.38863273761648	196790
2c780e1d1ce2e2008bc33685ed204d011c6bcd82	a tale of two stories: astrocyte regulation of synaptic depression and facilitation	exocytosis;oscillations;models neurological;computer model;neurotransmitter agents;hippocampus;neuronal plasticity;neurotransmitter release;glial cell;astrocyte;astrocytes;paired pulse depression;synaptic transmission;calcium;presynaptic terminals;mean field;information transfer;synaptic plasticity;signaling pathway;glutamate;synaptic depression;action potentials;intracellular ca2;computer simulation;depression;short term plasticity;synapses;numerical simulation	Short-term presynaptic plasticity designates variations of the amplitude of synaptic information transfer whereby the amount of neurotransmitter released upon presynaptic stimulation changes over seconds as a function of the neuronal firing activity. While a consensus has emerged that the resulting decrease (depression) and/or increase (facilitation) of the synapse strength are crucial to neuronal computations, their modes of expression in vivo remain unclear. Recent experimental studies have reported that glial cells, particularly astrocytes in the hippocampus, are able to modulate short-term plasticity but the mechanism of such a modulation is poorly understood. Here, we investigate the characteristics of short-term plasticity modulation by astrocytes using a biophysically realistic computational model. Mean-field analysis of the model, supported by intensive numerical simulations, unravels that astrocytes may mediate counterintuitive effects. Depending on the expressed presynaptic signaling pathways, astrocytes may globally inhibit or potentiate the synapse: the amount of released neurotransmitter in the presence of the astrocyte is transiently smaller or larger than in its absence. But this global effect usually coexists with the opposite local effect on paired pulses: with release-decreasing astrocytes most paired pulses become facilitated, namely the amount of neurotransmitter released upon spike i+1 is larger than that at spike i, while paired-pulse depression becomes prominent under release-increasing astrocytes. Moreover, we show that the frequency of astrocytic intracellular Ca(2+) oscillations controls the effects of the astrocyte on short-term synaptic plasticity. Our model explains several experimental observations yet unsolved, and uncovers astrocytic gliotransmission as a possible transient switch between short-term paired-pulse depression and facilitation. This possibility has deep implications on the processing of neuronal spikes and resulting information transfer at synapses.	action potential;astrocytes;computation;computational model;depressive disorder;hippocampus (brain);large;mass effect trilogy;modulation;neuroglia;neuronal plasticity;neurotransmitters;numerical analysis;simulation;small;synapse;synapses;video-in video-out;facilitation	Maurizio De Pittà;Vladislav Volman;Hugues Berry;Eshel Ben-Jacob	2011		10.1371/journal.pcbi.1002293	computer simulation;synaptic plasticity;biology;neuroscience;information transfer;calcium;neural facilitation;hippocampus;exocytosis	ML	17.978523544699144	-71.50291767168497	196946
f3500a65c16e7126d592bfb909c367fc6ff2742f	phase synchronizing in hindmarsh-rose neural networks with delayed chemical coupling	dynamical networks;coupled oscillators;phase synchronization;coupled oscillator;random networks;time delay;electrical coupling;synchronization;network structure;neuronal network;transmission time delay;dynamic networks;neural network	Although diffusive electrical connections in neuronal networks are instantaneous, excitatory/inhibitory couplings via chemical synapses encompass a transmission time-delay. In this paper neural networks with instantaneous electrical couplings and time-delayed excitatory/inhibitory chemical connections are considered and scaling of the spike phase synchronization with the unified time-delay in the connections, the phase synchronization could be enhanced by introducing time-delay. The role of the variability of the neuronal external current in the phase synchronization is also investigated. As individual neuron models, Hindmarsh–Rose model is adopted and the network structure of the electrical and chemical connections is considered to be Watts–Strogatz and directed random networks,	artificial neural network;computation;computer simulation;coupling constant;emoticon;flow network;graph theory;heart rate variability;hindmarsh–rose model;neuron;numerical analysis;synapse;synaptic package manager;the spike (1997);watts humphrey	Mahdi Jalili	2011	Neurocomputing	10.1016/j.neucom.2010.12.031	synchronization;biological neural network;coupling;synchronization networks;phase synchronization;computer science;machine learning;control theory;artificial neural network	ML	17.557754122451044	-70.39516327486626	198535
663362f541dcdc741512ac6be5dde2cdff8048f4	developmental pruning of synapses and category learning	category learning;self organization;circuit complexity;neuronal activity	After an initial peak, the number of synapses in mammalian cerebral cortex decreases in the formative period and throughout adult life. However, if synapses are taken to reflect circuit complexity, the issue arises of how to reconcile pruning with the increasing complexity of the representations acquired in successive stages of development. Taking these two conflicting requirements as an architectural constraint, we show here that a simple topographic self-organization process can learn increasingly complex representations when some of its synapses are progressively pruned. By addressing the learning-theoretic properties of increasing complexity, the model indicates how pruning may be computationally advantageous. This suggests a novel interpretation of the interplay between biological and acquired patterns of neuronal activation determining topographic organization in the cortex.	activation function;circuit complexity;concept learning;requirement;self-organization;synapse;theory;topography	Roberto Viviani;Manfred Spitzer	2003			pruning;synapse;premovement neuronal activity;machine learning;artificial intelligence;pattern recognition;cerebral cortex;circuit complexity;computer science;concept learning	ML	20.21270990002624	-69.76532417447007	198720
f805ef4bb8da62681381cb33d68743404a6fc87c	gaba transporter preserving ongoing spontaneous neuronal activity at firing subthreshold	oscillations;calcul neuronal;inhibicion;membrane potential;neural computation;membrane;potentiel membrane;seuil;tiempo reaccion;threshold;62m45;temps reaction;modele reseau neuronal;feedback;potencial membrana;neurone intermediaire;gabaergic interneuron;interneuron;gaba receptor;umbral;inhibition;neural network model;reseau neuronal;boucle reaction;58a25;retroalimentacion;neurona intermediaria;red neuronal;computacion neuronal;neuronal activity;membrana;reaction time;neural network	There has been compelling evidence that the GABA transporter is crucial not only for removing gamma-aminobutyric acid (GABA) from but also releasing it into extracellular space, thereby clamping ambient GABA (GABA in extracellular space) at a certain level. The ambient GABA is known to activate extrasynaptic GABA receptors and provide tonic inhibitory current into neurons. We investigated how the transporter regulates the level of ambient GABA, mediates tonic neuronal inhibition, and influences ongoing spontaneous neuronal activity. A cortical neural network model is proposed in which GABA transporters on lateral (L) and feedback (F) inhibitory (GABAergic) interneurons are functionally made. Principal (P) cell assemblies participate in expressing information about elemental sensory features. At membrane potentials below the reversal potential, there is net influx of GABA, whereas at membrane potentials above the reversal potential, there is net efflux of GABA. Through this transport mechanism, ambient GABA concentration is kept within a submicromolar range during an ongoing spontaneous neuronal activity time period. Here we show that the GABA transporter on L cells regulates the overall level of ambient GABA across cell assemblies, and that on F cells it does so within individual cell assemblies. This combinatorial regulation of ambient GABA allows P cells to oscillate near firing threshold during the ongoing time period, thereby reducing their reaction time to externally applied stimuli. We suggest that the GABA transporter, with its forward and reverse transport mechanism, could regulate the ambient GABA. This transporter-mediated ambient GABA regulation may contribute to establishing an ongoing subthreshold neuronal state by which the network can respond rapidly to subsequent sensory input.	activation action;ambient intelligence;ambient occlusion;artificial neural network;elemental;ephrin type-b receptor 1, human;extracellular space;gaba receptor;gaba transporter;hebbian theory;interneurons;lateral thinking;membrane potentials;membrane transport proteins;network model;spontaneous order;gamma-aminobutyric acid	Osamu Hoshino	2009	Neural Computation	10.1162/neco.2009.05-08-778	mental chronometry;neuroscience;membrane potential;computer science;machine learning;membrane;feedback;communication;oscillation;artificial neural network;premovement neuronal activity;models of neural computation	ML	19.684881203252377	-71.03460362827123	198748
6cdabc2b8821407148d8568bea627bdea075459c	dynamic updating of distributed neural representations using forward models	forward model;network model;motor imagery;motor control	In this paper, we present a continuous attractor network model that we hypothesize will give some suggestion of the mechanisms underlying several neural processes such as velocity tuning to visual stimulus, sensory discrimination, sensorimotor transformations, motor control, motor imagery, and imitation. All of these processes share the fundamental characteristic of having to deal with the dynamic integration of motor and sensory variables in order to achieve accurate sensory prediction and/or discrimination. Such principles have already been described in the literature by other high-level modeling studies (Decety and Sommerville in Trends Cogn Sci 7:527–533, 2003; Oztop et al. in Neural Netw 19(3):254–271, 2006; Wolpert et al. in Philos Trans R Soc 358:593–602, 2003). With respect to these studies, our work is more concerned with biologically plausible neural dynamics at a population level. Indeed, we show that a relatively simple extension of the classical neural field models can endow these networks with additional dynamic properties for updating their internal representation using external commands. Moreover, an analysis of the interactions between our model and external inputs also shows interesting properties, which we argue are relevant for a better understanding of the neural processes of the brain.	area striata structure;artificial neural network;cdisc adas-cog - commands summary score;guided imagery;high- and low-level;ian sommerville (technician);interaction;network model;sensorimotor cortex;velocity (software development);cell transformation	Eric L. Sauser;Aude Billard	2006	Biological Cybernetics	10.1007/s00422-006-0131-3	psychology;motor control;neuroscience;computer science;artificial intelligence;network model;machine learning;communication;motor imagery	ML	18.966909537252917	-68.20160797541901	199691
0fb4d98ac0a2f3c6f058ec4b2c25835ee8a24fcc	mechanisms of human facial recognition		"""This paper presents an extension and refinement of the author's theory for human visual information processing, which is then applied to the problem of human facial recognition. Several fundamental processes are implicated: encoding of visual images into neural patterns, detection of simple facial features, size standardization, reduction of the neural patterns in dimensionality, and finally correlation of the resulting sequence of patterns with all visual patterns already stored in memory. In the theory presented here, this entire process is automatically """"driven"""" by the storage system in what amounts to an hypothesis verification paradigm. Neural networks for carrying out these processes are presented and syndromes resulting from damage to the proposed system are analyzed. A correspondence between system component and brain anatomy is suggested, with particular emphasis on the role of the primary visual cortex in this process. The correspondence is supported by structural and electrophysiological properties of the primary visual cortex and other related structures. The logical (computational) role suggested for the primary visual cortex has several components: size standardization, size reduction, and object extraction. The result of processing by the primary visual cortex, it is suggested, is a neural encoding of the visual pattern at a size suitable for storage. (In this context, object extraction is the isolation of regions in the visual field having the same color, texture, or spatial extent.) It is shown in detail how the topology of the mapping from retina to cortex, the connections between retina, lateral geniculate bodies and primary visual cortex, and the local structure of the cortex itself may combine to encode the visual patterns. Aspects of this theory are illustrated graphically with human faces as the primary stimulus. However, the theory is not limited to facial recognition but pertains to Gestalt recognition of any class of familiar objects or scenes."""	computation;computer data storage;encode;facial recognition system;gestalt psychology;information processing;lateral thinking;neural networks;neural coding;programming paradigm;refinement (computing)	Robert J. Baron	1981	International Journal of Man-Machine Studies	10.1016/S0020-7373(81)80001-6	computer vision;form perception;computer science;artificial intelligence;surround suppression;human visual system model;biased competition theory	ML	21.692889834971904	-66.60268534495727	199773
