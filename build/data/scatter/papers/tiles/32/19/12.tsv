id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
c807d1bf2b90a11610878f0f5d30d246a698e98c	state aware imitation learning		Imitation learning is the study of learning how to act given a set of demonstrations provided by a human expert. It is intuitively apparent that learning to take optimal actions is a simpler undertaking in situations that are similar to the ones shown by the teacher. However, imitation learning approaches do not tend to use this insight directly. In this paper, we introduce State Aware Imitation Learning (SAIL), an imitation learning algorithm that allows an agent to learn how to remain in states where it can confidently take the correct action and how to recover if it is lead astray. Key to this algorithm is a gradient learned using a temporal difference update rule which leads the agent to prefer states similar to the demonstrated states. We show that estimating a linear approximation of this gradient yields similar theoretical guarantees to online temporal difference learning approaches and empirically show that SAIL can effectively be used for imitation learning in continuous domains with non-linear function approximators used for both the policy representation and the gradient estimate.	algorithm;gradient;linear approximation;linear function;nonlinear system;supervised learning;temporal difference learning	Yannick Schroecker;Charles Lee Isbell	2017			semi-supervised learning;linear approximation;computer science;machine learning;online machine learning;error-driven learning;temporal difference learning;active learning (machine learning);cognitive imitation;imitation;artificial intelligence	ML	20.865084463061276	-20.632004868549558	197727
bdefaa47744c83ad8d90939b7a2e4cbb76e9b49d	a novel neural network based reinforcement learning	mobile robot;path planning;reinforcement learning;neural network model;neural network	Many function-approaching methods such as neural network, fuzzy method are used in reinforcement learning methods for solving its huge problem space dimensions. This paper presents a novel ART2 neural network based reinforcement learning method (ART2-RL) to solve the space problem. Because of its adaptive resonance characteristic, ART2 neural network is used to process the space measurement of reinforcement learning and improve the learning speed. This paper also gives the reinforcement learning algorithm based on ART2. A simulation of path planning of mobile robot has been developed to prove the validity of ART2-RL. As the complexity of the simulation increased, the result shows that the number of collision between robot and obstacles is effectively decreased; the novel neural network model provides significant improvement in the space measurement of reinforcement learning.	artificial neural network;reinforcement learning	Jian Fan;Yang Song;Minrui Fei;Qijie Zhao	2007		10.1007/978-3-540-74769-7_6	temporal difference learning;unsupervised learning;robot learning;error-driven learning;simulation;types of artificial neural networks;wake-sleep algorithm;computer science;artificial intelligence;recurrent neural network;machine learning;time delay neural network;deep learning;learning classifier system;competitive learning;evolutionary robotics;reinforcement learning;artificial neural network	ML	18.114095427569318	-22.34678789477775	198022
e0f9e4e0f63661f2daf89abe1a8c752872f0b38e	predicting opponent actions by observation	opponent modeling;input output;machine learning;proof of principle	In competitive domains, the knowledge about the opponent can give players a clear advantage. This idea lead us in the past to propose an approach to acquire models of opponents, based only on the observation of their input-output behavior. If opponent outputs could be accessed directly, a model can be constructed by feeding a machine learning method with traces of the opponent. However, that is not the case in the Robocup domain. To overcome this problem, in this paper we present a three phases approach to model low-level behavior of individual opponent agents. First, we build a classifier to label opponent actions based on observation. Second, our agent observes an opponent and labels its actions using the previous classifier. From these observations, a model is constructed to predict the opponent actions. Finally, the agent uses the model to anticipate opponent reactions. In this paper, we have presented a proof-of-principle of our approach, termed OMBO (Opponent Modeling Based on Observation), so that a striker agent can anticipate a goalie. Results show that scores are significantly higher using the acquired opponentâ€™s model of actions.	action potential;adversary (cryptography);competitive programming;high- and low-level;online and offline;online machine learning;statistical classification;striker;tracing (software)	Agapito Ledezma;Ricardo Aler;Araceli Sanchis;Daniel Borrajo	2004		10.1007/978-3-540-32256-6_23	input/output;simulation;computer science;artificial intelligence;machine learning;proof of concept;fictitious play	AI	18.05050301507232	-19.98677790609083	198733
