id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
76ffa68d55657e92065aa827e2c6160c8eb6685b	a q-decomposition and bounded rtdp approach to resource allocation	real time dynamic programming;resource allocation;resource manager;marginal revenue;heuristic search;value function;q decomposition;lower bound	This paper contributes to solve effectively stochastic resource allocation problems known to be NP-Complete. To address this complex resource management problem, a Q-decomposition approach is proposed when the resources which are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent. The Q-decomposition allows to coordinate these reward separated agents and thus permits to reduce the set of states and actions to consider. On the other hand, when the resources are available to all agents, no Q-decomposition is possible and we use heuristic search. In particular, the bounded Real-time Dynamic Programming (bounded RTDP) is used. Bounded RTDP concentrates the planning on significant states only and prunes the action space. The pruning is accomplished by proposing tight upper and lower bounds on the value function.	backup;bellman equation;consumability;dynamic programming;experiment;heuristic;marginal model;np-completeness;qr decomposition;real-time clock;search algorithm	Pierrick Plamondon;Brahim Chaib-draa;Abder Rezak Benaskeur	2007		10.1145/1329125.1329368	mathematical optimization;simulation;heuristic;marginal revenue;resource allocation;resource management;bellman equation;upper and lower bounds	AI	21.574088880002655	-16.064309470666103	197679
2c582cb29ca284756891a32f77806993d53bcddc	a comparison of three fitness prediction strategies for interactive genetic algorithms	evaluation performance;fatigue;concepcion ingenieria;engineering design;learning algorithm;algorithmique;performance evaluation;fatiga;conception ingenierie;evaluacion prestacion;developpement produit;algorithmic probability;intelligence artificielle;algorithme apprentissage;probabilistic approach;algoritmo genetico;convergence numerique;numerical convergence;reseau bayes;algorithmics;red bayes;algoritmica;enfoque probabilista;approche probabiliste;interactive genetic algorithm;bayes network;algorithme genetique;human fatigue problem;artificial intelligence;genetic algorithm;inteligencia artificial;product design;reseau neuronal;algoritmo aprendizaje;fitness prediction;convergencia numerica;red neuronal;desarrollo producto;neural network;product development	The human fatigue problem is one of the most significant problems encountered by interactive genetic algorithms (IGA). Different strategies have been proposed to address this problem, such as easing evaluation methods, accelerating IGA convergence via speedup algorithms, and fitness prediction. This paper studies the performance of fitness prediction strategies. Three prediction schemes, the neural network (NN), the Bayesian learning algorithm (BLA), and a novel prediction method based on algorithmic probability (ALP), are examined. Numerical simulations are performed in order to compare the performances of these three schemes.	algorithmic probability;artificial neural network;computational fluid dynamics;genetic algorithm;in-game advertising;interactive evolutionary computation;mathematical optimization;numerical linear algebra;performance;simulation;speedup	Leuo-hong Wang	2007	J. Inf. Sci. Eng.		algorithmic probability;simulation;genetic algorithm;computer science;artificial intelligence;machine learning;bayesian network;product design;algorithmics;algorithm;new product development	AI	23.45409684538774	-11.643503209416103	198071
9a8d001f731a68a86b8e41dad20b6ecafb71ce40	analyzing the effect of information stagnancy on the distributed stochastic algorithm		Despite the fact that many real world problems change over time, many Distributed Constraint Optimization Problem (DCOP) algorithms assume that the problem is constant or changing at a negligible rate. In addition, these algorithms also assume that changes to the environment are instantaneously observable. However, in highly dynamic environments with communication delays, both of these assumptions can be violated resulting in problem solving with out-of-date information. In this study, we explore the relationship between environmental dynamics, information stagnancy, and solution quality in Dynamic DCOP problems. By using recent advances in the analysis of dynamic, distributed problems, we show that information stagnancy can be characterized and used to accurately predict the behavior of a protocol. To evaluate our finding, we use the Distributed Stochastic Algorithm (DSA) as a basis. Through extensive empirical testing, we show that the prediction function is accurate.	algorithm;dcop;distributed constraint optimization;information theory;observable;optimization problem;problem solving	Saeid Samadidana;Roger Mailler	2018			empirical research;computer science;distributed algorithm;constrained optimization;distributed computing;algorithm;observable;distributed constraint optimization	AI	20.133521627776126	-15.48459128956508	198243
8a6a6871e8729f28831f949e939b9f0f8b5cd3be	structure learning of probabilistic logic programs by searching the clause space	statistical relational learning	Learning probabilistic logic programming languages is receiving an increasing attention and systems are available for learning the parameters (PRISM, LeProbLog, LFI-ProbLog and EMBLEM) or both the structure and the parameters (SEM-CP-logic and SLIPCASE) of these languages. In this paper we present the algorithm SLIPCOVER for “Structure LearnIng of Probabilistic logic programs by searChing OVER the clause space”. It performs a beam search in the space of probabilistic clauses and a greedy search in the space of theories, using the log likelihood of the data as the guiding heuristics. To estimate the log likelihood SLIPCOVER performs Expectation Maximization with EMBLEM. The algorithm has been tested on five real world datasets and compared with SLIPCASE, SEM-CP-logic, Aleph and two algorithms for learning Markov Logic Networks (Learning using Structural Motifs (LSM) and ALEPH++ExactL1). SLIPCOVER achieves higher areas under the precision-recall and ROC curves in most cases.	beam search;expectation–maximization algorithm;file inclusion vulnerability;greedy algorithm;heuristic (computer science);local search (optimization);logic programming;markov chain;markov logic network;pl/p;prism (surveillance program);programming language;receiver operating characteristic;repository (version control);theory	Elena Bellodi;Fabrizio Riguzzi	2015	TPLP	10.1017/S1471068413000689	artificial intelligence;theoretical computer science;machine learning;programming language;algorithm	ML	22.509328901674458	-15.90036968021385	198428
4d75a4aaf5463144bd399e096159fd09d658000b	planning using hierarchical constrained markov decision processes	constrained markov decision processes;planning;uncertainty	Constrained Markov decision processes offer a principled method to determine policies for sequential stochastic decision problems where multiple costs are concurrently considered. Although they could be very valuable in numerous robotic applications, to date their use has been quite limited. Among the reasons for their limited adoption is their computational complexity, since policy computation requires the solution of constrained linear programs with an extremely large number of variables. To overcome this limitation, we propose a hierarchical method to solve large problem instances. States are clustered into macro states and the parameters defining the dynamic behavior and the costs of the clustered model are determined using a Monte Carlo approach. We show that the algorithm we propose to create clustered states maintains valuable properties of the original model, like the existence of a solution for the problem. Our algorithm is validated in various planning problems in simulation and on a mobile robot platform, and we experimentally show that the clustered approach significantly outperforms the non-hierarchical solution while experiencing only moderate losses in terms of objective functions.	markov chain;markov decision process	Seyedshams Feyzabadi;Stefano Carpin	2017	Auton. Robots	10.1007/s10514-017-9630-4	computer science;monte carlo method;computational complexity theory;decision problem;computation;mathematical optimization;machine learning;macro;markov decision process;mobile robot;artificial intelligence	AI	20.96805159977	-16.230843981515758	198482
03c2eee2002b796b6ce20b968b3ae2da945be6d3	probabilistic exploration in planning while learning	learning algorithm;artificial intelligent;reinforcement learn ing;hill climbing	Sequential decision tasks with incomplete infor­ mation are characterized by the exploration prob­ lem; namely the trade-off between further exploration for learning more about the environ­ ment and immediate exploitation of the accrued information for decision-making. Within artificial intelligence, there has been an increasing interest in studying planning-while-learning algorithms for these decision tasks. In this paper we focus on the exploration problem in reinforcement learn­ ing and Q-learning in particular. The existing exploration strategies for Q-learning are of a heu­ ristic nature and they exhibit limited scaleability in tasks with large (or infinite) state and action spaces. Efficient experimentation is needed for resolving uncertainties when possible plans are compared (i.e. exploration). The experimenta­ tion should be sufficient for selecting with statis­ tical significance a locally optimal plan (i.e. exploitation). For this purpose, we develop a probabilistic hill-climbing algorithm that uses a statistical selection procedure to decide how much exploration is needed for selecting a plan which is, with arbitrarily high probability, arbi­ trarily close to a locally optimal one. Due to its generality the algorithm can be employed for the exploration strategy of robust Q-learning. An experiment on a relatively complex control task shows that the proposed exploration strategy per­ forms better than a typical exploration strategy. continuous flow of events in time. Effective decision-mak­ ing requires resolution of uncertainty as early as possible. The . te?dency to minimize losses resulting from wrong predictions of future events necessitates the division of the problem solution into steps. A decision at each step must make use of the information from the evolution of the events experienced thus far, but that evolution, in fact, depends on the type of decision made at each step.	algorithm;artificial intelligence;co-ment;exploration problem;hill climbing;local optimum;machine learning;q-learning;reinforcement learning;risk aversion;semiconductor industry	Grigoris I. Karakoulas	1995			simulation;computer science;artificial intelligence;hill climbing;machine learning	AI	20.707003312291416	-16.114955416534112	198933
